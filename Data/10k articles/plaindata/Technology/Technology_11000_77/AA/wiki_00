{"id": "10974112", "url": "https://en.wikipedia.org/wiki?curid=10974112", "title": "Alternatives to car use", "text": "Alternatives to car use\n\nCurrent technological developments suggest that cars, as used today, will be replaced. Established alternatives to car use include public transit (buses, trolleybuses, trains, subways, monorails, tramways), cycling, walking, rollerblading and skateboarding.\n\nBicycle-sharing systems have been implemented in over 1000 cities worldwide, and are especially common in many European and Chinese cities of all sizes. Similar programs have been implemented across the United States as well, including large cities like Washington, D.C., and New York City, as well as smaller cities like Buffalo, New York and Fort Collins, Colorado.\n\nPersonal rapid transit is a scheme that has been discussed, in which small, automated vehicles would run on special elevated tracks spaced within walking distance throughout a city, and could provide direct service to a chosen station without stops. However, despite several concepts existing for decades personal rapid transit has failed to gain significant ground and several prototypes and experimental systems have been dismantled as failures. Another possibility is new forms of personal transport such as the Segway PT, which could serve as an alternative to cars and bicycles if they prove to be socially accepted.\n\nAll of these alternative modes of transport pollute less than the conventional (petroleum-powered) car and contribute to transport sustainability. They also provide other significant benefits such as reduced traffic-related injuries and fatalities, reduced space requirements, both for parking and driving, reduced resource usage and pollution related to both production and driving, increased social inclusion, increased economic and social equity, and more livable streets and cities. Some alternative modes of transportation, especially cycling, also provide regular, low-impact exercise, tailored to the needs of human bodies. Public transport is also linked to increased exercise, because they are combined in a multi-modal transport chain that includes walking or cycling.\n\nAccording to the MIT Future Car Workshop, the benefits of possible future car technologies not yet in widespread use (such as zero-emissions vehicles) over these alternatives, would be:\n\n"}
{"id": "225949", "url": "https://en.wikipedia.org/wiki?curid=225949", "title": "American Conference of Governmental Industrial Hygienists", "text": "American Conference of Governmental Industrial Hygienists\n\nThe American Conference of Governmental Industrial Hygienists (ACGIH) is a professional association of industrial hygienists and practitioners of related professions, with headquarters in Cincinnati, Ohio. One of its goals is to advance worker protection by providing timely, objective, scientific information to occupational and environmental health professionals.\n\nThe National Conference of Governmental Industrial Hygienists (NCGIH) convened on June 27, 1938, in Washington, D.C. NCGIH's original constitution limited full membership to two representatives from each governmental industrial hygiene agency. Associate membership was made available to other professional personnel of the agencies holding full memberships, and also to personnel of educational institutions engaged in teaching industrial hygiene. Governmental industrial hygiene personnel of other countries were eligible for affiliated membership.\n\nThe Conference came into being with 59 members, one affiliated member, and 16 associate members. Forty-three members, one associate and six guests, attended the initial Conference. All but five of the members were from health departments. The New York and Massachusetts state labor departments had two each present, and there was one from the West Virginia state compensation commission.\n\nAt the end of World War II, many individuals were leaving governmental employment and membership in the Conference declined from the peak of 281 in 1944 to 235 in 1946. Changes due to the transition to a peace-time economy, the development of other professional associations, and changes in the technical and administrative needs of state and local agencies, required the Conference to revise its constitution and make some major changes in its organizational structure.\n\nThe 1946 constitution revisions abandoned the concept of limiting full membership to only two individuals from each governmental industrial hygiene agency. This opened the doors to all of their professional personnel to participate in the activities of the organization on an equal basis. Governmental industrial hygiene personnel from foreign countries were also given the right to full membership. These changes, among others, were to have a salutary effect on the organization which, in 1946, changed its name to the American Conference of Governmental Industrial Hygienists (ACGIH).\n\nIn the mid 1950s, steady growth in membership resumed and by 1960 there were 511 members, including 54 from other countries. During the next decade these numbers more than doubled and in 1977 the total reached 1,800, of which 166 were from outside the United States.\n\nFor over 75 years, ACGIH has been dedicated to the industrial hygiene and occupational and environmental health and safety communities. They have grown and expanded without losing sight of their original goal - to encourage the interchange of experience among industrial hygiene workers and to collect and make accessible such information and data as might be of aid to them in the proper fulfillment of their duties. This original goal is reflected in both their current mission - the advancement of occupational and environmental health - and in their tagline: Defining the Science of Occupational and Environmental Health.\n\nThis scientific information is provided to members and others in the industry through our journal, professional conferences and seminars, as well as through a vast list of technical and scientific publications, including the \"TLVs and BEIs\" book.\n\nPresently, nine ACGIH committees focus their energies on a range of topics: agricultural safety and health, air sampling instruments, bioaerosols, biological exposure indices, industrial ventilation, international, small business, chemical substance TLVs, and physical agent TLVs.\n\nThe \"Applied Occupational and Environmental Hygiene\" journal, was published from 1990 through 2003 and was formerly published as \"Applied Industrial Hygiene\" from 1986 through 1989. This ACGIH peer-reviewed journal provided scientific information and data to members until ACGIH and AIHA began publishing a joint journal.\n\nThe \"Journal of Occupational and Environmental Hygiene\" (JOEH) is a joint publication of the American Industrial Hygiene Association (AIHA) and ACGIH. JOEH is a peer-reviewed journal devoted to enhancing the knowledge and practice of occupational and environmental hygiene and safety by widely disseminating research articles and applied studies of the highest quality.\n\nPublished monthly, JOEH provides a written medium for the communication of ideas, methods, processes, and research in the areas of occupational, industrial, and environmental hygiene; exposure assessment; engineering controls; occupational and environmental epidemiology, medicine and toxicology; ergonomics; and other related disciplines.\n\nThe activities of each Committee are directed by an individual mission statement.\n\nAgricultural Safety & Health Committee\n\nMission - To promote those activities and programs necessary to our suited for agriculture or agro-business and to increase awareness of occupational health, safety. and environmental issues affecting this underserved population worldwide.\n\nAir Sampling Instruments Committee\n\nMission - To report on the availability, efficiency, use, and limitations of existing and new sampling methodology and instrumentation.\n\n\"Bioaerosols Committee\nMission - To compile and disseminate information on biologically derived contaminants that may become airborne, to develop recommendations for assessment, control, remediation, and prevention of such hazards, and to establish criteria for bioaerosol exposure limits.\n\n\"Biological Exposure Indices Committee\nMission - To develop occupational biological exposure guidelines that are scientifically valid and supported by professional judgment, up-to-date, well-documented, understandable and clear, and produced by a clearly-defined process that is balanced and free of conflict of interest.\n\nIndustrial Ventilation\n\nMission - To provide a safe and healthful environment by integrating state-of-the-art information from government and industry sources, using them to develop and recommend ventilation and other engineering controls to capture, collect, filter, and remove airborne contaminants from the workplace.\n\n\"International Committee\n\nMission - To help develop and support ACGIH's international initiatives.\n\nSmall Business Committee\n\nMission - To develop and disseminate practical information that operators of small businesses and their employees can apply to the recognition, evaluation, and control of workplace hazards; and to assist and education safety and health professionals in working with small business concerns.\n\n\"Threshold Limit Values for Chemicals Substances Committee\n\nMission - To recommend airborne concentrations of agents and exposure conditions for use in the practice of industrial hygiene and by other qualified professionals to protect worker health.\n\n\"Threshold Limit Values for Physical Agents Committee\nMission - To develop and disseminate occupational exposure guidelines that are evidence-based, scientifically valid, and rigorously review.\n\nACGIH establishes the Threshold Limit Values (\"TLVs\") for chemical substances and physical agents and Biological Exposure Indices (\"BEIs\").\n\nThe Threshold Limit Values for Chemical Substances (TLV-CS) Committee was established in 1941. This group was charged with investigating, recommending, and annually reviewing exposure limits for chemical substances. It became a standing committee in 1944. Two years later, the organization adopted its first list of 148 exposure limits, then referred to as Maximum Allowable Concentrations. The term \"Threshold Limit Values (TLV)\" was introduced in 1956. The first list of Threshold Limit Values and Biological Exposure Indices (the \"TLVs and BEIs\" book) was published in 1962\".\" A new edition is now published annually. Today's list of TLVs and BEIs includes over 600 chemical substances and physical agents, as well as over 30 Biological Exposure Indices for selected chemicals.\n\nThe TLVs and BEIs are developed as guidelines to assist in the control of health hazards. These recommendations or guidelines are intended for use in the practice of industrial hygiene, to be interpreted and applied only by a person trained in this discipline.\n\nIn certain circumstances, individuals or organizations may wish to make use of these recommendations or guidelines if the use of TLVs and BEIs contributes to the overall improvement in worker protection.\n"}
{"id": "2642171", "url": "https://en.wikipedia.org/wiki?curid=2642171", "title": "Audio electronics", "text": "Audio electronics\n\nAudio electronics is the implementation of electronic circuit designs to perform conversions of sound/pressure wave signals to electrical signals, or vice versa. Electronic circuits considered a part of audio electronics may also be designed to achieve certain signal processing operations, in order to make particular alterations to the signal while it is in the electrical form. Additionally, audio signals can be created synthetically through the generation of electric signals from electronic devices. Audio Electronics were traditionally designed with analog electric circuit techniques until advances in digital technologies were developed. Moreover, digital signals are able to be manipulated by computer software much the same way audio electronic devices would, due to its compatible digital nature. Both analog and digital design formats are still used today, and the use of one or the other largely depends on the application. The following is a partial list of audio-related circuits/techniques/devices:\n\n\n\nMainstream:\n\nNiche markets:\n\n"}
{"id": "509950", "url": "https://en.wikipedia.org/wiki?curid=509950", "title": "Beat frequency oscillator", "text": "Beat frequency oscillator\n\nIn a radio receiver, a beat frequency oscillator or BFO is a dedicated oscillator used to create an audio frequency signal from Morse code radiotelegraphy (CW) transmissions to make them audible. The signal from the BFO is mixed with the received signal to create a heterodyne or beat frequency which is heard as a tone in the speaker. BFOs are also used to demodulate single-sideband (SSB) signals, making them intelligible, by essentially restoring the carrier that was suppressed at the transmitter. BFOs are sometimes included in communications receivers designed for short wave listeners; they are almost always fitted to amateur radio station receivers, which often receive CW and SSB signals. \n\nThe beat frequency oscillator was invented in 1901 by Canadian engineer Reginald Fessenden. What he called the \"heterodyne\" receiver was the first application of the heterodyne principle.\n\nIn continuous wave (CW) radio transmission, also called radiotelegraphy or on-off keying and designated by the International Telecommunication Union as emission type A1A, information is transmitted by pulses of unmodulated radio carrier wave which spell out text messages in Morse code. The different length pulses of carrier, called \"dots\" and \"dashes\" or \"dits\" and \"dahs\", are produced by the operator switching the transmitter on and off rapidly using a switch called a telegraph key. This was the first type of radio transmission, and during the early 20th century was widely used for private person-to-person messages by amateurs and commercial telegram traffic. With the rise of other types of modulation its use has declined, and CW is now only used for personal hobbyist messages by radio amateurs and is becoming obsolete. \n\nSince the pulses of carrier have no audio modulation, a CW signal received by an AM radio receiver simply sounds like silence. In order to make the carrier pulses audible in the receiver, a beat frequency oscillator is used. The BFO is a radio frequency electronic oscillator that generates a constant sine wave at a frequency \"f\" that is offset from the intermediate frequency \"f\" of the receiver. This signal is mixed with the IF before the receiver's second detector (demodulator). In the detector the two frequencies add and subtract, and a beat frequency (heterodyne) in the audio range results at the difference between them: \"f\" = |\"f\" - \"f\"| which sounds like a tone in the receiver's speaker. During the pulses of carrier, the beat frequency is generated, while between the pulses there is no carrier so no tone is produced. Thus the BFO makes the \"dots\" and \"dashes\" of the Morse code signal audible, sounding like different length \"beeps\" in the speaker. A listener who knows Morse code can decode this signal to get the text message.\n\nThe first BFOs, used in early tuned radio frequency (TRF) receivers in the 1910s-1920s, beat with the carrier frequency of the station. Each time the radio was tuned to a different station frequency, the BFO frequency had to be changed also, so the BFO oscillator had to be tunable across the entire frequency band covered by the receiver.\n\nSince in a superheterodyne receiver the different frequencies of the different stations are all translated to the same intermediate frequency (IF) by the mixer, modern BFOs which beat with the IF need only have a constant frequency. There may be a switch to turn off the BFO when it is not needed, when receiving other types of signals, such as AM or FM. There is also usually a knob on the front panel to adjust the frequency of the BFO, to change the tone over a small range to suit the operator's preference.\n\nA receiver is tuned to a Morse code signal, and the receiver's intermediate frequency (IF) is \"f\" = 45000 Hz. That means the dits and dahs have become pulses of a 45000 Hz signal, which is inaudible.\n\nTo make them audible, the frequency needs to be shifted into the audio range, for instance \"f\" = 1000 Hz. To achieve that, the desired BFO frequency is \"f\"\" = 44000 or 46000 Hz. \nWhen the signal at frequency \"f\" is mixed with the BFO frequency in the detector stage of the receiver, this creates two other frequencies or heterodynes: |\"f\" − \"f\"|, and |\"f\" + \"f\"|. The \"difference frequency\", \"f\" = |\"f\" − \"f\"| = 1000 Hz, is also known as the beat frequency.\n\nThe other, the \"sum frequency\", \"(F + F)\" = 89000 Hz, is unneeded. It can be removed by a lowpass filter, such as the radio's speaker, which cannot vibrate at such a high frequency.\n\n\"f\" = 46000 Hz also produces the desired 1000 Hz beat frequency and could be used instead.\n\nBy varying the BFO frequency around 44000 Hz, the listener can vary the output audio frequency; this is useful to correct for small differences between the tuning of the transmitter and the receiver, particularly useful when tuning in single sideband (SSB) voice. The waveform produced by the BFO \"beats\" against the IF signal in the mixer stage of the receiver. Any drift of the local oscillator or the beat-frequency oscillator will affect the pitch of the received audio, so stable oscillators are used.\n\nFor a radio signal like SSB with wider bandwidth than Morse code, low-side injection preserves the relative order of the frequency components. High-side injection reverses their order, which is often desirable to counteract a previous reversal in the radio receiver. For radio amateurs it is conventional practice to use lower sideband below 10 MHz and upper sideband above 10 MHz, however the 60 metre band is different, using upper sideband. Amateurs have the band as secondary users and that practice retains compatibility with the primary users.\n\n"}
{"id": "10910504", "url": "https://en.wikipedia.org/wiki?curid=10910504", "title": "Black Box Corporation", "text": "Black Box Corporation\n\nBlack Box Corporation, also doing business as Black Box Network Services, is headquartered in the Pittsburgh suburb of Lawrence, Pennsylvania, United States. The company is a provider of communications products.\n\nThe company has a long history. It was founded as Expandor, Inc. by Eugene Yost and Richard \"Dick\" Raub, and offered printer switches popularly called \"black boxes\", It published a popular \"Black Box Catalog\". the basis of changing its name to Black Box Corporation in 1982.\n\nAnother division, Interlan, sold local area network equipment.\n\nAfter a slow-down in business in the mid-1980s and Black Monday of 1987, Odyssey Partners acquired the company in 1988, through a leveraged buyout. \nA proposed sale was met with a lawsuit, although the Interlan division was sold to Racal in 1989. Black Box was re-organized in 1990 after losses from the debt servicing. Despite getting $5.5 million in fees for the deal, Drexel Burnham Lambert filed for its own bankruptcy the same year. The no-action letter from the United States Securities and Exchange Commission in June 1990 has been used as a legal precedent for similar cases.\n\nThe profitable catalog sales business moved from Simi Valley in California to Lawrence, Pennsylvania and changed its name to Black Box Incorporated.\n\nThe telecommunications product business was split out to a subsidiary called Micom Communications Corporation. An initial public offering was made in December 1992 to cover the debt used to finance the 1990 deal (after two previous failed attempts), under the name MB Communications.\n\nIn 1994, the Micom Communications unit was spun off, and acquired by Northern Telecom (Nortel) in June 1996.\n\nToday, Black Box continues to operate as an worldwide telecommunications provider.\n\nhttps://www.blackbox.com\n"}
{"id": "42840352", "url": "https://en.wikipedia.org/wiki?curid=42840352", "title": "Brainly", "text": "Brainly\n\nBrainly is an educational technology company based in New York City, New York, United States. It is owned by Polish company Brainly.pl.\n\nIt operates a group of social learning networks for students and educators. Brainly advocates to share and explore knowledge in a collaborative manner, and engaging in peer-to-peer educational assistance. The network has elements of gamification in the form of motivational points and ranks, and encourages users to engage in the online community by asking questions and answering those of other students.\n\nInitially called Zadane.pl, the company was founded in 2009 in Poland by Michał Borkowski (current chief executive officer), Tomasz Kraus and Łukasz Haluch. The first million unique users monthly was achieved within 6 months after the release. In January 2011, the company founded Znanija.com, the first international project dedicated to Russian language speakers. Several other versions in multiple languages for the following markets included Turkey (eodev.com), Latin America and Spain (brainly.lat) and Brazil (brainly.com.br). In December 2013, seven new language versions of Brainly were released, include the English language (brainly.com), Indonesian (brainly.co.id), Indian (brainly.in), Filipino (brainly.ph), Thai language (brainly-thailand.com), Romanian (brainly.ro) and Italian (brainly.it) sites.\nBrainly was initially funded by the co-founders, but then raised funds from Point Nine Capital\nIn October 2014, the company announced that it had raised another round of funding from General Catalyst Partners, Runa Capital and other venture capital firms. The total amount of the investment was $9 million and allowed further product development, as well as the opening of the US-based headquarters in New York City.\n\nIn May 2016, another funding round of $18 million of combined debt and equity was disclosed.\nIn June 2016, Brainly acquired the US-based OpenStudy.\n\nIn October 2017, Brainly raised $14 million in the funding round led by Kulczyk Investments. The total funds raised by the company since its establishment, is reported to be $38.5 million. In January 2018, Brainly announced it had acquired the video education start-up, Bask, to bring video technology to the Brainly platform.\n\nBrainly provides questions and answers for students looking for help with homework-related tasks. Students post questions to the community, who offer help in return. Users may post comments to every question and answer and can freely collaborate on problems. All questions are categorized by subject, respective of country and school level.\n\nEach user is given a fixed amount points upon registration, which they can use to ask questions. Points are mainly earned by answering questions, however there are specific activities, which can earn a user additional points. Each respondent is awarded half of the points given by the inquirer for each answer. Additionally, the author of the question gets the chance to choose one of the answers as the best (for the \"Brainliest\" award), which awards both the authors of the question and the answer additional points. The website also features rankings (daily, weekly, monthly and quarterly) of users with the highest number of points gained by answering questions respectively to the correct category chosen on the leaderboards. (“Brainliest Users”).\n"}
{"id": "42325019", "url": "https://en.wikipedia.org/wiki?curid=42325019", "title": "CALUX", "text": "CALUX\n\nChemical Activated LUciferase gene eXpression (CALUX) is a bioassay used in the detection of specific chemicals in samples. It consists of a modified cell line that has a DNA construct with a luciferase reporter gene in conjunction with response elements that can induce transcription of the inserted gene for the light-generating enzyme. The response elements can be varied in order to provide binding sites for other receptors that relate to a chemical of interest that is wished to be detected. The CALUX bioassay has thus been redesigned to detect certain chemicals of interest. Most applications have been oriented in detecting environmentally harmful chemicals like environmental hormones.\n\nCALUX is an effect based screening method as it measures the total effect ligands (from a sample) have on a receptor. Unlike chemical analysis, CALUX is thus able to measure all activity on the receptor of interest. This also includes unidentified congeners of certain ligands.\n\nDioxin Responsive (DR)CALUX is a bioassay used in the detection of dioxins and dioxin-like compounds. It is based on the mechanisms of the AhR-pathway.\nChronic activation of the AhR-pathway by these compounds has been shown to cause cancer in the predominantly the liver and can cause developmental defects in vertebrates. DR CALUX is used by companies that want to screen for dioxins and dioxin-like compounds in order to guarantee food safety of their products. Dioxins are considered to be the most toxic man-made chemical.\n\n \n"}
{"id": "6516374", "url": "https://en.wikipedia.org/wiki?curid=6516374", "title": "Ceramic water filter", "text": "Ceramic water filter\n\nCeramic water filters are an inexpensive and effective type of water filter, that rely on the small pore size of ceramic material to filter dirt, debris, and bacteria out of water. This makes them ideal for use in developing countries, and portable ceramic filters are commonly used in backpacking.\n\nSimilar to other methods of filtering water, the filter removes particles larger than the size of the pores in the filter material. Typically bacteria, protozoa, and microbial cysts are removed. However, filters are typically not effective against viruses since they are small enough to pass through to the \"clean\" side of the filter. Ceramic water filters (CWF) may be treated with silver in a form that will not leach away. The silver helps to kill or incapacitate bacteria and prevent the growth of mold and algae in the body of the filter.\n\nCeramic filtration does not remove chemical contaminants, \"per se\". However, some manufacturers (especially of ceramic candle filters) incorporate a high-performance activated carbon core inside the ceramic filter cartridge that reduces organic and metallic contaminants. The active carbon absorbs compounds such as chlorine. Filters with active carbon need to be replaced periodically because the carbon becomes clogged with foreign material.\n\nThe two most common types of ceramic water filter are pot-type and candle-type filters. Ceramic filter systems consist of a porous ceramic filter that is attached to, or sits on top of a plastic or ceramic receptacle. Contaminated water is poured into a top container. It passes through the filter(s) into the receptacle below. The lower receptacle usually is fitted with a tap.\n\nContaminants larger than the minute holes of the ceramic structure will remain in the top half of the unit. The filter(s) can be cleaned by brushing them with a soft brush and rinsing them with clean water. Hot water and soap can also be used.\n\nIn stationary use, ceramic candles have mechanical, operational and manufacturing advantages over simple inserts and pots. Filter candles allow sturdy metal and plastic receptacles to be used, which decreases the likelihood of a sanitary failure. Since their filter area is independent of the size of the attachment joint, there is less leakage than other geometries of replaceable filter, and more-expensive, higher-quality gaskets can be used. Since they are protected by the upper receptacle, rather than forming it, they are less likely to be damaged in ordinary use. They are easier to sanitize, because the sanitary side is inside the candle. The nonsanitary part is outside, where it is easy to clean. They fit more types of receptacles and applications than simple pots, and attach to a simple hole in a receptacle. They also can be replaced without replacing the entire upper receptacle, and larger receptacles can simply use more filter candles, permitting filter manufacture to be standardized. If a filter in a multifilter receptacle is found to be broken, the filter hole can be plugged, and use can continue with fewer filters and a longer refill-time until a replacement can be obtained. Also, standardizing the filter makes it economical to keep one or a few filters on hand.\n\nThere are also portable ceramic filters, such as the MSR Miniworks, which work via manual pumping, and in-line ceramic filters, which filters drinking water that comes through household plumbing. Cleaning these filters is the same as with the clay pot filter but also allows for reverse-flow cleaning, wherein clean water is forced through the filter backwards, pushing any contaminants out of the ceramic pores.\n\nThe major risks to the success of all forms of ceramic filtration are hairline cracks and cross-contamination. If the unit is dropped or otherwise abused, the brittle nature of ceramic materials can allow fine, hard to see cracks, and can allow larger contaminants through the filter. Work is being done to modify clay/sawdust ratios during manufacture to improve the brittle nature and fracture toughness of these clay ceramic water filter materials. \nAlso, if the \"clean\" water side of the ceramic membrane is brought into contact with dirty water, hands, cleaning cloths, etc., then the filtration will be ineffective. If such contact occurs, the clean side of the filter should be thoroughly sterilized before reuse.\n\nHenry Doulton invented the modern form of ceramic candle sanitary water filter in 1827. In 1835, Queen Victoria commissioned him to produce such a device for her personal use. By 1846, Doulton ceramics was widely recognized as a premier manufacturer of an effective prevention device for treating infective water. In 1887, Doulton was knighted, in part for his work with water filters. Louis Pasteur's research concerning bacteria also had provided a demonstrable reason for the filters' effect. Doulton's original organization for water filters remains in existence, although it has been sold and renamed several times. \"Doulton\" is currently (2013) a registered trademark of Fairey Ceramics.\n\nSeveral universities including MIT; Universities of Colorado; Princeton University; University of Wisconsin-Milwaukee; The Ohio State University; Universities of Tulane, West Virginia, North Carolina in the US; University of Delft, Strathclyde in Europe, USAID, UNICEF, Zamorano University in Honduras, Rafeal Landivar University in Guatemala, Earth University, Institute of Hydraulic resources, the Red Cross, Engineers Without Borders, United Nations, countries in Africa like Nigeria, Ghana, Burkina Faso, Kenya, etc. and countries in Asia like Nepal, Bangladesh, Cambodia, Sri Lanka, India, Vietnam, etc. and NGOs are supporting the expansion of the use of ceramic filters in drinking water development initiatives; most commonly, in the form of clay pot filters.\n\nMr. Fernando Mazariegos of Guatemala was responsible for developing Ceramic Pot Filter technology in 1981 while Director of Water Research at the Central American Research Institute in Guatemala City. He is currently the Director of Research and Development at Ecofiltro in Antigua, Guatemala. Ron Rivera studied under Mr. Mazariegos of Guatemala and was a key proponent and innovator in the field as part of the group to take the ceramic frustum shaped(pot) filter across international borders and helped developing nations to provide cheap high quality potable water. Ron Rivera also worked with Potters for Peace worldwide for the good and benefit of clay workers in developing nations to sustain their businesses \n\nThe latest development is in India, NGOs such as Enactus IIT Madras, Rupayan Sansthan, Sehgal Foundation are supporting the expansion and use of indigenized frustum shaped ceramic water filters locally named G filter for drinking water development initiatives in Tamil Nadu, Rajasthan, Bihar,and other states.\n\n"}
{"id": "4604136", "url": "https://en.wikipedia.org/wiki?curid=4604136", "title": "Commutation cell", "text": "Commutation cell\n\nThe commutation cell is the basic structure in power electronics. It is composed of an electronic switch (today a high-power semiconductor, not a mechanical switch) and a diode. It was traditionally referred to as a chopper, but since switching power supplies became a major form of power conversion, this new term has become more popular.\n\nThe purpose of the commutation cell is to \"chop\" DC power into square wave alternating current. This is done so that an inductor and a capacitor can be used in an LC circuit to change the voltage. This is in theory a lossless process, and in practice efficiencies above 80-90% are routinely achieved. The output is then usually run through a filter to produce clean DC power. By controlling the on and off times (the duty cycle) of the switch in the commutation cell, the output voltage can be regulated.\n\nThis basic principle is the core of most modern power supplies, from tiny DC-DC converters in portable devices to huge switching stations for high voltage DC power transmission.\n\nA Commutation cell connects two power elements, often referred to as sources, although they can either produce or absorb power.\n\nSome requirements to connect power sources exist. The impossible configurations are listed in figure 1. They are basically:\n\nThis applies to classical sources (battery, generator), but also to capacitors and inductors: At a small time scale, a capacitor is identical to a voltage source, and an inductor to a current source. Connecting two capacitors with different voltage level in parallel therefore corresponds to connecting two voltage sources, one of the forbidden connections of figure 1.\n\nThe figure 2 illustrates the poor efficiency of such connection. One capacitor is charged to a voltage V, and is connected to a capacitor with the same capacity, but discharged.\n\nBefore the connection, the energy in the circuit is formula_1, and the quantity of charges Q is equal to formula_2, where U is the potential energy.\n\nAfter the connection has been made, the quantity of charges is constant, and the total capacitance is formula_3. Therefore, the voltage across the capacitances is formula_4. The energy in the circuit is then formula_5. Therefore, half of the energy has been dissipated during the connection.\n\nThe same applies with the connections in series of two inductances. The magnetic flux (formula_6) remains constant before and after the commutation. As the total inductance after the commutation is 2L, the current becomes formula_7 (see figure 2). The energy before the commutation is formula_8. After, it is formula_9. Here again, half of the energy is dissipated during the commutation.\n\nAs a result, it can be seen that a commutation cell can only connect a voltage source to a current source (and vice versa). However, using inductors and capacitors, it is possible to transform the behaviour of a source: for example two voltage sources can be connected through a converter if it uses an inductor to transfer energy.\n\nAs told above, a commutation cell must be placed between a voltage source and a current source. Depending on the state of the cell, both sources are either connected, or isolated. When isolated, the current source must be shorted, as it is impossible for a current to be created in an open circuit. The basic schematic of a commutation cell is therefore given in figure 3 (top). It uses two switches with opposite states: In the configuration depicted in figure 3, both sources are isolated, and the current source is shorted. When the top switch is on (and the bottom switch is off), both sources are connected.\n\nIn reality, it is impossible to have a perfect synchronization between the switches. At one point during the commutation, they would be either both on (thus shorting the voltage source) or off (thus leaving the current source in an open circuit). This is why one of the switches has to be replaced by a diode. A diode is a natural commutation device, i.e. its state is controlled by the circuit itself. It will turn on or off at the exact moment it has to. The consequence of using a diode in a commutation cell is that it makes it unidirectional (see figure 3). A bidirectional cell can be built, but it is basically equivalent to two unidirectional cells connected in parallel.\n\nThe commutation cell can be found in any power electronic converter. Some examples are given in figure 4. As can be seen, a \"current source\" (actually a loop that contains an inductance) is always connected between the middle point and one of the external connections of the commutation cell, while a voltage source (or a capacitor, or a connection in series of voltage source and capacitor) is always connected to the two external connections.\n\n\n"}
{"id": "4106859", "url": "https://en.wikipedia.org/wiki?curid=4106859", "title": "Constant altitude plan position indicator", "text": "Constant altitude plan position indicator\n\nThe constant altitude plan position indicator, better known as CAPPI, is a radar display which gives a horizontal cross-section of data at constant altitude. It has been developed by McGill University in Montreal by the \"Stormy Weather Group\" to circumvent some problems with the PPI:\n\n\nIn 1954, McGill University obtained a new radar (CPS-9) which had a better resolution and used FASE (Fast Azimuth Slow Elevation) to program multi-angle soundings of the atmosphere.\n\nIn 1957, Langleben and Gaherty developed a scheme with FASE to keep only the data at a certain height at each angle and scan on 360 degrees. If we look at the diagram, each angle of elevation or PPI has data at height X at a certain distance from the radar. Using the data at the right distance, one forms an annular ring of data at height X. Assembling all the rings coming from the different angles gives you the CAPPI.\n\nThe CAPPI is composed of data from each angle that is at the height requested for the cross-section (bold lines in zig-zag on the left diagram). In the early days, the scan data collected where shown directly on the cathodic screen and a photo sensitive device captured each ring as it was completed. Then all those photographed rings were assembled. By 1958, East developed a real time assembly instead of a delayed one. By the mid 1970s, computer developments made it possible to gather data in electronic form and make CAPPIs more easily.\n\nToday, weather radars collect in real-time data on a large number of angles. Many countries such as Canada, UK and Australia, scan a large enough number of angles with their radars to have an almost continuous vertical view (taking into account the radar beam width) and produce CAPPIs. Other countries, like France and United States, use fewer angles and prefer PPIs or composite of maximum reflectivities above a point.\n\nAbove right is an example of a CAPPI at 1.5 km altitude. Looking at the diagram of angles, that depending on the height of the CAPPI, there comes a distance where no data is available. The portion beyond this distance on a CAPPI is then showing data from the lowest PPI. The higher is the CAPPI above ground, the smaller is that PPI zone.\n\n"}
{"id": "7739419", "url": "https://en.wikipedia.org/wiki?curid=7739419", "title": "Control track", "text": "Control track\n\nA control track is a track that runs along an outside edge of a standard analog videotape (including VHS). The control track encodes a series of pulses, each pulse corresponding to the beginning of each frame. This allows the video tape player to synchronize its scan speed and tape speed to the speed of the recording. Thus, the recorded control track defines the speed of playback (e.g. SP, LP, EP, etc.), and it is also what drives the relative counter clock that most VCRs have.\n\nThe control track is used to fine-tune the tape speed during playback, so that the high speed rotating heads remained exactly on their helical tracks rather than somewhere between two adjacent tracks (known as \"tracking\"). Since good tracking depends on precise distances between the rotating drum and the fixed control/audio head reading the linear tracks, which usually varies by a couple of micrometers between machines due to manufacturing tolerances, most VCRs offer tracking adjustment, either manual or automatic, to correct such mismatches.\n\nThe control track is also used to hold \"index marks\", which were normally written at the beginning of each recording session, and can be found using the VCR's \"index search\" function: this will fast-wind forward or backward to the \"n\"th specified index mark, and resume playback from there. At times, higher-end VCRs provided functions for the user to manually add and remove these marks — so that, for example, they coincide with the actual start of the television program — but this feature later became hard to find.\n\nBy the late 1990s, some high-end VCRs offered more sophisticated indexing. For example, Panasonic's Tape Library system assigned an ID number to each cassette, and logged recording information (channel, date, time and optional program title entered by the user) both on the cassette and in the VCR's memory for up to 900 recordings (600 with titles).\n\nA gap in the control track signal of a videotape usually causes a flaw during playback, as the player may lose tracking as soon as the track drops out. This usually leads to a gap in video and audio (since there's no information as to what speed the media is recorded at), the VCR's counter will freeze, and upon re-acquiring the control track, the video sync will be momentarily lost.\n\nProblems with control track signals are almost always caused by a gap in recording, e.g. a recording was resumed \"beyond\" (rather than \"at\") the point where recording was previously stopped. A control track error can also be caused by a \"dropout\" - a defect in the magnetic particles on a tape - but this is very uncommon, as the control track is immune to all but the most severe tape damage.\n\nA discontinuous control track was (and still is) especially problematic for analog videotape editing. Each gap in the control track, no matter how brief, destroys synchronization and continuity, which can make a segment of a recorded video difficult or impossible to use for editing.\n\nThe acronym \"LOCT\" (pronounced \"locked\"), used by many video professionals, can mean \"Loss Of Control Track\" and/or \"Loss Of Continuous Timecode\". Though either type of \"loss\" can pose similar problems, and both issues are sometimes directly related, control track and timecode are two different things (see timecode).\n\nControl track problems cannot be \"repaired\". You can duplicate the tape to restore control track continuity, but this will not fix visual and audible \"glitches\" caused by gaps in the original control track, and the video itself will suffer quality degradation due to \"generation loss\".\n\nFor this reason, many video professionals would \"pre-stripe\" every analog videotape to be used in the field or studio beforehand to help ensure good control track throughout. This was done by pre-recording black to the entire videotape. Pre-striping also facilitated spot-checking for dropouts on the video portion of the tape (usually seen as brief white horizontal \"dashes\") before use.\n\n"}
{"id": "158015", "url": "https://en.wikipedia.org/wiki?curid=158015", "title": "Cosmetics", "text": "Cosmetics\n\nCosmetics are substances or products used to enhance or alter the appearance of the face or fragrance and texture of the body. Many cosmetics are designed for use of applying to the face, hair, and body. They are generally mixtures of chemical compounds; some being derived from natural sources (such as coconut oil), and some being synthetics or artificial. Cosmetics applied to the face to enhance its appearance are often called make-up or makeup. Common make-up items include: lipstick, mascara, eye shadow, foundation. Whereas other common cosmetics can include skin cleansers and body lotions, shampoo and conditioner, hairstyling products (gel, hair spray, etc.), perfume and cologne.\n\nIn the U.S., the Food and Drug Administration (FDA), which regulates cosmetics, defines cosmetics as \"intended to be applied to the human body for cleansing, beautifying, promoting attractiveness, or altering the appearance without affecting the body's structure or functions\". This broad definition includes any material intended for use as a component of a cosmetic product. The FDA specifically excludes soap from this category.\n\nThe word \"cosmetics\" derives from the Greek κοσμητικὴ τέχνη (\"kosmetikē tekhnē\"), meaning \"technique of dress and ornament\", from κοσμητικός (\"kosmētikos\"), \"skilled in ordering or arranging\" and that from κόσμος (\"kosmos\"), meaning amongst others \"order\" and \"ornament\".\n\nCosmetics have been in use for thousands of years. The absence of regulation of the manufacture and use of cosmetics has led to negative side effects, deformities, blindness, and even death through the ages. Examples are the prevalent use of ceruse (white lead), to cover the face during the Renaissance, and blindness caused by the mascara Lash Lure during the early 20th century.\n\nEgyptian men and women used makeup to enhance their appearance. They were very fond of eyeliner and eye-shadows in dark colors including blue, red, and black. Ancient Sumerian men and women were possibly the first to invent and wear lipstick, about 5,000 years ago. They crushed gemstones and used them to decorate their faces, mainly on the lips and around the eyes. Also around 3000 BC to 1500 BC, women in the ancient Indus Valley Civilization applied red tinted lipstick to their lips for face decoration. Ancient Egyptians extracted red dye from fucus-algin, 0.01% iodine, and some bromine mannite, but this dye resulted in serious illness. Lipsticks with shimmering effects were initially made using a pearlescent substance found in fish scales. Six thousand year old relics of the hollowed out tombs of the Ancient Egyptian pharaohs are discovered.\n\nAccording to one source, early major developments include:\nThe Ancient Greeks also used cosmetics as the Ancient Romans did. Cosmetics are mentioned in the Old Testament, such as in 2 Kings 9:30, where Jezebel painted her eyelids—approximately 840 BC—and in the book of Esther, where beauty treatments are described.\n\nOne of the most popular traditional Chinese medicines is the fungus \"Tremella fuciformis\", used as a beauty product by women in China and Japan. The fungus reportedly increases moisture retention in the skin and prevents senile degradation of micro-blood vessels in the skin, reducing wrinkles and smoothing fine lines. Other anti-aging effects come from increasing the presence of superoxide dismutase in the brain and liver; it is an enzyme that acts as a potent antioxidant throughout the body, particularly in the skin. Tremella fuciformis is also known in Chinese medicine for nourishing the lungs.\n\nIn the Middle Ages, it seemed completely natural that the face should be whitened and the cheeks rouged.\n\nDuring the sixteenth century, the personal attributes of the women who used make-up created a demand for the product among the upper class.\n\nCosmetic use was frowned upon at many points in Western history. For example, in the 19th century, Queen Victoria publicly declared make-up improper, vulgar, and acceptable only for use by actors.\n\nMany women in the 19th century liked to be thought of as fragile ladies. They compared themselves to delicate flowers and emphasized their delicacy and femininity. They aimed always to look pale and interesting. Sometimes ladies discreetly used a little rouge on the cheeks and used \"belladonna\" to dilate their eyes so it would make them stand out more. Make-up was frowned upon in general, especially during the 1870s when social etiquette became more rigid. Teachers and clergywomen specifically were forbidden from the use of cosmetic products.\n\nDuring the 19th century, there was a high number of incidences of lead-poisoning because of the fashion for red and white lead makeup and powder. This led to swelling and inflammation of the eyes, weakened tooth enamel, and caused the skin to blacken. Heavy use was known to lead to death. However, in the second part of the 19th century, great advances were made in chemistry from the chemical fragrances that enabled a much easier production of cosmetic products.\n\nIt was socially acceptable for actresses in the 1800s to use makeup, and famous beauties such as Sarah Bernhardt and Lillie Langtry could be powdered. Most cosmetic products available were still either chemically dubious or found in the kitchen amid food coloring, berries and beetroot.\n\nBy the middle of the 20th century, cosmetics were in widespread use by women in nearly all industrial societies around the world.\n\nIn 1968 at the feminist Miss America protest, protestors symbolically threw a number of feminine products into a \"Freedom Trash Can.\" This included cosmetics, which were among items the protestors called \"instruments of female torture\" and accouterments of what they perceived to be enforced femininity.\n\nAs of 2016, the world's largest cosmetics company is L'Oréal, which was founded by Eugène Schueller in 1909 as the French Harmless Hair Colouring Company (now owned by Liliane Bettencourt 26% and Nestlé 28%; the remaining 46% is traded publicly). The market was developed in the US during the 1910s by Elizabeth Arden, Helena Rubinstein, and Max Factor. These firms were joined by Revlon just before World War II and Estée Lauder just after.\n\nAlthough modern make-up has been traditionally used mainly by women, an increasing number of men are using cosmetics usually associated to women to enhance or cover their own facial features such as blemishes, dark circles, and so on. Cosmetics brands release products specially tailored for men, and men are increasingly using them.\n\nCosmetics are intended to be applied externally. They include, but are not limited to, products that can be applied to the face: skin-care creams, lipsticks, eye and facial makeup, towelettes, and colored contact lenses; to the body: deodorants, lotions, powders, perfumes, baby products, bath oils, bubble baths, bath salts, and body butters; to the hands/nails: fingernail and toe nail polish, and hand sanitizer; to the hair: permanent chemicals, hair colors, hair sprays, and gels.\n\nA subset of cosmetics is called \"makeup\", refers primarily to products containing color pigments that are intended to alter the user's appearance. Manufacturers may distinguish between \"decorative\" and \"care\" cosmetics.\n\nCosmetics that are meant to be used on the face and eye area are usually applied with a brush, a makeup sponge, or the fingertips.\n\nMost cosmetics are distinguished by the area of the body intended for application.\n. Some formulations are intended only for the eye or only for the face. This product can also be used for contouring the face like ones nose, cheekbones, and jaw line to add a more defined look to the total face.\n\nCosmetics can be also described by the physical composition of the product. Cosmetics can be liquid or cream emulsions; powders, both pressed and loose; dispersions; and anhydrous creams or sticks.\n\nMakeup remover is a product used to remove the makeup products applied on the skin. It cleans the skin before other procedures, like applying bedtime lotion. Micellar waters are becoming a more common product used to remove makeup. It acts as a two-in-one by removing the makeup and cleansing the skin.\n\nCleansing is a standard step in skin care routines. Skin cleaning include some or all of these steps or cosmetics:\n\n\nAbrasive exfoliants include gels, creams or lotions, as well as physical objects. Loofahs, microfiber cloths, natural sponges, or brushes may be used to exfoliate skin, simply by rubbing them over the face in a circular motion. Gels, creams, or lotions may contain an acid to encourage dead skin cells to loosen, and an abrasive such as microbeads, sea salt, sugar, ground nut shells, rice bran, or ground apricot kernels to scrub the dead cells off the skin. Salt and sugar scrubs tend to be the harshest, while scrubs containing beads or rice bran are typically very gentle.\n\n\nA makeup brush is used to apply makeup onto the face. There are two types of makeup brushes: synthetic and natural. Synthetic brushes are best for cream products while natural brushes are ideal for powder products. Using the appropriate brush to apply a certain product allows the product to blend into the skin smoothly and evenly.\n\n\nThere are two categories of personal care products. The Federal Food, Drug and Cosmetic Act defines cosmetics as products intended to cleanse or beautify (for instance, shampoos and lipstick). A separate category exists for medications, which are intended to diagnose, cure, mitigate, treat, or prevent disease, or to affect the structure or function of the body (for instance, sunscreens and acne creams). Some products, such as moisturizing sunscreens and anti-dandruff shampoos, are regulated within both categories. There are also many types of tools used such as makeup brushes or face sponges, also known as the Beauty Blender The Beauty Blender is supposed to be run underwater to become dampened and then can be used to apply foundation, blend concealer, and apply powder or highlighter.\n\nA variety of organic compounds and inorganic compounds comprise typical cosmetics. Typical organic compounds are modified natural oils and fats as well as a variety of petrochemically derived agents. Inorganic compounds are processed minerals such as iron oxides, talc, and zinc oxide. The oxides of zinc and iron are classified as pigments, i.e. colorants that have no solubility in solvents.\n\nHandmade and certified organic products are becoming more mainstream, due to the fact that certain chemicals in some skincare products may be harmful if absorbed through the skin. Products claimed to be organic should, in the U.S., be certified \"USDA Organic\".\n\nThe term \"mineral makeup\" applies to a category of face makeup, including foundation, eye shadow, blush, and bronzer, made with loose, dry mineral powders. These powders are often mixed with oil-water emulsions. Lipsticks, liquid foundations, and other liquid cosmetics, as well as compressed makeups such as eye shadow and blush in compacts, are often called mineral makeup if they have the same primary ingredients as dry mineral makeups. However, liquid makeups must contain preservatives and compressed makeups must contain binders, which dry mineral makeups do not. Mineral makeup usually does not contain synthetic fragrances, preservatives, parabens, mineral oil, and chemical dyes. For this reason, dermatologists may consider mineral makeup to be gentler to the skin than makeup that contains those ingredients. Some minerals are nacreous or pearlescent, giving the skin a shining or sparking appearance. One example is bismuth oxychloride. There are various mineral-based makeup brands, including: Bare Minerals, Tarte, Bobbi Brown, and Stila.\n\nAlthough some ingredients in cosmetics may cause concerns, some are widely seen as beneficial. Titanium dioxide, found in sunscreens, and zinc oxide have anti-inflammatory properties.\n\nMineral makeup is noncomedogenic (as long as it does not contain talc) and offers a mild amount of sun protection (because of the titanium dioxide and zinc oxide).\n\nBecause they do not contain liquid ingredients, mineral makeups have long shelf-lives.\n\nThe term cosmetic packaging is used for primary packaging and secondary packaging of cosmetic products.\n\nPrimary packaging, also called cosmetic container, is housing the cosmetic product. It is in direct contact with the cosmetic product. Secondary packaging is the outer wrapping of one or several cosmetic container(s). An important difference between primary and secondary packaging is that any information that is necessary to clarify the safety of the product must appear on the primary package. Otherwise, much of the required information can appear on just the secondary packaging.\nCosmetic packaging is standardized by the ISO 22715, set by the International Organization for Standardization and regulated by national or regional regulations such as those issued by the EU or the FDA. Marketers and manufacturers of cosmetic products must be compliant to these regulations to be able to market their cosmetic products in the corresponding areas of jurisdiction.\n\nThe manufacture of cosmetics is dominated by a small number of multinational corporations that originated in the early 20th century, but the distribution and sales of cosmetics is spread among a wide range of businesses. The worlds largest cosmetic companies are L'Oréal, Procter & Gamble, Unilever, Shiseido, and Estée Lauder. In 2005, the market volume of the cosmetics industry in the US, Europe, and Japan was about EUR 70 Billion/a year. In Germany, the cosmetic industry generated €12.6 billion of retail sales in 2008, which makes the German cosmetic industry the third largest in the world, after Japan and the United States. German exports of cosmetics reached €5.8 billion in 2008, whereas imports of cosmetics totaled €3 billion.\n\nThe worldwide cosmetics and perfume industry currently generates an estimated annual turnover of US$170 billion (according to Eurostaf – May 2007). Europe is the leading market, representing approximately €63 billion, while sales in France reached €6.5 billion in 2006, according to FIPAR (Fédération des Industries de la Parfumerie – the French federation for the perfume industry). France is another country in which the cosmetic industry plays an important role, both nationally and internationally. According to data from 2008, the cosmetic industry has grown constantly in France for 40 consecutive years. In 2006, this industrial sector reached a record level of €6.5 billion. Famous cosmetic brands produced in France include Vichy, Yves Saint Laurent, Yves Rocher, and many others.\nThe Italian cosmetic industry is also an important player in the European cosmetic market. Although not as large as in other European countries, the cosmetic industry in Italy was estimated to reach €9 billion in 2007. The Italian cosmetic industry is dominated by hair and body products and not makeup as in many other European countries. In Italy, hair and body products make up approximately 30% of the cosmetic market. Makeup and facial care, however, are the most common cosmetic products exported to the United States.\n\nAccording to Euromonitor International, the market for cosmetics in China is expected to be $7.4 billion in 2021 up from $4.3 billion in 2016. The increase is due to social media and the changing attitudes of people in the 18-to-30-year age bracket.\n\nDue to the popularity of cosmetics, especially fragrances and perfumes, many designers who are not necessarily involved in the cosmetic industry came up with perfumes carrying their names. Moreover, some actors and singers (such as Celine Dion) have their own perfume line. Designer perfumes are, like any other designer products, the most expensive in the industry as the consumer pays for the product and the brand. Famous Italian fragrances are produced by Giorgio Armani, Dolce & Gabbana, and others.\n\nProcter & Gamble, which sells CoverGirl and Dolce & Gabbana makeup, funded a study concluding that makeup makes women seem more competent. Due to the source of funding, the quality of this Boston University study is questioned.\n\nDuring the 20th century, the popularity of cosmetics increased rapidly. Cosmetics are used by girls at increasingly young ages, especially in the United States. Because of the fast-decreasing age of make-up users, many companies, from high-street brands like Rimmel to higher-end products like Estee Lauder, cater to this expanding market by introducing flavored lipsticks and glosses, cosmetics packaged in glittery and sparkly packaging, and marketing and advertising using young models. The social consequences of younger and younger cosmetics use has had much attention in the media over the last few years.\n\nCriticism of cosmetics has come from a wide variety of sources including some feminists, religious groups, animal rights activists, authors, and public interest groups. It has also faced criticism from men in the manosphere, some of whom describe it as a form of deception or \"fakeup\".\n\nIn the United States: \"Under the law, cosmetic products and ingredients do not need FDA premarket approval.\" The EU and other regulatory agencies around the world have more stringent regulations. The FDA does not have to approve or review cosmetics, or what goes in them, before they are sold to the consumers. The FDA only regulates against some colors that can be used in the cosmetics and hair dyes. The cosmetic companies do not have to report any injuries from the products; they also only have voluntary recalls of products.\n\nThere has been a marketing trend towards the sale of cosmetics lacking controversial ingredients, especially those derived from petroleum, sodium lauryl sulfate (SLS), and parabens. Numerous reports have raised concern over the safety of a few surfactants, including 2-butoxyethanol. In some individuals, SLS may cause a number of skin problems, including dermatitis.\n\nParabens can cause skin irritation and contact dermatitis in individuals with paraben allergies, a small percentage of the general population. Animal experiments have shown that parabens have a weak estrogenic activity, acting as xenoestrogens.\nPerfumes are widely used in consumer products. Studies concluded from patch testing show fragrances contain some ingredients which may cause allergic reactions.\n\nBalsam of Peru was the main recommended marker for perfume allergy before 1977, which is still advised. The presence of Balsam of Peru in a cosmetic will be denoted by the INCI term \"Myroxylon pereirae\". In some instances, Balsam of Peru is listed on the ingredient label of a product by one of its various names, but it may not be required to be listed by its name by mandatory labeling conventions (in fragrances, for example, it may simply be covered by an ingredient listing of \"fragrance\").\n\nSome cosmetics companies have made pseudo-scientific claims about their products which are misleading or unsupported by scientific evidence.\n\nCosmetics testing on animals is particularly controversial. Such tests involve general toxicity, eye and skin irritancy, phototoxicity (toxicity triggered by ultraviolet light), and mutagenicity.\n\nDue to the controversy over the ethics of animal testing, alternatives to animal testing are in development, and some nations have chosen to legislate on animal testing with cosmetics. There are nearly 50 non-animal tests that have been validated for use, with many more in development, that may replace animal testing and are potentially more efficacious. Cosmetics testing is banned in the Netherlands, India, Norway, Israel, New Zealand, Belgium, and the UK, and in 2002, after 13 years of discussion, the European Union (EU) agreed to phase in a near-total ban on the sale of animal-tested cosmetics throughout the EU from 2009, and to ban all cosmetics-related animal testing. Animal testing is regulated in EC Regulation 1223/2009 on cosmetics. France, which is home to the world's largest cosmetics company, L'Oréal, has protested the proposed ban by lodging a case at the European Court of Justice in Luxembourg, asking that the ban be quashed. The ban is also opposed by the European Federation for Cosmetics Ingredients, which represents 70 companies in Switzerland, Belgium, France, Germany, and Italy. A plethora of cosmetic companies are cruelty free including: Bath and Body Works, Milani, Kat Von D, ELF, Too Faced Cosmetics, Lush, Physicians Formula, Urban Decay, Wet n Wild, Smashbox, and a variety of others. PETA has links on their website that users can sign encouraging makeup brands to become cruelty free.\n\nIn the European Union, the manufacture, labelling, and supply of cosmetics and personal care products are regulated by Regulation EC 1223/2009. It applies to all the countries of the EU as well as Iceland, Norway, and Switzerland. This regulation applies to single-person companies making or importing just one product as well as to large multinationals. Manufacturers and importers of cosmetic products must comply with the applicable regulations in order to sell their products in the EU. In this industry, it is common fall back on a suitably qualified person, such as an independent third party inspection and testing company, to verify the cosmetics’ compliance with the requirements of applicable cosmetic regulations and other relevant legislation, including REACH, GMP, hazardous substances, etc.\n\nIn the European Union, the circulation of cosmetic products and their safety has been a subject of legislation since 1976. One of the newest improvement of the regulation concerning cosmetic industry is a result of the ban animal testing. Testing cosmetic products on animals has been illegal in the European Union since September 2004, and testing the separate ingredients of such products on animals is also prohibited by law, since March 2009 for some endpoints and full since 2013.\n\nCosmetic regulations in Europe are often updated to follow the trends of innovations and new technologies while ensuring product safety. For instance, all annexes of the Regulation 1223/2009 were aimed to address potential risks to human health.\nUnder the EU cosmetic regulation, manufacturers, retailers, and importers of cosmetics in Europe will be designated as \"Responsible Person\". This new status implies that the responsible person has the legal liability to ensure that the cosmetics and brands they manufacture or sell comply with the current cosmetic regulations and norms. The responsible person is also responsible of the documents contained in the Product Information File (PIF), a list of product information including data such as Cosmetic Product Safety Report, product description, GMP statement, or product function.\n\nIn 1938, the U.S. passed the Food, Drug, and Cosmetic Act authorizing the Food and Drug Administration (FDA) to oversee safety via legislation in the cosmetic industry and its aspects in the United States. The FDA joined with 13 other federal agencies in forming the Interagency Coordinating Committee on the Validation of Alternative Methods (ICCVAM) in 1997, which is an attempt to ban animal testing and find other methods to test cosmetic products.\n\nANVISA (Agência Nacional de Vigilância Sanitária, \"Brazilian Health Surveillance Agency\") is the regulatory body responsible for cosmetic legislation and directives in the country. The rules apply to manufacturers, importers, and retailers of cosmetics in Brazil, and most of them have been harmonized so they can apply to the entire Mercosur.\n\nThe current legislation restricts the use of certain substances such as pyrogallol, formaldehyde, or paraformaldehyde and bans the use of others such as lead acetate in cosmetic products. All restricted and forbidden substances and products are listed in the regulation RDC 16/11 and RDC 162, 09/11/01.\n\nMore recently, a new cosmetic Technical Regulation (RDC 15/2013) was set up to establish a list of authorized and restricted substances for cosmetic use, used in products such as hair dyes, nail hardeners, or used as product preservatives.\n\nMost Brazilian regulations are optimized, harmonized, or adapted in order to be applicable and extended to the entire Mercosur economic zone.\n\nThe International Organization for Standardization (ISO) published new guidelines on the safe manufacturing of cosmetic products under a Good Manufacturing Practices (GMP) regime. Regulators in several countries and regions have adopted this standard, ISO 22716:2007, effectively replacing existing guidance and standards.\nISO 22716 provides a comprehensive approach for a quality management system for those engaged in the manufacturing, packaging, testing, storage, and transportation of cosmetic end products. The standard deals with all aspects of the supply chain, from the early delivery of raw materials and components until the shipment of the final product to the consumer.\n\nThe standard is based on other quality management systems, ensuring smooth integration with such systems as ISO 9001 or the British Retail Consortium (BRC) standard for consumer products. Therefore, it combines the benefits of GMP, linking cosmetic product safety with overall business improvement tools that enable organisations to meet global consumer demand for cosmetic product safety certification.\n\nIn July 2012, since microbial contamination is one of the greatest concerns regarding the quality of cosmetic products, the ISO has introduced a new standard for evaluating the antimicrobial protection of a cosmetic product by preservation efficacy testing and microbiological risk assessment.\n\nAn account executive is responsible for visiting department and specialty stores with counter sales of cosmetics. They explain new products and \"gifts with purchase\" arrangements (free items given out upon purchase of cosmetics items costing over some set amount).\n\nA beauty adviser provides product advice based on the client's skin care and makeup requirements. Beauty advisers can be certified by an Anti-Aging Beauty Institute.\n\nA cosmetician is a professional who provides facial and body treatments for clients. The term cosmetologist is sometimes used interchangeably with this term, but the former most commonly refers to a certified professional. A freelance make-up artist provides clients with beauty advice and cosmetics assistance. They are usually paid by the hour by a cosmetic company; however, they sometimes work independently.\n\nProfessionals in cosmetics marketing careers manage research focus groups, promote the desired brand image, and provide other marketing services (sales forecasting, allocation to retailers, etc.).\n\nMany involved within the cosmetics industry often specialize in a certain area of cosmetics such as special effects makeup or makeup techniques specific to the film, media, and fashion sectors.\n"}
{"id": "2156176", "url": "https://en.wikipedia.org/wiki?curid=2156176", "title": "Delay-insensitive minterm synthesis", "text": "Delay-insensitive minterm synthesis\n\nThe DIMS (delay-insensitive minterm synthesis) system is an asynchronous design methodology making the least possible timing assumptions. Assuming only the quasi-delay-insensitive delay model the generated designs need little if any timing hazard testing. The basis for DIMS is the use of two wires to represent each bit of data. This is known as a dual-rail data encoding. Parts of the system communicate using the early four-phase asynchronous protocol.\n\nThe construction of DIMS logic gates comprises generating every possible minterm using a row of C-elements and then gathering the outputs of these using OR gates which generate the true and false output signals. With two dual-rail inputs the gate would be composed of four two-input C-elements. A three input gate uses eight three-input C-elements.\n\nLatches are constructed using two C-elements to store the data and an OR gate to acknowledge the input once the data has been latched by attaching as its inputs the data output wires. The acknowledge from the forward stage is inverted and passed to the C-elements to allow them to reset once the computation has completed. This latch design is known as the 'half latch'. Other asynchronous latches provide a higher data capacity and levels of decoupling.\n\nDIMS designs are large and slow but they have the advantage of being very robust.\n"}
{"id": "29006979", "url": "https://en.wikipedia.org/wiki?curid=29006979", "title": "Enterprise appliance transaction module", "text": "Enterprise appliance transaction module\n\nAn enterprise appliance transaction module (EATM) is a device, typically used in the manufacturing automation marketplace, for the transfer of plant floor equipment and product status to manufacturing execution systems (MES), enterprise resource planning (ERP) systems and the like.\n\nSolutions that deliver manufacturing floor integration have evolved over time. Initially they took the form of custom integrated systems, designed and delivered by System Integrators. These solutions were largely based on separate Commercial off the Shelf (COTS) products that a System Integrator acquired and integrated into the custom solution. \n\nThese solutions have evolved into vendor supplied product solutions that no longer require the integration of separate components. EATM products, as they are known today, are available as COTS Appliance products, fully configurable, and not needing any software development or custom integration.\n\nHardware platform – embedded computer, computer appliance\n\nDevice communications software – Support for the device protocols from which data will be extracted. Device communications software typically operates through polled or change based protocols that are vendor specific. Data to be extracted is typically organized into related items, and transferred based on a machine status such as Cycle Complete, Job Start, System Downtime Event, Operator Change, etc.\n\nTypical protocols; Rockwell Automation CIP, ControlLogix backplane, EtherNet/IP, Siemens Industrial Ethernet, Modbus TCP. There are hundreds of automation device protocols and EATM solutions are typically targeting certain market segments and will be based on automation vendor relationships.\n\nEnterprise communications software – Software that will enable communications to Enterprise systems. Communications at this level are typically transaction oriented and require data transactions to be sent and acknowledged to ensure the data integrity. Examples include; Relational Database Adapters, Java Message Services (JMS), Oracle Database Interfaces and proprietary interfaces to specific products.\n\nTransaction application – Software that is configured to watch and collect device variables, formats them into required transactions, and transfer the results securely and reliably to the Enterprise Solutions. The Transaction Application resides between the Device Communications and the Enterprise communications.\n\nManufacturing solutions fall into many categories. Overall, the manufacturing environment is portrayed as a three layer Manufacturing Pyramid. At the base, Device Control Systems – Programmable Logic Controller (PLC) and Supervisory Control and Data Acquisition systems (SCADA) perform the process automation functions. A layer above that encompasses Plant Execution Systems that deliver the functions of; Asset Management, Genealogy, SPC, MES, Order Tracking, Quality Assurance and Scheduling. At the top most level, Enterprise Resource Planning (ERP) systems offer final control over the enterprise and track overall enterprise performance. \n\nIt is the job of EATM solutions to act as a bi-directional bridge between field devices and the supervisory control systems. These field devices could be located in a work cell or an assembly or process line. They could be very simple devices, or programmable controllers, machine controls, or PLCs. The upstream business systems could be ANDON and Kanban systems for that line, manufacturing execution systems (MES), and Archival Quality databases.\n\n"}
{"id": "16812320", "url": "https://en.wikipedia.org/wiki?curid=16812320", "title": "Event tree", "text": "Event tree\n\nEvent tree is an inductive analytical diagram in which an event is analyzed using Boolean logic to examine a chronological series of subsequent events or consequences. For example, event tree analysis is a major component of nuclear reactor safety engineering.\n\nAn event tree displays sequence progression, sequence end states and sequence-specific dependencies across time.\n\nEvent tree analysis is a logical evaluative process which works by tracing forward in time or forwards through a causal chain to model risk. It does not require the premise of a known hazard. An event tree is an inductive investigatory process.\n\nIn contrast, the Fault tree analysis (FTA) evaluates risk by tracing backwards in time or backwards through a cause chain. The analysis takes as a premise a given hazard. FTA is a deductive investigatory process.\n\nAn event tree may start from a specific initiator such as loss of critical supply, or component failure.\n\nSome industries use both fault trees and event trees. Software has been created for fault tree analysis and event tree analysis and is licensed for use at the world's nuclear power plants for Probabilistic Safety Assessment.\n\n\n"}
{"id": "10797223", "url": "https://en.wikipedia.org/wiki?curid=10797223", "title": "Finastra", "text": "Finastra\n\nFinastra is the third largest financial technology company in the world. The firm was formed in late 2017 through the combination of D+H and Misys, after Vista Equity Partners acquired Misys in June 2012 and subsequently purchased D+H in 2017.\n\nFinastra is led by chief executive officer, Simon Paris, who was appointed in June 2018. The company has offices in 42 countries with U.S. $2.1 billion in revenues. The company employs over 10,000 people and has over 9,000 customers across 130 countries.\n\nThe company was founded by Roger Morgan and Kevin Lomax as Misys to develop insurance software.\n\nThe purchase of multiple companies in respective markets by Misys under the leadership of Kevin Lomax (Chairman and/or CEO 1985–2006) allowed Misys to become a software supplier to the US healthcare industry, to banks (worldwide) and to fund managers worldwide. Many of the companies acquired and consolidated by Misys were themselves products of previous mergers.\n\nIn 1987 Misys shares were first traded on the Unlisted Securities Market. It was admitted to the Main List of the London Stock Exchange in 1989, cross selling to increased client base and economies of scale arising from consolidation.\nIn 1994 Misys entered the banking software space by purchasing Kapiti Ltd. In 1995 Misys purchased ACT (which included BIS and Kindle), thus giving Misys control of 3 of the 4 largest selling core banking packages at the time. At time of purchase, Midas had the biggest installed base of any \"off-the shelf\" banking software package.\n\nThe largest banking software company to be brought into the Misys fold was BIS. Business Intelligence Services or BIS Ltd in 1976 had acquired Kingsley-Smith and Associates and a software package named MIDAS developed by KSA. This banking software system was based on a concept of a core multi-currency accounting module. At the time, most other International Banks worldwide worked and accounted on a 'single currency' methodology. It was marketed to and operated by a number of client London Banks from 1979. From the mid to late-1980s competition to MIDAS came from vendors with systems called Kapiti, BankMaster (from Kindle) and IBIS. In 1991 ACT purchased Kindle followed by BIS in 1993 uniting two of the biggest selling products under one ownership albeit with different user bases. Midas was focused on its international branch banking in the world's financial centers. The simplicity of Bankmaster and its Branchpower front end appealed to tier 3 and 4 banks in Africa and South America.\n\nKapiti Ltd was a privately held packaged banking software house focused on using IBM's mid range computers to server the international branch banking market. Kapiti was founded in 1975 by a Mr. John Kennedy, a New Zealand native and competed directly with the commercially more successful Midas system. Initially headquartered in London, the company moved to Windsor then Slough, Berkshire. In 1990 Kapiti re-branded the well established Kapiti International Banking System (KIBS) as EQUATION 1, sales were still focused on international branch banking with clusters of users in financial centers like London, plus some small retail operations in the Gulf. The combination of the port to the new IBM AS/400 platform with its increased processing power, the user friendly EQ2 user interface rework, the performance boost from the EQ3 data base rewrite, and the addition of a client server Cashier system had allowed an expansion into retail banking. Kapiti also pursued wholesale banking by merging with Aregon International (dealing room information feeds), purchasing Future Systems (trade capture) and purchasing rights to some reporting tools. None of the acquired companies prospered under Kapiti. However the technical expertise of the Aregon team was exploited to build in house a trade finance system called Trade Innovation (TI) which saw commercial success for approximately 20 years.\n\nIn 1996 Misys bought Summit Systems, an American software company selling a front-office cross-asset management system targeting investment banks' trading rooms.\n\nIn 1997 it bought Medic Computer Systems, a healthcare software business.\n\nIn June 2001 Misys bought \"DBS Management\", a supplier of software to Independent Financial Advisors.\n\nIn June 2001 Misys bought Sunquest Information Systems, a United States based supplier of medical systems software.\n\nIn January 2004 Misys bought \"IQ Financial Systems\" from Deutsche Bank.\n\nIn July 2005 Misys bought \"Almonde\".\n\nIn February 2006 Misys bought \"Intesio\".\n\nIn March 2006 Misys bought \"NEOMAlogic\".\n\nIn October 2006 the CEO Kevin Lomax, one of the company's founders, resigned and IBM veteran Mike Lawrie was appointed as his replacement\n\nIn July 2007 Misys sold its diagnostic information software division to Vista Equity Partners.\n\nIn October 2008 Misys' subsidiary, Misys Healthcare, was merged with Allscripts, a medical records business, to become \"Allscripts-Misys Healthcare Solutions, Inc\"\n\nIn November 2010 Misys bought Sophis, a provider of portfolio and risk management software.\n\nIn February 2012, Misys entered into merger talks with its Swiss rival Temenos. The deal later collapsed when the two sides were unable to agree to terms.\n\nIn March 2012 private equity firm Vista Equity Partners announced that it had reached an agreement to acquire Misys.\n\nIn June 2012, upon the completion of acquisition by Vista Equity Partners, Misys was merged with Turaz, another acquisition of Vista. Turaz was formerly the treasury and risk management software division of Thomson Reuters.\n\nIn February 2014, Misys bought Hungary-based IND Group, a supplier of online and mobile banking software.\n\nIn August 2014, Misys bought Custom Credit Systems, a US-based supplier of credit workflow and loan origination software.\n\nIn March 2017, D+H announced that it would be acquired by Vista Equity Partners and combined with Misys\n\nIn June 2017, the merger with D+H was completed, and the company was rebranded as Finastra\n\nIn January 2018, Finastra acquired Olfa Soft for its FX e-trading platform for financial institutions.\n\nIn April 2018, Finastra launched an innovation lab in Hong Kong to enable collaboration between banks and other fintech companies.\n\nIn June 2018, Finastra announced it acquired Malauzai, a provider of mobile and internet banking solutions for community financial institutions.\n\nIn June 2018, Finastra, launched cloud-based platform to develop apps to collaborate with banks.\n\nIn 1875, D+H was founded as Davis & Henderson, a Canadian manufacturer specialising in bookbinding and printing. By the latter part of the 1890s, Canadian financial institutions began to form a significant portion of D+H customer base.\n\nIn the 1960s, D+H started to produce printed cheques with Magnetic Ink Character Recognition (MICR) encoding and began printing individually personalized bank cheques. It continued to focus on the cheque business through the 1970s and 1980s.\n\nFollowing a series of acquisitions starting in 2005, D+H shifted its business to providing financial technology services worldwide.\n\nIn 2011, D+H went public on the Toronto Stock Exchange. It acquired Mortgagebot and ASSET Inc. in 2011, Avista Solutions in 2012, Harland Financial Solutions and Compushare in 2013, and Fundtech in 2015.\n\nIn 2016, D+H added blockchain technology capabilities to its global payments platform. The added capabilities enabled banks that use its Global PAYplus services platform to access distributed ledgers to connect networks, move money in real time and improve access to liquidity.\n\nOn 17 June 2017, D+H announced that it would be acquired by Vista Equity Partners and combined with Misys, operating under the new company name Finastra for a total enterprise value of approximately $4.8 billion.\n\nGlobal head office is located in London, UK. North American head office is located in Toronto, Canada and New York, NY, USA. \n\n"}
{"id": "1728007", "url": "https://en.wikipedia.org/wiki?curid=1728007", "title": "Foodservice", "text": "Foodservice\n\nFoodservice (US English) or catering industry (British English) defines those businesses, institutions, and companies responsible for any meal prepared outside the home. This industry includes restaurants, school and hospital cafeterias, catering operations, and many other formats.\n\nThe companies that supply foodservice operators are called foodservice distributors. Foodservice distributors sell goods like small wares (kitchen utensils) and foods. Some companies manufacture products in both consumer and foodservice versions. The consumer version usually comes in individual-sized packages with elaborate label design for retail sale. The foodservice version is packaged in a much larger industrial size and often lacks the colorful label designs of the consumer version.\n\nThe food system, including food service and food retailing supplied $1.24 trillion worth of food in 2010 in the US, $594 billion of which was supplied by food service facilities, defined by the USDA as any place which prepares food for immediate consumption on site, including locations that are not primarily engaged in dispensing meals such as recreational facilities and retail stores. Full-service and Fast-food restaurants account for 77% of all foodservice sales, with full-service restaurants accounting for just slightly more than fast food in 2010. The shifts in the market shares between fast food and full-service restaurants to market demand changes the offerings of both foods and services of both types of restaurants.\n\nAccording to the National Restaurant Association a growing trend among US consumers for the foodservice industry is global cuisine with 66% of US consumers eating more widely in 2015 than in 2010, 80% of consumers eating 'ethnic' cuisines at least once a month, and 29% trying a new 'ethnic' cuisine within the last year.\n\nThe Foodservice distributor market size is as of 2015 $231 billion in the US; the national broadline market is controlled by US Foods and Sysco which combined have 60-70% share of the market and were blocked from merging by the FTC for reasons of market power.\n\nFoodservice tends to be, on average, higher in calories and lower in key nutrients than foods prepared at home. Most restaurants, including fast food, have added more salads and fruit offerings and either by choice or in response to local legislation provided nutrition labeling.\n\nIn the US the FDA is moving towards establishing uniform guidelines for fast food and restaurant labeling, proposed rules were published in 2011 and final regulations published on 1 December 2014 which supersede State and local menu-labeling provisions, going into effect 1 December 2015. Research has shown that the new labels may influence consumer choices, but primarily if it provides unexpected information and that health-conscious consumers are resistant to changing behaviors based on menu labeling Fast food restaurants are expected by the ERS to do better under the new menu labeling than full service restaurants as full-service restaurants tend to offer much more calorie dense foods, with 50% of fast food meals being between 400 and 800 calories and less than 20% above 1000 calories, in contrast, full-service restaurants 20% of meals are above 1,400 calories. When consumers are aware of the calorie counts at full-service restaurants 20% choose lower calorie options and consumers also reduce their calorie intake over the rest of the day.\n\nEating one meal away from home each week translates to 2 extra pounds each year or a daily increase of 134 calories and a decrease in diet quality by 2 points on the Healthy Eating Index.\n\nIn addition; the likelihood of contracting a food-borne illness (such as E. coli, hepatitis B, H. pylori, listeria, salmonella, norovirus and typhoid) is greatly increased due to food not being kept below 40 degrees Fahrenheit or cooked to a temperature of higher than 160 degrees Fahrenheit, not washing hands for at least 20 seconds for food handlers or not washing contaminated cutting boards and other kitchen tools in hot water.\n\nTable service is food service served to the customer's table by waiters and waitresses, also known as \"servers\". Table service is common in most restaurants, while for some fast food restaurants counter service is the common form. For pubs and bars, counter service is the norm in the United Kingdom. With table service, the customer generally pays at the end of meal. Various methods of table service can be provided, such as silver service.\n\nGueridon service is a form of food service provided by restaurants to their customers. This type of service encompasses preparing food (primarily salads, main dishes such as beef tartare, or desserts) in direct view of the guests, using a gueridon. A gueridon typically consists of a trolley that is equipped to transport, prepare, cook and serve food. There is a gas hob, chopping board, cutlery drawer, cold store (depending on the trolley type), and general working area.\n\n"}
{"id": "18866574", "url": "https://en.wikipedia.org/wiki?curid=18866574", "title": "Gerson Lehrman Group", "text": "Gerson Lehrman Group\n\nGerson Lehrman Group (GLG) is a New York based platform for professional learning that markets itself as a \"learning membership connecting businesspeople trying to solve problems to experts that can solve them\". The firm connects its clients with independent contractor consultants with topical or industry expertise in a variety of fields (termed \"Council Members\"), as well as executive education, larger team training, and the placement of experts in long-term advisory, operational, and board roles. GLG has a network of over 800,000 freelance consultants\n\nThe firm was founded in 1998 and has been backed by private equity firms Silver Lake Partners, Bessemer Venture Partners, and SFW Capital Partners. GLG is headquartered in New York City, with offices in 22 cities in 12 countries.\n\nGLG's clients include corporations, hedge funds, private equity firms, professional service firms, and non-profit organizations. GLG experts include consultants, physicians, scientists, engineers, lawyers, senior current and former c-level executives, and former government members.\n\nGerson Lehrman Group was founded by Yale Law School graduates Mark Gerson and Thomas Lehrman in 1998. Alexander Saint-Amand, a former Bloomberg reporter, joined GLG shortly after, and served as CEO until 2018.\n\nGLG, initially funded by friends and family, was formed as a publishing house to produce industry guidebooks for institutional investors. However, the founders discovered that their clients wanted to talk directly to experts in casual conversations, rather than reading formal written reports. Accordingly, in 1999, GLG abandoned its publishing business and began offering subscriptions to its network of experts. This business came to be called an expert network.\n\nGLG’s first customers were investors, and most of their growth from 1999 to 2005 was within the investment community. In 2006, they began working with big strategy consultancies and with companies in life sciences, chemicals and industrials, and technology. By 2010, The Wall Street Journal had described GLG as \"dominating the U.S. expert networking industry\".\n\nIn the design space, GLG is a pioneer of activity based working office design, which does away with seating assignments for workers.\n\nIn 2018, GLG named Paul Todd as its new CEO. Todd was previously the head of eBay's EMEA business since 2015.\n\nIn 2006, GLG worked with U.S. Department of Housing and Urban Development to provide new primary data and analysis relevant for any other recovery plans in the rebuilding of Louisiana following Hurricane Katrina and Hurricane Rita. GLG has also done other philanthropy work with smaller charities.\n\nGLG now operates a global Social Impact Fellowship, an initiative that provides learning resources and expertise to a select group of nonprofits and social enterprises, at no cost.\n\nAs the use of alternative research providers within the financial industry has grown, brokerages themselves have been partnering with firms like GLG. On September 10, 2008, the company announced a deal with Credit Suisse, whereby analysts from the financial services giant would be able to tap into GLG’s experts as part of their research process. About 300 Credit Suisse analysts became members of the GLG expert network. In 2009, Gerson Lehrman Group announced a partnership with Frost & Sullivan, making the company's 800+ market researchers available to consult with GLG clients.\n\nIn July 2009, GLG announced that it has partnered with Amba Research, a leading financial services knowledge process outsourcing (KPO) firm that provides equity and credit research.\n\nOn July 8, 2010, GLG and Bloomberg announced a partnership that enables clients of the Bloomberg Professional platform to access the GLG expert network through the Bloomberg platform.\n\nIn October 2011, GLG partnered with and acquired a minority stake in Ushi, a leading China-based business social networking provider, and the largest competitor of LinkedIn in the region.\n\n\nThe expert network business model has drawn scrutiny for concerns relating to adherence to disclosure rules and insider trading within the investment industry. During 2005, for example, doctors who served as paid consultants to investment firms as a result of their role in an expert network generated a “fair share of controversy”. GLG was part of a lawsuit filed by Biovail in March 2006, in which Biovail claimed that its shares had been manipulated. In September 2006, GLG announced the creation of a proprietary system that helped identify and manage conflicts, leveraging its experience as a pioneer in its industry to craft risk-reducing rules for expert engagements.\n\nIn January 2007, it was reported that the company and Vista Research, then a Standard & Poor's affiliate, were part of an inquiry by the New York State Attorney General’s office into the consulting practices of investment firm clients. As of December 2007, the Attorney General had taken no action, and the investigation appeared to be dormant.\n\nGLG's compliance program of over 50 individuals has been cited as a reason for SFW Capital's $200 million investment in GLG.\n\n\n"}
{"id": "986413", "url": "https://en.wikipedia.org/wiki?curid=986413", "title": "Hair dryer", "text": "Hair dryer\n\nA hair dryer, hairdryer or blow dryer is an electromechanical device that blows ambient or hot air over damp hair to speed the evaporation of water to dry the hair. Blow dryers enable better control over the shape and style of hair, by accelerating and controlling the formation of temporary hydrogen bonds within each strand. These bonds are powerful (allowing stronger hair shaping than the sulfur bonds formed by permanent waving products) but are temporary and extremely vulnerable to humidity. They disappear with a single washing of the hair.\n\nHairstyles using blow dryers usually have volume and discipline, which can be further improved with styling products and hairbrushes during drying to add tension, hold and lift.\n\nBlow dryers were invented in the late 19th century. The first model was created by Esther F. \"Beau\" Godefroy in his salon in France in 1890. The handheld, household hair dryer first appeared in 1920. Blow dryers are used in beauty salons by professional stylists and in the household by consumers.\n\nMost hair dryers consist of electric heating coils and a fan (usually powered by a universal motor). The heating element in most dryers is a bare, coiled nichrome wire that is wrapped around mica insulators. Nichrome is used because of two important properties: It is a poor conductor of electricity and it does not oxidize when heated.\n\nA survey of stores in 2007 showed that most hair dryers had ceramic heating elements (like ceramic heaters), because of their \"instant heat\" capability. This means that it takes less time for the dryers to heat up and for the hair to dry.\nMany of these dryers have \"normal mode\" buttons that turn off the heater and blow room-temperature air while the button is pressed. This function helps to maintain the hairstyle by setting it. The colder air reduces frizz and can help to promote shine in the hair.\n\nMany feature \"ionic\" operation, to reduce the build-up of static electricity in the hair, though the efficacy of ionic technology is of some debate. Manufacturers claim this makes the hair \"smoother\". Some stylists consider the introduction of ionic technology to be one of the most important advances in the beauty industry.\n\nHair dryers are available with attachments, such as diffusers, airflow concentrators, and comb nozzles. \n\nHair dryers have been cited as an effective treatment for head lice and earwax-prevention.\n\nToday there are two major types of blow dryers (hair dryers): the handheld and the rigid-hood dryer.\n\nA hood dryer has a hard plastic dome that fits over a person's head to dry their hair. Hot air is blown out through tiny openings around the inside of the dome so the hair is dried evenly. Hood dryers are mainly found in hair salons.\n\nIn 1890 the first hairdryer was invented by French stylist Fay Godefroy. His invention was a large, seated version that consisted of a bonnet that attached to the chimney pipe of a gas stove. Godefroy invented it for use in his hair salon in France, and it was not portable or handheld. It could only be used by having the person sit underneath it.\n\nArmenian American inventor Gabriel Kazanjian was the first to patent a blow dryer in the United States, in 1911.\n\nAround 1915, hair dryers began to go on the market in handheld form. This was due to innovations by National Stamping and Electricworks under the white cross brand, and later U.S. Racine Universal Motor Company and the Hamilton Beach Co., which allowed the dryer to be small enough to be held by hand. Even in the 1920s, the new dryers were often heavy, weighing in at approximately , and were difficult to use. They also had many instances of overheating and electrocution. Hair dryers were only capable of using 100 watts, which increased the amount of time needed to dry hair (the average dryer today can use up to 2000 watts of heat).\n\nSince the 1920s, development of the hair dryer has mainly focused on improving the wattage and superficial exterior and material changes. In fact, the mechanism of the dryer has not had any significant changes since its inception. One of the more important changes for the hair dryer is to be made of plastic, so that it is more lightweight. This really caught on in the 1960s with the introduction of better electrical motors and the improvement of plastics. Another important change happened in 1954 when GEC changed the design of the dryer to move the motor inside the casing.\nThe bonnet dryer was introduced to consumers in 1951. This type worked by having the dryer, usually in a small portable box, connected to a tube that went into a bonnet with holes in it that could be placed on top of a person's head. This worked by giving an even amount of heat to the whole head at once.\n\nThe 1950s also saw the introduction of the rigid-hood hair dryer which is the type most frequently seen in salons. It had a hard plastic helmet that wraps around the person's head. This dryer works similarly to the bonnet dryer of the 1950s but at a much higher wattage.\n\nIn the 1970s, the U.S. Consumer Product Safety Commission set up guidelines that hair dryers had to meet to be considered safe to manufacture. Since 1991 the CPSC has mandated that all dryers must use a ground fault circuit interrupter so that it cannot electrocute a person if it gets wet. By 2000, deaths by blowdryers had dropped to fewer than four people a year, a stark difference to the hundreds of cases of electrocution accidents during the mid-20th century.\n\n"}
{"id": "49304399", "url": "https://en.wikipedia.org/wiki?curid=49304399", "title": "Halogen Software", "text": "Halogen Software\n\nHalogen Software is a Canadian company that provides cloud-based talent management solutions to customers with between 100 and 10,000 employees. The firm was founded in 1996 and is headquartered in Ottawa, Ontario, Canada.\n\nThe firm was formerly known as Manta Corporation and in June 2001, changed its name to Halogen Software Inc. Halogen Software has been backed by JMI Equity since 2008.\n\nLes Rechan became Halogen Software’s new President and CEO in 2015 after serving as interim CEO and on Halogen’s board of directors from May 2015.\n\nHalogen opened an office in the United Kingdom in September 2011 and created its first Australian office in 2012. Between 2013 and 2015, it opened new offices in San Jose, North Carolina, Amsterdam and Dubai.\n\nIn May 2013, Halogen Software debuted in the Toronto Stock Exchange under the ticker symbol HGN. The company stock jumped to 17 percent on its first day. In September 2013, Halogen Software launched two software modules to implement one-on-one sessions between managers and employees in the workforce and improve employee communication by incorporating the Myers–Briggs Type Indicator.\n\nIn May 2017 Halogen Software was acquired by Saba Software for $293 million.\n\n"}
{"id": "3066436", "url": "https://en.wikipedia.org/wiki?curid=3066436", "title": "Howdah", "text": "Howdah\n\nA howdah, or houdah (Hindi: हौदा \"haudā\"), derived from the Arabic هودج (hawdaj), that means \"bed carried by a camel\", also known as \"hathi howdah\" (हाथी हौदा), is a carriage which is positioned on the back of an elephant, or occasionally some other animal such as camels, used most often in the past to carry wealthy people or for use in hunting or warfare. It was also a symbol of wealth for the owner and as a result was decorated with expensive gemstones.\n\nMost notable are the Golden Howdah, the one used in display at the Napier Museum at Thiruvananthapuram which was used by the Maharaja of Travancore and the one used traditionally during the Elephant Procession of the famous Mysore Dasara. The Mehrangarh Fort Museum in Jodhpur, Rajasthan has a gallery of royal howdahs.\n\nIn the present time, howdahs are used mainly for tourist or commercial purposes in South East Asia and are the subject of controversy as animal rights groups and organizations, such as Millennium Elephant Foundation, openly criticize the use of the howdah, citing information that howdahs can cause permanent damage to an elephant's spine, lungs, and other organs and can significantly shorten the animal's life. \n\nThe Mehrangarh Fort Museum, Jodhpur, has a gallery dedicated to an array of Hathi Howdah, used by the Maharaja of Mewar, mostly for ceremonial occasions.\nThe American author Herman Melville in Chapter 42 (\"The Whiteness of the Whale\") of \"Moby Dick\" (1851), writes \"To the native Indian of Peru, the continual site of the snow-howdahed Andes conveys naught of dread, except, perhaps, in the more fancy of the eternal frosted desolateness reigning at such vast altitudes, and the natural conceit of what a fearfulness it would be to lose oneself in such inhuman solitudes.\" It also appears in Chapter 11 of Jules Vernes' classic adventure novel \"Around the World in Eighty Days\" (1873), in which we are told \"The Parsee, who was an accomplished elephant driver, covered his back with a sort of saddle-cloth, and attached to each of his flanks some curiously uncomfortable howdahs.\" Tolkien also wrote in \"Lord of The Rings\" of the Mûmakil (Elephants) of Harad with howdahs on their backs.\n\nA derived symbol used in Europe is the \"elephant and castle\": an elephant carrying a castle on its back, being used especially to symbolize strength. The symbol was used in Europe in classical antiquity and more recently has been used in England since the 13th century, and in Denmark since at least the 17th century.\n\nIn antiquity, the Romans war elephants, and turreted elephants feature on the coinage of Juba II of Numidia, in the 1st century BC. Elephants were used in the Roman campaigns against the Celtiberians in Hispania, against the Gauls, and against the Britons, the ancient historian Polyaenus writing, \"Caesar had one large elephant, which was equipped with armor and carried archers and slingers in its tower. When this unknown creature entered the river, the Britons and their horses fled and the Roman army crossed over.\" However, he may have confused this incident with the use of a similar war elephant in Claudius' final conquest of Britain. \nAlternatively, modern uses may derive from later contacts with howdahs. Fanciful images of war elephants with elaborate castles on their back date to 12th century Spain, as at right.\n\nNotably, 13th century English use may come from the elephant given by Louis IX of France to Henry III of England, for his menagerie in the Tower of London in 1225, this being the first elephant in England since Claudius.\n\nToday the symbol is most known from the Elephant and Castle intersection in south London, which derives its name from a pub, using the old site of a cutler's, who had used the symbol of the Worshipful Company of Cutlers. The Cutlers, in turn, used the symbol due to the use of ivory in handles.\n\nThe elephant and castle symbol has also been used since the 13th century in the coat of arms of the city of Coventry, and was used in the 17th century by the English slaving monopoly, the Royal African Company, which led to its use on the guinea coin.\nThe symbol of an elephant and castle is also used in the Order of the Elephant, the highest order in Denmark, since 1693.\n\n"}
{"id": "3446698", "url": "https://en.wikipedia.org/wiki?curid=3446698", "title": "Humidistat", "text": "Humidistat\n\nA humidistat is an electronic device analogous to a thermostat but which responds to relative humidity, not temperature. Humidistats are used in a number of devices including dehumidifiers, humidifiers, and microwave ovens. In humidifiers and dehumidifiers the humidistat is used where constant relative humidity conditions need to be maintained such as a refrigerator, greenhouse, or climate-controlled warehouse. When adjusting the controls in these applications the humidistat would be what is being set. In microwaves they are used in conjunction with \"smart cooking\" one-button features such as those for microwave popcorn. Humidistats employ hygrometers but are not the same. A humidistat has the functionality of a switch and is not just a measuring instrument like a hygrometer is.\n\nFor heating, ventilation, and air conditioning (HVAC) of buildings, humidistats or humidity sensors are used to sense the air relative humidity in the controlled space and turn on and off the HVAC equipment.\n"}
{"id": "21491008", "url": "https://en.wikipedia.org/wiki?curid=21491008", "title": "Institute of Air Quality Management", "text": "Institute of Air Quality Management\n\nThe Institute of Air Quality Management (IAQM) was launched in November 2002 to provide a focal point for all air quality professionals \nThe IAQM is the largest professional body for air quality experts in the UK as well as the authoritative voice for UK air quality. \n\n"}
{"id": "55932126", "url": "https://en.wikipedia.org/wiki?curid=55932126", "title": "Jungle Creations", "text": "Jungle Creations\n\nJungle Creations is a British digital media company that operates various channels including VT, Twisted, and Nailed It. Collectively, its channels garner up to 4 billion views each month on Facebook, and, according to Tubular Labs, it is the fifth most-viewed online media company as of August 2017. Jungle Creations is based in London with an office in New York City.\n\nIn 2014, Jamie Bolding founded Viral Thread in West London. The first piece of content produced was a listicle entitled \"Twenty people you will meet at fresher's week.\" Initially, the site's focus was on collating already existent content.\nIn September 2016, the company's food-related channel, Twisted, partnered with Oreo to announce new Oreo flavours. Also that month, the Viral Thread channel posted a video featuring the Hövding airbag bicycle helmet which became the most watched video in the company's history with over 150 million views.\n\nIn February 2017, Jungle Creations attracted some controversy when it posted and began licensing a viral video featuring a female bicyclist being catcalled by men in a van. The video ended with the woman ripping the side mirror off the van and cycling away.\n\nIn June 2017, the company opened an office in New York City. The following month, it named Nat Poulter its chief operating officer and Sefton Monk its chief technology officer. In August 2017, the company officially changed the name of Viral Thread to VT.\n\nJungle Creations operates several, often themed video channels, largely on Facebook. Its channels include Twisted and Food Envy (for food and cooking), Bosh (for vegan food), Nailed It (for DIY), The Aardvark (for animals), and its flagship channel, Viral Thread (or VT) among others. Currently, around 40% of the videos it publishes are original while the other 60% is culled from content aggregators or user submissions. The company also partners with other businesses (like Oreo or YO! Sushi) to produce video content for their brand.\n\n"}
{"id": "18584853", "url": "https://en.wikipedia.org/wiki?curid=18584853", "title": "Location-allocation", "text": "Location-allocation\n\nLocation-allocation refers to algorithms used primarily in a geographic information system to determine an optimal location for one or more facilities that will service demand from a given set of points. Algorithms can assign those demand points to one or more facilities, taking into account factors such as the number of facilities available, their cost, and the maximum impedance from a facility to a point.\n\n"}
{"id": "3160940", "url": "https://en.wikipedia.org/wiki?curid=3160940", "title": "Low IF receiver", "text": "Low IF receiver\n\nIn a low-IF receiver, the RF signal is mixed down to a non-zero low or moderate intermediate frequency, typically a few megahertz (for TV), and even lower frequencies (typically 120-130kHz) in the case of FM radio band receivers. Low-IF receiver topologies have many of the desirable properties of zero-IF architectures, but avoid the DC offset and 1/f noise problems.\n\nThe use of a non-zero IF re-introduces the image issue. However, when there are relatively relaxed image and neighbouring channel rejection requirements they can be satisfied by carefully designed low-IF receivers. Image signal and unwanted blockers can be rejected by quadrature downconversion (complex mixing) and subsequent filtering.\n\nThis technique is now widely used in the tiny FM receivers incorporated into MP3 players and mobile phones and is becoming commonplace in both analog and digital TV receiver designs. Using advanced analog- and digital signal processing techniques, cheap, high quality receivers using no resonant circuits at all are now possible.\n"}
{"id": "41191171", "url": "https://en.wikipedia.org/wiki?curid=41191171", "title": "McAllister Hull", "text": "McAllister Hull\n\nMcAllister Hull (September 1, 1923 – February 9, 2011) was an American theoretical physicist who took part in the creation of the atomic bomb that was dropped over Nagasaki in 1945, ending World War II.\n\nHull was born on September 1, 1923, in Birmingham, Alabama. In his memoir, he describes how, in his boyhood, he read Isaac Asimov, Robert Heinlein, and other science-fiction writers, and discovered physics and Albert Einstein while doing school projects.\n\nIn a 2004 letter to the \"New York Review of Books\", Hull wrote that his father had “tried to enlist” in General John J. Pershing's expedition to bring Pancho Villa to justice, “but needed his mother’s permission, which was not given. In 1917, he did enlist, and went to France with Pershing in the 4th Alabama, where he was in combat until wounded at Soissons, and in hospital service until 1919.”\n\nIn the autumn of 1941, Hull entered Mississippi State University to study physics. After his freshman year, according to a 2007 newspaper profile, “he quit school and became a draftsman for an ordnance plant, where he was trained to test explosives to see if they were pure enough to be used for shells.”\n\nIn 1943 Hull was drafted into the Army Specialized Training Program, which identified enlisted personnel who had technical skills or some science education. He was assigned to the Special Engineer Detachment (SED) at Oak Ridge, Tennessee.\n\nIn the autumn of 1944, Hull and three other men were ordered to take a train from Oak Ridge to Lamy, New Mexico, where Hull dialed a number he had been told to call. A bus showed up and took Hull and his companions to Los Alamos. “I never saw those men again,” Hull later recalled. Although he had only a high-school diploma, he had been identified as promising student of physics. He was put to work developing methods for, and overseeing, the casting of explosive lenses which would compress a plutonium core, resulting in an implosion. Hull, a sergeant, acted as foreman, supervising the team of civilians who worked on the casting.\n\nDuring the months when he worked on the project to create Fat Man, the atomic bomb that would be dropped over Nagasaki, he was not told explicitly that he was working on a bomb. “But I knew enough physics to figure out what was going on.” His work, which lasted until the summer of 1945, was considered so dangerous that the lab where he worked with his team was 15 miles from the other scientists' labs.\n\nIn order to cast the explosives, according to the \"New York Times\", Hull used candy kettles and malted-milk stirrers. “We actually used a candy kettle ... to melt the explosives and then poured them into the mold to make the lenses,” Hull told MSNBC in an interview. An article in the Bulletin of the Atomic Scientists tells the story this way: “One of Hull's early assignments was to figure our how to eliminate air bubbles in the explosive lenses. Remembering his experience making milk shakes as a waiter at college, he devised a stirrer that could be drawn up through the molten mixture, effectively bringing the bubbles to the surface.”\n\nIn 1995, on the 50th anniversary of the atom bomb, Hull told CBS: “We were pouring two--two-speed explosives: a slow explosive called baritar, which was barium nitrate and TNT, mainly, and then a faster explosive called Comp-B, which had TNT in it and the rest of it either classified or I don't remember. It was--it was fast.” Asked by a CBS reporter whether there had been “white coats” or a “lab atmosphere,” Hull replied: “Oh, Lord, no.”\n\nHull later recalled “the day that J. Robert Oppenheimer, the director of Los Alamos, brought Gen. Leslie Groves out to the lab. While looking at the mold for the lens, Groves accidentally stepped on and disconnected a hot water line. A stream of extremely hot water came out of the socket behind him, hitting him squarely in the rear. Hull succeeded – just barely – in stifling his laughter, but only until Oppenheimer turned to him and remarked that the incident “just goes to show you the incompressibility of water.” There was no way Hull could contain himself after that. (Hull was relieved that Groves did not recognize him at the Trinity test several months later.)”\n\nHull was still at work on Fat Man At 5:30 A.M. on July 16, 1945, the test bomb produced by the project was detonated on the Alamogordo Bombing Range south of Los Alamos. “I was invited to go to a viewing site 20 miles away to watch,” Hull later said. “It was pitch dark, and then suddenly, this brilliant light flashed, but since we were 20 miles away, we heard no sound,” says Hull.\n\nHill later said that the biggest surprise of his life was discovering that despite “strict internal security” in the Manhattan Project, “Soviet KGB agents managed to get near enough to learn some things.”\n\nWhen he was first working on Fat Man, Hull “didn't want it dropped on people,” but rather wished it would be detonate “in the Tokyo Harbor or in an isolated area, to scare them. But when Hirohito didn't surrender, even after the Aug. 6 bombing of Hiroshima, we knew we had no choice.” In 2007, Hull expressed hope that nuclear bombs would never be used again.\n\nWhen it turned out that “the blast [over Nagasaki] was a good one,” Hull said in a 1995 interview with CNN, “it was a tremendous relief. All of us felt that way. All of us felt that way. We had done it. But at the same time, immediately both of us who knew the score began to think, OK, what have we done? What have we done?...How many of us knew everything about the bomb, enough to know what its potential was? Well, a relatively small number, maybe a thousand. Maybe a few of them. So let's say a thousand just to be generous. How many people were killed — 20,000. It means I'm responsible for 400? That's the kind of arithmetic that bothers you.”\n\n“Different kinds of people worked on the Manhattan Project, from the greatest scientist in the world to the ordinary people doing ordinary things,” Hull said in a 2006 interview. He also said that “[a]nyone who works on weapons has a responsibility for people who are killed with those weapons...As a consequence, I have a personal responsibility for some of those people who were killed in Nagasaki.” Still, as he told a 1985 interviewer, “my colleagues and I were very pleased that our efforts had not failed...We had no idea about the tactics or the strategy of using both bombs. And today, it is still controversial whether the second bomb should have been used or whether either bomb should have been used. But that's hindsight.”\n\nAsked in a 1995 interview on \"CBS News Sunday Morning\" whether, if he could go back in time, he would participate again in the creation of the bomb, Hull said: “I would again do whatever was necessary to defend our way of life, to save the life of this country and this society, which, as difficult, as fragile and as—as flawed as it is, it's still the best the world has ever seen. That I'd—that I'd do it again. Sorry.”\n\nAfter the war Hull attended Yale University, where he earned a B.S. in 1948 and a Ph.D. in 1951. He went on to teach physics at Yale for 20 years.\n\nHull later said that the person who had exerted the greated professional influence upon him was Gregory Breit, the theoretical nuclear physicist who “was my mentor and friend for 20 years through my career at Yale.”\n\nHe also taught at the State University of New York at Buffalo, where he was the dean of graduate and professional education, and at Oregon State University.\n\nHe moved to Albuquerque in 1977 to take up the position of provost at the University of New Mexico, where he also taught physics. He remained as provost until the mid 1980s, and retired from UNM in 1989.\n\nHull played an instrumental role in the establishment of UNM’s optics program. He was also a strong supporter of the university's Peace Studies program. In 2007 he said that his greatest professional pride was a course he had taught “for more than 30 years” called Physics and Society. He had “hoped to promote closer relations between the arts and sciences at the University of New Mexico by creating a modern Aristotelian program recognizing the unity of knowledge. I came close, but ran out of time.”\n\nIn his later years, when his wife began to lose her eyesight owing to macular degeneration, Hull created a device intended to improve her vision by “direct[ing] light to portions of the retina unaffected by the disease to compensate for the loss in the central vision field.” Patented and marketed as the MacVision lens technology, it was put on the market in 2009 by MacVision Technologies, a joint venture by the University of New Mexico and Select Universities Technology, Inc.\n\nHull said in 2007 that he had never met any survivors of Hiroshima or Nagasaki. “I have had Japanese students in my courses, perhaps children of survivors, in which I discussed the bombings,” he said. “The students seemed to accept the bombings as justified by the Japanese aggression.”\n\nHe moved to Charleston, South Carolina, in 2007.\n\nAt the time of his death, on February 9, 2011, aged 87, he was Professor Emeritus of Physics at UNM. Former University of New Mexico Provost and Professor Emeriti of Physics.\n\nHe said that his strongest belief was that “[t]here is no acceptable degree of nuclear war, for once launched, no one can control the course of it.”\n\nA reviewer wrote that “Hull's love of science permeates his memoir,” and quoted him as writing: “It usually surprises me – pleasurably – when I discover that something I've worked out in theory actually works out in the real world...To paraphrase Einstein, the greatest puzzle about the universe is that we understand it!”\n\nHe told a 2006 interviewer, “I believe people in general, who are not going to be physicists, need to understand how physics affects their lives.” In a letter the same year to the New York Times, reacting to a column about college education by David Brooks, Hull wrote: “The humanities without the sciences are incomplete, and the sciences without the humanities dangerous.”\n\nHis memoir, \"Rider of the Pale Horse: A Memoir of Los Alamos and Beyond\", co-authored with Amy Bianco, was published in 2006 by UNM Press. The title of the memoir refers to Revelation 6:8: “Behold a pale horse: and his name that sat on him was Death, and Hell followed with him.”\n\nHe was also the author of a 1969 textbook, \"The Calculus of Physics\".\n\nHull was married for 65 years to Mary Muska Hull, who survived him. They had two children, John Hull and Wendy McCabe, and three grandchildren, Isaac Hull, Damaris McDonald and Ursula McCabe.\n\nIn 2007 he listed his pastimes as “studying serious music, theater and art history” and said that in earlier years he had been interested in “flying light planes, mountain hiking, skydiving and building furniture.”\n"}
{"id": "12226501", "url": "https://en.wikipedia.org/wiki?curid=12226501", "title": "Michael Joseph Owens", "text": "Michael Joseph Owens\n\nMichael Joseph Owens (January 1, 1859 – December 27, 1923) was an inventor of machines to automate the production of glass bottles.\n\nHe was born in Mason County, West Virginia on January 1, 1859. He left school at the age of 10 to start a glassware apprenticeship at J. H. Hobbs, Brockunier and Company in Wheeling, West Virginia. \n\nIn 1888 he moved to Toledo, Ohio and worked for the Toledo Glass Factory owned by Edward Drummond Libbey. He was later promoted to foreman and then to supervisor. He formed the Owens Bottle Machine Company in 1903. His machines could produce glass bottles at a rate of 240 per minute, and reduce labor costs by 80%.\n\nOwens and Libbey entered into a partnership and the company was renamed the Owens Bottle Company in 1919. In 1929 the company merged with the Illinois Glass Company to become the Owens-Illinois Glass Company.\n\nHe died on December 27, 1923.\n\n\n"}
{"id": "30511252", "url": "https://en.wikipedia.org/wiki?curid=30511252", "title": "Nanocrystal solar cell", "text": "Nanocrystal solar cell\n\nNanocrystal solar cells are solar cells based on a substrate with a coating of nanocrystals. The nanocrystals are typically based on silicon, CdTe or CIGS and the substrates are generally silicon or various organic conductors. Quantum dot solar cells are a variant of this approach, but take advantage of quantum mechanical effects to extract further performance. Dye-sensitized solar cells are another related approach, but in this case the nano-structuring is part of the substrate.\n\nPrevious fabrication methods relied on expensive molecular beam epitaxy processes, but colloidal synthesis allows for cheaper manufacture. A thin film of nanocrystals is obtained by a process known as \"spin-coating\". This involves placing an amount of the quantum dot solution onto a flat substrate, which is then rotated very quickly. The solution spreads out uniformly, and the substrate is spun until the required thickness is achieved.\n\nQuantum dot based photovoltaic cells based on dye-sensitized colloidal TiO films were investigated in 1991\nand were found to exhibit promising efficiency of converting incident light energy to electrical energy, and to be incredibly encouraging due to the low cost of materials used. A single-nanocrystal (channel) architecture in which an array of single particles between the electrodes, each separated by ~1 exciton diffusion length, was proposed to improve the device efficiency and research on this type of solar cell is being conducted by groups at Stanford, Berkeley and the University of Tokyo.\n\nAlthough research is still in its infancy, in the future nanocrystal photovoltaics may offer advantages such as flexibility (quantum dot-polymer composite photovoltaics)\nlower costs, clean power generation and an efficiency of 65%, compared to around 20 to 25% for first-generation, crystalline silicon-based photovoltaics.\n\nIt is argued that many measurements of the efficiency of the nanocrystal solar cell are incorrect and that nanocrystal solar cells are not suitable for large scale manufacturing.\n\nRecent research has experimented with lead selenide (PbSe) semiconductor, as well as with cadmium telluride photovoltaics (CdTe), which has already been well established in the production of second-generation thin film solar cells. Other materials are being researched as well.\n\n\n\n"}
{"id": "22808542", "url": "https://en.wikipedia.org/wiki?curid=22808542", "title": "National e-Governance Plan", "text": "National e-Governance Plan\n\nThe National e-Governance Plan (NeGP) is an initiative of the Government of India to make all government services available to the citizens of India via electronic media. NeGP was formulated by the Department of Electronics and Information Technology (DeitY) and Department of Administrative Reforms and Public Grievances (DARPG). The Government approved the National e-Governance Plan, consisting of 27 \"Mission Mode Projects\" (MMPs) and Tencomponents, on 18 May 2006. This is an enabler of Digital India initiative, and UMANG (Unified Mobile Application for New-age Governance) in turn is an enabler of NeGP.\n\nThe 11th report of the Second Administrative Reforms Commission, titled \"Promoting e-Governance - The Smart Way Forward\", established the government's position that an expansion in e-Government was necessary in India. The ARC report was submitted to the Government of India on 20 December 2008. The report cited several prior initiatives as sources of inspiration, including references to the Singapore ONE programme. To pursue this goal, the National e-Governance Plan was formulated by the Department of Information Technology (DIT) and Department of Administrative Reforms & Public Grievances (DAR&PG). The program required the development of new applications to allow citizen access to government services through Common Service Centers; it aimed to both reduce government costs and improve access to services.\n\nLack of needs analysis, business process re-engineering, interoperability across MMPs, and coping with new technology trends (such as mobile interfaces, cloud computing, and digital signatures) were some of the limitations of the initiative.\n\n\n"}
{"id": "14065418", "url": "https://en.wikipedia.org/wiki?curid=14065418", "title": "Non-stick surface", "text": "Non-stick surface\n\nA non-stick surface is a surface engineered to reduce the ability of other materials to stick to it. Non-stick cookware is a common application, where the non-stick coating allows food to brown without sticking to the pan. Non-stick is often used to refer to surfaces coated with polytetrafluoroethylene (PTFE), a well-known brand of which is \"Teflon.\" In the twenty-first century other coatings have been marketed as non-stick, such as anodized aluminium, ceramics, silicone, enameled cast iron, and seasoned cookware. Superhydrophobic coating is the newest non-stick coating for sale.\n\nThe Mycenaean Greeks might have used non-stick pans to make bread more than 3,000 years ago. Mycenaean ceramic griddles had one smooth side and one side covered with tiny holes. The bread was probably placed on the side with the holes, since the dough tended to stick when cooked on the smooth side of the pan. The holes seem to be an ancient non-sticking technology, ensuring that oil spread evenly over the griddle.\n\nThe modern non-stick pans were made using a coating of Teflon (polytetrafluoroethylene or PTFE). PTFE was invented serendipitously by Roy Plunkett in 1938, while working for a joint venture of the DuPont company. The substance was found to have several unique properties, including very good corrosion-resistance and the lowest coefficient of friction of any substance yet manufactured. PTFE was first used to make seals resistant to the uranium hexafluoride gas used in development of the atomic bomb during World War II, and was regarded as a military secret. Dupont registered the Teflon trademark in 1944 and soon began planning for post-war commercial use of the new product.\n\nBy 1951 Dupont had developed applications for Teflon in commercial bread and cookie-making; however, the company avoided the market for consumer cookware due to potential problems associated with release of toxic gases if stove-top pans were overheated in inadequately ventilated spaces. While working at DuPont, NYU Tandon School of Engineering alumnus John Gilbert was asked to evaluate a newly- developed material called Teflon. His experiments using the fluorinated polymer as a surface coating for pots and pans helped usher in a revolution in non-stick cookware.\n\nA few years later, a French engineer had begun coating his fishing gear with Teflon to prevent tangles. His wife Colette suggested using the same method to coat her cooking pans. The idea was successful and a French patent was granted for the process in 1954. The Tefal company was formed in 1956 to manufacture non-stick pans.\n\nNot all non-stick pans use Teflon; other non-stick coatings have become available. For example, a mixture of titanium and ceramic can be sandblasted onto the pan surface, and then fired at to produce a non-stick ceramic coating.\n\n\nPolytetrafluoroethylene (PTFE) is a synthetic fluoropolymer used in various applications including non-stick coatings. Teflon is a brand of PTFE, often used as a generic term for PTFE. The metallic substrate is roughened by abrasive blasting, then sometimes electric-arc sprayed with stainless steel. The irregular surface promotes adhesion of the PTFE and also resists abrasion of the PTFE. Then one to seven layers of PTFE are sprayed or rolled on, with a larger number of layers and spraying being better. The number and thickness of the layers and quality of the material determine the quality of the non-stick coating. Better-quality coatings are more durable, and less likely to peel and flake, and keep their non-stick properties for longer. Any PTFE-based coating will rapidly lose its non-stick properties if overheated; all manufacturers recommend that temperatures be kept below, typically, .\n\nUtensils used with PTFE-coated pans can scratch the coating, if the utensils are harder than the coating; this can be prevented by using non-metallic (usually plastic or wood) cooking tools.\n\nWhen pans are overheated beyond approximately 350 °C (660 °F) the PTFE coating begins to dissociate, releasing byproducts (PFOA) which can cause polymer fume fever in humans and can be lethal to birds. Concerns have been raised over the possible negative effects of using PTFE-coated cooking pans.\n\nProcessing of PTFE in the past used to include Perfluorooctanoic acid (PFOA) as an emulsifier; however, PFOA is a persistent organic pollutant and poses both environmental and health concerns, and is now being phased out of use in PTFE processing.\n\nPTFE flakes that come off PTFE cookware poses no danger when ingested. This is because PTFE flakes are inert so they will travel through the body without being absorbed.\n\nWith other types of pans, some oil or fat is required to prevent hot food from sticking to the pan's surface. Food does not have the same tendency to stick to a non-stick surface; pans can be used with less, or no, oil, and are easier to clean, as residues do not stick to the surface.\n\nAccording to writer Tony Polombo, pans that are not non-stick are better for producing pan gravy, because the fond (the caramelized drippings that stick to the pan when meat is cooked) sticks to them, and can be turned into pan gravy by deglazing them—dissolving them in liquid.\n\n\n"}
{"id": "3535793", "url": "https://en.wikipedia.org/wiki?curid=3535793", "title": "OpenDocument adoption", "text": "OpenDocument adoption\n\nThe following article details governmental and other organizations from around the world who are in the process of evaluating the suitability of using (adopting) OpenDocument, an open document file format for saving and exchanging office documents that may be edited.\n\nThe OpenDocument format (ODF) was accepted as a standard by OASIS in May 2005, and by ISO in November 2006, as standard ISO/IEC 26300:2006.\n\nMicrosoft submitted another format, Office Open XML (aka OOXML), to Ecma International where it was accepted as a standard in December 2006. The Office Open XML specification was published as standard ISO/IEC 29500:2008 in November 2008.\n\nOpenDocument has been officially approved by national standards bodies of Brazil, Croatia, Denmark, Ecuador, Hungary, Italy, Malaysia, Russia, South Korea, South Africa and Sweden.\n\nNATO with its 28 members (Albania, Belgium, Bulgaria, Canada, Croatia, the Czech Republic, Denmark, Estonia, France, Germany, Greece, Hungary, Iceland, Italy, Latvia, Lithuania, Luxembourg, the Netherlands, Norway, Poland, Portugal, Romania, Slovakia, Slovenia, Spain, Turkey, the UK, and the USA) uses ODF as a mandatory standard for all members.\n\nOn October 23, 2007, the Department of Public Service and Administration of the South African government released a report on interoperability standards in government information systems. It specifies ODF as the standard for \"working office document formats\" (with UTF-8/ASCII text and comma-separated values data as the only alternatives).\n\nSince April 2008 ODF is a national standard too, not only the standard to be used by government departments. South African code for the ODF standard is \"SANS 26300:2008/ISO/IEC 26300:2006\". By September 2008 all departments will be able to read and write in the Open Document Format. In 2009, ODF will become the default document format for South African government departments.\n\nThe Hong Kong government releases an Interoperability Framework\nevery year recommending file formats for various tasks. In their latest version they recommend the use of Microsoft Office '97 or OpenOffice.org v2.0 (based on OpenDocument 1.0) file formats for collaborative editing of text documents, spreadsheets and slideshow presentations.\n\nChandershekhar, India's secretary of Ministry of Information and Technology, said, \"We are glad to note that with formation of a National ODF alliance, India too would be playing a pivotal role in spearheading the ODF revolution. Further, considering the huge potential of eGovernance in the nation as well as the need to adopt open standards to make our data systems more inter-operable and independent of any limiting proprietary tools, we feel that ODF is a great technological leap and a big boon to further propel IT right to India's grass root levels. I congratulate this initiative of leading private & public organisations and wish them all the best in this endeavor.\"\n\nThe Allahabad High Court of India has decided, as policy, to use OpenDocument format for its documents.\n\nGovernment agencies are required to:\n\nIn 2007 Kerala released an Information Technology Policy\ndesigned to turn Kerala into a knowledge society.\n\nThey decided that open standards such as ODF would be followed in e-governance projects to avoid proprietary lock in.\n\nOn June 29, 2007, the government of Japan published a new interoperability framework which gives preference to the procurement of products that follow open standards including the ODF standards. On July 2 the government declared that they wouldn't stop from adopting alternative document formats, because they hold the view that formats like Office Open XML which other organizations such as Ecma International and ISO had also approved was, according to them, an open standard, too. Also, they said that it was one of the preferences, whether the format is open, to choose which software the government shall deploy.\n\nGovernment ministries and agencies are required to:\n\nOpen formats:\n\nJapan's Diet passed an open standards software incentive as part of its omnibus Special Taxation Measures law:\n\nIn August 2007, The Malaysian government announced plans to adopt open standards and the Open Document Format (ODF) within the country's public sector. The Malaysian Administration Modernization and Management Planning Unit (MAMPU) issued a tender for a nine-month study to evaluate the usage of open standards.\n\nFrom April 2008 on the use of ODF is mandatory within the public sector.\n\nSouth Korean government adopted OpenDocument as a part of Korean Industrial Standards KS X ISO/IEC 26300 in 2007, but public documents are still made and distributed in .hwp format. There is no regulation of legislation about OpenDocument since 2007.\n\nAlready in 2009 ODF was chosen as official standard. But only from 2014 large scale migrations to ODF (and because of freedom of choice that brought) to LibreOffice started.\n\nThe European Commission has, since at least 2003, been investigating various options for storing documents in an XML-based format, commissioning technical studies such as the \"Valoris Report\". In March 2004, the Telematics between Administrations Committee (TAC) asked an OpenOffice team and a Microsoft team to present on the relative merits of their XML-based office document formats.\n\nIn May 2004, TAC issued a set of recommendations, in particular noting that, \"Because of its specific role in society, the public sector must avoid <nowiki>[a situation where]</nowiki> a specific product is forced on anyone interacting with it electronically. Conversely, any document format that does not discriminate against market actors and that can be implemented across platforms should be encouraged. Likewise, the public sector should avoid any format that does not safeguard equal opportunities to market actors to implement format-processing applications, especially where this might impose product selection on the side of citizens or businesses. In this respect standardisation initiatives will ensure not only a fair and competitive market but will also help safeguard the interoperability of implementing solutions whilst preserving competition and innovation.\" It then issued recommendations, including:\n\nAn official recommendation for a certain format was not issued however.\n\nA memorandum on the use of open standards for creating and exchanging office documents was approved by Belgium's federal Council of Ministers on June 23, 2006. OpenDocument was proposed as the standard for exchanging office documents such as texts, spreadsheets, presentations within the federal civil service.\n\nSince September 2007, every federal government department has to be able to accept and read OpenDocument documents.\n\nGovernment agencies are required to:\n\nThe Danish Parliament has decided that ODF must be used by state authorities after April 1, 2011\n\nUntil then Government agencies are required to:\n\nGovernment entities are permitted to:\n\nFinland's Ministry of Justice has chosen OpenOffice.org and thus the OpenDocument format as their main document format from the beginning of 2007. The decision was made after deep research of ODF possibilities. Other ministries may follow.\n\nAccording to French government's RGI (general interoperability framework), ODF is the \"recommended format for office documents within French administrations\". OOXML is tolerated for \"information exchange needs through tables\".\n\nGovernment agencies are required to:\n\nGovernment agencies are encouraged to:\n\nGovernment agencies are prohibited from:\n\nA large number of Bundesländer, state and goverenmental offices and cities widely use products that support ODF (e.g. StarOffice, OpenOffice.org).\n\nIn December 2008 the governmental IT-Board of the Bundesregierung decided to make use of the ODF-Format in the Federal administration in order to improve IT-security and interoperability.\n\nThe Federal Foreign Office has migrated totally to the use of ODF formats also in the 250 foreign offices abroad (it has reduced its IT costs to a third in comparison to other Ministries ). In a message to the participants of the first international ODF-workshop in October 2007 the Federal Foreign Minister stated: \"The Open Document Format, as a completely open and ISO standardized format, is an excellent vehicle for the free exchange of knowledge and information in the globalized age.\"\n\nThe Federal office for security in IT (Bundesamt für Sicherheit in der Informationstechnik ) uses with StarOffice on all computers the ODF format in a cast deployment.\n\nSince September 2007 all communications with the Federal Court of Justice (Bundesgerichtshof) and the Federal Patent Court (Bundespatentgericht) may be transmitted in the ODF format. The same has already applied for a while to other high courts, i.e. the Bundesarbeitsgericht, the Bundessozialgericht and many other courts in the Bundesland of Nordrhein-Westfalen and of the Free State of Saxony (Sachsen).\n\nFederal agencies will be able to receive, read, send and edit ODF documents beginning no later than 2010.\nUnder Germany's Standards and Architectures for eGovernment Applications 4.0 (SAGA 4.0), ODF recommended for editable text documents, a multi-stakeholder initiative that recommends and mandates standards to be used by the German federal government.\n\nThe City of Freiburg in Baden-Württemberg uses OpenOffice.org and OpenDocument.\nThe City of Munich in Bavaria has already migrated 14000 desktops to OpenOffice.org and OpenDocument while migrating as a whole to Linux.\n\nThe Italian standardization organization UNI adopted ISO/IEC 26300 (ODF 1.0) on January 26, 2007 (UNI CEI ISO/IEC 26300:2007).\n\nThe Italian region of Umbria announced its migration to LibreOffice and OpenDocument format\n\nIn 2015, the Italian Ministry of Defence announced that it would standardise on ODF and install LibreOffice on 150,000 PCs.\n\nFrom the beginning of 2009 onwards, open source software and the ODF format will be the standard for reading, publishing and the exchange of information for all governmental organisations. Whenever the software used is not open source special reasons have to be given.\n\nNorway's Ministry of Government Administration and Reform decided in December 2007 that ODF (ISO/IEC 26300) MUST be used from 2009 when publishing documents that are meant to be changed after downloading, i.e. forms that are to be filled in by the user. So all website forums will use this format.\n\nFrom 2010 it is mandatory for all government agencies to use document formats (PDF or) ODF when exchanging documents as attachments to e-mail between government and users.\n\nIn 2008, the Portuguese Parliament discussed a bill proposed by the PCP) determining that the adoption of open standard formats – namely ODF – shall be mandatory within all public administration agencies.\n\nOn 21 June 2011, the government published a law on Open Standards. On 8 November 2012, the list of mandatory standards was published: ODF version 1.1 becomes obligatory in July 2014.\n\nIn March 2012, the Câmara Municipal de Vieira do Minho (county of Vieira do Minho) in Portugal announced its migration to LibreOffice.\n\nDespite the heated discussions about adopting Free Software and Open Standards in Romanian public administration - especially at eLiberatica 2009 - the Romanian Ministry for Communications and Information Society passed no official bill to enforce the use of Open Standards, particularly OpenDocument. In June 2009, the ministry Gabriel Sandu declared, in an interview to Ziarul Financiar, that „we cannot give up Microsoft licenses overnight”, despite the harsh critics from the large national Free Software community and a few important local IT businesses.\n\nHowever, the OpenDocument format and OpenDocument-capable software are widely used, not only by enthusiasts, but by businesses as well, including large parts of the Romanian government agencies, mayoralties, courts, notaries, insurance firms, accountants, engineers, etc. A (grossly incomplete) list of implementations, including Romania, is maintained here \n\nThe Russian standardization organization Federal Agency on Technical Regulating and Metrology adopted ISO/IEC 26300 as national standard GOST R ISO/IEC 26300-2010 (ГОСТ Р ИСО/МЭК 26300-2010) on December 21, 2010.\nGovernment administration is required to:\n\nIf any other specialized file format is used for technical reasons, all parties in the document exchange must agree that there is a technical interoperability for using that file format.\n\nIn Slovakia, all public authorities should be able to read ODF format since August 2006 and can use this format for electronic communication and for publication of documents.\nSince October 2008, public authorities must be able to read text documents in ODF 1.0 format.\nSince July 2010, public authorities must be able to read text documents in ODF format up to version 1.2.\n\nSince March 2009, documents in ODF 1.0 format are allowed for use with the electronic signature and qualified electronic signature.\n\nGovernment agencies are required to:\n\nGovernment agencies are required to:\n\nGovernment agencies are required to:\n\nSweden has published ODF 1.0 as a national in August 2008. This has not been announced officially. The standards institute only added the prefix \"SS\" before the ISO number SS-ISO/IEC 26300:2008.\n\nA memorandum on the use of open standards for creating and exchanging office documents is being strongly suggested by Turkish Ministry of Development since 2008. According to \"Interoperability Report\" Turkish Ministry of Development, government agencies are required to:\n\nUnder the “Open Source, Open Standards and Re–Use: Government Action Plan”\n\nBECTA (British Education Communication Technology Agency) is the UK agency in charge of defining information technology (IT) policy for all schools in the United Kingdom, including standards for all the schools' infrastructure. In 2005 they published a comprehensive document describing the policy for infrastructure in schools.\n\nThis document highly recommends the use of OpenDocument and a few other formats for office document data. BECTA explains this as follows: \"Any office application used by institutions must be able to be saved to (and so viewed by others) using a commonly agreed format that ensures an institution is not locked into using specific software. The main aim is for all office based applications to provide functionality to meet the specifications described here (whether licensed software, open source or unlicensed freeware) and thus many application providers could supply the educational institution ICT market.\".\nIn July 2014, the UK Government formally adopted the use of OpenDocument for document exchange.\nBristol City Council has adopted the StarOffice suite and with it the OpenDocument format across 5500 desktop computers.\n\nIn September 2007 the Argentinian Province of Misiones decided via decrete that the use of ODF shall be mandatory within the government. Around a million people live in this province, which is one of the 23 provinces of Argentina.\n\nWith the publication of \"e-Ping Interoperability Framework\", Brazil became the first South American country to officially recommend the adoption of OpenDocuments within the government.\n\nAs stated in version v3.0 of 2007: \"Preferred adoption of Open Formats: e-PING defines that, whenever possible, open standards will be used in technical specifications. \"Proprietary standards\" will be accepted, in this transition period, with the perspective of replacement as soon as there are conditions for a complete migration. With no loss to these goals, are to be respected those situations when there is the need to consider security requisites and information integrity. When available, Free Software solutions are to be considered preferential, accordingly with the policy defined by the Comitê Executivo de Governo Eletrônico (CEGE)\"\n\nSince April 2008 ODF is a national standard in Brazil, coded as NBRISO/IEC26300.\n\nAs of the 2010 version, the transition period has officially ended, and proprietary document formats may no longer be used in the federal public administration, although in practice this is not the case in many federal agencies and departments.\n\nAdditionally, a number of parties, including local governments, companies and non-governmental organizations have signed into the Brasília Protocol, which formalize intentions and set goals for adopting the use of the Open Document standard.\n\nGovernment agencies and state-owned companies are required to:\n\nSince June 2008 the \"Agency for the Development of Government Electronic Management and Information and Knowledge Society of Uruguay\" recommends that public documents use either ODF or PDF. ODF should be used for documents in the process of being edited and the latter for documents in final form.\nAll organizations of the Federal Government of Venezuela must:\n\nCanadian governments do not have mandatory file formats. Microsoft office products are widely used by all levels of governments in Canada.\n\nThe United States state of Massachusetts has been examining its options for implementing XML-based document processing. In early 2005, Eric Kriss, Secretary of Administration and Finance in Massachusetts, was the first government official in the United States to publicly connect open formats to a public policy purpose: \"It is an overriding imperative of the American democratic system that we cannot have our public documents locked up in some kind of proprietary format, perhaps unreadable in the future, or subject to a proprietary system license that restricts access.\"\n\nAt a September 16, 2005 meeting with the Mass Technology Leadership Council Kriss stated that he believes this is fundamentally an issue of sovereignty. While supporting the principle of private intellectual property rights, he said sovereignty trumped any private company's attempt to control the state's public records through claims of intellectual property.\n\nSubsequently, in September 2005, Massachusetts became the first state to formally endorse OpenDocument formats for its public records and, at the same time, reject Microsoft's new XML format, now standardized as ISO/IEC 29500:2008 — Office Open XML. This decision was made after a two-year examination of file formats, including many discussions with Microsoft, other vendors, and various experts, plus some limited trial programs in individual communities like Saugus and Billerica. Microsoft Office, which has a nearly 100% market share among the state's employees, did not support OpenDocument formats until Service Pack 2 of Office 2007. Microsoft had indicated that OpenDocument formats will not be supported in new versions of Office, even though they support many other formats (including ASCII, RTF, and WordPerfect), and analysts believe it would be easy for Microsoft to implement the standard. If Microsoft chooses not to implement OpenDocument, Microsoft will disqualify themselves from future consideration. Several analysts (such as Ovum) believe that Microsoft will eventually support OpenDocument. On 6 July 2006 Microsoft announced that they would support the OpenDocument format and create a plugin to allow Office to save to ODF.\n\nAfter this announcement by Massachusetts supporting OpenDocument, a large number of people and organizations spoke up about the policy, both pro and con (see the references section). Adobe, Corel, IBM, and Sun all sent letters to Massachusetts supporting the measure. In contrast, Microsoft sent in a letter highly critical of the measure. A group named \"Citizens Against Government Waste\" (CAGW) also opposed the decision. The group claimed that Massachusetts' policy established \"an arbitrary preference for open source,\" though both open source software and proprietary software can implement the specification, and both kinds of developers were involved in creating the standard (CAGW, 2005). However, InternetNews and Linux Weekly News noted that CAGW has received funding from Microsoft, and that in 2001 CAGW was caught running an astroturfing campaign on behalf of Microsoft when two letters they submitted supporting Microsoft in Microsoft's anti-trust case were found to have the signatures of deceased persons (Linux Weekly News). James Prendergast, executive director of a coalition named \"Americans for Technology Leadership\" (ATL), also criticized the state's decision in a Fox News article. In the article, Prendergast failed to disclose that Microsoft is a founding member of ATL. Fox News later published a follow-up article disclosing that fact.\n\nState Senator Marc R. Pacheco and State Secretary William F. Galvin have expressed reservations about this plan. Pacheco held a hearing on October 31, 2005, on the topic of OpenDocument. Pacheco did not want OpenDocument to be declared as the executive branch standard, primarily on procedural grounds. Pacheco believed that the executive branch had to receive permission to set an executive standard from the multi-branch IT Advisory Board. In contrast, The Massachusetts Information Technology Division (ITD), and its general council, believe the Advisory board's role is to advise ITD, and ITD did discuss the issue with the IT Advisory Board, but ITD's Peter Quinn and Linda Hamel (ITD's General Counsel) asserted that there is no requirement that \"ITD approach the Advisory Board for permission to adopt policies that will impact only the Executive Department.\" Hamel later filed a legal briefing justifying ITD's position (Hamel, 2005). Massachusetts' Supreme Court has ruled that the various branches of government are prohibited from mandating IT standards on each other; this ruling appears to support ITD's claim. Pacheco also did not like the process used to select OpenDocument. However, Pacheco appears to have had many fundamental\nmisunderstandings of the issues. Andy Updegrove said that at the time, \"Senator Pacheco doesn't understand the difference between open source and open standards (and certainly doesn't understand the difference between OpenDocument and OpenOffice). More than once, he indicated that he thought that the policy would require the Executive Agencies to use OpenOffice.org, not realizing that there are other compliant alternatives. He also thought that this would act to the detriment of Massachusetts software vendors, who (he thinks) would be excluded from doing business with the Commonwealth.\" Pacheco also thought that OpenOffice.org was under the GPL, but in fact it is released under the LGPL (Jones, October 31, 2005) (Jones, November 14, 2005). He attempted to halt implementation of OpenDocument in the executive branch via an amendment (to S. 2256), but the amended bill was never sent to the governor.\n\nSince then in 2007 Massachusetts has amended its approved technical standards list to include Office Open XML.\n\nOfficial Information Documents from the Commonwealth of Massachusetts:\n\nIn November 2005, James Gallt, associate director for the National Association of State Chief Information Officers, said that a number of other state agencies are also exploring the use of OpenDocument (LaMonica, November 10, 2005).\n\nIn April 2006, a bill was introduced in the Minnesota state legislature to require all state agencies to use open data formats. It is expected that the OpenDocument Format will be advanced as a way of meeting the proposed requirement. (Gardner, April 7, 2006).\n\nIn late 2007 and early 2008, New York State issued a Request for Public Comment concerning electronic records policy.\n\n\nIt was announced on 31 March 2006, that the National Archives of Australia had settled on OpenDocument as their choice for a cross-platform/application document format.\n\nAccording to OASIS' OpenDocument datasheet, \"Singapore's Ministry of Defence, France's Ministry of Finance and its Ministry of Economy, Finance, and Industry, Brazil's Ministry of Health, the City of Munich, Germany, UK's Bristol City Council, and the City of Vienna in Austria are all adopting applications that support OpenDocument.\" (OASIS, 2005b).\n\n"}
{"id": "32627395", "url": "https://en.wikipedia.org/wiki?curid=32627395", "title": "Operation Shady RAT", "text": "Operation Shady RAT\n\nOperation Shady RAT is an ongoing series of cyber attacks starting in mid-2006 reported by Dmitri Alperovitch, Vice President of Threat Research at Internet security company McAfee in August 2011, who also led and named the Night Dragon Operation and Operation Aurora cyberespionage intrusion investigations. The attacks have hit at least 71 organizations, including defense contractors, businesses worldwide, the United Nations, and the International Olympic Committee.\n\nThe operation, named by Alperovitch as a derivation of the common computer security industry acronym for remote access tool, is characterized by McAfee as \"a five year targeted operation by one specific actress\". The report suggests that the targeting of various athletic oversight organizations around the time of the 2008 Summer Olympics \"potentially pointed a finger at a state actor behind the intrusions\". That state actor is widely assumed to be the People's Republic of China.\n\n"}
{"id": "3113736", "url": "https://en.wikipedia.org/wiki?curid=3113736", "title": "Plate heat exchanger", "text": "Plate heat exchanger\n\nA plate heat exchanger is a type of heat exchanger that uses metal plates to transfer heat between two fluids. This has a major advantage over a conventional heat exchanger in that the fluids are exposed to a much larger surface area because the fluids are spread out over the plates. This facilitates the transfer of heat, and greatly increases the speed of the temperature change. Plate heat exchangers are now common and very small brazed versions are used in the hot-water sections of millions of combination boilers. The high heat transfer efficiency for such a small physical size has increased the domestic hot water (DHW) flowrate of combination boilers. The small plate heat exchanger has made a great impact in domestic heating and hot-water. Larger commercial versions use gaskets between the plates, whereas smaller versions tend to be brazed.\n\nThe concept behind a heat exchanger is the use of pipes or other containment vessels to heat or cool one fluid by transferring heat between it and another fluid. In most cases, the exchanger consists of a coiled pipe containing one fluid that passes through a chamber containing another fluid. The walls of the pipe are usually made of metal, or another substance with a high thermal conductivity, to facilitate the interchange, whereas the outer casing of the larger chamber is made of a plastic or coated with thermal insulation, to discourage heat from escaping from the exchanger.\n\nThe plate heat exchanger (PHE) was invented by Dr Richard Seligman in 1923 and revolutionised methods of indirect heating and cooling of fluids. Dr Richard Seligman founded APV in 1910 as the Aluminium Plant & Vessel Company Limited, a specialist fabricating firm supplying welded vessels to the brewery and vegetable oil trades.\n\nThe plate heat exchanger (PHE) is a specialized design well suited to transferring heat between medium- and low-pressure fluids. Welded, semi-welded and brazed heat exchangers are used for heat exchange between high-pressure fluids or where a more compact product is required. In place of a pipe passing through a chamber, there are instead two alternating chambers, usually thin in depth, separated at their largest surface by a corrugated metal plate. The plates used in a plate and frame heat exchanger are obtained by one piece pressing of metal plates. Stainless steel is a commonly used metal for the plates because of its ability to withstand high temperatures, its strength, and its corrosion resistance.\n\nThe plates are often spaced by rubber sealing gaskets which are cemented into a section around the edge of the plates. The plates are pressed to form troughs at right angles to the direction of flow of the liquid which runs through the channels in the heat exchanger. These troughs are arranged so that they interlink with the other plates which forms the channel with gaps of 1.3–1.5 mm between the plates. The plates are compressed together in a rigid frame to form an arrangement of parallel flow channels with alternating hot and cold fluids. The plates produce an extremely large surface area, which allows for the fastest possible transfer. Making each chamber thin ensures that the majority of the volume of the liquid contacts the plate, again aiding exchange. The troughs also create and maintain a turbulent flow in the liquid to maximize heat transfer in the exchanger. A high degree of turbulence can be obtained at low flow rates and high heat transfer coefficient can then be achieved.\n\nAs compared to shell and tube heat exchangers, the temperature approach in a plate heat exchangers may be as low as 1 °C whereas shell and tube heat exchangers require an approach of 5 °C or more. For the same amount of heat exchanged, the size of the plate heat exchanger is smaller, because of the large heat transfer area afforded by the plates (the large area through which heat can travel). Increase and reduction of the heat transfer area is simple in a plate heat-exchanger, through the addition or removal of plates from the stack.\n\nAll plate heat exchangers look similar on the outside. The difference lies on the inside, in the details of the plate design and the sealing technologies used. Hence, when evaluating a plate heat exchanger, it is very important not only to explore the details of the product being supplied but also to analyze the level of research and development carried out by the manufacturer and the post-commissioning service and spare parts availability.\n\nAn important aspect to take into account when evaluating a heat exchanger are the forms of corrugation within the heat exchanger. There are two types: intermating and chevron corrugations. In general, greater heat transfer enhancement is produced from chevrons for a given increase in pressure drop and are more commonly used than intermating corrugations.\n\nTo achieve improvement in PHE's, two important factors namely amount of heat transfer and pressure drop have to be considered such that amount of heat transfer needs to be increased and pressure drops need to be decreased. In plate heat exchangers due to presence of corrugated plate, there is a significant resistance to flow with high friction loss. Thus to design plate heat exchangers, one should consider both factors.\nFor various range of Reynolds numbers, many correlations and chevron angles for plate heat exchangers exist. The plate geometry is one of the most important factor in heat transfer and pressure drop in plate heat exchangers, however such a feature is not accurately prescribed. In the corrugated plate heat exchangers, because of narrow path between the plates, there is a large pressure capacity and the flow becomes turbulent along the path. Therefore, it requires more pumping power than the other types of heat exchangers. Therefore, higher heat transfer and less pressure drop are targeted. The shape of plate heat exchanger is very important for industrial applications that are affected by pressure drop.\n\nDesign calculations of a plate heat exchanger include flow distribution and pressure drop and heat transfer. The former is an issue of Flow distribution in manifolds. A layout configuration of plate heat exchanger can\nbe usually simplified into a manifold system with two manifold\nheaders for dividing and combining fluids, which can be\ncategorized into U-type and Z-type arrangement according to\nflow direction in the headers, as shown in manifold arrangement. Bassiouny and Martin developed the previous theory of design. In recent years Wang unified all the main existing models and developed a most completed theory and design tool.\n\nThe total rate of heat transfer between the hot and cold fluids passing through a plate heat exchanger may be expressed as:\nQ = UA∆Tm\nwhere U is the Overall heat transfer coefficient, A is the total plate area, and ∆Tm is the Log mean temperature difference. U is dependent upon the heat transfer coefficients in the hot and cold streams.\n\n\n\n"}
{"id": "8646603", "url": "https://en.wikipedia.org/wiki?curid=8646603", "title": "Pleated blinds", "text": "Pleated blinds\n\nPleated blinds are shades made from a pleated fabric (which helps to add texture to a room) that pull up to sit flat at the top of a window to hide from sight when open.\n\nHoneycomb pleated blinds, or cellular blinds (actually cellular shades) are similar to pleated blinds except that they are made up of two or more layers joined at the pleats to form 'cellular' compartments that trap air, providing insulation. Due to their cellular construction, cellular blinds (aka energy saver) are known to the energy conscious as one of the highest energy-efficient and sound-absorbent blinds of any window treatment. For greater insulation, cellular blinds are available in a variety of cell sizes, including single cell, single cell, double cell and even triple cell. The more cells, the greater energy efficiency.\n\nOne disadvantage with these cellular pleated blinds is that small insects crawl into the hollow chambers and die. Removing them is not easy.\n\nAll of the above can also be motorized, which is a great option for safety in homes with children and/or pets.\n\n"}
{"id": "7530857", "url": "https://en.wikipedia.org/wiki?curid=7530857", "title": "Resonant-tunneling diode", "text": "Resonant-tunneling diode\n\nA resonant-tunneling diode (RTD) is a diode with a resonant-tunneling structure in which electrons can tunnel through some resonant states at certain energy levels. The current–voltage characteristic often exhibits negative differential resistance regions.\n\nAll types of tunneling diodes make use of quantum mechanical tunneling.\nCharacteristic to the current–voltage relationship of a tunneling diode is the presence of one or more negative differential resistance regions, which enables many unique applications. Tunneling diodes can be very compact and are also capable of ultra-high-speed operation because the quantum tunneling effect through the very thin layers is a very fast process. One area of active research is directed toward building oscillators and switching devices that can operate at terahertz frequencies.\n\nAn RTD can be fabricated using many different types of materials (such as III–V, type IV, II–VI semiconductor) and different types of resonant tunneling structures, such as the heavily doped p–n junction in Esaki diodes, double barrier, triple barrier, quantum well, or quantum wire. The structure and fabrication process of Si/SiGe resonant interband tunneling diodes are suitable for integration with modern Si complementary metal–oxide–semiconductor (CMOS) and Si/SiGe heterojunction bipolar technology.\n\nOne type of RTDs is formed as a single quantum well structure surrounded by very thin layer barriers. This structure is called a double barrier structure. Carriers such as electrons and holes can only have discrete energy values inside the quantum well. When a voltage is placed across an RTD, a terahertz wave is emitted, which is why the energy value inside the quantum well is equal to that of the emitter side. As voltage is increased, the terahertz wave dies out because the energy value in the quantum well is outside the emitter side energy.\n\nAnother feature seen in RTD structures is the negative resistance on application of bias as can be seen in the image generated from Nanohub. The forming of negative resistance will be examined in detail in operation section below.\n\nThis structure can be grown by molecular beam heteroepitaxy. GaAs and AlAs in particular are used to form this structure. AlAs/InGaAs or InAlAs/InGaAs can be used.\n\nThe operation of electronic circuits containing RTDs can be described by a Liénard system of equations, which are a generalization of the Van der Pol oscillator equation.\n\nThe following process is also illustrated from rightside figure. Depending on the number of barriers and number of confined states inside the well, the process described below could be repeated.\n\nFor low bias, as the bias increase, the 1st confined state between the potential barriers is getting closer to the source Fermi level, so the current it carries increases.\n\nAs bias increases further, the 1st confined state becomes lower in energy and gradually goes into the energy range of bandgap, so the current it carries decreases. At this time, the 2nd confined state is still too high above in energy to conduct significant current.\n\nSimilar to the first region, as the 2nd confined state becomes closer and closer to source Fermi level, it carries more current, causing the total current to increase again.\n\nIn quantum tunneling through a single barrier, the transmission coefficient, or the tunneling probability, is always less than one (for incoming particle energy less than the potential barrier height). Considering a potential profile which contains two barriers (which are located close to each other), one can calculate the transmission coefficient (as a function of the incoming particle energy) using any of the standard methods.\n\nTunneling through a double barrier was first solved in the Wentzel-Kramers-Brillouin (WKB) approximation by David Bohm in 1951, who pointed out the resonances in the transmission coefficient occur at certain incident electron energies. It turns out that, for certain energies, the transmission coefficient is equal to one, i.e. the double barrier is totally transparent for particle transmission. This phenomenon is called resonant tunneling. It is interesting that while the transmission coefficient of a potential barrier is always lower than one (and decreases with increasing barrier height and width), two barriers in a row can be completely transparent for certain energies of the incident particle.\n\nLater, in 1964, L. V. Iogansen discussed the possibility of resonant transmission of an electron through double barriers formed in semiconductor crystals. In the early 1970s, Tsu, Esaki, and Chang computed the two terminal current-voltage (I-V) characteristic of a finite superlattice, and predicted that resonances could be observed not only in the transmission coefficient but also in the I-V characteristic. Resonant tunneling also occurs in potential profiles with more than two barriers. Advances in the MBE technique led to observation of negative differential conductance (NDC) at terahertz frequencies, as reported by Sollner et al. in the early 1980s. This triggered a considerable research effort to study tunneling through multi-barrier structures.\n\nThe potential profiles required for resonant tunneling can be realized\nin semiconductor system using heterojunctions which utilize semiconductors\nof different types to create potential barriers or wells in the conduction\nband or the valence band.\n\nResonant tunneling diodes are typically realized in III-V compound material systems, where heterojunctions made up of various III-V compound semiconductors are used to create the double or multiple potential barriers in the conduction band or valence band. Reasonably high performance III-V resonant tunneling diodes have been realized. Such devices have not entered mainstream applications yet because the processing of III-V materials is incompatible with Si CMOS technology and the cost is high.\n\nMost of semiconductor optoelectronics use III-V semiconductors and so it is possible to combine III-V RTDs to make OptoElectronic Integrated Circuits (OEICS) that use the negative differential resistance of the RTD to provide electrical gain for optoelectronic devices. Recently, the device to device variability in an RTDs current-voltage characteristic has been used as a way to uniquely identify electronic devices, in what is known as a quantum confinement physical unclonable function (QC-PUF).\n\nResonant tunneling diodes can also be realized using the Si/SiGe materials system. Both hole tunneling and electron tunneling have been observed. However, the performance of Si/SiGe resonant tunneling diodes was limited due to the limited conduction band and valence band discontinuities between Si and SiGe alloys. Resonant tunneling of holes through Si/SiGe heterojunctions was attempted first because of the typically relatively larger valence band discontinuity in Si/SiGe heterojunctions than the conduction band discontinuity for (compressively) strained SiGe layers grown on Si substrates. Negative differential resistance was only observed at low temperatures but not at room temperature. Resonant tunneling of electrons through Si/SiGe heterojunctions was obtained later, with a limited peak-to-valley current ratio (PVCR) of 1.2 at room temperature. Subsequent developments have realized Si/SiGe RTDs (electron tunneling) with a PVCR of 2.9 with a PCD of 4.3 kA/cm and a PVCR of 2.43 with a PCD of 282 kA/cm at room temperature.\n\nResonant interband tunneling diodes (RITDs) combine the structures and behaviors of both \"intraband\" resonant tunneling diodes (RTDs) and conventional \"interband\" tunneling diodes, in which electronic transitions occur between the energy levels in the quantum wells in the conduction band and that in the valence band. Like resonant tunneling diodes, resonant interband tunneling diodes can be realized in both the III-V and Si/SiGe materials systems.\n\nIn the III-V materials system, InAlAs/InGaAs RITDs with peak-to-valley current ratios (PVCRs) higher than 70 and as high as 144 at room temperature and Sb-based RITDs with room temperature PVCR as high as 20 have been obtained. The main drawback of III-V RITDs is the use of III-V materials whose processing is incompatible with Si processing and is expensive.\n\nIn Si/SiGe materials system, Si/SiGe resonant interband tunneling diodes have also been developed which have the potential of being integrated into the mainstream Si integrated circuits technology.\n\nThe five key points to the design are:\n(i) an intrinsic tunneling barrier, \n(ii) delta-doped injectors, \n(iii) offset of the delta-doping planes from the heterojunction interfaces,\n(iv) low temperature molecular beam epitaxial growth (LTMBE), and\n(v) postgrowth rapid thermal annealing (RTA) for activation of dopants and reduction of density of point defects.\n\nA minimum PVCR of about 3 is needed for typical circuit applications. Low current density Si/SiGe RITDs are suitable for low-power memory applications, and high current density tunnel diodes are needed for high-speed digital/mixed-signal applications. Si/SiGe RITDs have been engineered to have room temperature PVCRs up to 4.0. The same structure was duplicated by another research group using a different MBE system, and PVCRs of up to 6.0 have been obtained. In terms of peak current density, peak current densities ranging from as low as 20 mA/cm and as high as 218 kA/cm, spanning seven orders of magnitude, have been achieved. A resistive cut-off frequency of 20.2 GHz has been realized on photolithography defined SiGe RITD followed by wet etching for further reducing the diode size, which should be able to improve when even smaller RITDs are fabricated using techniques such as electron beam lithography.\n\nIn addition to the realization of integration with Si CMOS and SiGe heterojunction bipolar transistors that is discussed in the next section, other applications of SiGe RITD have been demonstrated using breadboard circuits, including multi-state logic.\n\nIntegration of Si/SiGe RITDs with Si CMOS has been demonstrated. Vertical integration of Si/SiGe RITD and SiGe heterojunction bipolar transistors was also demonstrated, realizing a 3-terminal negative differential resistance circuit element with adjustable peak-to-valley current ratio. These results indicate that Si/SiGe RITDs is a promising candidate of being integrated with the Si integrated circuit technology.\n\n"}
{"id": "34706451", "url": "https://en.wikipedia.org/wiki?curid=34706451", "title": "Rumford furnace", "text": "Rumford furnace\n\nA Rumford furnace is a kiln for the industrial scale production in the 19th century of calcium oxide, popularly known as quicklime or burnt lime. It was named after its inventor, Benjamin Thompson, also known as Count Rumford, and is sometimes called a Rüdersdorf furnace after the location where it was first built and from where the design rapidly spread throughout Europe.\n\nRumford’s innovation was to separate the combustion chambers for the limestone and the fuel. In previous designs, the fuel (traditionally wood or charcoal, later coal) was mixed with the limestone before burning, which meant that the quicklime end product was contaminated with ash and had to be laboriously cleaned. By separating the two it was possible to produce large quantities of high-quality quicklime for the booming 19th-century European construction industry. \n\nIn a Rumford furnace, fuel compartments surround the actual combustion chamber, each one being connected to it by a single transverse channel through which the hot air produced by the burning fuel flows into the stack and moves upward through the pile of limestone. The fuel ash waste falls into a separate chamber, from which it can be removed.\nThe limestone in the central shaft collapses as it burns to quicklime, making space for new limestone to be added to the top of the stack. The quicklime cools down as it slowly descends through the space below the channel connecting the stack to the firing compartment and is finally removed through an outlet at the bottom of the shaft. \nThe upper part of the limestone shaft extends above the fuel compartments and its walls contain separate cavities filled with coal dust. This insulation supports the prewarming of the limestone before it moves into the vicinity of the hot air source.\n\nThe first Rumford kiln was constructed in 1802 in Rüdersdorf near Berlin, and one of them can still be explored at the Museumspark Rüdersdorf. The innovation rapidly spread throughout Europe because it made it possible to produce quicklime far more efficiently than predecessor technologies. These required either batch production, waiting for each load to be assembled, burn and cool before removal, or carefully mixing the fuel and limestone in the appropriate proportions, burning them together, and then cleaning the contaminated limestone afterward.\nIn a Rumford kiln continuous operation was possible for the first time, since the addition of fuel and limestone as well as the removal of ash and quicklime could be separately optimized according to need, and all these operations could take place simultaneously.\n\n"}
{"id": "19635871", "url": "https://en.wikipedia.org/wiki?curid=19635871", "title": "SBUV/2", "text": "SBUV/2\n\nThe Solar Backscatter Ultraviolet Radiometer, or SBUV/2, is a series of operational remote sensors on NOAA weather satellites in Sun-synchronous orbits which have been providing global measurements of stratospheric total ozone, as well as ozone profiles, since March 1985. The SBUV/2 instruments were developed from the SBUV experiment flown on the Nimbus-7 spacecraft which improved on the design of the original BUV instrument on Nimbus-4. These are nadir viewing radiometric instruments operating at mid to near UV wavelengths. SBUV/2 data sets overlap with data from SBUV and TOMS instruments on the Nimbus-7 spacecraft. These extensive data sets (January 1979 to the present) measure the density and vertical distribution of ozone in the Earth’s atmosphere from six to 30 miles.\n\nSBUV/2 looks down at the Earth’s atmosphere and the reflected sunlight at wavelengths characteristic of ozone. The SBUV/2 wavelength \"channels\" range from 252 nanometer (nm) to 340 nm. Ozone is measured as a ratio of sunlight incident on the atmosphere to the amount of sunlight scattered back into space. From this information, the total ozone between the instrument and the ground can be calculated.\n\nThe SBUV/2 measures solar irradiance and Earth radiance (backscattered solar energy) in the near ultraviolet spectrum (160 to 400 nm). The SBUV is capable of determining the global ozone concentration in the stratosphere to an absolute accuracy of 1 percent; the vertical distribution of atmospheric ozone to an absolute accuracy of 5 percent; the long-term solar spectral irradiance from 160 to 400 nm Photochemical process and the influence of “trace” constituents on the ozone layer.\n\nThe Ball Aerospace-built SBUV/2 helped to discover the ozone hole over Antarctica in 1987, and continues to monitor this phenomenon. Atmospheric ozone absorbs the sun’s ultraviolet rays, which are believed to cause gene mutations, skin cancer, and cataracts in humans. Ultraviolet rays may also damage crops and aquatic ecosystems. The first SBUV/2 instrument was launched on NOAA-9 in December 1984 and the last instrument in this series was launched in February 2009 aboard the NOAA-19 spacecraft.\n\n\n"}
{"id": "29687730", "url": "https://en.wikipedia.org/wiki?curid=29687730", "title": "Scanning mobility particle sizer", "text": "Scanning mobility particle sizer\n\nA scanning mobility particle sizer (SMPS) is an analytical instrument that measures the size and number concentration of aerosol particles with diameters from 2.5 nm to 1000 nm. They employ a continuous, fast-scanning technique to provide high-resolution measurements.\n\nThe particles that are investigated can be of biological or chemical nature. The instrument can be used for air quality measurement indoors, vehicle exhaust, research in bioaerosols, atmospheric studies, and toxicology testing.\n"}
{"id": "9612499", "url": "https://en.wikipedia.org/wiki?curid=9612499", "title": "Slot (computer architecture)", "text": "Slot (computer architecture)\n\nA slot comprises the operation issue and data paths machinery surrounding a collection of one or more functional units (FUs) which share these resources. The term slot is common for this purpose in the VLIW world where the relationship between operation in an instruction and pipeline to execute it is explicit. In dynamically scheduled machines the concept is more commonly called an execute pipeline.\n\nModern conventional CPUs have several compute pipelines (say two ALU, one FPU, one SSE/MMX, one branch) each of which can issue one instruction per basic cycle but can have several in flight. These are what correspond to slots. The pipelines may have several FUs - an adder and a multiplier, say - but only one FU in a pipeline can be issued to in a particular cycle. The FU population of a pipeline/slot is a design option in a CPU.\n"}
{"id": "42382285", "url": "https://en.wikipedia.org/wiki?curid=42382285", "title": "Social journalism", "text": "Social journalism\n\nSocial journalism is a media model consisting of a hybrid of professional journalism, contributor and reader content. It is similar to open publishing platforms, like Twitter and WordPress.com, except that some or most content is also created and/or screened by professional journalists. Examples include Forbes.com, Medium, BuzzFeed, Soapbox and Gawker. The model, which in some instances has generated monthly audiences in the tens of millions, has been discussed as one way for professional journalism to thrive despite a marked decline in the audience for traditional journalism.\n\nWriting in Re/code, Jonathan Glick, CEO of Sulia, said the model of publishers as platforms (which he calls a \"platisher\") is \"on the rise\". Glick cites as examples Medium (from Twitter co-founders Evan Williams and Biz Stone), Vox Media, Sulia, Skift, First Look Media (backed by eBay founder Pierre Omidyar) and BuzzFeed. On March 12, 2014, Mark Little, the CEO of Storyful.com, now a division of News Corp., proposed \"10 Principles that Power Social Journalism,\" including \"UGC [User Generated Content] is governed by the same legal and ethical code as any other content\" and \"The currency of social journalism is authenticity not authority. We are not experts in every subject.\"\n\nIn an interview in \"The New York Times\", the editor of \"The Guardian\", Alan Rusbridger, said the Guardian was in the process of converting into a platform as well as a publisher. \"For years, news organizations had a quasi monopoly on information simply because we had the means of distribution. I think if as a journalist you are not intensely curious about what has been created by people who are not journalists, then you’re missing out on a lot,\" he said.\n\nOn April 1, 2014, in a column in GigaOM entitled \"Social journalism and open platforms are the new normal — now we have to make them work\" Mathew Ingram asked \"How can media entities take advantage of this phenomenon without losing their way in the process?\" and proceeded to review suggested rules for social journalism proposed by former FastCompany.com president Ed Sussman, an early adopter of the model. Ingram summarized Sussman's suggestions, including clear labeling types of contributors (e.g. staff, guest contributor, reader contribution); establishing guidelines, such as conflict of interest rules, that posters must consent to before posting; providing wiki-like tools for social improvements to content; elevating the best content with curators and algorithms; deleting weak or problematic content via curators or algorithms.\n\nSocial journalism has been attacked by media critic Michael Wolff in \"USA Today\" as the \"Forbes vanity model letting ‘contributors’ write whatever they want under your brand (‘as I wrote in Forbes …’) and not having to pay them anything — ultimately, of course, devaluing your authority.\" \n\nIn a March 20, 2014 op-ed for \"The New York Observer\", former FastCompany.com president Ed Sussman argued that social journalism does not devalue the authority of brands and that the success of Forbes.com in attracting a wide audience with its 1,000+ bloggers proved that the model could be successful for traditional media companies. Following revelations that some Forbes.com contributors used their columns to allegedly participate in a \"pump and dump\" scheme to promote, then sell stocks, Sussman followed up with \"The New Rules of Social Journalism: A Proposal\" in Pando Daily, on March 29, 2014. Sussman proposed various rules for elevating the quality and ethics of social journalism content.\n\nAn early, or perhaps the first \"social journalism\" platform at a major media company was FastCompany.com, in 2008. After the platform launched, in its first six months, FastCompany.com signed up 2,000 bloggers and 50,000 members. \"Fast Company is the first, but certainly not last, mainstream publication to integrate the majority of their site as a social community,\" wrote media analyst Jeremiah Owyang in 2008, then a senior social computing analyst for Forrester Research. After Ed Sussman left the website, the Fast Company print magazine editors reverted it to a standard journalism website.\n"}
{"id": "17091561", "url": "https://en.wikipedia.org/wiki?curid=17091561", "title": "Sophia Bekele", "text": "Sophia Bekele\n\nSophia Bekele Eshete (Amharic: ሶፍያ በቀለ; Addis Ababa, Ethiopia) is a woman of Ethiopian ancestry. She is a business and corporate executive and writer. She is presently the Executive Director and Founder of DotConnectAfrica Trust (DCA).\n\nBekele founded and is the CEO of CBS International and SbCommunications Network, an Ethiopian IT company.\n\nBekele is the daughter of Ato Bekele Eshete Wolde Michael a founder and former Board Director of United Bank,S.C and United Insurance,S.C in Ethiopia and Sister Mulualem Beyene Engida, a medical nurse. She attended a private Catholic high school, then traveled to America to acquire her higher education. \n\nBekele has a Bachelor's Degree in Business Analysis and Information Systems from San Francisco State University and a Master of Business Administration from Golden Gate University. \n\nBekele was recruited out of college by Bank of America and later held managerial positions with UnionBanCal Corporation and PriceWaterhouseCoopers. Her initial field of expertise was Information Security.\n\nBekele set up CBS International, a company based in California focused on technology transfer to emerging economies. Bekele started an Ethiopian company SbCommunications Network, plc, (SbCnet) specializing in systems integration, technology integration and support services.\n\nDuring her tenure at ICANN from 2005-2007, and the Council of the GNSO (Generic Names Supporting Organization), Bekele was instrumental in initiating policy dialogue over IDNs that resulted in the introduction of IDNs under which new Internet Domain names in Arabic, Cyrillic, Russian, Chinese and non-Latin alphabets will become available, thereby providing non-English/non-Latin language native speakers an opportunity to access and communicate on the Internet in their native languages. \n\nIn 2006 Bekele turned her focus to .africa – a new generic Top-Level Domain (gTLD) for the Africa geographic name that she championed during her appointment at ICANN. The project has been introduced and will be delegated for operation under ICANN’s new gTLD programme.\n\nBekele re-started the DotAfrica initiative following the important experiences gained from her work as gNSO advisor to ICANN. Bekele made a clear case for a DotAfrica (.africa) gTLD for Africa within ICANN and also the global Internet Community on behalf of the global African and Pan-African constituency. She soon led the .africa initiative with DotConnectAfrica introducing it to the Pan-African inter-governmental organizations – the United Nations Economic Commission for Africa (UNECA) and the African Union Commission (AUC) and received endorsement for DCA.\nEnterprise Technology Magazine called her \"Sophia Bekele: Champion of .africa initiative\" \nCIO East Africa calling her views \"controversial\" on who should run gTLDs, saying \"Sophia Bekele has been in the news if not for promoting DotConnectAfrica’s bid for the DotAfrica geographical gTLD for the past three years, then in a controversial statement about the same\":\n\nBekele currently oversees the .africa new gTLD initiative of DotConnectAfrica Trust. As Executive Director of DCA Trust & CEO of DotConnectAfrica Registry Services Limited, she has led a multi-national team of experts to prepare and submit an application for the DotAfrica (.AFRICA) generic Top-Level Domain. DotConnectAfrica Trust (DCA Trust) has already submitted an application in May 2012 for the .dotafrica (pronounced as ‘DotAfrica’) geographic name string to the Internet Corporation for Assigned Names and Numbers (ICANN). \n\nBekele's eight year .africa journey was not without challenges over the issues of accountability and transparency on the bid for .africa, including the African Union Commission(AUC) who allegedly disavowed the first endorsement they gave her organization DotConnectAfrica in 2009 to apply for the .africa gTLD, to subsequently request for the same directly from ICANN, at the ICANN 42 International meeting held in Dakar Senegal from 23–28 October 2011. \n\nAUC requested ICANN to include all similar name strings \".africa\", \".afrique\" and \".afriqia\", in the list of \"reserved names\", to afford special legislative protection to the AUC to own these strings so as to make it unavailable to anyone, through a method of bypassing the formal application process of the ICANN's new generic top level domains (gTLDs). Bekele opposed to the AUC's request as it became known later that ICANN could not reserve the names for the AU, however ICANN did recommend to the AUC to use Government Advisory Committee to raise concerns that an applicant is seen as potentially sensitive or problematic, or to provide direct advice to the Board, so as to change the outcome of the gTLD. AUC implemented the advice during the GAC objection period at the detriment of DCA and subsequently went ahead to directly endorsed another firm from SouthAfrica to apply to ICANN, who was also supported by its own AU Dotafrica Task Force. \n\nAccording to the public part of their application, the South African company submitted a \"non-community\" application to ICANN naming AUC as a “co-applicant” and giving the .africa registry and intellectual property rights to the AUC in a separate contract \nThese were later challenged through the DCA Vs ICANN IRP case overseen by the constituted .africa International Tribunal. On 9 July 2015, the .Africa IRP International Tribunal in its final declaration[109] ruled that DCA Trust, was the prevailing party. The Tribunal also recommended that ICANN continue to refrain from delegating the .AFRICA gTLD and permit DCA Trust’s application to proceed through the remainder of the new gTLD application process. The Tribunal further declared that ICANN is to bear, pursuant to Article IV, Section 3, paragraph 18 of the Bylaws, Article 11 of Supplementary Procedures and Article 31 of the ICDR Rules, the totality of the costs of this IRP and the totality of the costs of the IRP Provider.\n\nFollowing July 2015 IRP ruling, Sophia Bekele's organization DCA Trust took ICANN to court over the way it handled the IRP declaration, on April 12, 2016. Following this, a United States District Court, Central District Of California - Western Division has granted a Preliminary Injunction for DotConnectAfrica, the decision for case no. 16-CV-00862 RGK (JCx) [PDF] The ruling detailed among other things that The balance of equities tips in favor of granting the preliminary injunction. This also follows a March 4, 2016 ruling which her organization was granted as an Ex Parte Application for Temporary Restraining Order (TRO), Interim Relief [PDF Case No: CV 16-00862 RGK (JCx)] that ICANN should hold off from delegating the .AFRICA top-level domain (TLD) to the competing application sponsored by the AUC - ZA Central Registry (ZACR).\n\nSince then, the case has been remanded from Federal court due to lack of jurisdiction. The Judge stated that in the event that DCA ultimately prevails in this action, the gTLD can be re-delegated. The case will be set in a jury trial in 2019 \n"}
{"id": "3042147", "url": "https://en.wikipedia.org/wiki?curid=3042147", "title": "Subpage", "text": "Subpage\n\nA subpage usually refers to a lower level web page in a website or wiki.\n\nThe subpage feature has been disabled in the main namespace (article namespace) of the English Wikipedia. But subpages are used in Wikiversity, where the structure permits contributions by students at any academic level, and in Wikibooks, where chapters are subpages of a book.\n\n"}
{"id": "2468756", "url": "https://en.wikipedia.org/wiki?curid=2468756", "title": "TNTmips", "text": "TNTmips\n\nTNTmips is a geospatial analysis system providing a fully featured GIS, RDBMS, and automated image processing system with CAD, TIN, surface modeling, map layout and innovative data publishing tools. TNTmips has a single integrated system with an identical interface, functionality, and geodata structure for use on Mac and Windows operating systems. The interface, database text content, messages, map production, and all other internal aspects of TNTmips have been localized for use in many languages, including, for example Arabic, Thai, and all romance languages. The professional version of TNTmips is in use in over 120 nations while the TNTmips Free version (restricted in project size) is used worldwide for educational, self learning, and small projects (e.g., archaeological sites, neighborhood planning, and precision farming).\n\nTNTmips is a professional system for fully integrated GIS, image processing, CAD, TIN, desktop cartography, and geospatial database management.\n\nTNTmips Pro (professional license,\nTNTmips Basic (low cost license),\nTNTmips Free (freeware).\n\nThe TNT products include: TNTmips, TNTedit, TNTview, TNTatlas and TNTsdk. There is no distinction between TNTmips and TNT products with regards to license levels.\nAll TNT products are available for Windows and Macintosh computers in a growing number of international languages.\n\n"}
{"id": "6517795", "url": "https://en.wikipedia.org/wiki?curid=6517795", "title": "The RNAi Consortium", "text": "The RNAi Consortium\n\nThe RNAi Consortium, or TRC, is a public-private partnership whose mission is to create libraries of small hairpin RNAs (shRNAs) for 15'000 human and 15'000 mouse genes. These libraries should help the scientific community to analyse gene function by RNAi. The consortium is based at the Broad Institute of the MIT and Harvard University, and includes 6 MIT- and Harvard-associated institutions and 5 international life sciences organizations. Verified RNAi clones and entire libraries are made available both by Sigma-Aldrich and Open Biosystems.\n\nA set of candidate hairpins are selected based on the 1st Refseq transcript from each NCBI gene. They should be 21mers, be at least 25bp from start of the coding sequence and no closer than 150bp from its end. Candidates are scored based on various empirical rules (see the Broad Institute's web site for a complete list ) and then BLASTed against 2 transcriptome sets. Hairpins that are unique for a Unigene cluster and a RefSeq NM identifier are preferred. Lastly, the candidates are spaced to have 1 hairpin in the 3' untranslated region and 4 in the coding sequence.\n\nSelected hairpins are cloned into the vector pLKO1, which is a multipurpose plasmid that can be propagated in bacteria, transfected into mammalian cell lines or used for generation of lentiviruses. It contains resistance genes against ampicillin and puromycin.\n\nRelease 1 of the TRC lentiviral shRNA libraries consist of about 35'000 shRNA constructs against 5300 human (25'000 clones) and 2200 mouse genes (10'000 clones). Release 2 of the human shRNA library contained an additional 9'500 clones. Releases occur roughly every quarter. The completing of both mouse and human libraries is envisioned for the end of 2006 or beginning of 2007.\n\n"}
{"id": "51478252", "url": "https://en.wikipedia.org/wiki?curid=51478252", "title": "Thinx", "text": "Thinx\n\nTHINX is a New York based company that makes feminine hygiene products.\n\nThe company has two brands, Icon Undies and THINX. THINX is underwear that can be worn during menstruation as a substitute or a supplement to traditional feminine hygiene products. Icon is underwear that can be worn for light and moderate incontinence.\n\nThe underwear come in a range of styles from boyshorts to thongs, and include two patented technologies. One is to absorb different amounts of blood, and the other absorbs different amounts of urine. The underwear is anti-microbial, moisture-wicking, absorbent and leak resistant.\n\nTIME named THINX period panties as one of the best inventions of 2015.\n\nFast Company named THINX one of the most innovative companies of 2017.\n\nTHINX was founded in 2011 by Antonia Saint Dunbar, Miki Agrawal and Radha Agrawal. \n\nTHINX has earned a reputation for its controversial ads. In October 2015, Outfront Media rejected THINX's subway advertisements due to the fact that the ads used the word \"period\" and included suggestive visuals of food. Following a social media outcry, the ads were finally allowed to be shown.\n\nIn 2016, the Company received attention for featuring trans men models in its ads for period underwear.\nIn November 2016, THINX launched an ad referencing the Donald Trump Access Hollywood tape. The San Francisco subway banned the ads due to the language.\n\nIn November 2017, they created a PMS Truck.\n\n"}
{"id": "44510114", "url": "https://en.wikipedia.org/wiki?curid=44510114", "title": "Tuned amplifier", "text": "Tuned amplifier\n\nA tuned amplifier is an electronic amplifier which includes bandpass filtering components within the amplifier circuitry. They are widely used in all kinds of wireless applications.\n\nThere are several tuning schemes in use,\n"}
{"id": "5031967", "url": "https://en.wikipedia.org/wiki?curid=5031967", "title": "Virtual power plant", "text": "Virtual power plant\n\nA virtual power plant (VPP) is a cloud-based distributed power plant that aggregates the capacities of heterogeneous Distributed Energy Resources (DERs) for the purposes of enhancing power generation, as well as trading or selling power on the electricity market. Examples of virtual power plants exist in the United States, Europe, and Australia.\n\nA virtual power plant is a system that integrates several types of power sources to give a reliable overall power supply. The sources often form a cluster of different types of dispatchable and non-dispatchable, controllable or flexible load (CL or FL) distributed generation (DG) systems that are controlled by a central authority and can include microCHPs, natural gas-fired reciprocating engines, small-scale wind power plants (WPP)s, photovoltaics (PVs), run-of-river hydroelectricity plants, small hydro, biomass, back-up gensets, and energy storage systems (ESS).\n\nThis system has benefits such as the ability to deliver peak load electricity or load-following power generation on short notice. Such a VPP can replace a conventional power plant while providing higher efficiency and more flexibility. More flexibility allows the system to react better to fluctuations, but whose complexity requires complicated optimization, control, and secure communications.\n\nAccording to a 2012 report by Pike Research, VPP capacity would, from 2011 to 2017, increase by 65%, from 55.6 gigawatts (GW) to 91.7 GW worldwide, generating from $5.3 billion to $6.5 billion in worldwide revenue in 2017. In a more aggressive forecast scenario, the clean-tech market intelligence firm forecasts that global VPP revenues could reach as high as $12.7 billion during the same period.\n\nVirtual power plants represent an 'Internet of Energy, said senior analyst Peter Asmus of Pike Research. \"These systems tap existing grid networks to tailor electricity supply and demand services for a customer. VPPs maximize value for both the end user and the distribution utility using a sophisticated set of software-based systems. They are dynamic, deliver value in real time, and can react quickly to changing customer load conditions.\nA virtual power plant is also a cloud-based central or distributed control center that takes advantage of information and communication technologies (ICTs) and Internet of things (IoT) devices to aggregate the capacities of heterogeneous Distributed Energy Resources (DERs) to form \"a coalition of heterogeneous DERs\" for the purpose of energy trading on the wholesale electricity markets or providing ancillary services for system operators on behalf of non-eligible individual DERs.\n\nA VPP acts as an intermediary between DERs and the wholesale electricity market and trades energy on behalf of DER owners who by themselves are unable to participate in that market. The VPP behaves as a conventional dispatchable power plant from the point of view of other market participants, although it is indeed a cluster of many diverse DERs. Also, in the competitive electricity markets, a virtual power plant acts as an arbitrageur between diverse energy trading floors (i.e., bilateral and PPA contracts, forward and futures markets, and the pool).\n\nSo far, for risk management purposes, five different risk-hedging strategies (i.e., IGDT, RO, CVaR, FSD, and SSD) have been applied to the decision-making problems of VPPs in the research articles to measure the level of conservatism of VPPs' decisions in diverse energy trading floors (e.g., day-ahead electricity market, derivatives exchange market, and bilateral contracts):\n\n\nEnergy markets are those commodities markets that deal specifically with the trade and supply of energy. In the United States, virtual power plants not only deal with the supply side, but also help manage demand, and ensure reliability of grid functions through demand response (DR) and other load-shifting approaches, in real time.\n\nAn often-reported energy crisis in America has opened the door for government-subsidized companies to enter an arena that has only been available to utilities and multinational billion-dollar companies until now. With the deregulation of markets around the United States, the wholesale market pricing became the exclusive domain of large retail suppliers; however local and federal legislation along with large end-users are beginning to recognize the advantages of wholesale activities.\n\nCalifornia is the leader in green technology, with governmental bodies subsidizing and pushing an agenda that is not shared by much of the rest of the United States. In California there are two electrical markets: private retail and wholesale. California Senate Bill 2X—which passed the California legislature on March 30, 2011—mandates 33% renewables by 2020 without mandating any particular method to reach that goal.\n\nThe Institute for Solar Energy Supply Technology of the University of Kassel in Germany pilot-tested a combined power plant that linked solar, wind, biogas, and hydrostorage to provide load-following power around the clock, completely from renewable sources.\nVirtual Power Station operators are also commonly referred to as aggregators.\n\nTo test the effects of micro combined heat and power on a smart grid, 45 natural gas SOFC units (each generating 1.5kW) from Republiq Power (Ceramic Fuel Cells) will be placed in 2013 on Ameland to function as a virtual power plant.\n\nAn example of a real-world virtual power plant can be found on the Scottish Inner Hebrides island of Eigg.\n\nIn August 2016, AGL Energy announced a 5MW virtual-power-plant scheme for Adelaide, Australia. The company will supply battery and photovoltaic systems from Sunverge Energy, of San Francisco, to 1000 households and businesses. The systems will cost consumers AUD $3500 and are expected to recoup the expense in savings in 7years under current distribution network tariffs. The scheme is worth AUD $20million and is being billed as the largest in the world.\n"}
{"id": "2181790", "url": "https://en.wikipedia.org/wiki?curid=2181790", "title": "X-ray microtomography", "text": "X-ray microtomography\n\nX-ray microtomography, like tomography and x-ray computed tomography, uses x-rays to create cross-sections of a physical object that can be used to recreate a virtual model (3D model) without destroying the original object. The prefix \"micro-\" (symbol: µ) is used to indicate that the pixel sizes of the cross-sections are in the micrometre range. These pixel sizes have also resulted in the terms high-resolution x-ray tomography, micro–computed tomography (micro-CT or µCT), and similar terms. Sometimes the terms high-resolution CT (HRCT) and micro-CT are differentiated, but in other cases the term high-resolution micro-CT is used. Virtually all tomography today is computed tomography.\n\nMicro-CT has applications both in medical imaging and in industrial computed tomography. In general, there are two types of scanner setups. In one setup, the X-ray source and detector are typically stationary during the scan while the sample/animal rotates. The second setup, much more like a clinical CT scanner, is gantry based where the animal/specimen is stationary in space while the X-ray tube and detector rotate around. These scanners are typically used for small animals (\"in vivo\" scanners), biomedical samples, foods, microfossils, and other studies for which minute detail is desired.\n\nThe first X-ray microtomography system was conceived and built by Jim Elliott in the early 1980s. The first published X-ray microtomographic images were reconstructed slices of a small tropical snail, with pixel size about 50 micrometers.\n\nThe fan-beam system is based on a one-dimensional (1D) X-ray detector and an electronic X-ray source, creating 2D cross-sections of the object. Typically used in human computed tomography systems.\n\nThe cone-beam system is based on a 2D X-ray detector (camera) and an electronic X-ray source, creating projection images that later will be used to reconstruct the image cross-sections.\n\nIn an open system, X-rays may escape or leak out, thus the operator must stay behind a shield, have special protective clothing, or operate the scanner from a distance or a different room. Typical examples of these scanners are the human versions, or designed for big objects.\n\nIn a closed system, X-ray shielding is put around the scanner so the operator can put the scanner on a desk or special table. Although the scanner is shielded, care must be taken and the operator usually carries a dosimeter, since X-rays have a tendency to be absorbed by metal and then re-emitted like an antenna. Although a typical scanner will produce a relatively harmless volume of X-rays, repeated scannings in a short timeframe could pose a danger. Digital detectors with small pixel pitches and micro-focus x-ray tubes are usually employed to yield in high resolution images. \nClosed systems tend to become very heavy because lead is used to shield the X-rays. Therefore, the smaller scanners only have a small space for samples.\n\nBecause microtomography scanners offer isotropic, or near isotropic, resolution, display of images does not need to be restricted to the conventional axial images. Instead, it is possible for a software program to build a volume by 'stacking' the individual slices one on top of the other. The program may then display the volume in an alternative manner.\n\nFor X-ray microtomography, powerful open source software is available, such as the ASTRA toolbox. The ASTRA Toolbox is a MATLAB toolbox of high-performance GPU primitives for 2D and 3D tomography, from 2009–2014 developed by iMinds-Vision Lab, University of Antwerp and since 2014 jointly developed by iMinds-VisionLab, UAntwerpen and CWI, Amsterdam.\nThe toolbox supports parallel, fan, and cone beam, with highly flexible source/detector positioning. A large number of reconstruction algorithms are available, including FBP, ART, SIRT, SART, CGLS.\n\nVolume rendering is a technique used to display a 2D projection of a 3D discretely sampled data set, as produced by a microtomography scanner. Usually these are acquired in a regular pattern (e.g., one slice every millimeter) and usually have a regular number of image pixels in a regular pattern. This is an example of a regular volumetric grid, with each volume element, or voxel represented by a single value that is obtained by sampling the immediate area surrounding the voxel.\n\nWhere different structures have similar threshold density, it can become impossible to separate them simply by adjusting volume rendering parameters. The solution is called segmentation, a manual or automatic procedure that can remove the unwanted structures from the image.\n\n\n\nDevelopmental biology\n\n\n\n\n\n\n\n\n\n\nIn geology it is used to analyze micro pores in the reservoir rocks, it can used in microfacies analysis for sequence stratigraphy. In petroleum exploration it is used to model the petroleum flow under micro pores and nano particles.\n\nIt can give a resolution up to 1 nm.\n\n\n\n\n\n\n\n"}
{"id": "31602203", "url": "https://en.wikipedia.org/wiki?curid=31602203", "title": "XAP Home Automation protocol", "text": "XAP Home Automation protocol\n\nxAP is an open protocol used for home automation and supports integration of telemetry and control devices primarily within the home. Common communications networks include RS232, RS485, Ethernet and wireless. xAP protocol always uses broadcast for sending the messages. All the receivers listens to the message and introspects the message header to verify whether the message is of its interest. xAP protocol has the following key advantages.\n\n\n"}
