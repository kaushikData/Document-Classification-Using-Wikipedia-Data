{"id": "664490", "url": "https://en.wikipedia.org/wiki?curid=664490", "title": "1912 Lawrence textile strike", "text": "1912 Lawrence textile strike\n\nThe Lawrence textile strike was a strike of immigrant workers in Lawrence, Massachusetts, in 1912 led by the Industrial Workers of the World (IWW). Prompted by a two-hour pay cut corresponding to a new law shortening the workweek for women, the strike spread rapidly through the town, growing to more than twenty thousand workers and involving nearly every mill in Lawrence. Starting January 1, 1912, the Massachusetts government started to enforce a law that allowed women to work a maximum of 54 hours, rather than 56 hours. Ten days later, they found out that pay had been reduced along with the cut in hours.\nThe strike united workers from more than 40 different nationalities. Carried on throughout a brutally cold winter, the strike lasted more than two months, from January to March, defying the assumptions of conservative trade unions within the American Federation of Labor (AFL) that immigrant, largely female and ethnically divided workers could not be organized. In late January, when a striker, Anna LoPizzo, was killed by police during a protest, IWW organizers Joseph Ettor and Arturo Giovannitti were framed and arrested on charges of being accessories to the murder.\n\nIWW leaders Bill Haywood and Elizabeth Gurley Flynn came to Lawrence to run the strike. Together they masterminded its signature move, sending hundreds of the strikers' hungry children to sympathetic families in New York, New Jersey, and Vermont. The move drew widespread sympathy, especially after police stopped a further exodus, leading to violence at the Lawrence train station. Congressional hearings followed, resulting in exposure of shocking conditions in the Lawrence mills and calls for investigation of the \"wool trust.\" Mill owners soon decided to settle the strike, giving workers in Lawrence and throughout New England raises of up to 20 percent. Within a year, however, the IWW had largely collapsed in Lawrence.\n\nThe Lawrence strike is often referred to as the \"Bread and Roses\" strike. It has also been called the \"strike for three loaves\". The phrase \"bread and roses\" actually preceded the strike, appearing in a poem by James Oppenheim published in \"The Atlantic Monthly\" in December 1911. A 1916 labor anthology, \"The Cry for Justice: An Anthology of the Literature of Social Protest\" by Upton Sinclair, attributed the phrase to the Lawrence strike, and the association stuck. \"Bread and roses\" has also been attributed to socialist union organizer Rose Schneiderman.\n\nA popular rally cry that was used at the protests and strikes:\nAs we come marching, marching, we battle too for men,\n\nFor they are women's children, and we mother them again.\n\nOur lives shall not be sweated from birth until life closes;\n\nHearts starve as well as bodies; give us bread, but give us roses!\n\n-James Oppenheim \nFounded in 1845, Lawrence was a flourishing but deeply-troubled textile city. By 1900, mechanization and the deskilling of labor in the textile industry enabled factory owners to eliminate skilled workers and to employ large numbers of unskilled immigrant workers, mostly women. Work in a textile mill took place at a grueling pace, and the labor was repetitive and dangerous. In addition, a number of children under 14 worked in the mills. Half of the workers in the four Lawrence mills of the American Woolen Company, the leading employer in the industry and the town, were females between 14 and 18.\n\nBy 1912, the Lawrence mills at maximum capacity employed about 32,000 men, women, and children. Conditions had worsened even more in the decade before the strike. The introduction of the two-loom system in the woolen mills led to a dramatic increase in the pace of work. The greater production enabled the factory owners to lay off large numbers of workers. Those who kept their jobs earned, on average, $8.76 for 56 hours of work and $9.00 for 60 hours of work.\n\nThe workers in Lawrence lived in crowded and dangerous apartment buildings, often with many families sharing each apartment. Many families survived on bread, molasses, and beans; as one worker testified before the March 1912 congressional investigation of the Lawrence strike, \"When we eat meat it seems like a holiday, especially for the children.\" Half of children died before they were six, and 36% of the adults who worked in the mill died before they were 25. The average life expectancy was 39.\n\nThe mills and the community were divided along ethnic lines: most of the skilled jobs were held by native-born workers of English, Irish, and German descent, whereas French-Canadian, Italian, Slavic, Hungarian, Portuguese, and Syrian immigrants made up most of the unskilled workforce. Several thousand skilled workers belonged, in theory at least, to the American Federation of Labor-affiliated United Textile Workers, but only a few hundred paid dues. The Industrial Workers of the World (IWW) had also been organizing for five years among workers in Lawrence but also had only a few hundred actual members.\n\nOn January 1, 1912, a new labor law took effect in Massachusetts reducing the working week of 56 hours to 54 hours for women and children. Workers opposed the reduction if it reduced their weekly home pay. The first two weeks of 1912, the unions tried to learn how the owners of the mills would deal with the new law. On January 11, a group of Polish women textile workers in Lawrence discovered that their employer at the Everett Mill had reduced about $0.32 from their total wages and walked out.\n\nOn January 12, workers in the Washington Mill of the American Woolen Company also found that their wages had been cut. Prepared for the events by weeks of discussion, they walked out, calling \"short pay, all out.\"\n\nJoseph Ettor of the IWW had been organizing in Lawrence for some time before the strike; he and Arturo Giovannitti of the Italian Socialist Federation of the Socialist Party of America quickly assumed leadership of the strike by forming a strike committee of 56 people, four representatives of fourteen nationalities, which took responsibility for all major decisions. The committee, which arranged for its strike meetings to be translated into 25 different languages, put forward a set of demands: a 15% increase in wages for a 54-hour work week, double time for overtime work, and no discrimination against workers for their strike activity.\n\nThe city responded to the strike by ringing the city's alarm bell for the first time in its history; the mayor ordered a company of the local militia to patrol the streets. When mill owners turned fire hoses on the picketers gathered in front of the mills, they responded by throwing ice at the plants, breaking a number of windows. The court sentenced 24 workers to a year in jail for throwing ice; as the judge stated, \"The only way we can teach them is to deal out the severest sentences.\" Governor Eugene Foss then ordered out the state militia and state police. Mass arrests followed.\n\nAt the same time, the United Textile Workers (UTW) attempted to break the strike by claiming to speak for the workers of Lawrence. The striking operatives ignored the UTW, as the IWW had successfully united the operatives behind ethnic-based leaders, who were members of the strike committee and able to communicate Ettor's message to avoid violence at demonstrations. Ettor did not consider intimidating operatives who were trying to enter the mills as breaking the peace.\n\nThe IWW was successful, even with AFL-affiliated operatives, as it defended the grievances of all operatives from all the mills. Conversely, the AFL and the mill owners preferred to keep negotiations between separate mills and their own operatives. However, in a move that frustrated the UTW, Oliver Christian, the national secretary of the Loomfixers Association and an AFL affiliate itself, said he believed John Golden, the Massachusetts UTW president, was a detriment to the cause of labor. That statement and missteps by William Madison Wood quickly shifted public sentiment to favor the strikers.\n\nA local undertaker and a member of the Lawrence school board attempted to frame the strike leadership by planting dynamite in several locations in town a week after the strike began. He was fined $500 and released without jail time. Later, William M. Wood, the president of the American Woolen Company, was shown to have made an unexplained large payment to the defendant shortly before the dynamite was found.\nThe authorities later charged Ettor and Giovannitti as accomplices to murder for the death of striker Anna LoPizzo, who was likely shot by the police. Ettor and Giovannitti had been away, where they spoke to another group of workers. They and a third defendant, who had not even heard of either Ettor or Giovannitti at the time of his arrest, were held in jail for the duration of the strike and several months thereafter. The authorities declared martial law, banned all public meetings, and called out 22 more militia companies to patrol the streets.\n\nThe IWW responded by sending Bill Haywood, Elizabeth Gurley Flynn, and a number of other organizers to Lawrence. Haywood participated little in the daily affairs of the strike. Instead, he set out for other New England textile towns in an effort to raise funds for the strikers in Lawrence, which proved very successful. Other tactics established were an efficient system of relief committees, soup kitchens, and food distribution stations, and volunteer doctors provided medical care. The IWW raised funds on a nationwide basis to provide weekly benefits for strikers and dramatized the strikers' needs by arranging for several hundred children to go to supporters' homes in New York City for the duration of the strike. When city authorities tried to prevent another 100 children from going to Philadelphia on February 24 by sending police and the militia to the station to detain the children and arrest their parents, the police began clubbing both the children and their mothers and dragged them off to be taken away by truck; one pregnant mother miscarried. The press, there to photograph the event, reported extensively on the attack. Moreover, when the women and children were taken to the Police Court, most of them refused to pay the fines levied and opted for a jail cell, some with babies in arms.\n\nThe police action against the mothers and children of Lawrence attracted the attention of the nation, in particular that of Helen Herron Taft, the wife of William Howard Taft. Soon, both the House and the Senate set out to investigate the strike. In the early days of March, a special House Committee heard testimony from some of the strikers' children, various city, state and union officials. In the end, both houses published reports detailing the conditions at Lawrence.\n\nThe national attention had an effect: the owners offered a 5% pay raise on March 1, but the workers rejected it. American Woolen Company agreed to most of the strikers' demands on March 12, 1912. The strikers had demanded an end to the Premium System in which a portion of their earnings were subject to month-long production and attendance standards. The mill owners' concession was to change the award of the premium from once every four weeks to once every two weeks. The rest of the manufacturers followed by the end of the month; other textile companies throughout New England, anxious to avoid a similar confrontation, then followed suit.\n\nThe children who had been taken in by supporters in New York City came home on March 30.\n\nEttor and Giovanniti, both members of IWW, remained in prison for months after the strike was over. Haywood threatened a general strike to demand their freedom, with the cry \"Open the jail gates or we will close the mill gates.\" The IWW raised $60,000 for their defense and held demonstrations and mass meetings throughout the country in their support; the Boston authorities arrested all of the members of the Ettor and Giovannitti Defense Committee. Then, 15,000 Lawrence workers went on strike for one day on September 30 to demand the release of Ettor and Giovannitti. Swedish and French workers proposed a boycott of woolen goods from the US and a refusal to load ships going there, and Italian supporters of the Giovannitti men rallied in front of the US consulate in Rome.\n\nIn the meantime, Ernest Pitman—l, a Lawrence building contractor who had done extensive work for the American Woolen Company, confessed to a district attorney that he had attended a meeting in the Boston offices of Lawrence textile companies, where the plan to frame the union by planting dynamite had been made. Pitman committed suicide shortly thereafter when he was subpoenaed to testify. Wood, the American Woolen Company owner, was formally exonerated.\n\nWhen the trial of Ettor and Giovannitti, as well as a third defendant, Giuseppe Caruso, accused of firing the shot that killed the picketer, began in September 1912 in Salem before Judge Joseph F. Quinn, the three defendants were kept in steel cages in the courtroom. All witnesses testified that Ettor and Giovannitti were miles away and that Caruso, the third defendant, was at home and eating supper at the time of the killing.\n\nEttor and Giovannitti both delivered closing statements at the end of the two-month trial. In Ettor's closing statement, he turned and faced the District Attorney:\n\nDoes Mr. Ateill believe for a moment that... the cross or the gallows or the guillotine, the hangman's noose, ever settled an idea? It never did. If an idea can live, it lives because history adjudges it right. And what has been considered an idea constituting a social crime in one age has in the next age become the religion of humanity. Whatever my social views are, they are what they are. They cannot be tried in this courtroom.\n\nAll three defendants were acquitted on November 26, 1912.\n\nThe strikers, however, lost nearly all of the gains they had won in the next few years. The IWW, disdaining written contracts as encouraging workers to abandon the daily class struggle, thus left the mill owners to chisel away at the improvements in wages and working conditions, to fire union activists, and to install labor spies to keep an eye on workers. The more persistent owners laid off further employees during a depression in the industry.\n\nBy then, the IWW had turned its attention to supporting the silk industry workers in Paterson, New Jersey. The Paterson Silk Strike of 1913 was defeated.\n\nThe strike had at least three casualties:\n\n\n\n"}
{"id": "15065999", "url": "https://en.wikipedia.org/wiki?curid=15065999", "title": "40 principles of invention", "text": "40 principles of invention\n\nThe \"forty principles\" of TRIZ are generic principles of Inventive Thinking and Creativity Engineering used together with so called Contradiction Matrix in solving hard technical problems.\n\nA Contradiction Matrix, accordingly to its cells, (ordered alongside columns and rows with the indicated properties of the analyzed system) collects - in a structured and systematic manner - the basic engineering parameters of common objects, or systems, such as weight, length, and manufacturing tolerances, etc. \n\nTRIZ methodology claims that by studying an individual parameter, which is causing a problem (e.g., the weight of an object needs to be reduced), and the other parameters which are in conflict with it (e.g., lower weight would require thinner material, which is more likely to undergo catastrophic failure if over-stressed), engineering solutions can be created for invention problems.\n\nAnd this is actually the simplification of some fact. Namely, the structured contents of each of the cells within Contradiction Matrix, (i.e. cells fulfilled with ordered principles and identified by their order numbers) - the principles of inventions have been chosen, due to realized necessary statistical extensive studies. These were the studies led by G. S. Altshuller, based on virtually all known technical solutions up to that time, as well as the registered patents.\n\nIn Altshuller's opinion, every technical problem which has not yet any effective or satisfactory solution, nonetheless can be categorized in terms of its Main Technical Contradiction for the analyzed System. Therefore, it could be then analyzed using proper column and row of the Contradiction Matrix indicated, by correctly defined Main Technical Contradiction of the System, finally indicating the dedicated and suggested in process of finding the necessary solution - principles of invention.\n\nAltshuller noticed that process of evolution of any given System is nonetheless ruled by commonly existing general Laws of Systems' Evolution. One such law says, that the very process of finding the necessary solution for the technical problem, can be facilitated and sped up in analogy to solutions which have already been found for another technical problem. However, the main point is to acquire the long-term necessary experience in correct defining of the Main Technical Contradiction for any given analyzed System.\n\n"}
{"id": "34580174", "url": "https://en.wikipedia.org/wiki?curid=34580174", "title": "Antisymmetric exchange", "text": "Antisymmetric exchange\n\nThe antisymmetric exchange is a contribution to the total magnetic exchange interaction between two neighboring magnetic spins, formula_1 and formula_2. It was first postulated by Igor Dzyaloshinskii on the grounds of phenomenological considerations based on Landau theory. Toru Moriya identified the spin-orbit coupling as the microscopic mechanism of the antisymmetric exchange interaction. It can be written as\nformula_3.\nThis term is also called the Dzyaloshinskii-Moriya interaction. In magnetically ordered systems, it favors a spin canting of otherwise (anti)parallel aligned magnetic moments and thus, e.g., is a source of weak ferromagnetic behavior in an antiferromagnet.\n\nThe orientation of the vector formula_4 is constrained by symmetry, as discussed already in Moriya’s original publication. Considering the case that the magnetic interaction between two neighboring ions is transferred via a single third ion (ligand) by the superexchange mechanism (see Figure), the orientation of formula_4 is obtained by the simple relation formula_6. This implies that formula_4 is oriented perpendicular to the triangle spanned by the involved three ions. formula_8 if the three ions are in line.\n\nThe antisymmetric exchange is of importance for the understanding of magnetism induced electric polarization in a recently discovered class of multiferroics: Here, small shifts of the ligand ions can be induced by magnetic ordering, because the systems tends to enhance the magnetic interaction energy at the cost of lattice energy. This mechanism is called “inverse Dzyaloshinskii-Moriya effect”. In certain magnetic structures, all ligand ions are shifted into the same direction, leading to a net electric polarization.\n"}
{"id": "39849257", "url": "https://en.wikipedia.org/wiki?curid=39849257", "title": "Behavioral analytics", "text": "Behavioral analytics\n\nBehavioral analytics is a recent advancement in business analytics that reveals new insights into the behavior of consumers on eCommerce platforms, online games, web and mobile applications, and IoT. The rapid increase in the volume of raw event data generated by the digital world enables methods that go beyond typical analysis by demographics and other traditional metrics that tell us what kind of people took what actions in the past. Behavioral analysis focuses on understanding how consumers act and why, enabling accurate predictions about how they are likely to act in the future. It enables marketers to make the right offers to the right consumer segments at the right time.\n\nBehavioral analytics utilizes the massive volumes of raw user event data captured during sessions in which consumers use application, game, or website, including traffic data like navigation path, clicks, social media interactions, purchasing decisions and marketing responsiveness. Also, the event-data can include advertising metrics like click-to-conversion time, as well as comparisons between other metrics like the monetary value of an order and the amount of time spent on the site. These data points are then compiled and analyzed, whether by looking at session progression from when a user first entered the platform until a sale was made, or what other products a user bought or looked at before this purchase. Behavioral analysis allows future actions and trends to be predicted based on the collection of such data.\n\nWhile business analytics has a more broad focus on the who, what, where and when of business intelligence, behavioral analytics narrows that scope, allowing one to take seemingly unrelated data points in order to extrapolate, predict and determine errors and future trends. It takes a more holistic and human view of data, connecting individual data points to tell us not only what is happening, but also how and why it is happening.\n\nData shows that a large percentage of users using a certain eCommerce platform found it by searching for “Thai food” on Google. After landing on the homepage, most people spent some time on the “Asian Food” page and then logged off without placing an order. Looking at each of these events as separate data points does not represent what is really going on and why people did not make a purchase. However, viewing these data points as a representation of overall user behavior enables one to interpolate how and why users acted in this particular case.\nBehavioral analytics looks at all site traffic and page views as a timeline of connected events that did not lead to orders. \nSince most users left after viewing the “Asian Food” page, there could be a disconnect between what they are searching for on Google and what the “Asian Food” page displays. Knowing this, a quick look at the “Asian Food” page reveals that it does not display Thai food prominently and thus people do not think it is actually offered, even though it is.\n\nBehavioral analytics is becoming increasingly popular in commercial environments. Amazon.com is a leader in using behavioral analytics to recommend additional products that customers are likely to buy based on their previous purchasing patterns on the site. Behavioral analytics is also used by Target to suggest products to customers in their retail stores, while political campaigns use it to determine how potential voters should be approached. In addition to retail and political applications, behavioral analytics is also used by banks and manufacturing firms to prioritize leads generated by their websites. Behavioral analytics also allow developers to manage users in online-gaming and web applications.\n\nIBM and Intel are creating ecosystems of connected solutions and advanced analytics. In retail, this is the Internet of Things (IoT) for tracking shopping behaviors.\n\n\nAn ideal behavioral analytics solution would include:\n"}
{"id": "4526", "url": "https://en.wikipedia.org/wiki?curid=4526", "title": "Brick", "text": "Brick\n\nA brick is building material used to make walls, pavements and other elements in masonry construction. Traditionally, the term brick referred to a unit composed of clay, but it is now used to denote any rectangular units laid in mortar. A brick can be composed of clay-bearing soil, sand, and lime, or concrete materials. Bricks are produced in numerous classes, types, materials, and sizes which vary with region and time period, and are produced in bulk quantities. Two basic categories of bricks are \"fired\" and \"non-fired\" bricks.\n\nBlock is a similar term referring to a rectangular building unit composed of similar materials, but is usually larger than a brick. Lightweight bricks (also called lightweight blocks) are made from expanded clay aggregate.\n\nFired bricks are one of the longest-lasting and strongest building materials, sometimes referred to as artificial stone, and have been used since circa 4000 BC. Air-dried bricks, also known as mudbricks, have a history older than fired bricks, and have an additional ingredient of a mechanical binder such as straw.\n\nBricks are laid in \"courses\" and numerous patterns known as \"bonds\", collectively known as brickwork, and may be laid in various kinds of mortar to hold the bricks together to make a durable structure.\n\nThe earliest bricks were \"dried brick\", meaning that they were formed from clay-bearing earth or mud and dried (usually in the sun) until they were strong enough for use. The oldest discovered bricks, originally made from shaped mud and dating before 7500 BC, were found at Tell Aswad, in the upper Tigris region and in southeast Anatolia close to Diyarbakir. The South Asian inhabitants of Mehrgarh also constructed, and lived in, airdried mudbrick houses between 7000–3300 BC. Other more recent findings, dated between 7,000 and 6,395 BC, come from Jericho, Catal Hüyük, the ancient Egyptian fortress of Buhen, and the ancient Indus Valley cities of Mohenjo-daro, Harappa, and Mehrgarh. Ceramic, or \"fired brick\" was used as early as 3000 BC in early Indus Valley cities like Kalibangan.\n\nThe earliest fired bricks appeared in Neolithic China around 4400 BC at Chengtoushan, a walled settlement of the Daxi culture. These bricks were made of red clay, fired on all sides to above 600°C, and used as flooring for houses. By the Qujialing period (3300 BC), fired bricks were being used to pave roads and as building foundations at Chengtoushan.\n\nBricks continued to be used during 2nd millennium BC at a site near Xi'an. Fired bricks were found in Western Zhou (1046–771 BC) ruins, where they were produced on a large scale. The carpenter's manual \"Yingzao Fashi\", published in 1103 at the time of the Song dynasty described the brick making process and glazing techniques then in use. Using the 17th-century encyclopaedic text \"Tiangong Kaiwu\", historian Timothy Brook outlined the brick production process of Ming Dynasty China: \n\nEarly civilisations around the Mediterranean adopted the use of fired bricks, including the Ancient Greeks and Romans. The Roman legions operated mobile kilns, and built large brick structures throughout the Roman Empire, stamping the bricks with the seal of the legion.\n\nDuring the Early Middle Ages the use of bricks in construction became popular in Northern Europe, after being introduced there from Northern-Western Italy. An independent style of brick architecture, known as brick Gothic (similar to Gothic architecture) flourished in places that lacked indigenous sources of rocks. Examples of this architectural style can be found in modern-day Denmark, Germany, Poland, and Russia.\n\nThis style evolved into Brick Renaissance as the stylistic changes associated with the Italian Renaissance spread to northern Europe, leading to the adoption of Renaissance elements into brick building. A clear distinction between the two styles only developed at the transition to Baroque architecture. In Lübeck, for example, Brick Renaissance is clearly recognisable in buildings equipped with terracotta reliefs by the artist Statius von Düren, who was also active at Schwerin (Schwerin Castle) and Wismar (Fürstenhof).\n\nLong-distance bulk transport of bricks and other construction equipment remained prohibitively expensive until the development of modern transportation infrastructure, with the construction of canal, roads, and railways.\n\nProduction of bricks increased massively with the onset of the Industrial Revolution and the rise in factory building in England. For reasons of speed and economy, bricks were increasingly preferred as building material to stone, even in areas where the stone was readily available. It was at this time in London that bright red brick was chosen for construction to make the buildings more visible in the heavy fog and to help prevent traffic accidents.\n\nThe transition from the traditional method of production known as hand-moulding to a mechanised form of mass-production slowly took place during the first half of the nineteenth century. Possibly the first successful brick-making machine was patented by Henry Clayton, employed at the Atlas Works in Middlesex, England, in 1855, and was capable of producing up to 25,000 bricks daily with minimal supervision. His mechanical apparatus soon achieved widespread attention after it was adopted for use by the South Eastern Railway Company for brick-making at their factory near Folkestone. The Bradley & Craven Ltd ‘Stiff-Plastic Brickmaking Machine’ was patented in 1853, apparently predating Clayton. Bradley & Craven went on to be a dominant manufacturer of brickmaking machinery. Predating both Clayton and Bradley & Craven Ltd. however was the brick making machine patented by Richard A. Ver Valen of Haverstraw, New York in 1852.\n\nThe demand for high office building construction at the turn of the 20th century led to a much greater use of cast and wrought iron, and later, steel and concrete. The use of brick for skyscraper construction severely limited the size of the building – the Monadnock Building, built in 1896 in Chicago, required exceptionally thick walls to maintain the structural integrity of its 17 storeys.\n\nFollowing pioneering work in the 1950s at the Swiss Federal Institute of Technology and the Building Research Establishment in Watford, UK, the use of improved masonry for the construction of tall structures up to 18 storeys high was made viable. However, the use of brick has largely remained restricted to small to medium-sized buildings, as steel and concrete remain superior materials for high-rise construction.\n\nThere are thousands of types of bricks that are named for their use, size, forming method, origin, quality, texture, and/or materials.\n\nCategorized by manufacture method:\n\nCategorized by use:\n\nSpecialized use bricks:\n\nBricks named for place of origin:\n\nThree basic types of brick are un-fired, fired, and chemically set bricks. Each type is manufactured differently.\n\nUnfired bricks, also known as mudbricks, are made from a wet, clay-containing soil mixed with straw or similar binders. They are air-dried until ready for use.\n\nFired bricks are burned in a kiln which makes them durable. Modern, fired, clay bricks are formed in one of three processes – soft mud, dry press, or extruded. Depending on the country, either the extruded or soft mud method is the most common, since they are the most economical.\n\nNormally, bricks contain the following ingredients:\n\n\nThree main methods are used for shaping the raw materials into bricks to be fired:\n\n\nIn many modern brickworks, bricks are usually fired in a continuously fired tunnel kiln, in which the bricks are fired as they move slowly through the kiln on conveyors, rails, or kiln cars, which achieves a more consistent brick product. The bricks often have lime, ash, and organic matter added, which accelerates the burning process.\n\nThe other major kiln type is the Bull's Trench Kiln (BTK), based on a design developed by British engineer W. Bull in the late 19th century.\n\nAn oval or circular trench is dug, 6–9 metres wide, 2-2.5 metres deep, and 100–150 metres in circumference. A tall exhaust chimney is constructed in the centre. Half or more of the trench is filled with \"green\" (unfired) bricks which are stacked in an open lattice pattern to allow airflow. The lattice is capped with a roofing layer of finished brick.\n\nIn operation, new green bricks, along with roofing bricks, are stacked at one end of the brick pile; cooled finished bricks are removed from the other end for transport to their destinations. In the middle, the brick workers create a firing zone by dropping fuel (coal, wood, oil, debris, and so on) through access holes in the roof above the trench.\n\nThe advantage of the BTK design is a much greater energy efficiency compared with clamp or scove kilns. Sheet metal or boards are used to route the airflow through the brick lattice so that fresh air flows first through the recently burned bricks, heating the air, then through the active burning zone. The air continues through the green brick zone (pre-heating and drying the bricks), and finally out the chimney, where the rising gases create suction that pulls air through the system. The reuse of heated air yields savings in fuel cost.\n\nAs with the rail process, the BTK process is continuous. A half-dozen labourers working around the clock can fire approximately 15,000–25,000 bricks a day. Unlike the rail process, in the BTK process the bricks do not move. Instead, the locations at which the bricks are loaded, fired, and unloaded gradually rotate through the trench.\n\nThe fired colour of tired clay bricks is influenced by the chemical and mineral content of the raw materials, the firing temperature, and the atmosphere in the kiln. For example, pink bricks are the result of a high iron content, white or yellow bricks have a higher lime content. Most bricks burn to various red hues; as the temperature is increased the colour moves through dark red, purple, and then to brown or grey at around . The names of bricks may reflect their origin and colour, such as London stock brick and Cambridgeshire White. \"Brick tinting\" may be performed to change the colour of bricks to blend-in areas of brickwork with the surrounding masonry.\n\nAn impervious and ornamental surface may be laid on brick either by salt glazing, in which salt is added during the burning process, or by the use of a slip, which is a glaze material into which the bricks are dipped. Subsequent reheating in the kiln fuses the slip into a glazed surface integral with the brick base.\n\nChemically set bricks are not fired but may have the curing process accelerated by the application of heat and pressure in an autoclave.\n\nCalcium-silicate bricks are also called sandlime or flintlime bricks, depending on their ingredients. Rather than being made with clay they are made with lime binding the silicate material. The raw materials for calcium-silicate bricks include lime mixed in a proportion of about 1 to 10 with sand, quartz, crushed flint, or crushed siliceous rock together with mineral colourants. The materials are mixed and left until the lime is completely hydrated; the mixture is then pressed into moulds and cured in an autoclave for three to fourteen hours to speed the chemical hardening. The finished bricks are very accurate and uniform, although the sharp arrises need careful handling to avoid damage to brick and bricklayer. The bricks can be made in a variety of colours; white, black, buff, and grey-blues are common, and pastel shades can be achieved. This type of brick is common in Sweden, especially in houses built or renovated in the 1970s. In India these are known as fly ash bricks, manufactured using the FaL-G (fly ash, lime, and gypsum) process. Calcium-silicate bricks are also manufactured in Canada and the United States, and meet the criteria set forth in ASTM C73 – 10 Standard Specification for Calcium Silicate Brick (Sand-Lime Brick).\n\nBricks formed from concrete are usually termed as blocks, and are typically pale grey. They are made from a dry, small aggregate concrete which is formed in steel moulds by vibration and compaction in either an \"egglayer\" or static machine. The finished blocks are cured, rather than fired, using low-pressure steam. Concrete blocks are manufactured in a much wider range of shapes and sizes than clay bricks and are also available with a wider range of face treatments – a number of which simulate the appearance of clay bricks.\n\nConcrete bricks are available in many colours and as an engineering brick made with sulfate-resisting Portland cement or equivalent. When made with adequate amount of cement they are suitable for harsh environments such as wet conditions and retaining walls. They are made to standards BS 6073, EN 771-3 or ASTM C55. Concrete bricks contract or shrink so they need movement joints every 5 to 6 metres, but are similar to other bricks of similar density in thermal and sound resistance and fire resistance.\n\nCompressed earth blocks are made mostly from slightly moistened local soils compressed with a mechanical hydraulic press or manual lever press. A small amount of a cement binder may be added, resulting in a \"stabilised compressed earth block\".\n\nFor efficient handling and laying, bricks must be small enough and light enough to be picked up by the bricklayer using one hand (leaving the other hand free for the trowel). Bricks are usually laid flat, and as a result, the effective limit on the width of a brick is set by the distance which can conveniently be spanned between the thumb and fingers of one hand, normally about four inches (about 100 mm). In most cases, the length of a brick is twice its width plus the width of a mortar joint, about eight inches (about 200 mm) or slightly more. This allows bricks to be laid \"bonded\" in a structure which increases stability and strength (for an example, see the illustration of bricks laid in \"English bond\", at the head of this article). The wall is built using alternating courses of \"stretchers\", bricks laid longways, and \"headers\", bricks laid crossways. The headers tie the wall together over its width. In fact, this wall is built in a variation of \"English bond\" called \"English cross bond\" where the successive layers of stretchers are displaced horizontally from each other by half a brick length. In true \"English bond\", the perpendicular lines of the stretcher courses are in line with each other.\n\nA bigger brick makes for a thicker (and thus more insulating) wall. Historically, this meant that bigger bricks were necessary in colder climates (see for instance the slightly larger size of the Russian brick in table below), while a smaller brick was adequate, and more economical, in warmer regions. A notable illustration of this correlation is the Green Gate in Gdansk; built in 1571 of imported Dutch brick, too small for the colder climate of Gdansk, it was notorious for being a chilly and drafty residence. Nowadays this is no longer an issue, as modern walls typically incorporate specialised insulation materials.\n\nThe correct brick for a job can be selected from a choice of colour, surface texture, density, weight, absorption, and pore structure, thermal characteristics, thermal and moisture movement, and fire resistance.\n\nIn England, the length and width of the common brick has remained fairly constant over the centuries (but see brick tax), but the depth has varied from about two inches (about 51 mm) or smaller in earlier times to about two and a half inches (about 64 mm) more recently. In the United Kingdom, the usual size of a modern brick is 215 × 102.5 × 65 mm (about × ×  inches), which, with a nominal 10 mm ( inch) mortar joint, forms a \"unit size\" of 225 × 112.5 × 75 mm (9 × × 3 inches), for a ratio of 6:3:2.\n\nIn the United States, modern standard bricks are specified for various uses; most are sized at about 8 ×   ×  inches (203 × 92 × 57 mm). The more commonly used is the modular brick   ×   ×  inches (194 × 92 × 57 mm). This modular brick of with a mortar joint eases the calculation of the number of bricks in a given wall.\n\nSome brickmakers create innovative sizes and shapes for bricks used for plastering (and therefore not visible on the inside of the building) where their inherent mechanical properties are more important than their visual ones. These bricks are usually slightly larger, but not as large as blocks and offer the following advantages:\n\nBlocks have a much greater range of sizes. Standard co-ordinating sizes in length and height (in mm) include 400×200, 450×150, 450×200, 450×225, 450×300, 600×150, 600×200, and 600×225; depths (work size, mm) include 60, 75, 90, 100, 115, 140, 150, 190, 200, 225, and 250. They are usable across this range as they are lighter than clay bricks. The density of solid clay bricks is around 2000 kg/m³: this is reduced by frogging, hollow bricks, and so on, but aerated autoclaved concrete, even as a solid brick, can have densities in the range of 450–850 kg/m³.\n\nBricks may also be classified as \"solid\" (less than 25% perforations by volume, although the brick may be \"frogged,\" having indentations on one of the longer faces), \"perforated\" (containing a pattern of small holes through the brick, removing no more than 25% of the volume), \"cellular\" (containing a pattern of holes removing more than 20% of the volume, but closed on one face), or \"hollow\" (containing a pattern of large holes removing more than 25% of the brick's volume). Blocks may be solid, cellular or hollow\n\nThe term \"frog\" can refer to the indentation or the implement used to make it. Modern brickmakers usually use plastic frogs but in the past they were made of wood.\n\nThe compressive strength of bricks produced in the United States ranges from about 1000 lbf/in² to 15,000 lbf/in² (7 to 105 MPa or N/mm² ), varying according to the use to which the brick are to be put. In England clay bricks can have strengths of up to 100 MPa, although a common house brick is likely to show a range of 20–40 MPa.\n\nIn the United States, bricks have been used for both buildings and pavements. Examples of brick use in buildings can be seen in colonial era buildings and other notable structures around the country. Bricks have been used in pavements especially during the late 19th century and early 20th century. The introduction of asphalt and concrete reduced the use of brick pavements, but it is used as a method of traffic calming or as a decorative surface in pedestrian precincts. For example, in the early 1900s, most of the streets in the city of Grand Rapids, Michigan, were paved with bricks. Today, there are only about 20 blocks of brick-paved streets remaining (totalling less than 0.5 percent of all the streets in the city limits). Much like in Grand Rapids, municipalities across the United States began replacing brick streets with inexpensive asphalt concrete by the mid-20th century.\n\nBricks in the metallurgy and glass industries are often used for lining furnaces, in particular refractory bricks such as silica, magnesia, chamotte and neutral (chromomagnesite) refractory bricks. This type of brick must have good thermal shock resistance, refractoriness under load, high melting point, and satisfactory porosity. There is a large refractory brick industry, especially in the United Kingdom, Japan, the United States, Belgium and the Netherlands.\n\nIn Northwest Europe, bricks have been used in construction for centuries. Until recently, almost all houses were built almost entirely from bricks. Although many houses are now built using a mixture of concrete blocks and other materials, many houses are skinned with a layer of bricks on the outside for aesthetic appeal.\n\nEngineering bricks are used where strength, low water porosity or acid (flue gas) resistance are needed.\n\nIn the UK a red brick university is one founded in the late 19th or early 20th century. The term is used to refer to such institutions collectively to distinguish them from the older Oxbridge institutions, and refers to the use of bricks, as opposed to stone, in their buildings.\n\nColombian architect Rogelio Salmona was noted for his extensive use of red bricks in his buildings and for using natural shapes like spirals, radial geometry and curves in his designs. Most buildings in Colombia are made of brick, given the abundance of clay in equatorial countries like this one.\n\nStarting in the 20th century, the use of brickwork declined in some areas due to concerns with earthquakes. Earthquakes such as the San Francisco earthquake of 1906 and the 1933 Long Beach earthquake revealed the weaknesses of unreinforced brick masonry in earthquake-prone areas. During seismic events, the mortar cracks and crumbles, and the bricks are no longer held together. Brick masonry with steel reinforcement, which helps hold the masonry together during earthquakes, was used to replace many of the unreinforced masonry buildings. Retrofitting older unreinforced masonry structures has been mandated in many jurisdictions.\n\n\n"}
{"id": "10284736", "url": "https://en.wikipedia.org/wiki?curid=10284736", "title": "CBU-87 Combined Effects Munition", "text": "CBU-87 Combined Effects Munition\n\nThe CBU-87 Combined Effects Munition is a cluster bomb used by the United States Air Force, developed by Aerojet General/Honeywell and introduced in 1986 to replace the earlier cluster bombs used in the Vietnam War. CBU stands for Cluster Bomb Unit. When the CBU-87 is used in conjunction with the Wind Corrected Munitions Dispenser guidance tail kit, it becomes a precision-guided weapon, and is designated CBU-103.\n\nThe basic CBU-87 is designed to be dropped from an aircraft at any altitude and any air speed. It is a free-falling bomb and relies on the aircraft to aim it before it drops; once dropped it needs no further instruction, as opposed to guided munitions or smart bombs. The bomb can be dropped by a variety of modern-day aircraft. It is long, has a diameter of , and weighs roughly . The price is US$14,000 per bomb.\n\nEach CBU-87 consists of an SUU-65B canister, a fuze with 12 time delay options and 202 submunitions (or bomblets) designated BLU-97/B Combined Effects Bomb. Each bomblet is a yellow cylinder with a length of 20 centimeters and a diameter of 6 centimeters. The BLU-97/B bomblets are designed to be used against armour, personnel and softskin targets and consist of a shaped charge, a scored steel fragmentation case and a zirconium ring for incendiary effects. The CBU-87 can also be equipped with an optional FZU-39/B proximity sensor with 10 altitude selections.\n\nWhen dropped from an aircraft, the bomb starts spinning. There are 6 speeds that can adjust the bomb's rate of spin. After it drops to a certain altitude, the canister breaks open and the submunitions are released. Each bomblet has a ring of tabs at the tail end, these orient the bomblet and deploy an inflatable decelerator to decrease the falling speed of the bomblet. When the submunitions hit the ground, they will cover a large area and the CBU-87 can be adjusted so it can cover a smaller or wider area. Depending on the rate of spin and the altitude at which the canister opens, it can cover an area between 20x20 meters (low release altitude and a slow rate of spin) to 120x240 meters (high release altitude and a high rate of spin).\n\nManufacturers and the Department of Defense have claimed that the failure rate for each bomb is about 5%. This would mean that of the 202 bomblets dropped, about 10 will not explode on impact. Landmine Action has claimed the failure rate of the BLU-97/Bs used in the Kosovo campaign was higher, between 7 and 8 percent.\n\nDuring Operation Desert Storm, the US Air Force dropped 10,035 CBU-87s. During Operation Allied Force, the US dropped about 1,100 cluster bombs, mostly CBU-87s.\n\nOn May 7, 1999, a CBU-87 was used in one of the most serious incidents involving civilian deaths and cluster bombs, the Niš cluster bombing.\n\n\n"}
{"id": "22508345", "url": "https://en.wikipedia.org/wiki?curid=22508345", "title": "CK Life Sciences", "text": "CK Life Sciences\n\nCK Life Sciences International (Holdings) Inc. () (), or CK Life Sciences, is the subsidiary of Cheung Kong Holdings in Hong Kong. It is engaged in the business of research and development, commercialization, marketing and sale of biotechnology products. The chairman is Mr. Victor Li, the elder son of Mr. Li Ka-shing, the chairman of Cheung Kong Holdings.\n\nIt was established in 2000 and listed on the Hong Kong Stock Exchange via Growth Enterprise Market in 2002 (Former stock code: ). It was transferred to Main Board of the Hong Kong Stock Exchange in 2008.\n\nCK Life Sciences has operations in Australia, and in 2012 acquired an Australian salt company (Cheetham Salt) as part of its expansion into the agricultural sector.\n"}
{"id": "5788420", "url": "https://en.wikipedia.org/wiki?curid=5788420", "title": "Cartridge paper", "text": "Cartridge paper\n\nCartridge paper is a high quality type of heavy paper used for illustration and drawing. Paper of this type was originally used for making paper cartridges for firearms.\n\n\n"}
{"id": "22380334", "url": "https://en.wikipedia.org/wiki?curid=22380334", "title": "Chevron CRUSH", "text": "Chevron CRUSH\n\nChevron CRUSH is an experimental \"in situ\" shale oil extraction technology to convert kerogen in oil shale to shale oil. The name stands for Chevron's Technology for the Recovery and Upgrading of Oil from Shale. It is developed jointly by Chevron Corporation and the Los Alamos National Laboratory.\n\nThe Chevron CRUSH technology bases on the earlier \"in situ\" efforts. Sinclair Oil Corporation conducted an experiment using both natural and induced fractures to establish communication between wells and developing an \"in situ\" combustion process. Geokinetics, the Sandia National Laboratories, and the Laramie Energy Technology Center of the U.S. Department of Energy conducted field tests fracturing oil-shale formation by explosives and hydraulic fracturing technology. Equity Oil Company, Continental Oil Company and the University of Akron studied the benefit of carbon dioxide as a carrier gas to facilitate a higher yield of shale oil. Based on these works, Chevron Corporation and the Los Alamos National Laboratory started a cooperation in 2006 to improve the recovery of hydrocarbons from oil shale. In 2006, the United States Department of the Interior issued a research, development and demonstration lease for Chevron's demonstration project on public lands in Colorado’s Piceance Basin. In February 2012, Chevron notified the Bureau of Land Management and the Department of Reclamation, Mining and Safety that it intends to divest this lease.\n\nFor decomposition kerogen in oil shale, the Chevron CRUSH process uses heated carbon dioxide. The process involves drilling vertical wells into the oil shale formation and applying horizontal fractures induced by injecting carbon dioxide through drilled wells and then pressured through the formation for circulation through the fractured intervals to rubblize the production zone. For further rubblization propellants and explosives may be used. The used carbon dioxide then be routed to the gas generator to be reheated and recycled. The remaining organic matter in previously heated and depleted zones is combusted \"in-situ\" to generate the heated gases required to process successive intervals. These gases would then be pressured from the depleted zone into the newly fractured portion of the formation and the process would be repeated. The hydrocarbon fluids are brought up in conventional vertical oil wells.\n\nThe processing area is isolated from surrounding groundwater by creating fractured areas (\"pockets\"), approximately wide and high within the center of the oil shale deposit. In this way, about of the confining layer would separate the process area from the water bearing layers above and below, keeping the aquifers out of the production zone.\n\n"}
{"id": "39408928", "url": "https://en.wikipedia.org/wiki?curid=39408928", "title": "China greentech initiative", "text": "China greentech initiative\n\nThe China Greentech Initiative (CGTI) is the only Chinese-international open source, commercial collaboration platform of over 100 of the world's leading technology and services companies and over 500 executives and industry experts accelerating the growth of China's greentech markets. Through the Initiative, these organizations have come together to define greentech market opportunities and solutions which will contribute to building a sustainable China and world. The organization was founded in 2008 by Ellen G. Carberry and Randall S. Hancock, and is produced by MangoStrategy, LLC.\n\nThe China Greentech Report is a free, annual report published in English and Chinese by the China Greentech Initiative, which can be downloaded on their website. The report is an update of the recent developments in the greentech sector in China. The China Greentech Report 2012 analyses four key factors that characterise challenges and opportunities in China's greentech markets, including:\n\n\nThe CGTI 2012 Partner Program examines opportunities in five sector tracks: cleaner conventional energy, renewable energy, green building, cleaner transportation, and clean water. In addition, CGTI is in discussion with partners interested in launching cross sector tracks which examine opportunities in low carbon zones, waste management, green supply chain, and China outbound markets.\n\nThe China Greentech Initiative's advisory services provide partner companies and organizations with projects to meet specific needs that are beyond the scope of what the Partner Program provides, including:\n\n\nThe China Greentech has been mentioned in many media outlets, notably The New York Times, The Guardian, The Economist, and the China Daily.\n"}
{"id": "3772254", "url": "https://en.wikipedia.org/wiki?curid=3772254", "title": "Cine 160", "text": "Cine 160\n\nCine 160 is a 35 mm film projection process proposed by Allan Silliphant whereby a single frame of film would occupy a length of six film perforations. This could then be used for either of two currently proposed applications: 3-D film projection from two images each occupying 3 perforations (thus attaining a 1.85 aspect ratio already in common use), or making anamorphically squeezed prints of 1.85 ratio films, which would use a greater amount of image area. The system is named Cine 160 because the six-perf frame uses 1.60 times the area of a conventional print. This system has not yet received any mainstream application, however, and it is unknown how receptive theater owners will be to the prospect, which will require significant expenses to re-fit projectors to the format.\n\n\n"}
{"id": "18949896", "url": "https://en.wikipedia.org/wiki?curid=18949896", "title": "Computer cluster", "text": "Computer cluster\n\nA computer cluster is a set of loosely or tightly connected computers that work together so that, in many respects, they can be viewed as a single system. Unlike grid computers, computer clusters have each node set to perform the same task, controlled and scheduled by software.\n\nThe components of a cluster are usually connected to each other through fast local area networks, with each \"node\" (computer used as a server) running its own instance of an operating system. In most circumstances, all of the nodes use the same hardware and the same operating system, although in some setups (e.g. using Open Source Cluster Application Resources (OSCAR)), different operating systems can be used on each computer, or different hardware.\n\nClusters are usually deployed to improve performance and availability over that of a single computer, while typically being much more cost-effective than single computers of comparable speed or availability.\n\nComputer clusters emerged as a result of convergence of a number of computing trends including the availability of low-cost microprocessors, high-speed networks, and software for high-performance distributed computing. They have a wide range of applicability and deployment, ranging from small business clusters with a handful of nodes to some of the fastest supercomputers in the world such as IBM's Sequoia. Prior to the advent of clusters, single unit fault tolerant mainframes with modular redundancy were employed; but the lower upfront cost of clusters, and increased speed of network fabric has favoured the adoption of clusters. In contrast to high-reliability mainframes clusters are cheaper to scale out, but also have increased complexity in error handling, as in clusters error modes are not opaque to running programs.\n\nThe desire to get more computing power and better reliability by orchestrating a number of low-cost commercial off-the-shelf computers has given rise to a variety of architectures and configurations.\n\nThe computer clustering approach usually (but not always) connects a number of readily available computing nodes (e.g. personal computers used as servers) via a fast local area network. The activities of the computing nodes are orchestrated by \"clustering middleware\", a software layer that sits atop the nodes and allows the users to treat the cluster as by and large one cohesive computing unit, e.g. via a single system image concept.\n\nComputer clustering relies on a centralized management approach which makes the nodes available as orchestrated shared servers. It is distinct from other approaches such as peer to peer or grid computing which also use many nodes, but with a far more distributed nature.\n\nA computer cluster may be a simple two-node system which just connects two personal computers, or may be a very fast supercomputer. A basic approach to building a cluster is that of a Beowulf cluster which may be built with a few personal computers to produce a cost-effective alternative to traditional high performance computing. An early project that showed the viability of the concept was the 133-node Stone Soupercomputer. The developers used Linux, the Parallel Virtual Machine toolkit and the Message Passing Interface library to achieve high performance at a relatively low cost.\n\nAlthough a cluster may consist of just a few personal computers connected by a simple network, the cluster architecture may also be used to achieve very high levels of performance. The TOP500 organization's semiannual list of the 500 fastest supercomputers often includes many clusters, e.g. the world's fastest machine in 2011 was the K computer which has a distributed memory, cluster architecture.\n\nGreg Pfister has stated that clusters were not invented by any specific vendor but by customers who could not fit all their work on one computer, or needed a backup. Pfister estimates the date as some time in the 1960s. The formal engineering basis of cluster computing as a means of doing parallel work of any sort was arguably invented by Gene Amdahl of IBM, who in 1967 published what has come to be regarded as the seminal paper on parallel processing: Amdahl's Law.\n\nThe history of early computer clusters is more or less directly tied into the history of early networks, as one of the primary motivations for the development of a network was to link computing resources, creating a de facto computer cluster.\n\nThe first production system designed as a cluster was the Burroughs B5700 in the mid-1960s. This allowed up to four computers, each with either one or two processors, to be tightly coupled to a common disk storage subsystem in order to distribute the workload. Unlike standard multiprocessor systems, each computer could be restarted without disrupting overall operation.\n\nThe first commercial loosely coupled clustering product was Datapoint Corporation's \"Attached Resource Computer\" (ARC) system, developed in 1977, and using ARCnet as the cluster interface. Clustering per se did not really take off until Digital Equipment Corporation released their VAXcluster product in 1984 for the VAX/VMS operating system (now named as OpenVMS). The ARC and VAXcluster products not only supported parallel computing, but also shared file systems and peripheral devices. The idea was to provide the advantages of parallel processing, while maintaining data reliability and uniqueness. Two other noteworthy early commercial clusters were the \"Tandem Himalayan\" (a circa 1994 high-availability product) and the \"IBM S/390 Parallel Sysplex\" (also circa 1994, primarily for business use).\n\nWithin the same time frame, while computer clusters used parallelism outside the computer on a commodity network, supercomputers began to use them within the same computer. Following the success of the CDC 6600 in 1964, the Cray 1 was delivered in 1976, and introduced internal parallelism via vector processing. While early supercomputers excluded clusters and relied on shared memory, in time some of the fastest supercomputers (e.g. the K computer) relied on cluster architectures.\n\nComputer clusters may be configured for different purposes ranging from general purpose business needs such as web-service support, to computation-intensive scientific calculations. In either case, the cluster may use a high-availability approach. Note that the attributes described below are not exclusive and a \"computer cluster\" may also use a high-availability approach, etc.\n\n\"Load-balancing\" clusters are configurations in which cluster-nodes share computational workload to provide better overall performance. For example, a web server cluster may assign different queries to different nodes, so the overall response time will be optimized. However, approaches to load-balancing may significantly differ among applications, e.g. a high-performance cluster used for scientific computations would balance load with different algorithms from a web-server cluster which may just use a simple round-robin method by assigning each new request to a different node.\n\nComputer clusters are used for computation-intensive purposes, rather than handling IO-oriented operations such as web service or databases. For instance, a computer cluster might support computational simulations of vehicle crashes or weather. Very tightly coupled computer clusters are designed for work that may approach \"supercomputing\".\n\n\"High-availability clusters\" (also known as failover clusters, or HA clusters) improve the availability of the cluster approach. They operate by having redundant nodes, which are then used to provide service when system components fail. HA cluster implementations attempt to use redundancy of cluster components to eliminate single points of failure. There are commercial implementations of High-Availability clusters for many operating systems. The Linux-HA project is one commonly used free software HA package for the Linux operating system.\n\nClusters are primarily designed with performance in mind, but installations are based on many other factors. Fault tolerance (\"the ability for a system to continue working with a malfunctioning node\") allows for scalability, and in high performance situations, low frequency of maintenance routines, resource consolidation(e.g. RAID), and centralized management. Advantages include enabling data recovery in the event of a disaster and providing parallel data processing and high processing capacity.\n\nIn terms of scalability, clusters provide this in their ability to add nodes horizontally. This means that more computers may be added to the cluster, to improve its performance, redundancy and fault tolerance. This can be an inexpensive solution for a higher performing cluster compared to scaling up a single node in the cluster. This property of computer clusters can allow for larger computational loads to be executed by a larger number of lower performing computers.\n\nWhen adding a new node to a cluster, reliability increases because the entire cluster does not need to be taken down. A single node can be taken down for maintenance, while the rest of the cluster takes on the load of that individual node.\n\nIf you have a large number of computers clustered together, this lends itself to the use of distributed file systems and RAID, both of which can increase the reliability and speed of a cluster.\n\nOne of the issues in designing a cluster is how tightly coupled the individual nodes may be. For instance, a single computer job may require frequent communication among nodes: this implies that the cluster shares a dedicated network, is densely located, and probably has homogeneous nodes. The other extreme is where a computer job uses one or few nodes, and needs little or no inter-node communication, approaching grid computing.\n\nIn a Beowulf cluster, the application programs never see the computational nodes (also called slave computers) but only interact with the \"Master\" which is a specific computer handling the scheduling and management of the slaves. In a typical implementation the Master has two network interfaces, one that communicates with the private Beowulf network for the slaves, the other for the general purpose network of the organization. The slave computers typically have their own version of the same operating system, and local memory and disk space. However, the private slave network may also have a large and shared file server that stores global persistent data, accessed by the slaves as needed.\n\nA special purpose 144-node DEGIMA cluster is tuned to running astrophysical N-body simulations using the Multiple-Walk parallel treecode, rather than general purpose scientific computations.\n\nDue to the increasing computing power of each generation of game consoles, a novel use has emerged where they are repurposed into High-performance computing (HPC) clusters. Some examples of game console clusters are Sony PlayStation clusters and Microsoft Xbox clusters. Another example of consumer game product is the Nvidia Tesla Personal Supercomputer workstation, which uses multiple graphics accelerator processor chips. Besides game consoles, high-end graphics cards too can be used instead. The use of graphics cards (or rather their GPU's) to do calculations for grid computing is vastly more economical than using CPU's, despite being less precise. However, when using double-precision values, they become as precise to work with as CPU's and are still much less costly (purchase cost).\n\nComputer clusters have historically run on separate physical computers with the same operating system. With the advent of virtualization, the cluster nodes may run on separate physical computers with different operating systems which are painted above with a virtual layer to look similar. The cluster may also be virtualized on various configurations as maintenance takes place. An example implementation is Xen as the virtualization manager with Linux-HA.\n\nAs the computer clusters were appearing during the 1980s, so were supercomputers. One of the elements that distinguished the three classes at that time was that the early supercomputers relied on shared memory. To date clusters do not typically use physically shared memory, while many supercomputer architectures have also abandoned it.\n\nHowever, the use of a clustered file system is essential in modern computer clusters. Examples include the IBM General Parallel File System, Microsoft's Cluster Shared Volumes or the Oracle Cluster File System.\n\nTwo widely used approaches for communication between cluster nodes are MPI (Message Passing Interface) and PVM (Parallel Virtual Machine).\n\nPVM was developed at the Oak Ridge National Laboratory around 1989 before MPI was available. PVM must be directly installed on every cluster node and provides a set of software libraries that paint the node as a \"parallel virtual machine\". PVM provides a run-time environment for message-passing, task and resource management, and fault notification. PVM can be used by user programs written in C, C++, or Fortran, etc.\n\nMPI emerged in the early 1990s out of discussions among 40 organizations. The initial effort was supported by ARPA and National Science Foundation. Rather than starting anew, the design of MPI drew on various features available in commercial systems of the time. The MPI specifications then gave rise to specific implementations. MPI implementations typically use TCP/IP and socket connections. MPI is now a widely available communications model that enables parallel programs to be written in languages such as C, Fortran, Python, etc. Thus, unlike PVM which provides a concrete implementation, MPI is a specification which has been implemented in systems such as MPICH and Open MPI.\n\nOne of the challenges in the use of a computer cluster is the cost of administrating it which can at times be as high as the cost of administrating N independent machines, if the cluster has N nodes. In some cases this provides an advantage to shared memory architectures with lower administration costs. This has also made virtual machines popular, due to the ease of administration.\n\nWhen a large multi-user cluster needs to access very large amounts of data, task scheduling becomes a challenge. In a heterogeneous CPU-GPU cluster with a complex application environment, the performance of each job depends on the characteristics of the underlying cluster. Therefore, mapping tasks onto CPU cores and GPU devices provides significant challenges. This is an area of ongoing research; algorithms that combine and extend MapReduce and Hadoop have been proposed and studied.\n\nWhen a node in a cluster fails, strategies such as \"fencing\" may be employed to keep the rest of the system operational. Fencing is the process of isolating a node or protecting shared resources when a node appears to be malfunctioning. There are two classes of fencing methods; one disables a node itself, and the other disallows access to resources such as shared disks.\n\nThe STONITH method stands for \"Shoot The Other Node In The Head\", meaning that the suspected node is disabled or powered off. For instance, \"power fencing\" uses a power controller to turn off an inoperable node.\n\nThe \"resources fencing\" approach disallows access to resources without powering off the node. This may include \"persistent reservation fencing\" via the SCSI3, fibre channel fencing to disable the fibre channel port, or global network block device (GNBD) fencing to disable access to the GNBD server.\n\nLoad balancing clusters such as web servers use cluster architectures to support a large number of users and typically each user request is routed to a specific node, achieving task parallelism without multi-node cooperation, given that the main goal of the system is providing rapid user access to shared data. However, \"computer clusters\" which perform complex computations for a small number of users need to take advantage of the parallel processing capabilities of the cluster and partition \"the same computation\" among several nodes.\n\nAutomatic parallelization of programs continues to remain a technical challenge, but parallel programming models can be used to effectuate a higher degree of parallelism via the simultaneous execution of separate portions of a program on different processors.\n\nThe development and debugging of parallel programs on a cluster requires parallel language primitives as well as suitable tools such as those discussed by the \"High Performance Debugging Forum\" (HPDF) which resulted in the HPD specifications.\nTools such as TotalView were then developed to debug parallel implementations on computer clusters which use MPI or PVM for message passing.\n\nThe Berkeley NOW (Network of Workstations) system gathers cluster data and stores them in a database, while a system such as PARMON, developed in India, allows for the visual observation and management of large clusters.\n\nApplication checkpointing can be used to restore a given state of the system when a node fails during a long multi-node computation. This is essential in large clusters, given that as the number of nodes increases, so does the likelihood of node failure under heavy computational loads. Checkpointing can restore the system to a stable state so that processing can resume without having to recompute results.\n\nThe GNU/Linux world supports various cluster software; for application clustering, there is distcc, and MPICH. Linux Virtual Server, Linux-HA - director-based clusters that allow incoming requests for services to be distributed across multiple cluster nodes. MOSIX, LinuxPMI, Kerrighed, OpenSSI are full-blown clusters integrated into the kernel that provide for automatic process migration among homogeneous nodes. OpenSSI, openMosix and Kerrighed are single-system image implementations.\n\nMicrosoft Windows computer cluster Server 2003 based on the Windows Server platform provides pieces for High Performance Computing like the Job Scheduler, MSMPI library and management tools.\n\ngLite is a set of middleware technologies created by the Enabling Grids for E-sciencE (EGEE) project.\n\nslurm is also used to schedule and manage some of the largest supercomputer clusters (see top500 list).\n\nAlthough most computer clusters are permanent fixtures, attempts at flash mob computing have been made to build short-lived clusters for specific computations. However, larger-scale volunteer computing systems such as BOINC-based systems have had more followers.\n\n\n"}
{"id": "695742", "url": "https://en.wikipedia.org/wiki?curid=695742", "title": "Cordwood construction", "text": "Cordwood construction\n\nCordwood construction (also called \"cordwood masonry\", \"cordwood building\", \"stackwall construction\", \"stovewood construction\" or \"stackwood construction\") is a term used for a natural building method in which short logs are piled crosswise to build a wall, using mortar or cob to permanently secure them. This technique can use local materials at minimal cost. \n\nWalls are usually constructed so that the log ends protrude from the mortar by a small amount (an inch or less). Walls typically range between 8 and 24 inches thick, though in northern Canada, some walls are as much as 36 inches thick.\n\nCordwood homes are attractive for their visual appeal, maximization of interior space (with a rounded plan), economy of resources, and ease of construction. Wood usually accounts for about 40-60% of the wall system, the remaining portion consisting of a mortar mix and insulating fill. Cordwood construction can be sustainable depending on design and process. There are two main types of cordwood construction, throughwall and M-I-M (mortar-insulation-mortar). In throughwall, the mortar mix itself contains an insulative material, usually sawdust, chopped newsprint, or paper sludge, in sometimes very high percentages by mass (80% paper sludge/20% mortar). In the more common M-I-M, and unlike brick or throughwall masonry, the mortar does not continue throughout the wall. Instead, three- or four-inch (sometimes more) beads of mortar on each side of the wall provide stability and support, with a separate insulation between them. Cordwood walls can be load-bearing (using built-up corners, or curved wall designed) or laid within a post and beam framework which provides structural reinforcement and is suitable for earthquake-prone areas. As a load-bearing wall, the compressive strength of wood and mortar allows for roofing to be tied directly into the wall. Different mortar mixtures and insulation fill material both affect the wall's overall R value, or resistance to heat flow; and conversely, to its inherent thermal mass, or heat/cool storage capacity.\n\nRemains of cordwood structures still standing date back as far as one thousand years in eastern Germany. However, more contemporary versions could be found in Europe, Asia, and the Americas. There is no detailed information about the origins of cordwood construction. However, it is plausible that forest dwellers eventually erected a basic shelter between a fire and a stacked wood pile. In the work of William Tischler of University of Wisconsin, he states that \"current\" cordwood probably started in the late 1800s (decade) in Quebec, Wisconsin, and Sweden. He believes that the technique started in these areas around the same time.\n\nCordwood construction is an economical use of log ends or fallen trees in heavily timbered areas. Other common sources for wood include sawmills, split firewood, utility poles (without creosote), split rail fence posts, and logging slash. It is more sustainable and often economical to use recycled materials for the walls. Regardless of the source, all wood must be debarked before the construction begins. While many different types of wood can be used, the most desirable rot resistant woods are Pacific yew, bald cypress (new growth), cedars, and juniper. Acceptable woods also include Douglas fir, western larch, Eastern White Pine, and Spruce Pine.\n\nLess dense and more airy woods are superior because they shrink and expand in lower proportions than dense hardwoods. Most wood can be used in a wall if it is dried properly and stabilized to the external climate's relative humidity. Furthermore, while log ends of different species can be mixed in a wall, log-ends of identical species and source limit expansion/contraction variables.\n\nVarious experts advise different recipes for mortar mix. One recipe which has proven to be successful since 1981 is 9 parts sand, 3 sawdust, 3 builder's lime (non-agricultural), 2 Portland cement by volume. Builder's lime makes the wall more flexible, breathable, and self-healing because it takes longer to completely set than cement. Portland cement chemically binds the mortar and should be either Type I or II. Another recipe uses 3 parts sand, 2 soaked sawdust, 1 Portland Cement and 1 Hydrated Lime; intended to have the advantage of curing slower and displaying less cracking.\n\nDepending on a variety of factors (wall thickness, type of wood, particular mortar recipe), the insulative value of a cordwood wall, as expressed in R-value is generally less than that of a high-efficiency stud wall. Cordwood walls have greater thermal mass than stud frame but less than common brick and mortar. This is because the specific heat capacity of clay brick is higher (0.84 versus wood's 0.42), and is denser than airy woods like cedar, cypress, or pine. However, the insulated mortar matrix utilized in most cordwood walls places useful thermal mass on both sides of the insulated internal cavity, helping to store heat in winter and \"coolth\" in summer. Thermal mass makes it easier for a building to maintain median interior temperatures while going through daily hot and cold phases. In climates like the desert with broad daily temperature swings thermal mass will absorb and then slowly release the midday heat and nighttime cool in sequence, moderating temperature fluctuations. Thermal mass does not replace the function of insulation material, but is used in conjunction with it.\n\nThe longer the logs (and thicker the wall), the better the insulation qualities. A common 16” cordwood wall for moderate climates comprises of perlite or vermiculite insulation between mortar joints. Another insulation option, used for over 40 years by Rob Roy and other cordwood builders is dry sawdust, passed through a half-inch screen, and treated with builder's (Type S) lime at the ratio of 12 parts sawdust to 1 part lime. With light airy sawdusts, this insulation is similar in its R-value to manufactured loose-fill insulations, at a fraction of the cost. \n\nHowever, wood is an anisotropic material with respect to heat flow. That means its thermal resistance depends on the direction of heat flow relative to the wood grain. While wood has a commonly quoted R-value of about 1.25 per inch (depending on the species and moisture content), that only applies if the heat flow is perpendicular to the grain, such as occurs in common wood frame construction. With cordwood/stackwall construction, the direction of heat flow is parallel to the grain. For this configuration, the R-value is only about 40% of that perpendicular to the grain. Thus, the actual R-value of wood, when used in cordwood/stackwall construction is closer to about 0.50 per inch. \n\nBut the R-value of a cordwood masonry wall must take into consideration both the wooden portion and the insulated mortar portion as a combined system. The only authoritative testing on the R-value of cordwood masonry was conducted by Dr. Kris J. Dick (PE) and Luke Chaput during the winter of 2004-2005, based on thermal sensors placed within a 24-inch thick wall at the University of Manitoba. A paper reporting on their findings appears in \"Cordwood and the Code: a building permit guide\" The authors' summary says, in part: \"Based on approximately three months of mid-winter temperature data, the wall was determined to have an RSI Value of 6.23 (m²K/W), R-35 for a 24-inch wall system.\" \n\nA thermal performance analysis in 1998 using “HOT 2000” computer software showed the relationship of domestic wall types and their insulating values. The simulation revealed an R value of 20.5 for the sample cordwood wall. Compare this to the basic 2 x 4 wooden stud wall, and 2 x 6 foam insulated and sheathed wall with R values of 15.8 and 25.7, respectively. Cordwood walls are not the best natural insulators but can be built to thermal efficient standards. The R value of a cordwood wall is directly related to its ratio of wood to mortar and insulation medium. However, R value in cordwood construction is not as significant as it is in stick-frame building due to the high thermal mass which increases a significantly higher \"effective R-value.” Builders tailor their design and ratios to the existing climate.\n\nR-value testing was completed at the University of Manitoba in the winter of 2005. The findings compiled by the Engineering Department, found that each inch of cordwood wall (mortar, log end and sawdust/lime insulation yielded an r-value of 1.47.\n\nA cordwood home in many cases is constructed for significantly less initial out of pocket cost than a standard stick frame house of comparable size since in many cases labor is done primarily by the owner or volunteers. Properly built cordwood walls tend to have less maintenance needs than standard stick frame since there are fewer manufactured components (such as fiberglass insulation, nailings, sidings, flashings, etc). Note that some maintenance still will be required as there is wood and concrete exposed to the elements on the exterior side of the wall. Also, a cordwood house that is poorly built without sufficient insulation, can result in higher heating costs than a traditional stud frame house.\n\nIn a 1998 comparative economic analysis of stud frame, cordwood, straw bale, and cob, cordwood appears to be an economically viable alternative. A two story cordwood house in Cherokee, North Carolina outfitted with \"high quality tile, tongue, and groove pine, Russian Woodstove, live earth roof, hand shaped cedar trim, raised panel cabinets, and a handmade pine door,\" cost the owner an estimated $52,000. With the owner providing 99% of the labor, the house cost him $20.70 per sq. ft. A comparably sized and furnished stick frame house in 1998 would cost between $75,000-$120,000 with zero owner labor. The 1997 residential cost data shows an \"average\" trim level 1,000- house costing $64.48-$81.76 per sq. ft. Both the acquisition of materials and source of labor play major roles in the initial cost of building a cordwood house.\n\nIn certain jurisdictions construction plans are subject to the building inspector's approval. Before building, soil conditions on the site must be verified to support heavy cordwood masonry walls.\n\nWith felled timber, bark should be removed in the spring with a shovel, chisel, or bark spud. The sap is still running in spring time and provides a lubricating layer of cambium between the bark and wood, making separation an easier task than if left until the fall when the two layers are well-bonded together. Once debarked, the logs should sit to dry for at least three summers to limit splitting and checking. It is important to cut the logs, once debarked to the chosen building length. Richard Flatau, \"Cordwood Construction: Best Practices\" (2012) suggest splitting 70% of the wood for better drying and seasoning.\n\nAfter drying, the logs must be cut to the desired length (usually 8, 12, 16, 18, or 24 in.). In this case a metal handsaw is preferable to a chainsaw because its finer cut helps to ward moisture and pest penetration. Actually a \"cut off \" saw or \"buzz saw\" will make quick work of cutting cordwood into chosen lengths. For especially furry ends like on cedar, rasps can be used for smoothing.\n\nThe wood then needs to be transported to the building site. It is convenient to have the source of cordwood and construction site nearby. Once a proper foundation has been poured which rises 12-24 inches above ground level with a splash guard, construction of the walls can begin. Temporary shelters can be used to cover the worksite and cordwood from rain. A post and beam frame supplies this shelter for subsequent cordwood mortaring.\n\nInexperienced homebuilders should experiment with a number of practice walls. This will ultimately expedite the building process and provide more satisfying results. When experimenting with M-I-M, (the more common form), two parallel 3 to 4 inch beads of mortar are laid down along the foundation, followed by a middle filling of insulation material. Then logs are laid on top with consistent mortar gaps, protruding no more than 1 inch on the inside and outside of the wall. Actual placement will depend on the size and shapes of the logs. Another layer of mortar is spread, then insulation poured in between, more logs follow and so on. When experimenting with Throughwall, a thin, even layer of insulative mortar is laid along the foundation, then the logs are seated firmly in the mortar bed, in an even fashion, leaving only enough space between them to \"point\" the mortar. The mortar gaps are filled to make a relatively flat top surface, then another thin layer of mortar is added and the process repeats. The shape and exterior orientation of logs is important only for appearance. Pre-split “firewood style” logs check less when in the wall and are easier to \"point\" or smooth and press evenly around than round pieces because the mortar gaps are generally smaller. \n\nRob and Jaki Roy, co-directors of Earthwood Building School in West Chazy, NY for 36 years, take a different point of view. They used to use mostly split wood, but now use mostly white cedar (or equal) rounds. The shrinkage is exactly the same in splits and rounds, and the Roys have found the wood easier to lay up because it more readily holds its shape from one side of the wall to the other. Further, the rounds are easier to point, because of the ragged edge that results on the bottom side of a split log. Finally, the greater amount of mortar using rounds is actually a plus because the mortared portion of the wall performs better, thermally, than the wooden portion. \n\nIf constructing a house with corners, each course of cordwood should be cross hatched for strength. Near the end, small filler slats of wood may be required to finish the joining or tops of walls. Windows and doors are framed with standard window boxes and wooden lintels. Glass bottles can be inserted for a creative stained glass effect. (Plumbing and electrical wiring are issues to consider but will not be elaborated on in this article).\n\nA cordwood house should have deep overhanging eaves of at least 12- 16 inches to keep the log ends dry and prevent fungal growth. If the ends are maintained to be dry and well aerated, they will age without problem. Some owners have coated their ends with linseed oil, or set the outside log ends flush with the mortar for further weatherproofing. Over time, some checking is normal, and can be remedied with periodic mortar or caulking maintenance.\n\nAlthough cordwood homes have been tested in -40F locations like Alberta, their thermal efficiency in any climate is below that of a purely cob house of comparable dimensions. In frigid areas it is appropriate to either build a thicker 24-36 inch wall, or two separate super insulated walls. In predominantly wet areas, the outside walls can be plastered, smothering the cordwood ends from air and moisture, but this hides cordwood's attractive log ends and the logs will rot. The quantity of labor relative to gaining a specific R value for cordwood is higher when compared to straw bale and stick frame construction. Funds saved in construction may need to be allocated for heating costs or longterm exterior maintenance. An organic, mortar-like cob creates less of an environmental impact because of the use of readily available mud and straw, whereas toxins emitted during the production of Portland cement are very harmful, albeit less tangible in the final product. Like many alternative building styles, the sustainability of cordwood construction is dependent upon materials and construction variables.\n\nFollowing the Cordwood Conference in 2005 at Merrill, Wisconsin, a document was published to address \"best practices\" in cordwood construction and building code compliance. The document entitled \"Cordwood and the Code: A Building Permit Guide\" assists cordwood builders get the necessary code permits.\n\n\n\n"}
{"id": "2528127", "url": "https://en.wikipedia.org/wiki?curid=2528127", "title": "Cryoprotectant", "text": "Cryoprotectant\n\nA cryoprotectant is a substance used to protect biological tissue from freezing damage (i.e. that due to ice formation). Arctic and Antarctic insects, fish and amphibians create cryoprotectants (antifreeze compounds and antifreeze proteins) in their bodies to minimize freezing damage during cold winter periods. Cryoprotectants are also used to preserve living materials in the study of biology and to preserve food products.\n\nCryoprotectants operate by increasing the solute concentration in cells. However, in order to be biologically viable they must easily penetrate and must not be toxic to cells.\n\nSome cryoprotectants function by lowering the glass transition temperature of a solution or of a material. In this way, the cryoprotectant prevents actual freezing, and the solution maintains some flexibility in a glassy phase. Many cryoprotectants also function by forming hydrogen bonds with biological molecules as water molecules are displaced. Hydrogen bonding in aqueous solutions is important for proper protein and DNA function. Thus, as the cryoprotectant replaces the water molecules, the biological material retains its native physiological structure and function, although they are no longer immersed in an aqueous environment. This preservation strategy is most often utilized in anhydrobiosis.\n\nMixtures of cryoprotectants have less toxicity and are more effective than single-agent cryoprotectants. A mixture of formamide with DMSO (dimethyl sulfoxide), propylene glycol, and a colloid was for many years the most effective of all artificially created cryoprotectants. Cryoprotectant mixtures have been used for vitrification (i.e. solidification without crystal ice formation). Vitrification has important applications in preserving embryos, biological tissues and organs for transplant. Vitrification is also used in cryonics, in an effort to eliminate freezing damage.\n\nConventional cryoprotectants are glycols (alcohols containing at least two hydroxyl groups), such as ethylene glycol , propylene glycol and glycerol. Ethylene glycol is commonly used as automobile antifreeze; while propylene glycol has been used to reduce ice formation in ice cream. Dimethyl sulfoxide (DMSO) is also regarded as a conventional cryoprotectant. Glycerol and DMSO have been used for decades by cryobiologists to reduce ice formation in sperm, oocytes, and embryos that are cold-preserved in liquid nitrogen. Cryoconservation of animal genetic resources is a practice that involves conventional cryoprotectants to store genetic material with the intention of future revival. Trehalose is non-reducing sugar produced by yeasts and insects in copious amounts. Its use as a cryoprotectant in commercial systems has been patented widely.\n\nInsects most often use sugars or polyols as cryoprotectants. One species that uses cryoprotectant is \"Polistes exclamans\" (a wasp). In this species, the different levels of cryoprotectant can be used to distinguish between morphologies.\n\nArctic frogs use glucose, but Arctic salamanders create glycerol in their livers for use as a cryoprotectant.\n\nCryoprotectants are also used to preserve foods. These compounds are typically sugars that are inexpensive and do not pose any toxicity concerns. For example, many (raw) frozen chicken products contain a sucrose and sodium phosphates solution in water.\n\n\n"}
{"id": "2260392", "url": "https://en.wikipedia.org/wiki?curid=2260392", "title": "DVD-by-mail", "text": "DVD-by-mail\n\nDVD-by-mail is a business model in which customers rent DVDs and Blu-rays of films and television shows, video games and VCDs, among other film media online, for delivery to the customer by mail. Generally, all interaction between the renter and the rental company takes place through the company's website, using an e-commerce model. Typically, a customer chooses from a list of film, show or game titles online and selects the titles she or he most wants to watch. As a customer's requested titles become available, the company sends them out to the customer through the mail. Once the customer has consumed the content, she or he sends the media back to the company via the mail.\n\nMost companies operate on the following model:\n\nMost companies let customers keep the films for as long as they want; customers are, however, limited to a set number of discs out at any one time. Commonly, once a disc is returned, another is sent out. Some companies or plans may have a limit on the total number of movies rented in a month. Memberships are usually billed monthly, and includes postage both ways.\n\nVariations exist; for example, some companies also offer video game rentals while others offer music. Redbox allows a user to reserve DVDs or Blu-ray discs online to retrieve and return the DVD at Interactive kiosks located in various retail establishments.\n\nNetflix began an online streaming program allowing for the online viewing of select movies and TV shows. however, around 4.2 million individuals in the U.S. still rent DVDs via mail from the company.\n\nGiven sufficiently fast mail delivery times, customers on \"Unlimited\" plans who return their discs quickly enough can receive enough shipments in a month that the company's actual cost of delivery exceeds the subscription fee, making this type of customer unprofitable. Even below this point, higher volume customers are by definition less profitable than customers who receive fewer discs per month. If these customers become too numerous, there are various measures which the rental company can take.\n\nOne is the so-called \"throttling\" approach, which received significant publicity with regard to Netflix (which refers to the practice as a \"fairness algorithm\"). In this case, high-volume customers may experience a greater likelihood of (slower) shipments from alternative warehouses, when the nearest shipment center does not have the requested disc. Also, if there is a high demand for a particular disc, it is more likely that an infrequent renter will get priority over the frequent renters, with the latter receiving a movie further down on their list. They are also less likely to receive replacement shipments on the same day a disc is received. Similar \"fair use\" caveats can be found in the Terms and Conditions of leading UK companies such as LOVEFiLM. In Canada, Zip.ca switched to \"Capped\" plans (with additional shipping charges for rentals over the cap) in part to avoid implementing \"throttling\".\n\nLOVEFiLM came under scrutiny from users over its claim to offer \"unlimited\" movie rentals. Some users reportedly found the company used long delays at the shipping stage to reduce the number of films a month a customer can rent. The company was subject to a dispute by the Advertising Standards Authority over the use of the word \"unlimited\" in their advertising. It was revealed that they practised throttling. The company itself claimed that this \"fair usage\" policy means all customers get a similar service.\n\nOn March 2, 2006, Blockbuster announced that their service does not implement throttling. \"We don't prioritize our customers' movie fulfillment based on how often they use our service, and we don't limit the number of movies a subscriber receives each month,\" according to Senior Vice-President Shayne Evangelist. However, the Terms and Conditions each customer has to agree to in order to subscribe to the service states \"BLOCKBUSTER Online reserves the right to determine product allocation among members in its sole discretion. In determining product allocation, we use various factors including, but not limited to, (i) the historical rental volume for each subscriber, (ii) historical number of outstanding rentals relative to the maximum number of outstanding BLOCKBUSTER Online Rentals allowed under a subscriber's plan, and (iii) the average rental queue position of BLOCKBUSTER Online Rentals that have shipped to a subscriber in the past.\"\n\nThe following is a summary of the major DVD-by-mail markets.\n\nNetflix ended 2008 with 9.39 million customers. Blockbuster Video claimed 1 million online customers in August 2005, 2 million by March 2006, and finished the first quarter of 2007 with 3 million. . By the end of 2013, Blockbuster had withdrawn from the DVD-by-mail market. Walmart briefly entered the market as well, but withdrew in 2005 and now has a cross-promotional agreement with Netflix. There are a number of smaller companies, some of which target specific niches:\neHit, the first such niche company, came online in 2000 targeting fans of Asian films; specifically Japan, China, and Korea, expanding to include other countries’ films over time.\n\nEstimates put the number of Canadian subscribers at 70–80,000, with Zip.ca having had around 50,000 before ceasing operations. Other competitors include Kaku.ca and DVDlink.ca. Cinemail.ca announced it would cease operations at the end of June 2013.\n\nBlockbuster Online started DVD Rentals in Mexico during 2007, after the chain acquired a local startup called MovieNet.\n\nBlockbuster Online started DVD rentals in Brazil during 2006 and now offers Blu-ray plans as well. The 3-disc unlimited rental plan costs R$49.90/month with unlimited exchanges. Along the decade, the number of online rental services in Brazil has rocketed up. Among the most popular are NetMovies and Pipoca Online.\n\nGiven the relatively small geographical area and high population density of the UK, online DVD rentals have some differences from the US, as a single shipping facility can serve the entire country. In April 2006, LoveFilm merged with its major rival Video Island, which had operated ScreenSelect and other brands, and in February 2008, LoveFilm acquired Amazon's DVD rental business in the UK and German markets. In return, Amazon became the largest shareholder of LoveFilm. LoveFilm ceased operating on 31st October 2017. Cinema Paradiso is now the only remaining supplier of rental DVDs in the UK.\n\nThere are several providers in Australia, the most prominent being Quickflix (listed on the Australian Stock Exchange) and BigPond Movies. BigPond Movies announced in June 2011 that they will be pulling out of the DVD-by-mail market at the end of September 2011 and are, instead, offering subscribers the option of downloading movies directly via their proprietary T-box device.\n\nThere were three online DVD rental companies in New Zealand, all offering flat-rate packages. The three companies were DVD Unlimited, Fatso and Movieshack. On June 7, 2008 all three companies merged into Fatso, owned by SKY Network Television. Fatso ceased operations on 23 November 2017 due to declining membership.\n\nHollywoodclicks and Videohub are the two most established online DVD rental services in Singapore. Hollywoodclicks was the first to market, followed by Video Ezy Online. Video Ezy Online rental service was shut down at the start of 2009 and was converted to a home delivery service.\n\nThere are several online DVD rental services in India, all running their own delivery systems and logistics. Unlike online DVD rental companies in other countries, online DVD rental services in India do not use the postal service as a means of delivery or exchange. India's first online DVD rental service Clixflix started in August 2004. Cinesprite, Seventymm and Reliance BigFlix have closed their operations. Clixflix (the oldest) is still in operation in Mumbai.\n\nMajor online rental Blu-ray Disc and DVD companies are Rakuten Rental, Tsutaya Discas, and Posren.\n\n"}
{"id": "37370802", "url": "https://en.wikipedia.org/wiki?curid=37370802", "title": "EFood-ERP", "text": "EFood-ERP\n\neFood-ERP is an enterprise resource planning (ERP) software product based on the Microsoft Dynamics NAV platform.\n\nThe product is part of the Certified for Microsoft Dynamics family. eFood-ERP is engineered by TOP 100 VAR eSoftware developers.\n\n"}
{"id": "346082", "url": "https://en.wikipedia.org/wiki?curid=346082", "title": "Electrical connection", "text": "Electrical connection\n\nAn electrical connection between discrete points allows the flow of electrons (electric current). A pair of connections is needed for a circuit.\n\nBetween points with a low voltage difference, direct current can be controlled by a switch. However, if the points are not connected, and the voltage difference between those points is high enough, electrical ionization of the atmosphere will occur, and current will tend to occur along the path of least resistance.\n\n"}
{"id": "10384126", "url": "https://en.wikipedia.org/wiki?curid=10384126", "title": "Facade engineering", "text": "Facade engineering\n\nBuilding façades are one of the largest, most important elements in the overall aesthetic and technical performance of a building. Façade engineering is the art and science of resolving aesthetic, environmental and structural issues to achieve the effective enclosure of buildings.\n\nSpecialist companies are dedicated to this niche sector of the building industry and engineers operate within technical divisions of façade manufacturing companies. Generally, façade engineers are specifically qualified in the discipline of façade engineering and consultants work with the design team on construction projects for architects, building owners, construction managers and product manufacturers.\n\nFaçade engineers must consider aspects such as the design, certification, fabrication and installation of the building façades with regards to the performance of materials, aesthetic appearance, structural behaviour, weathertightness, safety and serviceability, security, maintenance and build ability. The skill set will include matters such as computational fluid dynamics, heat transfer through two- and three-dimensional constructions, the behaviour of materials, manufacturing methodologies, structural engineering and logistics.\n\nOver time, the specialist skills necessary in this niche sector have surpassed the capabilities of architects, structural and mechanical engineers as buildings are designed with more complexity and with the introduction of Building Information Modelling (BIM).\n\nBuilding façades are considered to be one of the most expensive and potentially the highest risk element of any major project. Historically building facades have the greatest level of failure of any part of a building fabric and the pressure for change and adaptation due to environmental and energy performance needs is greater than any other element of a building. As a consequence façade engineering has become a science in its own right.\n\nIn the United Kingdom, a professional body associated with the industry is the Society of Façade Engineering. Qualifications in façade engineering recognised by the Society of Façade Engineering and international professional qualifications include the MSc in façade engineering. This may be from the University of Bath; Technical University Delft or \"Detmolder Schule fur Architektur und Innenarchitekter Hochschule\" or other qualifications subject to review by the Membership panel.\n"}
{"id": "1276234", "url": "https://en.wikipedia.org/wiki?curid=1276234", "title": "Harmonized System", "text": "Harmonized System\n\nThe Harmonized Commodity Description and Coding System, also known as the Harmonized System (HS) of tariff nomenclature is an internationally standardized system of names and numbers to classify traded products. It came into effect in 1988 and has since been developed and maintained by the World Customs Organization (WCO) (formerly the Customs Co-operation Council), an independent intergovernmental organization based in Brussels, Belgium, with over 200 member countries.\n\nThe HS is organized logically by economic activity or component material. For example, animals and animal products are found in one section of the HS, while machinery and mechanical appliances are found in another. The HS is organized into 21 sections, which are subdivided into 97 chapters. The 97 HS chapters are further subdivided into approximately 5,000 headings and subheadings.\n\nSection and Chapter titles describe broad categories of goods, while headings and subheadings describe products in more detail. Generally, HS sections and chapters are arranged in order of a product’s degree of manufacture or in terms of its technological complexity. Natural commodities, such as live animals and vegetables, for example, are described in the early sections of the HS, whereas more evolved goods such as machinery and precision instruments are described in later sections. Chapters within the individual sections are also usually organized in order of complexity or degree of manufacture. For example, within Section X (\"Pulp of wood or of other fibrous cellulosic material; Recovered (waste and scrap) paper or paperboard; Paper and paperboard and articles thereof\"), Chapter 47 provides for \"pulp of wood or of other fibrous cellulosic materials\", whereas Chapter 49 covers \"printed books, newspapers, and other printed matter\". Finally, the headings within individual Chapters follow a similar order. For example, the first heading in Chapter 50 (\"Silk\") provides for \"silk worm cocoons\" while articles made of silk are covered by the chapter's later headings.\n\nThe HS code consists of 6-digits. The first two digits designate the HS Chapter. The second two digits designate the HS heading. The third two digits designate the HS subheading. HS code 1006.30, for example indicates Chapter 10 (\"Cereals\"), Heading 06 (\"Rice\"), and Subheading 30 (\"Semi-milled or wholly milled rice, whether or not polished or glazed\").\n\nIn addition to the HS codes and commodity descriptions, each Section and Chapter of the HS is prefaced by Legal Notes, which are designed to clarify the proper classification of goods.\n\nTo ensure harmonization, the contracting parties to the Convention on the Harmonized Commodity Description and Coding System, have agreed to base their national tariff schedules on the HS nomenclature and Legal Notes. Parties are permitted to subdivide the HS nomenclature beyond 6-digits and add their own Legal Notes according to their own tariff and statistical requirements. Parties often set their customs duties at the 8-digit \"tariff code\" level. Statistical suffixes are often added to the 8-digit tariff code for a total of 10 digits.\n\nHS Chapter 77 is reserved for common use by the parties internationally. Chapters 98 and 99 are reserved for national use. Chapter 98 comprises special classification provisions, and chapter 99 contains temporary modifications pursuant to a parties' national directive or legislation.\n\nSince its creation, the HS has undergone several revisions - ostensibly, to either eliminate headings and subheadings describing commodities that are no longer traded, or to create headings and subheadings that address technological advancements and environmental concerns. The current version of the HS became effective on January 1, 2017.\n\nThe process of assigning HS codes is known as \"HS Classification\". All products can be classified in the HS by using the \"General Rules for the Interpretation of the Harmonized System\" (\"GRI\"). HS codes can be determined by a variety of factors including a product's composition, its form and its function. An example of a product classified according to its form would be whole potatoes. The classification will also change depending on whether the potatoes are fresh or frozen. Fresh potatoes are classified in position 0701.90, under the Header \"Potatoes, fresh or chilled\", Sub header \"Other\", while frozen potatoes are classified in position 0710.10 under the Header \"Vegetables (uncooked or cooked by steaming or boiling in water), frozen\", Subheader \"Potatoes\".\n\nAn example of a product classified according its material composition is a picture frame. Picture frames made of wood are classified under subheading 4414.00, which provides for \"Wooden frames for paintings, photographs, mirrors or similar objects\". Picture frames made of plastic are classified under subheading 3924.90, which provides for \"Tableware, kitchenware, other household articles and hygienic or toilet articles, of plastics. Other\". Picture frames made of glass are classified under subheading 7020.00, which provides for \"Other articles of glass\". And so on.\n\nAn example of a product classified according to its form is personal hygiene soap. When in the form of a bar, cake or moulded shape, such soap is classified under subheading 3401.11, which provides for \"Soap and organic surface-active products and preparations, in the form of bars, cakes, moulded pieces or shapes, and paper, wadding, felt and nonwovens, impregnated, coated or covered with soap or detergent: For toilet use (including medicated products)\". Conversely, liquid personal hygiene soap is classified under either 3401.20, which provides for \"Soap in other forms\", or 3401.30, which provides for \"Organic surface-active products and preparations for washing the skin, in the form of liquid or cream and put up for retail sale, whether or not containing soap\".\n\nAn example of a product classified according to its function is a carbon monoxide (CO) detector. If the CO detector captures and displays gas measurements, then it is properly classified under subheading 9027.10, which provides for \"Instruments and apparatus for physical or chemical analysis (for example, polarimeters, refractometers, spectrometers, gas or smoke analysis apparatus; instruments and apparatus for measuring or checking viscosity, porosity, expansion, surface tension or the like; instruments and apparatus for measuring or checking quantities of heat, sound or light (including exposure meters); microtomes. Gas or smoke analysis apparatus\". If the CO detector does not capture and display gas measurements, then it is properly classified under subheading 8531.10, which provides for \"Electric sound or visual signaling apparatus (for example, bells, sirens, indicator panels, burglar or fire alarms), other than those of heading 85.12 or 85.30. Burglar or fire alarms and similar apparatus\".\n\nAlthough every product and every part of every product is classifiable in the HS, very few are explicitly described in the HS nomenclature. Any product for which there is no explicit description can be classified under a \"residual\" or \"basket\" heading or subheading, which provide for \"Other\" goods. Residual codes normally occur last in numerical order under their related headings and subheadings.\n\nAn example of a product classified under a residual heading is a live dog, which must be classified under heading 01.06, which provides for \"Other live animals\" because dogs are not covered by headings 01.01 through 01.05, which explicitly provide for \"live equine\", \"live bovine\", \"live swine\", \"live sheep and goats\", and \"live poultry\", respectively.\n\nAs of 2015, there were 180 countries or territories applying the Harmonized System worldwide,\n\nHS code are used by Customs authorities, statistical agencies, and other government regulatory bodies, to monitor and control the import and export of commodities through:\n\nCompanies use HS codes to calculate the total landed cost of imported products and parts, and to identify selling and sourcing opportunities abroad.\n\nHS classification is not always straightforward. Many automotive parts, for example, are not classified under heading 87.08, which provides for \"Parts and accessories of the motor vehicles of headings 87.01 to 87.05\". Automotive seats are classified as articles of furniture under heading 94.01, which provides for \"Seats (other than those of heading 94.02), whether or not convertible into beds, and parts thereof\", and more specifically under subheading 9401.20, which provides for \"Seats of a kind used for motor vehicles\".\n\nIn many jurisdictions, traders alone bear the legal responsibility to accurately classify their goods. Depending on the severity of the infraction, incorrect classification can result in the imposition of non-compliance penalties, border delays or seizures, or denial of import privileges.\n\nThere are several resources available to traders to assist in properly classifying their goods including:\n\n\n\n"}
{"id": "24377715", "url": "https://en.wikipedia.org/wiki?curid=24377715", "title": "Headless computer", "text": "Headless computer\n\nA headless system is a computer system or device that has been configured to operate without a monitor (the missing \"head\"), keyboard, and mouse. A headless system is typically controlled over a network connection, although some headless system devices require a serial connection to be made over RS-232 for administration of the device. Headless operation of a server is typically employed to reduce operating costs.\n\nDuring bootup, some (especially older) PC BIOS versions will wait indefinitely for a user to press a key before proceeding. If some basic device, such as a video card or keyboard, are not installed or connected, this could effectively halt an unattended system.\n\nOn more modern systems, the BIOS factory setting will typically be configured to behave this way as well, but this setting can be changed with a BIOS setup utility to proceed without user intervention.\n\nEven in cases where a system has been set up to be managed remotely, a local keyboard and video card may still be needed from time to time. For example, diagnose boot problems that occur before a remote access application is initialized.\n\nSome servers provide for remote control with an internal network card and hardware that mirrors the console screen. For example, HP offers a system called Integrated Lights-Out (iLO) that provides this function. Remote access to the system is gained using a secure web connection to an IP address assigned to the iLO adapter, and allows for monitoring of the system during start-up, before the operating system is loaded.\n\nAnother hardware solution is to use a KVM-over-IP switch. Such a switch is a traditional Keyboard-Video-Mouse device with the added ability to provide remote control sessions over IP. Connection to the KVM device is gained using a web browser, which allows for remote monitoring of the connected system console port.\n\nAdministration of a headless system typically takes place with a text-based interface such as a command line in Unix or in GNU/Linux. These interfaces, often called \"virtual terminals\" or \"terminal emulators\", attempt to simulate the behavior of \"real\" interface terminals like the Digital Equipment Corporation's VT100, but over networks, usually using protocols such as Secure Shell.\n\nOne can also use systems such as X Window System and VNC combined with virtual display drivers - this setup allows remote connections to headless machines through ordinary graphical user interfaces, often running over network protocols like TCP/IP.\n\n"}
{"id": "10952377", "url": "https://en.wikipedia.org/wiki?curid=10952377", "title": "Material-handling equipment", "text": "Material-handling equipment\n\nMaterial handling equipment is mechanical equipment used for the movement, storage, control and protection of materials, goods and products throughout the process of manufacturing, distribution, consumption and disposal. The different types of handling equipment can be classified into four major categories: transport equipment, positioning equipment, unit load formation equipment, and storage equipment.\nTransport equipment is used to move material from one location to another (e.g., between workplaces, between a loading dock and a storage area, etc.), while positioning equipment is used to manipulate material at a single location. The major subcategories of transport equipment are conveyors, cranes, and industrial trucks. Material can also be transported manually using no equipment.\n\nConveyors are used when material is to be moved frequently between specific points over a fixed path and when there is a sufficient flow volume to justify the fixed conveyor investment. Different types of conveyors can be characterized by the type of product being handled: \"unit load\" or \"bulk load\"; the conveyor’s location: \"in-floor\", \"on-floor\", or \"overhead\", and whether or not loads can \"accumulate \"on the conveyor. Accumulation allows intermittent movement of each unit of material transported along the conveyor, while all units move simultaneously on conveyors without accumulation capability. For example, while both the roller and flat-belt are unit-load on-floor conveyors, the roller provides accumulation capability while the flat-belt does not; similarly, both the power-and-free and trolley are unit-load overhead conveyors, with the power-and-free designed to include an extra track in order to provide the accumulation capability lacking in the trolley conveyor. Examples of bulk-handling conveyors include the magnetic-belt, troughed-belt, bucket, and screw conveyors. A sortation conveyor system is used for merging, identifying, inducting, and separating products to be conveyed to specific destinations, and typically consists of flat-belt, roller, and chute conveyor segments together with various moveable arms and/or pop-up wheels and chains that deflect, push, or pull products to different destinations.\n\nCranes are used to transport loads over variable (horizontal and vertical) paths within a restricted area and when there is insufficient (or intermittent) flow volume such that the use of a conveyor cannot be justified. Cranes provide more flexibility in movement than conveyors because the loads handled can be more varied with respect to their shape and weight. Cranes provide less flexibility in movement than industrial trucks because they only can operate within a restricted area, though some can operate on a portable base. Most cranes utilize trolley-and-tracks for horizontal movement and hoists for vertical movement, although manipulators can be used if precise positioning of the load is required. The most common cranes include the jib, bridge, gantry, and stacker cranes.\n\nIndustrial trucks are trucks that are not licensed to travel on public roads (\"commercial trucks\" are licensed to travel on public roads). Industrial trucks are used to move materials over variable paths and when there is insufficient (or intermittent) flow volume such that the use of a conveyor cannot be justified. They provide more flexibility in movement than conveyors and cranes because there are no restrictions on the area covered, and they provide vertical movement if the truck has lifting capabilities. Different types of industrial trucks can be characterized by whether or not they have forks for \"handling pallets\", provide \"powered \"or require \"manual \"lifting and travel capabilities, allow the operator to \"ride \"on the truck or require that the operator \"walk \"with the truck during travel, provide load \"stacking\" capability, and whether or not they can operate in \"narrow aisles\".\n\nHand trucks (including carts and dollies), the simplest type of industrial truck, cannot transport or stack pallets, is non-powered, and requires the operator to walk. A pallet jack, which cannot stack a pallet, uses front wheels mounted inside the end of forks that extend to the floor as the pallet is only lifted enough to clear the floor for subsequent travel. A counterbalanced lift truck (sometimes referred to as a forklift truck, but other attachments besides forks can be used) can transport and stack pallets and allows the operator to ride on the truck. The weight of the vehicle (and operator) behind the front wheels of truck counterbalances weight of the load (and weight of vehicle beyond front wheels); the front wheels act as a fulcrum or pivot point. Narrow-aisle trucks usually require that the operator stand-up while riding in order to reduce the truck’s turning radius. Reach mechanisms and outrigger arms that straddle and support a load can be used in addition to the just the counterbalance of the truck. On a turret truck, the forks rotate during stacking, eliminating the need for the truck itself to turn in narrow aisles. An order picker allows the operator to be lifted with the load to allow for less-than-pallet-load picking. Automated guided vehicles (AGVs) are industrial trucks that can transport loads without requiring a human operator.\n\nPositioning equipment is used to handle material at a single location. It can be used at a workplace to feed, orient, load/unload, or otherwise manipulate materials so that are in the correct position for subsequent handling, machining, transport, or storage. As compared to manual handling, the use of positioning equipment can raise the productivity of each worker when the frequency of handling is high, improve product quality and limit damage to materials and equipment when the item handled is heavy or awkward to hold and damage is likely through human error or inattention, and can reduce fatigue and injuries when the environment is hazardous or inaccessible. In many cases, positioning equipment is required for and can be justified by the ergonomic requirements of a task. Examples of positioning equipment include lift/tilt/turn tables, hoists, balancers, manipulators, and industrial robots. Manipulators act as “muscle multipliers” by counterbalancing the weight of a load so that an operator lifts only a small portion (1%) of the load’s weight, and they fill the gap between hoists and industrial robots: they can be used for a wider range of positioning tasks than hoists and are more flexible than industrial robots due to their use of manual control. They can be powered manually, electrically, or pneumatically, and a manipulator’s end-effector can be equipped with mechanical grippers, vacuum grippers, electromechanical grippers, or other tooling.\n\nUnit load formation equipment is used to restrict materials so that they maintain their integrity when handled a single load during transport and for storage. If materials are self-restraining (e.g., a single part or interlocking parts), then they can be formed into a unit load with no equipment. Examples of unit load formation equipment include pallets, skids, slipsheets, tote pans, bins/baskets, cartons, bags, and crates. A pallet is a platform made of wood (the most common), paper, plastic, rubber, or metal with enough clearance beneath its top surface (or face) to enable the insertion of forks for subsequent lifting purposes. A slipsheet is a thick piece of paper, corrugated fiber, or plastic upon which a load is placed and has tabs that can be grabbed by special push/pull lift truck attachments. They are used in place of a pallet to reduce weight and volume, but loading/unloading is slower.\n\nStorage equipment is used for holding or buffering materials over a period of time. The design of each type of storage equipment, along with its use in warehouse design, represents a trade-off between minimizing handling costs, by making material easily accessible, and maximizing the utilization of space (or cube). If materials are stacked directly on the floor, then no storage equipment is required, but, on average, each different item in storage will have a stack only half full; to increase cube utilization, storage racks can be used to allow multiple stacks of different items to occupy the same floor space at different levels. The use of racks becomes preferable to floor storage as the number of units per item requiring storage decreases. Similarly, the depth at which units of an item are stored affects cube utilization in proportion to the number of units per item requiring storage.\n\nPallets can be stored using single- and double-deep racks when the number of units per item is small, while pallet-flow and push-back racks are used when the units per item are mid-range, and floor-storage or drive-in racks are used when the number of units per item is large, with drive-in providing support for pallet loads that cannot be stacked on top of each other. Individual cartons can either be picked from pallet loads or can be stored in carton-flow racks, which are designed to allow first-in, first-out (FIFO) carton access. For individual piece storage, bin shelving, storage drawers, carousels, and A-frames can be used. An automatic storage/retrieval system (AS/RS) is an integrated computer-controlled storage system that combines storage medium, transport mechanism, and controls with various levels of automation for fast and accurate random storage of products and materials.\n\n\n\n"}
{"id": "43099588", "url": "https://en.wikipedia.org/wiki?curid=43099588", "title": "Mobile workspace", "text": "Mobile workspace\n\nA mobile workspace is a user’s portable working environment that gives them access to the applications, files and services they need to do their job no matter where they are.\n\nMobile workspace technology describes a set of software and services that deliver corporate apps, files and services to a user on any device and over any network. This technology was designed for business users that require access to all of their content on both corporate and personally-owned devices, including PCs, smartphones and tablets. Mobile workspace technologies are formed by bringing together a set of software and services including desktop virtualization, application virtualization, enterprise mobility management, file sharing, virtual private networks and more (see the full list below).\n\nAccording to recent research, combined shipments of devices including PCs, tablets, ultramobiles and mobile phones are projected to reach 2.5 billion units in 2014, a 7.6 percent increase from 2013. This trend, which is expected to continue to grow, is being driven by users who utilize more than one device. In fact, today the average user has 3+ different devices that they use for work purposes on a daily basis. These mobile devices entering the enterprise has led to over 60% of information workers working outside of a traditional office. While the shift to mobility seems to be growing, it is causing problems for both the end user and IT department. End users don’t feel they are equipped to work outside of the office and IT is forced to manage the security risks presented by data and applications leaving the corporate network.\n\nTo address these challenges, organizations are looking to mobile workspace technology that can provide users access to their corporate applications, files and services while maintaining security for their IT department. Mark Bowker, senior analyst, Enterprise Strategy Group said, \"Mobile workspaces are playing a key role in addressing the new organizational imperative around secure mobility\".\n\nApplications \nMicrosoft (particularly Microsoft Office) have been the dominant office software suite for enterprise along with numerous other Windows applications. Newer to the market is the growth of Software-as-a-Service and web-based applications such as Salesforce.com, LinkedIn and Evernote. Additionally, mobile applications have seen growth of 200% from 2011 to 2012, a trend that is expected to continue into 2013. A mobile workspace includes access to all types of applications (Windows, mobile, web, SaaS, and HTML5-based) from the user device.\n\nData\nFiles access is important for business productivity as well as business collaboration. A user’s mobile workspace must give a user access to all of their corporate data and allow for this data to be accessible on multiple devices. This requirement for file sync and sharing has led to the growth of many online sharing options including Citrix Sharefile, Dropbox, Google Drive, Box, and iCloud.\n\nCollaboration \nDue to factors like globalization, telecommuting and employees working outside of traditional offices, collaboration tools are becoming increasingly important. Collaboration tools include electronic communication, online web-conferencing, file sharing and social collaboration software.\n\nNetwork access\nPeople now work from all locations – the office, a coffee shop, the airport, their homes – yet still require access to their content. When outside the office, people still need access to resources behind the firewall generally requiring VPN access. A mobile workspace allows secure access to apps, data and services no matter where the user is or what type of network they are using (LAN, WAN, 3G/4G, etc.).\n\nCloud\nAs cloud computing becomes more popular, IT organizations are looking to the cloud to host more services than ever before. IT departments now can choose where to host their apps, data, desktops, etc. in the most appropriate location whether that’s on-premises in a datacenter or a private, public or hybrid cloud. Users need access to these resources from their mobile workspace.\n\nAny device\nResearch suggests that mobile devices are set to outnumber the total number people on earth by 2014. As device types and manufacturers continue to grow, users still need to access their mobile workspace irrespective of device type, operating system or manufacturer.\n\n"}
{"id": "3607262", "url": "https://en.wikipedia.org/wiki?curid=3607262", "title": "Molecular Biology Core Facilities", "text": "Molecular Biology Core Facilities\n\nThis is an example of a molecular biology core developed in an academic institution over the past 35 years. These molecular biology cores are now commonplace and necessary in that they provide NIH funded academic labs access to expensive instrumentation in a shared use setting. \n\nThe Molecular Biology Core Facilities (MBCF) was created to allow investigators at the Dana-Farber Cancer Institute (DFCI) access to cutting edge molecular biology tools which would be tested and developed in a shared setting. Collaborations can be set up with anyone in the world. Although these services are primarily focused on Cancer and AIDS research, there is a broad spectrum of research that uses these resources.\n\nThe MBCF at DFCI was first started in 1984 to supply small oligonucleotides to researchers. Marvin H. Caruthers at the University of Colorado determined how to synthesize small oligonucleotides by solid-phase synthesis using the phosphoramidite method. Caruthers and Leroy Hood at the California Institute of Technology constructed the first automated DNA Synthesizer which the MBCF successfully tested after discussions with Don Taylor.\n\nBecause of the growing demand for oligonucleotide primers to initiate DNA replication and for probes, a plan was put into place to develop a core facility to produce reagents for molecular biologists as well as instrumentation for the analysis of DNA and protein samples. This plan stated that a charge-back method would be put in place to fairly spread the resources as a shared facility.\n\nA Peptide Synthesizer using Bruce Merrifield's solid phase peptide synthesis was brought online in 1988 when Fmoc chemistry became commercially available. A Protein Sequencer using Edman degradation was installed in 1989 quickly followed by several DNA Sequencers which were the first to use fluorescent dye terminator chemistry. Mass Spectrometers were acquired to provide analysis of synthesized peptides but soon grew into a stand-alone service in high demand. Biacore instrumentation added for ligand kinetics in 1996 (and updated by Shared Instrumentation Grant to a Biacore 3000 in 2004). In 2007 a large expansion of high throughput proteomics using mass spectrometry has been funded by private donation. Blais Proteomics Center. (BPC)\n\nThe Molecular Biology Core Facilities has had a continuous web presence since 1992 and began receiving primer sequence for high throughput DNA synthesis by web forms in 1995 when new high volume DNA synthesizers were brought on-line. Since 2004 all interactions between researchers and the core services have been moved to a web-based LIMS system.\n\nThe MBCF is constantly evaluating new instrumentation which could eventually become a shared resource. The future massively parallel DNA sequencers which can sequence entire genomes in just a few days, (454, Solexa+Illumina, Helicos, SOLiD, Visigen, PacBio, ION Torrent, Nanopore), are being made available to researchers by various methods. In 2009 the Heliscope from Helicos BioSciences began being evaluated in the genomics facility. After successfully completing 2,365 Chip-Seq samples the Helicos service was shut down in 2011. An Illumina MiSeq was evaluated on multiple projects and now a series of three MiSeqs have been streamlined for fast analysis of Next Gen samples. Library prep has been automated on an IntegenX Apollo 324 and Beckman robots and several instruments have been incorporated into quality control of samples including the Pippin Prep by Sage Biosciences, an Agilent Tapestation Bioanalyzer and an Echo QPCR system from Illumina. The Mass Spectrometry proteomics facility has also undergone a large expansion to undertake more complex high-throughput projects as part of the Blais Proteomics Center.\n\nRecent expansion of genomics includes three, now four Illumina NextSeq 500s as well as an Illumina Neoprep Library prep robot (which was returned as a rebate on another NextSeq500). The current platform for genomics in 2017 changes faster than a wiki page. MBCF can send you sample into a pipeline that is small and proof of concept or large production runs. We can run with Illumina, Pacbio, or Oxford Nanopore instruments. But that was yesterday. The big deal is library prep and amplification. We are a small production NGS lab that has done a thousand different types of NGS. We know how to prep a sample. If you want whole genome of human or chicken, go to BGI in China. They are much cheaper. That is where we send our full genome samples.\n\n\nThe MBCF is for the most part funded as a straight fee for service chargeback core facility. At times other sources of funding for research and development or expansion are incorporated into the overall budget.\n"}
{"id": "22600680", "url": "https://en.wikipedia.org/wiki?curid=22600680", "title": "Morse Diving", "text": "Morse Diving\n\nMorse Diving is an American manufacturer of diving equipment. It was founded in 1837 as Morse & Fletcher in Boston MA. The name was changed in 1864 to A J Morse and Son and it remained under that name until 1905 when the company was incorporated and Inc. was added to the name. In 1940 the company was purchased and the name was changed to Morse Diving Equipment Company Incorporated and later moved its operations to Rockland, MA and continued under that name until 1998 when it was purchased by Kenneth Downey, an employee, and did business under the name of Morse Diving Inc. Downey sold the company in 2014 to Watson \"Robbie\" Holland, and the name changed, yet again, to Morse International. Morse filed for bankruptcy and Diving Equipment and Supply Company (DESCO) acquired its assets in 2016. DESCO reverted to the name A J Morse & Son and Morse products will be marketed under that name. DESCO's business plan is to bring back the quality and products associated with the earlier name. DESCO has on re-introduced the breast plate feed (air being fed into the breast plate rather than the bonnet)helmet design from the early 1900s as its first offering. They also make the standard commercial model with the air feed in the rear of the helmet. The A J Morse & Son US Navy Mark V helmet is also offered. \n\nMorse Diving is the oldest manufacturer of diving equipment in the world and the 412th oldest officially recorded company ever, sharing its founding year (1837) with Tiffany and Co.\n\nMorse Diving Equipment Company was one of the primary manufacturers of the famous United States Navy Mark V diving helmet; during World War II, other manufacturers of this helmet were Schrader, DESCO (Diving Equipment and Salvage Company), and Miller-Dunn. Morse also manufactured the next generation Mark 12 free flow diving helmet which was used by the US Navy for almost 20 years.\n\nMorse Diving was based in Rockland, Massachusetts as a prime supplier and distributor of commercial diving equipment, scuba gear, and other diving-related items to commercial and government users.\n\nDESCO also continues to use their original DESCO equipment to produce Mark V helmets under their name as well.\n\n"}
{"id": "12437142", "url": "https://en.wikipedia.org/wiki?curid=12437142", "title": "NEC Green Wheels", "text": "NEC Green Wheels\n\nGreen Wheels is a project of the Northcoast Environmental Center (NEC) in Arcata, California. Green Wheels is a sustainable transportation advocacy group that promotes the benefits of alternatives to the single-occupancy vehicle and encourages and pressures governments, businesses, individuals and institutions to make choices that reduce transportation impacts and create balanced and sustainable transportation.\n\nGreen Wheels began as a student club of Humboldt State University in 2003 when it was called the Alternative Transportation Club. In 2005, the club's name was changed to Green Wheels. Green Wheels joined the NEC in July 2007. A chapter remains as a campus club at HSU.\n\n\n"}
{"id": "5235541", "url": "https://en.wikipedia.org/wiki?curid=5235541", "title": "New Zealand Dairy Workers Union", "text": "New Zealand Dairy Workers Union\n\nThe New Zealand Dairy Workers Union (NZDWU) is a national trade union in New Zealand. It represents 7,000 workers active in dairy factories, town milk supply, processing plants, stores and warehousing, packing, can-making, and other ancillary activities including drivers.\n\nThe NZDWU is affiliated with the New Zealand Council of Trade Unions, the IUF and the New Zealand Labour Party.\n\n"}
{"id": "1915691", "url": "https://en.wikipedia.org/wiki?curid=1915691", "title": "Off-the-Record Messaging", "text": "Off-the-Record Messaging\n\nOff-the-Record Messaging (OTR) is a cryptographic protocol that provides encryption for instant messaging conversations. OTR uses a combination of AES symmetric-key algorithm with 128 bits key length, the Diffie–Hellman key exchange with 1536 bits group size, and the SHA-1 hash function. In addition to authentication and encryption, OTR provides forward secrecy and malleable encryption.\n\nThe primary motivation behind the protocol was providing deniable authentication for the conversation participants while keeping conversations confidential, like a private conversation in real life, or off the record in journalism sourcing. This is in contrast with cryptography tools that produce output which can be later used as a verifiable record of the communication event and the identities of the participants. The initial introductory paper was named \"Off-the-Record Communication, or, Why Not To Use PGP\".\n\nThe OTR protocol was designed by cryptographers Ian Goldberg and Nikita Borisov and released on 26 October 2004. They provide a client library to facilitate support for instant messaging client developers who want to implement the protocol. A Pidgin and Kopete plugin exists that allows OTR to be used over any IM protocol supported by Pidgin or Kopete, offering an auto-detection feature that starts the OTR session with the buddies that have it enabled, without interfering with regular, unencrypted conversations. Version 4 of the protocol is currently been designed by a team lead by Sofía Celi and Ola Bini, and reviewed by Nik Unger and Ian Goldberg. This version aims to provide online and offline deniability, to update the cryptographic primitives, and to support out-of-order delivery and asynchronous communication. \nOTR was presented in 2004 by Nikita Borisov, Ian Avrum Goldberg, and Eric A. Brewer as an improvement over the OpenPGP and the S/MIME system at the \"Workshop on Privacy in the Electronic Society\" (WPES). The first version 0.8.0 of the reference implementation was published on November 21, 2004. In 2005 an analysis was presented by Mario Di Raimondo, Rosario Gennaro, and Hugo Krawczyk that called attention to several vulnerabilities and proposed appropriate fixes, most notably including a flaw in the key exchange. As a result, version 2 of the OTR protocol was published in 2005 which implements a variation of the proposed modification that additionally hides the public keys. Moreover, the possibility to fragment OTR messages was introduced in order to deal with chat systems that have a limited message size, and a simpler method of verification against man-in-the-middle attacks was implemented.\n\nIn 2007 Olivier Goffart published mod_otr for ejabberd, making it possible to perform man-in-the-middle attacks on OTR users who don't check key fingerprints. OTR developers countered this attack by introducing socialist millionaire protocol implementation in libotr. Instead of comparing key checksums, knowledge of an arbitrary shared secret can be utilised for which relatively low entropy can be tolerated by using the socialist millionaire protocol.\n\nVersion 3 of the protocol was published in 2012. As a measure against the repeated reestablishment of a session in case of several competing chat clients being signed on to the same user address at the same time, more precise identification labels for sending and receiving client instances were introduced in version 3. Moreover, an additional key is negotiated which can be used for another data channel.\n\nSeveral solutions have been proposed for supporting conversations with multiple participants. A method proposed in 2007 by Jiang Bian, Remzi Seker, and Umit Topaloglu uses the system of one participant as a \"virtual server\". The method called \"Multi-party Off-the-Record Messaging\" (mpOTR) which was published in 2009 works without a central management host and was introduced in Cryptocat by Ian Goldberg et al.\n\nIn 2013, the Signal Protocol was introduced, which is based on OTR Messaging and the Silent Circle Instant Messaging Protocol (SCIMP). It brought about support for asynchronous communication (\"offline messages\") as its major new feature, as well as better resilience with distorted order of messages and simpler support for conversations with multiple participants. OMEMO, introduced in an Android XMPP client called Conversations in 2015, integrates the Double Ratchet Algorithm used in Signal into the instant messaging protocol XMPP (\"Jabber\") and also enables encryption of file transfers. In the autumn of 2015 it was submitted to the XMPP Standards Foundation for standardisation.\n\nCurrently, version 4 of the protocol is been designed. It was presented by Sofía Celi and Ola Bini on PETS2018. \n\nIn addition to providing encryption and authentication — features also provided by typical public-key cryptography suites, such as PGP, GnuPG, and X.509 (S/MIME) — OTR also offers some less common features:\n\n\nAs of OTR 3.1, the protocol supports mutual authentication of users using a shared secret through the socialist millionaire protocol. This feature makes it possible for users to verify the identity of the remote party and avoid a man-in-the-middle attack without the inconvenience of manually comparing public key fingerprints through an outside channel.\n\nDue to limitations of the protocol, OTR does not support multi-user group chat but it may be implemented in the future. As of version 3 of the protocol specification, an extra symmetric key is derived during authenticated key exchanges that can be used for secure communication (e.g., encrypted file transfers) over a different channel. Support for encrypted audio or video is not planned. (SRTP with ZRTP exists for that purpose.) A project to produce a protocol for multi-party off-the-record messaging (mpOTR) has been organized by Cryptocat, eQualitie, and other contributors including Ian Goldberg.\n\nSince OTR protocol v3 (libotr 4.0.0) the plugin supports multiple OTR conversations with the same buddy who is logged in at multiple locations.\n\nThese clients support Off-the-Record Messaging out of the box.\n\nThe following clients require a plug-in to use Off-the-Record Messaging.\n\nAlthough Gmail's Google Talk uses the term \"off the record\", the feature has no connection to the Off-the-Record Messaging protocol described in this article, its chats are not encrypted in the way described above—and could be logged internally by Google even if not accessible by end-users.\n\n\n\n"}
{"id": "28242586", "url": "https://en.wikipedia.org/wiki?curid=28242586", "title": "Online Banking ePayments", "text": "Online Banking ePayments\n\nOnline Banking ePayments (OBeP) is a type of payments network, developed by the banking industry in conjunction with technology providers. It is specifically designed to address the unique requirements of payments made via the Internet.\n\nKey aspects of OBeP that distinguish it from other online payments systems are:\n\nNearly half of the bills paid in the US during 2013 were done via electronic bill payment. Also, during 2014, nearly 48% of all online shopping in North America were made with a credit card. Globally, online payments are expected to exceed 3 trillion Euros (approx. US$3.2 trillion) in the next 5 years.\n\nOBeP systems protect consumer personal information by not requiring the disclosure of account numbers or other sensitive personal data to online merchants or other third parties. During the checkout process, the merchant redirects the consumer to their financial institution’s online banking site where they login and authorize charges. After charges are authorized, the financial institution redirects the consumer back to the merchant site. All network communications are protected using industry standard encryption. Additionally, communications with the OBeP network take place on a virtual private network, not over the public Internet.\n\nIn order to be positive that your identity, information and other personal features are truly secure, the following cautions should be taken: \nRead all privacy policies provided. Many individuals simply skip over such important information that could spell out potential risks. If a risk seems unnecessary and odd, it would be safer to skip this payment rather than take the risk with one's hard earned money.\nKeep all personal information private. If phone numbers, social security numbers or other private, important information is asked for one should be cautious. Banking information is important information as it is, asking for unnecessary personal information should be a red flag of suspicious behavior. \nSelecting businesses that are trustworthy is key. \nMost companies will email a customer with a transaction receipt upon payment. Keeping a record of these is important in order to have proof of purchase or payment. \nLastly, checking bank statements regularly is crucial in keeping up-to-date with transactions.\n\nCosts associated with fraud, estimated at 1.2% of sales by online retailers in 2009, are reported to be dramatically reduced with OBeP, because the issuer bank is responsible for the authentication of the credit transaction and provides guaranteed funds to the merchant.\n\nBecause the merchant is not responsible for storing and protecting confidential consumer information, OBeP systems also reduce costs associated with mitigating fraud, fraud screening, and PCI audits.\n\nTransaction fees on Online Banking ePayments vary by network, but are often fixed, and lower than the average 1.9% merchant fees associated with credit card transactions – especially for larger purchases.\n\n\n\n\nThe idea of online payments and transactions has led numerous individuals, corporations and groups to be hesitant. Sharing of personal information to such a vast entity, such as the internet, can lead to potential problems. Remaining cautious and careful with what information is shared and to whom it is shared with is key in remaining safe and secure when using ePayments.\n\n\n\n\n"}
{"id": "3228801", "url": "https://en.wikipedia.org/wiki?curid=3228801", "title": "Optoelectric nuclear battery", "text": "Optoelectric nuclear battery\n\nAn opto-electric nuclear battery is a device that converts nuclear energy into light, which it then uses to generate electrical energy. A beta-emitter such as technetium-99 or strontium-90 is suspended in a gas or liquid containing luminescent gas molecules of the excimer type, constituting a \"dust plasma.\" This permits a nearly lossless emission of beta electrons from the emitting dust particles. The electrons then excite the gases whose excimer line is selected for the conversion of the radioactivity into a surrounding photovoltaic layer such that a lightweight, low-pressure, high-efficiency battery can be realised. These nuclides are relatively low-cost radioactive waste from nuclear power reactors. The diameter of the dust particles is so small (a few micrometers) that the electrons from the beta decay leave the dust particles nearly without loss. The surrounding weakly ionized plasma consists of gases or gas mixtures (such as krypton, argon, and xenon) with excimer lines such that a considerable amount of the energy of the beta electrons is converted into this light. The surrounding walls contain photovoltaic layers with wide forbidden zones as e.g. diamond which convert the optical energy generated from the radiation into electrical energy.\n\nThe technology was developed by researchers of the Kurchatov Institute in Moscow.\n\nThe battery would consist of an excimer of argon, xenon, or krypton (or a mixture of two or three of them) in a pressure vessel with an internal mirrored surface, finely-ground radioisotope, and an intermittent ultrasonic stirrer, illuminating a photocell with a bandgap tuned for the excimer. \nWhen the beta active nuclides (e.g., krypton-85 or argon-39) emit beta particles, they excite their own electrons in the narrow excimer band at a minimum of thermal losses that this radiation is converted in a high band gap photovoltaic layer (e.g. in p-n diamond) very efficiently into electricity. The electric power per weight compared with existing radionuclide batteries can then be increased by a factor 10 to 50 and more. If the pressure-vessel is carbon fiber/epoxy the weight-to-power ratio is said to be comparable to an air-breathing engine with fuel tanks. The advantage of this design is that precision electrode assemblies are not needed, and most beta particles escape the finely-divided bulk material to contribute to the battery's net power.\n\nThe inherent risk of failure is likely to limit this device to space-based applications, where the finely divided radioisotope source is only removed from a safe transport medium, and placed in the high-pressure gas, after the device has left Earth orbit.\n\n\n"}
{"id": "14421547", "url": "https://en.wikipedia.org/wiki?curid=14421547", "title": "Orbit (control theory)", "text": "Orbit (control theory)\n\nThe notion of orbit of a control system used in mathematical control theory is a particular case of the notion of orbit in group theory.\nLet \nformula_1 \nbe a formula_2 control system, where \nformula_3 \nbelongs to a finite-dimensional manifold formula_4 and formula_5 belongs to a control set formula_6. Consider the family formula_7\nand assume that every vector field in formula_8 is complete.\nFor every formula_9 and every real formula_10, denote by formula_11 the flow of formula_12 at time formula_10.\n\nThe orbit of the control system formula_1 through a point formula_15 is the subset formula_16 of formula_4 defined by\n\nThe difference between orbits and attainable sets is that, whereas for attainable sets only forward-in-time motions are allowed, both forward and backward motions are permitted for orbits. \nIn particular, if the family formula_8 is symmetric (i.e., formula_9 if and only if formula_21), then orbits and attainable sets coincide.\n\nThe hypothesis that every vector field of formula_8 is complete simplifies the notations but can be dropped. In this case one has to replace flows of vector fields by local versions of them.\n\nEach orbit formula_16 is an immersed submanifold of formula_4.\n\nThe tangent space to the orbit\nformula_16 at a point formula_26 is the linear subspace of formula_27 spanned by \nthe vectors formula_28 where formula_29 denotes the pushforward of formula_12 by formula_31, formula_12 belongs to formula_8 and formula_31 is a diffeomorphism of formula_4 of the form formula_36 with formula_37 and formula_38.\n\nIf all the vector fields of the family formula_8 are analytic, then formula_40 where formula_41 is the evaluation at formula_26 of the Lie algebra generated by formula_8 with respect to the Lie bracket of vector fields.\nOtherwise, the inclusion formula_44 holds true.\n\nIf formula_45 for every formula_46 and if formula_4 is connected, then each orbit is equal to the whole manifold formula_4.\n"}
{"id": "652501", "url": "https://en.wikipedia.org/wiki?curid=652501", "title": "Outhouse", "text": "Outhouse\n\nAn outhouse, also known by many other names, is a small structure, separate from a main building, which covers a toilet. This is typically either a pit latrine or a bucket toilet, but other forms of dry (non-flushing) toilets may be encountered. The term may also be used to denote the toilet itself, not just the structure itself.\n\nOuthouses were in use in cities of developed countries (e.g. Australia) well into the second half of the twentieth century. They are still common in rural areas and also in cities of developing countries. Outhouses that are covering pit latrines in densely populated areas can cause groundwater pollution.\n\nIn some localities and varieties of English, particularly outside North America, the term \"outhouse\" refers \"not\" to a toilet, but to outbuildings in a general sense: sheds, barns, workshops, etc.\n\nOuthouses vary in design and construction. They are by definition outside the dwelling, and are not connected to plumbing, sewer, or septic system. The World Health Organization recommends they be built a reasonable distance from the house balancing issues of easy access versus that of smell.\n\nThe superstructure exists to shelter the user, and also to protect the toilet itself. The primary purpose of the building is for privacy and human comfort, and the walls and roof provide a visual screen and some protection from the elements. The outhouse also has the secondary role of protecting the toilet hole from sudden influxes of rainwater, which would flood the hole and flush untreated wastes into the underlying soils before they can decompose.\n\nOuthouses are commonly humble and utilitarian, made of lumber or plywood. This is especially so they can easily be moved when the earthen pit fills up. Depending on the size of the pit and the amount of use, this can be fairly frequent, sometimes yearly. As pundit \"Jackpine\" Bob Cary wrote: \"Anyone can build an outhouse, but not everyone can build a good outhouse.\" Floor plans typically are rectangular or square, but hexagonal outhouses have been built.\n\nThe arrangements inside the outhouse vary by culture. In Western societies, many, though not all, have at least one seat with a hole in it, above a small pit. Others, often in more rural, older areas in European countries, simply have a hole with two indents on either side for your feet. In Eastern societies, there is a hole in the floor, over which the user crouches. A roll of toilet paper is usually available. Old corn cobs, leaves, or other types of paper may instead be used.\n\nThe decoration on the outhouse door has no standard. The well-known crescent moon on American outhouses was popularized by cartoonists and had a questionable basis in fact. There are authors who claim the practice began during the colonial period as an early \"mens\"/\"ladies\" designation for an illiterate populace (the sun and moon being popular symbols for the sexes during those times). Others dismiss the claim as an urban legend. What is certain is that the purpose of the hole is for venting and light and there were a wide variety of shapes and placements employed.\n\nThe shelter may cover very different sorts of toilets. \n\nAn outhouse often provides the shelter for a pit latrine, which collects human feces in a hole in the ground. When properly built and maintained they can decrease the spread of disease by reducing the amount of human feces in the environment from open defecation. When the pit fills to the top, it should be either emptied or a new pit constructed and the shelter moved or re-built at the new location. The management of the fecal sludge removed from the pit is complicated. There are both environment and health risks if not done properly. As of 2013 pit latrines are used by an estimated 1.77 billion people. This is mostly in the developing world as well as in rural and wilderness areas.\n\nAnother system is the bucket toilet, consisting of a seat and a portable receptacle (bucket or pail). These may be emptied by their owners into composting piles in the garden (a low-tech composting toilet), or collected by contractors for larger-scale disposal. Historically, this was known as the pail closet; the municipality employed workers, often known as \"nightmen\" (from night soil), to empty and replace the buckets. This system was associated in particular with the English town of Rochdale, to the extent that it was described as the \"Rochdale System\" of sanitation. 20th century books report that similar systems were in operation in parts of France and elsewhere in continental Europe. \n\nThe system of municipal collection was widespread in Australia; \"dunny cans\" persisted well into the second half of the twentieth century, see below. In Scandinavia and some other countries, outhouses are built over removable containers that enable easy removal of the waste and enable much more rapid composting in separate piles. A similar system operates in India, where hundreds of thousands of workers engage in manual scavenging, i.e. emptying pit latrines and bucket toilets without any personal protective equipment.\n\nA variety of systems are used in some national parks and popular wilderness areas, to cope with the increased volume of people engaged in activities such as mountaineering and kayaking. The growing popularity of paddling, hiking, and climbing has created special waste disposal issues throughout the world. It is a dominant topic for outdoor organizations and their members. For example, in some places the human waste is collected in drums which need to be helicoptered in and out at considerable expense. \n\nAlternatively, some parks mandate a \"pack it in, pack it out\" rule. Many reports document the use of containers for the removal of excrement, which must be packed in and packed out on Mount Everest. Also known as \"expedition barrels\" or \"bog barrels\", the cans are weighed to make sure that groups do not dump them along the way. \"Toilet tents\" are erected. There has been an increasing awareness that the mountain needs to be kept clean, for the health of the climbers at least. \n\nWorm hold privies, another variant of the composting toilet, are being used by Vermont's Green Mountain Club. These simple outhouses are stocked with red worms (a staple used by home composters). Despite their environmental benefits, composting toilets are likewise subject to regulations.\n\nThe \"Clivus Multrum\" is another type of composting toilet which can be inside of an outhouse. \n\nThere are other types of toilet that may be covered by an outhouse superstructure, or a toilet tent (e.g. in humanitarian relief operations), or even be installed inside a house that is beyond the reach of sewers. The Swedish Pacto toilet uses a continuous roll of plastic to collect and dispose of waste. Incinerating toilets are installed in several thousand cabins in Norway. These toilets incinerate waste into ashes, using only propane and 12 volt electricity.\n\nOuthouse design, placement, and maintenance has long been recognized as being important to the public health. See posters created by the Works Progress Administration.\n\nSome types of flying insects such as the housefly are attracted to the odor of decaying material, and will use it for food for their offspring, laying eggs in the decaying material. Other insects such as mosquitoes seek out standing water that may be present in the pit for the breeding of their offspring.\n\nBoth of these are undesirable pests to humans, but can be easily controlled without chemicals by enclosing the top of the pit with tight-fitting boards or concrete, using a sufficiently sealed toilet hole cover that is closed after every use, and by using fine-grid insect screen to cover the inlet and outlet vent holes. This prevents flying insect entry by all potential routes.\n\nIt is common (at least in the United States) for outhouses to have a bucket or a bag of powdered lime with a scoop of some kind in it. Either before or after using the outhouse (usually after but sometimes both) a scoop or two of lime was sprinkled into the lid holes to cover the waste as to suppress the odor which also can help with the insect issues. This method of using powdered lime was also used (and for the same reasons) in common/mass graves.\n\nOne of the purposes of outhouses is to avoid spreading parasites such as intestinal worms, notably hookworms, which might otherwise be spread via open defecation.\n\n\nOld outhouse pits are seen as excellent places for archeological and anthropological excavations, offering up a trove of common objects from the pasta veritable inadvertent time capsulewhich yields historical insight into the lives of the bygone occupants. This is also called privy digging. It is especially common to find old bottles, which seemingly were secretly stashed or trashed, so their content could be privately imbibed. Fossilised feces (coprolites) yield much information about diet and health.\n\n\"Dunny\" or \"dunny can\" are Australian and New Zealand words for a toilet, particularly an outhouse. For other uses of the word, see Dunny (disambiguation).\n\nIn suburban areas not connected to the sewerage, outhouses were not always built over pits. Instead, these areas utilized a pail closet, where waste was collected into large cans positioned under the toilet seat, to be collected by contractors (or night soil collectors) hired by property owners or the local council. The used cans were replaced with empty, cleaned cans. Brisbane relied on \"dunny carts\" until the 1950s (one source says until the 1970s); because the population was so dispersed, it was difficult to install sewerage. Tar, creosote, and disinfectant kept the smell down. Academic George Seddon claimed that \"the typical Australian back yard in the cities and country towns\" had, throughout the first half of the twentieth century, \"a dunny against the back fence, so that the pan could be collected from the dunny lane through a trap-door\" The person who appeared weekly to empty the buckets beneath the seats was known as the \"dunnyman\", see gong farmer.\n\nThe \"dunny lanes\" provided access to collectors. These access lanes can now be worth considerable sums see Ransom strip.\n\nThe Great Australian Dunny Race has become an icon during the Weerama Festival at Werribee.\n\nThe remains of a thousand year old Viking outhouse were discovered in 2017. This is the oldest known outhouse in the country, even though evidence cannot establish it to be \"the first.\" This discovery was considered to be culturally significant.\n\nOuthouses are typically built on one level, but two-story models are to be found in unusual circumstances. One double-decker was built to serve a two-story building in Cedar Lake, Michigan. The outhouse was connected by walkways. It still stands (but not the building). The waste from \"upstairs\" is directed down a chute separate from the \"downstairs\" facility in these instances, so contrary to various jokes about two-story outhouses, the user of the lower level has nothing to fear if the upper level is in use at the same time. The Boston Exchange Coffee House (1809–1818) was equipped with a four-story outhouse with windows on each floor\n\nSome outhouses were built surprisingly ornately, considering the time and the place. For example, an opulent 19th century antebellum example (a three-holer) is at the plantation area at the state park in Stone Mountain, Georgia. The outhouses of Colonial Williamsburg varied widely, from simple expendable temporary wood structures to high-style brick. Thomas Jefferson designed and had built two brick octagons at his vacation home. Such outhouses are sometimes considered to be overbuilt, impractical and ostentatious, giving rise to the simile \"built like a brick shithouse.\" That phrase's meaning and application is subject to some debate; but (depending upon the country) it has been applied to men, women, or inanimate objects.\n\nWith regards to anal cleansing, old newspapers and mail order catalogs, such as those from Montgomery Ward or Sears Roebuck, were common before toilet paper was widely available. Paper was often kept in a can or other container to protect it from mice, etc. The catalogs served a dual purpose, also giving one something to read.\n\nOutdoor toilets are referred to by many terms throughout the English-speaking world. The term \"outhouse\" is used in North American English for the structure over a toilet, usually a pit latrine (\"long drop\"). However, in British English \"outhouse\" means any outbuilding, such as a shed or barn.\n\nIn Australia and New Zealand, an outdoor toilet is known as a \"dunny\". \"Privy\", an archaic variant of \"private\", is used in North America, Scotland, and northern England. \"Bog\" is common throughout Britain (used to coin the neologism \"tree bog\"). The name \"little house\" (as \"\") continues as a euphemism for any toilet in both the Welsh language and the Welsh English dialect. Other terms include \"back house\", \"house of ease\", and \"house of office\". The last was common in 17th-century England and appeared in Samuel Pepys's \"Diary\" on numerous occasions.\n\nIn the Scouting Movement in North America, a widespread term for outhouse is \"kybo\". This appears to have originated from camps which used Kybo brand coffee cans to hold lye or lime which was sprinkled down the hole to reduce odor. \"Keep Your Bowels Open\" may be a backronym. Temporary encampments may use a tent or tarpaulin over a shallow pit; one name for this is a \"hudo\".\n\nIt is not easy to determine whether a given term was restricted to an outdoor toilet, or whether the meaning had extended, over time and with the development of indoor plumbing, to any toilet. \n\nTsi-Ku, also known as Tsi Ku Niang, is described as the Chinese goddess of the outhouse and divination. It is said that a woman could uncover the future by going to the outhouse to ask Tsi-Ku. See toilet god.\n\nConstruction and maintenance of outhouses in the US is subject to state and local governmental restriction, regulation and prohibition. It is potentially both a public health issue, which has been addressed both by law and by education of the public as to good methods and practices (\"e.g.\", separation from drinking water sources). This also becomes a more prevalent issue as urban and suburban development encroaches on rural areas, and is an external manifestation of a deeper cultural conflict. \"See also\" urban sprawl, urban planning, regional planning, suburbanization, urbanization and counterurbanization.\n\n\n\n\n"}
{"id": "42954942", "url": "https://en.wikipedia.org/wiki?curid=42954942", "title": "Phillip Hallam-Baker", "text": "Phillip Hallam-Baker\n\nPhillip Hallam-Baker is a computer scientist, mostly renowned for his contributions to Internet security, since the design of HTTP at CERN in 1992. Currently vice-president and principal scientist at Comodo Inc., he previously worked at Verisign Inc., and at MIT Artificial Intelligence Laboratory. He is a frequent participant in IETF meetings and discussions, and has written a number of RFCs. In 2007 he authored \"the dotCrime Manifesto: How to Stop Internet Crime\"; although the book is readable by novices, Ron Rivest still considered it a source of ideas for his course on Computer and Network Security at MIT in 2013.\n\nHallam-Baker has a degree in electronic engineering from the School of Electronics and Computer Science, University of Southampton and a doctorate in Computer Science from the Nuclear Physics Department at Oxford University. He was appointed a Post Doctoral Research Associate at DESY in 1992 and CERN Fellow in 1993.\n\nHallam-Baker worked with the Clinton-Gore ’92 Internet campaign. While at the MIT Laboratory for Artificial Intelligence, he worked on developing a security plan and performed seminal work on securing high-profile federal government internet sites.\n\n\n"}
{"id": "1117328", "url": "https://en.wikipedia.org/wiki?curid=1117328", "title": "Pickaxe", "text": "Pickaxe\n\nA pickaxe, pick-axe, or pick is a hand tool with a hard head attached perpendicular to the handle.\n\nThe head is usually made of metal, and the handle is most commonly wood, metal or fiberglass. The head is a spike ending in a sharp point, may curve slightly, and often has a counter-weight to improve ease of use. The stronger the spike, the more effectively the tool can pierce the surface. Rocking the embedded spike about and removing it can then break up the surface.\n\nThe counterweight nowadays is nearly always a second spike, often with a flat end for prying.\n\nThe pointed edge is most often used to break up rocky surfaces or other hard surfaces such as concrete or hardened dried earth. The large momentum of a heavy pickaxe on a small contact area makes it very effective for this purpose. The chiseled end, if present, is used for purposes including cutting through roots.\n\nOriginally used as agricultural tools as far back as prehistoric cultures, picks have also served for tasks ranging from traditional mining to warfare. The design has also evolved into other tools such as the plough and the mattock. In prehistoric times a large shed deer antler from a suitable species (e.g. red deer) was often cut down to its shaft and its lowest tine and used as a one-pointed pick, and with it sometimes a large animal's shoulder blade as a crude shovel.\nDuring war in medieval times, the pickaxe was used as a weapon.\n\nUnicode 5.2 in the Miscellaneous Symbols block introduces the glyph ⛏ (U+26CF PICK), representable in HTML as codice_1 or codice_2, to represent this tool.\n\nThe Oxford Dictionary of English states that both \"pick\" and \"pickaxe\" have the same meaning, that being a tool with a long handle at right angles to a curved iron or steel bar with a point at one end and a chisel or point at the other, used for breaking up hard ground or rock.\n\nThe term \"pickaxe\" is a folk etymology alteration of Middle-English \"picas\" via Anglo-French \"piceis\", Old French \"pocois\", and directly from Medieval Latin \"picosa\" \"pick\", related to Latin \"picus\" \"woodpecker\". Though modern picks usually feature a head with both a pointed end and an adze-like flattened blade on the other end, current spelling is influenced by \"axe\", and \"pickaxe\", \"pick-axe\", or sometimes just \"pick\" cover any and all versions of the tool.\n\nA pickaxe handle (sometimes called a \"pickhandle\" or \"pick helve\") without the head is sometimes used, often unofficially, as a baton: for example it is officially sometimes used as a baton in the British Army.\n\nIn \"The Grapes of Wrath\" by John Steinbeck, pick handles featured prominently as a weapon used against the migrant farmers.\n\nA normal pickaxe handle is made of ash or hickory wood and is about three feet long and weighs about 2.5 pounds. British Army pickaxe handles must, by regulation, be exactly three feet long, for use in measuring in the field. New variant designs are:\n\nIn former times they were sometimes made with a steel casing on the thick end.\n\n"}
{"id": "8501157", "url": "https://en.wikipedia.org/wiki?curid=8501157", "title": "Pneumatic stabilized platform", "text": "Pneumatic stabilized platform\n\nA Pneumatic stabilized platform (PSP) is a technology used to float a very large floating structure (VLFS). \n\nPSP utilizes indirect displacement, in which a platform rests on trapped air that displaces the water. The primary buoyancy force is provided by air pressure acting on the underside of the deck. The PSP is a distinct type of pneumatic platform, one in which the platform is composed of a number of cylindrical shaped components packed together in a rectangular pattern to form a module.\n\nThe Pneumatically Stabilized Platform was originally proposed for constructing a new floating airport for San Diego in the Pacific Ocean, at least three miles off the tip of Point Loma. However, this proposed design was rejected in October, 2003 due to high cost, the difficulty in accessing such an airport, the difficulty in transporting jet fuel, electricity, water, and gas to the structure, failure to address security concerns such as a bomb blast, inadequate room for high-speed exits and taxiways, and environmental concerns. \n\n\n"}
{"id": "239050", "url": "https://en.wikipedia.org/wiki?curid=239050", "title": "Project manager", "text": "Project manager\n\nA project manager is a professional in the field of project management. Project managers have the responsibility of the planning, procurement and execution of a project, in any undertaking that has a defined scope, defined start and a defined finish; regardless of industry. Project managers are first point of contact for any issues or discrepancies arising from within the heads of various departments in an organization before the problem escalates to higher authorities. Project management is the responsibility of a project manager. This individual seldom participates directly in the activities that produce the end result, but rather strives to maintain the progress, mutual interaction and tasks of various parties in such a way that reduces the risk of overall failure, maximizes benefits, and minimizes costs.\n\nA project manager is the person responsible for accomplishing the project objectives. Key project management responsibilities include \nA project manager is a client representative and has to determine and implement the exact needs of the client, based on knowledge of the organization they are representing. An expertise is required in the domain the Project Managers are working to efficiently handle all the aspects of the project. The ability to adapt to the various internal procedures of the client and to form close links with the nominated representatives, is essential in ensuring that the key issues of cost, time, quality and above all, client satisfaction, can be realized.\n\n\n\n\nUntil recently, the American construction industry lacked any level of standardization, with individual States determining the eligibility requirements within their jurisdiction. However, several Trade associations based in the United States have made strides in creating a commonly accepted set of qualifications and tests to determine a project manager's competency.\nThe profession has recently grown to accommodate several dozen Construction Management Bachelor of Science programs.\nMany universities have also begun offering a master's degree in Project Management. These programs generally are tailored to working professionals who have project management experience or project related experience; they provide a more intense and in depth education surrounding the knowledge areas within the project management body of knowledge.\n\nThe United States Navy construction battalions, nicknamed the SeaBees, puts their command through strenuous training and certifications at every level. To become a Chief Petty Officer in the SeaBees is equivalent to a BS in Construction Management with the added benefit of several years of experience to their credit. See ACE accreditation.\n\nArchitectural project manager are project managers in the field of architecture. They have many of the same skills as their counterpart in the construction industry. And will often work closely with the construction project manager in the office of the General Contractor (GC), and at the same time, coordinate the work of the design team and numerous consultants who contribute to a construction project, and manage communication with the client. The issues of budget, scheduling, and quality-control are the responsibility of the Project Manager in an architect's office.\n\nIn the insurance industry project managers often oversee and manage the restoration of a clients home/office after a fire, flood.\nCovering the fields from electronics through to the demolition and constructions contractors.\n\nIn Engineering, project management is the term used to describe the task of seeing a product/device through the stages of R&D/design to manufacturing stages, working with various professionals in different fields of engineering and manufacturing to go from concept to finished product. Optionally, this can include different versions and standards as required by different countries, requiring knowledge of laws, requirements and infrastructure.\n\nIT Project Management generally falls into two categories, namely Software (Development) Project Manager and Infrastructure Project Manager.\nSoftware Project Manager -- A Software Project Manager has many of the same skills as their counterparts in other industries. Beyond the skills normally associated with traditional project management in industries such as construction and manufacturing, a software project manager will typically have an extensive background in software development. Many software project managers hold a degree in Computer Science, Information Technology, Management of Information Systems or another related field.\n\nIn traditional project management a heavyweight, predictive methodology such as the waterfall model is often employed, but software project managers must also be skilled in more lightweight, adaptive methodologies such as DSDM, Scrum and XP. These project management methodologies are based on the uncertainty of developing a new software system and advocate smaller, incremental development cycles. These incremental or iterative cycles are time boxed (constrained to a known period of time, typically from one to four weeks) and produce a working subset of the entire system deliverable at the end of each iteration. The increasing adoption of lightweight approaches is due largely to the fact that software requirements are very susceptible to change, and it is extremely difficult to illuminate all the potential requirements in a single project phase before the software development commences.\n\nThe software project manager is also expected to be familiar with the Software Development Life Cycle (SDLC). This may require in depth knowledge of requirements solicitation, application development, logical and physical database design and networking. This knowledge is typically the result of the aforementioned education and experience. There is not a widely accepted certification for software project managers, but many will hold the Project Management Professional (PMP) designation offered by the Project Management Institute, PRINCE2 or an advanced degree in project management, such as a MSPM or other graduate degree in technology management.\n\nIT Infrastructure Project Management -- An infrastructure IT PM is concerned with the nuts and bolts of the IT department, including computers, servers, storage, networking, and such aspects of them as backup, business continuity, upgrades, replacement, and growth. Often, a secondary data center will be constructed in a remote location to help protect the business from outages caused by natural disaster or weather. Recently, cyber security has become a significant growth area within IT infrastructure management.\n\nThe infrastructure PM usually has an undergraduate degree in engineering or computer science, with a master's degree in project management required for senior level positions. Along with the formal education, most senior level PMs are certified, by the Project Management Institute, as a Project Management Professional. PMI also has several additional certification options, but PMP is by far the most popular.\n\nInfrastructure PMs are responsible for managing projects that have budgets from a few thousand dollars up to many millions of dollars. They must understand the business and the business goals of the sponsor and the capabilities of the technology in order to reach the desired goals of the project. The most difficult part of the infrastructure PM's job may be this translation of business needs / wants into technical specifications. Oftentimes, business analysts are engaged to help with this requirement. The team size of a large infrastructure project may run into several hundred engineers and technicians, many of whom have strong personalities and require strong leadership if the project goals are to be met.\n\nDue to the high operations expense of maintaining a large staff of highly skilled IT engineering talent, many organizations outsource their infrastructure implementations and upgrades to third party companies. Many of these companies have strong project management organizations with the ability to not only manage their clients projects, but to also generate high quality revenue at the same time.\n\nThe Project Manager is accountable for ensuring that everyone on the team knows and executes his or her role, feels empowered and supported in the role, knows the roles of the other team members and acts upon the belief that those roles will be performed. The specific responsibilities of the Project Manager may vary depending on the industry, the company size, the company maturity, and the company culture. However, there are some responsibilities that are common to all Project Managers, noting:\n\n\n\n"}
{"id": "10070596", "url": "https://en.wikipedia.org/wiki?curid=10070596", "title": "Pueblo Historical Aircraft Society", "text": "Pueblo Historical Aircraft Society\n\nThe Pueblo Historical Aircraft Society (PHAS), Pueblo, Colorado hosts a large collections of military aircraft in Colorado, United States through the operation of the Weisbrod Museum and the International B-24 Memorial Museum, together as the Pueblo Weisbrod Aircraft Museum.\n\nThe Society is a volunteer group of ex-military and civilian personnel who manage and operate the aircraft display as well as repairing and restoring the aircraft. The Society is open to all who wish to help preserve the aviation history in Pueblo County and Pueblo, Colorado.\n\n\n"}
{"id": "45203970", "url": "https://en.wikipedia.org/wiki?curid=45203970", "title": "Rivers State Ministry of Energy and Natural Resources", "text": "Rivers State Ministry of Energy and Natural Resources\n\nThe Rivers State Ministry of Energy and Natural Resources is a government ministry of Rivers State, Nigeria in charge of monitoring, controlling and regulating activities related to energy and natural resources in the state. The mandate of the ministry is \"To meet the energy needs of the Rivers State population and maximize their participation in the upstream/downstream sectors.\" The current Commissioner is Okey C. Amadi. Ministry headquarters are located at State Secretariat building, Port Harcourt.\n\nThe ministry consists of the following departments:\n\n\n"}
{"id": "10036536", "url": "https://en.wikipedia.org/wiki?curid=10036536", "title": "Sarah Bagley", "text": "Sarah Bagley\n\nSarah George Bagley (April 19, 1806 – January 15, 1889) was a labor leader in New England during the 1840s; an advocate of shorter workdays for factory operatives and mechanics, she campaigned to make ten hours of labor per day the maximum in Massachusetts.\n\nHer activities in support of the mill workers in Lowell, Massachusetts put her in contact with a broader network of reformers in areas of women’s rights, communitarianism, abolition, peace, prison reform, and health reform. Bagley and her coworkers became familiar with middle-class reform activities, demonstrating the ways in which working people embraced this reform impulse as they transformed and critiqued some of its key elements. Her activities within the labor movement reveal many of the tensions that underlay relations between male and female working people as well as the constraints of gender that female activists had to overcome.\n\nSarah George Bagley was born April 19, 1806 in Candia, New Hampshire to Rhoda (née Witham) and Nathan Bagley, both members of large New England families. Nathan and Rhoda farmed, sold land, and owned a small mill to support their family. She had two brothers, Thomas and Henry, and one sister, Mary Jane.\n\nIn 1835, Bagley first appeared in Lowell, Massachusetts, working at the Hamilton Mills. She worked initially as a weaver and then as a dresser, and by 1840 she had saved enough money to make a deposit on the house which her parents and siblings were living in. Bagley was dissatisfied with working conditions however and published one of her first pieces of writing, “Pleasures of Factory Life”, in an 1840 issue of the \"Lowell Offering\", a literary magazine written, edited, and published by working women. These \"pleasures\", she wrote, were like angels' visits: \"few and far between\".\n\nIn late November 1842, 70 weavers at the Middlesex Mills walked off their jobs, protesting the newly-introduced requirement to tend two looms instead of one. The workers were fired and blacklisted, and shortly afterwards, Bagley left the Hamilton Mills and went to work for Middlesex. Between 1842 and 1844, over 1,000 textile workers left Lowell as a result of wage cuts and stretch-outs due to an economic recession. In March 1844, under improved economic conditions, the textile corporations raised the wages of male – but not female – textile workers to the 1842 levels.\n\nIn December 1844, Bagley along with five other women met in Anti-Slavery Hall in downtown Lowell and formed the Lowell Female Labor Reform Association. Its aim was to improve health conditions and lobby for a ten-hour day; at that time, women worked from 12 to 14 hours a day in the Lowell textile mills. As president, Bagley saw the LFLRA grow to nearly 600 members, with branches in Waltham, Fall River, Manchester, Dover and Nashua.\n\nWith the encouragement and assistance of feminist Angelique Martin, the LFLRA purchased a printing press and published their own labor newspaper, \"The Voice of Industry,\" for which Bagley frequently contributed articles and edited a women’s column. This was particularly necessary as in 1842, the \"Lowell Offering\" had been purchased by state Representative William Schouler, a friend of factory owners in Lowell. Under his ownership, the magazine would no longer publish criticisms of factory management or conditions.\n\nIn 1845, Bagley and the LFLRA members gathered names of textile workers on petitions sent to the Massachusetts Legislature, demanding a ten-hour workday. As a result of dozens of petitions totalling over 2,100 signatures, a state legislature held hearings to investigate the conditions of labor in the manufacturing corporations. Bagley and others testified to the long hours and unhealthy working conditions in the mills. The committee, led by Representative William Schouler, reported that the legislature did not have the power to determine hours of work and that the ten-hour day must be decided between the corporations and the textile workers. The workers were furious and successfully campaigned to defeat Representative Schouler in the next election.\n\nBagley and the LFLRA continued sending petitions to the state legislature for a ten-hour day; they gathered over 10,000 names from throughout Massachusetts, more than 2,000 of which were from working women and men of Lowell. Again hearings were held to investigate working conditions, and again the Massachusetts Legislature refused to take action. However, labor and political pressure on the Lowell textile corporations was so great that in 1847 the mills shortened the workday by 30 minutes. As the labor reform movement persisted the corporations again reduced the hours of labor to eleven in 1853 and ten in 1874.\n\nBagley was also involved with other social justice movements. She supported the peace movement, which was developing as the Mexican–American War unfolded. She collected 146 signatures from Lowell and submitted a petition to Congress calling for an international tribunal to adjudicate disputes and therefore end the need for warfare. Bagley also supported electoral reform. In 1845, she invited Thomas Dorr to speak in the town; Dorr had previously led a group of property-less Rhode Islanders in protest against the state's voting laws, which required voters to own property. As a result of interactions with Angelique Martin, Bagley also became interested in women's rights and organised a series of lectures on the topic.\n\nIn June 1846, Bagley gave up her position as editor of the \"Voice of Industry\". Subsequent editors considered her writing too radical and undignified, and Bagley looked for another job; in February 1847, only two years after Samuel Morse's first successful demonstration of the electric telegraph, the New York and Boston Magnetic Telegraph Company opened an office in Lowell, and Bagley was hired as telegrapher. Early in 1847, Bagley was contracted to run the magnetic telegraph office in Springfield, Massachusetts. She was unhappy to discover she earned only three-quarters as much as the man she replaced, writing to a friend of her growing commitment to human equality and the rights of women.\n\nA year later, Bagley returned to Lowell, working for the Hamilton Mills and living with one of her brothers. While based in Lowell, she traveled throughout New England, writing about health care, working conditions, prison reform, and women’s rights. In 1849, she moved to Philadelphia, Pennsylvania where she worked with the Quakers as the executive secretary of the Rosine Home, providing a safe place for prostitutes and disadvantaged young women. While in Philadelphia, Bagley met James Durno (1795–1871), a native of Aberdeen, Scotland; they married on November 13, 1850.\n\nIn 1851, Sarah and James Durno moved to Albany, New York and began their practice as homeopathic physicians. At that time, homeopathic health care was a new field of medicine, which used herbs and medicines rather than the traditional procedures performed by doctors at the time – bleeding patients, or “purging” the body through vomiting. Their practice specialized in providing medical care for women and children. The price of their services was “to the rich, one dollar – to the poor gratis [free]”. The Durnos began manufacturing herbal medicines and Durno Catarrh Snuff. By 1867, the couple had moved their manufacturing company to New York and lived in a large brick house in Brooklyn Heights. On June 22, 1871, James Durno died in Brooklyn, Kings County (not yet part of New York City), aged 76, and was buried in Green-Wood Cemetery. On January 15, 1889, Sarah Bagley died in Philadelphia, Pennsylvania, aged 81, and was buried in Lloyd Bowers Hoppin Family Lot in Laurel Hill Cemetery. The couple had no children.\n\n"}
{"id": "192615", "url": "https://en.wikipedia.org/wiki?curid=192615", "title": "Satellite dish", "text": "Satellite dish\n\nA satellite dish is a dish-shaped type of parabolic antenna designed to receive or transmit information by radio waves to or from a communication satellite. The term most commonly means a dish used by consumers to receive direct-broadcast satellite television from a direct broadcast satellite in geostationary orbit.\n\nThe parabolic shape of a dish reflects the signal to the dish’s focal point. Mounted on brackets at the dish's focal point is a device called a feedhorn. This feedhorn is essentially the front-end of a waveguide that gathers the signals at or near the focal point and 'conducts' them to a low-noise block downconverter or LNB. The LNB converts the signals from electromagnetic or radio waves to electrical signals and shifts the signals from the downlinked C-band and/or K-band to the L-band range. Direct broadcast satellite dishes use an LNBF, which integrates the feedhorn with the LNB. (A new form of omnidirectional satellite antenna, which does not use a directed parabolic dish and can be used on a mobile platform such as a vehicle was announced by the University of Waterloo in 2004.\n\nThe theoretical gain (directive gain) of a dish increases as the frequency increases. The actual gain depends on many factors including surface finish, accuracy of shape, feedhorn matching. A typical value for a consumer type 60 cm satellite dish at 11.75 GHz is 37.50 dB.\n\nWith lower frequencies, C-band for example, dish designers have a wider choice of materials. The large size of dish required for lower frequencies led to the dishes being constructed from metal mesh on a metal framework. At higher frequencies, mesh type designs are rarer though some designs have used a solid dish with perforations.\n\nA common misconception is that the LNBF (low-noise block/feedhorn), the device at the front of the dish, receives the signal directly from the atmosphere. For instance, one BBC News downlink shows a \"red signal\" being received by the LNBF directly instead of being beamed to the dish, which because of its parabolic shape will collect the signal into a smaller area and deliver it to the LNBF.\n\nModern dishes intended for home television use are generally 43 cm (18 in) to 80 cm (31 in) in diameter, and are fixed in one position, for Ku-band reception from one orbital position. Prior to the existence of direct broadcast satellite services, home users would generally have a motorised C-band dish of up to 3 m in diameter for reception of channels from different satellites. Overly small dishes can still cause problems, however, including rain fade and interference from adjacent satellites.\n\nIn Europe, the frequencies used by DBS services are 10.7–12.75 GHz on two polarisations H (Horizontal) and V (Vertical). This range is divided into a \"low band\" with 10.7–11.7 GHz, and a \"high band\" with 11.7–12.75 GHz. This results in two frequency bands, each with a bandwidth of about 1 GHz, each with two possible polarizations. In the LNB they become down converted to 950–2150 MHz, which is the frequency range allocated for the satellite service on the coaxial cable between LNBF and receiver. Lower frequencies are allocated to cable and terrestrial TV, FM radio, etc. Only one of these frequency bands fits on the coaxial cable, so each of these bands needs a separate cable from the LNBF to a switching matrix or the receiver needs to select one of the 4 possibilities at a time.\n\nIn a single receiver residential installation there is a single coaxial cable running from the receiver set-top box in the building to the LNB on the dish. The DC electric power for the LNB is provided through the same coaxial cable conductors that carry the signal to the receiver. In addition, control signals are also transmitted from the receiver to the LNB through the cable. The receiver uses different power supply voltages (13 / 18 V) to select antenna polarization, and pilot tones (22 kHz) to instruct the LNB to select one of the two frequency bands. In larger installations each band and polarization is given its own cable, so there are 4 cables from the LNB to a switching matrix, which allows the connection of multiple receivers in a star topology using the same signalling method as in a single receiver installation.\n\nA satellite finder may aid in aiming.\n\nA dish that is mounted on a pole and driven by a stepper motor or a servo can be controlled and rotated to face any satellite position in the sky. Motor-driven dishes are popular with enthusiasts. There are three competing standards: DiSEqC, USALS, and 36 V positioners. Many receivers support all of these standards.\n\nSome designs enable simultaneous reception from multiple different satellite positions without re-positioning the dish. The vertical axis operates as an off-axis concave parabolic concave hyperbolic Cassegrain reflector, while the horizontal axis operates as a concave convex Cassegrain. The spot from the main dish wanders across the secondary, which corrects astigmatism by its varying curvature. The elliptic aperture of the primary is designed to fit the deformed illumination by the horns. Due to double spill-over, this makes more sense for a large dish.\n\nA common type of dish is the very small aperture terminal (VSAT). This provides two way satellite internet communications for both consumers and private networks for organizations. Today most VSATs operate in ; C band is restricted to less populated regions of the world. There is a move which started in 2005 towards new satellites operating at higher frequencies, offering greater performance at lower cost. These antennas vary from in most applications though C-band VSATs may be as large as .\n\n\nAny metal surface which concentrates a significant fraction of the reflected microwaves at a focus can be used as a dish antenna, at a lower gain. This has led to trash can lids, woks, and other items being used as \"dishes\". Only modern low noise LNBs and the higher transmission power of DTH satellites allows a usable signal to be received from such inefficient DIY antennas.\n\nParabolic antennas referred to as \"dish\" antennas had been in use long before satellite television. The term \"satellite dish\" was coined in 1978 during the beginning of the satellite television industry, and came to refer to dish antennas that send and/or receive signals from communications satellites. Taylor Howard of San Andreas, California adapted an ex-military dish in 1976 and became the first person to receive satellite television signals using it.\n\nThe first satellite television dishes were built to receive signals on the C-band analog, and were very large. The front cover of the 1979 Neiman-Marcus Christmas catalog featured the first home satellite TV stations on sale. The dishes were nearly in diameter. The satellite dishes of the early 1980s were in diameter and made of fiberglass with an embedded layer of wire mesh or aluminum foil, or solid aluminum or steel.\n\nSatellite dishes made of wire mesh first came out in the early 1980s, and were at first in diameter. As the front-end technology improved and the noise figure of the LNBs fell, the size shrank to a few years later, and continued to get smaller reducing to feet by the late 1980s and by the early 1990s. Larger dishes continued to be used, however. In December 1988 Luxembourg's Astra 1A satellite began transmitting analog television signals on the K band for the European market. This allowed small dishes (90 cm) to be used reliably for the first time.\n\nIn the early 1990s, four large American cable companies founded PrimeStar, a direct broadcasting company using medium power satellites. The relatively strong K band transmissions allowed the use of dishes as small as 90 cm for the first time. On 4 March 1996 EchoStar introduced Digital Sky Highway (Dish Network). This was the first widely used direct-broadcast satellite television system and allowed dishes as small as 20 inches to be used. This great decrease of dish size also allowed satellite dishes to be installed on vehicles. Dishes this size are still in use today. Television stations, however, still prefer to transmit their signals on the C-band analog with large dishes due to the fact that C-band signals are less prone to rain fade than K band signals.\n\n\n"}
{"id": "29468", "url": "https://en.wikipedia.org/wiki?curid=29468", "title": "Speech recognition", "text": "Speech recognition\n\nSpeech recognition is the inter-disciplinary sub-field of computational linguistics that develops methodologies and technologies that enables the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the linguistics, computer science, and electrical engineering fields.\n\nSome speech recognition systems require \"training\" (also called \"enrollment\") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. Systems that do not use training are called \"speaker independent\" systems. Systems that use training are called \"speaker dependent\".\n\nSpeech recognition applications include voice user interfaces such as voice dialing (e.g. \"call home\"), call routing (e.g. \"I would like to make a collect call\"), domotic appliance control, search (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), determining speaker characteristics, speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed direct voice input).\n\nThe term \"voice recognition\" or \"speaker identification\" refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process.\n\nFrom the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data. The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the worldwide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems. These speech industry players include Google, Microsoft, IBM, Baidu, Apple, Amazon, Nuance, SoundHound, iFLYTEK many of which have publicized the core technology in their speech recognition systems as being based on deep learning.\n\nIn 1952 three Bell Labs researchers, Stephen. Balashek , R. Biddulph, and K. H. Davis built a system called 'Audrey' an automatic digit recognizer for single-speaker digit recognition. Their system worked by locating the formants in the power spectrum of each utterance. The 1950s era technology was limited to single-speaker systems with vocabularies of around ten words.\n\nGunnar Fant developed the source-filter model of speech production and published it in 1960, which proved to be a useful model of speech production.\n\nAt the 1962 World's Fair, IBM demonstrated it's 16-word \"Shoebox\" machine's speech recognition capability.\n\nUnfortunately, funding at Bell Labs dried up for several years when, in 1969, the influential John Pierce wrote an open letter that was critical of speech recognition research. Pierce defunded speech recognition research at Bell Labs where no research on speech recognition was done until Pierce retired and James L. Flanagan took over.\n\nRaj Reddy was the first person to take on continuous speech recognition as a graduate student at Stanford University in the late 1960s. Previous systems required the users to make a pause after each word. Reddy's system was designed to issue spoken commands for the game of chess.\n\nAlso around this time Soviet researchers invented the dynamic time warping (DTW) algorithm and used it to create a recognizer capable of operating on a 200-word vocabulary. The DTW algorithm processed the speech signal by dividing it into short frames, e.g. 10ms segments, and processing each frame as a single unit. Although DTW would be superseded by later algorithms, the technique of dividing the signal into frames would carry on. Achieving speaker independence was a major unsolved goal of researchers during this time period.\n\nIn 1971, DARPA funded five years of speech recognition research through its Speech Understanding Research program with ambitious end goals including a minimum vocabulary size of 1,000 words. It was thought that speech \"understanding\" would be key to making progress in speech \"recognition\", although that later proved to not be true. BBN, IBM, Carnegie Mellon and Stanford Research Institute all participated in the program. The government funding revived speech recognition research that had been largely abandoned in the United States after John Pierce's letter.\n\nDespite the fact that CMU's Harpy system met the original goals of the program, many predictions turned out to be nothing more than hype, disappointing DARPA administrators. This disappointment led to DARPA not continuing the funding. Several innovations happened during this time, such as the invention of beam search for use in CMU's Harpy system. The field also benefited from the discovery of several algorithms in other fields such as linear predictive coding and cepstral analysis.\n\nIn 1972, the IEEE Acoustics, Speech, and Signal Processing group held a conference in Newton, Massachusetts. Four years later, the first ICASSP was held in Philadelphia, which since then has been a major venue for the publication of research on speech recognition.\n\nDuring the late 1960s Leonard Baum developed the mathematics of Markov chains at the Institute for Defense Analysis. A decade later, at CMU, Raj Reddy's students James Baker and Janet M. Baker began using the Hidden Markov Model (HMM) for speech recognition. James Baker had learned about HMMs from a summer job at the Institute of Defense Analysis during his undergraduate education. The use of HMMs allowed researchers to combine different sources of knowledge, such as acoustics, language, and syntax, in a unified probabilistic model.\n\nUnder Fred Jelinek's lead, IBM created a voice activated typewriter called Tangora, which could handle a 20,000 word vocabulary by the mid 1980s. Jelinek's statistical approach put less emphasis on emulating the way the human brain processes and understands speech in favor of using statistical modeling techniques like HMMs. (Jelinek's group independently discovered the application of HMMs to speech.) This was controversial with linguists since HMMs are too simplistic to account for many common features of human languages. However, the HMM proved to be a highly useful way for modeling speech and replaced dynamic time warping to become the dominant speech recognition algorithm in the 1980s. IBM had a few competitors including Dragon Systems founded by James and Janet M. Baker in 1982. The 1980s also saw the introduction of the n-gram language model. Katz introduced the back-off model in 1987, which allowed language models to use multiple length n-grams. During the same time, also CSELT was using HMM (the diphonies were studied since 1980) to recognize language like Italian. At the same time, CSELT led a series of European projects (Esprit I, II), and summarized the state-of-the-art in a book, later (2013) reprinted.\n\nMuch of the progress in the field is owed to the rapidly increasing capabilities of computers. At the end of the DARPA program in 1976, the best computer available to researchers was the PDP-10 with 4 MB ram. Using these computers it could take up to 100 minutes to decode just 30 seconds of speech. A few decades later, researchers had access to tens of thousands of times as much computing power. As the technology advanced and computers got faster, researchers began tackling harder problems such as larger vocabularies, speaker independence, noisy environments and conversational speech. In particular, this shifting to more difficult tasks has characterized DARPA funding of speech recognition since the 1980s. For example, progress was made on speaker independence first by training on a larger variety of speakers and then later by doing explicit speaker adaptation during decoding. Further reductions in word error rate came as researchers shifted acoustic models to be discriminative instead of using maximum likelihood estimation.\n\nIn the mid-Eighties new speech recognition microprocessors were released: for example RIPAC, an independent-speaker recognition (for continuous speech) chip tailored for telephone services, was presented in the Netherlands in 1986. It was designed by CSELT/Elsag and manufactured by SGS. This processor was extremely complex for that time, since it carried 70.000 transistors. However, nowadays the need of specific microprocessor aimed to speech recognition tasks is still alive: for example, in 2017 the MIT released such a microprocessor of new generation.\n\nThe 1990s saw the first introduction of commercially successful speech recognition technologies. Two of the earliest products were Dragon Dictate, a consumer product released in 1990 and originally priced at $9,000, and a recognizer from Kurzweil Applied Intelligence released in 1987. AT&T deployed the Voice Recognition Call Processing service in 1992 to route telephone calls without the use of a human operator. The technology was developed by Lawrence Rabiner and others at Bell Labs.\nBy this point, the vocabulary of the typical commercial speech recognition system was larger than the average human vocabulary. Raj Reddy's former student, Xuedong Huang, developed the Sphinx-II system at CMU. The Sphinx-II system was the first to do speaker-independent, large vocabulary, continuous speech recognition and it had the best performance in DARPA's 1992 evaluation. Handling continuous speech with a large vocabulary was a major milestone in the history of speech recognition. Huang went on to found the speech recognition group at Microsoft in 1993. Raj Reddy's student Kai-Fu Lee joined Apple where, in 1992, he helped develop a speech interface prototype for the Apple computer known as Casper.\n\nLernout & Hauspie, a Belgium-based speech recognition company, acquired several other companies, including Kurzweil Applied Intelligence in 1997 and Dragon Systems in 2000. The L&H speech technology was used in the Windows XP operating system. L&H was an industry leader until an accounting scandal brought an end to the company in 2001. The speech technology from L&H was bought by ScanSoft which became Nuance in 2005. Apple originally licensed software from Nuance to provide speech recognition capability to its digital assistant Siri.\n\nIn the 2000s DARPA sponsored two speech recognition programs: Effective Affordable Reusable Speech-to-Text (EARS) in 2002 and Global Autonomous Language Exploitation (GALE). Four teams participated in the EARS program: IBM, a team led by BBN with LIMSI and Univ. of Pittsburgh, \nCambridge University, and a team composed of ICSI, SRI and University of Washington. EARS funded the collection of the Switchboard telephone speech corpus containing 260 hours of recorded conversations from over 500 speakers. The GALE program focused on Arabic and Mandarin broadcast news speech. Google's first effort at speech recognition came in 2007 after hiring some researchers from Nuance. The first product was GOOG-411, a telephone based directory service. The recordings from GOOG-411 produced valuable data that helped Google improve their recognition systems. Google voice search is now supported in over 30 languages.\n\nIn the United States, the National Security Agency has made use of a type of speech recognition for keyword spotting since at least 2006. This technology allows analysts to search through large volumes of recorded conversations and isolate mentions of keywords. Recordings can be indexed and analysts can run queries over the database to find conversations of interest. Some government research programs focused on intelligence applications of speech recognition, e.g. DARPA's EARS's program and IARPA's Babel program.\n\nWhat, by early 2010s was often called \"speech\" recognition, so as to differentiate from \n\"speaker\" recognition, was also called voice recognition; this is what was commonly used.\n\nMoreover, speaker independence was considered a major breakthrough: until then, systems required a \"training\" period. A 1987 ad for a doll carried the tagline \"Finally, the doll that understands you.\" - despite the fact that it was described as \"which children could train to respond to their voice.\"\n\nIn the early 2000s, speech recognition was still dominated by traditional approaches such as Hidden Markov Models combined with feedforward artificial neural networks.\nToday, however, many aspects of speech recognition have been taken over by a deep learning method called Long short-term memory (LSTM), \na recurrent neural network published by Sepp Hochreiter & Jürgen Schmidhuber in 1997. LSTM RNNs avoid the vanishing gradient problem and can learn \"Very Deep Learning\" tasks that require memories of events that happened thousands of discrete time steps ago, which is important for speech.\nAround 2007, LSTM trained by Connectionist Temporal Classification (CTC) started to outperform traditional speech recognition in certain applications. In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which is now available through Google Voice to all smartphone users.\n\nThe use of deep feedforward (non-recurrent) networks for acoustic modeling was introduced during later part of 2009 by Geoffrey Hinton and his students at University of Toronto and by Li Deng and colleagues at Microsoft Research, initially in the collaborative work between Microsoft and University of Toronto which was subsequently expanded to include IBM and Google (hence \"The shared views of four research groups\" subtitle in their 2012 review paper). A Microsoft research executive called this innovation \"the most dramatic change in accuracy since 1979\". In contrast to the steady incremental improvements of the past few decades, the application of deep learning decreased word error rate by 30%. This innovation was quickly adopted across the field. Researchers have begun to use deep learning techniques for language modeling as well.\n\nIn the long history of speech recognition, both shallow form and deep form (e.g. recurrent nets) of artificial neural networks had been explored for many years during 1980s, 1990s and a few years into the 2000s.\nBut these methods never won over the non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.\nA number of key difficulties had been methodologically analyzed in the 1990s, including gradient diminishing and weak temporal correlation structure in the neural predictive models.\nAll these difficulties were in addition to the lack of big training data and big computing power in these early days. Most speech recognition researchers who understood such barriers hence subsequently moved away from neural nets to pursue generative modeling approaches until the recent resurgence of deep learning starting around 2009–2010 that had overcome all these difficulties. Hinton et al. and Deng et al. reviewed part of this recent history about how their collaboration with each other and then with colleagues across four groups (University of Toronto, Microsoft, Google, and IBM) ignited a renaissance of applications of deep feedforward neural networks to speech recognition.\n\nBoth acoustic modeling and language modeling are important parts of modern statistically-based speech recognition algorithms. Hidden Markov models (HMMs) are widely used in many systems. Language modeling is also used in many other natural language processing applications such as document classification or statistical machine translation.\n\nModern general-purpose speech recognition systems are based on Hidden Markov Models. These are statistical models that output a sequence of symbols or quantities. HMMs are used in speech recognition because a speech signal can be viewed as a piecewise stationary signal or a short-time stationary signal. In a short time-scale (e.g., 10 milliseconds), speech can be approximated as a stationary process. Speech can be thought of as a Markov model for many stochastic purposes.\n\nAnother reason why HMMs are popular is because they can be trained automatically and are simple and computationally feasible to use. In speech recognition, the hidden Markov model would output a sequence of \"n\"-dimensional real-valued vectors (with \"n\" being a small integer, such as 10), outputting one of these every 10 milliseconds. The vectors would consist of cepstral coefficients, which are obtained by taking a Fourier transform of a short time window of speech and decorrelating the spectrum using a cosine transform, then taking the first (most significant) coefficients. The hidden Markov model will tend to have in each state a statistical distribution that is a mixture of diagonal covariance Gaussians, which will give a likelihood for each observed vector. Each word, or (for more general speech recognition systems), each phoneme, will have a different output distribution; a hidden Markov model for a sequence of words or phonemes is made by concatenating the individual trained hidden Markov models for the separate words and phonemes.\n\nDescribed above are the core elements of the most common, HMM-based approach to speech recognition. Modern speech recognition systems use various combinations of a number of standard techniques in order to improve results over the basic approach described above. A typical large-vocabulary system would need context dependency for the phonemes (so phonemes with different left and right context have different realizations as HMM states); it would use cepstral normalization to normalize for different speaker and recording conditions; for further speaker normalization it might use vocal tract length normalization (VTLN) for male-female normalization and maximum likelihood linear regression (MLLR) for more general speaker adaptation. The features would have so-called delta and delta-delta coefficients to capture speech dynamics and in addition might use heteroscedastic linear discriminant analysis (HLDA); or might skip the delta and delta-delta coefficients and use splicing and an LDA-based projection followed perhaps by heteroscedastic linear discriminant analysis or a global semi-tied co variance transform (also known as maximum likelihood linear transform, or MLLT). Many systems use so-called discriminative training techniques that dispense with a purely statistical approach to HMM parameter estimation and instead optimize some classification-related measure of the training data. Examples are maximum mutual information (MMI), minimum classification error (MCE) and minimum phone error (MPE).\n\nDecoding of the speech (the term for what happens when the system is presented with a new utterance and must compute the most likely source sentence) would probably use the Viterbi algorithm to find the best path, and here there is a choice between dynamically creating a combination hidden Markov model, which includes both the acoustic and language model information, and combining it statically beforehand (the finite state transducer, or FST, approach).\n\nA possible improvement to decoding is to keep a set of good candidates instead of just keeping the best candidate, and to use a better scoring function (re scoring) to rate these good candidates so that we may pick the best one according to this refined score. The set of candidates can be kept either as a list (the N-best list approach) or as a subset of the models (a lattice). Re scoring is usually done by trying to minimize the Bayes risk (or an approximation thereof): Instead of taking the source sentence with maximal probability, we try to take the sentence that minimizes the expectancy of a given loss function with regards to all possible transcriptions (i.e., we take the sentence that minimizes the average distance to other possible sentences weighted by their estimated probability). The loss function is usually the Levenshtein distance, though it can be different distances for specific tasks; the set of possible transcriptions is, of course, pruned to maintain tractability. Efficient algorithms have been devised to re score lattices represented as weighted finite state transducers with edit distances represented themselves as a finite state transducer verifying certain assumptions.\n\nDynamic time warping is an approach that was historically used for speech recognition but has now largely been displaced by the more successful HMM-based approach.\n\nDynamic time warping is an algorithm for measuring similarity between two sequences that may vary in time or speed. For instance, similarities in walking patterns would be detected, even if in one video the person was walking slowly and if in another he or she were walking more quickly, or even if there were accelerations and deceleration during the course of one observation. DTW has been applied to video, audio, and graphics – indeed, any data that can be turned into a linear representation can be analyzed with DTW.\n\nA well-known application has been automatic speech recognition, to cope with different speaking speeds. In general, it is a method that allows a computer to find an optimal match between two given sequences (e.g., time series) with certain restrictions. That is, the sequences are \"warped\" non-linearly to match each other. This sequence alignment method is often used in the context of hidden Markov models.\n\nNeural networks emerged as an attractive acoustic modeling approach in ASR in the late 1980s. Since then, neural networks have been used in many aspects of speech recognition such as phoneme classification, isolated word recognition, audiovisual speech recognition, audiovisual speaker recognition and speaker adaptation.\n\nIn contrast to HMMs, neural networks make no assumptions about feature statistical properties and have several qualities making them attractive recognition models for speech recognition. When used to estimate the probabilities of a speech feature segment, neural networks allow discriminative training in a natural and efficient manner. Few assumptions on the statistics of input features are made with neural networks. However, in spite of their effectiveness in classifying short-time units such as individual phonemes and isolated words, neural networks are rarely successful for continuous recognition tasks, largely because of their lack of ability to model temporal dependencies.\n\nHowever, recently LSTM Recurrent Neural Networks (RNNs) and Time Delay Neural Networks(TDNN's) have been used which have been shown to be able to identify latent temporal dependencies and use this information to perform the task of speech recognition.\n\nDeep Neural Networks and Denoising Autoencoders were also being experimented with to tackle this problem in an effective manner.\n\nDue to the inability of feedforward Neural Networks to model temporal dependencies, an alternative approach is to use neural networks as a pre-processing e.g. feature transformation, dimensionality reduction, for the HMM based recognition.\n\nA deep feedforward neural network (DNN) is an artificial neural network with multiple hidden layers of units between the input and output layers. Similar to shallow neural networks, DNNs can model complex non-linear relationships. DNN architectures generate compositional models, where extra layers enable composition of features from lower layers, giving a huge learning capacity and thus the potential of modeling complex patterns of speech data.\n\nA success of DNNs in large vocabulary speech recognition occurred in 2010 by industrial researchers, in collaboration with academic researchers, where large output layers of the DNN based on context dependent HMM states constructed by decision trees were adopted.\nrecent overview articles.\n\nOne fundamental principle of deep learning is to do away with hand-crafted feature engineering and to use raw features. This principle was first explored successfully in the architecture of deep autoencoder on the \"raw\" spectrogram or linear filter-bank features, showing its superiority over the Mel-Cepstral features which contain a few stages of fixed transformation from spectrograms.\nThe true \"raw\" features of speech, waveforms, have more recently been shown to produce excellent larger-scale speech recognition results.\n\nSince 2014, there has been much research interest in \"end-to-end\" ASR. Traditional phonetic-based (i.e., all HMM-based model) approaches required separate components and training for the pronunciation, acoustic and language model. End-to-end models jointly learn all the components of the speech recognizer. This is valuable since it simplifies the training process and deployment process. For example, a n-gram language model is required for all HMM-based systems, and a typical n-gram language model often takes several gigabytes in memory making them impractical to deploy on mobile devices. Consequently, modern commercial ASR systems from Google and Apple (as of 2017) are deployed on the cloud and require a network connection as opposed to the device locally.\n\nThe first attempt of end-to-end ASR was with Connectionist Temporal Classification (CTC) based systems introduced by Alex Graves of Google DeepMind and Navdeep Jaitly of the University of Toronto in 2014. The model consisted of recurrent neural networks and a CTC layer. Jointly, the RNN-CTC model learns the pronunciation and acoustic model together, however it is incapable of learning the language due to conditional independence assumptions similar to a HMM. Consequently, CTC models can directly learn to map speech acoustics to English characters, but the models make many common spelling mistakes and must rely on a separate language model to clean up the transcripts. Later, Baidu expanded on the work with extremely large datasets and demonstrated some commercial success in Chinese Mandarin and English. In 2016, University of Oxford presented LipNet, the first end-to-end sentence-level lip reading model, using spatiotemporal convolutions coupled with an RNN-CTC architecture, surpassing human-level performance in a restricted grammar dataset.\n\nAn alternative approach to CTC-based models are attention-based models. Attention-based ASR models were introduced simultaneously by Chan et al. of Carnegie Mellon University and Google Brain and Bahdanaua et al. of the University of Montreal in 2016. The model named \"Listen, Attend and Spell\" (LAS), literally \"listens\" to the acoustic signal, pays \"attention\" to different parts of the signal and \"spells\" out the transcript one character at a time. Unlike CTC-based models, attention-based models do not have conditional-independence assumptions and can learn all the components of a speech recognizer including the pronunciation, acoustic and language model directly. This means, during deployment, there is no need to carry around a language model making it very practical for deployment onto applications with limited memory. By the end of 2016, the attention-based models have seen considerable success including outperforming the CTC models (with or without an external language model). Various extensions have been proposed since the original LAS model. Latent Sequence Decompositions (LSD) was proposed by Carnegie Mellon University, MIT and Google Brain to directly emit sub-word units which are more natural than English characters; University of Oxford and Google DeepMind extended LAS to \"Watch, Listen, Attend and Spell\" (WLAS) to handle lip reading surpassing human-level performance.\n\nTypically a manual control input, for example by means of a finger control on the steering-wheel, enables the speech recognition system and this is signalled to the driver by an audio prompt. Following the audio prompt, the system has a \"listening window\" during which it may accept a speech input for recognition.\nSimple voice commands may be used to initiate phone calls, select radio stations or play music from a compatible smartphone, MP3 player or music-loaded flash drive. Voice recognition capabilities vary between car make and model. Some of the most recent car models offer natural-language speech recognition in place of a fixed set of commands, allowing the driver to use full sentences and common phrases. With such systems there is, therefore, no need for the user to memorize a set of fixed command words.\n\nIn the health care sector, speech recognition can be implemented in front-end or back-end of the medical documentation process. Front-end speech recognition is where the provider dictates into a speech-recognition engine, the recognized words are displayed as they are spoken, and the dictator is responsible for editing and signing off on the document. Back-end or deferred speech recognition is where the provider dictates into a digital dictation system, the voice is routed through a speech-recognition machine and the recognized draft document is routed along with the original voice file to the editor, where the draft is edited and report finalized. Deferred speech recognition is widely used in the industry currently.\n\nOne of the major issues relating to the use of speech recognition in healthcare is that the American Recovery and Reinvestment Act of 2009 (ARRA) provides for substantial financial benefits to physicians who utilize an EMR according to \"Meaningful Use\" standards. These standards require that a substantial amount of data be maintained by the EMR (now more commonly referred to as an Electronic Health Record or EHR). The use of speech recognition is more naturally suited to the generation of narrative text, as part of a radiology/pathology interpretation, progress note or discharge summary: the ergonomic gains of using speech recognition to enter structured discrete data (e.g., numeric values or codes from a list or a controlled vocabulary) are relatively minimal for people who are sighted and who can operate a keyboard and mouse.\n\nA more significant issue is that most EHRs have not been expressly tailored to take advantage of voice-recognition capabilities. A large part of the clinician's interaction with the EHR involves navigation through the user interface using menus, and tab/button clicks, and is heavily dependent on keyboard and mouse: voice-based navigation provides only modest ergonomic benefits. By contrast, many highly customized systems for radiology or pathology dictation implement voice \"macros\", where the use of certain phrases – e.g., \"normal report\", will automatically fill in a large number of default values and/or generate boilerplate, which will vary with the type of the exam – e.g., a chest X-ray vs. a gastrointestinal contrast series for a radiology system.\n\nAs an alternative to this navigation by hand, cascaded use of speech recognition and information extraction has been studied as a way to fill out a handover form for clinical proofing and sign-off. The results are encouraging, and the paper also opens data, together with the related performance benchmarks and some processing software, to the research and development community for studying clinical documentation and language-processing.\n\nProlonged use of speech recognition software in conjunction with word processors has shown benefits to short-term-memory restrengthening in brain AVM patients who have been treated with resection. Further research needs to be conducted to determine cognitive benefits for individuals whose AVMs have been treated using radiologic techniques.\n\nSubstantial efforts have been devoted in the last decade to the test and evaluation of speech recognition in fighter aircraft. Of particular note have been the US program in speech recognition for the Advanced Fighter Technology Integration (AFTI)/F-16 aircraft (F-16 VISTA), the program in France for Mirage aircraft, and other programs in the UK dealing with a variety of aircraft platforms. In these programs, speech recognizers have been operated successfully in fighter aircraft, with applications including: setting radio frequencies, commanding an autopilot system, setting steer-point coordinates and weapons release parameters, and controlling flight display.\n\nWorking with Swedish pilots flying in the JAS-39 Gripen cockpit, Englund (2004) found recognition deteriorated with increasing g-loads. The report also concluded that adaptation greatly improved the results in all cases and that the introduction of models for breathing was shown to improve recognition scores significantly. Contrary to what might have been expected, no effects of the broken English of the speakers were found. It was evident that spontaneous speech caused problems for the recognizer, as might have been expected. A restricted vocabulary, and above all, a proper syntax, could thus be expected to improve recognition accuracy substantially.\n\nThe Eurofighter Typhoon, currently in service with the UK RAF, employs a speaker-dependent system, requiring each pilot to create a template. The system is not used for any safety-critical or weapon-critical tasks, such as weapon release or lowering of the undercarriage, but is used for a wide range of other cockpit functions. Voice commands are confirmed by visual and/or aural feedback. The system is seen as a major design feature in the reduction of pilot workload, and even allows the pilot to assign targets to his aircraft with two simple voice commands or to any of his wingmen with only five commands.\n\nSpeaker-independent systems are also being developed and are under test for the F35 Lightning II (JSF) and the Alenia Aermacchi M-346 Master lead-in fighter trainer. These systems have produced word accuracy scores in excess of 98%.\n\nThe problems of achieving high recognition accuracy under stress and noise pertain strongly to the helicopter environment as well as to the jet fighter environment. The acoustic noise problem is actually more severe in the helicopter environment, not only because of the high noise levels but also because the helicopter pilot, in general, does not wear a facemask, which would reduce acoustic noise in the microphone. Substantial test and evaluation programs have been carried out in the past decade in speech recognition systems applications in helicopters, notably by the U.S. Army Avionics Research and Development Activity (AVRADA) and by the Royal Aerospace Establishment (RAE) in the UK. Work in France has included speech recognition in the Puma helicopter. There has also been much useful work in Canada. Results have been encouraging, and voice applications have included: control of communication radios, setting of navigation systems, and control of an automated target handover system.\n\nAs in fighter applications, the overriding issue for voice in helicopters is the impact on pilot effectiveness. Encouraging results are reported for the AVRADA tests, although these represent only a feasibility demonstration in a test environment. Much remains to be done both in speech recognition and in overall speech technology in order to consistently achieve performance improvements in operational settings.\n\nTraining for air traffic controllers (ATC) represents an excellent application for speech recognition systems. Many ATC training systems currently require a person to act as a \"pseudo-pilot\", engaging in a voice dialog with the trainee controller, which simulates the dialog that the controller would have to conduct with pilots in a real ATC situation.\nSpeech recognition and synthesis techniques offer the potential to eliminate the need for a person to act as pseudo-pilot, thus reducing training and support personnel. In theory, Air controller tasks are also characterized by highly structured speech as the primary output of the controller, hence reducing the difficulty of the speech recognition task should be possible. In practice, this is rarely the case. The FAA document 7110.65 details the phrases that should be used by air traffic controllers. While this document gives less than 150 examples of such phrases, the number of phrases supported by one of the simulation vendors speech recognition systems is in excess of 500,000.\n\nThe USAF, USMC, US Army, US Navy, and FAA as well as a number of international ATC training organizations such as the Royal Australian Air Force and Civil Aviation Authorities in Italy, Brazil, and Canada are currently using ATC simulators with speech recognition from a number of different vendors.\n\nASR is now commonplace In the field of telephony, and is becoming more widespread in the field of computer gaming and simulation. Despite the high level of integration with word processing in general personal computing. However, ASR in the field of document production has not seen the expected increases in use.\n\nThe improvement of mobile processor speeds has made speech recognition practical in smartphones. Speech is used mostly as a part of a user interface, for creating predefined or custom speech commands. Leading software vendors in this field are: Google, Microsoft Corporation (Microsoft Voice Command), Digital Syphon (Sonic Extractor), LumenVox, Nuance Communications (Nuance Voice Control), Voci Technologies, VoiceBox Technology, Speech Technology Center, Vito Technologies (VITO Voice2Go), Speereo Software (Speereo Voice Translator), Verbyx VRX and SVOX.\n\nFor language learning, speech recognition can be useful for learning a second language. It can teach proper pronunciation, in addition to helping a person develop fluency with their speaking skills.\n\nStudents who are blind (see Blindness and education) or have very low vision can benefit from using the technology to convey words and then hear the computer recite them, as well as use a computer by commanding with their voice, instead of having to look at the screen and keyboard.\n\nStudents who are physically disabled or suffer from Repetitive strain injury/other injuries to the upper extremities can be relieved from having to worry about handwriting, typing, or working with scribe on school assignments by using speech-to-text programs. They can also utilize speech recognition technology to freely enjoy searching the Internet or using a computer at home without having to physically operate a mouse and keyboard.\n\nSpeech recognition can allow students with learning disabilities to become better writers. By saying the words aloud, they can increase the fluidity of their writing, and be alleviated of concerns regarding spelling, punctuation, and other mechanics of writing. Also, see Learning disability.\n\nUse of voice recognition software, in conjunction with a digital audio recorder and a personal computer running word-processing software has proven to be positive for restoring damaged short-term-memory capacity, in stroke and craniotomy individuals.\n\nPeople with disabilities can benefit from speech recognition programs. For individuals that are Deaf or Hard of Hearing, speech recognition software is used to automatically generate a closed-captioning of conversations such as discussions in conference rooms, classroom lectures, and/or religious services.\n\nSpeech recognition is also very useful for people who have difficulty using their hands, ranging from mild repetitive stress injuries to involve disabilities that preclude using conventional computer input devices. In fact, people who used the keyboard a lot and developed RSI became an urgent early market for speech recognition. Speech recognition is used in deaf telephony, such as voicemail to text, relay services, and captioned telephone. Individuals with learning disabilities who have problems with thought-to-paper communication (essentially they think of an idea but it is processed incorrectly causing it to end up differently on paper) can possibly benefit from the software but the technology is not bug proof. Also the whole idea of speak to text can be hard for intellectually disabled person's due to the fact that it is rare that anyone tries to learn the technology to teach the person with the disability.\n\nThis type of technology can help those with dyslexia but other disabilities are still in question. The effectiveness of the product is the problem that is hindering it being effective. Although a kid may be able to say a word depending on how clear they say it the technology may think they are saying another word and input the wrong one. Giving them more work to fix, causing them to have to take more time with fixing the wrong word.\n\n\nThe performance of speech recognition systems is usually evaluated in terms of accuracy and speed. Accuracy is usually rated with word error rate (WER), whereas speed is measured with the real time factor. Other measures of accuracy include Single Word Error Rate (SWER) and Command Success Rate (CSR).\n\nSpeech recognition by machine is a very complex problem, however. Vocalizations vary in terms of accent, pronunciation, articulation, roughness, nasality, pitch, volume, and speed. Speech is distorted by a background noise and echoes, electrical characteristics. Accuracy of speech recognition may vary with the following:\n\nAs mentioned earlier in this article, accuracy of speech recognition may vary depending on the following factors:\ne.g. the 10 digits \"zero\" to \"nine\" can be recognized essentially perfectly, but vocabulary sizes of 200, 5000 or 100000 may have error rates of 3%, 7% or 45% respectively.\ne.g. the 26 letters of the English alphabet are difficult to discriminate because they are confusable words (most notoriously, the E-set: \"B, C, D, E, G, P, T, V, Z\");\nan 8% error rate is considered good for this vocabulary.\nA speaker-dependent system is intended for use by a single speaker.\nA speaker-independent system is intended for use by any speaker (more difficult).\nWith isolated speech, single words are used, therefore it becomes easier to recognize the speech.\nWith discontinuous speech full sentences separated by silence are used, therefore it becomes easier to recognize the speech as well as with isolated speech. \nWith continuous speech naturally spoken sentences are used, therefore it becomes harder to recognize the speech, different from both isolated and discontinuous speech.\ne.g. Querying application may dismiss the hypothesis \"The apple is red.\" \ne.g. Constraints may be semantic; rejecting \"The apple is angry.\" \ne.g. Syntactic; rejecting \"Red is apple the.\" \nConstraints are often represented by a grammar. \nWhen a person reads it's usually in a context that has been previously prepared, but when a person uses spontaneous speech, it is difficult to recognize the speech because of the disfluencies (like \"uh\" and \"um\", false starts, incomplete sentences, stuttering, coughing, and laughter) and limited vocabulary. \nEnvironmental noise (e.g. Noise in a car or a factory) \nAcoustical distortions (e.g. echoes, room acoustics)\nSpeech recognition is a multi-levelled pattern recognition task.\ne.g. Phonemes, Words, Phrases, and Sentences;\ne.g. Known word pronunciations or legal word sequences, which can compensate for errors or uncertainties at lower level;\nBy combining decisions probabilistically at all lower levels, and making more deterministic decisions only at the highest level, speech recognition by a machine is a process broken into several phases. Computationally, it is a problem in which a sound pattern has to be recognized or classified into a category that represents a meaning to a human. Every acoustic signal can be broken in smaller more basic sub-signals. As the more complex sound signal is broken into the smaller sub-sounds, different levels are created, where at the top level we have complex sounds, which are made of simpler sounds on lower level, and going to lower levels even more, we create more basic and shorter and simpler sounds. The lowest level, where the sounds are the most fundamental, a machine would check for simple and more probabilistic rules of what sound should represent. Once these sounds are put together into more complex sound on upper level, a new set of more deterministic rules should predict what new complex sound should represent. The most upper level of a deterministic rule should figure out the meaning of complex expressions. In order to expand our knowledge about speech recognition we need to take into a consideration neural networks. There are four steps of neural network approaches: \nFor telephone speech the sampling rate is 8000 samples per second; \ncomputed every 10 ms, with one 10 ms section called a frame;\n\nAnalysis of four-step neural network approaches can be explained by further information. Sound is produced by air (or some other medium) vibration, which we register by ears, but machines by receivers. Basic sound creates a wave which has two descriptions: amplitude (how strong is it), and frequency (how often it vibrates per second).\n\nThe sound waves can be digitized: Sample a strength at short intervals as in picture above to get a bunch of numbers that approximate at each time step the strength of a wave. The set of these numbers represents an analog wave. This new wave is digital. Sound waves are complicated because they superimpose one on top of each other. This way they create odd-looking waves. For example, if there are two waves that interact with each other we can add them which creates a new odd-looking wave.\n\n\nGiven basic sound blocks that a machine digitized, one has a bunch of numbers which describe a wave and waves describe words. Each frame has a unit block of sound, which are broken into basic sound waves and represented by numbers which, after Fourier Transform, can be statistically evaluated to set to which class of sounds it belongs. The nodes in the figure on a slide represent a feature of a sound in which a feature of a wave from the first layer of nodes to the second layer of nodes based on statistical analysis. This analysis depends on programmer's instructions. At this point, a second layer of nodes represents higher level features of a sound input which is again statistically evaluated to see what class they belong to. Last level of nodes should be output nodes that tell us with high probability what original sound really was.\n\nSpeech recognition can become a means of attack, theft, or accidental operation. For example, activation words like \"Alexa\" spoken in an audio or video broadcast can cause devices in homes and offices to start listening for input inappropriately, or possibly take an unwanted action. Voice-controlled devices are also accessible to visitors to the building, or even those outside the building if they can be heard inside. Attackers may be able to gain access to personal information, like calendar, address book contents, private messages, and documents. They may also be able to impersonate the user to send messages or make online purchases.\n\nTwo attacks have been demonstrated that use artificial sounds. One transmits ultrasound and attempt to send commands without nearby people noticing. The other adds small, inaudible distortions to other speech or music that are specially crafted to confuse the specific speech recognition system into recognizing music as speech, or to make what sounds like one command to a human sound like a different command to the system.\n\nPopular speech recognition conferences held each year or two include SpeechTEK and SpeechTEK Europe, ICASSP, Interspeech/Eurospeech, and the IEEE ASRU. Conferences in the field of natural language processing, such as ACL, NAACL, EMNLP, and HLT, are beginning to include papers on speech processing. Important journals include the IEEE Transactions on Speech and Audio Processing (later renamed IEEE Transactions on Audio, Speech and Language Processing and since Sept 2014 renamed IEEE/ACM Transactions on Audio, Speech and Language Processing—after merging with an ACM publication), Computer Speech and Language, and Speech Communication.\n\nBooks like \"Fundamentals of Speech Recognition\" by Lawrence Rabiner can be useful to acquire basic knowledge but may not be fully up to date (1993). Another good source can be \"Statistical Methods for Speech Recognition\" by Frederick Jelinek and \"Spoken Language Processing (2001)\" by Xuedong Huang etc. More up to date are \"Computer Speech\", by Manfred R. Schroeder, second edition published in 2004, and \"Speech Processing: A Dynamic and Optimization-Oriented Approach\" published in 2003 by Li Deng and Doug O'Shaughnessey. The recently updated textbook of \"Speech and Language Processing (2008)\" by Jurafsky and Martin presents the basics and the state of the art for ASR. Speaker recognition also uses the same features, most of the same front-end processing, and classification techniques as is done in speech recognition. A most recent comprehensive textbook, \"Fundamentals of Speaker Recognition\" is an in depth source for up to date details on the theory and practice. A good insight into the techniques used in the best modern systems can be gained by paying attention to government sponsored evaluations such as those organised by DARPA (the largest speech recognition-related project ongoing as of 2007 is the GALE project, which involves both speech recognition and translation components).\n\nA good and accessible introduction to speech recognition technology and its history is provided by the general audience book \"The Voice in the Machine. Building Computers That Understand Speech\" by Roberto Pieraccini (2012).\n\nThe most recent book on speech recognition is \"Automatic Speech Recognition: A Deep Learning Approach\" (Publisher: Springer) written by D. Yu and L. Deng published near the end of 2014, with highly mathematically-oriented technical detail on how deep learning methods are derived and implemented in modern speech recognition systems based on DNNs and related deep learning methods. A related book, published earlier in 2014, \"Deep Learning: Methods and Applications\" by L. Deng and D. Yu provides a less technical but more methodology-focused overview of DNN-based speech recognition during 2009–2014, placed within the more general context of deep learning applications including not only speech recognition but also image recognition, natural language processing, information retrieval, multimodal processing, and multitask learning.\n\nIn terms of freely available resources, Carnegie Mellon University's Sphinx toolkit is one place to start to both learn about speech recognition and to start experimenting. Another resource (free but copyrighted) is the HTK book (and the accompanying HTK toolkit). For more recent and state-of-the-art techniques, Kaldi toolkit can be used.\n\nA Demo of an on-line speech recognizer is available on Cobalt's webpage.\n\nFor more software resources, see List of speech recognition software.\n\n\n"}
{"id": "4406537", "url": "https://en.wikipedia.org/wiki?curid=4406537", "title": "Sun SPOT", "text": "Sun SPOT\n\nSun SPOT (Sun Small Programmable Object Technology) was a sensor node for a wireless sensor network developed by Sun Microsystems announced in 2007. The device used the IEEE 802.15.4 standard for its networking, and unlike other available sensor nodes, used the Squawk Java virtual machine.\n\nAfter the acquisition of Sun Microsystems by Oracle Corporation, the SunSPOT platform was supported but its forum was shut down in 2012.\n\nThe completely assembled device fit in the palm of a hand.\n\nIts first processor board included an ARM architecture 32 bit CPU with ARM920T core running at 180 MHz. It had 512 KB RAM and 4 MB flash memory. A 2.4 GHz IEEE 802.15.4 radio had an integrated antenna and a USB interface was included.\n\nA sensor board included a three-axis accelerometer (with 2G and 6G range settings), temperature sensor, light sensor, 8 tri-color LEDs, analog and digital inputs, two momentary switches, and 4 high current output pins.\n\nThe unit used a 3.7V rechargeable 750 mAh lithium-ion battery, had a 30 uA deep sleep mode, and battery management provided by software.\n\nThe device's use of Java device drivers is unusual since Java is generally hardware-independent. Sun SPOT uses a small Java ME Squawk which ran directly on the processor without an operating system. Both the Squawk VM and the Sun SPOT code are open source.\nStandard Java development environments such as NetBeans can be used to create SunSPOT applications.\nThe management and deployment of application are handled by ant scripts, which can be called from a development environment, command line, or the tool provided with the SPOT SDK, \"solarium\".\n\nThe nodes communicate using the IEEE 802.15.4 standard including the base-station approach to sensor networking. Protocols such as Zigbee can be built on 802.15.4.\nSun Labs reported implementations of RSA and elliptic curve cryptography (ECC) optimized for small embedded devices.\n\nSun Microsystems Laboratories started research on sensor networks around 2004. After some initial experience using \"Motes\" from Crossbow Technology, a project began under Roger Meike to design an integrated hardware and software system.\nSun sponsored a project at the Art Center College of Design called Autonomous Light Air Vessels in 2005.\nThe first limited-production run of Sun SPOT development kits were released April 2, 2007, after months of delays. This introduction kit included two Sun SPOT demo sensor boards, a Sun SPOT base station, the software development tools, and a USB cable. The software was compatible with Windows XP, Mac OS X 10.4, and common Linux distributions. Some demonstration code was provided.\n\nA developer from Sun gave a demonstration in September 2007.\nAfter investigating commercial use, Sun moved to focus on educational users.\nThe entire project, hardware, operating environment, Java virtual machine, drivers and applications, was available as open source in January 2008.\n\nOracle Corporation acquired Sun Microsystems in 2010 and continued Sun SPOT development, through release 8 of the hardware (with Sun-Oracle logo) by March 2011.\nThe 2011 version included larger memories and a faster processor, but with fewer inputs.\n\nIn 2012 the forum said it would be \"down for maintenance\" until \"mid-June\". A new forum was started on the Oracle Technology Network on May 7, 2013. \nDavid G. Simmons, one of the SunSPOT developers for Sun Microsystems, maintained a blog through the end of 2010.\nHe opened an alternative developers forum in July 2013 not connected to Oracle.\n"}
{"id": "52768392", "url": "https://en.wikipedia.org/wiki?curid=52768392", "title": "Supra-entity", "text": "Supra-entity\n\nA supra-entity is a conceptual element derived from the Entity-Relationship (E-R) technique for information system modeling. It is similar to an entity, but it is defined at a higher level, encompassing individual entity occurrences, their parts, groups and groups of parts or parts of groups.\n\nThe concepts supra-entity, supra-relationship and supra-attribute were created and published by González & Muller in their work “\"Business Entity-Relationship Model: For Innovation, Entrepreneurship and Management \"”, and applied for the first time in this work to conceptually model the key elements and interrelationships of “business” reality.\n\nSupra-entities, supra-relationships and supra-attributes have the objective of covering the diversity of situations and perspectives existing in reality, through the integrated representation at a higher level of individual occurrences, its multiple parts and groups of entities and relationships according to different criteria, its structures, its properties or attributes and its association with other supra-entities at any level of partition or grouping.\n\nA supra-entity is formed by the individual entities, its multiple structures of entity parts, its multiple structures of entity groups and its multiple structures of groups of entity parts or parts of entity groups, associated with each other in a closed squared structure, as shown in the attached diagram.\n\nThe representation on the multiple structures of parts, groups, groups of parts or parts of entity groups that form the supra-entity is done through strips connecting the different elements, instead of the usual line symbol used for a single structure in the entity-relationship models.\n\nFor example, supra-entity \"Products\" in a business sector is as shown in the next diagram:\n\nIn addition to its application to modeling and development of databases and information systems, supra-entities and supra-relationships allow a redefinition of the concept of innovation, its degrees and types, with clarity and amplitude, which facilitates generation of ideas and innovation capacity, both in general and business.\n\nInnovation defined in a supra-entity domain or scope consists of the creation, generation or introduction of new occurrences of supra-entity elements with good results or success, and which are first invention (major innovation) or improved (minor or incremental innovation), not including those new occurrences that replicate to other similar previous ones.\n\nFor example, if we apply this definition of innovation to the supra-entity \"Products\", we obtain the product innovations, which consist of the creation, generation or introduction in the market of new element occurrences of the supra-entity \"Products\" with good results or success, and which are first invention (major innovation) or improved (minor or incremental innovation), not including those new occurrences that replicate to other similar previous ones.\nBusiness innovation is traditionally identified with the introduction and marketing of new or improved products or processes in the market. But innovation does not necessarily require being technological. You can innovate by identifying new needs, new customer segments, new ways of getting the product to the customer, new forms of agreements or contracting, new environments, etc.\n\nIn summary, it is possible to innovate on the different supra-entities and supra-relationships that constitute the Business Entity-Relationship Model, achieving different types of innovations and increasing the innovation capacity of the business.\n\n\n"}
{"id": "14695473", "url": "https://en.wikipedia.org/wiki?curid=14695473", "title": "Tagged architecture", "text": "Tagged architecture\n\nIn computer science, a tagged architecture is a particular type of computer architecture where every word of memory constitutes a tagged union, being divided into a number of bits of data, and a \"tag\" section that describes the type of the data: how it is to be interpreted, and, if it is a reference, the type of the object that it points to.\n\nTwo notable series of American tagged architectures were the Lisp machines, which had tagged pointer support at the hardware and opcode level, and the Burroughs large systems, which had a data-driven tagged and descriptor-based architecture. Another \"exemplary\" instance was the architecture of the Rice Computer. Both the Burroughs and Lisp machine were examples of high-level language computer architectures, where the tagging was used to support types from a high-level language at the hardware level.\n\nIn addition to this, the original Xerox Smalltalk implementation used the least-significant bit of each 16-bit word as a tag bit: if it was clear then the hardware would accept it as an aligned memory address while if it was set it was treated as a (shifted) 15-bit integer. Current Intel documentation mentions that the lower bits of a memory address might be similarly used by some interpreter-based systems.\n\nIn the Soviet Union, the Elbrus series of supercomputers pioneered the use of tagged architectures in 1973.\n\n"}
{"id": "30034086", "url": "https://en.wikipedia.org/wiki?curid=30034086", "title": "Technologies in Minority Report", "text": "Technologies in Minority Report\n\nThe 2002 science fiction neo-noir film \"Minority Report\" featured numerous fictional future technologies, which have proven prescient based on developments around the world. Before the film's production began, director Steven Spielberg invited fifteen experts to think about technologies that would be developed by 2054, the setting of the film.\n\nAfter \"E.T.\", Spielberg started to consult experts, and put more scientific research into his films. In 1999, he invited fifteen experts convened by the Global Business Network, its chairman, Peter Schwartz, and its co-founder Stewart Brand to a hotel in Santa Monica, California for a three-day \"think tank\". He also invited journalist Joel Garreau to cover the event. He wanted to consult with the group to create a plausible \"future reality\" for the year 2054 as opposed to a more traditional \"science fiction\" setting. Dubbed the \"think tank summit\", the experts included architect Peter Calthorpe, Douglas Coupland, computer scientist Neil Gershenfeld, biomedical researcher Shaun Jones, computer scientist Jaron Lanier, and former Massachusetts Institute of Technology (MIT) architecture dean William J. Mitchell. Production Designer Alex McDowell kept what was nicknamed the \"2054 bible\", an 80-page guide created in preproduction which listed all the decided upon aspects of the future world: architectural, socio-economical, political, and technological. While the discussions did not change key elements in the film's action sequences, they were influential in the creation of some of the more utopian aspects of the film, though John Underkoffler, the science and technology advisor for the film, described it as \"much grayer and more ambiguous\" than what was envisioned in 1999. John Underkoffler, who designed most of Anderton's interface after Spielberg told him to make it like \"conducting an orchestra,\" said \"it would be hard to identify anything [in the movie] that had no grounding in reality.\" For example, Underkoffler conscientiously treated his cinematic representation of the gestural interface as an actual prototype, “We worked so hard to make the gestural interface in the film real. I really did approach the project as if it were an R&D thing.” McDowell teamed up with architect Greg Lynn to work on some of the technical aspects of the production design. McDowell said that \"[a] lot of those things Alex cooked up for Minority Report, like the 3-D screens, have become real.\"\n\nProduct placement was used to depict the predicted lack of privacy and excessive publicity in a future society. The advertisements in \"Minority Report\" were handled by Jeff Boortz of Concrete Pictures, who said \"the whole idea, from a script point of view, was that the advertisements would recognize you -- not only recognize you, but recognize your state of mind. It's the kind of stuff that's going on now with digital set-top boxes and the Internet.\" Nokia designed the phones used by the characters, and Lexus paid the producers $5 million for the rights to design the futuristic \"Mag-Lev\" cars. All told money raised through product placement accounted for $25 million of the film's $102 million production budget.\n\nSpielberg described his ideas for the film's technology to Roger Ebert before the film's release:\n\nNews sources have noted the future technologies depicted in the film were prescient. \"The Guardian\" published a piece titled \"Why Minority Report was spot on\" in June 2010, and the following month \"Fast Company\" examined seven crime fighting technologies in the film similar to ones then currently appearing. It summarized that \"the police state imagined in the Tom Cruise flick feels a bit more real every day.\" Other major media outlets such as the \"Wall Street Journal\" have published articles dedicated to this phenomenon, and National Public Radio (NPR) published an August 2010 podcast which analyzed the film's accuracy in predicting future technologies.\n\nMulti-touch interfaces, similar to Anderton's, have been put out by Microsoft (2007), Obscura Digital (2008), MIT (2009), Intel (2009), and Microsoft again, this time for their Xbox 360 (2010). A company representative, at the 2007 premiere of the then Microsoft Surface (later renamed to Microsoft PixelSense,) promised it \"will feel like Minority Report,\" and when Microsoft released the Kinect motion sensing camera add-on for their Xbox 360 gaming console in 2010, the Kinect's technology allowed several programmers, including students at MIT, to create \"Minority Report\" inspired user interfaces.\n\nRetina scanners, a form of biometrics, already existed by the time the film appeared in theaters, but media outlets have described the new advanced systems such as those by a Manhattan company named Global Rainmakers Incorporated (GRI) (2010), as similar to the movie's. GRI disputed the notion that its technology could be the threat to privacy it is in the film. \"Minority Report is one possible outcome,\" a corporate official told \"Fast Company\". \"I don't think that's our company's aim, but I think what we're going to see is an environment well beyond what you see in that movie--minus the precogs, of course.\" The company is installing hundreds of the scanners in Bank of America locations in Charlotte, North Carolina, and has a contract to install them on several United States Air Force bases.\nIt still can't work as fast as it is in the movie, but with the progress of computer technology, it is predictable to reach that level before 2054.\n\nCompanies like Hewlett-Packard (HP) have announced they were motivated to do research by the film, in HP's case to develop cloud computing.\n\nIn the film, Anderton uses vehicles which can be both driven manually and autonomously; in one scene the police remotely override the vehicle in order to bring him into custody. Spielberg commissioned Lexus to design a car specifically for Minority Report, resulting in the Lexus 2054, a fuel-cell powered autonomous vehicle which is seen being built in an automated factory in the film. Autonomous, or self-driving cars have been in development since 1984. As Artificial Intelligence and ground-sensing technologies like LIDAR began to improve in the 2000's, major automotive manufacturers such as Ford, Nissan and General Motors began developing self-driving prototypes. Google began developing a self-driving vehicle prototype, named Waymo in 2009 while Tesla Motors rolled out the autopilot feature on their Model S vehicle in 2015. In 2011, the State of Nevada became the first jurisdiction in the world to formally legalise autonomous vehicles on public roads. Countries such as the United Kingdom (2013) France (2014), Switzerland (2015) and Singapore (2016) passed laws which allowed the testing of autonomous vehicles on public roads, with a view to further changes in legislation as the technologies improves.\n\nInsect robots similar to the film's spider robots are being developed by the United States Military. These insects will be capable of reconnaissance missions in dangerous areas not fit for soldiers, such as \"occupied houses\". They serve the same purpose in the film. According to the developer, BAE Systems, the \"goal is to develop technologies that will give our soldiers another set of eyes and ears for use in urban environments and complex terrain; places where they cannot go or where it would be too dangerous.\"\n\nMultiple gesture recognition technologies currently in existence or under development have been compared to the one in the film.\n\nMost of the advertising to consumers in \"Minority Report\" occurs when they are out of their homes. The advertisements interact in various ways; an Aquafina splashes water on its customers, Guinness recommends its products to the downtrodden to recover from \"a hard day at work\", a cereal box from which Anderton eats has a video advertisement, and when Anderton is fleeing the PreCrime force, an American Express advertisement observes, \"It looks like you need an escape, and Blue can take you there,\" and a Lexus ad says, \"A road diverges in the desert. Lexus. The road you're on, John Anderton, is the one less traveled.\" The advertisements in \"Minority Report\" were handled by Jeff Boortz of Concrete Pictures, who said \"the whole idea, from a script point of view, was that the advertisements would recognize you—not only recognize you, but recognize your state of mind. It's the kind of stuff that's going on now with digital set-top boxes and the Internet.\"\n\nAlthough the advertising-oriented website ClickZ called the film's interactive advertisements \"a bit farfetched\" in 2002, billboards capable of facial recognition are being developed by the Japanese company NEC. These billboards will theoretically be able to recognize passers-by via facial recognition, call them by name, and deliver customer specific advertisements. Thus far, the billboards can recognize age and gender and deliver demographically appropriate advertisements, but cannot discern individuals. According to \"The Daily Telegraph\", the billboards will \"behave like those in...\"Minority Report\",\" uniquely identifying and communicating to those in their vicinities. IBM is developing similar billboards, with plans to deliver customized advertisements to individuals who carry identity tags. Like NEC, the company feels they will not be obtrusive as their billboards will only advertise products in which a customer is interested. Advertisers are embracing these billboards as they attempt to reduce costs by wasting fewer advertisements on uninterested consumers.\n\nCrime prediction software, developed by a professor from the University of Pennsylvania (2010). The software, which was detailed in a \"Daily Mail\" article entitled \"The real Minority Report\" upon its announcement, \"collates a range of variables then uses an algorithm to work out who is at the highest chance of offending.\" As in the film, the program was announced for a trial run in Washington D.C., which, if successful, will lead to a national rollout. According to \"Fast Company\", \"IBM's new Blue Crime Reduction Utilizing Statistical History (CRUSH) programs feels almost directly inspired by Minority Report. Similar to the \"precogs,\" IBM's new system uses \"predictive analytics,\" mining years and years of incident reports and law enforcement data to \"forecast criminal 'hot spots.'\" Police in Memphis have already had great success with the $11-billion \"precrime\" predicting tool: Since installing Blue CRUSH, the city has seen a 31% drop in serious crime.\"\n\nXerox has been trying to develop something similar to e-paper since before the film was released in theaters. Electronic paper has been announced as being developed by MIT (2005), Germany (2006), media conglomerate Hearst Corporation (2008), and LG; a South Korean electronics manufacturer (2010). In 2005, when the \"Washington Post\" asked the chief executive of MIT's spin-off handling their research when \"the \"Minority Report\" newspaper\" would be released, he predicted \"around 2015.\" Tech Watch's 2008 article \"‘Minority Report’ e-newspaper on the way\" noted that Hearst was \"pushing large amounts of cash into\" the technology.\n\nSpielberg decided to add the jetpacks worn by the policemen as a tribute to old science-fiction serials such as Commando Cody, even though the scientists considered them unrealistic. The jet packs are the only future technology depicted in the film which originated in science fiction. They already exist and perhaps their most famous flights occurred in the pregame ceremonies before Super Bowl I in 1967 and in Los Angeles in the 1984 Summer Olympics during that event's ceremonies. They have been held back as there is currently no way to mitigate their dangers to the operator and have extremely limited range.\n\n"}
{"id": "10181468", "url": "https://en.wikipedia.org/wiki?curid=10181468", "title": "Technology aware design", "text": "Technology aware design\n\nTechnology Aware Design (TAD) is a research program that started in 2001 at imec, Leuven, Belgium. It anticipates the end of the traditional \"happy scaling\" paradigm, where CMOS technology and CMOS design evolved on formally separate tracks, the interface between the two being standard cell, or SPICE compact models (see transistor models for circuit design).\n\nToday, both sides (design and technology) are confronted with the need to understand the other in order to overcome new scaling induced issues. The TAD program pursues analysis and solutions for these scaling induced problems.\n"}
{"id": "4448919", "url": "https://en.wikipedia.org/wiki?curid=4448919", "title": "Thumb compass", "text": "Thumb compass\n\nA thumb compass is a type of compass commonly used in orienteering, a sport in which map reading and terrain association are paramount. In cases of homogenous terrain with few distinct features, a bearing between 2 known points on the map may be used. Consequently, most thumb compasses have minimal or no degree markings at all, and are normally used only to take bearings directly from a map, and to orient a map to magnetic north. Thumb compasses are also often transparent so that an orienteer can hold a map in the hand with the compass and see the map through the compass.\n\nThumb compasses attach to one's thumb using a small elastic band. \n\nThe first commercially successful orienteering thumb compass was the Norcompass, introduced by Suunto in 1983.\n\n\"The Gear Junkie,\" a syndicated newspaper columnist in the United States, has an in-depth review of thumb compasses.\n\nPlacing an even greater emphasis on speed over accuracy, the wrist compass lacks even a baseplate, consisting solely of a needle capsule strapped to the carpometacarpal joint at the base of the thumb; the thumb serves the function of a baseplate when taking and sighting bearings. It is often used for city and park race orienteering.\n\n\n"}
{"id": "30958", "url": "https://en.wikipedia.org/wiki?curid=30958", "title": "Time-sharing", "text": "Time-sharing\n\nIn computing, time-sharing is the sharing of a computing resource among many users by means of multiprogramming and multi-tasking at the same time.\n\nIts introduction in the 1960s and emergence as the prominent model of computing in the 1970s represented a major technological shift in the history of computing.\n\nBy allowing a large number of users to interact concurrently with a single computer, time-sharing dramatically lowered the cost of providing computing capability, made it possible for individuals and organizations to use a computer without owning one, and promoted the interactive use of computers and the development of new interactive applications.\n\nThe earliest computers were extremely expensive devices, and very slow in comparison to later models. Machines were typically dedicated to a particular set of tasks and operated by control panels, the operator manually entering small programs via switches in order to load and run a series of programs. These programs might take hours, or even weeks, to run. As computers grew in speed, run times dropped, and soon the time taken to start up the next program became a concern. Batch processing methodologies evolved to decrease these \"dead periods\" by queuing up programs so that as soon as one program completed, the next would start.\n\nTo support a batch processing operation, a number of comparatively inexpensive card punch or paper tape writers were used by programmers to write their programs \"offline\". When typing (or punching) was complete, the programs were submitted to the operations team, which scheduled them to be run. Important programs were started quickly; how long before less important programs were started was unpredictable. When the program run was finally completed, the output (generally printed) was returned to the programmer. The complete process might take days, during which time the programmer might never see the computer.\n\nThe alternative of allowing the user to operate the computer directly was generally far too expensive to consider. This was because users might have long periods of entering code while the computer remained idle. This situation limited interactive development to those organizations that could afford to waste computing cycles: large universities for the most part. Programmers at the universities decried the behaviors that batch processing imposed, to the point that Stanford students made a short film humorously critiquing it. They experimented with new ways to interact directly with the computer, a field today known as human–computer interaction.\n\nTime-sharing was developed out of the realization that while any single user would make inefficient use of a computer, a large group of users together would not. This was due to the pattern of interaction: Typically an individual user entered bursts of information followed by long pauses but a group of users working at the \"same time\" would mean that the pauses of one user would be filled by the activity of the others. Given an optimal group size, the overall process could be very efficient. Similarly, small slices of time spent waiting for disk, tape, or network input could be granted to other users.\n\nThe concept is claimed to have been first described by John Backus in the 1954 summer session at MIT, and later by Bob Bemer in his 1957 article \"How to consider a computer\" in \"Automatic Control Magazine\". In a paper published in December 1958 by W. F. Bauer, he wrote that \"The computers would handle a number of problems concurrently. Organizations would have input-output equipment installed on their own premises and would buy time on the computer much the same way that the average household buys power and water from utility companies.\"\n\nImplementing a system able to take advantage of this was initially difficult. Batch processing was effectively a methodological development on top of the earliest systems. Since computers still ran single programs for single users at any time, the primary change with batch processing was the time delay between one program and the next. Developing a system that supported multiple users at the same time was a completely different concept. The \"state\" of each user and their programs would have to be kept in the machine, and then switched between quickly. This would take up computer cycles, and on the slow machines of the era this was a concern. However, as computers rapidly improved in speed, and especially in size of core memory in which users' states were retained, the overhead of time-sharing continually decreased, relatively speaking.\nThe first project to implement time-sharing of user programs was initiated by John McCarthy at MIT in 1959, initially planned on a modified IBM 704, and later on an additionally modified IBM 709 (one of the first computers powerful enough for time-sharing). One of the deliverables of the project, known as the \"Compatible Time-Sharing System\" or CTSS, was demonstrated in November 1961. CTSS has a good claim to be the first time-sharing system and remained in use until 1973. Another contender for the first demonstrated time-sharing system was PLATO II, created by Donald Bitzer at a public demonstration at Robert Allerton Park near the University of Illinois in early 1961. But this was a special purpose system. Bitzer has long said that the PLATO project would have gotten the patent on time-sharing if only the University of Illinois had not lost the patent for 2 years. JOSS began time-sharing service in January 1964.\n\nThe first commercially successful time-sharing system was the Dartmouth Time Sharing System.\n\nThroughout the late 1960s and the 1970s, computer terminals were multiplexed onto large institutional mainframe computers (centralized computing systems), which in many implementations sequentially polled the terminals to see whether any additional data was available or action was requested by the computer user. Later technology in interconnections were interrupt driven, and some of these used parallel data transfer technologies such as the IEEE 488 standard. Generally, computer terminals were utilized on college properties in much the same places as \"desktop computers\" or \"personal computers\" are found today. In the earliest days of personal computers, many were in fact used as particularly smart terminals for time-sharing systems.\n\nThe Dartmouth Time Sharing System's creators wrote in 1968 that \"any response time which averages more than 10 seconds destroys the illusion of having one's own computer\". Conversely, timesharing users thought that their terminal was the computer.\n\nWith the rise of microcomputing in the early 1980s, time-sharing became less significant, because individual microprocessors were sufficiently inexpensive that a single person could have all the CPU time dedicated solely to their needs, even when idle.\n\nHowever, the Internet brought the general concept of time-sharing back into popularity. Expensive corporate server farms costing millions can host thousands of customers all sharing the same common resources. As with the early serial terminals, web sites operate primarily in bursts of activity followed by periods of idle time. This bursting nature permits the service to be used by many customers at once, usually with no perceptible communication delays, unless the servers start to get very busy.\n\nGenesis\n\nIn the 1960s, several companies started providing time-sharing services as service bureaus. Early systems used Teletype Model 33 KSR or ASR or Teletype Model 35 KSR or ASR machines in ASCII environments, and IBM Selectric typewriter-based terminals (especially the IBM 2741) with two different seven-bit codes. They would connect to the central computer by dial-up Bell 103A modem or acoustically coupled modems operating at 10–15 characters per second. Later terminals and modems supported 30–120 characters per second. The time-sharing system would provide a complete operating environment, including a variety of programming language processors, various software packages, file storage, bulk printing, and off-line storage. Users were charged rent for the terminal, a charge for hours of connect time, a charge for seconds of CPU time, and a charge for kilobyte-months of disk storage.\n\nCommon systems used for time-sharing included the SDS 940, the PDP-10, and the IBM 360. Companies providing this service included GE's GEISCO, IBM subsidiary The Service Bureau Corporation, Tymshare (founded in 1966), National CSS (founded in 1967 and bought by Dun & Bradstreet in 1979), Dial Data (bought by Tymshare in 1968), Bolt, Beranek, and Newman (BBN) and Time Sharing Ltd. in the U.K.. By 1968, there were 32 such service bureaus serving the US National Institutes of Health (NIH) alone. The \"Auerbach Guide to Timesharing\" (1973) lists 125 different timesharing services using equipment from Burroughs, CDC, DEC, HP, Honeywell, IBM, RCA, Univac, and XDS.\n\nIn 1975, it was said about one of the major super-mini computer manufacturers that \"The biggest end-user market currently is time-sharing.\" For DEC, for a while the second largest computer company (after IBM), this was also true: Their PDP-10 and IBM's 360/67 were widely used by commercial timesharing services such as CompuServe, On-Line Systems (OLS), Rapidata and Time Sharing Ltd.\n\nAlthough many time-sharing services simply closed, Rapidata held on, and became part of National Data Corporation. It was still of sufficient interest in 1982 to be the focus of \"A User's Guide to Statistics Programs: The Rapidata Timesharing System\". Even as revenue fell by 66% and National Data subsequently developed its own problems, attempts were made to keep this timesharing business going.\n\n\nBeginning in 1964, the Multics operating system was designed as a computing utility, modeled on the electrical or telephone utilities. In the 1970s, Ted Nelson's original \"Xanadu\" hypertext repository was envisioned as such a service. It seemed as the computer industry grew that no such consolidation of computing resources would occur as timesharing systems. In the 1990s the concept was, however, revived in somewhat modified form under the banner of cloud computing.\n\nTime-sharing was the first time that multiple processes, owned by different users, were running on a single machine, and these processes could interfere with one another. For example, one process might alter shared resources which another process relied on, such as a variable stored in memory. When only one user was using the system, this would result in possibly wrong output - but with multiple users, this might mean that other users got to see information they were not meant to see.\n\nTo prevent this from happening, an operating system needed to enforce a set of policies that determined which privileges each process had. For example, the operating system might deny access to a certain variable by a certain process.\n\nThe first international conference on computer security in London in 1971 was primarily driven by the time-sharing industry and its customers.\n\nSignificant early timesharing systems:\n\n\n\n"}
{"id": "35564703", "url": "https://en.wikipedia.org/wiki?curid=35564703", "title": "Vision Research (company)", "text": "Vision Research (company)\n\nVision Research is an international company that manufactures high-speed digital cameras based in Wayne, New Jersey. Their cameras are marketed under the Phantom brand, and are used in a broad variety of industries including: defense, industrial product development, manufacturing, automotive, scientific research, and entertainment. Vision Research is a business unit of the Materials Analysis Division of Ametek Inc., a global manufacturer of electronic instruments and electromechanical devices.\n\nFounded as Photographic Analysis Company in 1950, they specialized in high-speed photographic research utilizing film cameras. They applied high speed photography to numerous industries and applications, and designed, manufactured and marketed products specific to the high speed photographic needs of each industry.\n\nIn 1992, Photographic Analysis Company shifted focus to designing and fabricating high speed electronic imagers that did not rely on photographic film for imaging. They formed a separate entity for developing digital high-speed imaging systems known as Vision Research Inc. Their ultra slow motion digital video line of cameras are currently marketed under the Phantom trade mark and was granted U.S. Patent #5,625,412 for high speed electronic digital imaging.\n\nThe Phantom camera family consists of three camera lines and accessories.\nThe Miro line of small, lightweight cameras is used for mobile applications or situations where size and weight might be an issue in applications such as assembly line analysis, drop testing, particle image velocimetry (PIV), animal studies, bio-mechanical studies and automotive testing.\nv-Series cameras are a broad line of high performance cameras used when applications demand the highest imaging speeds and/or resolution. The v-Series is used in applications such as ballistics testing, military research, engine development, high-speed x-ray video, and medical research.\n\nThe third line of cameras are targeted at TV & Motion Picture production for live event broadcasts and new media applications. The Phantom HD, 65 and Flex cameras are extensively used to capture detail in digital cinema and HD television productions and have won an Academy Scientific and Technical Award in 2012 and a Technology & Engineering Emmy Award in 2010. Two Phantom 65 cores are used for IMAX's 3D Digital camera.\nVision Research also does custom engineering and specialty cameras for unique applications.\n\n\n"}
{"id": "4981782", "url": "https://en.wikipedia.org/wiki?curid=4981782", "title": "Waders (footwear)", "text": "Waders (footwear)\n\nWaders refers to a waterproof boot extending from the foot to the chest, traditionally made from vulcanised rubber, but available in more modern PVC, neoprene and Gore-Tex variants. Waders are generally distinguished from counterpart waterproof boots by shaft height; the hip boot extending to the thigh and the Wellington boot to the knee. They are therefore sometimes referred to as chest waders for emphasis. Waders are available with boots attached or can have attached stocking feet (usually made of the wader material), to wear inside boots.\n\nThe first manufactured waders were made as early as the 1850s by a company called Hodgman. When rubber became popular around 1912, they started making the waders out of this particularly waterproof and durable material.\nThen rubber was more or less perfected in 1942 for World War II, so they used the same technology to make waders that are closer to what we have today.\n\nThere are two main types of waders: stocking-foot and boot-foot. Stocking-foot is separate from the boot and connects to it, while boot-foot includes the boot already.\n\nWaders have a wide range of applications. Regarding leisure purposes, they are worn while angling, water gardening, playing with model boats, waterfowl hunting, and off-road riding of All-terrain vehicles. Industrially, heavy-duty waders are used by predominantly in the chemical industry, agriculture and in the maintenance of water supply, sewerage and other utilities. Waders are frequently worn by pastors during full immersion baptism and have an important application during flooding, while walking along the streets or indoor.\n\nTrench foot is common in those who spend a lot of time in the water without proper protection. People like fly fishermen use waders because they stay in the water for hours on end, and they need the proper protection.\n\nDepending on the kind of fish that the fisherman is catching, they might not need waders. Some fish are best caught on land. But some fish are best caught when the fisherman is soaked and chest deep in the water.\nWaders are also essential for keeping warm during colder months, because they keep the cold water off the skin, which otherwise could cause hypothermia or other problems. However many fishermen use them even in the summer to keep dry, but waders can get hot so sometimes men wear nothing but their boxers under chest waders.\n\nMany states in the US are beginning to ban certain types of waders, specifically those with porous, felt soles. These kinds of soles easily host various types of invasive species that could be carried from one water source to another. The invasive organisms and plants pose a threat to fish stocks and important fish habitats. For example, effective March 1, 2012, most counties in Missouri ban these kinds of waders while sport fishing in fresh water. And in all of Alaska, as of January 1, 2012, the same law applies. In New Zealand, the use of felt-soled waders and boots for sports fishing was banned in 2008 as part of the containment measures put in place following the discovery of the invasive alga, didymo, in South Island rivers in 2004.\n"}
