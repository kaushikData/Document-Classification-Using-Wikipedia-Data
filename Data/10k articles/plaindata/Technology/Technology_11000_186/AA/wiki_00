{"id": "2237220", "url": "https://en.wikipedia.org/wiki?curid=2237220", "title": "ADDIE Model", "text": "ADDIE Model\n\nADDIE is an instructional systems design (ISD) framework that many instructional designers and training developers use to develop courses. The name is an acronym for the five phases it defines for building training and performance support tools:\nMost current ISD models are variations of the ADDIE process. Other models include the Dick and Carey and Kemp ISD models. Rapid prototyping is another common alternative.\n\nInstructional theories are important in instructional materials design. These include behaviorism, constructivism, social learning, and cognitivism.\n\nFlorida State University initially developed the ADDIE framework to explain, “...the processes involved in the formulation of an instructional systems development (ISD) program for military interservice training that will adequately train individuals to do a particular job and which can also be applied to any interservice curriculum development activity.” The model originally contained several steps under its five original phases (analyze, design, develop, implement, and evaluate). The idea was to complete each phase before moving to the next. Subsequent practitioners revised the steps, and eventually the model became more dynamic and interactive than the original hierarchical version. By the mid-1980s, the version familiar today appeared.\n\nThe origin of the label itself is obscure, but the underlying ISD concepts come from a model developed for the U.S. armed forces in the mid 1970s. As Branson (1978) recounts, the Center for Educational Technology at Florida State University worked with a branch of the U.S. Army to develop a model, which evolved into the Interservice Procedures for Instructional Systems Development (IPISD), intended for the Army, Navy, Air Force, and Marine Corps. Branson provides a graphic overview of the IPISD, which shows five top-level headings: analyze, design, develop, implement, and control. Virtually all subsequent historical reviews of ID reference this model but, notably, users do not refer to it by the ADDIC acronym. The authors and users refer only to IPISD. Hence, it is clearly not the source of the ADDIE acronym.\n\nThe analysis phase clarifies the instructional problems and objectives, and identifies the learning environment and learner's existing knowledge and skills. Questions the analysis phase addresses include:\n\nThe process of asking these questions is often part of a needs analysis. During the needs analysis instructional designers (IDs) will determine constraints and resources in order to fine tune their plan of action. \n\nThe design phase deals with learning objectives, assessment instruments, exercises, content, subject matter analysis, lesson planning, and media selection. The design phase should be systematic and specific. \"Systematic\" means a logical, orderly method that identifies, develops, and evaluates a set of planned strategies for attaining project goals. \"Specific\" means the team must execute each element of the instructional design plan with attention to detail. The design phase may involve writing a \"design document/design proposal\" or \"concept and structure note\" to aid final development.\n\nIn the development phase, instructional designers and developers create and assemble content assets described in the design phase. If e-learning is involved, programmers develop or integrate technologies. Designers create storyboards. Testers debug materials and procedures. The team reviews and revises the project according to feedback.\n\nThe implementation phase develops procedures for training facilitators and learners. Training facilitators cover the course curriculum, learning outcomes, method of delivery, and testing procedures. Preparation for learners includes training them on new tools (software or hardware) and student registration. Implementation includes evaluation of the design.\n\nThe evaluation phase consists of two aspects: formative and summative. Formative evaluation is present in each stage of the ADDIE process, while summative evaluation is conducted on finished instructional programs or products. Donald Kirkpatrick's Four Levels of Learning Evaluation are often utilized during this phase of the ADDIE process.\n\nSome institutions have modified the ADDIE model to meet specific needs. For example, the United States Navy created a version they call PADDIE+M. The P phase is the planning phase, which develops project goals, project objectives, budget, and schedules. The M phase is the maintenance phase, which implements life cycle maintenance with continuous improvement methods. This model is gaining acceptance in the United States government as a more complete model of ADDIE. Some organizations have adopted the PADDIE model without the M phase. Pavlis Korres (2010), in her instructional model (ESG Framework), has proposed an expanded version of ADDIE, named ADDIE+M, where Μ=Maintenance of the Learning Community Network after the end of a course. The Maintenance of the Learning Community Network is a modern educational process that supports the continuous educational development of its members with social media and web tools.\n\n"}
{"id": "47580669", "url": "https://en.wikipedia.org/wiki?curid=47580669", "title": "Aurora (novel)", "text": "Aurora (novel)\n\nAurora is a 2015 novel by American science fiction author Kim Stanley Robinson. The novel concerns a generation ship traveling to Tau Ceti in order to begin a human colony. The novel's primary narrating voice is the starship's artificial intelligence. The novel was well received by critics.\n\nA generation ship is launched from Saturn in 2545. It includes twenty-four self-contained biomes and an average population of two thousand people. One hundred sixty years and approximately seven generations later, it is beginning its approach to the Tau Ceti system to begin colonization of a planet's moon, an Earth analog, which has been named Aurora.\n\nDevi, the ship's \"de facto\" chief engineer and leader, is concerned about the decaying infrastructure and biology of the ship: systems are breaking down, each generation has lower intelligence test scores than the last, and bacteria are mutating and evolving at a faster rate than humans. She tells the ship's AI, referred to simply as Ship, to keep a narrative of the voyage. After having some trouble with understanding the human concept of narrative, Ship eventually elects to follow the life of Devi's daughter Freya as a protagonist.\n\nAs a teenager, Freya travels around the ship on her \"wanderjahr\" and learns that many of the ship's inhabitants are dissatisfied with their enclosed existence and what they perceive as a dictatorship. Movement is strictly limited for most people, reproduction is tightly controlled, and education in science and mathematics is mandatory. Freya's \"wanderjahr\" comes to an end when she is called home as Devi grows sick from cancer and dies.\n\nThe ship arrives in the Tau Ceti system and begins to settle Aurora, a moon of Tau Ceti e. It soon becomes apparent that extraterrestrial life is present in the form of primitive prions, which infect and kill most of the landing party. The surviving settlers attempt to return to the ship, and some of those remaining onboard kill them in the airlock to maintain quarantine, leading to a violent political schism throughout the ship. The ship itself, which has been moving towards self-awareness, takes physical control of the situation by lowering oxygen levels and separating warring factions, referring to itself as \"the rule of law\". It then reveals to the crew that there were in fact two ships originally launched for the Tau Ceti expedition, but the other was destroyed during a period of severe civil unrest, and the collective memory of that event was erased from the history records. Under Ship's moderation, a more peaceful debate takes place between the inhabitants about what to do now that Aurora is known to be inhospitable. Unable to reach consensus, the factions agree to part ways, with those who wish to stay retaining as many resources as can be spared to pursue an unlikely attempt at terraforming the Mars-like planet Iris, while the other group, led by Freya, opt to try and return to Earth. Freya and the others who return do not receive any communication from those who remained in the Tau Ceti system.\n\nOn the voyage back to Earth, the ship's biomes continue to deteriorate as bacteria flourish and crops fail. The humans soon face famine and experiment with an untested form of cryogenic freezing, which is largely successful. Upon returning to the Solar system, Ship is forced to decelerate by means of gravity assist between various planets, a process which takes twelve years. During this time, with the full communications data of humanity available to it, it learns more about why it was launched in the first place—simply for expansionism—and denounces its builders as \"criminally negligent narcissists\". Ship manages to safely drop its humans off on a pass of Earth but fails to make a final gravity slowdown past the Sun. Ship is destroyed along with the last survivor of the landing on Aurora.\n\nFreya and the other \"starfarers\" have trouble adjusting to life on Earth, especially with many Terrans hostile to them for a perceived sense of ingratitude and cowardice. At a space colonization conference, a speaker says humanity will continue to send ships into interstellar space no matter how many fail and die, and Freya assaults him. Eventually she joins a group of terraformers who are attempting to restore the Earth's beaches after their loss during previous centuries' sea level rise. While swimming and surfing, she begins to come to terms with life on Earth.\n\nMajor themes in \"Aurora\" include complexities of life aboard a multi-generational starship, interpersonal psychology, artificial intelligence, human migration, and the feasibility of star travel.\n\nRobinson says that in researching the novel he met with his friend Christopher McKay who has helped him since the \"Mars Trilogy\". McKay arranged lunches at the NASA Ames Research Center where Robinson asked questions of NASA employees.\n"}
{"id": "47490", "url": "https://en.wikipedia.org/wiki?curid=47490", "title": "Biodegradation", "text": "Biodegradation\n\nBiodegradation is the breakdown of organic matter by microorganisms, such as bacteria, fungi.\n\nThe process of biodegradation can be divided into three stages: biodeterioration, biofragmentation, and assimilation. Biodeterioration is a surface-level degradation that modifies the mechanical, physical, and chemical properties of the material. This stage occurs when the material is exposed to abiotic factors in the outdoor environment and allows for further degradation by weakening the material's structure. Some abiotic factors that influence these initial changes are compression (mechanical), light, temperature, and chemicals in the environment. While biodeterioration typically occurs as the first stage of biodegradation, it can in some cases be parallel to biofragmentation.\n\nBiofragmentation of a polymer is the lytic process in which bonds within a polymer are cleaved, generating oligomers and monomers in its place. The steps taken to fragment these materials also differ based on the presence of oxygen in the system. The breakdown of materials by microorganisms when oxygen is present is aerobic digestion, and the breakdown of materials when is oxygen is not present is anaerobic digestion. The main difference between these processes is that anaerobic reactions produce methane, while aerobic reactions do not (however, both reactions produce carbon dioxide, water, some type of residue, and a new biomass). In addition, aerobic digestion typically occurs more rapidly than anaerobic digestion, while anaerobic digestion does a better job reducing the volume and mass of the material. Due to anaerobic digestion's ability to reduce the volume and mass of waste materials and produce a natural gas, anaerobic digestion technology is widely used for waste management systems and as a source of local, renewable energy.\n\nThe resulting products from biofragmentation are then integrated into microbial cells, this is the assimilation stage. Some of the products from fragmentation are easily transported within the cell by membrane carriers. However, others still have to undergo biotransformation reactions to yield products that can then be transported inside the cell. Once inside the cell, the products enter catabolic pathways that either lead to the production of adenosine triphosphate (ATP) or elements of the cells structure. \n\nIn practice, almost all chemical compounds and materials are subject to biodegradation processes. The significance, however, is in the relative rates of such processes, such as days, weeks, years or centuries. A number of factors determine the rate at which this degradation of organic compounds occurs. Factors include light, water, oxygen and temperature. The degradation rate of many organic compounds is limited by their bioavailability, which is the rate at which a substance is absorbed into a system or made available at the site of physiological activity, as compounds must be released into solution before organisms can degrade them.The rate of biodegradation can be measured in a number of ways. Respirometry tests can be used for aerobic microbes. First one places a solid waste sample in a container with microorganisms and soil, and then aerates the mixture. Over the course of several days, microorganisms digest the sample bit by bit and produce carbon dioxide – the resulting amount of CO serves as an indicator of degradation. Biodegradability can also be measured by anaerobic microbes and the amount of methane or alloy that they are able to produce.\n\nIt’s important to note factors that effect biodegradation rates during product testing to ensure that the results produced are accurate and reliable. Several materials will test as being biodegradable under optimal conditions in a lab for approval but these results may not reflect real world outcomes where factors are more variable. For example, a material may have tested as biodegrading at a high rate in the lab may not degrade at a high rate in a landfill because landfills often lack light, water, and microbial activity that are necessary for degradation to occur. Thus, it is very important that there are standards for plastic biodegradable products, which have a large impact on the environment. The development and use of accurate standard test methods can help ensure that all plastics that are being produced and commercialized will actually biodegrade in natural environments. One test that has been developed for this purpose is DINV 54900.\n\nThe term Biodegradable Plastics refers to a material that maintains its mechanical strength during practical use but break down into low-weight compounds and non-toxic byproducts after their use. This breakdown is made possible through an attack of microorganisms on the material, which is typically a non-water soluble polymer. Such materials can be obtained through chemical synthesis, fermentation by microorganisms, and from chemically modified natural products.\n\nPlastics biodegrade at highly variable rates. PVC-based plumbing is selected for handling sewage because PVC resists biodegradation. Some packaging materials on the other hand are being developed that would degrade readily upon exposure to the environment. Examples of synthetic polymers that biodegrade quickly include polycaprolactone, other polyesters and aromatic-aliphatic esters, due to their ester bonds being susceptible to attack by water. A prominent example is poly-3-hydroxybutyrate, the renewably derived polylactic acid, and the synthetic polycaprolactone. Others are the cellulose-based cellulose acetate and celluloid (cellulose nitrate).\nUnder low oxygen conditions plastics break down more slowly. The breakdown process can be accelerated in specially designed compost heap. Starch-based plastics will degrade within two to four months in a home compost bin, while polylactic acid is largely undecomposed, requiring higher temperatures. Polycaprolactone and polycaprolactone-starch composites decompose slower, but the starch content accelerates decomposition by leaving behind a porous, high surface area polycaprolactone. Nevertheless, it takes many months.\nIn 2016, a bacterium named \"Ideonella sakaiensis\" was found to biodegrade PET.\n\nMany plastic producers have gone so far even to say that their plastics are compostable, typically listing corn starch as an ingredient. However, these claims are questionable because the plastics industry operates under its own definition of compostable:\n\nThe term \"composting\" is often used informally to describe the biodegradation of packaging materials. Legal definitions exist for compostability, the process that leads to compost. Four criteria are offered by the European Union:\n\nNow biodegradable technology has become a highly developed market with applications in product packaging, production, and medicine. The biodegradation of biomass offers some guidances. Polyesters are known to biodegrade.\n\nOxo-biodegradation is defined by CEN (the European Standards Organisation) as \"degradation resulting from oxidative and cell-mediated phenomena, either simultaneously or successively.\" Whilst sometimes described as \"oxo-fragmentable,\" and \"oxo-degradable\" these terms describe only the first or oxidative phase and should not be used for material which degrades by the process of oxo-biodegradation defined by CEN: the correct description is \"oxo-biodegradable.\"\n\nBy combining plastic products with very large polymer molecules, which contain only carbon and hydrogen, with oxygen in the air, the product is rendered capable of decomposing in anywhere from a week to one to two years. This reaction occurs even without prodegradant additives but at a very slow rate. That is why conventional plastics, when discarded, persist for a long time in the environment. Oxo-biodegradable formulations catalyze and accelerate the biodegradation process but it takes considerable skill and experience to balance the ingredients within the formulations so as to provide the product with a useful life for a set period, followed by degradation and biodegradation.\n\nBiodegradable technology is especially utilized by the bio-medical community. Biodegradable polymers are classified into three groups:\nmedical, ecological, and dual application, while in terms of origin they are divided into two groups: natural and synthetic. The Clean Technology Group is exploiting the use of supercritical carbon dioxide, which under high pressure at room temperature is a solvent that can use biodegradable plastics to make polymer drug coatings. The polymer (meaning a material composed of molecules with repeating structural units that form a long chain) is used to encapsulate a drug prior to injection in the body and is based on lactic acid, a compound normally produced in the body, and is thus able to be excreted naturally. The coating is designed for controlled release over a period of time, reducing the number of injections required and maximizing the therapeutic benefit. Professor Steve Howdle states that biodegradable polymers are particularly attractive for use in drug delivery, as once introduced into the body they require no retrieval or further manipulation and are degraded into soluble, non-toxic by-products. Different polymers degrade at different rates within the body and therefore polymer selection can be tailored to achieve desired release rates.\n\nOther biomedical applications include the use of biodegradable, elastic shape-memory polymers. Biodegradable implant materials can now be used for minimally invasive surgical procedures through degradable thermoplastic polymers. These polymers are now able to change their shape with increase of temperature, causing shape memory capabilities as well as easily degradable sutures. As a result, implants can now fit through small incisions, doctors can easily perform complex deformations, and sutures and other material aides can naturally biodegrade after a completed surgery.\n\nThere is no universal definition for biodegradation and there are various definitions of composting, which has led to much confusion between the terms. They are often lumped together; however, they do not have the same meaning. Biodegradation is the naturally-occurring breakdown of materials by microorganisms such as bacteria and fungi or other biological activity. Composting is a human-driven process in which biodegradation occurs under a specific set of circumstances. The predominant difference between the two is that one process is naturally-occurring and one is human-driven.\n\nBiodegradable material is capable of decomposing without an oxygen source (anaerobically) into carbon dioxide, water, and biomass, but the timeline is not very specifically defined. Similarly, compostable material breaks down into carbon dioxide, water, and biomass; however, compostable material also breaks down into inorganic compounds. The process for composting is more specifically defined, as it controlled by humans. Essentially, composting is an accelerated biodegradation process due to optimized circumstances. Additionally, the end product of composting not only returns to its previous state, but also generates and adds beneficial microorganisms to the soil called humus. This organic matter can be used in gardens and on farms to help grow healthier plants in the future. Composting more consistently occurs within a shorter time frame since it is a more defined process and is expedited by human intervention. Biodegradation can occur in different time frames under different circumstances, but is meant to occur naturally without human intervention.\n\nEven within composting, there are different circumstances under which this can occur. The two main types of composting are at-home versus commercial. Both produce healthy soil to be reused - the main difference lies in what materials are able to go into the process. At-home composting is mostly used for food scraps and excess garden materials, such as weeds. Commercial composting is capable of breaking down more complex plant-based products, such as corn-based plastics and larger pieces of material, like tree branches. Commercial composting begins with a manual breakdown of the materials using a grinder or other machine to initiate the process. Because at-home composting usually occurs on a smaller scale and does not involve large machinery, these materials would not fully decompose in at-home composting. Furthermore, one study has compared and contrasted home and industrial composting, concluding that there are advantages and disadvantages to both.\n\nThe following studies provide examples in which composting has been defined as a subset of biodegradation in a scientific context. The first study, \"Assessment of Biodegradability of Plastics Under Simulated Composting Conditions in a Laboratory Test Setting,\" clearly examines composting as a set of circumstances that falls under the category of degradation. Additionally, this next study looked at the biodegradation and composting effects of chemically and physically crosslinked polylactic acid. Notably discussing composting and biodegrading as two distinct terms. The third and final study reviews European standardization of biodegradable and compostable material in the packaging industry, again using the terms separately.\n\nThe distinction between these terms is crucial because waste management confusion leads to improper disposal of materials by people on a daily basis. Biodegradation technology has led to massive improvements in how we dispose of waste; there now exist trash, recycling, and compost bins in order to optimize the disposal process. However, if these waste streams are commonly and frequently confused, then the disposal process is not at all optimized. Biodegradable and compostable materials have been developed to ensure more of human waste is able to breakdown and return to its previous state, or in the case of composting even add nutrients to the ground. When a compostable product is thrown out as opposed to composted and sent to a landfill, these inventions and efforts are wasted. Therefore, it is important for average citizens to understand the difference between these terms so that materials can be disposed of properly and efficiently.\n\nPlastic pollution from illegal dumping poses health risks to wildlife. Animals often mistake plastics for food, resulting in intestinal entanglement. Slow-degrading chemicals, like polychlorinated biphenyls (PCBs), nonylphenol (NP), and pesticides also found in plastics, can release into environments and subsequently also be ingested by wildlife.\n\nRachel Carson, a notable environmentalist in the 1960s, provided one of the first key studies on the consequences associated with chemical ingestion in wildlife, specifically birds. In her work \"Silent Spring\", she wrote on DDT, a pesticide commonly used in human agricultural activities. Birds that ate the tainted bugs, Carson found, were more likely to produce eggs with thin and weak shells.\n\nThese chemicals also play a role in human health, as consumption of tainted food (in processes called biomagnification and bioaccumulation) has been linked to issues such as cancers, neurological dysfunction, and hormonal changes. A well-known example of biomagnification impacting health in recent times is the increased exposure to dangerously high levels of mercury in fish, which can affect sex hormones in humans.\n\nIn efforts to remediate the damages done by slow-degrading plastics, detergents, metals, and other pollutants created by humans, economic costs have become a concern. Marine litter in particular is notably difficult to quanitfy and review. Researchers at the World Trade Institute estimate that cleanup initiatives' cost (specifically in ocean ecosystems) has hit close to thirteen billion dollars a year. The main concern stems from marine environments, with the biggest cleanup efforts centering around garbage patches in the ocean. In 2017, a garbage patch the size of Mexico was found in the Pacific Ocean. It is estimated to be upwards of a million square miles in size. While the patch contains more obvious examples of litter (plastic bottles, cans, and bags), tiny microplastics are nearly impossible to clean up. \"National Geographic\" reports that even more non-biodegradable materials are finding their way into vulnerable environments - nearly thirty-eight million pieces a year.\n\nMaterials that have not degraded can also serve as shelter for invasive species, such as tube worms and barnacles. When the ecosystem changes in response to the invasive species, resident species and the natural balance of resources, genetic diversity, and species richness is altered. These factors may support local economies in way of hunting and aquaculture, which suffer in response to the change. Similarly, coastal communities which rely heavily on ecotourism lose revenue thanks to a buildup of pollution, as their beaches or shores are no longer desirable to travelers. The World Trade Institute also notes that the communities who often feel most of the effects of poor biodegradation are poorer countries without the means to pay for their cleanup. In a positive feedback loop effect, they in turn have trouble controlling their own pollution sources.\n\nThe first known use of \"biodegradable\" in a biological context was in 1959 when it was employed to describe the breakdown of material into innocuous components by microorganisms. Now \"biodegradable\" is commonly associated with environmentally friendly products that are part of the earth's innate cycles and capable of decomposing back into natural elements.\n\n\n"}
{"id": "462707", "url": "https://en.wikipedia.org/wiki?curid=462707", "title": "Brush", "text": "Brush\n\nA brush is a common tool with bristles, wire or other filaments. It generally consists of a handle or block to which filaments are affixed in either a parallel or perpendicular orientation, depending on the way the brush is to be gripped during use. The material of both the block and bristles or filaments is chosen to withstand hazards of its intended use, such as corrosive chemicals, heat or abrasion. It is used for cleaning, grooming hair, make up, painting, surface finishing and for many other purposes. It is one of the most basic and versatile tools in use today, and the average household may contain several dozen varieties.\n\nA common way of setting the bristles, brush filaments, in the brush is the staple or anchor set brush in which the filament is forced with a staple by the middle into a hole with a special driver and held there by the pressure against all of the walls of the hole and the portions of the staple nailed to the bottom of the hole. The staple can be replaced with a kind of anchor, which is a piece of rectangular profile wire that is anchored to the wall of the hole, like in most toothbrushes. Another way to attach the bristles to the surface can be found in a fused brush, in which instead of being inserted into a hole, a plastic fibre is welded to another plastic surface, giving the option to use different diameters of bristles in the same brush.\n\nConfigurations include twisted-in wire (e.g. bottle brushes), cylinders and disks (with bristles spread in one face or radially).\n\nThe action of these brushes is mainly in the tip of each flexible bristle which dislodges particles of matter.\n\n\nThe action of such brushes is mostly from the sides, not the tip, contact with which releases material held by capillary action.\n\nThe action of these brushes is more akin to combing than brushing, that is they are used to straighten and untangle filaments. Certain varieties of hairbrush are however designed to brush the scalp itself free of material such as dead skin (dandruff) and to invigorate the skin of the scalp.\n\n\nBrushes used for cleaning come in various sizes, ranging from that of a toothbrush, to the standard household version accompanied by a dustpan, to 36\" deck brushes. There are brushes for cleaning tiny cracks and crevices and brushes for cleaning enormous warehouse floors. Brushes perform a multitude of cleaning tasks. For example, brushes lightly dust the tiniest figurine, they help scrub stains out of clothing and shoes, they remove grime from tires, and they remove the dirt and debris found on floors with the help of a dust pan. Specific brushes are used for diverse activities from cleaning vegetables, as a toilet brush, washing glass, cleaning tiles, and as a mild abrasive for sanding.\n\n\nGlossary of Brush Terms\n"}
{"id": "18334778", "url": "https://en.wikipedia.org/wiki?curid=18334778", "title": "Channel bank", "text": "Channel bank\n\nIn telecommunications, a channel bank is a device that performs multiplexing or demultiplexing (\"demux\") of a group of communications channels, such as analog or digital telephone lines, into one channel of higher bandwidth or higher digital bit rate, such as a DS-1 (T1) circuit, so that all the channels can be sent simultaneously over a single cable called a trunkline.\n\nA channel bank may be located in a telephone exchange, or in an enterprise's telephone closet or enclosure where it \"breaks out\" individual telephone lines from a high-capacity telephone trunk line connected to the central telephone office, or the enterprise's PBX system.\n\n"}
{"id": "16929046", "url": "https://en.wikipedia.org/wiki?curid=16929046", "title": "Custom-fit", "text": "Custom-fit\n\nCustom-fit means personalized with regard to shape and size. A customized product would imply the modification of some of its characteristics according to the customers requirements such as with a custom car. However, when fit is added to the term, customization could give the idea of both the geometric characteristics of the body and the individual customer requirements, \"e.g.\", the steering wheel of the Formula 1 driver Fernando Alonso.\n\nThe custom-fit concept can be understood as the idea of offering one-of-a-kind products that, due to their intrinsic characteristics and use, can be totally adapted to geometric characteristics in order to meet the user requirements.\n\nWith this new concept, industry moves from a resource based manufacturing system to a knowledge based manufacturing system and from mass production to individual production. This encourages the Lean Production trend as established by Toyota, or in other words, an efficiency-based production.\n\nThere are some studies referring to the positive impacts this concept would have on society:\n\nThe research studies found in February 2008 on the subject are the following:\n\nThe process starts with the capturing of data directly from the user by CAD techniques with the ultimate aim of manufacturing products using CAM techniques.\n\n\nAlthough all these developments have been of great interest, the RM-processes have not fallen behind, due to improvement of new Rapid Prototyping Direct digital manufacturing techniques.\n\n\nMPP aims to become the equivalent of a high speed 3D-printer that produces three-dimensional objects directly from powder materials. This technique is based on the process principles of xerographic printers, (for example, laser or LED printers) that combine electrostatic printing with photography. The MPP process approach uses the same fundamental principles to build solid objects on a layer-by-layer basis. Layers of powder materials are generated by attracting different metal- and/or ceramic powders to their respective position on a charged pattern on a photoreceptor by means of an electrostatic field. The attracted layer is transferred to a punch and transported to the consolidation unit where each layer of part material is sintered onto the previous by pressure and heat. The procedure is repeated layer-by-layer until the three-dimensional object is fully formed and consolidated.\n\nMPP has the ability to print different powders within the same layer and progressively change from one material to another, i.e., producing a functionally graded material. In addition to this, MPP uses external pressure to speed the densification process (sintering), which allows manufacturing with a wide range of materials and opens the possibility to produce unique material combinations and microstructures.\n\n\nIt has several print heads that produce continuous streams of material droplets at high frequency. The High Viscosity Inkjet Printing machine is also capable of printing multi-materials simultaneously and also enables the mixing and grading of materials in any combination that is desired. This will enable the manufacturing of products with two or more materials that are graded and there will be no distinct boundary between the materials. This will result in products with unique mechanical properties.\n\nDr. Michiel Willemse who is leading the project says, \"The process is unique in its capability to print highly viscous, UV curable, resins. Material formulations with viscosities up to 500 mPa•s (at ambient temperature) have been printed successfully. This offers the opportunity to print products with unequaled mechanical properties when compared to any other printing systems.\"\n\n\n\n"}
{"id": "43511798", "url": "https://en.wikipedia.org/wiki?curid=43511798", "title": "Dan Meredith", "text": "Dan Meredith\n\nDan Meredith, also known as Dan Blah, is a Internet freedom supporter, journalist, technologist, and media activist. He is currently the founding Director of the Open Technology Fund, a U.S. Government funded program created in 2012 at Radio Free Asia to support global Internet freedom, privacy-enhancing technologies, and Internet censorship circumvention technologies. Meredith joined Al Jazeera's Transparency Unit in 2011, led by Clayton Swisher, where he increased communication security between investigative field journalists and their sources. He was an early part of the Open Technology Institute in 2009, led by Sascha Meinrath. While at OTI, Meredith was involved with: The \"Internet in a Suitcase\" project, a U.S. Department of State funded effort to create \"ad hoc\" mesh wireless technologies; collaborated with Philadelphia community organizers to secure $11.8 million USD from the federal Broadband Technology Opportunities Program; and, worked on Network Neutrality court cases Hart v. Comcast and Comcast Corp. v. FCC with Robb Topolski, who discovered Comcast blocking Bittorrent traffic in 2007. Meredith was a co-founder and Senior Network Engineer of the CUWiN Foundation, a non-profit launched in 2000 that aimed to develop \"decentralized, community-owned networks that foster democratic cultures and local content\". He was an active Indymedia volunteer throughout the mid 2000s at the Champaign-Urbana Independent Media Center (UCIMC) and its low power FM radio station, Radio Free Urbana WRFU-LP. Meredith joined the Linux Foundation's Core Infrastructure Initiative as an inaugural appointee to its Advisory Board in 2014.\n"}
{"id": "41441336", "url": "https://en.wikipedia.org/wiki?curid=41441336", "title": "David McCourt", "text": "David McCourt\n\nDavid McCourt is an Irish-American entrepreneur with experience within the telecom and cable television industries. He grew up in Watertown, Massachusetts, and is a graduate of Georgetown University. McCourt was an early contributor to the development of transatlantic fiber networks and has gone on to found or buy 20 companies in nine countries. McCourt has completed capital raising and merger transactions valued in excess of $7 billion. McCourt's net worth was previously estimated at $750 million. The Economist described him as having \"impeccable credentials as a telecom revolutionary\".\n\nIn 1982, McCourt founded his first company, McCourt Cable Systems, operating as a designer and builder of cable wires. The company grew to be the largest privately owned designer and builder of cable systems in the US. In 1985, McCourt purchased the first independent TV station within the Caribbean Island of Grenada, Discovery TV. In 1987, McCourt founded the first competitive phone company in the US, Corporate Communications Network. This was merged with Metropolitan Fibre Systems (MFS) before being sold for $14.3bn to MCI before its eventual bankruptcy as park of Worldcom in 2002. McCourt’s next venture collaborated with engineering and construction firm Peter Kiewit Sons, Inc. From this partnership emerged McCourt/Kiewit International, based in London, which became the largest designer and builder of residential cable television and telephone networks in Europe.\n\nMcCourt was responsible for bringing the first competitive telephone and TV landscape to Mexico. McCourt also brought traditional waiting times for land line phones down from one year to less than thirty days and at a fraction of the cost compared to incumbent suppliers.\n\nIn 1993, he acquired control of C-TEC Corporation, a diversified telecommunications company that was recapitalised and split into four publicly traded entities: RCN Corporation; Cable Michigan, Inc.; Mercom, Inc.; and Commonwealth Telephone Enterprises, Inc. The annualized returns of these companies would reach three times the returns of the S&P 500 over the same time period. Following a 1995 rights offering which raised $100 million, Mercom, Inc., a cable provider with systems in Michigan and Florida was sold to Avalon Cable of Michigan along with Cable Michigan Inc. Avalon Cable was subsequently acquired by Charter Communications which went into bankruptcy in 2009.\n\nMcCourt was Chief Executive Officer and Chairman of RCN Corporation until its ￼bankruptcy in 2004 following 23 consecutive quarterly losses and $4 billion of total losses.\n\nMcCourt later turned his attention to TV and film, taking production roles. In 2005, McCourt won an Emmy for the series Reading Rainbow, a long running children's show that encourages reading. And McCourt produced Miracle's Boys with Spike Lee on Nickelodeon's new teenage network. He also served as Executive Producer on the ten-part documentary series “What's Going On?” which documented the impact of global conflict on children around the world.\n\nGranahan McCourt took a controlling stake in internet video company Narrowstep Inc in 2006. David McCourt became Chairman and CEO and led a $10.5 million equity financing for the company in 2007. In 2008 Narrowstep agreed to be acquired by Onstream Media for $11.8 million, however the deal value was cut to $5.1 million and eventually abandoned. Narrowstep was then sold to KIT Digital (later renamed Piksel) in a share deal in 2009 and KIT Digital subsequently filed for bankruptcy and its CEO was found guilty of fraud.\n\nIn 2013, in his role as Chairman and CEO of investment firm Granahan McCourt, McCourt led a consortium of companies including Oak Hill Advisers and the family of Walter Scott Jr. in acquiring Irish fiber company, enet. Its network is used by 70 different telcos to serve broadband to over a million people across Ireland. In 2014, McCourt acquired another Irish telecoms operator, Airspeed Telecom, for an undisclosed sum. McCourt and his associates have invested over €100m into Irish operations.\n\nIn July 2016, it was announced that a consortium bid headed by Granahan McCourt and enet was successfully shortlisted by the Irish Government in its ongoing tender process to develop Ireland’s €1bn National Broadband Plan. The consortium includes Granahan McCourt Capital, the John Laing Group and 3i Plc.\n\nIn 2016, McCourt announced the first Public Private Partnership (PPP) in the Kingdom of Saudi Arabia following the Vision 2030 reform plans outlined by deputy crown prince Mohammed bin Salman, seeking to open up opportunities for foreign investment. The partnership saw the creation of a new joint venture between McCourt’s satellite firm Skyware Technologies and Saudi Arabia based space and technology organizations, TAQNIA and KASCT. Through his ownership of Skyware Technologies, McCourt also operates global manufacturing facilities in key territories including China, where he has set up multiple strategic partnerships. \nIn June 2016, McCourt launched ALTV.com, a digital TV platform designed to meet the needs of technologically underserved people around the world. ALTV.com gives training and workshops in writing, filming, lighting, editing and wardrobe. ALTV launched with headquarters in Dublin, with initial plans for rollout across the Middle East and North Africa.\n\nMcCourt's investments also include a phone app called Findyr, a crowd sourcing technology being used for market research, due diligence and tracking economic trends.\n\n\nMcCourt resides with his wife and children outside of New York City, and he spends a considerable amount of time in Europe and the Middle East.\n"}
{"id": "667404", "url": "https://en.wikipedia.org/wiki?curid=667404", "title": "Diazomethane", "text": "Diazomethane\n\nDiazomethane is the chemical compound CHN, discovered by German chemist Hans von Pechmann in 1894. It is the simplest diazo compound. In the pure form at room temperature, it is an extremely sensitive explosive yellow gas; thus, it is almost universally used as a solution in diethyl ether. The compound is a popular methylating agent in the laboratory, but it is too hazardous to be employed on an industrial scale without special precautions. Use of diazomethane has been significantly reduced by the introduction of the safer and equivalent reagent trimethylsilyldiazomethane.\n\nFor safety and convenience diazomethane is always prepared as needed as a solution in ether and used as such. It converts carboxylic acids into their methyl esters or into their homologues (see Arndt-Eistert synthesis). In the Büchner–Curtius–Schlotterbeck reaction diazomethane reacts with an aldehyde to form ketones.\n\nWhen diazomethane reacts with alcohols or phenols in presence of boron trifluoride (BF), methyl ethers are obtained.\n\nDiazomethane is also frequently used as a carbene source. It readily takes part in 1,3-dipolar cycloadditions.\n\nDiazomethane is prepared by hydrolysis of an ethereal solution of an \"N\"-methyl nitrosamide with aqueous base. The traditional precursor is \"N\"-nitroso-\"N\"-methylurea, but this compound is itself somewhat unstable, and nowadays compounds such as \"N\"-methyl-\"N\"'-nitro-\"N\"-nitrosoguanidine (MNNG) and \"N\"-methyl-\"N\"-nitroso-\"p\"-toluenesulfonamide (Diazald) are preferred.\n\nCHN reacts with basic solutions of DO to give the deuterated derivative CDN.\n\nThe concentration of CHN can be determined in either of two convenient ways. It can be treated with an excess of benzoic acid in cold EtO. Unreacted benzoic acid is then back-titrated with standard NaOH. Alternatively, the concentration of CHN in EtO can be determined spectrophotometrically at 410 nm where its extinction coefficient, ε, is 7.2.\nThe gas-phase concentration of diazomethane can be determined using photoacoustic spectroscopy.\n\nDiazomethane is both isomeric and isoelectronic with the more stable cyanamide, but they cannot interconvert.\nMany substituted derivatives of diazomethane have been prepared:\n\nDiazomethane is toxic by inhalation or by contact with the skin or eyes (TLV 0.2ppm). Symptoms include chest discomfort, headache, weakness and, in severe cases, collapse. Symptoms may be delayed. Deaths from diazomethane poisoning have been reported. In one instance a laboratory worker consumed a hamburger near a fumehood where he was generating a large quantity of diazomethane, and died four days later from fulminating pneumonia. Like any other alkylating agent it is expected to be carcinogenic, but such concerns are overshadowed by its serious acute toxicity.\n\nCHN may explode in contact with sharp edges, such as ground-glass joints, even scratches in glassware. Glassware should be inspected before use and preparation should take place behind a blast shield. Specialized kits to prepare diazomethane with flame-polished joints are commercially available.\n\nThe compound explodes when heated beyond 100 °C, exposed to intense light, alkali metals, or calcium sulfate. Use of a blast shield is highly recommended while using this compound.\n\nProof-of-concept work has been done with microfluidics, in which continuous point-of-use synthesis from N-methyl-N-nitrosourea and 0.93M potassium hydroxide in water was followed by point-of-use conversion with benzoic acid, resulting in a 65% yield of the methyl benzoate ester within seconds at temperatures ranging from 0-50 C. The yield was better than under capillary conditions; the microfluidics were credited with \"suppression of hot spots, low holdup, isothermal conditions, and intensive mixing.\"\n\n"}
{"id": "33906124", "url": "https://en.wikipedia.org/wiki?curid=33906124", "title": "Die Neue Sammlung", "text": "Die Neue Sammlung\n\nDie Neue Sammlung is one of the leading design museums in the world, with the largest collection of industrial and product design.\nEstablished around 100 years ago, Die Neue Sammlung is considered the world’s oldest design museum – regarded as such long before the word \"design\" acquired this meaning. \nSince then, Die Neue Sammlung has been making design history with its international acquisitions and stimulating exhibitions.\n\nWith over 100,000 catalogued items, Die Neue Sammlung is among the world’s largest design collections. It is very wide-ranging, with the collection covering over 20 different areas, embracing Product and Industrial Design or Furniture and Graphic Design as well as topics such as Mobility and IT Design. Ceramics and glass are focal areas, as are Jewelry and Appliances, not to mention selected objects in the field of crafted design. \n\nFounded in 1907 in line with the ideas of the German Werkbund, it was inaugurated as an official state museum in 1925. From the very beginning it distinguished itself from the museums of arts and crafts of the day by committing firmly to espousing the Modernism of the times and thus contemporary design. To this day, the Neue Sammlung’s agenda pursue those initial objectives.\nA typical year will see more than a dozen different exhibitions being staged, covering all areas of the collection. Notable exhibitions of recent years have been:\n\n\n"}
{"id": "41228442", "url": "https://en.wikipedia.org/wiki?curid=41228442", "title": "Digital lollipop", "text": "Digital lollipop\n\nA digital lollipop is an electronic device that synthesizes virtual tastes by stimulating the human tongue with electric currents. The device is capable of producing four of the primary tastes: sweet, sour, salty and bitter. Digital lollipops were developed through research led by Nimesha Ranasinghe at the National University of Singapore.\n\nAccording to Ranasinghe, \"The system is capable of manipulating the properties of electric currents (magnitude, frequency, and polarity: inverse current) to formulate different stimuli. Currently, we are conducting experiments to analyze regional differences of the human tongue for electrical stimulation.\"\n\nThe devices generate alternating current signals through a sliver electrode, stimulating the tongue's taste receptors to emulate the major taste components. It also produces small, varying amounts of heat to simulate food.\n\nEventually, the digital lollipop could aid Alzheimer's patients by helping them \"either enhance or suppress certain senses\" and may also allow people with diabetes to experience sweetness without increasing their blood sugar levels. The National University of Singapore research team is developing Taste Over Internet Protocol (TOIP) that would allow taste information to be communicated between locations.\n\n\n"}
{"id": "154505", "url": "https://en.wikipedia.org/wiki?curid=154505", "title": "Digital signal processor", "text": "Digital signal processor\n\nA digital signal processor (DSP) is a specialized microprocessor (or a SIP block), with its architecture optimized for the operational needs of digital signal processing.\n\nThe goal of DSP is usually to measure, filter or compress continuous real-world analog signals. Most general-purpose microprocessors can also execute digital signal processing algorithms successfully, but may not be able to keep up with such processing continuously in real-time. Also, dedicated DSPs usually have better power efficiency, thus they are more suitable in portable devices such as mobile phones because of power consumption constraints. DSPs often use special memory architectures that are able to fetch multiple data or instructions at the same time.\n\nDigital signal processing algorithms typically require a large number of mathematical operations to be performed quickly and repeatedly on a series of data samples. Signals (perhaps from audio or video sensors) are constantly converted from analog to digital, manipulated digitally, and then converted back to analog form. Many DSP applications have constraints on latency; that is, for the system to work, the DSP operation must be completed within some fixed time, and deferred (or batch) processing is not viable.\n\nMost general-purpose microprocessors and operating systems can execute DSP algorithms successfully, but are not suitable for use in portable devices such as mobile phones and PDAs because of power efficiency constraints. A specialized digital signal processor, however, will tend to provide a lower-cost solution, with better performance, lower latency, and no requirements for specialised cooling or large batteries.\n\nSuch performance improvements have led to the introduction of digital signal processing in commercial communications satellites where hundreds or even thousands of analog filters, switches, frequency converters and so on are required to receive and process the uplinked signals and ready them for downlinking, and can be replaced with specialised DSPs with a significant benefits to the satellites' weight, power consumption, complexity/cost of construction, reliability and flexibility of operation. For example, the SES-12 and SES-14 satellites from operator SES, both intended for launch in 2017, are being built by Airbus Defence and Space with 25% of capacity using DSP. \n\nThe architecture of a digital signal processor is optimized specifically for digital signal processing. Most also support some of the features as an applications processor or microcontroller, since signal processing is rarely the only task of a system. Some useful features for optimizing DSP algorithms are outlined below.\n\nBy the standards of general-purpose processors, DSP instruction sets are often highly irregular; while traditional instruction sets are made up of more general instructions that allow them to perform a wider variety of operations, instruction sets optimized for digital signal processing contain instructions for common mathematical operations that occur frequently in DSP calculations. Both traditional and DSP-optimized instruction sets are able to compute any arbitrary operation but an operation that might require multiple ARM or x86 instructions to compute might require only one instruction in a DSP optimized instruction set.\n\nOne implication for software architecture is that hand-optimized assembly-code routines are commonly packaged into libraries for re-use, instead of relying on advanced compiler technologies to handle essential algorithms. Even with modern compiler optimizations hand-optimized assembly code is more efficient and many common algorithms involved in DSP calculations are hand-written in order to take full advantage of the architectural optimizations.\n\n\n\n\nIn engineering, hardware architecture refers to the identification of a system's physical components and their interrelationships. This description, often called a hardware design model, allows hardware designers to understand how their components fit into a system architecture and provides to software component designers important information needed for software development and integration. Clear definition of a hardware architecture allows the various traditional engineering disciplines (e.g., electrical and mechanical engineering) to work more effectively together to develop and manufacture new machines, devices and components.\n\nHardware is also an expression used within the computer engineering industry to explicitly distinguish the (electronic computer) hardware from the software that runs on it. But hardware, within the automation and software engineering disciplines, need not simply be a computer of some sort. A modern automobile runs vastly more software than the Apollo spacecraft. Also, modern aircraft cannot function without running tens of millions of computer instructions embedded and distributed throughout the aircraft and resident in both standard computer hardware and in specialized hardware components such as IC wired logic gates, analog and hybrid devices, and other digital components. The need to effectively model how separate physical components combine to form complex systems is important over a wide range of applications, including computers, personal digital assistants (PDAs), cell phones, surgical instrumentation, satellites, and submarines.\n\nDSPs are usually optimized for streaming data and use special memory architectures that are able to fetch multiple data or instructions at the same time, such as the Harvard architecture or Modified von Neumann architecture, which use separate program and data memories (sometimes even concurrent access on multiple data buses).\n\nDSPs can sometimes rely on supporting code to know about cache hierarchies and the associated delays. This is a tradeoff that allows for better performance. In addition, extensive use of DMA is employed.\n\nDSPs frequently use multi-tasking operating systems, but have no support for virtual memory or memory protection. Operating systems that use virtual memory require more time for context switching among processes, which increases latency.\n\n\nPrior to the advent of stand-alone DSP chips discussed below, most DSP applications were implemented using bit-slice processors. The AMD 2901 bit-slice chip with its family of components was a very popular choice. There were reference designs from AMD, but very often the specifics of a particular design were application specific. These bit slice architectures would sometimes include a peripheral multiplier chip. Examples of these multipliers were a series from TRW including the TDC1008 and TDC1010, some of which included an accumulator, providing the requisite multiply–accumulate (MAC) function.\n\nIn 1976, Richard Wiggins proposed the Speak & Spell concept to Paul Breedlove, Larry Brantingham, and Gene Frantz at Texas Instrument's Dallas research facility. Two years later in 1978 they produced the first Speak & Spell, with the technological centerpiece being the TMS5100, the industry's first digital signal processor. It also set other milestones, being the first chip to use Linear predictive coding to perform speech synthesis.\n\nIn 1978, Intel released the 2920 as an \"analog signal processor\". It had an on-chip ADC/DAC with an internal signal processor, but it didn't have a hardware multiplier and was not successful in the market. In 1979, AMI released the S2811. It was designed as a microprocessor peripheral, and it had to be initialized by the host. The S2811 was likewise not successful in the market.\n\nIn 1980 the first stand-alone, complete DSPs – the NEC µPD7720 and AT&T DSP1 – were presented at the International Solid-State Circuits Conference '80. Both processors were inspired by the research in PSTN telecommunications.\n\nThe Altamira DX-1 was another early DSP, utilizing quad integer pipelines with delayed branches and branch prediction.\n\nAnother DSP produced by Texas Instruments (TI), the TMS32010 presented in 1983, proved to be an even bigger success. It was based on the Harvard architecture, and so had separate instruction and data memory. It already had a special instruction set, with instructions like load-and-accumulate or multiply-and-accumulate. It could work on 16-bit numbers and needed 390 ns for a multiply–add operation. TI is now the market leader in general-purpose DSPs.\n\nAbout five years later, the second generation of DSPs began to spread. They had 3 memories for storing two operands simultaneously and included hardware to accelerate tight loops; they also had an addressing unit capable of loop-addressing. Some of them operated on 24-bit variables and a typical model only required about 21 ns for a MAC. Members of this generation were for example the AT&T DSP16A or the Motorola 56000.\n\nThe main improvement in the third generation was the appearance of application-specific units and instructions in the data path, or sometimes as coprocessors. These units allowed direct hardware acceleration of very specific but complex mathematical problems, like the Fourier-transform or matrix operations. Some chips, like the Motorola MC68356, even included more than one processor core to work in parallel. Other DSPs from 1995 are the TI TMS320C541 or the TMS 320C80.\n\nThe fourth generation is best characterized by the changes in the instruction set and the instruction encoding/decoding. SIMD extensions were added, and VLIW and the superscalar architecture appeared. As always, the clock-speeds have increased; a 3 ns MAC now became possible.\n\nModern signal processors yield greater performance; this is due in part to both technological and architectural advancements like lower design rules, fast-access two-level cache, (E)DMA circuitry and a wider bus system. Not all DSPs provide the same speed and many kinds of signal processors exist, each one of them being better suited for a specific task, ranging in price from about US$1.50 to US$300.\n\nTexas Instruments produces the C6000 series DSPs, which have clock speeds of 1.2 GHz and implement separate instruction and data caches. They also have an 8 MiB 2nd level cache and 64 EDMA channels. The top models are capable of as many as 8000 MIPS (instructions per second), use VLIW (very long instruction word), perform eight operations per clock-cycle and are compatible with a broad range of external peripherals and various buses (PCI/serial/etc). TMS320C6474 chips each have three such DSPs, and the newest generation C6000 chips support floating point as well as fixed point processing.\n\nFreescale produces a multi-core DSP family, the MSC81xx. The MSC81xx is based on StarCore Architecture processors and the latest MSC8144 DSP combines four programmable SC3400 StarCore DSP cores. Each SC3400 StarCore DSP core has a clock speed of 1 GHz.\n\nXMOS produces a multi-core multi-threaded line of processor well suited to DSP operations, They come in various speeds ranging from 400 to 1600 MIPS. The processors have a multi-threaded architecture that allows up to 8 real-time threads per core, meaning that a 4 core device would support up to 32 real time threads. Threads communicate between each other with buffered channels that are capable of up to 80 Mbit/s. The devices are easily programmable in C and aim at bridging the gap between conventional micro-controllers and FPGAs\n\nCEVA, Inc. produces and licenses three distinct families of DSPs. Perhaps the best known and most widely deployed is the CEVA-TeakLite DSP family, a classic memory-based architecture, with 16-bit or 32-bit word-widths and single or dual MACs. The CEVA-X DSP family offers a combination of VLIW and SIMD architectures, with different members of the family offering dual or quad 16-bit MACs. The CEVA-XC DSP family targets Software-defined Radio (SDR) modem designs and leverages a unique combination of VLIW and Vector architectures with 32 16-bit MACs.\n\nAnalog Devices produce the SHARC-based DSP and range in performance from 66 MHz/198 MFLOPS (million floating-point operations per second) to 400 MHz/2400 MFLOPS. Some models support multiple multipliers and ALUs, SIMD instructions and audio processing-specific components and peripherals. The Blackfin family of embedded digital signal processors combine the features of a DSP with those of a general use processor. As a result, these processors can run simple operating systems like μCLinux, velOSity and Nucleus RTOS while operating on real-time data.\n\nNXP Semiconductors produce DSPs based on TriMedia VLIW technology, optimized for audio and video processing. In some products the DSP core is hidden as a fixed-function block into a SoC, but NXP also provides a range of flexible single core media processors. The TriMedia media processors support both fixed-point arithmetic as well as floating-point arithmetic, and have specific instructions to deal with complex filters and entropy coding.\n\nCSR produces the Quatro family of SoCs that contain one or more custom Imaging DSPs optimized for processing document image data for scanner and copier applications.\n\nMicrochip Technology produces the PIC24 based dsPIC line of DSPs. Introduced in 2004, the dsPIC is designed for applications needing a true DSP as well as a true microcontroller, such as motor control and in power supplies. The dsPIC runs at up to 40MIPS, and has support for 16 bit fixed point MAC, bit reverse and modulo addressing, as well as DMA.\n\nMost DSPs use fixed-point arithmetic, because in real world signal processing the additional range provided by floating point is not needed, and there is a large speed benefit and cost benefit due to reduced hardware complexity. Floating point DSPs may be invaluable in applications where a wide dynamic range is required. Product developers might also use floating point DSPs to reduce the cost and complexity of software development in exchange for more expensive hardware, since it is generally easier to implement algorithms in floating point.\n\nGenerally, DSPs are dedicated integrated circuits; however DSP functionality can also be produced by using field-programmable gate array chips (FPGAs).\n\nEmbedded general-purpose RISC processors are becoming increasingly DSP like in functionality. For example, the OMAP3 processors include a ARM Cortex-A8 and C6000 DSP.\n\nIn Communications a new breed of DSPs offering the fusion of both DSP functions and H/W acceleration function is making its way into the mainstream. Such Modem processors include ASOCS ModemX and CEVA's XC4000.\n\nIn May 2018, Huarui-2 designed by Nanjing Research Institute of Electronics Technology passed acceptance. With a processing speed of 0.4 TFLOPS, the chip can achieve better performance than current mainstream DSP chips. The design team has begun to create Huarui-3, which has a processing speed in TFLOPS level and a support for AI.\n\n\n"}
{"id": "16775826", "url": "https://en.wikipedia.org/wiki?curid=16775826", "title": "Dyno Nobel", "text": "Dyno Nobel\n\nDyno Nobel is a manufacturer of explosives. It is a wholly owned subsidiary of Incitec Pivot Limited operating in Australia, Canada, the United States, Africa, Indonesia, Mexico, South America, Papua New Guinea and Turkey.\n\nThey provide the explosives used in coal and metal mining, quarry and construction as well as pipeline and seismic used for oil and gas exploration. The types of explosives manufactured includes ammonium nitrate, dynamite, electric, non electric and electronic detonators, detonating cord and cast boosters. They also produce surface and underground loading systems. In 2012 Dyno Nobel had over a million tons of ammonium nitrate capacity and over 30 manufacturing facilities on two continents.\n\nDyno Nobel's history dates back to 1865 with Swedish dynamite inventor Alfred Nobel. The invention of the safety fuse by William Bickford in 1831 was also instrumental in the company’s development.\n\nDyno Nobel ASA combined with the Ensign-Bickford Company in 2003 and were restructured again in 2005. By 2007 they had over 3,500 employees and 36 manufacturing facilities. In 2008 Australian agrochemical maker Incitec Pivot Limited (a ASX Top 50 company) bought Dyno Nobel for A$3.3 billion.\nAfter the coal company Peabody Energy filed for bankruptcy in April 2016, Dyno Nobel was listed as their largest creditor being owed more than A$4.3 million.\n\nDyno Nobel is organized into two groups, Dyno Americas and Dyno Nobel Asia Pacific.\n\nDyno Nobel Americas (DNA) serves North America and Chile. DNA also supplies nitrogen based products to agricultural and industrial chemical markets.\n\nDyno Nobel Asia Pacific (DNAP) supplies the mining industry in Australia, Europe, China, Africa, Turkey, Albania, Romania, Finland, Indonesia and Papua New Guinea. In particular, DNAP supplies surface and underground mining in the thermal coal, metallurgical coal, iron ore and other metals sectors.\n\nIn 2010, Dyno Nobel’s owners, Incitec Pivot Limited, approved a sustainability strategy that extends to workplace health and safety, environmental impacts, resource efficiency, community impact and engagement, as well as labor practices and products and services. In the 2012 Sustainability Report IPL states, \"“Sustainable growth requires us to balance our economic performance with our environmental and social responsibilities which include being a good corporate citizen and operating ethically.”\"\n\nIn 2012, IPL reported that the Total Recordable Injury Frequency Rate was 1.45, an increase of 17% from the previous year. In response to this, the IPL Board and Executive Team implemented new positions and structures in the company’s leadership. This was done to support a new Health, Safety and Environment (HSE) strategy put in place to eliminate workplace injuries, illnesses and environmental incidents.\"\n\nIn 2012, IPL established reduction targets for its Australian manufacturing operations for greenhouse gas emissions, water use, natural gas use for energy, and waste to landfill. They claim they are working to establish a baseline for future efficiency targets through gathering data from the global operations for energy use, water use and waste. They investigated the possibility of replacing the current materials used to manufacture bulking agents with recycled or renewable ones such as bio-fuels and green waste. The company carried out trials where waste oil was used in their fuel phase emulsion explosive product. They also researched ways to reduce nitrogen oxide (NOx) emissions from explosive blasting by using different products or blasting techniques.\n\nDuring 2012, the number of females employed by the company increased from 17% to 21%. The number of female graduates employed through the graduate recruitment programme increased from none in 2012 to five in 2013. In comparison, the national average for women in the workplace in the United States was 47%(2010) and 46% in Australia (2013).\n\n"}
{"id": "1001343", "url": "https://en.wikipedia.org/wiki?curid=1001343", "title": "Electrodermal activity", "text": "Electrodermal activity\n\nElectrodermal activity (EDA) is the property of the human body that causes continuous variation in the electrical characteristics of the skin. Historically, EDA has also been known as skin conductance, galvanic skin response (GSR), electrodermal response (EDR), psychogalvanic reflex (PGR), skin conductance response (SCR), sympathetic skin response (SSR) and skin conductance level (SCL). The long history of research into the active and passive electrical properties of the skin by a variety of disciplines has resulted in an excess of names, now standardized to electrodermal activity (EDA).\n\nThe traditional theory of EDA holds that skin resistance varies with the state of sweat glands in the skin. Sweating is controlled by the sympathetic nervous system, and skin conductance is an indication of psychological or physiological arousal. If the sympathetic branch of the autonomic nervous system is highly aroused, then sweat gland activity also increases, which in turn increases skin conductance. In this way, skin conductance can be a measure of emotional and sympathetic responses. More recent research and additional phenomena (resistance, potential, impedance, and admittance, sometimes responsive and sometimes apparently spontaneous) suggest that EDA is more complex than it seems, and research continues into the source and significance of EDA.\n\nThe study of EDA has led to such important and vital tools as the electrocardiograph (ECG or EKG) and the electroencephalograph (EEG).\n\nIn 1849, Dubois-Reymond in Germany first observed that human skin was electrically active. He immersed the limbs of his subjects in a zinc sulfate solution and found that electric current flowed between a limb with muscles contracted and one that was relaxed. He therefore attributed his EDA observations to muscular phenomena. Thirty years later, in 1878 in Switzerland, Hermann and Luchsinger demonstrated a connection between EDA and sweat glands. Hermann later demonstrated that the electrical effect was strongest in the palms of the hands, suggesting that sweat was an important factor.\n\nVigouroux (France, 1879), working with emotionally distressed patients, was the first researcher to relate EDA to psychological activity. In 1888, the French neurologist Féré demonstrated that skin resistance activity could be changed by emotional stimulation and that activity could be inhibited by drugs.\n\nIn 1889 in Russia, Ivane Tarkhnishvili observed variations in skin electrical potentials in the absence of any external stimuli, and he developed a meter to observe the variations as they happened in real time.\n\nThe scientific study of EDA began in the early 1900s. One of the first references to the use of EDA instruments in psychoanalysis is the book by C. G. Jung entitled \"Studies in Word Analysis\", published in 1906. Jung and his colleagues used the meter to evaluate the emotional sensitivities of patients to lists of words during word association. Jung was so impressed with EDA monitoring, he allegedly cried, \"Aha, a looking glass into the unconscious!\" Jung described his use of the device in counseling in his book, \"Studies in Word Association\", and such use has continued with various practitioners.\n\nThe controversial Austrian psychoanalyst Wilhelm Reich also studied EDA in his experiments at the Psychological Institute at the University of Oslo, in 1935 and 1936, to confirm the existence of a bio-electrical charge behind his concept of vegetative, pleasurable \"streamings\".\n\nBy 1972, more than 1500 articles on electrodermal activity had been published in professional publications, and today EDA is regarded as the most popular method for investigating human psychophysiological phenomena. As of 2013, EDA monitoring was still on the increase in clinical applications.\n\nSkin conductance is not under conscious control. Instead, it is modulated autonomously by sympathetic activity which drives human behavior, cognitive and emotional states on a subconscious level. Skin conductance, therefore, offers direct insights into autonomous emotional regulation.\n\nHuman extremities, including fingers, palms, and soles of feet display different bio-electrical phenomena.They can be detected with an EDA meter, a device that displays the change electrical conductance between two points over time. The two current paths are along the surface of the skin and through the body. Active measuring involves sending a small amount of current through the body.\n\nSome studies include the human skin's response to alternating current, including recently deceased bodies.\n\nThere is a relationship between emotional arousal and sympathetic activity, although the electrical change alone does not identify which specific emotion is being elicited. These autonomic sympathetic changes alter sweat and blood flow, which in turn affects GSR and GSP. The amount of sweat glands varies across the human body, being highest in hand and foot regions (200–600 sweat glands per cm2). The response of the skin and muscle tissue to external and internal stimuli can cause the conductance to vary by several microsiemens. A correctly calibrated device can record and display the subtle changes.\n\nThe combined changes between electrodermal resistance and electrodermal potential make up electrodermal activity. Galvanic skin resistance (GSR) is an older term that refers to the recorded electrical resistance between two electrodes when a very weak current is steadily passed between them. The electrodes are normally placed about an inch apart, and the resistance recorded varies according to the emotional state of the subject. Galvanic skin potential (GSP) refers to the voltage measured between two electrodes without any externally applied current. It is measured by connecting the electrodes to a voltage amplifier. This voltage also varies with the emotional state of the subject.\n\nA painful stimulus such as a pinprick elicits a sympathetic response by the sweat glands, increasing secretion. Although this increase is generally very small, sweat contains water and electrolytes, which increase electrical conductivity, thus lowering the electrical resistance of the skin. These changes in turn affect GSR. Another common manifestation is the vasodilation (dilation) of blood vessels in the face, referred to as blushing, as well as increased sweating that occurs when one is embarrassed.\n\nEDA is highly responsive to emotions in some people. Fear, anger, startled response, orienting response, and sexual feelings are among the reactions that may be reflected in EDA. These responses are utilized as part of the polygraph or lie detector test.\n\nEDA in regular subjects differs according to feelings of being treated fairly or unfairly, but psychopaths have been shown to manifest no such differences. This indicates that the EDA record of a polygraph may be deceptive in a criminal investigation.\n\nEDA is a common measure of autonomic nervous system activity, with a long history of being used in psychological research. Hugo D. Critchley, Chair of Psychiatry at the Brighton and Sussex Medical School states, \"EDA is a sensitive psychophysiological index of changes in autonomic sympathetic arousal that are integrated with emotional and cognitive states.\" Many biofeedback therapy devices utilize EDA as an indicator of the user's stress response with the goal of helping the user to control anxiety. EDA is used to assess an individual's neurological status without using traditional, but uncomfortable and expensive, EEG-based monitoring.\n\nOftentimes, EDA monitoring is combined with the recording of heart rate, respiratory rate, and blood pressure, because they are all autonomically dependent variables. EDA measurement is one component of modern polygraph devices, which are often used as lie detectors.\n\nThe E-meter used by the Church of Scientology as part of its practice of \"auditing\" and \"security checking\", is a custom EDA measurement device.\n\nExternal factors such as temperature and humidity affect EDA measurements, which can lead to inconsistent results. Internal factors such as medications and hydration can also change EDA measurements, demonstrating inconsistency with the same stimulus level. Also, the classic understanding has treated EDA as if it represented one homogeneous change in arousal across the body, but in fact different locations of its measurement can lead to different responses; for example, the responses on the left and right wrists are driven by different regions of the brain, providing multiple sources of arousal; thus, the EDA measured in different places on the body varies not only with different sweat gland density but also with different underlying sources of arousal. \nLastly, electrodermal responses are delayed 1–3 seconds. These show the complexity of determining the relationship between EDA and sympathetic activity. The skill of the operator may be a significant factor in the successful application of the tool.\n\n\n"}
{"id": "12840831", "url": "https://en.wikipedia.org/wiki?curid=12840831", "title": "Electronic Case Filing System", "text": "Electronic Case Filing System\n\nElectronic Case Filing System (ECFS) is an automated system developed in Tarrant County, Texas that enables law enforcement agencies, criminal district attorney, county criminal courts, criminal district courts, and the defense bar to process and exchange information about criminal offenses. ECFS software does not work on the Apple Mac platform.\n\nECFS was conceived in November 2002 in Tarrant County, Texas. Initially, the purpose of the system was to enable law enforcement agencies to submit offense reports to the criminal district attorney's office for possible prosecution. In July 2003, the Criminal District Attorney's accepted the first electronic case filing via ECFS. Since that time, more than 100,000 cases have been filed in ECFS by the 47 Law Enforcement Agencies located in Tarrant County, Texas. ECFS was expanded in June 2004 to incorporate the Grand Jury function which is able to return Indictments to the Criminal District Courts on the same day that a True Bill is decided.\n\nIn January 2005, ECFS was extended to enable the Judges and their Court Staff to effectively manage the docket (case load) for each of the nine (9) Criminal District Courts. Since the implementation of ECFS, Tarrant County has been able to control the Jail population, despite a significant increase in the number of cases being filed. In August 2005, ECFS was extended to enable members of the Tarrant County Criminal Defense Lawyers Association to browse and view defendant, offense, and evidence via ECFS. Through this process, defense attorneys are no longer required to visit the Criminal District Attorney's Office to view and copy file.\n\nSince January 2006, the Criminal District Attorney's Office has been completely paperless and all Offense Reports are submitted via ECFS and made available to Law Enforcement Agencies, County and District Courts, and Defense Attorneys. In July 2006, ECFS was extended to allow criminal defendants to be masgistrated electronically. This process also allows the Office of Attorney appointments to be notified that the defendant has requested that defense counsel be appointed which triggers a business process that captures financial information, facilitates a determination of indigency, and when appropriate appoints defense counsel.\n\nIn 2005, the Tarrant County Criminal Defense Lawyer's Association, a non-profit charitable association, implemented its own software design to enable all attorney's, whether members of TCCDLA or not, to access the ECFS system. The software only works on PCs and will not work on Apple's Mac platform. The process enables an attorney to access his or her case files from any computer on the World Wide Web, and is secure and reliable. TCCDLA has continuously revised its software process to enable access 24-7, with little down time. TCCDLA also installed computers in the Tarrant County Justice Center that allow subscribers to access files while in the courthouse. Storm's Edge Technologies is the computer software company that exclusively provides support and design of the TCCDLA ECFS access system.\n\nCriminal District Attorney's Office:\n\n\nInformation technology:\n\n\nTarrant County Criminal Defense Lawyer's Association:\n\nStorm's Edge Technologies:\n\n"}
{"id": "44756771", "url": "https://en.wikipedia.org/wiki?curid=44756771", "title": "Eurosignal", "text": "Eurosignal\n\nEurosignal was a European paging system in operation between 1974 and 1997. Transmissions were broadcast between 87.3 and 87.5 MHz in FM, so that their characteristic modulations could be heard on mainstream FM radio receivers.\n\nEurosignal was introduced in Germany in 1974, France in 1975, and Switzerland in 1985. In France and Switzerland, transmissions were stopped on 31 December 1997 and 1 April 1998, respectively.\n\nA receiver could handle up to four phone numbers. It included an acoustic signal and four optical signals that indicated which of the four numbers called. Eurosignal did not allow other transmissions; the actual four telephone numbers had to be established previously, and use of the signal was limited to telephones. Eurosignal numbers were not listed in a directory in order to limit abuse. Eurosignal had a major advantage over other radiotelephone networks due to its costs, which were about 10% of ordinary phone lines. It also enjoyed a larger coverage area compared to FM radio.\n\nData was transmitted by radio VHF. The following four channels were used.\n\n\nGermany used channels A and B, France used all four, and Switzerland used only channel D. The distribution network was composed of terrestrial transmitters with power of up to 2 kilowatts.\n\nEurosignal was based on audio transmissions, at first using amplitude modulation, and later using frequency modulation. In general, each digit of the phone number produced a particular frequency. Thus, the number 123456 resulted in the issuance of the frequencies f1, f2, f3, f4, f5, f6. If a number was repeated, its second occurrence was replaced by a particular frequency indicating repetition, fr. For example, the number 111111 was encoded by f1, fr, f1, fr, f1, fr. Two phone numbers were separated by the frequency fi for at least 0.22 seconds, with a time per called number of 0.82 seconds.\n\nThe frequencies used were as follows:\n\nThe distinctive audio tones of Eurosignal were sampled by the French electronica duo Air on their 1996 single b-side \"Les Professionnels\", now available on their compilation album \"Premiers Symptômes\" (1997).\n\n"}
{"id": "4894414", "url": "https://en.wikipedia.org/wiki?curid=4894414", "title": "Flash ADC", "text": "Flash ADC\n\nA flash ADC (also known as a direct-conversion ADC) is a type of analog-to-digital converter that uses a linear voltage ladder with a comparator at each \"rung\" of the ladder to compare the input voltage to successive reference voltages. Often these reference ladders are constructed of many resistors; however, modern implementations show that capacitive voltage division is also possible. The output of these comparators is generally fed into a digital encoder, which converts the inputs into a binary value (the collected outputs from the comparators can be thought of as a unary value).\n\nFlash converters are extremely fast compared to many other types of ADCs, which usually narrow in on the \"correct\" answer over a series of stages. Compared to these, a flash converter is also quite simple and, apart from the analog comparators, only requires logic for the final conversion to binary.\n\nFor best accuracy, often a track-and-hold circuit is inserted in front of the ADC input. This is needed for many ADC types (like successive approximation ADC), but for flash ADCs there is no real need for this, because the comparators are the sampling devices.\n\nA flash converter requires a huge number of comparators compared to other ADCs, especially as the precision increases. A flash converter requires formula_1 comparators for an \"n\"-bit conversion. The size, power consumption and cost of all those comparators makes flash converters generally impractical for precisions much greater than 8 bits (255 comparators). In place of these comparators, most other ADCs substitute more complex logic and/or analog circuitry that can be scaled more easily for increased precision.\n\nFlash ADCs have been implemented in many technologies, varying from silicon-based bipolar (BJT) and complementary metal–oxide FETs (CMOS) technologies to rarely used III-V technologies. Often this type of ADC is used as a first medium-sized analog circuit verification.\n\nThe earliest implementations consisted of a reference ladder of well matched resistors connected to a reference voltage. Each tap at the resistor ladder is used for one comparator, possibly preceded by an amplification stage, and thus generates a logical 0 or 1 depending on whether the measured voltage is above or below the reference voltage of the resistor tap. The reason to add an amplifier is twofold: it amplifies the voltage difference and therefore suppresses the comparator offset, and the kick-back noise of the comparator towards the reference ladder is also strongly suppressed. Typically designs from 4-bit up to 6-bit and sometimes 7-bit are produced.\n\nDesigns with power-saving capacitive reference ladders have been demonstrated. In addition to clocking the comparator(s), these systems also sample the reference value on the input stage. As the sampling is done at a very high rate, the leakage of the capacitors is negligible.\n\nRecently, offset calibration has been introduced into flash ADC designs. Instead of high-precision analog circuits (which increase component size to suppress variation) comparators with relatively large offset errors are measured and adjusted. A test signal is applied, and the offset of each comparator is calibrated to below the LSB value of the ADC.\n\nAnother improvement to many flash ADCs is the inclusion of digital error correction. When the ADC is used in harsh environments or constructed from very small integrated circuit processes, there is a heightened risk that a single comparator will randomly change state resulting in a wrong code. Bubble error correction is a digital correction mechanism that prevents a comparator that has, for example, tripped high from reporting logic high if it is surrounded by comparators that are reporting logic low.\n\nThe number of comparators can be reduced somewhat by adding a folding circuit in front, making a so-called \"folding ADC\". Instead of using the comparators in a flash ADC only once, during a ramp input signal, the folding ADC re-uses the comparators multiple times. If a \"m\"-times folding circuit is used in an \"n\"-bit ADC, the actual number of comparator can be reduced from formula_1 to formula_3 (there is always one needed to detect the range crossover). Typical folding circuits are the Gilbert multiplier and analog wired-OR circuits.\n\nThe very high sample rate of this type of ADC enables high-frequency applications (typically in a few GHz range) like radar detection, wideband radio receivers, electronic test equipment, and optical communication links. More often the flash ADC is embedded in a large IC containing many digital decoding functions.\n\nAlso a small flash ADC circuit may be present inside a delta-sigma modulation loop.\n\nFlash ADCs are also used in NAND flash memory, where up to 3 bits are stored per cell as 8 voltages level on floating gates.\n\n"}
{"id": "395585", "url": "https://en.wikipedia.org/wiki?curid=395585", "title": "Floating raft system", "text": "Floating raft system\n\nFloating raft is type of land-based foundation that protects against settlement and liquefaction of soft soil from seismic activity. It was a necessary innovation in the development of tall buildings in the wet soil of Chicago in the 19th century, when it was developed by John Wellborn Root who came up with the idea of interlacing the concrete slab with steel beams. The earliest precursor to the modern version may be the concrete rafts developed for the building of Millbank Prison in 1815 by Robert Smirke.\n\nFor a floating raft foundation – or simply \"floating foundation\" – the foundation has a volume such that, if that volume filled with soil, it would be equal in weight to the total weight of the structure.\n\nWhen the soil is so soft that not even friction piles will support the building load, these types of foundation are the final option and makes the building behave like a boat: obeying Archimedes' principle, buoyed up by the weight of the earth displaced in creating the foundation.\n\n"}
{"id": "14705292", "url": "https://en.wikipedia.org/wiki?curid=14705292", "title": "Floyd's triangle", "text": "Floyd's triangle\n\nFloyd's triangle is a right-angled triangular array of natural numbers, used in computer science education. It is named after Robert Floyd. It is defined by filling the rows of the triangle with consecutive numbers, starting with a 1 in the top left corner:\nBeginning programmers are often assigned the task of writing a program to print out the table in the format shown.\n\nThe numbers along the left edge of the triangle are the lazy caterer's sequence and the numbers along the right edge are the triangular numbers. The \"n\"th row sums to \"n\"(\"n\" + 1)/2, the constant of an \"n\" × \"n\" magic square .\n\nSumming up the row sums in Floyd's triangle reveals the doubly triangular numbers (triangular numbers with an index that is triangular)\n\n1            = 1 = T(T(1)) \n<br><br><br>\n\n1            = 6 = T(T(2)) <br> \n2 + 3\n<br><br><br>\n1 <br>\n2 + 3     = 21 = T(T(3))<br>\n4 + 5 + 6 <br>\n\n\n"}
{"id": "13741562", "url": "https://en.wikipedia.org/wiki?curid=13741562", "title": "Gas meter prover", "text": "Gas meter prover\n\nA gas meter prover is a device which verifies the accuracy of a gas meter. Provers are typically used in gas meter repair facilities, municipal gas meter shops, and public works shops. Provers work by passing a known volume of air through a meter while monitoring the gas meters register, index, or internal displacement. The prover then displays a proof, a value expressed as a percent which compares the volume of air passed with the volume of air measured to determine the meters accuracy.\n\nSince the early 1900s, bell provers have been the most common reference standard used in gas meter proving, and has provided standards for the gas industry that is unfortunately susceptible to a myriad of immeasurable uncertainties.\n\nA bell prover (commonly referred to in the industry as a \"bell\") consists of a vertical inner tank surrounded by an outer shell. A space between the inner tank and outer shell is filled with a sealing liquid, usually oil. An inverted tank, called the bell, is placed over the inner tank. The liquid provides an air-tight seal. Bell provers are typically counterweighted to provide positive pressure through a hose and valve connected to a meter. Sometimes rollers or guides are installed on the moving part of the bell which allows for smooth linear movement without the potential for immeasurable pressure differentials caused by the bell rocking back or forth.\n\nBells provide a volume of air that may be predetermined by calculated temperature, pressure and the effective diameter of the bell. Bell scales are unique to each bell and are usually attached vertically with a needle-like pointer. When proving a meter using a manually controlled bell, an operator must first fill the bell with a controlled air supply or raise it manually by opening a valve and pulling a chained mechanism, seal the bell and meter and check the sealed system for leaks, determine the flow rate needed for the meter, install a special flow-rate cap on the meter outlet, note the starting points of both the bell scale and meter index, release the bell valve to pass air through the meter, observe the meter index and calculate the time it takes to pass the predetermined amount of air, then manually calculate the meter's proof accounting for bell air and meter temperature and in some cases other environmental factors.\n\nUncertainties commonly experienced, and possibly unaccounted for within a test when using bell provers can lead to incorrect proofs, by which an operator may adjust a gas meter incorrectly. Temperature inconsistencies between the bell air, meter and connecting hoses can account for most meter proof inaccuracies. Other factors may be mechanical such as stuck or sticky bell rollers or guides, loose piping connections or valves, a dent in the test area of the bell, incorrect counterweights, and human errors in the operation or calculations.\n\nThe invention of programmable logic controllers (PLC) allowed gas meter repair facilities to automate most of the manual bell prover's process and calculations. Instead of manually raising and lowering the bell prover, solenoid valves connected to a PLC control air flows through the meter. Temperature, pressure, and humidity sensors can be used to feed data into an automated bell PLC, and calculations for meter proofs can be handled by a computer or electronic device programmed for such a purpose. In the early 1990s, the PLC was replaced by PACs (Programmable Automated Controls) and modern computer systems. Sensors to read the index of a meter were added to further automate the process, removing much of the human error associated with manual bell provers.\n\nThe natural evolution of the automated bell and PAC controls led itself to the use of vacuum driven provers with arrays of sonic nozzles (utilizing choked flow to provide precise flow rates. Such a use eliminated the need for a bell, as the flow rate is provided through the nozzles. When sufficient vacuum is applied to a sonic nozzle it creates a constant flow rate. Bernoulli's principle is applied to calculate the chosen flow rates chosen by the user or automated by a computer. Computers and PAC systems automate the process, and most sonic nozzle provers are capable of displaying not only meter proofs to a user, but are also capable of transmitting proofs as well as other important data to database systems across a computer network.\n"}
{"id": "5779959", "url": "https://en.wikipedia.org/wiki?curid=5779959", "title": "Harmonic mixer", "text": "Harmonic mixer\n\nThe harmonic mixer and subharmonic mixer are a type of frequency mixer, which is a circuit that changes one signal frequency to another. The ordinary mixer has two input signals and one output signal. If the two input signals are sinewaves at frequencies \"f\" and \"f\", then the output signal consists of frequency components at the sum \"f\"+\"f\" and difference \"f\"−\"f\" frequencies. In contrast, the harmonic and subharmonic mixers form sum and difference frequencies at a harmonic multiple of one of the inputs. The output signal then contains frequencies such as \"f\"+\"kf\" and \"f\"−\"kf\" where \"k\" is an integer.\n\nThe classic frequency mixer is a multiplier. Multiplying two sinewaves produces just the sum and difference frequencies; the input frequencies are suppressed, and, in theory, there are no other heterodyne products. In practice, the multiplier is not perfect, and the input frequencies and other heterodyne products will be present.\n\nAn actual multiplier is not needed. The significant requirement is a nonlinearity, and at microwave frequencies it is easier to use a nonlinearity rather than an ideal multiplier. A Taylor series expansion of a nonlinearity will show multiplications that give rise to the desired higher order products.\n\nDesign goals for mixers seek to select the desired heterodyne products and suppress the undesired ones.\n\nDiode mixers.\n\nOverdriven diode bridge mixers. Drive signal looks like odd harmonic waveform (essentially a square wave).\n\nOne classic design for a harmonic mixer uses a step recovery diode (SRD). The mixer's subharmonic input is first amplified to a power level that might be around 1 watt. That signal then drives a step recovery diode impulse generator circuit that turns the sine wave into something approximating an impulse train. The resulting impulse train has the harmonics of the input sine wave present to a high frequency (such as 18 GHz). The impulse train can then be used with a diode mixer (also called a sampler).\nThe SRD usually has a very high frequency multiplication ratio, and can be used as the basis of a comb receiver, monitoring several harmonically related frequencies at once. This forms the basis of many simple 'bug detectors' where the intention is to detect transmission on any frequency, even if not known in advance. (This is not the same as a 'rake' receiver which is a correlation device.)\n\nWhen the required frequency multiple is lower, such as doubling, tripling or quadrupling, then Schottky diode circuits are more common. The conduction angle can be adjusted by changing drive level or temperature, and determines which part of the I/V curve is used and therefore the relative strengths of the different harmonically related outputs. If an even multiple is desired then an anti-parallel pair of diodes will suppress the odd local oscillator contribution, to the level that the diodes can be made identical and experience the same source impedance. Unlike a normal mixer, there is a fairly clear optimum drive level, above which the conversion loss increases.\n\nA harmonic mixer can be used to avoid the complexity of generating a microwave local oscillator, and is common as a simple and reliable frequency extender to a low frequency design.\n\nSubharmonic mixers (a particular form of harmonic mixer where the LO is provided at a sub multiple of the frequency to be mixed with the incoming signal) are often used in direct-digital, or zero IF, communications system in order to eliminate the unwanted effects of LO self-mixing which occurs in many fundamental frequency mixers.\n\nUsed in frequency synthesizers and network analyzers.\n\nA variation on the subharmonic mixer exists that has two switching stages is used to improved mixer gain in a direct downconversion receiver. The first switching stage mixes a received RF signal to an intermediate frequency that is one-half the received RF signal frequency. The second switching stage mixes the intermediate frequency to baseband. By connecting the two switching stages in series, current is reused and harmonic content from the first stage is fed into the second stage thereby improving the mixer gain.\n\n\n"}
{"id": "12527374", "url": "https://en.wikipedia.org/wiki?curid=12527374", "title": "History of construction", "text": "History of construction\n\nThe History of construction overlaps many other fields like structural engineering and relies on other branches of science like archaeology, history and architecture to investigate how the builders lived and recorded their accomplishments. Those fields allow us to analyse constructed buildings and other structures built since prehistory, the tools used and the different uses of building materials.\n\nHistory of building is evolving by different trends in time, marked by few key principles : durability of the materials used, the increasing of height and span, the degree of control exercised over the interior environment and finally the energy available to the construction process.\n\n\"Neolithic\", also known as the \"New Stone Age\", was a time period roughly from 9000 BC to 5000 BC named because it was the last period of the age before wood working began. The tools available were made from natural materials including bone, antler, hide, stone, wood, grasses, animal fibers, and the use of water. These tools were used by people to cut such as with the hand axe, chopper, adze, and celt. Also to scrape, chop such as with a flake tool, pound, pierce, roll, pull, leaver, and carry.\n\nBuilding materials included bones such as mammoth ribs, hide, stone, metal, bark, bamboo, clay, lime plaster, and more. For example, the first bridges made by humans were probably just wooden logs placed across a stream and later timber trackways. In addition to living in caves and rock shelters, the first buildings were simple shelters, tents like the Inuit's tupiq, and huts sometimes built as pit-houses meant to suit the basic needs of protection from the elements and sometimes as fortifications for safety such as the crannog. Built self-sufficiently by their inhabitants rather than by specialist builders, using locally available materials and traditional designs and methods which together are called vernacular architecture.\n\nThe very simplest shelters, tents, leave no traces. Because of this, what little we can say about very early construction is mostly conjecture and based on what we know about the way nomadic hunter-gatherers and herdsmen in remote areas build shelters today. The absence of metal tools placed limitations on the materials that could be worked, but it was still possible to build quite elaborate stone structures with ingenuity using dry stone walling techniques such as at Skara Brae in Scotland, Europe's most complete Neolithic village. The first mud bricks, formed with the hands rather than wooden moulds, belong to the late Neolithic period and were found in Jericho. One of the largest structures of this period was the Neolithic long house. In all cases of timber framed and log structures in these very early cultures, only the very lowest parts of the walls and post holes are unearthed in archaeological excavations, making reconstruction of the upper parts of these buildings largely conjectural.\n\nNeolithic architecture ranges from the tent to the megalith (an arrangement of large stones) and rock-cut architecture which are frequently temples, tombs, and dwellings. The most remarkable Neolithic structure in Western Europe is the iconic megalith known as Stonehenge, regarded by some archaeologists as displaying methods of timber construction such as at woodhenge translated into stone, a process known as petrification. The now ruinous remains are of post and lintel construction and include massive sandstone lintels which were located on supporting uprights by means of mortise and tenon joints; the lintels themselves being end-jointed by the use of tongue and groove joints. There is also evidence of prefabrication of the stonework; the symmetrical geometric arrays of stone clearly indicate that the builders of Stonehenge had mastered sophisticated surveying methods. Neolithic villages large enough to have rural and urban features are called proto-cities to distinguish them from cities beginning with Eridu.\n\nThe Copper Age is the early part of the Bronze Age. Bronze is made when tin is added to copper and brass is copper with zinc. Copper came into use before 5,000 BC and bronze around 3,100 BC, although the times vary by region. Copper and bronze were used for the same types of tools as stone such as axes and chisels, but the new, less brittle, more durable material cut better. Bronze was cast into desired shapes and if damaged could be recast. A new tool developed in the copper age is the saw. Other uses of copper and bronze were to \"harden\" the cutting edge of tools such as the Egyptians using copper and bronze points for working soft stone including quarrying blocks and making rock-cut architecture.\n\nDuring the Bronze Age the corbelled arch came into use such as for beehive tombs. The wheel came into use but was not common until much later. Heavy loads were moved on boats, sledges (a primitive sled) or on rollers. The Egyptians began building stone temples with the post and lintel construction method and the Greeks and Romans followed this style.\n\nThe Iron Age is a cultural period from roughly 1200 BC to 50 BC with the widespread use of iron for tools and weapons. Iron is not much harder than bronze but by adding carbon iron becomes steel which was being produced after about 300 BC. Steel can be hardened and tempered producing a sharp, durable cutting edge. A new woodworking tool allowed by the use of steel is the hand-plane.\n\nThe earliest large-scale buildings for which evidence survives have been found in ancient Mesopotamia. The smaller dwellings only survive in traces of foundations, but the later civilizations built very sizeable structures in the forms of palaces, temples and ziggurats and took particular care to build them out of materials that last, which has ensured that very considerable parts have remained intact. Major technical achievement is evidenced by the construction of great cities such as Uruk and Ur. The Ziggurat of Ur is an outstanding building of the period, despite major reconstruction work. Another fine example is the ziggurat at Chogha Zanbil in modern Iran. Cities created demands for new technologies such as drains for animal and human sewage and paved streets.\n\nArchaeological evidence has shown the existence of pitched-brick vault s such as at Tell al-Rimah in what is now Iraq.\n\nThe chief building material was the mud-brick, formed in wooden moulds similar to those used to make adobe bricks. Bricks varied widely in size and format from small bricks that could be lifted in one hand to ones as big as large paving slabs. Rectangular and square bricks were both common. They were laid in virtually every bonding pattern imaginable and used with considerable sophistication. Drawings survive on clay tablets from later periods showing that buildings were set out on brick modules. By 3500 BC, fired bricks came into use and surviving records show a very complex division of labour into separate tasks and trades. Fired bricks and stone were used for pavement.\n\nLife in general was governed by complex ritual and this extended to rituals for setting-out buildings and moulding the first bricks. Contrary to popular belief the arch was not invented by the Romans, but was used in these civilizations. The later Mesopotamian civilizations, particularly Babylon and thence Susa, developed glazed brickwork to a very high degree, decorating the interiors and exteriors of their buildings with glazed brick reliefs, examples of which survive in the Tehran archaeological museum, the Louvre Museum in Paris and the Pergamon Museum in Berlin.\n\nAs opposed to the cultures of ancient Mesopotamia which built in brick, the pharaohs of Egypt built huge structures in stone. The arid climate has preserved much of the ancient buildings.\n\nAdobe (sun-baked mud brick) construction was used for ancillary buildings and normal houses in ancient times and is still commonly used in rural Egypt. The hot, dry climate was ideal for mud-brick, which tends to wash away in the rain. The Ramesseum in Thebes, Egypt (Luxor) provides one of the finest examples of mud brick construction. Extensive storehouses with mud-brick vaults also survive, all constructed with sloping courses to avoid the need for formwork.\n\nThe grandest buildings were constructed in stone, often from massive masonry blocks. The techniques used to move massive blocks used in pyramids and temples have been subject to extensive debate. Some authors have suggested that the larger may not be cut stone but fabricated with concrete.\n\nAlthough the Egyptians achieved extraordinary feats of engineering, they appear to have done so with relatively primitive technology. As far as is known they did not use wheels or pulleys. They transported massive stones over great distances using rollers, ropes and sledges hauled by large numbers of workers. The ancient Egyptians are credited with inventing the ramp, lever, lathe, oven, ship, paper, irrigation system, window awning, door, glass, a form of plaster of Paris, the bath, lock, shadoof, weaving, a standardized measurement system, geometry, silo, a method of drilling stone, saw, steam power, proportional scale drawings, enameling, veneer, plywood, rope truss, and more. There are no surviving Egyptian manuals so there has been considerable speculation on how stones were lifted to great heights and obelisks erected. Most theories centre on the use of ramps.\n\nImhotep, who lived circa 2650–2600 BC, is credited with being the first recorded architect and engineer.\n\nThe pyramids are chiefly impressive for their enormous size and the staggering manpower that must have been employed in their construction. The largest is the Great Pyramid of Giza which remained the tallest structure in the world for 3800 years (see List of tallest freestanding structures in the world). The engineering problems involved were chiefly to do with the transport of blocks, sometimes over long distances, their movement into location and exact alignment. It is now generally agreed that the skilled building workers were respected and well treated, but undoubtedly very large numbers of labourers were necessary to provide the brute force.\n\nThe methods used in the construction of the pyramids have been the subject of considerable research and discussion (see: Egyptian pyramid construction techniques).\n\nThe ancient Greeks, like the Egyptians and the Mesopotamians, tended to build most of their common buildings out of mud brick, leaving no record behind them. However many structures do survive, some of which are in a very good state of repair, although some have been partly reconstructed or re-erected in the modern era. The most dramatic are the Greek Temples. The Greeks made many advances in technology including plumbing, the spiral staircase, central heating, urban planning, the water wheel, the crane, and more.\n\nThe oldest \"construction drawing\" is in the Temple of Apollo at Didyma. An unfinished stone wall was etched with the profiles of columns and moldings, and the wall was never finished so the drawing was not erased: a rare glimpse into the history of working construction drawings.\n\nNo timber structures survive (roofs, floors etc.), so our knowledge of how these were put together is limited. The spans are, in the main, limited and suggest very simple beam and post structures spanning stone walls. For the longer spans it is uncertain if the Greeks or Romans invented the truss but the Romans certainly used timber roof trusses. Before 650 B.C.E. the now famous ancient Greek temples were built of wood, but after this date began to be built of stone. The process of a timber structure being repeated in stone is called petrification or \"petrified carpentry\".\n\nFired clay was mainly restricted to roofing tiles and associated decorations, but these were quite elaborate. The roof tiles allow a low roof pitch characteristic of ancient Greek architecture. Fired bricks began to be employed with lime mortar. Very prominent buildings were roofed in stone tiles, which mimicked the form of their terracotta counterparts. While later cultures tended to construct their stone buildings with thin skins of finished stones over rubble cores, the Greeks tended to build out of large cut blocks, joined with metal cramps. This was a slow, expensive and laborious process which limited the number of buildings that could be constructed. The metal cramps often failed through corrosion.\n\nBuilding structures mostly used a simple beam and column system without vaults or arches, which based strict limits on the spans that could achieved. However, the Greeks did construct some groin vaults, arch bridges and, with the Egyptians, the first \"high rise\", the Lighthouse of Alexandria, one of the Seven Wonders of the Ancient World.\n\nGreek mathematics was technically advanced and we know for certain that they employed and understood the principles of pulleys, which would have enabled them to build jibs and cranes to lift heavy stonework to the upper parts of buildings. Their surveying skills were exceptional, enabling them to set out the incredibly exact optical corrections of buildings like the Parthenon, although the methods used remain a mystery. Simpler decoration, such as fluting on columns, was simply left until the drums of the columns were cut in place.\n\nThe ancient Greeks never developed the strong mortars which became an important feature of Roman construction.\n\nIn striking contrast to previous cultures, an enormous amount is known about Roman building construction. A very large amount survives, including complete intact buildings like the Pantheon, Rome and very well preserved ruins at Pompeii and Herculaneum. We also have the first surviving treatise on architecture by Vitruvius which includes extensive passages on construction techniques.\n\nThe great Roman development in building materials was the use of hydraulic lime mortar called Roman cement. Previous cultures had used lime mortars but by adding volcanic ash called a pozzolana the mortar would harden under water. This provided them with a strong material for bulk walling. They used brick or stone to build the outer skins of the wall and then filled the cavity with massive amounts of concrete, effectively using the brickwork as permanent shuttering (formwork). Later they used wooden shuttering that was removed for the concrete to cure.\n\nAn example of a temple made of Roman concrete in the 1st century BC is the Temple of Vesta in Tivoli, Italy. The concrete was made of nothing more than rubble and mortar. It was cheap and very easy to produce and required relatively unskilled labour to use, enabling the Romans to build on an unprecedented scale. They not only used it for walls but also to form arches, barrel vaults and domes, which they built over huge spans. The Romans developed systems of hollow pots for making their domes and sophisticated heating and ventilation systems for their thermal baths. .\n\nThe Romans substituted bronze for wood in the roof truss(s) of the Pantheon's portico which was commissioned between 27 BC and 14 AD. The bronze trusses were unique but in 1625 Pope Urban VIII had the trusses replaced with wood and melted the bronze down for other uses. The Romans also made bronze roof tiles\n\nLead was used for roof covering material and water supply and waste pipes. The Latin name for lead is \"plumbum\" thus \"plumbing\". Romans also made use of glass in construction with colored glass in mosaics and clear glass for windows. Glass came to be fairly commonly used in windows of public buildings. Central heating in the form of a hypocaust, a raised floor heated by the exhaust of a wood or coal fire.\n\nThe Romans had trade guilds. Most construction was done by slaves or free men. The use of slave labour undoubtedly cut costs and was one of the reasons for the scale of some of the structures. The Romans placed a considerable emphasis in building their buildings extremely fast, usually within two years. For very large structures the only way this could be achieved was by the application of vast numbers of workers to the task.\n\nThe invention of the waterwheel, sawmill, arch, and were by the Romans. The Romans also began using glass for architectural purposes after about 100 CE and used double glazing as insulated glazing. Roman roads included corduroy roads and paved roads, sometimes supported on raft or pile foundations and bridges. \nVitruvius gives details of many Roman machines. The Romans developed sophisticated timber cranes allowing them to lift considerable weights to great heights. The upper limit of lifting appears to have been about 100 tonnes. Trajan's column in Rome contains some of the largest stones ever lifted in a Roman building, and engineers are still uncertain exactly how it was achieved.\n\nA list of the longest, highest and deepest Roman structures can be found in the List of ancient architectural records.\nRoman building ingenuity extended over bridges, aqueducts, and covered amphitheatres. Their sewerage and water-supply works were remarkable and some systems are still in operation today. The only aspect of Roman construction for which very little evidence survives is the form of timber roof structures, none of which seem to have survived intact. Possibly, triangulated roof trusses were built, this being the only conceivable way of constructing the immense spans achieved, the longest exceeding 30 metres (see List of Ancient Greek and Roman roofs).\n\nChina is a cultural hearth area of eastern Asia, many Far East building methods and styles evolved from China. A famous example of Chinese construction is the Great Wall of China built between the 7th and 2nd centuries BC. The Great Wall was built with rammed earth, stones, and wood and later bricks and tiles with lime mortar. Wooden gates blocked passageways. The oldest archaeological examples of mortise and tenon type woodworking joints were found in China dating to about 5000 BC.\n\nThe Yingzao Fashi is the oldest complete technical manual on Chinese architecture. The Chinese followed the state rules for thousands of years so many of the ancient, surviving buildings were built with the methods and materials still used in the 11th century. Chinese temples are typically wooden timber frames on an earth and stone base. The oldest wooden building is the Nanchan Temple (Wutai) dating from 782 CE. However, Chinese temple builders regularly rebuild the wooden temples so some parts of these ancient buildings are of different ages. Traditional Chinese timber frames do not use trusses but rely only on post and lintel construction. An important architectural element are the dougong bracket sets. The Songyue Pagoda is the oldest brick pagoda dating to 523 AD. It was built with yellow fired bricks laid in clay mortar, with twelve sides and fifteen levels of roofs. The Anji Bridge is the worlds oldest \"open-spandrel stone segmental arch bridge\" built in 595-605 AD. The bridge is built with sandstone joined with dovetail, iron joints.\n\nMost of the (restored) Great Wall sections we see today were built with bricks, and cut stone blocks/slabs. Where bricks and blocks weren't available, tamped earth, uncut stones, wood, and even reeds were used as local materials.\nWood was used for forts and as an auxiliary material. Where local timber wasn't enough, they had it delivered in.\nIn mountain areas, workers quarried stone to build the Great Wall. Using the mountains themselves as footings, the outer layer of the Great Wall was built with stone blocks (and bricks), and filled with uncut stone and anything else available (like earth and dead workers).\nOn the plains Great Wall workers made use of local soil (sand, loess, etc.) and rammed it into compact layers. Jiayuguan's Great Wall section in west China was mainly built with dusty loess soil — \"the most erodible soil on the planet\". It's amazing that sections 2,000 years old still remain mostly intact!\nSand doesn't stick together, so how could a wall be built with sand? Sand was used as a fill material between reed and willow layers.\n\nWest China around Dunhuang is desert. Innovative builders there made use of reeds and willow brought in from rivers and oases to build a strong wall. Jade Gate Pass (Yumenguan) Great Wall Fort was built with 20-cm layers of sand and reed, an impressive 9 meters high.\n\nThe Ming Dynasty Great Wall was mostly built with bricks. To build a strong wall with bricks, they used lime mortar. Workers built brick and cement factories with local materials near the wall.\n\nThe Middle Ages of Europe span from the 5th to 15th centuries AD from the fall of the Western Roman Empire to the Renaissance and is divided into Pre-Romanesque and Romanesque periods.\n\nFortifications, castles and cathedrals were the greatest construction projects. The Middle Ages began with the end of the Roman era and many Roman building techniques were lost. But some Roman techniques, including the use of iron ring-beams, appear to have been used in the Palatine Chapel at Aachen, c. 800 AD, where it is believed builders from the Langobard Kingdom in northern Italy contributed to the work. A revival of stone buildings in the 9th century and the Romanesque style of architecture began in the late 11th century. Also notable are the stave churches in Scandinavia.\n\nMost buildings in Northern Europe were constructed of timber until c. 1000 AD. In Southern Europe adobe remained predominant. Brick continued to be manufactured in Italy throughout the period 600–1000 AD but elsewhere the craft of brick-making had largely disappeared and with it the methods for burning tiles. Roofs were largely thatched. Houses were small and gathered around a large communal hall. Monasticism spread more sophisticated building techniques. The Cistercians may have been responsible for reintroducing brick-making to the area from the Netherlands, through Denmark and Northern Germany to Poland leading to Backsteingotik. Brick remained the most popular prestige material in these areas throughout the period. Elsewhere buildings were typically in timber or where it could be afforded, stone. Medieval stone walls were constructed using cut blocks on the outside of the walls and rubble infill, with weak lime mortars. The poor hardening properties of these mortars were a continual problem, and the settlement of the rubble filling of Romanesque and Gothic walls and piers is still a major cause for concern.\n\nThere were no standard textbooks on building in the Middle Ages. Master craftsmen transferred their knowledge through apprenticeships and from father to son. Trade secrets were closely guarded, as they were the source of a craftsman's livelihood. Drawings only survive from the later period. Parchment was too expensive to be commonly used and paper did not appear until the end of the period. Models were used for designing structures and could be built to large scales. Details were mostly designed at full size on tracing floors, some of which survive.\n\nIn general, medieval buildings were built by paid workers. Unskilled work was done by labourers paid by the day. Skilled craftsmen served apprenticeships or learned their trade from their parents. It is not clear how many women were members of a guild holding a monopoly on a particular trade in a defined area (usually within the town walls). Towns were in general very small by modern standards and dominated by the dwellings of a small number of rich nobles or merchants, and by cathedrals and churches.\n\nRomanesque buildings of the period 600–1100 AD were entirely roofed in timber or had stone barrel vaults covered by timber roofs. The Gothic style of architecture with its vaults, flying buttresses and pointed gothic arches developed in the twelfth century, and in the centuries that followed ever more incredible feats of constructional daring were achieved in stone. Thin stone vaults and towering buildings were constructed using rules derived by trial and error. Failures were frequent, particularly in difficult areas such as crossing towers.\n\nThe pile driver was invented around 1500.\n\nThe scale of fortifications and castle building in the Middle Ages was remarkable, but the outstanding buildings of the period were the Gothic cathedrals with thin masonry vaults and walls of glass. Outstanding examples are: Beauvais Cathedral, Chartres Cathedral, King's College Chapel and Notre Dame, Paris.\n\nThe Renaissance in Italy, the invention of moveable type and the Reformation changed the character of building. The rediscovery of Vitruvius had a strong influence. During the Middle Ages buildings were designed by the people that built them. The master mason and master carpenters learnt their trades by word of mouth and relied on experience, models and rules of thumb to determine the sizes of building elements. Vitruvius however describes in detail the education of the perfect architect who, he said, must be skilled in all the arts and sciences. Filippo Brunelleschi was one of the first of the new style of architects. He started life as a goldsmith and educated himself in Roman architecture by studying ruins. He went on to engineer the dome of Santa Maria del Fiore in Florence.\n\nThe major breakthroughs in this period were to do with the technology of conversion. Water mills in most of western Europe were used to saw timber and convert trees into planks. Bricks were used in ever increasing quantities. In Italy the brickmakers were organised into guilds although the kilns were mostly in rural areas because of the risk of fire and easy availability of firewood and brickearth. Brickmakers were typically paid by the brick, which gave them an incentive to make them too small. As a result, legislation was laid down regulating the minimum sizes and each town kept measures against which bricks had to be compared. An increasing amount of ironwork was used in roof carpentry for straps and tension members. The iron was fixed using forelock bolts. The screw-threaded bolt (and nut) could be made and are found in clockmaking in this period, but they were labour-intensive and thus not used on large structures. Roofing was typically of terracotta roof tiles. In Italy they followed Roman precedents. In northern Europe plain tiles were used. Stone, where available, remained the material of choice for prestige buildings.\n\nThe rebirth of the idea of an architect in the Renaissance radically changed the nature of building design. The Renaissance reintroduced the classical style of architecture. Leon Battista Alberti's treatise on architecture raised the subject to a new level, defining architecture as something worthy of study by the aristocracy. Previously it was viewed merely as a technical art, suited only to the artisan. The resulting change in status of architecture and more importantly the architect is key to understanding the changes in the process of design. The Renaissance architect was often an artist (a painter or sculptor) who had little knowledge of building technology but a keen grasp of the rules of classical design. The architect thus had to provide detailed drawings for the craftsmen setting out the disposition of the various parts. This was what is called the process of design, from the Italian word for drawing. Occasionally the architect would get involved in particularly difficult technical problems but the technical side of architecture was mainly left up to the craftsmen. This change in the way buildings were designed had a fundamental difference on the way problems were approached. Where the Medieval craftsmen tended to approach a problem with a technical solution in mind, the Renaissance architects started with an idea of what the end product needed to look like and then searched around for a way of making it work. This led to extraordinary leaps forward in engineering.\n\nLabour in the Renaissance was much the same as in the Middle Ages: buildings were built by paid workers. Unskilled work was done by labourers paid by the day. Skilled craftsmen served apprenticeships or learned their trade from their parents. Craftsmen were organized in guilds which provided a limited form of building regulation in return for members of the guild holding a monopoly on a particular trade in a defined area (usually within the town walls). Towns were in general very small by modern standards and dominated by the dwellings of a small number of rich nobles or merchants and cathedrals and churches.\n\nThe wish to return to classical architecture created problems for the Renaissance buildings. The builders did not use concrete and thus comparable vaults and domes had to be replicated in brick or stone. The greatest technical feats were undoubtedly in these areas. The first major breakthrough was Brunelleschi's project for the dome of Santa Maria del Fiore. Brunelleschi managed to devise a way of building a huge dome without formwork, relying instead on the weight of the bricks and the way they were laid to keep them in position and the shape of the dome to keep it standing. The exact way the dome was built is still subject to debate today as it is not possible to take the dome apart to study its construction without destroying it. The dome is a double skin, linked by ribs, with a series of wooden and stone chains around it at intervals to attempt to deal with hoop stresses.\n\nBrunelleschi's dome was completed (up to the base of the lantern) in 1446. Its size was soon surpassed by the dome of St Peter's, built using flying scaffolding supported on the cornices and constructed using two stone shells.\n\nThe seventeenth century saw the birth of modern science which would have profound effects on building construction in the centuries to come. The major breakthroughs were towards the end of the century when architect-engineers began to use experimental science to inform the form of their buildings. However it was not until the eighteenth century that engineering theory developed sufficiently to allow sizes of members to be calculated. Seventeenth-century structures relied strongly on experience, rules of thumb and the use of scale models.\n\nThe major breakthrough in this period was in the manufacture of glass, with the first cast plate glass being developed in France. Iron was increasingly employed in structures. Christopher Wren used iron hangers to suspend floor beams at Hampton Court Palace, and iron rods to repair Salisbury Cathedral and strengthen the dome of St Paul's Cathedral. Most buildings had stone ashlar surfaces covering rubble cores, held together with lime mortar. Experiments were made mixing lime with other materials to provide a hydraulic mortar, but there was still no equivalent of the Roman concrete. In England, France and the Dutch Republic, cut and gauged brickwork was used to provide detailed and ornate facades. The triangulated roof truss was introduced to England and used by Inigo Jones and Christopher Wren.\n\nMany tools have been made obsolete by modern technology, but the line gauge, plumb-line, the carpenter's square, the spirit level, and the drafting compass are still in regular use.\n\nDespite the birth of experimental science, the methods of construction in this period remained largely medieval. The same types of crane that had been used in previous centuries were still being employed. Flying scaffolds were employed at St Paul's Cathedral, England and in the dome of St Peters, Rome, but otherwise the same types of timber scaffolding that had been in use centuries before were retained. Cranes and scaffolding depended on timber. Complex systems of pulleys allowed comparatively large loads to be lifted, and long ramps were used to haul loads up to the upper parts of buildings.\n\nThe eighteenth century saw the development of many the ideas that had been born in the late seventeenth century. The architects and engineers became increasingly professionalised. Experimental science and mathematical methods became increasingly sophisticated and employed in buildings. At the same time the birth of the industrial revolution saw an increase in the size of cities and increase in the pace and quantity of construction.\n\nThe major breakthroughs in this period were in the use of iron (both cast and wrought). Iron columns had been used in Wren's designs for the House of Commons and were used in several early eighteenth-century churches in London, but these supported only galleries. In the second half of the eighteenth century the decreasing costs of iron production allowed the construction of major pieces of iron engineering. The Iron Bridge at Coalbrookdale (1779) is a particularly notable example. Large-scale mill construction required fire-proof buildings and cast iron became increasingly used for columns and beams to carry brick vaults for floors. The Louvre in Paris boasted an early example of a wrought-iron roof. Steel was used in the manufacture of tools but could not be made in sufficient quantities to be used for building.\n\nBrick production increased markedly during this period. Many buildings throughout Europe were built of brick, but they were often coated in lime render, sometimes patterned to look like stone. Brick production itself changed little. Bricks were moulded by hand and fired in kilns no different to those used for centuries before. Terracotta in the form of Coade stone was used as an artificial stone in the UK.\n\nThe industrial revolution was manifested in new kinds of transportation installations, such as railways, canals and macadam roads. These required large amounts of investment. New construction devices included steam engines, machine tools, explosives and optical surveying. The steam engine combined with two other technologies which blossomed in the nineteenth century, the circular saw and machine cut nails, lead to the use of balloon framing and the decline of traditional timber framing.\n\nAs steel was mass-produced from the mid-19th century, it was used, in form of I-beams and reinforced concrete. Glass panes also went into mass production, and changed from luxury to every man's property.\n\nPlumbing appeared, and gave common access to drinking water and sewage collection.\n\nBuilding codes have been applied since the 19th century, with special respect to fire safety.\n\nWith the Second Industrial Revolution in the early 20th century, elevators and cranes made high rise buildings and skyscrapers possible, while heavy equipment and power tools decreased the workforce needed. Other new technologies were prefabrication and computer-aided design.\n\nTrade unions were formed to protect construction workers' interests and occupational safety and health. Personal protective equipment such as hard hats and earmuffs also came into use, and have become mandatory at most sites.\n\nFrom the 20th century, governmental construction projects were used as a part of macroeconomic stimulation policies, especially during the Great depression (see New Deal). For economy of scale, whole suburbs, towns and cities, including infrastructure, are often planned and constructed within the same project (called megaproject if the cost exceeds US$1 billion), such as Brasília in Brazil, and the Million Programme in Sweden.\n\nIn the end of the 20th century, ecology, energy conservation and sustainable development have become more important issues of construction.\n\nThere is no established academic discipline of construction history but a growing number of researchers and academics are working in this field, including structural engineers, archaeologists, architects, historians of technology and architectural historians. Although the subject has been studied since the Renaissance and there were a number of important studies in the nineteenth century, it largely went out of fashion in the mid-twentieth century. In the last thirty years there has been an enormous increase in interest in this field, which is vital to the growing practice of building conservation.\n\nThe earliest surviving book detailing historical building techniques is the treatise of the Roman author, Vitruvius, but his approach was neither scholarly nor systematic. Much later, in the Renaissance, Vasari mentions Filippo Brunelleschi's interest in researching Roman building techniques, although if he wrote anything on the subject it does not survive. In the seventeenth century, Rusconi's illustrations for his version of Leon Battista Alberti's treatise explicitly show Roman wall construction but most of the interest in antiquity was in understanding its proportions and detail and the architects of the time were content to build using current techniques. While early archaeological studies and topographic works such as the engravings of Giovanni Battista Piranesi show Roman construction they were not explicitly analytical and much of what they do show is made up.\n\nIn the nineteenth century, lecturers increasingly illustrated their lectures with images of building techniques used in the past and these type of images increasingly appeared in construction text books, such as Rondelet's. The greatest advances however were made by English and French (and later German) architects attempting to understand, record and analyse Gothic buildings. Typical of this type of writing are the works of Robert Willis in England, Viollet-le-Duc in France and Ungewitter in Germany. None of these however were seeking to suggest that the history of construction represented a new approach to the subject of architectural history. August Choisy was perhaps the first author to seriously attempt to undertake such a study.\n\nSantiago Heurta has suggested that it was modernism, with its emphasis on the employment of new materials, that abruptly ended the interest in construction history that appeared to have been growing in the last few decades of the nineteenth century and the early years of the twentieth. With the advent of concrete and steel frame construction, architects, who had been the chief audience for such studies, were no longer as interested as they had been in understanding traditional construction, which suddenly appeared redundant. Very little was thus published between 1920 and 1950. The revival of interest started in archaeology with the studies of Roman construction in the 1950s, but it was not until the 1980s that construction history began to emerge as an independent field.\n\nBy the end of the twentieth century, steel and concrete construction were themselves becoming the subject of historical investigation.The Construction History Society was formed in the UK in 1982. It produces the only academic international journal devoted to the subject annually. The First International Congress on Construction History was held in Madrid in 2003. This was followed by the Second International Congress in 2006 in Queens College, Cambridge, England and the Third International Congress held in Cottbus in 2009, and the Fourth International Congress held in Paris in July 2012 \n\n\n"}
{"id": "33820592", "url": "https://en.wikipedia.org/wiki?curid=33820592", "title": "IEC 61162", "text": "IEC 61162\n\nIEC 61162 is a collection of IEC standards for \"Digital interfaces for navigational equipment within a ship\".\n\nThe 61162 standards are developed in Working Group 6 (WG6) of Technical Committee 80 (TC80) of the IEC.\n\nStandard \"IEC 61162\" is divided into four parts:\nThe 61162 standards are all concerning the transport of NMEA sentences, but the IEC\ndoes not define any of these. This is left to the NMEA Organization.\n\nSingle talker and multiple listeners\nis an international standard\n\nSingle talker and multiple listeners, high-speed transmission\n\nSerial data instrument network\n\nMultiple talkers Multiple listeners\n\nThis subgroup of TC80/WG6 has specified the use of Ethernet for shipboard navigational networks. The\nspecification describes the transport of NMEA sentences as defined in 61162-1 over IPv4. Due to\nthe low amount of protocol complexity it has been nicknamed Lightweight Ethernet or LWE in short.\n\nThe historical background and justification for LWE was presented at the ISIS2011 symposium. An overview article of LWE was\ngiven in the December issue of \"Digital Ship\"\n\nThe standard was published in the first edition 06/2011 . The second edition is in progress(state 05/2016). \n\nIEC 61162-460:2015(E) is an add-on to the IEC 61162-450 standard where higher safety and security standards are needed, e.g. due to higher exposure to external threats or to improve network integrity. This standard provides requirements and test methods for equipment to be used in an IEC 61162-460 compliant network as well as requirements for the network itself and requirements for interconnection from the network to other networks. This standard also contains requirements for a redundant IEC 61162-460 compliant network. This standard extends the informative guidance given in Annex D of IEC 61162-450:2011. It does not introduce new application level protocol requirements to those that are defined in IEC 61162-450. \n\nThe first edition was published in 08/2015.\n\nstate 05/2016 the first bridge and system manufacturers beginning with the implementation of IEC 61162-450 and IEC 61162-460 \n\nknown devices with -450 implementation :\n\nknown devices/systems with -460 implementation :\n\n"}
{"id": "18642648", "url": "https://en.wikipedia.org/wiki?curid=18642648", "title": "IVMS", "text": "IVMS\n\nAn IVMS \"(In Vehicle Monitoring System)\" combines the installation of an electronic device in a vehicle, or fleet of vehicles, with purpose-designed computer software at least at one operational base to enable the owner or a third party to track the vehicle's location, collecting data in the process from the field and deliver it to the base of operation. Modern vehicle tracking systems commonly use GPS technology for locating the vehicle, but other types of automatic vehicle location technology can also be used. Vehicle information can be viewed on electronic maps via the Internet or specialized software.\n\nUrban public transit authorities, Mining companies and Transport/Freight companies are an increasingly common user of vehicle tracking systems.\n\nSeveral types of vehicle tracking devices exist. Typically they are classified as \"passive\" and \"active\".\n\n\"Passive\" devices store GPS location, speed, heading and sometimes a trigger event such as key on/off, door open/closed. Once the vehicle returns to a predetermined point, the device is removed and the data downloaded to a computer for evaluation. One such example of a passive device would be a GPS Log Book that gathers the data and stores it for later upload\n\n\"Active\" devices also collect the same information but usually transmit the data in real-time via cellular or satellite networks to a computer or data center for evaluation. The information is typically analysed and presented using web based technologies.\n\nMany modern IVMS devices combine both active and semi-passive tracking abilities: when a cellular network is available and a tracking device is connected it transmits data to a server; when a network is not available the device stores data in internal memory and will transmit stored data to the server later when the network becomes available again. So, although the actual upload is Active, there is a time delay between the time the position is recorded and the time it is sent back-to-base, making the units semi-passive. Where IVMS is used to drive driver safety and improve on general driver behaviour, this makes no difference in reality: data gets uploaded (delayed) but is still available for post-processing purposes, so overnight safety reports are not affected by this provided the vehicle comes back into coverage later during that day.\n\nHistorically IVMS has been accomplished by installing a box into the vehicle, either self-powered with a battery or wired into the vehicle's power system. For detailed vehicle locating and tracking this is still the predominant method; however, many companies are increasingly interested in the emerging cell phone technologies that provide tracking of multiple entities, such as both a salesperson and their vehicle. These systems also offer tracking of calls, texts, Web use and generally provide a wider safety net for the staff member and the vehicle.\n\nIVMS Units track many different types of activity within a vehicle such as GPS Position, Various inputs such as seatbelt and 4x4 engagement, Speed, Impact/Rollover, Harsh events (acceleration and deceleration, usually performed by a Triaxial Accelerometer) over revving, excess idle time just to mention a few.\n\nThe implementation of IVMS often has significant cost savings: by getting drivers to slow down and drive more carefully, stress is also taken off vehicles. This in the end has savings not only in terms of fuel, but also in terms of general vehicle wear-and-tear like brake pads and disks and engine wear.\n"}
{"id": "8836590", "url": "https://en.wikipedia.org/wiki?curid=8836590", "title": "Integrated Geo Systems", "text": "Integrated Geo Systems\n\nIntegrated Geo Systems (IGS) is a computational architecture system developed for managing geoscientific data through systems and data integration.\n\nGeosciences often involve large volumes of diverse data which have to be processed by computer and graphics intensive applications. The processes involved in processing these large datasets are often so complex that no single applications software can perform all the required tasks. Specialized applications have emerged for specific tasks. To get the required results, it is necessary that all applications software involved in various stages of data processing, analysis and interpretation effectively communicate with each other by sharing data.\n\nIGS provides a framework for maintaining an electronic workflow between various geoscience software applications through data connectivity.\n\nThe main components of IGS are:\n\n\n"}
{"id": "10035905", "url": "https://en.wikipedia.org/wiki?curid=10035905", "title": "Integrated computational materials engineering", "text": "Integrated computational materials engineering\n\nIntegrated Computational Materials Engineering (ICME) is an approach to design products, the materials that comprise them, and their associated materials processing methods by linking materials models at multiple length scales. Key words are \"Integrated\", involving integrating models at multiple length scales, and \"Engineering\", signifying industrial utility. The focus is on the materials, i.e. understanding how processes produce material structures, how those structures give rise to material properties, and how to select materials for a given application. The key links are process-structures-properties-performance. The National Academies report describes the need for using multiscale materials modeling to capture the process-structures-properties-performance of a material.\n\nA fundamental requirement to meet the ambitious ICME objective of designing materials for specific products resp. components is an integrative and interdisciplinary computational description of the history of the component starting from the sound initial condition of a homogeneous, isotropic and stress free melt resp. gas phase and continuing via subsequent processing steps and eventually ending in the description of failure onset under operational load.\n\nIntegrated Computational Materials Engineering is an approach to design products, the materials that comprise them, and their associated materials processing methods by linking materials models at multiple length scales. ICME thus naturally requires the combination of a variety of models and software tools. It is thus a common objective to build up a scientific network of stakeholders concentrating on boosting ICME into industrial application by defining a common communication standard for ICME relevant tools.\n\nEfforts to generate a common language by standardizing and generalizing data formats for the exchange of simulation results represent a major mandatory step towards successful future applications of ICME. A future, structural framework for ICME comprising a variety of academic and/or commercial simulation tools operating on different scales and being modular interconnected by a common language in form of standardized data exchange will allow integrating different disciplines along the production chain, which by now have only scarcely interacted. This will substantially improve the understanding of individual processes by integrating the component history originating from preceding steps as the initial condition for the actual process. Eventually this will lead to optimized process and production scenarios and will allow effective tailoring of specific materials and component properties.\n\nThe ICMEg project aims to build up a scientific network of stakeholders concentrating on boosting ICME into industrial application by defining a common communication standard for ICME relevant tools. Eventually this will allow stakeholders from electronic, atomistic, mesoscopic and continuum communities to benefit from sharing knowledge and best practice and thus to promote a deeper understanding between the different communities of materials scientists, IT engineers and industrial users.\n\nICMEg will create an international network of simulation providers and users. It will promote a deeper understanding between the different communities (academia and industry) each of them by now using very different tools/methods and data formats. The harmonization and standardization of information exchange along the life-cycle of a component and across the different scales (electronic, atomistic, mesoscopic, continuum) are the key activity of ICMEg.\n\nThe mission of ICMEg is\n\nThe activities of ICMEg include\n\nThe ICMEg project ended in October 2016. Its major outcomes are\n\n\nMost of the activities being launched in the ICMEg project are continued by the European Materials Modelling Council and in the MarketPlace project\n\nMultiscale modeling aims to evaluate material properties or behavior on one level using information or models from different levels and properties of elementary processes.\nUsually, the following levels, addressing a phenomenon over a specific window of length and time, are recognized:\n\n\nThere are some software codes that operate on different length scales such as:\n\n\nA comprehensive compilation of software tools with relevance for ICME is documented in the Handbook of Software Solutions for ICME\n\n\nKatsuyo Thorton announced at the 2010 MS&T ICME Technical Committee meeting that NSF would be funding a \"Summer School\" on ICME at the University of Michigan starting in 2011. Northwestern began offering a Masters of Science Certificate in ICME in the fall of 2011. The first Integrated Computational Materials Engineering (ICME) course based upon Horstemeyer 2012 was delivered at Mississippi State University (MSU) in 2012 as a graduate course with distance learning students included [c.f., Sukhija et al., 2013]. It was later was taught in 2013 and 2014 at MSU also with distance learning students. In 2015, the ICME Course was taught by Dr. Mark Horstemeyer (MSU) and Dr. William (Bill) Shelton (Louisiana State University, LSU) with students from each institution via distance learning. The goal of the methodology embraced in this course was to provide students with the basic skills to take advantage of the computational tools and experimental data provided by EVOCD in conducting simulations and bridging procedures for quantifying the structure-property relationships of materials at multiple length scales. On successful completion of the assigned projects, students published their multiscale modeling learning outcomes on the ICME Wiki, facilitating easy assessment of student achievements and embracing qualities set by the ABET engineering accreditation board.\n\n\n\n"}
{"id": "2180501", "url": "https://en.wikipedia.org/wiki?curid=2180501", "title": "International Committee for Information Technology Standards", "text": "International Committee for Information Technology Standards\n\nThe InterNational Committee for Information Technology Standards (INCITS), (pronounced \"insights\"), is an ANSI-accredited standards development organization composed of Information technology developers. It was formerly known as the X3 and NCITS.\n\nINCITS is the central U.S. forum dedicated to creating technology standards. INCITS is accredited by the American National Standards Institute (ANSI) and is affiliated with the Information Technology Industry Council, a global policy advocacy organization that represents U.S. and global innovation companies.\n\nINCITS coordinates technical standards activity between ANSI in the USA and joint ISO/IEC committees worldwide. This provides a mechanism to create standards that will be implemented in many nations. As such, INCITS' executive board also serves as ANSI's Technical Advisory Group for ISO/IEC Joint Technical Committee 1. JTC 1 is responsible for International standardization in the field of information technology.\nINCITS operates through consensus. Many times, the U.S. standard that INCITS members develop then will become the baseline standard for the international community.\n\nINCITS is guided by its executive board. \nThe INCITS executive board established more than 50 Technical Committees, Task Groups and Study Groups that are constantly developing standards for new technologies and updating standards for older products.\n\nMore than 750 standards have been created and approved through the INCITS process, with another 800 in development. American National Standards are voluntary and serve U.S. interests well because all materially affected stakeholders have the opportunity to work together to create them. INCITS-approved standards only become mandatory when, and if, they are adopted or referenced by the government or when market forces make them imperative.\n\nGiven the responsibilities and the expenditures associated with U.S. participation in international standards activities, INCITS considers participation as a \"P\" member of ISO/IEC JTC 1, as a declaration of support for the international committee’s technical work. INCITS policy is to adopt as \"Identical\" American National Standards all ISO/IEC or ISO standards that fall within its program of work, with exceptions as outlined in our procedures. Accordingly, INCITS will adopt as \"Identical\" American National Standards all ISO/IEC or ISO standards that fall within its program of work. Similarly, INCITS will withdraw any such adopted American National Standard that has been withdrawn as an ISO/IEC or ISO International Standards.\n\nINCITS was established in 1961 as the Accredited Standards Committee X3, Information Technology'and is sponsored by Information Technology Industry Council (ITI), a trade association representing providers of information technology products and services then known as the Business Equipment Manufacturers Association (BEMA) and later renamed the Computer and Business Equipment Manufacturers' Association (CBEMA). The first organizational meeting was in February 1961 with ITI (CBEMA then) taking Secretariat responsibility. X3 was established under American National Standards Institute (ANSI) procedures. The forum was renamed Accredited Standards Committee NCITS, National Committee for Information Technology Standards in 1997, and the current name was approved in 2001.\n\n\n"}
{"id": "26019014", "url": "https://en.wikipedia.org/wiki?curid=26019014", "title": "KENTORT II", "text": "KENTORT II\n\nKENTORT II is an above-ground shale oil extraction process developed by the Center for Applied Energy Research of the University of Kentucky. It is a hot recycled solids fluidized bed retorting process developed since 1982 for processing the eastern United States Devonian oil shales. The concept of this process was initiated in 1986 in the test unit.\n\nThe KENTORT II retort consists of four fluidized bed vessels, configured in cascade. The raw oil shale is fed to the pyrolysis vessel the pyrolysis section, where it is fluidized by a mixture of steam and product oil shale gas from the gasification section below. Heat is transferred to the raw oil shale by a combination of fluidizing gas and recirculating hot spent shale from the gasification section. The pyrolysis takes place at to .\n\nThe pyrolyzed oil shale moves by gravity downward to the gasification section. Gasification, which takes place at to , converts remained carbon in the spent shale (char) to product oil shale gas. Steam from the cooling zone is used for fluidizing the sent shale while heat transferred by hot solids (oil shale ash) from the combustion section. The spent shale moves to the combustion section where it is burnt to heat the process, while oil shale ash moves to the cooling section before its removal from the retort.\n\n"}
{"id": "18263040", "url": "https://en.wikipedia.org/wiki?curid=18263040", "title": "Kar2ouche", "text": "Kar2ouche\n\nKar2ouche was an educational software product developed by Immersive Education, a joint project between Oxford University and Intel that started in 1999. In 2001, Kar2ouche won the British Educational Suppliers Association (BESA) Educational Resource Award for ICT.\n\nImmersive Education was originally started by Ian Maber who was an ex Creative Director for Sony Psygnosis and later Creative Director for Elixir Game Studios.\nThe original Kar2ouche -like all good ideas- was the result of an idea between Lloyd Sutton (also ex Psygnosis) and Ian Maber and created at 2.ooam\n\n"}
{"id": "36660129", "url": "https://en.wikipedia.org/wiki?curid=36660129", "title": "Lifting equipment", "text": "Lifting equipment\n\nLifting equipment, also known as lifting gear, is a general term for any equipment that can be used to lift loads. This includes jacks, block and tackle, vacuum lifts, hoists, rotating screws, gantries, A frames, gin poles, shear legs, sheerleg, windlasses, lifting harnesses, fork lifts, hydraulic lifting pads, air lift bags, and cranes.\n\nLifting equipment can be dangerous to use, and is the subject of safety regulations in most countries\n\n"}
{"id": "194655", "url": "https://en.wikipedia.org/wiki?curid=194655", "title": "List of bicycle parts", "text": "List of bicycle parts\n\nFor other cycling related terms (besides parts) see Glossary of cycling.\n\nList of bicycle parts by alphabetic order:\n\n\n"}
{"id": "2002565", "url": "https://en.wikipedia.org/wiki?curid=2002565", "title": "List of missiles by country", "text": "List of missiles by country\n\nThis list of missiles by country displays the names of missiles in order of the country where they originate (were developed), with the countries listed alphabetically and annotated with their continent (and defence alliance, if applicable). In cases where multiple nations have developed or produced a missile, it is listed under each significantly participating nation. Within the lists of each country, missiles are ordered by designation and/or calling name (the latter being especially relevant for Russian/Soviet missiles). In some cases multiple listings are used, in order to provide cross-references for easier navigation. \nThis is not a list of missiles in operational service by a particular country; nor a list of military rockets. Anti-tank missiles are listed elsewhere\n\nFor an alphabetical list by missile name, see the list of missiles.\n\n\n\n\nMissiles:\n\n\n\n\n\nantitank missile.\n\n \n\n, Iran has an active interest in developing, acquiring, and deploying a broad range of ballistic missiles, as well as developing a space launch capability. In mid-July 2008, Iran launched a number of ballistic missiles during military exercises, reportedly including the medium-range Shahab-3. Iran announced other missile and space launch tests in August and November 2008. In February 2009, Iran announced it launched a satellite into orbit and \"officially achieved a presence in space.\"\n\n\n\n\n\n\n\nHwa song 2 missile\nHwa song 1 missile\n\n\n\n\n\"The NATO reporting name of each missile is shown in parentheses behind the proper name.\"\nMissiles:\n\n\n\n\n(Above missile prototypes made by Houwteq, none entered production)\n\n(Above missiles made by Denel Dynamics)\n\n\n\n\n\nUS DoD 4120 Mission Design Series (MDS) Designators and Symbols for Guided Missiles, Rockets, Probes, Boosters, and Satellites.\n\nSample Missile MDS – \"BGM-109G\"\n\nThe list of U. S. missiles, sorted by ascending MDS number:\n\n\n\nTest Vehicle Designations\n\nSequence Numbers:\n\nAir Force: Consecutive numerical sequence for each missile mission type.\n\nArmy: Single numerical sequence until 1948 when the sequence numbers were restarted.\n\nNavy: Initially even numbers transitioning to sequential.\n\nSample Vehicle Designation \"SSM-A-2 Navaho\"\n\nSample Test Vehicle Designation \"RTV-G-1 WAC Corporal\"\n\nUnited States Air Force Designation System, 1947–1951\n\nThe list of missiles sorted by ascending Air Force 1947–1951 designations.\n\n\nUnited States Air Force Designation System, 1951–1955\n\nDuring this timeframe, the U.S. Air Force treated missiles as pilotless aircraft.\n\nThe list of missiles sorted by ascending Air Force 1951–1955 designations.\n\n\nA version of the Falcon missile was briefly designated the F-104 before it was redesignated as the F-98.\n\nThe X-11 and X-12 designations were assigned to one and three engine test missiles that would have been used to develop a five-engine version of the Atlas missile.\n\nUnited States Air Force Designation System, 1955–1963\n\nFor all basic missions except GAR (which started at 1) the sequence number started after 67 which was the last bomber designation used for guided missiles.\n\nSample Air Force 1955–1963 designation: \"XSM-73\"\n\nThe list of missiles sorted by ascending Air Force 1955–1963 designations.\n\n\n\nUnited States Navy Designation System 1941 – 1945\n\nThe list of missiles sorted by ascending Navy 1941 – 1945 designations.\n\n\nUnited States Navy Designation System 1946 – 1947\n\nThe list of missiles sorted by ascending Navy 1946–1947 designations.\n\n\nUnited States Navy Designation System 1947 – 1963\n\nThe list of missiles sorted by ascending Navy 1947–1963 designations.\n\n\n\nUnited States Army Designation System 1941 – 1947\n\nThe list of missiles sorted by ascending Army 1941–1947 designations.\n\n\nUnited States Army Designation System 1948 – 1955\n\nThe list of missiles sorted by ascending Army 1948 – 1955 designations.\n\n\nUnited States Army Designation System 1955 – 1963\n\nThe list of missiles sorted by ascending Army 1955–1963 designations.\n\n\nThe list of undesignated United States missiles sorted alphabetically:\n\n\n\nAustralian target missile briefly used by the United States Navy.\n\nThe United States procured Rapier missile systems for the air defense of United States Air Force Bases in the United Kingdom.\n\nThe list of X designated United States missiles numerically:\n\n\n"}
{"id": "53482446", "url": "https://en.wikipedia.org/wiki?curid=53482446", "title": "Minister of Energy, Science, Technology, Environment and Climate Change (Malaysia)", "text": "Minister of Energy, Science, Technology, Environment and Climate Change (Malaysia)\n\nThe current Malaysian Minister of Energy, Science, Technology, Environment and Climate Change (MESTECC) is Yeo Bee Yin, since 2 July 2018. The minister is supported by Deputy Minister of Energy, Science, Technology, Environment and Climate Change.\n\nThe minister administers the portfolio through the Ministry of Energy, Science, Technology, Environment and Climate Change.\n\nThe following individuals have been appointed as Minister of Energy, or any of its precedent titles:\n\nPolitical Party:\n\nThe following individuals have been appointed as Minister of Technology, or any of its precedent titles:\n\nPolitical Party:\n\nThe following individuals have been appointed as Minister of Science, or any of its precedent titles:\n\nPolitical Party:\n\nThe following individuals have been appointed as Minister of Climate Change, or any of its precedent titles:\n\nPolitical Party:\n\nThe following individuals have been appointed as Minister of Environment, or any of its precedent titles:\n\nPolitical Party:\n\nThe following individuals have been appointed as Minister of Research, or any of its precedent titles:\n\nPolitical Party:\n\nThe following individuals have been appointed as Minister of Green Technology, or any of its precedent titles:\n\nPolitical Party:\n\nThe following individuals have been appointed as Minister of Innovation, or any of its precedent titles:\n\nPolitical Party:\n"}
{"id": "271013", "url": "https://en.wikipedia.org/wiki?curid=271013", "title": "Minister of Science and Sport", "text": "Minister of Science and Sport\n\nThe Minister of Science and Sport, formerly the Minister of Science, is an office in the Cabinet of Canada that originally existing from 1990 to 1995 and was brought back in 2008.\n\nPrior to 1990, the responsibilities of the Industry, Science and Technology portfolio were divided between the now-defunct post of Minister of Regional Industrial Expansion and a Minister of State for Science and Technology.\n\nIn 1995, the portfolio was merged with that of the Minister of Consumer and Corporate Affairs to create the post of Minister of Industry. In the appointments to the Cabinet of Canada of October 30, 2008 under Stephen Harper, the portfolio was reintroduced as a Minister of State and given to Gary Goodyear.\n\nIn 2015, Kirsty Duncan was appointed a Minister of State styled as \"Minister of Science\" to assist the Minister of Industry (the senior portfolio was also renamed the Minister of Innovation, Science and Economic Development). Duncan's portfolio was expected to oversee basic research, while Navdeep Bains would oversee applied science. In July 2018, the office's portfolio was expanded, being renamed to \"Minister of Science and Sport\".\n\nThe Official Opposition Shadow Minister for Science is Matt Jeneroux Member of Parliament for Edmonton Riverbend.\n\nKey:\n\n"}
{"id": "34524479", "url": "https://en.wikipedia.org/wiki?curid=34524479", "title": "Ministry of Energy (Thailand)", "text": "Ministry of Energy (Thailand)\n\nThe Ministry of Energy of the Kingdom of Thailand (; RTGS: Krasuang Phalang Ngan) is a cabinet ministry in the Government of Thailand. \n\nDisparate energy departments were consolidated by the government with the establishment of the National Energy Policy Committee in 1992 (B.E.2535) under the National Energy Policy Council Act (1992). It is responsible for managing the energy sector in Thailand, including granting energy operating licenses and issuing energy pricing regulations. Thaksin Shinawatra, then prime minister in 2002, established the Bureau of Energy on 2 November 2001, B.E.2544, which was later upgraded to the Ministry of Energy in 2002 pursuant to the Restructuring of Government Organization Act (2002).\n\n\n\n\nThailand's Power Development Plan (PDP). is the nation's roadmap for electric power generation, distribution, and consumption. The plan, prepared by the Ministry of Energy (MOE) and EGAT, is issued iteratively. The previous edition, PDP2010 Revision 3, covered the years 2012-2030.\n\nAlong with the PDP, the MOE produces several subsidiary plans that roll up into the PDP:\n\nPDP2015 begins with the assumptions that:\n\nPDP2015 projects the following changes in Thailand electrical power generation fuel mix over the period 2014-2036:\n\nPDP2015 projects that Thailand's CO emissions from power generation will rise from 86,998,000 tons in 2015 to 104,075,000 tons in 2036.\n\n"}
{"id": "59175744", "url": "https://en.wikipedia.org/wiki?curid=59175744", "title": "Movandi", "text": "Movandi\n\nMovandi is an American venture backed startup company, founded by Maryam Reza and Rofougaran based in Newport Beach, California. The company is focused on solving the technical challenges of 5G high frequency and millimeter wave networks by providing scalable, integrated RF, antenna technology and systems. The business provides technology to carriers, handset providers, wired and wireless companies, and is embedded in semiconductor chips and systems.\n\nThe company was founded in 2016, by two siblings Maryam Rofougaran and Reza Rofougaran. Movandi’s BeamX solutions launched in 2018. \n\nMovandi builds mmWave antenna arrays that are more compact, more power-efficient over a wider power range, more spectrally efficient, and cheaper to produce.\n\nMovandi has 40 patents designed as a complete system combining the entire front-end and antenna system rather than a mix and match of individual components. They develop RF CMOS across cellular, microwave backhaul, wi-fi, Bluetooth, GPS/GNSS, NFC, Femto cell, and other standards including the integration of multiple standards into a single chip.\n\nCota Capital - Sierra Ventures - Wistron NeWeb Corporation\n\n\n\n\n\n\n\n\n\n"}
{"id": "30700886", "url": "https://en.wikipedia.org/wiki?curid=30700886", "title": "NFPA 1001", "text": "NFPA 1001\n\nNFPA 1001 (\"Standard for Firefighter Professional Qualifications\") is a standard published by the National Fire Protection Association.\n\nNFPA 1001 identifies the minimum job performance requirements (JPRs) for career and volunteer firefighters whose duties are primarily structural in nature.\n\nThe NFPA 1001 is sectioned as follows:\n\n"}
{"id": "31228166", "url": "https://en.wikipedia.org/wiki?curid=31228166", "title": "Occupations in electrical/electronics engineering", "text": "Occupations in electrical/electronics engineering\n\nThe field of electrical and electronics engineering has grown to include many related disciplines and occupations.\n\nThe Dictionary of Occupational Titles lists a number of occupations in electrical/electronics engineering. It describes them as concerned with applications of the laws of electrical energy and the principles of engineering for the generation, transmission and use of electricity, as well as the design and development of machinery and equipment for the production and utilization of electrical power:\n\n\nThe Institute of Electrical and Electronics Engineers (IEEE) has developed specialized groups (\"societies\") which professionals can join according to their specialization:\n\n"}
{"id": "13339949", "url": "https://en.wikipedia.org/wiki?curid=13339949", "title": "Pattern Oriented Rule Implementation", "text": "Pattern Oriented Rule Implementation\n\nThe Pattern Oriented Rule Implementation (PORI) table is a data structure invented by Amdocs for representation of algorithms for determining a price or set of prices in a financial transaction. The PORI table is used to describe the pricing logic associated with the input event. The PORI table enables business analysts to build a language to describe the business logic associated with financial pricing. The pricing logic is composed without the need for programming, and transformed into executable code using the PORI Core Processor execution engine. The unique PORI representation, and the inference engine for executing the PORI rules, is used to compute a variety of pricing problems on the input event in real-time fashion.\n"}
{"id": "5118821", "url": "https://en.wikipedia.org/wiki?curid=5118821", "title": "Potted shrimps", "text": "Potted shrimps\n\nPotted shrimps are a traditional British dish made with brown shrimp flavored with nutmeg. The dish consists of brown shrimp in nutmeg-flavoured butter, which has set in a small pot, the butter acting as a preservative. Cayenne pepper may also be included. It is traditionally eaten with bread.\n\nPotted shrimp was a favourite dish of Ian Fleming who passed on his predilection for the delicacy to his fictional creation James Bond. Fleming reputedly used to eat the dish at Scotts Restaurant on Mount Street in London.\n\n"}
{"id": "27193055", "url": "https://en.wikipedia.org/wiki?curid=27193055", "title": "Prefrontal cortex basal ganglia working memory", "text": "Prefrontal cortex basal ganglia working memory\n\nPrefrontal cortex basal ganglia working memory (PBWM) is an algorithm that models working memory in the prefrontal cortex and the basal ganglia. It can be compared to long short-term memory (LSTM) in functionality, but is more biologically explainable.\n\nIt uses the primary value learned value model to train prefrontal cortex working-memory updating system, based on the biology of the prefrontal cortex and basal ganglia.\n\nIt is used as part of the Leabra framework and was implemented in Emergent.\n\nThe prefrontal cortex has long been thought to subserve both working memory (the holding of information online for processing) and \"executive\" functions (deciding how to manipulate working memory and perform processing). Although many computational models of working memory have been developed, the mechanistic basis of executive function remains elusive.\n\nPBWM is a computational model of the prefrontal cortex to control both itself and other brain areas in a strategic, task-appropriate manner. These learning mechanisms are based on subcortical structures in the midbrain, basal ganglia and amygdala, which together form an actor/critic architecture. The critic system learns which prefrontal representations are task-relevant and trains the actor, which in turn provides a dynamic gating mechanism for controlling working memory updating. Computationally, the learning mechanism is designed to simultaneously solve the temporal and structural credit assignment problems.\n\nThe model's performance compares favorably with standard backpropagation-based temporal learning mechanisms on the challenging 1-2-AX working memory task, and other benchmark working memory tasks.\n\nFirst, there are multiple separate stripes (groups of units) in the prefrontal cortex and striatum layers. Each stripe can be independently updated, such that this system can remember several different things at the same time, each with a different \"updating policy\" of when memories are updated and maintained. The active maintenance of the memory is in prefrontal cortex (PFC), and the updating signals (and updating policy more generally) come from the striatum units (a subset of basal ganglia units).\n\nPVLV provides reinforcement learning signals to train up the dynamic gating system in the basal ganglia.\n\nThe sensory input is connected to the posterior cortex which is connected to the motor output. The sensory input is also linked to the PVLV system.\n\nThe posterior cortex form the hidden layers of the input/output mapping. The PFC is connected with the posterior cortex to contextualize this input/output mapping.\n\nThe PFC (for output gating) has a localist one-to-one representation of the input units for every stripe. Thus, you can look at these PFC representations and see directly what the network is maintaining. The PFC maintains the working memory needed to perform the task.\n\nThis is the dynamic gating system representing the striatum units of the basal ganglia. Every even-index unit within a stripe represents \"Go\", while the odd-index units represent \"NoGo.\" The Go units cause updating of the PFC, while the NoGo units cause the PFC to maintain its existing memory representation.\n\nThere are groups of units for every stripe.\n\nIn the PBWM model in Emergent, the matrices represent the striatum.\n\nAll of these layers are part of PVLV system. The PVLV system controls the dopaminergic modulation of the basal ganglia (BG). Thus, BG/PVLV form an actor-critic architecture where the PVLV system learns when to update.\n\nSNrThal represents the substantia nigra pars reticulata (SNr) and the associated area of the thalamus, which produce a competition among the Go/NoGo units within a given stripe and mediates competition using k-winners-take-all dynamics. If there is more overall Go activity in a given stripe, then the associated SNrThal unit gets activated, and it drives updating in PFC. For every stripe, there is one unit in SNrThal.\n\nVentral tegmental area (VTA) and substantia nigra pars compacta (SNc) are part of the dopamine layer. This layer models midbrain dopamine neurons. They control the dopaminergic modulation of the basal ganglia.\n"}
{"id": "5923267", "url": "https://en.wikipedia.org/wiki?curid=5923267", "title": "Protein nitrogen unit", "text": "Protein nitrogen unit\n\nThe protein nitrogen unit (PNU) measures the potency of the compounds used in allergy skin tests, and is equivalent to 0.01 microgram (µg) of phosphotungstic acid-precipitable protein nitrogen. Potency measurements depend on the measurement technique, so that results from different manufacturers cannot be reliably compared. as a result, PNUs are being replaced by bioequivalent allergy units (BAU), which are measured by skin testing using reference preparations of standard potency.\n\nThe end product of protein nitrogen in man is Urea\n"}
{"id": "5823188", "url": "https://en.wikipedia.org/wiki?curid=5823188", "title": "Rainwater tank", "text": "Rainwater tank\n\nA rainwater tank (sometimes called a rain barrel in North America in reference to smaller tanks, or a water butt in the UK) is a water tank used to collect and store rain water runoff, typically from rooftops via pipes. Rainwater tanks are devices for collecting and maintaining harvested rain. A rainwater catchment or collection (also known as \"rainwater harvesting\") system can yield of water from of rain on a roof.Rainfall Catchment Equation: Catchment Area (\"often roofs, in square feet, ft\") x Event Rainfall Depth (i\"n\") x 0.623 Conversion Factor \"=\" Gallons of Potential Rainwater Collected*Note this equation is for U.S., imperial measurements; i.e. calculations and conversion factor is for distance in feet and inches, and volume in gallons. (\"We need a metric system equation\".)Rainwater tanks are installed to make use of rain water for later use, reduce mains water use for economic or environmental reasons, and aid self-sufficiency. Stored water may be used for watering gardens, agriculture, flushing toilets, in washing machines, washing cars, and also for drinking, especially when other water supplies are unavailable, expensive, or of poor quality, and when adequate care is taken that the water is not contaminated and is adequately filtered.\n\nUnderground rainwater tanks can also be used for retention of stormwater for release at a later time and offer a variety of benefits described in more detail below. In arid climates, rain barrels are often used to store water during the rainy season for use during dryer periods.\n\nRainwater tanks may have a high (perceived) initial cost. However, many homes use small scale rain barrels to harvest minute quantities of water for landscaping/gardening applications rather than as a potable water surrogate. These small rain barrels, often recycled from food storage and transport barrels or, in some cases, whiskey and wine aging barrels, are often inexpensive. There are also many low cost designs that use locally available materials and village level technologies for applications in developing countries where there are limited alternatives for potable drinking water. While most are properly engineered to screen out mosquitoes, the lack of proper filtering or closed loop systems may create breeding grounds for larvae. With tanks used for drinking water, the user runs a health risk if maintenance is not carried out.\n\nIf rainwater is used for drinking, it is often filtered first. Filtration (such as reverse osmosis, ultraviolet sterilization, or ultrafiltration) may remove pathogens. While rain water is pure it may become contaminated during collection or by collection of particulate matter in the air as it falls. While rain water does not contain chlorine, contamination from airborne pollutants, which settles onto rooftops, may be a risk in urban or industrial areas. Many water suppliers and health authorities, such as the New South Wales Department of Health, do not advise using rainwater for drinking when there is an alternative mains water supply available. However, reports of illness associated with rainwater tanks are relatively infrequent, and public health studies in South Australia (the Australian state with the highest rainwater usage rate) have not identified a correlation. Rainwater is generally considered fit to drink if it smells, tastes and looks fine; However some pathogens, chemical contamination and sub-micrometre suspended metal may produce neither smell nor taste and may not be visible.\n\nAustralian standards may differ greatly from other places in the world where rainwater is commonly used for drinking water. In the United States, rainwater is being increasingly used throughout the country for various purposes. In the semi-arid western state of New Mexico, for instance, many residents in the Taos and Santa Fe areas in particular use rainwater either for landscaping purposes or even all household uses (including potable indoor water). The \"smells, tastes, and looks fine\" standard used in the above paragraph is not an absolute indicator of rainwater safety. Most people who are rainwater users for potable purposes in the USA make certain that their water is safe through filtration, ultraviolet sterilization, and testing.\n\nCertain paints and roofing materials may cause contamination. In particular, a Melbourne Water publication advises that lead-based paints never be used. Tar-based coatings are also not recommended, as they affect the taste of the water. Zinc can also be a source of contamination in some paints, as well as galvanized iron or zincalume roofs, particularly when new, should not collect water for potable use. Roofs painted with acrylic paints may have detergents and other chemicals dissolve in the runoff. Runoff from fibrous cement roofs should be discarded for an entire winter, due to leaching of lime. Chemically treated timbers and lead flashing should not be used in roof catchments. Likewise, rainwater should not be collected from parts of the roof incorporating flues from wood burners without a high degree of filtration. Overflows or discharge pipes from roof-mounted appliances such as air-conditioners or hot-water systems should not have their discharge feed into a rainwater tank.\n\n\"Copper Poisoning,\" a 2010 news article, linked copper poisoning to plastic tanks. The article indicated that rainwater was collected and stored in plastic tanks and that the tank did nothing to mitigate the low pH. The water was then brought into homes by copper piping. The copper was released by the high acid rainwater and caused poisoning in humans. It is important to note that, while the plastic tank is an inert container, the collected acid rain could and should be analysed and pH adjusted before being brought into a domestic water supply system. The solution is to monitor stored rainwater with \"swimming pool strips,\" cheap and available at swimming pool supply outlets.\n\nIf the water is too acidic, the state, county or local health officials may be contacted to obtain advice, precise solutions and pH limits, and guidelines as to what should be used to treat rainwater to be used as domestic drinking water.\n\nMaintenance includes checking roofs and rain gutters for vegetation and debris, maintaining screens around the tank, and occasionally desludging (removing sediment by draining and cleaning the tank of algae and other contaminants).\n\nRainwater tanks which are not properly sealed (secured at the top) may act as breeding grounds for mosquitoes.\n\nSeveral options for dealing with the mosquito issue are:\n1. Flushing ALL water once a week\n2. Using a small amount of cooking oil to suffocate the larvae (the water is still ok for landscape use after this)\n3. Adding the bacillus Bt to the water. This bacteria will not harm animals.\n4. Adding mosquito eating fish\n\nAnother way to store rainwater without worry of contamination by mosquitoes is to use underground storage tanks. Underground tanks keep the water too cool () for mosquito larvae and also are dark, preventing both mosquito, bacterial, and algae growth. An article by Richard Hill goes into depth about the benefits of underground rainwater storage.\n\nRainwater tanks may be constructed from materials such as plastic (polyethylene), concrete, galvanized steel, as well as fiberglass and stainless steel which are rust and chemical-resistant. Those most safely used are typically upcycled from previous food-grade purposes. Intermediate bulk containers (IBCs), caged IBC totes in particular, are very commonly used in rainwater applications. Full tanks are usually installed above ground, and are usually opaque to prevent the exposure of stored water to sunlight, to decrease algal bloom.\n\nTanks may be covered and have screen inlets to exclude insects, debris, animals and bird droppings. Almost all steel tanks currently produced for household rainwater collection come with a plastic inner lining to increase the life of the tank, prevent leaks and protect the water quality.\n\nApart from rooftops, tanks may also be set up to collect rainwater from concrete patios, driveways and other impervious surfaces.\n\nInitial sizes typically ranged in capacity from around , today modern technology has allowed modular and scalable applications to go into sizes of millions of litres or hundreds of thousands of US gallons.\n\nSmaller tanks, such as the plastic (-barrel) are also used in some cases. Modern modular systems which are scalable, like the Rainwater HOG module and the 500 litre (133 gallon) Stratco Aquabarrel can be used to decentralize the rainwater catchment by storing smaller volumes at each downspout. Larger tanks are commonly used where there is no access to a centralised water supply. Companies such as Solar Survival Architecture recommend a tank for a house supporting two people (if compost toilets are placed) and if the region receives at least of precipitation a year. If it receives less (between , 2 or 3 of these 300-gallon tanks can be placed so that more rain can be gathered at times when it does rain. Also affecting tank size is predicted rainfall and rainfall variability; the higher prices for larger tanks; intended use of rainwater and typical consumption for these uses; the area of roof draining into the tank; security of supply desired.Most rainwater catchment tanks used throughout the world are composed of virgin polyethylene, a substance which in the USA is both FDA and NSF approved for potable water storage. Other types of tanks used for rainwater storage include fiberglass, galvanized metal, stainless steel, and concrete. Each type of tank has positive and negative aspects. Polyethylene tanks, when placed above ground, can be subject to algae growth as well as the possibility of a short life (@20 years) due to normal UV exposure in sunlight. The very strong fiberglass tanks must undergo a specific coating process to be brought up to potable grade. Galvanized tanks must either be lined or coated both for potability as well as to prevent the inevitable rusting at any welded seams. Uncoated galvanized tanks will leach zinc into the stored water and are not recommended in most instances - certainly not for water stored for human consumption. Concrete tanks leach a more benign substance - lime - into stored water and many are used around the world for rainwater storage.\n\nOne method of harvesting rain water has been is modular, scalable systems which are installable underground. These came as an evolution of a geosynthetic applications called Infiltration Tanks, which when stacked provide a void space volume which allows for the storing of water. Improved and more cost effective industrial design now allow for theoretically limitless storage of water underground. Examples of these modular structures are Atlantis Matrix Tanks used in the Manly Stormwater Treatment and Re-use project of Manly Council in Australia.\n\nIn some cities, installation of rainwater tanks may be mandatory, or may help a new building be approved. For example, in Victoria, Australia new houses which have rainwater tank connected to all flush toilets are given an additional 1-star of the required 5-star House Energy Rating. Some governments subsidise purchases of rainwater tanks or provide rebates in areas where they are considered an important means of water supply augmentation. In the United States, Santa Fe County, New Mexico requires a rainwater collection system on all new construction with greater than 2,500 square feet, mostly for landscaping purposes and to prevent over-reliance on wells but in some instances because ground water is prohibitively expensive to obtain, if even available.\n\nRainwater to supplement drinking water supplies may be seen as an alternative to other water supply options such as recycling or seawater desalination. Tanks are often perceived to have environmental costs that are comparatively lower than other water supply augmentation options.\n\nRainwater collection can be made compatible with centralised water supply by tapping it using an electropump.\n\nWidespread use of rain barrels also changes the amount of rainwater reaching the ground in a particular area and draining into streams. Depending on the climate, this either helps prevent erosion, sedimentation, and/or pollution, and can reduce the strain on stormwater drainage systems; or it could cause rivers to dry up and ponds to stagnate if the water is diverted to a different watershed. If collected water is used in the same watershed in which it is collected, rainwater collection actually can stabilize flow in rivers and provide more regular and filtered groundwater transfer into ponds.\n\nIn the State of Colorado, United States, the installation of rainwater collection barrels is subject to the Constitution of the State of Colorado, state statutes and case law. This is a consequence of the system of water rights in the state; the movement and holding of rainwater is inextricably linked with ownership of water rights and is enshrined in the constitution of the State of Colorado. The use of water in Colorado and other western states is governed by what is known as the prior appropriation doctrine. This system of water allocation controls who uses how much water, the types of uses allowed, and when those waters can be used. This is often referred to as the priority system or \"first in time, first in right.\" Since all water arriving in Colorado has been allocated to \"senior water right holders\" since the 1850s, rainwater prevented from running downstream may not be available to its rightful owner. In 2009, legislation in Colorado was enacted that permits capture of rain water for residential use subject to strong limitations and conditions. To be permitted, a residence may not be connected to a domestic water supply system serving more than 3 single-family dwellings. The permit must be purchased from the State Engineer's office and is subject to water usage restrictions.\n\nRainwater tanks or drums may be used inside a house to provide thermal mass for a trombe wall (or water wall). Rainwater Hog modular tanks invented by Sally Dominguez to fit within building structure were used in the Modabode House of the Future floor and on the foyer wall of the Department of Sustainability building in Anglesea, Victoria, harnessing the higher value of the stored rainwater to add effective thermal mass to the enclosed spaces.\n\nSpecially designed rainwater tanks can also be embedded in or under the concrete slab of a building (\"stab tank\").\n\nA house in Cape Schanck Victoria, Australia uses an internal rainwater tank to provide cooling to the living room in summer. During winter the tank is drained and wrapped in an insulating jacket. The tank also provides structural support to the roof, and excess water is used for domestic use including drinking.\n\n\n\n"}
{"id": "15700811", "url": "https://en.wikipedia.org/wiki?curid=15700811", "title": "State Administration of Work Safety", "text": "State Administration of Work Safety\n\nThe State Administration of Work Safety (SAWS; ), reporting to the State Council, is the non-ministerial agency of the Government of the People's Republic of China responsible for the regulation of risks to occupational safety and health in China.\n\n\n"}
{"id": "5126240", "url": "https://en.wikipedia.org/wiki?curid=5126240", "title": "Stephen Molyneux", "text": "Stephen Molyneux\n\nStephen Molyneux (born Walton, Liverpool, 24 February 1955) is a British educational technologist whose work as Microsoft Professor of Advanced Learning Technology and is an Apple Distinguished Educator has led to him influencing the use of technologies across the British School system. His use of technology across public life led to his resigning as a Justice of the Peace on 25 April 2009 due to his refusal to stop reporting on Twitter the outcome of public criminal hearings.\n\nAfter spending 16 years in the multimedia and education industry working for ATARI, AriolaSoft and the German Ministry of Education and Science, Molyneux returned to the UK in 1991 when he founded, together with John Rodgers of TIME, the European Multimedia Awards. During the 1990s the EMMAs became one of the most prestigious awards in the Multimedia Industry, attracting as sponsors and judges pioneers of multimedia such as Douglas Adams.\n\nFor his pioneering work he was appointed, as one of only 25 people world-wide, a visiting Fellow of the British Computer giant ICL in 1994.\n\nIn 1995, whilst holding the Microsoft Chair of Advanced Learning Technologies at the University of Wolverhampton, he developed one of the first Virtual Learning Environments.\n\nIn 1996, as a British Association for the Advancement of Science media fellow, he developed with Ed Briffa, Editor of the BBC Science programme Tomorrow's World, one of the first real-time online science magazines. The magazine was set up to report live from the Annual Festival of Science held at University of Birmingham.\n\nIn 2002 he was appointed by Estelle Morris, the then Secretary of Education as a member of the Post-16 eLearning Strategy Taskforce. The taskforce was chaired by Steve Morrison, CEO of ITV. The report was published in July 2002 under the title \"Getting on with IT\".\nIn 2003 he proposed and attracted funding for an \"e-Innovation Centre\" at the university which could combine the research and development skills of the higher education sector with that of industry to promote Internet-based start-up companies.\n\nIn October 2010, UK Prime Minister David Cameron announced £200 million to develop such technology centres as part of the government strategy to enhance UK growth.\n\nIn 2005 he was appointed advisor to the UK Deputy Chief of Defence Staff to monitor the evaluation of a Defence Training Review initiative.\n\nMolyneux worked closely with Cambridge University Press and Abilene Christian University in Texas on redefining the 'textbooks' and the use of technologies to support mobile learning.\nIn 2012 he founded the Tablet Academy as an education consultancy and teacher training organisation focusing on the use of tablet technology in the classroom and in 2014 took up a visiting post as International Professor of Global Education Leadership working alongside Dr. George Saltsman in the College of Education and Human Development at Lamar University. (As a visiting academic based in the UK, Molyneux is not listed as a full faculty member )\n\nHe is a former patron of Shropshire Young Enterprise and from 2003-2007 was Mayor of Oakengates, in Telford where he currently resides. He broadcasts occasionally on BBC Radio Shropshire Morning Show on issues relating to technology and in October 2009 was elected to the West Midlands regional committee of the British Science Association. As a former serving member of the Royal Air Force he is an active member of the Royal Air Force Association and works closely with the RAF STEM Ambassadors Education outreach programme at RAF Cosford and RAF Waddington. \n\nIn 2018 he relocated to Fuerteventura in the Canary Islands looking to support Schools and Young Learners on the Island in association with the Fuerteventura Technology Park.\n\n"}
{"id": "18587028", "url": "https://en.wikipedia.org/wiki?curid=18587028", "title": "Suitability model", "text": "Suitability model\n\nA suitability model is a model that weights locations relative to each other based on given criteria. Suitability models might aid in finding a favorable location for a new facility, road, or habitat for a species of bird. Overlay analysis is a common method for creating a suitability model which involves using GIS techniques and software. Overlay techniques were originally advanced by Ian McHarg who used a manual overlay cartographic process which he describes in his 1969 book Design with Nature. With the advancement of computer mapping software, suitability modeling has become much easier and faster to implement, and today it is used for many varying tasks.\n\nThere are seven general steps required to create an acceptable suitability model: \n\nWithout a clear understanding of the problem that needs to be solved a suitability model cannot be successful. All other steps in the process will contribute to the objective of solving this problem. The components of this objective should also be defined, as well as a way of knowing when the problem has been solved. Consider the issue of deforestation, to lower deforestation rates a suitability model could be created to model areas most likely to be deforested in the immediate future; laws and regulating entities could then be focused on those areas most susceptible to deforestation. The overall goal of the deforestation suitability model would be to slow the rate of deforestation.\n\nThe complexity of most suitability modeling problems can be overwhelming and confusing; for this reason, it is advisable to break the model into submodels. For deforestation there are many different drivers, therefore a variety of submodels would be needed. Population, population density, movement of people, elevation, slope, land cover type, hydrology, location of protected areas, soil type, laws, roads and infrastructure, the list could go on, all of these things affect where deforestation happens and the intensity. Combining these factors could lead to a submodel for physical environment (elevation, slope, land cover, land use, soil type, and hydrology), for built environment (roads, infrastructure, and other relevant transportation networks), and for demographic characteristics (population, population density, population growth rate, and poverty rate) \n\nEach submodel should be defining an aspect of the overall model, and only submodel factors which contribute to solving the original problem should be included in a submodel. It is in this step that data must be gathered and layers created; for example, it may be known that deforestation usually happens a certain distance from city/road/agricultural areas, therefore a Euclidean distance tool (within a GIS software package) could be used to create a distance raster around these areas.\n\nThere are many different datasets going into the model, all with varying number systems; this means that attempting to combine these datasets would give meaningless results. Therefore, a common number scale should be chosen (usually 1 to 9 for a weighted overlay and 0 to 1 for a fuzzy overlay; with larger values signifying more favorable areas) and each dataset reclassified to the new scale (there should be a tool for this in most GIS applications).\n\nIf there is strong evidence that some factors contribute more to the main goal these factors should be weighted based on their level of contribution. For instance, focusing specifically on deforestation in Africa, previous research shows that one of the main causes of deforestation is fuel wood extraction; therefore, variables associated with fuel wood extraction should be weighted more heavily than other variables. It should be noted that weighting should not be done if a fuzzy overlay is used.\n\nTo complete the model, all factors must be combined, usually through a weighted overlay or fuzzy overlay technique. For a weighted overlay all the factors would be added together and reclassified to form a new data layer where high values signify more favorable locations and low values less favorable locations. A fuzzy overlay analysis produces the same type of results but through more complex methods.\n\nOnce the suitability model is complete the results should be analyzed. It is always a good idea to examine the results closely to verify that they make sense and no mistakes were made. Before the model is used the results should also be verified and validated. Ideally, the value of predictive methods based in habitat suitability to estimate for instance the population size of common species should be tested before conducting large-scale monitoring, rather than a posteriori. Although logistically challenging, this can be achieved by designing monitoring programs including an intensive sampling of abundance in ad hoc reference areas of variable size. After the analysis is complete locations can be selected using the model and this information can be applied to the original problem.\n"}
{"id": "351080", "url": "https://en.wikipedia.org/wiki?curid=351080", "title": "Transparency (projection)", "text": "Transparency (projection)\n\nA transparency, also known variously as a viewfoil, foil, or viewgraph, is a thin sheet of transparent flexible material, typically cellulose acetate, onto which figures can be drawn. These are then placed on an overhead projector for display to an audience. Many companies and small organizations use a system of projectors and transparencies in meetings and other groupings of people, though this system is being largely replaced by video projectors and interactive whiteboards.\n\nTransparencies can be printed on laser printers or copiers. Specialist transparencies are available for use with laser printers that are better able to handle the high temperatures present in the fuser unit. For inkjet printers, coated transparencies are available that can absorb and hold the liquid ink – although care must be taken to avoid excessive exposure to moisture, which can cause the transparency to become cloudy; they must also be loaded correctly into the printer as they are only usually coated on one side.\n\nUses for transparencies are as varied as the organizations that use them.\n\nCertain classes, such as those associated with mathematics or history, use transparencies to illustrate a point or problem. Math classes in particular use a roll of acetate to illustrate sufficiently long problems and to create illustrations a computer cannot, due to a lack of math symbols on a standard computer keyboard. This problem is typically limited to high school and college-level mathematics, because of the inclusion of algebra and calculus courses, respectively. In recent years, more and more colleges are switching to digital projectors and PowerPoint presentations.\n\nAerospace companies, like Boeing and Beechcraft, used transparencies for years in management meetings in order to brief engineers and relevant personnel about new aircraft designs and changes to existing designs, as well as bring up illustrated problems.\n\nSome churches and other religious organizations used them to show sermon outlines and illustrate certain topics such as Old Testament battles and Jewish artifacts during worship services, as well as outline business meetings.\n\nMany overhead projectors are used with a flat-panel LCD which, when used this way, is referred to as a spatial light modulator or SLM. Data projectors are often based on some form of SLM in a projection path. An LCD is a transmissive SLM, whereas other technologies such as Texas Instrument's DLP are reflective SLMs. Not all projectors use SLMs (e.g., some use devices that produce their own light rather than function as transparencies). An example of non-SLM system are organic light-emitting diodes (OLEDs).\n\n"}
{"id": "2638070", "url": "https://en.wikipedia.org/wiki?curid=2638070", "title": "Trekking pole", "text": "Trekking pole\n\nTrekking poles (also known as hiking poles, hiking sticks or walking poles) are a common hiking accessory used to assist walkers with their rhythm and provide stability on rough terrain.\n\nWhen in use, trekking poles resemble ski poles as they have many features in common, such as baskets at the bottom, rubber-padded handles and wrist straps. Their maximum length is usually 135 cm (54 inches), however, unlike ski poles, they are often made in two or three sections and can be extended and retracted as necessary for use and collapsed for storage or transport. When fully retracted it may be possible to attach them to a backpack. Some poles come with spring-loaded sections to aid walking under normal conditions and to reduce wrist strain, but such devices may only add unwanted weight and noise to the poles. They are usually made from lightweight aluminum or carbon fiber.\n\nDescendants of the common walking stick, trekking poles are usually used by hikers for the same reasons — to provide some rhythm to their walking pace and for added support. On flat, smooth terrain they really aren't necessary although using them can increase the exercise a hiker gets from the trip, as well as increase the speed. But on less certain terrain, or steep slopes, they provide useful lateral stability, and many turn to them for help with knee pain. They can also be used as aids when climbing rocks or boulders, to probe the depth of mud or water and facilitate a crossing. When traversing steep slopes for long distances, some hikers make one pole shorter than the other to make those trips feel more as if they were taking place on level ground.\n\nSome backpacking tents are designed to use trekking poles as tent poles. Along the same lines, trekking poles can be used to set up a Bivouac shelter. Hikers who take to snowshoes in winter find trekking poles especially useful.\n\nThey can also be used in Nordic walking in a rural or urban environment.\n\nThe Appalachian Trail Conservancy (ATC) estimates that pole usage rates on the Appalachian Trail vary from 90% among thru-hikers to 10–15% among day hikers.\n\nSome hikers have complained that pole use can leave a visible impact on the surrounding trail, for instance poking visible holes in the ground and damaging adjacent vegetation. In particular the most common complaint is that the carbide tips leave visible white scratches on rock, and make scraping sounds. All these can detract from the wilderness experience.\n\nThe Appalachian Trail Conservancy (ATC) recommends several measures to mitigate the environmental impact of trekking poles in accordance with Leave No Trace principles of low-impact backcountry recreation. Hikers, it says, should not only be aware of what they put their poles into, they should remove the pole baskets unless hiking in snow and use rubber tips to avoid scratch marks on rocks. On level sections, or in areas where the potential for adverse impact is high, the ATC suggests putting the poles away entirely.\n\n\"Nordic walking\", a type of walking with poles, has been found to have beneficial effects on resting heart rate, blood pressure, exercise capacity, maximal oxygen consumption, and quality of life in patients with various diseases, and to be superior to brisk walking without poles and in some endpoints to jogging.\n\n"}
{"id": "43370238", "url": "https://en.wikipedia.org/wiki?curid=43370238", "title": "Uptime On", "text": "Uptime On\n\nUptime On Corporation (Uptime On) is an American technology and consulting corporation founded in 2011 and incorporated in 2012 as a Maryland Corporation, with headquarters in Baltimore, Maryland, US. Uptime On manufactures and markets computer hardware and software, and offers infrastructure, hosting and consulting services in areas ranging from PCs to business systems.\nThe name of the company is derived using industry accepted terminology for \"uptime\" referring to a system's status of being \"on\" or \"off\". \nUptime On is primarily focused on backups and supports most backup software in use today.\n\nIn the 1990s web-based-backup software started to surface and was supported by the founder of Uptime On Corporation, Brian Furphy, while he managed technical support for Tivoli customers. In 1999, Mr. Furphy launched a start-up company called HostLabs, a pioneer in the hosting industry, with patents, products and services serving internet users.\n"}
{"id": "29762298", "url": "https://en.wikipedia.org/wiki?curid=29762298", "title": "Werner E. Reichardt", "text": "Werner E. Reichardt\n\nWerner E. Reichardt (30 January 1924 – 18 September 1992) was a German physicist and biologist who helped to establish the field of biological cybernetics. He co-founded the Max Planck Institute for Biological Cybernetics, and the \"Journal of Biological Cybernetics\".\n\nAs a young student, Werner Reichardt was a pupil in the laboratory of Hans Erich Hollmann, a pioneer of ultra-shortwave communication. Because of his knowledge he was drafted in 1941 to the German air force as a radio technician. There he came into contact with resistance elements, and built a secret radio link with the Western Allies. In 1944 Reichardt was arrested by the Gestapo and sentenced to death, but escaped, and hid in Berlin until the end of the war.\n\nFrom 1946 to 1950 he studied physics at the Technical University of Berlin. From 1950 he was a doctoral student of Ernst Ruska, studying solid state semiconductors at the Fritz-Haber-Institut of the Max-Planck-Gesellschaft, and received his doctorate in 1952. From 1952 to 1954 he was an assistant at the Institute where his teacher Max von Laue was a large influence to his later research. During the war, Reichardt had known Bernhard Hassenstein, who had studied otptomotor turning behaviour after the war. Realising these experiments could be formalised in a similar way to electronics experiments, he developed interdisciplinary theories of motion perception. In 1954, Reichardt became a Postdoctoral Fellow at the California Institute of Technology at the invitation of Max Delbrück. From 1955 he was assistant at the Max Planck Institute for Biophysical Chemistry in Göttingen under Karl Friedrich Bonhoeffer. In 1958 he founded together with Bernhard Hassenstein and Hans Wenking the cybernetics research group at the Max-Planck-Institute of Biology in Tübingen. In 1968 the department was transformed into the independent Max Planck Institute for Biological Cybernetics.\n\nReichardt died at the age of 68 years after collapsing at the end of a symposium organised in his honour.\n\nReichardt's findings have contributed to understanding of information processing in nervous systems. From joint work (with Bernhard Hassenstein and Hans Wenking) on the visual system of insects and its effect on the flight orientation, the correlation model developed the idea that the visual system of man could be similarly investigated, and led to a general theory of motion perception\n\n\"See Motion perception#First-order motion perception\"<br>\nIn the 1950s, Reichardt, along with Hassenstein proposed a model of how a neuron receiving input from photoreceptors, which only respond to changes in luminance, could be used to compute motion. Each photoreceptor, responded to a change in luminance at a given location in visual space. Comparison of the phase shift of activity in adjacent cells indicated the direction of movement from one neuron's receptive field to the other. This model of micro-circuitry became known as a Reichardt detector. Whilst there is experimental evidence consistent with the hypothetical behaviour of a Reichardt detector, the corresponding circuitry has not yet been found.\n\nIn honor of Reichardt's pioneering work, the Tübingen cluster of excellence Werner Reichardt Centre for Integrative Neuroscience (CIN; founded 2007/2008) was named after him.\n\n\n"}
