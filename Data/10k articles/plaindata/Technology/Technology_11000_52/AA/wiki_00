{"id": "23445475", "url": "https://en.wikipedia.org/wiki?curid=23445475", "title": "Advanced Fuel Cycle Initiative", "text": "Advanced Fuel Cycle Initiative\n\nThe Advanced Fuel Cycle Initiative (AFCI) is an extensive research and development effort of the United States Department of Energy (DOE). The mission and focus of AFCI is to enable the safe, secure, economic and sustainable expansion of nuclear energy by conducting research, development, and demonstration focused on nuclear fuel recycling and waste management to meet U.S. needs.\n\nThe program was absorbed into the GNEP project, which was renamed IFNEC.\n\n\nThe AFCI is an extensive RD&D effort to close the fuel cycle. The different areas within the AFCI are separated into campaigns. The RD&D of each campaign is completed by the United States Department of Energy's national laboratories.\n\n\nThe mission of the Transmutation Fuels Campaign is the generation of data, methods and models for fast reactor transmutation fuels and targets qualification by performing RD&D activities on fuel fabrication and performance. The campaign is led by Idaho National Laboratory.\n\nThe mission of the Reactor Campaign is to develop advanced recycling reactor technologies required for commercial deployment in a closed nuclear fuel cycle. The Reactor Campaign is led at Argonne National Laboratory.\n\nThe mission of the Separations Campaign is to develop and demonstrate industrially deployable and economically feasible technologies for the recycling of used nuclear fuel to provide improved safety, security and optimized waste management. The campaign is led by Idaho National Laboratory.\n\nThe mission of the Waste Forms Campaign is to develop and demonstrate durable waste forms and processes to enable safe and cost-effective waste management as an integral part of a closed nuclear fuel cycle by establishing a fundamental understanding of behavior through closely coupled theory, experiment and modeling. This campaign is led at Argonne National Laboratory.\n\nThe mission of the Grid Appropriate Reactor Campaign is to enable U.S. leadership in the global expansion of nuclear energy by conducting research, development, and demonstration of technologies and innovative reactor designs that offer enhanced safety, security, and proliferation resistance and that are appropriately sized for infrastructure-limited countries.\n\nThe mission of the Safeguards Campaign is to ensure that domestic fuel cycle facilities fully meet requirements under regulatory frameworks; thereby assuring that nuclear materials have not been diverted or misused. The campaign is led at Sandia National Laboratories.\n\nThe mission of the Systems Analysis Campaign is to conduct systems-wide analyses of nuclear energy development and infrastructure deployment to enable a requirements-driven process for all technical activities, and to inform strategic planning and key program decisions. The campaign is led at Idaho National Laboratory.\n\nThe mission of the Modeling and Simulation Campaign is to rapidly create, and deploy “science-based” verified and validated modeling and simulation capabilities essential for the design, implementation, and operation of future nuclear energy systems with the goal of improving future U.S. energy security. These AFCI activities are led at Argonne National Laboratory.\n\nThe mission of the Safety and Regulatory Campaign is to ensure that regulatory and licensing requirements for future facilities and technologies are appropriately considered and incorporated during the course of technology development.\n\n"}
{"id": "47288071", "url": "https://en.wikipedia.org/wiki?curid=47288071", "title": "Altitude3.Net", "text": "Altitude3.Net\n\nAltitude3.Net is an electronic business development platform that allows to create web and mobile solutions along with interactive communication strategies. The platform has the same functionalities than a content management system (CMS) and communicates with other systems (accounting systems, manufacturing management software (MRP), business management software (enterprise resource planning (ERP)), database, Excel files, XML, CSV or all other kinds of structural data).\n\nNmédia solutions developed Altitude3.Net in 2001 using Microsoft's .NET Framework technology. The platform is currently using the 4.5 version of Microsoft’s Framework.\n\nIn 2001, Nmédia solutions created the content management system Altitude. As it went on, many versions were developed:\n\nThe Altitude3.Net platform is structured in many modules:\n\n\n\n"}
{"id": "18785633", "url": "https://en.wikipedia.org/wiki?curid=18785633", "title": "Amiga video connector", "text": "Amiga video connector\n\nThe Commodore A520 RF modulator connects to the video port and outputs composite video and RF video.\n\nThe Amiga video connector is used on all Commodore Amiga computers. The 23-pin D-subminiature has signals for genlocking, RGB analog video (4096 colours), and a digital Red-Green-Blue-Intensity signal (16 colours).\n\nThe refresh frequency is 15 or 30 kHz HSync for standard Amiga video modes. This is not compatible with most VGA monitors but usually with most Televisions. A Multisync monitor is required for some higher resolutions.\n\n\n"}
{"id": "9773098", "url": "https://en.wikipedia.org/wiki?curid=9773098", "title": "Andrews Space", "text": "Andrews Space\n\nAndrews Space was founded in 1999 by Jason Andrews and Marian Joh to be a catalyst in the commercialization, exploration and development of space. Originally named Andrews Space & Technology, the company shortened its name in 2003 to Andrews Space. Over its life the company developed many unique technologies and space transportation architectures for the US Government (NASA, DARPA, others) and commercial customers. The company is now Spaceflight Systems, a subsidiary of Spaceflight Industries, Inc.\n\nAndrews Space developed a number of innovative technologies and space transportation concepts including:\n\n\nAndrews Space worked for most branches of the US Government.  Noteworthy efforts include:\n\n\nFrom 2008-2012 Andrews Space developed a series of cubesat and nanosat subsystems and components including the CORTEX avionics suite, a lithium ion battery, solar arrays and numerous spacecraft subsystems:\n\n\nAndrews Space was founded in 1999 by Jason Andrews and Marian Joh to be a catalyst in the commercialization, exploration and development of space. Originally named Andrews Space & Technology, the company shortened its name in 2003 to Andrews Space. Over its life the company developed many unique technologies and space transportation architectures for the US Government (NASA, DARPA, others) and commercial customers. These include the Gryphon horizontal takeoff horizontal landing system using the \"Alchemist\" Air Collection and Enrichment System (ACES) as part of the NASA Space Launch Initiative. The Peregrine small launch vehicle was developed as part of the DARPA FALCON program and used a reusable first stage with expendable solid motor upper stages. In 2003 Andrews Space was one of four companies selected by NASA to study commercial cargo logistics to the International Space Station. This later lead to the NASA COTS (Commercial Orbital Transportation System) program to fly cargo to the ISS commercially. Andrews Space was chosen in 2006 as one of six finalists for NASA's Commercial Orbital Transportation Services (COTS) program, though they were not subsequently selected for funding the COTS R&D by NASA. Andrews Space later supported Rocketplane Kistler's COTS effort. In 2010 and 2011 Andrews Space support Orbital's Cygnus COTS effort.\n\nIn November 2010, Andrews Space was selected by NASA for consideration for potential contract awards for heavy lift launch vehicle system concepts, and propulsion technologies. In 2011 Andrews Space was one of four companies selected by the US Air Force as part of its Rocket-back Booster System demonstrator program. As part of these awards the company was awarded a $250M IDIQ contract.\n\nIn 2013 Andrews Space was awarded a contract by the US Army SMDC to build the Kestrel Eye 2 small imaging spacecraft. This was the result of a pivot by Andrews Space, begun in 2008, to move into space hardware production and very small spacecraft. While the Kestrel Eye 2 program was delayed and ultimately cancelled due to funding cuts as a result of Sequestration, much of the technology would make it to orbit as part of the BlackSky Pathfinder spacecraft, which was launched successfully in September 2016.\n\nFrom 2011-2013 Andrews Space developed and built the Cargo Module Power Unit for Orbital's Cygnus cargo delivery spacecraft. The CMPU was responsible for supplying power and managing power distribution to powered experiments inside the Cygnus spacecraft during their trip to the ISS. Andrews Space also licensed the magnetic torque rod design from Sinclair Interplanetary and produced many units which were used in a range of spacecraft, including those for the BlackSky Pathfinder / Global spacecraft, the Skybox Skysat spacecraft, as well as others.\n\nIn 2015 Andrews Space was merged with Spacefight Inc. and BlackSky Global LLC and operated as Spaceflight Systems, a separate operating entity under the parent company of Spaceflight Industries.\n\n\n"}
{"id": "8152036", "url": "https://en.wikipedia.org/wiki?curid=8152036", "title": "Ascom EasyTicket", "text": "Ascom EasyTicket\n\nThe Ascom EasyTicket is a railway ticket issuing system used in Britain, consisting of a series of self-service (passenger-operated) machines at railway stations. Having been introduced in 2003 on a trial basis by several Train Operating Companies (TOCs) at various stations, the system did not spread into common usage, and most machines have since been removed.\n\nAscom AG was created in 1987 through a merger between three major telecommunications companies in Switzerland, although its origins can ultimately be traced back to the establishment of the Swiss Federal Telegraph Workshops in 1852. The Autelca AG division, which had been acquired in 1963 by one of Ascom AG's constituent companies, was involved in the manufacture of ticket vending machines (TVMs); it provided British Rail with the B8050 self-service machine, hundreds of which were installed at stations across Britain from 1989 onwards.\n\nThe EasyTicket system was developed while the Transport Revenue division, as it was then known, was still under Ascom's ownership; but as part of an attempt to focus on the telecommunications sector, the division was sold in August 2005 to Affiliated Computer Services, Inc for 130 million Swiss Francs.\n\nThe company's first attempt to move on from the successful B8050 machine was the B8070, a small evolutionary upgrade initially developed and delivered in 1999. In the meantime, however, a more significant, revolutionary design solution was being sought: Ascom realised that the late-1980s B8050 technology was no longer suitable for the modern railway environment, given the increasing use of credit and other payment cards, the anticipated adoption of the Chip and PIN secure-payments system, new disability regulations and the effects privatisation had on fragmenting and expanding the fare structure on Britain's railways.\n\nThe EasyTicket system (official code number TIS9000) was developed with these considerations in mind, and features:\n\nMachines began to be installed on a trial basis in March 2003, with a single machine (low-height, cash-and-card version) at Gatwick Airport station. More followed, as shown: \"(Train Operating Company names are those current at the time of installation)\"\n\nFollowing the trial at Gatwick Airport, Gatwick Express, the Train Operating Company responsible for providing ticketing facilities at Gatwick Airport and London Victoria, elected to install an additional five at the former and one at the latter. There were previously six Ascom B8050 machines at Gatwick Airport, so this turned out to be a like-for-like replacement. The machine at London Victoria is close to the Gatwick Express ticket office windows on the dedicated Express platforms; there were two B8050 machines there until around 1991.\n\n"}
{"id": "44847172", "url": "https://en.wikipedia.org/wiki?curid=44847172", "title": "Beard oil", "text": "Beard oil\n\nBeard oil is a cosmetic product that is used to nourish the skin under the beard, as well as the beard itself in order to keep it \"soft, shiny, and smooth\". Beard oil mimics the natural oils produced by skin, such as sebum, and is composed mainly of carrier oils and essential oils.\n\nBeard oil products are a blend of one or more ingredients, such as jojoba oil, argan oil, coconut oil, grape seed oil, hempseed oil that are used to address specific beard problems such as itching, sensitive skin or dryness by going straight to the follicle in a similar fashion to hair conditioner.\n\nBeard oil is usually scented (though unscented versions are available) using a variety of natural and artificial, typically masculine, scents ranging from sweet to bitter and can be used to substitute cologne or aftershave.\n\nBeard oil acts as a moisturizer that goes straight to the hair follicle and prevents hair from growing brittle, especially in cold, windy environments as these weather conditions cause the natural moisture of the beard area to wick. Hydration around this area helps prevent flaking, dandruff and skin dryness. Some brands of beard oil may contain added vitamins or nutrients such as vitamin E. This moisturization prevents general itchiness and irritation of the skin below the beard. \n\nBeard oil also works below the surface of the skin by working as a grooming tool, making the beard itself more manageable, neat and smooth. \n\nBeard oil also improves the way a beard looks by giving it a slight shine. This makes a beard look healthier as well. \n\nBeard oil can be applied daily and is generally put on the beard area after showering or cleansing in order to have the opened pores easily absorb the oil.\n\nBeard oils are manufactured with the intent to emulate the natural oils produced by the skin, known as sebum. Carrier type oils generally contain vitamins A, D and E, as well as a high concentration of linoleic acids and low concentrations of oleic acids. Natural and synthetic scents may be added to the oil in order to enhance its properties and aroma. Natural scents (essential oils, absolutes, and extracts) are derived differently than synthetic scents and contain different chemical compounds that enhance their functionality. This can give beard oils properties such as antibacterial, deodorant, anti-inflammatory and anti-aging. Beard oils sold in the United States making claims to be therapeutic must be approved by the FDA.\n\nNatural beard oils are typically created with both carrier oils and essential oils, absolutes, and/or extracts. Carrier oils are oils that make up the bulk of the volume of the product and are formulated to condition the skin and beard. The most popular types of carrier oils are coconut, grape seed, jojoba, saffron and argan. They are used as a natural fragrance for the beard oils, but also may have aromatherapy benefits. Since essential oils, absolutes, and extracts are usually very strong they may cause chemical burns or skin irritation, they are commonly paired up with carrier oils in order to dilute them.\n\nAwareness of beard oil and beard-grooming products has been increasing due to the fact that industries have become \"more permissive\" of facial hair.\n"}
{"id": "17050447", "url": "https://en.wikipedia.org/wiki?curid=17050447", "title": "CBU-98/B", "text": "CBU-98/B\n\nThe CBU-98/B (also known as a \"Direct Airfield Attack Combined Munition\" or DAACM) was a planned 385 kg (850 lb) anti-airfield cluster bomb, which consisted of 8 BLU-106/B BKEP (Boosted Kinetic Energy Penetrator) projectiles and 24 British HB876 mines in an SUU-64/B Tactical Munitions Dispenser (TMD). The BLU-106/B was cancelled, and thus the CBU-98/B, was never produced .\n\n"}
{"id": "9618303", "url": "https://en.wikipedia.org/wiki?curid=9618303", "title": "Cary Karp", "text": "Cary Karp\n\nCary Karp ( born 3 April 1947), is a museum curator based in Sweden, has been instrumental in developing online facilities for museums in the context of the International Council of Museums (ICOM). In particular, he was central in promoting and establishing the \".museum\" top-level domain as President of the international Museum Domain Management Association (MuseDoma). He has also been a principal contributor to establishment of standards for registration of internationalized domain names.\n\nKarp has a PhD in musicology and is Associate Professor of Organology at Uppsala University in Sweden. He has been professionally involved with museums since the late 1960s. He was curator of the musical instrument collections at the Music Museum in Stockholm from 1973 to 1990, especially concerned with conservation, and rose to be the museum's Deputy Director during the 1980s. He has been at the Swedish Museum of Natural History since 1990, first as Director of the Department of Information Technology, and currently as Director of Internet Strategy and Technology.\n\nCary Karp has been the Director of Internet Strategy for ICOM. Within the context of IT development for museums internationally, he has been:\n\n\n"}
{"id": "3841160", "url": "https://en.wikipedia.org/wiki?curid=3841160", "title": "Constant fraction discriminator", "text": "Constant fraction discriminator\n\nA constant fraction discriminator (CFD) is an electronic signal processing device, designed to mimic the mathematical operation of finding a maximum of a pulse by finding the zero of its slope.\nSome signals do not have a sharp maximum, but short rise times formula_1.\n\nTypical input signals for CFDs are pulses from plastic scintillation counters, such as those used for lifetime measurement in positron annihilation experiments. The scintillator pulses have identical rise times that are much longer than the desired temporal resolution. This forbids simple threshold triggering, which causes a dependence of the trigger time on the signal's peak height, an effect called \"time walk\" (see diagram). Identical rise times and peak shapes permit triggering not on a fixed threshold but on a \"constant fraction\" of the total peak height, yielding trigger times independent from peak heights.\n\nA time to digital converter assigns timestamps. The time to digital converter needs fast rising edges with normed height.\nThe plastic scintillation counter delivers fast rising edge with varying heights.\nTheoretically, the signal could be split into two parts. One part would be delayed and the other low pass filtered, inverted and then used in a variable gain amplifier to amplify the original signal to the desired height. Practically, it is difficult to achieve a high dynamic range for the variable gain amplifier, and analog computers have problems with the inverse value.\n\nThe incoming signal is split into three components.\nOne component is delayed by a time formula_2, with formula_3\n- it may be multiplied by a small factor to put emphasis on the leading edge of the pulse -\nand connected to the noninverting input of a comparator.\nOne component is connected to the inverting input of this comparator.\nOne component is connected to the noninverting input of another comparator.\nA threshold value is connected to the inverting input of the other comparator.\nThe output of both comparators is fed through an AND gate.\n\nNote from this description that a discriminator without that constant fraction would just be a comparator.\nTherefore the word discriminator is used for something different\n(namely for an FM-demodulator).\n\nNote also that often the logic levels are shifted from -15 V < low < 0 < high < 15 V delivered by the comparator to 0 V < low < 1.5 V < high < 3.3 V needed by CMOS logic.\n\nIf the discriminator triggers a sampler with a following comparator this is called a single channel analyzer (SCA).\nIf an Analog-to-digital converter is used, this is called a multi channel analyzer (MCA).\n\nhttp://www.ortec-online.com/download/Fast-Timing-Discriminator-Introduction.pdf\n\n"}
{"id": "21183438", "url": "https://en.wikipedia.org/wiki?curid=21183438", "title": "Cramster.com", "text": "Cramster.com\n\nCramster (now Chegg Study) provides online homework and textbook help for college and high school students in areas such as math, science, engineering, humanities, business, and writing help. Cramster uses a freemium model, allowing students to pay for or earn access to premium services. Founded in 2002 by Aaron Hawkey, Robert Angarita and Kavé Golabi, the company is headquartered in Pasadena, CA.\n\nCramster launched publicly in 2003. A few years later, Cramster introduced its Q&A Board feature, which allowed interaction between members to solve homework problems.\n\nCramster secured $3 million in Series A funding in 2008 and $6 million in Series B funding in 2010 led by Shai Reshef, who was then appointed company Chairman. Cramster had 100,000 active users in November 2008. In 2010, Cramster was acquired by Chegg, a provider of textbooks for the college market.\n\nCramster also owns and operates the Facebook application Courses 2.0, which has more than 400,000 users and was voted 4th best application by PC Magazine.\n\nIn April 2012, Chegg, the parent company, consolidated its Cramster operations to Chegg.com\n"}
{"id": "10626981", "url": "https://en.wikipedia.org/wiki?curid=10626981", "title": "Da Vinci Systems", "text": "Da Vinci Systems\n\nda Vinci Systems was a digital cinema company founded in 1984, based in Coral Springs, Florida and wholly owned by JDSU. It was known for products like the 888, 2K and 2K Plus (hardware based color correctors), TLC, Resolve (GPU-based color grading and digital mastering systems) and Revival (film restoration and remastering systems) and its innovations include the color control panel based on trackballs and other discrete controls that enable colorists to control the software that manipulates motion picture images.\n\nAs one of the earliest pioneers in post production products, da Vinci Systems introduced several innovative products and was considered a significant player in the post production industry during its 25 years of operation. da Vinci Systems equipment was initially developed by Video Tape Associates (VTA) in 1982 for use by the Hollywood, Florida, USA-based production/post production facility to alter and enhance colors from scanned film and video tape. The Wiz system, as it was later known, was marketed to other post production facilities, laying the foundation for the creation of the colorist and the post production color suite.\n\nIn September 2009, after the liquidation of the company, the assets of da Vinci Systems were acquired by Blackmagic Design, an Australian digital cinema company and manufacturer known for its products based in digital cinema and cameras.\n\n\n\n\n\nThe Wiz was the predecessor to the da Vinci Classic color corrector and was built in 1982 by VTA Technologies in Ft. Lauderdale. It was built on an Apple computer, the program was stored in EPROM and the list could be backed up to mini cassettes. \nThe Wiz was the first color correction system to have a customized external control panel and was also the first color corrector with internal primary and secondary processing. Prior to that, the primaries in the telecine were used. The Wiz had 10 vector patented secondary color correction. \nThe first two systems were bought by Editel, Chicago, which at the time used the color corrector on Bosch Fernseh's FDL60 telecine.\n\nda Vinci Classic analog system was manufactured from 1985 to 1990 and had customized external control panel with internal primary, secondary processing and an internal NTSC encoder. It ran on a Motorola 68000 Multi Bus 1 system computer. The program and color correction list were stored on a 20MB MFM hard disk, with backup to a 5.25\" floppy disk.\n\nIts features include:\n\nOptions include:\nUsed with FDL60/90 and MK3 telecines (Not URSA) and tape-to-tape. Early models had knob only color correction controls; trackball control was introduced later.\n\nda Vinci Renaissance was the analog system that followed the Classic and was manufactured from 1990 to 1993. It was similar to the Classic but ran on Motorola 68020 Multi Bus 1 system with a 3.5\" floppy.\n\nIts features included:\nThe early/budget 68000 models had two control panels and 16 vector secondaries, the same as the Classic. The later 68020 versions usually feature Kilovectors, advanced secondary correction and had three control panels.\n\nOptions included:\n\n12v panels were used on early versions with 5v panels with separate PSU on later versions (not interchangeable with 12v panels). Both 525 and 625 standards were offered and a B&W menu monitor was used. Often connected to a FDL60, FDL90, MK3 or URSA telecine, it was also used with videotape machines for tape-to-tape grading. Some versions used an extra interface module (the EMC unit) to function with the URSA serial control busses. Normally used with a separate TLC editor (1 or 2), an additional interface is required for this on URSA installations.\n\nda Vinci Renaissance 888 was similar to the above system, but had 888 digital video processing in place of the analog video processing. This system was manufactured from 1992–1998.\n\nIts features included:\n\nAdditional options for the digital 888 included:\n\nEarly versions had a double backplane chassis (4:2:2 only) and used an extra interface module, the EMC, to function with URSA control busses. 888s were normally used with a separate TLC1 or 2 editors and an additional interface was required for this on URSA installations.\n\nda Vinci also made the da Vinci Light. This was not marketed, so not many were sold. It is a da Vinci DUI 888 without the digital 888 cards. The telecine interface card controlled the telecine's internal color corrector. This came in two configurations: the first was the DUI with an SGI Indy workstation; the second DUI system used an SGI O2 workstation. These systems supported da Vinci's new control panels.\n\nThe da Vinci 2K Color Corrector, manufactured starting in 1998, was a completely revised color correction system that supported HDTV, SDTV, and 2K formats. The later system, the 2K Plus with improved color corrector tools was used on high-end DataCines and telecines like Thomson-Grass Valley's Spirit Datacine and Cintel's C-Reality & ITK Millennium. The 2K operated with a 4:2:2, 4:4:4 or 8:4:4 input in NTSC or Pal. In HDTV it operated with either 4:2:2 or 4:4:4 inputs. Many were interfaced to the Spirit DataCine.\n\nIts features included:\n\n2K and 2K Plus systems were intended for use with the then latest generation of telecines including Spirit, URSA or similar, C-Reality, Rascal, Sony and ITK telecines. MK3 / FDL / Quadra types were not supported. Many USA delivered 2Ks were supplied with an onboard TLC2 which supported 24/30 dual sync functionality for 24P operation. A TLC Assistant station comprising another IBM PC was available for dual operator installations. Alternatively, a standalone TLC2 of the traditional type could be interfaced if it was required.\n\nIn addition to telecine control, 2Ks were often used for tape-to-tape, virtual telecine and DDR applications.\n\nThe TLC was an edit controller for telecines and VTRs. It provided accurate 2/3 editing. TLC 1 was originally made in Moorpark, California, later TLC was acquired by da Vinci and the TLC 2 was released. The da Vinci DUI 888 had an option to have a built-in TLC built. If the TLC is not built in, an external A/B switch box is needed to switch control between the TLC and other Color Controllers. Some versions had a separate CPU and Telecine interface rack.\n\nPromoted for use with non-linear storage/SAN sources and a 2K or 2K Plus, the da Vinci Splice was to provide data management to/from DPX files (up to 2048x1556) to SD/HD/HSDL with a built-in DVE for XYZ sizing, rotation etc., with subsequent color correction in the conventional fashion through the 2K. Built with Resolve’s Transformer II, Splice had Resolve’s basic conform and I/O features, but was promoted to extend the life and capabilities of a 2K. Very few were delivered.\n\nResolve, when launched in 2004, was a next-generation color grading system and the first system to use multiple parallel processing engines within normal PC computer infrastructure for real time 2K resolution color grading. Powered at launch by da Vinci’s proprietary hardware cards, known as PowerPlant cards, Resolve delivered real time HD and up to 4K resolution non-linear color grading.\n\nResolve was the first scalable color grading system offering multiple levels of acceleration, features and capabilities, providing colorists with exacting and intuitive color control over static or moving objects. Resolve scaled lower-resolution SD and DV formats to HD, 2K and 4K without compromising quality by using proprietary da Vinci engineered Transformer technology.\n\nThe first generation Resolve systems were called the Resolve RT and Resolve DI. In 2008, DaVinci offered the Resolve R series (R-100, R-200, R-250, R-300, R-350, R-4K, R-3D), replacing DaVinci’s proprietary parallel processing PowerPlant cards with parallel processing NVIDIA GPUs, enabling super-computing image processing and a significantly greater feature set.\n\nResolve features offered by da Vinci Systems included:\n\n"}
{"id": "14069368", "url": "https://en.wikipedia.org/wiki?curid=14069368", "title": "Deep-sub-voltage nanoelectronics", "text": "Deep-sub-voltage nanoelectronics\n\nDeep-sub-voltage nanoelectronics are integrated circuits (ICs) operating near theoretical limits of energy consumption per unit of processing. These devices are intended to address the needs of applications such as wireless sensor networks which have dramatically different requirements from traditional electronics. For example, for microprocessors where performance is primary metric of interest, but for some new devices, energy per instruction may be a more sensible metric.\n\nThe important case of fundamental ultimate limit for logic operation is the reversible computing.\n\nThe tiny autonomous devices (for example smartdust or autonomous Microelectromechanical systems) are based on deep-sub-voltage nanoelectronics. .\n\n"}
{"id": "636084", "url": "https://en.wikipedia.org/wiki?curid=636084", "title": "DigitalGlobe", "text": "DigitalGlobe\n\nDigitalGlobe is an American commercial vendor of space imagery and geospatial content, and operator of civilian remote sensing spacecraft. The company went public on the New York Stock Exchange on 14 May 2009, selling 14.7 million shares at $19.00 each to raise $279 million in capital. On 5 October 2017, Maxar Technologies has completed its acquisition of DigitalGlobe.\n\nThe WorldView satellites should not be confused with WorldView company, a division of Paragon Space Development Corporation offering flights to near-space.\n\nWorldView Imaging Corporation was founded in January 1992 in Oakland, California in anticipation of the 1992 Land Remote Sensing Policy Act (enacted in October 1992) which permitted private companies to enter the satellite imaging business. Its founder was Dr Walter Scott, who was joined by co-founder and CEO Doug Gerull in late 1992. In 1993, the company received the first high resolution commercial remote sensing satellite license issued under the 1992 Act. The company was initially funded with private financing from Silicon Valley sources and interested corporations in N. America, Europe, and Japan. Dr. Scott was head of the Lawrence Livermore Laboratories \"Brilliant Pebbles\" and \"Brilliant Eyes\" projects which were part of the Strategic Defense Initiative. Doug Gerull was the executive in charge of the Mapping Sciences division at the Intergraph Corporation. The company's first remote sensing license from the United States Department of Commerce allowed it to build a commercial remote sensing satellite capable of collecting images with resolution.\n\nIn 1995, the company became EarthWatch Incorporated, merging WorldView with Ball Aerospace & Technologies Corp.'s commercial remote sensing operations. In September 2001, EarthWatch became DigitalGlobe.\n\nIn 2007, DigitalGlobe acquired online imagery provider GlobeXplorer to extend its imagery distribution capabilities via online APIs and web services.\n\nIn 2011, DigitalGlobe was inducted into the Space Foundation's Space Technology Hall of Fame for its role in advancing commercial Earth-imaging satellites.\n\nIn 2013, DigitalGlobe purchased GeoEye.\n\nIn February 2017, MDA and DigitalGlobe reached an agreement for MDA to acquire DigitalGlobe for US $2.4B.\n\nAs of May 2017, DigitalGlobe's image catalog contains 100 petabytes worth of data, and grows by 100 terabytes each day.\n\nAs on 5 October 2017, MDA has announced it has completed its acquisition of DigitalGlobe.\n\nEarlyBird-1 was launched for Earth Watch Inc. on December 24, 1997, from the Svobodny Cosmodrome by a Start-1 launch vehicle. It included a panchromatic camera with a resolution and a multispectral camera with a resolution. Early Bird 1 was the first commercial satellite to be launched from the Svobodny Cosmodrome.\n\nIKONOS was launched September 24, 1999. It was the world's first high-resolution commercial imaging satellite to collect panchromatic (black-and-white) images with 0.8 m resolution and multispectral (color) imagery with 3.2-meter resolution. On March 31, 2015, IKONOS was officially decommissioned after more than doubling her mission design life, spending 5,680 days in orbit and making 83,131 trips around the earth.\n\nQuickBird, launched on October 18, 2001, was DigitalGlobe's primary satellite until early 2015. It was built by Ball Aerospace, and launched by a Boeing Delta II. It is in a 450 km altitude, −98 degree inclination sun-synchronous orbit. An earlier launch attempt resulted in the loss of QuickBird-1. It included a panchromatic camera with a resolution and a multispectral camera with a resolution. On January 27, 2015, QuickBird was de-orbited, exceeding her initial life expectancy by nearly 300%.\n\nThe GeoEye-1 satellite collects images at .41-meter panchromatic (black-and-white) and 1.65-meter multispectral resolution. The satellite can collect up to 350,000 square kilometers of pan-sharpened multispectral imagery per day. This is used for large-scale mapping projects. GeoEye-1 can revisit any point on Earth once every three days or sooner.\n\nBall Aerospace built WorldView-1. It was launched on September 18, 2007 from Vandenberg Air Force Base on a Delta II 7920-10C. Launch services were provided by United Launch Alliance. The National Geospatial-Intelligence Agency is expected to be a major customer of WorldView-1 imagery. It included a panchromatic only camera with a maximum resolution.\n\nBall Aerospace built WorldView-2. It was launched on October 8, 2009. DigitalGlobe partnered with Boeing commercial launch services to deliver WorldView-2 into a sun-synchronous orbit. The satellite includes a panchromatic sensor with a maximum resolution and a multispectral sensor of \n\nBall Aerospace built WorldView-3. It was launched on August 13, 2014. It has a maximum resolution of . WorldView-3 operates at an altitude of , where it has an average revisit time of less than once per day. Over the course of a day it is able to collect imagery of up to .\n\nPreviously, DigitalGlobe was only licensed to sell images with a higher resolution than to the US military. However, DigitalGlobe obtained permission, in June 2014, from the U.S. Department of Commerce, to allow the company to more widely exploit its commercial satellite imagery. The company was permitted to offer customers the highest resolution imagery available from their constellation. Additionally, the updated approvals allowed the sale of imagery to customers at up to 25 cm panchromatic and multispectral ground sample distance (GSD), beginning six months after WorldView-3 became operational. WorldView-3 was launched aboard a United Launch Alliance Atlas V rocket in the 401 configuration on August 13, 2014, at 11:30 local time from SLC-3 at Vandenberg Air Force base.\n\nWorldView-3 is the industry's first multi-payload, super-spectral, high-resolution commercial satellite.\n\nThe WorldView-4 satellite is designed to provide panchromatic images at a highest resolution of , and multispectral images at . Originally named GeoEye-2, the spacecraft was designed and built by Lockheed Martin, while the camera payload was provided by ITT Corporation.\n\nFollowing the merger of GeoEye and DigitalGlobe, DigitalGlobe announced that GeoEye-2 would be completed as a ground spare to be launched if or when required. It was renamed to WorldView-4 in July 2014, when the company announced that it would be launched in Fall 2016. It was launched on 11 November 2016.\n\nCurrently being built by SSL, WorldView-Legion is DigitalGlobe's next generation of earth observation satellites. WorldView-Legion consists of six satellites planned to launch in the 2020/2021 time frame into a mix of sun-synchronous and mid-latitude orbits. These satellites will replace imaging capability currently provided by DigitalGlobe’s WorldView-1, WorldView-2 and GeoEye-1 Earth observation satellites.\n\nWorldView-Legion is contracted to launch on a two flight proven SpaceX Falcon 9 missions.\n\nDigitalGlobe’s customers range from urban planners, to conservation organizations like the Amazon Conservation Team, to the U.S. federal agencies, including NASA and the United States Department of Defense's National Geospatial-Intelligence Agency (NGA). Much of Google Earth and Google Maps high resolution-imagery is provided by DigitalGlobe, In recent years google has stopped using space collection and focused more on Aerial collection. This can be seen when attributes are displayed in google earth's historical viewer. They typically use imagery from \"USDA bureau of farms\", and harvested data from local and state level orthographic programs. as is imagery used in TerraServer and Apple Maps. DigitalGlobe's main competitors were GeoEye (formerly Orbimage and Space Imaging), before their merger with DigitalGlobe. Spot Image remains a competitor.\n\n"}
{"id": "16762070", "url": "https://en.wikipedia.org/wiki?curid=16762070", "title": "EcoCover", "text": "EcoCover\n\nEcoCover Limited of New Zealand has used biomimicry principles to develop an organically certified biodegradable mulch mat made from shredded waste paper bound with fish paste. The product, a substitute for black plastic sheeting used in agriculture, reduces the need for pesticides, conserves water, improves soil, and diverts fish waste and waste paper from landfills.\n"}
{"id": "8971728", "url": "https://en.wikipedia.org/wiki?curid=8971728", "title": "Econnect", "text": "Econnect\n\nEconnect is a Czech non-profit organisation. It works to \"help other non-profit organisations from the Czech Republic to use electronic communication systems and to have easy access to information technology.\" Founded in 1991, it came about because of the need for communication between diverse ecological action groups that were networked around the Green Circle umbrella organisation.\n\nAccording to the group itself, its name \"Econnect\" comes from an abbreviation of \"easy connection\", which reflects its goal of providing smooth communications between action groups.\n\nEconnect's goal has been to cover all parts of the Czech Republic and Slovakia. It created a network of 10 organisations, based in the big towns. These networking organisations were seen to be among the prominent players of the ecological movement at the time.\n\nInternet and e-mail access was provided to Econnect since 1991, when the Internet was yet to be accepted as a major tool of communication in the country.\n\nEconnect is a source of information from the non-profit sector. In 1991, Econnect became part of the alternate internet service providers, the Association for Progressive Communications.\n\nIts current services include connections to the Internet, building a web presence for the non-profit world, technical support, conference services, training, and drawing more visitors via the Internet to alternative groups.\n\n"}
{"id": "32546164", "url": "https://en.wikipedia.org/wiki?curid=32546164", "title": "Embedded C", "text": "Embedded C\n\nEmbedded C is a set of language extensions for the C programming language by the C Standards Committee to address commonality issues that exist between C extensions for different embedded systems. \n\nHistorically, embedded C programming requires nonstandard extensions to the C language in order to support exotic features such as fixed-point arithmetic, multiple distinct memory banks, and basic I/O operations.\nIn 2008, the C Standards Committee extended the C language to address these issues by providing a common standard for all implementations to adhere to. It includes a number of features not available in normal C, such as fixed-point arithmetic, named address spaces and basic I/O hardware addressing.\nEmbedded C uses most of the syntax and semantics of standard C, e.g., main() function, variable definition, datatype declaration, conditional statements (if, switch case), loops (while, for), functions, arrays and strings, structures and union, bit operations, macros, etc. \n\nA Technical Report was published in 2004 and a second revision in 2006.\n"}
{"id": "1295520", "url": "https://en.wikipedia.org/wiki?curid=1295520", "title": "Eugene Podkletnov", "text": "Eugene Podkletnov\n\nEugene Podkletnov (, Yevgeny Podkletnov) is a Russian ceramics engineer known for his claims made in the 1990s of designing and demonstrating gravity shielding devices consisting of rotating discs constructed from ceramic superconducting materials.\n\nPodkletnov graduated from the University of Chemical Technology, Mendeleyev Institute, in Moscow; he then spent 15 years at the Institute for High Temperatures in the Russian Academy of Sciences. He received a doctorate in materials science from Tampere University of Technology in Finland. After graduation he continued superconductor research at the university, in the Materials Science department, until his expulsion in 1997. After which he moved back to Moscow where it is reported that he took an engineering job. Since leaving Tampere in 1997 Podkletnov has avoided public contact or appearances. There is a report that he later returned to Tampere to work on superconductors at Tamglass Engineering Oy.\n\nAccording to the account Podkletnov gave to Wired reporter Charles Platt in a 1996 phone interview, during a 1992 experiment with a rotating superconducting disc:\n\nPodkletnov's first peer-reviewed paper on the apparent gravity-modification effect, published in 1992, attracted little notice. In 1996, he submitted a longer paper, in which he claimed to have observed a larger effect (2% weight reduction as opposed to 0.3% in the 1992 paper) to the \"Journal of Physics D\". According to Platt, a member of the editorial staff, Ian Sample, leaked the submitted paper to Robert Matthews, the science correspondent for the British newspaper, the \"Sunday Telegraph\".\n\nOn September 1, 1996, Matthews's story broke, leading with the startling statement: \"Scientists in Finland are about to reveal details of the world's first antigravity device.\" In the ensuing furor, the director of the laboratory where Podkletnov was working issued a defensive statement that Podkletnov was working entirely on his own. Vuorinen, listed as the paper's coauthor, disavowed prior knowledge of the paper and claimed that the name was used without consent. Podkletnov himself complained that he had never claimed to block gravity, only to have reduced its effect.\n\nPodkletnov withdrew his second paper after it had been initially accepted. The resulting furor over the alleged claims in the withdrawn paper is reported to be the primary reason for his expulsion from his lab and the termination of his employment at the university.\n\nIn a 1997 telephone interview with Charles Platt, Podkletnov insisted that his gravity-shielding work was reproduced by researchers at universities in Toronto and Sheffield, but none have come forward to acknowledge this. The Sheffield work is known to have only been intended as partial replication, aimed at observing any unusual effects which might be present, since the team involved lacked the necessary facilities to construct a large enough disc and the ability to duplicate the means by which the original disc was rotated. Podkletnov counters that the researchers in question have kept quiet \"lest they be criticized by the mainstream scientific community\". Podkletnov is reported to have visited the Sheffield team in 2000 and advised them on the conditions necessary to achieve his effect, conditions that they never achieved.\n\nIn a BBC news item, it was alleged that researchers at Boeing were funding a project called GRASP (Gravity Research for Advanced Space Propulsion) which would attempt to construct a gravity shielding device based on rotating superconductors, but a subsequent \"Popular Mechanics\" news item stated that Boeing had denied funding GRASP with company money, although Boeing acknowledged that it could not comment on \"black projects\". It is alleged that the GRASP proposal was presented to Boeing and Boeing chose not to fund it.\n\nAlso during the 1997 telephone interview with Platt, Podkletnov said that he was continuing to work on gravitation, claiming that with new collaborators at an unnamed \"chemical research center\" in Moscow he has built a new device. He said:\n\n\n"}
{"id": "42579609", "url": "https://en.wikipedia.org/wiki?curid=42579609", "title": "First office application", "text": "First office application\n\nIn telecommunication, First Office Application (FOA) is a phase in development of new equipment or technology. FOA phase is the first deployment (pilot) of the equipment or technology in actual customer environment after internal test and acceptance phase is completed.\n"}
{"id": "22293052", "url": "https://en.wikipedia.org/wiki?curid=22293052", "title": "Four-slide", "text": "Four-slide\n\nA four-slide, also known as a multislide, multi-slide, or four-way, is a metalworking machine tool used in the high-volume manufacture of small stamped components from bar or wire stock. The press is most simply described as a horizontal stamping press that uses cams to control tools. The machine is used for progressive or transfer stamping operations.\n\nA four-slide is quite different from most other presses. The key of the machine is its moving slides that have tools attached, which strike the workpiece together or in sequence to form it. These slides are driven by four shafts that outline the machine. The shafts are connected by bevel gears so that one shaft is driven by an electric motor, and then that shaft's motion drives the other three shafts. Each shaft then has cams which drive the slides, usually of a split-type. This shafting arrangement allows the workpiece to be worked for four sides, which makes this machine extremely versatile. A hole near the center of the machine is provided to expel the completed workpiece.\n\nThe greatest advantage of the four-slide machine is its ability to complete all of the operations required to form the workpiece from start to finish. Moreover, it can handle certain parts that transfer or progressive dies cannot, because it can manipulate from four axes. Due to this flexibility it reduces the cost of the finished part because it requires less machines, setups, and handling. Also, because only one machine is required, less space is required for any given workpiece. As compared to standard stamping presses the tooling is usually inexpensive, due to the simplicity of the tools. A four-slide can usually produce 20,000 to 70,000 finished parts per 16-hour shift, depending on the number of operations per part; this speed usually results in a lower cost per part.\n\nThe biggest disadvantage is its size constraints. The largest machines can handle stock up to wide, long, and thick. For wires the limit is . Other limits are the travel on the slides, which maxes out at , and the throw of the forming cams, which is between . The machine is also limited to only shearing and bending operations. Extrusion and upsetting operations are impractical because it hinders the movement of the workpiece to the next station. Drawing and stretching require too much tonnage and the mechanisms required for the operations are space prohibitive. Finally, this machine is only feasible to use on high volume parts because of the long lead time required to set up the tooling.\n\nThe material stock used in four-slides is usually limited by its formability and not the machine capabilities. Usually the forming characteristics and bending radii are the most limiting factors. The most commonly used materials are:\n\n\nItems that are commonly produced on this machine include: automotive stampings, hinges, links, clips, and razor blades.\n\n\n"}
{"id": "35823119", "url": "https://en.wikipedia.org/wiki?curid=35823119", "title": "Frequency meter", "text": "Frequency meter\n\nA frequency meter is an instrument that displays the frequency of a periodic electrical signal.\n\nVarious types of frequency meters are used. Many are instruments of the deflection type, ordinarily used for measuring low frequencies but capable of being used for frequencies as high as 900 Hz. These operate by balancing two opposing forces. Changes in the frequency to be measured cause a change in this balance that can be measured by the deflection of a pointer on a scale. Deflection-type meters are of two types, electrically resonant circuits and ratiometers.\n\nAn example of a simple electrically resonant circuit is a moving-coil meter. In one version, this device has two coils tuned to different frequencies and connected at right angles to one another in such a way that the whole element, with attached pointer, can move. Frequencies in the middle of the meter’s range cause the currents in the two coils to be approximately equal and the pointer to indicate the midpoint of a scale. Changes in frequency cause an imbalance in the currents in the two coils, causing them, and the pointer, to move.\n\nAnother type of frequency meter, Weston Frequency meter is not of the deflection type, is the resonant reed type, ordinarily used in ranges from 10 to 1,000 Hz, although special designs can operate at lower or higher frequencies.\n\nThe main principle of working of weston type frequency meter is that \"when an current flows through the two coils which are perpendicular to each other, due to these currents some magnetic fields will produce and thus the magnetic needle will deflects towards the stronger magnetic field showing the measurement of frequency on the meter\". Construction of weston frequency is as compared to ferrodynamic type of frequency meter. In order to construct a circuit diagram we need two coils, three inductors and two resistors.\n\nAxis of both coils are marked as shown. Scale of the meter is calibrated such that at standard frequency the pointer will take position at 45o. Coil 1 contains a series resistor marked R1 and reactance coil marked as L1, while the coil 2 has a series reactance coil marked as L2 and parallel resistor marked as R2. The indcuctor which is marked as L0 is connected in series with the supply voltage in order to reduce the higher harmonic means here this inductor is working as a filter circuit. Let us look at the working of this meter.\n\nNow when we apply voltage at standard frequency then the pointer will take normal position, if there increase the frequency of the applied voltage then we will see that the pointer will moves towards left marked as higher side as shown in the circuit diagram. Again we reduce the frequency the pointer will start moving towards the right side, if lower the frequency below the normal frequency then it cross the normal position to move towards left side marked lower side as shown in the figure.\n\nNow let us look at the internal working of this meter. Voltage drop across an inductor is directly proportion to frequency of the source voltage, as we increase the frequency of the applied voltage the voltage drop across the inductor L1 increase that means the voltage impressed between the coil 1 is increased hence the current through the coil 1 increase while the current through the coil 2 decreases.\nSince the current through the coil 1 increases the magnetic field also increases and the magnetic needle attracts more towards the left side showing the increment in the frequency. Similar action will takes if decrease the frequency but in this the pointer will moves towards the left side. \n\n"}
{"id": "1379192", "url": "https://en.wikipedia.org/wiki?curid=1379192", "title": "Geekcorps", "text": "Geekcorps\n\nGeekcorps is a non-profit organization that sends people with technical skills to developing countries to assist in computer infrastructure development.\n\nThe non-profit was created in 2000 by Ethan Zuckerman and Elisa Korentayer in North Adams, Massachusetts. In 2001 Geekcorps became a division of the International Executive Service Corps located in Washington, D.C.\n\nAfter a visit to a Ghana library by Zuckerman in 1993, the lack of up-to-date resources available prompted him to create Geekcorps years later. Humanitarian and banker Elisa Korentayer became co-founder of Geekcorps due to the organization's need of financial wisdom. In effort to increase access to current information and bridge the digital divide in developing nations Zuckerman, and associates from his now bought out internet company tripod, funded most of the $350,000 budget for Geekcorps' first year.\n\nStarting September 2000, with 6 volunteers selected from over 200 applicants, Geekcorps first mission was in Accra, Ghana. Co-founder Zuckerman was already familiar with the infrastructure of Ghana. Zuckerman stated, \"The government has relatively liberal telecommunications and investment policies, making it possible for IT businesses to be built there.\" Geekcorps initial focus in Ghana was assisting Accran companies with its IT expertise. Geekcorps had an understanding with local businesses, after receiving help, the businesses involved were to help the locals with their newfound resources. Initial challenges for Geekcorps were communication and teaching skills needed by volunteers, and reliance on outdated programming languages for local businesses. Geekcorps involvement lead to innovations such as a new java based payment system for local businesses in Ghana. Geekcorps was also instrumental in the creation of Ghana's internet exchange in 2005.\n\nInitial assistance in Mali came from the CMRT (Community Mobilization through Radio Technology) program sponsored by USAID. Under CMRT, Geekcorps installed 5 radio stations to enable local communication through the area. Later under another program Radio for Peace Building, Geekcorps installed another 11 stations, and renovations were done to older existing stations.\n\nGeekcorps set up ICT stations in less populated areas of Mali in 2006. These stations were updated by a memory stick delivered from a computer center with internet access in Ouelessebougou. Although this allowed many rural locations access to specific web resources, such as web pages and digital media, due to lack of interest the program was modified after a year. Yearly updates to more desired information such as Moulin, a French version of Wikipedia, became the focus.\n\nIn 2007 Geekcorps Director Wayan Vota was accused of disparaging the OLPC project through a \"OLPC News\" website without disclosing Geekcorps' promotion of Intel's rival laptop, the Classmate PC and Microsoft's Windows XP. \n\n\n"}
{"id": "47990003", "url": "https://en.wikipedia.org/wiki?curid=47990003", "title": "Goji (app)", "text": "Goji (app)\n\nGoji was a location-based virtual keyboard, created for iOS 8 by The Last Guide Company. The keyboard enabled users to output recommendations for places of interest around them by tapping on an icon. The keyboard was released in October 2014 and removed from the iOS App Store when the company was closed in August 2015.\n\nGoji would display a dynamic gradient background based on the current time of day and emoji icons related to the mobile device's current location and city. For example, when on Apple Campus, Apple specific icons would be shown.\n\n"}
{"id": "33956539", "url": "https://en.wikipedia.org/wiki?curid=33956539", "title": "Gyroscopic autopilot", "text": "Gyroscopic autopilot\n\nThe gyroscopic autopilot was a type of autopilot system developed primarily for aviation uses in the early 20th century. Since then, the principles of this autopilot has been the basis of many different aircraft control systems, both military and civilian. \n\nThe Sperry Corporation developed the original gyroscopic autopilot in 1912. The device was called a “gyroscopic stabilizer apparatus,” and its purpose was to improve stability and control of aircraft. It utilized the inputs from several other instruments to allow an aircraft to automatically maintain a desired compass heading and altitude. \n\nThe key feature of the gyroscopic stabilizer apparatus was that it incorporated a gyroscope to regulate the control surfaces of the aircraft. Lawrence Sperry managed to design a smaller and lighter version of a gyroscope, and the device was integrated into an aircraft's hydraulic control system. Using a negative feedback loop, the gyroscope automatically adjusted the control surfaces of an aircraft to maintain straight and level flight.\n\nLawrence Sperry’s autopilot was first demonstrated in France on June 18, 1914. Sperry was participating in an exhibition in which 57 planes were fitted with new improvements and innovations. Sperry’s aircraft, a Curtiss C-2, was the only one equipped with a gyroscopic stabilizer. Sperry, along with his assistant Emil Cachin, made three passes in front of a grandstand full of spectators and military observers. On his first pass, Sperry engaged the autopilot and flew past the grandstand with his hands held high off of the controls. On the second pass, Cachin climbed out onto the starboard wing seven feet away from the fuselage with Sperry’s hands still off the controls. When the aircraft banked due to the shift in weight, the autopilot immediately stabilized the wings. On his final pass, Sperry climbed out onto the opposite wing, leaving the pilot seat empty. The observers were amazed at the aircraft’s ability to maintain level flight without a pilot manually controlling it. Sperry also gave Joseph Barres, Commandant of the French Army Air Corps, a ride to demonstrate his device’s ability to perform an unassisted takeoff and landing.\n\nWiley Post became famous after his record setting trip around the world on June 23, 1931. In his aircraft, a Lockheed Vega nicknamed “Winnie Mae,” he managed to travel around the world in eight days 15 hours and 51 minutes. He accomplished this with the help of Harold Gatty, who served as his navigator and copilot. Two years later he set out to beat his previous record by flying around the world by himself. He wanted to prove that one man could accomplish the same trip without a copilot. In order to accomplish this, he equipped the Winnie Mae with a Sperry gyroscope autopilot and a radio direction finder. Although he experienced some problems with the autopilot, he completed the trip in seven days, 18 hours and 49 minutes. The use of the autopilot and radio direction finder is credited for making the task of navigating the aircraft much easier and more efficient. The use of an autopilot reduced the physical and mental demands on Post as he flew around the world.\n\nThe U.S. Army Air Corps and the U.S. Navy experimented with autopilots on military aircraft before and during World War II. Straight and level flight had become a necessity for new level bombing techniques that were being developed at the time. The Sperry Gyroscope Company developed many autopilot systems for use on military aircraft. When the Boeing B-17 Flying Fortress was delivered in the late 1930s, it came equipped a commercial Sperry A-3 Autopilot. The A-3 was a simple autopilot and only corrected angular deviations in the aircraft’s straight and level course. It utilized pneumatic hydraulic servos, which had a tendency to react slowly to inputs, and this often led to overcompensation of the aircraft’s corrected course. This caused navigation and control issues when pilots were flying in poor weather or rough air. \n\nTo fix these problems, the Sperry A-5 autopilot was developed. This was the first all electric autopilot. This new autopilot used three dual-element vacuum tube amplifiers and high-speed gyros. Each amplifier was associated with a different axis: Yaw, pitch, and roll. The high-speed gyros were more sensitive and established a base reference level of the aircraft’s level flight path. Whenever the aircraft deviated from the base reference level, the autopilot adjusted for the amount of time that occurred between the changes in reference levels. This allowed the autopilot to detect the velocity and acceleration of the change. The calculated change was then communicated quickly to the control surfaces by independent electro-hydraulic servos. This led to faster, more stable corrections of the aircraft. \n\nThe faster stabilization of the aircraft by the A-5 autopilot made it possible for new bombsights to be used on military aircraft. The Norden Bombsight and Sperry Bombsight were both used onboard Army and Navy bombers during the war. Both bombsights used gyroscopes, telescopes and analog computers to calculate the release point for bombers to drop their payloads accurately onto ground targets. The A-5 had the ability to be integrated with these bombsights. Once the bombardier found the target and adjusted the bombsight, the autopilot would be engaged to fly the aircraft straight and level to the target, where the bombsight would automatically calculate the release point of the bombs.\n\nThe German V-1 Buzz Bomb also used an autopilot system for guidance. It used a pendulum system that was damped by a gyroscope, similar to the ones used in Sperry autopilots. It also used radio direction finding to maintain course. The use of autopilot on unmanned weapons can be seen as the precursor to modern cruise missiles.\n\n"}
{"id": "6411538", "url": "https://en.wikipedia.org/wiki?curid=6411538", "title": "Hydrogen sensor", "text": "Hydrogen sensor\n\nA hydrogen sensor is a gas detector that detects the presence of hydrogen. They contain micro-fabricated point-contact hydrogen sensors and are used to locate hydrogen leaks. They are considered low-cost, compact, durable, and easy to maintain as compared to conventional gas detecting instruments.\n\nThere are five key issues with hydrogen detectors:\n\n\n\nThere are various types of hydrogen microsensors, which use different mechanisms to detect the gas. Palladium is used in many of these, because it selectively absorbs hydrogen gas and forms the compound palladium hydride. Palladium-based sensors have a strong temperature dependence which makes their response time too large at very low temperatures. Palladium sensors have to be protected against carbon monoxide, sulfur dioxide and hydrogen sulfide.\n\nSeveral types of optical fibre surface plasmon resonance (SPR) sensor are used for the point-contact detection of hydrogen:\n\n\n\nSensors are typically calibrated at the manufacturing factory and are valid for the service life of the unit.\n\nSiloxane enhances the sensitivity and reaction time of hydrogen sensors. Detection of hydrogen levels as low as 25 ppm can be achieved; far below hydrogen's lower explosive limit of around 40,000 ppm.\n\n\n"}
{"id": "11042276", "url": "https://en.wikipedia.org/wiki?curid=11042276", "title": "John Lees (inventor)", "text": "John Lees (inventor)\n\nJohn Lees of Turf Lane, Royton, Lancashire was an English inventor who made a substantial improvement to machinery for carding cotton. \nHe improved the carding machine in 1772 by adding a feeder to it in the form of a perpetually revolving cloth on which cotton wool was spread to convey the wool to the cylinder. On 25 June 1785, he proved this in the course of the trial concerning the validity of Richard Arkwright's second patent (dated 1775) for his cotton-spinning water frame. \n\nHe was one of the carding mill owners sued by Arkwright in 1781, having built a cotton mill at Fowleach at Greenacres Moor, in Oldham. He began by working a horsemill-powered cotton mill in 1776-78 but \"raised himself from the extremest drudgery of the spinning room to the position of one of the most opulent inhabitants\" of Oldham, with a mill and stock insured for over £2,000 in 1795.\n\nJohn Lees was a Quaker and was the father of James Lees. \n\n"}
{"id": "53841056", "url": "https://en.wikipedia.org/wiki?curid=53841056", "title": "Juicero", "text": "Juicero\n\nJuicero was a company that made a device for fruit and vegetable juicing. The company's product was called the Juicero Press, a Wi-Fi connected juicer that used single-serving packets of pre-juiced fruits and vegetables sold exclusively by the company by subscription. The San Francisco-based firm received $120 million in startup venture capital starting in 2014, from investors including Kleiner Perkins Caufield & Byers and Alphabet Inc. On September 1, 2017, the company announced that it was suspending sales of the juicer and the packets, repurchasing the juicer from its customers and searching for a buyer for the company and its intellectual property.\n\nJuicero was founded in 2013 by Doug Evans, who served as CEO until October 2016, when former president of Coca-Cola North America Jeff Dunn took over the position. The company's juicing press was originally priced at $699 when launched in March 2016, but was reduced to $399 in January 2017, 12 to 18 months ahead of schedule, in response to slow sales of the device.\n\nProduce packs for the press, containing blends of chopped fruits and vegetables prepared by workers in a Juicero facility, cost between $5 and $7 each, and each pack had a limited lifespan of about 8 days. Each pack had a QR code which was scanned and verified by the Internet-connected machine before it could be used. CEO Jeff Dunn claimed this was to prevent packs from being used past their expiration date, and to facilitate food safety recalls, though critics felt that the feature was a form of digital rights management as it would prevent operation of the press with any produce pack not made by the company. Industrial design for the press was completed by Yves Behar's studio Fuseproject, based in San Francisco.\n\nIn 2017, Juicero was the target of widespread criticism when Bloomberg News published a story suggesting that the company's produce packs could be squeezed by hand easily and effectively, and that hand-squeezing produced juice that was nearly indistinguishable in quantity and quality from the output of the company's expensive Press device. The company defended its product and its process, claiming that squeezing packs by hand created undue mess and promoted a poor user experience, and later offered full refunds to any customers unsatisfied with its Press device.\n\nAfter taking apart the device, venture capitalist Ben Einstein wrote that \"Juicero’s Press is an incredibly complicated piece of engineering\", but that the complexity was unnecessary and likely arose from a lack of cost constraints during the design process. A simpler and cheaper implementation, suggested Einstein, would likely have produced much the same quality of juice at a price several hundred US dollars cheaper. Prominent YouTube electronics and equipment reviewer AvE came to similar conclusions in his own disassembly and review video for the Juicero, demonstrating the ease with which the produce packets could be squeezed by hand, and the considerable over-design and high build quality of the interior – the parts cost of which appeared to be over $1000 per machine.\n\nJuicero filed a complaint in federal court in April 2017 against a competing cold-press juicing device, the Froothie Juisir, for allegedly infringing its patent and copying Juicero's trade dress.\n\n\n"}
{"id": "778233", "url": "https://en.wikipedia.org/wiki?curid=778233", "title": "LCD television", "text": "LCD television\n\nLiquid-crystal-display televisions (LCD TV) are television sets that use liquid-crystal displays to produce images. LCD televisions are thinner and lighter than cathode ray tube (CRTs) of similar display size, and are available in much larger sizes. When manufacturing costs fell, this combination of features made LCDs practical for television receivers.\n\nIn 2007, LCD televisions surpassed sales of CRT-based televisions worldwide for the first time, and their sales figures relative to other technologies are accelerating. LCD TVs are quickly displacing the only major competitors in the large-screen market, the plasma display panel and rear-projection television. LCDs are, by far, the most widely produced and sold television display type.\n\nLCDs also have a variety of disadvantages. Other technologies address these weaknesses, including organic light-emitting diodes (OLED), FED and SED.\n\nPassive matrix LCDs first became common in the 1980s for various portable computer roles. At the time they competed with plasma displays in the same market space. The LCDs had very slow refresh rates that blurred the screen even with scrolling text, but their light weight and low cost were major benefits. Screens using reflective LCDs required no internal light source, making them particularly well suited to laptop computers.\n\nRefresh rates of early devices were too slow to be useful for television. Portable televisions were a target application for LCDs. LCDs consumed far less battery power then even the miniature tubes used in portable televisions of the era. The earliest commercially made LCD TV was the Casio TV-10 made in 1983. Resolutions were limited to standard definition, although a number of technologies were pushing displays towards the limits of that standard; Super VHS offered improved color saturation, and DVDs added higher resolutions as well. Even with these advances, screen sizes over 30\" were rare as these formats would start to appear blocky at normal seating distances when viewed on larger screens. Projection systems were generally limited to situations where the image had to be viewed by a larger audience.\n\nNevertheless, some experimentation with LCD televisions took place during this period. In 1988, Sharp Corporation introduced the first commercial LCD television, a 14\" model with active matrix addressing using thin-film transistors (TFT). These were offered primarily as boutique items for discerning customers, and were not aimed at the general market. At the same time, plasma displays could easily offer the performance needed to make a high quality display, but suffered from low brightness and very high power consumption. However, a series of advances led to plasma displays outpacing LCDs in performance improvements, starting with Fujitsu's improved construction techniques in 1979, Hitachi's improved phosphors in 1984, and AT&T's elimination of the black areas between the sub-pixels in the mid-1980s. By the late 1980s, plasma displays were far in advance of LCDs.\n\nIt was the slow standardization of high definition television that first produced a market for new television technologies. In particular, the wider 16:9 aspect ratio of the new material was difficult to build using CRTs; ideally a CRT should be perfectly circular in order to best contain its internal vacuum, and as the aspect ratio becomes more rectangular it becomes more difficult to make the tubes. At the same time, the much higher resolutions these new formats offered were lost at smaller screen sizes, so CRTs faced the twin problems of becoming larger and more rectangular at the same time. LCDs of the era were still not able to cope with fast-moving images, especially at higher resolutions, and from the mid-1990s the plasma display was the only real offering in the high resolution space.\n\nThrough the halting introduction of HDTV in the mid-1990s into the early 2000s, plasma displays were the primary high-definition display technology. However, their high cost, both manufacturing and on the street, meant that older technologies like CRTs maintained a footprint in spite of their disadvantages. LCD, however, was widely considered to be unable to scale into the same space, and it was widely believed that the move to high-definition would push it from the market entirely.\n\nThis situation changed rapidly. Contrary to early optimism, plasma displays never saw the massive economies of scale that were expected, and remained expensive. Meanwhile, LCD technologies like Overdrive started to address their ability to work at television speeds. Initially produced at smaller sizes, fitting into the low-end space that plasmas could not fill, LCDs started to experience the economies of scale that plasmas failed to achieve. By 2004, 32\" models were widely available, 42\" sets were becoming common, and much larger prototypes were being demonstrated.\n\nAlthough plasmas continued to hold an arguable picture quality edge over LCDs, and even a price advantage for sets at the critical 42\" size and larger, LCD prices started falling rapidly in 2006 while their screen sizes were increasing at a similarly rapid rate. By late 2006, several vendors were offering 42\" LCDs, albeit at a price premium, encroaching on plasma's only stronghold. More critically, LCDs offer higher resolutions and true 1080p support, while plasmas were stuck at 720p, which made up for the price difference.\n\nPredictions that prices for LCDs would drop rapidly through 2007 led to a \"wait and see\" attitude in the market, and sales of all large-screen televisions stagnated while customers watched to see if this would happen. Plasmas and LCDs reached price parity in 2007, at which point the LCD's higher resolution was a winning point for many sales. By late 2007, it was clear that LCDs were going to outsell plasmas during the critical Christmas sales season. This was in spite of the fact that plasmas continued to hold an image quality advantage, but as the president of Chunghwa Picture Tubes noted after shutting down their plasma production line, \"Globally, so many companies, so many investments, so many people have been working in this area, on this product. So they can improve so quickly.\"\n\nWhen the sales figures for the 2007 Christmas season were finally tallied, pundits were surprised to find that LCDs had not only outsold plasma, but also outsold CRTs during the same period. This evolution drove competing large-screen systems from the market almost overnight. Plasma had overtaken rear-projection systems in 2005. The same was true for CRTs, which lasted only a few months longer; Sony ended sales of their famous Trinitron in most markets in 2007, and shut down the final plant in March 2008. The February 2009 announcement that Pioneer Electronics was ending production of the plasma screens was widely considered the tipping point in that technology's history as well.\n\nLCD's dominance in the television market accelerated rapidly. It was the only technology that could scale both up and down in size, covering both the high-end market for large screens in the 40 to 50\" class, as well as customers looking to replace their existing smaller CRT sets in the 14 to 30\" range. Building across these wide scales quickly pushed the prices down across the board.\n\nIn 2008, LCD TV shipments were up 33 percent year-on-year compared to 2007 to 105 million units.\nIn 2009, LCD TV shipments raised to 146 million units (69% from the total of 211 million TV shipments).\nIn 2010, LCD TV shipments reached 187.9 million units (from an estimated total of 247 million TV shipments).\n\nCurrent sixth-generation panels by major manufacturers such as Sony, Sharp Corporation, LG Display, Panasonic and Samsung have announced larger sized models:\n\n\nAlthough current LCD panels are able to deliver all sRGB colors using an appropriate combination of backlight's spectrum and optical filters, manufacturers want to display even more colors. One of the approaches is to use a fourth, or even fifth and sixth color in the optical color filter array. Another approach is to use two sets of suitably narrowband backlights (e.g. LEDs), with slightly differing colors, in combination with broadband optical filters in the panel, and alternating backlights each consecutive frame.\n\nFully using the extended color gamut will naturally require an appropriately captured material and some modifications to the distribution channel. Otherwise, the only use of the extra colors would be to let the looker boost the color saturation of the TV picture beyond what was intended by the producer, but avoiding the otherwise unavoidable loss of detail (\"burnout\") in saturated areas.\n\nIn spite of LCD's current dominance of the television field, there are several other technologies being developed that address its shortcomings. Whereas LCDs produce an image by selectively blocking a backlight, OLED, FED and SED all produce light directly on the front face of the display. In comparison to LCDs, all of these technologies offer better viewing angles, much higher brightness and contrast ratio (as much as 5,000,000:1), and better color saturation and accuracy, and use less power. In theory, they are less complex and less expensive to build.\n\nActually manufacturing these screens has proved more difficult than originally imagined. Sony abandoned their FED project in March 2009, but continue work on their OLED sets. Canon continues development of their SED technology, but announced that they will not attempt to introduce sets to market for the foreseeable future.\n\nSamsung has been displaying OLED sets at 14.1, 31 and 40 inch sizes for some time, and at the SID 2009 trade show in San Antonio they announced that the 14.1 and 31 inch sets are \"production ready\".\n\nLiquid crystals encompass a wide range of (typically) rod-shaped polymers that naturally form into thin, ordered layers, as opposed to the more random alignment of a normal liquid. Some of these, the \"nematic liquid crystals\", also show an alignment effect between the layers. The particular direction of the alignment of a nematic liquid crystal can be set by placing it in contact with an \"alignment layer\" or \"director\", which is essentially a material with microscopic grooves in it, on the supporting substrates. When placed on a director, the layer in contact will align itself with the grooves, and the layers above will subsequently align themselves with the layers below, the bulk material taking on the director's alignment. In the case of a \"Twisted Nematic\" (TN) LCD, this effect is utilized by using two directors arranged at right angles and placed close together with the liquid crystal between them. This forces the layers to align themselves in two directions, creating a twisted structure with each layer aligned at a slightly different angle to the ones on either side.\n\nLCD televisions produce a colored image by selectively filtering light produced by a backlight, originally provided by a series of cold cathode fluorescent lamps (CCFLs) but now typically by white or colored LEDs. Millions of individual LCD shutters, arranged in a grid, open and close to allow a metered amount of the white light through. Each shutter is paired with a colored filter to remove all but the red, green or blue (RGB) portion of the light from the original white source. Each shutter–filter pair forms a single \"sub-pixel\". The sub-pixels are so small that when the display is viewed from even a short distance, the individual colors blend together to produce a single spot of color, a \"pixel\". The shade of color is controlled by changing the relative intensity of the light passing through the sub-pixels.\n\nLCD shutters consist of a stack of three primary elements. On the bottom and top of the shutter are polarizer plates set at right angles. Normally light cannot travel through a pair of polarizers arranged in this fashion, and the display would be black. The polarizers also carry the directors to create the twisted structure aligned with the polarizers on either side. As the light flows out of the rear polarizer, it will naturally follow the liquid crystal's twist, exiting the front of the liquid crystal having been rotated through the correct angle, that allows it to pass through the front polarizer. LCDs are normally transparent in this mode of operation. To turn a shutter off, a voltage is applied across it from front to back. The rod-shaped molecules align themselves with the electric field instead of the directors, distorting the twisted structure. The light no longer changes polarization as it flows through the liquid crystal, and can no longer pass through the front polarizer. By controlling the voltage applied across the liquid crystal, the amount of remaining twist can be selected. This allows the transparency of the shutter to be controlled. To improve switching time, the cells are placed under pressure, which increases the force to re-align themselves with the directors when the field is turned off.\n\nSeveral other variations and modifications have been used in order to improve performance in certain applications. In-Plane Switching displays (IPS and S-IPS) offer wider viewing angles and better color reproduction, but are more difficult to construct and have slightly slower response times. Vertical Alignment (VA, S-PVA and MVA) offer higher contrast ratios and good response times, but suffer from color shifting when viewed from the side. In general, all of these displays work in a similar fashion by controlling the polarization of the light source.\n\nIn order to address a single shutter on the display, a series of electrodes is deposited on the plates on either side of the liquid crystal. One side has horizontal stripes that form rows, the other has vertical stripes that form columns. By supplying voltage to one row and one column, a field will be generated at the point where they cross. Since a metal electrode would be opaque, LCDs use electrodes made of a transparent conductor, typically indium tin oxide.\n\nSince addressing a single shutter requires power to be supplied to an entire row and column, some of the field always leaks out into the surrounding shutters. Liquid crystals are quite sensitive, and even small amounts of leaked field will cause some level of switching to occur. This partial switching of the surrounding shutters blurs the resulting image. Another problem in early LCD systems was the voltages needed to set the shutters to a particular twist was very low, but that voltage was too low to make the crystals re-align with reasonable performance. This resulted in slow response times and led to easily visible \"ghosting\" on these displays on fast-moving images, like a mouse cursor on a computer screen. Even scrolling text often rendered as an unreadable blur, and the switching speed was far too slow to use as a useful television display.\n\nIn order to address these problems, modern LCDs use an active matrix design. Instead of powering both electrodes, one set, typically the front, is attached to a common ground. On the rear, each shutter is paired with a thin-film transistor that switches on in response to widely separated voltage levels, say 0 and +5 volts. A new addressing line, the \"gate line\", is added as a separate switch for the transistors. The rows and columns are addressed as before, but the transistors ensure that only the single shutter at the crossing point is addressed; any leaked field is too small to switch the surrounding transistors. When switched on, a constant and relatively high amount of charge flows from the \"source line\" through the transistor and into an associated capacitor. The capacitor is charged up until it holds the correct control voltage, slowly leaking this through the crystal to the common ground. The current is very fast and not suitable for fine control of the resulting store charge, so pulse code modulation is used to accurately control the overall flow. Not only does this allow for very accurate control over the shutters, since the capacitor can be filled or drained quickly, but the response time of the shutter is dramatically improved as well.\n\nA typical shutter assembly consists of a sandwich of several layers deposited on two thin glass sheets forming the front and back of the display. For smaller display sizes (under 30 inches (760 mm)), the glass sheets can be replaced with plastic.\n\nThe rear sheet starts with a polarizing film, the glass sheet, the active matrix components and addressing electrodes, and then the director. The front sheet is similar, but lacks the active matrix components, replacing those with the patterned color filters. Using a multi-step construction process, both sheets can be produced on the same assembly line. The liquid crystal is placed between the two sheets in a patterned plastic sheet that divides the liquid into individual shutters and keeps the sheets at a precise distance from each other.\n\nThe critical step in the manufacturing process is the deposition of the active matrix components. These have a relatively high failure rate, which renders those pixels on the screen \"always on\". If there are enough broken pixels, the screen has to be discarded. The number of discarded panels has a strong effect on the price of the resulting television sets, and the major downward fall in pricing between 2006 and 2008 was due mostly to improved processes.\n\nTo produce a complete television, the shutter assembly is combined with control electronics and backlight. The backlight for small sets can be provided by a single lamp using a diffuser or frosted mirror to spread out the light, but for larger displays a single lamp is not bright enough and the rear surface is instead covered with a number of separate lamps. Achieving even lighting over the front of an entire display remains a challenge, and bright and dark spots are not uncommon.\n\nIn a CRT the electron beam is produced by heating a metal filament, which \"boils\" electrons off its surface. The electrons are then accelerated and focused in an electron gun, and aimed at the proper location on the screen using electromagnets. The majority of the power budget of a CRT goes into heating the filament, which is why the back of a CRT-based television is hot. Since the electrons are easily deflected by gas molecules, the entire tube has to be held in vacuum. The atmospheric force on the front face of the tube grows with the area, which requires ever-thicker glass. This limits practical CRTs to sizes around 30 inches; (76 cm) displays up to 40 inches (102 cm) were produced but weighed several hundred pounds, and televisions larger than this had to turn to other technologies like rear-projection.\n\nThe lack of vacuum in an LCD television is one of its advantages; there is a small amount of vacuum in sets using CCFL backlights, but this is arranged in cylinders which are naturally stronger than large flat plates. Removing the need for heavy glass faces allows LCDs to be much lighter than other technologies. \n\nLCD panels, like other flat panel displays, are also much thinner than CRTs. Since the CRT can only bend the electron beam through a critical angle while still maintaining focus, the electron gun has to be located some distance from the front face of the television. In early sets from the 1950s the angle was often as small as 35 degrees off-axis, but improvements, especially computer assisted convergence, allowed that to be dramatically improved and, late in their evolution, folded. Nevertheless, even the best CRTs are much deeper than an LCD.\n\nLCDs can, in theory, be built at any size, with production yields being the primary constraint. As yields increased, common LCD screen sizes grew, from 14\" (35 cm) to 30\" (70 cm), to 42\" (107 cm), then 52\" (132 cm), and 65\" (165 cm) sets are now widely available. This allowed LCDs to compete directly with most in-home projection television sets, and in comparison to those technologies direct-view LCDs have a better image quality. Experimental and limited run sets are available with sizes over 100 inches (254 cm).\n\nLCDs are relatively inefficient in terms of power use per display size, because the vast majority of light that is being produced at the back of the screen is blocked before it reaches the viewer. To start with, the rear polarizer filters out over half of the original un-polarized light. Examining the image above, you can see that a good portion of the screen area is covered by the cell structure around the shutters, which removes another portion. After that, each sub-pixel's color filter removes the majority of what is left to leave only the desired color. Finally, to control the color and luminance of a pixel as a whole, some light is lost when passing the front polarizer in the on-state by the imperfect operation of the shutters.\n\nFor these reasons the backlighting system has to be \"extremely\" powerful. In spite of using highly efficient CCFLs, most sets use several hundred watts of power, more than would be required to light an entire house with the same technology. As a result, LCD televisions using CCFLs end up with overall power usage similar to a CRT of the same size. Plasma displays are worse; the best are on par with LCDs, but typical sets draw much more.\n\nModern LCD sets have attempted to address the power use through a process known as \"dynamic lighting\" (originally introduced for other reasons, see below). This system examines the image to find areas that are darker, and reduces the backlighting in those areas. CCFLs are long cylinders that run the length of the screen, so this change can only be used to control the brightness of the screen as a whole, or at least wide horizontal bands of it. This makes the technique suitable only for particular types of images, like the credits at the end of a movie. In 2009 some manufacturers made some TVs using HCFL (more power efficient than CCFL). Sets using distributed LEDs behind the screen, with each LED lighting only a small number of pixels, typically a 16 by 16 patch, allow for better \"local dimming\" by dynamically adjusting the brightness of much smaller areas, which is suitable for a much wider set of images.\n\nAnother ongoing area of research is to use materials that optically route light in order to re-use as much of the signal as possible. One potential improvement is to use microprisms or dichromic mirrors to \"split\" the light into R, G and B, instead of absorbing the unwanted colors in a filter. A successful system would improve efficiency by three times. Another would be to direct the light that would normally fall on opaque elements back into the transparent portion of the shutters.\n\nSeveral newer technologies, OLED, FED and SED, have lower power use as one of their primary advantages. All of these technologies directly produce light on a sub-pixel basis, and use only as much power as that light level requires. Sony has demonstrated 36\" FED units displaying very bright images drawing only 14 W, less than 1/10 as much as a similarly sized LCD. OLEDs and SEDs are similar to FEDs in power terms. The lower power requirements make these technologies particularly interesting in low-power uses like laptop computers and mobile phones. These sorts of devices were the market that originally bootstrapped LCD technology, due to its light weight and thinness.\n\nEarly LCD sets were widely derided for their poor overall image quality, most notably the ghosting on fast-moving images, poor contrast ratio, and muddy colors. In spite of many predictions that other technologies would always beat LCDs, massive investment in LCD production, manufacturing, and electronic image processing has addressed many of these concerns.\n\nFor 60 frames per second video, common in North America, each pixel is lit for 17 ms before it has to be re-drawn (at 50 frames per second, it's 20 ms in Europe). Early LCDs had response times on the order of hundreds of milliseconds, which made them useless for television. A combination of improvements in materials technology since the 1970s greatly improved this, as did the active matrix techniques. By 2000, LCD panels with response times around 20 ms were relatively common in computer roles. This was still not fast enough for television use.\n\nA major improvement, pioneered by NEC, led to the first practical LCD televisions. NEC noticed that liquid crystals take some time to start moving into their new orientation, but stop rapidly. If the initial movement could be accelerated, the overall performance would be increased. NEC's solution was to boost the voltage during the \"spin up period\" when the capacitor is initially being charged, and then dropping back to normal levels to fill it to the required voltage. A common method is to double the voltage, but halve the pulse width, delivering the same total amount of power. Named \"Overdrive\" by NEC, the technique is now widely used on almost all LCDs.\n\nAnother major improvement in response time was achieved by adding memory to hold the contents of the display – something that a television needs to do anyway, but was not originally required in the computer monitor role that bootstrapped the LCD industry. In older displays the active matrix capacitors were first drained, and then recharged to the new value with every refresh. But in most cases, the vast majority of the screen's image does not change from frame to frame. By holding the before and after values in computer memory, comparing them, and only resetting those sub-pixels that actually changed, the amount of time spent charging and discharging the capacitors was reduced. Moreover, the capacitors are not drained completely; instead, their existing charge level is either increased or decreased to match the new value, which typically requires fewer charging pulses. This change, which was isolated to the driver electronics and inexpensive to implement, improved response times by about two times.\n\nTogether, along with continued improvements in the liquid crystals themselves, and by increasing refresh rates from 60 Hz to 120 and 240 Hz, response times fell from 20 ms in 2000 to about 2 ms in the best modern displays. But even this is not really fast enough because the pixel will still be switching while the frame is being displayed. Conventional CRTs are well under 1 ms, and plasma and OLED displays boast times on the order of 0.001 ms.\n\nOne way to further improve the \"effective\" refresh rate is to use \"super-sampling\", and it is becoming increasingly common on high-end sets. Since the blurring of the motion occurs during the transition from one state to another, this can be reduced by doubling the refresh rate of the LCD panel, and building intermediate frames using various motion compensation techniques. This smooths out the transitions, and means the backlighting is turned on only when the transitions are settled. A number of high-end sets offer 120 Hz (in North America) or 100 Hz (in Europe) refresh rates using this technique. Another solution is to only turn the backlighting on once the shutter has fully switched. In order to ensure that the display does not flicker, these systems fire the backlighting several times per refresh, in a fashion similar to movie projection where the shutter opens and closes several times per frame.\n\nEven in a fully switched-off state, liquid crystals allow some light to leak through the shutters. This limits their contrast ratios to about 1600:1 on the best modern sets, when measured using the ANSI measurement (ANSI IT7.215-1992). Manufacturers often quote the \"Full On/Off\" contrast ratio instead, which is about 25% greater for any given set.\n\nThis lack of contrast is most noticeable in darker scenes. To display a color close to black, the LCD shutters have to be turned to almost full opacity, limiting the number of discrete colors they can display. This leads to \"posterizing\" effects and bands of discrete colors that become visible in shadows, which is why many reviews of LCD TVs mention the \"shadow detail\". In comparison, the highest-end LED TVs offer regular contrast ratios of 5,000,000:1.\n\nSince the total amount of light reaching the viewer is a combination of the backlighting and shuttering, modern sets can use \"dynamic backlighting\" or \"local dimming\" to improve the contrast ratio and shadow detail. If a particular area of the screen is dark, a conventional set will have to set its shutters close to opaque to cut down the light. However, if the backlighting is reduced by half in that area, the shuttering can be reduced by half, and the number of available shuttering levels in the sub-pixels doubles. This is the main reason high-end sets offer dynamic lighting (as opposed to power savings, mentioned earlier), allowing the contrast ratio across the screen to be dramatically improved. While the LCD shutters are capable of producing about 1000:1 contrast ratio, by adding 30 levels of dynamic backlighting this is improved to 30,000:1.\n\nHowever, the area of the screen that can be dynamically adjusted is a function of the backlighting source. CCFLs are thin tubes that light up many rows (or columns) across the entire screen at once, and that light is spread out with diffusers. The CCFL must be driven with enough power to light the brightest area of the portion of the image in front of it, so if the image is light on one side and dark on the other, this technique cannot be used successfully. Displays backlit by full arrays of LEDs have an advantage, because each LED lights only a small patch of the screen. This allows the dynamic backlighting to be used on a much wider variety of images. \"Edge-lit\" displays do not enjoy this advantage. These displays have LEDs only along the edges and use a light guide plate covered with thousands of convex bumps that reflect light from the side-firing LEDs out through the LCD matrix and filters. LEDs on edge-lit displays can be dimmed only globally, not individually. For cost reasons, most of LCD TVs have edge-lit backlighting.\n\nThe massive on-paper boost this method provides is the reason many sets now place the \"dynamic contrast ratio\" in their specifications sheets. There is widespread debate in the audio-visual world as to whether or not dynamic contrast ratios are real, or simply marketing speak. Reviewers commonly note that even the best LCDs cannot match the contrast ratios or deep blacks of plasma displays, in spite of being rated, on paper, as having much higher ratios. However, since 2014 there are no major manufacturers of plasma displays left. Contrast leaders are now displays based on OLEDs. \n\nColor on an LCD television is produced by filtering down a white source and then selectively shuttering the three primary colors relative to each other. The accuracy and quality of the resulting colors are thus dependent on the backlighting source and its ability to evenly produce white light. The CCFLs used in early LCD televisions were not particularly white, and tended to be strongest in greens. Modern backlighting has improved this, and sets commonly quote a color space covering about 75% of the NTSC 1953 color gamut. Using white LEDs as the backlight improves this further.\n\nIn September 2009 Nanoco, a UK company, announced that it had signed a joint development agreement with a major Japanese electronics company under which it will design and develop quantum dots (QD) for use in LED backlights in LCD televisions. Quantum dots are valued for displays, because they emit light in very specific Gaussian distributions. This can result in a display that more accurately renders the colors that the human eye can perceive. To generate white light best suited as an LCD backlight, parts of the light of a blue-emitting LED are transformed by quantum dots into small-bandwidth green and red light such that the combined white light allows for a nearly ideal color gamut generated by the color filters of the LCD panel. In addition, efficiency is improved, as intermediate colors (wavelengths) are not present anymore and don't have to be filtered out by the RGB color filters of the LCD screen. US company QD Vision worked with Sony to launch LCD TVs using this technique under the marketing label \"Triluminos\" in 2013.\n\nAt the Consumer Electronics Show 2015, Samsung Electronics, LG Electronics, the Chinese TCL Corporation and Sony showed QD-enhanced LED-backlighting of LCD TVs.\n\nThe production of LCD screens uses nitrogen trifluoride (NF) as an etching fluid during the production of the thin-film components. NF is a potent greenhouse gas, and its relatively long half-life may make it a potentially harmful contributor to global warming. A report in \"Geophysical Research Letters\" suggested that its effects were theoretically much greater than better-known sources of greenhouse gasses like carbon dioxide. As NF was not in widespread use at the time, it was not made part of the Kyoto Protocols and has been deemed \"the missing greenhouse gas\".\n\nCritics of the report point out that it assumes that all of the NF produced would be released to the atmosphere. In reality, the vast majority of NF is broken down during the cleaning processes; two earlier studies found that only 2 to 3% of the gas escapes destruction after its use. Furthermore, the report failed to compare NF's effects with what it replaced, perfluorocarbon, another powerful greenhouse gas, of which anywhere from 30 to 70% escapes to the atmosphere in typical use.\n\n\n"}
{"id": "1075512", "url": "https://en.wikipedia.org/wiki?curid=1075512", "title": "Lewis (robot)", "text": "Lewis (robot)\n\nLewis (named after Meriwether Lewis) is an autonomous robot that performs the job of a wedding photographer: it attends social events, moves around, and takes digital photographs of people. It is a research project of the Media and Machines Laboratory at Washington University.\n\nLewis has been featured on slashdot, on CNN's website, and in various North American newspapers.\n\nIn 2002, Lewis received -- and declined -- an invitation to Nelly's 24th birthday party.\n\n"}
{"id": "7965503", "url": "https://en.wikipedia.org/wiki?curid=7965503", "title": "List of World War II infantry anti-tank weapons of Germany", "text": "List of World War II infantry anti-tank weapons of Germany\n\nList of World War II Infantry Anti-tank weapons of Germany\n\nPanzerbüchse (German: \"anti-tank rifles\")\n\nRocket weapons\n\nRecoilless guns\n\nMiscellaneous\n"}
{"id": "39531188", "url": "https://en.wikipedia.org/wiki?curid=39531188", "title": "MYRRHA", "text": "MYRRHA\n\nThe MYRRHA (Multi-purpose hYbrid Research Reactor for High-tech Applications) is a \"first of its kind\" design project of a nuclear reactor coupled to a proton accelerator (a so-called Accelerator-driven system (ADS)). MYRRHA will be a lead-bismuth cooled fast reactor with two possible configurations: sub-critical or critical.\n\nThe project is managed by SCK•CEN, the Belgium center for nuclear energy research. It will be built based on the experience gained from the first successful demonstration project: GUINEVERE. \n\nMYRRHA has an international recognition and has been listed in December 2010 by the European Commission as one of 50 projects to make Europe the leader in high-tech research in the next 20 years.\n\nMYRRHA is a research reactor aiming to demonstrate the feasibility of the ADS and the lead-cooled fast reactor concepts, with various applications from spent-fuel burning to material irradiation testing.\n\nMYRRHA is intended to be fully operational in 2033, with a first phase (100 MeV accelerator) ready in 2027.\n\nThe ADS concept consists of coupling a subcritical reactor core with a proton accelerator. The neutrons missing from the sub-critical core in order to sustain the chain reaction originate from the spallation reaction of high energy proton beam with a heavy metal target. In order to operate a subcritical core with multiplication factor of k=0.95, a particle accelerator able to provide a maximum current of 4 mA beam of 600 MeV protons is needed. The option of a LINAC is currently preferred.\n\nDue to the extreme rarity of such a stable accelerated proton beam, the MYRRHA project decided to include a fundamental physics experiment called Isol@MYRRHA using a fraction of the proton beam.\n\nThe spallation reaction requires a high Z material, chemical elements with a high atomic number (Z) of protons in the nucleus, as a target.\n\nMixed oxide fuel (MOX) with an enrichment of 30% is planned to be used in the core. 15-15 Ti Austenitic stainless steel cladding allows for good corrosion resistance along with known behavior in the nuclear industry.\n\nAn important feature is the flexibility of MYRRHA, allowing two possible core configurations: subcritical or critical.\n\nThe subcritical core has an effective multiplication factor of 0.95: this is a unique feature in the world of nuclear reactors and will allow very innovative applications for MYRRHA such as burning of actinides or very long cycle operation.\n\nMYRRHA is a pool type lead-bismuth reactor; therefore the mechanical features will be a very heavy core, and atmospheric pressure operation of primary coolant loop. The coolant is very corrosive but operating temperature is quite low. An additional constraint is the opacity of lead-bismuth.\n\nRecently, MYRRHA has entered a new phase in the validation process, with two new experiments aimed at providing insight into the thermo-hydraulic and resistance to earthquakes.\n\nThe EU Commission has supported MYRRHA in the past and can earmark funds in 2016 for it.\n\nThe three-phase implementation plan starts with the construction of the particle accelerator and PTF station (Proton Target Facility). This fully modular facility, called MINERVA, will be operational by 2027. \n\nDuring this first period (2018-2026), the research and development needed to develop the 600 MeV particle accelerator (phase 2) and to construct the reactor (phase 3) will also be carried out. Phases 2 and 3 will be carried out simultaneously between 2027 and 2033. The MYRRHA research facility will be operational by 2030.\n\nMYRRHA has a total budget of 1.6 billion euros. The Belgian government has financially supported the project since 2010 and has committed to financing 40% of the total infrastructure cost. A budget of 100 million euros has already been allocated for the R&D period, for the design of the facility and for the implementation plan (2010-2017). In March 2018, the Federal authorities confirmed the allocation of 19.5 million euros this year in the construction of MYRRHA as well as its commitment in the project from 2019 onwards. On 7 September 2018, the government decided to invest 558 million euros more in the MYRRHA project. This will be used to complete phase 1.\n\nIn addition to this Belgian state support, MYRRHA will be funded by the creation of an international consortium of investors and by European Investment Bank (EIB) financing. A large number of partners have already expressed interest, such as France, Japan, Sweden, the USA and Germany.\n\n"}
{"id": "3807440", "url": "https://en.wikipedia.org/wiki?curid=3807440", "title": "Ministry of Information", "text": "Ministry of Information\n\nMinistry of Information may refer to:\n\n\n\n"}
{"id": "5012268", "url": "https://en.wikipedia.org/wiki?curid=5012268", "title": "Neste Renewable Diesel", "text": "Neste Renewable Diesel\n\nNeste Renewable Diesel (formerly NExBTL) is a renewable diesel fuel production process commercialized by the Finnish oil and refining company Neste. Whether as an admixture or in its pure form, Neste Renewable Diesel is able to supplement or partially replace diesel fuel without problems. Unblended Neste Renewable Diesel meets the requirements set by the European pre-standard CEN TS 15940. Fuel blends meet the European diesel fuel standard EN 590.\n\nDespite the former name BTL, the feedstock is vegetable oil and waste animal fats, not whole plants. However, fuel quality is equal to the synthetic Fischer-Tropsch BTL and GTL diesel fuels.\n\nNeste Renewable Diesel is produced in a patented vegetable oil refining process. Chemically, it entails direct catalytic hydrodeoxygenation (hydrogenolysis) of plant oils, which are triglycerides, into the corresponding alkanes and propane. The glycerol chain of the triglyceride is hydrogenated to the corresponding C3 alkane, propane — there is no glycerol sidestream. This process removes oxygen from the oil; the diesel is not an oxygenate like traditional transesterified FAME biodiesel. Catalytic isomerization into branched alkanes is then done to adjust the cloud point in order to meet winter operability requirements. As it is chemically identical to ideal conventional diesel, it requires no modification or special precautions for the engine.\n\nTwo refineries in Porvoo, Finland were brought on stream in 2007 and 2009, each with a capacity of 0.2 million tons per year. Two larger refineries, with annual production of 0.8 million tons both, were brought on stream in Singapore and Rotterdam in 2010 and 2011, respectively.\n\nNeste has estimated that the use of NExBTL diesel cuts greenhouse gas emissions by 40 to 90 percent in comparison to fossil based diesel.\n\nDue to the chemistry of the process, the renewable diesel is pure alkane and contains no aromatics, oxygen (although oxygen would have promoted cleaner combustion) or sulfur.\n\nThe cloud point (or gel point) can be adjusted down to −40 °C (−40 °F) during the manufacturing process, compared to petrodiesel's cloud point of −30 °C (−22 °F), which could improve the cloud point of diesel when blended. The cloud point is the temperature when the wax precipitates out of the fuel in the form of small wax crystals, making the fuel cloudy and more difficult to move within the fuel lines and systems of vehicles. The lower the cloud point of a particular fuel is, the more suitable it is in colder environments.\n\nA mix of palm oil, rapeseed oil, and waste fat from the food industry can be used. Initially, palm oil was the principal (90%) feedstock, although its share was reduced to 53% by 2013 and to less than 20% by 2017. \n\nHowever, the EU biofuels industry has increased its use of palm oil by 365% during the years 2006–2012, from 0.4 to 1.9 million tonnes per year, and the trend is increasing. Also note that categorising Palm Fatty Acid Distillate, PFAD, as waste is controversial since it can be used to make e.g. soap, candles and animal fodder. In the UK PFAD is classified as a bi-product). PFAD is also omitted from sustainability requirements regarding biodiversity and high carbon stock areas (HCV).\n\nPalm oil may endanger the carbon neutrality of the fuel if forest is cleared and swamps drained to make way for palm plantations. In response to this concern, Neste has joined the Round Table on Sustainable Palm Oil (RSPO) to certify that the palm oil is produced in a carbon-neutral, environmentally responsible manner. Neste purchases most of its palm oil from IOI, but requires a separate production chain for the RSPO-certified palm oil, in order not to create demand for rainforest destruction.\n\nDeforestation would release carbon to the atmosphere, and reduce the overall carbon binding capacity of the land, thus it would be counterproductive with respect to the carbon balance. In 2007, Greenpeace protested the use of palm oil, concluding the potential for deforestation remains. According to Greenpeace, increasing the production of palm oil reduces the available land area, so indirectly generates demand for rainforest destruction, even if the palm oil itself is rainforest-certified. Greenpeace noted RSPO is voluntary organization and claimed government regulation in palm oil producing countries, such as Indonesia, cannot be relied on because of political corruption. Greenpeace also claimed palm oil diesel can actually produce three to 10 times more carbon dioxide emissions than petrodiesel because of the indirect effects of clearing of swamps, forest fires and indirect generation of demand for land area. Greenpeace demands that Neste should use domestic feedstocks such as rapeseed oil or biogas, instead. However, rapeseed is a slower-growing, cold-climate source with lesser carbon-binding potential than the oil palm, making emissions from cultivation and transport proportionally more severe.\n\nIn 2017, the share of palm oil in the feedstock has been reduced to less than 20%, being replaced by reclaimed waste oils such as used frying oil, animal and fish fat, and camelina, jatropha, soy and rapeseed oil. Use of reclaimed waste oil reduces the greenhouse gas impact by 88–91%. Neste is continuing to look into new feedstock, including algae, jatropha and microbial oil.\n\nThis diesel is blended with petrodiesel. A market is created because the European Union required 5.75% of transport fuels should be biofuels by 2010. The EU further decided on 18 December 2008, that by 2020, the share of energy from renewable sources in all forms of transportation be at least 10% of the final consumption of energy. Systems and regions without an electrical grid will be the long-term market for hydrotreated vegetable oils, as the EU prefers electrical use by factor 2.5. In the Helsinki area, the Helsinki Metropolitan Area Council and the Helsinki City Transport conducted a three-year experiment by running buses with 25% Neste Renewable Diesel at first, and then switching to 100% in 2008. The trial, which was the largest field test of a biofuel produced from renewable raw materials worldwide, was a success: local emissions were decreased significantly, with particle emissions decreased by 30% and nitrogen oxide emissions by 10%, with excellent winter performance and no problems with catalytic converters. Since then, Helsinki buses have run on Neste Renewable Diesel.\n\nAs a result of its hydrocarbon nature, Neste Renewable Diesel operates without problems in current diesel vehicles in all climatic conditions. It does not have any of the drawbacks of the traditional ester type FAME biodiesel, such as cold operability, 'best before' date, engine and fuel system deposit formation, risk for microbial growth and water pick up, engine oil dilution and deterioration.\n\nNeste Renewable Diesel can be blended into diesel fuel in any ratio, whereas the use of the traditional FAME biodiesel is limited to maximum 7% by the EN 590 standard in order to avoid technical problems in engines and vehicles.\n\nThe European Union requires the use of renewable fuel, therefore it is expected to be a major market for Neste Renewable Diesel.\n\nFollowing a proposal by VDA, Daimler Trucks and Daimler Buses recommend the biofuel Neste Renewable Diesel as an admixture to petrodiesel.\n\n"}
{"id": "56520290", "url": "https://en.wikipedia.org/wiki?curid=56520290", "title": "Neutral Point Clamped", "text": "Neutral Point Clamped\n\nNeutral point clamped or NPC inverters are widely used topology of multilevel inverters in high-power applications. This kind of inverters are able to be used for up to several megawatts applications. See below links for more information. \n\n"}
{"id": "654128", "url": "https://en.wikipedia.org/wiki?curid=654128", "title": "Novelty", "text": "Novelty\n\nNovelty (derived from Latin word novus for \"new\") is the quality of being new, or following from that, of being striking, original or unusual. Novelty may be the shared experience of a new cultural phenomenon or the subjective perception of an individual.\n\nFrom the meaning of being unusual usage is derived the concept of the novelty dance (a type of dance that is popular for being unusual or humorous); the novelty song (a musical item that capitalizes on something new, unusual, or a current fad); the novelty show (a competition or display in which exhibits or specimens are in some way novel); and novelty architecture (a building or other structure that is interesting because it has an amusing design). It is also this sense that applies to a novelty item, a small manufactured adornment, toy or collectible. These, in turn are often used as promotional merchandise in marketing. The chess term, novelty, is used for a move in chess which has never been played before in a recorded game.\n\nThe term can have pejorative sense and refer to a mere \"innovation\". However, novelty in patent law is part of the legal test to determine whether an invention is patentable. A novelty effect is the tendency for performance to initially improve when new technology is instituted.\n\n"}
{"id": "23160300", "url": "https://en.wikipedia.org/wiki?curid=23160300", "title": "Oil, Gas and Energy Law", "text": "Oil, Gas and Energy Law\n\nOil, Gas and Energy Law is a peer-reviewed academic journal covering all aspects of law pertaining to oil, gas, and energy in general. It publishes short comments, scholarly articles, opinion pieces, and book reviews. The journal was established in 2003 and is published by Maris BV. The founding editor-in-chief was Thomas W. Walde (University of Dundee). The current editors-in-chief are Kim Talus (Tulane University Law School), Anas F. Alhajji (Energy Economist), and P. Andrews-Speed (National University of Singapore).\n\n\"OGEL\" hosts the archives of OGELFORUM, the associated discussion list. It is one of the oldest virtual energy discussion fora for discussion, sharing of insights and intelligence of relevant issues related in a significant way to oil, gas and energy issues: Policy, legislation, contracting, security strategy, climate change related to energy. It incorporates the archives of the ENATRES discussion list. Anas F. Alhajji is the current OGELFORUM Moderator.\n"}
{"id": "2878626", "url": "https://en.wikipedia.org/wiki?curid=2878626", "title": "Optical computing", "text": "Optical computing\n\nOptical or photonic computing uses photons produced by lasers or diodes for computation. For decades, photons have promised to allow a higher bandwidth than the electrons used in conventional computers.\n\nMost research projects focus on replacing current computer components with optical equivalents, resulting in an optical digital computer system processing binary data. This approach appears to offer the best short-term prospects for commercial optical computing, since optical components could be integrated into traditional computers to produce an optical-electronic hybrid. However, optoelectronic devices lose 30% of their energy converting electronic energy into photons and back; this conversion also slows the transmission of messages. All-optical computers eliminate the need for optical-electrical-optical (OEO) conversions, thus lessening the need for electrical power.\n\nApplication-specific devices, such as synthetic aperture radar (SAR) and \"optical correlators\", have been designed to use the principles of optical computing. Correlators can be used, for example, to detect and track objects, and to classify serial time-domain optical data.\n\nThe fundamental building block of modern electronic computers is the transistor. To replace electronic components with optical ones, an equivalent optical transistor is required. This is achieved using materials with a non-linear refractive index. In particular, materials exist where the intensity of incoming light affects the intensity of the light transmitted through the material in a similar manner to the current response of a bipolar transistor. Such an optical transistor can be used to create optical logic gates, which in turn are assembled into the higher level components of the computer's CPU. These will be nonlinear optical crystals used to manipulate light beams into controlling other light beams.\n\nLike any computing system, an Optical computing system needs three things to function well:\nSubstituting electrical components will need data format conversion from photons to electrons, which will make the system slower.\n\nThere are disagreements between researchers about the future capabilities of optical computers; whether or not they may be able to compete with semiconductor-based electronic computers in terms of speed, power consumption, cost, and size is an open question. Critics note that real-world logic systems require \"logic-level restoration, cascadability, fan-out and input–output isolation\", all of which are currently provided by electronic transistors at low cost, low power, and high speed. For optical logic to be competitive beyond a few niche applications, major breakthroughs in non-linear optical device technology would be required, or perhaps a change in the nature of computing itself.\n\nA significant challenge to optical computing is that computation is a nonlinear process in which multiple signals must interact. Light, which is an electromagnetic wave, can only interact with another electromagnetic wave in the presence of electrons in a material, and the strength of this interaction is much weaker for electromagnetic waves, such as light, than for the electronic signals in a conventional computer. This may result in the processing elements for an optical computer requiring more power and larger dimensions than those for a conventional electronic computer using transistors.\n\nA further misconception is that since light can travel much faster than the drift velocity of electrons, and at frequencies measured in THz, optical transistors should be capable of extremely high frequencies. However, any electromagnetic wave must obey the transform limit, and therefore the rate at which an optical transistor can respond to a signal is still limited by its spectral bandwidth. However, in fiber optic communications, practical limits such as dispersion often constrain channels to bandwidths of 10s of GHz, only slightly better than many silicon transistors. Obtaining dramatically faster operation than electronic transistors would therefore require practical methods of transmitting ultrashort pulses down highly dispersive waveguides.\n\nPhotonic logic is the use of photons (light) in logic gates (NOT, AND, OR, NAND, NOR, XOR, XNOR). Switching is obtained using nonlinear optical effects when two or more signals are combined.\n\nResonators are especially useful in photonic logic, since they allow a build-up of energy from constructive interference, thus enhancing optical nonlinear effects.\n\nOther approaches currently being investigated include photonic logic at a molecular level, using photoluminescent chemicals. In a recent demonstration, Witlicki et al. performed logical operations using molecules and SERS.\n\nThe basic idea is to delay light (or any other signal) in order to perform useful computations. Of interest would be to solve NP-complete problems as those are difficult problems for the conventional computers.\n\nThere are 2 basic properties of light that are actually used in this approach:\n\n\nWhen solving a problem with time-delays the following steps must be followed:\n\n\nThe first problem attacked in this way was the Hamiltonian path problem. Later, other problems have been tackled in this way.\n\nThe simplest one is the subset sum problem. An optical device solving an instance with 4 numbers {a1, a2, a3, a4} is depicted below:\n\nThe light will enter in Start node. It will be divided into 2 (sub)rays of smaller intensity. These 2 rays will arrive into the second node at moments a1 and 0. Each of them will be divided into 2 subrays which\nwill arrive in the 3rd node at moments 0, a1, a2 and a1 + a2. These represents the all subsets of the set {a1, a2}. We expect fluctuations in the intensity of the signal at no more than 4 different moments. In the destination node we expect fluctuations at no more than 16 different moments (which are all the subsets of the given. If we have a fluctuation in the target moment B, it means that we have a solution of the problem, otherwise there is no subset whose sum of elements equals B. For the practical implementation we cannot have zero-length cables, thus all cables are increased with a small (fixed for all) value k. In this case the solution is expected at moment B+n*k.\n\nWavelength-based computing can be used to solve the 3-SAT problem with n variables, m clause and with no more than 3 variables per clause. Each wavelength, contained in a light ray, is considered as possible value-assignments to n variables. The optical device contains prisms and mirrors are used to discriminate proper wavelengths which satisfy the formula.\n\nThis approach uses a Xerox machine and transparent sheets for performing computations. k-SAT problem with n variables, m clauses and at most k variables per clause has been solved in 3 steps:\n\n\nThe travelling salesman problem has been solved in by using an optical approach. All possible TSP paths have been generated and stored in a binary matrix which was multiplied with another gray-scale vector containing the distances between cities. The multiplication is performed optically by using an optical correlator.\n\nMany computations, particularly in scientific applications, require frequent use of the 2D discrete Fourier transform (DFT) – for example in solving differential equations describing propagation of waves or transfer of heat. Though modern GPU technologies typically enable high-speed computation of large 2D DFTs, recently techniques have been developed that can perform DFTs optically by utilising the natural Fourier transforming property of lenses. The input is encoded using a liquid crystal spatial light modulator and the result is measured using a conventional CMOS or CCD image sensor. Such optical architectures can offer superior scaling of computational complexity due to the inherently highly interconnected nature of optical propagation, and have been used to solve 2D heat equations.\n\nPhysical computers whose design was inspired by the theoretical Ising model are called Ising machines.\n\nYoshihisa Yamamoto pioneered building Ising machines using photons. Initially Yamamoto and his colleagues built an Ising machine using lasers, mirrors, and other optical components commonly found on an optical table.\n\nLater a team at Hewlett Packard Labs including Dave Kielpinski developed photonic chip design tools and used them to build an Ising machine on a single chip, integrating 1,052 optical components on that single chip.\n\n\n\n"}
{"id": "40105963", "url": "https://en.wikipedia.org/wiki?curid=40105963", "title": "PlayStation models", "text": "PlayStation models\n\nA number of models of Sony's PlayStation video game console were produced.\n\nThe PlayStation went through a number of variants during its production run, each accompanied by a change in the part number. From an external viewpoint, the most notable change was the gradual reduction in the number of external connectors from the back of the unit. This started very early on with the original Japanese launch units; the SCPH-1000, released on December 3, 1994, was the only model that had an S-Video port, which was removed on the next release. This also led to a discrepancy where the US and European launch units had the same part number series (SCPH-100x) as the Japanese launch units, but had different hardware (Rev. B silicon and no S-Video port)—their technical equivalents were the Japanese SCPH-3000, so for consistency should have been SCPH-3001 and SCPH-3002. Inconsistent numbering was also used for the Yaroze machines, which were based on SCPH-5000 and later 1001/1002 hardware, but numbered DTL-H3000, DTL-H3001, and DTL-H3002. Also, the first models (DTL-H1000, DTL-H1001, DTL-H1002) had problems with printf function and developers had to use another function instead. \n\nThis series of machines had a reputation for CD drive problems — the original optical pickup sled (KSM-440AAM) was made of thermoplastic and placed close to the power supply, eventually leading to uneven wear that moved the laser into a position where it was no longer parallel with the CD surface. Late KSM-440ACM drives had the sled replaced with a die-cast one with hard nylon inserts in order to address the issue.\n\nThe original hardware design included dual-ported VRAM as graphics memory, but due to a shortage in parts, Sony redesigned the GPU to use SGRAM instead (which could simulate dual-porting to some extent by using two banks). At the same time the GPU was upgraded to utilize smoother shading, resulting in overall better image quality compared to earlier models, which were more prone to banding; additionally, performance for transparency effects was improved, resulting in less slowdown in scenes using this effect heavily. This Rev. C hardware first appeared in late 1995 and, unlike in Japan, was not marked with a model number change in NTSC-U and PAL territories - SCPH-1001/1002 systems can have either revision, as the change happened between revisions of the PU-8 mainboard.\n\nThe PAL region consoles from SCPH-1002 up to SCPH-5552 were different from the systems released in other regions in that they had a different menu design; a grey blocked background with square icons for the Memory Card (an icon showing a PlayStation with 2 memory cards inserted) and CD player (an icon with musical keyboards) menus. The CD player also included reverberation effects unique to those systems until the release of the PS one in 2000, which featured a slightly modified version of the BIOS.\n\nWith the release of the SCPH-5000 series being produced only in Japan, it followed the same exterior design as the Japanese SCPH-3xxx series, its only differences being that it was switched to Rev. C hardware (same as late 1001/1002 units) with some upgrades to flawed components from previous models and a reduced retail price. This was followed by the first major consolidation, SCPH-550x/5001 and PAL-exclusive SCPH-5552 units, released in April 1997. This model further addressed the reliability issues with the disc drive assembly by placing the drive further away from the power supply in order to reduce heat; the chipset was also redesigned to use digital servo for focus/tracking and also to auto-calibrate the drive, as opposed to manual gain/bias calibration on earlier models. Also, shielding and PSU wiring were simplified, and from the SCPH-5001 on the RCA jacks and RFU power connectors were removed from the rear panel and the printed text on the back was changed to reliefs of the same. Starting with the SCPH-550x series, PAL variants had the \"power\" and \"open\" buttons changed from text to symbols, something that would later appear on the redesigned PS one. Originally, the PlayStation was supposed to have provision on Video CD support, but this feature was only included on the Asian exclusive SCPH-5903 model.\n\nThese were followed by the SCPH-700x and SCPH-750x series, released in April 1998—they are externally identical to the SCPH-500x machines, but have internal changes made to reduce manufacturing costs (for example, the system RAM went from 4 chips to 1, and the CD controller went from 3 chips to 1) and these were the last models to support parallel port for Gameshark devices. In addition, a slight change of the start-up screen was made; the diamond is seen as longer and thinner and the trademark symbol (™) is now placed after \"Computer Entertainment\" instead of after the diamond, as it was on the earlier models. New to the SCPH-700x series was the introduction of the \"Sound Scope\" - light show music visualizations. These were accessible by pressing the Select button while playing any normal audio CD in the system's CD player. While watching these visualizations, players could also add various effects like color cycling or motion blur and can save/load their memory card. These were seen on the SCPH-700x, 750x, 900x, and PS one models.\n\nThe final revision to the original PlayStation was the SCPH-900x series, released in May 1999. These had the same hardware as the SCPH-750x models, except the parallel port was removed and the size of the PCB is further reduced. The removal of the parallel port is partially due to the fact that Sony did not release an official add-on for it; it was used for cheat cartridges for the parallel port to defeat the regional lockouts and copy protection. The SCPH-900x was the last model to support PlayStation Link Cable connection, as the Serial I/O port was removed on all PS one models.\n\nThe PS one, released on July 7, 2000, was originally based on essentially the same hardware as the SCPH-900x; the serial port was removed, the controller / memory card ports moved to the main PCB and the internal power supply replaced with an external 7.5VDC power adapter with the other required power rails being generated internally on the main using a mixture of regulators and DC/DC converters for the various rails. It also incorporated a slightly modified version of the menu design previously used only on PAL consoles. The later revision (still designated as SCPH-10x, but with a different PM-41(2) main circuit board) was functionally identical, but reduced manufacturing cost for a last time by moving to more highly integrated chips, namely the replacement of external RAM with on-chip RAM, which both reduced the parts count and allowed the use of smaller and cheaper packages by reducing the number of pins required.\n\nThere were also debugging consoles - these were generally in either blue or green cases, although there were some special production units (mostly intended for use as show demo units) that were grey, the same as the retail consoles. The debug units were designed to be as close as possible to retail consoles, so they only had 2MB of ram (the developer boards had 8MB) and had standard retail boot ROMs. The only real difference is that the CD controller was reprogrammed so that it would identify any disc that had a data track as being \"licensed\", rather than requiring the region code in the lead-in that was present on pressed PlayStation CDs. This was done to allow developers to burn games to CD-R for testing - a side effect of this was that most debug consoles would also boot discs from other regions (one notable exception being the later NTSC:J debugs, which only boot Japanese titles), although this was not officially supported - Sony made specific debug consoles for each region, and the technical required checklist provided by Sony for each region required you to test your title on the correct debug stations.\n\nThe reason for the two different case colors was a hardware change that Sony had made fairly early in the PlayStation production cycle - the original machines were built using Rev. A (early Japan market units) or Rev. B (later Japan units, US and Europe) hardware, both using the same GPU with VRAM to store the video data. Later models used Rev. C silicon and SGRAM - although the two chipsets had very similar performance, and Rev. C was explicitly designed with compatibility in mind, they were not identical - the Rev. C version was significantly faster at doing alpha blending, and hence the PS \"semitransparent\" writing mode - it was also rather slow at certain screen memory block moves (basically, ones involving narrow vertical strips of the display) on top of this there were some minor hardware bugs in the older silicon that had been addressed by including workarounds for them in the libraries - the later library versions checked the GPU type at startup time and disabled the patches if they were not needed. Because this made the two machine types quite significantly different from each other, you had to test your title on both of them before submitting it. The blue debugs (DTL-H100x, DTL-H110x) had the old silicon and the green ones (DTL-H120x) had the new silicon.\n\nIn 1997, Sony released a version of the PlayStation called the Net Yaroze. It was more expensive than the original PlayStation, colored black instead of the usual gray, and most importantly, came with tools and instructions that allowed a user to be able to program PlayStation games and applications without the need for a full developer suite, which cost many times the amount of a PlayStation and was only available to approved video game developers. Naturally, the Net Yaroze lacked many of the features the full developer suite provided. Programmers were also limited by the 2 MB of total game space that Net Yaroze allowed. The amount of space may seem small, but games like \"Ridge Racer\" ran entirely from the system RAM (except for the streamed music tracks). It was unique in that it was the only officially retailed PlayStation with no regional lockout; it would play games from any territory. It would not however play CD-R discs, so it was not possible to create self-booting Yaroze games without a modified PlayStation.\n\nThe PS one (alternatively spelled as PS One, PSOne or PSone) is a smaller, redesigned version of the original PlayStation platform. (Dimensions are 38 mm × 193 mm × 144 mm versus 45 mm × 260 mm × 185 mm.) It was released on July 7, 2000, and went on to outsell all other consoles throughout the remainder of the year—including Sony's own PlayStation 2. The PS one is fully compatible with all PlayStation software.\nThe PS one model also had additional changes, including the removal of the parallel and serial ports from the rear of the console, and the removal of the reset button (the power button is also labeled as a reset button, but the console cannot be reset without entirely turning it off).\n\nSony also released a version with a 5\" LCD screen and an adaptor (though it did not have a battery: it is powered by plugging the adaptor in a main socket, or in a car). It was called the Combo pack. However, it includes a headphone jack (for headphones or other audio connection) and an AV mini jack for connecting camcorders or other devices.\n\n<nowiki>*</nowiki>All models feature \"A/V Direct Out\", \"Parallel Port\" and \"Serial Port\"; none feature \"Sound Scope\"\n<nowiki>*</nowiki>All models use a low-quality CD drive.\n<nowiki>*</nowiki>All models (except those with later Japanese boot ROM) can boot software with any region code.\n\nThe last digit of the PlayStation model number denotes the region in which it was sold:\n\nThe first batch of PlayStations used a KSM-440AAM laser unit whose case and all movable parts were completely made out of plastic. Over time, friction caused the plastic tray to wear out—usually unevenly. The placement of the laser unit close to the power supply accelerated wear because of the additional heat, which made the plastic even more vulnerable to friction. Eventually, the tray would become so worn that the laser no longer pointed directly at the CD and games would no longer load. Sony first addressed the problem by making the tray out of die-cast metal, and additionally also placed the laser unit farther away from the power supply on later models of the PlayStation.\n\nSome units, particularly the early 100x models, would be unable to play FMV or music correctly, resulting in skipping or freezing. In more extreme cases the PlayStation would only work correctly when turned onto its side or upside down.\n\n"}
{"id": "48481073", "url": "https://en.wikipedia.org/wiki?curid=48481073", "title": "Playmation", "text": "Playmation\n\nPlaymation is a system of toys, wearables, and companion apps from Disney and Hasbro. The system is designed to keep kids active, replacing screens with pretend play. Players can receive missions through a companion app, and track scores and accomplishments.\n\nThe toy system launched in October 2015 with the Marvel Avengers collection. Star Wars and Frozen were planned for 2016 and 2017.\n\nIt is reported that much of the development team has been laid-off and that while the existing Avengers product line will continue to be sold through Christmas 2016 any further development is on-hold.\n\n\nHeroes\nVillains\n\nToys-to-life\n"}
{"id": "41865169", "url": "https://en.wikipedia.org/wiki?curid=41865169", "title": "ProductiveMuslim", "text": "ProductiveMuslim\n\nProductiveMuslim is a multinational virtual organisation and a website that specialises in providing materials for the Muslim Ummah combining teachings of Islam with contemporary productivity tips and advice. ProductiveMuslim is managed by an international team of volunteers from different countries. These include Saudi Arabia, Malaysia, Egypt and Morocco.\n\nProductiveMuslim.com started as a personal online journal for Mohammed Faris, the founder, where he recorded his thoughts on topics that connect Islam to productivity. Over time, Faris found that when properly practiced, Islam leads to a productive lifestyle. In July 2008, the website was launched. With the support of an international team of volunteers, the website grew and its content improved over time. In July 2011, ProductiveMuslim was registered as a company under the name \"ProductiveMuslim Ltd\".\n\nMohammed Faris, aka \"Abu Productive\", was born in Tanzania from Yemeni origins. He is an international coach, author, and speaker who helps executives, professionals, and entrepreneurs re-balance their lives Spiritually, Physically and Socially to achieve peak performance in their personal and professional lives.\n\nFaris is also the founder of ProductiveMuslim.com - a popular lifestyle blog for Muslims - and author of the\nbook “The Productive Muslim: Where Faith Meets Productivity”. He did his bachelors and masters in the UK, and formerly worked as Risk Management Specialist in the Islamic Development Bank.\n\nThe website has been providing articles, podcasts, retreats, animation videos, doodles and an online academy since October 2008. Today, there are hundreds of articles on the website. The articles represent lessons extracted from the Quran, seerah of the Prophet Muhammad, and history of Islamic civilization, collated with modern tips and tools that lead to a productive lifestyle. The articles are categorised into 6 categories: \"Ramadan & Productivity\", \"Islam & Productivity\", \"Get Motivated\", \"Feel Better\", \"Work Better\" and \"Help Others\". They are written by a variety of volunteer contributors, productivity experts and editors. Besides articles, the website also provides animation videos and doodles that demonstrate productivity tips in an educating and entertaining style. The use of ProductiveMuslim's popular \"orange stickman\" in the videos added to the distinctive branding of the videos.\n\nProductiveMuslim.com is localised according to languages, not regions. It is available in English, Arabic and French.\n\nProductiveMuslim delivered 2-day seminars in several countries, including Egypt, Malaysia and Saudi Arabia. Topics covered included: definition of productivity, why a Muslim should be productive, the link between spirituality and productivity, managing energy levels, balancing between different roles, building focus and afterlife productivity. Online seminars are delivered where attendees contribute via an online website either through mobile phones, Skype or similar means.\n\nWorkshops were organised by Faris for businesses to teach employees different aspects of productivity at work such as time management and personal productivity.\n\nIn 2011, Faris, the founder, appeared in an 8-minute TEDx talk, held in Saudi Arabia, where he talked about how Islam teaches Muslims productivity.\n\nProductiveMuslim won the Brass Crescent Awards several times. Brass Crescent Awards is an annual contest where visitors vote for the best Islam-themed online blog in various categories.\n\n"}
{"id": "7656324", "url": "https://en.wikipedia.org/wiki?curid=7656324", "title": "Pyrocollodion", "text": "Pyrocollodion\n\nPyrocollodion is a smokeless powder invented by Dmitri Mendeleev.\n\nDmitri Mendeleev discovered Pyrocollodion in 1892 and proposed its use as a replacement for gunpowder in the Russian Navy. This offer was rejected because of cost and efficiency. \n\nPyrocollodion is known to be spontaneously combustible, and explosive.\n\nWhen ignited Pyrocollodion will burn and explode quickly with excessive heat output compared to the existing gunpoweders of the time. Pyrocollodion was a reasonable choice or option at the time for military munitions use. \n\nIf ignited in a tight contained space, Pyrocollodion will leave little to no remnants, such as unburned powder, particular kinds of flame scarring, or smoke of any kind.\n\nPyrocollodion is a variant of nitrocellulose family of compunds. \n"}
{"id": "14723305", "url": "https://en.wikipedia.org/wiki?curid=14723305", "title": "Sevici", "text": "Sevici\n\nSevici is a community bicycle program in Seville inaugurated in April 2007, modeled after the Vélo'v service in Lyon and Vélib' in Paris. Its purpose is to cover the small and medium daily routes within the city in a climate friendly way, almost without pollution (specially the emission of finest particulate matter), roadway noise, traffic congestion and to reclaim the urban streets with non-polluting vehicles.\nThe Ayuntamiento de Sevilla (Municipal Government) and JCDecaux manage and maintain the system. Two membership options are available; a weekly pass, purchasable at each of the station kiosks by credit card at a cost of 13.33 euros, and a yearly pass, requiring an application to be sent to the municipal government at a cost of 33.33 euros . Before the end of 2008 more than 250 stations and 2500 bikes will be available. The stations are situated throughout the inner-city with a distance of around 200 metres between each one, with many situated next to public transport stops to allow for intermodal use. The bikes can be borrowed from, and returned to, any station in the system, making it suitable for one way travel. Each station has between 10 and 40 parking slots to fix and lock the bicycle.\n\nTo borrow a bike with a yearly pass, one simply swipes the contactless RFID-card at a station kiosk to be personally identified by the system, which then unlocks a bike from the support frame. With a weekly pass, a ticket is printed with an ID number that can be punched in at the station kiosks to identify the user account. Bicycles can be used for the first 30 minutes for free; the next 30 minutes are 1.03 euro. Subsequent hours are 2.04 euros. Hourly rates are discounted for the yearly pass. To return a bicycle one simply places the bike in a spare slot at a station, the bike is recognized automatically and is locked into place.\n\nThe yearly subscription costing 33.33 euros requires an address where the pass is sent, whereas the weekly alternative (13.33 euros) can be obtained directly at any station by presenting a credit card. In both cases a 150 euro deposit is authorized in order to deter theft. (If a debit card is used, the 150 euros will be taken immediately and returned at the end of the hire period.)\n\nWhen a weekly membership is purchased the user is provided with a member code ticket to be used at the stations rather than an RFID card, making the service immediately available to tourists.\n\n\n"}
{"id": "48923844", "url": "https://en.wikipedia.org/wiki?curid=48923844", "title": "Silicon Savannah", "text": "Silicon Savannah\n\nSilicon Savannah is a term used to refer to the technology ecosystem in Kenya. The term is a play on Silicon Valley and the grassland savanna ecosystem that is a dominant feature of Kenya's ecology. It is known for producing fast growing social enterprises like M-kopa and others.\n\nThis term became associated with Konza Technology City, a planned urban development in Machakos and Makueni counties that would focus on offering services related to information technology to support tech entrepreneurs in Kenya.\n"}
{"id": "387746", "url": "https://en.wikipedia.org/wiki?curid=387746", "title": "Social simulation", "text": "Social simulation\n\nSocial simulation is a research field that applies computational methods to study issues in the social sciences. The issues explored include problems in computational law, psychology, organizational behavior, sociology, political science, economics, anthropology, geography, engineering, archaeology and linguistics .\n\nSocial simulation aims to cross the gap between the descriptive approach used in the social sciences and the formal approach used in the natural sciences, by moving the focus on the processes/mechanisms/behaviors that build the social reality.\n\nIn social simulation, computers support human reasoning activities by executing these mechanisms. This field explores the simulation of societies as complex non-linear systems, which are difficult to study with classical mathematical equation-based models. Robert Axelrod regards social simulation as a third way of doing science, differing from both the deductive and inductive approach; generating data that can be analysed inductively, but coming from a rigorously specified set of rules rather than from direct measurement of the real world. Thus, simulating a phenomenon is akin to generating it—constructing artificial societies. These ambitious aims have encountered several criticisms.\n\nThe social simulation approach to the social sciences is promoted and coordinated by three regional associations, ESSA for Europe, North America (reorganizing under the new CSSS name), and PAAA Pacific Asia.\n\nThe history of the agent-based model can be traced back to the Von Neumann machine, a theoretical machine capable of reproducing itself. The device von Neumann proposed would follow precisely detailed instructions to fashion a copy of itself. The concept was then improved by von Neumann's friend Stanislaw Ulam, also a mathematician; Ulam suggested that the machine be built on paper, as a collection of cells on a grid. The idea intrigued von Neumann, who drew it up—creating the first of devices later termed cellular automata.\n\nAnother improvement was brought by mathematician, John Conway. He constructed the well-known Game of Life. Unlike the von Neumann's machine, Conway's Game of Life operated by simple rules in a virtual world in the form of a 2-dimensional checkerboard.\n\nThe birth of the agent-based model as a model for social systems was primarily brought about by a computer scientist, Craig Reynolds. He tried to model the reality of lively biological agents, known as the artificial life, a term coined by Christopher Langton.\n\nJoshua M. Epstein and Robert Axtell developed the first large scale agent model, the Sugarscape, to simulate and explore the role of social phenomena such as seasonal migrations, pollution, sexual reproduction, combat, transmission of disease, and even culture.\n\nKathleen M. Carley published \"Computational Organizational Science and Organizational Engineering\" defining the movement of simulation into\norganizations, established a journal for social simulation applied to organizations and complex socio-technical systems: Computational and Mathematical Organization Theory, and was the founding president of the North American Association of Computational Social and Organizational Systems that morphed into the current CSSSA.\n\nNigel Gilbert published with Klaus G. Troitzsch the first textbook on Social Simulation: Simulation for the Social Scientist (1999) and established its most relevant journal: the Journal of Artificial Societies and Social Simulation.\n\nMore recently, Ron Sun developed methods for basing agent-based simulation on models of human cognition, known as cognitive social simulation (see )\n\nHere are some sample topics that have been explored with social simulation:\n\n\nSocial simulation can refer to a general class of strategies for understanding social dynamics using computers to simulate social systems. Social simulation allows for a more systematic way of viewing the possibilities of outcomes.\n\nThere are four major types of social simulation: \n\nA social simulation may fall within the rubric of computational sociology which is a recently developed branch of sociology that uses computation to analyze social phenomena. The basic premise of computational sociology is to take advantage of computer simulations in the construction of social theories. It involves the understanding of social agents, the interaction among these agents, and the effect of these interactions on the social aggregate. Although the subject matter and methodologies in social science differ from those in natural science or computer science, several of the approaches used in contemporary social simulation originated from fields such as physics and artificial intelligence.\n\nSystem Level Simulation (SLS) is the oldest level of social simulation. System level simulation looks at the situation as a whole. This theoretical outlook on social situations uses a wide range of information to determine what should happen to society and its members if certain variables are present. Therefore, with specific variables presented, society and its members should have a certain response to the new situation. Navigating through this theoretical simulation will allow researchers to develop educated ideas of what will happen under some specific variables.\n\nFor example, if NASA were to conduct a system level simulation it would benefit the organization by providing a cost effective research method to navigate through the simulation. This allows the researcher to steer through the virtual possibilities of the given simulation and develop safety procedures, and to produce proven facts about how a certain situation will play out. \n\nSystem level modeling (SLM) aims to specifically predict (unlike system level simulation's generalization in prediction) and convey any number of actions, behaviors, or other theoretical possibilities of nearly any person, object, construct et cetera within a system using a large set of mathematical equations and computer programming in the form of models.\n\nA model is a representation of a specific thing ranging from objects and people to structures and products created through mathematical equations and are designed, using computers, in such a way that they are able to stand-in as the aforementioned things in a study. Models can be either simplistic or complex, depending on the need for either; however, models are intended to be simpler than what they are representing while remaining realistically similar in order to be used accurately. They are built using a collection of data that is translated into computing languages that allow them to represent the system in question. These models, much like simulations, are used to help us better understand specific roles and actions of different things so as to predict behavior and the like.\n\nAgent-based social simulation (ABSS) consists of modeling different societies after artificial agents, (varying on scale) and placing them in a computer simulated society to observe the behaviors of the agents. From this data it is possible to learn about the reactions of the artificial agents and translate them into the results of non-artificial agents and simulations. Three main fields in ABSS are agent-based computing, social science, and computer simulation.\n\nAgent-based computing is the design of the model and agents, while the computer simulation is the part of the simulation of the agents in the model and the outcomes. The social science is a mixture of sciences and social part of the model. It is where the social phenomena is developed and theorized. The main purpose of ABSS is to provide models and tools for agent-based simulation of social phenomena. With ABSS we can explore different outcomes for phenomena where we might not be able to view the outcome in real life. It can provide us valuable information on society and the outcomes of social events or phenomena.\n\nAgent-based modeling (ABM) is a system in which a collection of agents independently interact on networks. Each individual agent is responsible for different behaviors that result in collective behaviors. These behaviors as a whole help to define the workings of the network. ABM focuses on human social interactions and how people work together and communicate with one another without having one, single \"group mind\". This essentially means that it tends to focus on the consequences of interactions between people (the agents) in a population. Researchers are better able to understand this type of modeling by modeling these dynamics on a smaller, more localized level. Essentially, ABM helps to better understand interactions between people (agents) who, in turn, influence one another (in response to these influences). Simple individual rules or actions can result in coherent group behavior. Changes in these individual acts can affect the collective group in any given population.\n\nAgent-based modeling is an experimental tool for theoretical research. It enables one to deal with more complex individual behaviors, such as adaptation. Overall, through this type of modeling, the creator, or researcher, aims to model behavior of agents and the communication between them in order to better understand how these individual interactions impact an entire population. In essence, ABM is a way of modeling and understanding different global patterns.\n\nThere are several current research projects that relate directly to modeling and agent-based simulation the following are listed below with a brief overview.\n\n\nAgent-based modeling is most useful in providing a bridge between micro and macro levels, which is a large part of what sociology studies. Agent-based models are most appropriate for studying processes that lack central coordination, including the emergence of institutions that, once established, impose order from the top down. The models focus on how simple and predictable local interactions generate familiar but highly detailed global patterns, such as emergence of norms and participation of collective action. Michael W. Macy and Robert Willer researched a recent survey of applications and found that there were two main problems with agent-based modeling the self-organization of social structure and the emergence of social order . Below is a brief description of each problem Macy and Willer believe there to be;\n\nThese examples simply show the complexity of our environment and that agent-based models are designed to explore the minimal conditions, the simplest set of assumptions about human behavior, required for a given social phenomenon to emerge at a higher level of organization.\n\nSince its creation, computerized social simulation has been the target of some criticism in regard to its practicality and accuracy. Social simulation's simplification of the complex to form models from which we can better understand the latter is sometimes seen as a draw back, as using fairly simple models to simulate real life with computers is not always the best way to predict behavior.\n\nMost of the criticism seems to be aimed at agent-based models and simulation and how they work:\n\n\nResearchers working in social simulation might respond that the competing theories from the social sciences are far simpler than those achieved through simulation and therefore suffer the aforementioned drawbacks much more strongly. Theories in some social science tend to be linear models that are not dynamic, and are generally inferred from small laboratory experiments (laboratory tests are most common in psychology but rare in sociology, political science, economics and geography). The behavior of populations of agents under these models is rarely tested or verified against empirical observation.\n\n\n\n"}
{"id": "56671339", "url": "https://en.wikipedia.org/wiki?curid=56671339", "title": "Sophie's World (video game)", "text": "Sophie's World (video game)\n\nSophie's World is a 1997 educational adventure game developed by The MultiMedia Corporation and published by Voyager. it is an adaption of the novel Sophie's World, written by Norwegian writer Jostein Gaarder.\n\nThe game was developed by a small team of 6-7 core MAC developers and Win32 developers, 1 lead graphic designer, and producers, as well as contract workers. The scenes, audio, and timelines were collaborated between the departments. Sam Deane created a game engine from scratch to present the scenes. Simon Jenkins wrote the natural language engine for the in-game instant message application. The writing was aided by academics of philosophy to ensure realism and accuracy.\n\nThe player explores philosophical concepts through the eyes of the 14 year old Sophie. The title contains an in-game encyclopaedic database.\n\nRay Ivey of Just Adventure described it as short, imaginative, and delicious. Quandaryland's Steve Ramsey wrote that while the game only took an hor to complete, he didn't have a desire to stay in the world for longer.\n\nAt the 1997 Bima Awards, the British Interactive Multimedia Association presented the game with a craft award for sound and music.\n"}
{"id": "32628343", "url": "https://en.wikipedia.org/wiki?curid=32628343", "title": "Sustainable furniture design", "text": "Sustainable furniture design\n\nSustainable/ Industrial/ Recycled furniture design is an effort to address the environmental impact of furniture products on the environment by considering all aspects of the design and manufacturing process. Design considerations can include using recycled materials in the manufacturing process and using products that can be disassembled and recycled after their useful life. Sustainable furniture design strives to create a closed-loop cycle in which materials and products are perpetually recycled so as to avoid disposal in landfills. The Think Chair by Steelcase is a recent example of sustainable furniture design.\n\n\n"}
{"id": "7613634", "url": "https://en.wikipedia.org/wiki?curid=7613634", "title": "Title 21 CFR Part 11", "text": "Title 21 CFR Part 11\n\nTitle 21 CFR Part 11 is the part of Title 21 of the Code of Federal Regulations that establishes the United States Food and Drug Administration (FDA) regulations on electronic records and electronic signatures (ERES). Part 11, as it is commonly called, defines the criteria under which electronic records and electronic signatures are considered trustworthy, reliable, and equivalent to paper records (Title 21 CFR Part 11 Section 11.1 (a)).\n\nPractically speaking, Part 11 applies to drug makers, medical device manufacturers, biotech companies, biologics developers, CROs, and other FDA-regulated industries, with some specific exceptions. It requires that they implement controls, including audits, system validations, audit trails, electronic signatures, and documentation for software and systems involved in processing the electronic data that FDA predicate rules require them to maintain. A predicate rule is any requirement set forth in the Federal Food, Drug and Cosmetic Act, the Public Health Service Act, or any FDA regulation other than Part 11. \n\nThe rule also applies to submissions made to the FDA in electronic format (e.g., a New Drug Application) but not to paper submissions by electronic methods (i.e., faxes). It specifically does not require the 21CFR11 requirement for record retention for tracebacks by food manufacturers. Most food manufacturers are not otherwise explicitly required to keep detailed records, but electronic documentation kept for HACCP and similar requirements must meet these requirements.\n\nBroad sections of the regulation have been challenged as \"very expensive and for some applications almost impractical\", and the FDA has stated in guidance that it will exercise enforcement discretion on many parts of the rule. This has led to confusion on exactly what is required, and the rule is being revised. In practice, the requirements on access controls are the only part routinely enforced. The \"predicate rules\" that required organizations to keep records the first place are still in effect. If electronic records are illegible, inaccessible, or corrupted, manufacturers are still subject to those requirements.\n\nIf a regulated firm keeps \"hard copies\" of all required records, those paper documents can be considered the authoritative document for regulatory purposes, and the computer system is not in scope for electronic records requirements—though systems that control processes subject to predicate rules still require validation. Firms should be careful to make a claim that the \"hard copy\" of required records is the authoritative document. For the \"hard copy\" produced from electronic source to be the authoritative document, it must be a complete and accurate copy of the electronic source. The manufacturer must use the hard copy (rather than electronic versions stored in the system) of the records for regulated activities. The current technical architecture of computer systems increasingly makes the Part 11, Electronic Records; Electronic Signatures — Scope and Application for the complete and accurate copy requirement extremely high. \n\n\nVarious keynote speeches by FDA insiders early in the 21st century (in addition to high-profile audit findings focusing on computer system compliance) resulted in many companies scrambling to mount a defense against rule enforcement that they were procedurally and technologically unprepared for. Many software and instrumentation vendors released Part 11 \"compliant\" updates that were either incomplete or insufficient to fully comply with the rule. Complaints about the wasting of critical resources, non-value added aspects, in addition to confusion within the drug, medical device, biotech/biologic and other industries about the true scope and enforcement aspects of Part 11 resulted in the FDA release of:\nThis document was intended to clarify how Part 11 should be implemented and would be enforced. But, as with all FDA guidances, it was not intended to convey the full force of law—rather, it expressed the FDA's \"current thinking\" on Part 11 compliance. Many within the industry, while pleased with the more limited scope defined in the guidance, complained that, in some areas, the 2003 guidance contradicted requirements in the 1997 Final Rule.\nIn May 2007, the FDA issued the final version of their guidance on computerized systems in clinical investigations. This guidance supersedes the guidance of the same name dated April 1999; and supplements the guidance for industry on Part 11, Electronic Records; Electronic Signatures — Scope and Application and the Agency’s international harmonization efforts when applying these guidances to source data generated at clinical study sites.\n\nFDA had previously announced that a new Part 11 would be released late 2006. The Agency has since pushed that release date back. The FDA has not announced a revised time of release. John Murray, member of the Part 11 Working Group (the team at FDA developing the new Part 11), has publicly stated that the timetable for release is \"flexible\".\n\n\n"}
{"id": "55957034", "url": "https://en.wikipedia.org/wiki?curid=55957034", "title": "VideoKen", "text": "VideoKen\n\nVideoKen Inc. (earlier known as Yen4Ken Inc.) is an AI product company headquartered in Princeton, New Jersey with a subsidiary in Bangalore, India.It works on advanced video indexing technologies, which are built on a foundation of cutting edge research involving Big Data analytics, Machine Learning and Artificial Intelligence. VideoKen holds multiple granted and pending US patents in video technology.\n\nVideoKen enables videos with Table of Contents (index) and Phrase Cloud (glossary) - like a digital textbook - along with other features to make videos more consumable. \n\nVideoKen has raised $1 million from a consortium of angel investors including LG Chandrasekhar, chairman of Sutures India and SRI Capital, led by Sashi Reddi and others, including some former and current executives of Flipkart.\n\nVideoKen was founded in January 2017 by Manish Gupta, former Vice President (VP) and Director, Xerox Research India and Ashish Vikram, a former VP at Flipkart. The company is headquartered in Princeton, New Jersey and has a subsidiary in Bangalore, India. It has launched pilots in India, Australia, and the United States. VideoKen evolved out of the TutorSpace project at Xerox Research. The company purchased multiple patent applications and other IP from Xerox.\n\nVideoKen raised seed funding of $1 million in April 2017 from a consortium of angel investors which include LG Chandrasekhar, chairman of Sutures India and SRI Capital, led by Sashi Reddi, and some former and current executives of Flipkart. Among the Flipkart executives, Ravi Garikipati (CTO), Surojeet Chatterjee (former Senior Vice President (SVP) and Product Head), Hari Vasudevan (SVP) and Ashish Agarwal (SVP) have contributed to the funding. Ajay Lavakare, Co-President Stanford Angels and Entrepreneurs and other high net worth individuals from the US and Singapore have also participated in the round.\n\n\n"}
{"id": "8754562", "url": "https://en.wikipedia.org/wiki?curid=8754562", "title": "Wide Area GPS Enhancement", "text": "Wide Area GPS Enhancement\n\nWide Area GPS Enhancement (WAGE) is a method to increase the horizontal accuracy of the GPS encrypted P(Y) Code by adding additional range correction data to the satellite broadcast navigation message.\n\nPer a 1997 article, the navigation message for each satellite is updated once daily or as needed. This daily update of each satellite navigation message contains the range corrections for all the satellites in the constellation. Thus, more timely range correction information would be available for each satellite, resulting in increased horizontal accuracy. Potential improvements to the system include simplifying the upload procedure, uploading the data more often, and adding more monitor stations for better range correction.\n\nWAGE is available only to the Precise Positioning Service (PPS) or P(Y) Code receivers. It requires at least 12.5 minutes to obtain the most recent WAGE data. After that, the process of using the corrections data is automatic and transparent to the operator. Any time the receiver is on, it continually collects WAGE data (whether the WAGE mode is on or off). The receiver always uses the most recent WAGE data available to calculate position and it will not use the data that is over 6 hours old.\n\nA 1996 evaluation using a PLGR (a 5-channel L2 GPS receiver) found no clear advantage to using WAGE in its then-current configuration. Its overall average error of 9.1 meters was worse than when WAGE was not used. \n\nHowever, the specifications information for the Defense Advanced GPS Receiver, which has replaced the PLGR, lists its WAGE accuracy as better than 4.82 m, 95% Horizontal. PPS accuracy has improved beyond WAGE specification and accuracy improvement from WAGE is now negligible. Modern receivers & atomic clocks on a chip will also outperform WAGE. Some theorize that restrictions imposed by WAGE may limit precision for both C/A, P(Y), & WAGE users more than what it provides to WAGE users only.\n\nThe capability of WAGE has been superseded by Talon NAMATH. There is a push for WAGE users to upgrade to Talon NAMATH or move them to using P(Y) alone. This could lift WAGE restrictions & allow accuracy improvements for all users.\n\n"}
{"id": "40719434", "url": "https://en.wikipedia.org/wiki?curid=40719434", "title": "X32 Digital Mixing Console", "text": "X32 Digital Mixing Console\n\nThe X32 Digital Mixing Console is a digital mixing console conceived and designed by German manufacturer Behringer. The console features 40-input channels, 25-bus, 32 XLR microphone input and 16 XLR output busses. The console features 25 100mm motorised faders, a user assignable control panel, Ethernet connectivity and an iPad and iPhone control application.\n\n"}
{"id": "32194978", "url": "https://en.wikipedia.org/wiki?curid=32194978", "title": "Yangaroo", "text": "Yangaroo\n\nYANGAROO (TSXV: YOO) is a Canadian company specializing in providing technology solutions for the music, advertising, and awards industries. It was founded in 1999 by Cliff Hunt and David Staples. \n\nDigital Media Distribution System (DMDS) is a patented service of YANGAROO Inc.\n\nYANGAROO, powered by DMDS, is a secure distribution tool for audio and video media via the internet and is used by industry professionals. Systems like DMDS make it possible for broadcasters, TV advertisers, and post-production companies to move away from physical media like DigiBeta tapes to digital file based workflows. \n\nThe DMDS service supports:\n\nYANGAROO Music provides programmers, broadcasters, journalists and other industry professionals with the ability to preview and download pre-release media from music professionals including major record labels and independents. Organizations like MTV have selected DMDS to manage their digital video submission process and to review materials for broadcast.\n\nYANGAROO Advertising is used by post-production facilities and advertising agencies to expedite the delivery of advertising content to broadcasters, utilizing file based workflows.\n\nYANGAROO Awards is used by associations for submission and adjudication of Award Show entries. Groups like NARAS (The Grammy Awards\n\n) and CARAS (The Juno Awards) utilize the system to manage their Awards shows.\n\nIn December 2014, YANGAROO announced a partnership with IMD Fastrax, the music distribution arm of leading global delivery logistic company, IMD. The partnership will enable North American record labels and recording artists to deliver their music videos to the UK & Ireland via the IMD Fastrax platform.\n"}
