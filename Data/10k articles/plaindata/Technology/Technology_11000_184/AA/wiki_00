{"id": "23000019", "url": "https://en.wikipedia.org/wiki?curid=23000019", "title": "4,4'-Dinitro-3,3'-diazenofuroxan", "text": "4,4'-Dinitro-3,3'-diazenofuroxan\n\n4,4’-Dinitro-3,3’-diazenofuroxan (DDF) is a powerful experimental high explosive with performance comparable to that of other high-density high-explosives such as octanitrocubane. It is synthesised by oxidative coupling\nof 4-amino-3-(azidocarbonyl)furoxan followed by Curtius rearrangement and further oxidation.\n\n"}
{"id": "40701", "url": "https://en.wikipedia.org/wiki?curid=40701", "title": "Aerial insert", "text": "Aerial insert\n\nIn telecommunications an aerial insert is a segment of cabling that rises from ground to a point above ground, followed by an overhead run, e.g. on poles, followed by a drop back into the ground. An aerial insert is used in places where it is not possible or practical to place a cable underground. Aerial inserts might be encountered in crossing deep ditches, canals, rivers, or subway lines.\n\n"}
{"id": "1814485", "url": "https://en.wikipedia.org/wiki?curid=1814485", "title": "Antipasto", "text": "Antipasto\n\nAntipasto (plural antipasti) is the traditional first course of a formal Italian meal. Typical ingredients of a traditional antipasto include cured meats, olives, peperoncini, mushrooms, anchovies, artichoke hearts, various cheeses (such as provolone or mozzarella), pickled meats, and vegetables in oil or vinegar.\n\nThe contents of an antipasto vary greatly according to regional cuisine. It is quite possible to find different preparations of saltwater fish and traditional southern cured meats (like soppressata or 'nduja) in the south of Italy, whereas in northern Italy it will contain different kinds of cured meats and mushrooms and, especially near lakes, preparations of freshwater fish. The cheeses included also vary significantly between regions and backgrounds, and include hard and soft cheeses.\n\nMany compare antipasto to hors d'oeuvre, but antipasto is served at the table and signifies the official beginning of the Italian meal. It may also be referred to as a starter, or an appetizer.\n\n"}
{"id": "1872693", "url": "https://en.wikipedia.org/wiki?curid=1872693", "title": "Apollo Diamond", "text": "Apollo Diamond\n\nApollo Diamond Inc. was a company based in Boston, Massachusetts that was able to produce nearly flawless single crystal diamond wafers and crystals for potential use in the optoelectronics, nanotechnology, and consumer gem markets. The company used chemical vapor deposition (CVD) for the production of their gem-sized synthetic diamond crystals, and obtained several U.S. patents on the process. The company's techniques were able to produce colorless gems, in contrast to previous diamond-making techniques which usually produced colored diamonds.\n\nIn 2011, many assets of Apollo Diamond were acquired by Scio Diamond Technology Corporation, which said it would use the technology at its South Carolina facility.\n\n\n"}
{"id": "18411493", "url": "https://en.wikipedia.org/wiki?curid=18411493", "title": "Baku–Batumi pipeline", "text": "Baku–Batumi pipeline\n\nThe Baku–Batumi pipeline is the name given to several pipelines and pipeline projects to transport kerosene and crude oil from the Caspian region to the Georgian Batumi oil terminal at the Black Sea. When first constructed in 1906, it was the world's longest kerosene pipeline.\n\nTogether with oil developments in the Baku area, the need for construction of the oil pipeline from Baku to the Black Sea rose. The first pipeline proposal was submitted by Russian engineer I. Ilimov already in 1878. In 1880, Dmitri Mendeleev proposed the construction of Baku–Batum pipeline to ensure the transportation of Baku oil to the world market. In 1884, the chief engineer Vladimir Shukhov of Bari engineering company published a scientifically based draft and estimate of the Baku – Batum oil pipeline. Also the pipeline technical project was later designed by Vladimir Shukhov. In 1885, mining engineer I. Ilimov established the Caspian and Black Sea Oil Pipeline company. In December 1887, the Government of Russia granted to Ilimov the concession to establish the Society of the Caspian-Black Sea Oil Pipeline, a joint stock company. However, in 1891 the pipeline construction was postponed as premature, and the construction started only in 1896. At the first stage, the Batum-Mikhailovo (Khashuri) section was constructed, while the construction of Baku-Mikhailovo (Khashuri) section was finished only in 1906 [ref.11].\n\nThe first pipeline was kerosene pipeline with total length of and 16 pumping stations. The diameter of the pipeline was mainly , but some parts had diameter of and . The pipes were produced at plants in Mariupol, Sosnovtsa and Yekaterinoslav (now Dnipro). The pipeline was built along the railroad line and the telephone communication was arranged along the route. The initial pipeline capacity was 980,000 tons of kerosene per annum. Pumping stations were equipped with plunger pumps, driven by steam and diesel engines. The pipeline for its time was the longest pipeline in the world.\n\nAfter the Bolshevik Revolution, kerosene deliveries through the pipeline were relaunched in March 1921 and on 20 May 1921, the first delivery of kerosene arrived at Batum. After 1936 Batum was renamed to Batumi [ref.11].\n\nThe project of new pipeline was proposed in 1924. In 1925, the Soviet Union held negotiations with French companies to set up a joint venture to construct and operate the Baku-Batum crude oil pipeline. The intention was to use the pipeline for oil export to Europe, mainly to France. However, the negotiations failed as also failed negotiations with the United States companies. In 1927, the construction of the pipeline was awarded to Azneft, an Azerbaijani oil company. The project designer and construction manager was A.V. Bulgakov.\n\nThe construction started in May 1928 and the pipeline was opened on 30 April 1930. It supplied mainly Batum's refinery.\n\nThe crude oil pipeline had a diameter of and the length was . The pipeline had 13 pumping stations each equipped with three diesel pumps of 360 hp. The pipeline used over 60,000 German-manufactured pipes weighing a total of over 54,000 tons. Diesels for the pipeline were purchased from MAN AG, pumps from Crossley and generators from Theodor Bergmann.\nConstruction work was done on three sections simultaneously. The highest point was at the above of sea level. The first long section Mikhailovo (Khashuri)-Batum was completed on 13 February 1929, the second long section Mingechaur-Mikhailovo (Khashuri) was completed on 15 December 1929, and the third long section Baku-Mingechaur was completed on 13 February 1930. The pipeline cost 49 million rubles.\n\nThe operation of the oil pipeline showed that it was incapable of transporting oil in planned amount and the capacity was needed to increase by 750,000 tons. In August 1942, the pipeline was dismantled in connection with the threat of penetration of German troops in that direction and its pipes were used for the construction of the Astrakhan-Saratov pipeline. In 1990s, some part of the pipeline were used for the construction of the Baku-Supsa Pipeline.\n\nThere have been several proposals for the new Baku–Batumi pipeline. In 1994-1998, the Baku-Supsa Pipeline, which partly uses old Baku–Batum pipeline route, was constructed. On 2 March 1998, Chevron Corporation agreed to reconstruct existing Khashuri-Batumi pipeline and construct Dubandi (Baku)- Khashuri pipeline. However, in May 2001 Chevron canceled this project and started to ship its oil from Tengiz Field through the CPC pipeline.\n\nKazakhstan's national oil company KazMunayGas, owner of the Batumi Oil Terminal, has shown interest to build the new Baku–Batumi pipeline, which together with proposed Trans-Caspian and Batumi-Constanţa connections would allow to supply KazMunayGas oil refineries in Romania (Rompetrol) and planned refinery in Batumi.\n\n\nMir-Babayev M.F. Pipeline transportation in the Baku oil industry (dedicated to the 115th anniversary of commencing the construction of unique pipeline Baku-Batum) – “Azerbaijan Oil Industry”, 2012, #1, p. 63-69\n"}
{"id": "26140646", "url": "https://en.wikipedia.org/wiki?curid=26140646", "title": "Balloonomania", "text": "Balloonomania\n\nBalloonomania was a strong public interest or fad in hot air balloons that originated in France in the late 18th century and continued into the 19th century, during the advent of hot air balloon flights. The interest began with the first flights of the Montgolfier brothers in 1783, and quickly spread in France and across the channel in England.\n\nThe science of lighter-than-air gasses, and specifically the properties of oxygen, had been discovered as early as 1774 by Joseph Priestley, who noted its lightness and explosive qualities when heated. The chemistry of lighter-than-air and heated gasses was eventually put to the test by the Montgolfier brothers, two paper manufacturers in France, while experimenting with heated air caught in paper bags.\nBalloonomania saw its true origins, however, in the very first public balloon flight on June 5, 1783, with the launching of a large unmanned paper balloon in the countryside near Annonay. The balloon, which had been constructed by the Mongolfier brothers, was thirty feet tall, made of paper and appears to have been intended as an advertising gimmick for the Montgolfier’s paper manufacturing company. It was effective, as it drew an enormous crowd of onlookers.\nLater balloonists such as Jean-Pierre Blanchard and Vincent Lunardi exploited this wonder at the novelty of balloons to draw large crowds and gain personal fame, Lunardi going so far as to proclaim himself an “idol of the whole nation [of England]” in a letter to his guardian.\n\nEarly ballooning was met with mixed responses. Crowds of hundreds of thousands of enthusiastic onlookers would turn out for a balloon launch, even threatening to riot if the launch was delayed. Some, however, were not quite as impressed, as shown by the events of August 27, 1783 when professor Jacques Alexandre César Charles, who had been commissioned to build a rival balloon to the Mongolfier’s version using hydrogen, launched his balloon from the Champs de Mars before a large crowd including American scientist Benjamin Franklin. The balloon travelled for “forty-five minutes and fifteen miles to the village of Genoesse, where it was attacked by frightened peasants on landing.” \n\nRegardless of these negative reactions, which were not in the majority, ballooning quickly caught the imagination of the general populace, with a crowd of up to 400,000 clamoring to see Jacques Charles make a manned ascent in Paris on December 1, 1783. Both Blanchard and Lunardi became famous for their ballooning stunts, with Blanchard and his companion Dr. John Jeffries being the first to cross the English channel in a balloon on January 7, 1785.\n\nThe public reaction among intellectuals and academics was generally cooler, with some critics of balloonomania including the likes of Sir Joseph Banks and Samuel Johnson, who wrote in a 1783 letter to Hester Thrale, who had inquired about the nature of hot air balloons, “Happy are you, Madam, that have ease and leisure to want intelligence of air balloons. Their existence is, I believe, indubitable, but I know not that they can possibly be of any use.”\nSir Joseph Banks, a prominent natural scientist wrote that he was skeptical of the utility of balloons, though he recognized the revolutionary science behind it: “I see an inclination in the more respectable part of the Royal Society to guard against the Ballomania until some experiment like to prove beneficial either to society or to science is proposed.” However, both men and other scientists and academics would express some personal interest in ballooning, and suggest possible practical purposes, with Banks originally suggesting that perhaps balloons could be used as a way of counterbalancing the weight of a cart or coach, making them easier to move over the ground. Even Johnson recognized the potential for exploration, stating, \"How easily shall we trace the Nile through all its passages; pass over to distant regions and examine the face of nature, from one extremity of the Earth to the other.\" \nBlanchard's companion Dr. John Jeffries considered ballooning to be a major part of an exploration of the secrets of flight, the nature of the upper atmosphere, and the formation of weather, and took instruments such as a mercury barometer, a thermometer, a hydrometer and an electrometer to take different measurements of the upper atmosphere.\nThere were other positive scientific responses, as well. Upon receiving a letter from a friend chronicling a balloon flight, the astronomer William Herschel began to think of hot air balloons as possibly useful for observation, as they might carry telescopes into the upper air, where it was clearer. This idea would eventually evolve into sending telescopes into orbit, which became reality in 1990 with the launching of the Hubble Space Telescope.\n\nAt its peak, balloonomania triggered a revolution in souvenirs and collectibles, with balloons being featured on “plates, cups, clocks, ivory draughts pieces, snuffboxes, bracelets, tobacco pipes, hairclips, tiepins, even a porcelain bidet with a balloon design painted on the interior.\" These collectibles proved to be enormously popular among the French populace, starting in the winter of 1783. With the rise of public interest in ballooning, they soon became the subjects of mockery. \"Many sexually suggestive cartoons soon appeared: the inevitable balloon-breasted girls lifted off their feet, monstrous aeronauts inflated by gas enemas, or ‘inflammable’ women carrying men off into the clouds.”\n\nBalloonomania, merely as a novelty, served as the inspiration for various poets, such as Edward Nares, author of the Ballooniad, a street ballad about hot air ballooning, which mentioned the notion of flying to the moon.\n\nBalloonomania would exert a pull on the imaginations of some of the Romantic poets as well. Ballooning appealed to Romantic writer's ideas of the sublime, such as Samuel Taylor Coleridge, who wrote of balloons as being “an image of human longing and inspiration, both uplifting and terrifying” and William Wordsworth, who opened the poem “Peter Bell” with the image of a balloon boat:\nTheir point was not lost on the balloonists themselves, as Dr. Alexandre Charles found himself making the first solo voyage in a hot air balloon on December 1, 1783, an unplanned accident after Dr. Charles' companion stepped out of the balloon, which then relaunched itself with only Charles inside. He wrote, \"Never has a man felt so solitary, so sublime-and so utterly terrified.\" Dr. Charles never went up in a balloon again.\n\nPercy Shelley also wrote of balloons, saying, “It would seem a mere toy, a feather, in comparison with the splendid anticipations of the philosophical chemist. Yet it ought not to be altogether condemned, It promises prodigious faculties for locomotion, and will allow us to traverse vast tracts with ease and rapidity, and to explore unknown countries without difficulty. Why are we so ignorant of the interior of Africa?—Why do we not dispatch intrepid aeronauts to cross it in every direction and to survey the whole peninsula in a few weeks? The shadow of the first balloon… as it glided over that unhappy country, would virtually emancipate every slave, and would annihilate slavery forever.” \nShelley also wrote a sonnet entitled “To a balloon, laden with Knowledge” which reads:\n\nBalloonomania was not universal amongst the Romantic poets, however. In contrast to Coleridge, Wordsworth and Shelley, William Blake mocked and satirized the idea of manned flight in his unfinished prose work “An Island in the Moon”\n\nEven after the end of the Romantic period, Balloonomania continued to have an effect on later literary work, including on the early science fiction writer Jules Verne who wrote the book Five Weeks in a Balloon in 1863, about the ballooning adventures of two explorers and their manservant in Africa.\n\nThe military applications of hot air balloons were recognized early, with Joseph Montgolfier jokingly suggesting in 1782 that the French could fly an entire army suspended underneath hundreds of paper bags into Gibraltar to seize it from the British.\nMilitary leaders and political leaders soon began to see a more practical potential for balloons to be used in warfare; specifically in the role of reconnaissance. The first recorded use of a balloon in warfare was the deployment of a balloon called L'Entrepremant by the French at the battle of Fleurus in 1794, which resulted in a French victory over a coalition of British and Austrian forces.\nAfter that victory, Napoleon started an air balloon corps based in Meudon, and there were fears in England of an aerial invasion, though this never came to pass. Napoleon took his balloon corps to Egypt in 1798, but their equipment was destroyed by Horatio Nelson at the Battle of Aboukir, and Napoleon disbanded his balloon corps in 1799.\nBalloons would later be used in the American Civil War for reconnaissance and directing artillery barrages on foes that were out of view of the artillerymen on the ground.\n\n"}
{"id": "10959916", "url": "https://en.wikipedia.org/wiki?curid=10959916", "title": "Can wrench", "text": "Can wrench\n\nA can wrench is a wrench made to open a telephone distribution terminal also called a telco can or demarcation point box. One end of the can wrench is a 7/16 inch hex socket used for recessed fasteners on closures, and the other end is a 3/8 inch hex socket for use on binding posts. The wrench is also referred to as a 216C tool which was the Bell System specification version. They are often insulated against electric shock. The hex socket on each end is a thin wall thickness to allow the outside diameter to be placed into the tight recessed access often found on telco cans and demarc boxes. \n\n"}
{"id": "36913101", "url": "https://en.wikipedia.org/wiki?curid=36913101", "title": "Carbon-neutral fuel", "text": "Carbon-neutral fuel\n\nCarbon-neutral fuel is energy fuel or energy systems which have no net greenhouse gas emissions or carbon footprint. One class is synthetic fuel (including methane, gasoline, diesel fuel, jet fuel or ammonia) produced from renewable, sustainable or nuclear energy used to hydrogenate carbon dioxide directly captured from the air (DAC), recycled from power plant flue exhaust gas or derived from carbonic acid in seawater. Renewable energy sources include wind turbines, solar panels, and hydroelectric power stations.\nAnother type of renewable energy source is biofuel.\nSuch fuels are potentially carbon-neutral because they do not result in a net increase in atmospheric greenhouse gases.\n\nTo the extent that carbon-neutral fuels displace fossil fuels, or if they are produced from waste carbon or seawater carbonic acid, and their combustion is subject to carbon capture at the flue or exhaust pipe, they result in negative carbon dioxide emission and net carbon dioxide removal from the atmosphere, and thus constitute a form of greenhouse gas remediation.\n\nSuch power to gas carbon-neutral and carbon-negative fuels can be produced by the electrolysis of water to make hydrogen used in the Sabatier reaction to produce methane which may then be stored to be burned later in power plants as synthetic natural gas, transported by pipeline, truck, or tanker ship, or be used in gas to liquids processes such as the Fischer–Tropsch process to make traditional fuels for transportation or heating.\n\nCarbon-neutral fuels are used in Germany and Iceland for distributed storage of renewable energy, minimizing problems of wind and solar intermittency, and enabling transmission of wind, water, and solar power through existing natural gas pipelines. Such renewable fuels could alleviate the costs and dependency issues of imported fossil fuels without requiring either electrification of the vehicle fleet or conversion to hydrogen or other fuels, enabling continued compatible and affordable vehicles. A 250 kilowatt synthetic methane plant has been built in Germany and it is being scaled up to 10 megawatts.\n\nCarbon-neutral fuels are synthetic hydrocarbons. They can be produced in chemical reactions between carbon dioxide, which can be captured from power plants or the air, and hydrogen, which is created by the electrolysis of water using renewable energy. The fuel, often referred to as electrofuel, stores the energy that was used in the production of the hydrogen. Coal can also be used to produce the hydrogen, but that would not be a carbon-neutral source. Carbon dioxide can be captured and buried, making fossil fuels carbon-neutral, although not renewable. Carbon capture from exhaust gas can make carbon-neutral fuels carbon negative. Other hydrocarbons can be broken down to produce hydrogen and carbon dioxide which could then be stored while the hydrogen is used for energy or fuel, which would also be carbon-neutral.\n\nThe most energy-efficient fuel to produce is hydrogen gas, which can be used in hydrogen fuel cell vehicles, and which requires the fewest process steps to produce.\n\nMethanol can be made from a chemical reaction of a carbon-dioxide molecule with three hydrogen molecules to produce methanol and water. The stored energy can be recovered by burning the methanol in a combustion engine, releasing carbon dioxide, water, and heat. Methane can be produced in a similar reaction. Special precautions against methane leaks are important since methane is nearly 100 times as potent as CO, in terms of Global warming potential. More energy can be used to combine methanol or methane into larger hydrocarbon fuel molecules.\n\nResearchers have also suggested using methanol to produce dimethyl ether. This fuel could be used as a substitute for diesel fuel due to its ability to self ignite under high pressure and temperature. It is already being used in some areas for heating and energy generation. It is nontoxic, but must be stored under pressure. Larger hydrocarbons and ethanol can also be produced from carbon dioxide and hydrogen.\n\nAll synthetic hydrocarbons are generally produced at temperatures of 200–300 °C, and at pressures of 20 to 50 bar. Catalysts are usually used to improve the efficiency of the reaction and create the desired type of hydrocarbon fuel. Such reactions are exothermic and use about 3 mol of hydrogen per mole of carbon dioxide involved. They also produce large amounts of water as a byproduct.\n\nThe most economical source of carbon for recycling into fuel is flue-gas emissions from fossil-fuel combustion where it can be obtained for about USD $7.50 per ton. Automobile exhaust gas capture has also been seen as economical but would require extensive design changes or retrofitting. Since carbonic acid in seawater is in chemical equilibrium with atmospheric carbon dioxide, extraction of carbon from seawater has been studied. Researchers have estimated that carbon extraction from seawater would cost about $50 per ton. Carbon capture from ambient air is more costly, at between $94 and $232 per ton and is considered impractical for fuel synthesis or carbon sequestration. Direct air capture is less developed than other methods. Proposals for this method involve using a caustic chemical to react with carbon dioxide in the air to produce carbonates. These can then be broken down and hydrated to release pure CO gas and regenerate the caustic chemical. This process requires more energy than other methods because carbon dioxide is at much lower concentrations in the atmosphere than in other sources.\n\nResearchers have also suggested using biomass as a carbon source for fuel production. Adding hydrogen to the biomass would reduce its carbon to produce fuel. This method has the advantage of using plant matter to cheaply capture carbon dioxide. The plants also add some chemical energy to the fuel from biological molecules. This may be a more efficient use of biomass than conventional biofuel because it uses most of the carbon and chemical energy from the biomass instead of releasing as much energy and carbon. Its main disadvantage is, as with conventional ethanol production, it competes with food production.\n\nNighttime wind power is considered the most economical form of electrical power with which to synthesize fuel, because the load curve for electricity peaks sharply during the warmest hours of the day, but wind tends to blow slightly more at night than during the day. Therefore, the price of nighttime wind power is often much less expensive than any alternative. Off-peak wind power prices in high wind penetration areas of the U.S. averaged 1.64 cents per kilowatt-hour in 2009, but only 0.71 cents/kWh during the least expensive six hours of the day. Typically, wholesale electricity costs 2 to 5 cents/kWh during the day. Commercial fuel synthesis companies suggest they can produce gasoline for less than petroleum fuels when oil costs more than $55 per barrel.\n\nIn 2010, a team of process chemists led by Heather Willauer of the U.S. Navy, estimates that 100 megawatts of electricity can produce 41,000 gallons of jet fuel per day and shipboard production from nuclear power would cost about $6 per gallon. While that was about twice the petroleum fuel cost in 2010, it is expected to be much less than the market price in less than five years if recent trends continue. Moreover, since the delivery of fuel to a carrier battle group costs about $8 per gallon, shipboard production is already much less expensive.\n\nWillauer said seawater is the \"best option\" for a source of synthetic jet fuel. By April 2014, Willauer's team had not yet made fuel to the standard required by military jets, but they were able in September 2013 to use the fuel to fly a radio-controlled model airplane powered by a common two-stroke internal combustion engine. Because the process requires a large input of electrical energy, a plausible first step of implementation would be for American nuclear-powered aircraft carriers (the Nimitz-class and the Gerald R. Ford-class) to manufacture their own jet fuel. The U.S. Navy is expected to deploy the technology some time in the 2020s.\n\nA 250 kilowatt methane synthesis plant was constructed by the Center for Solar Energy and Hydrogen Research (ZSW) at Baden-Württemberg and the Fraunhofer Society in Germany and began operating in 2010. It is being upgraded to 10 megawatts, scheduled for completion in autumn, 2012.\n\nThe George Olah carbon dioxide recycling plant operated by Carbon Recycling International in Grindavík, Iceland has been producing 2 million liters of methanol transportation fuel per year from flue exhaust of the Svartsengi Power Station since 2011. It has the capacity to produce 5 million liters per year.\n\nAudi has constructed a carbon-neutral liquefied natural gas (LNG) plant in Werlte, Germany. The plant is intended to produce transportation fuel to offset LNG used in their A3 Sportback g-tron automobiles, and can keep 2,800 metric tons of CO out of the environment per year at its initial capacity.\n\nCommercial developments are taking place in Columbia, South Carolina, Camarillo, California, and Darlington, England. A demonstration project in Berkeley, California proposes synthesizing both fuels and food oils from recovered flue gases.\n\nCarbon-neutral fuels can lead to greenhouse gas remediation because carbon dioxide gas would be reused to produce fuel instead of being released into the atmosphere. Capturing the carbon dioxide in flue gas emissions from power plants would eliminate their greenhouse gas emissions, although burning the fuel in vehicles would release that carbon because there is no economical way to capture those emissions. This approach would reduce net carbon dioxide emission by about 50% if it were used on all fossil fuel power plants. Most coal and natural gas power plants have been predicted to be economically retrofittable with carbon dioxide scrubbers for carbon capture to recycle flue exhaust or for carbon sequestration. Such recycling is expected to not only cost less than the excess economic impacts of climate change if it were not done, but also to pay for itself as global fuel demand growth and peak oil shortages increase the price of petroleum and fungible natural gas.\n\nCapturing CO directly from the air or extracting carbonic acid from seawater would also reduce the amount of carbon dioxide in the environment, and create a closed cycle of carbon to eliminate new carbon dioxide emissions. Use of these methods would eliminate the need for fossil fuels entirely, assuming that enough renewable energy could be generated to produce the fuel. Using synthetic hydrocarbons to produce synthetic materials such as plastics could result in permanent sequestration of carbon from the atmosphere.\n\nSome authorities have recommended producing methanol instead of traditional transportation fuels. It is a liquid at normal temperatures and can be toxic if ingested. Methanol has a higher octane rating than gasoline but a lower energy density, and can be mixed with other fuels or used on its own. It may also be used in the production of more complex hydrocarbons and polymers. Direct methanol fuel cells have been developed by Caltech's Jet Propulsion Laboratory to convert methanol and oxygen into electricity. It is possible to convert methanol into gasoline, jet fuel or other hydrocarbons, but that requires additional energy and more complex production facilities. Methanol is slightly more corrosive than traditional fuels, requiring automobile modifications on the order of USD $100 each to use it.\n\nIn 2016, a method using carbon spikes, copper nanoparticles and nitrogen that converts carbon dioxide to ethanol was developed.\n\nInvestigation of carbon-neutral fuels has been ongoing for decades. A 1965 report suggested synthesizing methanol from carbon dioxide in air using nuclear power for a mobile fuel depot. Shipboard production of synthetic fuel using nuclear power was studied in 1977 and 1995. A 1984 report studied the recovery of carbon dioxide from fossil fuel plants. A 1995 report compared converting vehicle fleets for the use of carbon-neutral methanol with the further synthesis of gasoline.\n\n\n"}
{"id": "162843", "url": "https://en.wikipedia.org/wiki?curid=162843", "title": "Color television", "text": "Color television\n\nColor television is a television transmission technology that includes information on the color of the picture, so the video image can be displayed in color on the television set. It is an improvement on the earliest television technology, monochrome or black and white television, in which the image is displayed in shades of gray (grayscale). Television broadcasting stations and networks in most parts of the world upgraded from black and white to color transmission in the 1970s and 1980s. The invention of color television standards is an important part of the history of television, and it is described in the technology of television article.\n\nTransmission of color images using mechanical scanners had been conceived as early as the 1880s. A practical demonstration of mechanically-scanned color television was given by John Logie Baird in 1928, but the limitations of a mechanical system were apparent even then. Development of electronic scanning and display made an all-electronic system possible. Early monochrome transmission standards were developed prior to the Second World War, but civilian electronics developments were frozen during much of the war. In August 1950, Baird gave the world's first demonstration of a practical fully electronic color television display. In the United States, commercially competing color standards were developed, finally resulting in the NTSC standard for color that retained compatibility with the prior monochrome system. Although the NTSC color standard was proclaimed in 1953 and limited programming became available, it was not until the early 1970s that color television in North America outsold black and white or monochrome units. Color broadcasting in Europe was not standardized on the PAL and SECAM formats until the 1960s.\n\nAround 2006 countries began to switch from analog color television technology to digital television. This changeover is now complete in many developed countries, but analog television is still the standard in many developing countries.\n\nThe human eye's detection system, which is in the retina, consists primarily of two types of light detectors: rod cells that capture light, dark, and shapes/figures, and the cone cells that detect color. A typical retina contains 120 million rods and 4.5 million to 6 million cones, which are divided among three groups that are sensitive to red, green, and blue light. This means that the eye has far more resolution in brightness, or \"luminance\", than in color. However, post-processing of the optic nerve and other portions of the human visual system combine the information from the rods and cones to re-create what appears to be a high-resolution color image.\n\nThe eye has limited bandwidth to the rest of the visual system, estimated at just under 8 Mbit/s. This manifests itself in a number of ways, but the most important in terms of producing moving images is the way that a series of still images displayed in quick succession will appear to be continuous smooth motion. This illusion starts to work at about 16 frame/s, and common motion pictures use 24 frame/s. Television, using power from the electrical grid, tunes its rate in order to avoid interference with the alternating current being supplied – in North America, some Central and South American countries, Taiwan, Korea, part of Japan, the Philippines, and a few other countries, this is 60 video fields per second to match the 60 Hz power, while in most other countries it is 50 fields per second to match the 50  Hz power.\n\nIn its most basic form, a color broadcast can be created by broadcasting three monochrome images, one each in the three colors of red, green, and blue (RGB). When displayed together or in rapid succession, these images will blend together to produce a full-color image as seen by the viewer. One of the great technical challenges of introducing color broadcast television was the desire to conserve bandwidth, potentially three times that of the existing black-and-white standards, and not use an excessive amount of radio spectrum. In the United States, after considerable research, the National Television Systems Committee approved an all-electronic system developed by RCA which encoded the color information separately from the brightness information and greatly reduced the resolution of the color information in order to conserve bandwidth. The brightness image remained compatible with existing black-and-white television sets at slightly reduced resolution, while color televisions could decode the extra information in the signal and produce a limited-resolution color display. The higher resolution black-and-white and lower resolution color images combine in the eye to produce a seemingly high-resolution color image. The NTSC standard represented a major technical achievement.\n\nExperiments in television systems using radio broadcasts date to the 19th century, but it was not until the 20th century that advances in electronics and light detectors made development practical. A key problem was the need to convert a 2D image into a \"1D\" radio signal; some form of image scanning was needed to make this work. Early systems generally used a device known as a \"Nipkow disk\", which was a spinning disk with a series of holes punched in it that caused a spot to scan across and down the image. A single photodetector behind the disk captured the image brightness at any given spot, which was converted into a radio signal and broadcast. A similar disk was used at the receiver side, with a light source behind the disk instead of a detector.\n\nA number of such systems were being used experimentally in the 1920s. The best-known was John Logie Baird's, which was actually used for regular public broadcasting in Britain for several years. Indeed, Baird's system was demonstrated to members of the Royal Institution in London in 1926 in what is generally recognized as the first demonstration of a true, working television system. In spite of these early successes, all mechanical television systems shared a number of serious problems. Being mechanically driven, perfect synchronization of the sending and receiving discs was not easy to ensure, and irregularities could result in major image distortion. Another problem was that the image was scanned within a small, roughly rectangular area of the disk's surface, so that larger, higher-resolution displays required increasingly unwieldy disks and smaller holes that produced increasingly dim images. Rotating drums bearing small mirrors set at progressively greater angles proved more practical than Nipkow discs for high-resolution mechanical scanning, allowing images of 240 lines and more to be produced, but such delicate, high-precision optical components were not commercially practical for home receivers.\n\nIt was clear to a number of developers that a completely electronic scanning system would be superior, and that the scanning could be achieved in a vacuum tube via electrostatic or magnetic means. Converting this concept into a usable system took years of development and several independent advances. The two key advances were Philo Farnsworth's electronic scanning system, and Vladimir Zworykin's Iconoscope camera. The Iconoscope, based on Kálmán Tihanyi's early patents, superseded the Farnsworth-system. With these systems, the BBC began regularly scheduled black-and-white television broadcasts in 1936, but these were shut down again with the start of World War II in 1939. In this time thousands of television sets had been sold. The receivers developed for this program, notably those from Pye Ltd., played a key role in the development of radar.\n\nBy 22 March 1935, 180-line black-and-white television programs were being broadcast from the Paul Nipkow TV station in Berlin. In 1936, under the guidance of \"Minister of Public Enlightenment and Propaganda\" Joseph Goebbels, direct transmissions from fifteen mobile units at the Olympic Games in Berlin were transmitted to selected small television houses () in Berlin and Hamburg.\n\nIn 1941, the first NTSC meetings produced a single standard for US broadcasts. US television broadcasts began in earnest in the immediate post-war era, and by 1950 there were 6 million televisions in the United States.\n\nThe basic idea of using three monochrome images to produce a color image had been experimented with almost as soon as black-and-white televisions had first been built.\n\nAmong the earliest published proposals for television was one by Maurice Le Blanc in 1880 for a color system, including the first mentions in television literature of line and frame scanning, although he gave no practical details. Polish inventor Jan Szczepanik patented a color television system in 1897, using a selenium photoelectric cell at the transmitter and an electromagnet controlling an oscillating mirror and a moving prism at the receiver. But his system contained no means of analyzing the spectrum of colors at the transmitting end, and could not have worked as he described it. An Armenian inventor, Hovannes Adamian, also experimented with color television as early as 1907. The first color television project is claimed by him, and was patented in Germany on March 31, 1908, patent № 197183, then in Britain, on April 1, 1908, patent № 7219, in France (patent № 390326) and in Russia in 1910 (patent № 17912).\n\nScottish inventor John Logie Baird demonstrated the world's first color transmission on July 3, 1928, using scanning discs at the transmitting and receiving ends with three spirals of apertures, each spiral with filters of a different primary color; and three light sources, controlled by the signal, at the receiving end, with a commutator to alternate their illumination. The demonstration was of a young girl wearing different colored hats. Noele Gordon went on to become a successful TV actress, famous for the soap opera \"Crossroads\". Baird also made the world's first color broadcast on February 4, 1938, sending a mechanically scanned 120-line image from Baird's Crystal Palace studios to a projection screen at London's Dominion Theatre.\n\nMechanically scanned color television was also demonstrated by Bell Laboratories in June 1929 using three complete systems of photoelectric cells, amplifiers, glow-tubes, and color filters, with a series of mirrors to superimpose the red, green, and blue images into one full color image.\n\nAs was the case with black-and-white television, an electronic means of scanning would be superior to the mechanical systems like Baird's. The obvious solution on the broadcast end would be to use three conventional Iconoscopes with colored filters in front of them to produce an RGB signal. Using three separate tubes each looking at the same scene would produce slight differences in parallax between the frames, so in practice a single lens was used with a mirror or prism system to separate the colors for the separate tubes. Each tube captured a complete frame and the signal was converted into radio in a fashion essentially identical to the existing black-and-white systems.\n\nThe problem with this approach was there was no simple way to recombine them on the receiver end. If each image was sent at the same time on different frequencies, the images would have to be \"stacked\" somehow on the display, in real time. The simplest way to do this would be to reverse the system used in the camera: arrange three separate black-and-white displays behind colored filters and then optically combine their images using mirrors or prisms onto a suitable screen, like frosted glass. RCA built just such a system in order to present the first electronically scanned color television demonstration on February 5, 1940, privately shown to members of the US Federal Communications Commission at the RCA plant in Camden, New Jersey. This system, however, suffered from the twin problems of costing at least three times as much as a conventional black-and-white set, as well as having very dim pictures, the result of the fairly low illumination given off by tubes of the era. Projection systems of this sort would become common decades later, however, with improvements in technology.\n\nAnother solution would be to use a single screen, but break it up into a pattern of closely spaced colored phosphors instead of an even coating of white. Three receivers would be used, each sending its output to a separate electron gun, aimed at its colored phosphor. Although obvious, this solution was not practical. The electron guns used in monochrome televisions had limited resolution, and if one wanted to retain the resolution of existing monochrome displays, the guns would have to focus on individual dots three times smaller. This was beyond the state of the art at the time.\n\nInstead, a number of hybrid solutions were developed that combined a conventional monochrome display with a colored disk or mirror. In these systems the three colored images were sent one after each other, in either complete frames in the \"field-sequential color system\", or for each line in the \"line-sequential\" system. In both cases a colored filter was rotated in front of the display in sync with the broadcast. Since three separate images were being sent in sequence, if they used existing monochrome radio signaling standards they would have an effective refresh rate of only 20 fields, or 10 frames, a second, well into the region where flicker would become visible. In order to avoid this, these systems increased the frame rate considerably, making the signal incompatible with existing monochrome standards.\n\nThe first practical example of this sort of system was again pioneered by John Logie Baird. In 1940 he publicly demonstrated a color television combining a traditional black-and-white display with a rotating colored disk. This device was very \"deep\", but was later improved with a mirror folding the light path into an entirely practical device resembling a large conventional console. However, Baird was not happy with the design, and as early as 1944 had commented to a British government committee that a fully electronic device would be better.\n\nIn 1939, Hungarian engineer Peter Carl Goldmark introduced an electro-mechanical system while at CBS, which contained an Iconoscope sensor. The CBS field-sequential color system was partly mechanical, with a disc made of red, blue, and green filters spinning inside the television camera at 1,200 rpm, and a similar disc spinning in synchronization in front of the cathode ray tube inside the receiver set. The system was first demonstrated to the Federal Communications Commission (FCC) on August 29, 1940, and shown to the press on September 4.\n\nCBS began experimental color field tests using film as early as August 28, 1940, and live cameras by November 12. NBC (owned by RCA) made its first field test of color television on February 20, 1941. CBS began daily color field tests on June 1, 1941. These color systems were not compatible with existing black-and-white television sets, and as no color television sets were available to the public at this time, viewing of the color field tests was restricted to RCA and CBS engineers and the invited press. The War Production Board halted the manufacture of television and radio equipment for civilian use from April 22, 1942 to August 20, 1945, limiting any opportunity to introduce color television to the general public.\n\nAs early as 1940, Baird had started work on a fully electronic system he called the \"Telechrome\". Early Telechrome devices used two electron guns aimed at either side of a phosphor plate. The phosphor was patterned so the electrons from the guns only fell on one side of the patterning or the other. Using cyan and magenta phosphors, a reasonable limited-color image could be obtained. He also demonstrated the same system using monochrome signals to produce a 3D image (called \"stereoscopic\" at the time). Baird's demonstration on August 16, 1944 was the first example of a practical color television system. Work on the Telechrome continued and plans were made to introduce a three-gun version for full color. However, Baird's untimely death in 1946 ended development of the Telechrome system.\n\nSimilar concepts were common through the 1940s and 50s, differing primarily in the way they re-combined the colors generated by the three guns. The Geer tube was similar to Baird's concept, but used small pyramids with the phosphors deposited on their outside faces, instead of Baird's 3D patterning on a flat surface. The Penetron used three layers of phosphor on top of each other and increased the power of the beam to reach the upper layers when drawing those colors. The Chromatron used a set of focusing wires to select the colored phosphors arranged in vertical stripes on the tube.\n\nIn the immediate post-war era the Federal Communications Commission (FCC) was inundated with requests to set up new television stations. Worrying about congestion of the limited number of channels available, the FCC put a moratorium on all new licenses in 1948 while considering the problem. A solution was immediately forthcoming; rapid development of radio receiver electronics during the war had opened a wide band of higher frequencies to practical use, and the FCC set aside a large section of these new UHF bands for television broadcast. At the time, black and white television broadcasting was still in its infancy in the U.S., and the FCC started to look at ways of using this newly available bandwidth for color broadcasts. Since no existing television would be able to tune in these stations, they were free to pick an incompatible system and allow the older VHF channels to die off over time.\n\nThe FCC called for technical demonstrations of color systems in 1948, and the Joint Technical Advisory Committee (JTAC) was formed to study them. CBS displayed improved versions of its original design, now using a single 6 MHz channel (like the existing black-and-white signals) at 144 fields per second and 405 lines of resolution. Color Television Inc. demonstrated its line-sequential system, while Philco demonstrated a dot-sequential system based on its beam-index tube-based \"Apple\" tube technology. Of the entrants, the CBS system was by far the best-developed, and won head-to-head testing every time.\n\nWhile the meetings were taking place it was widely known within the industry that RCA was working on a dot-sequential system that was compatible with existing black-and-white broadcasts, but RCA declined to demonstrate it during the first series of meetings. Just before the JTAC presented its findings, on August 25, 1949, RCA broke its silence and introduced its system as well. The JTAC still recommended the CBS system, and after the resolution of an ensuing RCA lawsuit, color broadcasts using the CBS system started on June 25, 1951. By this point the market had changed dramatically; when color was first being considered in 1948 there were fewer than a million television sets in the U.S., but by 1951 there were well over 10 million. The idea that the VHF band could be allowed to \"die\" was no longer practical.\n\nDuring its campaign for FCC approval, CBS gave the first demonstrations of color television to the general public, showing an hour of color programs daily Mondays through Saturdays, beginning January 12, 1950, and running for the remainder of the month, over WOIC in Washington, D.C., where the programs could be viewed on eight 16-inch color receivers in a public building. Due to high public demand, the broadcasts were resumed February 13–21, with several evening programs added. CBS initiated a limited schedule of color broadcasts from its New York station WCBS-TV Mondays to Saturdays beginning November 14, 1950, making ten color receivers available for the viewing public. All were broadcast using the single color camera that CBS owned. The New York broadcasts were extended by coaxial cable to Philadelphia's WCAU-TV beginning December 13, and to Chicago on January 10, making them the first network color broadcasts.\n\nAfter a series of hearings beginning in September 1949, the FCC found the RCA and CTI systems fraught with technical problems, inaccurate color reproduction, and expensive equipment, and so formally approved the CBS system as the U.S. color broadcasting standard on October 11, 1950. An unsuccessful lawsuit by RCA delayed the first commercial network broadcast in color until June 25, 1951, when a musical variety special titled simply \"Premiere\" was shown over a network of five East Coast CBS affiliates. Viewing was again restricted: the program could not be seen on black-and-white sets, and \"Variety\" estimated that only thirty prototype color receivers were available in the New York area. Regular color broadcasts began that same week with the daytime series \"The World Is Yours\" and \"Modern Homemakers\".\n\nWhile the CBS color broadcasting schedule gradually expanded to twelve hours per week (but never into prime time), and the color network expanded to eleven affiliates as far west as Chicago, its commercial success was doomed by the lack of color receivers necessary to watch the programs, the refusal of television manufacturers to create adapter mechanisms for their existing black-and-white sets, and the unwillingness of advertisers to sponsor broadcasts seen by almost no one. CBS had bought a television manufacturer in April, and in September 1951, production began on the only CBS-Columbia color television model, with the first color sets reaching retail stores on September 28. But it was too little, too late. Only 200 sets had been shipped, and only 100 sold, when CBS discontinued its color television system on October 20, 1951, ostensibly by request of the National Production Authority for the duration of the Korean War, and bought back all the CBS color sets it could to prevent lawsuits by disappointed customers. RCA chairman David Sarnoff later charged that the NPA's order had come \"out of a situation artificially created by one company to solve its own perplexing problems\" because CBS had been unsuccessful in its color venture.\n\nWhile the FCC was holding its JTAC meetings, development was taking place on a number of systems allowing true simultaneous color broadcasts, \"dot-sequential color systems\". Unlike the hybrid systems, dot-sequential televisions used a signal very similar to existing black-and-white broadcasts, with the intensity of every dot on the screen being sent in succession.\n\nIn 1938 Georges Valensi demonstrated an encoding scheme that would allow color broadcasts to be encoded so they could be picked up on existing black-and-white sets as well. In his system the output of the three camera tubes were re-combined to produce a single \"luminance\" value that was very similar to a monochrome signal and could be broadcast on the existing VHF frequencies. The color information was encoded in a separate \"chrominance\" signal, consisting of two separate signals, the original blue signal minus the luminance (B'–Y'), and red-luma (R'–Y'). These signals could then be broadcast separately on a different frequency; a monochrome set would tune in only the luminance signal on the VHF band, while color televisions would tune in both the luminance and chrominance on two different frequencies, and apply the reverse transforms to retrieve the original RGB signal. The downside to this approach is that it required a major boost in bandwidth use, something the FCC was interested in avoiding.\n\nRCA used Valensi's concept as the basis of all of its developments, believing it to be the only proper solution to the broadcast problem. However, RCA's early sets using mirrors and other projection systems all suffered from image and color quality problems, and were easily bested by CBS's hybrid system. But solutions to these problems were in the pipeline, and RCA in particular was investing massive sums (later estimated at $100 million) to develop a usable dot-sequential tube. RCA was beaten to the punch by the Geer tube, which used three B&W tubes aimed at different faces of colored pyramids to produce a color image. All-electronic systems included the Chromatron, Penetron and beam-index tube that were being developed by various companies. While investigating all of these, RCA's teams quickly started focusing on the shadow mask system.\n\nIn July 1938 the shadow mask color television was patented by Werner Flechsig (1900–1981) in Germany, and was demonstrated at the International radio exhibition Berlin in 1939. Most CRT color televisions used today are based on this technology. His solution to the problem of focusing the electron guns on the tiny colored dots was one of brute-force; a metal sheet with holes punched in it allowed the beams to reach the screen only when they were properly aligned over the dots. Three separate guns were aimed at the holes from slightly different angles, and when their beams passed through the holes the angles caused them to separate again and hit the individual spots a short distance away on the back of the screen. The downside to this approach was that the mask cut off the vast majority of the beam energy, allowing it to hit the screen only 15% of the time, requiring a massive increase in beam power to produce acceptable image brightness.\n\nIn spite of these problems in both the broadcast and display systems, RCA pressed ahead with development and was ready for a second assault on the standards by 1950.\n\nThe possibility of a compatible color broadcast system was so compelling that the NTSC decided to re-form, and held a second series of meetings starting in January 1950. Having only recently selected the CBS system, the FCC heavily opposed the NTSC's efforts. One of the FCC Commissioners, R. F. Jones, went so far as to assert that the engineers testifying in favor of a compatible system were \"in a conspiracy against the public interest\".\n\nUnlike the FCC approach where a standard was simply selected from the existing candidates, the NTSC would produce a board that was considerably more pro-active in development.\n\nStarting before CBS color even got on the air, the U.S. television industry, represented by the National Television System Committee, worked in 1950–1953 to develop a color system that was compatible with existing black-and-white sets and would pass FCC quality standards, with RCA developing the hardware elements. (\"Compatible color,\" a phrase from advertisements for early sets, appears in the song \"America\" of \"West Side Story\", 1957.) RCA first made publicly announced field tests of the dot sequential color system over its New York station WNBT in July 1951. When CBS testified before Congress in March 1953 that it had no further plans for its own color system, the National Production Authority dropped its ban on the manufacture of color television receivers, and the path was open for the NTSC to submit its petition for FCC approval in July 1953, which was granted on December 17. The first publicly announced network demonstration of a program using the NTSC \"compatible color\" system was an episode of NBC's \"Kukla, Fran and Ollie\" on August 30, 1953, although it was viewable in color only at the network's headquarters. The first network broadcast to go out over the air in NTSC color was a performance of the opera \"Carmen\" on October 31, 1953.\n\nColour broadcasts from the United States were available to Canadian population centers near the border since the mid-1950s. At the time that NTSC colour broadcasting was officially introduced into Canada in 1966, less than one percent of Canadian households had a colour television set. Colour television in Canada was launched on the Canadian Broadcasting Corporation's (CBC) English language TV service on September 1, 1966. Private television broadcaster CTV also started colour broadcasts in early September 1966.\nThe CBC's French-language TV service, Radio-Canada, was broadcasting colour programming for 15 hours a week in 1968. Full-time colour transmissions started in 1974 on the CBC, with other private sector broadcasters in the country doing so by the end of the 1970s.\n\nThe following provinces and areas of Canada introduced colour television by the years as followed:\n\nCuba in 1958 became the second country in the world to introduce color television broadcasting, with Havana's Channel 12 using standards established by the NTSC Committee of United States Federal Communications Commission in 1940, and American technology patented by the American electronics company RCA, or Radio Corporation of America. But the color transmissions ended when broadcasting stations were seized in the Cuban Revolution in 1959, and did not return until 1975, using equipment acquired from Japan's NEC Corporation, and SECAM equipment from the Soviet Union, adapted for the American NTSC standard.\n\nGuillermo González Camarena independently invented and developed a field-sequential tricolor disk system in México in the late 1930s, for which he requested a patent in México on August 19 of 1940 and in the USA in 1941. González Camarena produced his color television system in his laboratory Gon-Cam for the Mexican market and exported it to the Columbia College of Chicago, who regarded it as the best system in the world. Goldmark actually applied patent for the same field-sequential tricolor system in USA on September 7 of 1940; while González Camarena made his application in México nineteen days before, on August 19 of 1940.\n\nOn August 31, 1946 González Camarena sent his first color transmission from his lab in the offices of The Mexican League of Radio Experiments at Lucerna St. No. 1, in Mexico City. The video signal was transmitted at a frequency of 115 MHz. and the audio in the 40 metre band. He obtained authorization to make the first publicly announced color broadcast in Mexico, on February 8, 1963, of the program \"Paraíso Infantil\" on Mexico City's XHGC-TV, using the NTSC system which had by now been adopted as the standard for color programming.\n\nGonzález Camarena also invented the \"Simplified Mexican Color TV system\" as a much simpler and cheaper alternative to the NTSC system. Due to its simplicity, NASA used a modified version of the \"Simplified Mexican Color TV system\" in his Voyager mission of 1979, to take pictures and video of Jupiter.\n\nAlthough all-electronic color was introduced in the U.S. in 1953, high prices and the scarcity of color programming greatly slowed its acceptance in the marketplace. The first national color broadcast (the 1954 Tournament of Roses Parade) occurred on January 1, 1954, but over the next dozen years most network broadcasts, and nearly all local programming, continued to be in black-and-white. In 1956 NBC's \"The Perry Como Show\" became the first live network television series to present a majority of episodes in color. CBS's \"The Big Record\", starring pop vocalist Patti Page, was the first television show broadcast in color for the entire 1957-1958 season; its production costs were greater than most movies were at the time not only because of all the stars featured on the hour-long extravaganza but the extremely high-intensity lighting and electronics required for the new RCA TK-41 cameras. It was not until the mid-1960s that color sets started selling in large numbers, due in part to the color transition of 1965 in which it was announced that over half of all network prime-time programming would be broadcast in color that autumn. The first all-color prime-time season came just one year later.\n\nNBC made the first coast-to-coast color broadcast when it telecast the Tournament of Roses Parade on January 1, 1954, with public demonstrations given across the United States on prototype color receivers by manufacturers RCA, General Electric, Philco, Raytheon, Hallicrafters, Hoffman, Pacific Mercury, and others. A color model from Westinghouse H840CK15 ($1,295, or ) became available in the New York area on February 28, 1954 and is generally agreed to be the first production receiver using NTSC color offered to the public; a less expensive color model from RCA (CT-100) reached dealers in April 1954. Television's first prime time network color series was \"The Marriage\", a situation comedy broadcast live by NBC in the summer of 1954. NBC's anthology series \"Ford Theatre\" became the first network color filmed series that October.\n\nEarly color telecasts could be preserved only on the black-and-white kinescope process introduced in 1947. It was not until September 1956 that NBC began using color film to time-delay and preserve some of its live color telecasts. Ampex introduced a color videotape recorder in 1958, which NBC used to tape \"An Evening With Fred Astaire,\" the oldest surviving network color videotape. This system was also used to unveil a demonstration of color television for the press. On May 22, 1958, President Dwight D. Eisenhower visited the WRC-TV NBC studios in Washington, D.C. and gave a speech touting the new technology's merits. His speech was recorded in color, and a copy of this videotape was given to the Library of Congress for posterity.\n\nSeveral syndicated shows had episodes filmed in color during the 1950s, including \"The Cisco Kid\", \"The Lone Ranger\", \"My Friend Flicka\", and \"Adventures of Superman\". The first two were carried by some stations equipped for color telecasts well before NBC began its regular weekly color dramas in 1959, beginning with the Western series \"Bonanza\".\n\nNBC was at the forefront of color programming because its parent company RCA manufactured the most successful line of color sets in the 1950s, and by 1959 RCA was the only remaining major manufacturer of color sets. CBS and ABC, which were not affiliated with set manufacturers and were not eager to promote their competitor's product, dragged their feet into color. CBS broadcast color specials and sometimes aired its big weekly variety shows in color, but it offered no regularly scheduled color programming until the fall of 1965. At least one CBS show, \"The Lucy Show\", was filmed in color beginning in 1963 but continued to be telecast in black and white through the end of the 1964–65 season. ABC delayed its first color programs until 1962, but these were initially only broadcasts of the cartoon shows \"The Flintstones\", \"The Jetsons\" and \"Beany and Cecil\". The DuMont network, although it did have a television-manufacturing parent company, was in financial decline by 1954 and was dissolved two years later.\n\nThe relatively small amount of network color programming, combined with the high cost of color television sets, meant that as late as 1964 only 3.1 percent of television households in the U.S. had a color set. But by the mid-1960s, the subject of color programming turned into a ratings war. A 1965 ARB study that proposed an emerging trend in color television set sales convinced NBC that a full shift to color would gain a ratings advantage over its two competitors. As a result, NBC provided the catalyst for rapid color expansion by announcing that its prime time schedule for fall 1965 would be almost entirely in color. ABC and CBS followed suit and over half of their combined prime-time programming also was in color that season, but they were still reluctant to telecast all their programming in color due to production costs. All three broadcast networks were airing full color prime time schedules by the 1966–67 broadcast season, and ABC aired its last new black-and-white daytime programming in December 1967. Public broadcasting networks like NET, however, did not use color for a majority of their programming until 1968. The number of color television sets sold in the U.S. did not exceed black-and-white sales until 1972, which was also the first year that more than fifty percent of television households in the U.S. had a color set. This was also the year that \"in color\" notices before color television programs ended, due to the rise in color television set sales, and color programming having become the norm.\n\nIn a display of foresight, Disney had filmed many of its earlier shows in color so they were able to be repeated on NBC, and since most of Disney's feature-length films were also made in color, they could now also be telecast in that format. To emphasize the new feature, the series was re-dubbed \"Walt Disney's Wonderful World of Color\", which premiered in September 1961, and retained that moniker until 1969.\n\nBy the mid-1970s the only stations broadcasting in black-and-white were a few high-numbered UHF stations in small markets, and a handful of low-power repeater stations in even smaller markets such as vacation spots. By 1979, even the last of these had converted to color and by the early 1980s, B&W sets had been pushed into niche markets, notably low-power uses, small portable sets, or use as video monitor screens in lower-cost consumer equipment. By the late 1980s, even those areas switched to color sets.\n\nColor broadcasting in Hawaii started in September 1965, and in Alaska a year later. One of the last television stations in North America to convert to color, WQEX (now WINP-TV) in Pittsburgh, started broadcasting in color on October 16, 1986 after its black-and-white transmitter, which dated from the 1950s, broke down in February 1985 and the parts required to fix it were no longer available. The then-owner of WQEX, PBS member station WQED, diverted some of its pledge money into getting a color transmitter for WQEX.\n\nEarly color sets were either floor-standing console models or tabletop versions nearly as bulky and heavy, so in practice, they remained firmly anchored in one place. The introduction of GE's relatively compact and lightweight Porta-Color set in the spring of 1966 made watching color television a more flexible and convenient proposition. In 1972, sales of color sets finally surpassed sales of black-and-white sets. Also in 1972, the last holdout among daytime network programs converted to color, resulting in the first completely all-color network season.\n\nThe first regular colour broadcasts in Europe were by the UK's BBC2 beginning on July 1, 1967 (PAL). West Germany's first colour broadcast occurred in August (PAL), followed by the Netherlands in September (PAL), and by France in October (SECAM). Denmark, Norway, Sweden, Finland, Austria, East Germany, Czechoslovakia, and Hungary all started regular colour broadcasts around 1969/1970. Ireland's national TV station RTÉ began using colour in 1968 for recorded programmes; the first outside broadcast made in colour for RTÉ Television was when Ireland hosted the Eurovision Song Contest in Dublin in 1971. The PAL system spread through most of Western Europe.\n\nMore European countries introduced colour television using the PAL system in the 1970s and early 1980s; examples include Belgium (1971), Yugoslavia/Serbia (1971), Spain (1972, but not fully implemented until 1978), Iceland (1973), Portugal (1975, but not fully implemented until 1980), Albania (1981), Turkey (1981) and Romania (1983, but not fully implemented until 1990). In Italy there were debates to adopt a national colour television system, the \"ISA\", developed by Indesit, but that idea was scrapped. As a result, Italy was one of the last European countries to officially adopt the PAL system in 1977.\n\nFrance, Luxembourg, and most of the Eastern Bloc along with their overseas territories opted for SECAM. SECAM was a popular choice in countries with much hilly terrain, and technologically backward countries with a very large installed base of monochrome equipment, since the greater ruggedness of the SECAM signal could cope much better with poorly maintained equipment. However, for many countries the decision was more down to politics than technical merit.\n\nA drawback of SECAM for production is that, unlike PAL or NTSC, certain post-production operations of encoded SECAM signals are not really possible without a significant drop in quality. As an example, a simple fade to black is trivial in NTSC and PAL: one merely reduces the signal level until it is zero. However, in SECAM the colour difference signals, which are frequency modulated, need first to be decoded to e.g. RGB, then the fade-to-black is applied, and finally the resulting signal is re-encoded into SECAM. Because of this, much SECAM video editing was actually done using PAL equipment, then the resultant signal was converted to SECAM. Another drawback of SECAM is that comb filtering, allowing better colour separation, is not possible in TV receivers. This was not, however, much of a drawback in the early days of SECAM as such filters were not readily available in high-end TV sets before the 1990s.\n\nThe first regular colour broadcasts in SECAM were started on October 1, 1967, on France's Second Channel (ORTF 2e chaîne).\nIn France and the UK colour broadcasts were made on UHF frequencies, the VHF band being used for black and white, 405 lines in UK or 819 lines in France, until the beginning of the 1980s. Countries elsewhere that were already broadcasting 625-line monochrome on VHF and UHF, simply transmitted colour programs on the same channels.\n\nSome British television programs, particularly those made by or for ITC Entertainment, were shot on colour film before the introduction of colour television to the UK, for the purpose of sales to U.S. networks. The first British show to be made in colour was the drama series \"The Adventures of Sir Lancelot\" (1956–57), which was initially made in black and white but later shot in colour for sale to the NBC network in the United States. Other British colour television programs include \"Stingray\" (1964–1965), which was the first British TV show to be filmed entirely in colour, \"Thunderbirds\" (1965–1966) and \"Captain Scarlet and the Mysterons\" (1967–1968). However, most UK series predominantly made using videotape, such as \"Doctor Who\" (1963–89; 2005–present) did not begin colour production until later, with the first colour \"Doctor Who\" episodes not airing until 1970.\n\nIn Japan, NHK and NTV introduced color television, using a variation of the NTSC system (called NTSC-J) on September 10, 1960, making it the first country in Asia to introduce color television. The Philippines (1966) and Taiwan (1969) also adopted the NTSC system.\n\nOther countries in the region instead used the PAL system, starting with Australia (1967, originally scheduled for 1972, but not fully implemented until 1975), and then Thailand (1969; this country converted from a 525-line system to 625 lines), Hong Kong (1970), the People's Republic of China (1971), New Zealand (1973), North Korea (1974), Singapore (1974), Pakistan (1976, but not fully implemented until 1982), Kazakhstan (1978), Vietnam (1978), Malaysia (1978, but not fully implemented until 1980), Indonesia (1979), India (1979, but not fully implemented until 1982), and Bangladesh (1980). South Korea did not introduce color television (using NTSC) until 1980 (full-time color transmissions began in 1981), although it was already manufacturing color television sets for export. Cambodia was the last country in Asia to introduce color television, officially introduced in 1981 using the PAL system, with full-time color transmissions since 1985.\n\nNearly all of the countries in the Middle East use PAL. The first country in the Middle East to introduce color television was Iraq in 1967. Saudi Arabia, the United Arab Emirates, Kuwait, Bahrain, and Qatar followed in the mid-1970s, but Israel, Lebanon, and Cyprus continued to broadcast in black and white until the early 1980s. Israeli television even erased the color signals using a device called the \"mekhikon\".\n\nThe first color television service in Africa was introduced on the Tanzanian island of Zanzibar, in 1973, using PAL. In 1973 also, MBC of Mauritius broadcast the OCAMM Conference, in color, using SECAM. At the time, South Africa did not have a television service at all, owing to opposition from the apartheid regime, but in 1976, one was finally launched. Nigeria adopted PAL for color transmissions in 1974 in the then Benue Plateau state in the north central region of the country, but countries such as Ghana and Zimbabwe continued with black and white until 1984. The Sierra Leone Broadcasting Service (SLBS) started television broadcasting in 1963 as a cooperation between the SLBS and commercial interests; coverage was extended to all districts in 1978 when the service was also upgraded to color.\n\nIn contrast to most other countries in the Americas, which had adopted NTSC, Brazil began broadcasting in color using PAL-M. Its first color transmission was on February 19, 1972. However Ecuador was the first South American country to use NTSC color. Its first color transmission was on November 5, 1974. In 1978, Argentina started broadcasting in color using PAL-N in connection with the country's hosting of the FIFA World Cup. Some countries in South America, including Bolivia, Paraguay, Peru, and Uruguay, continued to broadcast in black and white until the early 1980s.\n\nCor Dillen, director and later CEO of the South American branch of Philips, was responsible for bringing color television to South America.\n\nThere are three main analog broadcast television systems in use around the world, PAL (Phase Alternating Line), NTSC (National Television System Committee), and SECAM (Séquentiel Couleur à Mémoire—Sequential Color with Memory).\n\nThe system used in The Americas and part of the Far East is NTSC. Most of Asia, Western Europe, Australia, Africa, and Eastern South America use PAL (though Brazil uses a hybrid PAL-M system). Eastern Europe and France uses SECAM. Generally, a device (such as a television) can only read or display video encoded to a standard which the device is designed to support; otherwise, the source must be converted (such as when European programs are broadcast in North America or vice versa).\n\nThis table illustrates the differences:\nDigital television broadcasting standards, such as ATSC, DVB-T, DVB-T2, and ISDB, have superseded these analog transmission standards in many countries.\n\n\n"}
{"id": "24023788", "url": "https://en.wikipedia.org/wiki?curid=24023788", "title": "Crystatech", "text": "Crystatech\n\nCrystaTech Inc. is a supplier of process technology to the energy industry. CrystaTech commercializes the patented Crystasulf process. CrystaSulf is the first commercially available product to provide low cost hydrogen sulfide (HS) removal from gas streams.\n\nThe company was founded in 1999 and is financially backed by the Gas Technology Institute and major energy companies through sponsored clean energy technology development. The corporate office is located in Austin, Texas.\n\nCrystaTech is a member of the Gas Processors Suppliers Association.\n\nRegional offices are in Alberta, Canada and Houston, Texas. All early stage R&D takes place at the Gas Technology Institute in Des Plaines, Illinois. Representative customers include Total, Petrobank Energy and Resources Ltd., Queensland Energy Resources, U.S. Department of Energy, Luminant, and American Electric Power.\n\nKey People\n\n\n\n"}
{"id": "39556989", "url": "https://en.wikipedia.org/wiki?curid=39556989", "title": "Drawing-in frame", "text": "Drawing-in frame\n\nA Drawing-in frame was a piece of equipment used in the cotton industry. It was the drawers-in job to thread each of ends on a new beam through the correct healds, and then though the reed. On a Lancashire loom weaving grey cloth this was a simple but time consuming task, but for a complex pattern on a Jacquard tapestry loom, great care was needed. To do this, the new beam was mounted on the back of a drawing-in frame, the healds were held next in a vertical position, and in front the reed would be clamped. This job was done by a \"reacher-in\" and a \"loomer\". The reacher-in, who would be young and usually a boy, passed each end in order to the loomer who threaded it through the healds and the reed.\nA 'drawer-in' was sometimes referred to as a beamer. The drawers-in sat between the two beams on low three legged stool, head and shoulders at beam height and so surrounded was humorously given the nickname “Yutick’s nest.”\n\nFootnotes\n\nCitations\n\nBibliography\n"}
{"id": "33216231", "url": "https://en.wikipedia.org/wiki?curid=33216231", "title": "Drift meter", "text": "Drift meter\n\nA drift meter, also drift indicator and drift sight, is an optical device used to improve dead reckoning for aircraft navigation. It consists of a small telescope extended vertically through the bottom of the aircraft with the eyepiece inside the fuselage at the navigator's station. A reticle, normally consisting of spaced parallel lines, is rotated until objects on the ground are seen to be moving parallel to the lines. The angle of the reticle then indicates the aircraft's drift angle due to winds aloft, and can be used to calculate the ground speed.\n\n"}
{"id": "985187", "url": "https://en.wikipedia.org/wiki?curid=985187", "title": "Driving simulator", "text": "Driving simulator\n\nDriving simulators are used for entertainment as well as in training of driver's education courses taught in educational institutions and private businesses. They are also used for research purposes in the area of human factors and medical research, to monitor driver behavior, performance, and attention and in the car industry to design and evaluate new vehicles or new advanced driver assistance systems.\n\nDriving simulators are being increasingly used for training drivers all over the world. Research has shown that driving simulators are proven to be excellent practical and effective educational tools to impart safe driving training techniques for all drivers. There are various types of driving simulators that are being used like train simulators, bus simulator, car simulator, truck simulator etc. \n\nManufacturers such Hindustan Simulators. Simworx in Australia, Virage Simulation in Canada, Vertex Research Centre, India, Tecknotrove, Faros Simulation System, XPI Simulation are active in developing Driver Training Simulators that are relevant to Road Safety as well as immersive Student Driver Training. Hazard Perception and Disability Simulators for training amputees or the disabled in the use of hand controls whilst still abiding with local road rules are one of many features found in their simulators.\n\n\n\n\n\n\nAdvances in processing power have led to more realistic simulators in recent years, beginning with Papyrus Design Group's groundbreaking \"Grand Prix Legends\" for the PC, released in 1998\n\nOccasionally, a racing game or driving simulator will also include an attachable steering wheel that can be used to play the game in place of a controller. The wheel, which is usually plastic, may also include pedals to add to the game's reality. These wheels are usually used only for computer games.\n\nIn addition to the myriad commercial releases there is a bustling community of amateur coders working on closed and open source free simulators. Some of the major features popular with fans of the genre are online racing, realism and diversity of cars and tracks.\n\nDutch simulator designer Cruden offers private use car simulators for personal entertainment. Cruden car simulator is not connected to a computer game, system uses the same software that is designed for Formula 1 teams and engineers to improve their skills and their vehicles. The simulator provides different types of settings. Such as; different cars, terrains or any other desired settings. Also, telemetry analyze is available as an optional feature.\n\nDriving simulators are used at research facilities for many purposes. The Center for Addiction and Mental Health (CAMH) in Toronto use the Virage Simulation VS500M driving simulator to study and measure the effects of cannabis on driving (Alex Ballingall, Toronto Star. Nov. 26, 2015). Many vehicle manufacturers operate driving simulators, e.g. BMW, Ford, Renault. Many universities also operate simulators for research. In addition to studying driver training issues, driving simulators allow researchers to study driver behavior under conditions in which it would be illegal and/or unethical to place drivers. For instance, studies of driver distraction would be dangerous and unethical (because of the inability to obtain informed consent from other drivers) to do on the road.\n\nWith the increasing use of various in-vehicle information systems (IVIS) such as satellite navigation systems, cell phones, DVD players and e-mail systems, simulators are playing an important rule in assessing the safety and utility of such devices.\n\nSimulators are also used in psychometric testing, driver behaviour mapping, analysis of driving patterns for driverless car development, etc. Example: Central Road Research Institute and Faros Simulation System have jointly developed a car simulator for extensive research purposes. CRRI Faros Research Simulator\n\nThere exists a number of types research driving simulators, with a wide range of capabilities. The most complex, like the National Advanced Driving Simulator, have a full-sized vehicle body, with six-axis movement and 360-degree visual displays. On the other end of the range are simple desktop simulators such as the York Driving Simulator that are often implemented using a computer monitor for the visual display and a videogame-type steering wheel and pedal input devices. These low cost simulators are used readily in the evaluation of basic and clinically oriented scientific questions.\n\nThe issue is complicated by political and economic factors, as facilities with low-fidelity simulators claim their systems are \"good enough\" for the job, while the high-fidelity simulator groups insist that their (considerably more expensive) systems are necessary. Research into motion fidelity indicates that, while some motion is necessary in a research driving simulator, it does not need to have enough range to match real-world forces. Recent research has also considered the use of the real-time photo-realistic video content that reacts dynamically to driver behaviour in the environment.\n\nThere is a question of validity—whether results obtained in the simulator are applicable to real-world driving. Given the inability to replicate some simulator studies on the sidewalk this is likely to remain an issue for some time. Some research teams are using automated vehicles to recreate simulator studies on a test track, enabling a more direct comparison between the simulator study and the real world. As computers have grown faster and simulation is more widespread in the automotive industry, commercial vehicle math models that have been validated by manufacturers are seeing use in simulators. Driving simulators can also be used for the training of Airplane, Train, Tramway and other vehicles drivers. The simulation software can be seen as serious game, and some companies became specialists for the delivery of simulators systems (such as Oktal for instance).\n\nSimulator Adaptation Syndrome (\"SAS\"), is an issue with all simulators, not just driving simulators. The main cause of Simulator Adaptation Syndrome are system delays between the driver's command and the response of the simulator. In effect the brain, referencing driving a real vehicle, expects the simulator's response to be the same as a car, the greater the deviation the greater the \"adaption burden\" on the brain.\n\nIf the deviation is large, the driver may experience symptoms of headaches, motion sickness, disorientation, etc. due to SAS although this is very dependent on the individual. Likewise the simulator \"cues\" also have an effect, that is some individuals will experience discomfort due to a simulator not having motion cues, where others may not have a problem with such simulators. Some individuals will show high tolerance to visual system delays, where others may not.\n\nThe \"Adaptation\" in SAS relates to the brain's accepting these simulator's disparities relative to a real vehicle and thus slowly changing its reference point to that of the simulation. Thus with gradual introduction to the simulator environment the brain will slowly adapt and the negative effects of SAS (headaches, motion sickness, disorientation, etc.) will be greatly reduced. Once a person spends several hours in a driving simulator a real vehicle can evoke SAS again however the adaption time is greatly reduced: the brain quickly \"remembers\" and \"resets\" the real vehicle reference point.\n\nAs an example of SAS timing, when airline pilots go through regular simulator training they are not allowed to fly an airplane for 1 week in order to allow their brains to \"forget\" the SAS reference point induced by the simulator.\n\nTo sum up, , there is however a physiological response to simulators called \"SAS\" which can result in headaches, motion sickness, and disorientation. By minimizing the simulator's system delays, rendering all the cues economically possible, and slowly introducing susceptible individuals, the effects of SAS can be mitigated.\n\nThe primary reason for the $54 million price tag of the National Advanced Driving Simulator was to reduce SAS the maximum amount possible. The National Advanced Driving Simulator is capable of rendering, including full motion cues, a driver passing a vehicle on a 2 lane highway from the beginning of the pass to the end (thus its enormous motion base).\n\nNowadays, driving simulators are not only used for research purposes but are also used in the development process of a vehicle by either the car manufacturers or their suppliers. This is for example the case with the car projectors development: to reduce costs and delays, car projectors are tested with virtual prototypes before any physical prototype is built.\n"}
{"id": "726015", "url": "https://en.wikipedia.org/wiki?curid=726015", "title": "Dry box", "text": "Dry box\n\nA dry box is a storage container in which the interior is kept at a low level of humidity. It may be as simple as an airtight and watertight enclosure, or it may use active means to remove water vapor from the air trapped inside.\n\nDry boxes are used to safely store items that would otherwise be damaged or adversely affected by excessive humidity, such as cameras and lenses (to prevent fungal growth), and musical instruments (to prevent humidity induced swelling or shrinkage of wooden instrument parts). They are also used in the storage of surface mount electronic components prior to circuit board assembly, to prevent water absorption that could flash into steam during soldering, destroying the part.\n\nA simple dry box can consist of nothing more than a sealed, airtight box containing a desiccant, such as silica gel or anhydrous calcium chloride. These can be easily built at relatively low cost. However, the humidity level in such boxes cannot be controlled or regulated, owing to the difficulty of gauging the quantity of desiccant required to achieve a certain humidity level. Repeated opening of such boxes, allowing humid ambient air to enter, can saturate the desiccant, and some desiccants can have corrosive or other harmful effects on the contents of the box if they collect enough water to dissolve.\n\nElectronic dry boxes contain a small Peltier cooler, which removes moisture from the air by condensing it out. A control dial is usually provided that permits the user rough adjustment of the humidity level. More sophisticated designs link the cooler to a settable digital hygrometer, allowing very precise humidity level control.\n\nAnother form of electronic dry box technology utilizes multi-porous molecular sieve desiccants to adsorb moisture. This moisture and humidity control technology is renewable without having to replace desiccants. Many electronic dry box manufacturers have utilize or switch to this technology as there are less limitation than the Peltier cooler which is less effective in removing moisture in colder ambient temperatures.\n\n\n"}
{"id": "37871408", "url": "https://en.wikipedia.org/wiki?curid=37871408", "title": "Foreground detection", "text": "Foreground detection\n\nForeground detection is one of the major tasks in the field of computer vision and image processing whose aim is to detect changes in image sequences. Background subtraction is any technique which allows an image's foreground to be extracted for further processing (object recognition etc.).\n\nMany applications do not need to know everything about the evolution of movement in a video sequence, but only require the information of changes in the scene, because an image's regions of interest are objects (humans, cars, text etc.) in its foreground. After the stage of image preprocessing (which may include image denoising, post processing like morphology etc.) object localisation is required which may make use of this technique.\n\nDetecting foreground to separate these changes taking place in the foreground of the background. It is a set of techniques that typically analyze the video sequences in real time and are recorded with a stationary camera.\n\nAll detection techniques are based on modelling the background of the image, i.e. set the background and detect which changes occur. Defining the background can be very difficult when it contains shapes, shadows, and moving objects. In defining the background it is assumed that the stationary objects could vary in color and intensity over time.\n\nScenarios where these techniques apply tend to be very diverse. There can be highly variable sequences, such as images with very different lighting, interiors, exteriors, quality, and noise. In addition to processing in real time, systems need to be able to adapt to these changes.\n\nA very good foreground detection system should be able to:\n\nBackground subtraction is a widely used approach for detecting moving objects in videos from static cameras. The rationale in the approach is that of detecting the moving objects from the difference between the current frame and a reference frame, often called \"background image\", or \"background model\". Background subtraction is mostly done if the image in question is a part of a video stream. Background subtraction provides important cues for numerous applications in computer vision, for example surveillance tracking or human poses estimation.\n\nBackground subtraction is generally based on a static background hypothesis which is often not applicable in real environments. With indoor scenes, reflections or animated images on screens lead to background changes. Similarly, due to wind, rain or illumination changes brought by weather, static backgrounds methods have difficulties with outdoor scenes.\n\nThe temporal average filter is a method that was proposed at the Velastin. This system estimates the background model from the median of all pixels of a number of previous images.\nThe system uses a buffer with the pixel values of the last frames to update the median for each image.\n\nTo model the background, the system examines all images in a given time period called training time. At this time we only display images and will find the median, pixel by pixel, of all the plots in the background this time.\n\nAfter the training period for each new frame, each pixel value is compared with the input value of funds previously calculated. If the input pixel is within a threshold, the pixel is considered to match the background model and its value is included in the pixbuf. Otherwise, if the value is outside this threshold pixel is classified as foreground, and not included in the buffer.\n\nThis method can not be considered very efficient because they do not present a rigorous statistical basis and requires a buffer that has a high computational cost.\n\nA robust background subtraction algorithm should be able to handle lighting changes, repetitive motions from clutter and long-term scene changes. The following analyses make use of the function of \"V\"(\"x\",\"y\",\"t\") as a video sequence where \"t\" is the time dimension, \"x\" and \"y\" are the pixel location variables. e.g. \"V\"(1,2,3) is the pixel intensity at (1,2) pixel location of the image at \"t\" = 3 in the video sequence.\n\nA motion detection algorithm begins with the segmentation part where foreground or moving objects are segmented from\nthe background. The simplest way to implement this is to take an image as background and take the frames obtained at the time\nt, denoted by I(t) to compare with the background image denoted by B. Here using simple arithmetic calculations, we can\nsegment out the objects simply by using image subtraction technique of computer vision meaning for each pixels in I(t), take the\npixel value denoted by P[I(t)] and subtract it with the corresponding pixels at the same position on the background image\ndenoted as P[B].\n\nIn mathematical equation, it is written as:\n\nThe background is assumed to be the frame at time \"t\". This difference image would only show some intensity for the pixel locations which have changed in the two frames. Though we have seemingly removed the background, this approach will only work for cases where all foreground pixels are moving and all background pixels are static.\n\nA threshold \"Threshold\" is put on this difference image to improve the subtraction (see Image thresholding).\n\nThis means that the difference image's pixels' intensities are 'thresholded' or filtered on the basis of value of Threshold.\n\nThe accuracy of this approach is dependent on speed of movement in the scene. Faster movements may require higher thresholds.\n\nFor calculating the image containing only the background, a series of preceding images are averaged. For calculating the background image at the instant \"t\",\n\nwhere \"N\" is the number of preceding images taken for averaging. This averaging refers to averaging corresponding pixels in the given images. \"N\" would depend on the video speed (number of images per second in the video) and the amount of movement in the video. After calculating the background \"B\"(\"x\",\"y\",\"t\") we can then subtract it from the image \"V\"(\"x\",\"y\",\"t\") at time \"t\" = t and threshold it. Thus the foreground is\n\nwhere Th is threshold. Similarly we can also use median instead of mean in the above calculation of \"B\"(\"x\",\"y\",\"t\").\n\nUsage of global and time-independent thresholds (same Th value for all pixels in the image) may limit the accuracy of the above two approaches.\n\nFor this method, Wren et al. propose fitting a Gaussian probabilistic density function (pdf) on the most recent formula_5 frames. In order to avoid fitting the pdf from scratch at each new frame time formula_6, a running (or on-line cumulative) average is computed.\n\nThe pdf of every pixel is characterized by mean formula_7 and variance formula_8 . The following is a possible initial condition (assuming that initially every pixel is background):\n\nwhere formula_11 is the value of the pixel's intensity at time formula_6. In order to initialize variance, we can, for example, use the variance in x and y from a small window around each pixel.\n\nNote that background may change over time (e.g. due to illumination changes or non-static background objects). To accommodate for that change, at every frame formula_6, every pixel's mean and variance must be updated, as follows:\n\nWhere formula_17 determines the size of the temporal window that is used to fit the pdf (usually formula_18 ) and formula_19 is the Euclidean distance between the mean and the value of the pixel.\n\nWe can now classify a pixel as background if its current intensity lies within some confidence interval of its distribution's mean:\n\nwhere the parameter formula_22 is a free threshold (usually formula_23 ). A larger value for formula_24 allows for more dynamic background, while a smaller formula_24 increases the probability of a transition from background to foreground due to more subtle changes.\n\nIn a variant of the method, a pixel's distribution is only updated if it is classified as background. This is to prevent newly introduced foreground objects from fading into the background. The update formula for the mean is changed accordingly:\n\nwhere formula_27 when formula_11 is considered foreground and formula_29 otherwise. So when formula_27 , that is, when the pixel is detected as foreground, the mean will stay the same. As a result, a pixel, once it has become foreground, can only become background again when the intensity value gets close to what it was before turning foreground. This method, however, has several issues: It only works if all pixels are initially background pixels (or foreground pixels are annotated as such). Also, it cannot cope with gradual background changes: If a pixel is categorized as foreground for a too long period of time, the background intensity in that location might have changed (because illumination has changed etc.). As a result, once the foreground object is gone, the new background intensity might not be recognized as such anymore.\n\nMixture of Gaussians method approaches by modelling each pixel as a mixture of Gaussians and uses an on-line approximation to update the model. In this technique, it is assumed that every pixel's intensity values in the video can be modeled using a Gaussian mixture model. A simple heuristic determines which intensities are most probably of the background. Then the pixels which do not match to these are called the foreground pixels.\nForeground pixels are grouped using 2D connected component analysis.\n\nAt any time t, a particular pixel (formula_31)'s history is\n\nThis history is modeled by a mixture of \"K\" Gaussian distributions:\n\nwhere\n\nFirst, each pixel is characterized by its intensity in RGB color space. Then probability of observing the current pixel is given by the following formula in the multidimensional case\n\nWhere K is the number of distributions, ω is a weight associated to the ith Gaussian at time t and µ, Σ are the mean and standard deviation of said Gaussian respectively.\n\nOnce the parameters initialization is made, a first foreground detection can be made then the parameters are updated. The first B Gaussian distribution which exceeds the threshold \"T\" is retained for a background distribution\n\nThe other distributions are considered to represent a foreground distribution. Then, when the new frame incomes at times formula_38, a match test is made of each pixel. A pixel matches a Gaussian distribution if the Mahalanobis distance\n\nwhere \"k\" is a constant threshold equal to formula_40.Then, two cases can occur:\n\nCase 1: A match is found with one of the \"k\" Gaussians. For the matched component, the update is done as follows\n\nPower and Schoonees [3] used the same algorithm to segment the foreground of the image\n\nThe essential approximation to formula_43 is given by formula_44\n\nCase 2: No match is found with any of the formula_46 Gaussians. In this case, the least probable distribution formula_46 is replaced with a new one with parameters\n\nOnce the parameter maintenance is made, foreground detection can be made and so on. An on-line K-means approximation is used to update the Gaussians. Numerous improvements of this original method developed by Stauffer and Grimson have been proposed and a complete survey can be found in Bouwmans et al. A standard method of adaptive backgrounding is averaging the images over time, creating a background approximation which is similar to the current static scene except where motion occur.\n\nSeveral surveys which concern categories or sub-categories of models can be found as follows:\n\n\n\n\n\nSeveral comparison/evaluation papers can be found in the literature:\n\n\n\n\n\n\n\nThe Background Subtraction Website (T. Bouwmans, Univ. La Rochelle, France) contains a comprehensive list of the references in the field, and links to available datasets and software.\n\n\nThe BackgroundSubtractorCNT library implements a very fast and high quality algorithm written in C++ based on OpenCV. It is targeted at low spec hardware but works just as fast on modern Linux and Windows. (For more information: https://github.com/sagi-z/BackgroundSubtractorCNT).\n\nThe BGS Library (A. Sobral, Univ. La Rochelle, France) provides a C++ framework to perform background subtraction algorithms. The code works either on Windows or on Linux. Currently the library offers more than 30 BGS algorithms. (For more information: https://github.com/andrewssobral/bgslibrary)\n\n\n"}
{"id": "7273383", "url": "https://en.wikipedia.org/wiki?curid=7273383", "title": "Gamma ray logging", "text": "Gamma ray logging\n\nGamma ray logging is a method of measuring naturally occurring gamma radiation to characterize the rock or sediment in a borehole or drill hole. It is a wireline logging method used in mining, mineral exploration, water-well drilling, for formation evaluation in oil and gas well drilling and for other related purposes. Different types of rock emit different amounts and different spectra of natural gamma radiation. In particular, shales usually emit more gamma rays than other sedimentary rocks, such as sandstone, gypsum, salt, coal, dolomite, or limestone because radioactive potassium is a common component in their clay content, and because the cation exchange capacity of clay causes them to adsorb uranium and thorium. This difference in radioactivity between shales and sandstones/carbonate rocks allows the gamma ray tool to distinguish between shales and non-shales. But it cannot distinguish between carbonates and sandstone as they both have similar deflections on the gamma ray log. Thus gamma ray logs cannot be said to make good lithological logs by themselves, but in practice, gamma ray logs are compared side-by-side with stratigraphic logs. \n\nThe gamma ray log, like other types of well logging, is done by lowering an instrument down the drill hole and recording gamma radiation variation with depth. In the United States, the device most commonly records measurements at 1/2-foot intervals. Gamma radiation is usually recorded in API units, a measurement originated by the petroleum industry. Gamma rays attenuate according to the diameter of the borehole mainly because of the properties of the fluid filling the borehole, but because gamma logs are generally used in a qualitative way, amplitude corrections are usually not necessary. \n\nThree elements and their decay chains are responsible for the radiation emitted by rock: potassium, thorium and uranium. Shales often contain potassium as part of their clay content and tend to adsorb uranium and thorium as well. A common gamma-ray log records the total radiation and cannot distinguish between the radioactive elements, while a spectral gamma ray log (see below) can.\n\nFor standard gamma-ray logs, the measured value of gamma-ray radiation is calculated from concentration of uranium in ppm, thorium in ppm, and potassium in weight percent: e.g., GR API = 8 × uranium concentration in ppm + 4 × thorium concentration in ppm + 16 × potassium concentration in weight percent. Due to the weighted nature of uranium concentration in the GR API calculation, anomalous concentrations of uranium can cause clean sand reservoirs to appear shaley. For this reason, spectral gamma ray is used to provide an individual reading for each element so that anomalous concentrations can be found and properly interpreted.\n\nAn advantage of the gamma log over some other types of well logs is that it works through the steel and cement walls of cased boreholes. Although concrete and steel absorb some of the gamma radiation, enough travels through the steel and cement to allow for qualitative determinations. \n\nIn some places, non-shales exhibit elevated levels of gamma radiation. For instance, sandstones can contain uranium minerals, potassium feldspar, clay filling, or lithic fragments that cause the rock to have higher than usual gamma readings. Coal and dolomite may contain adsorbed uranium. Evaporite deposits may contain potassium minerals such as sylvite and carnallite. When this is the case, spectral gamma ray logging should be done to identify the source of these anomalies.\n\nSpectral logging is the technique of measuring the spectrum, or number and energy, of gamma rays emitted via natural radioactivity of the rock formation. There are three main sources of natural radioactivity on Earth: potassium (40K), thorium (principally 232Th and 230Th), and uranium (principally 238U and 235U). These radioactive isotopes each emit gamma rays that have a characteristic energy level measured in MeV. The quantity and energy of these gamma rays can be measured by a scintillometer. A log of the spectroscopic response to natural gamma ray radiation is usually presented as a total gamma ray log that plots the weight fraction of potassium (%), thorium (ppm) and uranium (ppm). The primary standards for the weight fractions are geological formations with known quantities of the three isotopes. Natural gamma ray spectroscopy logs became routinely used in the early 1970s, although they had been studied from the 1950s.\n\nThe characteristic gamma ray line that is associated with each radioactive component:\n\n\nAnother example of the use of spectral gamma ray logs is to identify specific clay types, like kaolinite or illite. This may be useful for interpreting the environment of deposition as kaolinite can form from feldspars in tropical soils by leaching of potassium; and low potassium readings may thus indicate the presence of one or more paleosols. The identification of specific clay minerals is also useful for calculating the effective porosity of reservoir rock.\n\nGamma ray logs are also used in mineral exploration, especially exploration for phosphates, uranium, and potassium salts.\n"}
{"id": "49020252", "url": "https://en.wikipedia.org/wiki?curid=49020252", "title": "Ghodganga Sugar Factory", "text": "Ghodganga Sugar Factory\n\nGhodganga Sahakari Sakhar Karkhana Ltd is an establishment set up around 70 km east to Pune. Factory is recently named after founder Raosahebdada Pawar Ghodganga Sahakari Sakhar Karkhana Ltd (RPGSSKL). This project turned out to be the best achievement of Cooperative sugar factories and rural development. This set up is being conducive as sugar production is prime business for region. The project brings employment and social development. Sugarcane grower/farmers in the region are really encouraged by the establishment. The project started with capital share of local farmers.\n\nAfter the outstanding achievements of Cooperative sugar factories and rural development set up in Maharashtra state. The establishment was started with the capital shares of local farmers, Pune District Central Cooperative Bank and perpetual support by Raosahebdada Pawar on 16 May 1990 with of 2500 TCD crushing capacity . Cooperative movement for sugar industry started in 1960s in Maharashtra with announcement of the potential 12 places in the states where sugar factories could be established. Then called Bombay state government announced a capital share of Indian Rupee 1 million, for the cooperatives societies to come forward for establishing sugar factories at these potential 12 places. A central committee was formed by Bombay State Cooperative Bank under the chairmanship. Factory caters to the almost Shirur, Maharashtra sugarcane growers & interstate too. This is prominently conducive to the local region.\n\nHarvesting of sugarcane is done with the help of machine and for this bank also provided the fund to farmers. Currently factory has owned four harvester machines. Taking into account scarcity of workers in future factory has taken this initiative.\n\nFor increasing production of sugarcane factory provides guidance, field work training to the farmers. In last year factory appointed officials from research centre to do this work. This implementation is followed & monitored completely.\n\nRaosahebdada Pawar Ghodganga Sahakari Sakhar Karkhana Ltd, which had set up a co-generation power unit at Shirur in Pune district of Maharashtra.The Bagasse based power unit with a capacity of 20 MW is expected to entail a cost of Rs 2 billion. The power project raised request to upgrade capacity to 20.5 MW. This Project is economically strong by converting its own waste in useful\nproduct and power generation. The pollution generated from this unit can be successfully managed through EMP implementation or in fact \nn be converted to useful irrigation water with nutrients and Electricity. This project justifies as to curb Electricity issue & wastage of cane trash & Bagasse lead to environment risk.\n\nThe Raosahebdada Pawar Ghodganga Sahakari Sakhar Karkhana Ltd to set up distilleries at their existing set up with INR 99.5 million approximately USD 15.5 Million. The project involves setting up to 30 KLPD Molasses based distillery unit for production of 30 KLPD of rectified spirit of 27 KLPD of Extra Neutral Alcohol of 27.5 KLPD absolute Alcohol.\nThe project have proposed to establish distillery unit within same premises adopting continuous fermentation process\n\nFactory stands ahead in the queue from the first couple of years of its establishment.\n\nThe project achieved Best Cooperative sugar factory award on its fourth year.\n\nEstablishment set itself at First position on Maharashtra Energy Development Agency award for year 2012–13 in Sugar Cooperative category\n"}
{"id": "19045168", "url": "https://en.wikipedia.org/wiki?curid=19045168", "title": "Gina Bianchini", "text": "Gina Bianchini\n\nGina Bianchini (born 1972) is an American entrepreneur and investor. She was CEO of Ning, which she co-founded with Marc Andreessen. Since leaving Ning in March 2010, she has been an entrepreneur in residence at the Andreessen Horowitz venture firm.\n\nIn September 2011, Bianchini became head of a privately funded Palo Alto start-up Mighty Networks. Additionally in 2011, Bianchini became an angel investor in Levo League along with Facebook COO Sheryl Sandberg.\n\nPrior to Ning, Bianchini was co-founder and president of Harmonic Communications which was acquired by Dentsu. She has also held positions at CKS Group and Goldman Sachs & Co.\n\nShe graduated from Stanford University in 1994 and has also been featured on the cover of \"Fortune\" magazine.\n\n"}
{"id": "38002632", "url": "https://en.wikipedia.org/wiki?curid=38002632", "title": "GravityLight", "text": "GravityLight\n\nGravityLight is a gravity-powered lamp designed by the company Deciwatt for use in developing or third-world nations, as a replacement for kerosene lamps. It uses a bag filled with rocks or earth, attached to a cord, which slowly descends similar to the weight drive in a cuckoo clock. This action powers the light for up to twenty minutes.\n\nAn early Gravity Light concept was developed concurrently by Clay Moulton and also by Mike Wofsey as part of his Ph.D. in applied physics from the University of Alabama in 2006. While Moulton did not reportedly develop a prototype, Wofsey did develop a rudimentary prototype that used a custom-machined rare earth magnet motor with minimal gearing. Wofsey secured the www.gravitylight.com domain to disseminate the findings, however he did not pursue the gravity light as he decided the efficiency was too low to be commercially viable. The theoretical efficiency of the device is limited by the simple potential energy in raising a mass to a specified height, and then dividing by the time that the lamp is desired to stay lit. So even a relatively large mass of 10 kg, raised to 1 meter, produces a maximum available energy of about 98 joules. When divided by just 5 minutes of light, this returns a usable power of 0.32 watts at an unrealistic 100% efficiency, and only 0.16 watts at 50% total conversion efficiency, which was close to the efficiency of the University of Alabama prototype. At 5.5 operating voltage of an LED, that left only 20 milliamperes for the LED. This is sufficient to light an LED, but the available light from the LED would not likely be useful for reading or night activities. A modification to this approach was suggested where the power draw can be adjusted by the user to trade illumination brightness for illumination time.\n\nThe first IndieGoGo campaign of GravityLight was ended on January 18, 2013 with $399,590 funded by 6219 funders.\n\nThe second IndieGoGo campaign, GravityLight 2: Made in Africa ended on July 18, 2015. It featured an improved design and the goal of manufacturing them in Kenya.\n\nMartin Riddiford and Jim Reeves worked on GravityLight as a side project for four years.\n\nThere are no operating costs after the initial purchase of the appliance. A standard GravityLight kit comes with an adjustable lamp and a ballast bag. The light can be turned on by filling the bag with approximately 20 pounds weight (10 kg) and lifting it up to the base of the device; the weight falls over a period of 25 minutes, pulling a cord/strap that spins gears and drives an electric generator, which continuously powers an LED. This creates enough energy to last 25 minutes whenever it is needed.\n\nThe second model, GL02, also includes two SatLights and connecting cables. These are separate lights that are wired in series from the main GravityLight unit. Each SatLight can be turned on or off separately. When used with SatLights, the light on the main unit can be turned on or off. Up to 4 SatLights can be connected, giving extra light to different locations in the house. The rate of the bag drop is almost not affected by the number of SatLights attached.\n\nThe original GravityLight used a strap for pulling up the weight. The improved GL02 used a plastic-bead chain on a pulley system. The pulley system requires less strength to pull up.\n\nGravityLight was called one of \"The 25 Best Inventions of the Year 2013\" by Time Magazine.\n\n\n"}
{"id": "37628090", "url": "https://en.wikipedia.org/wiki?curid=37628090", "title": "Hans Hammarskiöld", "text": "Hans Hammarskiöld\n\nHans Arvid Hammarskiöld (17 May 1925 – 12 November 2012) was a Swedish professional photographer. He was active in most genres—for many years he worked as an industrial photographer, but was especially noted for his portraits.\n\nHammarskiöld was born in 1925 in Stockholm. His breakthrough as a professional photographer came in the 1950s when he worked all over the world. In 1955 his work, along with that of his wife Caroline Hebbe, was selected by Edward Steichen for the world-touring Museum of Modern Art exhibition and book The Family of Man, seen by 9 million visitors. For some years he was employed by the British \"Vogue\".\n\nHammarskiöld was the last surviving member of the group \"Tio fotografer\" (Ten Photographers), which formed in 1958 and was influential in Swedish photography for decades as the illustrations agency \"Tiofoto\". The group included Sten Didrik Bellander (1921–2001), Harry Dittmar, Sven Gillsäter (1921–2001), Rune Hassner (1928–2003), Hans Malmberg (1927-1977), Pål-Nils Nilsson (1929–2002), Georg Oddner (1923-2007), and Lennart Olson (1925–2010).\n\nIn 2009, a selection of seventy of Hammarskiöld's portraits was on display at the National Museum in Stockholm. \n\nHis portraits were later donated to the National Swedish Portrait Gallery. \n\nHammarskiöld died in 2012 in Lidingö, east of Stockholm, after a short illness.\n\n\n"}
{"id": "167339", "url": "https://en.wikipedia.org/wiki?curid=167339", "title": "Head transplant", "text": "Head transplant\n\nA head transplant is an experimental surgical operation involving the grafting of one organism's head onto the body of another; in many experiments the recipient's head was not removed but in others it has been. Experimentation in animals began in the early 1900s. , no durable success have been achieved.\n\nThere are three main technical challenges. As with any organ transplant, managing the immune response to avoid transplant rejection is necessary. Also, the brain is highly dependent on continuous flow of blood to provide oxygen and nutrients and remove waste products, with damage setting in quickly at normal temperatures when blood flow is cut off. Finally, managing the nervous systems in both the body and the head is essential, in several ways. The autonomic nervous system controls essential functions like breathing and the heart beating and is governed largely by the brain stem; if the recipient body's head is removed this can no longer function. Additionally each nerve coming out of the head via the spinal cord needs to be connected to the putatively corresponding nerve in the recipient body's spinal cord in order for the brain to control movement and receive sensory information. Finally, the risk of systematic neuropathic pain is high and had largely been unaddressed in research.\n\nOf these challenges, dealing with blood supply and transplant rejection have been addressed in the field of transplant medicine generally, making transplantation of several types of organs fairly routine; however in a field as common as liver transplantation around a quarter of organs are rejected within the first year and overall mortality is still much higher than the general population. The challenge of grafting the nervous system remained in early stages of research .\n\nAlexis Carrel was a French surgeon who had developed improved surgical methods to connect blood vessels in the context of organ transplantation. In 1908 he collaborated with the American Charles Claude Guthrie to attempt to graft the head of one dog on an intact second dog; the grafted head showed some reflexes early on but deteriorated quickly and the animal was killed after a few hours. Carrel's work on organ transplantation later earned a Nobel Prize; Guthrie was probably excluded because of this controversial work on head transplantation.\n\nIn 1954, Vladimir Demikhov, a Soviet surgeon who had done important work to improve coronary bypass surgery, performed an experiment in which he grafted a dog's head and upper body including the front legs, onto another dog; the effort was focused on how to provide blood supply to the donor head and upper body and not on grafting the nervous systems. The dogs generally survived a few days; one survived 29 days. The grafted body parts were able to move and react to stimulus. The animals died due to transplant rejection.\n\nIn the 1950s and '60s immunosuppressive drugs were developed and organ transplantation techniques were developed that eventually made transplantation of kidneys, livers, and other organs standard medical procedures.\n\nIn 1965 Robert J. White did a series of experiments in which he attempted to graft only the vascular system of isolated dog brains onto existing dogs, to learn how to manage this challenge. He monitored brain activity with EEG and also monitored metabolism, and showed that he could maintain high levels of brain activity and metabolism by avoiding any break in the blood supply. The animals survived between 6 hours and 2 days. In 1970 he did four experiments in which he cut the head off of a monkey and connected the blood vessels of another monkey head to it; he did not attempt to connect the nervous systems. White used deep hypothermia to protect the brains during the times when they were cut off from blood during procedure. The recipient bodies had to be kept alive with mechanical ventilation and drugs to stimulate the heart. The grafted heads were able to function - the eyes tracked moving objects and it could chew and swallow. There were problems with the grafting of blood vessels that led to blood clots forming, and White used high doses of immunosuppressive drugs that had severe side effects; the animals died between 6 hours and 3 days after the heads were engrafted. These experiments were reported and criticized in the media and were considered barbaric by animal rights activists. There were few animal experiments on head transplantation for many years after this.\n\nIn 2012 Xiaoping Ren published work in which he grafted the head of a mouse onto another mouse's body; again the focus was on how to avoid harm from the loss of blood supply; with his protocol the grafted heads survived up to six months.\n\nIn 2013 Sergio Canavero published a protocol that he said would make human head transplantation possible.\n\nIn 2015 Ren published work in which he cut off the heads of mice but left the brain stem in place, and then connected the vasculature of the donor head to the recipient body; this work was an effort to address whether it was possible to keep the body of the recipient animal alive without life support. All prior experimental work that involved removing the recipient body's head had cut the head off lower down, just below the second bone in the spinal column. Ren also used moderate hypothermia to protect the brains during the procedure.\n\nIn 2016 Ren and Canavero published a review of attempted as well as possible neuroprotection strategies that they said should be researched for potential use in a head transplantation procedure; they discussed various protocols for connecting the vasculature, the use of various levels of hypothermia, the use of blood substitutes, and the possibility of using hydrogen sulfide as a neuroprotective agent.\n\nArthur Caplan, a bioethicist, has written \"Head transplants are fake news. Those who promote such claims and who would subject any human being to unproven cruel surgery merit not headlines but only contempt and condemnation.\"\n\nWhite became a target for protestors because of his head transplantation experiments. One interrupted a banquet in his honor by offering him a bloody replica of a human head. Others called his house asking for \"Dr. Butcher\". When White testified in a civil hearing about Dr. Sam Sheppard's murder case, lawyer Terry Gilbert compared Dr. White to Dr. Frankenstein. The People for the Ethical Treatment of Animals described White's experiments as \"epitomizing the crude, cruel vivisection industry\".\n\nIn general the field of transplantation medicine has been met with resistance and alarm from some quarters as advances have been made; Joseph Murray, who performed the first kidney transplant in 1954, was described as doing something unnatural or as playing God. These continued as other organs were transplanted, but perhaps became the most sharp as hand transplants and face transplants emerged in 1998 and 2005, as each of these are visible, personal, and social in ways that internal organs are not. The medical ethics of each of these procedures was extensively discussed and worked out before clinical experimental and regular usage began.\n\nWith regard to head transplantation, there had been little formal ethical discussion published in the literature and little dialogue among stakeholders ; the plans of Canavero were running well ahead of society's and the medical establishment's readiness or acceptance. There was no accepted protocol for conducting the procedure to justify the risk to the people involved, methods of obtaining informed consent were unclear, especially for the person whose body would be used; issues of desperation render the truly informed consent of a head donor questionable. With regard to societal costs, the body of a person willing to be an organ donor can save the lives of many people, and the supply of tissues and organs from people willing to be organ donors did not meet the medical need of recipients; the notion of an entire donor body going to one other person was difficult to justify at that time. Basic legal issues were also unclear with regard to whether only one or both of the people involved in a head transplantation would have any legal rights in the post-procedure person.\n\nThe most appropriate initial form of the procedure was unclear . Because grafting the head onto the spinal cord was not possible at that time, the only feasible procedure would be one where the head was only connected to the blood supply of the donor body, leaving the person completely paralyzed, with the accompanying limited quality of life and high societal cost to maintain.\n\nThe psychological results of the procedure were unclear as well. While concerns were raised about whether recipients of a face transplant and their social circle would have difficulty adjusting, studies had found that disruptions had been minimal. But no transplant had ever been performed where the entire body of an individual is unfamiliar at the conclusion of the procedure, and one of the few documents discussing the ethics in the biomedical literature, a letter to the editor of a journal published in 2015, foresaw a high risk of insanity as a result of the procedure.\n\nPopular opinion about Canavero's plans for head transplantation had been generally negative . Many of these criticisms focus on the state of technology and the timeframe in which Canavero says he will be able to successfully conduct the procedure.\n\nLiterature\nFilm\nVideo games\n\n\n"}
{"id": "1118626", "url": "https://en.wikipedia.org/wiki?curid=1118626", "title": "Herbert Akroyd Stuart", "text": "Herbert Akroyd Stuart\n\nHerbert Akroyd-Stuart (28 January 1864, Halifax, Yorkshire, England – 19 February 1927, Halifax) was an English inventor who is noted for his invention of the hot bulb engine, or heavy oil engine.\n\nAkroyd-Stuart had lived in Australia in his early years. He was educated at Newbury Grammar School (now St. Bartholomew's School) and Finsbury Technical College on Cowper Street. He was the son of Charles Stuart, founder of the Bletchley Iron and Tinplate Works, and joined his father in the business in 1887.\n\nIn 1885, Akroyd Stuart accidentally spilt paraffin oil (kerosene) into a pot of molten tin. The paraffin oil vaporised and caught fire when in contact with a paraffin lamp. This gave him an idea to pursue the possibility of using paraffin oil (very similar to modern-day diesel) for an engine, which unlike petrol would be difficult to be vaporised in a carburettor as its volatility is not sufficient to allow this.\n\nHis first prototype engines were built in 1886. In 1890, in collaboration with Charles Richard Binney, he filed Patent 7146 for Richard Hornsby and Sons of Grantham, Lincolnshire, England. The patent was entitled: \"Improvements in Engines Operated by the Explosion of Mixtures of Combustible Vapour or Gas and Air\". One such engine was sold to Newport Sanitary Authority, but the compression ratio was too low to get it started from cold, and it needed a heat poultice to get it going.\n\nAkroyd-Stuart's engines were built from 26 June 1891 by Richard Hornsby and Sons as the \"Hornsby Akroyd Patent Oil Engine\" under licence and were first sold commercially on 8 July 1892. It was the first internal combustion engine to use a pressurised fuel injection system.\n\nThe Hornsby-Akroyd engine used a comparatively low compression ratio, so that the temperature of the air compressed in the combustion chamber at the end of the compression stroke was not high enough to initiate combustion. Combustion instead took place in a separated combustion chamber, the \"vaporizer\" (also called the \"hot bulb\") mounted on the cylinder head, into which fuel was sprayed. It was connected to the cylinder by a narrow passage and was heated either by the cylinder's coolant or by exhaust gases while running; an external flame such as a blowtorch was used for starting. Self-ignition occurred from contact between the fuel-air mixture and the hot walls of the vaporizer. By contracting the bulb to a very narrow neck where it attached to the cylinder, a high degree of turbulence was set up as the ignited gases flashed through the neck into the cylinder, where combustion was completed. As the engine's load increased, so did the temperature of the bulb, causing the ignition period to advance; to counteract pre-ignition, water was dripped into the air intake.\n\nHot bulb engines were produced until the late 1920s, often being called \"semi-diesels\", even though they were not as efficient as compression ignition engines. They had the advantage of comparative simplicity, since they did not require the air compressor used by early Diesel engines; fuel was injected mechanically (solid injection) near the start of the compression stroke, at a much lower pressure than that of Diesel engines.\n\nRichard Hornsby and Sons built the world's first oil-engined railway locomotive LACHESIS for the Royal Arsenal, Woolwich, England, in 1896. They also built the first compression-ignition powered automobile.\n\nSimilar engines were built by Bolinder in Sweden and some of these still survive in canal boats.\n\nHot bulb engines were built in the USA by the De La Vergne Company of New York City, later the New York Refrigerating Company - inventing the modern refrigerator in 1930, who purchased a licence in 1893.\n\nThe modern Diesel engine is a hybrid incorporating the features of direct (airless) injection and compression-ignition, both patented (No. 7146) as \"Improvements in Engines Operated by the Explosion of Mixtures of Combustible Vapour or Gas and Air\" by Akroyd-Stuart and Charles Richard Binney in May 1890. Another patent (No. 15,994) was taken out on 8 October 1890, which details the working of a complete engine where air and fuel are introduced separately. But such a system was not feasible for the essential vaporization of heavier fuels for combustion in an enigne and moreover the modern diesel engine operation is based on vaporization by the fuel delivery system and not by internal thermal vaporization as in hot bulb which bears more similarity to original solution of Rudolph Diesel by employing injection and vaporization through air blast system which was the only technology available at that time capable of vaporizing and introducing heavier fuel at high pressure in the cylinder.\n\nIn 1890, Akroyd-Stuart patented the hot-bulb engine which had only about a 12% thermal efficiency. The reason for this, which he was not able to identify, was due to the lower pressures used (around 90 PSI) as opposed to the Diesel engine's c. 500 PSI).\n\nRudolf Diesel patented the compression-ignition engine in 1892; however his injection system, where combustion was produced isobarically (the technique having been patented by George Brayton in 1874 for his carburettor), was not subsumed into later engines, Akroyd-Stuart's injection system with isochoric combustion developed at Hornsbys being preferred. However the airblast method was the only capable technique for introducing and vaporizing heavier fuel in the cylinder until diesel injection systems were developed by Bosch, and although similar in mechanism to modern diesel injection system the fuel delivery system in hot oil engines did not vaporize oil so in essence Rudolph Diesel's solution is the forerunner of modern diesel engines where fuel is vaporized as well as pressurized by the fuel delivery system and not by thermal vaporization in an internal hot chamber.\n\nIn 1892, Akroyd-Stuart patented a water-jacketed vaporiser to allow compression ratios to be increased. In the same year, Thomas Henry Barton (who later founded Barton Transport) at Hornsbys built a working high-compression version for experimental purposes, whereby the vaporiser was replaced with a cylinder head, therefore not relying on air being preheated, but by combustion through higher compression ratios. It ran for six hours - the first time automatic ignition was produced by compression alone. However such an engine would have required fuel to be injected at the end of compression stroke while the original hot bulb engine design had fuel introduced at the intake stroke although separately from air. So the success of such an engine with respect to performance and efficiency is not known and such a success would have overshadowed Rudolph Diesel's efforts and thus seems doubtful. Moreover since the fuel system of hot bulb engine did not vaporize the fuel so it is not viable to remove the hot bulb since heavier fuel do not burn at high pressures without sufficient vapours. \n\nOn the other hand, Diesel understood thermodynamics and the theoretical and practical constraints on fuel efficiency. He knew that as much as 90% of the energy available in the fuel is wasted in a steam engine, and began to focus on designs that will achieve higher efficiency ratios. From 1893 to 1897, Rudolf Diesel continued to develop and improve his designs. He first experimented with a Carnot Cycle engine, and then developed his own approach. In his engine, fuel was injected at the end of compression and the fuel was ignited by the high temperature resulting from compression. This led to his design of the well-known high-compression prototype which was patented in 1897 in Germany and other countries, including the U.S. ( and ).\n\nIn 1900, he moved to Australia and set up a company Sanders & Stuart with his brother Charles, late in life moving back to Yorkshire, England. He died on 19 February 1927 of throat cancer and was buried in All Souls church in Boothtown, Halifax.\n\nThe University of Nottingham has hosted the Akroyd-Stuart Memorial Lecture on occasional years in his memory since 1928. One was presented by Sir Frank Whittle in 1946. Akroyd Stuart had worked with Professor William Robinson in the late 19th century, who was professor of engineering from 1890 to 1924 at University College Nottingham.\n\nAkroyd-Stuart also left money to the Institution of Mechanical Engineers, Royal Aeronautical Society and Institute of Marine Engineering, which provided for their respective bi-annual Akroyd-Stuart Prizes.\n\n\n\n"}
{"id": "11469558", "url": "https://en.wikipedia.org/wiki?curid=11469558", "title": "Intensive crop farming", "text": "Intensive crop farming\n\nIntensive crop farming is a modern form of intensive farming that refers to the industrialized production of crops. Intensive crop farming's methods include innovation in agricultural machinery, farming methods, genetic engineering technology, techniques for achieving economies of scale in production, the creation of new markets for consumption, patent protection of genetic information, and global trade. These methods are widespread in developed nations.\n\nThe practice of industrial agriculture is a relatively recent development in the history of agriculture, and the result of scientific discoveries and technological advances. Innovations in agriculture beginning in the late 19th century generally parallel developments in mass production in other industries that characterized the latter part of the Industrial Revolution. The identification of nitrogen and phosphorus as critical factors in plant growth led to the manufacture of synthetic fertilizers, making more intensive uses of farmland for crop production possible.\n\nSimilarly, the discovery of vitamins and their role in animal nutrition, in the first two decades of the 20th century, led to vitamin supplements, which in the 1920s allowed certain livestock to be raised indoors, reducing their exposure to adverse natural elements. The discovery of antibiotics and vaccines facilitated raising livestock in larger numbers by reducing disease. Chemicals developed for use in World War II gave rise to synthetic pesticides. Developments in shipping networks and technology have made long-distance distribution of produce feasible.\n\nCertain crops have proven more amenable to intensive farming than others.\n\n\nCritics of intensively farmed crops cite a wide range of concerns. On the food quality front, it is held by critics that quality is reduced when crops are bred and grown primarily for cosmetic and shipping characteristics. Environmentally, industrial farming of crops is claimed to be responsible for loss of biodiversity, degradation of soil quality, soil erosion, food toxicity (pesticide residues) and pollution (through agrichemical build-ups and runoff, and use of fossil fuels for agrichemical manufacture and for farm machinery and long-distance distribution).\n\nThe projects within the Green Revolution spread technologies that had already existed, but had not been widely used outside of industrialized nations. These technologies included pesticides, irrigation projects, and synthetic nitrogen fertilizer.\n\nThe novel technological development of the Green Revolution was the production of what some referred to as “miracle seeds.” Scientists created strains of maize, wheat, and rice that are generally referred to as HYVs or “high-yielding varieties.” HYVs have an increased nitrogen-absorbing potential compared to other varieties. Since cereals that absorbed extra nitrogen would typically lodge, or fall over before harvest, semi-dwarfing genes were bred into their genomes. Norin 10 wheat, a variety developed by Orville Vogel from Japanese dwarf wheat varieties, was instrumental in developing Green Revolution wheat cultivars. IR8, the first widely implemented HYV rice to be developed by IRRI, was created through a cross between an Indonesian variety named “Peta” and a Chinese variety named “Dee Geo Woo Gen.”\n\nWith the availability of molecular genetics in Arabidopsis and rice the mutant genes responsible (\"reduced height(rht)\", \"gibberellin insensitive (gai1)\" and \"slender rice (slr1)\") have been cloned and identified as cellular signalling components of gibberellic acid, a phytohormone involved in regulating stem growth via its effect on cell division. Stem growth in the mutant background is significantly reduced leading to the dwarf phenotype. Photosynthetic investment in the stem is reduced dramatically as the shorter plants are inherently more stable mechanically. Assimilates become redirected to grain production, amplifying in particular the effect of chemical fertilisers on commercial yield.\n\nHYVs significantly outperform traditional varieties in the presence of adequate irrigation, pesticides, and fertilizers. In the absence of these inputs, traditional varieties may outperform HYVs. One criticism of HYVs is that they were developed as F1 hybrids, meaning they need to be purchased by a farmer every season rather than saved from previous seasons, thus increasing a farmer’s cost of production.\n\nWheat is a grass that is cultivated worldwide. Globally, it is the most important human food grain and ranks second in total production as a cereal crop behind maize; the third being rice. Wheat and barley were the first cereals known to have been domesticated. Cultivation and repeated harvesting and sowing of the grains of wild grasses led to the domestication of wheat through selection of mutant forms with tough years which remained intact during harvesting, and larger grains. Because of the loss of seed dispersal mechanisms, domesticated wheats have limited capacity to propagate in the wild.\n\nAgricultural cultivation using horse collar leveraged plows (3000 years ago) increased cereal grain productivity yields, as did the use of seed drills which replaced broadcasting sowing of seed in the 18th century. Yields of wheat continued to increase, as new land came under cultivation and with improved agricultural husbandry involving the use of fertilizers, threshing machines and reaping machines (the 'combine harvester'), tractor-draw cultivators and planters, and better varieties (see Green Revolution and Norin 10 wheat). With population growth rates falling, while yields continue to rise, the area devoted to wheat may now begin to decline for the first time in modern human history.\n\nWhile winter wheat lies dormant during a winter freeze, wheat normally requires between 110 and 130 days between planting and harvest, depending upon climate, seed type, and soil conditions. Crop management decisions require the knowledge of stage of development of the crop. In particular, spring fertilizers applications, herbicides, fungicides, growth regulators are typically applied at specific stages of plant development. For example, current recommendations often indicate the second application of nitrogen be done when the ear (not visible at this stage) is about 1 cm in size (Z31 on Zadoks scale).\n\nMaize was planted by the Native Americans in hills, in a complex system known to some as the Three Sisters: beans used the corn plant for support, and squashes provided ground cover to stop weeds. This method was replaced by single species hill planting where each hill apart was planted with 3 or 4 seeds, a method still used by home gardeners. A later technique was \"checked corn\" where hills were placed apart in each direction, allowing cultivators to run through the field in two directions. In more arid lands this was altered and seeds were planted in the bottom of deep furrows to collect water. Modern technique plants maize in rows which allows for cultivation while the plant is young, although the hill technique is still used in the cornfields of some Native American reservations. Haudenosaunee Confederacy is what a group of Native Americans who are preparing for climate change through seed banking. Now this group is known as the Iroquois.\nWith a climate changing more crops are able to grow in different areas that they previously weren't able to grow in. This will open growing areas for maize.\nIn North America, fields are often planted in a two-crop rotation with a nitrogen-fixing crop, often alfalfa in cooler climates and soybeans in regions with longer summers. Sometimes a third crop, winter wheat, is added to the rotation. Fields are usually plowed each year, although no-till farming is increasing in use. Many of the maize varieties grown in the United States and Canada are hybrids. Over half of the corn area planted in the United States has been genetically modified using biotechnology to express agronomic traits such as pest resistance or herbicide resistance.\n\nBefore about World War II, most maize in North America was harvested by hand (as it still is in most of the other countries where it is grown). This often involved large numbers of workers and associated social events. Some one- and two-row mechanical pickers were in use but the corn combine was not adopted until after the War. By hand or mechanical picker, the entire ear is harvested which then requires a separate operation of a corn sheller to remove the kernels from the ear. Whole ears of corn were often stored in \"corn cribs\" and these whole ears are a sufficient form for some livestock feeding use. Few modern farms store maize in this manner. Most harvest the grain from the field and store it in bins. The combine with a corn head (with points and snap rolls instead of a reel) does not cut the stalk; it simply pulls the stalk down. The stalk continues downward and is crumpled into a mangled pile on the ground. The ear of corn is too large to pass through a slit in a plate and the snap rolls pull the ear of corn from the stalk so that only the ear and husk enter the machinery. The combine separates the husk and the cob, keeping only the kernels.\n\nSoybeans are one of the \"biotech food\" crops that are being genetically modified, and GMO soybeans are being used in an increasing number of products. Monsanto Company is the world's leader in genetically modified soy for the commercial market. In 1995, Monsanto introduced \"Roundup Ready\" (RR) soybeans that have had a copy of a gene from the bacterium, \"Agrobacterium\" sp. strain CP4, inserted, by means of a gene gun, into its genome that allows the transgenic plant to survive being sprayed by this non-selective herbicide, glyphosate. Glyphosate, the active ingredient in Roundup, kills conventional soybeans. The bacterial gene is EPSP (= 5-enolpyruvyl shikimic acid-3-phosphate) synthase. Soybean also has a version of this gene, but the soybean version is sensitive to glyphosate, while the CP4 version is not.\n\nRR soybeans allow a farmer to reduce tillage or even to sow the seed directly into an unplowed field, known as 'no-till' or conservation tillage. No-till agriculture has many advantages, greatly reducing soil erosion and creating better wildlife habitat; it also saves fossil fuels, and sequesters CO2, a greenhouse effect gas.\n\nIn \"1997\", about 8% of all soybeans cultivated for the commercial market in the United States were genetically modified. In 2006, the figure was 89%. As with other \"Roundup Ready crops\", concern is expressed over damage to biodiversity. However, the RR gene has been bred into so many different soybean cultivars that the genetic modification itself has not resulted in any decline of genetic diversity.\n\nThe largest commercial hydroponics facility in the world is Eurofresh Farms in Willcox, Arizona, which sold more than 200 million pounds of tomatoes in 2007. Eurofresh has under glass and represents about a third of the commercial hydroponic greenhouse area in the U.S. Eurofresh does not consider their tomatoes organic, but they are pesticide-free. They are grown in rockwool with top irrigation.\n\nSome commercial installations use no pesticides or herbicides, preferring integrated pest management techniques. There is often a price premium willingly paid by consumers for produce which is labeled \"organic\". Some states in the USA require soil as an essential to obtain organic certification. There are also overlapping and somewhat contradictory rules established by the US Federal Government. So some food grown with hydroponics can be certified organic. In fact, they are the cleanest plants possible because there is no environment variable and the dirt in the food supply is extremely limited. Hydroponics also saves an incredible amount of water; It uses as little as 1/20 the amount as a regular farm to produce the same amount of food. The water table can be impacted by the water use and run-off of chemicals from farms, but hydroponics may minimize impact as well as having the advantage that water use and water returns are easier to measure. This can save the farmer money by allowing reduced water use and the ability to measure consequences to the land around a farm.\n\nThe environment in a hydroponics greenhouse is tightly controlled for maximum efficiency and this new mindset is called soil-less/controlled-environment agriculture (S/CEA). With this growers can make ultra-premium foods anywhere in the world, regardless of temperature and growing seasons. Growers monitor the temperature, humidity, and pH level constantly.\n"}
{"id": "1929872", "url": "https://en.wikipedia.org/wiki?curid=1929872", "title": "Irreversible circuit", "text": "Irreversible circuit\n\nAn irreversible circuit is a circuit whose inputs cannot be reconstructed from its outputs.\nSuch a circuit, of necessity, consumes energy. See also reversible computing.\n"}
{"id": "42648456", "url": "https://en.wikipedia.org/wiki?curid=42648456", "title": "Laneros", "text": "Laneros\n\nLaneros (often stylized as LANeros) is a digital community of over 370,000 members worldwide, mostly of Hispanic countries, started in 1999. Its slogan is \"Digital Addiction\".\nThe name comes from the so-called LAN (Local area network) parties, which were where the founders gathered to play videogames 15 years ago, and would translate to English as \"LANners\".\n"}
{"id": "2227503", "url": "https://en.wikipedia.org/wiki?curid=2227503", "title": "Langmuir–Blodgett film", "text": "Langmuir–Blodgett film\n\nA Langmuir–Blodgett (LB) film is a nanostructured system formed when Langmuir films - or Langmuir monolayers (LM)- are transferred from the liquid-gas interface to solid supports during the vertical passage of the support through the monolayers. LB films can contain one or more monolayers of an organic material, deposited from the surface of a liquid onto a solid by immersing (or emersing) the solid substrate into (or from) the liquid. A monolayer is adsorbed homogeneously with each immersion or emersion step, thus films with very accurate thickness can be formed. This thickness is accurate because the thickness of each monolayer is known and can therefore be added to find the total thickness of a Langmuir–Blodgett film.\n\nThe monolayers are assembled vertically and are usually composed either of amphiphilic molecules (see Chemical polarity) with a hydrophilic head and a hydrophobic tail (example: fatty acids) or nowadays commonly of nanoparticles.\n\nLangmuir–Blodgett films are named after Irving Langmuir and Katharine B. Blodgett, who invented this technique while working in Research and Development for General Electric Co. \n\nAdvances to the discovery of LB and LM films began with Benjamin Franklin in 1773 when he dropped about a teaspoon of oil onto a pond. Franklin noticed that the waves were calmed almost instantly and that the calming of the waves spread for about half an acre. What Franklin did not realize was that the oil had formed a monolayer on top of the pond surface. Over a century later, Lord Rayleigh quantified what Benjamin Franklin had seen. Knowing that the oil, oleic acid, had spread evenly over the water, Rayleigh calculated that the thickness of the film was 1.6 nm by knowing the volume of oil dropped and the area of coverage.\n\nWith the help of her kitchen sink, Agnes Pockels showed that area of films can be controlled with barriers. She added that surface tension varies with contamination of water. She used different oils to deduce that surface pressure would not change until area was confined to about 0.2 nm. This work was originally written as a letter to Lord Rayleigh who then helped Agnes Pockels become published in the journal, \"Nature\", in 1891.Agnes Pockels’ work set the stage for Irving Langmuir who continued to work and confirmed Pockels’ results. Using Pockels’ idea, he developed the Langmuir (or Langmuir–Blodgett) trough. His observations indicated that chain length did not impact the affected area since the organic molecules were arranged vertically.\n\nLangmuir’s breakthrough did not occur until he hired Katherine Blodgett as his assistant. Blodgett initially went to seek for a job at General Electric (GE) with Langmuir during her Christmas break of her senior year at Bryn Mawr College, where she received a BA in Physics. Langmuir advised to Blodgett that she should continue her education before working for him. She thereafter attended University of Chicago for her MA in Chemistry. Upon her completion of her Master's, Langmuir hired her as his assistant. However, breakthroughs in surface chemistry happened after she received her PhD degree in 1926 from Cambridge University.\n\nWhile working for GE, Langmuir and Blodgett discovered that when a solid surface is inserted into an aqueous solution containing organic moieties, the organic molecules will deposit a monolayer homogeneously over the surface. This is the Langmuir–Blodgett film deposition process. Through this work in surface chemistry and with the help of Blodgett, Langmuir was awarded the Nobel Prize in 1932. In addition, Blodgett used Langmuir–Blodgett film to create 99% transparent anti-reflective glass by coating glass with fluorinated organic compounds, forming a simple anti-reflective coating.\n\nLangmuir films are formed when amphiphilic (surfactants) molecules or nanoparticles are spread on the air at an air–water interface. Surfactants (or surface-acting agents) are molecules with hydrophobic 'tails' and hydrophilic 'heads'. When surfactant concentration is less than the minimum surface concentration of collapse and it is completely insoluble in water , the surfactant molecules arrange themselves as shown in Figure 1 below. This tendency can be explained by surface-energy considerations. Since the tails are hydrophobic, their exposure to air is favoured over that to water. Similarly, since the heads are hydrophilic, the head–water interaction is more favourable than air–water interaction. The overall effect is reduction in the surface energy (or equivalently, surface tension of water).\n\n<br> Figure 1: Surfactant molecules arranged on an air–water interface\n\nFor very small concentrations, far from the surface density compatible with the collapse of the monolayer (which leads to polylayers structures) the surfactant molecules execute a random motion on the water–air interface. This motion can be thought to be similar to the motion of ideal-gas molecules enclosed in a container. The corresponding thermodynamic variables for the surfactant system are, surface pressure (formula_1), surface area (A) and number of surfactant molecules (N). This system behaves similar to a gas in a container. The density of surfactant molecules as well as the surface pressure increases upon reducing the surface area A ('compression' of the 'gas'). Further compression of the surfactant molecules on the surface shows behavior similar to phase transitions. The ‘gas’ gets compressed into ‘liquid’ and ultimately into a perfectly closed packed array of the surfactant molecules on the surface corresponding to a ‘solid’ state. The liquid state is usually separated in the liquid-expanded and liquid-condensed states. All the Langmuir film states are classified according to the compressionality factor of the films, defined as -A(d (formula_1)/dA), usually related to the in-plane elasticity of the monolayer. \n\nThe condensed Langmuir films (in surface pressures usually higher than 15 mN/m - typically 30 mN/m) can be subsequently transferred onto a solid substrate to create highly organized thin film coatings. Langmuir–Blodgett troughs \n\nBesides LB film from surfactants depicted in Figure 1, similar monolayers can also be made from inorganic nanoparticles.\n\nAdding a monolayer to the surface reduces the surface tension, and the surface pressure, formula_1 is given by the following equation:\n\nwhere formula_5 is equal to the surface tension of the water and formula_6 is the surface tension due to the monolayer. But the concentration-dependence of surface tension (similar to Langmuir isotherm) is as follows:\n\nThus,\n\nor\n\nThe last equation indicates a relationship similar to ideal gas law. However, it should be noted that the concentration-dependence of surface tension is valid only when the solutions are dilute and concentrations are low. Hence, at very low concentrations of the surfactant, the molecules behave like ideal gas molecules.\n\nExperimentally, the surface pressure is usually measured using the Wilhelmy plate. A pressure sensor/electrobalance arrangement detects the pressure exerted by the monolayer. Also monitored is the area to the side of the barrier which the monolayer resides.\n\nFigure 2. A Wilhelmy plate\n\nA simple force balance on the plate leads to the following equation for the surface pressure:\n\nonly when formula_11. Here, formula_12 and formula_13 are the dimensions of the plate, and formula_14 is the difference in forces. The Wilhelmy plate measurements give pressure – area isotherms that show phase transition-like behaviour of the LM films, as mentioned before (see figure below). In the gaseous phase, there is minimal pressure increase for a decrease in area. This continues until the first transition occurs and there is a proportional increase in pressure with decreasing area. Moving into the solid region is accompanied by another sharp transition to a more severe area dependent pressure. This trend continues up to a point where the molecules are relatively close packed and have very little room to move. Applying an increasing pressure at this point causes the monolayer to become unstable and destroy the monolayer forming polylayer structures towards the air phase. The surface pressure during the monolayer collapse may remain approximately constant (in a process near the equilibrium) or may decay abruptly (out of equilibrium - when the surface pressure was over-increased because lateral compression was too fast for monomolecular rearrangements). \nFigure 3. (i) Surface pressure – Area isotherms. (ii) Molecular configuration in the three regions marked in the formula_1-A curve; (a) gaseous phase, (b) liquid-expanded phase, and (c) condensed phase. (Adapted from Osvaldo N. Oliveira Jr., Brazilian Journal of Physics, vol. 22, no. 2, June 1992)\n\nMany possible applications have been suggested over years for LM and LB films. Their characteristics are extremely thin films and high degree of structural order. These films have different optical, electrical and biological properties which are composed of some specific organic compounds. Organic compounds usually have more positive responses than inorganic materials for outside factors (pressure, temperature or gas change). LM films can be used also as models for half a cellular membrane.\n\n\n\n"}
{"id": "6911203", "url": "https://en.wikipedia.org/wiki?curid=6911203", "title": "Lightbox", "text": "Lightbox\n\nA lightbox is a translucent surface illuminated from behind, used for situations where a shape laid upon the surface needs to be seen with high contrast.\n\nSeveral varieties exist, depending on their purpose:\n\n\n"}
{"id": "57180939", "url": "https://en.wikipedia.org/wiki?curid=57180939", "title": "List of WWII Maybach engines", "text": "List of WWII Maybach engines\n\nThis is an incomplete list of gasoline engines designed by Maybach AG, used in various German tracked military vehicles (mostly tanks and half-tracks) before and during World War II.\n\n1. Performance\nThis is followed by the approx. engine capacity (e.g. HL42 = approx. 4.2 litres)\n\n2. Installation\ne.g. HL45 Z, HL230 P30. These letters were only used on some models.\n\n3. Lubrication \n\n4. Transmission\n\n\n5. Compressor\n\n6. Ignition\n\nExamples:\n\nGerman WWII halftrack numbering may appear not to be strictly logical: in order of engine size and therefore towing capacity, they were numbered Sd.Kfz. 10, 11, 6, 7, 8, 9. The two smallest vehicles were introduced after most of the larger prime movers were in production.\n\nA number of Maybach motors shared the same basic design but had different engine sizes, the larger engines having bigger cylinders to increase the capacity. Similar engine designs had shared parts lists, e.g. the NL38 and HL42; the HL57 and HL62; and the HL108 and HL120.\n\nThe 6-cylinder Maybach engines used a single Solex 40 JFF II down-draught carburetor, and earlier V-12s used two. Later V-12s used Solex 52 JFFs.\n\n\n\n\n\nPhoto gallery of various Maybach engine types at Fahrzeuge der Wehrmacht (in German) \n"}
{"id": "59102278", "url": "https://en.wikipedia.org/wiki?curid=59102278", "title": "Margaret Faul", "text": "Margaret Faul\n\nMargaret M. Faul is an Irish / American chemist and executive who has won multiple awards for innovations in process chemistry.\n\nFaul received her undergraduate degrees from University College, Dublin before embarking on doctoral studies with Professor David A. Evans at Harvard. Her studies focused mostly on metal-catalyzed nitrene transfer reactions to produce aziridines, strained nitrogen precursors valued as pharmaceutical intermediates. Faul introduced multiple new wrinkles into this chemistry, including using chiral copper(I) catalysts to produce enantiomerically-enriched aziridines, and using a variety of different nitrene sources for the transfer.\n\nFaul joined the process chemistry group at Eli Lilly in 1993, and joined Amgen's process group in 2003, rising eventually to its Executive Director. According to a biosketch at \"Organic Syntheses\", Faul has expertise in Good Manufacturing Process scale-up of both chemical and biological therapeutics, and coordinates groups of external partners through licensing, regulatory, and program development issues. She attributes much of Amgen's success in this area to early adoption of new technologies, such as supercritical carbon dioxide purification and ultra-high performance liquid chromatography (uPLC).\n\nDr. Faul is an Editorial Board member at Thieme journal \"Science of Synthesis.\" She has served as the Chair of the Enabling Technologies Consortium.\n\n"}
{"id": "2339949", "url": "https://en.wikipedia.org/wiki?curid=2339949", "title": "Mirror lock-up", "text": "Mirror lock-up\n\nMirror lock-up (often abbreviated to MLU) is a feature employed in many Single Lens Reflex (SLR) cameras. It allows the operator to reduce vibration-induced motion blur during exposure. It also allows the mounting of lenses which extend into the SLR's mirror box when mounted.\n\nNormal operation in an SLR camera involves flipping the mirror up out of the light-path just before the shutter opens, and then returning it when the shutter closes (although very early SLR's required the shutter to be cocked for the mirror to return). This causes vibration of the camera, particularly when the mirror slaps into the top of the mirror box. This vibration quickly dies away so the most motion blur is actually seen with short shutter times that capture multiple 'swings' of the vibration (shutter speeds of 1/2 to 1/60 second are often affected by this). While longer exposures will capture all of the vibrations, the exposure will be dominated by light captured when the camera is vibration-free (assuming a steady mount).\n\nMirror lock-up involves flipping the mirror up well before the shutter opens, allowing the vibrations to die down before exposing the film. On some cameras MLU may be operated by an extra push of the shutter button, the second push resulting in the actual opening of the shutter. Other arrangements may involve an extra lever or button that flips the mirror up before using the shutter release button normally. On some cameras MLU is not a separate feature, but operated as part of the self timer (notably the Nikon FE line of cameras).\n\nNote that when the mirror is in the up and locked position, the subject is no longer visible through the viewfinder. Therefore, the photographer must compose the photograph prior to activating mirror lock-up and keep the camera from moving. Use of a tripod helps prevent movement of the camera during this operation.\n\nCombined with a remote or cable release, this greatly reduces the potential for vibration of the camera.\n\nSome lenses are designed such that they extend into the mirror box when properly mounted on an SLR. These may include early wide-angle lenses for SLR's, certain lenses designed for rangefinder cameras, and certain other non-SLR lenses adapted for SLR use. Mirror lock-up must be activated prior to mounting such lenses, and kept in place in order to prevent damage to the lens and/or camera. This method of operation prevents any use of the SLR viewfinder with the lens. Prior to the development of lenses utilizing the Angénieux retrofocus concept, mirror lock-up was essential to wide-angle SLR photography.\n\n"}
{"id": "34088391", "url": "https://en.wikipedia.org/wiki?curid=34088391", "title": "OpenBSD Journal", "text": "OpenBSD Journal\n\nThe OpenBSD Journal is an online newspaper dedicated to coverage of OpenBSD software and related events.\n\nThe OpenBSD Journal was founded in 2000 and operated until 1 April 2004 at . On 1 April 2004 the editors James Phillips and Jose Nazario announced that the site ceased its operation. Daniel Hartmeier backed up the contents of the journal in order to preserve them. Further investigation to the articles' structure lead to creation of CGI-based engine that would enable access to the deadly.org's content on a backup server. Consequently, the functionality of adding new articles was implemented and the previous editors allowed to re-publish articles. The OpenBSD Journal was therefore reintroduced at on 9 April 2004.\n\nThe OpenBSD Journal is widely recognized as a of OpenBSD-related information. It is a primary reporter for such events as Hackathons. The site also hosts the OpenBSD developers' blogs.\n\nThe current editors of OpenBSD Journal are:\n"}
{"id": "21284823", "url": "https://en.wikipedia.org/wiki?curid=21284823", "title": "Plug computer", "text": "Plug computer\n\nA plug computer is an external device, often configured for use in the home or office as a compact computer. It consists of a high-performance, low-power system-on-a-chip processor with several I/O ports (USB ports, ...) and typically runs any of a number of Linux distributions. Most versions do not have provisions for connecting a display and are best suited as for running media server, back-up services, file sharing and remote access functions, thus acting as a bridge between in-home protocols such as Digital Living Network Alliance (DLNA) and Server Message Block (SMB) and cloud based services. There are, however, plug computer offerings that have analog VGA monitor and/or HDMI connectors, which, along with multiple USB ports, permit the use of a display, keyboard, and mouse, thus making them full-fledged, low-power alternatives to desktop and notebook computers.\n\nThe name \"plug computer\" is derived from the small configuration of such devices: plug computers are often enclosed in an AC power plug or AC adapter.\n\nPlug computers typically consume little power and are inexpensive. One manufacturer claims its $119 plug computer draws 1.2 watts and can cost $2 a year to run.\n\nA number of other devices of this type began to appear at the 2009 Consumer Electronics Show.\n\n\n"}
{"id": "3282143", "url": "https://en.wikipedia.org/wiki?curid=3282143", "title": "Robust control", "text": "Robust control\n\nIn control theory, robust control is an approach to controller design that explicitly deals with uncertainty. Robust control methods are designed to function properly provided that uncertain parameters or disturbances are found within some (typically compact) set. Robust methods aim to achieve robust performance and/or stability in the presence of bounded modelling errors.\n\nThe early methods of Bode and others were fairly robust; the state-space methods invented in the 1960s and 1970s were sometimes found to lack robustness, prompting research to improve them. This was the start of the theory of robust control, which took shape in the 1980s and 1990s and is still active today.\n\nIn contrast with an adaptive control policy, a robust control policy is static; rather than adapting to measurements of variations, the controller is designed to work assuming that certain variables will be unknown but\nbounded.\n\nInformally, a controller designed for a particular set of parameters is said to be robust if it also works well under a different set of assumptions. High-gain feedback is a simple example of a robust control method; with sufficiently high gain, the effect of any parameter variations will be negligible. From the closed loop transfer function perspective, high open loop gain leads to substantial disturbance rejection in the face of system parameter uncertainty. Other examples on robust control include sliding mode and terminal sliding mode control.\n\nThe major obstacle to achieving high loop gains is the need to maintain system closed loop stability. Loop shaping which allows stable closed loop operation can be a technical challenge.\n\nRobust control systems often incorporate advanced topologies which include multiple feedback loops and feed-forward paths. The control laws may be represented by high order transfer functions required to simultaneously accomplish desired disturbance rejection performance with robust closed loop operation.\n\nHigh-gain feedback is the principle that allows simplified models of operational amplifiers and emitter-degenerated bipolar transistors to be used in a variety of different settings. This idea was already well understood by Bode and Black in 1927.\n\nThe theory of robust control began in the late 1970s and early 1980s and soon developed a number of techniques for dealing with bounded system uncertainty.\n\nProbably the most important example of a robust control technique is H-infinity loop-shaping, which was developed by Duncan McFarlane and Keith Glover of Cambridge University; this method minimizes the sensitivity of a system over its frequency spectrum, and this guarantees that the system will not greatly deviate from expected trajectories when disturbances enter the system.\n\nAn emerging area of robust control from application point of view is sliding mode control (SMC), which is a variation of variable structure control (VSC). The robustness properties of SMC with respect to matched uncertainty as well as the simplicity in design attracted a variety of applications.\n\nWhile robust control has been traditionally dealt with along deterministic approaches, in the last two decades this approach has been criticized on the basis that it is too rigid to describe real uncertainty, while it often also leads to over conservative solutions. Probabilistic robust control has been introduced as an alternative, see e.g. that interprets robust control within the so-called scenario optimization theory.\n\nAnother example is loop transfer recovery (LQG/LTR), which was developed to overcome the robustness problems of linear-quadratic-Gaussian control (LQG) control.\n\nOther robust techniques includes quantitative feedback theory (QFT), passivity based control, Lyapunov based control, etc.\n\nWhen system behavior varies considerably in normal operation, multiple control laws may have to be devised. Each distinct control law addresses a specific system behavior mode. An example is a computer hard disk drive. Separate robust control system modes are designed in order to address the rapid magnetic head traversal operation, known as the seek, a transitional settle operation as the magnetic head approaches its destination, and a track following mode during which the disk drive performs its data access operation.\n\nOne of the challenges is to design a control system that addresses these diverse system operating modes and enables smooth transition from one mode to the next as quickly as possible.\n\nSuch state machine driven composite control system is an extension of the gain scheduling idea where the entire control strategy changes based upon changes in system behavior.\n\n\n"}
{"id": "18685301", "url": "https://en.wikipedia.org/wiki?curid=18685301", "title": "Sediment basin", "text": "Sediment basin\n\nA sediment basin is a temporary pond built on a construction site to capture eroded or disturbed soil that is washed off during rain storms, and protect the water quality of a nearby stream, river, lake, or bay. The sediment-laden soil settles in the pond before the runoff is discharged. Sediment basins are typically used on construction sites of or more, where there is sufficient room. They are often used in conjunction with erosion controls and other sediment control practices. On smaller construction sites, where a basin is not practical, sediment traps may be used.\n\nEssential sediment abundance is prevalent in the construction industry which gives insight to future endeavors.\n\nOn some construction projects, the sediment basin is cleaned out after the soil disturbance (earth-moving) phase of the project, and modified to function as a permanent stormwater management system for the completed site, either as a detention basin or a retention basin.\n\nA sediment trap is a temporary settling basin installed on a construction site to capture eroded or disturbed soil that is washed off during rain storms, and protect the water quality of a nearby stream, river, lake, or bay. The trap is basically an embankment built along a waterway or low-lying area on the site. They are typically installed at the perimeter of a site and above storm drain inlets, to keep sediment from entering the drainage system. Sediment traps are commonly used on small construction sites, where a sediment basin is not practical. Sediment basins are typically used on construction sites of or more, where there is sufficient room.\n\nSediment traps are installed before land disturbance (earth moving, grading) begins on a construction site. The traps are often used in conjunction with erosion controls and other sediment control practices.\n\n\n"}
{"id": "34508819", "url": "https://en.wikipedia.org/wiki?curid=34508819", "title": "Setting spray", "text": "Setting spray\n\nSetting spray (also finishing spray, makeup setting spray, finish spray, or makeup spray) is a cosmetic product designed to preserve applied make-up long periods of time. Usually sold in small spray bottles, setting spray is applied by spritzing mist over the face, keeping the make-up application moist for several hours.\n\nOriginally produced for the entertainment industry by cosmetic companies, setting sprays were developed to limit make-up reapplication for actors in theatre and film.\n\nCosmetic company Skindinavia released a consumer-facing setting spray to the market, credited as being the first to do so. The product, called \"Makeup Finishing Spray\", gained popularity in Hollywood, where it was used on programs such as Project Runway and Dancing With The Stars. Many celebrities adopted the use of setting sprays as well. \n\nSince Skindinavia's launch, many other setting sprays have entered the market, including products made by Eyes Lips Face (J.A. Cosmetics Corp), Matrix, and Sephora.\n\nBecause setting sprays can preserve make-up through heat, sweat and physical exertion, it has become a widely used product in the entertainment industry, favored by stylists in theatre, film and television.\n\nThe cosmetics industry has accepted setting sprays as a complement to powders in keeping the skin from drying for longer periods of time, however there are product reviewers who have wondered if a setting spray is effective when not wearing foundation even though it has proven effective with minimal makeup.\n"}
{"id": "38365671", "url": "https://en.wikipedia.org/wiki?curid=38365671", "title": "Shunra", "text": "Shunra\n\nShunra Software, LLC was a privately held company that provided network virtualization solutions for software testing. On March 4, 2014, HP announced that it signed a definitive agreement to acquire the network virtualization business and technology of Shunra, which was an HP partner.\n\nFounded in Israel in 1998, the company received investment from Insight Venture Partners and Carmel Ventures in 2004 and moved its headquarters to Philadelphia in 2006. In 2010, Gary Jackson was appointed the CEO. In 2013, Shunra announced an open integration platform for mobile app performance. As part of this launch, a partnership was formed with Hewlett-Packard (HP), Jamo, Capgemini, SOASTA, and Keynote DeviceAnywhere. In October 2012, the company signed a global distribution agreement with HP which placed two of Shunra's products on the HP commercial price list.\n\nOn March 4, 2014, HP announced that it had signed a definitive agreement to acquire the network virtualization business and technology of Shunra.\n\nOn April 1, 2014, HP acquired the network virtualization business and technology of Shunra.\n"}
{"id": "15376806", "url": "https://en.wikipedia.org/wiki?curid=15376806", "title": "Smart pipe", "text": "Smart pipe\n\nSmart pipe, related to a mobile network operator (MNO or \"operator\"), refers to an operator’s network which leverages existing or unique service abilities, and the operator’s customer relationships, to provide value beyond that of data connectivity only. The use of the term “smart” refers to the operator’s ability to add value for added, and often unique, types of services and content beyond bandwidth and network speed only.\n\nAmong the commonly understood operational models for a MNO are smart pipes, walled gardens, and dumb pipes.\n\nWhile there is no real industry standard definition of a smart pipe, there are several operators, bloggers, and researchers who have described aspects of a smart pipe which are generally accepted.\n\nNetwork operator 3 led the way with the release of its X-Series line of devices in 2006. The X-Series platform bundles a set of services, several of which are unique to 3, and provides unlimited data access in exchange for a fixed monthly premium. While not necessarily a full realization of a smart pipe, 3 is one of the first operators to offer such a combination of bandwidth and value-added services for flat-rate pricing.\n\nIn addition to pricing, there is a set of services commonly viewed as parts of a smart pipe, such as: \n\nBy exposing these types of services to the mobile ecosystem, network operators can maintain the value of their pipes while enabling entrepreneurs to create new business models and generate entirely new revenue streams.\n\nThe need for operators to innovate around a smart pipe is rising as they face rising pressure from media companies and new technologies, such as Apple’s iPhone, Nokia’s consumer portal Ovi, and even the open-access policies of the U.S. Federal Communications Commission (FCC) introduced around its upcoming 700 MHz spectrum auction.\n\nRecently, Arun Sarin, CEO of Vodafone, was criticized for some statements he made about how Vodafone will always have a unique relationship with its subscribers through its billing relationship. Although Sarin’s point has been true traditionally, as the article points out there are several new threats to that exclusivity such as traditional credit card, Obopay, PayPal Mobile, and existing media companies like Google via the Android platform, Yahoo via Go, and Apple via iPhone.\n\n"}
{"id": "666031", "url": "https://en.wikipedia.org/wiki?curid=666031", "title": "Spur", "text": "Spur\n\nA spur is a metal tool designed to be worn in pairs on the heels of riding boots for the purpose of directing a horse or other animal to move forward or laterally while riding. It is usually used to refine the riding aids (commands) and to back up the natural aids (the leg, seat, hands, and voice). The spur is used in many equestrian disciplines. Most equestrian organizations have rules in about spur design and use and penalties for using spurs in any manner that constitutes animal abuse.\n\nThis very old word derives from Anglo-Saxon \"spura, spora\", related to \"spornan, spurnan\", to kick, \"spurn\"; cf. Medieval High German \"Sporn\", modern German \"Sporn\", Dutch \"spoor\", Frisian \"spoar\". The generalized sense of \"anything that urges on, stimulus\" is recorded in English from\" circa\" 1390.\n\nThe parts of a spur include:\n\nSpurs are usually held on by a leather or leather-like spur strap that goes over the arch of the foot and under the sole in front of the boot heel. Some western designs have a leather strap that goes only over the top, with a heel chain or a rubber \"tiedown\" instead of a strap under the boot. Also, some styles have no straps, where the heel band is simply very tight and slips on wedged between the sole and heel of the boot. Some spur designs have a slot for running the spur strap through, others have \"buttons\", sometimes on the heel band itself and sometimes attached to the heel band by hinges that allow a strap with buttonholes to be attached.\n\nWhen used in military ranks, senior officers, and officers of all ranks in cavalry and other formerly mounted units of some armies, wear a form of spur in certain orders of dress which is known as the box spur, having no spur strap, but a long metal prong opposite the neck, extending between the arms of the heel band, which is inserted into a specially fitted recess or \"box\" in the base of the boot heel. Due to the prong, such spurs can only be worn with appropriately equipped boots. This construction is shown in the photos of the swan neck and Waterford spurs below.\n\nSpurs seen in western riding may also have small curved-up hooks on the shank in front of the rowel, called \"chap guards\", that were originally used to prevent the rider's chaps from interfering with the rowels of the spur. The shank angle from the yoke can vary from \"full\" to \"one half\" to \"one quarter\" to \"straight\". Some cowboys also added small metal \"pajados,\" also known as \"jingo bobs\" or \"jingle bobs,\" near the rowel, to create a jingling sound whenever the foot moved. Rowels can vary in size and number of points.\n\nIn the history of veterinary science, the word \"rowel\" described a small disk of leather or other material that was used as a seton stitch.\n\nThe spur was used by the Celts during the La Tène period (which began in the fifth century BC), and is also mentioned by Xenophon (\"circa\" 430 - 354 BC.) Iron or bronze spurs were also used throughout the Roman Empire. The spur also existed in the medieval Arab world. Early spurs had a neck that ended in a point, called a prick, riveted to the heel band. Prick spurs had straight necks in the 11th century and bent ones in the 12th. The earliest form of the spur armed the heel with a single prick. In England, the rowel spur is shown upon the first seal of Henry III and on monuments of the 13th century, but it did not come into general use until the 14th century. The earliest rowels probably did not revolve, but were fixed.\n\nThe spurs of medieval knights were gilt and those of squires were silvered. To \"win his spurs\" meant to gain knighthood, as gilded spurs were reckoned the badge of knighthood. In the rare cases of ceremonious degradation, the spurs were hacked from the disgraced knight's heels with the cook's chopper. After the battle of the Golden Spurs in 1302, where the French chivalry suffered a humbling defeat, the victors hung up bushels of knights' gilt spurs in the churches of Kortrijk as trophies of what is still remembered by the Flemings as the \"Guldensporenslag\" (the battle of the golden spurs). The English named the French rout from Thérouanne as the Battle of the Spurs, due to the rapidity of the French cavalry's flight.\n\nPrick spurs were the standard form until the 14th century, when the rowel began to become more common. The prick design never died out entirely, but instead became a thicker, shorter neck with a dulled end, such as the modern \"Prince of Wales\" design commonly seen in English riding.\nThough often decorated throughout history, in the 15th century, spurs became an art form in both decoration and design, with elaborate engraving, very long shanks, and large rowels. Though sometimes it has been claimed that the design changes were used because of barding, the use of barding had fallen out of fashion by the time the most elaborate spur designs were created. More likely, the elaborate designs reflected the increased abundance of precious metals, particularly silver, that followed the European exploration of the Americas that began in 1492. Spur designs in Spain and colonial Mexico were particularly elaborate. For example, the spurs of the Spanish \"conquistadores\" were sometimes called \"espuela grande,\" the \"grand spur\", and could have rowels as large as 6 inches around.\n\nIn northern Europe, the spur became less elaborate after the 16th century, particularly following the Stuart Restoration, but elaborate spur designs persisted, particularly in the Americas, descendants of which are still seen today, particularly in Mexico and the western United States, where the spur has become an integral part of the \"vaquero\" and cowboy traditions. The spur as an art form, as well as a tool, is still seen in western riding, where spurs with engraving and other artistic elements, often handmade and using silver or other precious metals, are still worn.\n\nCollecting of particularly beautiful antique spurs is a popular pastime for some individuals, particularly aficionados of western history and cowboy culture.\n\nJust as a medieval knight was said to have \"earned his spurs\", the awarding of spurs has continued in the modern era as an honour bestowed upon individuals in organizations with military heritages, and among motorcycle riders. Members of the Papal Orders of Knighthood receive gilt spurs directly from the hands of the pope; members of the British Order of the Garter similarly receive gilt spurs from the monarch. Inductees into the American Order of the Spur receive gold-coloured (usually brass) spurs if they have earned their membership through combat, or silver-coloured (usually nickel) spurs if they have not seen combat, but complete a rite of passage.\n\nSpurs are worn with the tip of the neck pointed downward, sitting on the spur rest of the riding boot, if there is one, with the buckle of the spur strap worn on the outside of the foot.\n\nSpur styles differ between disciplines. Spurs for western riding tend to be heavier, often decorated, and have rowels that rotate. The neck of western spurs is usually longer and the rowel wide in diameter, to accommodate the leg position of the western-style rider, where the stirrup is adjusted long, and the heavy leather used for the saddle's fenders and stirrups places the rider's leg a bit farther from the horse.\nSpurs in English riding tend to be very sleek, slim, and conservative in design, with a shorter neck, as the saddle and leg position is closer to the horse. They usually have a rounded or blunt end. Rowels are not as popular as the plain blunt end, although some types include a rowel or smooth disk on the end. When used in sports requiring finesse, such as dressage, the spur's purpose is not to speed up the horse, but to give accurate and precise aids in lateral and complex movements, such as pirouettes, \"travers\", and \"renvers\", and the airs above the ground. Dressage riders tend to ride in Waterford- style spurs with a rounded knob at the end. Conversely, show hunter and jumper riders may use a flatter end to encourage forward movement, such as the Prince of Wales design.\nAnother type of modern spur is those used on motorcycles. They are characterized by rowels worn as foot jewelry, hung off of boots. They can be similar in appearance to spurs worn by equestrians. Their bright material attracts motor vehicle drivers to the presence of motorcyclists, especially to their feet where riders are most vulnerable when stopped in traffic. Their owners may further customize them by adding miniature strobing LED lights. They are also awarded by motorcycle clubs.\n\nThe spur is a refined tool, designed to allow the rider to transmit very subtle signals to the horse that are nearly invisible to any other observer. No matter the discipline, it is important that a rider has a correct position before using spurs, with a deep seat, legs lengthened to the extent allowed by the stirrups, heels down, with knees and thighs rolled in so that the rider has a solid base of support. A swinging or unstable leg may inadvertently jab the horse with the spur as the rider sits, thus irritating, harming, and frightening the horse, and chronic misuse may deaden it to the leg aids. Improper use may also provoke dangerous or undesirable behaviors such as bucking or running away.\n\nSpurs are rarely used in sports such as horse racing, where the rider's leg is not significantly in contact with the horse.\n\nMost spurs are activated by the rider flexing the heel slightly up and in. A roweled spur permits an additional type of action; a rider can roll the spur lightly against the side of the horse rather than being limited to simply pressing inward.\n\nThe exception to the use of spurs in a subtle fashion is in the rodeo events of bull riding and saddle bronc and bareback riding, where the rider is required to spur in an elaborate, stylized fashion, touching the horse or bull at every stride. This requirement is designed to resemble the behavior of old-time horse-breakers, who would deliberately provoke a horse to buck. In modern times, riders are required to use spurs in a manner that is merely encouraging a horse that is already predisposed to buck; they are not to produce pain. Spur design and use is strictly defined by rodeo rules, spurs are dull, and rowels must turn freely. In fact, the way spurs are to be used in bucking events generally makes it harder for the rider to stay on; in bareback bronc competition, the spurs must be above the point of the horse's shoulder at the first jump and remain forward at all times, deliberately creating a very awkward position for the rider that requires both strength and coordination to stay on the horse. In saddle-bronc competition, the rider must make a full sweep with the spurs from shoulder to flank with each jump, requiring great concentration, and any error in balance puts the rider in a position to be quickly unseated. Bull riders are allowed a position that is the closest to that of classic riding, they are not required to spur the bull, but if they choose to spur, may do so with their legs down in a style that resembles a normal riding position.\n\nSpurs are divided into men's, women's, and children's, according to width (which must fit on the heel of the rider's boot). Spurs are further divided according to the length of the neck, with being relatively small (and a common size in children's spurs), with some being 2–3 in (5–7.5 cm) long. Many competition rules limit the length of the neck.\n\n\n"}
{"id": "13961210", "url": "https://en.wikipedia.org/wiki?curid=13961210", "title": "Stack Resource Policy", "text": "Stack Resource Policy\n\nThe Stack Resource Policy (SRP) is a resource allocation policy used in real-time computing, used for accessing shared resources when using earliest deadline first scheduling. It was defined by T. P. Baker. SRP is not the same as the Priority ceiling protocol which is for fixed priority tasks (FP).\n\nEach task is assigned a preemption level based upon the following formula where formula_1 denotes the deadline of task formula_2 and formula_3 denotes the preemption level of task i:\n\nformula_4\n\nEach resource R has a current ceiling formula_5 that represents the maximum of the preemption levels of the tasks that may be blocked, when there are formula_6 units of formula_7 available and formula_8 is the maximum units of formula_7 that formula_10 may require at any one time. formula_5 is assigned as follows: \n\nformula_12\n\nThere is also a system ceiling formula_13 which is the maximum of all current ceilings of the resources.\n\nformula_14\n\nAny task formula_10 that wishes to preempt the system must first satisfy the following constraint:\n\nformula_16\n\nThis can be refined for Operating System implementation (as in MarteOS) by removing the multi-unit resources and defining the stack resource policy as follows\n\nThe 2011 book \"Hard Real-Time Computing Systems: Predictable Scheduling Algorithms and Applications\" by Giorgio C. Buttazzo featured a dedicated section to reviewing SRP from Baker 1991 work.\n"}
{"id": "541680", "url": "https://en.wikipedia.org/wiki?curid=541680", "title": "Standards organization", "text": "Standards organization\n\nA standards organization, standards body, standards developing organization (SDO), or standards setting organization (SSO) is an organization whose primary activities are developing, coordinating, promulgating, revising, amending, reissuing, interpreting, or otherwise producing technical standards that are intended to address the needs of a group of affected adopters.\n\nMost standards are voluntary in the sense that they are offered for adoption by people or industry without being mandated in law. Some standards become mandatory when they are adopted by regulators as legal requirements in particular domains.\n\nThe term \"formal standard\" refers specifically to a specification that has been approved by a standards setting organization. The term \"de jure standard\" refers to a standard mandated by legal requirements or refers generally to any formal standard. In contrast, the term \"de facto standard\" refers to a specification (or protocol or technology) that has achieved widespread use and acceptance – often without being approved by any standards organization (or receiving such approval only after it already has achieved widespread use). Examples of de facto standards that were not approved by any standards organizations (or at least not approved until after they were in widespread \"de facto\" use) include the Hayes command set developed by Hayes, Apple's TrueType font design and the PCL protocol used by Hewlett-Packard in the computer printers they produced.\n\nNormally, the term \"standards organization\" is not used to refer to the individual parties participating within the standards developing organization in the capacity of founders, benefactors, stakeholders, members or contributors, who themselves may function as the standards organizations.\n\nThe implementation of standards in industry and commerce became highly important with the onset of the Industrial Revolution and the need for high-precision machine tools and interchangeable parts. Henry Maudslay developed the first industrially practical screw-cutting lathe in 1800, which allowed for the standardisation of screw thread sizes for the first time.\n\nMaudslay's work, as well as the contributions of other engineers, accomplished a modest amount of industry standardization; some companies' in-house standards spread a bit within their industries. Joseph Whitworth's screw thread measurements were adopted as the first (unofficial) national standard by companies around the country in 1841. It came to be known as the British Standard Whitworth, and was widely adopted in other countries.\n\nBy the end of the 19th century differences in standards between companies was making trade increasingly difficult and strained. For instance, an iron and steel dealer recorded his displeasure in \"The Times\": \"Architects and engineers generally specify such unnecessarily diverse types of sectional material or given work that anything like economical and continuous manufacture becomes impossible. In this country no two professional men are agreed upon the size and weight of a girder to employ for given work\".\n\nThe Engineering Standards Committee was established in London in 1901 as the world's first national standards body. It subsequently extended its standardization work and became the British Engineering Standards Association in 1918, adopting the name British Standards Institution in 1931 after receiving its Royal Charter in 1929. The national standards were adopted universally throughout the country, and enabled the markets to act more rationally and efficiently, with an increased level of cooperation.\n\nAfter the First World War, similar national bodies were established in other countries. The Deutsches Institut für Normung was set up in Germany in 1917, followed by its counterparts, the American National Standard Institute and the French Commission Permanente de Standardisation, both in 1918.\n\nBy the mid to late 19th century, efforts were being made to standardize electrical measurement. An important figure was R. E. B. Crompton, who became concerned by the large range of different standards and systems used by electrical engineering companies and scientists in the early 20th century. Many companies had entered the market in the 1890s and all chose their own settings for voltage, frequency, current and even the symbols used on circuit diagrams. Adjacent buildings would have totally incompatible electrical systems simply because they had been fitted out by different companies. Crompton could see the lack of efficiency in this system and began to consider proposals for an international standard for electric engineering.\n\nIn 1904, Crompton represented Britain at the Louisiana Purchase Exposition in St. Louis, Missouri, as part of a delegation by the Institute of Electrical Engineers. He presented a paper on standardisation, which was so well received that he was asked to look into the formation of a commission to oversee the process. By 1906 his work was complete and he drew up a permanent constitution for the first international standards organization, the International Electrotechnical Commission. The body held its first meeting that year in London, with representatives from 14 countries. In honour of his contribution to electrical standardisation, Lord Kelvin was elected as the body's first President.\nThe International Federation of the National Standardizing Associations (ISA) was founded in 1926 with a broader remit to enhance international cooperation for all technical standards and specifications. The body was suspended in 1942 during World War II.\n\nAfter the war, ISA was approached by the recently formed United Nations Standards Coordinating Committee (UNSCC) with a proposal to form a new global standards body. In October 1946, ISA and UNSCC delegates from 25 countries met in London and agreed to join forces to create the new International Organization for Standardization (ISO); the new organization officially began operations in February 1947.\n\nStandards organizations can be classified by their role, position, and the extent of their influence on the local, national, regional, and global standardization arena.\n\nBy geographic designation, there are international, regional, and national standards bodies (the latter often referred to as NSBs). By technology or industry designation, there are standards developing organizations (SDOs) and also standards setting organizations (SSOs) also known as consortia. Standards organizations may be governmental, quasi-governmental or non-governmental entities. Quasi- and non-governmental standards organizations are often non-profit organizations.\n\nBroadly, an international standards organization develops international standards. (This does not necessarily restrict the use of other published standards internationally.)\n\nThere are many international standards organizations. The three largest and most well-established such organizations are the International Organization for Standardization, the International Electrotechnical Commission, and the International Telecommunication Union, which have each existed for more than 50 years (founded in 1947, 1906, and 1865, respectively) and are all based in Geneva, Switzerland. They have established tens of thousands of standards covering almost every conceivable topic. Many of these are then adopted worldwide replacing various incompatible \"homegrown\" standards. Many of these standards are naturally evolved from those designed in-house within an industry, or by a particular country, while others have been built from scratch by groups of experts who sit on various technical committees (TCs). These three organizations together comprise the World Standards Cooperation (WSC) alliance.\n\nISO is composed of the national standards bodies (NSBs), one per member economy. The IEC is similarly composed of national committees, one per member economy. In some cases, the national committee to the IEC of an economy may also be the ISO member from that country or economy. ISO and IEC are private international organizations that are not established by any international treaty. Their members may be non-governmental organizations or governmental agencies, as selected by ISO and IEC (which are privately established organizations).\n\nThe ITU is a treaty-based organization established as a permanent agency of the United Nations, in which governments are the primary members, although other organizations (such as non-governmental organizations and individual companies) can also hold a form of direct membership status in the ITU as well. Another example of a treaty-based international standards organization with government membership is the Codex Alimentarius Commission.\n\nIn addition to these, a large variety of independent international standards organizations such as the ASME, the ASTM International, the IEEE, the Internet Engineering Task Force (IETF), SAE International, TAPPI, the World Wide Web Consortium (W3C), and the Universal Postal Union (UPU) develop and publish standards for a variety of international uses. In many such cases, these international standards organizations are not based on the principle of one member per country. Rather, membership in such organizations is open to those interested in joining and willing to agree to the organization's by-laws – having either organizational/corporate or individual technical experts as members.\n\nThe Airlines Electronic Engineering Committee (AEEC) was formed in 1949 to prepare avionics system engineering standards with other aviation organizations RTCA, EUROCAE, and ICAO. The standards are widely known as the ARINC Standards.\n\nRegional standards bodies also exist, such as the European Committee for Standardization (CEN), the European Committee for Electrotechnical Standardization (CENELEC), the European Telecommunications Standards Institute (ETSI), and the Institute for Reference Materials and Measurements (IRMM) in Europe, the Pacific Area Standards Congress (PASC), the Pan American Standards Commission (COPANT), the African Organisation for Standardisation (ARSO), the Arabic industrial development and mining organization (AIDMO), and others.\n\nIn the European Union, only standards created by CEN, CENELEC, and ETSI are recognized as \"European standards\", and member states are required to notify the European Commission and each other about all the draft technical regulations concerning ICT products and services before they are adopted in national law. These rules were laid down in Directive 98/34/EC with the goal of providing transparency and control with regard to technical regulations.\n\nSub-regional standards organizations also exist such as the MERCOSUR Standardization Association (AMN), the CARICOM Regional Organisation for Standards and Quality (CROSQ), and the ASEAN Consultative Committee for Standards and Quality (ACCSQ), EAC East Africa Standards Committee www.eac-quality.net, and the GCC Standardization Organization (GSO) for Arab States of the Persian Gulf.\n\nIn general, each country or economy has a single recognized national standards body (NSB). A national standards body is likely the sole member from that economy in ISO; ISO currently has 161 members. National standards bodies usually do not prepare the technical content of standards, which instead is developed by national technical societies.\nNSBs may be either public or private sector organizations, or combinations of the two. For example, the Standards Council of Canada is a Canadian Crown Corporation, Dirección General de Normas is a governmental agency within the Mexican Ministry of Economy, and ANSI is a 501(c)(3) non-profit U.S. organization with members from both the private and public sectors. The National Institute of Standards and Technology (NIST), the U.S. government's standards agency, cooperates with ANSI under a memorandum of understanding to collaborate on the United States Standards Strategy. The determinates of whether an NSB for a particular economy is a public or private sector body may include the historical and traditional roles that the private sector fills in public affairs in that economy or the development stage of that economy.\n\nWhereas, the term \"national standards body\" (NSB) generally refers to the one-per-country standardization organization that is that country’s member of the ISO, the term \"standards developing organization\" (SDO) generally refers to the thousands of industry- or sector-based standards organizations that develop and publish industry specific standards. Some economies feature only an NSB with no other SDOs. Large economies like the United States and Japan have several hundred SDOs, many of which are coordinated by the central NSBs of each country (ANSI and JISC in this case). In some cases, international industry-based SDOs such as the IEEE and the Audio Engineering Society (AES) may have direct liaisons with international standards organizations, having input to international standards without going through a national standards body. SDOs are differentiated from standards setting organizations (SSOs) in that SDOs may be accredited to develop standards using open and transparent processes.\n\nDevelopers of technical standards are generally concerned with interface standards, which detail how products interconnect with each other, and safety standards, which established characteristics ensure that a product or process is safe for humans, animals, and the environment. The subject of their work can be narrow or broad. Another area of interest is in defining how the behavior and performance of products is measured and described in data sheets.\n\nOverlapping or competing standards bodies tend to cooperate purposefully, by seeking to define boundaries between the scope of their work, and by operating in a hierarchical fashion in terms of national, regional and international scope; international organizations tend to have as members national organizations; and standards emerging at national level (such as BS 5750) can be adopted at regional levels (BS 5750 was adopted as EN 29000) and at international levels (BS 5750 was adopted as ISO 9000).\n\nUnless adopted by a government, standards carry no force in law. However, most jurisdictions have truth in advertising laws, and ambiguities can be reduced if a company offers a product that is \"compliant\" with a standard.\n\nWhen an organization develops standards that may be used openly, it is common to have formal rules published regarding the process. This may include:\n\nThough it can be a tedious and lengthy process, formal standard setting is essential to developing new technologies. For example, since 1865, the telecommunications industry has depended on the ITU to establish the telecommunications standards that have been adopted worldwide. The ITU has created numerous telecommunications standards including telegraph specifications, allocation of telephone numbers, interference protection, and protocols for a variety of communications technologies. The standards that are created through standards organizations lead to improved product quality, ensured interoperability of competitors’ products, and they provide a technological baseline for future research and product development. Formal standard setting through standards organizations has numerous benefits for consumers including increased innovation, multiple market participants, reduced production costs, and the efficiency effects of product interchangeability.\n\nSome standards – such as the SIF Specification in K12 education – are managed by a non-profit organizations composed of public entities and private entities working in cooperation that then publish the standards under an open license at no charge and requiring no registration.\n\nA technical library at a university may have copies of technical standards on hand. Major libraries in large cities may also have access to many technical standards.\n\nSome users of standards mistakenly assume that all standards are in the public domain. This assumption is correct only for standards produced by the central governments whose publications are not amenable to copyright or to organizations that issue their standard under an open license. Any standards produced by non-governmental entities remain the intellectual property of their developers (unless specifically designed otherwise) and are protected, just like any other publications, by copyright laws and international treaties. However, the intellectual property extends only to the standard itself and not to its use. For instance if a company sells a device that is compliant with a given standard, it is not liable for further payment to the standards organization except in the special case when the organization holds patent rights or some other ownership of the intellectual property described in the standard.\n\nIt is, however, liable for any patent infringement by its implementation, just as with any other implementation of technology. The standards organizations give no guarantees that patents relevant to a given standard have been identified. ISO standards draw attention to this in the foreword with a statement like the following: \"Attention is drawn to the possibility that some of the elements of this document may be the subject of patent rights. ISO and IEC shall not be held responsible for identifying any or all such patent rights\". If the standards organization is aware that parts of a given standard fall under patent protection, it will often require the patent holder to agree to Reasonable and non-discriminatory licensing before including it in the standard. Such an agreement is regarded as a legally binding contract, as in the 2012 case \"Microsoft v. Motorola\".\n\nThe ever-quickening pace of technology evolution is now more than ever affecting the way new standards are proposed, developed and implemented.\n\nSince traditional, widely respected standards organizations tend to operate at a slower pace than technology evolves, many standards they develop are becoming less relevant because of the inability of their developers to keep abreast with the technological innovation. As a result, a new class of standards setters appeared on the standardization arena: the industry consortia or standards setting organizations (SSOs). Despite having limited financial resources, some of them enjoy truly international acceptance. One example is the World Wide Web Consortium (W3C), whose standards for HTML, CSS, and XML are used universally. There are also community-driven associations such as the Internet Engineering Task Force (IETF), a worldwide network of volunteers who collaborate to set standards for lower-level software solutions.\n\nSome industry-driven standards development efforts don't even have a formal organizational structure. They are projects funded by large corporations. Among them are the OpenOffice.org, an Apache Software Foundation-sponsored international community of volunteers working on an open-standard software that aims to compete with Microsoft Office, and two commercial groups competing fiercely with each other to develop an industry-wide standard for high-density optical storage.\n\n\n"}
{"id": "50580984", "url": "https://en.wikipedia.org/wiki?curid=50580984", "title": "Sybrin", "text": "Sybrin\n\nSybrin is a multinational Information Technology company headquartered in Johannesburg, South Africa that architects, develops, implements, and supports end-to-end payment and information management software solutions and services. Sybrin implemented the first cheque truncation system in Africa, the third in the world, in Malawi.\n\nAt a modular level, Sybrin’s offerings include EFT, ISO 20022, RTGS, and SWIFT processing, payment switches, mobile payments, cheque processing and truncation, cheque book ordering, Integrated Verification Systems, statements, reconciliation, Automated Clearing House (ACH) systems, document management, case management, account opening and KYC, biometrics, and information distribution solutions.\n\nSybrin’s clients include corporates, banking institutions, central banks, and national Automated Clearing Houses.\n\nSybrin has key support offices in Kenya, Mauritius, Mozambique, Rwanda, Tanzania, Zambia, and Zimbabwe. Sybrin’s systems have been implemented in Botswana, Egypt, Ghana, Gibraltar, Isle of Man, Kenya, Kuwait, Lesotho, Malawi, Mauritius, Mozambique, Nigeria, Rwanda, Seychelles, South Africa, Swaziland, Tanzania, Uganda, United Kingdom, Zambia, and Zimbabwe.\n\nSybrin has more than 600 software installations in 21 countries and a staff complement of 200. The company attained the Microsoft Line of Business Award and has been accredited as an Oracle Platinum Partner.\n\nFounded in 1991, Sybrin shifted its operational headquarters to Johannesburg, South Africa in 2002.\n\nIn 2003, Sybrin obtained the contract for the implementation of the first cheque truncation system in Africa, the third in the world, which was implemented in Malawi.\n\nIn 2005, Sybrin opened a support office in Kenya after being appointed to provide a Pan-African cheque solution as well as other solutions to all the major banks in Kenya.\n\nOn 13 October 2013, Sybrin was acquired by EOH, a multinational technology company listed on the Johannesburg Stock Exchange with over 11 000 employees.\n"}
{"id": "15737244", "url": "https://en.wikipedia.org/wiki?curid=15737244", "title": "Teaching machine", "text": "Teaching machine\n\nTeaching machines were originally mechanical devices. They presented educational materials and taught students. They were first invented by Sidney L. Pressey in the mid-1920s. His machine originally administered multiple-choice questions. The machine could be set so it moved on only when the student got the right answer. Tests showed that learning had taken place. This was an example of how knowledge of results causes learning. Much later, Norman Crowder developed the Pressey idea further.\n\nB.F. Skinner was responsible for a different type of machine called GLIDER, which used his ideas on how learning should be directed with positive reinforcement. Skinner advocated the use of teaching machines for a broad range of students (e.g., preschool aged to adult) and instructional purposes (e.g., reading and music). The instructional potential of the teaching machine stemmed from several factors: it provided automatic, immediate and regular reinforcement without the use of aversive control; the material presented was coherent, yet varied and novel; the pace of learning could be adjusted to suit the individual. As a result, students were interested, attentive, and learned efficiently by producing the desired behavior, \"learning by doing\".\n\nThere is extensive experience that both methods worked well, and so did programmed learning in other forms, such as books.\nThe ideas of teaching machines and programmed learning provided the basis for later ideas such as open learning and computer-assisted instruction.\n\n\n\n"}
{"id": "10399425", "url": "https://en.wikipedia.org/wiki?curid=10399425", "title": "Thermostatic mixing valve", "text": "Thermostatic mixing valve\n\nA thermostatic mixing valve (TMV) is a valve that blends hot water with cold water to ensure constant, safe shower and bath outlet temperatures, preventing scalding.\n\nThe storage of water at high temperature removes one possible breeding ground for Legionella; the use of a thermostat, rather than a static mixing valve, provides increased safety against scalding, and increased user comfort, because the hot-water temperature remains constant.\n\nMany TMVs use a wax thermostat for regulation. They also shut off rapidly in the event of a hot or cold supply failure to prevent scalding or thermal shock. \n\nIt is increasingly common practice around the world to regulate the storage water temperature to above 60 °C (140 °F), and to circulate or distribute water at a temperature less than 50 °C (122 °F). Water above these temperatures can cause scald injuries. Many countries, states, or municipalities now require that the temperature of all bath water in newly built and extensively refurbished domestic properties be controlled to a maximum of 48 °C (118 °F). Installing thermostatic mixing valves can ensure that water is delivered at the required temperature, thereby reducing the risk of scalding accidents; it also reduces hot water consumption from a supply that is maintained at a higher temperature.\n\nThere are three main categories for water temperature controlling devices: Heat Source, Group Control, and Point-of-Use. \n\nThese are used with central heating systems that use water as a medium.\n\nThese provide a uniform distribution temperature for all hot water outlets in a household.\n\nThese are single Outlet Thermostatic Mixing Valves, often called \"thermostatic faucets\", \"thermostat taps\" or \"thermostat valves\".\n\nAlthough other temperature regulating valves exist, thermostatic mixing valves are the preferred type in health care facilities, as they limit maximum outlet temperature, regardless of pressure or flow.\n\n\n"}
{"id": "58471", "url": "https://en.wikipedia.org/wiki?curid=58471", "title": "Timeline of temperature and pressure measurement technology", "text": "Timeline of temperature and pressure measurement technology\n\nTimeline of temperature and pressure measurement technology. A history of temperature measurement and pressure measurement technology.\n\n\n\n\n\n\n"}
{"id": "30848", "url": "https://en.wikipedia.org/wiki?curid=30848", "title": "Turbine", "text": "Turbine\n\nA turbine (from the Latin \"turbo\", a vortex, related to the Greek , \"tyrbē\", meaning \"turbulence\") is a rotary mechanical device that extracts energy from a fluid flow and converts it into useful work. The work produced by a turbine can be used for generating electrical power when combined with a generator. A turbine is a turbomachine with at least one moving part called a rotor assembly, which is a shaft or drum with blades attached. Moving fluid acts on the blades so that they move and impart rotational energy to the rotor. Early turbine examples are windmills and waterwheels.\n\nGas, steam, and water turbines have a casing around the blades that contains and controls the working fluid. Credit for invention of the steam turbine is given both to Anglo-Irish engineer Sir Charles Parsons (1854–1931) for invention of the reaction turbine, and to Swedish engineer Gustaf de Laval (1845–1913) for invention of the impulse turbine. Modern steam turbines frequently employ both reaction and impulse in the same unit, typically varying the degree of reaction and impulse from the blade root to its periphery.\n\nThe word \"turbine\" was coined in 1822 by the French mining engineer Claude Burdin from the Latin \"turbo\", or vortex, in a memo, \"Des turbines hydrauliques ou machines rotatoires à grande vitesse\", which he submitted to the Académie royale des sciences in Paris. Benoit Fourneyron, a former student of Claude Burdin, built the first practical water turbine.\n\nA working fluid contains potential energy (pressure head) and kinetic energy (velocity head). The fluid may be compressible or incompressible. Several physical principles are employed by turbines to collect this energy:\n\nImpulse turbines change the direction of flow of a high velocity fluid or gas jet. The resulting impulse spins the turbine and leaves the fluid flow with diminished kinetic energy. There is no pressure change of the fluid or gas in the turbine blades (the moving blades), as in the case of a steam or gas turbine, all the pressure drop takes place in the stationary blades (the nozzles). Before reaching the turbine, the fluid's \"pressure head\" is changed to \"velocity head\" by accelerating the fluid with a nozzle. Pelton wheels and de Laval turbines use this process exclusively. Impulse turbines do not require a pressure casement around the rotor since the fluid jet is created by the nozzle prior to reaching the blades on the rotor. Newton's second law describes the transfer of energy for impulse turbines. Impulse turbines are most efficient for use in cases where the flow is low and the inlet pressure is high. \n\nReaction turbines develop torque by reacting to the gas or fluid's pressure or mass. The pressure of the gas or fluid changes as it passes through the turbine rotor blades. A pressure casement is needed to contain the working fluid as it acts on the turbine stage(s) or the turbine must be fully immersed in the fluid flow (such as with wind turbines). The casing contains and directs the working fluid and, for water turbines, maintains the suction imparted by the draft tube. Francis turbines and most steam turbines use this concept. For compressible working fluids, multiple turbine stages are usually used to harness the expanding gas efficiently. Newton's third law describes the transfer of energy for reaction turbines. Reaction turbines are better suited to higher flow velocities or applications where the fluid head (upstream pressure) is low. \n\nIn the case of steam turbines, such as would be used for marine applications or for land-based electricity generation, a Parsons-type reaction turbine would require approximately double the number of blade rows as a de Laval-type impulse turbine, for the same degree of thermal energy conversion. Whilst this makes the Parsons turbine much longer and heavier, the overall efficiency of a reaction turbine is slightly higher than the equivalent impulse turbine for the same thermal energy conversion.\n\nIn practice, modern turbine designs use both reaction and impulse concepts to varying degrees whenever possible. Wind turbines use an airfoil to generate a reaction lift from the moving fluid and impart it to the rotor. Wind turbines also gain some energy from the impulse of the wind, by deflecting it at an angle. Turbines with multiple stages may use either reaction or impulse blading at high pressure. Steam turbines were traditionally more impulse but continue to move towards reaction designs similar to those used in gas turbines. At low pressure the operating fluid medium expands in volume for small reductions in pressure. Under these conditions, blading becomes strictly a reaction type design with the base of the blade solely impulse. The reason is due to the effect of the rotation speed for each blade. As the volume increases, the blade height increases, and the base of the blade spins at a slower speed relative to the tip. This change in speed forces a designer to change from impulse at the base, to a high reaction-style tip.\n\nClassical turbine design methods were developed in the mid 19th century. Vector analysis related the fluid flow with turbine shape and rotation. Graphical calculation methods were used at first. Formulae for the basic dimensions of turbine parts are well documented and a highly efficient machine can be reliably designed for any fluid flow condition. Some of the calculations are empirical or 'rule of thumb' formulae, and others are based on classical mechanics. As with most engineering calculations, simplifying assumptions were made.\n\nVelocity triangles can be used to calculate the basic performance of a turbine stage. Gas exits the stationary turbine nozzle guide vanes at absolute velocity \"V\". The rotor rotates at velocity \"U\". Relative to the rotor, the velocity of the gas as it impinges on the rotor entrance is \"V\". The gas is turned by the rotor and exits, relative to the rotor, at velocity \"V\". However, in absolute terms the rotor exit velocity is \"V\". The velocity triangles are constructed using these various velocity vectors. Velocity triangles can be constructed at any section through the blading (for example: hub, tip, midsection and so on) but are usually shown at the mean stage radius. Mean performance for the stage can be calculated from the velocity triangles, at this radius, using the Euler equation:\n\nHence:\n\nwhere:\n\nThe turbine pressure ratio is a function of formula_7 and the turbine efficiency.\n\nModern turbine design carries the calculations further. Computational fluid dynamics dispenses with many of the simplifying assumptions used to derive classical formulas and computer software facilitates optimization. These tools have led to steady improvements in turbine design over the last forty years.\n\nThe primary numerical classification of a turbine is its specific speed. This number describes the speed of the turbine at its maximum efficiency with respect to the power and flow rate. The specific speed is derived to be independent of turbine size. Given the fluid flow conditions and the desired shaft output speed, the specific speed can be calculated and an appropriate turbine design selected.\n\nThe specific speed, along with some fundamental formulas can be used to reliably scale an existing design of known performance to a new size with corresponding performance.\n\nOff-design performance is normally displayed as a turbine map or characteristic.\n\n\nAlmost all electrical power on Earth is generated with a turbine of some type. Very high efficiency steam turbines harness around 40% of the thermal energy, with the rest exhausted as waste heat.\n\nMost jet engines rely on turbines to supply mechanical work from their working fluid and fuel as do all nuclear ships and power plants.\n\nTurbines are often part of a larger machine. A gas turbine, for example, may refer to an internal combustion machine that contains a turbine, ducts, compressor, combustor, heat-exchanger, fan and (in the case of one designed to produce electricity) an alternator. Combustion turbines and steam turbines may be connected to machinery such as pumps and compressors, or may be used for propulsion of ships, usually through an intermediate gearbox to reduce rotary speed.\n\nReciprocating piston engines such as aircraft engines can use a turbine powered by their exhaust to drive an intake-air compressor, a configuration known as a turbocharger (turbine supercharger) or, colloquially, a \"turbo\".\n\nTurbines can have very high power density (i.e. the ratio of power to weight, or power to volume). This is because of their ability to operate at very high speeds. The Space Shuttle main engines used turbopumps (machines consisting of a pump driven by a turbine engine) to feed the propellants (liquid oxygen and liquid hydrogen) into the engine's combustion chamber. The liquid hydrogen turbopump is slightly larger than an automobile engine (weighing approximately 700 lb) and produces nearly 70,000 hp (52.2 MW).\n\nTurboexpanders are widely used as sources of refrigeration in industrial processes.\n\nMilitary jet engines, as a branch of gas turbines, have recently been used as primary flight controller in post-stall flight using jet deflections that are also called thrust vectoring. The U.S. Federal Aviation Administration has also conducted a study about civilizing such thrust vectoring systems to recover jetliners from catastrophes.\n\n\n\n"}
{"id": "44550150", "url": "https://en.wikipedia.org/wiki?curid=44550150", "title": "VREAM", "text": "VREAM\n\nVREAM, Inc. was a US technology company that functioned between 1991 and 1996. It was one of the first companies to develop PC-based software for authoring and viewing virtual reality (VR) environments.\n\nThe company was founded in Chicago in 1991 by former McKinsey & Company consultant Edward R. LaHood, who derived the name VREAM from the phrase \"virtual reality dream.\" LaHood was joined by co-founders Ken Gaebler. and Dan Malven in 1993.\n\nIn 1991, LaHood created VREAMScript, a scripting language for virtual reality environments that allowed for the definition of complex 3D objects, environment attributes, object attributes, and triggers for cause-and-effect relationships. The company then created a PC-based authoring tool, the VREAM Virtual Reality Development System, to build virtual reality environments and an accompanying runtime player, the VREAM Runtime System, that allowed end users to experience the virtual environments, moving through them in real-time while interacting with the rendered, virtual objects.\n\nThe VREAM Virtual Reality System, which included the VREAM Virtual Reality Development System and the VREAM Runtime System, was released for purchase in December, 1992, with a $1,495 price point and with support for a wide range of immersive devices, including the Power Glove.\n\nPrior to 1992, rendering real-time 3D graphics had only been possible on high-end workstations, and creating a real-time 3D graphics simulation required strong programming skills. The growing power of PCs, driven by such innovations as the Pentium chip (introduced by Intel in March 1993), made bringing virtual reality simulations to the PC a possibility, and LaHood's strong programming skills allowed him to be first to market with a PC-based virtual reality authoring solution that could be used by non-programmers. (LaHood was not, however, the first to bring virtual reality to the PC; that credit goes to Sense8 founders Patrice Gelband and Eric Gullichsen who, earlier in 1992, introduced WorldToolKit, a VR programmer's library that allowed developers to build \"virtual world\" applications that ran on desktop computers.)\n\nDue to its accessibility to non-programmers and those who could not afford high-end workstations, VREAM's software quickly became popular in the hobbyist virtual reality community. It was used for architectural walkthroughs, manufacturing training, game development, engineering prototyping, data visualization and other simulations. The software was even used in the treatment of fear of public speaking, acrophobia and male erectile disorder.\n\nIn addition to its software offerings, the company also provided virtual reality development services. A notable services client was Burger King, which hired VREAM to build a \"restaurant of the future\" for its franchisee conference. VREAM also created virtual reality product demonstrations for Bombardier Recreational Products for their Sea-Doo and Ski-Doo lines.\n\nIn 1994, based on the emergence of the World Wide Web and at the suggestion of co-founder Malven, the company recast its runtime player as a plug-in for Netscape Navigator, dubbing it WIRL. VREAM would later release WIRL as an ActiveX control for Microsoft Internet Explorer.\n\nAt a time when web-based software was in its earliest stages, WIRL was quite popular, ranking fourth on the list of downloaded software, surpassed only by Netscape Navigator, MPEG Player NET TOOB and HTML editor HotDog Pro. Within the first month of being available, WIRL was downloaded over 30,000 times, providing VREAM with a very efficient way to promote its software.\n\nWIRL was also used by PC Magazine to test the capabilities of PC graphics cards. Bill Gates, CEO of Microsoft, and Craig Barrett, of Intel, used WIRL demonstrations in their keynote speeches to showcase the potential of more powerful PCs.\n\nIn November 1994, VRML, a standard file format for representing Web-based, real-time interactive 3D environments, was introduced. The introduction of VRML, backed by SGI and other companies, lessened the future potential of VREAM's proprietary VREAMScript language. To mitigate its risks, VREAM adopted the approach of supporting both VRML and VREAMScript in its software products.\n\nIn May 1995, VREAM rebranded the next generation of its VR authoring software as VRCreator, and touted its support for \"multi-participant VR...across the World Wide Web.\"\n\nIn November 1995, based on the company's track record and based on growing interest in the venture capital community for Web-focused software companies, VREAM received $750,000 in venture funding from PLATINUM Venture Partners. At this time, co-founder Malven left to pursue other opportunities.\n\nVREAM's primary competitor in its earliest days was Sense8 Corp. of Sausalito, Calif., which offered a virtual reality programming toolkit for the PC.\n\nBy March 1995, however, the VR space was getting crowded, with many authoring tools on the market, including VREAM's VRCreator, Virtus VR, Virtus Walkthrough Pro, and Superscape VRT.\n\nAs virtual reality grabbed market attention, a number of new competitors emerged, including Paper Software (founded by Mike McCue), Intervista (founded by Tony Parisi) and Silicon Graphics.\n\nAfter Netscape acquired Paper Software in February 1996 for $20 million, VREAM hired an investment banker to sell the company.\n\nWhile working to sell the company, VREAM continued to work on the next generation of its VRML authoring and browsing software. After releasing a new version, the company's VRML browser, WIRL, received the PC Magazine Editor's Choice award in November 1996.\n\nIn that same month, November 1996, VREAM was acquired by PLATINUM Technology, Inc. for $10.3 million and became a business unit within PLATINUM.\n\nAs the World Wide Web's popularity grew, creating many opportunities for tech companies, interest in VR and VRML quickly faded and many VR companies struggled. The VREAM division of PLATINUM Technology acquired Intervista and the assets of SGI's VR division. PLATINUM Technology deployed VREAM's virtual reality software in some of its enterprise software products, but closed down the VREAM business unit just prior to the acquisition of PLATINUM Technology by Computer Associates in 1999. VREAM co-founders Gaebler and LaHood left PLATINUM at that time and started a new venture, BeautyJunglecom, selling cosmetics online.\n\nThe closing down of PLATINUM's VREAM Division marked a turning point for the VR industry, which had experienced a flurry of interest, activity and investment during the nineties. Interest in VR would be mostly dormant for the next fifteen years, at which point virtual reality technology company Oculus VR would be acquired by Facebook for US$2 billion in cash and Facebook stock, shocking many VR veterans of the nineties and stimulating significant interest in virtual reality from a new generation of entrepreneurs.\n"}
{"id": "25851348", "url": "https://en.wikipedia.org/wiki?curid=25851348", "title": "Wall padding", "text": "Wall padding\n\nWall padding may go by a variety of names, including; Softwall, Wainscott, Cushion Wall, Safety Padding, and more.\n\nThe primary purpose of wall padding is to provide a safe level of absorption for an individual making impact with an established object. Often these objects are walls (hence “wall padding”), but also these pads commonly protect bleachers, stage fronts, I-Beams, goal posts, columns, and fences.\n\nTypical indoor wall panels are 24” wide x 72” high. In recent history however many newly constructed schools and universities have made a push for 84” high padding in their basketball facilities, where athletes are often elevated in motion. In grappling and wrestling facilities pads are regularly 60” since most movement occurs on the ground.\n\nOutdoor stadium padding is 48” wide x 96” high. However, when working with a skilled manufacturer all dimensions can often be easily customized to fit the unique needs of your facility.\n\nStandard quality indoor wall panel will begin with a 7/16” osb (oriented strand board) backer. To that, a layer of foam will be laminated. For indoor panels the standard thickness of foam is 2”, but which type 2” foam is laminated to the backer is the most important part of the pad (see foam). After these steps have been completed a 16 oz. vinyl cover is stapled over the face and onto the back of the pad.\n\nA basic stadium pad, will spend most of its life outdoors, and should always begin with a ½” to ¾” weather durable backer board. Most often plywood is used, but other experienced companies like Mancino Manufacturing use a higher grade backer called extira. To that, a layer of foam will be laminated. For outdoor stadium panels the standard thickness of foam is 3”. Foam should be a minimum 1.8 pound density, with a 50-pound IFD. After these steps a 16o oz. to 17 oz. fabric will be stapled over the face of the pad.\nFoam used in manufacturing wall padding is typically a polyurethane based foam, but can vary based on a specification written, or simply customer preference.\n\nVinyl range from 14 oz. to 18 oz. in weight, and typically has a light leather emboss pattern engrained into the good side of the material. There are at least 16 standard colors to choose from. Vinyl should also be expected to meet fire retardant test NFPA-701 and ASTM E84 \n\nThe most common installation methods are; standard method, nailing margin, and z-clip.\n\nStandard method is a process in which panels are toe nailed in with a finishing nailer. Most often this method is done by professionals. If the surface the pads are being installed on is capable of being nailed into, pads will often be mounted directly to that surface. However, if the surface is unable to be nailed into, like cinder block, then firing strip will be mounted first, and the pads mounted to them.\n\nNailing margin is a process in which the manufacturer will intentionally leave 1” of foam off the top and bottom of the panel. This 1” area is meant to allow the installer to drill directly through the panel, without damaging, or compromising the pad. If the surface the pads are being installed on is capable of being screwed into, pads will often be mounted directly to that surface. However, if the surface is unable to be drilled into, like cinder block, then firing strip will be mounted first, and the pads mounted to them.\n\nZ-clip is a process in which the panels are hung with male/female aluminum channeling. An installer will first mount the female side of the aluminum clip to the wall (typically in 6’ lengths). The installer will then mount the male side of the clip to the panel (typically 22” lengths). To hang the panels, the installer will drop the panel with the male clip into the female clip on the wall. These pads will appear the same as standard method pads on from head on, but can also be removed from the wall because of the male/female clip.\n\nGraphics art work is common upgrade that can be made to most any pad, in the manufacturing process.\n\nProtective Padding Manufacturing\n\nProtective Padding Manufacturing\n"}
{"id": "67488", "url": "https://en.wikipedia.org/wiki?curid=67488", "title": "Well drilling", "text": "Well drilling\n\nWell drilling is the process of drilling a hole in the ground for the extraction of a natural resource such as ground water, brine, natural gas, or petroleum, for the injection of a fluid from surface to a subsurface reservoir or for subsurface formations evaluation or monitoring. Drilling for the exploration of the nature of the material underground (for instance in search of metallic ore) is best described as \"borehole\" drilling.\n\nThe earliest wells were water wells, shallow pits dug by hand in regions where the water table approached the surface, usually with masonry or wooden walls lining the interior to prevent collapse. Modern drilling techniques utilize long drill shafts, producing holes much narrower and deeper than could be produced by digging.\n\nWell drilling can be done either manually or mechanically and the nature of required equipment varies from extremely simple and cheap to very sophisticated.\n\nManaged Pressure Drilling (MPD) is defined by the International Association of Drilling Contractors (IADC) as “an adaptive drilling process used to more precisely control the annular pressure profile throughout the wellbore.\" The objectives of MPD are “to ascertain the downhole pressure environment limits and to manage the annular hydraulic pressure profile accordingly.\"\n\nThe earliest record of well drilling dates from 347 AD in China. Petroleum was used in ancient China for \"lighting, as a lubricant for cart axles and the bearings of water-powered drop hammers, as a source of carbon for inksticks, and as a medical remedy for sores on humans and mange in animals.\" In ancient China, deep well drilling machines were in the forefront of brine well production by the 1st century BC. The ancient Chinese developed advanced sinking wells and were the first civilization to use a well-drilling machine and to use bamboo well casings to keep the holes open. \n\nIn the modern era, the first roller cone patent was for the rotary rock bit and was issued to American businessman and inventor Howard Hughes Sr. in 1909. It consisted of two interlocking cones. American businessman Walter Benona Sharp worked very closely with Hughes in developing the rock bit. The success of this bit led to the founding of the Sharp-Hughes Tool Company. In 1933 two Hughes engineers, one of whom was Ralph Neuhaus, invented the tricone bit, which has three cones. The Hughes patent for the tricone bit lasted until 1951, after which other companies made similar bits. However, Hughes still held 40% of the world's drill bit market in 2000. The superior wear performance of PDC bits gradually eroded the dominance of roller cone bits and early in this century PDC drill bit revenues overtook those of roller cone bits. The technology of both bit types has advanced significantly to provide improved durability and rate of penetration of the rock. This has been driven by the economics of the industry, and by the change from the empirical approach of Hughes in the 1930s, to modern day domain Finite Element codes for both the hydraulic and cutter placement software.\n\nThe factors effecting drill bit selection include the type of geology and the capabilities of the rig. Due to the high number of wells that have been drilled, information from an adjacent well is most often used to make the appropriate selection. Two different types of drill bits exist: fixed cutter and roller cone. A fixed cutter bit is one where there are no moving parts, but drilling occurs due to shearing, scraping or abrasion of the rock. Fixed cutter bits can be either polycrystalline diamond compact (PDC) or grit hotpressed inserts (GHI) or natural diamond. Roller cone bits can be either tungsten carbide inserts (TCI) for harder formations or milled tooth (MT) for softer rock. The manufacturing process and composites used in each type of drill bit make them ideal for specific drilling situations. Additional enhancements can be made to any bit to increase the effectiveness for almost any drilling situation.\n\nA major factor in drill bit selection is the type of formation to be drilled. The effectiveness of a drill bit varies by formation type. There are three types of formations: soft, medium and hard. A soft formation includes unconsolidated sands, clays, soft limestones, red beds and shale. Medium formations include calcites, dolomites, limestones, and hard shale. Hard formations include hard shale, calcites, mudstones, cherty lime stones and hard and abrasive formations.\n\nUntil 2006, market share was divided primarily among Hughes Christensen, Security-DBS (now Halliburton Drill Bits and Services), Smith Bits (a subsidiary of Schlumberger), and REEDHycalog (acquired by NOV in 2008).\n\nBy 2014, Ulterra (now a subsidiary of ESCO Corp.) and Varel International (now a subsidiary of Swedish engineering group Sandvik) had together gained nearly 30% of the U.S. bit market and eroded the historical dominance of the Smith, Halliburton, and Baker Hughes.\nEvaluation of the dull bit grading is done by a uniform system promoted by the International Association of Drilling Contractors (IADC). See Society of Petroleum Engineers / IADC Papers SPE 23938 & 23940. See also PDC Bits \n\n\n"}
{"id": "10420935", "url": "https://en.wikipedia.org/wiki?curid=10420935", "title": "Well kill", "text": "Well kill\n\nA well kill is the operation of placing a column of heavy fluid into a well bore in order to prevent the flow of reservoir fluids without the need for pressure control equipment at the surface. It works on the principle that the hydrostatic head of the \"kill fluid\" or \"kill mud\" will be enough to suppress the pressure of the formation fluids. Well kills may be planned in the case of advanced interventions such as workovers, or be contingency operations. The situation calling for a well kill will dictate the method taken.\n\nNot all well kills are deliberate. Sometimes, the unintended buildup of fluids, either from injection of chemicals like methanol from surface, or from liquids produced from the reservoir, can be enough to kill the well, particularly gas wells, which are notoriously easy to kill.\n\nWell control in general is an extremely expensive and dangerous operation. Extensive training, testing, proof of competence, and experience are prerequisites for planning and performing a well kill, even a seemingly simple one. Many people have died through incorrectly performed well kills.\n\nThe principle of a well kill revolves around the weight of a column of fluid and hence the pressure exerted at the bottom.\n\nformula_1\n\nWhere P is the pressure at depth h in the column, g is the acceleration of gravity and ρ is the density of the fluid. It is common in the oil industry to use weight density, which is the product of mass density and the acceleration of gravity. This reduces the equation to:\n\nformula_2\n\nWhere γ is the weight density. Weight density may also be described as the pressure gradient because it directly determines how much extra pressure will be added by increasing depth of the column of fluid.\n\nThe objective in a well kill, is to make the pressure at the bottom of the kill fluid equal (or slightly greater) than the pressure of the reservoir fluids.\n\nThe pressure of the reservoir fluids at the bottom of the hole is 38MPa. We have a kill fluid with a weight density of 16kN.m. What will need to be the height of the hydrostatic head in order to kill the well?\n\nFrom the equation:\nformula_3\n\nformula_4\n\nformula_5\n\nTherefore, a column of 2375m of this fluid is needed. This refers to the true vertical depth of the column, not the measured depth, which is always larger than true vertical depth due to deviations from vertical.\n\nIn the oil industry, a pure SI system is extremely rare. Weight densities are commonly either given as specific gravity or in pounds per gallon. Simple conversion factors (0.433 for specific gravity and 0.052 for ppg) convert these values to a pressure gradient in psi per foot. Multiplying by the depth in feet gives the pressure at the bottom of the column.\n\nOf course, when the well is being drilled in metres as the depth unit, the maths gets more complicated. Since well-kill certification is normally (in the US/ UK) done in \"oil field units\" (feet for length, inches for diameters, oilfield barrels for volume-pumped, psi for pressures), complex workarounds are often performed to keep the planned calculations in line with local regulations and industry \"best practice\".\n\nDuring all well kills, careful attention must be paid to not exceeding the formation strength at the weakest point of the wellbore (or casing/ liner pipes, as appropriate), the \"fracture pressure\", otherwise fluid will be lost from the wellbore to the formation. Since this lost volume is unknown, it becomes very hard to tell how the kill is proceeding, especially if gas is involved with its large volume change through different parts of the wellbore. Combining a well kill with such a \"lost circulation\" situation is a serious problem.\n\nLost circulation situations can, of course, also lead to well kill situations.\n\nThis is often the tidiest way of making a planned well kill. It involves pumping kill fluid down the 'A' annulus of the well, through a point of communication between it and the production tubing just above the production packer and up the tubing, displacing the lighter well bore fluids, which are allowed to flow to production.\n\nThe point of communication was traditionally a device called a sliding sleeve, or sliding side door, which is a hydraulically operated device, built into the production tubing. During normal operation, it would remain closed sealing off the tubing and the annulus, but for events such as this, it would be opened to allow the free flow of fluids between the two regions. These components have fallen out of favour as they were prone to leaking. Instead, it is now more common to punch a hole in the tubing for circulation kills. Although this permanently damages the tubing, given that most planned well kills are for workovers, this is not an issue, since the tubing is being pulled for replacement anyway.\n\nThis is the most common method of a contingency well kill. If there is a sudden need to kill a well quickly, without the time for rigging up for circulation, the more blunt instrument of bullheading may be used. This involves simply pumping the kill fluid directly down the well bore, forcing the well bore fluids back into the reservoir. This can be effective at achieving the central aim of a well kill; building up a sufficient hydrostatic head in the well bore. However, it can be limited by the burst-pressure capabilities of the tubing or casing, and can risk damaging the reservoir by forcing undesired materials into it. The principal advantage is that it can be done with little advanced planning.\n\nThis is similar to reverse circulation, except the kill fluid is pumped into the production tubing and circulated out through the annulus. Though effective, it is not as desirable since it is preferred that the well bore fluids be displaced out to production, rather than the annulus.\n\nThis is the most time consuming form of well kill. It involves repeatedly pumping in small quantities of kill mud into the well bore and then bleeding off excess pressure. It works on the principle that the heavier kill mud will sink below the lighter well bore fluids and so bleeding off the pressure will remove the latter leaving an increasing quantity of kill mud in the well bore with successive steps.\n\nDuring drilling, pressure control is maintained through the use of precisely concocted drilling fluid, which balances out the pressure at the bottom of the hole. In the event of suddenly encountering a high pressure pocket, pressure due to drilling fluid may not be able to counter the high formation pressure. Allowing formation fluid to enter into the well-bore. This influx of formation fluid is called kick and then it becomes necessary to kill the well. This is done by pumping kill mud down the drill pipe, where it circulates out the bottom and into the well bore.\n\nThe intention of a well kill (or the reality of an unintentional well kill) is to stop reservoir fluids flowing to surface. This of course creates problems when it is desirable to get the well flowing again. In order to reverse the well kill, the kill fluid must be displaced from the well bore. This involves injecting a gas at high pressure, usually nitrogen since it is inert and relatively cheap. A gas can be put under sufficient pressure to allow it to push heavy kill fluid, but will then expand and become light once pressure is removed. This means that having displaced the kill fluid, it will not itself kill the well. Low density (\"light\") liquids such as diesel fuel, or the \"base fluid\" for a \"(synthetic) oil-based mud\" can also be used, depending on availability and pressure-management issues for a specific well. The reservoir fluids should be able to flow to surface, displacing the gas.\n\nThe cheapest way to do it is similar to bullheading, where the light fluid (nitrogen, or low density liquid) is pumped in under high pressure to force the kill fluid into the reservoir. This, of course, runs a high risk of causing well damage. The most effective way is to use coiled tubing, pumping the gas/diesel down the coil and circulating out the bottom into the well bore, where it will displace the kill mud to production. (Of course, getting a coiled tubing spread to the location may take weeks of work and logistics.)\n\n"}
{"id": "265551", "url": "https://en.wikipedia.org/wiki?curid=265551", "title": "X-ray generator", "text": "X-ray generator\n\nAn X-ray generator is a device that produces X-rays. Together with an X-ray detector, it is commonly used in a variety of applications including medicine, fluorescence, electronic assembly inspection, and measurement of material thickness in manufacturing operations. In medical applications, X-ray generators are used by radiographers to acquire x-ray images of the internal structures (e.g., bones) of living organisms, and also in sterilization.\n\nAn X-ray generator generally contains an X-ray tube to produce the X-rays. Possibly, radioisotopes can also be used to generate X-rays.\n\nAn X-ray tube is a simple vacuum tube that contains a cathode, which directs a stream of electrons into a vacuum, and an anode, which collects the electrons and is made of tungsten to evacuate the heat generated by the collision. When the electrons collide with the target, about 1% of the resulting energy is emitted as X-rays, with the remaining 99% released as heat. Due to the high energy of the electrons that reach relativistic speeds the target is usually made of tungsten even if other material can be used particularly in XRF applications.\n\nAn X-ray generator also needs to contain a cooling system to cool the anode; many X-ray generators use water or oil recirculating systems.\n\nIn medical imaging applications, an x-ray machine has a control console that is used by a radiologic technologist to select x-ray attributes suitable for the specific exam, a power supply that creates and produces the desired kVp (peak kilovoltage), mA (milliamperes, sometimes referred to as mAs which is actually mA multiplied by the desired exposure length) for the x-ray tube, and the x-ray tube itself.\n\nThe discovery of X-rays came from experimenting with Crookes tubes, an early experimental electrical discharge tube invented by English physicist William Crookes around 1869-1875. In 1895, Wilhelm Röntgen discovered X-rays emanating from Crookes tubes and the many uses for X-rays were immediately apparent. One of the first X-ray photographs was made of the hand of Röntgen's wife. The image displayed both her wedding ring and bones. On January 18, 1896 an \"X-ray machine\" was formally displayed by Henry Louis Smith. A fully functioning unit was introduced to the public at the 1904 World's Fair by Clarence Dally.\n\nIn the 1940s and 1950s, X-ray machines were used in stores to help sell footwear. These were known as Shoe-fitting fluoroscopes. However, as the harmful effects of X-ray radiation were properly considered, they finally fell out of use. Shoe-fitting use of the device was first banned by the state of Pennsylvania in 1957. (They were more a clever marketing tool to attract customers, rather than a fitting aid.) Together with Robert J. Van de Graaff, John G. Trump developed one of the first million-volt X-ray generators.\n\nAn X-ray imaging system consists of a generator control console where the operator selects desired techniques to obtain a quality readable image(kVp, mA and exposure time), an x-ray generator which controls the x-ray tube current, x-ray tube kilovoltage and x-ray emitting exposure time, an X-ray tube that converts the kilovoltage and mA into actual x-rays and an image detection system which can be either a film (analog technology) or a digital capture system and a PACS.\n\nX-ray machines are used in health care for visualising bone structures, during surgeries (especially orthopedic) to assist surgeons in reattaching broken bones with screws or structural plates, assisting cardiologists in locating blocked arteries and guiding stent placements or performing angioplasties and for other dense tissues such as tumours. Non-medicinal applications include security and material analysis.\n\nThe main fields in which x-ray machines are used in medicine are radiography, radiotherapy, and fluoroscopic type procedures.\n\nRadiography is generally used for fast, highly penetrating images, and is usually used in areas with a high bone content but can also be used to look for tumors such as with mammography imaging. Some forms of radiography include:\n\nRadiotherapy — the use of x-ray radiation to treat malignant and benign cancer cells, a non-imaging application\n\nFluoroscopy is used in cases where real-time visualization is necessary (and is most commonly encountered in everyday life at airport security). Some medical applications of fluoroscopy include:\n\nX-rays are highly penetrating, ionizing radiation, therefore X-ray machines are used to take pictures of dense tissues such as bones and teeth. This is because bones absorb the radiation more than the less dense soft tissue. X-rays from a source pass through the body and onto a photographic cassette. Areas where radiation is absorbed show up as lighter shades of grey (closer to white). This can be used to diagnose broken or fractured bones. In fluoroscopy, imaging of the digestive tract is done with the help of a radiocontrast agent such as barium sulfate, which is opaque to X-rays.\n\nX-ray machines are used to screen objects non-invasively. Luggage at airports and student baggage at some schools are examined for possible weapons, including bombs. Prices of these Luggage X-rays vary from $50,000 to $300,000. The main parts of an X-ray Baggage Inspection System are the generator used to generate x-rays, the detector to detect radiation after passing through the baggage, signal processor unit (usually a PC) to process the incoming signal from the detector, and a conveyor system for moving baggage into the system. Portable pulsed X-ray Battery Powered X-ray Generator used in Security as shown in the figure provides EOD responders safer analysis of any possible target hazard.\n\nWhen baggage is placed on the conveyor, it is moved into the machine by the operator. There is an infrared transmitter and receiver assembly to detect the baggage when it enters the tunnel. This assembly gives the signal to switch on the generator and signal processing system. The signal processing system processes incoming signals from the detector and reproduce an image based upon the type of material and material density inside the baggage. This image is then sent to the display unit.\n\nThe colour of the image displayed depends upon the material and material density : organic material such as paper, clothes and most explosives are displayed in orange. Mixed materials such as aluminum are displayed in green. Inorganic materials such as copper are displayed in blue and non-penetrable items are displayed in black (some machines display this as a yellowish green or red). The darkness of the color depends upon the density or thickness of the material.\n\nThe material density determination is achieved by two-layer detector. The layers of the detector pixels are separated with a strip of metal. The metal absorbs soft rays, letting the shorter, more penetrating wavelengths through to the bottom layer of detectors, turning the detector to a crude two-band spectrometer.\n\nA film of carbon nanotubes (as a cathode) that emits electrons at room temperature when exposed to an electrical field has been fashioned into an X-ray device. An array of these emitters can be placed around a target item to be scanned and the images from each emitter can be assembled by computer software to provide a 3-dimensional image of the target in a fraction of the time it takes using a conventional X-ray device. The system also allows rapid, precise control, enabling prospective physiological gated imaging.\n\nEngineers at the University of Missouri (MU), Columbia, have invented a compact source of x-rays and other forms of radiation.\nThe radiation source is the size of a stick of gum and could be used to create portable x-ray scanners. A prototype handheld x-ray scanner using the source could be manufactured in as soon as three years.\n\n\n"}
