{"id": "44056879", "url": "https://en.wikipedia.org/wiki?curid=44056879", "title": "ABV Reactor", "text": "ABV Reactor\n\nABV Reactor is a design for a pressurized water reactor being developed by OKBM Afrikantov and is designed to produce an output of 3 to 10 MWe based on its variants (ABV-3, ABV-6 and ABV-6M).\n"}
{"id": "40215556", "url": "https://en.wikipedia.org/wiki?curid=40215556", "title": "Aethalometer", "text": "Aethalometer\n\nAn aethalometer is an instrument for measuring the concentration of optically absorbing (‘black’) suspended particulates in a gas colloid stream; commonly visualized as smoke or haze, often seen in ambient air under polluted conditions. The word aethalometer is derived from the Classical Greek verb ‘aethaloun’, meaning ‘to blacken with soot’.\n\nThe gas stream (frequently ambient air) passes through a filter material which traps the suspended particulates, creating a deposit of increasing density. A light beam projected through the deposit is attenuated by those particles which are absorbing (‘black’) rather than scattering (‘white’). Measurements are made at successive regular time intervals. The increase in attenuation from one measurement to the next is proportional to the increase in the density of optically absorbing material on the filter: which, in turn, is proportional to the concentration of the material in the sampled air stream. The sample is collected as a spot on a roll of filter tape. When the density of the deposit spot reaches a pre-set limit, the tape advances to a fresh spot and the measurements continue. Measurement of the sample gas flow rate and knowledge of the instrument’s optical and mechanical characteristics permit a calculation of the average concentration of absorbing particles in the gas stream during the sampling period. Aethalometers may operate on time-base periods as rapid as 1 second, providing quasi-real-time data. Comparison of aethalometer data with other physical and chemical analyses allows the output to be expressed as a concentration of black carbon.\n\nThe Aethalometer principle is based upon the continuous filter-tape sampler developed in the 1950s for the measurement of coefficient of haze. This instrument drew the sample air stream through a filter tape spot for a fixed time duration (usually 1 or 2 hours). The tape was advanced and its gray coloration measured optically by either transmittance or reflectance. However, the data units were arbitrary, and were not interpreted in terms of a mass concentration of a defined material in the air stream until retrospective studies linked the ‘COH unit’ to quantitative analyses of atmospheric trace constituents.\n\nWork in the 1970s at Tihomir Novakov's lab at the Lawrence Berkeley National Laboratory established the quantitative relationship between the optical attenuation of a deposit of particles on a fibrous filter, and the carbon content of that deposit. Improvements in optical and electronic technology permitted the measurement of very small increases in attenuation, such as would occur during the passage of typical ambient air through a filter on a 5- or 10-minute timebase. The development of personal computers and analog-digital interfaces permitted the real-time calculation of data, and mathematical conversion of the signals to a concentration of black carbon expressed in units of nanograms or micrograms of Black Carbon per cubic meter of air.\n\nThe first-ever Aethalometer was developed at the Lawrence Berkeley National Laboratory by Anthony D. A. Hansen (who would later found Magee Scientific), Hal Rosen and Tihomir Novakov, and was utilised in an EPA visibility study at Houston in September 1980, with the first real-time data chart of black carbon concentrations in ambient air published in 1981. The instrument was first flown on board a NOAA research aircraft in the Arctic in 1984, and coupled with previous ground-level work showed that the Arctic haze contains a strong component of soot.\n\nThe aethalometer was commercialized in 1986 and an improved version patented in 1988. Its earliest uses were in geophysical research at remote locations, using black carbon as a tracer of the long-range transport of air pollution from industrialized source areas to remote receptor regions. In the 1990s, increasing concerns about the health effects of diesel exhaust particulates led to increasing need for measurements using the blackness of the carbon content as an indicator. In the 2000s, increasing interest in the role that optically absorbing particles play in climate change led to expanded measurement programs in both developed and developing countries. The effect of these particles is believed to contribute to the accelerated melting of the Arctic and the thawing of glaciers in the Himalayas.\n\nA comprehensive summary of black carbon (including a review of aethalometer data) was submitted to the U.S. Congress by the U.S. Environmental Protection Agency in 2012.\n\nThe Aethalometer has been developed into rack-mounted instruments for use in stationary air quality monitoring installations; transportable instruments which are often used at off-grid locations, operating from batteries or photovoltaic panels in order to make measurements at remote locations; and hand-held portable versions for measurements of personal exposure to combustion emissions.\n\nThe main uses of aethalometers relate to air quality measurements, with the data being used for studies of the impact of air pollution on public health; climate change; and visibility. Other uses include measurements of the emission of black carbon from combustion sources such as vehicles; industrial processes; and biomass burning, both in wild fires and in domestic and industrial settings.\n\nThe aethalometer model AE-31 was tested by the Environmental Technology Verification Program administered by the U.S. Environmental Protection Agency, and a validation report was issued in 2001. The Aethalometer Model AE-33 was tested under the same program in 2013, report pending.\n\nThe pollutant species \"black carbon\" appears gray or black due to the absorption of electromagnetic energy by partially mobile electrons in the graphitic microstructure of the black carbon particles. This absorption is purely ‘resistive’ and displays no resonant bands: consequently, the material appears gray rather than colored. The attenuation of light transmitted through a deposit of these particles increases linearly with the frequency of the electromagnetic radiation, i.e. inversely with respect to wavelength. Aethalometer measurements of optical attenuation on a filter deposit will increase at shorter wavelengths as λ where the parameter α (the Angstrom exponent) has the value α = 1 for ‘gray’ or ‘black’ materials. However, other species may be co-mingled with the black carbon particles. Aromatic organic compounds associated with tobacco smoke and biomass smoke from wood-burning are known to have increased optical absorption at shorter wavelengths in the yellow, blue and near-ultraviolet portions of the spectrum.\n\nAethalometers are now constructed to perform their optical analyses simultaneously at multiple wavelengths, typically spanning the range from 370 nm (near-ultraviolet) to 950 nm (near-infrared). In the absence of aromatic components, the aethalometer data for black carbon concentration is identical at all wavelengths, after factoring in the standard λ response for ‘resistive’ gray materials. The angstrom exponent of the attenuation for these materials is 1. If aromatic components are present, they will contribute increased absorption at shorter wavelengths. The aethalometer data will increase at shorter wavelengths, and the apparent angstrom exponent will increase. Measurements of pure biomass smoke may show data represented by an angstrom exponent as large as 2. Due to different artifacts, the angstrom exponent measured by aethalometers might be biased but comparison with other techniques have found that the aethalometer model AE-31 provides fair absorption angstrom exponent results. Many areas of the world are impacted by emissions both from high-temperature fossil fuel combustion, such as diesel exhaust, which has a gray or black color and is characterized by an angstrom exponent of 1; together with emissions from biomass burning such as wood smoke, which is characterized by a larger value of angstrom exponent. These two sources of pollution may have different geographic origins and temporal patterns, but maybe co-mingled at the point of measurement. Real-time aethalometer measurements at multiple wavelengths are claimed to separate these different contributions and can apportion the total impact to different categories of sources. This analysis is an essential input to the design of effective and acceptable public policy and regulation.\n\nThe accuracy, and even the ability, of the Aethalometer to differentiate smoke sources is disputed\n\nThe aethalometer measurement principle is based upon air filtration, optics, and electronics. It does not require any physical or chemical support infrastructure such as high vacuum, high temperature, or specialized reagents or gases. Its only consumable is a filter which needs to be replaced every one or two days in portable models, but larger units have a roll of filtration tape which usually lasts from months to years. Consequently, the instrument is rugged, miniaturizable and may be deployed in research projects at remote locations, or at sites with minimal local support. Examples include: \n"}
{"id": "28156448", "url": "https://en.wikipedia.org/wiki?curid=28156448", "title": "AlertMe", "text": "AlertMe\n\nAlertMe was a UK smart Tech company that provides energy and home monitoring hardware and services. AlertMe produces hardware and software to enable users to monitor and control their home energy use.\n\nIn 2015 the company was acquired by British Gas Trading Limited for £65 million. In April 2016 the limited company was renamed to Centrica Connected Home Limited.\n\nAlertMe uses Non Intrusive Load Monitoring to extrapolate trends from customer data, which is then used to make energy saving recommendations.\n\nThe AlertMe platform is based on a home hub that connects to cloud servers provided by Amazon Web Services via the home broadband router. The hub communicates via wireless to devices in the home, both AlertMe devices and third party enabled devices.\n\nThe AlertMe platform is open and expandable to allow users to add new devices and applications to their personal dashboard or smartphone app. Applications such as electricity monitoring (SmartEnergy), remote heating control (SmartHeating) and home monitoring (SmartMonitoring) are the core applications, but with the development of many new third party devices in the future the platform can adopt further applications such as lighting controls and automation, locks and keypads as well as smart appliances when they become available.\n\nAlertMe has strategic partnerships with British Gas in the UK and Lowe's in the USA.\nBritish Gas is the largest domestic energy provider in the UK serving 10 million homes and 15.9 million energy accounts.\nLowe's is the second largest home improvement retailer in the world with 15 million consumers visiting its 1725 stores every week.\n\nAlertMe has won eight awards since 2009 for product innovation and business strategy, including:\n\nRed Herring Europe 100 2012\n\nSmart Metering UK & Europe Best In Home Display 2012\n\nRosenblatt New Energy Award for Best Deal 2011\n\nIET Innovation Award for Software in Design 2010\n\nGlobal Cleantech 100 2009 & 2010\n\nBloomberg New Energy Pioneer 2010\n\nDesign Week Consumer Product Design 2009\n\n\n"}
{"id": "1414984", "url": "https://en.wikipedia.org/wiki?curid=1414984", "title": "Attitude and heading reference system", "text": "Attitude and heading reference system\n\nAn attitude and heading reference system (AHRS) consists of sensors on three axis that provide attitude information for aircraft, including roll, pitch and yaw. These are sometimes referred to as MARG (Magnetic, Angular Rate, and Gravity) sensors and consist of either solid-state or microelectromechanical systems (MEMS) gyroscopes, accelerometers and magnetometers. They are designed to replace traditional mechanical gyroscopic flight instruments. \n\nThe key difference between an inertial measurement unit (IMU) and an AHRS is the addition of an on-board processing system in an AHRS which provides attitude and heading information versus an IMU which just delivers sensor data to an additional device that computes attitude and heading. \nIn addition to attitude determination an AHRS may also form part of an inertial navigation system.\n\nA form of non-linear estimation such as an Extended Kalman filter is typically used to compute the solution from these multiple sources.\n\nAHRS have proven themselves to be highly reliable and are in common use in commercial and business aircraft. AHRS are typically integrated with electronic flight instrument systems (EFIS) which are the central part of so-called glass cockpits, to form the primary flight display. AHRS can be combined with air data computers to form an \"air data, attitude and heading reference system\" (ADAHRS), which provide additional information such as airspeed, altitude and outside air temperature.\n\n"}
{"id": "13178513", "url": "https://en.wikipedia.org/wiki?curid=13178513", "title": "Audio boot", "text": "Audio boot\n\nAn audio boot or audio shoe is an electronic device used by the hard of hearing with hearing aids; hearing aids often come with a special set of metal contacts for audio input. Typically the audio boot will fit around the end of the hearing aid (a behind-the-ear model, as in-the-ear do not afford any purchase for the connection) link these with another device, like an FM system or a cellphone or even a digital audio player.\n\n"}
{"id": "55827818", "url": "https://en.wikipedia.org/wiki?curid=55827818", "title": "Awok", "text": "Awok\n\nAwok is an e-commerce company headquartered in Dubai, U.A.E. The company sells items such as fashion accessories, kitchenware, home appliances, mobiles, cameras, as well as health and beauty products.\n\nAwok was founded in 2013 by Ulugbek Yuldashev. The company is headquartered in Dubai, U.A.E.\n\n"}
{"id": "10233558", "url": "https://en.wikipedia.org/wiki?curid=10233558", "title": "Biostar", "text": "Biostar\n\nBiostar Microtech International Corp (Biostar; ) is a Taiwanese company which designs and manufactures computer hardware products such as motherboards, video cards, expansion cards, thermal grease, headphones, home theater PCs, remote controls, desktops, barebone computers, system-on-chip solutions and industrial PCs.\n\nAwarded Taiwan's Top 20 Global Brand in 2008, Biostar, with an estimated brand value of US$46 million, was ranked #1 as the top motherboard brand for iCafe in China.\n\nBiostar is an independent company listed on the main floor of Taiwan Stock Market, stock ID number . Biostar caters to the entry-level through mainstream to high-end markets.\n\nThe company was founded in 1986, manufacturing XT form factor mainboards and in later years add-on cards. In 1999 Biostar was listed on the Taiwan Stock Exchange & also certified ISO 9001 standard within the same year. Biostar has shifted from the SI/OEM market to the channel market with the portion of branded products closer to 100 percent.\n\nOn 1 August 2004, having already had a successful collaboration of Nvidia nForce based motherboards, Biostar announced it was to become a first tier partner with Nvidia of graphics solutions.\n\nBiostar was the first manufacturer to launch a motherboard with built-in Wireless LAN back in May 2003, the P4TCA, which is based on at the time Intel’s flagship i875P “Canterwood” chipset. Biostar introduced the first AM2/AM2+ motherboard in the world (TF560 A2+) in June 2007, which was able to host Socket AM2 Athlon 64 and AM2+ AMD Phenom processors.\n\nBiostar was awarded “Top 20 Taiwan Global Brand” in 2008 conducted by Taiwan External Trade Development Council (TAITRA) with an estimate brand value of US$46 million. Only three companies had a chance to crack the Top 20 list for the first time – and Biostar was among them.\n\nBiostar was also the first manufacturer in the world to launch a motherboard readily available with integrated USB 3.1 in February 2015, the Gaming Z97X, which is based on Intel's Z97 “Wildcat Point” chipset.\n\nBiostar has the distinction of being the first to allow end-users to modify voltages and frequencies of the video card's GPU and memory to boost performance to extreme limits (overclocking). Those cards are either called \"V-Ranger\" or \"V-Series\".\n\nTheir mainboards, notably the \"T-Power\" and \"T-Series\", have been widely reviewed to be highly overclockable achieving world record FSB overclocks.\n\nBiostar is located in New Taipei City, Taiwan and is represented in many continents throughout the world through its 5 regional headquarters.\n\n"}
{"id": "55609116", "url": "https://en.wikipedia.org/wiki?curid=55609116", "title": "British investment in Argentina", "text": "British investment in Argentina\n\nArgentina had a unique relationship with Great Britain during the time period after Argentinian independence all the way up to World War 2. British Investment would begin in the 1820s, with investments into industries such as mining, immigration, and agriculture. How ever this investment would be worthless by the 1930s and would not prove to be profitable until the 1950s and on. It would not be until the 1960s that there would be any amount of investment that would be continuous enough to be impactful on either the Argentinian or the British economy. Early investment failed in Argentina because of multiple factors, such as war with Brazil, an overestimated labour force that did not exist after the revolution, and the lack of resources being able to flow out of the Rio de la Plate because of a wartime blockade. These economic inhibitors were followed by the \"cattle baron\", Juan Manuel de Rosas's caudillo-ship which also deterred foreign investment, and then the defaulting of payment towards loans from the time of revolution. All of these factors made Argentina appear unstable and therefore diminished foreign investors willingness to invest in the new state. Therefore, industrialization of Argentina and its vast pampas (plains) would have to wait until the industrializing capital of Great Britain would be imported decades later.\n\nBernardino Rivadavia was the First President of Argentina, from 1926-27. Rivadavia wanted to open Argentina to liberal policies that he thought would increase wealth and industrialization inside of Argentina. Some examples of how he wanted to accomplish this was by allowing free trade and lowering tariffs by replacing them with taxes on the selling and renting of the plentiful reserves of land which Argentina had. Rivadavia wanted to make Argentinian products desirable on foreign markets by lowering tariffs, whilst also selling off Argentinian land to foreign investors who would increase the productivity of the land as liberal policies at the time claimed to do. Another way Rivadavia tried to secure foreign capital was by encouraging the development of businesses that focused on agriculture and immigration. Rivadavia created policies that could have been beneficial to the development of Argentina, but since the Cisplatine War between Argentina and Brazil took place during these reforms, the benefits were diminished and never experienced their full effects. During the Cisplatine War, the Brazilian Navy had blockaded the Rio de la Plata, which meant that trade could now flow freely between Argentina or Great Britain, thus harming the new economic ties.\n\nWith the end of Juan Manuel de Rosas dictatorship in 1952, and followed by liberal government, investment to Argentina was increasing. A result of this investment was the building of infrastructure was built to enable international trade. As development of industry grew, and the economy of Argentina strengthened, the quality of life for Argentineans ended up being among the highest in the world. Argentineans towards the end of the 20th century were well-fed, well-educated, and industrious compared to most of the world.\n\nTowards the 1870s, it was clear that the British were extremely influential in the economics of Argentina. Therefore, this made a very unique power dynamic between the two sovereign states of Great Britain and Argentina. Some scholars have even argued that Argentina was a product of British imperialism. A. G. Hopkins explains that for imperialism to occur, the sovereignty of one state is being diminished by the state with more structural power. Susan Strange, an English Political Scientist, identified four main traits of structural power in her study: States and Markets. One of these four equal parts of structural control is \"control over credit.\" For a developing nation it is clear that a free line of credit is essential to nation-building. Capital is needed to build industries, and the Argentinians lost their line of credit when they declared independence from Bourbon Spain. Argentina as well as most of Latin America lacked the domestic capital to rebuild after the revolutions seeing as revolutions are generally violent and consequently destroyed infrastructure. Therefore, the City of London stepped in and funded the capital where Argentina could not. In return for this foreign capital, the Argentinians would make sure that the policies and regime was to stay stable and keep the interests of the investors in mind.\n\nAn essential component of the investment and development of the Argentine economy was the importation of a modern railway system. The British were responsible for the creation of the Argentine railway system. From 1860 there were 39 kilometers of railways in Argentina, and by 1910 there were 23,994 kilometers. At the beginning of the twentieth century the Argentinian railway system was the 10th largest in the world, most of it being a product of English capital financing the new projects. By 1937 there were around 40,000 kilometers of railways, of which 66% was British owned which was unheard of in an independent country at this time. Since Argentina at this time did not have a well developed steel industry, the Argentinean railway projects needed to be funded by English capital since capital was limited domestically in Argentina. As well, the technology of the railway system needed to be imported into Argentina from Europe or the USA.\n\nThe purpose of the Argentine railway system was not built to transport people, but in fact to ship the agrarian products of the pampas (plains). Since Argentina was developing as an agrarian export economy, railroads were built to connect the rural farmland to the main ports of Argentina, as seen in the map of Argentinean Railways in 1910-1911. Most of the lines centre around the cities like Buenos Aires or Bahia Blanca and then branch out to the pampas in order to retrieve goods to bring back and ship out to European markets, mainly the British. The British would send in industrial goods to make Argentinean agriculture more modern and productive, while the Argentines would export those primary products back to Great Britain.\n\nWhen a military coup occurred in 1943 in Argentina, and then with the populist president Juan Perón succeeding to power, investment in Argentina was looking less profitable for investment. Especially when Perón nationalized the Argentinian railway system in 1948 which still was mainly owned by the British at the time. The military dictators of Latin America in the 20th century were keen to practicing Import Substitution Industry, which meant isolating the Latin American economies from the economies of western European and North America. The method attempted to discourage the buying of finished goods from other countries and instead produce said goods domestically. Therefore, in the latter half of the twentieth century, dictators like Perón nationalized industries which would lead to less foreign investment, in hopes to domestically improve industrialization without foreign capital.\n"}
{"id": "28300791", "url": "https://en.wikipedia.org/wiki?curid=28300791", "title": "Cité du Multimédia", "text": "Cité du Multimédia\n\nThe Cité du Multimédia is a neighborhood in Montreal, Quebec, Canada, located between Old Montreal, Griffintown and Downtown Montreal. The neighborhood is the result of a vast real-estate project launched by the Quebec government in the late 1990s which redeveloped abandoned nineteenth century industrial buildings into a business cluster for information technology companies.\n\nGroupe Cardinal Hardy and Groupe Provencher & Roy architectes collaborated on the district's urban design.\n\nThe Société de Développement de Montréal possessed land holdings dating back to the Doré administration's failed Quartier des Récluses initiative, in which lands were bought by the SIMPA to make way for the project. In exchange for handing this over, the SDM gained a minority stake in a joint venture with the real estate arms of the Caisse de dépôt et placement du Québec (CDPQ) and the FTQ Fonds de Solidarité to fund the construction of the Cité, which occurred in eight phases, with additional phases having been planned. The Parti Québécois Bouchard Government of Québec then gave payroll tax credits to employers for moving to the new Cité du Multimédia buildings.\n\nAfter the bursting of the dot-com bubble, and the elimination of tax incentives for information technology jobs in the district by the Liberal government of Jean Charest in 2003, phase 9 and others were cancelled, and the structures were sold by the joint venture to the private sector.\n\nMany high-tech companies are still located in the area, while phase 8 is occupied by the municipal government. The property values in this part of Montreal are very high. The neighbourhood has 6,000 workers, and an average salary of $73,000 per year. This figure is almost 25% above Montréal's average.\n\nThen-finance minister Bernard Landry had been criticized by members of Montreal's real-estate community and some high-tech entrepreneurs when Finance Ministry programs enticed companies to relocate to Cité Multimédia and nearby Cité du commerce électronique in order to receive tax assistance. In response, Landry announced the creation of CDTIs located in mid-sized cities throughout Quebec, and later the installation of technology clusters in rural areas, such as the Technoparc Rolland in Sainte-Adèle, Quebec.\n\nThe Cité du Multimédia was also put under investigation in the context of the Charbonneau commission inquiries.\n\n"}
{"id": "1589838", "url": "https://en.wikipedia.org/wiki?curid=1589838", "title": "Cobas Mira", "text": "Cobas Mira\n\nThe Roche Cobas Mira, now discontinued by Roche, is a benchtop, random access biochemistry analyzer. This system allows for the selective analysis of chemistries in either a routine or STAT mode. Testing sequence is by patient rather than test (batch).\nThe optional ISE module for sodium, potassium and chloride determinations expand the capabilities of the instrument.\n\nThe Roche Cobas Mira was once an ideal instrument in the laboratory situation where patient samples arrive throughout the day and night, however since its discontinuation spare parts are becoming scarce. With random access instrumentation, these samples may be placed on the analyzer and results will be available within minutes. With random access, samples do not have to be saved for a major run as with batch analyzers. STAT samples may be placed on the instrument at any time. Once the STAT has been completed, the Cobas Mira automatically returns to processing the routine samples. Communication with the Roche Cobas Mira is done via the integrated control panel. Through the control panel the operator selects tests, reviews results and programs tests, profiles and system parameters.\n\nOther features of the Roche Cobas Mira include; a throughput of 140 tests/hr, microprocessor controlled XYZ pipetting system, 104 test channels including 23 preprogrammed methods and the capacity to hold 2100 patient results, capacity for 72 cuvettes (six 12 cuvette segments), cuvette volume of 150-600µL, and an analysis interval of 25 seconds. The system also includes a security system for laboratory-defined parameters.\n\nThe instrument checks for the integrity of the cuvette, the rack reader, temperature, the monitor and printer.\n\nThe Cobas Mira (classic) was introduced in the mid 80's, followed by the Mira S and finally the Mira Plus in the early 90's. Bi-directional communication was possible through an interface to a computer. Data from the instrument can be captured using HyperTerminal but must be decoded to be fully comprehensible.\nAll Cobas Miras came with the following wavelength filters: 340 nm, 405 nm, 500 nm, 550 nm, and 600 nm, with space for three more for a total of eight filters.\n\n"}
{"id": "17442229", "url": "https://en.wikipedia.org/wiki?curid=17442229", "title": "Creative Partnerships", "text": "Creative Partnerships\n\nCreative Partnerships was the UK government's flagship creative learning programme, established in 2002 as part of the Council's SR2000 settlement to develop young people's creativity through artists' engagement with schools in nominated areas across England. Following the 2010 election of the coalition government, funding was cut by the Department for Culture, Media and Sport and Arts Council England in 2011, with activity in schools ending in summer 2011.\n\nIt aimed to build sustainable learning partnerships between schools, creative and cultural organisations and individuals.\n\nCreative Partnerships facilitated many long-term links between schools and creative professionals, such as artists, architects, scientists and multimedia developers. Research reports covering many different aspects of the programme, conducted by researchers from a number of universities and consultancies, are available online. Reports include literature reviews about creativity and education, teacher identity, pupil wellbeing and tracking progression in creativity; evaluations and surveys of the programme; and qualitative research investigating pedagogy, wellbeing and progression. https://archive.is/20131017205459/http://www.creativitycultureeducation.org/tag/research\n\nBetween 2002 - 2009, local Arts Council offices delivered the programme in 36 areas of England. Peter Jenkinson OBE was the first national director of the programme and he was succeeded by Paul Collard. \nBetween 2009 - 2011, responsibility for Creative Partnership was transferred to Creativity, Culture and Education. Creativity, Culture and Education, chaired by Paul Roberts with Paul Collard as CEO, funded 25 organisations to deliver Creative Partnerships in their local area. These were a range of independent and ‘host’ cultural organisations, with many of the individuals working there transferring from Arts Council England.\n\nThe organisation was managed by Arts Council England and funded by the Department for Culture, Media and Sport with additional funding from the Department for Education. It was finally managed by Creativity, Culture and Education (CCE).\n\n\n"}
{"id": "45183985", "url": "https://en.wikipedia.org/wiki?curid=45183985", "title": "DF-26", "text": "DF-26\n\nThe Dong-Feng 26 (DF-26, ) is an intermediate-range ballistic missile deployed by the People's Liberation Army Rocket Force produced by the China Aerospace Science and Technology Corporation (CASC).\n\nThe DF-26 has a range of , and may be used in the nuclear, conventional, and anti-ship strike roles. The DF-26 is China's first conventionally-armed ballistic missile capable of reaching Guam and the American military installations located there; this has led to the missile being referred to by netizens as the \"Guam Express\" or \"Guam Killer\".\n\nThe ambiguity of whether or not a DF-26 unit has conventional or nuclear warheads makes it risky for an adversary to target these missiles in a first strike.\n\nThe U.S. Department of Defense did not report the missile being in service in 2013, but it seems likely to have become operational by September 2015 when it was first publicly revealed during the parade in Beijing celebrating 70 years since the end of World War II.\n\nThe DF-26 is deployed on a 12x12 transporter-erector-launcher (TEL), likely based on CASC's Tai'an Corporation HTF5680.\n\nUS Air Force National Air and Space Intelligence Center estimates that as of June 2017 over 16 launchers were operationally deployed.\n\nIn April 2018 China’s Ministry of National Defense (MND) has confirmed that the locally designed and developed Dong Feng-26 (DF-26) intermediate-range ballistic missile (IRBM) has entered service with the People’s Liberation Army Rocket Force (PLARF).\n\n\n\n"}
{"id": "9470237", "url": "https://en.wikipedia.org/wiki?curid=9470237", "title": "David Orrell", "text": "David Orrell\n\nDavid John Orrell (born 1962 in Edmonton) is a Canadian writer and mathematician. He received his doctorate in mathematics from the University of Oxford. His work in the prediction of complex systems such as the weather, genetics and the economy has been featured in New Scientist, the Financial Times, Adbusters, BBC Radio, Russia-1, and CBC TV. He now conducts research and writes in the areas of systems biology and economics, and runs a mathematical consultancy Systems Forecasting. He is the son of theatre historian and English professor John Orrell.\n\nHis books have been translated into over ten languages. \"Apollo's Arrow: The Science of Prediction and the Future of Everything\" was a national bestseller and finalist for the 2007 Canadian Science Writers' Award. \"Economyths: Ten Ways Economics Gets It Wrong\" was a finalist for the 2011 National Business Book Award.\n\nA consistent topic in Orrell’s work is the limitations of mathematical models, and the need to acknowledge these limitations if we are to understand the causes of forecast error. He argues for example that errors in weather prediction are caused primarily by model error, rather than the butterfly effect. Economic models are seen as particularly unrealistic. In \"Truth or Beauty: Science and the Quest for Order\", he suggests that many such theories, along with areas of physics such as string theory, are motivated largely by the desire to conform with a traditional scientific aesthetic, that is currently being subverted by developments in complexity science.\n\nIn \"The Evolution of Money\" (coauthored with journalist Roman Chlupatý) and two articles Orrell proposed a quantum theory of money and value, which states that money has dualistic properties because it combines the properties of an owned and valued thing, with those of abstract number. The fact that these two sides of money are incompatible leads to its complex and often unpredictable behavior. In \"Quantum Economics: The New Science of Money\" he argues that these dualistic properties feed up to affect the economy as a whole.\n\n\n\n"}
{"id": "451964", "url": "https://en.wikipedia.org/wiki?curid=451964", "title": "Defect tracking", "text": "Defect tracking\n\nIn engineering, defect tracking is the process of tracking the logged defects in a product from beginning to closure (by inspection, testing, or recording feedback from customers), and making new versions of the product that fix the defects. Defect tracking is important in software engineering as complex software systems typically have tens or hundreds or thousands of defects: managing, evaluating and prioritizing these defects is a difficult task. When the numbers of defects gets quite large, and the defects need to be tracked over extended periods of time, use of a defect tracking system can make the management task much easier.\n\n"}
{"id": "42570879", "url": "https://en.wikipedia.org/wiki?curid=42570879", "title": "Energy-Safety and Energy-Economy", "text": "Energy-Safety and Energy-Economy\n\nEnergy Safety and Energy Economy () is a peer-reviewed scientific and technical journal covering energy safety and economy, safety regulations, personnel training, innovation, and recent trends in alternative power sources research. The editor-in-chief is Svetlana Zernes (). It was established in 2005 as \"Energy Safety in Documents and Facts Journal\", obtaining its current title in 2008.\n\nThe journal is included in AGRIS, Ulrich's Periodicals Directory, the Higher Attestation Commission's official list, EBSCO, Russian Science Citation Index, Global Impact Factor, Research Bible, SHERPA/RoMEO, WorldCat, Open Academic Journals Index (OAJI) and VINITI Database RAS.\n\nIn addition to bimonthly issues, \"Energy Safety and Energy Economy\" publishes a quarterly appendix.\n\n\"Energy Safety and Energy Economy\" is a winner of the National Ecological Prize, Russian Energy Olympus contest, Social and Economic Significant Projects in Education, Culture and Ecology contest, Save Energy contest, and others.\n\n"}
{"id": "146276", "url": "https://en.wikipedia.org/wiki?curid=146276", "title": "Enrico Fermi Award", "text": "Enrico Fermi Award\n\nThe Enrico Fermi Award is an award honoring scientists of international stature for their lifetime achievement in the development, use, or production of energy. It is administered by the U.S. government's Department of Energy. The recipient receives $50,000, a certificate signed by the President and the Secretary of Energy, and a gold medal featuring the likeness of Enrico Fermi.\n\nSource: US Department of Energy\n\n"}
{"id": "786016", "url": "https://en.wikipedia.org/wiki?curid=786016", "title": "Executive information system", "text": "Executive information system\n\nAn Executive information system (EIS), also known as an Executive support system (ESS), is a type of management support system that facilitates and supports senior executive information and decision-making needs. It provides easy access to internal and external information relevant to organizational goals. It is commonly considered a specialized form of decision support system (DSS).\n\nEIS emphasizes graphical displays and easy-to-use user interfaces. They offer strong reporting and drill-down capabilities. In general, EIS are enterprise-wide DSS that help top-level executives analyze, compare, and highlight trends in important variables so that they can monitor performance and identify opportunities and problems. EIS and data warehousing technologies are converging in the marketplace.\n\nIn recent years, the term EIS has lost popularity in favor of business intelligence (with the sub areas of reporting, analytics, and digital dashboards).\n\nTraditionally, executive information systems were mainframe computer-based programs. The purpose was to package a company's data and to provide sales performance or market research statistics for decision makers, such as, marketing directors, chief executive officer, who were not necessarily well acquainted with computers. The objective was to develop computer applications that highlighted information to satisfy senior executives' needs. Typically, an EIS provides only data that supported executive level decisions, not all company data.\n\nToday, the application of EIS is not only in typical corporate hierarchies, but also at lower corporate levels. As some client service companies adopt the latest enterprise information systems, employees can use their personal computers to get access to the company's data and identify information relevant to their decision making. This arrangement provides relevant information to upper and lower corporate levels.\n\nEIS components can typically be classified as:\n\nWhen talking about computer hardware for an EIS environment, we should focus on the hardware that meets the executive's needs. The executive must be put first and the executive's needs must be defined before the hardware can be selected. The basic hardware needed for a typical EIS includes four components: \n\nIn addition, with the advent of local area networks (LAN), several EIS products for networked workstations became available. These systems require less support and less expensive computer hardware. They also increase EIS information access to more company users.\n\nChoosing the appropriate software is vital to an effective EIS. Therefore, the software components and how they integrate the data into one system are important. A typical EIS includes four software components:\n\nAn EIS must be efficient to retrieve relevant data for decision makers, so the user interface is very important. Several types of interfaces can be available to the EIS structure, such as scheduled reports, questions/answers, menu driven, command language, natural language, and input/output.\n\nAs decentralizing is becoming the current trend in companies, telecommunications will play a pivotal role in networked information systems. Transmitting data from one place to another has become crucial for establishing a reliable network. In addition, telecommunications within an EIS can accelerate the need for access to distributed data.\n\nEIS helps executives find data according to user-defined criteria and promote information-based insight and understanding. Unlike a traditional management information system presentation, EIS can distinguish between vital and seldom-used data, and track different key critical activities for executives, both which are helpful in evaluating if the company is meeting its corporate objectives. After realizing its advantages, people have applied EIS in many areas, especially, in manufacturing, marketing, and finance areas.\n\nManufacturing is the transformation of raw materials into finished goods for sale, or intermediate processes involving the production or finishing of semi-manufactures. It is a large branch of industry and of secondary production. Manufacturing operational control focuses on day-to-day operations, and the central idea of this process is effectiveness.\n\nIn an organization, marketing executives' duty is managing available marketing resources to create a more effective future. For this, they need make judgments about risk and uncertainty of a project and its impact on the company in short term and long term. To assist marketing executives in making effective marketing decisions, an EIS can be applied. EIS provides sales forecasting, which can allow the market executive to compare sales forecast with past sales. EIS also offers an approach to product price, which is found in venture analysis. The market executive can evaluate pricing as related to competition along with the relationship of product quality with price charged. In summary, EIS software package enables marketing executives to manipulate the data by looking for trends, performing audits of the sales data, and calculating totals, averages, changes, variances, or ratios.\n\nFinancial analysis is one of the most important steps to companies today. Executives needs to use financial ratios and cash flow analysis to estimate the trends and make capital investment decisions. An EIS integrates planning or budgeting with control of performance reporting, and it can be extremely helpful to finance executives. EIS focuses on financial performance accountability, and recognizes the importance of cost standards and flexible budgeting in developing the quality of information provided for all executive levels.\n\n\n\nThe future of Banu is not bound by mainframe computer systems. This trend free executives from learning different computer operating systems, and substantially decreases implementation costs. Because this trend includes using existing software applications, executives don't need to learn a new or special language for the EIS package.\n\n\n"}
{"id": "11774223", "url": "https://en.wikipedia.org/wiki?curid=11774223", "title": "Function (engineering)", "text": "Function (engineering)\n\nIn engineering, a function is interpreted as a specific process, action or task that a system is able to perform.\n\nIn the lifecycle of engineering projects, there are usually distinguished subsequently: \"Requirements\" and \"Functional specification\" documents. The \"Requirements\" usually specifies the most important attributes of the requested system. In the \"Design specification\" documents, physical or software processes and systems are frequently the requested functions\n\nFor advertising and marketing of technical products, the number of functions they can perform is often counted and used for promotion. For example a calculator capable of the basic mathematical operations of addition, subtraction, multiplication, and division, would be called a \"four-function\" model; when other operations are added, for example for scientific, financial, or statistical calculations, advertisers speak of \"57 scientific functions\", etc. A wristwatch with stopwatch and timer facilities would similarly claim a specified number of functions. To maximise the claim, trivial operations which do not significantly enhance the functionality of a product may be counted.\n\n"}
{"id": "42344621", "url": "https://en.wikipedia.org/wiki?curid=42344621", "title": "GATC Biotech", "text": "GATC Biotech\n\nGATC Biotech is a German company specialist in DNA and RNA sequencing for academic and industrial partners worldwide. The company offers sequencing and bioinformatics solutions from single samples up to large-scale projects. 'The Genome and Diagnostic Centre' focusing on Next and Third Generation sequencing is based in the headquarters in Constance, Germany. The fully automated NGS laboratories are certified under ISO 17025. The Sanger sequencing business is located in 'The European Custom Sequencing Centre' in Cologne, Germany. The proximity to Cologne Bonn Cargo GmbH serves as the logistical hub within Europe.\n\nFounded in 1990 the company concentrated in the early days on commercialising a nonradioactive sequencing technology platform, patented by Prof. Pohl one of the founder of GATC. \nThe direct blotting electrophoresis system, the GATC 1500 was utilised in the European \"Saccharomyces cerevisiae\" genome project. In 1996 started the transformation from an instrument manufacturer to a service provider in the field of DNA and RNA sequencing. As part of the Gene Alliance, founded in 1998, the company was involved in the large-scale genome analysis project Aspergillus Niger funded by DSM (The Netherlands) and finished in 2001.\nIn 2006 the company started to develop the next generation sequencing business division adding the Europe's first commercial PacBio RS platform in 2011.\n\nGATC Biotech is a limited company has several subsidiaries in Europe.\n\n\nThe range of products covers the Sanger sequencing applications as well as next generation sequencing, such as \"de novo\" sequencing, human exome sequencing for clinical settings and RNA-Seq.\n\n"}
{"id": "30856603", "url": "https://en.wikipedia.org/wiki?curid=30856603", "title": "Gas core reactor rocket", "text": "Gas core reactor rocket\n\nGas core reactor rockets are a conceptual type of rocket that is propelled by the exhausted coolant of a gaseous fission reactor. The nuclear fission reactor core may be either a gas or plasma. They may be capable of creating specific impulses of 3,000–5,000 s (30 to 50 kN·s/kg, effective exhaust velocities 30 to 50 km/s) and thrust which is enough for relatively fast interplanetary travel. Heat transfer to the working fluid (propellant) is by thermal radiation, mostly in the ultraviolet, given off by the fission gas at a working temperature of around 25,000 °C.\n\nNuclear gas-core-reactor rockets can provide much higher specific impulse than solid core nuclear rockets because their temperature limitations are in the nozzle and core wall structural temperatures, which are distanced from the hottest regions of the gas core. Consequently, nuclear gas core reactors can provide much higher temperatures to the propellant. Solid core nuclear thermal rockets can develop higher specific impulse than conventional chemical rockets due to the low molecular weight of a hydrogen propellant, but their operating temperatures are limited by the maximum temperature of the solid core because the reactor's temperatures cannot rise above its components' lowest melting temperature.\n\nDue to the much higher temperatures achievable by the gaseous core design, it can deliver higher specific impulse and thrust than most other conventional nuclear designs. This translates into shorter mission transit times for future astronauts or larger payload fractions. It may also be possible to use partially ionized plasma from the gas core to generate electricity magnetohydrodynamically, subsequently negating the need for an additional power supply.\n\nAll gas-core reactor rocket designs share several properties in their nuclear reactor cores, and most designs share the same materials. The closest terrestrial design concept is the gaseous fission reactor.\n\nThe fissile fuel is usually highly enriched uranium pellets or a uranium containing gas (U-235 or U-233). Sometimes uranium tetrafluoride is required due to its chemical stability; the propellant is usually hydrogen.\n\nMost gas core reactors are surrounded by a radial first wall capable of taking the brunt of the extreme environment present inside the core, a pressure shell to hold everything together, and a radial neutron moderator usually made up of beryllium oxide. The propellant also provides moderation.\n\nThe hydrogen propellant cools the reactor and its various structural parts. Hydrogen is first pumped through the nozzle, then through the walls and back down through the core region. Once it passes through the core region, the hydrogen is exhausted. If cooling from the propellant is not enough, external radiators are required. The internal gas core temperatures in most designs vary, but the designs with the highest specific impulses generally have fissioning gas plasmas heating a low mass propellant. This heating occurs primarily through radiation.\n\nAt high temperatures, heat is transferred predominantly by thermal radiation (rather than thermal conduction). However, the hydrogen gas used as propellant is almost completely transparent to this radiation. Therefore, in most gas core reactor rocket concepts, some sort of \"seeding\" of the propellant by opaque solid or liquid particles is considered necessary. Particles of carbon [soot] (which is highly opaque and remains solid up to 3915 K, its sublimation point) would seem to be a natural choice; however, carbon is chemically unstable in a hydrogen-rich environment at high temperatures and pressures. Thus, rather than carbon, dust particles or liquid droplets of a material such as tungsten (melting point 3695 K, boiling point 6203 K) or tantalum hafnium carbide (melting point 4263 K, boiling point some unknown higher temperature) are preferred. These particles would make up up to 4% of the mass of the exhaust gas, which would considerably increase the cost of propellant and slightly lower the rocket's specific impulse.\n\nAt the temperatures necessary to reach a specific impulse of 5000-7000 s, however, no solid or liquid material would survive (the required reactor temperature would be at least 50,000–100,000 K), and the propellant would become transparent; as a result, most of the heat would be absorbed by the chamber walls. This would preclude the use of a nuclear thermal rocket with this high of a specific impulse, unless some other means of seeding or heat transfer to the propellant is found.\n\nControl can be accomplished by either changing the relative or overall densities of the fissile fuel and the propellant or by having outside control drives moving neutron absorbing drums or the radial moderator.\n\nThere are two main variations of the gas core reactor rocket: open cycle designs, which do not contain the fuel within a vessel, and closed cycle designs, which contain the gas reaction core within a solid structure.\n\nThe disadvantage of the open cycle is that the fuel can escape with the working fluid through the nozzle before it reaches significant burn-up levels. Thus, finding a way to limit the loss of fuel is required for open-cycle designs. Unless an outside force is relied upon (i.e. magnetic forces, rocket acceleration), the only way to limit fuel-propellant mixing, is through flow hydrodynamics. Another problem is that the radioactive efflux from the nozzle makes the design totally unsuitable for operation within Earth's atmosphere.\n\nThe advantage of the open cycle design is that it can attain much higher operating temperatures than the closed cycle design, and does not require the exotic materials needed for a suitable closed cycle design.\n\nThe shape of the fissile gas core can be either cylindrical, toroidal, or counter flow toroidal. Since there are issues regarding the loss of fissile fuel with the cylindrical and toroidal designs, the counter-flow toroidal gas core geometry is the primary source of research. The counter flow toroid is the most promising because it has the best stability and theoretically prevents mixing of the fissile fuel and propellant more effectively than the aforementioned concepts. In this design, the fissile fuel is kept mostly in a base injection stabilized recirculation bubble by hydrodynamic confinement. Most designs utilize a cylindrical gas core wall for ease of modeling. However, previous cold flow tests have shown that hydrodynamic containment is more easily achieved with a spherical internal wall geometry design.\n\nThe formation of the fuel vortex is complex. It basically comes down to flow over a projectile shape with a blunt base. The vortex is formed by placing a semi-porous wall in front of the desired location of the fuel vortex but leaves room along its sides for hydrogen propellant. Propellant is then pumped inside the reactor cavity along an annular inlet region. A dead space then develops behind the semi-porous wall; due to viscous and shear forces, a counter toroidal rotation develops. Once the vortex develops, fissile fuel can be injected through the semi-porous plate to bring the reactor critical. The formation and location of the fuel vortex now depends on the amount of fissile fuel that bleeds into the system through the semi-porous wall. When more fuel bleeds into the system through the wall, the vortex moves farther downstream. When less bleeds through, the vortex moves farther upstream. Of course, the upstream location is constrained by the placement of the semi-porous wall.\n\nThe closed cycle is advantageous because its design virtually eliminates loss of fuel, but the necessity of a physical wall between the fuel and the propellant leads to the obstacle of finding a material with extremely optimized characteristics. One must find a medium that is transparent to a wide range of gamma energies, but can withstand the radiation environment present in the reactor, specifically particle bombardment from the nearby fission reactions. This barrage of particles can lead to sputtering and eventual wall erosion.\n\nOne closed cycle gas core rocket design (often called the \"nuclear lightbulb\") contains the fissioning gas in a quartz enclosure that is separate from the propellant. First, the hydrogen coolant runs through the nozzle and inside the walls of the quartz enclosure for cooling. Next, the coolant is run along the outside of the quartz fuel enclosure. Since the fissile gas would be directly in contact with the walls, the operating temperature is not as great as other designs because the walls would eventually ablate away.\n\nBarring an external force, hydrodynamic containment is the only way to increase the residence time of the fuel in the reactor. However, one may ask why bar an outside force, could not magnetic confinement be used since the fuel would be highly ionized (three or four times ionized) while the propellant is only partially ionized? To answer this question one must understand a little about magnetic plasma confinement. The key parameter of interest for magnetic confinement is the ratio of kinetic pressure to magnetic pressure, β.\n\nWhen β<1 magnetic confinement is possible (most fusion schemes have a β close to 0.05). However, the pressures in a gas core rocket are much higher than pressures in fusion devices, approximately 1000 atm (100 MPa). For these pressures, the necessary magnetic field strength required is close to 16 teslas just to produce β=1. For a magnetic field of this magnitude, superconducting technology is necessary and the added mass of such a system would be detrimental. Also, even with a β<1, resistive diffusion will cause the fuel core to collapse almost immediately unless β«1, which would require an even larger magnetic field.\n\nHowever, because the propellant and fuel can be at the same pressure, a magnetic field could retain the fuel merely by impeding convective mixing with the propellant, and would play no role in maintaining pressure in the reactor chamber: The pressure of the fuel is not relevant to a calculation of β. Because the situation is entirely unlike that of the confinement of a fusion plasma in vacuum, the required strength of a magnetic field for fission fuel retention must be estimated based on magnetohydrodynamic considerations (in particular, the suppression of turbulent mixing).\n\nAnother important aspect to GCRs is the impact of the rocket acceleration on the containment of the fuel in the fuel bubble. A rocket acceleration of only 0.001 \"g\" (10 mm/s²) will cause buoyancy effects to decrease core containment by 35% if all other flow-rates are held constant from a zero g startup. Ultimately, the fuel-propellant flows will have to be throttled until the rocket approaches some sort of steady state.\n\nSince steep temperature gradients will be present in any such gas core reactor, several implications for neutronics must be considered. The open-cycle gas-core reactor (OCGCR) is typically a thermal/epithermal reactor. Most types of OCGCR require external moderation due to the steep temperature gradients inside the gaseous core. Neutrons born in the fuel region travel relatively unimpeded to the external moderator where some are thermalized and sent back into the gas core. Due to the high core temperatures, however, on the return trip the neutrons are up scattered in the fuel region, which leads to a significant negative reactor worth. To achieve criticality, this reactor is operated at very high pressure and the exterior radial wall is made up of a moderator of some sort, generally beryllium oxide. Moderation can also come from introducing moderating particles into either the fuel or propellant streams, but by doing so, the benefits in neutronics is canceled by loss of rocket performance.\n\nThe open-cycle gas-core rocket has many unique design attributes that make it a serious challenger to other proposed propulsion for interplanetary missions. Due to the necessity of having a transparent wall inside the reactor for a closed cycle concept, the benefit of moving to a gas core from a solid core are nearly negated. The high specific impulse and large thrust possible for the OCGCR correspond to shorter mission times and higher payload fractions. However, the technical challenges and unknowns inherent in its design are many. Additionally, any test of the system performed on earth would be under a gravity field of 1 \"g\", which would bring buoyancy effects into play inside the gaseous core.\n\nDue to the inability to perform live testing on earth, research is focused primarily on computational modeling of such a system. It was previously mentioned that the specific impulse could be as high as or higher than 3000 s. However, results of computational modeling point towards this number being somewhat optimistic. When thermal hydraulics were modeled more completely for a typical base injection stabilized recirculation bubble gas core rocket by D. Poston, the specific impulse dropped from >3000 s to <1500 s. In the base injection stabilized recirculation bubble gas core rocket concept, it is thought that some additional method of fuel confinement will be beneficial. As mentioned earlier, relying completely on magnetic containment of the fuel bubble is not yet practical. However, a magnetic field may be able to assist in containment or help suppress turbulence that would lead to fuel-propellant mixing.\n\nThe primary areas of future research for such an OCGCR would therefore be centered on keeping the fuel and propellant from mixing as much as possible. Although this article has focused on enriched uranium for the fuel and hydrogen for the propellant, this may not be the optimal choice for either. Other fuels, such as plutonium, and other propellants, including helium or even helium-3, have also been considered and in certain situations provide advantages.\n\n\n"}
{"id": "25535209", "url": "https://en.wikipedia.org/wiki?curid=25535209", "title": "Hegman gauge", "text": "Hegman gauge\n\nA Hegman gauge, sometimes referred to as a grind gauge or grindometer, is a device used to determine how finely ground the particles of pigment (or other solid) dispersed in a sample of paint (or other liquid) are. The gauge consists of a steel block with a series of very small parallel grooves machined into it. The grooves decrease in depth from one end of the block to the other, according to a scale stamped next to them. A typical Hegman gauge is 170mm by 65mm by 15mm, with a channel of grooves running lengthwise, 12.5mm across and narrowing uniformly in depth from 100 μm to zero.\n\nA Hegman gauge is used by puddling a sample of paint at the deep end of the gauge and drawing the paint down with a flat edge along the grooves. The paint fills the grooves, and the location where a regular, significant \"pepperyness\" in the appearance of the coating appears, marks the coarsest-ground dispersed particles. The reading is taken from the scale marked next to the grooves, in dimensionless \"Hegman units\" and/or mils or micrometres. Hegman units are defined in terms of an inverted size scale as shown below:\n\nDetermining the fineness of a paint's grind is important, because too coarse a grind may reduce the paint's color uniformity, gloss, and opacity. The Hegman gauge is widely used for this purpose because it requires minimal skill and only a few seconds' work.\n\nHegman gauges are commonly available in the following ranges: 0 to 100 micrometres, 0 to 50 micrometres, 0 to 25 micrometres, 0 to 15 micrometres, and 0 to 10 micrometres.\n"}
{"id": "27510151", "url": "https://en.wikipedia.org/wiki?curid=27510151", "title": "Hole opener", "text": "Hole opener\n\nA hole opener is a device used to enlarge the borehole during a well drilling operation.\nIt can be positioned either above the drill bit or above a pilot run inside the existing borehole.\n\nNumerous designs exist, in sizes varying from a couple of inches to above 50\". Usages range from hydrocarbon drilling operations to water drilling or horizontal drilling. \n\nHole opener arms have to sustain heavy loads during operations and are generally made of high-grade alloy steel, welded onto a solid alloy steel body. Some designs feature replaceable arms, allowing for size changes but decreasing overall robustness.\n\n\n"}
{"id": "44367003", "url": "https://en.wikipedia.org/wiki?curid=44367003", "title": "IC extractor", "text": "IC extractor\n\nAn IC extractor is a tool for safely and quickly removing integrated circuits (ICs) from their sockets. The main purpose of using this tool is to avoid bending the socket pins and to avoid damage through electrostatic discharge (ESD).\n"}
{"id": "28500068", "url": "https://en.wikipedia.org/wiki?curid=28500068", "title": "Infrared open-path detector", "text": "Infrared open-path detector\n\nInfrared open-path gas detectors send out a beam of infrared light, detecting gas anywhere along the path of the beam. This linear 'sensor' is typically a few metres up to a few hundred metres in length. Open-path detectors can be contrasted with infrared point sensors.\n\nThey are widely used in the petroleum and petrochemical industries, mostly to achieve very rapid gas leak detection for flammable gases at concentrations comparable to the lower flammable limit (typically a few percent by volume). They are also used, but so far to a lesser extent, in other industries where flammable concentrations can occur, such as in coal mining and water treatment. In principle the technique can also be used to detect toxic gases, for instance hydrogen sulfide, at the necessary parts-per-million concentrations, but the technical difficulties involved have so far prevented widespread adoption for toxic gases.\n\nUsually, there are separate transmitter and receiver units at either end of a straight beam path. Alternatively, the source and receiver are combined, and the beam bounced off a retroreflector at the far end of the measurement path. For portable use, detectors have also been made which use the natural albedo of surrounding objects in place of the retroreflector. The presence of a chosen gas (or class of gases) is detected from its absorption of a suitable infrared wavelength in the beam. Rain, fog etc. in the measurement path can also reduce the strength of the received signal, so it is usual to make a simultaneous measurement at one or more reference wavelengths. The quantity of gas intercepted by the beam is then inferred from the ratio of the signal losses at the measurement and reference wavelengths. The calculation is typically carried out by a microprocessor which also carries out various checks to validate the measurement and prevent false alarms.\n\nThe measured quantity is the sum of all the gas along the path of the beam, sometimes termed the \"path-integral concentration\" of the gas. Thus the measurement has a natural bias (desirable in many applications) towards the total size of an unintentional gas release, rather than the concentration of the gas that has reached any particular point. Whereas the natural units of measurement for an Infrared point sensor are parts-per-million (ppm) or the percentage of the lower flammable limit (%LFL), the natural units of measurement for an open path detector are \"ppm.metres\" (ppmm) or \"LFL.metres\" (LFLm). For instance, the fire and gas safety system on an offshore platform in the North Sea typically has detectors set to a full-scale reading of 5LFLm, with low and high alarms triggered at 1LFLm and 3LFLm respectively.\n\nAn open path detector usually costs more than a single point detector, so there is little incentive for applications that play to a point detector's strengths: where the point detector can be placed at the known location of the highest gas concentration, and a relatively slow response is acceptable. The open path detector excels in outdoor situations where, even if the likely source of the gas release is known, the evolution of the developing cloud or plume is unpredictable. Gas will almost certainly enter an extended linear beam before finding its way to any single chosen point. Also, point detectors in exposed outdoor locations require weather shields to be fitted, increasing the response time significantly. Open path detectors can also show a cost advantage in any application where a row of point detectors would be required to achieve the same coverage, for instance monitoring along a pipeline, or around the perimeter of a plant. Not only will one detector replace several, but the costs of installation, maintenance, cabling etc. are likely to be lower.\n\nIn principle any source of infrared radiation could be used, together with an optical system of lenses or mirrors to form the transmitted beam. In practice the following sources have been used, always with some form of modulation to aid the signal processing at the receiver:\n\nAn incandescent light bulb, modulated by pulsing the current powering the filament or by a mechanical chopper. For systems used outdoors, it is difficult for an incandescent source to compete with the intensity of sunlight when the sun shines directly into the receiver. Also, it is difficult to achieve modulation frequencies distinguishable from those that can be produced naturally, for instance by heat shimmer or by sunlight reflecting off waves at sea.\n\nA gas-discharge lamp is capable of exceeding the spectral power of direct sunlight in the infrared, especially when pulsed. Modern open path systems typically use a xenon flashtube powered by a capacitor discharge. Such pulsed sources are inherently modulated.\n\nA semiconductor laser provides a relatively weak source, but one that can be modulated at high frequency in wavelength as well as amplitude. This property permits various signal processing schemes based on Fourier analysis, of use when the absorption of the gas is weak but narrow in spectral linewidth.\n\nThe precise wavelength passbands used must be isolated from the broad infrared spectrum. In principle any conventional spectrometer technique is possible, but the NDIR technique with multilayer dielectric filters and beamsplitters is most often used. These wavelength-defining components are usually located in the receiver, although one design has shared the task with the transmitter.\n\nAt the receiver, the infrared signal strengths are measured by some form of infrared detector. Generally photodiode detectors are preferred, and are essential for the higher modulation frequencies, whereas slower photoconductive detectors may be required for longer wavelength regions. The signals are fed to low-noise amplifiers, then invariably subject to some form of digital signal processing. The absorption coefficient of the gas will vary across the passband, so the simple Beer–Lambert law cannot be applied directly. For this reason the processing usually employs a calibration table, applicable for a particular gas, type of gas, or gas mixture, and sometimes configurable by the user.\n\nThe choice of infrared wavelengths used for the measurement largely defines the detector's suitability for a particular applications. Not only must the target gas (or gases) have a suitable absorption spectrum, the wavelengths must lie within a spectral window so the air in the beam path is itself transparent. These wavelength regions have been used:\n\n\nThe first open-path detector offered for routine industrial use, as distinct from research instruments built in small numbers, was the Wright and Wright 'Pathwatch' in the US, 1983. Acquired by Det-tronics in 1992, the detector operated in the 3.4 μm region with a powerful incandescent source and a mechanical chopper. It did not achieved large volume sales, mainly because of cost and doubts about long-term reliability with moving parts. Beginning in 1985, Shell Research in UK was funded by Shell Natural Gas to develop an open-path detector with no moving parts. The advantages of the 2.3 μm wavelength were identified, and a research prototype was demonstrated. This design had a combined transmitter-receiver with a corner-cube retroreflector at 50 m. It used a pulsed incandescent lamp, PbS photoconductive detectors in the gas and reference channels, and an Intel 8031 microprocessor for signal processing. In 1987 Shell licensed this technology to Sieger-Zellweger (later Honeywell) who designed and marketed their industrial version as the 'Searchline', using a retro-reflective panel made up of multiple corner-cubes. This was the first open-path detector to be certified for use in hazardous areas and to have no moving parts. Later work by Shell Research used two alternately pulsed incandescent sources in the transmitter and a single PbS detectors in the receiver, avoiding zero drifts caused by the variable responsivity of PbS detectors. This technology was offered to Sieger-Zellweger, and later licensed to PLMS. a company part-owned by Shell Ventures UK. The PLMS GD4001/2 in 1991 were the first detectors to achieve a truly stable zero without moving parts or software compensation of slow drifts. They were also the first infrared gas detectors of any kind to be certified intrinsically safe. The Israeli company Spectronix (also Spectrex) made an important advance in 1996 with their SafEye, the first to use a flash tube source, followed by Sieger-Zellweger with their Searchline Excel in 1998. In 2001 the PLMS Pulsar, soon afterwards acquired by Dräger as their Polytron Pulsar, was the first detector to incorporate sensing to monitor the mutual alignment of the transmitter and receiver during both installation and routine operation.\n\n"}
{"id": "18675609", "url": "https://en.wikipedia.org/wiki?curid=18675609", "title": "Light-emitting electrochemical cell", "text": "Light-emitting electrochemical cell\n\nA light-emitting electrochemical cell (LEC or LEEC) is a solid-state device that generates light from an electric current (electroluminescence). LECs are usually composed of two metal electrodes connected by (e.g. sandwiching) an organic semiconductor containing mobile ions. Aside from the mobile ions, their structure is very similar to that of an organic light-emitting diode (OLED).\n\nLECs have most of the advantages of OLEDs, as well as additional ones:\nThere are two distinct types of LECs, those based on inorganic transition metal complexes (iTMC) or light emitting polymers. iTMC devices are often more efficient than their LEP based counterparts due to the emission mechanism being phosphorescent rather than fluorescent.\n\nWhile electroluminescence had been seen previously in similar devices, the invention of the polymer LEC is attributed to Pei et al. Since then, numerous research groups and a few companies have worked on improving and commercializing the devices.\n\nIn 2012 the first inherently stretchable LEC using an elastomeric emissive material (at room temperature) was reported. Dispersing an ionic transition metal complex into an elastomeric matrix enables the fabrication of intrinsically stretchable light-emitting devices that possess large emission areas (∼175 mm2) and tolerate linear strains up to 27% and repetitive cycles of 15% strain. This work demonstrates the suitability of this approach to new applications in conformable lighting that require uniform, diffuse light emission over large areas.\n\nIn 2012 fabrication of organic light-emitting electrochemical cells (LECs) using a roll-to-roll compatible process under ambient conditions was reported.\n\nIn 2017, a new design approach developed by a team of Swedish researchers promised to deliver substantially higher efficiency: 99.2 cd A at a bright luminance of 1910 cd m.\n\n"}
{"id": "28352809", "url": "https://en.wikipedia.org/wiki?curid=28352809", "title": "List of HD channels in Ireland", "text": "List of HD channels in Ireland\n\nThis is a list of current high definition channels that are available in Ireland, those coming in the future and those that have ceased broadcasting.\n\nAll HD channels in Ireland broadcast at 1080i.\n\nThere is currently seventeen channels available to eir Vision viewers, two channels available to Saorsat viewers, two channels available to Saorview viewers, one-hundred-and-two channels available to Sky Ireland viewers and fifty-six channels available to Virgin Media Ireland viewers on their respective EPGs.\n\n\n"}
{"id": "37996970", "url": "https://en.wikipedia.org/wiki?curid=37996970", "title": "List of firearm brands", "text": "List of firearm brands\n\nThis is a list of firearm brands.\n\n"}
{"id": "12759394", "url": "https://en.wikipedia.org/wiki?curid=12759394", "title": "Lung counter", "text": "Lung counter\n\nA Lung counter is a system consisting of a radiation detector, or detectors, and associated electronics that is used to measure radiation emitted from radioactive material that has been inhaled by a person and is sufficiently insoluble as to remain in the lung for weeks, months, or years.\n\nOften, such a system is housed in a low background counting chamber whose thick walls will be made of low-background steel (~20 cm thick) and will be lined with ~1 cm of lead, then perhaps thin layers of cadmium, or tin, with a final layer of copper. The purpose of the lead, cadmium (or tin), and copper is to reduce the background in the low energy region of a gamma spectrum (typically less than 200 keV)\n\nAs a lung counter is primarily measuring radioactive materials that emit low energy gamma rays or x-rays, the phantom used to calibrate the system must be anthropometric. An example of such a phantom is the Lawrence Livermore National Laboratory Torso Phantom.\n\n"}
{"id": "351015", "url": "https://en.wikipedia.org/wiki?curid=351015", "title": "Meze", "text": "Meze\n\nMeze or mezze (, also spelled \"mazzeh\" or \"mazze\"; Kurdish: meze; ; ; ; ; ; ; ; ; ; ) is a selection of small dishes served to accompany alcoholic drinks in the Near East, the Balkans, and parts of Central Asia. In Levantine, South Caucasian, and Balkan cuisines, meze is often served at the beginning of multi-course meals.\n\nThe word is found in all the cuisines of the former Ottoman Empire and comes from Persian (' \"taste, snack\" < ' \"to taste\").\n\nIn Turkey, meze often consist of \"beyaz peynir\" (literally \"white cheese\"), \"kavun\" (sliced ripe melon), \"acılı ezme\" (hot pepper paste often with walnuts), \"haydari\" (thick strained yogurt with herbs), \"patlıcan salatası\" (cold eggplant salad), \"beyin salatası\" (brain salad), \"kalamar tava\" (fried calamari or squid), midye dolma and \"midye tava\" (stuffed or fried mussels), enginar (artichokes), \"cacık\" (yogurt with cucumber and garlic), \"pilaki\" (foods cooked in a special sauce), \"dolma\" or \"sarma\" (rice-stuffed vine leaves or other stuffed vegetables, such as bell peppers), arnavut ciğeri (a liver dish, served cold), octopus salad, and \"çiğ köfte\" (raw meatballs with bulgur).\n\nIn Greece, Cyprus and the Balkans, \"mezé\", \"mezés\", or \"mezédhes\" (plural) are small dishes, hot or cold, spicy or savory. Seafood dishes such as grilled octopus may be included, along with salads, sliced hard-boiled eggs, garlic bread, kalamata olives, fava beans, fried vegetables, melitzanosalata (eggplant salad), taramosalata, fried or grilled cheeses called saganaki, and sheep, goat, or cow cheeses. \n\nPopular meze dishes include:\n\nIn Syria, Lebanon and Cyprus, meze is often a meal in its own right. There are vegetarian, meat or fish mezes. Groups of dishes arrive at the table about 4 or 5 at a time (usually between five and ten groups). There is a set pattern to the dishes: typically olives, tahini, salad and yogurt will be followed by dishes with vegetables and eggs, then small meat or fish dishes alongside special accompaniments, and finally more substantial dishes such as whole fish or meat stews and grills. Establishments will offer their own specialities, but the pattern remains the same. Naturally the dishes served will reflect the seasons. For example, in late autumn, snails will be prominent. As so much food is offered, it is not expected that every dish be finished, but rather shared at will and served at ease. Eating a Cypriot meze is a social event.\n\nIn the Balkans, meze is very similar to Mediterranean antipasti in the sense that cured cold-cuts, cheese and salads are dominant ingredients and that it typically doesn't include cooked meals. In Serbia, Croatia, Bosnia and Montenegro it includes hard or creamy cheeses, kajmak (clotted cream) or smetana cream, salami, ham and other forms of \"suho/suvo meso\" (cured pork or beef), kulen (paprika flavoured, cured sausage), cured bacon, ajvar, and various pastry; In Bosnia and Herzegovina, depending on religious food restrictions one obeys, meze excludes pork products and replaces them with sudžuk (dry, spicy sausage) and pastrami-like cured beef. In southern Croatia, Herzegovina and Montenegro more mediterranean forms of cured meat like pršut and panceta and regional products like olives are common. Albanian-style meze platters typically include prosciutto ham, salami and brined cheese, accompanied with roasted bell peppers (\"capsicum\") or green olives marinated in olive oil with garlic. In Bulgaria, popular mezes are lukanka (a spicy sausage), soujouk (a dry and spicy sausage), sirene (a white brine cheese), and Shopska salad made with tomatoes, cucumbers, onion, roasted peppers and sirene. In Romania, mezelic means quick appetizer and includes Zacuscă, cheeses and salamis, often accompanied by Țuică.\n\n\"Meze\" is generally accompanied by the distilled drinks rakı, arak (Syria), ouzo, Aragh Sagi, rakia, mastika, or tsipouro. It may also be consumed with beer, wine and other alcoholic drinks. Cyprus Brandy (served neat) is a favourite drink to accompany \"meze\" in Cyprus, although lager or wine are popular with some.\n\nThe same dishes, served without alcoholic drinks, are termed \"muqabbilat\" (starters) in Arabic.\n\nIn Bulgaria, meze is served primarily at the consumption of wine, rakia and mastika, but also accompanying other alcoholic drinks that are not local to the region. In addition to traditional local foods, meze can include nuts, sweets or pre-packaged snacks. The term meze is generally applied to any foods and snacks consumed alongside an alcoholic beverage.\nIn Greece, meze is served in restaurants called \"mezedopoleíon\" and \"tsipourádiko\" or \"ouzerí\", a type of café that serves ouzo or tsipouro. A \"tavérna\" (tavern) or \"estiatório\" (restaurant) offer a mezé as an \"orektikó\" (appetiser). Many restaurants offer their house \"poikilía\" (\"variety\")—a platter with a smorgasbord of mezédhes that can be served immediately to customers looking for a quick or light meal. Hosts commonly serve mezédhes to their guests at informal or impromptu get-togethers, as they are easy to prepare on short notice. \"Krasomezédhes\" (literally \"wine-meze\") is a meze that goes well with wine; \"ouzomezédhes\" are meze that goes with ouzo.\n"}
{"id": "13682267", "url": "https://en.wikipedia.org/wiki?curid=13682267", "title": "Mobile-to-mobile convergence", "text": "Mobile-to-mobile convergence\n\n\"The term mobile to mobile calling is used in many mobile phone plans to refer to making calls to other mobile phones using the same provider's network--which is often cheaper than other calls.\"\n\nMobile to mobile convergence (MMC) is a term to describe a technology used in modern computing and telephony. The term is an offshoot of fixed mobile convergence (FMC) and uses dual mode (cellular network and WiFi) phones with a special software client and an application server to connect voice calls and business applications via a VoWLAN and/or through a cellular service. \n\nMobile to mobile convergence differs from conventional FMC in that the technology uses the WLAN to route calls via the internet as a primary function, and uses the wireless carrier network if the WLAN is not present as a secondary function. It is significant since it is viewed as a means to compete with carrier companies since the calls are routed around the cellular network. This is viewed as a more efficient use of networking technology than standard FMC solutions that are available as well, since most of the latter use the carrier network as the primary means of communication and do not leverage the lower cost and controls of internet protocol-based networks that are generally installed at most modern businesses. In theory, it also provides the capability of providing a greater voice coverage area than either carrier or WLAN technology alone since some areas do not have cellular service coverage and others do not have WiFi.\n\nThe first offering known in the market successfully deploying MMC is beCherry, which is delivered by Belgian company Mondial Telecom. They offer a MMC solution on Symbian, iOS and Android. Other smartphone OS's are also considered.\n\n"}
{"id": "24852563", "url": "https://en.wikipedia.org/wiki?curid=24852563", "title": "Mobile workflow", "text": "Mobile workflow\n\nMobile Workflows are specialized workflows aimed to address deployment of workflows in mobile device infrastructure enabling automation of process interaction with traditional business processes from within the device.\n\nMobile Process - Workflows for Mobile centric services composition \n\n"}
{"id": "42152869", "url": "https://en.wikipedia.org/wiki?curid=42152869", "title": "Nxt", "text": "Nxt\n\nNxt is an open source cryptocurrency and payment network launched in 2013 by anonymous software developer \"BCNext\". It uses proof-of-stake to reach consensus for transactions—as such there is a static money supply and, unlike bitcoin, no mining. Nxt was specifically conceived as a flexible platform around which to build applications and financial services.\n\nNxt has been covered extensively in the \"Call for Evidence\" report by ESMA.\n\nNXT was created with a total of one billion coins. On 28 September 2013 Bitcointalk.org member BCNext created a forum thread announcing the proposed launch of Nxt as a second generation cryptocurrency and asking for small bitcoin donations to determine how to distribute the initial stake. On 18 November 2013 fundraising for Nxt was closed.\n\nThe initial coin offering collected 21 bitcoins that were worth USD$17,000.\n\nOn November 9, 2015 Farla Webmedia launched a project on the NXT Asset Exchange. In July 2016 NXT launched Smart Transaction templates. Meant to serve as building blocks for businesses to construct Blockchain solutions for particular problems.\n\nThe core infrastructure of Nxt is complex. This adds risks as compared to the more lean bitcoin, but makes it easier for external services to be built on top of the blockchain.\n\nA peer-peer exchange allowing decentralized trading of shares, crypto assets. Since the blockchain is an unalterable public ledger of transactions, the Asset Exchange provides a trading record for items other than Nxt. To do this, Nxt allows the designation or \"coloring\" of a particular coin, which builds a bridge from the virtual crypto-currency world to the physical world. The \"colored coin\" can represent property, stocks/bonds, commodities, or even concepts.\n\nArbitrary Messages enable the sending of encrypted or plain text, which can also function to send and store up to 1000 bytes of data permanently, or 42 kilobytes of data for a limited amount of time. As a result, it can be used to build file-sharing services, decentralized applications, and higher-level Nxt services.\n\nAllowing holders of the currency or Nxt-Assets to vote in a cryptographically proven and externally verifiable way. This can be used for future development decisions, or for shareholder voting. It can also be applied to public elections and community based decision-making.\n\nThis makes it possible for external developers to add features or usability enhancements. The plug-ins are not contained in a sandbox.\n\n\n"}
{"id": "11486206", "url": "https://en.wikipedia.org/wiki?curid=11486206", "title": "OSCAR railway costing", "text": "OSCAR railway costing\n\nOSCAR (operational simplified costing analysis for railways) is a railway costing model which was developed by CPCS Transcom Limited in Canada. Developed with World Bank funding, it is currently being used by almost 20 railways throughout Africa and Asia. OSCAR is used as a tool to help railway companies understand the cost of doing business. Railway companies use OSCAR to set appropriate tariffs, reduce costs, and rationalize service. OSCAR can be used for passenger, rail, and mixed systems.\n\nRailway costing is the calculation of the variable and fixed cost components of rail movements. Due to the vast array of data that can be used in the calculations, railway costing is typically done using a mathematical model like OSCAR For example, OSCAR would be able to calculate the cost of shipping 80 tonnes of coal from Point A to Point B, or the cost of moving 3000 passengers from Point C to Point D.\n\nThe OSCAR model will estimate the variable costs of a railway due to the movement of traffic, the provision of a service, or a variety of other factors. Once costs are determined, appropriate tariffs can be set to generate the revenue necessary to offset the chosen cost level (plus profit). OSCAR can also be used for line analysis and investment analysis.\n\nThe costs output by the OSCAR model are classified into four categories:\n\n\nWhen setting a tariff it is desirable to set it such that the revenues would recover all costs plus profit; however, current market forces may not permit such tariffs. Tariffs may not be reflective of the costs on all segments of the railways because costs will vary by segment. As a result, the railway may operate at a loss in one area while in another operate with a large profit thus balancing out. If the railway can not cover even the Direct Variable Cost, given the market conditions, then perhaps the decision might be to forgo this traffic altogether. Alternatively decision-makers could ask what measures could be taken to reduce the cost, so that the revenue will offset the cost.\n\nA simplified block diagram of OSCAR is provided below. The model takes various operating, accounting, movement, and train parameters as input and provides costing information as output.\n\nThere are many examples where OSCAR can provide critical cost saving analysis. One example would be determining the benefit of adding grades in a shunting yard. Will the reduction in time spent shunting locomotives benefit the rail network economically for the years to follow? Or if a new signaling system was added allowing for increased speed limits, would the benefits out-weigh the costs?\n\n"}
{"id": "38639200", "url": "https://en.wikipedia.org/wiki?curid=38639200", "title": "Oklahoma Ordnance Works", "text": "Oklahoma Ordnance Works\n\nThe Oklahoma Ordnance Works (OOW) was a government-owned, contractor-operated (GOCO) facility that was built in Mayes County, Oklahoma to produce smokeless powder and other military explosives that were to be used during World War II. The facility was closed from 1946 until 1954, when production resumed until 1956, then closed again. In 1960, it was sold to the Oklahoma Ordnance Works Authority (OOWA), which converted most of the facility to become the mid America Industrial Park.\n\nIn July 1941, the War Department decided to build a munitions manufacturing facility between Chouteau and Pryor in Mayes County, Oklahoma. Site selection criteria included an ample supply of water and hydroelectric power, relatively level ground and an available local work force. Completion of the nearby Pensacola Dam in 1940 had assured this site would have adequate supplies of electricity and water.\n\nDuring the summer of 1941, the government began buying approximately of land for the facility. Concurrently, the duPont Company began designing the plant to produce smokeless powder. Although the project, now designated as the Oklahoma Ordnance Works (OOW), was originally estimated to cost $32 million, by September 1941, the estimate had risen to about $80 million. The plant started up in June 1942 and began actual production of smokeless powder in September.\n\nIn January 1942, the government formed a War Production Board began to expand powder and munitions production plants. In March 1942, a TNT plant was constructed. Other production plants included those for nitric acid, sulfuric acid and tetryl. Production continued until the war ended in 1945. By the end of the war, the complex covered containing 487 buildings, 24 residences, of railroad track, and four complete water systems. Ultimately, the OOW produced more than of smokeless powder and of TNT and tetryl. Production ceased on August 16, 1945.\n\nDuring autumn 1944, some vacant dormitories at the OOW complex were used to house German prisoners of war. They were separated from the rest of the complex by fenced perimeters patrolled by U. S. Army military police and manned guard towers. Some prisoners were allowed to perform farm work, escorted by armed guards, as allowed by the Geneva Convention. Apparently none worked in the OOW facilities. Prisoners were repatriated beginning in the fall of 1945.\n\nThe complex was put up for sale in 1946, but there were no bidders. The General Services Administration (GSA) sold the electric power and water plants to the state of Oklahoma. The Department of Defense (DOD), successor to the War Department, decided to keep the rest of the complex, then reactivated the plant for production from 1954 until 1956.\n\nGSA then offered the facility for lease in 1958. Instead, it was sold in 1960 to the State of Oklahoma. The Oklahoma Ordnance Works Authority (OOWA), a public trust, was formed in December 1960 to redevelop and administer twelve thousand acres of the complex into the mid America Industrial Park.\n\nOOW had a major impact on the economy of eastern Oklahoma. Most importantly, it was a source of employment for thousands of people. Other projects were begun to house the five to ten thousand workers who would be needed to build and operate the facility. The area enclosed by Chouteau, Pryor and Locust Grove, Oklahoma began to be known as the \"Golden Triangle,\" because of its sudden economic boom. A $500,000 sewer and water improvement project for Chouteau was funded by OOW. The United States Housing Authority built 500 homes for workers in Pryor, while the Home Owners Loan Corporation funded 335 more. Because of the conversion to an industrial park, the net positive impact has continued into the 21st Century.\n\nMidAmerica Industrial Park\n"}
{"id": "29113463", "url": "https://en.wikipedia.org/wiki?curid=29113463", "title": "Percent allocation management module", "text": "Percent allocation management module\n\nA percent allocation management module, commonly known as PAMM, also sometimes referred to as percent allocation money management, describes a software application used predominantly by foreign exchange (forex) brokers to allow their clients to attach money to a specific trader managing one or more accounts appointed on the basis of a limited power of attorney. PAMM solution allows the trader on one trading platform to manage simultaneously unlimited quantity of managed accounts. Depending on the size of the deposit each managed account has its own ratio in PAMM. Trader's activity results (trades, profit and loss) are allocated between managed accounts according to the ratio.\n\nBecause currency trading and other forms of arbitrage achieve profitability within very narrow margins, typically, a PAMM system allows more money to be brought into play while distributing the risk of one trader across (usually) multiple investors. \n\nAssume that there are 3 managed accounts under trader's management: \n\n1. USD account with deposit of $ 100,000 and ratio 9.3%\n\n2. EUR account with deposit of € 400,000 and ratio 49.5%\n\n3. GBP account with deposit of £ 300,000 and ratio 41.2%\n\nDepending on funded amounts different ratios are applied for managed account (for ratio calculation all amounts are converted in USD equivalent based on market rate).\nIn case if, for example, Trader/Money Manager decides to BUY 10 million EURUSD, PAMM allocates the order between managed accounts according to its ratio. Each managed account has its own part of position and corresponding Profit & Loss. In current example the first managed account will get position LONG 930,000 EUR/USD, the second - LONG 4,950,000 EUR/USD and the third - LONG 4,120,000 EUR/USD. Resulting profit & loss will be automatically calculated for each account depending on market prices.\n\nThe percent allocation management module is a form of piggybacking a large investor’s money onto the smaller account of a trader. The trader's own money remains at risk, which theoretically reduces the chance of irresponsible management of the combined funds. PAMM is actually a more advanced descendent of \"LAMM\", which is a \"lot allocation management module\". In a LAMM trading system, if the trader buys one standard lot of a currency, each of the customers' accounts will also be increased with a standard lot of the currency, regardless of the relative size of the customer’s account. This makes sense for accounts where the customer's assets are about the same size as the trader’s, but it makes less sense when the customer’s portfolio is much larger than the trader's. Thus, PAMM was developed as an alternative to LAMM. Alpari launched the PAMM service in 2008 and registered PAMM-account trademark in 2010.\n\nDepending on which currency with which the customer has funded their own account, the ratio calculation of each customer's share is typically converted to United States dollar amounts. This facilitates an ideal investor-trader relationship.\n\nIn this infographic example, there are four investors each with a different amount of risk capital that choose the same money manager. The money manager trades the entire capital, $100,000 total, on one master account (the money manager can only trade the accounts; no access to the investor’s funds is permitted to withdraw any money). The money manager then makes a profit of $12,500, and because in this example the commission fee is 20% that equals to $2,500 in commissions for the money manager. This leaves $110,000 in the master account. The $10,000 profit made will now be credited back to the investors accounts based on the percentage of their share of risk capital in the pool.\n\nOne book mentions that Forex Bot Limited is one type of PAMM software that ensures that \"all accounts are given their appropriate signals, traded together in a PAMM account.\"\n"}
{"id": "72750", "url": "https://en.wikipedia.org/wiki?curid=72750", "title": "Prosthesis", "text": "Prosthesis\n\nIn medicine, a prosthesis (plural: prostheses; from Ancient Greek \"prosthesis\", \"addition, application, attachment\") is an artificial device that replaces a missing body part, which may be lost through trauma, disease, or congenital conditions. Prosthetics are intended to restore the normal functions of the missing body part. Prosthetic amputee rehabilitation is primarily coordinated by a prosthetist and an inter-disciplinary team of health care professionals including psychiatrists, surgeons, physical therapists, and occupational therapists. Prosthetics are commonly created with CAD (Computer-Aided Design), a software interface that helps creators visualize the creation in a 3D form. But they can also be designed by hand.\n\nA person's prosthesis should be designed and assembled according to the person's appearance and functional needs. For instance, a person may need a transradial prosthesis, but need to choose between an aesthetic functional device, a myoelectric device, a body-powered device, or an activity specific device. The person's future goals and economical capabilities may help them choose between one or more devices.\n\nCraniofacial prostheses include intra-oral and extra-oral prostheses. Extra-oral prostheses are further divided into hemifacial, auricular (ear), nasal, orbital and ocular. Intra-oral prostheses include dental prostheses such as dentures, obturators, and dental implants.\n\nProstheses of the neck include larynx substitutes, trachea and upper esophageal replacements,\n\nSomato prostheses of the torso include breast prostheses which may be either single or bilateral, full breast devices or nipple prostheses.\n\nPenile prostheses are used to treat erectile dysfunction.\n\nLimb prostheses include both upper- and lower-extremity prostheses.\n\nUpper-extremity prostheses are used at varying levels of amputation: forequarter, shoulder disarticulation, transhumeral prosthesis, elbow disarticulation, transradial prosthesis, wrist disarticulation, full hand, partial hand, finger, partial finger. A transradial prosthesis is an artificial limb that replaces an arm missing below the elbow.\n\nUpper limb prostheses can be categorized in three main categories: Passive devices, Body Powered devices, Externally Powered (myoelectric) devices. Passive devices can either be passive hands, mainly used for cosmetic purpose, or passive tools, mainly used for specific activities (e.g. leisure or vocational). An extensive overview and classification of passive devices can be found in a literature review by Maat \"et.al.\" A passive device can be static, meaning the device has no movable parts, or it can be adjustable, meaning its configuration can be adjusted (e.g. adjustable hand opening). Despite the absence of active grasping, passive devices are very useful in bimanual tasks that require fixation or support of an object, or for gesticulation in social interaction. According to scientific data a third of the upper limb amputees worldwide use a passive prosthetic hand. Body Powered or cable operated limbs work by attaching a harness and cable around the opposite shoulder of the damaged arm. The third category of prosthetic devices available are myoelectric arms. These work by sensing, via electrodes, when the muscles in the upper arm move, causing an artificial hand to open or close. In the prosthetics industry, a trans-radial prosthetic arm is often referred to as a \"BE\" or below elbow prosthesis.\n\nLower-extremity prostheses provide replacements at varying levels of amputation. These include hip disarticulation, transfemoral prosthesis, knee disarticulation, transtibial prosthesis, Syme's amputation, foot, partial foot, and toe. The two main subcategories of lower extremity prosthetic devices are trans-tibial (any amputation transecting the tibia bone or a congenital anomaly resulting in a tibial deficiency) and trans-femoral (any amputation transecting the femur bone or a congenital anomaly resulting in a femoral deficiency).\n\nA transfemoral prosthesis is an artificial limb that replaces a leg missing above the knee. Transfemoral amputees can have a very difficult time regaining normal movement. In general, a transfemoral amputee must use approximately 80% more energy to walk than a person with two whole legs. This is due to the complexities in movement associated with the knee. In newer and more improved designs, hydraulics, carbon fiber, mechanical linkages, motors, computer microprocessors, and innovative combinations of these technologies are employed to give more control to the user. In the prosthetics industry a trans-femoral prosthetic leg is often referred to as an \"AK\" or above the knee prosthesis.\n\nA transtibial prosthesis is an artificial limb that replaces a leg missing below the knee. A transtibial amputee is usually able to regain normal movement more readily than someone with a transfemoral amputation, due in large part to retaining the knee, which allows for easier movement. Lower extremity prosthetics describes artificially replaced limbs located at the hip level or lower. In the prosthetics industry a trans-tibial prosthetic leg is often referred to as a \"BK\" or below the knee prosthesis.\n\nPhysical therapists are trained to teach a person to walk with a leg prosthesis. To do so, the physical therapist may provide verbal instructions and may also help guide the person using touch, or tactile cues. This may be done in a clinic or home. There is some research suggesting that such training in the home may be more successful if the treatment includes the use of a treadmill. Using a treadmill, along with the physical therapy treatment, helps the person to experience many of the challenges of walking with a prosthesis.\n\nIn the United Kingdom, 75% of lower limb amputations are performed due to inadequate circulation (dysvascularity). This condition is often associated with many other medical conditions (co-morbidities) including diabetes and heart disease that may make it a challenge to recover and use a prosthetic limb to regain mobility and independence. For people who have inadequate circulation and have lost a lower limb, there is insufficient evidence due to a lack of research, to inform them regarding their choice of prosthetic rehabilitation approaches.\n\nLower extremity prostheses are often categorized by the level of amputation or after the name of a surgeon:\n\n\nProsthetic are made lightweight for better convenience for the amputee. Some of these materials include:\n\nWheeled prostheses have also been used extensively in the rehabilitation of injured domestic animals, including dogs, cats, pigs, rabbits, and turtles.\n\nProsthetics have been mentioned throughout history. The earliest recorded mention is the warrior queen Vishpala in the Rigveda. The Egyptians were early pioneers of the idea, as shown by the wooden toe found on a body from the New Kingdom. Roman bronze crowns have also been found, but their use could have been more aesthetic than medical.\n\nAn early mention of a prosthetic comes from the Greek historian Herodotus, who tells the story of Hegesistratus, a Greek diviner who cut off his own foot to escape his Spartan captors and replaced it with a wooden one.\n\nPliny the Elder also recorded the tale of a Roman general, Marcus Sergius, whose right hand was cut off while campaigning and had an iron hand made to hold his shield so that he could return to battle. A famous and quite refined historical prosthetic arm was that of Götz von Berlichingen, made at the beginning of the 16th century. The first confirmed use of a prosthetic device, however, is from 950–710 BC. In 2000, research pathologists discovered a mummy from this period buried in the Egyptian necropolis near ancient Thebes that possessed an artificial big toe. This toe, consisting of wood and leather, exhibited evidence of use. When reproduced by bio-mechanical engineers in 2011, researchers discovered that this ancient prosthetic enabled its wearer to walk both barefoot and in Egyptian style sandals. Previously, the earliest discovered prosthetic was an artificial leg from Capua.\nAround the same time, François de la Noue is also reported to have had an iron hand, as is, in the 17th Century, René-Robert Cavalier de la Salle. Henri de Tonti had a prosthetic hook for a hand. During the Middle Ages, prosthetic remained quite basic in form. Debilitated knights would be fitted with prosthetics so they could hold up a shield, grasp a lance or a sword, or stabilize a mounted warrior. Only the wealthy could afford anything that would assist in daily life.\n\nOne notable prosthesis was that belonging to an Italian man, who scientists estimate replaced his amputated right hand with a knife. Scientists investigating the skeleton, which was found in a Longobard cemetery in Povegliano Veronese, estimated that the man had lived sometime between the 6th and 8th centuries AD. Materials found near the man's body suggest that the knife prosthesis was attached with a leather strap, which he repeatedly tightened with his teeth.\n\nDuring the Renaissance, prosthetics developed with the use of iron, steel, copper, and wood. Functional prosthetics began to make an appearance in the 1500s.\n\nAn Italian surgeon recorded the existence of an amputee who had an arm that allowed him to remove his hat, open his purse, and sign his name. Improvement in amputation surgery and prosthetic design came at the hands of Ambroise Paré. Among his inventions was an above-knee device that was a kneeling peg leg and foot prosthesis with a fixed position, adjustable harness, and knee lock control. The functionality of his advancements showed how future prosthetics could develop.\n\nOther major improvements before the modern era:\n\nAt the end of World War II, the NAS (National Academy of Sciences) began to advocate better research and development of prosthetics. Through government funding, a research and development program was developed within the Army, Navy, Air Force, and the Veterans Administration.\n\nSocket technology for lower extremity limbs saw a revolution during the 1980s when John Sabolich C.P.O., invented the Contoured Adducted Trochanteric-Controlled Alignment Method (CATCAM) socket, later to evolve into the Sabolich Socket. He followed the direction of Ivan Long and Ossur Christensen as they developed alternatives to the quadrilateral socket, which in turn followed the open ended plug socket, created from wood. The advancement was due to the difference in the socket to patient contact model. Prior to this, sockets were made in the shape of a square shape with no specialized containment for muscular tissue. New designs thus help to lock in the bony anatomy, locking it into place and distributing the weight evenly over the existing limb as well as the musculature of the patient. Ischial containment is well known and used today by many prosthetist to help in patient care. Variations of the ischial containment socket thus exists and each socket is tailored to the specific needs of the patient. Others who contributed to socket development and changes over the years include Tim Staats, Chris Hoyt, and Frank Gottschalk. Gottschalk disputed the efficacy of the CAT-CAM socket- insisting the surgical procedure done by the amputation surgeon was most important to prepare the amputee for good use of a prosthesis of any type socket design.\n\nThe first microprocessor-controlled prosthetic knees became available in the early 1990s. The Intelligent Prosthesis was the first commercially available microprocessor controlled prosthetic knee. It was released by Chas. A. Blatchford & Sons, Ltd., of Great Britain, in 1993 and made walking with the prosthesis feel and look more natural. An improved version was released in 1995 by the name Intelligent Prosthesis Plus. Blatchford released another prosthesis, the Adaptive Prosthesis, in 1998. The Adaptive Prosthesis utilized hydraulic controls, pneumatic controls, and a microprocessor to provide the amputee with a gait that was more responsive to changes in walking speed. Cost analysis reveals that a sophisticated above-knee prosthesis will be about $1 million in 45 years, given only annual cost of living adjustments.\n\nIn 2005, DARPA started the Revolutionizing Prosthetics program.\n\nA prosthesis is a functional replacement for an amputated or congenitally malformed or missing limb. Prosthetists are responsible for the prescription, design and management of a prosthetic device.\n\nIn most cases, the prosthetist begins by taking a plaster cast of the patient's affected limb. Lightweight, high-strength thermoplastics are custom-formed to this model of the patient. Cutting-edge materials such as carbon fiber, titanium and Kevlar provide strength and durability while making the new prosthesis lighter. More sophisticated prostheses are equipped with advanced electronics, providing additional stability and control.\n\nOver the years, there have been advancements in artificial limbs. New plastics and other materials, such as carbon fiber, have allowed artificial limbs to be stronger and lighter, limiting the amount of extra energy necessary to operate the limb. This is especially important for trans-femoral amputees. Additional materials have allowed artificial limbs to look much more realistic, which is important to trans-radial and transhumeral amputees because they are more likely to have the artificial limb exposed.\n\nIn addition to new materials, the use of electronics has become very common in artificial limbs. Myoelectric limbs, which control the limbs by converting muscle movements to electrical signals, have become much more common than cable operated limbs. Myoelectric signals are picked up by electrodes, the signal gets integrated and once it exceeds a certain threshold, the prosthetic limb control signal is triggered which is why inherently, all myoelectric controls lag. Conversely, cable control is immediate and physical, and through that offers a certain degree of direct force feedback that myoelectric control does not. Computers are also used extensively in the manufacturing of limbs. Computer Aided Design and Computer Aided Manufacturing are often used to assist in the design and manufacture of artificial limbs.\n\nMost modern artificial limbs are attached to the residual limb (stump) of the amputee by belts and cuffs or by suction. The residual limb either directly fits into a socket on the prosthetic, or—more commonly today—a liner is used that then is fixed to the socket either by vacuum (suction sockets) or a pin lock. Liners are soft and by that, they can create a far better suction fit than hard sockets. Silicone liners can be obtained in standard sizes, mostly with a circular (round) cross section, but for any other residual limb shape, custom liners can be made. The socket is custom made to fit the residual limb and to distribute the forces of the artificial limb across the area of the residual limb (rather than just one small spot), which helps reduce wear on the residual limb. The custom socket is created by taking a plaster cast of the residual limb or, more commonly today, of the liner worn over the residual limb, and then making a mold from the plaster cast. Newer methods include laser-guided measuring which can be input directly to a computer allowing for a more sophisticated design.\n\nOne problem with the residual limb and socket attachment is that a bad fit will reduce the area of contact between the residual limb and socket or liner, and increase pockets between residual limb skin and socket or liner. Pressure then is higher, which can be painful. Air pockets can allow sweat to accumulate that can soften the skin. Ultimately, this is a frequent cause for itchy skin rashes. Over time, this can lead to breakdown of the skin.\n\nArtificial limbs are typically manufactured using the following steps:\n\nCurrent technology allows body powered arms to weigh around one-half to one-third of what a myoelectric arm does.\n\nCurrent body-powered arms contain sockets that are built from hard epoxy or carbon fiber. These sockets or \"interfaces\" can be made more comfortable by lining them with a softer, compressible foam material that provides padding for the bone prominences. A self-suspending or supra-condylar socket design is useful for those with short to mid-range below elbow absence. Longer limbs may require the use of a locking roll-on type inner liner or more complex harnessing to help augment suspension.\n\nWrist units are either screw-on connectors featuring the UNF 1/2-20 thread (USA) or quick-release connector, of which there are different models.\n\nTwo types of body-powered systems exist, voluntary opening \"pull to open\" and voluntary closing \"pull to close\". Virtually all \"split hook\" prostheses operate with a voluntary opening type system.\n\nMore modern \"prehensors\" called GRIPS utilize voluntary closing systems. The differences are significant. Users of voluntary opening systems rely on elastic bands or springs for gripping force, while users of voluntary closing systems rely on their own body power and energy to create gripping force.\n\nVoluntary closing users can generate prehension forces equivalent to the normal hand, upwards to or exceeding one hundred pounds. Voluntary closing GRIPS require constant tension to grip, like a human hand, and in that property, they do come closer to matching human hand performance. Voluntary opening split hook users are limited to forces their rubber or springs can generate which usually is below 20 pounds.\n\nAn additional difference exists in the biofeedback created that allows the user to \"feel\" what is being held. Voluntary opening systems once engaged provide the holding force so that they operate like a passive vice at the end of the arm. No gripping feedback is provided once the hook has closed around the object being held. Voluntary closing systems provide directly proportional control and biofeedback so that the user can feel how much force that they are applying.\n\nA recent study showed that by stimulating the median and ulnar nerves, according to the information provided by the artificial sensors from a hand prosthesis, physiologically appropriate (near-natural) sensory information could be provided to an amputee. This feedback enabled the participant to effectively modulate the grasping force of the prosthesis with no visual or auditory feedback.\n\nResearchers from École Polytechnique Fédérale De Lausanne in Switzerland and the Scuola Superiore Sant'Anna in Italy, implanted the electrodes into the amputee's arm in February 2013. The study, published Wednesday in \"Science Translational Medicine\", details the first time sensory feedback has been restored allowing an amputee to control an artificial limb in real-time. With wires linked to nerves in his upper arm, the Danish patient was able to handle objects and instantly receive a sense of touch through the special artificial hand that was created by Silvestro Micera and researchers both in Switzerland and Italy.\n\nTerminal devices contain a range of hooks, prehensors, hands or other devices.\n\nVoluntary opening split hook systems are simple, convenient, light, robust, versatile and relatively affordable.\n\nA hook does not match a normal human hand for appearance or overall versatility, but its material tolerances can exceed and surpass the normal human hand for mechanical stress (one can even use a hook to slice open boxes or as a hammer whereas the same is not possible with a normal hand), for thermal stability (one can use a hook to grip items from boiling water, to turn meat on a grill, to hold a match until it has burned down completely) and for chemical hazards (as a metal hook withstands acids or lye, and does not react to solvents like a prosthetic glove or human skin).\n\nProsthetic hands are available in both voluntary opening and voluntary closing versions and because of their more complex mechanics and cosmetic glove covering require a relatively large activation force, which, depending on the type of harness used, may be uncomfortable. A recent study by the Delft University of Technology, The Netherlands, showed that the development of mechanical prosthetic hands has been neglected during the past decades. The study showed that the pinch force level of most current mechanical hands is too low for practical use. The best tested hand was a prosthetic hand developed around 1945. In 2017 however, a research has been started with bionic hands by Laura Hruby of the Medical University of Vienna. A few open-hardware 3-d printable bionic hands have also become available. Some companies are also producing robotic hands with integrated forearm, for fitting unto a patient's upper arm.\n\nHosmer and Otto Bock are major commercial hook providers. Mechanical hands are sold by Hosmer and Otto Bock as well; the Becker Hand is still manufactured by the Becker family. Prosthetic hands may be fitted with standard stock or custom-made cosmetic looking silicone gloves. But regular work gloves may be worn as well. Other terminal devices include the V2P Prehensor, a versatile robust gripper that allows customers to modify aspects of it, Texas Assist Devices (with a whole assortment of tools) and TRS that offers a range of terminal devices for sports. Cable harnesses can be built using aircraft steel cables, ball hinges, and self-lubricating cable sheaths. Some prosthetics have been designed specifically for use in salt water.\n\nLower-extremity prosthetics describes artificially replaced limbs located at the hip level or lower. Concerning all ages Ephraim et al. (2003) found a worldwide estimate of all-cause lower-extremity amputations of 2.0–5.9 per 10,000 inhabitants. For birth prevalence rates of congenital limb deficiency they found an estimate between 3.5–7.1 cases per 10,000 births.\n\nThe two main subcategories of lower extremity prosthetic devices are trans-tibial (any amputation transecting the tibia bone or a congenital anomaly resulting in a tibial deficiency), and trans-femoral (any amputation transecting the femur bone or a congenital anomaly resulting in a femoral deficiency). In the prosthetic industry, a trans-tibial prosthetic leg is often referred to as a \"BK\" or below the knee prosthesis while the trans-femoral prosthetic leg is often referred to as an \"AK\" or above the knee prosthesis.\n\nOther, less prevalent lower extremity cases include the following:\n\nThe socket serves as an interface between the residuum and the prosthesis, ideally allowing comfortable weight-bearing, movement control and proprioception. Socket issues, such as discomfort and skin breakdown, are rated among the most important issues faced by lower-limb amputees.\n\nThis part creates distance and support between the knee-joint and the foot (in case of an upper-leg prosthesis) or between the socket and the foot. The type of connectors that are used between the shank and the knee/foot determines whether the prosthesis is modular or not. Modular means that the angle and the displacement of the foot in respect to the socket can be changed after fitting. In developing countries prosthesis mostly are non-modular, in order to reduce cost. When considering children modularity of angle and height is important because of their average growth of 1.9 cm annually.\n\nProviding contact to the ground, the foot provides shock absorption and stability during stance. Additionally it influences gait biomechanics by its shape and stiffness. This is because the trajectory of the center of pressure (COP) and the angle of the ground reaction forces is determined by the shape and stiffness of the foot and needs to match the subject's build in order to produce a normal gait pattern. Andrysek (2010) found 16 different types of feet, with greatly varying results concerning durability and biomechanics. The main problem found in current feet is durability, endurance ranging from 16–32 months These results are for adults and will probably be worse for children due to higher activity levels and scale effects. Evidence comparing different types of feet and ankle prosthetic devices is not strong enough to determine if one mechanism of ankle/foot is superior to another. When deciding on a device, the cost of the device, a person's functional need, and the availability of a particular device should be considered.\n\nIn case of a trans-femoral amputation, there also is a need for a complex connector providing articulation, allowing flexion during swing-phase but not during stance.\n\nTo mimic the knee's functionality during gait, microprocessor-controlled knee joints have been developed that control the flexion of the knee. Some examples are Otto Bock’s C-leg, introduced in 1997, Ossur's Rheo Knee, released in 2005, the Power Knee by Ossur, introduced in 2006, the Plié Knee from Freedom Innovations and DAW Industries’ Self Learning Knee (SLK).\n\nThe idea was originally developed by Kelly James, a Canadian engineer, at the University of Alberta.\n\nA microprocessor is used to interpret and analyze signals from knee-angle sensors and moment sensors. The microprocessor receives signals from its sensors to determine the type of motion being employed by the amputee. Most microprocessor controlled knee-joints are powered by a battery housed inside the prosthesis.\n\nThe sensory signals computed by the microprocessor are used to control the resistance generated by hydraulic cylinders in the knee-joint. Small valves control the amount of hydraulic fluid that can pass into and out of the cylinder, thus regulating the extension and compression of a piston connected to the upper section of the knee.\n\nThe main advantage of a microprocessor-controlled prosthesis is a closer approximation to an amputee's natural gait. Some allow amputees to walk near walking speed or run. Variations in speed are also possible and are taken into account by sensors and communicated to the microprocessor, which adjusts to these changes accordingly. It also enables the amputees to walk downstairs with a step-over-step approach, rather than the one step at a time approach used with mechanical knees. There is some research suggesting that people with microprocessor-controlled prostheses report greater satisfaction and improvement in functionality, residual limb health, and safety. People may be able to perform everyday activities at greater speeds, even while multitasking, and reduce their risk of falls.\n\nHowever, some have some significant drawbacks that impair its use. They can be susceptible to water damage and thus great care must be taken to ensure that the prosthesis remains dry.\n\nA myoelectric prosthesis uses the electrical tension generated every time a muscle contracts, as information. This tension can be captured from voluntarily contracted muscles by electrodes applied on the skin to control the movements of the prosthesis, such as elbow flexion/extension, wrist supination/pronation (rotation) or opening/closing of the fingers. A prosthesis of this type utilizes the residual neuromuscular system of the human body to control the functions of an electric powered prosthetic hand, wrist, elbow or foot. This is different from an electric switch prosthesis, which requires straps and/or cables actuated by body movements to actuate or operate switches that control the movements of the prosthesis. There is no clear evidence concluding that myoelectric upper extremity prostheses function better than body-powered prostheses. Advantages to using a myoelectric upper extremity prosthesis include the potential for improvement in cosmetic appeal (this type of prosthesis may have a more natural look), may be better for light everyday activities, and may be beneficial for people experiencing phantom limb pain. When compared to a body-powered prosthesis, a myoelectric prosthesis may not be as durable, may have a longer training time, may require more adjustments, may need more maintenance, and does not provide feedback to the user.\n\nThe USSR was the first to develop a myoelectric arm in 1958, while the first myoelectric arm became commercial in 1964 by the Central Prosthetic Research Institute of the USSR, and distributed by the Hangar Limb Factory of the UK.\n\nResearchers at the Rehabilitation Institute of Chicago announced in September 2013 that they have developed a robotic leg that translates neural impulses from the user's thigh muscles into movement, which is the first prosthetic leg to do so. It is currently in testing.\n\nRobots can be used to generate objective measures of patient's impairment and therapy outcome, assist in diagnosis, customize therapies based on patient's motor abilities, and assure compliance with treatment regimens and maintain patient's records. It is shown in many studies that there is a significant improvement in upper limb motor function after stroke using robotics for upper limb rehabilitation.\nIn order for a robotic prosthetic limb to work, it must have several components to integrate it into the body's function: Biosensors detect signals from the user's nervous or muscular systems. It then relays this information to a controller located inside the device, and processes feedback from the limb and actuator, e.g., position or force, and sends it to the controller. Examples include surface electrodes that detect electrical activity on the skin, needle electrodes implanted in muscle, or solid-state electrode arrays with nerves growing through them. One type of these biosensors are employed in myoelectric prostheses.\n\nA device known as the controller is connected to the user's nerve and muscular systems and the device itself. It sends intention commands from the user to the actuators of the device and interprets feedback from the mechanical and biosensors to the user. The controller is also responsible for the monitoring and control of the movements of the device.\n\nAn actuator mimics the actions of a muscle in producing force and movement. Examples include a motor that aids or replaces original muscle tissue.\n\nTargeted muscle reinnervation (TMR) is a technique in which motor nerves, which previously controlled muscles on an amputated limb, are surgically rerouted such that they reinnervate a small region of a large, intact muscle, such as the pectoralis major. As a result, when a patient thinks about moving the thumb of his missing hand, a small area of muscle on his chest will contract instead. By placing sensors over the reinnervated muscle, these contractions can be made to control the movement of an appropriate part of the robotic prosthesis.\n\nA variant of this technique is called targeted sensory reinnervation (TSR). This procedure is similar to TMR, except that sensory nerves are surgically rerouted to skin on the chest, rather than motor nerves rerouted to muscle. Recently, robotic limbs have improved in their ability to take signals from the human brain and translate those signals into motion in the artificial limb. DARPA, the Pentagon's research division, is working to make even more advancements in this area. Their desire is to create an artificial limb that ties directly into the nervous system.\n\nAdvancements in the processors used in myoelectric arms have allowed developers to make gains in fine-tuned control of the prosthetic. The Boston Digital Arm is a recent artificial limb that has taken advantage of these more advanced processors. The arm allows movement in five axes and allows the arm to be programmed for a more customized feel. Recently the i-Limb hand, invented in Edinburgh, Scotland, by David Gow has become the first commercially available hand prosthesis with five individually powered digits. The hand also possesses a manually rotatable thumb which is operated passively by the user and allows the hand to grip in precision, power, and key grip modes.\n\nAnother neural prosthetic is Johns Hopkins University Applied Physics Laboratory Proto 1. Besides the Proto 1, the university also finished the Proto 2 in 2010. Early in 2013, Max Ortiz Catalan and Rickard Brånemark of the Chalmers University of Technology, and Sahlgrenska University Hospital in Sweden, succeeded in making the first robotic arm which is mind-controlled and can be permanently attached to the body (using osseointegration).\n\nAn approach that is very useful is called arm rotation which is common for unilateral amputees which is an amputation that affects only one side of the body; and also essential for bilateral amputees, a person who is missing or has had amputated either both arms or legs, to carry out activities of daily living. This involves inserting a small permanent magnet into the distal end of the residual bone of subjects with upper limb amputations. When a subject rotates the residual arm, the magnet will rotate with the residual bone, causing a change in magnetic field distribution. EEG (electroencephalogram) signals, detected using small flat metal discs attached to the scalp, essentially decoding human brain activity used for physical movement, is used to control the robotic limbs. This allows the user to control the part directly.\n\nThe research of Robotic legs has made some advancement over time, allowing exact movement and control. A company in Switzerland called Ossur, has created a robotic leg that moves through algorithms and sensors that automatically adjust the angle of the foot during different points in its wearer's stride. Also there are brain-controlled bionic legs that allow an individual to move his limbs with a wireless transmitter. \n\nThe main goal of a robotic prosthesis is to provide active actuation during gait to improve the biomechanics of gait, including, among other things, stability, symmetry, or energy expenditure for amputees. There are several powered prosthetic legs currently on the market, including fully powered legs, in which actuators directly drive the joints, and semi-active legs, which use small amounts of energy and a small actuator to change the mechanical properties of the leg but do not inject net positive energy into gait. Specific examples include The emPOWER from BionX, the Proprio Foot from Ossur, and the Elan Foot from Endolite. Various research groups have also experimented with robotic legs over the last decade. Central issues being researched include designing the behavior of the device during stance and swing phases, recognizing the current ambulation task, and various mechanical design problems such as robustness, weight, battery-life/efficiency, and noise-level. However, scientists from Stanford University and Seoul National University has developed artificial nerves system that will help prosthetic limbs feel. This synthetic nerve system enables prosthetic limbs sense braille, feel the sense of touch and respond to the environment.\n\nMost prostheses can be attached to the exterior of the body, in a non-permanent way. Some others however can be attached in a permanent way. One such example are exoprostheses (see below).\n\nOsseointegration is a method of attaching the artificial limb to the body. This method is also sometimes referred to as exoprosthesis (attaching an artificial limb to the bone), or endo-exoprosthesis.\n\nThe stump and socket method can cause significant pain in the amputee, which is why the direct bone attachment has been explored extensively. The method works by inserting a titanium bolt into the bone at the end of the stump. After several months the bone attaches itself to the titanium bolt and an abutment is attached to the titanium bolt. The abutment extends out of the stump and the (removable) artificial limb is then attached to the abutment. Some of the benefits of this method include the following:\nThe main disadvantage of this method is that amputees with the direct bone attachment cannot have large impacts on the limb, such as those experienced during jogging, because of the potential for the bone to break.\n\nCosmetic prosthesis has long been used to disguise injuries and disfigurements. With advances in modern technology, cosmesis, the creation of lifelike limbs made from silicone or PVC has been made possible. Such prosthetics, including artificial hands, can now be designed to simulate the appearance of real hands, complete with freckles, veins, hair, fingerprints and even tattoos.\nCustom-made cosmeses are generally more expensive (costing thousands of U.S. dollars, depending on the level of detail), while standard cosmeses come premade in a variety of sizes, although they are often not as realistic as their custom-made counterparts. Another option is the custom-made silicone cover, which can be made to match a person's skin tone but not details such as freckles or wrinkles. Cosmeses are attached to the body in any number of ways, using an adhesive, suction, form-fitting, stretchable skin, or a skin sleeve.\n\nUnlike neuromotor prostheses, neurocognitive prostheses would sense or modulate neural function in order to physically reconstitute or augment cognitive processes such as executive function, attention, language, and memory. No neurocognitive prostheses are currently available but the development of implantable neurocognitive brain-computer interfaces has been proposed to help treat conditions such as stroke, traumatic brain injury, cerebral palsy, autism, and Alzheimer's disease.\nThe recent field of Assistive Technology for Cognition concerns the development of technologies to augment human cognition. Scheduling devices such as Neuropage remind users with memory impairments when to perform certain activities, such as visiting the doctor. Micro-prompting devices such as PEAT, AbleLink and Guide have been used to aid users with memory and executive function problems perform activities of daily living.\n\nIn addition to the standard artificial limb for everyday use, many amputees or congenital patients have special limbs and devices to aid in the participation of sports and recreational activities.\n\nWithin science fiction, and, more recently, within the scientific community, there has been consideration given to using advanced prostheses to replace healthy body parts with artificial mechanisms and systems to improve function. The morality and desirability of such technologies are being debated by transhumanists, other ethicists, and others in general. Body parts such as legs, arms, hands, feet, and others can be replaced.\n\nThe first experiment with a healthy individual appears to have been that by the British scientist Kevin Warwick. In 2002, an implant was interfaced directly into Warwick's nervous system. The electrode array, which contained around a hundred electrodes, was placed in the median nerve. The signals produced were detailed enough that a robot arm was able to mimic the actions of Warwick's own arm and provide a form of touch feedback again via the implant.\n\nThe DEKA company of Dean Kamen developed the \"Luke arm\", an advanced nerve-controlled prosthetic. Clinical trials began in 2008, with FDA approval in 2014 and commercial manufacturing by Universal Instruments Corporation expected in 2017. The price offered at retail by Mobius Bionics is expected to be around $100,000.\n\nIn early 2008, Oscar Pistorius, the \"Blade Runner\" of South Africa, was briefly ruled ineligible to compete in the 2008 Summer Olympics because his transtibial prosthesis limbs were said to give him an unfair advantage over runners who had ankles. One researcher found that his limbs used twenty-five percent less energy than those of an able-bodied runner moving at the same speed. This ruling was overturned on appeal, with the appellate court stating that the overall set of advantages and disadvantages of Pistorius' limbs had not been considered.\n\nPistorius did not qualify for the South African team for the Olympics, but went on to sweep the 2008 Summer Paralympics, and has been ruled eligible to qualify for any future Olympics. He qualified for the 2011 World Championship in South Korea and reached the semifinal where he ended last timewise, he was 14th in the first round, his personal best at 400m would have given him 5th place in the finals. At the 2012 Summer Olympics in London, Pistorius became the first amputee runner to compete at an Olympic Games. He ran in the 400 metres race semifinals, and the 4 × 400 metres relay race finals. He also competed in 5 events in the 2012 Summer Paralympics in London.\n\nThere are multiple factors to consider when designing a transtibial prosthesis. Manufacturers must make choices about their priorities regarding these factors.\n\nNonetheless, there are certain elements of socket and foot mechanics that are invaluable for the athlete, and these are the focus of today's high-tech prosthetics companies:\n\nThe buyer is also concerned with numerous other factors:\n\nIn the USA a typical prosthetic limb costs anywhere between $15,000 and $90,000, depending on the type of limb desired by the patient. With medical insurance, a patient will typically pay 10%–50% of the total cost of a prosthetic limb, while the insurance company will cover the rest of the cost. The percent that the patient pays varies on the type of insurance plan, as well as the limb requested by the patient. In the United Kingdom, much of Europe, Australia and New Zealand the entire cost of prosthetic limbs is met by state funding or statutory insurance. For example, in Australia prostheses are fully funded by state schemes in the case of amputation due to disease, and by workers compensation or traffic injury insurance in the case of most traumatic amputations. The National Disability Insurance Scheme, which is being rolled out nationally between 2017 and 2020 also pays for prostheses.\n\nTransradial (below the elbow amputation) and transtibial prostheses (below the knee amputation) typically cost between US $6,000 and $8,000, while transfemoral (above the knee amputation) and transhumeral prosthetics (above the elbow amputation) cost approximately twice as much with a range of $10,000 to $15,000 and can sometimes reach costs of $35,000. The cost of an artificial limb often recurs, while a limb typically needs to be replaced every 3–4 years due to wear and tear of everyday use. In addition, if the socket has fit issues, the socket must be replaced within several months from the onset of pain. If height is an issue, components such as pylons can be changed.\n\nNot only does the patient need to pay for their multiple prosthetic limbs, but they also need to pay for physical and occupational therapy that come along with adapting to living with an artificial limb. Unlike the reoccurring cost of the prosthetic limbs, the patient will typically only pay the $2000 to $5000 for therapy during the first year or two of living as an amputee. Once the patient is strong and comfortable with their new limb, they will not be required to go to therapy anymore. Throughout one's life, it is projected that a typical amputee will go through $1.4 million worth of treatment, including surgeries, prosthetics, as well as therapies.\n\nLow-cost above-knee prostheses often provide only basic structural support with limited function. This function is often achieved with crude, non-articulating, unstable, or manually locking knee joints. A limited number of organizations, such as the International Committee of the Red Cross (ICRC), create devices for developing countries. Their device which is manufactured by CR Equipments is a single-axis, manually operated locking polymer prosthetic knee joint.\n\nTable. List of knee joint technologies based on the literature review.\n\nA plan for a low-cost artificial leg, designed by Sébastien Dubois, was featured at the 2007 International Design Exhibition and award show in Copenhagen, Denmark, where it won the . It would be able to create an energy-return prosthetic leg for US $8.00, composed primarily of fiberglass.\n\nPrior to the 1980s, foot prostheses merely restored basic walking capabilities. These early devices can be characterized by a simple artificial attachment connecting one's residual limb to the ground.\n\nThe introduction of the Seattle Foot (Seattle Limb Systems) in 1981 revolutionized the field, bringing the concept of an Energy Storing Prosthetic Foot (ESPF) to the fore. Other companies soon followed suit, and before long, there were multiple models of energy storing prostheses on the market. Each model utilized some variation of a compressible heel. The heel is compressed during initial ground contact, storing energy which is then returned during the latter phase of ground contact to help propel the body forward.\n\nSince then, the foot prosthetics industry has been dominated by steady, small improvements in performance, comfort, and marketability.\n\nWith 3D printers, it is possible to manufacture a single product without having to have metal molds, so the costs can be drastically reduced.\n\n\"Jaipur Foot\", an artificial limb from Jaipur, India, costs about US$40.\n\nThere is currently an open-design Prosthetics forum known as the \"Open Prosthetics Project\". The group employs collaborators and volunteers to advance Prosthetics technology while attempting to lower the costs of these necessary devices. Open Bionics is a company that is developing open-source robotic prosthetic hands. It uses 3D printing to manufacture the devices and low-cost 3D scanners to fit them, with the aim of lowering the cost of fabricating custom prosthetics. A review study on a wide range of printed prosthetic hands, found that although 3D printing technology holds a promise for individualised prosthesis design, it is not necessarily cheaper when all costs are included. The same study also found that evidence on the functionality, durability and user acceptance of 3D printed hand prostheses is still lacking.\n\nIn the USA an estimate was found of 32,500 children (<21 years) that suffer from major paediatric amputation, with 5,525 new cases each year, of which 3,315 congenital. Carr et al. (1998) investigated amputations caused by landmines for Afghanistan, Bosnia and Herzegovina, Cambodia and Mozambique among children (<14 years), showing estimates of respectively 4.7, 0.19, 1.11 and 0.67 per 1000 children. Mohan (1986) indicated in India a total of 424,000 amputees (23,500 annually), of which 10.3% had an onset of disability below the age of 14, amounting to a total of about 43,700 limb deficient children in India alone.\n\nFew low-cost solutions have been created specially for children. Underneath some of them can be found.\nThis hand-held pole with leather support band or platform for the limb is one of the simplest and cheapest solutions found. It serves well as a short-term solution, but is prone to rapid contracture formation if the limb is not stretched daily through a series of range-of motion (RoM) sets.\n\nThis also fairly simple solution comprises a plaster socket with a bamboo or PVC pipe at the bottom, optionally attached to a prosthetic foot. This solution prevents contractures because the knee is moved through its full RoM. The David Werner Collection, an online database for the assistance of disabled village children, displays manuals of production of these solutions.\n\nThis solution is built using a bicycle seat post up side down as foot, generating flexibility and (length) adjustability. It is a very cheap solution, using locally available materials.\n\nIt is an endoskeletal modular lower limb from India, which uses thermoplastic parts. Its main advantages are the small weight and adaptability.\n\nMonolimbs are non-modular prostheses and thus require more experienced prosthetist for correct fitting, because alignment can barely be changed after production. However, their durability on average is better than low-cost modular solutions.\n\nA number of theorists have explored the meaning and implications of prosthetic extension of the body. Elizabeth Grosz writes, \"Creatures use tools, ornaments, and appliances to augment their bodily capacities. Are their bodies lacking something, which they need to replace with artificial or substitute organs?...Or conversely, should prostheses be understood, in terms of aesthetic reorganization and proliferation, as the consequence of an inventiveness that functions beyond and perhaps in defiance of pragmatic need?\" Elaine Scarry argues that every artifact recreates and extends the body. Chairs supplement the skeleton, tools append the hands, clothing augments the skin. In Scarry's thinking, \"furniture and houses are neither more nor less interior to the human body than the food it absorbs, nor are they fundamentally different from such sophisticated prosthetics as artificial lungs, eyes and kidneys. The consumption of manufactured things turns the body inside out, opening it up \"to\" and \"as\" the culture of objects.\" Mark Wigley, a professor of architecture, continues this line of thinking about how architecture supplements our natural capabilities, and argues that \"a blurring of identity is produced by all prostheses.\" Some of this work relies on Freud's earlier characterization of man's relation to objects as one of extension.\n\n\n\n"}
{"id": "51300397", "url": "https://en.wikipedia.org/wiki?curid=51300397", "title": "Rice-cooking utensils", "text": "Rice-cooking utensils\n\nRice-cooking utensils are tools used for cooking rice and similar foods.\n\nDedicated rice-cooking utensils have a long history. A ceramic rice steamer dated to 1250 BC is on display in the British Museum.\n\nRice absorbs a great deal of water as it cooks, expanding its volume and using up the cooking water. The moisture and heat gelatinize and soften the starch granules in the rice. The cooking time for raw rice (not parboiled beforehand) ranges from about 15 minutes and up, depending upon the type and freshness of rice, method, and desired result (from separate grains to disintegrated porridge). Some rices, such as white rice, long-grain rice and African rice, break up more easily. Some cooking methods are more likely to break the rice (fragmenting it with a mortar and pestle before cooking, or stirring frequently). Some rice is stickier. Most recipes will therefore not work for all rices.\n\nRice can be cooked by heating in boiling water or steam, or a combination of both (boiling until water evaporates, then continuing in steam generated by continued heating).\n\nRice cooking utensils may be divided into\n\n\nA microwave rice cooker is a container designed specifically for cooking rice.\nSome container consists of three parts: an outer bowl, a fitted lid with steam vents, and an inner bowl with a finely perforated base. Some others have only one container and the double-layered lid fitted with a steam vent.\n\nA measured amount of dry rice is placed within the bowl. For long-grain rice or scented rice (e.g., basmati rice, Thai jasmine rice), neither washing or soaking is usually necessary unless contaminating dirt is suspected. For Japanese rice (e.g., Calrose or medium/short grain rice), the rice is washed to remove surface starch powder and the trace of rice bran from the grains. For washing, a generous amount of water is added to the rice then the mixture is stirred a few times with a hand quickly. The water is then drained immediately while the lighter starch is still in the water, and the heavier rice grains settle at the bottom of the container. The washing process may need to be repeated up to three times until the water draining out is clear of starch. Excessive washing, however, is believed to be detrimental, since it will remove too many water-soluble nutrients, e.g., vitamins.\n\nWith the three-part model, the inner bowl is then placed within the outer bowl and a small amount of water is added, so that the rice is just about covered. The lid is then fitted and the cooker is microwaved at full power for between 8 and 15 minutes (depending on the rice type, power output of the appliance and personal texture preference). Cooking occurs with the water boiling away and steaming the grains. It is very important to follow the manufacturer's instructions regarding the length of time and amount of water added, otherwise the rice can burn. This method works well with long-grain rice that does not release much starch and is conventionally cooked with the boil/strain or steam methods.\n\nMicrowave rice cookers can also be used for cooking Japanese rice, or medium- or short grain rice, but the absorption method should be used. The rice is washed well to remove starch and allowed to stand to absorb water for at least 15 min before cooking. The water absorption step is essential, otherwise the cooking may result in cooked rice which is still too hard. The regular amount of water as the conventional cooking method can be used. The container is placed in the microwave and it is brought up to the boil at high power. Then, it can be cooked at low or low-medium power until all the water is evaporated (about 20 min - 30 min, depending on the rice type) and not much steam is coming out. The container is then left to stand for 5 min before the rice is stirred for serving.\n"}
{"id": "192232", "url": "https://en.wikipedia.org/wiki?curid=192232", "title": "Scaffolding", "text": "Scaffolding\n\nScaffolding, also called scaffold or staging, is a temporary structure used to support a work crew and materials to aid in the construction, maintenance and repair of buildings, bridges and all other man made structures. Scaffolds are widely used on site to get access to heights and areas that would be otherwise hard to get to. Unsafe scaffolding has the potential to result in death or serious injury. Scaffolding is also used in adapted forms for formwork and shoring, grandstand seating, concert stages, access/viewing towers, exhibition stands, ski ramps, half pipes and art projects.\n\nThere are five main types of scaffolding used worldwide today. These are Tube and Coupler (fitting) components, prefabricated modular system scaffold components, H-frame / facade modular system scaffolds, timber scaffolds and bamboo scaffolds (particularly in China). Each type is made from several components which often include:\nSpecialized components used to aid in their use as a temporary structure often include heavy duty load bearing transoms, ladders or stairway units for the ingress and egress of the scaffold, beams ladder/unit types used to span obstacles and rubbish chutes used to remove unwanted materials from the scaffold or construction project.\n\nSockets in the walls around the paleolithic cave paintings at Lascaux, suggest that a scaffold system was used for painting the ceiling, over 17,000 years ago.\n\nThe Berlin Foundry Cup depicts scaffolding in ancient Greece (early 5th century BC). Egyptians, Nubians and Chinese are also recorded as having used scaffolding-like structures to build tall buildings. Early scaffolding was made of wood and secured with rope knots.\n\nIn days gone by, scaffolding was erected by individual firms with wildly varying standards and sizes. Scaffolding was revolutionized by Daniel Palmer Jones and David Henry Jones. Modern day scaffolding standards, practices and processes can be attributed to these men and their companies. With Daniel being the better known and patent applicant and holder for many scaffold components still in use today see inventor: Daniel Palmer-Jones. He is considered the grandfather of Scaffolding. The history of scaffolding being that of the Jones brothers and their company's Patent Rapid Scaffold Tie Company Ltd, Tubular Scaffolding Company and Scaffolding Great Britain Ltd (SGB).\n\nDavid Palmer-Jones patented the \"Scaffixer\", a coupling device far more robust than rope which revolutionized scaffolding construction. In 1913, his company was commissioned for the reconstruction of Buckingham Palace, during which his Scaffixer gained much publicity. Palmer-Jones followed this up with the improved \"Universal Coupler\" in 1919 - this soon became the industry standard coupling and has remained so to this day.\n\nOr as Daniel would say “Be it known that I, DANIEL PALMER JONES, manufacturer, subject of the King of England, residing at 124 Victoria Street, Westminster, London, England, have invented certain new and useful Improvements in Devices for Gripping, Fastening, or Locking Purposes” segment from a patent application.\n\nWith the advancements in metallurgy throughout the early 20th century. Saw the introduction of tubular steel water pipes (instead of timber poles) with standardized dimensions, allowing for the industrial interchangeability of parts and improving the structural stability of the scaffold. The use of diagonal bracings also helped to improve stability, especially on tall buildings. The first frame system was brought to market by SGB in 1944 and was used extensively for the postwar reconstruction.\n\nThe European Standard, BS EN 12811-1, specifies performance requirements and methods of structural and general design for access and working scaffolds. Requirements given are for scaffold structures that rely on the adjacent structures for stability. In general these requirements also apply to other types of working scaffolds.\n\nThe purpose of a working scaffold is to provide a safe working platform and access suitable for work crews to carry out their work. The European Standard sets out performance requirements for working scaffolds. These are substantially independent of the materials of which the scaffold is made. The standard is intended to be used as the basis for enquiry and design.\n\nThe basic components of scaffolding are tubes, couplers and boards.\nThe basic lightweight tube scaffolding that became the standard and revolutionised scaffolding, becoming the baseline for decades, was invented and marketed in the mid-1950s. With one basic 24 pound unit a scaffold of various sizes and heights could be assembled easily by a couple of labourers without the nuts or bolts previously needed.\n\nTubes are usually made either of steel or aluminium; although there is composite scaffolding which uses filament-wound tubes of glass fibre in a nylon or polyester matrix, because of the high cost of composite tube, it is usually only used when there is a risk from overhead electric cables that cannot be isolated. If steel, they are either 'black' or galvanised. The tubes come in a variety of lengths and a standard diameter of 48.3 mm. (1.5 NPS pipe). The chief difference between the two types of metal tubes is the lower weight of aluminium tubes (1.7 kg/m as opposed to 4.4 kg/m). However they are more flexible and have a lower resistance to stress. Tubes are generally bought in 6.3 m lengths and can then be cut down to certain typical sizes. Most large companies will brand their tubes with their name and address in order to deter theft.\n\nBoards provide a working surface for scaffold users. They are seasoned wood and come in three thicknesses (38 mm (usual), 50 mm and 63 mm) are a standard width (225 mm) and are a maximum of 3.9 m long. The board ends are protected either by metal plates called hoop irons or sometimes nail plates, which often have the company name stamped into them. Timber scaffold boards in the UK should comply with the requirements of BS 2482. As well as timber, steel or aluminium decking is used, as well as laminate boards. In addition to the boards for the working platform, there are sole boards which are placed beneath the scaffolding if the surface is soft or otherwise suspect, although ordinary boards can also be used. Another solution, called a scaffpad, is made from a rubber base with a base plate moulded inside; these are desirable for use on uneven ground since they adapt, whereas sole boards may split and have to be replaced.\nCouplers are the fittings which hold the tubes together. The most common are called scaffold couplers, and there are three basic types: \"right-angle couplers\", \"putlog couplers\" and \"swivel couplers\". To join tubes end-to-end \"joint pins\" (also called spigots) or \"sleeve couplers\" are used. Only right angle couplers and swivel couplers can be used to fix tube in a 'load-bearing connection'. Single couplers are not load-bearing couplers and have no design capacity.\n\nOther common scaffolding components include base plates, ladders, ropes, anchor ties, reveal ties, gin wheels, sheeting, etc.\nMost companies will adopt a specific colour to paint the scaffolding with, in order that quick visual identification can be made in case of theft. All components that are made from metal can be painted but items that are wooden should never be painted as this could hide defects.\nDespite the metric measurements given, many scaffolders measure tubes and boards in imperial units, with tubes from 21 feet down and boards from 13 ft down.\n\nBamboo scaffolding is widely used in Hong Kong and Macau, with nylon straps tied into knots as couplers. In India, bamboo or other wooden scaffolding is also mostly used, with poles being lashed together using ropes made from coconut hair (coir).\n\nThe key elements of the scaffolding are \"the standard\", \"ledger\" and \"transoms\". The standards, also called uprights, are the vertical tubes that transfer the entire weight of the structure to the ground where they rest on a square \"base plate\" to spread the load. The base plate has a shank in its centre to hold the tube and is sometimes pinned to a \"sole board\". Ledgers are horizontal tubes which connect between the standards. Transoms rest upon the ledgers at right angles. \"Main transoms\" are placed next to the standards, they hold the standards in place and provide support for boards; \"intermediate transoms\" are those placed between the main transoms to provide extra support for boards. In Canada this style is referred to as \"English\". \"American\" has the transoms attached to the standards and is used less but has certain advantages in some situations. \nAs well as the tubes at right angles there are \"cross braces\" to increase rigidity, these are placed diagonally from ledger to ledger, next to the standards to which they are fitted. If the braces are fitted to the ledgers they are called ledger braces. To limit sway a \"facade brace\" is fitted to the face of the scaffold every 30 metres or so at an angle of 35°-55° running right from the base to the top of the scaffold and fixed at every level.\n\nOf the couplers previously mentioned, right-angle couplers join ledgers or transoms to standards, putlog or single couplers join board bearing transoms to ledgers - Non-board bearing transoms should be fixed using a right-angle coupler. Swivel couplers are to connect tubes at any other angle. The actual joints are staggered to avoid occurring at the same level in neighbouring standards.\nThe spacings of the basic elements in the scaffold are fairly standard. For a general purpose scaffold the maximum bay length is 2.1 m, for heavier work the bay size is reduced to 2 or even 1.8 m while for inspection a bay width of up to 2.7 m is allowed.\n\nThe scaffolding width is determined by the width of the boards, the minimum width allowed is 600 mm but a more typical four-board scaffold would be 870 mm wide from standard to standard. More heavy-duty scaffolding can require 5, 6 or even up to 8 boards width. Often an \"inside board\" is added to reduce the gap between the inner standard and the structure.\n\nThe lift height, the spacing between ledgers, is 2 m, although the base lift can be up to 2.7 m. The diagram above also shows a kicker lift, which is just 150 mm or so above the ground.\n\nTransom spacing is determined by the thickness of the boards supported, 38 mm boards require a transom spacing of no more than 1.2 m while a 50 mm board can stand a transom spacing of 2.6 m and 63 mm boards can have a maximum span of 3.25 m. The minimum overhang for all boards is 50 mm and the maximum overhang is no more than 4x the thickness of the board.\n\nGood foundations are essential. Often scaffold frameworks will require more than simple base plates to safely carry and spread the load. Scaffolding can be used without base plates on concrete or similar hard surfaces, although base plates are always recommended. For surfaces like pavements or tarmac base plates are necessary. For softer or more doubtful surfaces sole boards must be used, beneath a single standard a sole board should be at least with no dimension less than , the thickness must be at least . For heavier duty scaffold much more substantial baulks set in concrete can be required. On uneven ground steps must be cut for the base plates, a minimum step size of around is recommended. A working platform requires certain other elements to be safe. They must be close-boarded, have double guard rails and toe and stop boards. Safe and secure access must also be provided.\n\nScaffolds are only rarely independent structures. To provide stability for a scaffolding (at left) framework ties are generally fixed to the adjacent building/fabric/steelwork.\n\nGeneral practice is to attach a tie every 4 m on alternate lifts (traditional scaffolding). Prefabricated System scaffolds require structural connections at all frames - i.e. 2–3 m centres (tie patterns must be provided by the System manufacturer/supplier). The ties are coupled to the scaffold as close to the junction of standard and ledger (node point) as possible. Due to recent regulation changes, scaffolding ties must support +/- loads (tie/butt loads) and lateral (shear) loads.\n\nDue to the different nature of structures there is a variety of different ties to take advantage of the opportunities.\n\n\"Through ties\" are put through structure openings such as windows. A vertical inside tube crossing the opening is attached to the scaffold by a transom and a crossing horizontal tube on the outside called a bridle tube. The gaps between the tubes and the structure surfaces are packed or wedged with timber sections to ensure a solid fit.\n\n\"Box ties\" are used to attach the scaffold to suitable pillars or comparable features. Two additional transoms are put across from the lift on each side of the feature and are joined on both sides with shorter tubes called tie tubes. When a complete box tie is impossible a l-shaped \"lip tie\" can be used to hook the scaffold to the structure, to limit inward movement an additional transom, a \"butt transom\", is placed hard against the outside face of the structure.\n\nSometimes it is possible to use \"anchor ties\" (also called \"bolt ties\"), these are ties fitted into holes drilled in the structure. A common type is a ring bolt with an expanding wedge which is then tied to a node point.\nThe least 'invasive' tie is a \"reveal tie\". These use an opening in the structure but use a tube wedged horizontally in the opening. The reveal tube is usually held in place by a reveal screw pin (an adjustable threaded bar) and protective packing at either end. A transom tie tube links the reveal tube to the scaffold. Reveal ties are not well regarded, they rely solely on friction and need regular checking so it is not recommended that more than half of all ties be reveal ties.\n\nIf it is not possible to use a safe number of ties \"rakers\" can be used. These are single tubes attached to a ledger extending out from the scaffold at an angle of less than 75° and securely founded. A transom at the base then completes a triangle back to the base of the main scaffold.\n\nBamboo scaffolding is a type of scaffolding made from bamboo and widely used in construction work for centuries. Many famous landmarks, notably The Great Wall of China, were built using bamboo scaffolding, and its use continues today in some parts of the world.\n\nBamboo Scaffolding was first introduced into the building industry in Hong Kong immediately after colonization in the 1800s. It was widely used in the building of houses and multi-story buildings (up to four stories high) prior to the development of metal scaffolding. It was also useful for short-term construction projects, such as framework for temporary sheds for Cantonese Opera performances.\nThere are three types of scaffolding in Hong Kong: \n\nIn 2013, there were 1,751 registered bamboo scaffolders and roughly 200 scaffolding companies in Hong Kong. The use of bamboo scaffolding is diminishing due to shortages in labor and material. Despite the lack of labor force and material, recently safety issues have become another serious concern.\n\nThe labor shortage may be due to the reluctance of younger generations to become scaffolders. “They even think that it’s a dirty and dangerous job. They are not going to do that kind of work,” said Yu Hang Flord, who has been a scaffolder for 30 years and later became the director of Wui Fai Holdings, a member of the Hong Kong and Kowloon Scaffolders General Merchants Association. “They refuse to step in, although we give them high pay. They are scared of it. Young generations do not like jobs that involve hard work.” Another reason fewer people are becoming scaffolders is that new recruits need to undergo training with the Hong Kong Construction Industry Council in order to acquire a license. Older scaffolders generally learned in apprenticeships, and may have been able to gather more hands-on experience.\n\nMaterial shortages are also a contributing factor to the decline. The bamboo scaffolding material was imported from mainland China. Bamboo—which matures after three years to the wide diameter and thick skin perfect for scaffolding—came from the Shaoxing area in Guangdong. Over the past two decades, firms have had to look to Guangxi instead. The industry's fear is that one day supplies will be blocked due to export embargoes and environmental concerns. Attempts to import bamboo from Thailand, or switch to synthetic or plastic bamboo, have so far proved unsuccessful.\n\nIn many African countries, notably Nigeria, bamboo scaffolding is still used for small scale construction in urban areas. In rural areas, the use of bamboo scaffolding for construction is common. In fact, bamboo is an essential building and construction commodity in Nigeria; the bamboo materials are transported on heavy trucks and trailers from rural areas (especially the tropical rain forest) to cities and the northern part of Nigeria.\n\nSome of the structures in relaxation and recreation centres, both in urban and rural areas of Nigeria, are put in place using bamboo materials. This is not for reasons of poverty (especially in the cities) but to add more aesthetics to these centres. Bamboo materials are still used in the construction of some bukas (local restaurants) in rural areas.\n\nForms of bamboo scaffolding include:\nOnly double-row bamboo scaffold is allowed to be used for working at height.\nThe perimeter of bamboo scaffold should be covered by nylon mesh against falling objects. The lapping of nylon mesh should be at least 100 mm wide.\nSuitable means of access should be provided from the building or ground level to the scaffold such as gangway, stairs and ladder etc.\nSloping catch fans shall be erected at a level close to the first floor and at no more than 15 metres, vertical intervals should give a minimum horizontal protection coverage of 1500 mm. Large catch fans should be erected at specific locations to protect the public and/or workers underneath.\nA suitable receptacle, covered with galvanized zinc sheet, should be provided within each catch-fan to trap falling objects.\nSteel brackets shall be provided for supporting the standard of scaffold at about six floor intervals. The horizontal distance between steel brackets is about 3 metres.\nMild steel bars or similar materials are required to tie any structure to maintain the bamboo scaffold in its position on every floor. The distance of adjacent putlogs is about 3 to 4 metres.\nEvery working platform must be at least 400 mm wide and closely boarded by planks. The edges of working platforms should be protected by no less than 2 horizontal bamboo members of the scaffold, at intervals between 750 mm to 900 mm and suitable toe-boards no less than 200 mm high.\nAll scaffolds with a height excess of 15 metres shall be designed by an Engineer. \nThey should complete a formal training in bamboo scaffolding work or hold a trade test certificate on bamboo scaffolding and have at least 10 years of relevant experience.\nThey should complete formal training in bamboo scaffolding work or hold a trade test certificate on bamboo scaffolding and have at least 3 years of relevant experience.\n\nBamboo scaffolding is a temporary structure to support people and materials when constructing or repairing building exteriors and interiors. In bamboo scaffolding, plastic fibre straps and bamboo shoots are bound together to form a solid and secure scaffold structure without screws. Bamboo scaffolding does not need to have a foundation on the ground, as long as the scaffolding has a fulcrum for structural support.\n\nBamboo scaffolding is mostly seen in developing Asian countries such as India, Bangladesh, Sri Lanka, and Indonesia.\n\nChinese Opera is one of the world's \"Intangible Cultural Heritages\". One of bamboo scaffolding's main alternative uses is in drama theatres. The flexibility and convenience of this type of scaffolding suits stages set up for temporary use and also separates the audience from the performers.\n\nRespecting and promoting the traditional cultures of Chinese Opera, a huge event called the West Kowloon Bamboo Theatre has been held at the West Kowloon Waterfront Promenade annually since 2012.\n\nStages are built from bamboo scaffolding for the live Chinese operas and Chiu Chow–style dramas performed during every Yu Lan Ghost Festival to worship ghostly ancestors.\n\nThe bamboo tower used in the famous Bun Scrambling Competition during the Cheung Chau Bun Festival on the island of Cheung Chau is constructed out of bamboo scaffolding. Nine thousand buns, representing fortune and blessing, are supported on the fourteen-meter tall bamboo tower in front of the Pak Tai Temple. For the Piu Sik Parade, bamboo stands and racks are used to hold the young costumed performers above the crowds.\n\nTypes of scaffolding covered by the Occupational Health and Safety Administration in the United States include the following categories: Pole; tube and coupler; fabricated frame (tubular welded frame scaffolds); plasterers’, decorators’, and large area scaffolds; bricklayers' (pipe); horse; form scaffolds and carpenters’\nbracket scaffolds; roof brackets; outrigger; pump jacks; ladder jacks; window jacks; crawlingboards (chicken ladders); step, platform, and trestle ladder\nscaffolds; single-point adjustable suspension; two-point adjustable suspension (swing stages); multipoint adjustable suspension; stonesetters’ multipoint adjustable suspension scaffolds, and masons’ multipoint adjustable suspension scaffolds; catenary; float (ship); interior hung; needle beam; multilevel suspended; mobile; repair bracket scaffolds; and stilts.\n\nIn addition to the putlog couplers (discussed above), there are also putlog tubes. These have a flattened end or have been fitted with a blade. This feature allows the end of the tube to be inserted into or rest upon the brickwork of the structure.\n\nA putlog scaffold may also be called a bricklayer's scaffold. As such, the scaffold consists only of a single row of standards with a single ledger. The putlogs are transoms - attached to the ledger at one end but integrated into the bricks at the other.\n\nSpacing is the same on a putlog scaffold as on a general purpose scaffold, and ties are still required.\nIn recent years a number of new innovations have meant an increased scope of use for scaffolding, such as ladderbeams for spanning spaces that cannot accommodate standards and the increased use of sheeting and structure to create temporary roofs.\n\nA pump-jack is a type of portable scaffolding system. The scaffold rests on supports attached to two or more vertical posts. The user raises the scaffolding by pumping the foot pedals on the supports, like an automobile jack.\n\nBaker staging is a metal scaffold which is easy to assemble. Rolling platforms typically wide by long and tall sections which can be stacked up to three high with the use of added outriggers. The work platform height is adjustable.\n\nLow level scaffolding that is height adjustable. It is a hybrid ladder scaffold work platform.\n\nThe widespread use of scaffolding systems, along with the profound importance that they earned in modern applications such as civil engineering projects and temporary structures, led to the definition of a series of standards covering a vast number of specific issues involving scaffolding. Among the standards there are:\n\n\n"}
{"id": "461876", "url": "https://en.wikipedia.org/wiki?curid=461876", "title": "Schlumberger", "text": "Schlumberger\n\nSchlumberger Limited () is the world's largest oilfield services company. Schlumberger employs approximately 100,000 people \nrepresenting more than 140 nationalities working in more than 85 countries. Schlumberger has four principal executive offices located in Paris, Houston, London, and the Hague.\n\nSchlumberger is incorporated in Willemstad, Curaçao as Schlumberger N.V. and trades on the New York Stock Exchange, Euronext Paris, the London Stock Exchange, and SIX Swiss Exchange. Schlumberger is a Fortune Global 500 company, ranked 287 in 2016, and also listed in Forbes Global 2000, ranked 176 in 2016.\n\nSchlumberger was founded in 1926 by brothers Conrad and Marcel Schlumberger from the German occupied Alsace region in France as the Electric Prospecting Company (). The company recorded the first-ever electrical resistivity well log in Merkwiller-Pechelbronn, France in 1927. Today Schlumberger supplies the petroleum industry with services such as seismic acquisition and processing, formation evaluation, well testing and directional drilling, well cementing and stimulation, artificial lift, well completions, flow assurance and consulting, and software and information management. The company is also involved in the groundwater extraction and carbon capture and storage industries.\n\nThe Schlumberger brothers had experience conducting geophysical surveys in countries such as Romania, Canada, Serbia, South Africa, the Democratic Republic of the Congo and the United States. The new company sold electrical-measurement mapping services, and recorded the first-ever electrical resistivity well log in Merkwiller-Pechelbronn, France in 1927. The company quickly expanded, logging its first well in the U.S. in 1929, in Kern County, California. In 1935, the Schlumberger Well Surveying Corporation was founded in Houston, later evolving into Schlumberger Well Services, and finally Schlumberger Wireline & Testing. Schlumberger invested heavily in research, inaugurating the Schlumberger-Doll Research Center in Ridgefield, Connecticut in 1948, contributing to the development of a number of new logging tools. In 1956, Schlumberger Limited was incorporated as a holding company for all Schlumberger businesses, which by now included American testing and production company Johnston Testers.\n\nOver the years, Schlumberger continued to expand its operations and acquisitions. In 1960, Dowell Schlumberger (50% Schlumberger, 50% Dow Chemical), which specialized in pumping services for the oil industry, was formed. In 1962, Schlumberger Limited became listed on the New York Stock Exchange. That same year, Schlumberger purchased Daystrom, an electronic instruments manufacturer in South Boston, Virginia which was making furniture by the time the division was sold to Sperry & Hutchinson in 1971. Schlumberger purchased 50% of Forex in 1964 and merged it with 50% of Languedocienne to create the Neptune Drilling Company. The first computerized reservoir analysis, SARABAND, was introduced in 1970. The remaining 50% of Forex was acquired the following year; Neptune was renamed Forex Neptune Drilling Company. In 1979, Fairchild Camera and Instrument (including Fairchild Semiconductor) became a subsidiary of Schlumberger Limited.\n\nIn 1981, Schlumberger established the first international data links with e-mail. In 1983, Schlumberger opened their Cambridge Research Center in Cambridge, England and in 2012 it was renamed the Schlumberger Gould Research Center after the company's former CEO Andrew Gould.\n\nThe SEDCO drilling rig company and half of Dowell of North America were acquired in 1984, resulting in the creation of the Anadrill drilling segment, a combination of Dowell and The Analysts' drilling segments. Forex Neptune was merged with SEDCO to create the Sedco Forex Drilling Company the following year, when Schlumberger purchased Merlin and 50% of GECO.\n\nIn 1987, Schlumberger completed their purchases of Neptune (North America), Bosco and Cori (Italy), and Allmess (Germany). That same year, National Semiconductor acquired Fairchild Semiconductor from Schlumberger for $122 million. In 1991, Schlumberger acquired PRAKLA-SEISMOS, and pioneered the use of geosteering to plan the drill path in horizontal wells.\n\nSchlumberger acquired software company GeoQuest Systems in 1992. With the purchase came the conversion of SINet to TCP/IP and www capability. In the 1990s Schlumberger bought out the petroleum division, AEG meter, and ECLIPSE reservoir study team Intera Technologies Corp. A joint venture between Schlumberger and Cable & Wireless resulted with the creation of Omnes, which then handled all of Schlumberger's internal IT business. Oilphase and Camco International were also purchased.\n\nIn 1999, Schlumberger and Smith International created a joint venture, M-I L.L.C., the world's largest drilling fluids (or mud) company. The company consists of 60% Smith International, and 40% Schlumberger. Since the joint venture was prohibited by a 1994 antitrust consent decree barring Smith from selling or combining their fluids business with certain other companies, including Schlumberger, the U.S. District Court in Washington, D.C. found Smith International Inc. and Schlumberger Ltd. guilty of criminal contempt and fined each company $750,000 and placed each company on five years probation. Both companies also agreed to pay a total of $13.1 million, representing a full disgorgement of all of the joint venture's profits during the time the companies were in contempt.\n\nIn 2000, the Geco-Prakla division was merged with Western Geophysical to create the seismic contracting company WesternGeco, of which Schlumberger held a 70% stake, the remaining 30% belonging to competitor Baker Hughes. Sedco Forex was spun off, and merged with Transocean Drilling company in 2000.\n\nIn 2001, Schlumberger acquired the IT consultancy company Sema plc for $5.2 billion. The company was an Athens 2004 Summer Olympics partner, but Schlumberger's venture into IT consultancy did not pay off, and divestiture of Sema to Atos Origin was completed that year for $1.5 billion. The cards division was divested through an IPO to form Axalto, which later merged with Gemplus to form Gemalto, and the Messaging Solutions unit was spun off and merged with Taral Networks to form Airwide Solutions. In 2003, the Automated Test Equipment group, part of the 1979 Fairchild Semiconductor acquisition, was spun off to NPTest Holding, which later sold it to Credence.\n\nIn 2004, Schlumberger Business Consulting was launched. Based in Paris, it is the company's management consultancy arm.\nIn 2005, Schlumberger purchased Waterloo Hydrogeologic, which was followed by several other groundwater industry related companies, such as Westbay Instruments, and Van Essen Instruments. Also that year, Schlumberger relocated its U.S. corporate offices from New York to Houston.\n\nIn 2006, Schlumberger purchased the remaining 30% of WesternGeco from Baker Hughes for US$2.4 billion. Also that year, the Schlumberger-Doll Research Center was relocated to a newly built research facility in Cambridge, Massachusetts to replace the Ridgefield, Connecticut research center. The facility joins the other research centers operated by the company in Cambridge, England; Moscow, Russia; Stavanger, Norway; and Dhahran, Saudi Arabia.\n\nIn 2010, the acquisition of Smith International in an all-stock deal valued at $11.3 billion was announced. The sale price is 45.84-a-share price was 37.5 percent higher than Smith closing price on 18 February 2010. The deal is the biggest acquisition in Schlumberger history. The merger was completed on August 27, 2010. Also announced in 2010 were Schlumberger plans to acquire Geoservices, a French-based company specializing in energy services, in a deal valued at $1.1 billion, including debt.\n\nIn 2014, Schlumberger announced the purchase of the remaining shares of SES Holdings Limited (“Saxon”), a Calgary-based provider of international land drilling services, from First Reserve and certain members of Saxon management. The transaction is subject to customary closing conditions, including the receipt of regulatory approvals. Schulmberger had a minority share in Saxon previously.\n\nIn 2015, Schlumberger was indicted by the US Department of Justice for sanction violations of conducting business in Iran and Sudan; the company was fined $233 million, amounting to the largest fine for sanctions to date.\n\nDue to a downturn in the global oil & gas industry in 2015, Schlumberger announced 21,000 layoffs accounting for 15% of the company’s total workforce.\n\nIn August 2015, Schlumberger agreed to acquire oilfield equipment manufacturer Cameron International for $14.8 billion.\n\nIn January 2018, Schlumberger announced that WesternGeco would be exiting the seismic acquisition business, both onshore and offshore, while retaining its multiclient data processing and interpretation segments. This decision followed the bankruptcy filings of several competitors in the seismic services sector.\n\nThe company offices in the La Défense business district of Puteaux and in Paris.\n\nSchlumberger maintains a campus at the northeast corner of U.S. Highway 90A and Gillingham Lane in Sugar Land, Texas; as of 2017 Schlumberger is the third largest employer in the city. In 2015 Schlumberger announced that it was moving its U.S. corporate headquarters to the Sugar Land facility from its Houston office building. The company plans to build new buildings with a scheduled completion time of late 2017. They include a total of of Class A office space and an \"amenities\" building with of space.\n\nIn the 1930s the North American headquarters were established in Houston. In the 1970s, the company's top executives in North America were relocated to New York City. By 2006 the head office had 50 executives and support staff. By that year the company was moving that office to a building near the Houston Galleria.\n\nSchlumberger moved its Houston-area offices from 5000 Gulf Freeway in Houston to the Sugar Land campus in 1995.\n\nIn 2009, \"Newsweek\" released their \"Green Rankings\" a ranking of the 500 largest corporations on track records on a number of environmental issues. Schlumberger was ranked 118th out of 500 overall, and 3rd out of 31 in their industry. \"Newsweek\" remarked that to mitigate global warming Schlumberger has invested in carbon sequestration which involves long-term storage of CO2 and that the company's seismic survey ships are 20% to 25% more fuel-efficient than those of other seismic contractors from using fuels that emit less pollution and towing equipment that creates less drag on the vessels.\n\nIn 2006, a radioactive canister imported by Schlumberger was recovered in the Western Australian outback desert. The canister had been lost \nby the company's transport partner, when the improperly secured container fell off the trailer on which it was being transported.\n\nIn 2010, the Aberdeen Sheriff court fined Schlumberger Oilfield UK £300,000 for losing a radioactive source on the rig floor on the Ensco 101 mobile drilling rig in the North Sea for 4 hours.\n\nIn 2009, the Pennsylvania Department of Environmental Protection fined Chesapeake Appalachia LLC and Schlumberger Technology Corp. more than $15,500 each for a hydrochloric acid spill in February 2009 at Chesapeake's Chancellor natural gas well site in Asylum Township, Bradford County, Pennsylvania. Officials said the leak did not contaminate groundwater.\n\nIn 2006, as the current owner of a facility in Pickens, South Carolina, Schlumberger agreed to pay $11.8 million to federal and state agencies for a problem caused by the previous owner, Sangamo-Weston, a capacitor manufacturing plant. The cause of the problem was from polychlorinated biphenyls (PCB) released into the environment by Sangamo-Weston from 1955 to 1987. According to the Justice Department's Environment and Natural Resources Division, an additional agreement by Schlumberger to purchase and remove dams will directly improve the Twelvemile Creek, South Carolina ecosystem and provide significant environmental benefits for the affected communities.\n\nSchlumberger was contracted to perform wireline logging on the Deepwater Horizon oil rig in the Gulf of Mexico in 2010. However, the wireline log was cancelled and the Schlumberger standby crew was released by BP and left the rig earlier on the same day of the Deepwater Horizon explosion.\n\n"}
{"id": "965734", "url": "https://en.wikipedia.org/wiki?curid=965734", "title": "Special sensor microwave/imager", "text": "Special sensor microwave/imager\n\nThe Special Sensor Microwave/Imager (SSM/I) is a seven-channel, four-frequency, linearly polarized passive microwave radiometer system. It is flown on board the United States Air Force Defense Meteorological Satellite Program (DMSP) Block 5D-2 satellites. The instrument measures surface/atmospheric microwave brightness temperatures (TBs) at 19.35, 22.235, 37.0 and 85.5 GHz. The four frequencies are sampled in both horizontal and vertical polarizations, except the 22 GHz which is sampled in the vertical only.\n\nThe SSM/I has been a very successful instrument, superseding the across-track and Dicke radiometer designs of previous systems. Its combination of constant-angle rotary-scanning and total power radiometer design has become standard for passive microwave imagers, e.g. TRMM Microwave Imager, AMSR.\n\nIts predecessor, the Scanning Multichannel Microwave Radiometer (SMMR), provided similar information. Its successor, the Special Sensor Microwave Imager / Sounder (SSMIS), is an enhanced eleven-channel, eight-frequency system.\n\nAlong with its predecessor SMMR, the SSM/I contributes to an archive of global passive microwave products from late 1978 to present.\n\nInformation within the SSM/I TBs measurements allow the retrieval of four important meteorological parameters over the ocean: near-surface wind speed (note scalar not vector), total columnar water vapor, total columnar cloud liquid water (liquid water path) and precipitation. Accurate and quantitative measurement of these parameters from the SSM/I TBs is, however, a non-trivial task. Variations within the meteorological parameters significantly modify the TBs. As well as open ocean retrievals, it is also possible to retrieve quantitatively reliable information on sea ice, land snow cover and over-land precipitation.\n\nThe Block 5D-2 satellites are in circular or near-circular Sun-synchronous and near-polar orbits at altitudes of 833 km with inclinations of 98.8° and orbital periods of 102.0 minutes, each making 14.1 full orbits per day. The scan direction is from the left to the right with the active scene measurements lying ± 51.2 degrees about when looking in the F8 forward (F10–F15) or aft (F8) direction of the spacecraft travel. This results in a nominal swath width of 1394 km allowing frequent ground coverage, especially at higher latitudes. All parts of the globe at latitudes greater than 58° are covered at least twice daily except for small unmeasured circular sectors of 2.4° about the poles. Extreme polar regions (> 72° N or S) receive coverage from two or more overpasses from both the ascending and descending orbits each day.\n\nThe spin rate of the SSM/I provides a period of 1.9 sec during which the DMSP spacecraft sub-satellite point travels 12.5 km. Each scan 128 discrete, uniformly spaced radiometric samples are taken at the two 85 GHz channels and, on alternate scans, 64 discrete samples are taken at the remaining 5 lower frequency channels. The resolution is determined by the Nyquist limit and the Earth surface's contribution of 3 dB bandwidth of the signal at a given frequency (see Table). The radiometer direction intersects the Earth’s surface at a nominal incidence angle of 53.1 degrees, as measured from the local Earth normal.\n\nTable 1 Radiometric characteristics of the SSM/I (Hollinger 1989).\n\nThe SMMR was flown on Seasat and NASA Nimbus 7 in 1978. The latter operated until 1987.\n\nThe SSM/I has been operating almost continuously on Block 5D-2 flights F8-F15 (not F9) since June 1987. Concerns about the radiometer's performance over the full range of space environmental conditions led to the F8 instrument being switched off in early December 1987 to avoid overheating. The 85 GHz vertical polarization channel failed to switch on in January 1988. Analysis showed inadequate thermal shielding of the sensor's radiometers due to excessive heating at perihelion. The 85 GHz horizontal polarization subsequently had a large increase in radiometric errors and was switched off in summer 1988.\n\nThe launch of the next SSM/I, on board the F10 satellite, took place on 1 December 1990, but was not fully successful. The explosion of the booster rocket left the F10 in an elliptical orbit. The incidence angle of the F10 SSM/I boresight would vary in relation to the Earth throughout each orbit and this also altered the surface area of the Earth viewed by the radiometer. The deviations in the incidence angle of up to 1.4° were quite large and would alter the responses of several geophysical algorithms if not taken into consideration. Further, related changes in the swath width from a minimum of 1226 km at perigee to 1427 km at apogee altered the amounts of radiation viewed by the F10 SSM/I radiometers. The non-circular orbit also caused slight precession of the equatorial crossing time of the F10 by 50 seconds per week.\n\nThe F12 imager had a delayed launch date (the spacecraft was out of the DMSP build sequence) due to a faulty SSM/I. The extra time and costs taken to rectify the problem did not, however, help. The SSM/I failed to ‘spin-up’ after launch, and consequently data were not available from this instrument. The SSM/Is on F11, F13, F14 and F15 have all produced excellent data.\n\nBefore the F8 was decommissioned, it aided investigations into measuring passive microwaves at higher Earth incidence angles (i.e. > 51 degrees). An increase in angle would allow a greater swath width to be utilised, giving a greater amount of coverage at the Earth's surface. The F8 Tilt Experiment (see links) was carried out between 25 June and 13 July 1993.\n\nTable 2 SSM/I Satellites characteristics (source: DSMP website & www.ssmi.com). \n\nF17 and F19 will follow at 3 year intervals, or as needed.\n\n"}
{"id": "2233721", "url": "https://en.wikipedia.org/wiki?curid=2233721", "title": "SpreadsheetML", "text": "SpreadsheetML\n\nSpreadsheetML is the XML schema for Microsoft Office Excel 2003.\n\nThe Office 2003 XML Reference Schemas are included in the Microsoft Open Specification Promise, a legal statement concerning unrestricted use of Microsoft intellectual property.\n\n\n"}
{"id": "30505668", "url": "https://en.wikipedia.org/wiki?curid=30505668", "title": "Suction trap", "text": "Suction trap\n\nA suction trap is a machine that uses air to suck liquid or mucus. It is widely used to extract mucus from recently born babies that are unable to do it by themselves.\n"}
{"id": "27404257", "url": "https://en.wikipedia.org/wiki?curid=27404257", "title": "Swiss Fort Knox", "text": "Swiss Fort Knox\n\nSwiss Fort Knox are two highly secured data centers under the Swiss Alps that are designed to provide \"long-term access to our digital cultural and scientific assets\".\n\n"}
{"id": "33914237", "url": "https://en.wikipedia.org/wiki?curid=33914237", "title": "Systems-oriented design", "text": "Systems-oriented design\n\nSystems-oriented design (S.O.D.) uses system thinking in order to capture the complexity of systems addressed in design practice. The main mission of S.O.D. is to build the designers' own interpretation and implementation of systems thinking. S.O.D. aims at enabling systems thinking to fully benefit from design thinking and practice, and design thinking and practice to fully benefit from systems thinking. S.O.D. addresses design for human activity systems, and can be applied to any kind of design problem ranging from product design and interaction design, through architecture to decision making processes and policy design.\n\nDesign is getting more and more complex for a number of reasons, for example due to globalization, need for sustainability, and the introduction of new technology and increased use of automation. Many of the challenges designers meet today can be considered wicked problems. The characteristics of a wicked problem include among others that there are no definitive formulation of the problem and that the solutions are never true-or-false, but rather better or worse. A traditional problem solving approach is not sufficient in addressing for such design problems. S.O.D. is an approach that addresses the challenges the designer faces when working with complex systems and wicked problems, providing tools and techniques which makes it easier for the designer to grasp the complexity of the problem at hand. With a systems-oriented approach towards design, the designer acknowledges that the starting point for the design process is constantly moving, and that \"every implemented solution is consequential. It leaves \"traces\" that cannot be undone.\" (see Rittel and Webber's 5th property of wicked problems).\n\nDesigners are well suited to work with complexity and wicked problems for a number of reasons:\n\nS.O.D. emphasizes these abilities as central and seeks to further train the designer in systems thinking and systems practice as a skill and an art.\n\nSystems-oriented design builds on systems theory and systems thinking in order to develop practices for addressing complexity in design. Particularly influential is soft systems methodology (SSM), acknowledging conflicting worldviews and people's purposeful actions, and a systems view on creativity. However, S.O.D. is inspired by critical systems thinking and approaches systems theories in an eclectic way transforming the thoughts of the different theories to fit the design process. The design disciplines build on their own traditions and have a certain way of working with problems, often referred to as \"design thinking\" or \"the design way\". Design thinking is a creative process based around the \"building up\" of ideas. This style of thinking is one of the advantages of the designer and is the reason why simply employing one of the existing systems approach into design, like for example systems engineering, is not found sufficient by the advocates of S.O.D.\n\nCompared with other systems approaches, S.O.D. is less concerned with hierarchies and borders of systems, modelling and feedback loops, and more focused on the whole fields of relations and patterns of interactions. S.O.D. seeks richness rather than simplification of the complex systems.\n\nImportant in the systems-oriented design process is to carry out activities in order to grasp the complexity of the system designed for, and to accommodate for a creative process taking place. In order to gain the necessary understanding of the complex system, a comprehensive data capture and analysis phase is needed. Visualizations play a vital role in the design process, both as a means to analyze and understand the information gathered, and in order to communicate. The obtained insight into the complex system is used to come up with solutions for the design problem.\n\nMethods and techniques from other disciplines are used in order to understand the complexity of the system, including for example ethnographic studies, risk analysis, and scenario thinking. Methods and concepts unique to S.O.D. include for example the Rich Design Space, GIGA-mapping, and Incubation Techniques.\n\nThe Rich Design Space includes the physical space of the design studio with sketches, ideas, mood boards, plans etc. and the virtual digital design space of the design software, media like video and sound recording, traditional design techniques and even the social space created in the project. By paying attention to the richness of the design space, the design space can become a very efficient tool to maintain an overview of the complexity of the project all the way.\n\nGIGA-mapping is the central tool for registering, analyzing and managing complexity in S.O.D.. It is used for systematizing and interrelating the information and knowledge obtained.\n\nIncubation is one of the 4 proposed stages of creativity: preparation, incubation, illumination, and verification. Incubation Techniques are techniques used by the designer in order to acquire the background knowledge needed for an incubation process to take place.\n\nS.O.D. has proved valuable for designers to quickly acquire knowledge of a new domains, and to reveal unexpected parts of a system where a new design can be beneficial.\n\nThe concept of systems-oriented design was initially proposed by professor Birger Sevaldson at the Oslo School of Architecture and Design (AHO) in the context of the OCEAN design research network. The S.O.D. approach is currently under development through teaching and research projects, as well as through the work of design practitioners. AHO provides Master courses in Systems Oriented Design each term as part of their Industrial Design program. In these courses, design students are trained in using the tools and techniques of S.O.D. in projects with outside partners. Research projects in systems-oriented design are carried out at the Centre for Design Research at AHO in order to develop the concept, methods and tools further. In 2016 the project Systemic Approach to Architectural Performance was announced as an institutional cooperation between the Faculty of Art and Architecture at the Technical University of Liberec and the Oslo School of Architecture and Design. Its mission is to link the methodology of systems-oriented design with performance-oriented architecture on the case study Marie Davidova's project Wood as a Primary Medium to Architectural Performance.\nThe Oslo-based design consultancy Halogen (www.halogen.no) has used SOD extensively with a varied range of customers. They have suggested a typology of Gigamaps and contributed with other methodological developments.\nGigamapping and other SOD methods are used by a growing range of businesses and educational institutions.\n\n\n"}
{"id": "2833358", "url": "https://en.wikipedia.org/wiki?curid=2833358", "title": "Toilet Paper (South Park)", "text": "Toilet Paper (South Park)\n\n\"Toilet Paper\" is the third episode of the seventh season of the American animated television series \"South Park\", and the 99th episode of the series overall. It first aired on Comedy Central on April 2, 2003. \n\nIn the episode, the boys decide to get revenge on their art teacher for giving them detention by covering her house in toilet paper. Kyle starts having nightmares about the ordeal and is desperate to confess, but Cartman plans to kill him so he doesn't rat them out.\n\nThe episode was written and directed by series co-creator Trey Parker and is rated TV-MA L in the United States. The character Josh parodied Anthony Hopkins' portrayal of Hannibal Lecter in films based on the novels of Thomas Harris.\n\nMrs. Streibel, the art teacher, gives the boys detention for making a phallus out of clay in art class. Enraged, they take revenge by toilet papering her house that night. Kyle is horrified to discover that she has kids and soon regrets the deed, later having nightmares about it. The next day, the boys are called to the counsellor's office, and Cartman comes up with a ridiculously elaborate alibi. With Kyle struggling to comprehend the details of this convoluted story, Cartman grows concerned that he may confess. Cartman decides to eliminate the risk of Kyle confessing by taking matters into his own hands. He takes Kyle on a boat ride on Stark's Pond and begins to beat him to death with a wiffle bat, which was the only weapon he could afford. Kyle, nevertheless, is so guilt-ridden that he does not defend himself.\n\nOfficer Barbrady absurdly exaggerates the weight of the crime and begins an investigation (since he really has nothing better to do that day), but is unable to come up with any solid leads. He seeks help from Josh, a convicted toilet-paperer, who is serving a three-week sentence in Park County Juvenile Hall for toilet papering over 600 houses in less than a year. After several interviews, during which Josh applies psychological pressure on Barbrady, he comes a little closer to solving the case. Later, Barbrady forces a confession out of Butters after injecting him with sodium pentothal and interrogating him for over forty hours, but Butters' parents, furious, arrive to absolve him for confessing (yet again) to a crime that he did not commit. After seeing Butters get in trouble for their actions, Stan and Kenny are finally convinced that they ought to confess. Stan tells Cartman that, if he has a conscience, he will do the same. Cartman, however, is completely oblivious to the concept of \"feeling bad for other people\" and is utterly bewildered at his friends' reasoning.\n\nThe next morning, Barbrady brings Josh along with him to Principal Victoria's office, but before he can speak, Mr. Mackey announces that the true toilet paperer has already confessed. Just then, Stan, Kyle and Kenny rush into the office, only to find out that it is Cartman, having obviously done it in a bid to secure a better deal for himself: each of the boys ends up with two-week detention, except for Cartman, who gets only one for \"being brave\" (Cartman considered it a pyrrhic victory, as he later laments having to spend one week of detention). Kyle is outraged and he finds this unfair because he was supposed to confess and do the right thing not Cartman. Josh manages to trick the police and flee. At the conclusion of the episode, he calls Officer Barbrady and thanks him for enabling his escape. Despite Barbrady's pleas—\"Josh, you have to go back to Juvenile Hall: you only have a three-week sentence!\"—Josh puts down the phone and, armed with bags of toilet paper, slowly approaches the White House as sinister music plays in the background.\n\n\n\n"}
{"id": "15184417", "url": "https://en.wikipedia.org/wiki?curid=15184417", "title": "Tongue cleaner", "text": "Tongue cleaner\n\nA tongue cleaner (also called a tongue scraper or tongue brush) is an oral hygiene device designed to clean the coating on the upper surface of the tongue. While there is tentative benefit from the use of a tongue cleaner it is insufficient to draw clear conclusions regarding bad breath. A 2006 Cochrane review found tentative evidence of decreased levels of odor molecules.\n\nThe large surface area and lingual papilla are anatomical features of the tongue that promote tongue coating by retaining microorganisms and oral debris consisting of food, saliva and dead epithelial cells. Tongue cleaning is done less often than tooth brushing, flossing, and using mouthwash.\n\nWhile there is tentative benefit from the use of a tongue cleaner it is insufficient to draw clear conclusions with respect to bad breath. A 2006 Cochrane review found tentative evidence of decreased levels of odor molecules.\n\nSome studies have shown that it is the bacteria on the tongue which often produce malodorous compounds and fatty acids that may account for 80% to 85% of all cases of bad breath. The remaining 15% to 20% of cases originate in the stomach, from the tonsils, from decaying food stuck between the teeth, gum disease, dental caries (cavities or tooth decay) or plaque accumulated on the teeth. In addition, degradation of oral debris by microorganisms, produce organosulfur compounds (volatile sulphur compounds) on the posterior (rear) of the tongue.\n\nThe tongue is normally pink in appearance. It may acquire a white or colored coating due to diet, reduced salivary flow, reduced oral hygiene or tongue anatomy. The thickness of the tongue coating can also vary. Tongue cleaning can reduce this coating to make it cleaner and to help return it to its natural pink color.\n\nThe tongue surface can be a reservoir for tooth pathogens and periodontal pathogens. It can contribute to the recolonization of tooth surfaces. People with periodontal disease are more likely to have a thicker tongue coating and a microbial flora that produces more volatile sulphur compounds compared to those who have healthy periodontal tissues. Tongue cleaning might help to reduce halitosis, dental caries and periodontal disease.\n\nTongue cleaning can cause discomfort. Improper use of a tongue cleaner may induce the gag reflex and/or vomiting. Overuse of a tongue cleaner may also cause damage to the taste buds. Some people have inappropriately used the tongue cleaner to scrape or brush the lingual tonsils (tongue tonsils).\n\nThere has been one reported case where a woman possibly had infective endocarditis from bacteremia following the use of a tongue cleaner. Individuals with previous infective endocarditis and high-risk cardiac valves may be at a higher risk from bacteremia.\n\nAyurveda, the practice of traditional Indian medicine, recommends tongue cleaning as part of one's daily hygiene regimen, to remove the toxic debris, known as Ama. Tongue cleaning has existed in Ayurvedic practice since ancient times, using tongue scrapers made from copper, silver, gold, tin or brass. In modern time, plastic scrapers are used in India and the Far East.\n\nTongue hygiene has been practiced for centuries in Africa, Arabia, Europe, South America and many eastern and oriental cultures. The various materials used for tongue cleaners include thin flexible wood sections, metals, ivory, mother-of-pearl, whalebone, celluloid, tortoiseshell, and plastic.\n\nWestern civilizations placed less emphasis on tongue cleaning. Between the 15th and 19th century, tongue cleaning was primarily practiced by those who were affluent. It was recorded in Europe in the 18th and 19th centuries that Romans also performed tongue cleaning. In the 20th century, a wide variety of tongue cleaning devices came on the commercial market.\n\nThe top surface of the tongue can be cleaned using a tongue cleaner, a tongue brush/scraper or a toothbrush. However, toothbrushes are not considered as effective for this purpose because they have a smaller width and are designed for brushing teeth, which have a solid structure unlike the spongy tissue of the tongue. This can reduce its ability to remove debris and microorganisms. Some toothbrush designs have projections on the back of their heads to act as a tongue cleaner. An electric tongue cleaner is also available.\n\nErgonomic tongue cleaners are shaped in accordance with the anatomy of the tongue, and are optimized to lift and trap the plaque coating and effectively clean the surface of the tongue. There are many different types of tongue cleaners. They can be plastic or metal straps, plastic and/or small brush bristles that form \"rakes\" or circular devices with handles. Their effectiveness varies depending on the shape, dimensions, configuration, quality of the contact surfaces and materials used. Tongue cleaners are mostly inexpensive, small, easy to clean and durable. In addition, tongue cleaning gels can be used in conjunction with tongue cleaners. Its antibacterial agents may enhance the cleaning effects.\n\n\n"}
{"id": "18014816", "url": "https://en.wikipedia.org/wiki?curid=18014816", "title": "Tractor PTO auger", "text": "Tractor PTO auger\n\nCommonly referred to as Post Hole Diggers and Earth Augers, Tractor PTO Augers are implements used in conjunction with a tractor's Power Take Off drive, and a tractor's 3 point hitch.\n\nThe basic concept of a tractor PTO auger is to harness the tractor's available energy by attaching a PTO shaft to a tractor's PTO drive in order to drill a hole of predetermined size (size of the auger shaft and diameter) and depth into the ground. This in turn will provide power to the Tractor PTO Auger's gearbox. Most modern Tractor PTO Auger gearboxes come standard with a shear bolt to protect the gear drive if the auger encounters an obstruction such as rock during drilling a hole.\n\nTractor PTO Augers connect via 3 point hitch to subcompact tractors and mid-size tractors.\n"}
{"id": "4256633", "url": "https://en.wikipedia.org/wiki?curid=4256633", "title": "Trimble (company)", "text": "Trimble (company)\n\nTrimble Inc. is a Sunnyvale, California-based developer of Global Navigation Satellite System (GNSS) receivers, laser rangefinders, unmanned aerial vehicles (UAVs), inertial navigation systems and software processing tools. The company was founded in November 1978.\n\nTrimble Navigation was founded in November 1978 by Charles Trimble and two partners from Hewlett Packard, initially operating from Los Altos, California.\n\nBy the end of 2016, the company had 8,388 employees, with more than half of employees in locations outside the United States. \n\nThe company's acquisitions include Pocket Mobile AB, @Road, Cengea Solutions Inc., Datacom Software Research, Spectra Precision Group, Tripod Data Systems, Advanced Public Safety, Inc., ALK Technologies, Apache Technologies, Acutest Engineering Solutions Ltd, Applanix, Géo-3D, INPHO, Gatewing, Gehry Technologies, Meridian Systems, NTech Industries, Pacific Crest, Quantm, Accubid Systems, SketchUp, QuickPen International, SECO Mfg. Co., Inc., Visual Statement, XYZ Solutions, Inc, Tekla, ThingMagic, Spime Inc., Punch Telematix NV, TMW Systems and TopoSys Gmbh.\n\nTheir role in building information modeling (BIM), architecture and construction has been growing. Most publicised was their 2012 acquisition of the 3D modeling software package SketchUp from Google. As of 2014 they also own Tekla (BIM modelling), Vico Office (BIM data handling) and Gehry Technologies' GTeam (project coordination). In 2016, Trimble acquired Sefaira (sustainability analysis software including energy modeling and daylight visualization). On April 23, 2018, Trimble entered into an agreement to acquire privately held Viewpoint from investment firm Bain Capital in an all-cash transaction of US$1.2bn, with an expected completion in Q3 of 2018.\n\nThe company changed its name from Trimble Navigation Limited to Trimble Inc.; the name change and change in legal domicile became effective October 1, 2016. Trimble Inc. continued to operate without change or material impacts to stakeholders. The corporate headquarters remained in California.\n\nTrimble sells products and services into the following industries: land survey, construction, agriculture, transportation, telecommunications, asset tracking, mapping, utilities, mobile resource management, and government.\n"}
{"id": "35783374", "url": "https://en.wikipedia.org/wiki?curid=35783374", "title": "VersaLogic", "text": "VersaLogic\n\nVersaLogic is a privately held corporation located in Tualatin, Oregon, USA that designs and manufactures board-level products for embedded systems. The company's products are used by OEMs in a wide range of markets including medical, transportation, security, instrumentation, robotics, avionics, and unmanned vehicles.\n\nVersaLogic was founded in 1976 in Eugene, Oregon, by Len Crane and Gary Harris. The company consolidated operation in 2010 to the western edge of Eugene into the former headquarters of Obie Media along 11th Avenue. At that time, the privately held VersaLogic had grown to 70 employees. In November 2012, the company announced it would relocate to the Portland metropolitan area where it had opened a research office in 2010 in Tigard. The new headquarters opened in 2013 in Tualatin.\n\nThe company's headquarters are located in Tualatin Oregon within the Portland metropolitan area. VersaLogic is registered to the ISO 9001:2015 standard.\n\nVersaLogic designs and manufactures embedded computer products in several standards-based form factors including EBX, EPIC, PC/104, SUMIT, and COM Express. In 2012, the company provided computer components for torpedoes to the United States military. VersaLogic components have been used in UAVs.\n\n\n"}
{"id": "4075466", "url": "https://en.wikipedia.org/wiki?curid=4075466", "title": "Weeder", "text": "Weeder\n\nA number of common weeding tools are designed to ease the task of removing weeds from gardens and lawns.\n\n\n\n"}
