{"id": "14658070", "url": "https://en.wikipedia.org/wiki?curid=14658070", "title": "Allomothering", "text": "Allomothering\n\nAllomothering, \"alloparental\", \"infant handling\", or non-maternal infant care, is performed by any group member other than the mother or genetic father and thus is distinguished from parental care. It is a widespread phenomenon among mammals and birds. Depending on age-sex composition of groups alloparents, helpers or \"handlers\" can be non-reproductive males in polyandrous systems, reproductive or non-reproductive adult females, young or older juveniles or older siblings interested in abetting their own genetic material via their siblings (Theory of Inclusive Fitness). Allomothering comprises a wide variety of behaviours including: carrying, provisioning, grooming, touching, nursing (allonursing) and protecting the infants from predators or conspecifics.\n\nAllomothering is particularly common among the Primate order. Vervets, cebus monkeys, squirrel monkeys, and macaques are all known for allomothering performed by females not closely related to the pair. These alloparents help by carrying the infant, providing food, and guarding the infant from predators. Cebus monkey females have been known to regularly nurse (allonurse) infants who are not their own (cf. wet nurse). In these species allonursing is performed by related and unrelated females. Moreover, about 10% of nursing bouts are attributed to allonursing. Allomothering can also be performed by non-reproductive helpers like in the callitrichids (marmosets and tamarins). In the Callitrichids, allomothering care goes beyond many other species and infants are spontaneously provisioned by all group members without a prior begging calls on part of the infants. These species practice facultative cooperative breeding, where a single dominant female reproduces and other group members (fathers, other males and non-reproductive juveniles) provide the majority of care to the infants.\n\nA number of adaptive functions have been proposed to account for the widespread incidences of allomaternal care in mammalian and avian species. Jane Lancaster noted the reproductive benefits for primates as k-strategists in learning to be better mothers, or acquiring mothering skills. Her learning-to-mother hypothesis postulates that primate females with no children of their own participate in allomothering, and evidence from studies by Sarah Hrdy and Lynn Fairbanks shows that females without offspring \"tried to allomother more frequently than what you'd expected based on their proportion of the group's population, while parous females tried it much less than expected from their population in the group.\" The hypothesis is supported by evidence of the success of allomothering as a learning technique. \"Lynn Fairbanks studied vervets and found that first-time mothers with high alloparenting experience raised 100% of their first offspring to maturity, but mothers with low experience had less than a 50% survival rate of their first infant.\" Other hypothesis include \"alliance-formation\", where subordinate allomothers endeavour to form social alliances with dominant mothers by interacting with their infants. Under kin-selection theory, related allomothers may improve their inclusive fitness if the allomothering behaviour contributes to the survival or faster reproductive rate of the mother. Finally, allomaternal care has been suggested to be a by-product of maternal care. However, this hypothesis would not explain the high levels of care seen by juvenile, subadult or unrelated adult males in many [primate] species.\n\nAn infant's birthmother, in a climate of allomothering, may gain time relieved from parental duties, allowing her to forage more efficiently or reproduce more quickly (i.e. reduce her inter-birth interval). In some cases it may also improve the chances for her infant to be adopted by another resident female should she die. Infants may also benefit through a faster maturation rate or earlier weaning time. They may also gain valuable social skills by interacting with alloparents. Finally, infants may form social-alliances of their own and improve their chances of having future dispersal partners.\n\nAllomothering care may not always be beneficial. In some cases \"aunting-to-death\" has been reported where females withhold an infant from their mother until the point where the infant dies. In other cases infants may be kidnapped and receive life-threatening bites or hits from an alloparent. Mothers are often restrictive of allomothers attempts to touch or handle their infants in species where the risk of injury or death is high (e.g. resident-nepotistic Cercopithecine species like Japanese macaques).\n\n"}
{"id": "24284875", "url": "https://en.wikipedia.org/wiki?curid=24284875", "title": "Antenna amplifier", "text": "Antenna amplifier\n\nIn electronics, an antenna amplifier (also: aerial eamplifier (booster), Am antennefier) is a device that amplifies an antenna signal, usually into an output with the same impedance as the input impedance. Typically 75 ohm for coaxial cable and 300 ohm for twin-lead cable.\n\nAn \"antenna amplifier\" boosts a radio signal considerably for devices that receive radio waves. Many devices have an RF amplifier stage in their circuitry, that amplifies the antenna signal, these include, but are not limited to; radios, televisions, mobile phones and Wi-Fi and Bluetooth devices. Amplifiers amplify everything, both the desired signal present at the antenna, and the noise. Typical signal noises include: ambient background noise (electric brush noise from electric motors, high voltage sources from, for example a gasoline engine ignition, or large dispersed currents in the vicinity of the desired reception electric fence). To add, consideration must be taken for the noise generated by the amplifier itself and all other electrical noise which may be generated by the device that is to receive a signal, for example a lot of consideration has to go into mobile phone circuitry design to eliminate as much noise from its own circuitry in order to not disturb the desired transmission signals from its own antenna(ae).\n\nAn indoor antenna may include an amplifier circuit, whereby powered reception of the signal can help with capturing as much of an FM, UHF/VHF signal, for amplifying a radio or television signal. Its draw backs are that any noise is usually amplified as well, and a common result from this is amplification of ghost images (for analog signals), and any other perturberances that may be existing locally or even extra terrestrially like the Cosmic microwave background radiation for devices that work in that frequency range.\n\nThe key to a \"good\" level of input at your receiver with the minimum amount noise includes many design considerations in an electrical amplifier. In theory it is best if you amplify a \"clean\" signal to a higher level than a \"noisy\" signal to a higher level, and many circuits include filters to remove all but the desired reception signal. Some consideration has to be taken for cable loss and the signal frequencies desired for example higher frequency (VHF or higher: 2.4 GHz Wi-Fi/third generation mobile phone.) the more the loss that the cable has, and the more susceptible the transmission cable is to noise degradation. Starting with a signal from the antenna which is then directed through a coaxial cable, the amount of loss depends upon a number of factors, cable type and cable length are the two most important. Cable is rated in db loss per length of cable at a specified frequency, for example RG-6 cable is the cable most used for Television reception.\nChannel 32 which is 580 MHz, Channel 52 is 700 MHz a 5 db loss At TV channel 2, the cable would have a loss of 1.4 db. So at channel 18 you would lose more than 1/2 the power in 100' of cable between the antenna and the TV.\n\nSee Also:\nBalun\nCosmic microwave background radiation\n"}
{"id": "2769868", "url": "https://en.wikipedia.org/wiki?curid=2769868", "title": "Arrilaser", "text": "Arrilaser\n\nThe Arrilaser is a digital film recorder made by Arri which writes digital movie files onto film after compositing and audio mastering on the computer. Files are sent to the device via a fast gigabit Ethernet connection. The Arrilaser uses three solid-state lasers (red, green, and blue) as a light source, and significantly reduces the cost of recording digital images onto film. Its chief competitor is Celco's Film Fury CRT-based recorder. As of July 2009, there were more than 250 Arrilasers installed worldwide.\n\nIn 2002, the manufacturer of the Arrilaser was honored with the \"Scientific and Engineering Award\" (Oscar Academy Awards) from the \"Academy of Motion Pictures, Arts and Sciences\" (AMPAS) in Hollywood: \"The Arrilaser film recorder demonstrates a high level of engineering resulting in a compact, user-friendly, low-maintenance device while at the same time maintaining outstanding speed, exposure ratings and image quality.\" (The Oscars Official site).\n\nAgain in 2012, AMPAS awarded an Oscar to the makers of the Arrilaser film recorder. The \"Academy Award of Merit\" Oscar from the Academy Scientific and Technical Award ceremony was given on Feb. 11, 2012 to Franz Kraus, Johannes Steurer and Wolfgang Riedel.\n\n\n"}
{"id": "432000", "url": "https://en.wikipedia.org/wiki?curid=432000", "title": "Arthur Compton", "text": "Arthur Compton\n\nArthur Holly Compton (September 10, 1892 – March 15, 1962) was an American physicist who won the Nobel Prize in Physics in 1927 for his 1923 discovery of the Compton effect, which demonstrated the particle nature of electromagnetic radiation. It was a sensational discovery at the time: the wave nature of light had been well-demonstrated, but the idea that light had both wave and particle properties was not easily accepted. He is also known for his leadership of the Manhattan Project's Metallurgical Laboratory, and served as Chancellor of Washington University in St. Louis from 1945 to 1953.\n\nIn 1919, Compton was awarded one of the first two National Research Council Fellowships that allowed students to study abroad. He chose to go to Cambridge University's Cavendish Laboratory in England, where he studied the scattering and absorption of gamma rays. Further research along these lines led to the discovery of the Compton effect. He used X-rays to investigate ferromagnetism, concluding that it was a result of the alignment of electron spins, and studied cosmic rays, discovering that they were made up principally of positively charged particles.\n\nDuring World War II, Compton was a key figure in the Manhattan Project that developed the first nuclear weapons. His reports were important in launching the project. In 1942, he became head of the Metallurgical Laboratory, with responsibility for producing nuclear reactors to convert uranium into plutonium, finding ways to separate the plutonium from the uranium and to design an atomic bomb. Compton oversaw Enrico Fermi's creation of Chicago Pile-1, the first nuclear reactor, which went critical on December 2, 1942. The Metallurgical Laboratory was also responsible for the design and operation of the X-10 Graphite Reactor at Oak Ridge, Tennessee. Plutonium began being produced in the Hanford Site reactors in 1945.\n\nAfter the war, Compton became Chancellor of Washington University in St. Louis. During his tenure, the university formally desegregated its undergraduate divisions, named its first female full professor, and enrolled a record number of students after wartime veterans returned to the United States.\n\nArthur Compton was born on September 10, 1892, in Wooster, Ohio, the son of Elias and Otelia Catherine (née Augspurger) Compton, who was named American Mother of the Year in 1939. They were an academic family. Elias was dean of the University of Wooster (later The College of Wooster), which Arthur also attended. Arthur's eldest brother, Karl, who also attended Wooster, earned a PhD in physics from Princeton University in 1912, and was president of MIT from 1930 to 1948. His second brother Wilson likewise attended Wooster, earned his PhD in economics from Princeton in 1916 and was president of the State College of Washington, later Washington State University from 1944 to 1951. All three brothers were members of the Alpha Tau Omega fraternity.\n\nCompton was initially interested in astronomy, and took a photograph of Halley's Comet in 1910. Around 1913, he described an experiment where an examination of the motion of water in a circular tube demonstrated the rotation of the earth. That year, he graduated from Wooster with a Bachelor of Science degree and entered Princeton, where he received his Master of Arts degree in 1914. Compton then studied for his PhD in physics under the supervision of Hereward L. Cooke, writing his dissertation on \"The intensity of X-ray reflection, and the distribution of the electrons in atoms\".\n\nWhen Arthur Compton earned his PhD in 1916, he, Karl and Wilson became the first group of three brothers to earn PhDs from Princeton. Later, they would become the first such trio to simultaneously head American colleges. Their sister Mary married a missionary, C. Herbert Rice, who became the principal of Forman Christian College in Lahore. In June 1916, Compton married Betty Charity McCloskey, a Wooster classmate and fellow graduate. They had two sons, Arthur Alan and John Joseph Compton.\n\nCompton spent a year as a physics instructor at the University of Minnesota in 1916–17, then two years as a research engineer with the Westinghouse Lamp Company in Pittsburgh, where he worked on the development of the sodium-vapor lamp. During World War I he developed aircraft instrumentation for the Signal Corps.\n\nIn 1919, Compton was awarded one of the first two National Research Council Fellowships that allowed students to study abroad. He chose to go to Cambridge University's Cavendish Laboratory in England. Working with George Paget Thomson, the son of J. J. Thomson, Compton studied the scattering and absorption of gamma rays. He observed that the scattered rays were more easily absorbed than the original source. Compton was greatly impressed by the Cavendish scientists, especially Ernest Rutherford, Charles Galton Darwin and Arthur Eddington, and he ultimately named his second son after J. J. Thomson.\n\nFor a time Compton was a deacon at a Baptist church. \"Science can have no quarrel\", he said, \"with a religion which postulates a God to whom men are as His children.\"\n\nReturning to the United States, Compton was appointed Wayman Crow Professor of Physics, and Head of the Department of Physics at Washington University in St. Louis in 1920. In 1922, he found that X-ray quanta scattered by free electrons had longer wavelengths and, in accordance with Planck's relation, less energy than the incoming X-rays, the surplus energy having been transferred to the electrons. This discovery, known as the \"Compton effect\" or \"Compton scattering\", demonstrated the particle concept of electromagnetic radiation.\n\nIn 1923, Compton published a paper in the \"Physical Review\" that explained the X-ray shift by attributing particle-like momentum to photons, something Einstein had invoked for his 1905 Nobel Prize–winning explanation of the photo-electric effect. First postulated by Max Planck in 1900, these were conceptualized as elements of light \"quantized\" by containing a specific amount of energy depending only on the frequency of the light. In his paper, Compton derived the mathematical relationship between the shift in wavelength and the scattering angle of the X-rays by assuming that each scattered X-ray photon interacted with only one electron. His paper concludes by reporting on experiments that verified his derived relation:\nwhere\n\nThe quantity is known as the Compton wavelength of the electron; it is equal to . The wavelength shift lies between zero (for ) and twice the Compton wavelength of the electron (for ). He found that some X-rays experienced no wavelength shift despite being scattered through large angles; in each of these cases the photon failed to eject an electron. Thus the magnitude of the shift is related not to the Compton wavelength of the electron, but to the Compton wavelength of the entire atom, which can be upwards of 10,000 times smaller.\n\n\"When I presented my results at a meeting of the American Physical Society in 1923,\" Compton later recalled, \"it initiated the most hotly contested scientific controversy that I have ever known.\" The wave nature of light had been well demonstrated, and the idea that it could have a dual nature was not easily accepted. It was particularly telling that diffraction in a crystal lattice could only be explained with reference to its wave nature. It earned Compton the Nobel Prize in Physics in 1927. Compton and Alfred W. Simon developed the method for observing at the same instant individual scattered X-ray photons and the recoil electrons. In Germany, Walther Bothe and Hans Geiger independently developed a similar method.\n\nIn 1923, Compton moved to the University of Chicago as Professor of Physics, a position he would occupy for the next 22 years. In 1925, he demonstrated that the scattering of 130,000-volt X-rays from the first sixteen elements in the periodic table (hydrogen through sulfur) were polarized, a result predicted by J. J. Thomson. William Duane from Harvard University spearheaded an effort to prove that Compton's interpretation of the Compton effect was wrong. Duane carried out a series of experiments to disprove Compton, but instead found evidence that Compton was correct. In 1924, Duane conceded that this was the case.\n\nCompton investigated the effect of X-rays on the sodium and chlorine nuclei in salt. He used X-rays to investigate ferromagnetism, concluding that it was a result of the alignment of electron spins. In 1926, he became a consultant for the Lamp Department at General Electric. In 1934, he returned to England as Eastman visiting professor at Oxford University. While there General Electric asked him to report on activities at General Electric Company plc's research laboratory at Wembley. Compton was intrigued by the possibilities of the research there into fluorescent lamps. His report prompted a research program in America that developed it.\n\nCompton's first book, \"X-Rays and Electrons\", was published in 1926. In it he showed how to calculate the densities of diffracting materials from their X-ray diffraction patterns. He revised his book with the help of Samuel K. Allison to produce \"X-Rays in Theory and Experiment\" (1935). This work remained a standard reference for the next three decades.\n\nBy the early 1930s, Compton had become interested in cosmic rays. At the time, their existence was known but their origin and nature remained speculative. Their presence could be detected using a spherical \"bomb\" containing compressed air or argon gas and measuring its electrical conductivity. Trips to Europe, India, Mexico, Peru and Australia gave Compton the opportunity to measure cosmic rays at different altitudes and latitudes. Along with other groups who made observations around the globe, they found that cosmic rays were 15 per cent more intense at the poles than at the equator. Compton attributed this to the effect of cosmic rays being made up principally of charged particles, rather than photons as Robert Millikan had suggested, with the latitude effect being due to Earth's magnetic field.\n\nIn April 1941, Vannevar Bush, head of the wartime National Defense Research Committee (NDRC), created a special committee headed by Compton to report on the NDRC uranium program. Compton's report, which was submitted in May 1941, foresaw the prospects of developing radiological weapons, nuclear propulsion for ships, and nuclear weapons using uranium-235 or the recently discovered plutonium. In October he wrote another report on the practicality of an atomic bomb. For this report, he worked with Enrico Fermi on calculations of the critical mass of uranium-235, conservatively estimating it to be between and . He also discussed the prospects for uranium enrichment with Harold Urey, spoke with Eugene Wigner about how plutonium might be produced in a nuclear reactor, and with Robert Serber about how the plutonium produced in a reactor might be separated from uranium. His report, submitted in November, stated that a bomb was feasible, although he was more conservative about its destructive power than Mark Oliphant and his British colleagues.\n\nThe final draft of Compton's November report made no mention of using plutonium, but after discussing the latest research with Ernest Lawrence, Compton became convinced that a plutonium bomb was also feasible. In December, Compton was placed in charge of the plutonium project. He hoped to achieve a controlled chain reaction by January 1943, and to have a bomb by January 1945. To tackle the problem, he had the different research groups working on plutonium and nuclear reactor design at Columbia University, Princeton University and the University of California, Berkeley, concentrated together as the Metallurgical Laboratory in Chicago. Its objectives were to produce reactors to convert uranium to plutonium, to find ways to chemically separate the plutonium from the uranium, and to design and build an atomic bomb.\n\nIn June 1942, the United States Army Corps of Engineers assumed control of the nuclear weapons program and Compton's Metallurgical Laboratory became part of the Manhattan Project. That month, Compton gave Robert Oppenheimer responsibility for bomb design. It fell to Compton to decide which of the different types of reactor designs that the Metallurgical Laboratory scientists had devised should be pursued, even though a successful reactor had not yet been built.\n\nWhen labor disputes delayed construction of the Metallurgical Laboratory's new home in the Red Gate Woods, Compton decided to build Chicago Pile-1, the first nuclear reactor, under the stands at Stagg Field. Under Fermi's direction, it went critical on December 2, 1942. Compton arranged for Mallinckrodt to undertake the purification of uranium ore, and with DuPont to build the plutonium semi-works at Oak Ridge, Tennessee.\n\nA major crisis for the plutonium program occurred in July 1943, when Emilio Segrè's group confirmed that plutonium created in the X-10 Graphite Reactor at Oak Ridge contained high levels of plutonium-240. Its spontaneous fission ruled out the use of plutonium in a gun-type nuclear weapon. Oppenheimer's Los Alamos Laboratory met the challenge by designing and building an implosion-type nuclear weapon.\nCompton was at the Hanford site in September 1944 to watch the first reactor being brought online. The first batch of uranium slugs was fed into Reactor B at Hanford in November 1944, and shipments of plutonium to Los Alamos began in February 1945. Throughout the war, Compton would remain a prominent scientific adviser and administrator. In 1945, he served, along with Lawrence, Oppenheimer, and Fermi, on the Scientific Panel that recommended military use of the atomic bomb against Japan. He was awarded the Medal for Merit for his services to the Manhattan Project.\n\nAfter the war ended, Compton resigned his chair as Charles H. Swift Distinguished Service Professor of Physics at the University of Chicago and returned to Washington University in St. Louis, where he was inaugurated as the university's ninth Chancellor in 1946. During Compton's time as Chancellor, the university formally desegregated its undergraduate divisions in 1952, named its first female full professor, and enrolled record numbers of students as wartime veterans returned to the United States. His reputation and connections in national scientific circles allowed him to recruit many nationally renowned scientific researchers to the university. Despite Compton's accomplishments, he was criticized then, and subsequently by historians, for moving too slowly toward full racial integration, making Washington University the last major institution of higher learning in St. Louis to open its doors to African Americans.\n\nCompton retired as Chancellor in 1954, but remained on the faculty as Distinguished Service Professor of Natural Philosophy until his retirement from the full-time faculty in 1961. In retirement he wrote \"Atomic Quest\", a personal account of his role in the Manhattan Project, which was published in 1956.\n\nCompton was one of a handful of scientists and philosophers to propose a two-stage model of free will. Others include William James, Henri Poincaré, Karl Popper, Henry Margenau, and Daniel Dennett. In 1931, Compton championed the idea of human freedom based on quantum indeterminacy, and invented the notion of amplification of microscopic quantum events to bring chance into the macroscopic world. In his somewhat bizarre mechanism, he imagined sticks of dynamite attached to his amplifier, anticipating the Schrödinger's cat paradox, which was published in 1935.\n\nReacting to criticisms that his ideas made chance the direct cause of people's actions, Compton clarified the two-stage nature of his idea in an \"Atlantic Monthly\" article in 1955. First there is a range of random possible events, then one adds a determining factor in the act of choice.\n\nCompton died in Berkeley, California, from a cerebral hemorrhage on March 15, 1962. He was survived by his wife and sons, and buried in the Wooster Cemetery in Wooster, Ohio. Before his death, he was Professor-at-Large at the University of California, Berkeley for Spring 1962.\n\nCompton received many awards in his lifetime, including the Nobel Prize for Physics in 1927, the Matteucci Gold Medal in 1933, the Royal Society's Hughes Medal and the Franklin Institute's Benjamin Franklin Medal in 1940. He is commemorated in various ways. The Compton crater on the Moon is co-named for Compton and his brother Karl. The physics research building at Washington University in St Louis is named in his honor, as is the university's top fellowship for undergraduate students studying math, physics, or planetary science. Compton invented a more gentle, elongated, and ramped version of the speed bump called the \"Holly hump,\" many of which are on the roads of the Washington University campus. The University of Chicago Residence Halls remembered Compton and his achievements by dedicating Arthur H. Compton House in Chicago in his honor. It is now listed as a National Historic Landmark. Compton also has a star on the St. Louis Walk of Fame. NASA's Compton Gamma Ray Observatory was named in honor of Compton. The Compton effect is central to the gamma ray detection instruments aboard the observatory.\n\n\n\n"}
{"id": "6313537", "url": "https://en.wikipedia.org/wiki?curid=6313537", "title": "Artificial pancreas", "text": "Artificial pancreas\n\nThe artificial pancreas is a technology in development to help people with diabetes, primarily type 1, automatically and continuously control their blood glucose level by providing the substitute endocrine functionality of a healthy pancreas.\n\nThe endocrine functionality of the pancreas is provided by islet cells which produce the hormones insulin and glucagon. Artificial pancreatic technology mimics the secretion of these hormones into the bloodstream in response to the body's changing blood glucose levels. Maintaining balanced blood sugar levels is crucial to the function of the brain, liver, and kidneys. Therefore, for type 1 patients, it is necessary that the levels be kept balanced when the body cannot produce insulin itself.\n\nThe artificial pancreas is a broad term for different bio-engineering strategies currently in development to achieve these requirements. Different bio-engineering approaches under consideration include:\n\n\nThe medical equipment approach involves combining a continuous glucose monitor and an implanted insulin pump that can function together with a computer-controlled algorithm to replace the normal function of the pancreas. The development of continuous glucose monitors has led to the progress in artificial pancreas technology using this integrated system.\n\nThe original devices for use in type 1 diabetes were blood glucose meters. Continuous blood glucose monitors are one of the set of devices that make up an artificial pancreas device system, the other being an insulin pump, and a glucose meter to calibrate the device. Continuous glucose monitors are a more recent breakthrough and have begun to hit the markets for patient use after approval from the FDA. Both the traditional and the continuous monitor require manual insulin delivery or carbohydrate intake depending on the readings from the devices. While the traditional blood glucose meters require the user to prick their finger every few hours to obtain data, continuous monitors use sensors placed just under the skin on the arm or abdomen to deliver blood sugar level data to receivers or smartphone apps as often as every few minutes. The sensors can be used for up to ten days. A number of different continuous monitors are currently approved by the FDA.\n\nThe first continuous glucose monitor (CGM) was approved in December 2016. Developed by Dexcom, the G5 Mobile Continuous Monitoring System requires users to prick their fingers twice a day (as opposed to the typical average 8 times daily with the traditional meters) in order to calibrate the sensors. The sensors last up to seven days. The device uses Bluetooth technology to warn the user either through a handheld receiver or app on a smartphone if blood glucose levels reach below a certain point. The cost for this device excluding any co-insurance is an estimated $4,800 a year.\n\nAbbott Laboratories' FreeStyle Libre CGM was approved in September 2017. It does not support smartphone use. This device does not require finger pricks at all and the sensor, placed on the upper arm, lasts 10 days. The estimated cost for this monitor is $1,300 a year.\n\nDexcom's next G6 model CGM was approved in March 2018, which can last up to ten days and does not need finger prick calibration. Like Medtronic's monitor, it can predict glucose level trends. It is compatible for integration into insulin pumps. \n\nUnlike the continuous sensor alone, the closed-loop system requires no user input in response to reading from the monitor; the monitor and insulin pump system automatically delivers the correct amount of hormone calculated from the readings transmitted. The system is what makes up the artificial pancreas device.\n\nIn September 2016, the FDA approved the Medtronic MiniMed 670G, which was the first approved hybrid closed loop system. The device senses a diabetic person's basal insulin requirement and automatically adjusts its delivery to the body. It is made up of a continuous glucose monitor, an insulin pump, and a glucose meter for calibration. It automatically functions to modify the level of insulin delivery based off the detection of blood glucose levels by continuous monitor. It does this by sending the blood glucose data through an algorithm that analyzes and makes the subsequent adjustments. The system has two modes. Manual mode lets the user choose the rate at which basal insulin is delivered. Auto mode regulates basal insulin levels from the continuous monitor's readings every five minutes.\n\nThe device was originally available only to those aged 14 or older, and in June 2018 was approved by the FDA for use in children aged 7-14. Families have reported better sleep quality from use of the new system, as they do not have to worry about manually checking blood glucose levels during the night. The full cost of the system is $3700, but patients have the opportunity to get it for less.\n\nA team at Boston University working in collaboration with Massachusetts General Hospital on a dual hormone artificial pancreas system began clinical trials on their device called the Bionic Pancreas in 2008. In 2016, the Public Benefit Corporation Beta Bionics was formed. In conjunction with the formation of the company, Beta Bionics changed the preliminary name for their device from the Bionic Pancreas to the iLet. The device uses a closed-loop system to deliver both insulin and glucagon in response to sensed blood glucose levels. While not yet approved for public use, the 4th generation iLet prototype, presented in 2017, is around the size of an iPhone, with a touchscreen interface. It contain two chambers for both insulin and glucagon, and the device is configurable for use with only one hormone, or both. While trials continue to be run, the iLet has a projected final approval for the insulin-only system in 2020.\n\nFour studies on different artificial pancreas systems are being conducted starting in 2017 and going into the near future. The projects are funded by the National Institute of Diabetes and Digestive and Kidney Diseases(NIDDK), and are the final part of testing the devices before applying for approval for use. Participants in the studies are able to live their lives at home while using the devices and being monitored remotely for safety, efficacy, and a number of other factors.\n\nThe International Diabetes Closed-Loop trial, led by researchers from the University of Virginia, is testing a closed-loop system called inControl, which has a smartphone user interface. 240 people of ages 14 and up are participating for 6 months.\n\nA full-year trial led by researchers from the University of Cambridge started in May 2017 and has enrolled an estimated 150 participants of ages 6 to 18 years. The artificial pancreas system being studied uses a smartphone and has a low glucose feature to improve glucose level control.\n\nThe International Diabetes Center in Minneapolis, Minnesota, in collaboration with Schneider Children's Medical Center in Petah Tikva, Israel, are planning a 6-month study that will begin in early 2019 and will involve 112 adolescents and young adults, ages 14 to 30. The main object of the study is to compare the current Medtronic 670G system to a new Medtronic-developed system. The new system has programming that aims to improve glucose control around mealtime, which is still a big challenge in the field.\n\nThe current 6-month study lead by the Bionic Pancreas team started in mid-2018 and enrolled 312 participants of ages 18 and above.\n\nThe biotechnical company Defymed, based in France, is developing an implantable bio-artificial device called MailPan which features a bio-compatible membrane with selective permeability to encapsulate different cells types, including pancreatic beta cells. The implantation of the device does not require conjunctive immuno-supressive therapy because the membrane prevents antibodies of the patient from entering the device and damaging the encapsulated cells. After being surgically implanted, the membrane sheet will be viable for years.The cells that the device holds can be produced from stem cells rather than human donors, and may also be replaced over time using input and output connections without surgery. Defymed is partially funded by JDRF, formerly known as the Juvenile Diabetes Research Foundation, but is now defined as an organization for all ages and all stages of type 1 diabetes.\n\nIn November 2018, it was announced that Defymed would partner with the Israel-based Kadimastem, a bio-pharmaceutical company developing stem-cell based regenerative therapies, to receive a two-year grant worth approximately $1.47 million for the development of a bio-artificial pancreas that would treat type 1 diabetes. Kadimastem's stem cell technology uses differentiation of human embryonic stem cells to obtain pancreatic endocrine cells. These include insulin-producing beta cells, as well as alpha cells, which produce glucagon. Both cells arrange in islet-like clusters, mimicking the structure of the pancreas. The aim of the partnership is to combine both technologies in a bio-artificial pancreas device, which releases insulin in response to blood glucose levels, to bring to clinical trial stages.\n\nThe San Diego, California based biotech company ViaCyte has also developed a product aiming to provide a solution for type 1 diabetes which uses an encapsulation device made of a semi-permeable immune reaction-protective membrane. The device contains pancreatic progenitor cells that have been differentiated from embryonic stem cells. After surgical implantation in an outpatient procedure, the cells mature into endocrine cells which arrange in islet-like clusters and mimic the function of the pancreas, producing insulin and glucagon. The technology advanced from pre-clinical studies to FDA approval for phase 1 clinical trials in 2014, and presented two-year data from the trial in June 2018. They reported that their product, called PEC-Encap, has so far been safe and well tolerated in patients at a dose below therapeutic levels. The encapsulated cells were able to survive and mature after implantation, and immune system rejection was decreased due to the protective membrane. The second phase of the trial will evaluate the efficacy of the product. ViaCyte has also been receiving financial support from JDRF on this project.\n\nIn the United States in 2006, JDRF (formerly the Juvenile Diabetes Research Foundation) launched a multi-year initiative to help accelerate the development, regulatory approval, and acceptance of continuous glucose monitoring and artificial pancreas technology.\n\nGrassroots efforts to create and commercialize a fully automated artificial pancreas system have also arisen directly from patient advocates and the diabetes community. Bigfoot Biomedical, a company founded by parents of children with T1D have created algorithms and are developing a closed loop device that monitor blood sugar and appropriately provide insulin.\n"}
{"id": "24624498", "url": "https://en.wikipedia.org/wiki?curid=24624498", "title": "Asa Lees", "text": "Asa Lees\n\nAsa Lees was a firm of textile machine manufacturers in Oldham, Lancashire. Their headquarters was the Soho Iron Works, Greenacres. It was second only in size to Platt Brothers.\n\nSamuel Lees founded a roller making works in the 1790s, it was called the Soho Works. His second son Asa Lees (1816–62) inherited the premises. He expanded the business, exporting fustian power looms to St Petersberg. He abandoned looms to concentrate on manufacturing preparation and spinning machinery. The Soho Cotton mills was converted to a Mule carriage works.\nAsa Lees became a limited company in 1868, four years after Platts and the shares were quoted on the Oldham share market until the 1890s. It never published its accounts, though its dividends were consistently higher than Platts, and remained profitable in 1928 when Platts made a loss. They were conservative in their trading, dealing only with reliable firms. They did not push for exports. They experienced rapid expansion in the 1880s under the management of Robert Taylor (1823–1912) and production peaked in 1906 when they were employing 3000 men.\n\nIn the recession of the 1930s, Platt Brothers, Howard and Bullough, Brooks and Doxey, Asa Lees, Dobson and Barlow, Joseph Hibbert, John Hetherington and Tweedales and Smalley merged to become Textile Machinery Makers Ltd., but the individual units continued to trade under their own names until the 1970, when they were rationalised into one company called Platt UK Ltd. In 1991 the company name changed to Platt Saco Lowell.\n"}
{"id": "92791", "url": "https://en.wikipedia.org/wiki?curid=92791", "title": "Audio power amplifier", "text": "Audio power amplifier\n\nAn audio power amplifier (or power amp) is an electronic amplifier that reproduces low-power electronic audio signals such as the signal from radio receiver or electric guitar pickup at a level that is strong enough for driving (or powering) loudspeakers or headphones. This includes both amplifiers used in home audio systems and musical instrument amplifiers like guitar amplifiers. It is the final electronic stage in a typical audio playback chain before the signal is sent to the loudspeakers and speaker enclosures.\n\nThe preceding stages in such a chain are low power audio amplifiers which perform tasks like pre-amplification of the signal (this is particularly associated with record turntable signals, microphone signals and electric instrument signals from pickups, such as the electric guitar and electric bass), equalization (e.g., adjusting the bass and treble), tone controls, mixing different input signals or adding electronic effects such as reverb. The inputs can also be any number of audio sources like record players, CD players, digital audio players and cassette players. Most audio power amplifiers require these low-level inputs, which are line level.\n\nWhile the input signal to an audio power amplifier, such as the signal from an electric guitar, may measure only a few hundred microwatts, its output may be a few watts for small consumer electronics devices, such as clock radios, tens or hundreds of watts for a home stereo system, several thousand watts for a nightclub's sound system or tens of thousands of watts for a large rock concert sound reinforcement system. While power amplifiers are available in standalone units, typically aimed at the hi-fi audiophile market (a niche market) of audio enthusiasts and sound reinforcement system professionals, most consumer electronics sound products, such as clock radios, boom boxes and televisions have relatively small power amplifiers that are integrated inside the chassis of the main product.\n\nThe audio amplifier was invented around 1912 by Lee De Forest, made possible by his invention of the first practical amplifying electrical component, the triode vacuum tube (or \"valve\" in British English) in 1907. The triode was a three terminal device with a control grid that can modulate the flow of electrons from the filament to the plate. The triode vacuum amplifier was used to make the first AM radio. Early audio power amplifiers were based on vacuum tubes and some of these achieved notably high audio quality (e.g., the Williamson amplifier of 1947-9). Audio power amplifiers based on transistors became practical with the wide availability of inexpensive transistors in the late 1960s. Since the 1970s, most modern audio amplifiers are based on solid state devices (transistors such as BJTs, FETs and MOSFETs). Transistor-based amplifiers are lighter in weight, more reliable and require less maintenance than tube amplifiers. In the 2010s, there are still audio enthusiasts, musicians (particularly electric guitarists, electric bassists, Hammond organ players and Fender Rhodes electric piano players, among others), audio engineers and music producers who prefer tube-based amplifiers, and what is perceived as a \"warmer\" tube sound.\nKey design parameters for audio power amplifiers are frequency response, gain, noise, and distortion. These are interdependent; increasing gain often leads to undesirable increases in noise and distortion. While negative feedback actually reduces the gain, it also reduces distortion. Most audio amplifiers are linear amplifiers operating in class AB.\n\nUntil the 1970s, most amplifiers were tube amplifiers which used vacuum tubes. During the 1970s, tube amps were increasingly replaced with transistor-based amplifiers, which were lighter in weight, more reliable, and lower maintenance. Nevertheless, there are still niche markets of consumers who continue to use tube amplifiers and tube preamplifiers in the 2010s, such as with home hi-fi enthusiasts, audio engineers and music producers (who use tube preamplifiers in studio recordings to \"warm up\" microphone signals) and electric guitarists, electric bassists and Hammond organ players, of whom a minority continue to use tube preamps, tube power amps and tube effects units. While hi-fi enthusiasts and audio engineers doing live sound or monitoring tracks in the studio typically seek out amplifiers with the lowest distortion, electric instrument players in genres such as blues, rock music and heavy metal music, among others, use tube amplifiers because they like the natural overdrive that tube amps produce when pushed hard.\n\nIn the 2000s, the Class-D amplifier, which is much more efficient than Class AB amplifiers, is widely used in consumer electronics audio products, bass amplifiers and sound reinforcement system gear, as Class D amplifiers are much lighter in weight and produce much less heat.\n\nSince modern digital devices, including CD and DVD players, radio receivers and tape decks already provide a \"flat\" signal at line level, the preamp is not needed other than as a volume control and source selector. One alternative to a separate preamp is to simply use passive volume and switching controls, sometimes integrated into a power amplifier to form an \"integrated amplifier\".\n\nThe final stage of amplification, after preamplifiers, is the output stage, where the highest demands are placed on the transistors or tubes. For this reason, the design choices made around the output device (for single-ended output stages, such as in single-ended triode amplifiers) or devices (for push-pull output stages), such as the Class of operation of the output devices is often taken as the description of the whole power amplifier. For example, a Class B amplifier will probably have just the high power output devices operating cut off for half of each cycle, while the other devices (such as differential amplifier, voltage amplifier and possibly even driver transistors) operate in Class A. In a transformerless output stage, the devices are essentially in series with the power supply and output load (such as a loudspeaker), possibly via some large capacitor and/or small resistances.\n\nFor some years following the introduction of solid state amplifiers, their perceived sound did not have the excellent audio quality of the best valve amplifiers (see valve audio amplifier). This led audiophiles to believe that \"tube sound\" or valve sound had an intrinsic quality due to the vacuum tube technology itself. In 1970, Matti Otala published a paper on the origin of a previously unobserved form of distortion: transient intermodulation distortion (TIM), later also called slew-induced distortion (SID) by others. TIM distortion was found to occur during very rapid increases in amplifier output voltage. \n\nTIM did not appear at steady state sine tone measurements, helping to hide it from design engineers prior to 1970. Problems with TIM distortion stem from reduced open loop frequency response of solid state amplifiers. Further works of Otala and other authors found the solution for TIM distortion, including increasing slew rate, decreasing preamp frequency bandwidth, and the insertion of a lag compensation circuit in the input stage of the amplifier. In high quality modern amplifiers the open loop response is at least 20 kHz, canceling TIM distortion.\n\nThe next step in advanced design was the Baxandall Theorem, created by Peter Baxandall in England. This theorem introduced the concept of comparing the ratio between the input distortion and the output distortion of an amplifier. This new idea helped audio design engineers to better evaluate the distortion processes within an amplifier.\n\nImportant applications include public address systems, theatrical and concert sound reinforcement systems, and domestic systems such as a stereo or home-theatre system. Instrument amplifiers including guitar amplifiers and electric keyboard amplifiers also use audio power amplifiers. In some cases, the power amplifier for an instrument amplifier is integrated into a single amplifier \"head\" which contains a preamplifier, tone controls, and electronic effects. These components may be mounted in a wooden speaker cabinet to create a \"combo amplifier\". Musicians with unique performance needs and/or a need for very powerful amplification may create a custom setup with separate rackmount preamplifiers, equalizers, and a power amplifier mounted in a 19\" road case.\n\nPower amplifiers are available in standalone units, which are used by hi-fi audio enthusiasts and designers of public address systems (PA systems) and sound reinforcement systems. A hi-fi user of power amplifiers may have a stereo power amplifier to drive left and right speakers and a \"monoblock\" single channel power amplifier to drive a subwoofer. The number of power amplifiers used in a sound reinforcement setting depends on the size of the venue. A small coffeehouse may have a single power amp driving two PA speakers. A nightclub may have several power amps for the main speakers, one or more power amps for the monitor speakers (pointing towards the band) and an additional power amp for the subwoofer. A stadium concert may have a large number of power amps mounted in racks. Most consumer electronics sound products, such as TVs, boom boxes, home cinema sound systems, Casio and Yamaha electronic keyboards, \"combo\" guitar amps and car stereos have power amplifiers integrated inside the chassis of the main product.\n\n"}
{"id": "30107409", "url": "https://en.wikipedia.org/wiki?curid=30107409", "title": "Barium azide", "text": "Barium azide\n\nBarium azide is an inorganic azide with the formula Ba(N). Like most azides, it is explosive. It is less sensitive to mechanical shock than lead azide.\n\nBarium azide can be used to make azides of magnesium, sodium, potassium, lithium, rubidium and zinc with their respective sulfates.\n\nIt can also be used as a source for high pure nitrogen by heating:\n\nThis reaction liberates metallic barium, which used as a getter in vacuum applications.\n\n"}
{"id": "1935701", "url": "https://en.wikipedia.org/wiki?curid=1935701", "title": "Beauty mark", "text": "Beauty mark\n\nA beauty mark or beauty spot is a euphemism for a type of dark facial mole, so named because such birthmarks are sometimes considered an attractive feature. Medically, such \"beauty marks\" are generally melanocytic nevus, more specifically the compound variant. Moles of this type may also be located elsewhere on the body, and may also be considered beauty marks if located on the face, shoulder, neck or breast. Artificial beauty marks have been fashionable in some periods.\n\nArtificial beauty marks became fashionable in sixteenth-century France, and the fashion persisted into the eighteenth century, applied to the face as a form of make-up. They were often in fanciful shapes such as hearts or stars. Besides their decorative value, they could hide smallpox scars or syphilis sores. They could be purchased as silk or velvet patches known as \"mouches\" (flies). Alexander Pope's 1712 poem \"The Rape of the Lock\" mentions such patches as indicators of \"secular love\":\n\n<poem>Here Files of Pins extend their shining Rows,\nPuffs, Powders, Patches, Bibles, Billet-doux.\nNow awful Beauty puts on all its Arms;\nThe Fair each moment rises in her Charms,\nRepairs her Smiles, awakens ev'ry Grace,\nAnd calls forth all the Wonders of her Face;</poem>\n\nThe fashion spread to Spain and the Spanish Empire in the eighteenth century, under the name \"chiqueador\".\n\nNatural beauty marks are also often enhanced with color from an eyebrow pencil or pen.\n\nThe Monroe piercing has gained popularity in recent years as a flexible way of approximating a beauty mark.\n\nMany women sex symbols, actresses, and other celebrities are known for their beauty marks:\n\nA few male actors are also known for their beauty marks:\n\nIn the conclusion of the book \"The Silence of the Lambs\", the heroine Clarice Starling gains an artificial beauty mark when burnt gunpowder gets lodged in the flesh of her cheek. She retains this mark in the sequel novel \"Hannibal\". This symbolism (along with Dr. Lecter's polydactylism) did not get carried over into the film.\n\nJoan Crawford had a prominent beauty mark in her role as Sadie Thompson in \"Rain\".\n\n"}
{"id": "168387", "url": "https://en.wikipedia.org/wiki?curid=168387", "title": "Business intelligence", "text": "Business intelligence\n\nBusiness intelligence (BI) comprises the strategies and technologies used by enterprises for the data analysis of business information. BI technologies provide historical, current and predictive views of business operations. Common functions of business intelligence technologies include reporting, online analytical processing, analytics, data mining, process mining, complex event processing, business performance management, benchmarking, text mining, predictive analytics and prescriptive analytics. BI technologies can handle large amounts of structured and sometimes unstructured data to help identify, develop and otherwise create new strategic business opportunities. They aim to allow for the easy interpretation of these big data. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.\n\nBusiness intelligence can be used by enterprises to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions involve priorities, goals and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a complete picture which, in effect, creates an \"intelligence\" that cannot be derived from any singular set of data. Amongst myriad uses, business intelligence tools empower organizations to gain insight into new markets, to assess demand and suitability of products and services for different market segments and to gauge the impact of marketing efforts.\n\nOften BI applications use data gathered from a data warehouse (DW) or from a data mart, and the concepts of BI and DW combine as \"BI/DW\"\nor as \"BIDW\". A data warehouse contains a copy of analytical data that facilitate decision support.\n\nThe earliest known use of the term \"business intelligence\" is in Richard Millar Devens' \"Cyclopædia of Commercial and Business Anecdotes\" (1865). Devens used the term to describe how the banker Sir Henry Furnese gained profit by receiving and acting upon information about his environment, prior to his competitors:\nThe ability to collect and react accordingly based on the information retrieved, Devens says, is central to business intelligence.\n\nWhen Hans Peter Luhn, a researcher at IBM, used the term \"business intelligence\" in an article published in 1958, he employed the \"Webster's Dictionary\" definition of intelligence: \"the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal.\" Business intelligence as it is understood today is said to have evolved from the decision support systems (DSS) that began in the 1960s and developed throughout the mid-1980s. DSS originated in the computer-aided models created to assist with decision making and planning.\n\nIn 1989, Howard Dresner (later a Gartner analyst) proposed \"business intelligence\" as an umbrella term to describe \"concepts and methods to improve business decision making by using fact-based support systems.\" It was not until the late 1990s that this usage was widespread.\n\nCritics see BI merely as an evolution of business reporting together with the advent of increasingly powerful and easy-to-use data analysis tools. In this respect it has also been criticized as a marketing buzzword in the context of the \"big data\" surge.\n\nAccording to Forrester Research, business intelligence is \"a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making.\" Under this definition, business intelligence encompasses information management (data integration, data quality, data warehousing, master-data management, text- and content-analytics, et al.). Therefore, Forrester refers to \"data preparation\" and \"data usage\" as two separate but closely linked segments of the business-intelligence architectural stack.\n\nSome elements of business intelligence are:\n\n\nForrester distinguishes this from the \"business-intelligence market\", which is \"just the top layers of the BI architectural stack, such as reporting, analytics, and dashboards.\"\n\nThough the term business intelligence is sometimes a synonym for competitive intelligence (because they both support decision making), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can include the subset of competitive intelligence.\n\nBusiness intelligence and business analytics are sometimes used interchangeably, but there are alternate definitions. Thomas Davenport, professor of information technology and management at Babson College argues that business intelligence should be divided into querying, reporting, Online analytical processing (OLAP), an \"alerts\" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.\n\nThe Business Intelligence landscape reflects the complex system which data goes through in order to get processed into information. One of the first steps of starting a BI program, is to understand all components of this landscape. The particularities of this system tend to differ based on the industry and organization, but at a macro level, all BI landscapes have the same format. It’s usually composed of five pillars and five foundation blocks:\n\nThe five pillars:\n\n\nThe five foundation blocks:\n\n\nBusiness operations can generate a very large amount of information in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material. According to Merrill Lynch, more than 85% of all business information exists in these forms; a company might only use such a document a single time. Because of the way it is produced and stored, this information is either unstructured or semi-structured.\n\nThe management of semi-structured data is an unsolved problem in the information technology industry. According to projections from Gartner (2003), white collar workers spend 30–40% of their time searching, finding, and assessing unstructured data. BI uses both structured and unstructured data. The former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision making. Because of the difficulty of properly searching, finding and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task or project. This can ultimately lead to poorly informed decision making.\n\nTherefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data.\n\nUnstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered columns and rows. One type of unstructured data is typically stored in a BLOB (binary large object), a catch-all data type available in most relational database management systems. Unstructured data may also refer to irregularly or randomly repeated (nonrepetitive) column patterns that vary from row to row within each file or document.\n\nMany of these data types, however, like e-mails, word processing text files, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database.\nTherefore, it may be more accurate to talk about this as semi-structured documents or data, but no specific consensus seems to have been reached.\n\nUnstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.\n\nThere are several challenges to developing BI with semi-structured data. According to Inmon & Nesavich, some of those are:\n\n\nTo solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of metadata. Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content – e.g. summaries, topics, people or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and information extraction.\n\nBusiness intelligence can be applied to the following business purposes:\n\n\nIn a 2013 report, Gartner categorized business intelligence vendors as either an independent \"pure-play\" vendor or a consolidated \"megavendor\". In 2012 business intelligence services received $13.1 billion in revenue.\n\nA 2009 paper predicted these developments in the business intelligence market:\n\n\nA 2009 \"Information Management\" special report predicted the top BI trends: \"green computing, social networking services, data visualization, mobile BI, predictive analytics, composite applications, cloud computing and multitouch\". Research undertaken in 2014 indicated that employees are more likely to have access to, and more likely to engage with, cloud-based BI tools than traditional tools.\n\nOther business intelligence trends include the following:\n\nOther lines of research include the combined study of business intelligence and uncertain data. In this context, the data used is not assumed to be precise, accurate and complete. Instead, data is considered uncertain and therefore this uncertainty is propagated to the results produced by BI.\n\nAccording to a study by the Aberdeen Group, there has been increasing interest in Software-as-a-Service (SaaS) business intelligence over the past years, with twice as many organizations using this deployment approach as one year ago – 15% in 2009 compared to 7% in 2008.\n\nAn article by InfoWorld's Chris Kanaracus points out similar growth data from research firm IDC, which predicts the SaaS BI market will grow 22 percent each year through 2013 thanks to increased product sophistication, strained IT budgets, and other factors.\n\nAn analysis of top 100 Business Intelligence and Analytics scores and ranks the firms based on several open variables\n\n"}
{"id": "52777663", "url": "https://en.wikipedia.org/wiki?curid=52777663", "title": "C-STEM Studio", "text": "C-STEM Studio\n\nC-STEM Studio is a platform for hands-on integrated learning of computing, science, technology, engineering, and mathematics (C-STEM) with robotics. It can be used to control multiple Linkbot, Lego Mindstorms NXT and EV3, Arduino boards.\n\nC-STEM Studio is developed by the UC Davis C-STEM Center’s. C-STEM Studio includes the software modules, programs, comprehensive documentations, teacher’s guides, and textbooks used in the C-STEM curriculum.\n\nC-STEM Studio is specially designed for instructors to organize diverse teaching resources and for students to conduct computer homework assignments conveniently in formal computer teaching labs. C-STEM Studio is provided free of charge.\n\n"}
{"id": "56699033", "url": "https://en.wikipedia.org/wiki?curid=56699033", "title": "CISBOT", "text": "CISBOT\n\nCISBOT (cast-iron sealing robot) is a cast iron pipe-repair robot that seals the joints in natural gas pipelines from the inside, thus extending their use for up to fifty years without unearthing the joints. \n\nMany cast-iron pipes installed over a century ago have joints of jute sealed with lead that deteriorate over time and are often the causes of cast-iron pipe failure. Jute was an effective joint sealant when the pipes carried coal-based town gas, but natural gas, used since the 1950s in New York and the 1970s in the UK, dries out the jute. A fifty-year life expectancy of the new seal has been proven in studies by Cornell University.\n\nCISBOT is in diameter, is long, weighs , and is deployed with a truck containing the control unit, which is attached to the robot with a tether for the power, communication, and control cables, as well as sealant tubing. It is capable of repairing pipe.\n\nCISBOT is introduced into a pipeline through a hole and can repair the joints with an anaerobic sealant for a distance of while the pipe is still in operation. External repair without the robot would require the arranging of temporary gas supply to customers through tanks, purging the pipeline of gas, then digging up each joint, usually every , repairing the joint, and re-covering the pipeline. The cost of internal robotic repair is approximately 25% of manual resealing and 10% of total pipeline replacement.\n\nThe robot was developed by ULC Robotics of Hauppauge, New York with cooperation from Con Edison and National Grid and costs approximately $1 million. ULC announced the robot in 2015; however it has been in use since at least 2010 and by mid-2012 had sealed over 5000 joints.\n\nCISBOT has been used in the United States in Manhattan, Brooklyn, Staten Island, Rhode Island, and Boston. In Europe, it has been used in London and Glasgow by SGN.\n\nA flange is first attached to the pipe, through which CISBOT drills an access hole. It then rolls into the pipe and propels itself on two wheels down the pipe. It can travel up to per minute. Upon reaching a joint, it rotates 360° within the pipe to apply the sealant, using its third perpendicular wheel. It drills holes into the joints through which it injects the sealant. It is controlled by two operators monitoring through six cameras. It is equipped with lights and other sensors.\n\nIn New York City, CISBOT has discovered foreign objects within gas pipelines, including a baseball and stiletto shoes. It found a 1939 newspaper under Boston's Berkeley Street.\n\nIn 2015 CISBOT received a \"game changer\" award from Robotics Business Review because of its significant improvement in the process used to fix pipe joints.\n\n__notoc__\n"}
{"id": "1804284", "url": "https://en.wikipedia.org/wiki?curid=1804284", "title": "Campden tablet", "text": "Campden tablet\n\nCampden tablets (potassium or sodium metabisulfite) are a sulfur-based product that is used primarily to sterilize wine, cider and in beer making to kill bacteria and to inhibit the growth of most wild yeast: this product is also used to eliminate both free chlorine and the more stable form, chloramine, from water solutions (e.g., drinking water from municipal sources). Campden tablets allow the amateur brewer to easily measure small quantities of sodium metabisulfite, so it can be used to protect against wild yeast and bacteria without affecting flavour. Untreated cider must frequently suffers from acetobacter contamination causing vinegar spoilage. Yeast are resistant to the tablets but the acetobacter are easily killed off, hence treatment is important in cider production.\n\nTypical use is one crushed Campden tablet per US gallon (3.8 L) of must or wort. This dosage contributes 67 ppm sulfur dioxide to the wort but the level of active sulfur dioxide diminishes rapidly as it reacts with chlorine and chloramine, and with aldehydes (particularly in wine). Therefore, the concentration of free sulfur dioxide is greatly diminished by the time the beer or wine is consumed. However, when used only for the purpose of dechlorinating tap water before brewing, one tablet will effectively treat 20 US gallons (75 L) of water.\n\nCampden tablets are also used as an anti-oxidizing agent when transferring wine between containers. The sodium metabisulfite in the Campden tablets will trap oxygen that enters the wine, preventing it from doing any harm.\n\nIt is a common misconception that Campden tablets can be used to halt the ferment process in wine before all the available sugars are converted by the yeast, hence controlling the amount of residual sweetness in the final product. This however is not true. In order to halt fermentation, enough Campden tablets would have to be added to render the wine undrinkable. Alternatively, when used in conjunction with potassium sorbate, the yeast population will be greatly reduced and prevented from reproducing. Without the addition of potassium sorbate the yeast population will only be stunned and eventually repopulate if provided with enough fermentable sugars.\n\nCampden tablets typically contain 0.44 g each of sodium metabisulfite (plus filler) and 10 of these are equivalent to one level teaspoon (5 mL) of sodium metabisulfite. Other related substances are sodium/potassium sulfite/bisulfite. Further complicating the subject, each is also referred to interchangeably as—sulfites, and the 'bi' can be found as 'di'. In terms of usage, sodium thiosulfate is a closely related compound.\n\nThe name Campden tablet comes from the town of Chipping Campden in Gloucestershire, England, where the original solution was developed in the 1920s by the Fruit and Vegetable Preserving Research Station - now Campden BRI. The idea was then taken up by the Boots Co., who developed the tablet.\n\nCampden tablets are also useful in decontamination and neutralization after exposure to tear gas.\n\nThe molar mass (commonly called molecular weight or MW) of potassium metabisulfite is 222 g/mol, while the molecular weight of sodium metabisulfite is 190 g/mol.\n\nSome people have allergies or intolerance to sulfites. Allergies are less common than intolerance, but in rare cases may cause life-threatening anaphylactic reaction. Products containing residual amounts of sulfites, including potassium metabisulfite and sodium metabisulfite, should be labelled.\n\n"}
{"id": "4243241", "url": "https://en.wikipedia.org/wiki?curid=4243241", "title": "Cellular architecture", "text": "Cellular architecture\n\nA cellular architecture is a type of computer architecture prominent in parallel computing. Cellular architectures are relatively new, with IBM's Cell microprocessor being the first one to reach the market. Cellular architecture takes multi-core architecture design to its logical conclusion, by giving the programmer the ability to run large numbers of concurrent threads within a single processor. Each 'cell' is a compute node containing thread units, memory, and communication. Speed-up is achieved by exploiting thread-level parallelism inherent in many applications.\n\nCell, a cellular architecture containing 9 cores, is the processor used in the PlayStation 3. Another prominent cellular architecture is Cyclops64, a massively parallel architecture currently under development by IBM. \n\nCellular architectures follow the low-level programming paradigm, which exposes the programmer to much of the underlying hardware. This allows the programmer to greatly optimize their code for the platform, but at the same time makes it more difficult to develop software. \n"}
{"id": "552609", "url": "https://en.wikipedia.org/wiki?curid=552609", "title": "Code of the Lifemaker", "text": "Code of the Lifemaker\n\nCode of the Lifemaker is a 1983 novel by British science fiction author James P. Hogan. NASA's Advanced Automation for Space Missions was the direct inspiration for this novel detailing first contact between Earth explorers and the Taloids, clanking replicators who have colonized Saturn's moon Titan.\n\nA sequel titled \"The Immortality Option\" was published in 1995.\n\nAbout 1,000,000 B.C., an unidentified alien race sent out robotic factories to many worlds in their part of the galaxy to prepare for future settlement. One of those factory ships suffers severe radiation damage from a near-miss by a supernova and goes off course, drifting in space for a hundred thousand years before landing on the Saturnian moon Titan. Due to a malfunction in its database, it begins producing imperfect copies that begin to evolve on their own. (The description of this background is presented in a prologue that proved sufficiently popular among readers that it was later anthologized on its own in a collection of Hogan's short fiction.) The resulting machine ecosystem eventually gives rise to humanoid robots with human-like intellects, who develop a civilization similar to early civilization of Earth. Almost all of them have reverence for their mythical creator, a being they call the \"Lifemaker\".\n\nEarly in the 21st century, the North Atlantic Space Organization (combining NASA and NATO) dispatched the \"Orion\" with a cover story of terraforming Mars for human habitation. Karl Zambendorf, a con artist who is present on this expedition to verify ESP over interplanetary distances, prematurely learns that the \"Orion\" and its crew of researchers is headed for Titan, where the discovery of the Taloids has been kept need-to-know on Earth.\n\nWhen the \"Orion\" arrives, the first landing party sets down in a freethinking state where Thirg, a Taloid who was cast out of his home state Kroaxia, has fled. They are mistaken for the Lifemaker because they have come from the sky, which the Taloids cannot see out of due to Titan's atmosphere. But Thirg becomes more discerning as he and the humans begin to understand more of each other's speech. Thirg's brother Groork has come from Kroaxia to apprehend him, but Zambendorf intercepts him and sends him back to Kroaxia as a prophet.\n\nZambendorf learned that NASO plans to exploit Titan's natural resources and use the Taloids to build the factories they need, reducing them to slaves. The NASO business administrators on the \"Orion\" are already in agreement with the Kroaxian government to use human (the Taloids call humans \"Lumians\" because they glow brightly in their infrared vision) weapons to conquer Titan, believing the Kroaxian leadership buttressed by priests will be the easiest to control. Zambendorf, in his unanticipated role as Messenger for the Lifemaker, has given Groork guidelines akin to the Ten Commandments for his people to prevent a war from starting. \"All Taloids are brothers\" and \"No Taloid is to enslave or be a slave\" does not sit well with the ruling establishment of Kroaxia, and Groork is saved by the \"Orion\" crew not working for NASO. There will be use of Titan's resources, but the partnership between humans and Taloids will be one of equals.\n\n"}
{"id": "22846593", "url": "https://en.wikipedia.org/wiki?curid=22846593", "title": "Colour recovery", "text": "Colour recovery\n\nColour recovery (or colour restoration) is a process which can restore lost colour, specifically to television programmes which were originally transmitted from colour video tape, but for which only black & white copies remain archived. Not to be confused with colourisation, colour recovery is a newer process and is fundamentally different from colourisation for several reasons. Firstly, colour recovery can only be performed if the originally transmitted colour signal can be reconstructed or recovered from some source, whereas this is not usually the case for traditional colourisation. Secondly, colourisation can be used to colourise films and programmes that were made in black and white, using still colour photos and/or some educated guesswork to manually choose a colour palette. Conversely, the goal of colour recovery is to reinstate (as closely as possible) the colour signals of programmes originally made in colour as they were first seen. Colour recovery reconstructs the colour information from actual recovered signals and theoretically without depending on guesswork. As of 2018, colour recovery has successfully been applied to episodes of the BBC TV programmes \"Doctor Who\", \"Dad's Army\", and \"Are You Being Served?\".\n\nDue to the well-documented practice of wiping, many original videotape recordings of colour programmes were lost. However, in the case of the BBC, many telerecorded black & white film copies of affected programmes survived. These black & white copies were made for overseas commercial exploitation of BBC programmes. For a variety of technical and practical reasons (for example various incompatible international TV standards, and the then-high cost of videotape over that of film), black & white film copies were the preferred medium for selling programmes overseas. This practice ultimately led to many programmes which were originally made and transmitted in colour only existing in black and white form after the practice of wiping finally ceased.\n\nDuring the 1970s, various off-air NTSC video-recordings were made by American and Canadian \"Doctor Who\" fans, which were later returned to the BBC. Whilst the quality of these early domestic video recordings was not suitable for broadcast, the lower-definition chrominance signal could be retrieved from them. This signal could be successfully combined with the luminance signal from digitally-scanned existing broadcast-quality monochrome telerecordings to make new colour master copies, suitable for broadcast and sales. In the 1990s this method was carried out by the Doctor Who Restoration Team. Several colour \"Doctor Who\" serials were subsequently released on VHS. Combining the video-recorded colour signals with the monochrome telerecordings is a non-trivial task, requiring digital processing (for example matching up the different screen sizes of the two recordings). Thus, it wasn't until the early 1990s that cheaply available, sufficiently powerful computer hardware and software made this task particularly practical at that time.\n\nBlack & white TV systems predate colour, and so subsequent analogue colour broadcast systems have been designed with backwards-compatibility in mind (known as a \"compatible colour\" system). Thus, the chrominance (colour) signal is typically 'shoe-horned' into the same channel as the luminance (brightness) signal, modulated on a fixed frequency, known as the \"colour subcarrier\". Black and white televisions do not decode this extra colour information in the subcarrier, using only the luminance to provide a monochrome picture. However, due to limited bandwidth in the video channel, the chrominance and luminance signals bleed into each other considerably, resulting in the colour information showing up visibly as Chroma Crawl, or Chroma dots on black & white TV sets. This is normally considered a nuisance in analogue broadcasting. However, since telerecordings were made from black & white TV screens and technicians at the time often decided not to apply a filter to remove this interference, these patterns are retained even in the existing monochrome film prints and theoretically contain the original colour information. (Occasionally the colour information was filtered out using a notch filter and is lost.) The idea to recover this information was originally suggested by BBC researcher James Insell.\n\nIn practice however, the recovery of this colour information from telerecordings is highly complex for several reasons. Firstly, the colour reference timing signal, known as the \"colour burst,\" is absent from telerecordings, as it is nominally off the edge of the visible screen area being recorded. This timing has to effectively be recovered since the \"phase\" of the chroma dots, which is represented by their horizontal position on the screen, determines the hue of the reconstructed colours. Distortions in the geometry of the telerecordings due to the nature of physically recording from a non-flat CRT screen onto film means that a transformation has to be applied in order to infer the original positions of the chroma dots within the broadcast.\n\nHowever, these technical obstacles were finally overcome in 2008, and software written by developer Richard Russell at the informal \"Colour Recovery Working Group\" was put to use, finally resulting in the broadcast and release of colour-recovered episodes of \"Dad's Army\" and \"Doctor Who\".\n\n\"Example of the chroma dot reconstruction:\"\n\n"}
{"id": "19377388", "url": "https://en.wikipedia.org/wiki?curid=19377388", "title": "Design controls", "text": "Design controls\n\nDesign controls designates the application of a formal methodology to the conduct of product development activities.\nIt is often mandatory (by regulation) to implement such practice when designing and developing products within regulated industries (e.g. medical devices).\n\nSince 1990, the Food and Drug Administration (FDA) has required that medical device manufacturers that want to market certain categories of medical devices in the USA follow Design Control requirements (21 CFR 820.30). At a high level, this regulation requires:\n\nThe Medical Devices Directive (MDD 93/42/EEC) similarly lists several requirements regarding the design of a medical device.\nISO 13485 is a voluntary standard that contains section 7.3 Design and Development recommending which procedures should be put in place by manufacturers in order to have a quality system that will comply with MDD 93/42/EEC.\n\nThe objective of Design Controls, in this context, is to require that manufacturers follow a methodologically-sound process to develop a medical device, with the intent of improving the probability that the device will reach an acceptable level of efficacy and safety.\n\nExamples of design input:\n"}
{"id": "199264", "url": "https://en.wikipedia.org/wiki?curid=199264", "title": "Designer", "text": "Designer\n\nA designer is a person who makes designs for objects. A fashion designer designs clothing, a web designer designs web pages, and an automobile designer designs automobiles. In each case, the designer works with the help of a technician or engineer who understands deeper level concepts of manufacturing and engineering, and the designer themself is largely confined to work at a surface level. \n\nMore formally, a designer is an agent that \"specifies the structural properties of a design object\". In practice, anyone who creates tangible or intangible objects, products, processes, laws, games, graphics, services, and experiences is referred to as a designer.\n\nClassically, the main area of design was only architecture, which was understood as the major arts. The design of clothing, furniture, and other common artifacts were left mostly to tradition or artisans specializing in hand making them.\n\nDesign is a part of the communication arts, design is something you plan to create, it communicates to the viewer or user a visual and emotional message to change or guide through an emotional connection with a product or service enhancing their experience of the product or brand\n\nWith the increasing complexity in industrial design of today’s society, and due to the needs of mass production where more time is usually associated with more cost, the production methods became more complex and with them, the way designs and their production is created. The classical areas are now subdivided into smaller and more specialized domains of design (landscape design, urban design, exterior design, interior design, industrial design, furniture design, cloth design, and much more) according to the product designed or perhaps its means of production. Despite the variety of specializations within the design industry, all of them are very closely connected in terms of the need of professionals having a eye for design and being inspired by each other.\n\nWhen designers speak of graphic design they speak of it in terms of simplifying complexity so users do not feel overwhelmed when faced with complex systems. More information can lead to a better, simpler solution to whatever problem the designer is trying to solve. Using research methodology to solve problems is one of the most important aspects of design, it enables the designer to properly assess the clients and audience needs. Part of a designer's job is to get to know the audience they intend on canvassing and what will be the placement of any produced material created.\n\nThe education, experience, and genetic blocks that form the base of a competent designer is normally similar no matter the area of specialization, only in a later stages of training and work will designer diverge to a specialized field. The methods of teaching or the program and theories followed vary according to schools and field of study.\nToday, a design team, no matter the scale of the equipment, is usually composed by a master designer (the head of the team) that will have the responsibility to take decisions about the way the creative process should evolve, and a number of technical designers (the hands of the team) specialized in diverse areas according to the product proposed, depending on the type of design industrial design vs graphic design for example. For more complex products, the team will also be composed of professionals from other areas like engineers, advertising specialists, and others as required.\nThe relationships established between team members will vary according to proposed product, the processes of production, the equipment available, or the theories followed during the idea development, but normally they are not too restrictive, giving an opportunity to everyone in the team to take a part in the creation process or at least to express an idea.\n\nDifferent types of designers include:\n\n"}
{"id": "14449501", "url": "https://en.wikipedia.org/wiki?curid=14449501", "title": "Distributed GIS", "text": "Distributed GIS\n\nDistributed GIS refers to GI Systems that do not have all of the system components in the same physical location. This could be the processing, the database, the rendering or the user interface. Examples of distributed systems are web-based GIS and Mobile GIS.\n\nThe term Distributed GIS was coined by Bruce Gittings at the University of Edinburgh. He was responsible for one of the first Internet-based distributed GIS. In 1994, he designed and implemented the World Wide Earthquake Locator, which provided maps of recent earthquake occurrences to a location-independent user, which used the Xerox PARC mapping system (based in California, USA), managed by an interface based in Edinburgh (Scotland), which drew data in real-time from the National Earthquake Information Center (USGS) in Colorado, USA. Gittings first taught a course in this subject in 2005 as part of the Masters Programme in GIS at that institution. Since there was no Wikipedia article relating to Distributed GIS at the time, he set his students the task of creating one in 2007 as a class exercise.\n\nEnterprise GIS refers to a geographical information system that integrates geographic data across multiple departments and serves the whole organisation. The basic idea of an enterprise GIS is to deal with departmental needs collectively instead of individually. When organisations started using GIS in the 1960s and 1970s, the focus was on individual projects where individual users created and maintained data sets on their own desktop computers. Due to extensive interaction and work-flow between departments, many organisations have in recent years switched from independent, stand-alone GIS systems to more integrated approaches that share resources and applications.\n\nSome of the potential benefits that an enterprise GIS can provide include significantly reduced redundancy of data across the system, improved accuracy and integrity of geographic information, and more efficient use and sharing of data. Since data is one of the most significant investments in any GIS program, any approach that reduces acquisition costs while maintaining data quality is important. The implementation of an enterprise GIS may also reduce the overall GIS maintenance and support costs providing a more effective use of departmental GIS resources. Data can be integrated and used in decision making processes across the whole organisation.\n\nA corporate Geographical Information System, is similar to Enterprise GIS and satisfies the spatial information needs of an organisation as a whole in an integrated manner. Corporate GIS consists of four technological elements which are data, standards, information technology and personnel with expertise. It is a coordinated approach that moves away from fragmented desktop GIS. The design of a corporate GIS includes the construction of a centralised corporate database that is designed to be the principle resource for an entire organisation. The corporate database is specifically designed to efficiently and effectively suit the requirements of the organisation. Essential to a corporate GIS is the effective management of the corporate database and the establishment of standards such as OGC for mapping and database technologies.\n\nBenefits include that all the users in the organisation have access to shared, complete, accurate, high quality and up-to-date data. All the users in the organisation also have access to shared technology and people with expertise. This improves the efficiency and effectiveness of the organisation as a whole. A successfully managed corporate database reduces redundant collection and storage of information across the organisation. By centralising resources and efforts, it reduces the overall cost.\n\nWith ~80% of all data deemed to have a spatial component, modern Mobile GIS are a powerful geo-centric business process integration platform enabling the Spatial Enterprise.\nThe number of mobile devices in circulation has surpassed the world’s population (2013) with a rapid acceleration in iOS, Android and Windows 8 tablet up-take. \nTablets are fast becoming popular for Utility field use. Low-cost MIL-STD-810 certified cases transform consumer tablets into fully ruggedised, yet lightweight field use units at 10% of legacy ruggedised laptop costs.\n\nAlthough not all applications of mobile GIS are limited by the device, many are. \nThese limitations are more applicable to smaller devices such as cell phones and PDAs. \nSuch devices have: small screens with a poor resolution, limited memory and processing power, a poor (or no) keyboard, and short battery life.\nAdditional limitations can be found in web client based tablet applications: poor web GUI and device integration, on-line reliance, and very limited off-line web client cache.\n\nLocation-based services (LBS) are services that are distributed wirelessly and provide information relevant to the user’s current location. These services include such things as ‘find my nearest …’, directions, and various vehicle monitoring systems, such as the GM OnStar system amongst others. Location-based services are generally run on mobile phones and PDAs, and are intended for use by the general public more than Mobile GIS systems which are geared towards commercial enterprise. Devices can be located by triangulation using the mobile phone network and/or GPS.\n\nA web mapping service is a means of displaying and interacting with maps on the Web. The first web mapping service was the Xerox PARC Map Viewer built in 1993 and decommissioned in 2000.\n\nThere have been 3 generations of web map service. The first generation was from 1993 onwards and consisted of simple image maps which had a single click function. The second generation was from 1996 onwards and still used image maps the one click function. However, they also had zoom and pan capabilities (although slow) and could be customised through the use of the URL API. The third generation was from 1998 onwards and were the first to include slippy maps. They utilise AJAX technology which enables seamless panning and zooming. They are customisable using the URL API and can have extended functionality programmed in using the DOM.\n\nWeb map services are based on the concept of the image map whereby this defines the area overlaying an image (e.g. GIF). An image map can be processed client or server side. As functionality is built into the web server, performance is good. Image maps can be dynamic. When image maps are used for geographic purposes, the co-ordinate system must be transformed to the geographical origin to conform to the geographical standard of having the origin at the bottom left corner.\n\nWeb maps are used for location-based services.\n\nLocal Search is a recent approach to internet searching that incorporates geographical information into search queries so that the links that you return are more relevant to where you are. It developed out of an increasing awareness that many search engine users are using it to look for a business or service in the local area. Local search has stimulated the development of web mapping, which is used either as a tool to use in geographically restricting your search (see Live Search Maps) or as an additional resource to be returned along with search result listings (see Google Maps). It has also led to an increase in the number of small businesses \nadvertising on the web.\n\nIn distributed GIS, the term mashup refers to a generic web service which combines content and functionality from disparate sources; mashups reflect a separation of information and presentation. Mashups are increasingly being used in commercial and government applications as well as in the public domain.\nWhen used in GIS, it reflects the concept of connecting an application with a mapping service. An examples is combining Google maps with Chicago crime statistics to create the Chicago crime statistics map.\nMashups are fast, provide value for money and remove responsibility for the data from the creator.\n\nSecond generation systems provide mashups mainly based on URL parameters, while Third generation systems (e.g. Google Maps) allow customisation via script (e.g. JavaScript).\n\nThe development of the European Union (EU) \"Infrastructure for Spatial Information in the European Community\" (INSPIRE) initiative indicates this is a matter that is gaining more awareness at the national and EU scale. This states that there is a need to create ‘quality geo-referenced information’ that would be useful for a better understanding of human activities on environmental processes. Therefore, it is an ambitious project that aims to develop a European spatial information database.\n\nThe GI strategy for Scotland was introduced in 2005 to provide a sustainable SDI, through the ’’One Scotland – One Geography’’ implementation plan. This documentation notes that it should be able to provide linkages to the ’’Spaces, Faces and Places of Scotland’’.\nAlthough plans for a GI strategy have been in existence for some time, it was revealed at the AGI Scotland 2007 conference that a recent budget review by the Scottish Government indicated there will not be an allocation of resources to fund this initiative within the next term. Therefore, a business plan will need to be presented in order to outline the cost-benefits involved with taking up the strategy.\n\nThe main standards for Distributed GIS are provided by the Open Geospatial Consortium (OGC). OGC is a non-profit international group which seeks to Web-Enable GIS and in turn Geo-Enable the web. One of the major issues concerning distributed GIS is the interoperability of the data since it can come in different formats using different projection systems. OGC standards seek to provide interoperability between data and to integrate existing data.\n\nIn terms of interoperability, the use of communication standards in Distributed GIS is particularly important. General standards for Geospatial Data have been developed by the Open Geospatial Consortium (OGC). For the exchange of Geospatial Data over the web, the most important OGC standards are Web Map Service (WMS) and Web Feature Service (WFS).\n\nUsing OGC compliant gateways allows for building very flexible Distributed GI Systems. Unlike monolithic GI Systems, OGC compliant systems are naturally web-based and do not have strict definitions of servers and clients. For instance, if a user (client) accesses a server, that server itself can act as a client of a number of further servers in order to retrieve data requested by the user. This concept allows for data retrieval from any number of different sources, providing consistent data standards are used.\nThis concept allows data transfer with systems not capable of GIS functionality. A key function of OGC standards is the integration of different systems already existing and thus geo-enabling the web. Web services providing different functionality can be used simultaneously to combine data from different sources (mash-ups). Thus, different services on distributed servers can be combined for ‘service-chaining’ in order to add additional value to existing services. Providing a wide use of OGC standards by different web services, sharing distributed data of multiple organisations becomes possible.\n\nSome important languages used in OGC compliant systems are described in the following. XML stands for eXtensible Markup language and is widely used for displaying and interpreting data from computers. Thus the development of a web-based GI system requires several useful XML encodings that can effectively describe two-dimensional graphics such as maps SVG and at the same time store and transfer simple features GML. Because GML and SVG are both XML encodings, it is very straightforward to convert between the two using an XML Style Language Transformation XSLT. This gives an application a means of rendering GML, and in fact is the primary way that it has been accomplished among existing applications today. XML can introduce innovative web services, in terms of GIS. It allows geographic information to be easily translated in graphic and in these terms scalar vector graphics (SVG) can produce high quality dynamic outputs by using data retrieved from spatial databases. In the same aspect Google, one of the pioneers in web-based GIS, has developed its own language which also uses a XML structure. Keyhole Markup Language (KML) is a file format used to display geographic data in an earth browser, such as Google Earth, Google Maps, and Google Maps for mobile browsers \n\nGlobal System for Mobile Communications (GSM) is a global standard for mobile phones around the world. Networks using the GSM system offer transmission of voice, data and messages in text and multimedia form and provide web, telenet, ftp, email services etc. over the mobile network. Almost two million people are now using GSM. Five main standards of GSM exist: GSM 400, GSM 850, GSM 900, GSM-1800 (DCS) and GSM1900 (PCS). GSM 850 and GSM 1900 is used in North America, parts of Latin America and parts of Africa. In Europe, Asia and Australia GSM 900/1800 standard is used.\n\nGSM consists of two components: the mobile radio telephone and Subscriber Identity Module. GSM is a cellular network, which is a radio network made up of a number of cells. For each cell, the transmitter (known as a base station) is transmitting and receiving signals. The base station is controlled through the Base Station Controller via the Mobile Switching Centre.\n\nFor GSM enhancement General Packet Radio Service (GPRS), a packet-oriented data service for data transmission, and Universal Mobile Telecommunications System (UTMS), the Third Generation (3G) mobile communication system, technology was introduced. Both provide similar services to 2G, but with greater bandwidth and speed.\n\nWireless Application Protocol (WAP) is a standard for the data transmission of internet content and services. It is a secure specification that allows users to access the information instantly via mobile phones, pagers, two-way radios, smartphones and communicators. WAP supports HTML and XML, and WML language, and is specifically designed for small screens and one-hand navigation without a keyboard. WML is scalable from two-line text displays up to the graphical screens found on smart phones. It is much stricter than HTML and is similar to JavaScript.\n\nGeotagging is the process of adding geographical identification metadata to resources such as websites, RSS feed, images or videos. The metadata usually consist of latitude and longitude coordinates but may also include altitude, camera holding direction, place information and so on. Flickr website is one of the famous web services which host photos and provides functionality to add latitude and longitude information to the picture. The main idea is to use metadata related to pictures and photo collection. A geotag is simply a properly-formed XML tag giving the geographic coordinates of a place. The coordinates can be specified in latitude and longitude or in UTM (Universal Transverse Mercator) coordinates.\n\nThe RDFIG Geo vocabulary from the W3C is the common basis for the recommendations. It supplies official global names for the latitude, longitude, and altitude properties. These are given in a system of coordinates known as \"the WGS84 datum\". A geographic datum specifies an ellispoidal approximation to the Earth's surface; WGS84 is the most commonly used such datum.\n\nParallel processing is the use of multiple CPU’s to execute different sections of a program together. Remote sensing and surveying equipment have been providing vast amounts of spatial information, and how to manage, process or dispose of this data have become major issues in the field of Geographic Information Science (GIS). To solve these problems there has been much research into the area of parallel processing of GIS information. This involves the utilization of a single computer with multiple processors or multiple computers that are connected over a network working on the same task. There are many different types of distributed computing, two of the most common are clustering and grid processing.\n\nSome consider grid computing to be “the third information technology wave” after the Internet and Web, and will be the backbone of the next generation of services and applications that are going to further the research and development of GIS and related areas. Grid computing allows for the sharing of processing power, enabling the attainment of high performances in computing, management and services. Grid computing, (unlike the conventional supercomputer that does parallel computing by linking multiple processors over a system bus) uses a network of computers to execute a program.\n\nThe problem of using multiple computers lies in the difficulty of dividing up the tasks among the computers, without having to reference portions of the code being executed on other CPUs.\nAmdahl's law expresses the speedup of a program as a result of parallelization.\nIt states that potential program speedup is defined by the fraction of code (P) that can be parallelized: 1/(1-P).\nIf the code cannot be broken up to run over multiple processors, P = 0 and the speedup = 1 (no speedup). If it is possible to break up the code to be perfectly parallel then P = 1 and the speedup is infinite, in theory though practical limits occur. Thus, there is an upper bound on the usefulness of adding more parallel execution units.\nGustafson's law is a law closely related to Amdahl's law but doesn’t make as many assumptions and tries to model these factors in the representation of performance. The equation can be modelled by S(P) = P − α * (P − 1) where P is the number of processors, S is the speedup, and α the non-parallelizable part of the process.\n\n\n"}
{"id": "14536825", "url": "https://en.wikipedia.org/wiki?curid=14536825", "title": "Douglas Complex", "text": "Douglas Complex\n\nThe Douglas Complex is a high system of three linked platforms in the Irish Sea, off the North Wales coast. The Douglas oil field was discovered in 1990, and production commenced in 1996. Now operated by Eni, the complex consists of the wellhead platform, which drills into the seabed, a processing platform, which separates oil, gas and water, and thirdly an accommodation platform, which is composed of living quarters for the crew. This accommodation module was formerly the Morecambe Flame jack-up drilling rig.\n\nThe Douglas Complex is also the control hub for other platforms in the area, and provides power for all platforms. It also offers recreational, catering and medical facilities for up to 80 personnel. Oil from the Lennox, Hamilton, and Hamilton North unmanned satellite platforms is received and blended at the complex.\n\nFluids from the Lennox installation via the gas pipeline are treated on the Douglas installation in the 3-phase (oil, gas and produced water) Lennox Production Separator. Following separation, gas flows to the Offgas Compressor suction manifold. Oil is directed to the Oil Stripper where the liquid is stripped of sour gas using a counter-current flow of stripping gas. Produced water from the separator is directed to the Produced Water Hydrocyclones where hydrocarbon liquids are removed prior to overboard disposal. Well fluids from the Douglas Wellhead tower are treated in the 3-phase Douglas Production Separator. Gas flows to the Offgas Compressor suction manifold and hydrocarbon liquids are directed to the Oil Stripper, and water to hydrocyclones as described above. Oil from the Oil Stripper is pumped by the Oil Transfer Pumps via Fiscal metering to the Main Oil Transfer Pumps to tanker loading. Gas from the Oil Stripper is compressed and sent to the Offgas Compressor.\n\nGas is sent through a pipeline long to a processing plant at Point of Ayr, in Flintshire, North Wales. After processing, almost the entire output is sold to E.ON to fire the combined cycle gas turbine power station at Connah's Quay, on Deeside, in Flintshire. Oil produced in Liverpool Bay is sent through another pipeline, 17 km long, to the Offshore Storage Installation, a permanently anchored barge which acts as a floating oil terminal, capable of holding of oil. From the floating terminal oil is transferred to tankers approximately once every month.\n\n\n"}
{"id": "891537", "url": "https://en.wikipedia.org/wiki?curid=891537", "title": "Electric switchboard", "text": "Electric switchboard\n\nAn electric switchboard is a device that directs electricity from one or more sources of supply to several smaller regions of usage. It is an assembly of one or more panels, each of which contains switches that allow electricity to be redirected. \n\nThe U.S. National Electrical Code (NEC) defines a switchboard as \"a large single panel, frame, or assembly of panels on which are mounted, on the face, back, or both, switches, over-current and other protective devices, buses, and usually instruments\". The role of a switchboard is to allow the division of the current supplied to the switchboard into smaller currents for further distribution and to provide switching, current protection and (possibly) metering for those various currents. In general, switchboards may distribute power to transformers, panelboards, control equipment, and, ultimately, to individual system loads.\n\nInside a switchboard there will be one or more busbars. These are flat strips of copper or aluminum, to which the switchgear is connected. Busbars carry large currents through the switchboard, and are supported by insulators. Bare busbars are common, but many types are now manufactured with an insulating cover on the bars, leaving only connection points exposed.\n\nThe operator is protected from electrocution by safety switches and fuses. There may also be controls for the supply of electricity to the switchboard, coming from a generator or bank of electrical generators, especially frequency control of AC power and load sharing controls, plus gauges showing frequency and perhaps a synchroscope. The amount of power going into a switchboard must always equal to the power going out to the loads.\n\nModern industrial switchboards are metal enclosed and of \"dead front\" construction; no energized parts are accessible when the covers and panels are closed. Previously, open switchboards were made with switches and other devices were mounted on panels made of slate, granite, or ebony asbestos board. The metal enclosure of the switchboard is bonded to earth ground for protection of personnel. Large switchboards may be free-standing floor-mounted enclosures with provision for incoming connections at either the top or bottom of the enclosure. A switchboard may have incoming bus bars or bus duct for the source connection, and also for large circuits fed from the board. A switchboard may include a metering or control compartment separated from the power distribution conductors.\n"}
{"id": "7005062", "url": "https://en.wikipedia.org/wiki?curid=7005062", "title": "Energy conversion efficiency", "text": "Energy conversion efficiency\n\nEnergy conversion efficiency (η) is the ratio between the useful output of an energy conversion machine and the input, in energy terms. The input, as well as the useful output may be chemical, electric power, mechanical work, light (radiation), or heat.\n\nEnergy conversion efficiency depends on the usefulness of the output. All or part of the heat produced from burning a fuel may become rejected waste heat if, for example, work is the desired output from a thermodynamic cycle. Energy converter is an example of an energy transformation. For example a light bulb falls into the categories energy converter. \nformula_1\nEven though the definition includes the notion of usefulness, efficiency is considered a technical or physical term. Goal or mission oriented terms include effectiveness and efficacy.\n\nGenerally, energy conversion efficiency is a dimensionless number between 0 and 1.0, or 0% to 100%. Efficiencies may not exceed 100%, e.g., for a perpetual motion machine. However, other effectiveness measures that can exceed 1.0 are used for heat pumps and other devices that move heat rather than convert it.\n\nWhen talking about the efficiency of heat engines and power stations the convention should be stated, i.e., HHV ( Gross Heating Value, etc.) or LCV (a.k.a. Net Heating value), and whether gross output (at the generator terminals) or net output (at the power station fence) are being considered. The two are separate but both must be stated. Failure to do so causes endless confusion.\n\nRelated, more specific terms include\n\nIn Europe the usable energy content of fuel is typically calculated using the lower heating value (LHV) of that fuel, the definition of which assumes that the water vapor produced during fuel combustion (oxidation), remains gaseous, and is not condensed to liquid water so the latent heat of vaporization of that water is not usable. Using the LHV, a condensing boiler can achieve a \"heating efficiency\" in excess of 100% (this does not violate the first law of thermodynamics as long as the LHV convention is understood, but does cause confusion). This is because the apparatus recovers part of the heat of vaporization, which is not included in the definition of the lower heating value of fuel. In the U.S. and elsewhere, the higher heating value (HHV) is used, which includes the latent heat for condensing the water vapor, and thus the thermodynamic maximum of 100% efficiency cannot be exceeded with HHV's use.\n\nIn optical systems such as lighting and lasers, the energy conversion efficiency is often referred to as wall-plug efficiency. The wall-plug efficiency is the measure of output radiative-energy, in watts (joules per second), per the total of the input electrical-energy in watts. The output-energy is usually measured in terms of absolute irradiance and the wall-plug efficiency is given as a percentage of the total input-energy, with the inverse percentage representing the losses.\n\nThe wall-plug efficiency differs from the \"luminous efficiency\" in that wall-plug efficiency describes the direct output/input conversion of energy (the amount of work that can be performed) whereas luminous efficiency takes into account the human eye's varying sensitivity to different wavelengths (how well it can illuminate a space). Instead of using watts, the power of a light source to produce wavelengths proportional to human perception is measured in lumens. The human eye is most sensitive to wavelengths of 555 nanometers (greenish-yellow) but the sensitivity decreases dramatically to either side of this wavelength, following a Gaussian power-curve and dropping to zero sensitivity at the red and violet ends of the spectrum. Due to this the eye does not usually see all of the wavelengths emitted by a particular light-source, nor does it see all of the wavelengths within the visual spectrum equally. Yellow and green, for example, make up more than 50% of what the eye perceives as being white, even though in terms of radiant energy white-light is made from equal portions of all colors (i.e.: a 5 mw green laser appears brighter than a 5 mw red laser, yet the red laser stands-out better against a white background). Therefore, the radiant intensity of a light source may be much greater than its luminous intensity, meaning that the source emits more energy than the eye can use. Likewise, the lamp's wall-plug efficiency is usually greater than its luminous efficiency. The effectiveness of a light source to convert electrical energy into wavelengths of visible light, in proportion to the sensitivity of the human eye, is referred to as luminous efficacy, which is measured in units of lumens per watt (lm/w) of electrical input-energy. \n\nUnlike efficacy (effectiveness), which is a unit of measurement, efficiency is unitless number expressed as a percentage, requiring only that the input and output units be of the same type. Therefore, the luminous efficiency of a light source is the percentage of luminous efficacy per the theoretical-maximum efficacy at a specific wavelength. The amount of energy carried by a photon of light is determined by its wavelength. In lumens, this energy is offset by the eye's sensitivity to the selected wavelengths. For example, a green laser pointer can have greater than 30 times the apparent brightness of a red pointer of the same power output. At 555 nm in wavelength, 1 watt of radiant energy is equivalent to 685 lumens, thus a monochromatic light source at this wavelength, with a luminous efficacy of 685 lm/w, has a luminous efficiency of 100%. The theoretical-maximum efficacy lowers for wavelengths at either side of 555 nm. For example, low-pressure sodium lamps produce monochromatic light at 589 nm with a luminous efficacy of 200 lm/w, which is the highest of any lamp. The theoretical-maximum efficacy at that wavelength is 525 lm/w, so the lamp has a luminous efficiency of 38.1%. Because the lamp is monochromatic, the luminous efficiency nearly matches the wall-plug efficiency of < 40%. \n\nCalculations for luminous efficiency become more complex for lamps that produce white light or a mixture of spectral lines. Fluorescent lamps have higher wall-plug efficiencies than low-pressure sodium lamps, but only have half the luminous efficacy of ~ 100 lm/w, thus the luminous efficiency of fluorescents is lower than sodium lamps. A xenon flashtube has a typical wall-plug efficiency of 50--70%, exceeding that of most other forms of lighting. Because the flashtube emits large amounts of infrared and ultraviolet radiation, only a portion of the output energy is used by the eye. The luminous efficacy is therefore typically around 50 lm/w. However, not all applications for lighting involve the human eye nor are restricted to visible wavelengths. For laser pumping, the efficacy is not related to the human eye so it is not called \"luminous\" efficacy, but rather simply \"efficacy\" as it relates to the absorption lines of the laser medium. Krypton flashtubes are often chosen for pumping s, even though their wall-plug efficiency is typically only ~ 40%. Krypton's spectral lines better match the absorption lines of the neodymium-doped crystal, thus the efficacy of krypton for this purpose is much higher than xenon; able to produce up to twice the laser output for the same electrical input. All of these terms refer to the amount of energy and lumens as they exit the light source, disregarding any losses that might occur within the lighting fixture or subsequent output optics. \"Luminaire efficiency\" refers to the total lumen-output from the fixture per the lamp output.\n\nWith the exception of a few light sources, such as incandescent light bulbs, most light sources have multiple stages of energy conversion between the \"wall plug\" (electrical input point, which may include batteries, direct wiring, or other sources) and the final light-output, with each stage producing a loss. Low-pressure sodium lamps initially convert the electrical energy using an electrical ballast, to maintain the proper current and voltage, but some energy is lost in the ballast. Similarly, fluorescent lamps also convert the electricity using a ballast (electronic efficiency). The electricity is then converted into light energy by the electrical arc (electrode efficiency and discharge efficiency). The light is then transferred to a fluorescent coating that only absorbs suitable wavelengths, with some losses of those wavelengths due to reflection off and transmission through the coating (transfer efficiency). The number of photons absorbed by the coating will not match the number then reemitted as fluorescence (quantum efficiency). Finally, due to the phenomenon of the Stokes shift, the reemitted photons will have a shorter wavelength (thus lower energy) than the absorbed photons (fluorescence efficiency). In very similar fashion, lasers also experience many stages of conversion between the wall plug and the output aperture. The terms \"wall-plug efficiency\" or \"energy conversion efficiency\" are therefore used to denote the overall efficiency of the energy-conversion device, deducting the losses from each stage, although this may exclude external components needed to operate some devices, such as coolant pumps.\n\n"}
{"id": "16141471", "url": "https://en.wikipedia.org/wiki?curid=16141471", "title": "Gaiter (vehicle)", "text": "Gaiter (vehicle)\n\nOn a vehicle, a gaiter or boot refers to a protective flexible sleeve covering a moving part, intended to keep the part clean.\n\nGaiters are pleated rubber tubes enclosing the front suspension tubes of some motorcycles and mountain bikes with telescopic front forks. Gaiters protect the sliding parts of the front suspension from dirt and water.\n\nSimilar gaiters to those described above find multiple uses on most vehicles. They are used at both ends of driveshafts, protecting constant-velocity joints from the ingress of dirt, and retaining the grease. They also prevent the ingress of dirt where one component slides within another, for example, on suspension struts or the ends of steering racks. Finally, they are also usually used to perform the same function on ball joints, which appear on suspension wishbones and steering tie rod ends. \nThe gear stick gaiter is to resist dirt entering the ball joint at the bottom of the stick and to not have oil or grease from the joint exposed to passengers. They are commonly leather, faux leather, rubber or a waterproof cloth.\n"}
{"id": "842493", "url": "https://en.wikipedia.org/wiki?curid=842493", "title": "High-electron-mobility transistor", "text": "High-electron-mobility transistor\n\nA High-electron-mobility transistor (HEMT), also known as heterostructure FET (HFET) or modulation-doped FET (MODFET), is a field-effect transistor incorporating a junction between two materials with different band gaps (i.e. a heterojunction) as the channel instead of a doped region (as is generally the case for MOSFET). A commonly used material combination is GaAs with AlGaAs, though there is wide variation, dependent on the application of the device. Devices incorporating more indium generally show better high-frequency performance, while in recent years, gallium nitride HEMTs have attracted attention due to their high-power performance. Like other FETs, HEMTs are used in integrated circuits as digital on-off switches. FETs can also be used as amplifiers for large amounts of current using a small voltage as a control signal. Both of these uses are made possible by the FET’s unique current-voltage characteristics. HEMT transistors are able to operate at higher frequencies than ordinary transistors, up to millimeter wave frequencies, and are used in high-frequency products such as cell phones, satellite television receivers, voltage converters, and radar equipment. They are widely used in satellite receivers, in low power amplifiers and in the defense industry.\n\nAdvantages of HEMTs are that they have high gain, this makes them useful as amplifiers; high switching speeds, which are achieved because the main charge carriers in MODFETs are majority carriers, and minority carriers are not significantly involved; and extremely low noise values because the current variation in these devices is low compared to other FETs.\n\nThe invention of the HEMT is usually attributed to Takashi Mimura (三村 高志) (Fujitsu, Japan) who demonstrated it in 1980. In America, Ray Dingle, Arthur Gossard and Horst Störmer also played an important role in the invention of the HEMT who also filed for a patent on such a device in April 1978. Daniel Delagebeaudeuf and Trong Linh Nuyen from Thomson-CSF (France) filed for a patent of this device in March 1979.\n\nHEMTs are heterojunctions. This means that the semiconductors used have dissimilar band gaps. For instance, silicon has a band gap of 1.1 electron volts (eV), while germanium has a band gap of 0.67 eV. When a heterojunction is formed, the conduction band and valence band throughout the material must bend in order to form a continuous level.\n\nThe HEMTs' exceptional carrier mobility and switching speed come from the following conditions: The wide band element is doped with donor atoms; thus it has excess electrons in its conduction band. These electrons will diffuse to the adjacent narrow band material’s conduction band due to the availability of states with lower energy. The movement of electrons will cause a change in potential and thus an electric field between the materials. The electric field will push electrons back to the wide band element’s conduction band. The diffusion process continues until electron diffusion and electron drift balance each other, creating a junction at equilibrium similar to a p-n junction. Note that the undoped narrow band gap material now has excess majority charge carriers. The fact that the charge carriers are majority carriers yields high switching speeds, and the fact that the low band gap semiconductor is undoped means that there are no donor atoms to cause scattering and thus yields high mobility.\n\nAn important aspect of HEMTs is that the band discontinuities across the conduction and valence bands can be modified separately. This allows the type of carriers in and out of the device to be controlled. As HEMTs require electrons to be the main carriers, a graded doping can be applied in one of the materials making the conduction band discontinuity smaller, and keeping the valence band discontinuity the same. This diffusion of carriers leads to the accumulation of electrons along the boundary of the two regions inside the narrow band gap material. The accumulation of electrons leads to a very high current in these devices. The accumulated electrons are also known as 2DEG or two-dimensional electron gas.\n\nThe term \"modulation doping\" refers to the fact that the dopants are spatially in a different region from the current carrying electrons. This technique was invented by Horst Störmer at Bell Labs.\n\nTo allow conduction, semiconductors are doped with impurities which donate either mobile electrons or holes. However, these electrons are slowed down through collisions with the impurities (dopants) used to generate them in the first place. HEMTs avoid this through the use of high mobility electrons generated using the heterojunction of a highly doped wide-bandgap n-type donor-supply layer (AlGaAs in our example) and a non-doped narrow-bandgap channel layer with no dopant impurities (GaAs in this case).\n\nThe electrons generated in the thin n-type AlGaAs layer drop completely into the GaAs layer to form a depleted AlGaAs layer, because the heterojunction created by different band-gap materials forms a quantum well (a steep canyon) in the conduction band on the GaAs side where the electrons can move quickly without colliding with any impurities because the GaAs layer is undoped, and from which they cannot escape. The effect of this is to create a very thin layer of highly mobile conducting electrons with very high concentration, giving the channel very low resistivity (or to put it another way, \"high electron mobility\"). \n\nSince GaAs has higher electron affinity, free electrons in the AlGaAs layer are transferred to the undoped GaAs layer where they form a two dimensional high mobility electron gas within 100 ångström (10 nm) of the interface. The n-type AlGaAs layer of the HEMT is depleted completely through two depletion mechanisms:\n\nThe Fermi level of the gate metal is matched to the pinning point, which is 1.2 eV below the conduction band. With the reduced AlGaAs layer thickness, the electrons supplied by donors in the AlGaAs layer are insufficient to pin the layer. As a result, band bending is moving upward and the two-dimensional electrons gas does not appear. When a positive voltage greater than the threshold voltage is applied to the gate, electrons accumulate at the interface and form a two-dimensional electron gas.\n\nMODFETs can be manufactured by epitaxial growth of a strained SiGe layer. In the strained layer, the germanium content increases linearly to around 40-50%. This concentration of germanium allows the formation of a quantum well structure with a high conduction band offset and a high density of very mobile charge carriers. The end result is a FET with ultra-high switching speeds and low noise. InGaAs/AlGaAs, AlGaN/InGaN, and other compounds are also used in place of SiGe. InP and GaN are starting to replace SiGe as the base material in MODFETs because of their better noise and power ratios.\n\nIdeally, the two different materials used for a heterojunction would have the same lattice constant (spacing between the atoms). In practice, the lattice constants are typically slightly different (e.g. AlGaAs on GaAs), resulting in crystal defects. As an analogy, imagine pushing together two plastic combs with a slightly different spacing. At regular intervals, you'll see two teeth clump together. In semiconductors, these discontinuities form deep-level traps and greatly reduce device performance.\n\nA HEMT where this rule is violated is called a pHEMT or pseudomorphic HEMT. This is achieved by using an extremely thin layer of one of the materials – so thin that the crystal lattice simply stretches to fit the other material. This technique allows the construction of transistors with larger bandgap differences than otherwise possible, giving them better performance.\n\nAnother way to use materials of different lattice constants is to place a buffer layer between them. This is done in the mHEMT or metamorphic HEMT, an advancement of the pHEMT. The buffer layer is made of AlInAs, with the indium concentration graded so that it can match the lattice constant of both the GaAs substrate and the GaInAs channel. This brings the advantage that practically any Indium concentration in the channel can be realized, so the devices can be optimized for different applications (low indium concentration provides low noise; high indium concentration gives high gain).\n\nHEMTs made of semiconductor hetero-interfaces lacking interfacial net polarization charge, such as AlGaAs/GaAs, require positive gate voltage or appropriate donor-doping in the AlGaAs barrier to attract the electrons towards the gate, which forms the 2D electron gas and enables conduction of electron currents. This behaviour is similar to that of commonly used field-effect transistors in the enhancement mode, and such a device is called enhancement HEMT, or eHEMT.\n\nWhen a HEMT is built from AlGaN/GaN, higher power density and breakdown voltage can be achieved. Nitrides also have different crystal structure with lower symmetry, namely the wurtzite one, which has built-in electrical polarisation. Since this polarization differs between the GaN \"channel\" layer and AlGaN \"barrier\" layer, a sheet of uncompensated charge in the order of 0.01-0.03 C/mformula_1 is formed. Due to the crystal orientation typically used for epitaxial growth (\"gallium-faced\") and the device geometry favorable for fabrication (gate on top), this charge sheet is positive, causing the 2D electron gas to be formed even if there is no doping. Such a transistor is normally on, and will turn off only if the gate is negatively biased - thus this kind of HEMT is known as \"depletion HEMT\", or dHEMT. By sufficient doping of the barrier with acceptors (e.g. Mg), the built-in charge can be compensated to restore the more customary eHEMT operation, however high-density p-doping of nitrides is technologically challenging due to dopant diffusion into the channel.\n\nIn contrast to a modulation-doped HEMT, an induced high electron mobility transistor provides the flexibility to tune different electron densities with a top gate, since the charge carriers are \"induced\" to the 2DEG plane rather than created by dopants. The absence of a doped layer enhances the electron mobility significantly when compared to their modulation-doped counterparts.\nThis level of cleanliness provides opportunities to perform research into the field of Quantum Billiard for quantum chaos studies, or applications in ultra stable and ultra sensitive electronic devices.\n\nApplications (eg for AlGaAs on GaAs) are similar to those of MESFETs – microwave and millimeter wave communications, imaging, radar, and radio astronomy – any application where high gain and low noise at high frequencies are required. HEMTs have shown current gain to frequencies greater than 600 GHz and power gain to frequencies greater than 1 THz. (Heterojunction bipolar transistors were demonstrated at current gain frequencies over 600 GHz in April 2005.) Numerous companies worldwide develop and manufacture HEMT-based devices. These can be discrete transistors but are more usually in the form of a 'monolithic microwave integrated circuit' (MMIC). \nHEMTs are found in many types of equipment ranging from cellphones and DBS receivers to electronic warfare systems such as radar and for radio astronomy.\n\nFurthermore, gallium nitride HEMTs on silicon substrates are used as power switching transistors for voltage converter applications. Compared to silicon power transistors gallium nitride HEMTs feature low on-state resistances, and low switching losses due to the wide bandgap properties. Gallium nitride power HEMTs are commercially available up to voltages of 200 V-600 V.\n\nHeterojunction bipolar transistors can be used for giga hertz applications.\n\n"}
{"id": "39517637", "url": "https://en.wikipedia.org/wiki?curid=39517637", "title": "Knowre", "text": "Knowre\n\nKnowre is an education technology company founded in Seoul and headquartered in New York City.\n\nKnowre launched a nationwide campaign with schools to pilot its online adaptive math program for Pre-Algebra and Algebra I in 2013.\n\n\nIn 2012, Knowre was awarded the grand prize at Global K-Startup, a competition sponsored by Google, the Korean Internet & Security Agency, and the Korea Communications Commission.\nKnowre was called one of four “South Korean Startups to Watch.”\n\nIn May 2013, Knowre won first place in the GapApp Challenge sponsored by the New York City Department of Education.\n"}
{"id": "42385374", "url": "https://en.wikipedia.org/wiki?curid=42385374", "title": "List of Windows Phone 8.1 devices", "text": "List of Windows Phone 8.1 devices\n\nThis is a list of all devices running Microsoft's Windows Phone 8.1 operating system.\n\nIn addition to existing Windows Phone 8 partners HTC, Samsung and Huawei, Gionee, JSR, Karbonn, LG, Lenovo, Longcheer, XOLO, and ZTE signed on to create Windows Phone 8.1 devices in early 2014. Miia, Micromax, Prestigio, Yezz, BLU, K-Touch and InFocus were subsequently named as hardware partners later on in the year. \n\nNokia's devices division was acquired by Microsoft in early 2014 and has since been rebranded as Microsoft Mobile. Microsoft Mobile continued to release Nokia-branded handsets running Windows Phone until a clearer strategy for aligning the Microsoft and Nokia brands was decided on in October 2014. This was to replace the Nokia name on future Lumia devices with Microsoft Lumia branding. The first device released without Nokia branding was the Lumia 535.\n\nDevices feature a dual core processor, 480p screen with 480x800 resolution and a microSD card reader. \nThese devices feature quad core processors, 480p screens with 480x800 or 480x854 resolution and microSD card readers. The devices use Snapdragon 200, 400 or 410 processors.\nThe Microsoft Lumia 535 features a quad core Snapdragon 200 processor, qHD screen at 540x960 resolution and a microSD card reader.\nThese devices feature quad core processors, 720p screens at 720x1280 resolution and microSD card readers. The devices use Snapdragon 200, 400 or 410 processors.\nThese devices feature quad core processors and 1080p screens at 1080x1920 resolution. The Nokia Lumia 930 lacks a microSD card reader and uses a Snapdragon 800 chipset, while the HTC One (M8) with Windows Phone features a card reader and uses a Snapdragon 801 chipset.\n"}
{"id": "4155566", "url": "https://en.wikipedia.org/wiki?curid=4155566", "title": "List of defunct network processor companies", "text": "List of defunct network processor companies\n\nDuring the dot-com/internet bubble of the late 1990s and early 2000, the proliferation of many dot-com start-up companies created a secondary bubble in the telecommunications/computer networking infrastructure and telecommunications service provider markets. Venture capital and high tech companies rushed to build next generation infrastructure equipment for the expected explosion of internet traffic. \nAs part of that investment fever, Network processors were seen as a method of dealing with the desire for\nmore network services and the ever-increasing data-rates of communication networks.\n\nIt has been estimated that dozens of start-up companies were created in the race to build the processors that would be a component of the next generation telecommunications equipment. Once the internet investment bubble burst, the telecom network upgrade cycle was deferred for years (perhaps for a decade). As a result, the majority of these new companies went bankrupt.\n\nAs of 2007, the only companies that are shipping network processors in sizeable volumes are Cisco Systems, Marvell, Freescale, Cavium Networks and AMCC.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "19607125", "url": "https://en.wikipedia.org/wiki?curid=19607125", "title": "List of iPod models", "text": "List of iPod models\n\nThe Apple iPod line has been upgraded many times, and each significant revision is called a \"generation\". Only the most recent generation of the iPod Touch line is available from Apple. Each new generation usually had more features and refinements while typically being physically smaller and lighter than its predecessor, while usually (but not always) retaining the older model's price tag. Notable changes included the touch-sensitive click wheel replacing the mechanical scroll wheel, use of color displays, and flash memory replacing hard disks.\n\nThe software bundled with the first generation iPod was Macintosh-only, so Windows users had to use third-party updates like ephPod or XPlay to manage their music. When Apple introduced the second generation of iPods in July 2002, they sold two versions, one that included iTunes for Macintosh users and another that included Musicmatch Jukebox for Windows users. In October 2003, Apple released the Windows version of iTunes, and started selling iPods that included both Macintosh and Windows versions of iTunes so that they could be used with either platform. Current iPods no longer ship with iTunes, which must be downloaded from Apple's website.\n\nIn December 2004, Apple unveiled its first limited edition iPods, with either Madonna’s, Tony Hawk’s, or Beck’s signature or No Doubt's band logo engraved on the back for an extra US$50. On 26 October 2004, Apple introduced a special edition of its fourth generation monochrome iPod, designed in the color scheme of the album \"How to Dismantle an Atomic Bomb\" by Irish rock band U2. It had a black case with a red click wheel and the back had the engraved signatures of U2's band members. This iPod was updated alongside the iPod Photo and fifth generation iPod.\n\nOn October 13, 2006, Apple released a special edition 4 GB red iPod nano as part of the Product Red campaign. An 8 GB version was released three weeks later and both of them sold for the same price as the standard models. US$10 from each sale is donated to The Global Fund to Fight AIDS, Tuberculosis & Malaria. On September 5, 2007, Apple also added a Product Red iPod Shuffle model. They did not disclose how much will be donated to charity from this model. Apple also released Special Edition Harry Potter iPods to accompany the iPod Photo. These were engraved with the Hogwarts Crest on the back and were only available to purchasers of the \"Harry Potter\" audiobooks. They were updated when the fifth generation iPods were released, but were only available for a limited time.\n\nOn July 27, 2017, Apple discontinued the iPod Shuffle and iPod Nano. Apple plans to keep the iPod Touch, as it is the most popular iPod device.\n\n\n"}
{"id": "38429409", "url": "https://en.wikipedia.org/wiki?curid=38429409", "title": "List of largest hourglasses", "text": "List of largest hourglasses\n\nThis article lists the largest hourglasses that have been built.\n"}
{"id": "43287831", "url": "https://en.wikipedia.org/wiki?curid=43287831", "title": "Marion C. Thurnauer", "text": "Marion C. Thurnauer\n\nMarion Charlotte Thurnauer is an American chemist at the Argonne National Laboratory. She was the first woman to be promoted to a staff position in the Chemistry Division (CHM), the first woman director of CHM, and the first woman division director at Argonne. She is an Argonne Distinguished Fellow Emeritus of the Chemical Sciences and Engineering Division, and has received numerous awards for her work in chemistry and her support of women in science.\n\nMarion Thurnauer was born in Chattanooga, Tennessee, later moving to Minnesota. Her father, Hans Thurnauer was a ceramic engineer. Her mother, Lotte Oettinger Thurnauer, died in 1959, while Marion was still young. One of her aunts, Luise Herzberg, was an astrophysicist. Thurnaeur credits family interests in science as a formative influence.\n\nShe received her B.A. (1968), M.S. (1969) and Ph.D. (1974) in chemistry, from the University of Chicago. Other women students in the chemistry program included Jeannette Manello and Barbara Warren. Thurnauer credits her future husband, Alexander Trifunaci, with convincing her to study chemistry and learn techniques that could then be applied to biological systems.\n\nThurnauer's doctoral thesis advisor was Gerhard L. Closs. For her thesis work, she studied magnetic interaction in radical pairs using electron paramagnetic resonance (EPR). The final experiments for her thesis research were conducted with an electron paramagnetic resonance spectrometer at the Argonne National Laboratory because equipment at the University of Chicago had been damaged in an explosion.\n\nThurnauer obtained a postdoctoral position in the Chemistry Division (CHM) of the Argonne National Laboratory in Argonne, Illinois. There she worked with James R. Norris and Joseph J. Katz. In her postdoctorate work, she used electron paramagnetic resonance (EPR) spectroscopy to study photochemical energy conversion in natural photosynthesis. She was the first woman to be promoted to a staff position, that of Assistant Chemist, and for several years was the only female staff scientist in the chemistry division. From 1993\nto 1995 she was the Group Leader for the Photochemical Energy Sciences Group in the Chemistry Division. In 1995 she became the first woman director of CHM, and the first woman division director at Argonne. She continued as director until 2003, when she was named Argonne Distinguished Fellow, Chemistry Division. In 2006, she became Argonne Distinguished Fellow Emeritus of the Chemical Sciences and Engineering Division. She has published more than 100 articles and holds at least 2 patents. She has actively encouraged the careers of other women scientists such as Tijana Rajh, an award-winning Argonne nanoscientist from Yugoslavia.\n\nDr. Thurnaeur studies fundamental mechanisms in photophysics and photochemistry and their applications to the design of artificial photocatalytic systems. A major area of her research has been solar photochemical energy conversion in bacterial and oxygenic photosynthesis and model photosynthetic systems. Oxygenic photosynthesis is the main process providing energy to the biosphere of the planet, creating the protective ozone layer and consuming carbon dioxide. Photosynthetic organisms use solar energy, converting it into high-energy biochemical compounds. She has modelled the spin and polarization of electrons in photosynthetic systems and helped to develop time-resolved magnetic resonance techniques for the study of sequential electron transfer in photochemical energy conversion. With researchers such as Tijana Rajh, she has studied bio-inspired nanomaterials that mimic the energy transduction of natural photosynthesis. She is a co-editor of \"The Purple Photosynthetic Bacteria\" (2008), a collection of authoritative reviews on bacterial photosynthesis. She has also engaged in research relating to heavy elements chemistry, separations chemistry, radiation chemistry, and environmental management.\n\nThurnauer was awarded the Argonne National Laboratory Pacesetter Award in 1989 for her work in establishing the \"Science Careers in Search of Women\" conference, an extremely successful program which enables students to learn about technical careers in science and engineering and meet with women working actively in science. The first year involved college women, but in ensuing years the program has worked with high school students.\n\nAfter the second conference, Argonne leadership and women scientists launched the Argonne Women in Science and Technology (WIST) program. Thurnauer and others emphasized that outreach and internal career development were closely linked, promoting change within the organization as well as without: \"Young women could not be brought to Argonne and successfully encouraged to be scientists and engineers if they observed only a few women in relatively lower level positions.\" Thurnauer served a two-year term (1992-1994) as the Women in Science Program Initiator, a paid 30% appointment, and for several years was a member of the WIST Steering Committee.\n\n"}
{"id": "45136023", "url": "https://en.wikipedia.org/wiki?curid=45136023", "title": "Mechanical device test stands", "text": "Mechanical device test stands\n\nA mechanical device test stand is one specific type of test stand. It is a facility used to develop, characterize and test mechanical components. The facility allows for the testing of the component and, it offers measurement of several physical variables associated to the functionality of the component. Such components could be electromechanical, motors or tools. The intended use of the test stand is for compliance testing of predetermined desired values and fatigue testing.\nA sophisticated mechanical component test stand houses several integrated measurement and control (imc) components, such as sensors, data acquisition devices and actuators to control the component. The sensors measure several physical variables, such as:\n\n\nInformation gathered from the sensors is processed and logged through the use of data acquisition systems. Actuators allow for attaining a desired state. Test stands for mechanical devices are often custom-built according to the requirements of the customer. They often include a feedback control system.\n\n\nResearch and Development (R&D) activities on mechanical components have necessitated sophisticated mechanical component test stands. For example, automobile OEMs or aviation OEMs are usually interested in developing mechanical components that meet the following objectives:\n\nConsequently, R&D mechanical component test stands perform a variety of exercises including measurement, control and recording of several relevant engine variables.\n\n\n"}
{"id": "54311407", "url": "https://en.wikipedia.org/wiki?curid=54311407", "title": "Mobility portfolio", "text": "Mobility portfolio\n\nA mobility portfolio is a suite of transportation options accessible with a smartphone. It interfaces with ride-sharing services such as Uber and Lyft, public transportation such as trains and buses, ride sharing and carpooling, bike sharing, and other neighborhood-based networking, focusing on \"smarter travel\" and \"smarter places\".\n"}
{"id": "51066859", "url": "https://en.wikipedia.org/wiki?curid=51066859", "title": "Mühlsteinbrüche", "text": "Mühlsteinbrüche\n\nThe Mühlsteinbrüche (\"Millstone Quarries\") south of Jonsdorf in the Zittau Mountains in Saxony are a region of bizarre rock formations, which have been formed by the quarrying of sandstone for millstones and also by weathering processes. They are a popular hiking and climbing area. An educational trail runs through the region which has an area of about 35 hectares.\n\nThe Mühlsteinbrüche are located south of Jonsdorf at a height of 480 to 560 metres. To the southwest they are adjoined by the Jonsdorfer Felsenstadt. About 600 metres to the south is the border with the Czech Republic.\n\nThe sandstone in the Mühlsteinbrüche region was formed in the geological age of the Middle Turonian and, based on its formation, belongs to the Bohemian Cretaceous Basin. It is characterised by several special features. For example, the sandstone, which was formed by sedimentation, has been breached multiple times by basaltic and phonolitic intrusions as a result of contact with occurrences of Tertiary volcanism in North Bohemia. Often, in these cases, the rock has been exposed to thermal changes (contact with magma and hydrothermal fluids), which has led to vitrification of the rock (a melting of mineral grains at their grain boundaries). The sandstone retains its porosity, is hardened by the strength of its grain structure and thus its abrasion resistance is increased.\nThe area of Mühlsteinbrüche has as additional geological feature: its pronounced columnar sandstone. Its best known rock formation is the \"Große und Kleine Orgel\" (\"Great and Little Organ\"), whose appearance is easily confused with that of columnar basalt. The columns, which are completely atypical for sandstone, have a diameter of up to 15 centimetres and are bundled vertically on a solid, non-columnar sandstone block. They have been formed by thermal processes as a result of contact with magma. In 1852 Alexander von Humboldt sent an artist and two geologists here to study this phenomenon.\n\nThe use of sandstone in the Zittau Mountains for the production of millstones was already under way from the 16th century. The Jonsdorf Mühlsteinbrüche were one of more than 40 quarrying sites in the surrounding region. In 1560, quarrying began in Jonsdorf itself. Over 350 years, sandstone was quarried here and made into millstones with a diameter of up to 2.70 metres. To do this, the millstones were initially hewn out of one piece of rock. From about 1850, millstones were assembled from several pieces as there were no longer any sufficiently large and homogeneous sandstone blocks being quarried. Production was maintained until 1918. The main markets were in Russia and England. Moscow and Saint Petersburg had their own offices for the millstone trade.\nIn the 1950s, a nature trail was laid out by friends of nature and local history. From 1990, a restoration of the quarries was begun in order to make them accessible. In 2002, a demonstration workshop was built in the \"Schwarzer Loch\" (\"Black Hole\"). Today the Mühlsteinbrüche are a major attraction in the Zittau Mountains.\n\n\n\n"}
{"id": "55809797", "url": "https://en.wikipedia.org/wiki?curid=55809797", "title": "Online youth radicalization", "text": "Online youth radicalization\n\nOnline youth radicalization is the process of by which an young individual, or group of people comes to adopt increasingly extreme political, social, or religious ideals and aspirations that reject or undermine the status quo or undermine contemporary ideas and expressions of the nation. As for radicalization, online youth radicalization can be both violent or non-violent.\n\nThe phenomenon often referred to as \"incitement to radicalization towards violent extremism\" (or \"violent radicalization\") has grown in recent years. This is mainly in relation to the Internet in general and social media in particular. In parallel to the increased attention to online \"incitement to extremism and violence\", attempts to prevent this phenomenon have created challenges for freedom of expression. These range from indiscriminate blocking, censorship over-reach (affecting both journalists and bloggers), and privacy intrusions—right through to the suppression or instrumentalisation of media at the expense of independent credibility).\n\nIn a quick and easy way to show action after terrorist attacks, political pressure is ease to accuse social media companies of responsibility and call them to do more to prevent online radicalization of young people leading to violent extremism. UNESCO calls for \"a policy that is constructed on the basis of facts and evidence, and not founded on hunches—or driven by panic and fearmongering.\"\n\nCyberspace is used to denote the Internet, as a network of networks, and social media as a social network that may combine various Internet platforms and applications to exchange and publish online: the online production of radical (political, social, religious) resources or content, the presence of terrorist or radicalized groups within the social networks, and the participation of young people in radical conversations.\n\nThere is no consensus definition, broadly speaking \"radicalization\" refers to a process in which individuals are moved towards beliefs deemed \"extreme\" by the status quo. Not all processes of radicalization, however, have acts of violence as either their goal or their outcome. Concern is with radicalization processes which intentionally result in violence, and particularly when that violence is terroristic in targeting civilians. Communications—offline and offline—play a part in radicalization processes, along with events and how individuals interpret their life experiences.\n\nYet distinctions need to be made between communications that may be perceived as \"extreme\", but which do not rise to the level of constituting criminal incitement or recruitment, and those which advocate for violent acts to be committed. Although scholars emphasize different aspects, there are three main recurring characteristics in the way that they conceptualize specifically violent radicalization.\n\nIn this sense, the concept of violent radicalization (or radicalization leading to violent acts) covers an observable process involving the individual person’s search for fundamental meaning, origin and return to a root ideology, the polarization of the social space and the collective construction of a threatened ideal \"us\" against \"them\", where others are dehumanized by a process of scapegoating, a group’s adoption of violence as a legitimate means for the expansion of root ideologies and related oppositional objectives.\n\nTwo major schools of theory can be discerned in the reception of Internet and social media. These schools largely originate in pre-digital media, but are still being applied (usually implicitly) to the Internet era. The effects-based school perceives the Internet and social media as highly powerful means of communication and propaganda that over-determine other communication tools and processes. Social media are seen as highly effective drivers of propaganda, conspiracy theories and the rise of extremism through de-sensitization which leads to individuals accepting the use of violence. The uses-based school sheds doubts on the structuring effects of social media by empirically identifying only indirect and limited effects. In this paradigm, \"the role of social media in violent radicalization and extremism constitutes a reflection of real offline social ruptures\".\n\nChatrooms can be embedded within most Internet-based media. Reports that have looked into the use of chatrooms by violent extremist groups, describe these as the space where atrisk youth without previous exposure would be likely to come across radicalizing religious narratives. This goes in line with Sageman’s emphasis on the role of chatrooms and forums, based on his distinction between websites as passive sources of news and chat rooms as active sources of interaction. According to Sageman, \"networking is facilitated by discussion forums because they develop communication among followers of the same ideas (experiences, ideas, values), reinforce interpersonal relationships and provide information about actions (tactics, objectives, tutorials)\". Chatrooms can also include spaces where extremist people share information such as photos, videos, guides, and manuals.\n\nMany extremist groups are ideologically and strategically anti-Facebook, but a strong presence still exists on this platform either directly or through supporters. Facebook does not seem to be used for direct recruitment or planning, possibly because it has mechanisms of tracking and can link users with real places and specific times. Facebook appears to have been more often used by extremists as a decentralized center for the distribution of information and videos or a way to find like-minded supporters and show support rather than direct recruitment. This may be on the possibility that young sympathizers can share information and images and create Facebook groups in a decentralized way.\n\nMicro-blogging sites like Twitter present more advantages for extremist groups because traceability of the identity and the source of the tweets are harder to achieve, thus increasing the communication potential for recruiters. Analyses of Twitter feeds generated by Islamist violent extremist groups show that they are mostly used for engaging with the opposition and the authorities, in what appear to be tweetclashes that mobilize the two sides, and also used for provocation. Through Twitter, extremists can easily comment publicly on international events or personalities, in several languages, enabling the activists to be vocal and timely when mounting campaigns.\n\nYouTube has the advantage of being difficult to trace the identity of people posting content, while offering the possibility for users to generate comments and share contents. Several researchers have conducted content analyses of YouTube and Facebook extremist discourses and video contents to identify the production features most used, including their modus operandi and intended effects. Studies that have focused on the rhetorical strategy of extremist groups show the multifaceted use of online resources by extremist groups. That is, they produce \"hypermedia seduction\" via the use of visual motifs that are familiar to young people online; and they provide content in several languages, mostly Arabic, English and French using subtitles or audio dubbing, to increase the recruitment capacity of youth across nations. These videos provide rich media messaging that combines nonverbal cues and vivid images of events that can evoke psychological and emotional responses as well as violent reactions. Terrorists capture their attacks on video and disseminate them though the Internet, communicating an image of effectiveness and success. Such videos in turn are used to mobilize and recruit members and sympathizers. Videos also serve as authentication and archive, as they preserve live footage of actual damage and they validate terrorist performance acts.\n\nThe Internet and Social media has numerous advantages for extremist groups using religion as part of a radicalization strategy. The advantages stem from the very nature of Internet and social media channels and the way they are used by extremist groups. These include communication channels that are not bound to national jurisdictions and that are informal, large group, cheap, decentralized and anonymous. This allows terrorists to network across borders and to bypass time and space. Specifically, these channels provide networks of recruiters, working horizontally, in all the countries they target due to the transborder nature of the Internet.\n\nWeinmann describes extremist groups’ use of Internet and social media in eight process strategies: \"psychological warfare, publicity and propaganda, data mining, fundraising, recruitment and mobilization, networking, information sharing and planning and coordination\". Conway identifies five-core terrorist uses of the Internet and social media: \"information provision, financing, networking, recruitment and information gathering\". The ones most relevant to social media and radicalization of young people are information provision, such as profiles of leaders, manifestos, publicity and propaganda, and recruitment. Some studies show that social media enable people to isolate themselves in an ideological niche by seeking and consuming only information consistent with their views, as well as simultaneously self-identifying with geographically distant international groups of international which, therefore, creates a sense of community beyond borders. This ability to communicate can promote membership and identity quests faster and in more efficient ways than in the \"real\" social world.\n\nWhile recruitment is a process, and not instantaneous, it is seen in the literature as a phase of radicalization, taking the process to a new level of identification and possible action. Indoctrination is easier post-recruitment and often occurs in specific virtual spaces where the extremist rhetoric is characterized by a clear distinction between \"them\" (described negatively) and \"us\" (described positively), and where violent actions are legitimized according to the principle of \"no other option available\". These advantages of Internet and social media open up prospects for extremist groups, by facilitating what used to be referred previously as block recruitment and by substituting group decision to individual decision-making.\n\nSome reports show that extreme right-wing groups take advantage of the freedom of speech guaranteed by many countries’ legislations, to post hateful comments that however do not represent full hate-speech or illegal acts. Furthermore, these groups seem to mobilize efforts on Internet and social media to convey a more acceptable public image and recruit new members who would otherwise be offended by blatantly racist or hate-based discourse.\n\nFor example, the discourse found on Stormfront is particularly noteworthy because it clearly shows the transition towards a more \"acceptable\" form of racist discourse. The discourses seem to be less aggressive, even condemning violence and refusing to resort to an incendiary rhetoric. Instead, the discourses that are posted use seemingly scientific or intellectual theories about racial differences, in a watered-down version of racist discourse, relying on apparently reliable sources of information that appeal to the general public. More specifically, the racist discourses used by Stormfront relies on a \"us\" vs. \"them\" rhetoric, portraying them or ‘the other’ in five ways as: a) tyrannical (submits white people to rules and laws that serve him, e.g., Jews control the media and the economy); b) manipulator (uses deceit to achieve aims, e.g., brainwashing children with pro-black school programs); c) genocidal (e.g., multiculturalism and interracial marriage are seen as ways to eradicate the white race); d) inferior (e.g., less capacities than white people); and e) a false martyr (e.g., manipulates history to be seen as a victim).\n\nAuthors argue that cyberspace is helping to create a strong extreme right-wing collective identity and a sense of belonging to a global scale via a process of networking, sharing of information (values, symbols and fears, not just facts), discussion, recruitment and event organization, in similar ways to religious extremist movements. Exposure to extreme right-wing discourses also seems to lead to a significant radicalization of attitudes among certain individuals from diverse political affiliations. Conversely, this was not the case for exposure to extreme left-wing discourses or exposure to moderate media and mixed media (control group), which generally resulted in a decrease of extremist attitudes. Some researchers argue that spreading the message to a bigger audience and inspiring violence can make the recruitment easier, but there is no evidence that it leads to a full process of violent radicalization or actual acts of violence being committed.\n\nBouzar, Caupenne and Sulayman (2014) present the results of interviews with 160 French families with radicalized (though not violent) children aged mainly between 15 and 21. The vast majority of the young people claimed to have been radicalized through the Internet, and this was the case regardless of their family characteristics and dynamics. The vast majority of the families (80%) did not follow any specific religious beliefs or practices and only 16% belonged to the working class.\n\nWojcieszak analysed cross-sectional and textual data obtained from respondents in neo-Nazi online discussion forums. The author found that \"extremism increases with increased online participation, probably as a result of the informational and normative influences within the online groups\". In addition, exposure to different parties/views offline that are dissimilar to the extremist group’s values has in some instances reinforced radical beliefs online.\n\nMany authors hypothesize potential causation by associating online radicalization with external factors such as: search for identity and meaning, the growing inequalities in European and other societies, unemployment and fewer opportunities for development especially for minority youth, exclusion, discrimination and inequality that are massively used in extremist discourses.\n\nThe analysis of the profiles of researchers and publications on violent radicalization from the Arab world reveals the prominence of specialists on Islamist movements. They are, most often, humanities and social science researchers and some are specialists in media and public opinion, international relations, or even security. Another specificity of research on violent radicalization in the Arabic-speaking region is the involvement of religious researchers in this field. The main objective of this contribution is part of a state strategy to counter faith advocated by violent radical groups. In this logic, radicalization or jihadism are replaced by the term terrorist in referral to these groups. In other regions, experts use terms such as jihadist Salafism or jihadism or violent radicalization. There is a clear tendency among most Arabic-speaking researchers to avoid the use of the word Islam and its semantic field to denote violent radical groups. This is also why researchers from the region prefer to use the Arabic acronym Daesh or the State Organization instead of the ‘Islamic State.’ Most research published from the Arab world does not focus on the relation between violent radicalization and Internet or social media, nor does it evaluate the effect of prevention or intervention cyberinitiatives.\n\nArab youth are major consumers of social media networks and especially Facebook, which is one of the top ten most used sites by Arab Internet users, a tendency that quickly found its translation into the Arab political realm. According to a study by Mohamed Ibn Rachid Faculty for governance in the United Arab Emirates, the number of Facebook users in 22 Arab countries increased from 54.5 million in 2013 to 81.3 million in 2014 with a majority being young people. The study of literature in the region reveals the role played by social networks, especially Facebook and Twitter, as platforms for collective expression for Arab youth on current issues, conflicts and wars (e.g., Gaza situation in particular). In Iraq, for example, young Internet users and bloggers launched several campaigns on Facebook and Twitter at the beginning of military operations to free the major cities occupied by ISIS (Fallujah and Mosul). In Morocco, other initiatives with the same objective were launched such as the one by Hamzah al-Zabadi on Facebook ( مغاربة_ضد_داعش# ; Moroccans against Daesh), which consisted of sharing all kinds of content (images, texts,etc.) to contradict and challenge ISIS’s narratives. The involvement of civil society actors on the web in the fight against terrorism and violent radicalization in the Arab region remains modest for many reasons including the lack of media policies dedicated to this struggle.\n\nResearchers in Asia have developed a complex understanding of radicalization as being deeply connected to psychosocial and economic grievances such as poverty and unemployment, marginalization through illiteracy and lack of education, as well as admiration for charismatic leaders, pursuit of social acceptability and psychological trauma. These factors are considered by authors to facilitate online radicalization-oriented recruitment, especially among young people, who are more vulnerable and who spend more time online.\n\nA report by \"We Are Social\" in 2016 reveals that East Asia and Southeast Asia are the first and second social media markets worldwide, with North America in the third rank. According to the same report, Facebook and Facebook Messenger are the predominant social and communications tools, followed by Twitter, Line and Skype. China is the notable exception as Facebook Messenger is outpaced by far by Chinese social media tools. China presents a very different profile from most countries in its mainstream social media and networks. American platforms such as Google, Yahoo!, Facebook, Twitter and YouTube have very little penetration due to state restrictions and the strong monopoly of homegrown search engines and Internet platforms in Chinese language.\n\nThere is rising interest among Chinese researchers in examining the relationship between social media and violent radicalization. Research into violent radicalization or terrorism in China is mainly on radicalization in Xinjiang. This could be linked to the fact that most of the recent terrorist attacks in China were not perpetrated by local residents, but by outsider violent extremist organizations that seek to separate the Xinjiang area from China. Terrorist organizations spread their messages via TV, radio and the Internet. Though there is no empirical evidence linking youth radicalization to online social media, the anonymity and transborder capacity of such media is seen as a \"support for organized terrorist propaganda\". The Chinese government has been responding to terrorist attacks by taking down sites, blocking and filtering content. In return, Chinese government also uses the social media for messaging against terrorism.\n\nIndonesia has an estimated 76 million Indonesians who connect regularly on Facebook, establishing the nation as the fourth largest user of the world, after India, the United States and Brazil. Indonesia is also the fifth largest user of Twitter, after the United States, Brazil, Japan and the United Kingdom. The Institute for Policy Analysis of Conflict (IPAC) examines how Indonesian extremists use Facebook, Twitter and various mobile phone applications such as WhatsApp and Telegram. In recent research, Lefevre shows that the use of social media in Indonesia by extremists is progressing. They use social media, such as Facebook and Twitter, to communicate with young people, to train and to fundraise online. Recruitment is done through online games, propaganda videos on YouTube and calls to purchase weapons. The proliferation of ISIS propaganda via individual Twitter accounts has raised concerns about the possibility of \"lone actor\" attacks. That being said, the report points out that such attacks are extremely rare in Indonesia.\n\nThere is little contemporary research on online radicalization in Sub-Saharan Africa. Yet Africa carries at its heart a powerful extremist group: \"Boko Haram\" whose real name is Jama’atu Ahlu-Sunna wal Jihad Adda’wa Li («Group of the People of Sunnah for Preaching and Jihad») since 2002 and has pledged allegiance to the Daesh. The network is less resourceful and financed compared to Daesh, but it seems to have entered in a new era of communication by the use of social media networks, more so since its allegiance to Daesh. To spread their principles this terrorist group uses the Internet and adapts Daesh communication strategies to the sub-Saharan African context to spread its propaganda (also in French and English) with more sophisticated videos. By its presence on the most used digital networks (Twitter, Instagram), Boko Haram breaks with traditional forms of communication in the region such as propaganda videos sent to agencies on flash drives or CD-ROM. Video content analyses has also shown a major shift from long monologues from the leader Abubakar Shekau, that had poor editing and translation, to messages and videos that have increased its attractiveness among sub-Saharan youth. Today, Boko-Haram owns a real communications agency called «al-Urwa Wuqta» (literally «the most trustworthy», «the most reliable way»). Moreover, the group multiplies its activities on Twitter especially via their smartphones, as well as through YouTube news channels. Most tweets and comments of the group’s supporters denounce the Nigerian government and call for support for Boko Haram movement. The tweets are written in Arabic at first and then translated and passed on in English and French, which reflect the group’s desire to place itself in the context of what it sees as global jihad. In a recent study conducted in 2015, researchers have shown how Boko Haram related tweets include rejection of the movement by non-members of the organisation.\n\nVan Eerten, Doosje, Konijn, De Graaf, and De Goede suggest that counter or alternative narratives could be a promising prevention strategy. Some researchers argue that a strong alternative narrative to violent jihadist groups is to convey the message that they mostly harm Muslims. During the last decade, the United States government has set up two online programs against radicalization designed to counter anti-American propaganda and misinformation from al-Qaeda or the Islamic state. These programs seek to win the \"war of ideas\" by countering self-styled jihadist rhetoric.\n\nPrivate sector counter-initiatives involve the YouTube Creators for Change with young \"ambassadors\" mandated to \"drive greater awareness and foster productive dialogue around social issues through content creation and speaking engagements\"; the \"redirectmethod.org\" pilot initiative to use search queries in order to direct vulnerable young people to online videos of citizen testimonies, on-the-ground reports, and religious debates that debunk narratives used for violent recruitement. The initiative avoids \"government-produced content and newly or custom created material, using only existing and compelling YouTube content\".\n\nSeveral governments are opting to invest in primary prevention through education of the public at large, and of young public in particular, via various \"innoculatory\" tactics that can be grouped under the broad label of Media and Information Literacy (MIL). Based on knowledge about the use of MIL in other domains, this initiative can be seen, interalia, as a long term comprehensive preventive strategy for reducing the appeal of violent radicalization.\n\nA knowledge hub for alternative narratives http://www.idareact.org\n\nThe Knowledge Hub is an online platform whereby different contributions and expertise about specific themes are gathered and inserted in the framework of a synergetic learning process. Hence, I-Dare has launched this knowledge hub in order to be a powerful resource and a virtual point of aggregation for promoting alternative narratives. I-Dare acknowledges the need for a comprehensive approach towards tackling such a multifaceted topic. Here is why the Alternative Narratives Knowledge Hub is organized into several different categories, such as Articipate, research, alternativism, violent extremism and other content relevant to providing alternative narratives.\n\nAlternative Narratives are needed to balance the current paradigms which rely on intellectually wrong assumptions and perpetuate stereotypes which increase the risks of extremism, hate, and violence and constitute obstacles to intercultural dialogue. Hence, the mission of the knowledge hub is to promote alternative narratives for reducing such risks and dismantle these obstacles. Raising above stereotypes is not an easy task as they are enshrined in the popular culture, in the common discourse and sometimes even in the official discourse. They are so hard to be balanced since they are based on illusory correlations which simplify our representation of the world. In other words, stereotypes are intellectual shortcuts. To be effective, alternative narratives must then be based on facts, data and clear evidence. Their ultimate goal is to convince people to abandon the aforementioned intellectual shortcuts and to base their opinions on solid and proved correlations, not on illusory ones.\n\nMIL has a long tradition of dealing with harmful content and violent representations, including propaganda. In its early history, MIL was mostly put in place to fight misinformation (particularly in advertising) by developing critical skills about the media. By the 1980s, MIL also introduced cultural and creative skills to use the media in an empowering way, with active pedagogies. Since the years 2000, MIL has enlarged the media definition to incorporate the Internet and social media, adding issues related to ethical uses of online media to the traditional debates over harmful content and harmful behavior and aligning them more with the perspectives that consider issues of gratifications of media users.\n"}
{"id": "21641725", "url": "https://en.wikipedia.org/wiki?curid=21641725", "title": "Piero Magni", "text": "Piero Magni\n\nPiero Magni (Genoa, December 22, 1898 - April 17, 1988) was an Italian aeronautical engineer. He was heavily involved with the National Advisory Committee for Aeronautics later in his career.\n\nThanks to some of his intuitions, he has been the precursor of some devices extremely actual nowadays, like the variable incidence wings, the development of the canard wings or monoplane aircraft with the wing held by a single post. His activity has not been limited to the design, becoming also an entrepreneur, establishing a firm initially dealing with the manufacture under license of gliders, and then taking charge of the audit of aircraft for the Regia Aeronautica. He designed the Vittoria 1924 sport plane.\n\nHe succeeded also in manufacturing some of his designed aircraft characterized by his designed innovative features. On that sphere he experimented the most important one, a patent of a particular cowling to be applied on a radial engine, that allowed to remarkably increase the aircraft aerodynamics penetration and allowing at the same time an efficient system of temperature regulation of the aircraft itself. This project, known as Anello Magni, has been further developed by the National Advisory Committee for Aeronautics, becoming known as NACA cowling\n\nAlso his activity in the publishing sector are worth mentioning, either as writer of technical books, either as director of “L’Aeronautica”, supplement of the fortnightly magazine of the fascist aviation “L’Ala d’Italia”.\n"}
{"id": "1796566", "url": "https://en.wikipedia.org/wiki?curid=1796566", "title": "Poverty industry", "text": "Poverty industry\n\nThe terms poverty industry or poverty business refer to a wide range of money-making activities that attract a large portion of their business from the poor because they are poor. Businesses in the poverty industry often include payday loan centers, pawnshops, rent-to-own centers, casinos, liquor stores, lotteries, tobacco stores, credit card companies, and bail-bond services. Illegal ventures such as loansharking might also be included. The poverty industry makes roughly US$33 billion a year in the United States. In 2010, elected American federal officials received more than $1.5 million in campaign contributions from poverty-industry donors.\n\n\n"}
{"id": "29572828", "url": "https://en.wikipedia.org/wiki?curid=29572828", "title": "Quality control system for paper, board and tissue machines", "text": "Quality control system for paper, board and tissue machines\n\nA quality control system (QCS) refers to a system used to measure and control the quality of moving sheet processes on-line as in the paper produced by a paper machine. Generally, a control system is concerned with measurement and control of one or multiple properties in time in a single dimension. A QCS is designed to continuously measure and control the material properties of the moving sheet in two dimensions: in the machine direction (MD) and the cross-machine direction (CD). The ultimate goal is maintaining a good and homogenous quality and meeting users' economic goals.A basic quality measurement system generally includes basis weight and moisture profile measurements and in addition average basis weight of the paper web and moisture control related to these variables. Caliper is also one of the basic measurements. \nOther commonly used continuous measurements include: ash content, color, brightness, smoothness and gloss, coat weight, formation], porosity, fiber orientation, and surface properties (topography). \n\nQCS is used in paper machines, board machines, tissue machines, pulp drying machines, and other plastic or metal film processes\n\nIn modern systems QCS applications can be embedded to distributed control systems.\n\nSensors measuring the paper quality (online meters) are attached to a sensor platform that move across the web guided by the scanner beam. A typical crossing time for a sensor platform is 10–30 s (an 8 m web, 60 cm/s). The sensor platform scans across the paper web and continuously measures paper characteristics from edge to edge. It can also be directed and stopped to a specific, fixed point on the web to measure the machine-direction (MD) variation at a single point.\n\nThe QCS scanner beam is an essential part of a QCS system. Wide machines and accurate profile calculations require beam stability and accuracy of mechanical movement. As high accuracy in demanding and variable conditions is required, the sensitive sensors must be securely fastened. The most important goal is maintaining the exact respective position of the upper and lower measurement platforms in relation to their distance from each other, in MD and in CD . This is achieved through a robust construction and by reducing the effects of temperature and other environmental effects and through a moving mechanism with minimized backlash of the measurement platform.\n\nUsually the scanner beam also contains all the cables and the air, cooling liquid and protection gas pipes. The base of the scanner beam contains elements that dampen vertical vibration.\n\nA basic quality measurement system generally includes basis weight and moisture profile measurements and in addition average basis weight of the paper web and moisture control related to these variables. Caliper is also one of the basic measurements. \nOther commonly used continuous measurements include: ash content, color, brightness, smoothness and gloss, coat weight, formation], porosity, fiber orientation, and surface properties (topography).\n\nOnline sensors are set on the scanner beam to scan across the web. Typical crossing time of the web in new systems is 10–30 s (8m web, 60 cm/s). If the web speed is 1200 m/min and web width 8.5 m, the web moves 280 m during a scan, and the sensor moves the same distance diagonally across the web. The measurements are taken on the diagonal line and act as basis for profile (machine and cross-direction) and variation calculations. This value is subject to integration depending on the machine speed. If the measurement signal sampling frequency is in size range 2000/s, then the smallest measurement element is about 0.2 cm in cross-direction. The measurement data is integrated to eliminate a small-scale formation variation from the measurement result. \n\nThe measurement value is averaged so that each sensor gives one measurement value per one data box which is typically from 5 mm to one centimeter of web width. For a 1 meter wide web, for instance, 100 - 200 measurement values are taken. These measurement values from a single scan (profile points) are called 'raw profiles'. In modern quality control systems, the width of these data boxes can be changed, and accurate profiles can be formed using several thousands profile data boxes.\n\nTypically, the sensor's output is the instantaneous value, the profile average value and the complete profile.\n\nThe requirements for an ideal paper machine online sensor include the following: \nthe sensor is calibrated to a natural constant during the measurement; \nthe sensor and the related electronics include fault diagnostics; \ndigital processing of the signal is possible from the start without destroying the possibility of analyzing large frequency components; \nthe sensor system does not disturb the production; \nthe measurements are performed real-time and can be adjusted without delays; \nthe measurement concerns the entire production, not just small sample values. \nIt must be possible to distinguish between the machine-directional and cross-directional deviation and the residual deviation as the control system handles these three deviations separately and in different ways. The earlier systems calculated a long-term average profile to filter the profile. As several quality profiles can be adjusted automatically it is important to get the right profile data with high resolution quickly to the control system. This is especially important during changes, after breaks and during grade changes. In advanced systems, algorithms are used to calculate the profile data.\n\n"}
{"id": "23692678", "url": "https://en.wikipedia.org/wiki?curid=23692678", "title": "Quantum dot display", "text": "Quantum dot display\n\nA quantum dot display is a display device that uses quantum dots (QD), semiconductor nanocrystals which can produce pure monochromatic red, green, and blue light.\n\n\"Photo-emissive\" quantum dot particles are used in a QD layer which converts the backlight to emit pure basic colors; this improves display brightness and color gamut by reducing light losses and color crosstalk in RGB color filters. This technology is used in LED-backlit LCDs, though it is applicable to other display technologies which use color filters, such as white or blue/UV OLED.\n\n\"Electro-emissive\" or \"electroluminiscent\" quantum dot displays are an experimental type of new display based on quantum-dot light-emitting diodes (QLED, QD-LED; also EL-QLED, ELQD, QDEL). These displays are similar to active-matrix organic light-emitting diode (AMOLED) and MicroLED displays, in that light would be produced directly in each pixel by applying electric current to inorganic nano-particles. QD-LED displays could support large, flexible displays and would not degrade as readily as OLEDs, making them good candidates for flat-panel TV screens, digital cameras, mobile phones and handheld game consoles.\n\nAs of 2019, all commercial products, such as LCD TVs using quantum dots and branded as \"QLED\", use \"photo-emissive\" particles. \"Electro-emissive\" QD-LED TVs exist in laboratories only.\n\nThe idea of using quantum dots as a light source emerged in the 1990s. Early applications included imaging using QD infrared photodetectors, light emitting diodes and single-color light emitting devices. Starting from early 2000, scientists started to realize the potential of developing quantum dot for light sources and displays.\n\nQDs are both \"photo-emissive\" (photoluminescent) and \"electro-emissive\" (electroluminescent) allowing them to be readily incorporated into new emissive display architectures. Quantum dots naturally produce monochromatic light, so they are more efficient than white light sources when color filtered and allow more saturated colors that reach nearly 100% of Rec. 2020 color gamut. \n\nA widespread practical application is using quantum dot enhancement film (QDEF) layer to improve the LED backlighting in LCD TVs. Light from a blue LED backlight is converted by QDs to relatively pure red and green, so that this combination of blue, green and red light incurs less blue-green crosstalk and light absorption in the color filters after the LCD screen, thereby increasing useful light throughput and providing a better color gamut. \n\nThe first manufacturer shipping TVs of this kind was Sony in 2013 as \"Triluminos\", Sony's trademark for the technology. At the Consumer Electronics Show 2015, Samsung Electronics, LG Electronics, TCL Corporation and Sony showed QD-enhanced LED-backlighting of LCD TVs. At the CES 2017, Samsung rebranded their 'SUHD' TVs as 'QLED'; later in April 2017, Samsung formed the QLED Alliance with Hisense and TCL to produce and market QD-enhanced TVs.\n\nQuantum dot on glass (QDOG) replaces QD film with a thin QD layer coated on top of the light-guide plate (LGP), reducing costs and improving efficiency.\n\nTraditional white LED backlights that use blue LEDs with on-chip or on-rail red-green QD structures are being researched, though high operating temperatures negatively affect their lifespan.\n\n'QDCF' LED-backlit LCDs would use QD film or ink-printed QD layer with red/green sub-pixel patterned quantum dots, aligned to precisely match the red and green subpixels; blue subpixels can be transparent to pass through the pure blue LED backlight, or can be made with blue patterned quantum dots in case of UV-LED backlight. This configuration effectively replaces passive color filters, which incur substantial losses by filtering out 2/3 of passing light, with photo-emissive QD structures, improving power efficiency and/or peak brightness, and enhancing color purity. Because quantum dots depolarize the light, output polarizer (the analyzer) needs to be moved behind the color filter and embedded in-cell of the LCD glass; this would improve viewing angles as well. In-cell arrangement of the analyzer and/or the polarizer would also reduce depolarization effects in the LC layer, increasing contrast ratio. To reduce self-excitement of QD film and to improve efficiency, the ambient light can be blocked using traditional color filters, and reflective polarizers can direct light from QD filters towards the viewer. As only blue or UV light passes through the liquid crystal layer, it can be made thinner, resulting in faster pixel response times. \n\nQD color filters can also be back-lit with OLED or micro-LED panels, improving their efficiency and color gamut. Samsung intends to start pilot production of QD-OLED panels in 2019.\n\nNanosys made presentations of their photo-emissive technology during 2017; commercial products are expected by 2019, though in-cell polarizer remains a major challenge. \n\nAMQLED displays will use electroluminescent QD nanoparticles functioning as Quantum-dot-based LEDs (QD-LEDs or QLEDs) arranged in an active matrix array. Rather than requiring a separate LED backlight for illumination and TFT LCD to control the brightness of color primaries, these QLED displays would natively control the light emitted by individual color subpixels, greatly reducing pixel response times by eliminating the liquid crystal layer.\n\nThe structure of a QD-LED is similar to the basic design of an OLED. The major difference is that the light emitting devices are quantum dots, such as cadmium selenide (CdSe) nanocrystals. A layer of quantum dots is sandwiched between layers of electron-transporting and hole-transporting organic materials. An applied electric field causes electrons and holes to move into the quantum dot layer, where they are captured in the quantum dot and recombine, emitting photons. The demonstrated color gamut from QD-LEDs exceeds the performance of both LCD and OLED display technologies.\n\nMass production of active-matrix QLED displays using ink-jet printing is expected to begin in 2020-2021. InP ink-jet solutions are being researched by Nanosys, Nanoco, Nanophotonica, OSRAM OLED, Fraunhofer IAP, and Seoul National University, among others. \n\nPerformance of QDs is determined by the size and/or composition of the QD structures. Unlike simple atomic structures, a quantum dot structure has the unusual property that energy levels are strongly dependent on the structure's size. For example, CdSe quantum dot light emission can be tuned from red (5 nm diameter) to the violet region (1.5 nm dot). The physical reason for QD coloration is the quantum confinement effect and is directly related to their energy levels. The bandgap energy that determines the energy (and hence color) of the fluorescent light is inversely proportional to the square of the size of quantum dot. Larger QDs have more energy levels that are more closely spaced, allowing the QD to emit (or absorb) photons of lower energy (redder color). In other words, the emitted photon energy increases as the dot size decreases, because greater energy is required to confine the semiconductor excitation to a smaller volume.\n\nNewer quantum dot structures employ indium instead of cadmium, as the latter is not exempted for use in lighting by the European Commission RoHS directive.\n\nQD-LEDs are characterized by pure and saturated emission colors with narrow bandwidth, with FWHM (full width at half maximum) in the range of 20-40 nm. Their emission wavelength is easily tuned by changing the size of the quantum dots. Moreover, QD-LED offer high color purity and durability combined with the efficiency, flexibility, and low processing cost of comparable organic light-emitting devices. QD-LED structure can be tuned over the entire visible wavelength range from 460 nm (blue) to 650 nm (red) (the human eye can detect light from 380 to 750 nm). The emission wavelengths have been continuously extended to UV and NIR range by tailoring the chemical composition of the QDs and device structure.\n\nQuantum dots are solution processable and suitable for wet processing techniques. The two major fabrication techniques for QD-LED are called phase separation and contact-printing.\n\nPhase separation is suitable for forming large-area ordered QD monolayers. A single QD layer is formed by spin casting a mixed solution of QD and an organic semiconductor such as TPD (N,N′-Bis(3-methylphenyl)-N,N′-diphenylbenzidine). This process simultaneously yields QD monolayers self-assembled into hexagonally close-packed arrays and places this monolayer on top of a co-deposited contact. During solvent drying, the QDs phase separate from the organic under-layer material (TPD) and rise towards the film's surface. The resulting QD structure is affected by many parameters: solution concentration, solvent ration, QD size distribution and QD aspect ratio. Also important is QD solution and organic solvent purity.\n\nAlthough phase separation is relatively simple, it is not suitable for display device applications. Since spin-casting does not allow lateral patterning of different sized QDs (RGB), phase separation cannot create a multi-color QD-LED. Moreover, it is not ideal to have an organic under-layer material for a QD-LED; an organic under-layer must be homogeneous, a constraint which limits the number of applicable device designs.\n\nThe contact printing process for forming QD thin films is a solvent-free water-based suspension method, which is simple and cost efficient with high throughput. During the process, the device structure is not exposed to solvents. Since charge transport layers in QD-LED structures are solvent-sensitive organic thin films, avoiding solvent during the process is a major benefit. This method can produce RGB patterned electroluminescent structures with 1000 ppi (pixels-per-inch) resolution.\n\nThe overall process of contact printing:\n\n\nThe array of quantum dots is manufactured by self-assembly in a process known as spin casting: a solution of quantum dots in an organic material is poured onto a substrate, which is then set spinning to spread the solution evenly.\n\nContact printing allows fabrication of multi-color QD-LEDs. A QD-LED was fabricated with an emissive layer consisting of 25-µm wide stripes of red, green and blue QD monolayers. Contact printing methods also minimize the amount of QD required, reducing costs.\n\nNanocrystal displays would render as much as a 30% increase in the visible spectrum, while using 30 to 50% less power than LCDs, in large part because nanocrystal displays wouldn't need backlighting. QD LEDs are 50-100 times brighter than CRT and LC displays, emitting 40,000 cd/m. QDs are dispersable in both aqueous and non-aqueous solvents, which provides for printable and flexible displays of all sizes, including large area TVs. QDs can be inorganic, offering the potential for improved lifetimes compared to OLED (however, since many parts of QD-LED are often made of organic materials, further development is required to improve the functional lifetime.) In addition to OLED displays, pick-and-place microLED displays are emerging as competing technologies to nanocrystal displays.\nOther advantages include better saturated green colors, manufacturability on polymers, thinner display and the use of the same material to generate different colors.\n\nOne disadvantage is that blue quantum dots require highly precise timing control during the reaction, because blue quantum dots are just slightly above the minimum size. Since sunlight contains roughly equal luminosities of red, green and blue across the entire spectrum, a display also needs to produce roughly equal luminosities of red, green and blue to achieve pure white as defined by CIE Standard Illuminant D65. However, the blue component in the display can have relatively lower color purity and/or precision (dynamic range) in comparison to green and red, because the human eye is three to five times less sensitive to blue in daylight conditions according to CIE luminosity function.\n\n"}
{"id": "2010605", "url": "https://en.wikipedia.org/wiki?curid=2010605", "title": "R-46 (missile)", "text": "R-46 (missile)\n\nThe R-46 was an intercontinental ballistic missile (ICBM) design by the Soviet Union during the Cold War.\n\nThe R-46 concept was to launch a very large hydrogen bomb into orbit as a fractional orbital bombardment system (FOBS). The existence of the system was alluded to in a speech by then Soviet Premier Nikita Khrushchev in 1961. The design was not developed, however, and the FOBS concept was abandoned as part of SALT II.\n\n"}
{"id": "9781478", "url": "https://en.wikipedia.org/wiki?curid=9781478", "title": "Rapid anti-personnel minefield breaching system", "text": "Rapid anti-personnel minefield breaching system\n\nThe rapid anti-personnel minefield breaching system or RAMBS II is a British rifle grenade used to breach obstacles and clear minefields. It is designed to replace the Bangalore torpedo, offering a more effective and flexible alternative. When deployed, the RAMBS II will clear a path 60 metres long and 0.6 metres wide, compared to the Bangalore's cleared area that is 15 metres long and 1 metre wide.\n"}
{"id": "1617571", "url": "https://en.wikipedia.org/wiki?curid=1617571", "title": "Safety engineer", "text": "Safety engineer\n\nThe scope of a safety engineer is to perform their professional functions. Safety engineering professionals must have education, training and experience in a common body of knowledge. They need to have a fundamental knowledge of physics, chemistry, biology, physiology, statistics, mathematics, computer science, engineering mechanics, industrial processes, business, communication and psychology. Professional safety studies include industrial hygiene and toxicology, design of engineering hazard controls, fire protection, ergonomics, system and process safety, system safety, safety and health program management, accident investigation and analysis, product safety, construction safety, education and training methods, measurement of safety performance, human behavior, environmental safety and health, and safety, health and environmental laws, regulations and standards. Many safety engineers have backgrounds or advanced study in other disciplines, such as management and business administration, engineering, system engineering / industrial engineering, requirements engineering, reliability engineering, maintenance, human factors, operations, education, physical and social sciences and other fields. Others have advanced study in safety. This extends their expertise beyond the basics of the safety engineering profession.\n\nThe major areas relating to the protection of people, property and the environment are:\n\nOddly enough, personality issues can be paramount in a safety engineer. They must be personally pleasant, intelligent, and ruthless with themselves and their organization. In particular, they have to be able to \"sell\" the failures that they discover, as well as the attendant expense and time needed to correct them. They can be the messengers of bad news.\n\nSafety engineers have to be ruthless about getting facts from other engineers. It is common for a safety engineer to consider software, chemical, electrical, mechanical, procedural, and training problems in the same day. Often the facts can be very uncomfortable as many safety related issues point towards mediocre management systems or worse, questionable business ethics.\n\nIt's difficult and expensive to retrofit safety into an unsafe system. To prevent safety problems, an organization should therefore treat safety as an early design and \"architecture\" activity, using the principles of Inherent safety, rather than as a \"paperwork requirement\" to be cleaned up after the real design is done.\n\nSafety engineers also must work in a team that includes other engineering specialties, quality assurance, quality improvement, regulatory compliance specialists, educators and lawyers.\n\nSubpar safety and quality problems often indicate underlying deficiencies in an organization's goals, recruitment, succession, training, management systems and business culture.\n\nSafety often works well in a true matrix-management organization, in which safety is a managed discipline integrated into a project plan.\n\n\n"}
{"id": "369526", "url": "https://en.wikipedia.org/wiki?curid=369526", "title": "Single-wire earth return", "text": "Single-wire earth return\n\nSingle-wire earth return (SWER) or single-wire ground return is a single-wire transmission line which supplies single-phase electric power from an electrical grid to remote areas at low cost. Its distinguishing feature is that the earth (or sometimes a body of water) is used as the return path for the current, to avoid the need for a second wire (or \"neutral wire\") to act as a return path.\n\nSingle-wire earth return is principally used for rural electrification, but also finds use for larger isolated loads such as water pumps. It is also used for high-voltage direct current over submarine power cables. Electric single-phase railway traction, such as light rail, uses a very similar system. It uses resistors to earth to reduce hazards from rail voltages, but the primary return currents are through the rails.\n\nLloyd Mandeno, OBE (1888–1973) fully developed SWER in New Zealand around 1925 for rural electrification. Although he termed it \"Earth Working Single Wire Line\", it was often called \"Mandeno’s Clothesline\". More than 200,000 kilometres have now been installed in Australia and New Zealand. It is considered safe, reliable and low cost, provided that safety features and earthing are correctly installed. The Australian standards are widely used and cited. It has been applied around the world, such as in the Canadian province of Saskatchewan; Brazil; Africa; and portions of the United States' Upper Midwest and Alaska (Bethel).\n\nSWER is a viable choice for a distribution system when conventional return current wiring would cost more than SWER’s isolation transformers and small power losses. Power engineers experienced with both SWER and conventional power lines rate SWER as equally safe, more reliable, less costly, but with slightly lower efficiency than conventional lines. SWER can cause fires when maintenance is poor, and bushfire is a risk.\n\nPower is supplied to the SWER line by an isolating transformer of up to 300 kVA. This transformer isolates the grid from ground or earth, and changes the grid voltage (typically 22 or 33 kV line-to-line) to the SWER voltage (typically 12.7 or 19.1 kV line-to-earth).\n\nThe SWER line is a single conductor that may stretch for tens or even hundreds of kilometres, with a number of distribution transformers along its length. At each transformer, such as a customer's premises, current flows from the line, through the primary coil of a step-down isolation transformer, to earth through an earth stake. From the earth stake, the current eventually finds its way back to the main step-up transformer at the head of the line, completing the circuit. SWER is therefore a practical example of a phantom loop.\n\nIn areas with high-resistance soil, the resistance of the soil wastes energy. Another issue is that the resistance may be high enough that insufficient current flows into the earth neutral, causing the grounding rod to float to higher voltages. Self-resetting circuit breakers usually reset because of a difference in voltage between line and neutral. Therefore, with dry, high-resistance soils, the reduced difference in voltage between line and neutral may prevent breakers from resetting. In Australia, locations with very dry soils need the grounding rods to be extra deep.\nExperience in Alaska shows that SWER needs to be grounded below permafrost, which is high-resistance.\n\nThe secondary winding of the local transformer will supply the customer with either single ended single phase (N-0) or split phase (N-0-N) power in the region’s standard appliance voltages, with the 0 volt line connected to a safety earth that does not normally carry an operating current.\n\nA large SWER line may feed as many as 80 distribution transformers. The transformers are usually rated at 5 kVA, 10 kVA and 25 kVA. The load densities are usually below 0.5 kVA per kilometer (0.8 kVA per mile) of line. Any single customer’s maximum demand will typically be less than 3.5 kVA, but larger loads up to the capacity of the distribution transformer can also be supplied. \n\nSome SWER systems in the USA are conventional distribution feeders that were built without a continuous neutral (some of which were obsoleted transmission lines that were refitted for rural distribution service). The substation feeding such lines has a grounding rod on each pole within the substation; then on each branch from the line, the span between the pole next to and the pole carrying the transformer would have a grounded conductor (giving each transformer two grounding points for safety reasons).\n\nProper mechanical design of a SWER line can lower its lifetime cost and increase its safety.\n\nSince the line is high voltage, with small currents, the conductor used in historic SWER lines was Number-8 galvanized steel fence wire. More modern installations use specially-designed AS1222.1 high-carbon steel, aluminum-clad wires. Aluminum clad wires corrode in coastal areas, but are otherwise more suitable. Because of the long spans and high mechanical tensions, vibration from wind can cause damage to the wires. Modern systems install spiral vibration dampers on the wires.\n\nInsulators are often porcelain because polymers are prone to ultraviolet damage. Some utilities install higher-voltage insulators so the line can be easily upgraded to carry more power. For example, 12 kV lines may be insulated to 22 kV, or 19 kV lines to 33 kV.\n\nReinforced concrete poles have been traditionally used in SWER lines because of their low cost, low maintenance, and resistance to water damage, termites and fungi. Local labor can produce them in most areas, further lowering costs. In New Zealand, metal poles are common (often being former rails from a railway line). Wooden poles are acceptable. In Mozambique, poles had to be at least high to permit safe passage of giraffes beneath the lines.\n\nIf an area is prone to lightning, modern designs place lightning ground straps in the poles when they are constructed, before erection. The straps and wiring can be arranged to be a low-cost lightning arrestor with rounded edges to avoid attracting a lightning strike.\n\nSWER is promoted as safe due to isolation of the ground from both the generator and user. Most other electrical systems use a metallic neutral connected directly to the generator or a shared ground.\n\nGrounding is critical. Significant currents on the order of 8 amperes flow through the ground near the earth points. A good-quality earth connection is needed to prevent risk of electric shock due to earth potential rise near this point. Separate grounds for power and safety are also used. Duplication of the ground points assures that the system is still safe if either of the grounds is damaged.\n\nA good earth connection is normally a 6 m stake of copper-clad steel driven vertically into the ground, and bonded to the transformer earth and tank. A good ground resistance is 5–10 ohms which can be measured using specialist earth test equipment. SWER systems are designed to limit the voltage in the earth to 20 volts per meter to avoid shocking people and animals that might be in the area.\n\nOther standard features include automatic reclosing circuit breakers (reclosers). Most faults (overcurrent) are transient. Since the network is rural, most of these faults will be cleared by the recloser. Each service site needs a rewirable drop out fuse for protection and switching of the transformer. The transformer secondary should also be protected by a standard high-rupture capacity (HRC) fuse or low voltage circuit breaker. A surge arrestor (spark gap) on the high voltage side is common, especially in lightning-prone areas.\n\nMost fire safety hazards in electrical distribution are from aging equipment: corroded lines, broken insulators, etc. The lower cost of SWER maintenance can reduce the cost of safe operation in these cases.\n\nSWER avoid lines clashing in wind, a substantial fire-safety feature, but a problem surfaced in the official investigation into the Black Saturday bushfires in Victoria, Australia. These demonstrated that a broken SWER conductor can short to ground across a resistance similar to the circuit's normal load; in that particular case, a tree. This can cause large currents without a ground-fault indication. This can present a danger in fire-prone areas where a conductor may snap and current may arc through trees or dry grass.\n\nBare-wire or ground-return telecommunications can be compromised by the ground-return current if the grounding area is closer than 100 m or sinks more than 10 A of current. Modern radio, optic fibre channels, and cell phone systems are unaffected.\n\nMany national electrical regulations (notably the U.S.) require a metallic return line from the load to the generator. In these jurisdictions, each SWER line must be approved by exception.\n\nSWER’s main advantage is its low cost. It is often used in sparsely populated areas where the cost of building an isolated distribution line cannot be justified. Capital costs are roughly 50% of an equivalent two-wire single-phase line. They can cost 30% of 3-wire three-phase systems. Maintenance costs are roughly 50% of an equivalent line.\n\nSWER also reduces the largest cost of a distribution network: the number of poles. Conventional 2-wire or 3-wire distribution lines have a higher power transfer capacity, but can require 7 poles per kilometre, with spans of 100 to 150 metres. SWER's high line voltage and low current also permits the use of low-cost galvanized steel wire (historically, No. 8 fence wire). Steel's greater strength permits spans of 400 metres or more, reducing the number of poles to 2.5 per kilometre.\n\nIf the poles also carry optical fiber cable for telecommunications (metal conductors may not be used), capital expenditures by the power company may be further reduced.\n\nSWER can be used in a grid or loop, but is usually arranged in a linear or radial layout to save costs. In the customary linear form, a single-point failure in a SWER line causes all customers further down the line to lose power. However, since it has fewer components in the field, SWER has less to fail. For example, since there is only one line, winds can’t cause lines to clash, removing a source of damage, as well as a source of rural bush fires.\n\nSince the bulk of the transmission line has low resistance attachments to earth, excessive ground currents from shorts and geomagnetic storms are more rare than in conventional metallic-return systems. So, SWER has fewer ground-fault circuit-breaker openings to interrupt service.\n\nA well-designed SWER line can be substantially upgraded as demand grows without new poles. The first step may be to replace the steel wire with more expensive copper-clad or aluminum-clad steel wire.\n\nIt may be possible to increase the voltage. Some distant SWER lines now operate at voltages as high as 35 kV. Normally this requires changing the insulators and transformers, but no new poles are needed.\n\nIf more capacity is needed, a second SWER line can be run on the same poles to provide two SWER lines 180 degrees out of phase. This requires more insulators and wire, but doubles the power without doubling the poles. Many standard SWER poles have several bolt holes to support this upgrade. This configuration causes most ground currents to cancel, reducing shock hazards and interference with communication lines.\n\nTwo-phase service is also possible with a two-wire upgrade: Though less reliable, it is more efficient. As more power is needed, the lines can be upgraded to match the load, from single wire SWER to two wire, single phase and finally to three wire, three phase. This ensures a more efficient use of capital and makes the initial installation more affordable.\n\nCustomer equipment installed before these upgrades will all be single phase, and can be reused after the upgrade. If small amounts of three-phase power are needed, it can be economically synthesized from two-phase power with on-site equipment.\n\nSWER lines tend to be long, with high impedance, so the voltage drop along the line is often a problem, causing poor regulation. Variations in demand cause variation in the delivered voltage. To combat this, some installations have automatic variable transformers at the customer site to keep the received voltage within legal specifications.\n\nAfter some years of experience, the inventor advocated a capacitor in series with the ground of the main isolation transformer to counteract the inductive reactance of the transformers, wire and earth return path. The plan was to improve the power factor, reduce losses and improve voltage performance due to reactive power flow. Though theoretically sound, this is not standard practice. It does also allow the use of a DC test loop, to distinguish a legitimate variable load from (for example) a fallen tree, which would be a DC path to ground.\n\nIn addition to New Zealand and Australia, single-wire earth return is used throughout the globe.\n\nIn 1981 a high-power 8.5 mile prototype SWER line was successfully installed from a diesel plant in Bethel to Napakiak in Alaska, United States. It operates at 80 kV, and was originally installed on special lightweight fiberglass poles that formed an A-frame. Since then, the A frames have been removed and standard wooden power poles were installed. The A-framed poles could be carried on lightweight snow machines, and could be installed with hand tools on permafrost without extensive digging. Erection of \"anchoring\" poles still required heavy machinery, but the cost savings were dramatic.\n\nResearchers at the University of Alaska Fairbanks, United States estimate that a network of such lines, combined with coastal wind turbines, could substantially reduce rural Alaska’s dependence on increasingly expensive diesel fuel for power generation. Alaska’s state economic energy screening survey advocated further study of this option to use more of the state’s underutilized power sources.\n\nAt present, certain developing nations have adopted SWER systems as their mains electricity systems, notably Laos, South Africa and Mozambique. SWER is also used extensively in Brazil.\n\nMany high-voltage direct current systems (HVDC) using submarine power cables are single wire earth return systems. Bipolar systems with both positive and negative cables may also retain a seawater grounding electrode, used when one pole has failed. To avoid electrochemical corrosion, the ground electrodes of such systems are situated apart from the converter stations and not near the transmission cable.\n\nThe electrodes can be situated in the sea or on land. Bare copper wires can be used for cathodes, and graphite rods buried in the ground, or titanium grids in the sea are used for anodes. To avoid electrochemical corrosion (and passivation of titanium surfaces) the current density at the surface of the electrodes must be small, and therefore large electrodes are required.\n\nExamples of HVDC systems with single wire earth return include the Baltic Cable and Kontek.\n\n"}
{"id": "20475183", "url": "https://en.wikipedia.org/wiki?curid=20475183", "title": "Solar architecture", "text": "Solar architecture\n\nSolar architecture is an architectural approach that takes in account the Sun to harness clean and renewable solar power. It is related to the fields of optics, thermics, electronics and materials science. Both active and passive solar housing skills are involved in solar architecture.\n\nThe use of flexible thin-film photovoltaic modules provides fluid integration with steel roofing profiles, enhancing the building's design. Orienting a building to the Sun, selecting materials with favorable thermal mass or light dispersing properties, and designing spaces that naturally circulate air also constitute solar architecture.\n\nInitial development of solar architecture has been limited by the rigidity and weight of standard solar power panels. The continued development of photovoltaic (PV) thin film solar has provided a lightweight yet robust vehicle to harness solar energy to reduce a building's impact on the environment. \n\nThe idea of passive solar building design first appeared in Greece around the fifth century BC. Up until that time, the Greeks' main source of fuel had been charcoal, but due to a major shortage of wood to burn they were forced to find a new way of heating their dwellings. With necessity as their motivation, the Greeks revolutionized the design of their cities. They began using building materials that absorbed solar energy, mostly stone, and started orienting the buildings so that they faced south. These revolutions, coupled with overhangs that kept out the hot summer sun, created structures which required very little heating and cooling. Socrates wrote, \"In houses that look toward the south, the sun penetrates the portico in winter, while in summer the path of the sun is right over our heads and above the roof so that there is shade.\" \n\nFrom this point on, most civilizations have oriented their structures to provide shade in the summer and heating in the winter. The Romans improved on the Greeks' design by covering the southern-facing windows with different types of transparent materials. \n\nAnother simpler example of early solar architecture is the cave dwellings in the southwestern regions of North America. Much like the Greek and Roman buildings, the cliffs in which the indigenous people of this region built their homes were oriented towards the south with an overhang to shade them from the midday sun during the summer months and capture as much of the solar energy during the winter as possible. \n\nActive solar architecture involves the moving of heat and/or coolness between a temporary heat storage medium and a building, typically in response to a thermostat's call for heat or coolness within the building. While this principle sounds useful in theory, significant engineering problems have thwarted almost all active solar architecture in practice. The most common form of active solar architecture, rock bed storage with air as a heat transfer medium, usually grew toxic mold in the rock bed which was blown into houses, along with dust and radon in some cases. \n\nA more complex and modern incarnation of solar architecture was introduced in 1954 with the invention of the photovoltaic cell by Bell Labs. Early cells were extremely inefficient and therefore not widely used, but throughout the years government and private research has improved the efficiency to a point where it is now a viable source of energy. \n\nUniversities were some of the first buildings to embrace the idea of solar energy. In 1973, the University of Delaware built Solar One, which was one of the world's first solar-powered houses. \n\nAs photovoltaic technologies keep advancing, solar architecture becomes easier to accomplish. In 1998 Subhendu Guha developed photovoltaic shingles, and recently a company called Oxford Photovoltaics has developed perovskite solar cells that are thin enough to incorporate into windows. Although the windows are not scaled to a size that can be taken advantage of on a commercial level yet, the company believes that the outlook is promising. In the company’s mission statement they state, \"Moreover, through the deployment of solar cells in areas where solar has traditionally struggled, for example the glass façades of high-rise commercial or residential buildings. In both cases, allowing solar energy to contribute a much higher proportion of electricity than is possible today, and helping to position PV as a significant factor in the global energy market.\"\n\nA greenhouse keeps heat from the Sun. In a double glazed greenhouse, three effects occur: no convection (air blocking), ray keeping (the ground absorbs a photon, emits it with lower infrared energy, and the glass reflects this infrared to the ground), and little conduction (double glazing). It seems that the convection effect is the most important, as greenhouses in poor countries are made of plastic.\n\nThe greenhouse can be used to grow plants in the winter, to grow tropical plants, as a terrarium for reptiles or insects, or simply for air comfort. It must be ventilated, but not too much, otherwise the convection will make the inside colder, losing the desired effect. The greenhouse may be combined with heat storage or an opaque mask.\n\nPhotothermic modules convert solar light into heat. They easily heat domestic water to 80 °C (353 K). They are put facing the sunny cardinal point, rather pointing towards the horizon to avoid overheating in summer, and take more calories in the winter. In a 45° North place, the module should face the south and the angle to the horizontal should be about 70°.\n\nThe use of intermediate solar heat systems like evacuated tubes, compound parabolic, and parabolic trough, is discussed as they correspond to specific, intermediate needs. A customer who wants a cheap system will prefer the photothermic, giving 80 °C (353 K) hot water with 70-85 % efficiency. A customer who wants high temperatures will prefer the solar parabola, giving 200 °C (573 K) with 70-85 % efficiency.\n\nDo it yourself photothermic modules are cheaper and can use a spiral pipe, with hot water coming from the center of the module. Other geometries exist, like serpentine or quadrangular.\n\nIf on a flat roof, a mirror can be placed in front of the photothermic module to give it more sunlight.\n\nThe photothermic module has become popular in Mediterranean countries, with Greece and Spain counting with 30-40 % of homes equipped with this system, and becoming part of the landscape.\n\nPhotovoltaic modules convert solar light into electricity. Classical silicon solar modules have up to 25 % efficiency but they are rigid and cannot easily be placed on curves. Thin film solar modules are flexible, but they have lower efficiency and lifetime.\n\nPhotovoltaic tiles combine the useful to the pleasant by providing tile-like photovoltaic surfaces.\n\nA pragmatic rule is to put the photovoltaic surface facing the sunny cardinal point, with a latitude-equal angle to the horizontal. For example, if the house is 33° South, the photovoltaic surface should face the north with 33° to the horizontal. From this rule comes a general standard of roof angle, that is the norm in solar architecture.\n\nThe simplest solar heat water system is to place a hot water storage tank towards the Sun and paint it black.\n\nA thick ground of rock in a greenhouse will keep some heat through the night. The rock will absorb heat in the day and emit it in the night. Water has the best thermal capacity for a common material and remains a sure value.\n\nIn autonomous (off-grid) photovoltaic systems, batteries are used to store the excess of electricity, and deliver it when needed in the night.\n\nGrid-connected systems can use interseasonal storage thanks to pumped-storage hydroelectricity. An innovative storage method, compressed air energy storage, is also being studied, and may be applied at the scale of a region or a home, whether a cave or a tank is used to store the compressed air.\n\nIn the Greek islands, the houses are painted in white to keep from absorbing heat. The white walls covered with lime and the blue roofs make the Greek islands' traditional style appreciated by tourists for its colors, and by the inhabitants for the cooler interior air.\n\nIn Nordic countries, this is the opposite: the houses are painted in black to better absorb the irradiation heat. Basalt is an interesting material as it is naturally black and exhibits high thermal storage capacity.\n\nPart or all of the house can track the Sun's race in the sky to catch its light. The Heliotrope, the first positive energy house in the world, rotates to catch the sunlight, converted into electricity by photovoltaic modules, heating the house through the translucent glass.\n\nTracking requires electronics and automatics. There are two ways to let the system know where the Sun is: instrumental and theoretical. The instrumental method uses captors of light to detect the Sun's position. The theoretical method uses astronomical formulas to know the Sun's place. One or two axis motors will make the solar system rotate to face the Sun and catch more of its Sunlight.\n\nA photovoltaic or photothermic module can gain more than 50% of production, thanks to a tracker system.\n\nSometimes the heat becomes too high, so a shadow may be desired. The Heliodome has been built in such a way that the roof hides the Sun in the summer to avoid overheating, and lets the sunlight pass in the winter.\n\nAs a mask, any opaque material is fine. A curtain, a cliff, or a wall can be solar masks. If a leafy tree is put in front of a greenhouse, it may hide the greenhouse in the summer, and let the sunlight enter in the winter, when the leaves have fallen. The shadows will not work the same according to the season. Using the seasonal change to get shadow in the summer, light in the winter, is a general rule for a solar mask.\n\nA solar chimney is a chimney of outside black color. They were used in Roman antiquity as a ventilation system. The black surface makes the chimney heat with sunlight. The air inside gets warmer and moves up, pumping the air from the underground, that is at 15 °C (288 K) all the year. This traditional air-ground exchanger was used to make the houses cool in the summer, mild in the winter.\n\nThe solar chimney may be coupled with a badgir or a wood chimney for stronger effect.\n\nA solar parabola is a parabolic mirror that concentrates the sunlight to reach high temperatures. In Auroville's collective kitchen, a large solar parabola on the roof provides heat for cooking.\n\nThe solar parabola can also be used for industrial building. The Odeillo solar furnace, one of the largest solar parabola in the world, concentrates the sunlight 10,000 times and reaches temperatures above 3,200 K. No material resists, even diamond melts. It opens the vision of a futuristic metallurgy, using a clean and renewable source of energy.\n\nOne of the first large commercial buildings to exemplify solar architecture is 4 Times Square (also known as the Condé Nast Building) in New York City. It has built-in solar panels on the 37th through the 43rd floors, and incorporated more energy-efficient technology than any other skyscraper at the time of its construction. The National Stadium in Kaohsiung, Taiwan, designed by the Japanese architect Toyo Ito, is a dragon-shaped structure that has 8,844 solar panels on its roof. It was built in 2009 to house the 2009 world games. Constructed completely of recycled materials, it is the largest solar-powered stadium in the world and powers the surrounding neighborhood when it is not in use. The Sundial Building in China was built to symbolize the need for replacing fossil fuels with renewable energy sources. The building is shaped like a fan and is covered in of solar panels. It was named the world's largest solar-powered office building in 2009.\n\nAlthough it is not yet completed, the Solar City Tower in Rio de Janeiro is another example of what solar architecture might look like in the future. It is a power plant that generates energy for the city during the day while also pumping water to the top of the structure. At night, when the sun is not shining, the water will be released to run over turbines that will continue to generate electricity. It was set to be revealed at the 2016 Olympic Games in Rio, although the project is still in the proposal phase.\n\nUsing solar power in architecture contributes to a world of clean and renewable energy. This is an investment: the initial price is high, but afterwards, there is nearly nothing to pay. On the contrary, fossil and fissile energies are cheap in the beginning, but cost tremendous amounts to humans and nature. The Fukushima catastrophe is evaluated to cost 210 billion dollars to Japan. Global warming has already been a cause of species extinction.\n\nSolar architecture is then anti-crisis. If all houses were to be rebuilt to meet solar architecture standards, this would bring hope, jobs, money, and economical growth.\n\nAccording to an article on ECN's website titled \"Architects just want to develop attractive buildings\", an architect's main purpose is to \"create a spatial object with lines, shapes, colours and texture. These are the challenges for the architect within the customer's programme of requirements. But they do not immediately think of using a solar panel as an interesting building material. There is still much to be achieved here.\" In the article it is stated multiple times that solar panels are not an architect's first choice for building material because of their cost and aesthetics.\nAnother criticism of installing solar panels is their upfront cost. According to energyinfomative.org, the average cost for a residential solar system is between $15,000 and $40,000 (USD), and about $7 per watt. In the article, it says that at today's rates, it would take 10 years to pay off an average system. As a solar panel may last more than 20 years, in the end, it becomes a benefit.\n\n"}
{"id": "41555646", "url": "https://en.wikipedia.org/wiki?curid=41555646", "title": "State-transition equation", "text": "State-transition equation\n\nThe state-transition equation is defined as the solution of the linear homogeneous state equation. The linear time-invariant state equation given by\nwith state vector \"x\", control vector \"u\", vector \"w\" of additive disturbances, and fixed matrices \"A\", \"B\", and \"E\", can be solved by using either the classical method of solving linear differential equations or the Laplace transform method. The Laplace transform solution is presented in the following equations.\nThe Laplace transform of the above equation yields \nwhere x(0) denotes initial-state vector evaluated at formula_3 . Solving for formula_4 gives\nSo, the state-transition equation can be obtained by taking inverse Laplace transform as \n\nThe state-transition equation as derived above is useful only when the initial time is defined to be at formula_3 . In the study of control systems, specially discrete-data control systems, it is often desirable to break up a state-transition process into a sequence of transitions, so a more flexible initial time must be chosen. Let the initial time be represented by formula_8 and the corresponding initial state by formula_9, and assume that the input formula_10 and the disturbance formula_11 are applied at t≥0. \nStarting with the above equation by setting formula_12 and solving for formula_13, we get \nOnce the state-transition equation is determined, the output vector can be expressed as a function of the initial state.\n\n\n"}
{"id": "4325991", "url": "https://en.wikipedia.org/wiki?curid=4325991", "title": "Test engineer", "text": "Test engineer\n\nA test engineer is a professional who determines how to create a process that would best test a particular product in manufacturing and related disciplines in order to assure that the product meets applicable specifications. Test engineers are also responsible for determining the best way a test can be performed in order to achieve adequate test coverage. Often test engineers also serve as a liaison between manufacturing, design engineering, sales engineering and marketing communities as well. \n\nTest engineers can have different expertise which depends on what test process they are more familiar with (although many test engineers have full familiarity from the PCB level processes like ICT, JTAG, and AXI) to PCBA and system level processes like board functional test (BFT or FT), burn-in test, system level test (ST). Some of the processes used in manufacturing where a test engineer is needed are:\n\nIdeally, a test engineer's involvement with a product begins with the very early stages of the design phase. Depending on the culture of the firm, the early stages could refer to Product Requirements Document (PRD) and Marketing Requirements Document (MRD)—some of the earliest work done during a new product introduction (NPI). \n\nBy working with or as part of the NPI group, a test engineer ensures that a product is designed for both testability and manufacturability. In other words, to make sure that the product can be readily tested and built.\n\nThe following are some general rules to ensure testability and manufacturability of a product:\n\n\nBy following the general rules above, test engineers minimize future surprises (like adding extra components, re-layout of the boards, etc.) which drives up costs and development delays of the final product.\n\nOften people take shortcuts to be able to deliver final products. Because of these shortcuts, the product's manufacturability and testability becomes complicated (inability to read and write information, creating deviation from the process, etc.) which impacts the manufacturing complexity of a product. Because of this complexity, bottlenecks in the manufacturing and delivery schedule delays are introduced.\n\nWith this in mind, test engineers always get involved in the following reviews as well:\n\nProducts' yield plays a very important part during their lifespan. There are usually three stages for a product, engineering, initial production (IP) and full production (FP). \nIn addition, yields will show if another process needs to be introduced (e.g., because processes already used cannot capture certain test errors). Yields can also decide if an existing test process can be trimmed down (step-wise or time-wise) or even fully eliminated. E.g., if the ESS errors can be captured during the 3rd hour, test time can be cut down from a normal 24 hours down to maybe 4. Or if a process consistently yields 100% during a 15-month period, teams can get together and decide to eliminate that process at all.\n\nTest automation refers to the automation of the process to test a product through the use of machines. Depending on the product, the machines that we are referring to could mean a combination of Automatic Test Equipment (ATE), handler, interface board, and test program that drives the ATE, as with the case of the IC chip testing.\n\nTest automation is a big part of a test engineer's job. \n\nThe whole intention of automating the test is as follows:\n\nOverall, this drives manufacturing reliability and quality at the end of the line making sure that all units shipped out to customers are well tested, stressed, filtered out of any errors, and configured properly.\n\nFollowing are some of the documents that the test engineers maintain or define:\n\n\nA contract manufacturer (CM) also provides a test engineer for their customers. The function of these test engineers varies depending on the level of support they provide for their customers: providing \"interactive and first level of defense\"-only support or providing partial or ground-up solutions.\n\nProviding \"interactive and first level-of-defense\"-only support is the usual job of the CM TE. Here are some typical job functions for a CM test engineer:\n\nBecause of their close involvement with the test line, they monitor the products going through the line and inspect the failed boards to decide if it really failed or if the failure was just caused by some improper test setup. Some examples of these false failures are:\n\nThere is a small number of companies who prefer to outsource their test engineering work to their corresponding CM. In that case, the CM TEs will be in charge of providing the test automation solution, test fixture design, yield gathering plus the usual interactive and first level of defense for their customers.\n\nOf course, outsourcing test solutions to the CM has its pros and cons. \n\nSome of the advantages are:\n\nSome of the disadvantages are:\n\nBecause it is hard to find a test engineer who knows every aspect of testing methodology (from PCB tests like ICT, JTAG test, flying probe test, and X-Ray test to PCBA test which includes writing test automation from functional test to FQA test among others), companies usually outsource part of the development of this missing test piece to their CM. For example, if none of the in-house TEs know much about ICT fixtures, they will ask their CM to develop the ICT test solutions for them instead.\n\n\n"}
{"id": "57039712", "url": "https://en.wikipedia.org/wiki?curid=57039712", "title": "ToneTag", "text": "ToneTag\n\nToneTag is a Fintech and Retail Solutions company, which uses encrypted sound waves to make offline, proximity-based contactless payments on any device. The company uses proprietary Software Development Kit (SDK) that encodes data into sounds. These sounds are transmitted over air and can enable payments over the existing payments infrastructure.\n\nThe idea for ToneTag germinated when the founder and current CEO of the company, Kumar Abhishek was asked the following question by a Singaporean friend, “Why is it in India we get candies instead of balance cash?” Kumar saw an opportunity, and along with his friend and co-founder Vivek Singh started a research project on “How to enable small payments through the mobile”. The research was followed by experimentation with various alternate payment technologies to find a solution to this common problem. In September 2017, Infosys partnered with ToneTag to enable contactless payments solutions.\n\n\n\n\n\n"}
{"id": "3764368", "url": "https://en.wikipedia.org/wiki?curid=3764368", "title": "Torque limiter", "text": "Torque limiter\n\nA torque limiter is an automatic device that protects mechanical equipment, or its work, from damage by mechanical overload. A torque limiter may limit the torque by slipping (as in a friction plate slip-clutch), or uncouple the load entirely (as in a shear pin). The action of a torque limiter is especially useful to limit any damage due to crash stops and jams.\n\nTorque limiters may be packaged as a shaft coupling or as a hub for sprocket or sheave. A torque limiting device is also known as an overload clutch.\n\nDisconnect types will uncouple the drive, with little or no residual torque making its way to the load. They may reset automatically or manually\n\nA shear pin type sacrifices a mechanical component, the pin, to disconnect the shafts.\n\nA synchronous magnetic torque limiter uses permanent magnets mounted to each shaft, with an air gap between. They are very fast acting, but may have more backlash than mechanical types. Because there is no mechanical contact between the two shafts, they are also used to transmit torque through a physical barrier like a thin plastic wall. On some models, the torque limit may be adjusted by changing the gap between the magnets.\n\nA ball detent type limiter transmits force through hardened balls which rest in detents on the shaft and are held in place with springs. An over-torque condition pushes the balls out of their detents, thereby decoupling the shaft. It can have single or multiple detent positions, or a snap acting spring which requires a manual reset. There may be a compression adjustment to adjust the torque limit.\n\nMany cordless drills incorporate this type of torque limiter in a planetary gearset. It may be a part of an assembly of multiple gearsets used to primarily reduce speed and multiply torque as well as perform ratio changes. The torque limiter is typically the last gearset in the transmission. It uses the planet carrier as the input with the sun gear as the output, and the annulus normally locked. A series of ball detents act on the annulus to lock it, allowing power to be transmitted from the planet carrier to the sun gear. When the torque transmitted through the gearset reaches a determinate amount, the torque acting on the annulus causes it to unlock from its ball detents and freely rotate, causing power to be diverted from the load on the sun gear to the annulus and thereby stalling the output until torque is reduced to an amount where the ball detents can lock the annulus again. This system equally limits torque in both directions of rotation and also works with the sun gear as the input. The compression of the ball detents (and therefore the amount of torque at which the limiter is utilized) is typically adjusted by means of a rotating collar accessible to the user which is indexed and held in place with its own separate ball detents.\n\nThis mechanical type uses a spring to hold a drive pawl against a notch in the rotor. It may feature automatic or manual reset. A compression adjustment on the spring determines the torque limit.\n\nTorque limiting types will limit the torque by slipping (i.e. letting the drive shaft run faster than the driven shaft.) Excess power is dissipated as heat. They don't need to be reset.\n\nThis type is similar to a friction plate clutch. Over-torque will cause the plates to slip. A simple example is found in a fixed-spool fishing reel, where the slipping torque is set by means of a large hand nut in order that the reel will turn and allow more line to unwind before the line breaks under the pull of a fish.\n\nA magnetic particle clutch can be used effectively as a torque limiter. The torque setting fairly approximates a linear relationship with the current passing through the windings, which can be statically or dynamically set depending on needs.\n\nThis type is non-synchronous in normal operation, so there is always some slippage.\n\n"}
{"id": "722199", "url": "https://en.wikipedia.org/wiki?curid=722199", "title": "USNS Kingsport (T-AG-164)", "text": "USNS Kingsport (T-AG-164)\n\nUSNS \"Kingsport\" (T-AG-164) was built as SS \"Kingsport Victory, a United States Maritime Commission VC2-S-AP3 (Victory) type cargo ship. During the closing days of World War II the ship was operated by the American Hawaiian Steamship Company under an agreement with the War Shipping Administration. After a period of layup the ship was operated as USAT \"Kingsport Victory by the Army under bareboat charter effective 8 July 1948. When Army transports were transferred to the Navy's Military Sea Transportation Service the ship continued as USNS \"Kingsport Victory\" (T-AK-239), a cargo transport . On 14 November 1961, after conversion into the first satellite communication ship, the ship was renamed \"Kingsport\", reclassified as a general auxiliary, and operated as USNS \"Kingsport\" (T-AG-164).\n\n\"Kingsport Victory\", a United States Maritime Commission VC2-S-AP3 (Victory) type cargo ship, was laid down 4 April 1944 with launch on 29 May and completion on 12 July 1944 with delivery to the War Shipping Administration (WSA) on the same day at Los Angeles. She was built under the Emergency Shipbuilding program under cognizance of the U.S. Maritime Commission. Basic dimensions, not counting modifications for satellite communications, were length (LBP), beam and .\n\n\"Kingsport Victory\" was immediately placed under operation by the American Hawaiian Steamship Company under War Shipping Administration general agency agreement. \"Kingsport Victory\" serviced a cargo ship to supply troop in the Pacific War. \"Kingsport Victory\" made trips between the California to Pearl Harbor. She streamed from California on 17 October 1944 arriving at Milne Bay, New Guinea on 2 November. She moved cargo for the war to US troops and ships at Eniwetok, Iwo Jima, Guam, Ulithi atoll and Okinawa. After the war on December 1945 streamed from Okinawa to Hong Kong, then Calcutta. Through the Suez Canal she arrived at New York City on 27 February 1946. During 1946 she moved cargo from the US East and West Coasts. \"Kingsport Victory\" was active in delivering support for the Battle of Iwo Jima from 19 February to 26 March 1945. In each battle she had to use her deck guns to defend against air attacks. \"Kingsport Victory\" received one Battle stars for her World War II service.\n\nAfter the war, on 29 September 1947, she was taken out of service and placed in the reserve fleet at Lee Hall, Virginia. On 8 April 1948 the ship was taken out of reserve and bareboat chartered to the War Department for operation as the USAT \"\"Kingsport Victory\" a US Army Transport. During this time the ship was involved in a legal case, \"Johansen, v. United States\", involving rights of an Army civil service employee crew member in personal injury cases. On 1 March 1950 she was removed from US Army Transport and transferred the US Navy.\n\n\"Kingsport Victory\" was among large Army ships transferred to the Navy's Military Sea Transportation Service (MSTS) with \"Kingsport Victory\" being transferred effective 1 March 1950. The ship carried military cargo for the next eleven years as USNS \"Kingsport Victory\" (T-AK-239). \"Kingsport Victory\" is seen in an Air Force documentary film on the construction of the Dew Line loading supplies at Norfolk, Virginia and unloading at Halifax, Nova Scotia.\n\nAssigned to duty supporting the U.S. Army Satellite Communications Agency USNS \"Kingsport\" was further modified and, in August 1963 while in Lagos harbor, transmitted the first satellite voice call between heads of state when John F. Kennedy and Nigerian Prime Minister Abubakar Balewa aboard \"Kingsport\" spoke in a two-way call. A demonstration of transmission of oceanographic data was made between a research vessel off Africa via the ship and satellite to Washington. The first air to ship satellite communication took place when Navy aircraft off Virginia established voice communication with \"Kingsport\" which was off Morocco. Further satellite communications work took place in the Pacific and Indian Oceans. \"Kingsport\" then supported Project Gemini into March 1966. After conversion from satellite configuration, particularly removal of the large and very visible dome, \"Kingsport\" was engaged in acoustic work for the Navy supporting undersea surveillance programs.\n\nOn 24 September 1961, she was delivered to the Portland, Oregon facilities of Willamette Iron & Steel Company where she underwent conversion to become the first satellite communications ship. On 14 November 1961 she was renamed \"Kingsport\" and reclassified AG-164.\n\nDesigned for use by the United States Army Satellite Communications Agency in the defense satellite communications programs, Project ADVENT, USNS \"Kingsport\" underwent extensive alteration during conversion. A special high frequency radio station was installed for ship-to-shore communications. She received advanced tracking and telemetry equipment and anti-roll stabilization tanks. In addition, a 30-foot, gyro-stabilized, computer-oriented, triaxial, parabolic antenna was installed on her afterdeck. Housed in a 53-foot, plastic, air-pressurized radome, this antenna permitted precision tracking of a high altitude satellite at any angle above the horizon.\n\n\"Kingsport\" sailed to Lagos, Nigeria after Syncom 2 had been successfully launched on 26 July 1963 to serve as the terminal control station during testing and evaluation of the satellite. On 23 August 1963, President John F. Kennedy in Washington, D.C., telephoned Nigerian Prime Minister Abubakar Balewa aboard the \"Kingsport\" docked in Lagos Harbor via Syncom 2, the first geosynchronous communication satellite. It was the first live two-way call between heads of state by satellite. Syncom 2 and Relay 1 linked Nigeria, Brazil and the United States with \"Kingsport\" transmitting through Syncom 2 to New Jersey and New Jersey via Relay 1 to Rio de Janeiro. During this period Gulf of Guinea oceanographic data, composed of depths temperature and salinity from a station, were transmitted from the to the National Oceanographic Data Center via \"Kingsport\" and Syncom 2.\n\n\"Kingsport\" departed Lagos 23 September and during transit off Morocco on 2 October demonstrated the first satellite communications between an aircraft in flight when a Navy aircraft off the Virginia coast made voice contact with the ship via satellite. The ship reached Rota, Spain on 3 October, staying until 6 October, then sailed supporting communication tests in the Mediterranean from 7 to 25 October. Tests of voice and teletype links between the United States and ships of the 6th Fleet successful with the ship visiting Leghorn, Italy and Beirut, Lebanon during the voyage. After arriving in Rota 26 October and completing additional experiments she sailed for Norfolk 9 November and arrived 21 November.\n\n\"Kingsport\" departed for the Pacific 17 February 1964 via Puerto Rico and the Panama Canal stopping at San Diego 13 March and reaching Pearl Harbor on 25 March 1964. For the next ten months the ship operated between Pearl Harbor and Guam supporting further communication experiments including those related to the evaluation of SYNCOM 3 after its launching 19 August 1964. Further experiments extended throughout the Western Pacific and into the Indian Ocean until July 1965.\n\nShe then provided support for NASA's Gemini manned space shots serving as on station communications ship between Okinawa and the Philippines for Gemini 5 from 21 to 29 August. She supported three more Gemini flights between 4 December and 16 March 1966 before returning to the West Coast in April. She remained at San Francisco from 18 April to 27 October in a ready reserve status. During November she steamed to the East Coast, and in early 1967 was at New York undergoing repairs and alterations.\n\nAfter completion of her communications support role the USNS \"Kingsport\" became a bathymetric and acoustic survey ship supporting undersea surveillance. Among the now published reports, declassified in 2006, of the ship's work is a description of the Indian Ocean exercise code named BEARING STAKE that took place from January to April 1977.\n\nKingsport was placed out of service on 31 Jan 1984, transferred to the Maritime Administration for layup on 29 August 1984 then transferred back to the Military Sealift Command for scientific research on 1 March 1990. The ship was withdrawn from the reserve fleet on 21 January 1992 for scrapping in India.\n\nCrew of Naval Armed Guard on the SS \"Kingsport Victory\"' earned \"Battle Stars\" in World War II for war action during the assault occupation of Assault occupation of Iwo Jima from 13 March 1945 to 16 March 1945.\n\n\n"}
{"id": "6370681", "url": "https://en.wikipedia.org/wiki?curid=6370681", "title": "VBS2", "text": "VBS2\n\nVBS2 (Virtual Battlespace 2) is the successor of the battlefield simulation system VBS1. It was developed in close cooperation with the USMC, Australian Defence Force and other military customers of VBS1. VBS2 was officially launched on 17 April 2007.\n\nVBS2 offers realistic battlefield simulations and the ability to operate land, sea, and air vehicles. Instructors may create new scenarios and then engage the simulation from multiple viewpoints. The squad-management system enables participants to issue orders to squad members.\n\nVBS2 was designed for federal, state, and local government agencies and can be specifically tailored to meet the individual needs of military, law enforcement, homeland defense, loadmaster, and first responder training environments.\n\nVBS2 may be used to teach doctrine, tactics, techniques, and procedures during squad and platoon offensive, defensive, and patrolling operations. VBS2 delivers a synthetic environment for the practical exercise of the leadership and organizational behavior skills required to successfully execute unit missions.\n\nVBS2 is suitable for training small teams in urban tactics, entire combat teams in combined arms operations or even non-military usage such as emergency response procedures in lethal and non-lethal environments or terrain visualization.\n\nVBS2 is based on the VBS2Real Virtuality 2, also used in the video game Arma 2.\n\nThe simulation engine driving VBS2 is Real Virtuality 2, developed by Bohemia Interactive. VBS2 allows a user to develop large terrain areas, over in size (at any terrain resolution) and populate the terrain area with millions of objects in accordance with VMAP shape data, and then texture-map the entire representation with high-resolution satellite imagery or aerial photography.\n\nOnce the terrain representation is exported into VBS2, the simulation engine will provide a simulation of the real world, incorporating moving trees and grass, ground clutter, ambient animal life, shadows, dynamic lighting, weather and time of day.\n\nA new streaming capability provides an efficient means of loading complex terrain areas as object and texture data is processed only when required. View distances are now typically five times greater than in VBS1 (depending on processor speed) – level of detail culling has been improved to allow attack helicopters and armoured vehicles to engage at realistic ranges, and forward observers to call artillery fire from greater distances.\n\nVBS2 supports large multiplayer network sessions allowing join-in-progress and improved administrator functionality. VBS2 provides improved simulation of complex urban areas, including destructible buildings, round penetration through walls and operable and destructible doors. Weapon platforms are capable of thermal imaging, simulation of fire control systems and turret override. Multiple vehicle turrets are possible. Weapon ballistics have been improved.\n\nThe After-Action-Review module allows detailed review of a completed training mission, with every player, AI, vehicle movement being recorded, as well as any bullet path and any destruction to objects or terrain.\n\nThe VBS2 terrain editing tool, Visitor 3, will support direct import of terrain and shape data to recreate any area of operation in the simulation. VBS2 also includes real-time command and control functionality for large numbers of AI or human participants.\n\nThe VBS1 HLA/DIS gateway is updated and improved for VBS2 to meet HLA and DIS compliance.\n\nIn 2001, Bohemia Interactive Australia (BIA) was formed to take Operation Flashpoint and develop a militarised version suitable for Tactical training. By late 2001, the US Marine Corps was provided an alpha version of VBS1 and from their feedback, the after actions review (AAR) tool was added. The Australian Army used VBS1 for experimentation in 2003 and training trials in 2004 with the software being accepted, in 2005, as an Infantry and combined arms operations training tool.\nIn 2006 Calytrix Technologies developed LVCGame as a HLA and DIS gateway for VBS to allow constructive simulation entities and VBS entities to interact in wider exercises. VBS2 was released in beta in early 2007 and trialed as part of the ADF's Combined Arms Tactical Training (CATT) events. BIA then used feedback from both the ADF and USMC to finalize VBS2 throughout 2007 with the Virtual Tool Kit addition being released in 2008.\nIn January 2009 the United States Army announced a new training program of record called Games for Training (GFT). VBS2 was selected as the first person simulation flagship for the GFT program. This announcement replaced a DARPA initiative known as DARWARS Ambush! Convoy Simulator which was subsequently retired as a training tool. Games for Training is currently managed by the TRADOC Capability Manager Gaming, located at Fort Leavenworth, Kansas.\n\n\n\"VBS2\" was developed by Bohemia Interactive Australia (now known as Bohemia Interactive Simulations, spinoff company of former Bohemia Interactive Studio (now known as Bohemia Interactive).\n\n\"VBS2\" since version 1.40 are under development by Bohemia Interactive Simulations (abbrev BISIM), which is separate company to/from Bohemia Interactive (abbrev. BI).\n\n\n"}
{"id": "4791594", "url": "https://en.wikipedia.org/wiki?curid=4791594", "title": "Vladimir Kotelnikov", "text": "Vladimir Kotelnikov\n\nVladimir Aleksandrovich Kotelnikov (Russian \"Владимир Александрович Котельников\", scientific transliteration \"Vladimir Alexandrovič Kotelnikov\", 6 September 1908 in Kazan – 11 February 2005 in Moscow) was an information theory and radar astronomy pioneer from the Soviet Union. He was elected a member of the Russian Academy of Science, in the Department of Technical Science (radio technology) in 1953. From 30 July 1973 to 25 March 1980 Kotelnikov served as Chairman of the RSFSR Supreme Council.\n\n\nHe is mostly known for having discovered, before e.g. Edmund Whittaker, Harry Nyquist, Claude Shannon, the sampling theorem in 1933. \nThis result of Fourier Analysis was known in harmonic analysis since the end of the 19th century and circulated in the 1920s and 1930s in the engineering community. He was the first to write down a precise statement of this theorem in relation to signal transmission. He also was a pioneer in the use of signal theory in modulation and communications.\n\nHe is also a creator of the theory of optimum noise immunity. \nHe obtained several scientific prizes for his work in radio astronomy and signal theory. In 1961, he oversaw one of the first efforts to probe the planet Venus with radar. In June 1962 he led the first probe of the planet Mercury with radar.\n\nKotelnikov was also involved in cryptography, proving the absolute security of the one-time pad; his results were delivered in 1941, the time of Nazi Germany's invasion of the Soviet Union, in a report that apparently remains classified. In this, as with the above-mentioned sampling theorem, he and Claude Shannon in the US reached the same conclusions independently of each other.\nFor his achievements Kotelnikov was awarded the IEEE 2000 Gold Medal of Alexander Graham Bell and the honorable IEEE Third Millennium Medal. Prof. Bruce Eisenstein, the President of the IEEE, described Kotelnikov as follows: \"The outstanding hero of the present. His merits are recognized all over the\nworld. In front of us is the giant of radio engineering thought, who has made the most significant contribution to media communication development\". \n\n"}
