{"id": "42441880", "url": "https://en.wikipedia.org/wiki?curid=42441880", "title": "Advisory circular", "text": "Advisory circular\n\nAdvisory circular (AC) refers to a type of publication offered by the Federal Aviation Administration (FAA) to provide guidance for compliance with [Airworthiness, pilot certification, operational standards, training standards, and any other rules within the 14 CFR Aeronautics and Space Title.]]. They define acceptable means, but not the only means, of accomplishing or showing compliance with airworthiness regulations. Generally informative in nature, Advisory Circulars are neither binding nor regulatory; yet some have the effect of \"de facto\" standards or regulations.\n\nAdvisory circulars typically refer to industry standards from SAE and RTCA. Some Advisory circulars are only a few pages long and do little more than reference a recommended standard; for example, AC 20-115B referencing DO-178B. Others, like AC 20-115C, are considerably longer; in this case giving guidance in how to transition from DO-178 revision B to C.\n\n"}
{"id": "1188303", "url": "https://en.wikipedia.org/wiki?curid=1188303", "title": "Bat detector", "text": "Bat detector\n\nA bat detector is a device used to detect the presence of bats by converting their echolocation ultrasound signals, as they are emitted by the bats, to audible frequencies, usually about 120 Hz to 15 kHz. There are other types of detectors which record bat calls so that they can be analysed afterward, but these are more commonly referred to by their particular function.\n\nBats emit calls from about 12 kHz to 160 kHz, but the upper frequencies in this range are rapidly absorbed in air. Many bat detectors are limited to around 15 kHz to 125 kHz at best. Bat detectors are available commercially and also can be self-built.\n\nBat detectors are used to detect the presence of bats and also help form conclusions about their species. Some bat calls are distinct and easy to recognise such as the Horseshoe bats; other calls are less distinct between similar species. While bats can vary their calls as they fly and hunt, the ear can be trained to recognise species according to the frequency ranges and repetition rates of the echolocation calls. Bats also emit social calls (non-echolocation calls) at ultrasound frequencies.\n\nA major limitation of acoustic bat detectors is their range, which is limited by the absorption of ultrasound in air. At mid range frequencies around 50 kHz, the maximum range is only about 25 to 30 metres in average atmospheric conditions when bats fly. This decreases with increasing frequency. Some bat calls have components around 20 kHz or even lower and sometimes these can be detected at 2 or 3 times the usual range. However, only the lower frequency components will be detected at a distance. The usable range of bat detectors decreases with humidity and in misty conditions the maximum range can be very low.\n\nIt is important to recognise three types of bat echolocation call: frequency modulation (FM), constant frequency (CF) (sometimes called amplitude modulation), and composite calls with both FM and CF components. The following illustrates a bat making an FM type call followed by a bat which uses a CF type call:\n\nThe FM call is heard as rapid dry clicks and the CF call as peeps. These vary in frequency due to the Doppler effect as the bat flies past. A heterodyne bat detector exaggerates the Doppler effect. As the bat making the CF calls flies toward the detector, the pitch falls.\n\nSeveral species of bat use a composite FM and CF call starting with a rapid falling FM call which slows to become a CF call at the end, giving a \"hockey stick\" shape to the graph. This makes the call sound different on a bat detector:\n\nThis gives a much wetter sound that the pure FM call. Pipistrelles generally use the hockey stick call for general echolocation, but use only the FM part at times. The end frequencies for the Common Pipistrelle and the Soprano Pipistrelle are around 45 kHz and 55 kHz respectively, but these frequencies can vary widely.\n\nThere are three types of \"real time\" audio bat detector in common use: the heterodyne, frequency division, and time expansion. Some bat detectors combine two or all three types.\n\nHeterodyne detectors are the most commonly used, and most self-build detectors are of this type. A heterodyne function is often also built into the other types of detector. A heterodyne bat detector simply shifts all the ultrasound frequencies downward by a fixed amount so we can hear them.\n\nA \"heterodyne\" is a beat frequency such as can be heard when two close musical notes are played together. A heterodyne bat detector combines the bat call with a constant internal frequency so that sum and difference frequencies are generated. For instance, a bat call at 45 kHz and an internal frequency of 43 kHz produces output frequencies of 2 kHz and 88 kHz. The 88 kHz frequency is inaudible and is filtered out and the 2 kHz frequency is fed to a loudspeaker or headphones. The internal frequency is displayed on a dial or on a display.\n\nA better quality version of a heterodyne, or direct conversion, bat detector is the super-heterodyne detector. In this case the bat signal is mixed with a high frequency oscillator, typically around 450–600 kHz. The difference frequency is then amplified and filtered in an 'intermediate frequency' or i.f. amplifier before being converted back to audible frequencies again. This design, which is based on standard radio design, gives improved frequency discrimination and avoids problems with interference from the local oscillator.\n\nIn more recent DSP-based detectors, the heterodyne conversion can be done entirely digitally.\n\nThe tuning issue can be addressed by using a scanning circuit to enable the detector to scan the spectrum automatically and stop scanning when a bat call is heard. One example of such a detector is the Bat Scanner.\n\nIt is also possible to use a 'comb spectrum' generator as the local oscillator so that the detector is effectively tuned to lots of frequencies, 10 kHz apart, all at once.\n\nSome early bat detectors used ex Navy, low frequency radio sets, simply replacing the aerial with a microphone and pre-amplifier. It is also possible to modify a portable Long Wave radio to be a bat detector by adjusting the tuning frequencies and replacing the ferrite rod aerial with a microphone and pre-amplifier.\n\nThe operator guesses the likely species to be present and tunes the frequency accordingly. Many users will start listening around 45 kHz. If a bat is seen or a bat-like call is heard, the frequency is tuned up and down until the clearest sound is heard.\n\nSpecies like Pipistrelles which end their call with a \"hockey stick\" CF component can be recognised according to the lowest frequency which gives the clearest \"plop\" sound. Horseshoe bats give a peeping sound at a frequency depending on their species. FM calls all tend to sound like clicks, but the start and end frequencies and the call repetition pattern can give clues as to the species.\n\nThe advantages of a heterodyne bat detector is that it works in real time, exaggerates the frequency changes of a bat call, is easy to use, and is the least expensive. It is easy to recognise a doppler shift in CF calls of flying bats due to their speed of flight. Stereo listening and recording is possible with models such as the CSE stereo heterodyne detector (broken link!), and this can help to track bats when visibility is poor.\n\nThe disadvantages of a heterodyne bat detector are that it can only convert a narrow band of frequencies, typically 5 kHz, and has to be continually retuned, and can easily miss species out of its current tuned range.\n\nFrequency division (FD) bat detectors synthesise a sound which is a fraction of the bat call frequencies, typically 1/10. This is done by converting the call into a square wave, otherwise called a zero crossing signal. This square wave is then divided using an electronic counter by 10 to provide another square wave. Square waves sound harsh and contain harmonics which can cause problems in analysis so these are filtered out where possible. Some recent all-digital detectors can synthesise a sine wave instead of a square wave. One example of a detector which synthesises a sine-wave FD output is the Griffin\n\nSome FD detectors output this constant level signal which renders background noise and bat calls at the same high level. This causes problems with both listening and analysis. More sophisticated FD detectors such as the Batbox Duet measure the incoming volume level, limiting the noise threshold, and use this to restore the output level variations. This and other sophisticated FD detectors also include a heterodyne detector and provide a jack output so that independent outputs can be recorded for later analysis.\n\nWith dual output FD detectors, headphones can be used to monitor both outputs simultaneously, or the loudspeaker used with the heterodyne function and the FD output recorded and analysed later. Alternatively, listening to the FD output gives an audible rendering of the bat call at 1/10 frequency. An example of a dual detector is the Ciel CDB301.\n\nDual FD/heterodyne detectors are useful for cross country transects especially when there is a function provided for recording voice notes such as times, locations and recognised bat calls. The output or outputs are recorded on cassette tape, Minidisc or solid state recorders, downloaded to a computer, and analysed using custom software. Calls missed by the heterodyne function, if present, can be seen and measured on the analysis.\n\nAdvantages, As with a heterodyne detector, an FD detector works in real time with or without a heterodyne function. Bat calls can be heard in their entirety over their whole range rather than over a limited frequency range. Retuning with an FD detector is not required although this is done with a dual type with heterodyne. By analysing the recording later, the entire call frequency range and the call pattern can be measured.\n\nA serious disadvantage with real time listening is that the speed of a bat call remains fast, often too fast for the species to be recognised. The frequency changes of CF calls are not exaggerated as with a heterodyne detector and so are less noticeable. Also with some species such as the Lesser Horseshoe bat with a call around 110 kHz, the resulting frequency is still quite high although it can be recorded. The synthesising of the call means that only one bat call can be reproduced at a time and a muddle is caused by simultaneous calls. Surprisingly, this is not a great disadvantage when analysing a recording later\n\nTime expansion (TE) detectors work by digitising the bat calls at a high sampling rate using an analog-to-digital converter and storing the digitised signal in an on-board memory.\n\nTE detectors are \"real time\" devices in that they can be monitored at the time of recording, but there is an inevitable delay while the high speed sampled extract is slowed down and replayed.\n\nIn real time mode, with or without an associated heterodyne or FD detector, the slowed down calls can be heard as a drawn-out bat call at audible frequencies. Therefore, fast FM calls can be heard as a descending note instead of a click. Thus it is possible to hear the difference between FM calls which just sound like clicks on the other types of detector.\n\nAfter downloading an audio recording to a computer, the original calls are analysed as if they were still at the original non-expanded rate.\n\nThe output can be recorded with an audio recorder as with FD detectors, or with more recent units, the signal can be recorded directly to an internal digital memory such as a compact flash card. The whole waveform is recorded with the full call range being preserved, rather than 1/10 of the waveform as in a FD detector. Since both frequency and amplitude information are preserved in the recorded call, more data is available for species analysis.\n\nEarly units were equipped with small memories which limited the length of time that could be digitised. Once the memory was filled (usually only a few seconds maximum), the unit would then replay the recording at slower rate, typically between 1/10 to 1/32 of the rate of the original recording. While the recorded sample is being played back slowly, nothing is being recorded, so the bat calls are being sampled intermittently. For instance, when a 1-second call is being played back at 1/32 rate, 32 seconds of bat calls are not being recorded.\n\nMore recent time-expansion recorders use large flash-based memories (such as removable compact-flash cards) and high-bandwidth direct-to-card recording to provide continuous, full-bandwidth real-time recording. Such units can record continuously for many hours while maintaining the maximum information within the signal.\n\nSome units are also equipped with an auto-record function and these can be left in-the-field for many days.\n\nSome units also include a pre-buffer feature to capture events that happened shortly before the 'record' button was pressed which can be useful for manual surveys.\n\nTE detectors are typically used for professional and research work, as they allow a complete analysis of the bats' calls at a later time.\n\nResearch in 2010 observed that frequencies used by bats can be has high as 250 kHz.). The Nyquist–Shannon sampling theorem observes that the minimum sampling frequency required to record a signal successfully must be greater than twice the bandwidth of the signal. To record a bandwidth of 250 kHz therefore requires a sampling frequency in excess of 500 kHz. Modern Time-Expansion capable units typically sample at between 300 kHz and 700 kHz. In general, faster is better, though a higher sampling frequency does use more storage space.\n\nZCA is best known as an add-on device to the Anabat bat detector. The original bat calls are digitised and the zero crossing points used to produce a data stream which is recorded on a Compact Flash card. There are sophisticated timing controls and the device can be set to respond to bat calls, so that many hours of recording are available in unmanned situations. The purpose of ZCA is to reduce the amount of data which must be recorded to the memory and may be considered as a simple form of lossy data compression. Historically, to achieve long recording times, such information reduction has been necessary due to memory capacity limitations and memory cost.\n\nThe solid state ZCA recording is analysed by custom software to produce a time/frequency plot of each call which can be examined for species recognition in a similar way to FD or TE recordings.\n\nThe ZCA detector is usually placed in a bat roost or bat flight path and left for a number of days to collect data. Thus it is less labour-intensive than using a manned bat detector in real time.\n\nWhile the ZCA detector can also be used in real time, its value is for remote recording over long periods. The analysis is similar to that for FD recordings, but there is no amplitude data included. However it does accurately record each zero crossing point, rather than only one in ten. As with all recording devices triggered by an input, a ZCA detector recording automatically is prone to ultrasonic interference from insects such as crickets. Filters can be written to select a characteristic frequency of certain species and ignore others; some (CF species) are more easily filtered, others are nigh on impossible.\n\nThis can be done by using a high speed digitiser peripheral on a computer such as a laptop. This is not a bat detector as such, but recordings of bat calls can be analysed similarly to TE recordings. This method produces large data files and produces no means of detecting bat calls without the simultaneous use of a bat detector. There are however also more sophisticated systems such as the Avisoft-UltraSoundGate that can replace a conventional bat detector. These advanced systems additionally provide a real-time spectrographic display, automated call parameter measurement and classification tools, integrated GPS functionality and a versatile metadata input tool for documenting the recordings.\n\nDSP bat detectors aim to provide an acoustically accurate portrayal of bat calls by using a digital signal processor to map bats' ultrasounds signals to audible sounds; different algorithms are being used to accomplish this, and there is active development and tuning of algorithms going on.\nOne strategy called \"frequency shifting\" uses a FFT signal analysis in order to find the main frequency and signal power, then using digital simulation a new audible wave is synthesized from the original one divided by a defined value.\n\nThe processes of Frequency Division and Heterodyne conversion can also be performed digitally.\n\nThis type of bat detector is believed to be in pre-production or experimental and is not available commercially. Research is in progress for analysing many types of ultrasound calls and sounds besides those of bats.\n\nA TDSC detector digitises the original calls and derives a two dimensional data string by analysing the parameters of each call with respect to time. This is analysed by a neural network to provide pattern recognition for each species.\n\nVisual observation is the obvious means of detecting bats, but of course this can only be done in daylight or crepuscular conditions (i.e., dusk and dawn). Emergence counts are done visually at dusk, using a bat detector for confirmation of species. In lower light conditions a night vision device can be used but the more affordable generation 1 type has a lag time which fails to provide a suitable image of a flying bat.\n\nInfrared (IR) cameras and camcorders are used with an IR illuminator to observe bat emergences and bat behaviour inside and outside roosts. The problem with this method is that deriving a count from a recording is tedious and time consuming, but camcorders can be useful as a backup in roost emergence counts to observe bats re-entering the roost. Many Sony camcorders are sensitive to infrared.\n\nInfrared beam devices usually consist of a dual array of invisible IR beams. The size of the roost entrance determines the number of beams necessary and thus the power required and potential for off-mains use. Single beam DIY systems are available for bat boxes but these do not log the direction of transit. Almost all the systems in use today are non-commercial or DIY. A system in use in some mines in Wisconsin uses two arrays of beams however they are spaced quite far apart and consequently only log approximately 50% of the bats although extrapolated figures are achieved through correlation of time stamped video and beam break data. The Countryside Council for Wales (CCW) uses two similar systems with beams spaced close enough together that every bat transiting the entrance is logged along with the temperature. These systems require either mains power or 12 V deep cycle batteries. They can be used in conjunction with an Anabat Zcaim installed in a 6\" soil pipe and pointed across the roost entrance to discriminate between species by correlating the time stamp data from the IR array and filtered Anabat Zcaim data for horsehoe bats (relatively easy due to their easily identifiable CF echolocation which can be filtered automatically using Anabat software).\n\nData from beam break systems must be carefully analysed to eliminate \"light sampling behaviour\" (environment sampling) where the bats repeatedly leave the roost and return immediately if the conditions are not suitable. Some systems discriminate for bat sized animals; they determine if the beams are broken by a bat sized animal and ignore all other transits. It is important that data is analysed using a methodology which takes light sampling behaviour into account. The method which seems to give the most accurate results is as follows: \"out\" transit assigned 1, \"in\" transit assigned -1. Start count is set to zero at 4 p.m. daily. Using a spreadsheet, the counts are added cumulatively from 4 p.m. each day until 9 a.m. the next day. The maximum \"positive\" count can easily be found for each day. Since every transit is time stamped, the exact time of the maximum daily count is also known. Light sampling counts are eliminated from the data since an \"out\" 1 is cancelled by an \"in\" -1, resulting in a cumulative count of zero for light sampling bats.\n\nThermal imagers which are of a high enough definition to register bats at over 30 metres range are expensive, but have been used to assess the dangers of wind turbines to birds and bats. \"Affordable\" thermal imagers have a bat detecting range about the same order of acoustic bat detectors due to the small size and the low heat emissions of bats.\n\nPassive infrared sensors are slow with a response speed of the order of a tenth of a second and will normally not detect a small fast mammal like a bat.\n\nRadar has been used to detect bats beyond the acoustic limit, but is very costly in equipment and man hours. Bird Aircraft Strike Hazard (BASH) installations are capable of detecting bats, but are usually situated where few bats fly. There are very few suitable mobile terrestrial radars available anywhere. Hand-held doppler radar modules have been used in the field to allow researchers to compensate for the doppler shift imposed on recordings of bat signals due to their flight speed. This allows the researchers to tell whether the bats are changing the pitch of their calls in flight.\n\n\n"}
{"id": "5742067", "url": "https://en.wikipedia.org/wiki?curid=5742067", "title": "Biohydrogen", "text": "Biohydrogen\n\nBiohydrogen is H that is produced biologically. Interest is high in this technology because H is a clean fuel and can be readily produced from certain kinds of biomass. Many challenges characterize this technology, including those intrinsic to H, such as storage and transportation of a noncondensible gas. Hydrogen producing organisms are poisoned by O. Yields of H are often low.\n\nThe main reactions involve fermentation of sugars. Important reactions start with glucose, which is converted to acetic acid:\nA related reaction gives formate instead of carbon dioxide:\nThese reactions are exergonic by 216 and 209 kcal/mol, respectively.\n\nH production is catalyzed by two hydrogenases. One is called [FeFe]-hydrogenase; the other is called [NiFe]-hydrogenase. Many organisms express these enzymes. Notable examples are members of the genera Clostridium, Desulfovibrio, Ralstonia, and the pathogen \"Helicobacter\". \"E. coli\" is the workhorse for genetic engineering of hydrogenases.\n\nIt has been estimated that 99% of all organisms utilize dihydrogen (H). Most of these species are microbes and their ability to use H as a metabolite arises from the expression of H metalloenzymes known as hydrogenases. Hydrogenases are sub-classified into three different types based on the active site metal content: iron-iron hydrogenase, nickel-iron hydrogenase, and iron hydrogenase. \nIn 1933, Marjory Stephenson and her student Stickland reported that cell suspensions catalysed the reduction of methylene blue with H. Six years later, Hans Gaffron observed that the green photosynthetic alga \"Chlamydomonas reinhardtii\", would sometimes produce hydrogen. Gaffron and many other scientists failed to elucidate how it induced hydrogen production. In the late 1990s Anastasios Melis discovered that deprivation of sulfur induces the alga to switch from the production of oxygen (normal photosynthesis) to the production of hydrogen. He found that the enzyme responsible for this reaction is hydrogenase, but that the hydrogenase lost this function in the presence of oxygen. Melis also discovered that depleting the amount of sulfur available to the algae interrupted their internal oxygen flow, allowing the hydrogenase an environment in which it can react, causing the algae to produce hydrogen. \"Chlamydomonas moewusii\" is also a promising strain for the production of hydrogen.\n\nCompeting for biohydrogen, at least for commercial applications, are many mature industrial processes. Hydrogen is usually derived from fossil fuels by steam reforming of natural gas - sometimes referred to as steam methane reforming (SMR) - is the most common method of producing bulk hydrogen at about 95% of the world production.\n"}
{"id": "14528756", "url": "https://en.wikipedia.org/wiki?curid=14528756", "title": "Bris sextant", "text": "Bris sextant\n\nThe Bris sextant is not a sextant proper, but is a small angle-measuring device that can be used for navigation. The \"Bris\" is, however, a true reflecting instrument which derives its high accuracy from the same principle of double reflection which is fundamental to the octant, the true sextant, and other reflecting instruments. It differs from other sextants primarily in being a fixed angle sextant, capable of measuring a few specific angles.\n\nSven Yrvind (Lundin) developed his Bris sextant as part of his quest for low-cost, low-technology equipment for ocean crossings. The \"Bris\" is a low-technology, high-precision, fixed-interval instrument. It is made of two narrow, flat pieces of glass (microscope slides) permanently and rigidly mounted in a V-shape to a third flat piece of #12 welding glass to make viewing the sun eye safe. When the sun or moon is viewed through the V, it is split into eight images. The instrument is small and rugged enough that it can be kept in a 35mm film canister (about 2 cm radius, 3 cm tall) on a lanyard around one's neck.\n\nThe Bris sextant is calibrated at a known geographic position with a good clock and a nautical almanac. As the day passes, one works the sight reductions backwards to develop exact angles for each of the images' tops and bottoms. The Sun and Moon have approximately the same angular size from the surface of the Earth, and can use the same calibrations.\n\nIn use, one waits until an image's edge touches the horizon, and then records the time and reduces the sight using the recorded angle for that edge of the image.\n\n\"Bris\" is Swedish for \"breeze\". It would appear that the name \"Bris\" is used by Yrvind for a number of his sail boats and is a favourite of his.\n\n\n"}
{"id": "308034", "url": "https://en.wikipedia.org/wiki?curid=308034", "title": "Bung Enterprises", "text": "Bung Enterprises\n\nBung Enterprises Ltd. was an electronics company based in Hong Kong. Its products were controversial backup and development units for videogame consoles, which could allow the user to play a game without owning the original cartridge. Backup units for Nintendo Entertainment System, Super NES, Neo-Geo, Neo Geo Pocket, Genesis, Game Boy and Nintendo 64 were developed and produced by Bung.\n\n\"Bung\" is the approximate pronunciation of the company's Chinese name 邦谷. Both written forms were used in product packaging and marketing literature, and the somewhat crude connotation of the English word appears to have been an unintended coincidence. Although in business for more than a decade, the company was relatively unknown in the West until the late 1990s. In 1997 it began selling the Doctor V64, the first commercially available copier for the Nintendo 64, which quickly gained popularity in the\nUnited States and Europe. The following year, it released the V64 Jr, a lower cost model without a CD-ROM drive. The company also began selling rewritable flash memory cards for the Game Boy, and a programming interface called the GB Xchanger.\n\nDuring this time period, Nintendo became increasingly aggressive at trying to stop Bung and the sale of its products. Nintendo first filed lawsuits in the United States against retailers Carl Industries and Upstate Games, accusing them of contributory copyright infringement for selling Bung products. This was initially a somewhat difficult case for Nintendo, as under the\nBetamax precedent, copying devices were legal in the United States if they\nhad non-infringing uses. It turned out that due to a severe shortage of official development hardware from Nintendo, many game developers had been buying from Bung instead. When Bung became aware of this, they began promoting their products for this purpose, and even sponsored a coding competition for amateur game developers. The US lawsuits were eventually settled, with the retailers agreeing to stop selling the copying devices.\n\nNintendo tried a different tactic to stop sales in Europe by reporting Bung products as not being CE mark certified. This resulted in Bung having their power adapters certified, and sales continued. Nintendo next filed a lawsuit against Bung in Hong Kong. The owners of Bung Enterprises responded by dissolving the corporation in March 2000. A few months later they formed a new company under the name First Union Technology, and began selling the same products with different labeling, using the brand name Mr. Flash. The V64 Jr. was sold as the E64, and the GB Xchanger became the Flash Linker.\n\nAfter Nintendo succeeded in having shipments of the new \"Mr. Flash\" products confiscated by US Customs, the company stopped selling directly to US residents, but sales through third parties continued. Nintendo next focused on Hong Kong retailer Lik Sang, which Microsoft and Sony were also targeting due to sales of modchips. This led to a slightly embarrassing incident in February 2002 in which US Customs intercepted a rather innocuous package containing a serial cable. Package seizures apparently ceased after this incident. In October 2002, Nintendo, Sony, and Microsoft jointly filed suit against Lik-Sang, resulting in the company shutting down for two weeks, and then reopening sans the disputed products.\n\nKnown in Hong Kong as \"超任博士\" (literally means \"Super Nintendo Doctor\"), this unit was designed for the backing up of Super Famicom/Super NES games.\n\nThis unit was designed for the backing up of Game Boy and Game Boy Color games. It was also used to write said backups onto a flash cartridge for use on the consoles in question. The unit was designed with a Game Boy-specific slot on the top of the unit and took 6 AAA batteries or a 9V adapter for power, and it connected to a PC via a parallel port.\n\nThis unit was designed for the backing up of Neo-Geo Pocket and Neo-Geo Pocket Color games. The physical construction was identical to that of the GB Xchanger unit, save for the Neo-Geo Pocket slot on top, as opposed to the Game Boy slot. NGP flash carts were also available.\n\n"}
{"id": "246431", "url": "https://en.wikipedia.org/wiki?curid=246431", "title": "Burma-Shave", "text": "Burma-Shave\n\nBurma-Shave was an American brand of brushless shaving cream, famous for its advertising gimmick of posting humorous rhyming poems on small sequential highway roadside signs.\n\nBurma-Shave was introduced in 1925 by the Burma-Vita company in Minneapolis owned by Clinton Odell. The company's original product was a liniment made of ingredients described as having come \"from the Malay Peninsula and Burma\" (hence its name). Sales were sparse, and the company sought to expand sales by introducing a product with wider appeal.\n\nThe result was the Burma-Shave brand of brushless shaving cream and its supporting advertising program. Sales increased. At its peak, Burma-Shave was the second-highest-selling brushless shaving cream in the US. Sales declined in the 1950s, and in 1963 the company was sold to Philip Morris. The signs were removed at that time. The brand decreased in visibility and eventually became the property of the American Safety Razor Company.\n\nIn 1997, the American Safety Razor Company reintroduced the Burma-Shave brand with a nostalgic shaving soap and brush kit, though the original Burma-Shave was a brushless shaving cream, and Burma-Shave's own roadside signs frequently ridiculed \"Grandpa's old-fashioned shaving brush.\"\n\nBurma-Shave sign series first appeared on U.S. Highway 65 near Lakeville, Minnesota, in 1926, and remained a major advertising component until 1963 in most of the contiguous United States. The first series read: \"Cheer up, face - the war is over! Burma-Shave\". The exceptions were New Mexico, Arizona, and Nevada (deemed to have insufficient road traffic), and Massachusetts (eliminated due to that state's high land rentals and roadside foliage). Typically, six consecutive small signs would be posted along the edge of highways, spaced for sequential reading by passing motorists. The last sign was almost always the name of the product. The signs were originally produced in two color combinations: red-and-white and orange-and-black, though the latter was eliminated after a few years. A special white-on-blue set of signs was developed for South Dakota, which restricted the color red on roadside signs to official warning notices.\n\nThis use of a series of small signs, each of which bore part of a commercial message, was a successful approach to highway advertising during the early years of highway travel, drawing the attention of passing motorists who were curious to learn the punchline. As the Interstate system expanded in the late 1950s and vehicle speeds increased, it became more difficult to attract motorists' attention with small signs. When the company was acquired by Philip Morris, the signs were discontinued on advice of counsel.\n\nSome of the signs featured safety messages about speeding instead of advertisements.\n\nExamples of Burma-Shave advertisements are at The House on the Rock in Spring Green, Wisconsin. Re-creations of Burma-Shave sign sets also appear on Arizona State Highway 66, part of the original U.S. Route 66, between Ash Fork, Arizona, and Kingman, Arizona, (though they were not installed there by Burma-Shave during its original campaigns) and on Old U.S. Highway 30 near Ogden, Iowa. Other examples are displayed at The Henry Ford in Dearborn, Michigan, the Interstate 44 in Missouri rest area between Rolla and Springfield (which has old Route 66 building picnic structures), the Forney Transportation Museum in Denver, Colorado and the Virginia Museum of Transportation in Roanoke, Virginia.\n\nThe complete list of the 600 or so known sets of signs is listed in \"Sunday Drives\" and in the last part of \"The Verse by the Side of the Road\". The content of the earliest signs is lost, but it is believed that the first recorded signs, for 1927 and soon after, are close to the originals. The first ones were prosaic advertisements. Generally the signs were printed with all capital letters. The style shown below is for readability:\n\nAs early as 1928, the writers were displaying a puckish sense of humor:\n\nIn 1929, the prosaic ads began to be replaced by actual verses on four signs, with the fifth sign merely a filler for the sixth:\n\nPreviously there were only two to four sets of signs per year. 1930 saw major growth in the company, and 19 sets of signs were produced. The writers recycled a previous joke. They continued to ridicule the \"old\" style of shaving. And they began to appeal to the wives as well:\n\nIn 1931, the writers began to reveal a \"cringe factor\" side to their creativity, which would increase over time:\n\nIn 1932, the company recognized the popularity of the signs with a self-referencing gimmick:\n\nIn 1935, the first known appearance of a road safety message appeared, combined with a punning sales pitch:\n\nSafety messages began to increase in 1939, as these examples show. (The first of the four is a parody of \"Paul Revere's Ride\" by Henry Wadsworth Longfellow.)\n\nIn 1939 and subsequent years, demise of the signs was foreshadowed, as busy roadways approaching larger cities featured shortened versions of the slogans on one, two, or three signs — the exact count is not recorded. The puns include a play on the Maxwell House Coffee slogan, standard puns, and yet another reference to the \"H\" joke:\n\nThe war years found the company recycling a lot of their old signs, with new ones mostly focusing on World War II propaganda:\n\n1963 was the last year for the signs, most of which were repeats, including the final slogan, which had first appeared in 1953:\n\nPossibly the ultimate in self-referencing signs, leaving out the product name. This one also adorns the cover of the book:\n\n\nA number of films and television shows set between the 1920s and 1950s have used the Burma-Shave roadside billboards to help set the scene. Examples include \"Bonnie and Clyde\", \"A River Runs Through It\", \"The World's Fastest Indian\", \"Stand By Me\", \"Tom and Jerry\", \"M*A*S*H\" and the pilot episode (\"Genesis\") of \"Quantum Leap\". The long-running series \"Hee Haw\" borrowed the style for program bumpers, transitioning from one show segment to the next or to commercials.\n\nThe final episode of the popular television series \"M*A*S*H\" featured a series of road signs in Korea \"Hawk was gone, now he's here. Dance til dawn, give a cheer. Burma-Shave\" in the style of Burma-Shave adverts in US roads in the 1950s.\n\nRoger Miller's song \"Burma Shave\" (the B-side to his 1961 single \"Fair Swiss Maiden\") has the singer musing that he's \"seen a million rows of them little red poetic signs up and down the line\", while reciting rhymes in the manner of the ads. Tom Waits' song \"Burma-Shave\" (from his 1977 album \"Foreign Affairs\") uses the signs as an allegory for an unknown destination. (\"I guess I'm headed that-a-way, Just as long as it's paved, I guess you'd say I'm on my way to Burma-Shave\") Chuck Suchy's song \"Burma Shave Boogie\" (from his 2008 album \"Unraveling Heart\") incorporates several of the Burma Shave rhymes into its lyrics.\n\nThe pedestrian passageway between the Times Square and Port Authority Bus Terminal stations in the New York City subway system contains a piece of public art inspired by the Burma-Shave ads; Norman B. Colp's \"The Commuter's Lament, or A Close Shave\" consists of a series of signs attached to the roof of the passageway, displaying the following text:\n\nSeveral highway departments in the United States use signs in the same style to dispense travel safety advice to motorists.\n\nSeveral writers of doggerel and humorously bad poetry (such as David Burge), often use \"Burma Shave\" on the last line of their poems to indicate their non-serious nature.\n\nRandal Munroe author of xkcd published xkcd: Twitter referencing the rhyme pattern of Burma-Shave.\nThe word \"burmashaving\" is used in Canada to describe politicians holding signs and waving to traffic by the side of the road, a common sight during election campaigns. One of the first to use the phrase was Nova Scotia Progressive Conservative premier John Buchanan, who would stand at the end of a long line of party signs and wave to morning traffic.\n\n\n"}
{"id": "561270", "url": "https://en.wikipedia.org/wiki?curid=561270", "title": "Caseless ammunition", "text": "Caseless ammunition\n\nCaseless ammunition is a type of small arms ammunition that eliminates the cartridge case that typically holds the primer, propellant, and projectile together as a unit.\n\nCaseless ammunition is an attempt to reduce the weight and cost of ammunition by dispensing with the case, which is typically precision made of brass or steel, as well as to simplify the operation of repeating firearms by eliminating the need to extract and eject the empty case after firing. Its acceptance has been hampered by problems with production expenses, heat sensitivity, sealing, and fragility. Its use to date has been limited to prototypes and low-powered firearms.\n\nAn early predecessor to modern caseless ammunition, Walter Hunt's Rocket Ball cartridge, was developed in 1850s and guns using them were sold during that time, primarily by Volcanic Repeating Arms. These cartridges were severely under-powered and never saw wide acceptance for self-protection, hunting, or military use. \n\nDuring World War II, Germany began an intensive program to research and develop a practical caseless ammunition for military use, which was driven by the rising scarcity of metals, especially copper used to make cartridge cases. The Germans had some success, but not sufficient to produce a caseless cartridge system during the war. Japan successfully developed the Ho-301 40mm autocannon during the war for mounting on aircraft. It saw active, though relatively limited, use in the defense of the Japanese home islands during the waning months of the war.\n\nModern caseless ammunition consists of a solid mass of propellant (originally nitrocellulose) cast to form the body of the cartridge. Cavities exist in the body to accept the bullet and a primer (both of which are glued into place). The completed cartridge might also contain a booster charge of powdered propellant to help ignite the body and provide initial thrust to the bullet.\n\nMany caseless cartridges are also telescoped, with the bulk of the bullet held within the body of the cartridge, to cut down on cartridge length. A shorter cartridge cuts down on the distance the firearm's action must reciprocate to load a new round, which allows for higher cyclic rates and greater probability of multiple hits on a target at long range. Lack of a case also reduces the weight of the cartridge substantially, especially in small bore rifles. For example, the caseless ammunition designed by Austrian inventor Hubert Usel (1926—2010) for the Voere VEC-91 weighs about one third as much as regular ammunition for the same caliber.\n\nWhile it seems a simple operation to replace the case with a piece of solid propellant, the cartridge case provides more than just a way to keep the cartridge components together, and these other functions must be replaced if the case is to be replaced. Caseless ammunition is not without its drawbacks, and it is these drawbacks that have kept modern caseless ammunition from achieving wider success.\n\nThe first major problem, of special concern in military applications, which often involve sustained firing, is the heat sensitivity of the ammunition. Nitrocellulose, the primary component of modern gunpowder, ignites at a relatively low temperature of around 170 °C (338 °F). One of the functions of the metallic cartridge case is as a heat sink; when extracted after firing, every metallic case carries away a significant amount of the heat from the combustion of the propellant, slowing the rate at which heat builds up in the chamber. The thermal insulation provided by the case also works the other way around, shielding the propellant from built-up heat in the chamber walls.\n\nWithout a case to provide these functions, caseless rounds using nitrocellulose will begin to cook off, firing from the residual chamber heat, much sooner than cased cartridges do. Cooking off can be avoided by designing the weapon to fire from an open bolt, but this greatly impacts accuracy, and thus is only suitable for machine guns and submachine guns.\n\nThe normal solution to the problem of heat is to increase the heat resistance by switching to a propellant with a higher ignition temperature, typically a non-crystalline explosive carefully formulated to provide an appropriate rate of combustion. Heckler & Koch, in concert with Dynamit Nobel, managed such a task by producing relatively heat-resistant caseless ammunition.\n\nAnother important function provided by the cartridge is to seal the rear of the chamber. During firing of a cased cartridge, the pressure in the chamber expands the metallic case which obturates to the chamber. This prevents gas exiting from the rear of the chamber, and it has also been experimentally shown to provide a significant amount of support to the bolt. Without the case to provide this seal, the firearm design must account for this and provide a means of sealing the rear of the chamber. This problem was also encountered with the Dreyse needle gun; the French Chassepot solved the leaking-breech problem with the addition of a rubber seal to the bolt.\n\nTelescoping caseless rounds must also deal with the issue of blocking the bore, as the bullet is surrounded by propellant. The booster charge is used to address this issue, providing an initial burst of pressure to force the bullet out of the cartridge body and into the barrel before the body combusts.\n\nCaseless rounds are limited by the fact that the cartridge body is primarily a propellant, and structural properties are secondary to the combustion properties. The primary issue is one of extraction. While caseless ammunition eliminates the need to extract a fired case, unfired caseless rounds must be extractable to unload the firearm or to clear a misfire. With metallic cases, this ability is provided by a rim or extractor groove machined into the rear of the case. Even in completely plastic bodied cartridges, such as the \"Activ\" brand shotgun shells, a thin metal ring is molded into the rim to provide support for the extractor. A secondary issue is that ammunition in use can be exposed to air, water, lubricants, and solvents. Primer and propellant in caseless rounds is unprotected, while cartridge cases provide a high degree of protection.\n\nOne of the first caseless firearm and ammunition systems produced was made by Daisy, the airgun maker, in 1968. The Daisy V/L Rifle used a .22 caliber (5.5 mm) low-powered caseless round with no primer. The rifle was basically a spring-piston air rifle, but when used with the V/L ammunition the energy from the compression of the piston heated the air behind the caseless cartridge enough to ignite the propellant, and this generated the bulk of the energy of firing. The Daisy V/L Rifle system was discontinued in 1969 after the ATF ruled that it was not an airgun, but a firearm, which Daisy was not licensed to produce.\n\nSome assault rifles have used caseless ammunition. One of the better-known weapons of this type is the G11 made by Heckler & Koch as a potential replacement for the G3 battle rifle. Although the G11 never entered full production, it went through a number of prototype stages as well as field testing, including testing as part of the American Advanced Combat Rifle program. While it was scheduled to be adopted by the West German military with a plan set out to procure 300,000 G11K2 rifles over a period from 1990 to 2002, the expenses created by the reunification of Germany and the impossibility of modifying the G11 to use NATO-standard ammunition led to the cancellation of the G11 project and the adoption of a cheaper, more conventional NATO-standardised assault rifle, the 5.56mm G36. The G11's caseless ammunition was later used as the basis for the caseless round development in the US Lightweight Small Arms Technologies program.\n\nThe first commercial caseless rifle featuring electronic firing was the Voere VEC-91.\n\nSeveral Russian grenade launchers use caseless ammunition including the GP-25 under-barrel grenade launcher and the AGS‑40 Balkan automatic grenade launcher.\n\n\n\n"}
{"id": "43043614", "url": "https://en.wikipedia.org/wiki?curid=43043614", "title": "CastAR", "text": "CastAR\n\ncastAR (formerly Technical Illusions) was a Palo Alto-based technology startup company founded in March 2013 by Jeri Ellsworth and Rick Johnson. Its first product was to be the castAR, a pair of augmented reality and virtual reality glasses. castAR was a founding member of the nonprofit Immersive Technology Alliance.\n\ncastAR was founded by two former Valve Corporation employees; the castAR glasses were born out of work that started inside Valve. While still at Valve, their team had spent over a year working on the project. They obtained legal ownership of their work after their departure.\n\nIn August 2015, Playground Global funded $15M into castAR to build its product and create mixed-reality experiences. In August 2016, Darrell Rodriguez, former President of LucasArts, joined as the new CEO. In addition, Steve Parkis became President and COO, after leading teams at The Walt Disney Company and Zynga. In September 2016, they opened castAR Salt Lake City, a new development studio formed from a team hired out of the former Avalanche Software, which worked on the Disney Infinity series.\n\nIn October 2016, they announced the acquisition of Eat Sleep Play, the developer best known for Twisted Metal, also in Salt Lake City UT. \n\nIn December 2016, Parkis, who had been President and COO, was named CEO to replace Rodriguez. \n\nIn June 2017 it was reported by Polygon that CastAR was shutting down, laying off 70 employees. A core group of administrators was expected to remain, to sell off the company's technology.\n\nThe castAR glasses combine elements of augmented reality and virtual reality. After winning Educator's and Editor's Choice ribbons at the 2013 Bay Area Maker Faire, the castAR project was successfully crowdfunded via Kickstarter. castAR surpassed its funding goal two days after the project went live and raised over $1 million on a $400,000 goal. castAR creates hologram-like images unique to each user by projecting an image into the user's surroundings using a technology that Technical Illusions calls \"Projected Reality\". The image bounces off a retro-reflective surface back to the wearer's eyes. castAR can also be used for virtual reality purposes, using its VR clip-on. Before the time of the 2017 company shutdown all Kickstarter funds had been paid back to the original backers. Along with the repayment, a coupon for a free set of the production AR glasses was given to each backer. This happened at the time of the 2015 Playground Global investment.\n\n\n"}
{"id": "30875975", "url": "https://en.wikipedia.org/wiki?curid=30875975", "title": "Century egg", "text": "Century egg\n\nCentury egg or Pidan (), also known as preserved egg, hundred-year egg, thousand-year egg, thousand-year-old egg, millennium egg, skin egg and black egg, is a Chinese preserved food product and delicacy made by preserving duck, chicken or quail eggs in a mixture of clay, ash, salt, quicklime, and rice hulls for several weeks to several months, depending on the method of processing.\n\nThrough the process, the yolk becomes a dark green to grey color, with a creamy consistency and strong flavor due to the hydrogen sulfide and ammonia present, while the white becomes a dark brown, translucent jelly with a salty flavor. The transforming agent in the century egg is an alkaline salt, which gradually raises the pH of the egg to around 9–12, during the curing process. This chemical process breaks down some of the complex, flavorless proteins and fats, which produces a variety of smaller flavorful compounds.\n\nSome eggs have patterns near the surface of the egg white that are likened to pine branches, and that gives rise to one of its Chinese names, the pine-patterned egg.\n\nThe method for creating century eggs likely came about through the need to preserve eggs in times of plenty by coating them in alkaline clay, which is similar to methods of egg preservation in some Western cultures. The clay hardens around the egg and results in the curing and creation of century eggs instead of spoiled eggs.\n\nAccording to some, the century egg has over five centuries of history behind its production. Its discovery, though not verifiable, was said to have occurred around 600 years ago in Hunan during the Ming Dynasty, when a homeowner discovered duck eggs in a shallow pool of slaked lime that was used for mortar during construction of his home two months before. Upon tasting the eggs, he set out to produce more — this time with the addition of salt to improve their flavor — resulting in the present recipe of the century egg.\n\nThe traditional method for producing century eggs developed through improvement of the aforementioned primitive process. Instead of using just clay, a mixture of wood ash, calcium oxide, and salt is included in the plastering mixture, thereby increasing its pH and sodium content. The addition of calcium oxide and wood ash to the mixture lowers the risk of spoilage and also increases the speed of the process. A recipe for creating century eggs starts with the infusion of three pounds of tea in boiling water. To the tea, three pounds of calcium oxide (or seven pounds, if done in winter), nine pounds of sea salt, and seven pounds of ash from burned oak is mixed into a smooth paste. Each egg is individually covered by hand, with gloves worn to protect the skin from chemical burns. It is then rolled in a mass of rice chaff, to keep the eggs from adhering to one another, before the eggs are placed in cloth-covered jars or tightly woven baskets. The mud slowly dries and hardens into a crust over several months. The eggs are then ready for consumption.\n\nEven though the traditional method is still widely practiced, modern understanding of the chemistry behind the formation of century eggs has led to many simplifications in the recipe. For instance, soaking raw eggs in a solution of table salt, calcium hydroxide, and sodium carbonate for 10 days, followed by several weeks of aging while wrapped in plastic, is said to achieve the same effect as the traditional method. This is because the reaction needed to produce century eggs is accomplished by introducing hydroxide and sodium ions into the egg, regardless of the method used.\n\nThe extremely toxic compound lead(II) oxide speeds up the reactions which create century eggs, leading to its use by some unscrupulous producers. However, zinc oxide is now the recommended alternative. Although zinc is essential for life, excessive zinc consumption can lead to copper deficiency, so the finished product should have its zinc level assessed for safety.\n\nCentury eggs can be eaten without further preparation than peeling and rinsing them -- on their own, or as a side dish. As an \"hors d'œuvre\", the Cantonese wrap chunks of this egg with slices of pickled ginger root (sometimes sold on a stick as street food). A Shanghainese recipe mixes chopped century eggs with chilled tofu. In Taiwan, it is popular to eat sliced century eggs placed on top of cold tofu with \"katsuobushi\", soy sauce, and sesame oil, in a style similar to Japanese \"hiyayakko\". A variation of this recipe common in northern China is to slice century eggs over chilled silken (soft) tofu, adding liberal quantities of shredded young ginger and chopped spring onions as a topping, and then drizzling light soy sauce and sesame oil over the dish, to taste. They are also used in a dish called old-and-fresh eggs, where chopped century eggs are combined with (or used to top) an omelet made with fresh eggs. The century eggs may also be cut into chunks and stir fried with vegetables, which is most commonly found in Taiwanese cuisine.\n\nSome Chinese households cut them up into small chunks and cook them with rice porridge to create \"century egg and lean pork \"congee\"\" (). This is sometimes served in dim sum restaurants. Rice congee, lean pork, and century egg are the main ingredients. Peeled century eggs are cut into quarters or eighths and simmered with the seasoned marinated lean slivers of pork until both ingredients are cooked into the rice congee. Fried dough sticks known as \"youtiao\" are commonly eaten with century egg congee. Another common variation of this dish is the addition of salted duck eggs into the congee mixture.\n\nAt special events like wedding banquets or birthday parties, a first course platter of sliced barbecued pork, pickled baby leeks, sliced abalone, pickled julienned carrots, pickled julienned daikon radish, seasoned julienned jellyfish, sliced pork, head cheese and the quartered century eggs is served. This is called a \"lahng-poon\" in Cantonese, which simply means \"cold dish\".\n\nAccording to a common misconception, century eggs are, or were once, prepared by soaking eggs in horse urine. The myth may have arisen from the urine-like odor of ammonia and other amines produced by the chemical reaction used to make century eggs. However, this myth is unfounded as horse urine has a pH ranging from 7.5 to 7.9 and therefore would not work for this process.\n\nIn Thai and Lao, the common word for century egg translates to \"horse urine egg\", due to the distinctive urine-like odor of the delicacy:\n\nHeavy metals were used to speed up the process to turn more profit for less time and artificially increase the quality of the preserved egg. It was an unscrupulous practice in some small factories but it became rampant in China and forced many honest manufacturers to label their boxes \"Lead Free\" after the scandal went mainstream in 2013. Many production factories in China were using industrial quality copper sulphate which was contaminated with arsenic, lead, cadmium, and other heavy metals and toxic chemicals, to make eggs more translucent, lessen odor, provide smoother texture and encourage faster curing.\nThe Chinese government has recently been trying to regulate food additives and license law abiding establishments to combat the food safety incidents in China posed by bad manufacturing practice.\n\n"}
{"id": "37850969", "url": "https://en.wikipedia.org/wiki?curid=37850969", "title": "Comparison of power management software suites", "text": "Comparison of power management software suites\n\nThe following tables compare technical information for a commercial PC Power Management software suites. Please see the individual products' articles for further information. The table only includes systems that are widely used and currently available.\n"}
{"id": "23106357", "url": "https://en.wikipedia.org/wiki?curid=23106357", "title": "Defense Transportation Reporting and Control System", "text": "Defense Transportation Reporting and Control System\n\nDTRACS was a tracking and control system used by United States military units in Europe and South West Asia (SWA). The system is actually a commercial off the shelf (COTS) product from Omnitracs, and is in use by commercial trucking fleets throughout the world. Initially, when the system was first in use by the US Army in Europe (USAREUR), it was run through an Alcatel contract out of Paris with a data feed to the USAREUR G4 Logistics Automation Division (LAD).\n\nAfter it became more heavily used, it was determined that the Army needed its own secure hub on a military base in Germany. This hub was located in Mannheim, Germany outside of Coleman Barracks at a location known as the \"tank-farm\". This location was chosen because it already housed other satellite hubs, one of which was for AFN.\n\nThe Land Earth Station (LES) portion of the system was maintained by a company called Data Path (which was acquired by Rockwell Collins in 2009). The servers that processed the messages and GPS location information through the QTRACS software were initially housed in Friedrichsfeld, Germany and maintained by the USAREUR G4 office. The G4 contracted this work out to the Titan Corporation (now owned by L3 Communications). Management of the program was eventually turned over to PEO EIS. The servers were eventually moved to Kilbourne Kasserne in Schwetzingen. This move was made because the facility at Friedrichsfeld was not robust enough and had a poor communications path with no redundancy. The site in Schwetzingen, also known as \"Site-S\", had better comms and was also an official military Network Operations Center (NOC).\n\nAt its height, there were three server stacks for processing DTRACS data. One stack for European devices, another for devices in SWA and a third for receiving a split-feed from the KBR (Kellog Brown and Root) owned and operated server. While KBR had their own vehicles and server, all three systems used the same satellite for their communications. As of January 2010 the KBR solution still exists, but was moved to Ramstein Airbase.\n\nOn the military side, the system was replaced by a newer system called Movement Tracking System, or MTS. The last use of DTRACS by US military personnel was in early 2007. While some devices are still used by KBR in their deployment, the military decided to switch to MTS instead of DTRACS for its logistics tracking solution. The primary reason for the switch is that MTS provided a mapping capability in the vehicle. Of course this came at a price. A typical DTRACS device cost in the neighborhood of $2,500, while an MTS device cost approximately $20,000 or more. Further funding for MTS from the military allowed the system to have Radio Frequency Identification (RFID) included in the system. This allowed the trucks to read active ANSI and ISO18000 RFID tags that were included in its cargo and report live locations back to the Radio Frequency - In-Transit Visibility (RF-ITV) system maintained by PEO EIS Product Director Automated Movement and Identification Solutions (PD AMIS)\n\nDTRACS devices were capable of sending messages to each other and their dispatchers. The dispatchers used either a DTRACS device in a box (referred to as a \"Fly Away Kit\" or FAK) or online software (QTRACS) to send and receive messages from the drivers. While the DTRACS FAK only allowed limited communications, the QTRACS software allowed the dispatcher to see all communications they sent to and from their trucks as well as their assigned vehicles \"truck to truck\" messages. It also provided topographical maps, and where necessary, satellite imagery provided by NIMA (now NGA).\n\nVehicles could send free text messages containing up to 1,900 characters. There were all preformatted or 'canned' messages where the driver only needed to fill in blank fields and hit send. Such messages included NBC1 reports, NEDIVAC message and arrival and departure messages among others.\n\n"}
{"id": "24106109", "url": "https://en.wikipedia.org/wiki?curid=24106109", "title": "Dental prosthesis", "text": "Dental prosthesis\n\nA dental prosthesis is an intraoral (inside the mouth) prosthesis used to restore (reconstruct) intraoral defects such as missing teeth, missing parts of teeth, and missing soft or hard structures of the jaw and palate. Prosthodontics is the dental specialty that focuses on dental prostheses. Such prostheses are used to rehabilitate mastication (chewing), improve aesthetics, and aid speech. A dental prosthesis may be held in place by connecting to teeth or dental implants, by suction, or by being held passively by surrounding muscles. Like other types of prostheses, they can either be fixed permanently or removable; fixed prosthodontics and removable dentures are made in many variations. Permanently fixed dental prostheses use dental adhesive or screws, to attach to teeth or dental implants. Removal prostheses may use friction against parallel hard surfaces and undercuts of adjacent teeth or dental implants, suction using the mucous retention (with or without aid from denture adhesives), and by exploiting the surrounding muscles and anatomical contours of the jaw to passively hold in place. \n\nSome examples of dental prostheses include:\n\n"}
{"id": "8960255", "url": "https://en.wikipedia.org/wiki?curid=8960255", "title": "Design history", "text": "Design history\n\nWith a broad definition, the contexts of design history include the social, the cultural, the economic, the political, the technical and the aesthetic. Design history has as its objects of study all designed objects including those of architecture, fashion, crafts, interiors, textiles, graphic design, industrial design and product design.\n\nDesign history has had to incorporate criticism of the 'heroic' structure of its discipline, in response to the establishment of material culture, much as art history has had to respond to visual culture, (although visual culture has been able to broaden the subject area of art history through the incorporation of the televisual, film and new media). Design history has done this by shifting its focus towards the acts of production and consumption.\n\nDesign history also exists as a component of many practice-based courses.\n\nThe teaching and study of design history within art and design programs in Britain are one of the results of the National Advisory Council on Art Education in the 1960s. Among its aims was making art and design education a legitimate academic activity, to which ends a historical perspective was introduced. This necessitated the employment or ‘buying in’ of specialists from art history disciplines, leading to a particular style of delivery: “Art historians taught in the only way that art historians knew how to teach; they switched off the lights, turned on the slide projector, showed slides of art and design objects, discussed and evaluated them and asked (art and design) students to write essays – according to the scholarly conventions of academia”.\n\nThe most obvious effect of the traditional approach design history as sequential, in which X begat Y and Y begat Z. This has pedagogical implications in that the realization that assessment requires a fact-based regurgitation of received knowledge leads students to ignore discussions of the situations surrounding a design’s creation and reception and to focus instead on simple facts such as who designed what and when.\n\nThis 'heroic/aesthetic' view – the idea that there are a few great designers who should be studied and revered unquestioningly – arguably instills an unrealistic view of the design profession. Although the design industry has been complicit in promoting the heroic view of history, the establishment of the UK government of Creative & Cultural Skills has led to calls for design courses to be made less 'academic' and more attuned to the 'needs' of the industry. Design history, as a component of design courses, is under increasing threat in the UK at least and it has been argued that its survival depends on an increased focus on the study of the processes and effects of design rather than the lives of designers themselves.\n\nUltimately it appears that design history for practice-based courses is rapidly becoming a branch of social and cultural studies, leaving behind its art historical roots. This has led to a great deal of debate as the two approaches forge distinct pedagogical approaches and philosophies.\n\nThe debate over the best way to approach the teaching of design history to practice-based students is often heated, but it is notable that the biggest push to adopt a 'realistic' approach (i.e. non-hero-based, analysing the production and consumption of design that would otherwise be viewed as ephemeral) comes from teachers delivering these programmes, while critics are predominantly those who teach 'pure' design history courses.\n\nThe biggest criticism of the 'realistic' approach appears to be that it imposes anonymity on designers, while the counter argument is that the vast majority of designers \"are\" anonymous and that it is the uses and users of design that are more important.\n\nThe research literature suggests that, contrary to critics' predictions of the death of design history, this realistic approach is beneficial. Baldwin and McLean at the University of Brighton (now at the University of Dundee and Edinburgh College of Art respectively) reported attendance figures for courses using this model rising dramatically, and improved interest in the subject, as did Rain at Central St. Martin's. This compares with the often-reported low attendance and low grades of practice-based students facing the 'death by slideshow' model.\n\n\n\n"}
{"id": "930128", "url": "https://en.wikipedia.org/wiki?curid=930128", "title": "Directive (programming)", "text": "Directive (programming)\n\nIn computer programming, a directive or pragma (from \"pragmatic\") is a language construct that specifies how a compiler (or other translator) should process its input. Directives are not part of the grammar of a programming language, and may vary from compiler to compiler. They can be processed by a preprocessor to specify compiler behavior, or function as a form of in-band parameterization.\n\nIn some cases directives specify global behavior, while in other cases they only affect a local section, such as a block of programming code. In some cases, such as some C programs, directives are optional compiler hints, and may be ignored, but normally they are prescriptive, and must be followed. However, a directive does not perform any action in the language itself, but rather only a change in the behavior of the compiler.\n\nThis term could be used to refer to proprietary third party tags and commands (or markup) embedded in code that result in additional executable processing that extend the existing compiler, assembler and language constructs present in the development environment. The term \"directive\" is also applied in a variety of ways that are similar to the term \"command\".\n\nIn C and C++, the language supports a simple macro preprocessor. Source lines that should be handled by the preprocessor, such as codice_1 and codice_2 are referred to as \"preprocessor directives\".\n\nAnother C construct, the codice_3 directive, is used to instruct the compiler to use pragmatic or implementation-dependent features. Two notable users of this directive are OpenMP and OpenACC.\n\nSyntactic constructs similar to C's preprocessor directives, such as C#'s codice_4, are also typically called \"directives\", although in these cases there may not be any real preprocessing phase involved.\n\nAll preprocessor commands begin with a hash symbol (#).\n\nDirectives date to ALGOL 68, where they are known as (from \"pragmatic\"), and denoted pragmat or pr; in newer languages, notably C, this has been abbreviated to \"pragma\" (no 't').\n\nA common use of pragmats in ALGOL 68 is in specifying a stropping regime, meaning \"how keywords are indicated\". Various such directives follow, specifying the POINT, UPPER, RES (reserved), or quote regimes. Note the use of stropping for the pragmat keyword itself (abbreviated pr), either in the POINT or quote regimes:\n\nToday directives are best known in the C language, of early 1970s vintage, and continued through the current C99 standard, where they are either instructions to the C preprocessor, or, in the form of codice_3, directives to the compiler itself. They are also used to some degree in more modern languages; see below.\n\n\n\n\n\n"}
{"id": "31540771", "url": "https://en.wikipedia.org/wiki?curid=31540771", "title": "FORTRAS", "text": "FORTRAS\n\nFortras (\"Forschungs und Entwicklungsgesellschaft für Transportwesen\" or: \"Research and Development Corporation for the Transportation Sector\") is an EDI standard for data exchange between carriers.\n\nFortras was the designation of an independent company until 2001, when it merged with System Alliance. The designation 'Fortras' is now solely used for the EDI standard that also is used outside of System Alliance. \n\nImportant Fortras message types are \"BORD\" for consignment information (comparable to EDIFACT's IFCSUM), \"ENTL\" for unloading reports from the receiving carrier, and \"STAT\" for status information (comparable to EDIFACT's IFTSTA).\n\nFortras messages consist of a sequence of lines with a fixed number of characters, e.g. 80, 128, or 512 and data elements are placed on fixed positions on each line. The document type is specified by a letter and a record number at the beginning of the line.\n\n"}
{"id": "10304355", "url": "https://en.wikipedia.org/wiki?curid=10304355", "title": "Fab Lab Barcelona", "text": "Fab Lab Barcelona\n\nFab Lab Barcelona is a centre of production, investigation and education, that uses last generation (as of 2008) computer-assisted design software for the creation of prototypes and scale models for Architecture, Construction, Industrial Design and any activity that needs the connection to a computer to manipulate materials according to digital instructions.\n\nFab Lab Pro: to serve as place for prototype and scale models manufacture for professionals and companies of any field related to Design.\n\nFab Lab Masters to serve as research and high education centre, mainly tie to the Institute for Advanced Architecture of Catalonia’s Master Program, for national and international professionals and organizations.\n\nCentral Fab Lab to serve as educational centre for young people and children, according to the principles of Bits and Atoms Centre of the MIT, thus participating in the worldwide network of Fab Labs.\n\n"}
{"id": "1560443", "url": "https://en.wikipedia.org/wiki?curid=1560443", "title": "Fossil fuel power station", "text": "Fossil fuel power station\n\nA fossil fuel power station is a power station which burns a fossil fuel such as coal, natural gas, or petroleum to produce electricity. Central station fossil fuel power plants are designed on a large scale for continuous operation. In many countries, such plants provide most of the electrical energy used. Fossil fuel power stations have machinery to convert the heat energy of combustion into mechanical energy, which then operates an electrical generator. The prime mover may be a steam turbine, a gas turbine or, in small plants, a reciprocating internal combustion engine. All plants use the energy extracted from expanding gas, either steam or combustion gases. \nAlthough different energy conversion methods exist, all thermal power station conversion methods have efficiency limited by the Carnot efficiency and therefore produce waste heat.\n\nBy-products of thermal power plant operation must be considered in their design and operation. Waste heat energy, which remains due to the finite efficiency, is released directly to the atmosphere or to a body of water, or indirectly to the atmosphere using a cooling tower with river or lake water used as a cooling medium. \nThe flue gas from combustion of the fossil fuels is discharged to the air. \nThis gas contains carbon dioxide and water vapor, as well as other substances such as nitrogen oxides (NO), sulfur oxides (SO), mercury, traces of other metals, and, for coal-fired plants, fly ash. \nSolid waste ash from coal-fired boilers must also be removed. \nSome coal ash can be recycled for building materials.\n\nFossil fueled power stations are major emitters of carbon dioxide (CO), a greenhouse gas which is a major contributor to global warming. \nThe results of a recent study show that the net income available to shareholders of large companies could see a significant reduction from the greenhouse gas emissions liability related to only natural disasters in the United States from a single coal-fired power plant. \nHowever, as of 2015, no such cases have awarded damages in the United States. \nPer unit of electric energy, brown coal emits nearly two times as much CO as natural gas, and black coal emits somewhat less than brown. \nCarbon capture and storage of emissions has been proposed to limit the environmental impact of fossil fuel power stations, but it is still at a demonstration stage.\n\nIn a fossil fuel power plant the chemical energy stored in fossil fuels such as coal, fuel oil, natural gas or oil shale and oxygen of the air is converted successively into thermal energy, mechanical energy and, finally, electrical energy. Each fossil fuel power plant is a complex, custom-designed system. Construction costs, , run to US$1,300 per kilowatt, or $650 million for a 500 MWe unit. Multiple generating units may be built at a single site for more efficient use of land, natural resources and labor. Most thermal power stations in the world use fossil fuel, outnumbering nuclear, geothermal, biomass, or solar thermal plants.\n\nThe second law of thermodynamics states that any closed-loop cycle can only convert a fraction of the heat produced during combustion into mechanical work. The rest of the heat, called waste heat, must be released into a cooler environment during the return portion of the cycle. The fraction of heat released into a cooler medium must be equal or larger than the ratio of absolute temperatures of the cooling system (environment) and the heat source (combustion furnace). Raising the furnace temperature improves the efficiency but complicates the design, primarily by the selection of alloys used for construction, making the furnace more expensive. The waste heat cannot be converted into mechanical energy without an even cooler cooling system. However, it may be used in cogeneration plants to heat buildings, produce hot water, or to heat materials on an industrial scale, such as in some oil refineries, plants, and chemical synthesis plants.\n\nTypical thermal efficiency for utility-scale electrical generators is around 37% for coal and oil-fired plants, and 56 – 60% (LEV) for combined-cycle gas-fired plants. Plants designed to achieve peak efficiency while operating at capacity will be less efficient when operating off-design (i.e. temperatures too low.)\n\nPractical fossil fuels stations operating as heat engines cannot exceed the Carnot cycle limit for conversion of heat energy into useful work. Fuel cells do not have the same thermodynamic limits as they are not heat engines.\n\nTemperature of Hot Steam.\nUsing the reported efficiencies and the efficiency of an ideal Carnot engine one can estimate the engine temperature. This estimate is the minimum heat water/steam temperature as we neglect other losses. For example, the effective temperature of the cooling water can be significantly higher.\nAssume the cold temperature formula_1 is 10 °C, or 280 K than formula_2 equals:\n\nThis temperature is presumably much lower than the actual steam temperature due to several losses.\n\nCoal is the most abundant fossil fuel on the planet, and widely used as the source of energy in thermal power stations. It is a relatively cheap fuel, with some of the largest deposits in regions that are stable politically, such as China, India and the United States. This contrasts with natural gas, the largest deposits of which are located in Russia, Iran, Qatar, Turkmenistan and the US. Solid coal cannot directly replace natural gas or petroleum in most applications, petroleum is mostly used for transportation and the natural gas not used for electricity generation is used for space, water and industrial heating. Coal can be converted to gas or liquid fuel, but the efficiencies and economics of such processes can make them unfeasible. Vehicles or heaters may require modification to use coal-derived fuels. Coal is an impure fuel and produces more greenhouse gas and pollution than an equivalent amount of petroleum or natural gas. For instance, the operation of a 1000-MWe coal-fired power plant results in a nuclear radiation dose of 490 person-rem/year, compared to 136 person-rem/year, for an equivalent nuclear power plant including uranium mining, reactor operation and waste disposal.\n\nCoal is delivered by highway truck, rail, barge, collier ship or coal slurry pipeline. Some plants are even built near coal mines and coal is delivered by conveyors. A large coal train called a \"unit train\" may be long, containing 130-140 cars with around of coal in each one, for a total load of over . A large plant under full load requires at least one coal delivery this size every day. Plants may get as many as three to five trains a day, especially in \"peak season\" during the hottest summer or coldest winter months (depending on local climate) when power consumption is high. A large thermal power plant such as the now decommissioned Nanticoke, Ontario stores several million metric tons of coal for winter use when the lakes are frozen.\n\nModern unloaders use rotary dump devices, which eliminate problems with coal freezing in bottom dump cars. The unloader includes a train positioner arm that pulls the entire train to position each car over a coal hopper. The dumper clamps an individual car against a platform that swivels the car upside down to dump the coal. Swiveling couplers enable the entire operation to occur while the cars are still coupled together. Unloading a unit train takes about three hours.\n\nShorter trains may use railcars with an \"air-dump\", which relies on air pressure from the engine plus a \"hot shoe\" on each car. This \"hot shoe\" when it comes into contact with a \"hot rail\" at the unloading trestle, shoots an electric charge through the air dump apparatus and causes the doors on the bottom of the car to open, dumping the coal through the opening in the trestle. Unloading one of these trains takes anywhere from an hour to an hour and a half. Older unloaders may still use manually operated bottom-dump rail cars and a \"shaker\" attached to dump the coal. Generating stations adjacent to a mine may receive coal by conveyor belt or massive diesel-electric-drive trucks.\n\nA collier (cargo ship carrying coal) may hold 40,000 long tons of coal and takes several days to unload. Some colliers carry their own conveying equipment to unload their own bunkers; others depend on equipment at the plant. For transporting coal in calmer waters, such as rivers and lakes, flat-bottomed barges are often used. Barges are usually unpowered and must be moved by tugboats or towboats.\n\nFor start up or auxiliary purposes, the plant may use fuel oil as well. Fuel oil can be delivered to plants by pipeline, tanker, tank car or truck. Oil is stored in vertical cylindrical steel tanks with capacities as high as ' worth. The heavier no. 5 \"bunker\" and no. 6 fuels are typically steam-heated before pumping in cold climates.\n\nCoal is prepared for use by crushing the rough coal to pieces less than in size. The coal is then transported from the storage yard to in-plant storage silos by conveyor belts at rates up to 4,000 short tons per hour.\n\nIn plants that burn pulverized coal, silos feed coal to pulverizers (coal mills) that take the larger pieces, grind them to the consistency of talcum powder, sort them, and mix them with primary combustion air which transports the coal to the boiler furnace and preheats the coal in order to drive off excess moisture content. A 500 MWe plant may have six such pulverizers, five of which can supply coal to the furnace at 250 tons per hour under full load.\n\nIn plants that do not burn pulverized coal, the larger pieces may be directly fed into the silos which then feed either mechanical distributors that drop the coal on a traveling grate or the cyclone burners, a specific kind of combustor that can efficiently burn larger pieces of fuel.\n\nCombined heat and power (CHP), also known as cogeneration, is the use of a thermal power station to provide both electric power and heat (the latter being used, for example, for district heating purposes). This technology is practiced not only for domestic heating (low temperature) but also for industrial process heat, which is often high temperature heat. Calculations show that Combined Heat and Power District Heating (CHPDH) is the cheapest method in reducing (but not eliminating) carbon emissions, if conventional fossil fuels remain to be burned.\n\nOne type of fossil fuel power plant uses a gas turbine in conjunction with a heat recovery steam generator (HRSG). It is referred to as a combined cycle power plant because it combines the Brayton cycle of the gas turbine with the Rankine cycle of the HRSG. The thermal efficiency of these plants has reached a record heat rate of 5690 Btu/(kW·h), or just under 60%, at a facility in Baglan Bay, Wales.\n\nThe turbines are fueled either with natural gas, syngas or fuel oil. While more efficient and faster to construct (a 1,000 MW plant may be completed in as little as 18 months from start of construction), the economics of such plants is heavily influenced by the volatile cost of fuel, normally natural gas. The combined cycle plants are designed in a variety of configurations composed of the number of gas turbines followed by the steam turbine. For example, a 3-1 combined cycle facility has three gas turbines tied to one steam turbine. The configurations range from (1-1), (2-1), (3-1), (4-1), (5-1), to (6-1)\n\nSimple-cycle or open cycle gas turbine plants, without a steam cycle, are sometimes installed as emergency or peaking capacity; their thermal efficiency is much lower. The high running cost per hour is offset by the low capital cost and the intention to run such units only a few hundred hours per year. Other gas turbine plants are installed in stages, with an open cycle gas turbine the first stage and additional turbines or conversion to a closed cycle part of future project plans.\n\nThe dash for gas occurred in the 1990s and was when 30 gas-fired power stations were built in Britain due to plentiful gas supplies from North Sea oil wells. According to the 2012 forecast by the U.S. Energy Information Administration, 27 gigawatts of capacity from coal-fired generators is to be retired from 175 US coal-fired power plants before 2016. Natural gas showed a corresponding jump, increasing by a third over 2011. Some coal power plants such as the 1200 MW Hearn Generating Station have stopped burning coal by switching the plant to natural gas. Coal's share of electricity generation dropped to just over 36%. Natural gas accounted for 81% of new power generation in the US between 2000 and 2010. Coal-fired generation puts out about twice the amount of carbon dioxide - around 2,000 pounds for every megawatt hour generated - than electricity generated by burning natural gas at 1,100 pounds of greenhouse gas per megawatt hour. As the fuel mix in the United States has changed to reduce coal and increase natural gas generation, carbon dioxide emissions have unexpectedly fallen. Carbon dioxide measured in the first quarter of 2012 was the lowest recorded of any year since 1992. The list of natural gas power stations has over 100 power stations that generate between 100 MW and 5,600 MW of electricity. Natural gas plants are increasing in popularity and in 2014 generated 22% of the worlds total electricity.\n\nDiesel engine generator sets are often used for prime power in communities not connected to a widespread power grid. Emergency (standby) power systems may use reciprocating internal combustion engines operated by fuel oil or natural gas. Standby generators may serve as emergency power for a factory or data center, or may also be operated in parallel with the local utility system to reduce peak power demand charge from the utility. Diesel engines can produce strong torque at relatively low rotational speeds, which is generally desirable when driving an alternator, but diesel fuel in long-term storage can be subject to problems resulting from water accumulation and chemical decomposition. Rarely used generator sets may correspondingly be installed as natural gas or LPG to minimize the fuel system maintenance requirements.\n\nSpark-ignition internal combustion engines operating on gasoline (petrol), propane, or LPG are commonly used as portable temporary power sources for construction work, emergency power, or recreational uses.\n\nReciprocating external combustion engines such as the Stirling engine can be run on a variety of fossil fuels, as well as renewable fuels or industrial waste heat. Installations of Stirling engines for power production are relatively uncommon.\n\nThermal power plants are one of the main artificial sources of producing toxic gases and particulate matter. Fossil fuel power plants cause the emission of pollutants such as NOx, SOx, CO2, CO, PM, organic gases and polycyclic aromatic hydrocarbons. World organizations and international agencies, like the IEA, are concerned about the environmental impact of burning fossil fuels, and coal in particular. The combustion of coal contributes the most to acid rain and air pollution, and has been connected with global warming. Due to the chemical composition of coal there are difficulties in removing impurities from the solid fuel prior to its combustion. Modern day coal power plants pollute less than older designs due to new \"scrubber\" technologies that filter the exhaust air in smoke stacks; however emission levels of various pollutants are still on average several times greater than natural gas power plants. In these modern designs, pollution from coal-fired power plants comes from the emission of gases such as carbon dioxide, nitrogen oxides, and sulfur dioxide into the air.\n\nAcid rain is caused by the emission of nitrogen oxides and sulfur dioxide. These gases may be only mildly acidic themselves, yet when they react with the atmosphere, they create acidic compounds such as sulfurous acid, nitric acid and sulfuric acid which fall as rain, hence the term acid rain. In Europe and the U.S.A., stricter emission laws and decline in heavy industries have reduced the environmental hazards associated with this problem, leading to lower emissions after their peak in 1960s.\n\nIn 2008, the European Environment Agency (EEA) documented fuel-dependent emission factors based on actual emissions from power plants in the European Union.\n\nElectricity generation using carbon-based fuels is responsible for a large fraction of carbon dioxide (CO) emissions worldwide and for 34% of U.S. man-made carbon dioxide emissions in 2010. In the U.S. 70% of electricity is generated by combustion of fossil fuels.\n\nCoal contains more carbon than oil or natural gas fossil fuels, resulting in greater volumes of carbon dioxide emissions per unit of electricity generated. In 2010, coal contributed about 81% of CO emissions from generation and contributed about 45% of the electricity generated in the United States. In 2000, the carbon intensity (CO emissions) of U.S. coal thermal combustion was 2249 lbs/MWh (1,029 kg/MWh) while the carbon intensity of U.S. oil thermal generation was 1672 lb/MWh (758 kg/MWh or 211 kg/GJ) and the carbon intensity of U.S. natural gas thermal production was 1135 lb/MWh (515 kg/MWh or 143 kg/GJ).\n\nThe Intergovernmental Panel on Climate Change (IPCC) reports that increased quantities of the greenhouse gas carbon dioxide within the atmosphere will \"very likely\" lead to higher average temperatures on a global scale (global warming). Concerns regarding the potential for such warming to change the global climate prompted IPCC recommendations calling for large cuts to CO emissions worldwide.\n\nEmissions can be reduced with higher combustion temperatures, yielding more efficient production of electricity within the cycle. Carbon capture and storage (CCS) of emissions from coal-fired power stations is another alternative but the technology is still being developed and will increase the cost of fossil fuel-based production of electricity. CCS may not be economically viable, unless the price of emitting CO to the atmosphere rises.\n\nAnother problem related to coal combustion is the emission of particulates that have a serious impact on public health. Power plants remove particulate from the flue gas with the use of a bag house or electrostatic precipitator. Several newer plants that burn coal use a different process, Integrated Gasification Combined Cycle in which synthesis gas is made out of a reaction between coal and water. The synthesis gas is processed to remove most pollutants and then used initially to power gas turbines. Then the hot exhaust gases from the gas turbines are used to generate steam to power a steam turbine. The pollution levels of such plants are drastically lower than those of \"classic\" coal power plants.\n\nParticulate matter from coal-fired plants can be harmful and have negative health impacts. Studies have shown that exposure to particulate matter is related to an increase of respiratory and cardiac mortality. Particulate matter can irritate small airways in the lungs, which can lead to increased problems with asthma, chronic bronchitis, airway obstruction, and gas exchange.\n\nThere are different types of particulate matter, depending on the chemical composition and size. The dominant form of particulate matter from coal-fired plants is coal fly ash, but secondary sulfate and nitrate also comprise a major portion of the particulate matter from coal-fired plants. Coal fly ash is what remains after the coal has been combusted, so it consists of the incombustible materials that are found in the coal.\n\nThe size and chemical composition of these particles affects the impacts on human health. Currently coarse (diameter greater than 2.5 μm) and fine (diameter between 0.1 μm and 2.5 μm) particles are regulated, but ultrafine particles (diameter less than 0.1 μm) are currently unregulated, yet they pose many dangers. Unfortunately much is still unknown as to which kinds of particulate matter pose the most harm, which makes it difficult to come up with adequate legislation for regulating particulate matter.\n\nThere are several methods of helping to reduce the particulate matter emissions from coal-fired plants. Roughly 80% of the ash falls into an ash hopper, but the rest of the ash then gets carried into the atmosphere to become coal-fly ash. Methods of reducing these emissions of particulate matter include:\nThe baghouse has a fine filter that collects the ash particles, electrostatic precipitators use an electric field to trap ash particles on high-voltage plates, and cyclone collectors use centrifugal force to trap particles to the walls. A recent study indicates that sulfur emissions from fossil fueled power stations in China may have caused a 10-year lull in global warming (1998-2008).\n\nCoal is a sedimentary rock formed primarily from accumulated plant matter, and it includes many inorganic minerals and elements which were deposited along with organic material during its formation. As the rest of the Earth's crust, coal also contains low levels of uranium, thorium, and other naturally occurring radioactive isotopes whose release into the environment leads to radioactive contamination. While these substances are present as very small trace impurities, enough coal is burned that significant amounts of these substances are released. A 1,000 MW coal-burning power plant could have an uncontrolled release of as much as 5.2 metric tons per year of uranium (containing of uranium-235) and 12.8 metric tons per year of thorium. In comparison, a 1,000 MW nuclear plant will generate about 30 metric tons of high-level radioactive solid packed waste per year. It is estimated that during 1982, US coal burning released 155 times as much uncontrolled radioactivity into the atmosphere as the Three Mile Island incident. The collective radioactivity resulting from all coal burning worldwide between 1937 and 2040 is estimated to be 2,700,000 curies or 0.101 EBq. During normal operation, the effective dose equivalent from coal plants is 100 times that from nuclear plants. Normal operation however, is a deceiving baseline for comparison: just the Chernobyl nuclear disaster released, in iodine-131 alone, an estimated 1.76 EBq. of radioactivity, a value one order of magnitude above this value for total emissions from all coal burned within a century, while the iodine-131, the major radioactive substance which comes out in accident situations, has a half life of just 8 days.\n\nA study released in August 2010 that examined state pollution data in the United States by the organizations Environmental Integrity Project, the Sierra Club and Earthjustice found that coal ash produced by coal-fired power plants dumped at sites across 21 U.S. states has contaminated ground water with toxic elements. The contaminants including the poisons arsenic and lead.\n\nArsenic has been shown to cause skin cancer, bladder cancer and lung cancer, and lead damages the nervous system. Coal ash contaminants are also linked to respiratory diseases and other health and developmental problems, and have disrupted local aquatic life. Coal ash also releases a variety of toxic contaminants into nearby air, posing a health threat to those who breath in fugitive coal dust.\nCurrently, the EPA does not regulate the disposal of coal ash; regulation is up to the states and the electric power industry has been lobbying to maintain this status quo. Most states require no monitoring of drinking water near coal ash dump sites. The study found an additional 39 contaminated U.S. sites and concluded that the problem of coal ash-caused water contamination is even more extensive in the United States than has been estimated. The study brought to 137 the number of ground water sites across the United States that are contaminated by power plant-produced coal ash.\n\nU.S. government scientists tested fish in 291 streams around the country for mercury contamination. They found mercury in every fish tested, according to the study by the U.S. Department of the Interior. They found mercury even in fish of isolated rural waterways. Twenty five percent of the fish tested had mercury levels above the safety levels determined by the U.S. Environmental Protection Agency for people who eat the fish regularly. The largest source of mercury contamination in the United States is coal-fueled power plant emissions.\n\nSeveral methods exist to improve the efficiency of fossil fuel power plants. A frequently used and cost-efficient method is to convert a plant to run on a different fuel. This includes conversions of coal power plants to biomass or waste and conversions of natural gas power plants to biogas. Conversions of coal powered power plants to waste-fired power plants have an extra benefit in that they can reduce landfilling. In addition, waste-fired power plants can be equipped with material recovery, which is also beneficial to the environment. In some instances, torrefaction of biomass may be needed if biomass is the material the converted fossil fuel power plant will be using.\n\nImproving energy efficiency of a coal-fired power plant also reduces emissions. For example, emissions can be reduced by upgrading existing plants or building new high-efficiency, low-emissions plants. Such plants emit almost 20% less CO than a subcritical unit operating at a similar load. Over the longer term, HELE plants can further facilitate emission reductions because coal-fired plants operating at the highest efficiencies are also the most appropriate option for carbon capture and storage retrofit.\n\nRegardless of the conversion, a truly low-carbon fossil fuel power plant implements carbon capture and storage, which means that the exhaust CO is not released into the environment and the fossil fuel power plant becomes an emissionless power plant. A 2006 example of a carbon capture and storage fossil fuel power plant is the pilot Elsam power station near Esbjerg, Denmark.\n\nCoal Pollution Mitigation is a process whereby coal is chemically washed of minerals and impurities, sometimes gasified, burned and the resulting flue gases treated with steam, with the purpose of removing sulfur dioxide, and reburned so as to make the carbon dioxide in the flue gas economically recoverable, and storable underground (the latter of which is called \"carbon capture and storage\"). The coal industry uses the term \"clean coal\" to describe technologies designed to enhance both the efficiency and the environmental acceptability of coal extraction, preparation and use, but has provided no specific quantitative limits on any emissions, particularly carbon dioxide. Whereas contaminants like sulfur or mercury can be removed from coal, carbon cannot be effectively removed while still leaving a usable fuel, and clean coal plants without carbon sequestration and storage do not significantly reduce carbon dioxide emissions. James Hansen in an open letter to U.S. President Barack Obama has advocated a \"moratorium and phase-out of coal plants that do not capture and store CO\". In his book \"Storms of My Grandchildren\", similarly, Hansen discusses his \"Declaration of Stewardship\" the first principle of which requires \"a moratorium on coal-fired power plants that do not capture and sequester carbon dioxide\".\n\nGas-fired power plants can also be modified to run on hydrogen, the latter of which can be created on-site from natural gas. Since 2013, the conversion process has been improved by scientists at Karlsruhe Liquid-metal Laboratory (KALLA) as they succeeded in allowing the soot to be easily removed (soot is a byproduct of the process and damaged the working parts in the past -most notably the nickel-iron-cobaltcatalyst-). The soot (which contains the carbon) can then be stored underground and is not released into the atmosphere.\n\nAlternatives to fossil fuel power plants include nuclear power, solar power, geothermal power, wind power, hydropower, biomass power plants and other renewable energies (see non-carbon economy). Some of these are proven technologies on an industrial scale (i.e. nuclear, wind, tidal, hydroelectric and biomass fired power) others are still in prototype form.\n\nNuclear power, and geothermal power may be classed as heat pollutants as they add heat energy to the biosphere that would not otherwise be released. The net quantity of energy conversion within the biosphere due to the utilisation of wind power, solar power, tidal power, hydroelectric power (hydroelectricity) is static and is derived from the effects of sunlight and the movement of the moon and planets.\n\nGenerally, the cost of electrical energy produced by non fossil fuel burning power plants is greater than that produced by burning fossil fuels. This statement however only includes the cost to produce the electrical energy and does not take into account indirect costs associated with the many pollutants created by burning fossil fuels (e.g. increased hospital admissions due to respiratory diseases caused by fine smoke particles).\n\nWhen comparing power plant costs, it is customary to start by calculating the cost of power at the generator terminals by considering several main factors. External costs such as connections costs, the effect of each plant on the distribution grid are considered separately as an additional cost to the calculated power cost at the terminals.\n\nInitial factors considered are:\n\nThese costs occur over the 30–50 year life of the fossil fuel power plants, using discounted cash flows. In general large fossil plants are attractive due to their low initial capital costs—typically around £750–£1000 per kilowatt electrical compared to perhaps £1500 per kilowatt for onshore wind.\n\n\n\n \n"}
{"id": "37562074", "url": "https://en.wikipedia.org/wiki?curid=37562074", "title": "High-temperature operating life", "text": "High-temperature operating life\n\nHigh-temperature operating life (HTOL) is a reliability test applied to integrated circuits (ICs) to determine their intrinsic reliability. This test stresses the IC at an elevated temperature, high voltage and dynamic operation for a predefined period of time. The IC is usually monitored under stress and tested at intermediate intervals. This reliability stress test is sometimes referred to as a \"lifetime test\", \"device life test\" or \"extended burn in test\" and is used to trigger potential failure modes and assess IC lifetime.\n\nThere are several types of HTOL:\n\nThe main aim of the HTOL is to age the device such that a short experiment will allow the lifetime of the IC to be predicted (e.g. 1,000 HTOL hours shall predict a minimum of \"X\" years of operation). Good HTOL process shall avoid relaxed HTOL operation and also prevents overstressing the IC. This method ages all IC's building blocks to allow relevant failure modes to be triggered and implemented in a short reliability experiment. A precise multiplier, known as the Acceleration Factor (AF) simulates long lifetime operation.\n\nThe AF represents the accelerated aging factor relative to the useful life application conditions.\n\nFor effective HTOL stress testing, several variables should be considered:\n\nA detailed description of the above variables, using a hypothetical, simplified IC with several RAMs, digital logic, an analog voltage regulator module and I/O ring, together with the HTOL design considerations for each are provided below.\n\nThe digital toggling factor (DTF) represents the number of transistors that change their state during the stress test, relative to the total number of gates in the digital portion of the IC. In effect, the DTF is the percentage of transistors toggling in one time unit. The time unit is relative to the toggling frequency, and is usually limited by the HTOL setup to be in the range of 10–20Mhz.\n\nReliability engineers strive to toggle as many as possible transistors for each time unit of measure. The RAMs (and other memory types) are usually activated using the BIST function, while the logic is usually activated with the SCAN function, LFSR or logic BIST.\n\nThe power and the self-heating of the digital portion of the IC are evaluated and the device's aging estimated. These two measures are aligned so that they are similar to the aging of other elements of the IC. The degrees of freedom for aligning these measures are the voltage stress and/or the time period during which the HTOL program loops these blocks relative to other IC blocks.\n\nThe recent trend of integrating as many electronic components as possible into a single chip is known as system on a chip (SoC).\n\nThis trend complicates reliability engineers' work because (usually) the analog portion of the chip dissipates higher power relative to the other IC elements.\n\nThis higher power may generate hot spots and areas of accelerated aging. Reliability engineers must understand the power distribution on the chip and align the aging so that it is similar for all elements of an IC.\n\nIn our hypothetical SoC the analog module only includes a voltage regulator. In reality, there may be additional analog modules e.g. PMIC, oscillators, or charge pumps. To perform efficient stress tests on the analog elements, reliability engineers must identify the worst-case scenario for the relevant analog blocks in the IC. For example, the worst-case scenario for voltage regulators may be the maximum regulation voltage and maximum load current; for charge pumps it may be the minimum supply voltage and maximum load current.\n\nGood engineering practice calls for the use of external loads (external R,L,C) to force the necessary currents. This practice avoids loading differences due to the chip's different operational schemes and operation trimming of its analog parts.\n\nStatistical methods are used to check statistical tolerances, variation and temperature stability of the loads used, and to define the right confidence bands for the loads to avoid over/under stress at HTOL operating range. The degrees of freedom for aligning the aging magnitude of analog parts is usually the duty-cycle, external load values and voltage stress.\n\nThe interface between the \"outside world\" and the IC is made via the input/output (I/O) ring. This ring contains power I/O ports, digital I/O ports and analog I/O ports. The I/Os are (usually) wired via the IC package to the \"outside world\" and each I/O executes its own specific command instructions, e.g. JTAG ports, IC power supply ports etc. Reliability engineering aims to age all I/Os in the same way as the other IC elements. This can be achieved by using a Boundary scan operation.\n\nAs previously mentioned, the main aim of the HTOL is aging the samples by dynamic stress at elevated voltage and/or temperature. During the HTOL operation, we need to assure that the IC is active, toggling and constantly functioning.\n\nAt the same time, we need to know at what point the IC stops responding, these data are important for calculating price reliability indices and for facilitating the FA. This is done by monitoring the device via one or more vital IC parameters signals communicated and logged by the HTOL machine and providing continuous indication about the IC's functionality throughout the HTOL run time. Examples of commonly used monitors include the BIST \"done\" flag signal, the SCAN output chain or the analog module output.\n\nThere are three types of monitoring: \n\nAccording to JEDEC standards, the environmental chamber should be capable of maintaining the specified temperature within a tolerance of ±5 °C throughout while parts are loaded and unpowered. Today's environmental chambers have better capabilities and can exhibit temperature stability within a range of ±3 °C throughout. \n\nLow power ICs can be stressed without major attention to self-heating affects. However, due to technology scaling and manufacturing variations, power dissipation within a single production lot of devices can vary by as much as 40%. This variation, in addition to high power IC makes advanced contact temperature controls necessary for facilitating individual control systems for each IC\n\nThe operating voltage should be at least the maximum specified for the device. In some cases a higher voltage is applied to obtain lifetime acceleration from voltage as well as temperature. \n\nTo define the maximum permitted voltage stress, the following methods can be considered:\n\nReliability engineers must check that V does not exceed the maximum rated voltage for the relevant technology, as specified by the FAB.\n\nThe Acceleration factor (AF) is a multiplier that relates a product’s life at an accelerated stress level to the life at the use stress level. \n\nAn AF of 20 means 1 hour at stress condition is equivalent to 20 hours at useful condition. \n\nThe voltage acceleration factor is represented by AFv. Usually the stress voltage is equal to or higher than the maximum voltage. An elevated voltage provides additional acceleration and can be used to increase effective device hours or achieve an equivalent life point.\n\nThere are several AFv models: \n\nAFtemp is the acceleration factor due to changes in temperature and is usually based on the Arrhenius equation. The total acceleration factor is the product of AFv and AFtemp\n\nThe reliability test duration assures the device’s adequate lifetime requirement. \n\nFor example, with an activation energy of 0.7 eV, 125 °C stress temperature and 55 °C use temperature, the acceleration factor (Arrhenius equation) is 78.6. This means that 1,000 hours' stress duration is equivalent to 9 years of use. The reliability engineer decides on the qualification test duration. Industry good practice calls for 1,000 hours at a junction temperature of 125 °C.\n\nThe challenge for new reliability assessment and qualification systems is determining the relevant failure mechanisms to optimize sample size. \n\nSample plans are statistically derived from manufacturer risk, consumer risk, and the expected failure rate. The commonly used sampling plan of zero rejects out of 230 samples is equal to three rejects out of 668 samples assuming LTPD =1 and a 90% confidence interval.\n\nSamples shall include representative samples from at least three nonconsecutive lots to represent manufacturing variability. All test samples shall be fabricated, handled, screened and assembled in the same way as during the production phase.\n\nSamples shall be tested prior to stress and at predefined checkpoints. It is good engineering practice to test samples at maximum and minimum rating temperatures as well as at room temperature. Data logs of all functional and parametric tests shall be collated for further analysis. \n\nAssuming Tj = 125 °C, commonly used checkpoints are after 48, 168, 500 and 1,000 hours. \n\nDifferent checkpoints for different temperatures can be calculated by using the Arrhenius equation. For example, with an activation energy of 0.7e V, T of 135°C and T of 55 °C the equivalent checkpoints will be at 29, 102, 303 and 606 hours. \n\nElectrical testing should be completed as soon as possible after the samples are removed. If the samples cannot be tested soon after their removal, additional stress time should be applied. The JEDEC standard requires samples be tested within 168 hours of removal. \n\nIf testing exceeds the recommended time window, additional stress should be applied according to the table below:\n\nThe merit number is the outcome of statistical sampling plans.\n\nSampling plans are inputted to SENTENCE, an audit tool, to ensure that the output of a process meets the requirements. SENTENCE simply accepts or rejects the tested lots. The reliability engineer implements statistical sampling plans based on predefined Acceptance Quality Limits, LTPD, manufacturer risk and customer risk. For example, the commonly used sampling plan of 0 rejects out of 230 samples is equal to 3 rejects out of 668 samples assuming LTPD=1.\n\nThe aging process of an IC is relative to its standard use conditions. The tables below provide reference to various commonly used products and the conditions under which they are used.\n\nReliability engineers are tasked with verifying the adequate stress duration. For example, for an activation energy of 0.7eV, a stress temperature of 125 °C and a use temperature of 55 °C, an expected operational life of five years is represented by a 557-hour HTOL experiment.\n\nExample Automotive Use Conditions\nExample European Telecom use Conditions definition\n\nExample US Telecom use conditions definition\nExample military use conditions\n\n"}
{"id": "23344482", "url": "https://en.wikipedia.org/wiki?curid=23344482", "title": "Ice pond", "text": "Ice pond\n\nAn ice pond is a large volume of ice or snow produced by natural winter freezing. The ice is then used for cooling or air conditioning.\n\nBefore refrigeration was common, ice ponds were mined by ice companies, with product transported to consumers and food businesses through much of the year. Refrigeration technology replaced this technology.\n\nIn more recent times, ice ponds have been revived as an environmentally friendly way to air condition buildings in the summer. The best known experiment is the 'Princeton ice pond' by Ted Taylor in 1981. He then persuaded the Prudential Insurance Company to use a bigger pond to provide air conditioning for a larger building. Taylor also investigated the possibility of using the technology for water purification, which he demonstrated during a non-fiction segment on the 1984 educational series \"The Voyage of the Mimi'.\n\n"}
{"id": "7067473", "url": "https://en.wikipedia.org/wiki?curid=7067473", "title": "Industrial applications of nanotechnology", "text": "Industrial applications of nanotechnology\n\nNanotechnology is impacting the field of consumer goods, several products that incorporate nanomaterials are already in a variety of items; many of which people do not even realize contain nanoparticles, products with novel functions ranging from easy-to-clean to scratch-resistant. Examples of that car bumpers are made lighter, clothing is more stain repellant, sunscreen is more radiation resistant, synthetic bones are stronger, cell phone screens are lighter weight, glass packaging for drinks leads to a longer shelf-life, and balls for various sports are made more durable. Using nanotech, in the mid-term modern textiles will become \"smart\", through embedded \"wearable electronics\", such novel products have also a promising potential especially in the field of cosmetics, and has numerous potential applications in heavy industry. Nanotechnology is predicted to be a main driver of technology and business in this century and holds the promise of higher performance materials, intelligent systems and new production methods with significant impact for all aspects of society.\n\nA complex set of engineering and scientific challenges in the food and bioprocessing industry for manufacturing high quality and safe food through efficient and sustainable means can be solved through nanotechnology. Bacteria identification and food quality monitoring using biosensors; intelligent, active, and smart food packaging systems; nanoencapsulation of bioactive food compounds are few examples of emerging applications of nanotechnology for the food industry. Nanotechnology can be applied in the production, processing, safety and packaging of food. A nanocomposite coating process could improve food packaging by placing anti-microbial agents directly on the surface of the coated film.\nNanocomposites could increase or decrease gas permeability of different fillers as is needed for different products. They can also improve the mechanical and heat-resistance properties and lower the oxygen transmission rate. Research is being performed to apply nanotechnology to the detection of chemical and biological substances for sensanges in foods.\n\nIn general, food substances are not allowed to be adulterated, according to the Food, Drug and Cosmetic Act (section 402). Additives to food must conform to all regulations in the food additives amendment of 1958 as well as the FDA Modernization Act of 1997. In addition, color additives are obliged to comply with all regulations stipulated by the Color Additive Amendments of 1960. A safety assessment must be performed on all food substances for submission and approval by the US FDA. The mandatory information in this assessment includes the identity, technical effects, self-limiting levels of use, dietary exposure and safety studies for the manufacturing processes used, including the use of nanotechnology. Food manufacturers are obliged to assess whether the identity, safety or regulatory status of a food substance is affected by significant changes in manufacturing processes, such as the use of nanotechnology. In their guidance document published in April 2012, the US FDA discusses what considerations and recommendations may apply to such an assessment.\n\nNew foods are among the nanotechnology-created consumer products coming onto the market at the rate of 3 to 4 per week, according to the Project on Emerging Nanotechnologies (PEN), based on an inventory it has drawn up of 609 known or claimed nano-products. On PEN's list are three foods—a brand of canola cooking oil called Canola Active Oil, a tea called Nanotea and a chocolate diet shake called Nanoceuticals Slim Shake Chocolate. According to company information posted on PEN's Web site, the canola oil, by Shemen Industries of Israel, contains an additive called \"nanodrops\" designed to carry vitamins, minerals and phytochemicals through the digestive system and urea. The shake, according to U.S. manufacturer RBC Life Sciences Inc., uses cocoa infused \"NanoClusters\" to enhance the taste and health benefits of cocoa without the need for extra sugar.\n\nThe most prominent application of nanotechnology in the household is self-cleaning or \"easy-to-clean\" surfaces on ceramics or glasses. Nanoceramic particles have improved the smoothness and heat resistance of common household equipment such as the flat iron.\n\nThe first sunglasses using protective and anti-reflective ultrathin polymer coatings are on the market. For optics, nanotechnology also offers scratch resistant surface coatings based on nanocomposites. Nano-optics could allow for an increase in precision of pupil repair and other types of laser eye surgery.\n\nThe use of engineered nanofibers already makes clothes water- and stain-repellent or wrinkle-free. Textiles with a nanotechnological finish can be washed less frequently and at lower temperatures. Nanotechnology has been used to integrate tiny carbon particles membrane and guarantee full-surface protection from electrostatic charges for the wearer. Many other applications have been developed by research institutions such as the Textiles Nanotechnology Laboratory at Cornell University, and the UK's Dstl and its spin out company P2i.\n\nOne field of application is in sunscreens. The traditional chemical UV protection approach suffers from its poor long-term stability. A sunscreen based on mineral nanoparticles such as titanium oxide offer several advantages. Titanium oxide nanoparticles have a comparable UV protection property as the bulk material, but lose the cosmetically undesirable whitening as the particle size is decreased.\n\nNanotechnology may also play a role in sports such as soccer, football, and baseball. Materials for new athletic shoes may be made in order to make the shoe lighter (and the athlete faster). Baseball bats already on the market are made with carbon nanotubes that reinforce the resin, which is said to improve its performance by making it lighter. Other items such as sport towels, yoga mats, exercise mats are on the market and used by players in the National Football League, which use antimicrobial nanotechnology to prevent parasuram from illnesses caused by bacteria such as Methicillin-resistant Staphylococcus aureus (commonly known as MRSA).\n\nLighter and stronger materials will be of immense use to aircraft manufacturers, leading to increased performance. Spacecraft will also benefit, where weight is a major factor. Nanotechnology might thus help to reduce the size of equipment and thereby decrease fuel-consumption required to get it airborne. Hang gliders may be able to halve their weight while increasing their strength and toughness through the use of nanotech materials. Nanotech is lowering the mass of supercapacitors that will increasingly be used to give power to assistive electrical motors for launching hang gliders off flatland to thermal-chasing altitudes.\n\nMuch like aerospace, lighter and stronger materials would be useful for creating vehicles that are both faster and safer. Combustion engines might also benefit from parts that are more hard-wearing and more heat-resistant.\n\nNanotechnology can improve the military’s ability to detect biological agents. By using nanotechnology, the military would be able to create sensor systems that could detect biological agents. The sensor systems are already well developed and will be one of the first forms of nanotechnology that the military will start to use.\n\nNanoparticles can be injected into the material on soldiers’ uniforms to not only make the material more durable, but also to protect soldiers from many different dangers such as high temperatures, impacts and chemicals. The nanoparticles in the material protect soldiers from these dangers by grouping together when something strikes the armor and stiffening the area of impact. This stiffness helps lessen the impact of whatever hit the armor, whether it was extreme heat or a blunt force. By reducing the force of the impact, the nanoparticles protect the soldier wearing the uniform from any injury the impact could have caused.\n\nAnother way nanotechnology can improve soldiers’ uniforms is by creating a better form of camouflage. Mobile pigment nanoparticles injected into the material can produce a better form of camouflage. These mobile pigment particles would be able to change the color of the uniforms depending upon the area that the soldiers are in. There is still much research being done on this self-changing camouflage.\n\nNanotechnology can improve thermal camouflage. Thermal camouflage helps protect soldiers from people who are using night vision technology. Surfaces of many different military items can be designed in a way that electromagnetic radiation can help lower the infrared signatures of the object that the surface is on. Surfaces of soldiers’ uniforms and surfaces of military vehicle are a few surfaces that can be designed in this way. By lowering the infrared signature of both the soldiers and the military vehicles the soldiers are using, it will provide better protection from infrared guided weapons or infrared surveillance sensors.\n\nThere is a way to use nanoparticles to create coated polymer threads that can be woven into soldiers’ uniforms. These polymer threads could be used as a form of communication between the soldiers. The system of threads in the uniforms could be set to different light wavelengths, eliminating the ability for anyone else to listen in. This would lower the risk of having anything intercepted by unwanted listeners.\n\nA medical surveillance system for soldiers to wear can be made using nanotechnology. This system would be able to watch over their health and stress levels. The systems would be able to react to medical situations by releasing drugs or compressing wounds as necessary. This means that if the system detected an injury that was bleeding, it would be able to compress around the wound until further medical treatment could be received. The system would also be able to release drugs into the soldier’s body for health reasons, such as pain killers for an injury. The system would be able to inform the medics at base of the soldier’s health status at all times that the soldier is wearing the system. The energy needed to communicate this information back to base would be produced through the soldier’s body movements.\n\nNanoweapon is the name given to military technology currently under development which seeks to exploit the power of nanotechnology in the modern battlefield.\n\n\nChemical catalysis benefits especially from nanoparticles, due to the extremely large surface-to-volume ratio. The application potential of nanoparticles in catalysis ranges from fuel cell to catalytic converters and photocatalytic devices. Catalysis is also important for the production of chemicals. For example, nanoparticles with a distinct chemical surrounding (ligands), or specific optical properties.\n\nPlatinum nanoparticles are being considered in the next generation of automotive catalytic converters because the very high surface area of nanoparticles could reduce the amount of platinum required. However, some concerns have been raised due to experiments demonstrating that they will spontaneously combust if methane is mixed with the ambient air. Ongoing research at the Centre National de la Recherche Scientifique (CNRS) in France may resolve their true usefulness for catalytic applications. Nanofiltration may come to be an important application, although future research must be careful to investigate possible toxicity.\n\nNanotechnology has the potential to make construction faster, cheaper, safer, and more varied. Automation of nanotechnology construction can allow for the creation of structures from advanced homes to massive skyscrapers much more quickly and at much lower cost. In the near future,\nNanotechnology can be used to sense cracks in foundations of architecture and can send nanobots to repair them.\n\nNanotechnology is an active research area that encompasses a number of disciplines such as electronics, bio-mechanics and coatings. These disciplines assist in the areas of civil engineering and construction materials. If nanotechnology is implemented in the construction of homes and infrastructure, such structures will be stronger. If buildings are stronger, then fewer of them will require reconstruction and less waste will be produced.\n\nNanotechnology in construction involves using nanoparticles such as alumina and silica. Manufacturers are also investigating the methods of producing nano-cement. If cement with nano-size particles can be manufactured and processed, it will open up a large number of opportunities in the fields of ceramics, high strength composites and electronic applications. \nNanomaterials still have a high cost relative to conventional materials, meaning that they are not likely to feature in high-volume building materials. The day when this technology slashes the consumption of structural steel has not yet been contemplated.\n\nMuch analysis of concrete is being done at the nano-level in order to understand its structure. Such analysis uses various techniques developed for study at that scale such as Atomic Force Microscopy (AFM), Scanning Electron Microscopy (SEM) and Focused Ion Beam (FIB). This has come about as a side benefit of the development of these instruments to study the nanoscale in general, but the understanding of the structure and behavior of concrete at the fundamental level is an important and very appropriate use of nanotechnology. One of the fundamental aspects of nanotechnology is its interdisciplinary nature and there has already been cross over research between the mechanical modeling of bones for medical engineering to that of concrete which has enabled the study of chloride diffusion in concrete (which causes corrosion of reinforcement). Concrete is, after all, a macro-material strongly influenced by its nano-properties and understanding it at this new level is yielding new avenues for improvement of strength, durability and monitoring as outlined in the following paragraphs\n\nSilica (SiO2) is present in conventional concrete as part of the normal mix. However, one of the advancements made by the study of concrete at the nanoscale is that particle packing in concrete can be improved by using nano-silica which leads to a densifying of the micro and nanostructure resulting in improved mechanical properties. Nano-silica addition to cement based materials can also control the degradation of the fundamental C-S-H (calcium-silicatehydrate) reaction of concrete caused by calcium leaching in water as well as block water penetration and therefore lead to improvements in durability. Related to improved particle packing, high energy milling of ordinary Portland cement (OPC) clinker and standard sand, produces a greater particle size diminution with respect to conventional OPC and, as a result, the compressive strength of the refined material is also 3 to 6 times higher (at different ages).\n\nSteel is a widely available material that has a major role in the construction industry. The use of nanotechnology in steel helps to improve the physical properties of steel. Fatigue, or the structural failure of steel, is due to cyclic loading. Current steel designs are based on the reduction in the allowable stress, service life or regular inspection regime. This has a significant impact on the life-cycle costs of structures and limits the effective use of resources. Stress risers are responsible for initiating cracks from which fatigue failure results. The addition of copper nanoparticles reduces the surface un-evenness of steel, which then limits the number of stress risers and hence fatigue cracking. Advancements in this technology through the use of nanoparticles would lead to increased safety, less need for regular inspection, and more efficient materials free from fatigue issues for construction.\n\nSteel cables can be strengthened using carbon nanotubes. Stronger cables reduce the costs and period of construction, especially in suspension bridges, as the cables are run from end to end of the span.\n\nThe use of vanadium and molybdenum nanoparticles improves the delayed fracture problems associated with high strength bolts. This reduces the effects of hydrogen embrittlement and improves steel micro-structure by reducing the effects of the inter-granular cementite phase.\n\nWelds and the Heat Affected Zone (HAZ) adjacent to welds can be brittle and fail without warning when subjected to sudden dynamic loading. The addition of nanoparticles such as magnesium and calcium makes the HAZ grains finer in plate steel. This nanoparticle addition leads to an increase in weld strength. The increase in strength results in a smaller resource requirement because less material is required in order to keep stresses within allowable limits.\n\nNanotechnology represents a major opportunity for the wood industry to develop new products, substantially reduce processing costs, and open new markets for biobased materials.\n\nWood is also composed of nanotubes or “nanofibrils”; namely, lignocellulosic (woody tissue) elements which are twice as strong as steel. Harvesting these nanofibrils would lead to a new paradigm in sustainable construction as both the production and use would be part of a renewable cycle. Some developers have speculated that building functionality onto lignocellulosic surfaces at the nanoscale could open new opportunities for such things as self-sterilizing surfaces, internal self-repair, and electronic lignocellulosic devices. These non-obtrusive active or passive nanoscale sensors would provide feedback on product performance and environmental conditions during service by monitoring structural loads, temperatures, moisture content, decay fungi, heat losses or gains, and loss of conditioned air. Currently, however, research in these areas appears limited.\n\nDue to its natural origins, wood is leading the way in cross-disciplinary research and modelling techniques. BASF have developed a highly water repellent coating based on the actions of the lotus leaf as a result of the incorporation of silica and alumina nanoparticles and hydrophobic polymers. Mechanical studies of bones have been adapted to model wood, for instance in the drying process.\n\nResearch is being carried out on the application of nanotechnology to glass, another important material in construction. Titanium dioxide (TiO) nanoparticles are used to coat glazing since it has sterilizing and anti-fouling properties. The particles catalyze powerful reactions that break down organic pollutants, volatile organic compounds and bacterial membranes. TiO is hydrophilic (attraction to water), which can attract rain drops that then wash off the dirt particles. Thus the introduction of nanotechnology in the Glass industry, incorporates the self-cleaning property of glass.\n\nFire-protective glass is another application of nanotechnology. This is achieved by using a clear intumescent layer sandwiched between glass panels (an interlayer) formed of silica nanoparticles (SiO), which turns into a rigid and opaque fire shield when heated. Most of glass in construction is on the exterior surface of buildings. So the light and heat entering the building through glass has to be prevented. The nanotechnology can provide a better solution to block light and heat coming through windows.\n\nCoatings is an important area in construction coatings are extensively use to paint the walls, doors, and windows. Coatings should provide a protective layer bound to the base material to produce a surface of the desired protective or functional properties. The coatings should have self healing capabilities through a process of \"self-assembly\". Nanotechnology is being applied to paints to obtained the coatings having self healing capabilities and corrosion protection under insulation. Since these coatings are hydrophobic and repels water from the metal pipe and can also protect metal from salt water attack.\n\nNanoparticle based systems can provide better adhesion and transparency. The TiO coating captures and breaks down organic and inorganic air pollutants by a photocatalytic process, which leads to putting roads to good environmental use.\n\nFire resistance of steel structures is often provided by a coating produced by a spray-on-cementitious process. The nano-cement has the potential to create a new paradigm in this area of application because the resulting material can be used as a tough, durable, high temperature coating. It provides a good method of increasing fire resistance and this is a cheaper option than conventional insulation.\n\nIn building construction nanomaterials are widely used from self-cleaning windows to flexible solar panels to wi-fi blocking paint. The self-healing concrete, materials to block ultraviolet and infrared radiation, smog-eating coatings and light-emitting walls and ceilings are the new nanomaterials in construction. Nanotechnology is a promise for making the \"smart home\" a reality. Nanotech-enabled sensors can monitor temperature, humidity, and airborne toxins, which needs nanotech-based improved batteries. The building components will be intelligent and interactive since the sensor uses wireless components, it can collect the wide range of data.\n\nIf nanosensors and nanomaterials become an everyday part of the buildings, as with smart homes, what are the consequences of these materials on human beings?\n\n\n"}
{"id": "3565119", "url": "https://en.wikipedia.org/wiki?curid=3565119", "title": "Infone", "text": "Infone\n\nInfone was a service launched by Metro One Telecommunications in 2003. The service was discontinued effective December 14, 2005.\n\nInfone included directory assistance and other services via a toll-free phone number. A user could call 888-411-1111 to request directory assistance, directions, traffic information, movie times, call completion, dinner reservation assistance and other services. \n\nInfone provided a number of innovative 411 'concierge' like services, including movie listings from a live operator, and offered a feature where they could provide information from a linked Microsoft Outlook calendar when set up in advance. For a period of time they advertised heavily on U.S. television, featuring ads with then Governor of Minnesota Jesse Ventura, emphasizing their use of all U.S. based operators. The price offered was $0.89 per call up to 15 minutes (for use when the operator connects you to the requested number, as well as for additional information requests afterwards), with $0.05 for each additional minute, making Infone also a competitively priced long distance service. New users received 5-10 free calls.\n\nInfone identified a registered user (along with billing information; the service was only payable by credit card) by caller ID (numbers were registered on signing up) and by an advanced voiceprint recognition system from SpeechWorks that identified the user when the user called from an un-registered telephone number (or no caller ID) through the use of a personal phrase spoken by the user (e.g., \"Hello Infone!\") after the welcome tone.\n"}
{"id": "14776965", "url": "https://en.wikipedia.org/wiki?curid=14776965", "title": "International Association of Online Engineering", "text": "International Association of Online Engineering\n\nThe International Association of Online Engineering (IAOE) is an international non-profit organization with the objective of encouraging the wider development, distribution and application of Online Engineering (OE) technologies and its influence to the society. The association seeks to foster practices in education and research in universities, higher education institutions and the industry on OE. Moreover, the IAOE promotes OE for the improvement of living and working conditions. The IAOE encourages the exchange of knowledge as well as the exchange of staff and students between co-operating institutions.\n\nIAOE proclaims a growing complexity of engineering tasks, increasingly specialized and expensive equipment as well as software tools and simulators. It also observes the necessary use of expensive equipment and software tools/simulators in short time projects. As a consequence, it is increasingly necessary to allow and organize a shared use of equipment, but also specialized software as for example simulators.\n\nAims of the association in general are:\n\nThis includes the promotion of:\n\nThe International Journal of Online Engineering is the official publication of IAOE.\nIt is published quarterly. The association furthermore supports the International Journal of Emerging Technologies in Learning (iJET) and the International Journal of Interactive Mobile Technologies (iJIM). IAOE also runs the scientific journal hosting platform Online-Journals.org, thus aiming at contributing to the advancement of science by providing an efficient and cost-effective way for making qualitative scientific results easily accessible to the largest possible audience.\n\nThe annual conference of the International Association of Online Engineering is the International Conference on Remote Engineering and Virtual Instrumentation (REV). Furthermore, IAOE is organizer or co-organizer of the following conferences:\nInternational Conference on Interactive Computer aided Learning (ICL), Conference on Interactive Mobile and Computer aided Learning (IMCL), Conference on Interactive Computer aided Blended Learning (ICBL).\n\n"}
{"id": "605080", "url": "https://en.wikipedia.org/wiki?curid=605080", "title": "International Color Consortium", "text": "International Color Consortium\n\nThe International Color Consortium (ICC) was formed in 1993 by eight vendors in order to create an open, vendor-neutral color management system which would function transparently across all operating systems and software packages.\n\nThe ICC specification, currently on version 4.3, allows for matching of color when moved between applications and operating systems, from the point of creation to the final output, whether display or print. This specification is technically identical to ISO 15076-1:2010, available from ISO.\n\nThe ICC profile which describe the color attributes of a particular device or viewing requirement by defining a mapping between the source or target color space and a \"profile connection space\" (PCS).\n\nThe ICC defines the format precisely but does not define algorithms or processing details. This means there is room for variation between different applications and systems that work with ICC profiles.\n\nICC has also published a preliminary specification for iccMAX, a next-generation color management architecture with significantly expanded functionality and a choice of colorimetric, spectral or material connection space. Details are at http://www.color.org/iccmax/\n\nThe eight founding members of the ICC were Adobe, Agfa, Apple, Kodak, Microsoft, Silicon Graphics, Sun Microsystems, and Taligent.\n\nSince then Sun Microsystems, Silicon Graphics, and Taligent have left the organization, and many other firms have become ICC members, including, ,\nCanon,\nFuji,\nFujitsu,\nHeidelberg Printing Machines AG,\nHewlett–Packard,\nKonica Minolta,\nKyocera,\nLexmark,\nNEC,\nNikon,\nNokia,\nOKI Data,\nSun Chemical,\nHeidelberger Druckmaschinen,\nand X-Rite.\n\nAt the beginning of 2014, ICC membership has grown to a total of 61 members, including their founding, regular, and honorary members. Aside from members of the photography, printing, and painting industry, new members from several different industries include MathWorks, Nokia, Sony Corporation, and Signazon.com.\n\n\n"}
{"id": "25283098", "url": "https://en.wikipedia.org/wiki?curid=25283098", "title": "K-distribution", "text": "K-distribution\n\nIn probability and statistics, the K-distribution is a three-parameter family of continuous probability distributions. \nThe distribution arises by compounding two gamma distributions. In each case, a re-parametrization of the usual form of the family of gamma distributions is used, such that the parameters are:\n\nThe model is that random variable formula_1 has a gamma distribution with mean formula_2 and shape parameter formula_3, with formula_2 being treated as a random variable having another gamma distribution, this time with mean formula_5 and shape parameter formula_6. The result is that formula_1 has the following probability density function (pdf) for formula_8:\n\nwhere formula_10 formula_11 formula_12 and formula_13 is a modified Bessel function of the second kind. In this derivation, the K-distribution is a compound probability distribution. It is also a product distribution: it is the distribution of the product of two independent random variables, one having a gamma distribution with mean 1 and shape parameter formula_3, the second having a gamma distribution with mean formula_5 and shape parameter formula_6. \n\nThis distribution derives from a paper by Eric Jakeman and Peter Pusey (1978) who used it to model microwave sea echo. Jakeman and Tough (1987) derived the distribution from a biased random walk model. Ward (1981) derived the distribution from the product for two random variables, \"z\" = \"a\" \"y\", where \"a\" has a chi distribution and \"y\" a complex Gaussian distribution. The modulus of \"z\", \"|z|\", then has K distribution.\n\nThe moment generating function is given by\n\nwhere formula_18 is the Whittaker function.\n\nThe n-th moments of K-distribution is given by\nSo the mean and variance are given by\n\nAll the properties of the distribution are symmetric in formula_3 and formula_23\n\nK-distribution arises as the consequence of a statistical or probabilistic model used in Synthetic Aperture Radar (SAR) imagery. The K-distribution is formed by compounding two separate probability distributions, one representing the radar cross-section, and the other representing speckle that is a characteristic of coherent imaging. It is also used in wireless communication to model composite fast fading and shadowing effects.\n\n\n"}
{"id": "604868", "url": "https://en.wikipedia.org/wiki?curid=604868", "title": "Kathryn P. Hire", "text": "Kathryn P. Hire\n\nKathryn Patricia \"Kay\" Hire (born August 26, 1959 in Mobile, Alabama) is a NASA astronaut and Captain in the U.S. Navy Reserve who has flown aboard two Space Shuttle missions.\n\nShe attended St. Pius X Catholic Elementary School, Mobile, Alabama, 1973 and Murphy High School, Mobile, Alabama, 1977.\n\nShe received her Bachelor of Science degree in Engineering and Management from the United States Naval Academy, 1981 and her\nMaster of Science degree in Space Technology from Florida Institute of Technology in 1991.\n\nAfter earning her Naval Flight Officer wings in October 1982, Hire conducted worldwide airborne oceanographic research missions with Oceanographic Development Squadron Eight (VXN-8) based at NAS Patuxent River, Maryland. She flew as Oceanographic Project Coordinator, Mission Commander and Detachment Officer-in-Charge on board specially configured RP-3A and RP-3D Orion aircraft.\n\nHire later instructed student naval flight officers while assigned to Naval Air Training Unit Mather (NAVAIRTU Mather), a tenant command associated with the 323d Flying Training Wing at Mather Air Force Base, California. While assigned to NAVAIRTU Mather, Hire progressed from Navigation Instructor and Course Manager to Curriculum Manager while concurrently assigned as a navigator flight instructor in the USAF T-43A aircraft.\n\nIn January 1989, Hire resigned her Regular Navy commission and left active duty, accepting a Reserve commission and joining the Naval Reserve at NAS Jacksonville, Florida. Her initial tours of duty included Patrol Squadron Augment Unit VP-0545 and Anti-Submarine Warfare Operations Center 0574 and 0374.\n\nFollowing the repeal of the Title 10 U.S.C. combat aircraft restriction for female military aviators in 1993, Hire was the first female in the U.S. military assigned to a combat aircrew, reporting to Patrol Squadron Sixty-Two (VP-62) on May 13, 1993 as a Navigator/Communicator flying the P-3C Update III Orion maritime patrol aircraft. VP-62 was also based at NAS Jacksonville and Hire routinely deployed for flight operations throughout the North Atlantic, Europe and the Caribbean.\n\nHire later served with Detachment 0482, Tactical Support Center 0682, U.S. Seventh Fleet Detachment 111, and U.S. Naval Forces Central Command Detachment 108. Recalled to full-time active military duty from 2001 to 2003, Hire supported Operation Enduring Freedom and Operation Iraqi Freedom as a member of the Headquarters, U.S. Central Command Current Operations and Joint Operations Center (JOC) staffs at MacDill AFB, Florida during Operation Enduring Freedom, followed by reassignment to the U.S. Naval Forces Central Command/U.S. Fifth Fleet staff for Operation Iraqi Freedom. She was promoted to her current rank of Captain in the U.S. Navy on December 1, 2002.\n\nHire began work at the Kennedy Space Center in May 1989,first as an Orbiter Processing Facility 3 Activation Engineer and later as a Space Shuttle Orbiter Mechanical Systems Engineer for Lockheed Space Operations Company. In 1991 she was certified as a Space Shuttle Test Project Engineer (TPE) and headed the checkout of the Extravehicular Mobility Units (spacesuits) and Russian Orbiter Docking System. She was assigned as Supervisor of Space Shuttle Orbiter Mechanisms and Launch Pad Swing Arms in 1994.\n\nSelected by NASA in December 1994, Hire reported to the Johnson Space Center in March 1995. After a year of training, she worked in mission control as a spacecraft communicator (CAPCOM). Hire flew as Mission Specialist-2 on STS-90 Neurolab (1998) and logged over 381 hours in space. She served as the Astronaut Office Lead for Shuttle Avionics Integration Laboratory (SAIL), Shuttle Payloads, and Flight Crew Equipment. Hire was assigned to the Astronaut Support Personnel (ASP) team for Kennedy Space Center Operations. She then journeyed to the International Space Station as a Mission Specialist for Space Shuttle mission STS-130.\n\nSTS-90 Neurolab (April 17 – May 3, 1998). During the 16-day Spacelab flight, the seven-member crew aboard Space Shuttle \"Columbia\" served as both experimental subjects and operators for 26 life science experiments focusing on the effects of microgravity on the brain and nervous system. The STS-90 flight orbited the Earth 256 times, and covered 6.3 million miles.\n\nSTS-130 (February 8, 2010 – February 21, 2010) She also journeyed to the International Space Station as a Mission Specialist for Space Shuttle mission STS-130.\n\nAs of June 2018, Hire is considered a \"NASA Management Astronaut\", which means that she is employed at NASA but are no longer eligible for spaceflight assignments.\n\n\n\n"}
{"id": "41848782", "url": "https://en.wikipedia.org/wiki?curid=41848782", "title": "Kodaikanal mercury poisoning", "text": "Kodaikanal mercury poisoning\n\nKodaikanal mercury poisoning is a proven case of mercury contamination by (Hindustan Unilever) in the process of making mercury thermometers for export around the world. The exposure of the environmental abuse led to the closure of the factory in 2001 and opened up a series of issues in India such as corporate liability, corporate accountability and corporate negligence.\n\nKodaikanal is a hill station located 7,000 feet above the sea level in the high ranges of Palani Hills in the Southern Indian state of Tamil Nadu, often referred as Princess of Hill Stations. It is a popular vacation spot in Southern India with Indian and international tourists visiting the site in the summer months of March to June. Kodaikanal was established in 1845 by the British Raj as a place of escape from the high temperatures and tropical diseases of the plains. The region's economy thrives on tourism and a large number of hotels and restaurants have come up in the region following the growth of the hospitality industry. Kodaikanal also has the rare distinction of being home to Blue Kurinji (\"Strobilanthes kunthiana\"), flowering once in 12 years. The flowering season attracts tourists from around the world.\n\nMercury is a heavy metal and is commonly known as quicksilver. It is the only metal that is a liquid in standard condition of pressure and temperature. Heavy and silvery, mercury is dangerous to flora and fauna even in small doses. In human beings, continuous exposure to mercury leads to damage or impairment of the brain, kidneys and liver. Mercury is used for industrial purposes such as making thermometers, barometers, sphygmomanometers, and fluorescent lamps. Of late, use of mercury-dependent devices have been phased out around the world and the industry has moved to digital devices replacing mercury devices.\n\nThe mercury contamination in Kodaikanal originated at a thermometer factory that was owned by Hindustan Unilever. Unilever acquired the thermometer factory from cosmetics maker Pond's India Ltd. Pond's moved the factory from the United States to India in 1982 after the plant owned there by its parent, Chesebrough-Pond's, had to be dismantled following increased awareness in developed countries of polluting industries. In 1987, Pond's India and the thermometer factory went to Hindustan Unilever when it acquired Cheseborough-Pond's globally.\n\nThe factory imported mercury from the United States, and exported finished thermometers to markets in the United States and Europe. Around 2001, a number of workers at the factory began complaining of kidney and related ailments. Public interest groups such as Tamil Nadu Alliance Against Mercury (TNAAC) alleged that the Company had been disposing mercury waste without following proper protocols. In early 2001, public interest groups unearthed a pile of broken glass thermometers with remains of Mercury from an interior of part of the shola forest, which they suspected could have come from the company. In March, a public protest led by local workers' union and international environmental organisation Greenpeace forced the company to shut down the factory. Soon the company admitted that it did dispose of mercury contaminated waste. The company said in its 2002 annual report and its latest Sustainability Report that it did not dump glass waste contaminated with mercury on the land behind its factory, but only a quantity of 5.3 metric tonnes of glass containing 0.15% residual mercury had been sold to a scrap recycler located about three kilometers from the factory, in breach of the company procedures. Quoting a report prepared by an international environmental consultant, Unilever said there was no health effect on the workers of the factory or any impact on the environment.\n\nOnce the factory was shut down, public interest groups demanded the return of the remaining mercury waste to the United States for recycling, remediation of the factory site, and address of the health complaints of the workers. Local groups and workers' union under the leadership of Greenpeace, represented to the company, regulatory bodies, and the government, besides initiating legal action against the company.\n\nGreenpeace campaigner Ameer Shahul led the public affairs groups and workers collaboration in forcing the Company to collect 290 tonnes of dumped mercury waste from the shola forest and send back to the United States for recycling in 2003. This was widely hailed by the media as ‘reverse dumping’. Later Greenpeace campaigners Ameer Shahul and Navroz Mody led the groups in lobbying for remediation of the site and initiated an investigation by the Department of Atomic Energy of Government of India, which found that the free mercury level in the atmosphere of Kodaikanal was 1000 times more than what is found in normal conditions. Analysis of water, sediment and fish samples collected from Kodaikanal Lake by a team of scientists of the Department of Atomic Energy showed elevated levels of mercury four years after the stoppage of mercury emissions. A series of scientific studies have also been carried out by Governmental and non-governmental organisations to determine the extent of damage caused to the environment and to the people who were exposed to mercury in the factory.\n\nGreenpeace and workers' unions continued to mount pressure on the company to take responsibility for the dumping crimes it had committed and for meddling with a pristine environment. They asked the regulatory bodies to prosecute the company. With these demands, public interest groups led by Greenpeace campaign head Shahul spooked the annual general body meeting of Hindustan Unilever in 2004. Consequently, the company began working with the regulatory body Tamil Nadu Pollution Control Board (TNPCB) to remediate the soil, de-contaminate and scrap the thermometer-making equipment at the Kodaikanal site. The company appointed National Environmental Engineering Research Institute (NEERI) to finalise the scope for remediation, which was vehemently opposed by environmentalists. In 2006, the plant, machinery and materials used in thermometer manufacturing at the site were decontaminated and disposed off as scrap to industrial recyclers. In the following year, NEERI conducted trials at the factory for remediation of the contaminated soil on site, and recommended a remediation protocol of soil washing and thermal retorting. These were hotly contested by environmental groups under the leadership of Nityanand Jayaraman. Ultimately, the Tamil Nadu Pollution Control Board (TNPCB) recommended a remediation standard of up to 20 mg/kg of mercury concentration in soil, which means 95% of the samples analysed after the remediation process should be of less than 20 mg/kg. Consequently, pre-remediation work started in May 2009.\n\nPublic interest groups contested the soil clean-up criteria and alleged that TNPCB is helping Unilever clean up to lower standards to cut costs. The acceptable mercury level being suggested by TNPCB is at least 20 times higher than what Unilever would have been required to do if they had caused the same contamination in the United Kingdom, where they are based. They also called for transparency and public participation in the process of deciding the levels of clean-up and in the process of clean-up.\n\nAfter the shut down of the factory, health specialists from Bangalore-based Community Health Centre conducted a survey among the former workers of the factory. It found that former workers of the factory had visible signs of mercury poisoning such as gum and skin allergy and related problems, ‘which appeared to be due to exposure to mercury'.\n\nThe company claims that comprehensive occupational safety and health systems existed at the Kodaikanal factory prior to its closure in 2001. Internal monitoring within the factory and external audits carried out by statutory authorities during the operations of the factory showed that there were no adverse health effects to the workers on account of their employment at the factory. It says there had been a comprehensive medical examination conducted by a panel of doctors using a questionnaire developed by Mine Safety and Health Administration (MSHA) of the United States Department of Labor; a study by the Certifying Surgeon from the Inspectorate of Factories; an assessment by Dr P N Viswanathan of Indian Institute of Toxicology Research (IITR); a study by Dr Tom van Teuenbroek of TNO; and a study by IITR, formerly known as Industrial Toxicology Research Centre(ITRC) as directed by a Monitoring Committee set up by the Supreme Court of India.\n\nThe company says its conclusions of its occupational health surveillance were also endorsed by the All India Institute of Medical Sciences (AIIMS) and the National Institute of Occupational Health (NIOH).\n\nIn February 2006, a group of ex-employees of the factory approached the Madras High Court seeking directions for conducting a fresh health survey and providing economic rehabilitation. A year later, the Madras High Court constituted a five-member expert committee, with representatives from ITRC, AIIMS and NIOH to decide whether the alleged health conditions of the workers and their families were related to mercury exposure, and recommend whether there was need for a new health study. The Committee after examining the ex-workers, questioning the Hindustan Unilever Limited (HUL) officials and after a visit to the factory in October 2007 submitted its report suggesting that there is \"no sufficient evidence to link the current clinical condition of the factory workers to the mercury exposure in the factory in the past\". Accepting the report, the Madras High Court ruled out the need for any fresh health study. In the meantime, the Ministry of Labour and Employment, which is also a respondent in the case before the Madras High Court conducted a detailed study by a team comprising experts from various fields found that there is prima facie evidence to suggest that not only the workers of the factory, but even the children of the workers, have suffered because of exposure to mercury. The Ministry submitted its report to the Madras High Court in 2011. It also recommended setting up a Board to examine the extent of damage or disability suffered by workers and their children because of exposure to mercury, and based on the assessment of the Board workers can approach the Employment Compensation Commissioner to seek compensation. The case before the Madras High Court is still to be decided.\n"}
{"id": "1166480", "url": "https://en.wikipedia.org/wiki?curid=1166480", "title": "LOMO", "text": "LOMO\n\nLOMO () is a manufacturer of medical and motion-picture lenses and equipment based in St. Petersburg, Russia. The company was awarded three Order of Lenin decorations by the Soviet Union.\n\nIts Lomo LC-A consumer camera was the inspiration for the lomography photographic movement.\n\nThe company was founded in 1914 in Petrograd (now Saint Petersburg).\n\nIt was established as a French – Russian limited company to produce lenses and cameras. It manufactured gun sights during World War I. In 1919, it was nationalised. In the ensuing years, the state optical industries were reorganised several times. In 1921, the factory was named the Factory of State Optics, G.O.Z. In 1925, camera production was resumed, and several lens designs tested between 1925 and 1929. In 1928, the factory was ordered to manufacture a 9x12 camera, known as the FOTOKOR.\n\nFurther reorganisations of the Soviet optical factories in several stages finally resulted in that the factory at Leningrad became GOMZ, the Russian Optical and Mechanical Factory.\n\nIn the transition period 1932 to 1935 a copy of the Leica camera was developed, the VOOMP I. It was followed by the VOOMP II or the \"Pioneer\" that was manufactured in small numbers. Simultaneously designers began the development of a single-lens reflex camera for 35mm cine film, possibly inspired by similar work in Germany, especially at Zeiss Ikon in Dresden, since the lens mount is quite similar to that of the Contax cameras of the time. Zeiss themselves were not allowed to pursue their ideas, due to the German armament. The new camera, called the \"Sport\", was introduced at about the same time as the Ihagee Kine Exakta in 1936.\n\nToday LOMO makes military optics, scientific research instruments, criminological microscopes, medical equipment, and a range of consumer products. It produced the first Russian camera in 1930.\n\nKnown as GOMZ (State Optical-Mechanical Plant), the company was transformed under the direction of Mikhail Panfilov, who united several industries and founded the LOMO Association in 1962. Panfilov was the General Director until 1984, when he retired and was replaced by Georgy Khizha as Director General. In 1990 - 1997 Ilya Klebanov was the Director General of LOMO Association.\n\nAmateur movie equipment brands were LOMO and AVRORA for cameras and Russ for projectors. Ritm is also used after LOMO for professional sound equipment. It produced the one and only Super 8 sound camera and projector in the USSR. It was also the main Soviet producer for professional grade cine lenses, including anamorphic lenses.\n\nThe company went public in 1993, and was renamed LOMO PLC; it is traded on the RTS Classic Stock Market.\n\nThe company is ISO 9001 certified and exports worldwide. Night-vision devices and telescopes account for 30% of the company's exports. Germany is the largest importer of LOMO products. Medical equipment, fiber optic cables and Endoscopes, optical components and cameras are consumed mainly by the Russian market and other states of the former Soviet Union. Military equipment and science research instruments make a significant share of production for export to such countries as Israel, India, United States, Canada, Mexico, and other international markets.\n\nIts Maksutov telescope range is highly regarded by amateur astronomers.\n\nThe word LOMO is often used to refer to the Lomo LC-A and other LOMO branded consumer cameras; while the cameras have a large following around the world, the cameras themselves are a minor product for LOMO PLC.\n\nLOMO is also currently a manufacturer of die-cast models in 1:43 scale. LOMO factory mass-produces 1:43 scale models of the 1930s-1950s vehicles on AMO, ZiS, ZiL and GAZ chassis.\n\nLOMO is regarded in St. Petersburg for its advanced medical centre, with well equipped facilities for advanced treatment of a range of medical conditions. LOMO medical centre has a steady reputation in general surgery and trauma, oncology and radiology, OB/GYN, pediatrics, physical therapy and other specialties of medicine.\n\nMedical instruments, such as Endoscopes and Surgical Microscopes made by LOMO are used by the doctors at LOMO medical centre, thus providing prompt feedback from doctors to engineers and designers of medical equipment.\n\n"}
{"id": "5396543", "url": "https://en.wikipedia.org/wiki?curid=5396543", "title": "Lawn aerator", "text": "Lawn aerator\n\nA lawn aerator is a garden tool designed to create holes in the soil in order to help lawn grasses grow. In compacted lawns, aeration improves soil drainage and encourages worms, microfauna and microflora which require oxygen.\n\nLawn aeration constitutes three things: controlling lawn thatch, and reducing soil compaction, making grass roots multiply. Aerating either by coring or spiking causes the roots to divide or sever apart. This is something rarely done naturally. By severing roots it makes the roots multiply. When roots multiply the blades multiply thereby keeping the lawn thick and rooting deeply. the holes become engorged with roots. Lawn thatch is a layer of dead organic tissue that can protect the lawn by moderating temperature and reducing evapotranspiration when it is a reasonable thickness, but too much thatch can limit soil oxygenation and reduce watering effectiveness. Soil compaction makes it difficult for grass to develop long roots and disturbs both natural rainwater and artificial irrigation.\n\nThere are two types of lawn aerators. Spike aerators use wedge shaped solid spikes to punch holes in the soil, and core aerators have hollow tines that pull out plugs (or \"cores\") from soil. \nCore/plug aerator vs. spike aerator\n\nA spike aerator creates holes in the ground by pushing the soil sideways as wedge-shaped spikes penetrate the soil. Since there is no soil removed from the ground, watering will cause the compacted soil around the holes to expand and close. A core/plug aerator removes soil from the ground and leaves the core on the turf. This reduces compaction in the soil, and the holes can stay open for a long time allowing air, fertilisers, and water to reach the roots. Core aeration is suitable for heavy clay soils, and spike aeration is more suited to sandy or loamy soils.\n\nPowered aerator vs. manual aerator\nPowered aerators employ the power from ground propulsion to drive multiple tines into ground. The machines can aerate a large lawn in a relatively short time (similar to mowing speed).\n\nManual aerators usually have two to five hollow tines mounted on a step bar. The operator puts one foot on the step bar and push it downward, forcing the tines to penetrate into the soil. Then he pulls the handle on the step bar upward to remove the soil cores out of the ground. By repeating the same operation, the cores left in the tines will be pushed out by the next ones. Manual aerators are much cheaper than powered ones. The trade-off is the speed. For a typical residential lawn (1/4 acre lot), it will take hours to finish. Some products also have issues with the tines becoming clogged with soil, which can slow down the operation even more. However, a well-made manual aerator offers advantages such as ease of use, selective aeration, and economy.\n\n"}
{"id": "19006537", "url": "https://en.wikipedia.org/wiki?curid=19006537", "title": "Malibu Hydro", "text": "Malibu Hydro\n\nMalibu Hydro System was designed to provide electricity to the Malibu Club in Canada. This hydro system starts from a high alpine lake where water is diverted from the lake through a steel penstock to a power house nearly below near the shore of Jervis Inlet. The flow of water turns a pelton wheel which is attached to a generator to create electricity. The electricity is then transmitted to camp by a submarine cable running under Jervis Inlet at a high voltage to reduce losses. Power is then distributed throughout camp on the existing and upgraded electrical system.\n\nThe hydro turbine is feed by a yearlong creek called McCannel Creek, which is directly across Jervis Inlet from Malibu. The creek's source is McCannel Lake, at an elevation of . At the outlet of the lake, a dam was built to maintain the lake level and control the flow of the creek.\n\nA small weir type dam was built at the lake outlet, and, by limiting the discharge into the creek, aids in maintaining the lake level. The weir has a large pipe and valve in its base to pass the minimum required to maintain the creek. This is in addition to the water which will be spilled to create power.\n\nWater regulated at the lake will flow down the creek from the level to the level. Here a second weir was built across the creek that forms a large deep pool from which the penstock draws water at a rate of up to .\n\nThe power house contains the turbine, generator, and the necessary equipment to control and distribute the electricity for Malibu. It is located a short distance above the beach. The following diagram and table explains what happens once the water flow reaches the Power house (read the diagram right to left)\n\nTransformers at the hydro generator raise 600 volts to 15,000 volts for transmission. Power is transmitted across Jervis inlet, a distance of two miles (3 km) via a underwater power line consisting of three parallel conductors. On the Malibu side of the inlet, a transformer steps the 15,000 volts down to 120 and 208 volts for use at the camp.\n\nIt began installation in 2005.\n"}
{"id": "3316275", "url": "https://en.wikipedia.org/wiki?curid=3316275", "title": "Mitchum", "text": "Mitchum\n\nMitchum is a brand of antiperspirant-deodorant, later owned by Revlon. It was widely known for its marketing slogan, \"So effective you can skip a day,\" but that slogan hasn't been used since 2007.\n\nMitchum is a brand that was purchased by the Revlon Corporation in the late sixties. Originally known as the Paris Toilet Company and then the Golden Peacock Company, the company carried a full line of cosmetics. Bill McNutt is credited with inventing the antiperspirant. Other products launched by Mitchum, include \"Esoterica\" which helped with removing age spots. Before the company was sold to Revlon, it had existed for two generations and was headquartered in Paris, Tennessee.\n\nAll versions of their product used to contain 20% of the antiperspirant Aluminium zirconium tetrachlorohydrex gly and the roll-on still does. In 2007, they re-branded their entire line with a new active ingredient, Aluminum sesquichlorohydrate 25%. They went on to release a Smart Solid line, a water-based solid with a differing texture than most deodorants, that contained the original active ingredient. A standard invisible solid was released with the old active ingredient as well, with the name \"Mitchum Advanced Control\".\n\nDuring the 1990 media controversy surrounding Arthur Scargill's handling of money donated for striking British miners, Mitchum used an image of the NUM leader, without his consent, under the slogan \"Mitchum, for when you're really sweating!\" Scargill complained to the UK's Advertising Standards Association who criticised the advertisement as \"highly distasteful\".\n\n"}
{"id": "15955754", "url": "https://en.wikipedia.org/wiki?curid=15955754", "title": "Multi-scale camouflage", "text": "Multi-scale camouflage\n\nMulti-scale camouflage is a type of military camouflage combining patterns at two or more scales, often (though not necessarily) with a digital camouflage pattern created with computer assistance. The function is to provide camouflage over a range of distances, or equivalently over a range of scales (scale-invariant camouflage), in the manner of fractals, so some approaches are called fractal camouflage. Not all multiscale patterns are composed of rectangular pixels, even if they were designed using a computer. Further, not all pixellated patterns work at different scales, so being pixellated or digital does not of itself guarantee improved performance.\n\nThe first standardized pattern to be issued was the single-scale Italian \"telo mimetico\". The root of the modern multi-scale camouflage patterns can be traced back to 1930s experiments in Europe for the German and Soviet armies. This was followed by Canadian development of Canadian Disruptive Pattern (CADPAT), first issued in 2002, and then with US work which created Marine pattern (MARPAT), launched between 2002 and 2004.\n\nThe scale of camouflage patterns is related to their function. Large structures need larger patterns than individual soldiers to disrupt their shape. At the same time, large patterns are more effective from afar, while small scale patterns work better up close. Traditional single scale patterns work well in their optimal range from the observer, but an observer at other distances will not see the pattern optimally. Nature itself is very often fractal, where plants and rock formations exhibit similar patterns across several magnitudes of scale. The idea behind multi-scale patterns is both to mimic the self-similarity of nature, and also to offer scale invariant or so-called fractal camouflage.\n\nAnimals such as the flounder have the ability to adapt their camouflage patterns to suit the background, and they do so extremely effectively, selecting patterns that match the spatial scales of the current background.\n\nWhen a pattern is called digital, this most often means that it is visibly composed of computer-generated pixels. The term is sometimes also used of computer generated patterns like the non-pixellated Multicam and the Italian fractal \"Vegetato\" pattern. Neither pixellation nor digitization contribute to the camouflaging effect. The pixellated style, however, simplifies design and eases printing on fabric, compared to traditional patterns. While digital patterns are becoming widespread, critics maintain that the pixellated look is a question of fashion rather than function.\n\nThe design process involves trading-off different factors, including colour, contrast and overall disruptive effect. A failure to consider all elements of pattern design tends to result in poor results. The US Army's Universal Camouflage Pattern (UCP), for example, adopted after limited testing in 2003–4, performed poorly because of low pattern contrast (\"isoluminance\"—beyond very close range, the design looks like a field of solid light grey, failing to disrupt an object's outlines) and arbitrary colour selection, neither of which could be saved by quantizing (digitizing) the pattern geometry. The design was replaced from 2015 with Operational Camouflage Pattern, a non-pixellated pattern.\n\nThe idea of patterned camouflage extends back to the interwar period in Europe. The first printed camouflage pattern was the 1929 Italian \"telo mimetico\", which used irregular areas of three colours at a single scale.\n\nDuring the Second World War, Johann Georg Otto Schick designed a series of patterns such as \"Platanenmuster\" (plane tree pattern) and \"erbsenmuster\" (pea-dot pattern) for the Waffen-SS, combining micro- and macro-patterns in one scheme.\n\nThe German Army developed the idea further in the 1970s into Flecktarn, which combines smaller shapes with dithering; this softens the edges of the large scale pattern, making the underlying objects harder to discern.\n\nPixel-like shapes pre-date computer-aided design by many years, already being used in Soviet Union experiments with camouflage patterns, such as \"TTsMKK\" developed in 1944 or 1945. The pattern uses areas of olive green, sand, and black running together in broken patches at a range of scales.\n\nIn 1976, Timothy O'Neill created a pixellated pattern named \"Dual-Tex\". He called the digital approach \"texture match\". The initial work was done by hand on a retired M113 armoured personnel carrier; O'Neill painted the pattern on with a 2-inch (5 centimetre) roller, forming squares of colour by hand. Field testing showed that the result was good compared to the U. S. Army's existing camouflage patterns, and O'Neill went on to become an instructor and camouflage researcher at West Point military academy.\n\nBy 2000, development was under way to create pixellated camouflage patterns for battledress like the Canadian Forces' CADPAT, issued in 2002, and then the US Marines' MARPAT, rolled out between 2002 and 2004. The CADPAT and MARPAT patterns were somewhat self-similar (in the manner of fractals and patterns in nature such as vegetation), being designed to work at two different scales; a genuinely fractal pattern would be statistically similar at all scales. A target camouflaged with MARPAT takes about 2.5 times longer to detect than older NATO camouflage which worked at only one scale, while recognition, which begins after detection, took 20 percent longer than with older camouflage.\n\nFractal-like patterns work because the human visual system efficiently discriminates images which have different fractal dimension or other second-order statistics like Fourier spatial amplitude spectra; objects simply appear to pop out from the background. Timothy O'Neill helped the Marine Corps to develop first a digital pattern for vehicles, then fabric for uniforms, which had two colour schemes, one designed for woodland, one for desert.\n"}
{"id": "24036785", "url": "https://en.wikipedia.org/wiki?curid=24036785", "title": "Ory Okolloh", "text": "Ory Okolloh\n\nOry Okolloh (or Ory Okolloh Mwangi) is a Kenyan activist, lawyer, and blogger. She is Director of Investments at Omidyar Network. She was formerly the Policy Manager for Africa with Google.\n\nOkolloh was born into a relatively poor family. She has said that her parents sent her to a private elementary school that they could \"barely afford,\" which \"set the foundation for what ended up being my career.\" She earned an undergraduate degree in Political Science from the University of Pittsburgh and graduated from Harvard Law School in 2005. Her father died of AIDS in 1999. Okolloh lives in Nairobi, Kenya, with her husband and three children.\n\nIn 2006 she co-founded the parliamentary watchdog site \"Mzalendo\" (Swahili: \"Patriot\"). The site sought to increase government accountability by systematically recording bills, speeches, MPs, standing orders, etc.\n\nWhen Kenya was engulfed in violence following a disputed presidential election in 2007, Okolloh helped create \"Ushahidi\" (Swahili: \"Witness\"), a website that collected and recorded eyewitness reports of violence using text messages and Google Maps. The technology has since been adapted for other purposes (including monitoring elections and tracking pharmaceutical availability) and used in a number of other countries.\n\nOkolloh has a personal blog, \"Kenyan Pundit\", which was featured on Global Voices Online.\n\nShe has worked as a legal consultant for NGOs and has worked at Covington and Burling, the Kenya National Commission on Human Rights, and the World Bank.\n\nOkolloh was appointed on the Board of Thomson Reuters Founders Share Company, the body that acts as a guardian of the Thomson Reuters Trust Principles in May 2015.\n\n"}
{"id": "8515253", "url": "https://en.wikipedia.org/wiki?curid=8515253", "title": "Outline of industrial machinery", "text": "Outline of industrial machinery\n\nThe following outline is provided as an overview of and topical guide to industrial machinery:\n\n\n\n\n\n"}
{"id": "491484", "url": "https://en.wikipedia.org/wiki?curid=491484", "title": "Paper machine", "text": "Paper machine\n\nA paper machine (or paper-making machine) is an industrial machine used in the pulp and paper industry\nto create paper in large quantities at high speed. Modern paper-making machines are based on the principles of the Fourdrinier Machine, which uses a moving woven mesh to create a continuous paper web by filtering out the fibres held in a paper stock and producing a continuously moving wet mat of fibre. This is dried in the machine to produce a strong paper web. \n\nThe basic process is an industrialised version of the historical process of hand paper-making, which could not satisfy the demands of developing modern society for large quantities of a printing and writing substrate. The first modern paper machine was invented in Britain by Henry and Sealy Fourdrinier, and patented in 1806.\n\nThe same process is used to produce paperboard on a paperboard machine.\n\nPaper machines usually have at least five distinct operational sections:\n\nThere can also be a coating section to modify the surface characteristics with coatings such as china clay.\n\nBefore the invention of continuous paper making, paper was made in individual sheets by stirring a container of pulp slurry and either pouring it into a fabric sieve called a sheet mould or dipping and lifting the sheet mould from the vat. While still on the fabric in the sheet mould, the wet paper is pressed to remove excess water and then the sheet is lifted off to be hung over a rope or wooden rod to air dry.\n\nIn 1799, Louis-Nicolas Robert of Essonnes, France, was granted a patent for a continuous paper making machine. At the time Robert was working for Saint-Léger Didot, with whom he quarreled over the ownership of the invention. Didot thought that England was a better place to develop the machine. But during the troubled times of the French Revolution, he could not go there himself, so he sent his brother-in-law, John Gamble, an Englishman living in Paris. Through a chain of acquaintances, Gamble was introduced to the brothers Sealy and Henry Fourdrinier, stationers of London, who agreed to finance the project. Gamble was granted British patent 2487 on 20 October 1801. The Fourdrinier machine used a specially woven fabric mesh conveyor belt (known as a \"wire,\" as it was once woven from bronze) in the forming section, where a slurry of fibre (usually wood or other vegetable fibres) is drained to create a continuous paper web. The original Fourdrinier forming section used a horizontal drainage area, referred to as the \"drainage table\".\n\nWith the help particularly of Bryan Donkin, a skilled and ingenious mechanic, an improved version of the Robert original was installed at Frogmore Paper Mill, Apsley, Hertfordshire, in 1803, followed by another in 1804. A third machine was installed at the Fourdriniers' own mill at Two Waters. The Fourdriniers also bought a mill at St Neots intending to install two machines there and the process and machines continued to develop.\n\nThomas Gilpin is most often credited for creating the first U.S cylinder type papermaking machine at Brandywine Creek, Delaware in 1817. This machine was also developed in England, but it was a cylinder mould machine. The Fourdrinier machine wasn't introduced into the USA until 1827.\n\nRecords show Charles Kinsey of Paterson, NJ had already patented a continuous process papermaking machine in 1807. Kinsey’s machine was built locally by Daniel Sawn and by 1809 the Kinsey machine was successfully making paper at the Essex Mill in Paterson. Financial stress and potential opportunities created by the Embargo of 1807 eventually persuaded Kinsey and his backers to change the mill’s focus from paper to cotton and Kinsey's early papermaking successes were soon overlooked and forgotten.\n\nGilpin's 1817 patent was similar to Kinsey's, as was the John Ames patent of 1822. The Ames patent was challenged by his competitors, asserting that Kinsey was the original inventor and Ames had been pilfering other peoples' ideas, their evidence being the employment of Daniel Sawn to work on his machine.\n\nThe method of continuous production demonstrated by the paper machine influenced the development of continuous rolling of iron and later steel and other continuous production processes.\n\nThe plant fibres used for pulp are composed mostly of cellulose and hemi-cellulose, which have a tendency to form molecular linkages between fibres in the presence of water. After the water evaporates the fibres remain bonded. It is not necessary to add additional binders for most paper grades, although both wet and dry strength additives may be added.\n\nRags of cotton and linen were the major source of pulp for paper before wood pulp. Today almost all pulp is of wood fibre. Cotton fibre is used in speciality grades, usually in printing paper for such things as resumes and currency.\n\nSources of rags often appear as waste from other manufacturing such as denim fragments or glove cuts. Fibres from clothing come from the cotton boll. The fibres can range from 3 to 7 cm in length as they exist in the cotton field. Bleach and other chemicals remove the colour from the fabric in a process of cooking, usually with steam. The cloth fragments mechanically abrade into fibres, and the fibres get shortened to a length appropriate for manufacturing paper with a cutting process. Rags and water dump into a trough forming a closed loop. A cylinder with cutting edges, or knives, and a knife bed is part of the loop. The spinning cylinder pushes the contents of the trough around repeatedly. As it lowers slowly over a period of hours, it breaks the rags up into fibres, and cuts the fibres to the desired length. The cutting process terminates when the mix has passed the cylinder enough times at the programmed final clearance of the knives and bed.\n\nAnother source of cotton fibre comes from the cotton ginning process. The seeds remain, surrounded by short fibres known as linters for their short length and resemblance to lint. Linters are too short for successful use in fabric. Linters removed from the cotton seeds are available as first and second cuts. The first cuts are longer.\n\nThe two major classifications of pulp are chemical and mechanical. Chemical pulps formerly used a sulphite process, but the kraft process is now predominant. Kraft pulp has superior strength to sulphite and mechanical pulps and kraft process spent pulping chemicals are easier to recover and regenerate. Both chemical pulps and mechanical pulps may be bleached to a high brightness.\n\nChemical pulping dissolves the lignin that bonds fibres to one another, and binds the outer fibrils that compose individual fibres to the fibre core. Lignin, like most other substances that can separate fibres from one another, acts as a debonding agent, lowering strength. Strength also depends on maintaining long cellulose molecule chains. The kraft process, due to the alkali and sulphur compounds used, tends to minimize attack on the cellulose and the non-crystalline hemicellulose, which promotes bonding, while dissolving the lignin. Acidic pulping processes shorten the cellulose chains.\n\nKraft pulp makes superior linerboard and excellent printing and writing papers.\n\nGroundwood, the main ingredient used in newsprint and a principal component of magazine papers (coated publications), is literally ground wood produced by a grinder. Therefore, it contains a lot of lignin, which lowers its strength. The grinding produces very short fibres that drain slowly.\n\nThermomechanical pulp (TMP) is a variation of groundwood where fibres are separated mechanically while at high enough temperatures to soften the lignin.\n\nBetween chemical and mechanical pulps there are semi-chemical pulps that use a mild chemical treatment followed by refining. Semi-chemical pulp is often used for corrugating medium.\n\nBales of recycled paper (normally old corrugated containers) for unbleached (brown) packaging grades may be simply pulped, screened and cleaned. Recycling to make white papers is usually done in a deinking plant, which employs screening, cleaning, washing, bleaching and flotation. Deinked pulp is used in printing and writing papers and in tissue, napkins and paper towels. It is often blended with virgin pulp.\n\nAt integrated pulp and paper mills, pulp is usually stored in high density towers before being pumped to stock preparation. Non integrated mills use either dry pulp or wet lap (pressed) pulp, usually received in bales. The pulp bales are slushed in a [re]pulper.\n\nStock preparation is the area where pulp is usually refined, blended to the appropriate proportion of hardwood, softwood or recycled fibre, and diluted to as uniform and constant as possible consistency. The pH is controlled and various fillers, such as whitening agents, size and wet strength or dry strength are added if necessary. Additional fillers such as clay, calcium carbonate and titanium dioxide increase opacity so printing on reverse side of a sheet will not distract from content on the obverse side of the sheet. Fillers also improve printing quality.\n\nPulp is pumped through a sequence of tanks that are commonly called \"chests\", which may be either round or more commonly rectangular. Historically these were made of special ceramic tile faced reinforced concrete, but mild and stainless steels are also used. Low consistency pulp slurries are kept agitated in these chests by propeller like agitators near the pump suction at the chest bottom.\n\nIn the following process, different types of pulp, if used, are normally treated in separate but similar process lines until combined at a blend chest:\n\nFrom high density storage or from slusher/pulper the pulp is pumped to a low density storage chest (tank). From there it is typically diluted to about 4% consistency before being pumped to an unrefined stock chest. From the unrefined stock chest stock is again pumped, with consistency control, through a refiner. Refining is an operation whereby the pulp slurry passes between a pair of discs, one of which is stationary and the other rotating at speeds of typically 1,000 or 1,200 RPM for 50 and 60 Hz AC, respectively. The discs have raised bars on their faces and pass each other with narrow clearance. This action unravels the outer layer of the fibres, causing the fibrils of the fibres to partially detach and bloom outward, increasing the surface area to promoting bonding. Refining thus increases tensile strength. For example, tissue paper is relatively unrefined whereas packaging paper is more highly refined. Refined stock from the refiner then goes to a refined stock chest, or blend chest, if used as such.\n\nHardwood fibres are typically 1 mm long and smaller in diameter than the 4 mm length typical of softwood fibres. Refining can cause the softwood fibre tube to collapse resulting in undesirable properties in the sheet.\n\nFrom the refined stock, or blend chest, stock is again consistency controlled as it is being pumped to a machine chest. It may be refined or additives may be added en route to the machine chest.\n\nThe machine chest is basically a consistency levelling chest having about 15 minutes retention. This is enough retention time to allow any variations in consistency entering the chest to be levelled out by the action of the basis weight valve receiving feedback from the on line basis weight measuring scanner. (Note: Many paper machines mistakenly control consistency coming out of the machine chest, interfering with basis weight control.)\n\nThere are four main sections on this paper machine. The forming section makes the pulp into the basis of for sheets along the wire. The press section, which removes much of the remaining water via a system of nips formed by rolls pressing against each other aided by press felts that support the sheet and absorb the pressed water. The dryer section of the paper machine, as its name suggests, dries the paper by way of a series of internally steam-heated cylinders that evaporate the moisture. Calenders are used to make the paper surface extra smooth and glossy. In practice calender rolls are normally placed vertically in a \"stack\".\n\nFrom the machine chest stock is pumped to a head tank, commonly called a \"head tank\" or \"stuff box\", whose purpose is to maintain a constant head (pressure) on the fiber slurry or \"stock\" as it feeds the basis weight valve. The stuff box also provides a means allowing air bubbles to escape. The consistency of the pulp slurry at the stuff box is in the 3% range. Flow from the stuff box is by gravity and is controlled by the basis weight valve on its way to the fan pump suction where it injected into main flow of water to the fan pump. The main flow of water pumped by the fan pump is from a whitewater chest or tank that collects all the water drained from the forming section of the paper machine. Before the fiber stream from the stuff box is introduced, the whitewater is very low in fiber content. The whitewater is constantly recirculated by the fan pump through the headbox and recollected from the wire pit and various other tanks and chests that receive drainage from the forming wire and vacuum assisted drainage from suction boxes and wet fiber web handling rolls. On the way to the head box the pulp slurry may pass through centrifugal cleaners, which remove heavy contaminants like sand, and screens, which break up fibre clumps and remove over-sized debris. The fan pump ultimately feeds the headbox, whether or not any centrifugal cleaners or screens are present.\n\nThe purpose of the headbox is create turbulence to keep the fibers from clumping together and to uniformly distribute the slurry across the width of the wire. Wood fibers have a tendency to attract one another, forming clumps, the effect being called flocculation. Flocculation is lessened by lowering consistency and or by agitating the slurry; however, de-flocculation becomes very difficult at much above 0.5% consistency. Minimizing the degree of flocculation when forming is important to physical properties of paper.\n\nThe consistency in the headbox is typically under 0.4% for most paper grades, with longer fibres requiring lower consistency than short fibres. Higher consistency causes more fibres to be oriented in the z direction, while lower consistency promotes fibre orientation in the x-y direction. Higher consistency promotes higher calliper (thickness) and stiffness, lower consistency promotes higher tensile and some other strength properties and also improves formation (uniformity). Many sheet properties continue to improve down to below 0.1% consistency; however, this is an impractical amount of water to handle. (Most paper machine run a higher headbox consistency than optimum because they have been sped up over time without replacing the fan pump and headbox. There is also an economic trade off with high pumping costs for lower consistency).\n\nThe stock slurry, often called \"white water\" at this point, exits the head box through a rectangular opening of adjustable height called the \"slice\", the white water stream being called the \"jet\" and it is pressurized on high speed machines so as to land gently on the moving fabric loop or \"wire\" at a speed typically between plus or minus 3% of the wire speed, called \"rush\" and \"drag\" respectively. Excessive \"rush\" or \"drag\" causes more orientation of fibres in the machine direction and gives differing physical properties in machine and cross directions; however, this phenomenon is not completely avoidable on Fourdrinier machines.\n\nOn lower speed machines at 700 feet per minute, gravity and the height of the stock in the headbox creates sufficient pressure to form the jet through the opening of the slice. The height of the stock is the head, which gives the headbox its name. The speed of the jet compared to the speed of the wire is known as the \"jet-to-wire ratio\". When the jet-to-wire ratio is less than unity, the fibres in the stock become drawn out in the machine direction. On slower machines where sufficient liquid remains in the stock before draining out, the wire can be driven back and forth with a process known as \"shake\". This provides some measure of randomizing the direction of the fibres and gives the sheet more uniform strength in both the machine and cross-machine directions. On fast machines, the stock does not remain on the wire in liquid form long enough and the long fibres line up with the machine. When the jet-to-wire ratio exceeds unity, the fibers tend to pile up in lumps. The resulting variation in paper density provides the antique or parchment paper look.\n\nTwo large rolls typically form the ends of the drainage section, which is called the \"drainage table\". The \"breast roll\" is located under the flow box, the jet being aimed to land on it at about the top centre. At the other end of the drainage table is the suction (\"couch\") roll. The couch roll is a hollow shell, drilled with many thousands of precisely spaced holes of about 4 to 5 mm diameter. The hollow shell roll rotates over a stationary suction box, normally placed at the top centre or rotated just down machine. Vacuum is pulled on the suction box, which draws water from the web into the suction box. From the suction roll the sheet feeds into the press section.\n\nDown machine from the suction roll, and at a lower elevation, is the \"wire turning roll\". This roll is driven and pulls the wire around the loop. The wire turning roll has a considerable angle of wrap in order to grip the wire.\n\nSupporting the wire in the drainage table area are a number of drainage elements. In addition to supporting the wire and promoting drainage, the elements de-flocculate the sheet. On low speed machines these table elements are primarily \"table rolls\". As speed increases the suction developed in the nip of a table roll increases and at high enough speed the wire snaps back after leaving the vacuum area and causes stock to jump off the wire, disrupting the formation. To prevent this drainage foils are used. The foils are typically sloped between zero and two or three degrees and give a more gentle action. Where rolls and foils are used, rolls are used near the headbox and foils further down machine.\n\nApproaching the dry line on the table are located low vacuum boxes that are drained by a barometric leg under gravity pressure. After the dry line are the suction boxes with applied vacuum. Suction boxes extend up to the couch roll. At the couch the sheet consistency should be about 25%.\n\nThe forming section type is usually based on the grade of paper or paperboard being produced; however, many older machines use a less than optimum design. Older machines can be upgraded to include more appropriate forming sections.\n\nA second headbox may be added to a conventional fourdrinier to put a different fibre blend on top of a base layer. A \"secondary headbox\" is normally located at a point where the base sheet is completely drained. This is not considered a separate ply because the water action does a good job of intermixing the fibers of the top and bottom layer. Secondary headboxes are common on linerboard.\n\nA modification to the basic fourdrinier table by adding a second wire on top of the drainage table is known as a top wire former. The bottom and top wires converge and some drainage is up through the top wire. A top wire improves formation and also gives more drainage, which is useful for machines that have been sped up.\n\nThe Twin Wire Machine or Gap former uses two vertical wires in the forming section, thereby increasing the de-watering rate of the fibre slurry while also giving uniform two sidedness.\n\nThere are also machines with entire Fourdrinier sections mounted above a traditional Fourdrinier. This allows making multi-layer paper with special characteristics. These are called top Fourdriniers and they make multi-ply paper or paperboard. Commonly this is used for making a top layer of bleached fibre to go over an unbleached layer.\n\nAnother type forming section is the cylinder mould machine using a mesh-covered rotating cylinder partially immersed in a tank of fibre slurry in the wet end to form a paper web, giving a more random distribution of the cellulose fibres. Cylinder machines can form a sheet at higher consistency, which gives a more three dimensional fibre orientation than lower consistencies, resulting in higher calliper (thickness) and more stiffness in the machine direction (MD). High MD stiffness is useful in food packaging like cereal boxes and other boxes like dry laundry detergent.\n\nTissue machines typically form the paper web between a wire and a special fabric (felt) as they wrap around a forming roll. The web is pressed from the felt directly onto a large diameter dryer called a \"yankee\". The paper sticks to the yankee dryer and is peeled off with a scraping blade called a \"doctor\". Tissue machines operate at speeds of up to 2000 m/min.\n\nThe second section of the paper machine is the press section, which removes much of the remaining water via a system of nips formed by rolls pressing against each other aided by press felts that support the sheet and absorb the pressed water. The paper web consistency leaving the press section can be above 40%.\n\nPressing is the most efficient method of de-watering the sheet as only mechanical action is required. Press felts historically were made from wool. However, today they are nearly 100% synthetic. They are made up of a polyamide woven fabric with thick batt applied in a specific design to maximise water absorption.\n\nPresses can be single or double felted. A single felted press has a felt on one side and a smooth roll on the other. A double felted press has both sides of the sheet in contact with a press felt. Single felted nips are useful when mated against a smooth roll (usually in the top position), which adds a two-sidedness—making the top side appear smoother than the bottom. Double felted nips impart roughness on both sides of the sheet. Double felted presses are desirable for the first press section of heavy paperboard.\n\nSimple press rolls can be rolls with grooved or blind drilled surface. More advanced press rolls are suction rolls. These are rolls with perforated shell and cover. The shell made of metal material such as bronze stainless steel is covered with rubber or a synthetic material. Both shell and cover are drilled throughout the surface. A stationary suction box is fitted in the core of the suction roll to support the shell being pressed. End face mechanical seals are used for the interface between the inside surface of the shell and the suction box. For the smooth rolls, they are typically made of granite rolls. The granite rolls can be up to long and in diameter.\n\nConventional roll presses are configured with one of the press rolls is in a fixed position, with a mating roll being loaded against this fixed roll. The felts run through the nips of the press rolls and continues around a felt run, normally consisting of several felt rolls. During the dwell time in the nip, the moisture from the sheet is transferred to the press felt. When the press felt exits the nip and continues around, a vacuum box known as an Uhle Box applies vacuum (normally -60 kPa) to the press felt to remove the moisture so that when the felt returns to the nip on the next cycle, it does not add moisture to the sheet.\n\nSome grades of paper use suction pick up rolls that use vacuum to transfer the sheet from the couch to a lead in felt on the first press or between press sections. Pickup roll presses normally have a vacuum box that has two vacuum zones (low vacuum and high vacuum). These rolls have a large number of drilled holes in the cover to allow the vacuum to pass from the stationary vacuum box through the rotating roll covering. The low vacuum zone picks up the sheet and transfers, while the high vacuum zone attempts to remove moisture. Unfortunately, at high enough speed centrifugal force flings out vacuumed water, making this less effective for dewatering. Pickup presses also have standard felt runs with Uhle boxes. However, pickup press design is quite different, as air movement is important for the pickup and dewatering facets of its role.\n\nCrown Controlled Rolls (also known as CC Rolls) are usually the mating roll in a press arrangement. They have hydraulic cylinders in the press rolls that ensure that the roll does not bow. The cylinders connect to a shoe or multiple shoes to keep the crown on the roll flat, to counteract the natural \"bend\" in the roll shape due to applying load to the edges.\n\nExtended Nip Presses (or ENP) are a relatively modern alternative to conventional roll presses. The top roll is usually a standard roll, while the bottom roll is actually a large CC roll with an extended shoe curved to the shape of the top roll, surrounded by a rotating rubber belt rather than a standard roll cover. The goal of the ENP is to extend the dwell time of the sheet between the two rolls thereby maximising the de-watering. Compared to a standard roll press that achieves up to 35% solids after pressing, an ENP brings this up to 45% and higher—delivering significant steam savings or speed increases. ENPs densify the sheet, thus increasing tensile strength and some other physical properties.\n\nThe dryer section of the paper machine, as its name suggests, dries the paper by way of a series of internally steam-heated cylinders that evaporate the moisture. Steam pressures may range up to 160 psig. Steam enters the end of the dryer head (cylinder cap) through a steam joint and condensate exits through a siphon that goes from the internal shell to a centre pipe. From the centre pipe the condensate exits through a joint on the dryer head. Wide machines require multiple siphons. In faster machines, centrifugal force holds the condensate layer still against the shell and turbulence generating bars are typically used to agitate the condensate layer and improve heat transfer.\n\nThe sheet is usually held against the dryers by long felt loops on the top and bottom of each dryer section. The felts greatly improve heat transfer. Dryer felts are made of coarse thread and have a very open weave that is almost see through, It is common to have the first bottom dryer section unfelted to dump broke on the basement floor during sheet breaks or when threading the sheet.\n\nPaper dryers are typically arranged in groups called \"sections\" so that they can be run at a progressively slightly slower speed to compensate for sheet shrinkage as the paper dries. Some grades of paper may also stretch as they run through the machine, requiring increasing speed between sections. The gaps between sections are called \"draws\".\n\nThe drying sections are usually enclosed to conserve heat. Heated air is usually supplied to the pockets where the sheet breaks contact with the driers. This increases the rate of drying. The pocket ventilating tubes have slots along their entire length that face into the pocket. The dryer hoods are usually exhausted with a series of roof mounted hood exhausts fans down the dryer section.\n\nAdditional sizing agents, including resins, glue, or starch, can be added to the web to alter its characteristics. Sizing improves the paper's water resistance, decreases its ability to fuzz, reduces abrasiveness, and improves its printing properties and surface bond strength. These may be applied at the wet (internal sizing) or on the dry end (surface sizing), or both. At the dry end sizing is usually applied with a \"size press\". The size press may be a roll applicator (flooded nip) or Nozzle applicator . It is usually placed before the last dryer section. Some paper machines also make use of a 'coater' to apply a coating of fillers such as calcium carbonate or china clay usually suspended in a binder of cooked starch and styrene-butadiene latex. Coating produces a very smooth, bright surface with the highest printing qualities.\n\nA calender consists of two or more rolls, where pressure is applied to the passing paper. Calenders are used to make the paper surface extra smooth and glossy. It also gives it a more uniform thickness. The pressure applied to the web by the rollers determines the finish of the paper.\n\nAfter calendering, the web has a moisture content of about 6% (depending on the furnish). The paper is wound onto metal spools using a large cylinder called a \"reel drum\". Constant nip pressure is maintained between the reel drum and the spool, allowing the resulting friction to spin the spool. Paper runs over the top of the reel drum and is wound onto the spool to create a \"master roll\".\n\nTo be able to keep the paper machine running continuously, the reel must be able to quickly switch from winding a finished roll to an empty spool without stopping the flow of paper. To accomplish this, each reel section will have two or more spools rotating through the process. Using an overhead crane, empty spools will be loaded onto two \"primary arms\" above the reel drum. When the master roll reaches its maximum diameter, the arms will lower the new spool into contact with the reel drum and a machine behind the drum will run a tape along the moving sheet of paper, swiftly tearing it and attaching incoming paper onto the new spool. The spool is then lowered onto the \"secondary arms\", which steadily guide the spool away from the reel drum as the diameter of paper on the spool increases.\nThe roll hardness should be checked, obtained and adjusted accordingly to insure that the roll hardness is within the acceptable range for the product.\nReels of paper wound up at the end of the drying process are the full trimmed width, minus shrinkage from drying, of the web leaving the wire. In the winder section reels of paper are slit into smaller rolls of a width and roll diameter range specified by a customer order. To accomplish this the reel is placed on an unwind stand and the distances between the slitters (sharp cutting wheels), are adjusted to the specified widths for the orders. The wider is run until the desired roll diameter is reached and the rolls are labeled according to size and order before being sent to shipping or the warehouse. A reel usually has sufficient diameter to make two or more sets of rolls.\n\nbroke: waste paper, either made during a sheet break or trimmings. It is gathered up and put in a repulper for recycling back into the process.\n\nconsistency: the percent dry fibre in a pulp slurry.\n\ncouch: French meaning \"to lie down\". Following the couch roll the sheet is lifted off the wire and transferred into the press section.\n\ndandy roll: a mesh covered hollow roll that rides on top of the Fourdrinier. It breaks up fibre clumps to improve the sheet formation and can also be used to make an imprint, as with laid paper. See also watermark.\n\nfan pump: the large pump that circulates white water from the white water chest to the headbox. The pump is a special low pulse design that minimizes the effect of vane pulses which would cause uneven basis weight of paper in the machine direction known as \"barring\". The flow from the fan pump may go through screens and cleaners, if used. On large paper machines fan pumps may be rated in tens of thousands of gallons per minute.\n\nfelt: a loop of fabric or synthetic material that goes between press rolls and serves as a place to receive the pressed out water. Felts also support the wet paper web and guide it through the press section. Felts are also used in the dryer section to keep the sheet in close contact with the dryers and increase heat transfer.\n\nfiller: a finely divided substance added to paper in the forming process. Fillers improve print quality, brightness and opacity. The most common fillers are clay and calcium carbonate. Titanium dioxide is a filler but also improves brightness and opacity. Use of calcium carbonate filler is the process called \"alkaline sizing\" and uses different chemistry than acid sizing. Alkaline sized paper has superior ageing properties.\n\nformation: the degree of uniformity of fiber distribution in finished paper, which is easily seen by holding paper up to the light.\n\nheadbox: the pressure chamber where turbulence is applied to break up fibre clumps in the slurry. The main job of the headbox is to distribute the fiber slurry uniformly across the wire.\n\nnip: the contact area where two opposing rolls meet, such as in a press or calender.\n\npH: the degree of acidity or alkalinity of a solution. Alkaline paper has a very long life. Acid paper deteriorates over time, which caused libraries to either take conservation measures or replace many older books.\n\nsize: a chemical (formerly rosin derived but now a different chemical) or starch, applied to paper to retard the rate of water penetration. Sizing prevents \"bleeding\" of ink during printing, improving the sharpness of printing.\n\nslice: the adjustable rectangular orifice, usually at the bottom of the headbox, through which the whitewater jet discharges onto the wire. The slice opening and water pressure together determine the amount and velocity of whitewater flow through the slice. The slice usually has some form of adjustment mechanism to even out the paper weight profile across the machine (CD profile), although a newer methods is to inject water into the whitewater across the headbox slice area, thereby using localized consistency to control CD weight profile.\n\nstock: a pulp slurry that has been processed in the stock preparation area with necessary additives, refining and pH adjustment and ready for making paper\n\nweb: the continuous flow of un-dried fibre from the couch roll down the paper machine\n\nwhite water: filtrate from the drainage table. The white water from the table is usually stored in a white water chest from which it is pumped by the fan pump to the headbox.\n\nwire: the woven mesh fabric loop that is used for draining the pulp slurry from the headbox. Until the 1970s bronze wires were used but now they are woven from coarse mono-filament synthetics similar to fishing line but very stiff.\n\nStainless steels are used extensively in the Pulp and Paper industry for two primary reasons, to avoid iron contamination of the product and their corrosion resistance to the various chemicals used in the paper making process. Type 316 stainless steel is a common material used in paper machines.\n\n\n\n"}
{"id": "8102960", "url": "https://en.wikipedia.org/wiki?curid=8102960", "title": "Pearl River Tower", "text": "Pearl River Tower\n\nPearl River Tower (; or ) is a 71-story, , clean technology neofuturistic skyscraper at the junction of Jinsui Road/Zhujiang Avenue West, Tianhe District, Guangzhou, China. The tower's architecture and engineering were performed by Skidmore, Owings & Merrill with Adrian D. Smith and Gordon Gill (now at their own firm, AS+GG) as architects. Ground broke on the tower on 8 September 2006 and construction was completed in March 2011. It is intended for office use and is partially occupied by the China National Tobacco Corporation.\n\nThe design of the Pearl River Tower is intended to minimise harm to the environment and it will extract energy from the natural and passive forces surrounding the building. Major accomplishments are the technological integration of form and function in a holistic approach to engineering and architectural design.\n\nThe building is designed with energy conservation in mind, including wind turbines and solar collectors, photovoltaic cells, underfloor air distribution, and radiant heating and cooling ceilings. It is one of the most environmentally friendly buildings in the world.\n\nOf Pearl River Tower’s accomplishments, many are related to the sustainable design features include:\n\nIn a report presented at the 2008 Council on Tall Buildings and Urban Habitat it was reported that the building's sustainable design features will allow a 58% energy usage reduction when compared to similar stand alone buildings. The building would have been able to be carbon neutral and actually sell power back to the surrounding neighborhood if the micro-turbines had been installed into the building. However the local power company in Guangzhou does not allow independent energy producers to sell electricity back to the grid. Without the financial incentive to add the micro-turbines the developers removed them from the design. If they had been added excess power would have been produced from the building, at the very least, after office hours when the power needed by the building itself had been reduced.\n\n\n"}
{"id": "7914829", "url": "https://en.wikipedia.org/wiki?curid=7914829", "title": "Plastic bicycle", "text": "Plastic bicycle\n\nA small number of plastic bicycle models have been produced. A notable example is the \"Itera\" Plastic Bicycle, a 1980s Swedish attempt to modernize the bicycle by replacing metal with injection moulded plastic composite materials. The project ended in technical and commercial failure after three years.\n\nIn December 1977, Volvo in Gothenburg wanted to produce minicars and investigated conditions and designs. In January 1978, it proposed plastic composite materials in many parts of the car. Other products were considered, the bicycle one of them. In October 1978, a grant of SEK 54,000 from the Swedish National Board for Technical Development financed a prototype plastic bicycle.\n\nIn February 1980, a rideable bicycle was demonstrated to the state-owned bank and a loan of SEK 6 million granted.\nIn March 1980, \"Itera Development Center AB\" was born to design, produce, and market composite-material bicycles.\n\nIn September 1981, the first bicycles were shown to retailers and the press. Nationwide advertising was scheduled for spring 1982. The bikes were delivered unassembled with tools provided in the box. In autumn 1981, more than 100,000 Swedes indicated they were interested. In February 1982, production started in Vilhelmina. However, sales were slow and the news media lost interest. The fact that not every box contained every part to assemble a bicycle frustrated many customers.\n\nThe following year was worse. Boxes of bicycles piled up in shops. Among them were returns, mostly with broken parts. Where metal bends, plastic breaks. Getting replacements was not simple as the parts were incompatible with other bikes. \nAttempts to save the project were unsuccessful.\n\nIn 1983, 1,000 \"Itera bicycles\" were purchased for a national five-day orienteering contest. They were rented to participants who were invited to buy them after the contest. All were sold. It showed there was a market but it was too late. Campaigns such as lower prices failed.\n\nProduction ended in 1985. Approximately 30,000 Itera bicycles were produced. The stock was sold to the Caribbean region where they became popular as rust is a problem with metal bicycles.\n\nThis was not, however, the first attempt at a plastic bicycle. Popular Science featured ads in 1973 from \"The Original Plastic Bike Inc\", with plans to make the bicycle - including the chain and hubs, possibly excluding the spokes - entirely from injection-molded Lexan. The company was founded on December 15, 1971 by John P. Marzullo, Roger Stark, and William Thompson, based on ideas by Joseph Dorrity and Charles Cadorette. It is unknown whether the company sold a single bicycle, but some prototypes were made. The claim was that it would weigh less than 17 pounds, but would be stronger than steel.\n\nThe EADS Airbike is a bicycle constructed from laser-formed nylon powder and made by EADS.\n\nThe FRII Injection molding recycled plastic bicycles is a rather new design, from 2011. It is to be made entirely from injection-molded recycled plastic. \nThe chair set can be pulled out and replaced to adjust to the height of the rider. It is to be produced by local industries for economy and ecological reasons, as its manufacture process is simple and low cost in large quantities.\nDesigner: Dror Peleg, a graduate of Bezalel Academy of Art and Design in Jerusalem.\n\n\n\nAll links are in Swedish:\n"}
{"id": "7616600", "url": "https://en.wikipedia.org/wiki?curid=7616600", "title": "Polytechnic Museum", "text": "Polytechnic Museum\n\nThe Polytechnic Museum () is one of the oldest science museums in the world, located in Moscow that emphasizes the progress of Russian and Soviet technology and science, as well as modern inventions and developments. It was founded in 1872 after the first All-Russian Technical Exhibition on the bicentennial anniversary of the birth of Peter the Great at the initiative of the Society of Devotees of Natural Science, Anthropology, and Ethnography. The first stage of the museum was designed by Ippolit Monighetti and completed in 1877. Almost from the beginning, the collection was too big for the space. The north wing was added in 1896 and the south wing in 1907. At present the main building of the museum (Novaya sq., 3/4) is closed for reconstruction. The opening of the renovated museum in this building is scheduled for 2018. From 2013 and for the whole period of renovation the museum will work in partnership venues in Moscow.\n\nIt is the largest technical museum in Russia, offering a wide array of historical inventions and technological achievements, including humanoid automata of the 18th century and the first Soviet computers. Its collection contains more than 160,000 items in 65 halls including, chemistry, mining, metallurgy, transport, energy, optics, automation, computer engineering, radio electronics, communications, and space exploration.\n\nHighlights include the first achromatic telescope; an early solar microscope, created by German anatomists Johann Nathanael Lieberkühn; an early seismograph created by Boris Borisovich Galitzine; galvanoplastics by Moritz von Jacobi; and early electric lights by Pavel Yablochkov. The automobile exhibit includes a Russo-Balt K12/20 and a GAZ-M20 Pobeda.\n\nThe Society of Devotees of Natural Science was formed in Moscow in 1863. The society's first President was Gregory Ephimovich Shchurovsky and he together with other leading members of the society discussed having a museum. Their first move in this direction was to establish a library this held books documenting the history of science and technology. This became the Central Polytechnic Library but this established their ambitions. In 1871 Moscow council set aside half a million roubles to create a museum. A committee was formed with Grand Duke Konstantin Nikolayevich as honorary chair. The formation of a museum was timely as Peter the Great's 200th anniversary would inspire the All-Russian Technical Exhibition that would be used to launch the new museum. The exhibits at the Exhibition were dedicated to the 200th anniversary of the birth of Peter the Great.\n\n\nAs of January 1, 2013 the museum fund of the museum consisted of 229,348 items. The museum features about 150 museum collections of national and world importance. The funds of the museum are dominated by scientific and technical typological collections consisting of functional technical objects. They can be used as a source of information about the main stages and directions of development of a particular type of technology.\n\nA highlight of the museum is the collection of vehicles, including the sections Bicycles, Motorcycles, Cars. The pearl of the car collection is the world only surviving Russian car built before the Revolution Russo-Balt K12/20 released in 1911.\n\nAll technologies combined in the thematic fund \"Electronics and Communication\" is presented in the collections \"Exchanges\", \"Sound recording and playback technique\", \"TV receivers\", \"Radio receivers\". Museum collection \"Telephones\" shows the development of terminal telephone systems from Bell's receiver to modern mobile phones. The collection \"Telegraph technique\" includes all types of telegraph and facsimile technique since creation of the first electromagnetic machine by P.L. Schilling in 1832 and until 1980-s.\n\nThe collection of computing equipment is the most comprehensive display in Russia and includes rare copyrights devices, such as automated abacus by Viktor Bunyakovsky, one of the first models of Odner's adding machine, the only surviving copy of the domestic computer \"Ural\", hydraulic integrator by , the world's only computer based on ternary logic, \"Syetun\" and many other rarities.\n\nStart of modernization\nIn April 2010 the President of the Russian Federation Dmitry Medvedev instructed the Russian government to develop a concept of a museum of science on the basis of the Polytechnic Museum. Modernization of the museum is a large-scale state innovation project. The modernization program for 2017-2018 envisages the creation of a fundamentally new museum and educational center covering the greatest achievements of science, technology and society.\nModernization goals\nTender for concept development\nOn the basis of the decree of the Ministry of Culture of the Russian Federation the Development fund of the Polytechnic Museum held a tender for the development of the museum concept. As a result of a choice from 14 competitive bids provided by Russian and foreign companies specializing in museum design, British company Event Communications Ltd. was selected.\n\nFrom October 2010 to February 2011 Event Communications with the active participation of Russian experts and staff of the museum developed the concept of the museum of science on the basis of the Polytechnic Museum. The concept was coordinated by the Board of Trustees of the museum in February 2011 and approved by decree of the Ministry of Culture of the Russian Federation.\n\nThe concept of the museum provides, in particular, for the reconstruction of its historic building, extension of educational areas and public spaces, development of educational and other communications programs for visitors of all ages.\n\nThe concept also includes updating the content and design of the permanent exhibition and its division into three sections – Matter, Energy and Information. These sections represent all major areas of science and technology, as well as interdisciplinary areas, including developing fields such as life-science.\n\nMoreover, according to the concept, Moscow plans to create a second site - a museum and educational center of the Polytechnic Museum and Lomonosov MSU. that will be located in the campus of Moscow State University on Vorobyevy Gory.\n\nTender for the architectural concept of historical building reconstruction\nIn accordance with the order of the Ministry of Culture of the Russian Federation of 28 April 2011, a two-stage international architectural competition for the completion of an architectural project of reconstruction and restoration of the building of the Polytechnic Museum in Moscow was announced on 6 June 2011. The tender attracted a number of famous Russian and foreign architectural companies.\n\nFinally, the concept of a Japanese architect Junia Ishigami was chosen as a winner.\n\nThe concept of a \"museum - park\" demonstrates a new approach to the historical building of the museum as a more open architectural object that plays an important role in the urban space.\n\nThe architectural concept by Junia Ishigami provides for extremely delicate attitude to historical restoration of the building. At the same time the architect offered the covering of museum courtyards with innovative designs constructions that shall keep the same three-dimensional structure of the building, its shape in the urban landscape and at the same time increase the area of public spaces that are required by all contemporary museums. The basement of the building will be open for the visitors, thus creating additional spatial volume that can play an important social role in the composition of the center of Moscow.\nTender for architectural concepts of the museum and educational center of the Polytechnic Museum and Lomonosov MSU.\nThe tender for architectural concepts of the museum and educational center of the Polytechnic Museum and Lomonosov MSU was announced in October 2012. Projects were evaluated based on a number of criteria such as marketability, quality planning, urban planning and conceptual solutions, as well as the planned level of spending on construction and operation. The winner of the competition was a consortium of two companies: Massimiliano Fuksas Architetto (Italy) and Speech (Russia). The winners the draft of a three-storey building on the glazed two-level basis, 9.2 m high, with the volume of the upper level making up 25 m, a terrace level that is about 6.4 m high, 2 underground levels, as well as an open-plan lounge with passages to different zones.\n\nIn the future, the museum and educational center is expected to take an important place in the life of Moscow, to become a multi-functional cultural and educational center, a platform of communication for both the scientific community and the general audience interested in modern science. The project is expected to be completed in 2017.\n\nSites of the Polytechnic Museum at the time of reconstruction\nOn 9 January 2013 permanent exhibitions of the museum were closed to visitors. During the time of reconstruction all fund, exposition and educational activities of the museum will be conducted in three major venues in Moscow.\n\nExposition \"Russia Does It Itself\" will be placed in pavilion No. 26 (former transport pavilion) of the WEC. The main idea of the exhibition is to introduce the visitors to the achievements of Russian science and technology as a significant part of the global scientific context. At the time of reconstruction of the historical building the WEC site will not only serve as a representative of the museum, but also become an active space for development and introduction of modern technologies and methods of museum communication that will later be widely used in the modernized museum.\n\nThe display in the Transport pavilion is scheduled to open in spring 2014. \nThe funds of the museum during the period of reconstruction will be located in the former AZLK as well as the museum library.\nSince autumn 2013 a lecture hall of the museum and a research laboratory for children has been working in the ZIL Cultural Center. \nAreas of activity\nDuring the renovation of the historic building the Polytechnic Museum will operate on temporary and partner sites in Moscow and regions. The main areas of work will be exhibitions, lectures, conferences, festivals and other projects that meet the core mission of the museum - the popularization of science and enlightenment.\n\"Museum exhibition activities\" are implemented jointly with various Russian and foreign partners. For example, the Polytechnic Museum presented the international exhibition Science Tunnel of Max Planck that has been successfully displayed all over the world for 13 years. One of the most vivid and memorable interactive exhibits was Game: A Look into the Future organized by the Polytechnic Museum on the site in ZIL cultural center together with Science Gallery (Dublin, Ireland) and a team of world-famous game designers, artists, game developers and curators. Exhibition Inventing the Bicycle that took place in the WEC gained much popularity. It presented exhibits from the funds of the museum and other private and museum collections.\n\nPolytechnic Museum has an active exhibition program in the regions on the partner sites of other museums (in Yaroslavl, Tula, Vologda, Rybinsk and other cities).\n\"Educational programs\" of the museum include a variety of areas for children and adults. \"Scientific laboratories\" of the Polytechnic Museum are fun and informative workshops that take place in the form of games. The programs have been developed by the best scientists specifically for children and the lessons are given by wonderful young teachers. During training the children not only get additional knowledge about different disciplines, but also carry out their own research, as well as create exhibits for displays. Today the museum offers laboratories in chemistry, physics, biology, mathematics, robotics, as well as a children's center of automotive design and a children's lecture hall. Moreover, in September 2013 a University of Children was opened. This is an educational project of the museum created in partnership with HSE (Higher School of Economics). University of Children is a center of additional education for pupils aged 8–11. During lectures and seminars, university students make experiments and carry out their own research with the help of scientists and experts from different fields of science. \nSeptember 2013 was distinguished by the start of the \"Festival of the Science Culture Polifest\" - an outdoor science holiday that was held in the Industry square of WEC. \nSince 2011, the Polytechnic Museum has been organizing a \"Contemporary Science Film Festival 360°\", showing visitors from Moscow and other regions the best documentaries from around the world that received awards at major international festivals. Most of the films are Russian premieres, and the festival has always had full houses in cinemas, partner universities and other sites, wherever the screenings take place.\nIn 2013, together with the RIA Novosti news agency the museum launched a topical educational project Science Monday, aimed at the promotion of modern science. The project shows the best documentaries on scientific topics. The purpose of the project is to unite people of different generations that share the ideals of self-education and enlightenment. Also, the project aims to promote the development of science journalism. The viewers are invited to become participants in the dialogue, allies or opponents of the filmmakers.\nIn September 2013, in partnership with The World Science Festival the Polytechnic Museum in Moscow presented a contemporary reading of the legend of Icarus—a multimedia performance \"Icarus at the Edge of Time\", based on the eponymous book by the famous scientist, theoretical physicist, professor at Columbia University, Brian Greene, one of the most famous contemporary composers Philip Glass and video artists duo Al + Al. The premiere took place in Moscow in the presence of the author. \nIn 2013, the Polytechnic Museum joined the board of the International Committee for Museums and Collections of Science and Technology ICIMUSET of International Council of Museums ICOM under the aegis of UNESCO.\n\nIn addition to its function as a museum, the Polytechnic Museum has been an important place for dissemination of science and culture in Russian. From 1913 to 1918 it was the centre of discussions about Russian avant-garde, with public lectures given by Vladimir Mayakovsky, David Burlyuk, Andrei Bely, Alexei Kruchenykh, Velimir Khlebnikov. In the period of Khruschev thaw, its main auditorium was the place for public performances of Andrei Voznesensky, Robert Rozhdestvensky and Bulat Okudzhava. This was also a place for popular science lectures given by Élie Metchnikoff, Alexander Fersman and Niels Bohr.\n\n"}
{"id": "419615", "url": "https://en.wikipedia.org/wiki?curid=419615", "title": "Power cord", "text": "Power cord\n\nA power cord, line cord, or mains cable is an electrical cable that temporarily connects an appliance to the mains electricity supply via a wall socket or extension cord. The terms are generally used for cables using a power plug to connect to a single-phase alternating current power source at the local line voltage—(generally 100 to 240 volts, depending on the location). The terms power cable, mains lead, flex or kettle lead are also used. A lamp cord (also known as a zip cord) is a light-weight, ungrounded, single-insulated two-wire cord used for small loads such as a table or floor lamp.\n\nA cord set includes connectors molded to the cord at each end (see Appliance coupler). Cord sets are detachable from both the power supply and the electrical equipment, and consist of a flexible cord with electrical connectors at either end, one male, and one female. One end of the cord set is attached to a molded electrical plug; the other is typically a molded electrical receptacle to prevent the possibility of having an exposed live prong or pin which would cause electric shock. The female connector attaches to the piece of equipment or appliance while the male plug connects to the electrical receptacle or outlet.\n\nPower cables may be either fixed or detachable from the appliance. In the case of detachable leads, the appliance end of the power cord has a female connector to link it to the appliance, to avoid the dangers from having a live protruding pin. Cords may also have twist-locking features, or other attachments to prevent accidental disconnection at one or both ends. A cord set may include accessories such as fuses for overcurrent protection, a pilot lamp to indicate voltage is present, or a leakage current detector. Power cords for sensitive instruments, or audio/video equipment may also include a shield over the power conductors to minimize electromagnetic interference.\n\nA power cord or appliance coupler may have a retaining clamp, a mechanical device that prevents it from inadvertently being pulled or shaken loose. Typical application areas with stricter safety requirements include medical technology, stage and lighting technology, and computing equipment. For specialty equipment such as construction machinery, sound and lighting equipment, emergency medical defibrillators and electrical power tools, used in locations without a convenient power source, extension cords are used to carry the electric current up to hundreds of feet away from an outlet.\n\nIn North America, the National Electrical Manufacturers Association develops standards for electrical plugs and receptacles and cables.\n\nInternational power cords and plug adapters are used in conjunction with electrical appliances in countries different from those in which they were designed to operate. Besides a cord with one end compatible to receptacles or a device from one country and the other end compatible with receptacles or devices from another country, a voltage converter is usually necessary, as well, to protect travelers' electronic devices, such as laptops, from the differing voltages between the United States and places like Europe or Africa.\n\nNorth American lamp cords have two single-insulated conductors designed for low-current applications. The insulator covering one of the conductors is ribbed (parallel to wire) for the entire length of the cord, while the other conductor's insulator is smooth. The smooth one is hot and the ribbed one is neutral.\n\nIEC 60320 power cables come in normal and high-temperature variants, as well as various rated currents. The connectors have slightly different shapes to ensure that it is not possible to substitute a cable with a lower temperature or current rating, but that it \"is\" possible to use an overrated cable. Cords also have different types of exterior jackets available to accommodate environmental variables such as moisture, temperature, oils, sunlight, flexibility, and heavy wear. For example, a heating appliance may come with a cord designed to withstand accidental contact with heated surfaces.\n\nWorldwide, more than a dozen different types of AC power plugs and sockets are used for fixed building wiring. Products sold in many different markets can use a standardized IEC 60320 connector and then use a detachable power cord to match the local electrical outlets. This simplifies safety approvals, factory testing, and production since the power cord is a low-cost item available as a commodity. Since the same types of appliance-side connectors are used with both 110 V and 230 V power cables, the user must ensure the connected equipment will operate with the available voltage. Some devices have a slide-switch to adapt to different voltages, or wide-ranging power supplies.\n\nNational electrical codes may apply to power cords and related items. For example, in the United States, power cords must meet UL Standards 62 and 817.\n\nCord sets must be distinguished from AC adapters, where the connector also contains a transformer, and possibly rectifiers, filters and regulators. Unwary substitution of a standard mains-voltage connector for the power supply would result in application of full line voltage to the connected device, resulting in its destruction and possible fire or personal injury.\n\n"}
{"id": "2941730", "url": "https://en.wikipedia.org/wiki?curid=2941730", "title": "Powered speakers", "text": "Powered speakers\n\nPowered speakers, also known as self-powered speakers and active speakers, are loudspeakers that have built-in amplifiers. Powered speakers are used in a range of settings, including in sound reinforcement systems (used at live music concerts), both for the main speakers facing the audience and the monitor speakers facing the performers; by DJs performing at dance events and raves; in private homes as part of hi-fi or home cinema audio systems and as computer speakers. They can be connected directly to a mixing console or other low-level audio signal source without the need for an external amplifier. Some active speakers designed for sound reinforcement system use have an onboard mixing console and microphone preamplifier, which enables microphones to be connected directly to the speaker. \n\nActive speakers have several advantages, the most obvious being their compactness and simplicity. Additionally the amplifier(s) can be designed to closely match the optimal requirements of the speaker it will power; and the speaker designer is not required to include a passive crossover, decreasing production cost and possibly sound quality. Some also claim that the shorter distances between components can decrease external interference and increase fidelity; although this is highly dubious, and the reciprocal argument can also be made. Disadvantages include heavier loudspeaker enclosures; reduced reliability due to active electronic components within; and the need to supply both the audio signal and power to every unit separately, typically requiring two cables to be run to each speaker (as opposed to the single cable required with passive speakers and an external amplifier).\n\nPowered speakers are available with passive or active crossovers built into them. Since the early 2000s, powered speakers with active crossovers and other DSP have become common in sound reinforcement applications and in studio monitors. Home theater and add-on domestic/automotive subwoofers have used active powered speaker technology since the late 1980s.\n\nThe terms \"powered\" and \"active\" have been used interchangeably in loudspeaker designs, however, a differentiation may be made between the terms:\n\nHybrid active designs exist such as having three drivers powered by two internal amplifiers. In this case, an active two-way crossover splits the audio signal, usually into low frequencies and mid-high frequencies. The low-frequency driver is driven by its own amplifier channel while the mid- and high-frequency drivers share an amplifier channel, the output of which is split by a passive two-way crossover.\n\nThe term \"active speakers\" can also refer to an integrated \"active system\" in which passive loudspeakers are mated to an external system of multiple amplifiers fed by an active crossover. These active loudspeaker systems may be built for professional concert touring such as the pioneering JM-3 system designed in 1971 by Harry McCune Sound Service, or they may be built for high-end home use such as various systems from Naim Audio and Linn Products.\n\nSome of the first powered loudspeakers were JBL monitor speakers. With the addition of the SE401 Stereo Energizer, introduced in 1964, any pair of monitor speakers could be converted to self-powered operation with the second speaker powered by the first. The first studio monitor with an active crossover was the OY invented 1967 by Klein-Hummel. It was a hybrid three-way design with two internal amplifier channels. An early example of a bi-amplified powered studio monitor is the Altec 9846B, introduced in 1971, which combined the passive 9846-8A speaker with the new 771B Bi-amplifier with 60 watts for the woofer and 30 watts for the high frequency compression driver. In the late 1970s, Paramount Pictures contracted with AB Systems to design a powered speaker system.\n\nIn 1980, Meyer Sound Laboratories produced an integrated active 2-way system, the passive UPA-1, which incorporated lessons John Meyer learned on the McCune JM-3. It used active electronics mounted outside of the loudspeaker enclosure, including Meyer's integrated active crossover with feedback comparator circuits determining the level of limiting, often connected to third-party customer-specified amplifiers. In 1990, Meyer produced its first powered speaker: the HD-1, a 2-way studio monitor with all internal electronics. In the early '90s, after years of dealing with the disadvantages of passive systems, especially varying gain settings on third-party amplifiers, John Meyer decided to stop making passive speakers and devote his company to active designs. Meyer said he \"hired an ad agency to research how people felt about powered speakers for sound reinforcement, and they came back after a survey and said that nobody wanted them.\" Sound reinforcement system operators said they did not want loudspeakers in which they could not see the amplifier meters to determine whether the loudspeakers were working properly during a concert. Nevertheless, Meyer kept to his decision and produced the MSL-4 in 1994, the first powered loudspeaker intended for concert touring. The UPA-1 was converted to a self-powered configuration in 1996 and the rest of Meyer's product line followed suit.\n\nThe main benefit of active versus passive speakers is in the higher fidelity associated with active crossovers and multiple amplifiers, including less IMD, higher dynamic range and greater output power. The amplifiers within the loudspeaker enclosure may be ideally matched to the individual drivers, eliminating the need for each amplifier channel to operate in the entire audio bandpass. Driver characteristics such as power handling and impedance may be matched to amplifier capabilities. More specifically, active speakers have very short speaker cables inside the enclosure, so very little voltage and control is lost in long speaker cables with higher resistance.\n\nAn active speaker often incorporates equalization tailored to each driver's response in the enclosure. This yields a flatter, more neutral sound. Limiting circuits (high-ratio audio compression circuits) can be incorporated to increase the likelihood of the driver surviving high-SPL use. Such limiters may be carefully matched to driver characteristics, resulting in a more dependable loudspeaker requiring less service. Distortion detection may be designed into the electronics to help determine the onset of protective limiting, reducing output distortion and eliminating clipping.\n\nPassive speakers need only one speaker cable but active speakers need two cables: an audio signal cable and an AC power cable. For multiple-enclosure high-power concert systems, the AC cabling is often smaller in diameter than the equivalent speaker cable bundles, so less copper is used. Some powered speaker manufacturers are now incorporating UHF or more frequently Wi-Fi wireless receivers so the speaker requires only an AC power cable.\n\nA powered speaker usually weighs more than an equivalent passive speaker because the internal amplifier circuitry usually outweighs a speaker-level passive crossover. A loudspeaker associated with an integrated active system is even lighter because it has no internal crossover. A lightweight loudspeaker can be more easily carried and it is less of a load in rigging (flying). However, active speakers using lightweight Class-D amplifiers have narrowed the difference. Trucking for a sound system involves transporting all of the various components including amplifier racks, speaker cabling and loudspeaker enclosures. Overall shipping weight for an active loudspeaker system may be less than for a passive system because heavy passive speaker cable bundles are replaced by lighter AC cables and small diameter signal cables. Truck space and weight is reduced by eliminating amplifier racks.\n\nThe expense of a large concert active speaker system is less than the expense of an equivalent passive system. The passive system, or integrated active system with external electronics, requires separate components such as crossovers, equalizers, limiters and amplifiers, all mounted in rolling racks. Cabling for passive concert systems is heavy, large-diameter speaker cable, more expensive than smaller diameter AC power cables and much smaller audio signal cables. For high-end home use, active speakers usually cost more than passive speakers because of the additional amplifier channels required.\n\nIn professional audio and some home cinema and hi-fi applications, the active speaker may be easier to use because it eliminates the complexity of properly setting crossover frequencies, equalizer curves and limiter thresholds. Cabling is not as simple, however, because active speakers require two cables instead of one (an AC power cable and a cable with the signal, typically an XLR cable). In home audio, some audio engineers argue that a passive speaker, in which an unpowered speaker is connected to an amplifier, is the easiest to install and operate.\n\nBy including a negative feedback loop in the amplifier-speaker system, distortion can be substantially reduced. If mounted at the speaker cone, the sensor is usually an accelerometer. It is possible to monitor the back emf generated by the driver voice coil as it moves within the magnetic gap. In either case, specialist amplifier designs are needed and so servo speakers are inherently powered speakers.\n\nSome bass amplifier manufacturers sell powered speakers designed for adding to the stage power of a combo bass amp. The user plugs a patch cord or XLR cable from the combo amp into the powered speaker.\n\n"}
{"id": "13228814", "url": "https://en.wikipedia.org/wiki?curid=13228814", "title": "Production (economics)", "text": "Production (economics)\n\nProduction is a process of combining various material inputs and immaterial inputs (plans, know-how) in order to make something for consumption (the output). It is the act of creating output, a good or service which has value and contributes to the utility of individuals.\n\nEconomic well-being is created in a production process, meaning all economic activities that aim directly or indirectly to satisfy human wants and needs. The degree to which the needs are satisfied is often accepted as a measure of economic well-being. In production there are two features which explain increasing economic well-being. They are improving quality-price-ratio of goods and services and increasing incomes from growing and more efficient market production.\n\nThe most important forms of production are:\n\nIn order to understand the origin of the economic well-being, we must understand these three production processes. All of them produce commodities which have value and contribute to well-being of individuals.\n\nThe satisfaction of needs originates from the use of the commodities which are produced. The need satisfaction increases when the quality-price-ratio of the commodities improves and more satisfaction is achieved at less cost. Improving the quality-price-ratio of commodities is to a producer an essential way to improve the competitiveness of products but this kind of gains distributed to customers cannot be measured with production data. Improving the competitiveness of products means often to the producer lower product prices and therefore losses in incomes which are to compensated with the growth of sales volume.\n\nEconomic well-being also increases due to the growth of incomes that are gained from the growing and more efficient market production. Market production is the only production form which creates and distributes incomes to stakeholders. Public production and household production are financed by the incomes generated in market production. Thus market production has a double role in creating well-being, i.e. the role of producing goods and services and the role of creating income. Because of this double role market production is the “primus motor” of economic well-being and therefore here under review.\n\nIn principle there are two main activities in an economy, production and consumption. Similarly there are two kinds of actors, producers and consumers. Well-being is made possible by efficient production and by the interaction between producers and consumers. In the interaction, consumers can be identified in two roles both of which generate well-being. Consumers can be both customers of the producers and suppliers to the producers. The customers’ well-being arises from the commodities they are buying and the suppliers’ well-being is related to the income they receive as compensation for the production inputs they have delivered to the producers.\n\nStakeholders of production are persons, groups or organizations with an interest in a producing company. Economic well-being originates in efficient production and it is distributed through the interaction between the company’s stakeholders. The stakeholders of companies are economic actors which have an economic interest in a company. Based on the similarities of their interests, stakeholders can be classified into three groups in order to differentiate their interests and mutual relations. The three groups are as follows:\nThe interests of these stakeholders and their relations to companies are described briefly below. Our purpose is to establish a framework for further analysis. \n\"Customers\"\n\nThe customers of a company are typically consumers, other market producers or producers in the public sector. Each of them has their individual production functions. Due to competition, the price-quality-ratios of commodities tend to improve and this brings the benefits of better productivity to customers. Customers get more for less. In households and the public sector this means that more need satisfaction is achieved at less cost. For this reason the productivity of customers can increase over time even though their incomes remain unchanged.\n\n\"Suppliers\"\n\nThe suppliers of companies are typically producers of materials, energy, capital, and services. They all have their individual production functions. The changes in prices or qualities of supplied commodities have an effect on both actors’ (company and suppliers) production functions. We come to the conclusion that the production functions of the company and its suppliers are in a state of continuous change.\n\n\"Producer community\"\n\nThe incomes are generated for those participating in production, i.e., the labour force, society and owners. These stakeholders are referred to here as producer communities or, in shorter form, as producers. The producer communities have a common interest in maximizing their incomes. These parties that contribute to production receive increased incomes from the growing and developing production.\n\nThe well-being gained through commodities stems from the price-quality relations of the commodities. Due to competition and development in the market, the price-quality relations of commodities tend to improve over time. Typically the quality of a commodity goes up and the price goes down over time. This development favourably affects the production functions of customers. Customers get more for less. Consumer customers get more satisfaction at less cost. This type of well-being generation can only partially be calculated from the production data. The situation is presented in this study.\nThe producer community (labour force, society, and owners) earns income as compensation for the inputs they have delivered to the production. When the production grows and becomes more efficient, the income tends to increase. In production this brings about an increased ability to pay salaries, taxes and profits. The growth of production and improved productivity generate additional income for the producing community. Similarly the high income level achieved in the community is a result of the high volume of production and its good performance. This type of well-being generation – as mentioned earlier - can be reliably calculated from the production data.\n\nA producing company can be divided into sub-processes in different ways; yet, the following five are identified as main processes, each with a logic, objectives, theory and key figures of its own. It is important to examine each of them individually, yet, as a part of the whole, in order to be able to measure and understand them. The main processes of a company are as follows:\n\nProduction output is created in the real process, gains of production are distributed in the income distribution process and these two processes constitute the production process. The production process and its sub-processes, the real process and income distribution process occur simultaneously, and only the production process is identifiable and measurable by the traditional accounting practices. The real process and income distribution process can be identified and measured by extra calculation, and this is why they need to be analyzed separately in order to understand the logic of production and its performance.\n\nReal process generates the production output from input, and it can be described by means of the production function. It refers to a series of events in production in which production inputs of different quality and quantity are combined into products of different quality and quantity. Products can be physical goods, immaterial services and most often combinations of both. The characteristics created into the product by the producer imply surplus value to the consumer, and on the basis of the market price this value is shared by the consumer and the producer in the marketplace. This is the mechanism through which surplus value originates to the consumer and the producer likewise. Surplus values to customers cannot be measured from any production data. Instead the surplus value to a producer can be measured. It can be expressed both in terms of nominal and real values. The real surplus value to the producer is an outcome of the real process, real income, and measured proportionally it means productivity.\n\nThe concept “real process” in the meaning quantitative structure of production process was introduced in Finnish management accounting in 1960s. Since then it has been a cornerstone in the Finnish management accounting theory. (Riistama et al. 1971)\n\nIncome distribution process of the production refers to a series of events in which the unit prices of constant-quality products and inputs alter causing a change in income distribution among those participating in the exchange. The magnitude of the change in income distribution is directly proportionate to the change in prices of the output and inputs and to their quantities. Productivity gains are distributed, for example, to customers as lower product sales prices or to staff as higher income pay.\n\nThe production process consists of the real process and the income distribution process. A result and a criterion of success of the owner is profitability. The profitability of production is the share of the real process result the owner has been able to keep to himself in the income distribution process. Factors describing the production process are the components of profitability, i.e., returns and costs. They differ from the factors of the real process in that the components of profitability are given at nominal prices whereas in the real process the factors are at periodically fixed prices.\n\nMonetary process refers to events related to financing the business. Market value process refers to a series of events in which investors determine the market value of the company in the investment markets.\n\nEconomic growth is often defined as a production increase of an output of a production process. It is usually expressed as a growth percentage depicting growth of the real production output. The real output is the real value of products produced in a production process and when we subtract the real input from the real output we get the real income. The real output and the real income are generated by the real process of production from the real inputs.\n\nThe real process can be described by means of the production function. The production function is a graphical or mathematical expression showing the relationship between the inputs used in production and the output achieved. Both graphical and mathematical expressions are presented and demonstrated. The production function is a simple description of the mechanism of income generation in production process. It consists of two components. These components are a change in production input and a change in productivity.\n\nThe figure illustrates an income generation process(exaggerated for clarity). The Value T2 (value at time 2) represents the growth in output from Value T1 (value at time 1). Each time of measurement has its own graph of the production function for that time (the straight lines). The output measured at time 2 is greater than the output measured at time one for both of the components of growth: an increase of inputs and an increase of productivity. The portion of growth caused by the increase in inputs is shown on line 1 and does not change the relation between inputs and outputs. The portion of growth caused by an increase in productivity is shown on line 2 with a steeper slope. So increased productivity represents greater output per unit of input.\n\nThe growth of production output does not reveal anything about the performance of the production process. The performance of production measures production’s ability to generate income. Because the income from production is generated in the real process, we call it the real income. Similarly, as the production function is an expression of the real process, we could also call it “income generated by the production function”.\n\nThe real income generation follows the logic of the production function. Two components can also be distinguished in the income change: the income growth caused by an increase in production input (production volume) and the income growth caused by an increase in productivity. The income growth caused by increased production volume is determined by moving along the production function graph. The income growth corresponding to a shift of the production function is generated by the increase in productivity. The change of real income so signifies a move from the point 1 to the point 2 on the production function (above). When we want to maximize the production performance we have to maximize the income generated by the production function.\n\nThe sources of productivity growth and production volume growth are explained as follows. Productivity growth is seen as the key economic indicator of innovation. The successful introduction of new products and new or altered processes, organization structures, systems, and business models generates growth of output that exceeds the growth of inputs. This results in growth in productivity or output per unit of input. Income growth can also take place without innovation through replication of established technologies. With only replication and without innovation, output will increase in proportion to inputs. (Jorgenson et al. 2014,2) This is the case of income growth through production volume growth.\n\nJorgenson et al. (2014,2) give an empiric example. They show that the great preponderance of economic growth in the US since 1947 involves the replication of existing technologies through investment in equipment, structures, and software and expansion of the labor force. Further they show that innovation accounts for only about twenty percent of US economic growth.\n\nIn the case of a single production process (described above) the output is defined as an economic value of products and services produced in the process. When we want to examine an entity of many production processes we have to sum up the value-added created in the single processes. This is done in order to avoid the double accounting of intermediate inputs. Value-added is obtained by subtracting the intermediate inputs from the outputs. The most well-known and used measure of value-added is the GDP (Gross Domestic Product). It is widely used as a measure of the economic growth of nations and industries.\n\nThe production performance can be measured as an average or an absolute income. Expressing performance both in average (avg.) and absolute (abs.) quantities is helpful for understanding the welfare effects of production. For measurement of the average production performance, we use the known productivity ratio\n\n\nThe absolute income of performance is obtained by subtracting the real input from the real output as follows:\n\nThe growth of the real income is the increase of the economic value which can be distributed between the production stakeholders. With the aid of the production model we can perform the average and absolute accounting in one calculation. Maximizing production performance requires using the absolute measure, i.e. the real income and its derivatives as a criterion of production performance.\n\nMaximizing productivity also leads to the phenomenon called \"jobless growth\" This refers to economic growth as a result of productivity growth but without creation of new jobs and new incomes from them. A practical example illustrates the case. When a jobless person obtains a job in market production we may assume it is a low productivity job. As a result, average productivity decreases but the real income per capita increases. Furthermore, the well-being of the society also grows. This example reveals the difficulty to interpret the total productivity change correctly. The combination of volume increase and total productivity decrease leads in this case to the improved performance because we are on the “diminishing returns” area of the production function. If we are on the part of “increasing returns” on the production function, the combination of production volume increase and total productivity increase leads to improved production performance. Unfortunately we do not know in practice on which part of the production function we are. Therefore, a correct interpretation of a performance change is obtained only by measuring the real income change.\n\nA production model is a numerical description of the production process and is based on the prices and the quantities of inputs and outputs. There are two main approaches to operationalize the concept of production function. We can use mathematical formulae, which are typically used in macroeconomics (in growth accounting) or arithmetical models, which are typically used in microeconomics and management accounting. We do not present the former approach here but refer to the survey “Growth accounting” by Hulten 2009.\n\nWe use here arithmetical models because they are like the models of management accounting, illustrative and easily understood and applied in practice. Furthermore, they are integrated to management accounting, which is a practical advantage. A major advantage of the arithmetical model is its capability to depict production function as a part of production process. Consequently, production function can be understood, measured, and examined as a part of production process.\n\nThere are different production models according to different interests. Here we use a production income model and a production analysis model in order to demonstrate production function as a phenomenon and a measureable quantity.\n\nThe scale of success run by a going concern is manifold, and there are no criteria that might be universally applicable to success. Nevertheless, there is one criterion by which we can generalise the rate of success in production. This criterion is the ability to produce surplus value. As a criterion of profitability, surplus value refers to the difference between returns and costs, taking into consideration the costs of equity in addition to the costs included in the profit and loss statement as usual. Surplus value indicates that the output has more value than the sacrifice made for it, in other words, the output value is higher than the value (production costs) of the used inputs. If the surplus value is positive, the owner’s profit expectation has been surpassed.\n\nThe table presents a surplus value calculation. We call this set of production data a basic example and we use the data through the article in illustrative production models. The basic example is a simplified profitability calculation used for illustration and modelling. Even as reduced, it comprises all phenomena of a real measuring situation and most importantly the change in the output-input mix between two periods. Hence, the basic example works as an illustrative “scale model” of production without any features of a real measuring situation being lost. In practice, there may be hundreds of products and inputs but the logic of measuring does not differ from that presented in the basic example.\n\nIn this context we define the quality requirements for the production data used in productivity accounting. The most important criterion of good measurement is the homogenous quality of the measurement object. If the object is not homogenous, then the measurement result may include changes in both quantity and quality but their respective shares will remain unclear. In productivity accounting this criterion requires that every item of output and input must appear in accounting as being homogenous. In other words, the inputs and the outputs are not allowed to be aggregated in measuring and accounting. If they are aggregated, they are no longer homogenous and hence the measurement results may be biased.\n\nBoth the absolute and relative surplus value have been calculated in the example. Absolute value is the difference of the output and input values and the relative value is their relation, respectively. The surplus value calculation in the example is at a nominal price, calculated at the market price of each period.\n\nA model used here is a typical production analysis model by help of which it is possible to calculate the outcome of the real process, income distribution process and production process. The starting point is a profitability calculation using surplus value as a criterion of profitability. The surplus value calculation is the only valid measure for understanding the connection between profitability and productivity or understanding the connection between real process and production process. A valid measurement of total productivity necessitates considering all production inputs, and the surplus value calculation is the only calculation to conform to the requirement. If we omit an input in productivity or income accounting, this means that the omitted input can be used unlimitedly in production without any cost impact on accounting results.\n\nThe process of calculating is best understood by applying the term \"ceteris paribus\", i.e. \"all other things being the same,\" stating that at a time only the impact of one changing factor be introduced to the phenomenon being examined. Therefore, the calculation can be presented as a process advancing step by step. First, the impacts of the income distribution process are calculated, and then, the impacts of the real process on the profitability of the production.\n\nThe first step of the calculation is to separate the impacts of the real process and the income distribution process, respectively, from the change in profitability (285.12 – 266.00 = 19.12). This takes place by simply creating one auxiliary column (4) in which a surplus value calculation is compiled using the quantities of Period 1 and the prices of Period 2. In the resulting profitability calculation, Columns 3 and 4 depict the impact of a change in income distribution process on the profitability and in Columns 4 and 7 the impact of a change in real process on the profitability.\n\nThe accounting results are easily interpreted and understood. We see that the real income has increased by 58.12 units from which 41.12 units come from the increase of productivity growth and the rest 17.00 units come from the production volume growth. The total increase of real income (58.12) is distributed to the stakeholders of production, in this case 39.00 units to the customers and to the suppliers of inputs and the rest 19.12 units to the owners.\n\nHere we can make an important conclusion. Income formation of production is always a balance between income generation and income distribution. The income change created in a real process (i.e. by production function) is always distributed to the stakeholders as economic values within the review period. Accordingly, the changes in real income and income distribution are always equal in terms of economic value.\n\nBased on the accounted changes of productivity and production volume values we can explicitly conclude on which part of the production function the production is. The rules of interpretations are the following:\n\nThe production is on the part of “increasing returns” on the production function, when\n\nThe production is on the part of “diminishing returns” on the production function, when\n\nIn the basic example the combination of volume growth (+17.00) and productivity growth (+41.12) reports explicitly that the production is on the part of “increasing returns” on the production function (Saari 2006 a, 138–144).\n\nAnother production model (Production Model Saari 1989) also gives details of the income distribution (Saari 2011,14). Because the accounting techniques of the two models are different, they give differing, although complementary, analytical information. The accounting results are, however, identical. We do not present the model here in detail but we only use its detailed data on income distribution, when the objective functions are formulated in the next section.\n\nAn efficient way to improve the understanding of production performance is to formulate different objective functions according to the objectives of the different interest groups. Formulating the objective function necessitates defining the variable to be maximized (or minimized). After that other variables are considered as constraints or free variables. The most familiar objective function is profit maximization which is also included in this case. Profit maximization is an objective function that stems from the owner’s interest and all other variables are constraints in relation to maximizing of profits in the organization.\n\nThe procedure for formulating different objective functions, in terms of the production model, is introduced next. In the income formation from production the following objective functions can be identified:\nThese cases are illustrated using the numbers from the basic example. The following symbols are used in the presentation:\nThe equal sign (=) signifies the starting point of the computation or the result of computing and the plus or minus sign (+ / -) signifies a variable that is to be added or subtracted from the function. A producer means here the producer community, i.e. labour force, society and owners.\n\nObjective function formulations can be expressed in a single calculation which concisely illustrates the logic of the income generation, the income distribution and the variables to be maximized.\n\nThe calculation resembles an income statement starting with the income generation and ending with the income distribution. The income generation and the distribution are always in balance so that their amounts are equal. In this case it is 58.12 units. The income which has been generated in the real process is distributed to the stakeholders during the same period. There are three variables which can be maximized. They are the real income, the producer income and the owner income. Producer income and owner income are practical quantities because they are addable quantities and they can be computed quite easily. Real income is normally not an addable quantity and in many cases it is difficult to calculate.\n\nHere we have to add that the change of real income can also be computed from the changes in income distribution. We have to identify the unit price changes of outputs and inputs and calculate their profit impacts (i.e. unit price change x quantity). The change of real income is the sum of these profit impacts and the change of owner income. This approach is called the dual approach because the framework is seen in terms of prices instead of quantities (ONS 3, 23).\n\nThe dual approach has been recognized in growth accounting for long but its interpretation has remained unclear. The following question has remained unanswered: “Quantity based estimates of the residual are interpreted as a shift in the production function, but what is the interpretation of the price-based growth estimates?” (Hulten 2009, 18). We have demonstrated above that the real income change is achieved by quantitative changes in production and the income distribution change to the stakeholders is its dual. In this case the duality means that the same accounting result is obtained by accounting the change of the total income generation (real income) and by accounting the change of the total income distribution.\n\n\n"}
{"id": "13789744", "url": "https://en.wikipedia.org/wiki?curid=13789744", "title": "Radar MASINT", "text": "Radar MASINT\n\nRadar MASINT is a subdiscipline of measurement and signature intelligence (MASINT) and refers to intelligence gathering activities that bring together disparate elements that do not fit within the definitions of signals intelligence (SIGINT), imagery intelligence (IMINT), or human intelligence (HUMINT).\n\nAccording to the United States Department of Defense, MASINT is technically derived intelligence (excluding traditional imagery IMINT and signals intelligence) that – when collected, processed, and analyzed by dedicated MASINT systems – results in intelligence that detects, tracks, identifies, or describes the distinctive characteristics target sources. in the US MASINT was recognized as a formal intelligence discipline in 1986.\n\nAs with many branches of MASINT, specific techniques may overlap with the six major conceptual disciplines of MASINT defined by the Center for MASINT Studies and Research, which divides MASINT into electro-optical, nuclear, geophysical, radar, materials, and radiofrequency disciplines.\n\nRadar MASINT is complementary to SIGINT. While the ELINT subdiscipline of SIGINT analyzes the structure of radar directed on a target, radar MASINT is concerned with using specialized radar techniques that measure characteristics of targets.\n\nAnother MASINT subdiscipline, radiofrequency MASINT, considers the unintentional radiation emitted from a radar transmitter (e.g., sidelobes)\n\nMASINT radar sensors may be on space, sea, air, and fixed or mobile platforms. Specialized MASINT radar techniques include line-of-sight (LOS), over-the-horizon, synthetic aperture radar (SAR), inverse synthetic aperture radar (ISAR) and multistatic. It involves the active or passive collection of energy reflected from a target or object by LOS, bistatic, or over-the-horizon radar systems. RADINT collection provides information on radar cross-sections, tracking, precise spatial measurements of components, motion and radar reflectance, and absorption characteristics for dynamic targets and objectives.\n\nRadar MASINT can be active, with the MASINT platform both transmitting and receiving. In multistatic applications, there is physical separation among two or more receivers and transmitters. MASINT can also passively receive signals reflected from an enemy beam.\n\nAs with many intelligence disciplines, it can be a challenge to integrate the technologies into the active services, so they can be used by warfighters. Still, radar has characteristics especially appropriate for MASINT. While there are radars (ISAR) that can produce images, radar pictures are generally not as sharp as those taken by optical sensors, but radar is largely independent of day or night, cloud or sun. Radar can penetrate many materials, such as wooden buildings. Improving the resolution of an imaging radar requires that the antenna size is many times that of the radar wavelength. Wavelength is inversely proportional to frequency, so increasing the radar frequency can improve resolution. It can be difficult to generate high power at the higher frequencies, or problems such as attenuation by water in the atmosphere limit performance. In general, for a fixed sensor, electro-optical sensors, in UV, visual, or infrared spectra, will outperform imaging radar.\n\nSAR and ISAR are means of combining multiple radar samples, taken over time, to create the effect of a much larger antenna, far larger than would physically be possible, for a given radar frequency. As SAR and ISAR develop better resolution, there can be an argument if they still are MASINT sensors, or if they create images sufficiently sharp that they properly are IMINT sensors. Radar can also merge with other sensors to give even more information, such as moving target indicator. Radar generally must acquire its images from an angle, which often means that it can look into the sides of buildings, producing a movie-like record over time, and being able to form three-dimensional views over time.\n\nSee Counter-battery radar\n\nThree US radar systems exist for detecting hostile artillery fire and backtracking to its source, serving the dual requirements of warning of incoming fires and counterattacking the firer. While they are intended to be used in three tiers against artillery of different ranges, there can be a problem of having a threat of an unexpected type fired into an area covered by the wrong tier. Proper site selection and preparation is necessary for all types.\n\nProper planning includes avoiding clutter sources such as land surfaces, vegetation, buildings, complex terrain, aircraft (particularly rotary wing) and particulate matter kicked up by wind or aircraft. The enemy may attempt to avoid the directional radar systems or even use electronic countermeasures, so active patrolling, and activating the radar at random times and in random directions will act as a counter-countermeasure. Complementary acoustic and electro-optical systems can compensate for the lack of omnidirectional coverage by the AN/TPQ-36 and AN/TPQ-37.\n\nTo complement the counterartillery radars, additional MASINT sensors include acoustic and electro-optical systems.\n\nA variety of ground-to-ground radars serve in counterbattery and surveillance roles, and also have some capability to detect helicopters. The LCMR, AN/TPQ-36, and AN/TPQ-37 radars are ideally used in a layered detection system, for short, medium, and long range detection. LCMR is omnidirectional, but the other two are directional and need cueing from omnidirectional sensors such as the combined electro-optical and acoustic Rocket Launch Spotter or a pure acoustic system such as HALO or UTAMS\n\nThese 1980-vintage systems are not man-portable, and are directional, but they do have longer range than the LCMR.\n\nPhysically heavier than the LCMR, the AN/TPQ-36 Firefinder radar can detect cannon, rockets, and mortars within its range:\n\nIt has a moving rather than omnidirectional antenna. Current improvements are intended to replace its old control computer with a laptop, enhance performance in high clutter environments, and increase the probability of detecting certain rockets.\n\nFirst intended to provide a third tier against long-range threats, the AN/TPQ-37 Firefinder radar basic software filters out all other radar tracks with signatures of lesser-ranged threats. New software, required by the mortar threat in the Balkans, allows it to duplicate the Q-36 mortar detection range of 18 kilometers, while still detecting longer-range threats. Proper crew training should compensate for the reduced clutter rejection caused by accepting mortar signatures.\n\nStandard TPQ-36/37 radars are semi-manual in their plotting. An Israeli enhancement makes the plotting fully digital\n\nPortable, and intended for tactical use, is the man-portable surveillance and target acquisition radar (MSTAR), originally developed for British use in artillery spotting, as the primary users of MSTAR, like its predecessor, were and are artillery observation parties, although it may be used for ground reconnaissance and surveillance. The MSTAR entered UK service in early 1991, slightly accelerated for use in the Gulf War. Its official UK designation is Radar, GS, No 22. MSTAR was developed and produced in UK in the mid 1980s by Thorn EMI Electronics (now part of Thales).\n\nIt is a Doppler radar operating in the J Band, and is capable of detecting, recognising and tracking helicopters, slow moving fixed-wing aircraft, tracked and wheeled vehicles and troops, as well as observing and adjusting the fall of shot. The US uses it used as AN/PPS-5B and −5C Ground Surveillance Radar (GSR) Sets, and Australia calls its version AMSTAR.\n\nThe GSR is a ground-to-ground surveillance radar set for use by units such as infantry and tank battalions. and BCT RSTA units. It can detect and locate moving personnel at ranges of 6 km and vehicles at ranges of 10 km, day or night under virtually all weather conditions. The radar has a maximum display range of 10,000 meters and the radar can alert the operator both aurally and visually. The APS/PPS-15 is a lighter, shorter ranged version intended for airborne, light infantry, and special operations force use. These radars are more MASINT then general purpose radar, as the simpler ones have very little imaging power, but perhaps a light or sound indicating the direction and range of the threat.\n\nRecognizing the threat of ground surveillance radar, the Australian military is exploring personal radar warning receivers (RWR), approximately the size of a credit card, and intended principally for special operations forces who have to evade ground surveillance radar.\n\nThe COBRA DANE ground station radar is an \"AN/FPS-108, a phased array L-Band antenna containing 15,360 radiating elements occupying 95% of the roughly 100 by area of one face of the building housing the system. The antenna is oriented toward the west, monitoring the northern Pacific missile test areas.\"\n\nMethods continue to evolve. COBRA JUDY was intended to gather information on long-range missiles, in a strategic role. One developmental system, COBRA GEMINI, is intended to complement COBRA JUDY. It can be used for observing long-range missiles, but is also appropriate for theater-level weapons, which may be addressed in regional arms limitation agreements, such as the Missile Technology Control Regime (MCTR). Where COBRA JUDY is built into a ship, this dual frequency (S- and X-band) radar is transportable, capable of operating on ships or on land, and optimized for monitoring medium range ballistic missiles and antimissile systems. It is air-transportable to deal with sudden monitoring contingencies.\n\nThe AN/SPQ-11 Cobra Judy radar, on , could also be guided by the COBRA BALL electro-optical sensors on an RC-135. Cobra Judy was supplemented by Cobra Gemini on starting around 2000 and was replaced by Cobra King in 2014 on .\n\nThe Soviet Union used a number of radar-equipped ocean reconnaissance satellites (RORSAT), which used strong radar systems, powered by an onboard nuclear reactor, to visualize vessels. These operated in the \"pushbroom\" manner, scanning a swath straight down.\n\nUS radar satellites, however, have emphasized SAR and ISAR.\n\nA synthetic aperture radar (SAR) system, exploits the fast movement of an aircraft or satellite, simulating a large antenna by combining samples over time. This simulation is called the synthetic aperture.\n\nCoupled with other MASINT and IMINT sensors, SAR can provide a high-resolution day and night collection capability. Recorded over time, it can be excellent for tracking changes. When operated at appropriate frequencies, it has ground- and water-penetrating capability, and is good for picking objects out of deliberate or natural clutter.\n\nSAR is not, however, a trivial computational task. As the real antenna moves past the target, the range between target and antenna changes, which must be considered in synthesizing the aperture. In discussing SAR principles, Sandia National Laboratories also notes that, \"for fine resolution systems, the range and azimuth processing is coupled (dependent on each other) which also greatly increases the computational processing\".\n\nIn spite of the difficulties, SAR has evolved to a size that can fit aboard a UAV. Flying on the MQ-1 Predator, the Northrop Grumman AN/ZPQ-1 Tactical Endurance Synthetic Aperture Radar (Tesar) started operations, in March 1996, over Bosnia. The AN/ZPQ-1 uses a radar signal in the 10 – 20 GHz J-band, and can work in strip map, spot map, and MTI modes. These modes are applicable to a wide range of MASINT sensors.\n\nStrip map imaging observes terrain parallel to the flight path or along a specified ground path. Resolution depends on range and swath width, and can vary from 0.3 to 1.0 metres.\n\nCompare the two. The radar is not affected by night or weather.\n\nSpot map mode covers 800 x 800 metres or 2400 × 2400 metres. In MTI mode, moving targets are overlaid on a digital map.\n\nAs well as large SAR aircraft such as the E-8 Joint Surveillance Target Attack Radar System (Joint STARS), whose AN/APY-3 radar has multiple modes including ground moving target indication, the US has highly classified radar satellites. Quill launched in 1964, was the first radar satellite, essentially a prototype. A system originally called Lacrosse (or Lacros), Indigo, and finally Onyx appears to be the only US radar satellite system, using pushbroom scans and \"spotlighting\" SAR.\n\nGiven that the E-8 is a large aircraft that cannot defend itself, there are US attempts to move the E-8 capability into space, under a variety of names, most recently a simple \"Space Radar\". In an era of budget demands, however, this extremely costly new generation has not been launched.\n\nISAR can produce actual images, but the discipline is generally called MASINT rather than IMINT.A much more modest ISAR capability is on the Navy's SH-60 multimission helicopter, carried on destroyers, cruisers, and aircraft carriers. If budgets permit, the proposed E-8 aircraft, the replacement for the P-3 maritime surveillance aircraft, will carry ISAR.\n\nP-3 aircraft carry the AN/APS-137B(V)5 radar, which has SAR and ISAR capability. This is part of the general upgrading of the P-3 to make it a capable land surveillance platform.\n\nThe German Armed Forces' (Bundeswehr) military SAR-Lupe reconnaissance satellite system has been fully operational since 22 July 2008.\n\nThis technique, first demonstrated in the 1970s from an army airborne system, has evolved considerably. At first, it estimated the angle-of-arrival of backscatter power from a pixel on the ground by comparing the phase difference of the backscattered wave as measured at two different locations. This information along with the traditional range and azimuth (Doppler) information allowed one to locate the imaged pixel in three-dimensions, and hence estimate the elevation of that pixel. Elevation-mapping interferometric SAR systems have since become an important remote sensing technology, with a very specific height-mapping mission. Interferometric SAR systems can now be obtained as commercial off-the-shelf (COTS) products.\n\nDetection of mines, both on the active battlefield and in reconstituting nations with unexploded ordnance (UXO) remains a critical problem. As part of the Strategic Environmental Research and Development Program (SERDP), the U.S. Army Research Laboratory (ARL), starting in 1997, began an effort to collect, under extremely controlled condition, a library of UXO signatures.\n\nAs part of a research program that began in 1988 to explore new opportunities opened by developments in analog-to-digital (A/D) converter technology, the UWB SAR radar was built by scientists from the Army Research Laboratory to collect data on studies surrounding the foliage and ground-penetrative capabilities of impulse radar. The instrument is a 20–1100 MHz UWB SAR, mounted on a boom that travels, at 1 km/h, over a precisely measured range. \n\nThe UWB SAR system allowed surveillance over large areas, which made it appealing to use for minefield detection and finding mine clusters rather than detecting individual mines. Capable of validating electromagnetic models and aiding in the development of target detection algorithms, it could transmit and receive waveforms with usable bandwidth from 50 to 1200 MHz. It was also fully polarimetric, allowing access to HH, HV, VH, and VV imagery.\n\nThe SAR is mounted on a 30-ton boom-lift platform, such that it can reproduce, consistently, the depression angles and swath of a representative airborne SAR. Multiple runs can be made with different frequency bands, power levels, resolution, polarization, and motion compensation methods.\n\nBasic signatures are recorded over an area of ground that is free of buried metal, but otherwise identical to the UXO test area. Once the ground signatures are complete, runs are made over the test area.\n\nA precursor to the UWB SAR system was the UWB test-bed radar, which acted as the basic sensor system for gathering foliage and ground-penetrating radar data. Constructed using commercial-off-the-shelf components, this system provided at least 1 GHz of bandwidth as well as the full polarization matrix for its tasks.\n\nIn the mid-1990s, the U.S. Army Research Laboratory designed and fielded BoomSAR, an ultra-wideband synthetic aperture radar (UWB SAR) technology, to detect and classify targets embedded in foliage or underground.\n\nResearchers tested the feasibility of detecting and identifying buried unexploded bombs at low RF frequencies (50 to 1100 MHz). Originally, the system was mounted on a 150-ft-high mobile boom lift that was attached to a rail system on a building at ARL in Adelphi, MD.\n\nInitial data analysis supported the establishment of test sites at the U.S. Army Yuma Proving Ground in Arizona and Eglin Air Force Base in Florida to collect more extensive field measurements. \n\nNon-DoD applications of BoomSAR include environmental remediation, military base cleanup, mine detection, tunnel detection, and pre-construction site analysis. The system also offers the possibility of high-resolution imaging. \n\nOnce the basic terrain signature is known, signatures are being collected from terrain that has been disturbed in a controlled manner. One such environment is at the Yuma Proving Grounds, a desert area where an existing Unexploded Ordnance (UXO) test site, the Steel Crater Test Area, has been used for a variety of sensor calibrations. It contains buried land mines, wires, pipes, vehicles, 55-gallon drums, storage containers and arms caches. For the Army studies to define the signatures of UXO detection, over 600 additional pieces of inert UXO were added to the Steel Crater Test Area, including bombs (250, 500, 750, 1000, and 2000 lb), mortars (60 and 81 mm), artillery shells (105 and 155 mm), 2.75-in. rockets, cluster submunitions (M42, BLU-63, M68, BLU-97, and M118), and mines (Gator, VS1.6, M12, PMN, and POM-Z).\n\nIn the 1990s, a new SAR application of coherent SAR showed the ability the detection and measurement of very small changes in the earth's surface. The simplest form of this technology, known as coherent change detection (CCD), had obvious military and intelligence applications, and is now a valuable tool for analysts. CCD complements other sensors: knowing that the surface changed may mean that analysts can direct ground-penetrating radar on it, measure thermal signatures to see if something is generating heat under the ground, etc.\n\nCompare radar CCD and optical equivalents of the same subject. The CCD would not have been affected by night or weather.\n\nMoving target indications (MTI), at first, might seem just an adjunct to imaging radar, allowing the operator to focus on the moving target. That which makes them peculiarly MASINT, however is, especially in combination with other sensors and reference material, allows the measurement of a movement signature. For example, a tank and a truck both might be measured at 40 km/h when on a road. If both turn onto unpaved ground, however, the signature of the truck is that it might slow significantly, or demonstrate much lateral instability. The tracked vehicle, however, might exhibit a signature of not slowing when going off-pavement.\n\nThere are several electronic approaches to MTI. One is a refinement of CCD. Differential interferometric SAR is even more precise than CCD. Its use in measuring the ground motion of earthquakes can complement seismic sensors for detecting concealed underground explosions, or the characteristics of those above ground.\n\nCurrent research and development involves multiple coherent SAR collections to make even more sensitive measurements, with the capability to detect motion as small as 1 mm per year. The new techniques address many of the limiting factors associated with SAR interferometry, such as atmospheric induced distortions.\n\nUHF and VHF SAR have begun limited operations on Army RC-12 aircraft and may be implemented on the Global Hawk. DARPA's WATCH-IT program developed robust low false\nalarm density change detection software to detect vehicles and smaller targets under foliage, under camouflage and in urban clutter, and developed tomographic (3D) imaging to detect and identify targets that have not relocated. VHF/UHF SAR for building penetration, urban mapping and performing change detection of objects inside buildings.\n\nTerrain characterization technologies were also developed, including the abilities to rapidly generate bald-earth terrain height estimates and to classify terrain features from multipass VHF/UHF SAR imagery. In September 2004, DARPA demonstrated real-time onboard change detection (vehicles and IEDs) and rapid ground-station tomographic processing, as well as rapid generation of bald earth digital elevation models (DEMs) using stereo processing. In parallel, the Air Force Targets Under Trees (TUT) program enhanced the VHF SAR by adding a 10 km swath width VHF-only mode, developing a real-time VHF change detection capability/\n\nDriving research into Non-Cooperative Target Recognition (NCTR) is the fratricide problem, which, according to Army Maj. Bill McKean, is that \"... our weapons can kill at a greater range than we can identify a target as friend or foe. Yet if you wait until you're close enough to be sure you are firing at an enemy, you've lost your advantage.\" The procedural approach of more restrictive rules of engagement (ROE), according to McKean, \"What they found was, if you tighten the rules of engagement to the point that you reduce fratricide, the enemy begins inflicting greater casualties on you. \"Waiting until you're sure in combat could mean becoming a casualty yourself.\". Technical approaches to fratricide prevention include:\n\nRadar offers the potential of non-cooperative target recognition (NCTR). These techniques, which could work if IFF systems fail, have been especially secret. No one has yet proposed, however, NCTR that will be effective if a coalition partner is flying the same aircraft type as the enemy, as in Desert Storm. IFF, presumably with encryption, probably is the only answer to that problem.\n\nOne open-literature study combined several pieces of radar information: cross-section, range, and Doppler measurements. A 1997 Defense Department report mentions \"Air Force and Navy combat identification efforts focus on noncooperative target recognition technologies, including inverse synthetic aperture radar imaging, jet engine modulation (JEM), and unintentional modulation on pulse-based specific emitters\".\n\nNCTR on JEM specifically depends on the periodic rotation of the blades of a turbine, with variations caused by the geometry of the elements of the engine (e.g., multiple rotors, the cowling, exhaust, and stators). More generally, the idea of \"micro-Doppler\" mechanisms, from any mechanical movements in the target structure (\"micro-motion dynamics\"), extends the problem to cover more than rotating aircraft structures, but also automatic gait recognition of human beings. The micro-Doppler idea is more general than those used in JEM alone to consider objects that have vibrational or other kinds of mechanical movement. The basisc of JEM is described in\n. One non-rotational effect would be the surface vibrations of a ground vehicle, caused by the engine, which would be different for gas turbines of tanks and diesel engines of trucks. ISAR is especially useful for NCTR, since it can provide a two-dimensional map of the micromovements.\n\nMoving surfaces cause amplitude, Doppler frequency, and pulse modulation of the return. The amplitude modulation comes from moving surfaces of different reflectivity and angle of reflection. Doppler shifting of the returned signals is a function of the radar carrier frequency, as well as the speed of the radar source and target, with positive Doppler shift from surfaces moving toward the illuminator and negative shift of surfaces moving away from it. Moving surfaces impose a pulse width modulation.\n\nDetecting modulation depends on the angle of the source versus the target; if the source is too far off-center with a turbine or other moving surface, the modulation may not be evident because the moving part of the engine is shielded by the engine mounting. Modulation increases, however, when the source is at right angles to the axis of rotation of the moving element of the target. For fully exposed moving elements, (e.g., propeller blades or helicopter rotors), modulation is a function of the radar beam being off-center to the center of the moving element.\n\nThe first radars used separate antennas for transmitting and receiving, until the development of the diplexer allowed the antenna to be shared, producing much more compact radar systems. Until the development of low-observability \"stealth\" technologies, compact antenna size was prized.\n\nOne of the first principles of stealth technology was to shape the surface of aircraft so that they did not reflect the transmitted beam directly back at the shared antenna. Another technique was to absorb some of the radar in the coating of the aircraft.\n\nThe more separate radar receiving antennas there are, the more likely it is that a reflection will go to a receiver distant from the transmitter. The graphic shows the terminology in bistatic radar, with a separate receiver and transmitter.\n\nHuman activities generate a great deal of radio energy, as in communications, navigation, and entertainment applications. Some of these sources provide enough energy such that their reflection or transillumination can enable passive covert radar (PSR) MASINT, which is also called passive coherent location (PCL).\n\nA foreign transmitter, preferably a purpose-built radar transmitter such as used in air traffic control, but really any powerful transmitted such as TV or FM, potentially can produce reflected signals that do not return to the designated receiver of the foreign radar operator. A signal may reflect such that it can be intercepted and fed into a friendly radar receiver, giving at least information on the presence of a radar target illuminated by the foreign transmitter. This is the simple case with the unintended reflection going to a single radar support receiver.\n\nInterferometry is also possible with such systems. This is especially attractive for naval vessels, which, since they often travel in groups, will have different times difference of arrival (TDOA) of the reflections from the foreign receiver. To restate an important difference, basic PCR works with a single radar receiver and conventional display format, from a single reflection. TDOA works with a set of reflections, from the same target, arriving at multiple points. \"Passive sensors are shown to make a valuable contribution to the air defence mission.\"\n\nAnother group evaluated the PCR technology in an environment like that of a naval task group Ships have more space, and thus the equipment and power are less limited than for airborne or man-portable systems. This British study tested illumination with a Watchman air traffic control pulse doppler radar, and a Bridgemaster marine radar, against experimental receiver types. The researchers also developed simulations of the system.\n\nAgainst the marine transmitter, the receiver combined a square-law: Power-level detector with cross-collation of a local copy of the pulse against the received signal. This method improved sensitivity for poorer time resolution, because correlated peaks are twice the width of uncorrelated peaks.\n\nUsing the air traffic control illuminator, the receiver used pulse compression filtering of a chirp signal, which provided processing gain along with the ability to separate closely spaced targets. This also implemented a moving target indicator that suppressed clutter, but it was recognized that an MTI signal would not be available in a noncooperative environment. They concluded their work demonstrated feasible convergence of PCR and TDOA, using a shipborne R-ESM system with communications among the receivers, such that the processed signal is an interferometric process.\n"}
{"id": "42257619", "url": "https://en.wikipedia.org/wiki?curid=42257619", "title": "Recode", "text": "Recode\n\nRecode (formerly Re/code) is a technology news website that focuses on the business of Silicon Valley. Walt Mossberg and Kara Swisher founded it in January 2014, after they left Dow Jones and the similar website they had previously co-founded, \"AllThingsD\". Vox Media acquired \"Recode\" in May 2015.\n\nIn September 2013, technology journalists Walt Mossberg and Kara Swisher left AllThingsD, the technology news site they had founded and developed for Dow Jones and News Corp. Mossberg left the \"Wall Street Journal\" at the end of the year, leaving behind a popular, weekly technology column. The two launched their new, independent technology news website, \"Recode\", on January 2, 2014. Its holding company, Revere Digital, received minority investments from NBCUniversal and Terry Semel's Windsor Media. The total investment was estimated between and 15 million. Mossberg and Swisher held the company's majority stake and noted its comfortable financial stance. \"Recode\" also provided breaking technology coverage for NBCUniversal, and received video resources and exposure in return via a formal partnership. Mossberg saw the investment as an opportunity to implement new ways of covering the technology field, and planned to add six employees on technology policy and mobile beats. The CNBC partnership also explored new advertising efforts and shared office space. At launch, the 23-person team included all former members of \"AllThingsD\". The staff also received equity in the company.\n\nMossberg and Swisher planned to continue their prominent, annual \"AllThingsD\" conference, which they renamed the \"Code\" conference and scheduled for the same time and location: late May at Terranea Resort in Rancho Palos Verdes, California. \"Recode\" also kept plans to continue their separate mobile and media conferences. CNBC became a partner in these conferences. A part-time team of 12 employees runs the conferences.\n\nThe site developed a reputation for breaking tech industry news but ultimately did not reach the level of popularity it expected, with just 1.5 million regular monthly visitors. Vox Media acquired the website in May 2015 in a move that \"The New York Times\" described as a reflection of tumult in online technology journalism. Vox purchased all of the company's stock, but the details of the transaction were not released. At the time of the acquisition, \"Recode\" had 44 employees and three additional employees by contract. They were expected to join Vox. Mossberg and Swisher planned to stay with the website. The two were impressed with Vox Media's audience reach. Vox's technology news website, \"The Verge\", had eight times the traffic, in comparison. The scopes of the two sites were not expected to overlap with \"Recode\" emphasis on technology industry business and \"The Verge\" on \"being a new kind of culture publication\". An internal study found a three percent overlap in content between the two sites. Recode started publishing podcasts in July 2015.\n\nOn May 8, 2016, Recode relaunched with a new design under editor-in-chief Dan Frommer.\n\nAs continued from \"AllThingsD\", \"Recode\" focuses on technology and digital media news, particularly pertaining to the business of Silicon Valley. The site also reviews new enterprises, and consumer hardware and software, and conducts original reports.\n"}
{"id": "13482338", "url": "https://en.wikipedia.org/wiki?curid=13482338", "title": "Reflecting instrument", "text": "Reflecting instrument\n\nReflecting instruments are those that use mirrors to enhance their ability to make measurements. In particular, the use of mirrors permits one to observe two objects simultaneously while measuring the angular distance between the objects. While reflecting instruments are used in many professions, they are primarily associated with celestial navigation as the need to solve navigation problems, in particular the problem of the longitude, was the primary motivation in their development.\n\nThe purpose of reflecting instruments is to allow an observer to measure the altitude of a celestial object or measure the angular distance between two objects. The driving force behind the developments discussed here was the solution to the problem of finding one's longitude at sea. The solution to this problem was seen to require an accurate means of measuring angles and the accuracy was seen to rely on the observer's ability to measure this angle by simultaneously observing two objects at once.\n\nThe deficiency of prior instruments was well known. Requiring the observer to observe two objects with two divergent lines of sight increased the likelihood of an error. Those that considered the problem realized that the use of specula (mirrors in modern parlance) could permit two objects to be observed in a single view. What followed is a series of inventions and improvements that refined the instrument to the point that its accuracy exceeded that which was required for determining longitude. Any further improvements required a completely new technology.\n\nSome of the early reflecting instruments were proposed by scientists such as Robert Hooke and Isaac Newton. These were little used or may not have been built or tested extensively. The van Breen instrument was the exception, in that it was used by the Dutch. However, it had little influence outside of the Netherlands.\n\nInvented in 1660 by the Dutch Joost van Breen, the spiegelboog (mirror-bow) was a reflecting cross staff. This instrument appears to have been used for approximately 100 years, mainly in the Zeeland Chamber of the VOC (The Dutch East India Company).\n\nHooke's instrument was a single-reflecting instrument. It used a single mirror to reflect the image of an astronomical object to the observer's eye. This instrument was first described in 1666 and a working model was presented by Hooke at a meeting of the Royal Society some time later.\n\nThe device consisted of three primary components, an index arm, a radial arm and a graduated chord. The three were arranged in a triangle as in the image on the right. A telescopic sight was mounted on the index arm. At the point of rotation of the radial arm, a single mirror was mounted. This point of rotation allowed the angle between the index arm and the radial arm to be changed. The graduated chord was connected to the opposite end of the radial arm and the chord was permitted to rotate about the end. The chord was held against the distant end of the index arm and slid against it. The graduations on the chord were uniform and, by using it to measure the distance between the ends of the index arm and the radial arm, the angle between those arms could be determined. A table of chords was used to convert a measurement of distance to a measurement of angle. The use of the mirror resulted in the measured angle being twice the angle included by the index and the radius arm.\n\nThe mirror on the radial arm was small enough that the observer could see the reflection of an object in half the telescope's view while seeing straight ahead in the other half. This allowed the observer to see both objects at once. Aligning the two objects together in the telescopes view resulted in the angular distance between them to be represented on the graduated chord.\n\nWhile Hooke's instrument was novel and attracted some attention at the time, there is no evidence that it was subjected to any tests at sea. The instrument was little used and did not have any significant effect on astronomy or navigation.\n\nIn 1692, Edmond Halley presented the design of a reflecting instrument to the Royal Society.\n\nThis is an interesting instrument, combining the functionality of a radio latino with a double telescope. The telescope (AB in the adjacent image), has an eyepiece at one end and a mirror (D) partway along its length with one objective lens at the far end (B). The mirror only obstructs half the field (either left or right) and permits the objective to be seen on the other. Reflected in the mirror is the image from the second objective lens (C). This permits the observer to see both images, one straight through and one reflected, simultaneously besides each other. It is essential that the focal lengths of the two objective lenses be the same and that the distances from the mirror to either lens be identical. If this condition is not met, the two images cannot be brought to a common focus.\n\nThe mirror is mounted on the staff (DF) of the radio latino portion of the instrument and rotates with it. The angle this side of the radio latino's rhombus makes to the telescope can be set by adjusting the rhombus' diagonal length. In order to facilitate this and allow for fine adjustment of the angle, a screw (EC) is mounted so as to allow the observer to change the distance between the two vertexes (E and C).\n\nThe observer sights the horizon with the direct lens' view and sights a celestial object in the mirror. Turning the screw to bring the two images directly adjacent sets the instrument. The angle is determined by taking the length of the screw between E and C and converting this to an angle in a table of chords.\n\nHalley specified that the telescope tube be rectangular in cross section. This makes construction easy, but is not a requirement as other cross section shapes can be accommodated. The four sides of the radio latino portion (CD, DE, EF, FC) must be equal in length in order for the angle between the telescope and the objective lens side (AD-DC) to be precisely twice the angle between the telescope and the mirror (AD-DF) (or in other words – to enforce the angle of incidence being equal to the angle of reflection). Otherwise, instrument collimation will be compromised and the resulting measurements would be in error.\n\nThe celestial object's elevation angle could have been determined by reading from graduations on the staff at the slider, however, that's not how Halley designed the instrument. This may suggest that the overall design of the instrument was coincidentally like a radio latino and that Halley may not have been familiar with that instrument.\n\nThere is no knowledge of whether this instrument was ever tested at sea.\n\nNewton's reflecting quadrant was similar in many respects to Hadley's first reflecting quadrant that followed it.\n\nNewton had communicated the design to Edmund Halley around 1699. However, Halley did not do anything with the document and it remained in his papers only to be discovered after his death. However, Halley did discuss Newton's design with members of the Royal Society when Hadley presented his reflecting quadrant in 1731. Halley noted that Hadley's design was quite similar to the earlier Newtonian instrument.\n\nAs a result of this inadvertent secrecy, Newton's invention played little role in the development of reflecting instruments.\n\nWhat is remarkable about the octant is the number of persons who independently invented the device in a short period of time. John Hadley and Thomas Godfrey both get credit for inventing the octant. They independently developed the same instrument around 1731. They were not the only ones, however.\n\nIn Hadley's case, two instruments were designed. The first was an instrument very similar to Newton's reflecting quadrant. The second had essentially the same form as the modern sextant. Few of the first design were constructed, while the second became the standard instrument from which the sextant derived and, along with the sextant, displaced all prior navigation instruments used for celestial navigation.\n\nCaleb Smith, an English insurance broker with a strong interest in astronomy, had created an octant in 1734. He called it an \"Astroscope\" or \"Sea-Quadrant\". He used a fixed prism in addition to an index mirror to provide reflective elements. Prisms provide advantages over mirrors in an era when polished speculum metal mirrors were inferior and both the silvering of a mirror and the production of glass with flat, parallel surfaces was difficult. However, the other design elements of Smith's instrument made it inferior to Hadley's octant and it was not used significantly.\n\nJean-Paul Fouchy, a mathematics professor and astronomer in France invented an octant in 1732. His was essentially the same as Hadley's. Fouchy did not know of the developments in England at the time, since communications between the two country's instrument makers was limited and the publications of the Royal Society, particularly the \"Philosophical Transactions\", were not being distributed in France. Fouchy's octant was overshadowed by Hadley's.\n\nThe origin of the sextant is straightforward and not in dispute. Admiral John Campbell, having used Hadley's octant in sea trials of the method of lunar distances, found that it was wanting. The 90° angle subtended by the arc of the instrument was insufficient to measure some of the angular distances required for the method. He suggested that the angle be increased to 120°, yielding the sextant. John Bird made the first such sextant in 1757.\n\nWith the development of the sextant, the octant became something of a second class instrument. The octant, while occasionally constructed entirely of brass, remained primarily a wooden-framed instrument. Most of the developments in advanced materials and construction techniques were reserved for the sextant.\n\nThere are examples of sextants made with wood, however most are made from brass. In order to ensure the frame was stiff, instrument makers used thicker frames. This had a drawback in making the instrument heavier, which could influence the accuracy due to hand-shaking as the navigator worked against its weight. In order to avoid this problem, the frames were modified. Edward Troughton patented the double-framed sextant in 1788. This used two frames held in parallel with spacers. The two frames were about a centimetre apart. This significantly increased the stiffness of the frame. An earlier version had a second frame that only covered the upper part of the instrument, securing the mirrors and telescope. Later versions used two full frames. Since the spacers looked like little pillars, these were also called \"pillar sextants\".\nTroughton also experimented with alternative materials. The scales were plated with silver, gold or platinum. Gold and platinum both minimized corrosion problems. The platinum-plated instruments were expensive, due to the scarcity of the metal, though less expensive than gold. Troughton knew William Hyde Wollaston through the Royal Society and this gave him access to the precious metal. Instruments from Troughton's company that used platinum can be easily identified by the word \"Platina\" engraved on the frame. These instruments remain highly valued as collector's items and are as accurate today as when they were constructed.\n\nAs the developments in dividing engines progressed, the sextant was more accurate and could be made smaller. In order to permit easy reading of the vernier, a small magnifying lens was added. In addition, to reduce glare on the frame, some had a diffuser surrounding the magnifier to soften the light. As accuracy increased, the circular arc vernier was replaced with a drum vernier.\n\nFrame designs were modified over time to create a frame that would not be adversely affected by temperature changes. These frame patterns became standardized and one can see the same general shape in many instruments from many different manufacturers.\n\nIn order to control costs, modern sextants are now available in precision-made plastic. These are light, affordable and of high quality.\n\nWhile most people think of navigation when they hear the term \"sextant\", the instrument has been used in other professions.\n\n\nIn addition to these types, there are terms used for various sextants.\n\nA \"pillar sextant\" can be either:\nThe former is the most common use of the term.\n\nSeveral makers offered instruments with sizes other than one-eighth or one-sixth of a circle. One of the most common was the \"quintant\" or fifth of a circle (72° arc reading to 144°). Other sizes were also available, but the odd sizes never became common. Many instruments are found with scales reading to, for example, 135°, but they are simply referred to as sextants. Similarly, there are 100° octants, but these are not separated as unique types of instruments.\n\nThere was interest in much larger instruments for special purposes. In particular a number of full circle instruments were made, categorized as reflecting circles and repeating circles.\n\nThe reflecting circle was invented by the German geometer and astronomer Tobias Mayer in 1752, with details published in 1767. His development preceded the sextant and was motivated by the need to create a superior surveying instrument.\n\nThe reflecting circle is a complete circular instrument graduated to 720° (To measure distances between heavenly bodies, there is no need to read an angle greater than 180°, since the minimum distance will always be less than 180°.). Mayer presented a detailed description of this instrument to the Board of Longitude and John Bird used the information to construct one sixteen inches in diameter for evaluation by the Royal Navy. This instrument was one of those used by Admiral John Campbell during his evaluation of the lunar distance method. It differed in that it was graduated to 360° and was so heavy that it was fitted with a support that attached to a belt. It was not considered better than the Hadley octant and was less convenient to use. As a result, Campbell recommended the construction of the sextant.\n\nJean-Charles de Borda further developed the reflecting circle. He modified the position of the telescopic sight in such a way that the mirror could be used to receive an image from either side relative to the telescope. This eliminated the need to ascertain that the mirrors were precisely parallel when reading zero. This simplified the use of the instrument. Further refinements were performed with the help of Etienne Lenoir. The two of them refined the instrument to its definitive form in 1777. This instrument was so distinctive it was given the name \"Borda circle\".\n\nJosef de Mendoza y Ríos redesigned Borda's reflecting circle (London, 1801). The goal was to use it together with his Lunar Tables published by the Royal Society (London, 1805). He made a design with two concentric circles and a vernier scale and recommended averaging three sequential readings to reduce the error. Borda's system was not based on a circle of 360° but 400 grads (Borda spent years calculating his tables with a circle divided in 400°). Mendoza's lunar tables have been used through almost the entire nineteenth century (see Lunar distance (navigation)).\n\nEdward Troughton also modified the reflecting circle. He created a design with three index arms and verniers. This permitted three simultaneous readings to average out the error.\n\nAs a navigation instrument, the reflecting circle was more popular with the French navy than with the British.\n\nOne instrument derived from the reflecting circle is the repeating circle. Invented by Lenoir in 1784, Borda and Lenoir developed the instrument for geodetic surveying. Since it was not used for the celestial measures, it did not use double reflection and substituted two telescope sights. As such, it was not a reflecting instrument. It was notable as being the equal of the great theodolite created by the renowned instrument maker, Jesse Ramsden.\n\nThe Bris sextant is not a true sextant, but it is a true reflecting instrument based on the principle of double reflection and subject to the same rules and errors as common octants and sextants. Unlike common octants and sextants, the Bris sextant is a fixed angle instrument capable of accurately measuring a few specific angles unlike other reflecting instruments which can measure any angle within the range of the instrument. It is particularly suited to determining the altitude of the sun or moon.\n\nFrancis Ronalds invented an instrument for recording angles in 1829 by modifying the octant. A disadvantage of reflecting instruments in surveying applications is that optics dictate that the mirror and index arm rotate through half the angular separation of the two objects. The angle thus needs to be read, noted and a protractor employed to draw the angle on a plan. Ronalds’ idea was to configure the index arm to rotate through twice the angle of the mirror, so that the arm could then be used to draw a line at the correct angle directly onto the drawing. He used a sector as the basis of his instrument and placed the horizon glass at one tip and the index mirror near the hinge connecting the two rulers. The two revolving elements were linked mechanically and the barrel supporting the mirror was twice the diameter of the hinge to give the required angular ratio.\n\n"}
{"id": "14083276", "url": "https://en.wikipedia.org/wiki?curid=14083276", "title": "Smart toy", "text": "Smart toy\n\nA smart toy is a toy which effectively has its own intelligence by virtue of on-board electronics. These enable it to learn, behave according to pattern, and alter its actions depending upon environmental stimuli. Typically, it can adjust to the abilities of the player. A modern smart toy has electronics consisting of one or more microprocessors or microcontrollers, volatile and/or non-volatile memory, storage devices, and various forms of input–output devices. It may be networked together with other smart toys or a personal computer in order to enhance its play value or educational features. Generally, the smart toy may be controlled by software which is embedded in firmware or else loaded from an input device such as a USB flash drive, Memory Stick or CD-ROM. Smart toys frequently have extensive multimedia capabilities, and these can be utilized to produce a realistic, animated, simulated personality for the toy. Some commercial examples of smart toys are \"Amazing Amanda\", Furby and iDog.\n\nSmart toys are frequently confused with toys for which it is claimed that children who play with them become smarter. Examples are educational toys that may or may not provide on-board intelligence features. A toy which merely contains a media player for telling the child a story should not be classified as a smart toy even if the player contains its own microprocessor. What best distinguishes a smart toy is the way the on-board intelligence is holistically integrated into the play experience in order to create simulated human-like intelligence or its facsimile.\n\nModern smart toys have their early roots in clockworks such as those of the eighteenth and nineteenth century cuckoo clocks, music boxes of the nineteenth, and Disney audio-animatronics of the twentieth. Perhaps the biggest early contribution is from novelty and toy makers from the 1800s who made automatons such as Vaucanson's mechanical duck, von Kempelen's The Turk, and the Silver Swan. All pre-twentieth-century precursors had in common that they were mechanical contrivances. By the second half of the 1900s toys featuring built-in media players became common. For example, Mattel introduced a variety of dolls in the 1960s and 1970s that used a pull string activated talking device to make the dolls \"talk\" such as the talking Crissy doll and Chatty Cathy.\n\nHowever, it remained until the introduction of the microprocessor in the mid-1970s for smart toys to come into their own. Texas Instrument's Speak & Spell which came on the market in the late 1970s was one of the first full-featured smart toys. The device is similar to a very limited laptop with LED read-out. It is used for spelling games and guessing a \"mystery code\". It speaks and makes a variety of sound effects. Another early example is Teddy Ruxpin, a robotic teddy bear which came out in the 1980s. It reads children's stories via a recording device built into its back and swivels its eyes and mouth.\n\nEven the earliest toys, from the nineteenth century on, have in common with their modern-day smart toy counterparts that they appear to be sentient and lifelike, at least to the extent possible using the technology available at the time. Contemporary smart toys utilize speech recognition and activation; that is, they appear to comprehend and react to words that are spoken. Through speech synthesis, smart toys speak prerecorded words and phrases. These kinds of technologies, when combined together, animate the toys and give them a lifelike persona.\n\nAnother hardware feature of modern smart toys is sensors which enable the smart toy to be aware of what is going on in its environment. These permit the toy to tell its orientation, determine if it is being played with indoors or outdoors, and know who is playing with it based upon the strength of the squeeze the child's hand gives it or similar factors.\nA typical example is Lego Mindstorms, a series of robotic-like devices, which integrate LEGO pieces with sensors and accessories. These toys include microcontrollers which control the robots. They are pre-programmed by a personal computer and utilize light and touch sensors along with accelerometers. Accelerometers and temperature, pressure and humidity sensors, can also be used to create various effects by smart toy designers.\n\nThe development of smart toys received a major boost in 1998 when semiconductor manufacturer, Intel, and toy maker, Mattel, Inc. entered into a joint venture to open a \"Smart Toy Lab\" in Portland, Oregon. This led to products that were marketed under the Intel Play brand. The first product in the line was the \"QX3 Computer Microscope\". The Lab evolved into a toy company known today as Digital Blue, a division of Prime Entertainment, Inc. of Marietta, GA.\n\nWidespread commercialization of smart toys is mainly a 21st-century phenomenon. As they have gained acceptance in the marketplace, controversy has been brewing. One of the chief criticisms has been that despite often being technical marvels, many smart toys have only limited play value. In short, these toys neither involve the child in play activity nor do they stimulate his or her imagination. Consequently, regardless of store-shelf attractiveness, the child tires quickly of them after only one or two play sessions, and the parents' investment is largely wasted. Stevanne Auerbach, in her book \"Smart Play—Smart Toys\" introduces the notion of \"Play Quotient\" or simply \"PQ\".\n\nAuerbach criticizes smart toys for often having low PQs. PQ is a rating system based upon a weighted average constructed from a comprehensive list of \"play value\" attributes. Playthings with higher PQs are desirable from the standpoint of stimulating the child's imagination, creativity, and inquisitiveness. Generally, children choose to play with these products over and over again. Those toys with low PQs are quickly set aside. The child finds them boring and uninteresting.\n\nMany child development experts prefer open-ended toys such as construction toys, blocks, dolls, etc. over smart toys.\nFor example, a cardboard box that the child turns into a pretend play house will be played with continuously by the child for many hours whereas an expensive smart toy can quickly exhaust the child's interest once its novelty has worn off.\n\nJillian Trezise typifies the attitude often taken by child development specialists and educators towards smart toys, \"...if kids can't take their expensive toys to the sandpit or open them up to see how they work, then they don't provide much educational value. All they do is entertain and they don't hold young people's attention for very long.\"\n\nAnother implicit concern about smart toys is that even when they hold the child's attention they could become so entertaining that parents may be tempted to turn over some of the child-rearing to the smart toys. Thus, children will be deprived of needed parental attention. In other words, because of their strong multimedia capabilities children may watch presentations provided by the smart toys and be entertained, but will not really play with the devices nor be otherwise engaged by them.\n\nJudy Shackelford, a toy industry veteran, has a more positive view regarding smart toys. She cautions that children may even be deprived should they be not exposed to them. She sees smart toys as part of the surrounding environment that children will need to adapt to as they mature. Should they not be given access to these kinds of toys, they may become less well adapted to thrive and benefit as technology evolves.\n\nSmart toy advocates also point to research indicating that children learn more effectively with good interactive software. This seems to support the idea that smart toys may have many educational benefits as well.\n\nThere have been increasing concerns that smart toys, especially ones that directly connect to the Internet, are becoming easy targets for cybercriminals, who can use hacking to easily obtain personal data collected from a smart toy, especially personal names.\n\nMarket research company, GfK Australia, found that parents are spending record amounts on electronic and interactive toys.\n\nMark Allen states that the greatest impediment to the further growth of the smart toy industry is the lack of development of artificial intelligence and speech recognition. At their present stage of evolution smart toys really can't learn so they are limited to predefined actions and speech. Present artificial intelligence capabilities are too expensive to implement in a toy, but this will change as computational power and speed come down in price. Eventually, this will result in cheaper technology, enhanced functionality, and a richer play experience. Some toy designers think it could be five years or more before the technology is cheap enough to be widely available.\n\nOthers have cited the high cost of MEMS-based sensors and actuators as a factor constraining the rapid development of smart toys. These costs are expected to come down eventually also, thereby helping toy companies to hit their price targets.\n\nAccording to figures from the NPD Group, at the end of 1999, the smart toy segment made up 2.5 percent of the $23 billion toy market.\n\nThe smart toy industry grew out of several other product categories, which include children's software, electronic toys, and video games. A 2001 Forrester Research study projected that the smart toy segment would grow to more than $2 billion by the year 2003. Factors enhancing the growth of the smart toy segment include the greatly more sophisticated tastes of children today as well as the spread of home PCs.\n\nA 2005 market research study by Tangull America LLC of New York, NY indicated that toys with embedded information technologies—that is, nano, bio and cognitive technologies—are growing over 15% annually, and will grow to sales of US $146 billion by 2015. As an example, one of the \"smart toys\" the study cites are \"interactive puppets\" that become \"real playmates\" through the combination of artificial intelligence and ultrafine sensors. The latter can measure changes in facial expressions, movements, and environment and the puppets react accordingly.\n\nThe issue of balance is often mentioned in connection with smart toys—namely, that their use should be kept in proportion with other play activities. They should also be age appropriate and not become a substitute for interaction with parents. Playing with smart toys should be a supplement, not a replacement, for traditional play activities.\n\nStevanne Auerbach emphasizes smart toys which have strong \"play value\" for the child, and are the \"right toy at the right time.\" She does not favor those toys which fail to encourage discovery and exploration. Auerbach quips that \"a toy playing with a child, as opposed to a child playing with a toy, is not beneficial for the child.\n\nThose toys that give the child control over interaction are best according to some child development researchers. Kiely Gouley argues that \"...some of these toys are very entertaining and they make the child a passive observer.\" She continues: \"...you want the child to engage with the world. If the toy does everything if it sings and beeps and shows pictures, what does the child have to do?\"\n\nSmart toys should have very clean, easy-to-understand and navigate user interfaces. Claire Lerner, a child-development specialist, says that pretend play can be inhibited by highly structured toys: \"They superimpose someone else's story on the kids. So kids don't develop their imaginations.\" In her view, simpler toys are preferable, because they are more flexible.\n\nFrom a designer of smart toy's viewpoint, this means that in order to achieve simplicity technologies need to be combined so as to render a very naturalistic user interface within the limits of other design constraints.\n\nChildren by nature are unpredictable and often fail to follow the same rules followed by adults. One of the tasks of the designer is to anticipate ways that interaction with children can fail to be as expected and to guide the user into one of the expected responses. This can be achieved by giving the child options to select and other types of cues to follow.\n\nFor parents and child development specialists alike, the task remains to select the right toys at the right time. However, from the toy designer's standpoint, the challenge is to identify the best technologies at a feasible cost, and then to develop products around those capabilities and limitations of the technologies used in smart toys.\n\nAnthropologist David Lancy argues that parent-child play is largely an artifact of wealthy developed countries not practiced by most of the world's population. It results from competitive pressures to ready children for survival in an information-based economy. He views the promotion of interaction between parents and children in \"play activities\" as a form of cultural imperialism practiced by the upper and upper middle class upon lower income socioeconomic strata. This is possibly one reservation on a completely unrestricted view that parents should always be involved in selecting appropriate smart toys for their children.\n\n"}
{"id": "2168581", "url": "https://en.wikipedia.org/wiki?curid=2168581", "title": "Sunrise industry", "text": "Sunrise industry\n\nA sunrise industry is one that is new or relatively new, is growing fast and is expected to become important in the future. Examples of sunrise industries include hydrogen fuel production, petrochemical industry, food processing industry, space tourism, and online encyclopedias.\n\n"}
{"id": "10113932", "url": "https://en.wikipedia.org/wiki?curid=10113932", "title": "Talking clock", "text": "Talking clock\n\nA talking clock (also called a speaking clock and an auditory clock) is a timekeeping device that presents the time as sounds. It may present the time solely as sounds, such as a phone-based time service (see \"Speaking clock\") or a clock for the hearing impaired, or may have a sound feature in addition to an analog or digital face.\n\nAlthough they would not be considered to be speaking, clocks have incorporated noisemakers such as clangs, chimes, gongs, melodies, and the sounds of cuckoos or roosters from almost the beginning of the mechanical clock. Soon after Thomas Edison's invention of the phonograph, the earliest attempts to make a clock that incorporated a voice were made. Around 1878, Frank Lambert invented a machine that used a voice recorded on a lead cylinder to call out the hours. Lambert used lead in place of Edison's soft tinfoil. In 1992, the Guinness Book of World Records recognized this as the oldest known sound recording that was playable (though that status now rests with a phonautogram of Édouard-Léon Scott de Martinville, recorded in 1857). It is on display at the National Watch and Clock Museum in Columbia, Pennsylvania.\n\nAlthough there have been rumors that other talking clocks may have been produced afterward, it is not until around 1910 that another talking clock was introduced, when Bernhard Hiller created a clock that used a belt with a recording on it to announce the time. However, these belts were often broken by the hand-tightening required, and all attempts to reproduce the celluloid ribbon have so far failed.\n\nIn 1933, the first practical use of talking clocks was seen when Ernest Esclangon created a talking telephone time service in Paris, France. On its first day, February 14, 1933, more than 140,000 calls were received. London began a similar service three years later. This type of talking time service is still around, and more than a million calls per year are received for the NIST's Telephone Time-of-Day Service.\n\nIn 1954, Ted Duncan, Inc., released the \"Hickory Dickory Clock\", a crank toy intended for children. This clock used a record, needle, and tone arm to produce its sound.\n\nIn 1968, the first truly portable talking clock, the \"Mattel-a-Time\" Talking Clock, was released. \n\nIn 1979 Sharp released the world first quartz-based talking clock, the \"Talking Time CT-660E\" (German version \"CT-660G\"). Its silver transistor-radio-like case contained complex LSI circuitry with 3 SMD ICs (likely clock CPU, speech CPU and sound IC), producing a Speak&Spell-like synthetic voice. At the front rim was a small LCD. The alarm spoke the time and also had a melody \"Boccherini's Minuet\"; after 5 minutes the alarm repeated with the words \"Please hurry!\". It also had stopwatch and countdown timer modes. The tiny controls to turn off alarm or set functions are hard to reach under a small bottom lid.\n\nIn 1984, the Hattori Seiko Co. released their famous pyramid-shaped talking clock, the \"Pyramid Talk\". As a futuristic design object even its LCD was hidden at the bottom, thus people were supposed to always push its top to hear it talk, which is not very practical.\n\nCurrent talking clocks often include many more features than just giving the time; in these, the ability to speak the time is part of a wide range of voice capabilities, such as reading the weather and other information to the user.\n\nAfter the telephone time service, the next practical application of the talking clock was in the teaching of timetelling to children. The first talking clock to be used for this purpose was the Mattel \"Mattel-a-Time Talking Clock\" of 1968. Several other clocks of this type followed, including one featuring Thomas the Tank Engine. One of the latest ones, the \"Talking Clever Clock\", includes a quiz button which asks questions such as \"What time is it?\", \"What time will it be in an hour?\", and \"How much time has passed between 1:00 and 2:30?\" Other educational talking clocks come in a kit designed to be assembled by children.\n\nTalking clocks can also be used with children whose learning disabilities may be partially offset by the reinforcement provided by hearing the time as well as seeing it.\n\nTalking clocks have found a natural home as an assistive technology for people who are blind or visually impaired. There are over 150 tabletop clocks and 50 types of watches that talk. Manufacturers of such clocks include Sharp, Panasonic, RadioShack, and Reizen. In addition, one manufacturer purportedly produced a clock that would announce the time upon detecting a user's whistling signal.\n\nMany companies have used talking clocks as a novelty item to promote their brand. In 1987, the H. J. Heinz Company released a clock with the figure of \"Mr. Aristocrat\", a tomato with a motif similar to Mr. Peanut. At alarm time, the clock said, \"It's time to get up; get up right away! Wait any longer and it's 'ketchup' all day! Remember, Heinz is the thick rich one.\" At roughly the same time, Pillsbury created a similar clock with the character of Little Sprout. In recent years, the Coca-Cola polar bear, the Red and Yellow M&M's characters, the Pillsbury Doughboy, a Campbell's Soup girl, and others have at one time appeared on a talking clock. One of the more interesting branded clocks was produced by Energizer and was a soft, battery-shaped clock whose alarm was turned off by punching it or throwing it against a hard surface.\n\nThe inexpensiveness of modern speech technology has allowed manufacturers to include talking clock capabilities into a wide range of products. Many of these are intended as conversation pieces or speak merely for the entertainment of hearing sounds or words spoken by an inanimate object. Such timepieces include Darth Vader clocks, calculators with time features, and even a painting of Leonardo da Vinci's The Last Supper that announces the time on the hour along with a quote from Jesus.\n\nOther themes of talking timepieces include fortune-telling, astrology, clocks with moving lips, animated creatures, sports and athletes, and movies, among others.\n\nMost modern talking clocks are based on speech-synthesis integrated circuits that generate speech from sampled, stored data. The rapid technological progress of the 1980s enabled today's high-quality talking products. Early talking clocks employed chips that linked phonemes to generate speech. These products could generate unlimited speech, but it was of relatively poor quality that sounded robotic, at worst, unintelligible. Today's higher-quality speech is produced by sampled-data systems that take elements of an actual human voice. Modern voice synthesis technologies can produce synthesized vocabularies that retain the style of the speaker exactly and are not limited to just perfect English, but can be as varied as Scottish accents, Japanese, and even the voice of a young child. Such voices are all generated using tiny, inexpensive voice chips that are readily available. \n\nAlmost all of the latest voice-chipped talking clocks incorporate the female human voice to announce the time. Dr. Mark McKinley, the president of the International Society of Talking Clock Collectors, proposes three possible explanations for this phenomenon. The female voice may be considered more soothing psychologically; it may be a relic of the female voice being historically associated with secretarial (Administrative Assistant) functions; or a feminine voice may possibly simply be softer in a less intrusive way.\n\nMany talking clocks include a light sensor or a setting that will automatically silence them between certain hours (usually between 10 p.m. and 8 a.m.).\n\nMany talking clocks of the 1970s utilized an Ozen box, which is a mechanism similar to a phonograph, in which a needle-like stylus tracks on a 2.25 inch platter similar to a vinyl phonograph record. The Janex Corporation produced most of the clocks which use this device, and they are highly prized among collectors.\n\nA very large number of popular characters have appeared on talking clocks. The following list is not exhaustive, nor is it intended to be---The International Society of Talking Clocks Collectors (ISTCC)has a Museum collection of over 800 talking clocks.\n\n\n\n"}
{"id": "3025266", "url": "https://en.wikipedia.org/wiki?curid=3025266", "title": "Ternary computer", "text": "Ternary computer\n\nA ternary computer (also called trinary computer) is a computer that uses ternary logic (three possible values) instead of the more popular binary system (\"Base 2\") in its calculations.\n\nOne early calculating machine, built by Thomas Fowler entirely from wood in 1840, operated in balanced ternary. The first modern, electronic ternary computer Setun was built in 1958 in the Soviet Union at the Moscow State University by Nikolay Brusentsov, and it had notable advantages over the binary computers which eventually replaced it, such as lower electricity consumption and lower production cost. In 1970 Brusentsov built an enhanced version of the computer, which he called Setun-70. In the USA, the ternary computing emulator Ternac working on a binary machine was developed in 1973.\n\nThe ternary computer QTC-1 was developed in Canada.\n\nTernary computing is commonly implemented in terms of balanced ternary, which uses the three digits −1, 0, and +1. The negative value of any balanced ternary digit can be obtained by replacing every + with a − and vice versa. It is easy to subtract a number by inverting the + and − digits and then using normal addition. Balanced ternary can express negative values as easily as positive ones, without the need for a leading negative sign as with decimal numbers. These advantages make some calculations more efficient in ternary than binary. Considering that digit signs are mandatory, and nonzero digits are magnitude 1 only, notation using only zero and signs alone is more concise than when 1's are used.\n\nWith the advent of mass-produced binary components for computers, ternary computers have diminished in significance. However, Donald Knuth argues that they will be brought back into development in the future to take advantage of ternary logic's elegance and efficiency. One possible way this could happen is by combining an optical computer with the ternary logic system. A ternary computer using fiber optics could use dark as 0 and two orthogonal polarizations of light as 1 and −1. IBM also reports infrequently on ternary computing topics (in its papers), but it is not actively engaged in it.\n\nThe Josephson junction has been proposed as a balanced ternary memory cell, using circulating superconducting currents, either clockwise, counterclockwise, or off. \"The advantages of the proposed memory circuit are capability of high speed computation, low power consumption and very simple construction with fewer elements due to the ternary operation.\"\n\nIn 2009, a quantum computer was proposed which uses a quantum ternary state, a qutrit, rather than the typical qubit. When the number of basic states of quantum element is \"d\", it is called qudit.\n\nIn Robert A. Heinlein's novel \"Time Enough for Love\", the sapient computers of Secundus, the planet on which part of the framing story is set, including Minerva, use an unbalanced ternary system. Minerva, in reporting a calculation result, says \"three hundred forty one thousand six hundred forty... the original ternary readout is unit pair pair comma unit nil nil comma unit pair pair comma unit nil nil point nil\".\n\nVirtual Adepts in the roleplaying game \"\" use ternary computers.\n\nIn Howard Tayler's webcomic \"Schlock Mercenary\", every modern computer is a ternary computer. AIs use the extra digit as \"maybe\" in boolean (true/false) operations, thus having a much more intimate understanding of fuzzy logic than is possible with binary computers.\n\nThe Conjoiners, in Alastair Reynolds' \"Revelation Space\" series, use ternary logic to program their computers and nanotechnology devices.\n\n\n"}
{"id": "636225", "url": "https://en.wikipedia.org/wiki?curid=636225", "title": "Thrust reversal", "text": "Thrust reversal\n\nThrust reversal, also called reverse thrust, is the temporary diversion of an aircraft engine's thrust so that it is directed forward, rather than backward. Reverse thrust acts against the forward travel of the aircraft, providing deceleration. Thrust reverser systems are featured on many jet aircraft to help slow down just after touch-down, reducing wear on the brakes and enabling shorter landing distances. Such devices affect the aircraft significantly and are considered important for safe operations by airlines. There have been accidents involving thrust reversal systems, including fatal ones.\n\nReverse thrust is also available on many propeller-driven aircraft through reversing the controllable-pitch propellers to a negative angle. The equivalent concept for a ship is called astern propulsion.\n\nA landing roll consists of touchdown, bringing the aircraft to taxi speed, and eventually to a complete stop. However, most commercial jet engines continue to produce thrust in the forward direction, even when idle, acting against the deceleration of the aircraft. The brakes of the landing gear of most modern aircraft are sufficient in normal circumstances to stop the aircraft by themselves, but for safety purposes, and to reduce the stress on the brakes, another deceleration method is needed. In scenarios involving bad weather, where factors like snow or rain on the runway reduce the effectiveness of the brakes, and in emergencies like rejected takeoffs, this need is more pronounced.\n\nA simple and effective method is to reverse the direction of the exhaust stream of the jet engine and use the power of the engine itself to decelerate. Ideally, the reversed exhaust stream would be directed straight forward. However, for aerodynamic reasons, this is not possible, and a 135° angle is taken, resulting in less effectiveness than would otherwise be possible. Thrust reversal can also be used in flight to reduce airspeed, though this is not common with modern aircraft. There are three common types of thrust reversing systems used on jet engines: the target, clam-shell, and cold stream systems. Some propeller-driven aircraft equipped with variable-pitch propellers can reverse thrust by changing the pitch of their propeller blades. Most commercial jetliners have such devices, and it also has applications in military aviation.\n\nSmall aircraft typically do not have thrust reversal systems, except in specialized applications. On the other hand, large aircraft (those weighing more than 12,500 lb) almost always have the ability to reverse thrust. Reciprocating engine, turboprop and jet aircraft can all be designed to include thrust reversal systems.\n\nPropeller-driven aircraft generate reverse thrust by changing the angle of their controllable-pitch propellers so that the propellers direct their thrust forward. This reverse thrust feature became available with the development of controllable-pitch propellers, which change the angle of the propeller blades to make efficient use of engine power over a wide range of conditions. Single-engine aircraft tend not to have reverse thrust. However, single-engine turboprop aircraft such as the PAC P-750 XSTOL, Cessna 208 Caravan, and Pilatus PC-6 Porter do have this feature available.\n\nOne special application of reverse thrust comes in its use on multi-engine seaplanes and flying boats. These aircraft, when landing on water, have no conventional braking method and must rely on slaloming and/or reverse thrust, as well as the drag of the water in order to slow or stop. In addition, reverse thrust is often necessary for maneuvering on the water, where it is used to make tight turns or even propel the aircraft in reverse, maneuvers which may prove necessary for leaving a dock or beach.\n\nOn aircraft using jet engines, thrust reversal is accomplished by causing the jet blast to flow forward. The engine does not run or rotate in reverse; instead, thrust reversing devices are used to block the blast and redirect it forward. High bypass ratio engines usually reverse thrust by changing the direction of only the fan airflow, since the majority of thrust is generated by this section, as opposed to the core. There are three jet engine thrust reversal systems in common use:\n\nThe target thrust reverser uses a pair of hydraulically-operated 'bucket' type doors to reverse the hot gas stream. For forward thrust, these doors form the propelling nozzle of the engine. In the original implementation of this system on the Boeing 707, and still common today, two reverser buckets were hinged so when deployed they block the rearward flow of the exhaust and redirect it with a forward component. This type of reverser is visible at the rear of the engine during deployment.\n\nThe clam-shell door, or cascade, system is pneumatically operated. When activated, the doors rotate to open the ducts and close the normal exit, causing the thrust to be directed forward. The cascade thrust reverser is commonly used on turbofan engines. On turbojet engines, this system would be less effective than the target system, as the cascade system only makes use of the fan airflow and does not affect the main engine core, which continues to produce thrust.\n\nIn addition to the two types used on turbojet and low-bypass turbofan engines, a third type of thrust reverser is found on some high-bypass turbofan engines. Doors in the bypass duct are used to redirect the air that is accelerated by the engine's fan section but does not pass through the combustion chamber (called bypass air) such that it provides reverse thrust. The cold stream reverser system is activated by an air motor. During normal operation, the reverse thrust vanes are blocked. On selection, the system folds the doors to block off the cold stream final nozzle and redirect this airflow to the cascade vanes. This system can redirect both the exhaust flow of the fan and of the core.\n\nThe cold stream system is known for structural integrity, reliability, and versatility. During thrust reverser activation, a sleeve mounted around the perimeter of the aircraft engine nacelle moves aft to expose cascade vanes which act to redirect the engine fan flow. This thrust reverser system can be heavy and difficult to integrate into nacelles housing large engines.\n\nIn most cockpit setups, reverse thrust is set when the thrust levers are on idle by pulling them further back. Reverse thrust is typically applied immediately after touchdown, often along with spoilers, to improve deceleration early in the landing roll when residual aerodynamic lift and high speed limit the effectiveness of the brakes located on the landing gear. Reverse thrust is always selected manually, either using levers attached to the thrust levers or moving the thrust levers into a reverse thrust 'gate'.\n\nThe early deceleration provided by reverse thrust can reduce landing roll by a quarter or more. Regulations dictate, however, that an aircraft must be able to land on a runway without the use of thrust reversal in order to be certified to land there as part of scheduled airline service.\n\nOnce the aircraft's speed has slowed, reverse thrust is shut down to prevent the reversed airflow from throwing debris in front of the engine intakes where it can be ingested, causing foreign object damage. If circumstances require it, reverse thrust can be used all the way to a stop, or even to provide thrust to push the aircraft backward, though aircraft tugs or towbars are more commonly used for that purpose. When reverse thrust is used to push an aircraft back from the gate, the maneuver is called a powerback. Some manufacturers warn against the use of this procedure during icy conditions as using reverse thrust on snow- or slush-covered ground can cause slush, water, and runway deicers to become airborne and adhere to wing surfaces.\n\nIf the full power of reverse thrust is not desirable, thrust reverse can be operated with the throttle set at less than full power, even down to idle power, which reduces stress and wear on engine components. Reverse thrust is sometimes selected on idling engines to eliminate residual thrust, in particular in icy or slick conditions, or when the engines' jet blast could cause damage.\n\nSome aircraft, notably some Russian and Soviet aircraft, are able to safely use reverse thrust in flight, though the majority of these are propeller-driven. Many commercial aircraft, however, cannot. In-flight use of reverse thrust has several advantages. It allows for rapid deceleration, enabling quick changes of speed. It also prevents the speed build-up normally associated with steep dives, allowing for rapid loss of altitude, which can be especially useful in hostile environments such as combat zones, and when making steep approaches to land.\n\nThe Douglas DC-8 series of airliners has been certified for in-flight reverse thrust since service entry in 1959. Safe and effective for facilitating quick descents at acceptable speeds, it nonetheless produced significant aircraft buffeting, so actual use was less common on passenger flights and more common on cargo and ferry flights, where passenger comfort is not a concern.\n\nThe Hawker Siddeley Trident, a 120- to 180-seat airliner, was capable of descending at up to 10,000 ft/min (3,050 m/min) by use of reverse thrust, though this capability was rarely used.\n\nThe Concorde supersonic airliner could use reverse thrust in the air to increase the rate of descent. Only the inboard engines were used, and the engines were placed in reverse idle only in subsonic flight and when the aircraft was below 30,000 ft in altitude. This would increase the rate of descent to around 10,000 ft/min.\n\nThe Boeing C-17 Globemaster III is one of the few modern aircraft that uses reverse thrust in flight. The Boeing-manufactured aircraft is capable of in-flight deployment of reverse thrust on all four engines to facilitate steep tactical descents up to 15,000 ft/min (4,600 m/min) into combat environments (a descent rate of just over 170 mph, or 274 km/h). The Lockheed C-5 Galaxy, introduced in 1969, also has in-flight reverse capability, although on the inboard engines only.\n\nThe Saab 37 Viggen (retired in November 2005) also had the ability to use reverse thrust both before landing, to shorten the needed runway, and taxiing after landing, allowing many Swedish roads to double as wartime runways.\n\nThe Shuttle Training Aircraft, a highly modified Grumman Gulfstream II, used reverse thrust in flight to help simulate Space Shuttle aerodynamics so astronauts could practice landings. A similar technique was employed on a modified Tupolev Tu-154 which simulated the Russian Buran space shuttle.\n\nThe amount of thrust and power generated are proportional to the speed of the aircraft, making reverse thrust more effective at high speeds. For maximum effectiveness, it should be applied quickly after touchdown. If activated at low speeds, foreign object damage is possible. There is some danger of an aircraft with thrust reversers applied momentarily leaving the ground again due to both the effect of the reverse thrust and the nose-up pitch effect from the spoilers. For aircraft susceptible to such an occurrence, pilots must take care to achieve a firm position on the ground before applying reverse thrust. If applied before the nose-wheel is in contact with the ground, there is a chance of asymmetric deployment causing an uncontrollable yaw towards the side of higher thrust, as steering the aircraft with the nose wheel is the only way to maintain control of the direction of travel in this situation.\n\nReverse thrust mode is used only for a fraction of aircraft operating time but affects it greatly in terms of design, weight, maintenance, performance, and cost. Penalties are significant but necessary since it provides stopping force for added safety margins, directional control during landing rolls, and aids in rejected take-offs and ground operations on contaminated runways where normal braking effectiveness is diminished. Airlines consider thrust reverser systems a vital part of reaching a maximum level of aircraft operating safety.\n\nIn-flight deployment of reverse thrust has directly contributed to the crashes of several transport-type aircraft:\n\n\n\n"}
{"id": "46707998", "url": "https://en.wikipedia.org/wiki?curid=46707998", "title": "Ultra HD Blu-ray", "text": "Ultra HD Blu-ray\n\nUltra HD Blu-ray is a digital optical disc data storage format that supersedes Blu-ray. Ultra HD Blu-ray discs are incompatible with existing Blu-ray players. Ultra HD Blu-ray supports 4K UHD (3840 × 2160 resolution) video at frame rates up to 60 frames per second, encoded using High Efficiency Video Coding. The discs support both high dynamic range by increasing the color depth to 10-bit per color and a greater color gamut than supported by conventional Blu-ray video by using the Rec. 2020 color space.\n\nThe specification allows for three disc capacities, each with its own data rate: 50 GB with 82 Mbit/s, 66 GB with 108 Mbit/s, and 100 GB with 128 Mbit/s. Ultra HD Blu-ray technology was licensed in mid 2015, and players had an expected release date of Christmas 2015. Ultra HD Blu-ray uses a new revision of AACS DRM, AACS 2.\n\nOn May 12, 2015, the Blu-ray Disc Association revealed completed specifications and the official Ultra HD Blu-ray logo. Unlike conventional DVDs and Blu-rays, the new 4K format does not have region coding.\n\nOn March 1, 2016, the BDA released Ultra HD Blu-ray with mandatory support for HDR10 Media Profile video and optional support for Dolby Vision.\n\nAs of January 23, 2018, the BDA spec v3.2 also includes optional support for HDR10+ and Philips/Technicolor’s SL-HDR2.\n\nThe first Ultra HD 4K Blu-ray Discs were officially released from four studios on March 1, 2016.:\n<br>\nThe first Ultra HD 4K Blu-ray Discs officially released from other studios after March 1, 2016:\n"}
