{"id": "4021459", "url": "https://en.wikipedia.org/wiki?curid=4021459", "title": "Air knife", "text": "Air knife\n\nAn air knife is a tool used to blow off liquid or debris from products as they travel on conveyors. Air knives are normally used in manufacturing or as the first step in a recursive recycling process to separate lighter or smaller particles from other components for use in later or subsequent steps, post manufacturing parts drying and conveyor cleaning, part of component cleaning. The knife consists of a high-intensity, uniform sheet of laminar airflow sometimes known as streamline flow.\n\nAn industrial air knife is a pressurized air plenum containing a series of holes or continuous slots through which pressurized air exits in a laminar flow pattern. The exit air velocity then creates an impact air velocity onto the surface of whatever object the air is directed. This impact air velocity can range from a gentle breeze to greater than Mach 0.6 (40,000 ft/min) to alter the surface of a product without mechanical contact.\n\nAir knives remove liquids, control the thickness of liquids, dry the liquid coatings, remove foreign particles, cool product surfaces or create a hold down force to assist in the mechanical bonding of materials to the surface. Electrical currents from anti-static bars can also be injected into the exit air knife stream to neutralize the static electricity charge on some surfaces.\n\nIn the majority of manufacturing applications for air knives, the air knives are stationary while the product passes through the air velocity air stream. In other circumstances, the product is stationary and the air knives move (reciprocate or rotate) over the surface of the stationary product. Although there are very few applications where an air knife can actually cut a product (break mechanical bonds between two points), air knives are often the most efficient method of removing or controlling unwanted or foreign substances on any surface.\n\nIn the 1950s and 60s, the term \"air doctor\" was first used to refer to the non-contact method of debris blow-off using compressed air. The printing and textile industries were some of the largest users of air doctors at that time. They often needed wide paths of air from a compressed air system to control the thickness of liquids on a surface, or to blow debris off the surface of materials prior to the next process. Other terms used were air bar, air squeegee, air curtain, air jet, air blast, air blow off, air nozzle, air comb, air blade and air doctor blade. Today the most commonly used term is simply \"air knife\".\n\nAlthough air knives powered by compressed plant air are used in a wide variety of industrial applications, industrial blower-powered air knives have proven to reduce the energy usage versus compressed air knives by 50–75% for most applications. Blower-powered air knife systems came of age with the advent of the 1987 Montreal Protocol, which started the clock on the worldwide phase-out of atmospheric ozone depleting CFC’s (chlorofluorocarbons) then used as cleaning agents in many industries.\n\nMost of these solvent-based cleaning agents simply evaporated which required no blow-off or other drying methods. Although the printed circuit board industry was still in its infancy, it was among the first to initiate the conversion to aqueous and semi-aqueous-based parts cleaning systems.\n\nWith nearly every existing and all future circuit board factories using the new environmentally friendly cleaning technology, they also needed a new method of drying the p.c. boards following their water-based cleaning to remove solder fluxes and other contaminants. The trend away from other types of solvent-based parts cleaning to water-based cleaning for other industries began soon thereafter. Additionally, the conversion to water-based inks, paints, coatings, adhesives and other solutions used in various manufacturing sectors has resulted in the need for air knife dryers where none had previously existed. As a result of the Montreal Protocol and worldwide industry compliance with environmental stewardship mandates, the former niche business of air knives became an industry.\n\nAir knives on a production line commonly range from in length with a discharge air slot or holes ranging from . A stationary air knife configuration can require from one to a dozen air knives depending on the application criteria. Air is blasted through the air knife slots via an air generator, either an industrial blower or air compressor, to deliver the predetermined exit air volume and velocity needed.\n\nThere are many application, environmental, efficiency and duty cycle aspects to consider when choosing between compressors and blowers. Compressed air, which is least efficient when used for air knives discharging into free air, allows for use of primary plant air. The piping sizes supplying the air knives can be as little as diameter, so they are ideal for confined spaces. Blower-powered air knives must be larger in size along with larger diameter supply piping, but the efficiency improvement over compressed air is easily justified with the electrical power cost savings.\n\nAir knife designs today have evolved to where some manufacturers produce a very efficient “teardrop” shape with a .95 coefficient of discharge. These blower-powered air knife designs typically have a profile of approximately wide x tall x any length, but the teardrop profile can range from tall depending on the criteria of the product for which the impact air velocity must be engineered. With construction ranging from thick aluminum extrusion to 11 gauge fabricated stainless steel, air knives can weigh 1 lb/ft to 25 lbs/ft. Depending on the width and speed of the product, the air knife can provide effective blow-off performance from or more away from the surface of the product. Round air nozzles of diameter can be effective against surfaces which are up to several feet (1 to 2 meters) from the product surface when engineered for such applications.\n\nThe most common use of air knives is to contain or remove free-standing materials (liquids or solids) from the surface of material. The applications include drying bottles and cans after filling and rinsing, printed circuit boards following the conveyorized wash to remove solder paste and flux, metals castings after automatic machining and many more. They can also deliver heated or cooled air to a surface, or create an invisible air barrier to separate heated or cooled environments from one another in industrial applications such as continuous metal heat treating ovens, cold process or storage areas in food processing or dust containment for the entrance to clean rooms.\n\nThere is a variety of uses for air knives in many different industries, applications and environments. The invisible-high velocity air streams can be discharged by air knife designs of numerous shapes and sizes. These range from “garage built” devices with a low level of precision to the most exotic metals of construction used in air knives for class 100 clean rooms.\n\nIn instances where noise reduction and moisture containment around a conveyorized air knife installation becomes important, some manufacturing facilities have installed air knives within an enclosure. These enclosures keep water contained, reduce the amount of air knife noise and even eliminate any liquid that could create safety concerns.\n\nThere has always been a wide assortment of blow-off appliances. Air knives and nozzles for compressed air blow-off range from home made round pipes with holes to engineered high pressure air knives. In order to achieve the highest efficiency using compressed air, many manufacturers of compressed air knives utilize the Coandă effect to improve compressed air knife design over other types of knives and nozzles. Although the efficiency of compressed air for low-pressure blow-off air is much lower than blowers, the Coanda-inspired air knives entrain ambient air into the high-velocity stream to enhance the blow off effect.\n\nThe teardrop-shaped air knife has a bulbous plenum which tapers down to a precise air discharge slot as the standard of the blower driven air knife industry. Whereas a round pipe with holes drilled has an average coefficient of discharge of 0.6 (60% efficient), the teardrop-shaped air knife is commonly 0.95 (95% efficient), which provides much higher-impact air velocity to the surface at which the air is directed with the lowest blower motor power demand. These teardrop designs are available in extruded aluminum shapes as well as fabricated carbon and stainless steels.\n\nInformation is needed regarding the Occupational Safety and Health Administration (OSHA) and their standards and directives on Compressed Air, especially when used for cleaning.\n"}
{"id": "2085909", "url": "https://en.wikipedia.org/wiki?curid=2085909", "title": "Army Battle Command System", "text": "Army Battle Command System\n\nThe Army Battle Command System (ABCS) is a digital Command, Control, Communications, Computers and Intelligence (C4I) system for the US Army. It includes a mix of fixed/semi-fixed and mobile networks. It is also designed for interoperability with US and Coalition C4I systems.\n\nArmy Battle Command System (ABCS) Version 6.4 is an integrated suite that allows troops to obtain an automated view of friendly activity and supply movement; plan fires, receive situation and intelligence reports, view the airspace and receive automatically disseminated weather reports.\n\n\n\nABCS combines six packages into a single system:\n\nAdditional systems that are integrated with the ABCS suite include:\n\n\n"}
{"id": "57198929", "url": "https://en.wikipedia.org/wiki?curid=57198929", "title": "Autonomous trucks", "text": "Autonomous trucks\n\nAutonomous trucks is a concept for autonomous vehicles applied for commercial uses.\n\nAs recorded in June 1995 in Popular Science Magazine, self-driving trucks were being developed for combat convoys, whereby only the lead truck would be driven by a human and the following trucks would rely on satellite, an inertial guidance system and ground-speed sensors. \n\nCaterpillar Inc. made early developments in 2013 with the Robotics Institute at Carnegie Mellon University to improve efficiency and reduce cost at various mining and construction sites. Companies such as Suncor Energy, a Canadian energy company, and Rio Tinto Group were among the first to replace human-operated trucks with driverless commercial trucks run by computers. \n\nIn April 2016, trucks from major manufacturers including Volvo and the Daimler Company completed a week of autonomous driving across Europe, organized by the Dutch, in an effort to get self-driving trucks on the road. With developments in self-driving trucks progressing, U.S. self-driving truck sales is forecast to reach 60,000 by 2035 according to a report released by IHS Inc. in June 2016. \n\nUber has also joined the project with \"Uber Freight\" which already delivers in Arizona. Another big player investing in this technology is Google, through its spin-off Waymo which also delivers freights in Atlanta.\n\n"}
{"id": "183952", "url": "https://en.wikipedia.org/wiki?curid=183952", "title": "Backpack", "text": "Backpack\n\nA backpack—also called bookbag, knapsack, rucksack, rucksac, pack, sackpack or backsack—is, in its simplest frameless form, a cloth sack carried on one's back and secured with two straps that go over the shoulders, but it can have an external frame, internal frame, and there are bodypacks.\n\nBackpacks are commonly used by hikers and students, and are often preferred to handbags for carrying heavy loads or carrying any sort of equipment, because of the limited capacity to carry heavy weights for long periods of time in the hands.\n\nLarge backpacks, used to carry loads over , as well as smaller sports backpacks (e.g. running, cycling, hiking and hydration), usually offload the largest part (up to about 90%) of their weight onto padded hip belts, leaving the shoulder straps mainly for stabilising the load. This improves the potential to carry heavy loads, as the hips are stronger than the shoulders, and also increases agility and balance, since the load rides nearer the wearer's own center of mass.\n\nThe word backpack was coined in the United States in the 1910s. \"Moneybag\" and \"packsack\" were used prior, and now occur mainly as regionalisms.\n\nThe word rucksack is a German loanword mainly used in the UK, US and in other Western military forces. In German, \"Rücken\" means \"back\" and \"Sack\" means \"bag\". The name rucksack is cognate with the Danish \"rygsæk\", Norwegian \"ryggsekk\", Dutch \"rugzak\", Afrikaans \"rugsak\", Swedish \"ryggsäck\", and Russian \"рюкзак\".\n\nThe word knapsack was the usual name for a rucksack or backpack up until the middle of the 20th century. This is commonly used in Canada.\n\nAlternative names include haversack from the German \"Hafersack\" meaning \"oat sack\" (which more properly describes a small cloth bag on a strap worn over one shoulder and originally referred to the bag of oats carried as horse fodder), \"Kraxe\" (a German rucksack with a rigid framework), and \"bergen\" (a large load-carrying rucksack, from a design issued by the British Army during the Second World War). In fact, Britons used to call Alpine-style backpacks \"Bergen rucksacks\", maybe from the name of their creator, Norwegian Ole F. Bergan, combined with the name of the Norwegian city of Bergen.\n\nBackpacks can often simply be referred to as \"packs\", especially in outdoors contexts; though sometimes ambiguous compared to other bags such as saddlebags and duffel bags, context is generally sufficient for identification. They are also used in recreational activities, and have long since been used to carry sports equipment and other materials.\n\nLong before its various terminologies began appearing in print, evidence of early backpacks was scarce. A contender for the earliest was found within the mummified remains of Ötzi in 3300BC.\n\nBackpacks in general fall into one of four categories: frameless, external frame, internal frame, and bodypack. A pack frame, when present, serves to support the pack and distribute the weight of its contents across the body more appropriately, by transferring much of the weight to the hips and legs. Most of the weight is therefore taken off the shoulders, reducing the chance of injury from shoulder strap pressure (many backpacks equipped solely with shoulder straps can affect the posture of a person carrying more than 14 kg (30 lbs)), as well as being less restrictive of the upper body range of motion. Most backpacks are capable of being closed with either a buckle mechanism, a zipper, or a dry-bag type closure, though a few models use a drawstring fitted with a cord lock for the main compartment.\n\nA bodypack is a backpack fitted with one or more pockets that are suspended on the wearer's chest and loaded in such a way that the load in the front and the load in the back are close to equal. The majority of the load in a bodypack is carried by the hips. The ideal load carrying system should not disturb the wearer's natural posture, balance and maneuverability. The load must be dispersed onto the skeletal structure in an even manner, and should not produce unbalanced forces on the body.\n\nThe simplest backpack design is a bag attached to a set of shoulder straps. Such packs are used for general transportation of goods, and have variable capacity. The simplest designs consist of one main pocket. This may be combined with webbing or cordage straps, while more sophisticated models add extra pockets, waist straps, chest straps, padded shoulder straps, padded backs, and sometimes reflective materials for added safety at night. These packs are generally produced inexpensively.\n\nSome outdoor packs, particularly those sold for day hikes, ultralight backpacking and mountaineering are sometimes frameless as well.\n\nSports and hydration backpacks are smaller with a profile closer to the body, wider straps and can come with water bladders and hip belts for running, cycling or hiking. Running hydration packs are the smallest and lightest, many under 2 litres and most under six litres. Compression straps across the top of one's body are common as are hip belts. Cycling hydration packs are six to ten litres sitting high on the back. Although daypacks are small averaging ten to thirty litres, all Trekking and Hiking hydration packs are generally the largest and heaviest. Thirty five up to sixty five litres and above are common.\n\nExternal frame packs were designed to carry heavy loads (>20 kg or 40 lb), giving the wearer more support and protection and better weight distribution than a simple, frameless strapped bag. Wooden pack frames were used for centuries around the world. Ötzi the Iceman may have used one in Copper Age Alpine Italy, though some archaeologists believe the frame found with the body was part of a snowshoe. Such packs are common in military and mountaineering applications; metal versions first appeared in the mid-20th century.\n\nThe external frame is typically made from aluminum, other lightweight metal alloy, and recently reinforced synthetic polymers or plastic and is equipped with a system of straps and tautly-stretched netting which prevents contact between the metal frame and user's back. In addition to comfort, this \"stand-off\" provides the additional benefit of creating air circulation between the frame and the wearer's back. For this reason, external frame packs are generally considered to be a \"cooler load\" than internal frame designs. External frame packs have a fabric \"sack\" portion which is usually smaller than that of internal frame packs, but have exposed frame portions above and below the sack to accommodate attachment of larger items. In addition, the sack can often be removed entirely, permitting the user to customize the configuration of their load, or to transport a non-conventional load such as a quartered game animal. Military packs are often external frame designs due to their ability to carry loads of different shapes, sizes and weights.\n\nThe other type of external frame which recently was proposed, is made from composite plastic which is not flexible like current backpack straps and also it is a kind of material that can be shaped like human spine curvature. In this type of backpack, load directly transfers to the shoulders through the non-flexible straps. This non-flexible structure diminishes the momentum at lumbar region of the back. Strap curvature is shaped close to spine curvature and there are two flexible drawstrings to prevent backpack movement in transverse plane. The straps of this backpack are wide enough to distribute the pressure on shoulders and also a white glass wool layer is added to the internal part of them to absorb dynamic forces, which could be produced through walking. This backpack type is an experimental sample that need further options to be prepared for usage. One of the benefits of backpack with external frame is preventing the spine to incline forward during walking that would be helpful in preventing damage of long term backpack carrying.\n\nThe internal frame backpack is a recent innovation, invented in 1967 by Greg Lowe, who went on to found Lowe Alpine and Lowepro, companies specializing in backpacks and other forms of carrying bags for various equipment. An internal-frame pack has a large fabric section around an internal frame composed of strips of either aluminum, titanium or plastic, sometimes with additional metal stays to reinforce the frame. A complex series of straps works with the frame to distribute the weight and hold it in place. The internal frame permits the pack to fit closely to the wearer's back and minimizes shifting of the load, which is desirable when participating in activities that involve upper-body movement such as scrambling over rocky surfaces and skiing. However, the tight fit reduces ventilation, so these type of packs tend to be more sweaty than external frame packs. The internal construction also allows for a large storage compartment; a few lash points (including webbing loops and straps for sleeping bags and other large items) may be present, but as the frame is completely integrated, it is difficult to securely lash larger and heavier items which do not fit inside the compartment to the outside of the pack. Internal frame packs originally suffered from smaller load capacity and less comfortable fit during steady walking, but newer models have improved greatly in these respects. In addition, because of their snug fit, the improved internal frame models have largely replaced external frame backpacks for many activities.\n\nA daypack is a smaller, frameless backpack that can hold enough contents for a day hike, or a day's worth of other activities. They are not large enough for average wilderness backpacking that use full-sized sleeping bags and backpacking tents, but may be large enough for ultralight backpacking. Padded or unpadded waist straps may be provided to distribute weight across the body.\nIn many countries, backpacks are heavily identified with students, and are a primary means of transporting educational materials to and from school. In this context they are sometimes known as bookbags or schoolbags. The purchase of a suitably fashionable, attractive, and useful backpack is a crucial back-to-school ritual for many students.\nTypical school backpacks generally lack the rigid frame of an outdoor-style backpack and include only a few pockets in the front in addition to the main storage compartment. While traditionally very simple in design, school backpacks are often made with padded shoulder straps and backs as well as additional reinforcement to hold large numbers of heavy textbooks, as well as safety features such as reflective panels to make the wearer of the pack more visible at night.\n\nBackpacks are sometimes worn as fashion accessories, in which they perform the same function as a purse. Some such backpacks designed specifically for women are no larger than a typical purse, and are generally associated with younger women.\n\nSome backpacks are specifically designed to carry certain items. Common examples include backpacks for small valuable items such as laptops and cameras; backpacks designed to hold laptop computers in particular generally have a padded compartment to hold the computer and medium-sized pockets and flaps to accommodate accessories such as charger cables and mice. These are especially common in college and university settings. In order to supply these devices with electricity, a few high-end backpacks are equipped with solar panels.\n\nRolling backpacks are backpacks with wheels on the bottom and an extending handle. Because of their design, rolling backpacks reduce the strain on the user, though the shoulder straps may be used to carry the pack for short distances when the terrain is not suitable for wheels. Rolling backpacks are most commonly used while traveling by airplane or train.\n\nHydration backpacks are also available. These light daypacks are especially designed to hold water in a special water bladder (also known as reservoir), and their purpose is to allow the carrier constant fluid hydration handsfree, so that the carrier can focus on the mission ahead without having to stop for water breaks.\n\nBackpacks are a standard part of the load-bearing equipment of soldiers, especially infantry, in most countries, and military-style packs are regularly available to civilians in military surplus stores. Well-known examples include the United States ALICE field pack and the British Army PLCE rucksack attachment, both of which are widely available to civilian markets both as actual military surplus (new or used) and as replicas. Such packs are often, though not always (e.g. the USMC's ILBE pack), external-frame packs, with the pack itself lashed or pinned to a metal or plastic carrying frame. For units that are entering combat situations, packs may be loaded heavily and can weigh in excess of 100 lbs. Each soldier may carry extra weapons, ammunition, rations, medical supplies, tents or other shelter material, and extra clothing.\n\nMany police tactical units, as well as players of military-style combat games such as paintball and airsoft, use these military-style tactical backpacks and webbing for storing gear and ammunition. There is also a small but thriving industry devoted to creating historical reproductions of military gear; such companies generally produce period-appropriate uniforms and other gear in addition to backpacks.\n\nSome more recent military/tactical designs, especially the MOLLE and ILBE packs used by the United States armed forces, are covered with webbing loop attachment points for increased carrying capacity.\n\nRecently, at least one brand of backpack has been specially designed for professional cooks and culinary students. This sort of backpack is meant to safely carry knives, cooking tools, and other miscellaneous equipment such as notebooks, towels, and uniforms.\n\nSpecialist backpacks are used by fire services for wildfire fighting, as well as by rescue services for Search and Rescue. These backpacks are generally very modular, allowing the pack to be reconfigured to the users wishes and are designed to load around the wearers hips. They may include features such as sections for water bladders and specially designed pouches, such as those used to carry personal fire shelters.\n\nBackpacks are sometimes used as luggage, particularly as carry-on bags for airplane travel.\n\nIn addition to their use in outdoors pursuits, backpacks are sometimes used in other sports as well. Hydration packs, sometimes used by athletes and military personnel, carry water (in either a bladder or a rigid bottle) and have a tube connected to them from which the wearer can drink without removing the pack; this feature is also included in some more general-purpose hiking backpacks. Backpacks that carry skateboards have also become more popular in the youth culture.\n\n"}
{"id": "19244343", "url": "https://en.wikipedia.org/wiki?curid=19244343", "title": "Balisor", "text": "Balisor\n\nBalisor is a system of illuminated beacons for high voltage power lines using a cold-cathode low-pressure neon lamp, used as an aircraft warning light.\n\nHigh voltage power cables, particularly those close to airports, need to be visible day and night. During the day, brightly coloured balls positioned along the length of the cables are sufficient, but during the night, lighting is necessary. These beacons provide this lighting by glowing red, the standard colour used in aviation for warning beacons.\n\nThe system is extremely simple, efficient and reliable and is considered the most economically viable solution. Its working principle is attractive because it uses a phenomenon usually considered as a weakness. However a large disadvantage is that it does not work when the line is switched off. It cannot also be used on HVDC powerlines, but similar devices may be also used on mast radiators.\n\nThe interest of the system lies in the way it obtains a power supply directly from the single cable on which it is mounted. It is difficult to obtain a very small amount of energy from a line which transports an enormous amount. \n\nEach high voltage cable creates an electric field around itself, in the same way as any live electrical conductor. When the electric potential of such a cable is sufficiently high, its electric field causes a significant voltage difference between the cable and its immediate neighbour. (It is the strength of the electric field that makes it difficult use low powers.)\n\nAs a result, a second conductor a few metres long, insulated from but parallel to the high voltage cable, will have a potential different from that of the main cable. Together the conductors make a capacitor charged across an air gap (dielectric).\n\nUnder certain conditions, the accumulated charge (and hence the potential difference) is sufficient to trigger a discharge lamp. This what is used in practice, giving a reliable and robust beacon.\n\n\n"}
{"id": "28229149", "url": "https://en.wikipedia.org/wiki?curid=28229149", "title": "Ben Thompson (writer)", "text": "Ben Thompson (writer)\n\nBen Thompson is an American business, technology, and media analyst, who is based in Taiwan. He is known principally for writing Stratechery, a subscription-based newsletter featuring in-depth commentary on tech and media news that has been called a \"must-read in Silicon Valley circles\". He also is the co-host of Exponent, a podcast focused on the same themes.\n\nThompson's career includes stints at Apple, where he interned at Apple University; Microsoft, where he worked on its Windows Apps team; and at WordPress developer Automattic as a growth engineer. Thompson launched Stratechery as a blog while still a Microsoft employee, and in April 2014 devoted himself to the site full-time, operating on a \"freemium\" subscription model. As of April 2015, Thompson had more than 2,000 paying subscribers. Thompson's success with Stratechery has drawn comparisons to Andrew Sullivan's Daily Dish and Nate Silver's FiveThirtyEight. However, Thompson has stated his primary inspiration was John Gruber, author of the site Daring Fireball.\n\nThompson's undergraduate education was at the University of Wisconsin-Madison, and his graduate education at Northwestern University, where he received an MBA from the Kellogg School as well as an MEM from the McCormick School of Engineering.\n\n"}
{"id": "1673867", "url": "https://en.wikipedia.org/wiki?curid=1673867", "title": "Blobotics", "text": "Blobotics\n\nBlobotics is a term describing research into chemical based computer processors based on ions rather than electrons. Andrew Adamatzky, a computer scientist at the University of the West of England in Bristol used the term in an article in New Scientist March 28, 2005 . \n\nThe aim is to create 'liquid logic gates' which would be 'infinitely reconfigurable and self-healing'. The process relies on the Belousov-Zhabotinsky reaction, a repeating cycle of three separate sets of reactions. Such a processor could form the basis of a robot which, using artificial sensors, interact with its surroundings in a way which mimics living creatures. \n\nThe coining of the term was featured by ABC radio in Australia . \n\n"}
{"id": "3957510", "url": "https://en.wikipedia.org/wiki?curid=3957510", "title": "Capacity factor", "text": "Capacity factor\n\nThe net capacity factor is the unitless ratio of an actual electrical energy output over a given period of time to the maximum possible electrical energy output over that period. The capacity factor is defined for any electricity producing installation, such as a fuel consuming power plant or one using renewable energy, such as wind or the sun. The average capacity factor can also be defined for any class of such installations, and can be used to compare different types of electricity production. \n\nThe maximum possible energy output of a given installation assumes its continuous operation at full nameplate capacity over the relevant period. The actual energy output during that period and the capacity factor vary greatly depending on a range of factors. The capacity factor can never exceed the availability factor, or uptime during the period. Uptime can be reduced due to, for example, reliability issues and maintenance, scheduled or unscheduled. Other factors include the design of the installation, its location, the type of electricity production and with it either the fuel being used or, for renewable energy, the local weather conditions. Additionally, the capacity factor can be subject to regulatory constraints and market forces, potentially affecting both its fuel purchase and its electricity sale. \n\nThe capacity factor is often computed over a timescale of a year, averaging out most temporal fluctuations. However, it can be also computed for a month to gain insight into seasonal fluctuations. Alternatively, it be computed over the lifetime of the power source, both while operational and after decommissioning.\n\nNuclear power plants are at the high end of the range of capacity factors, ideally reduced only by the availability factor, i.e. maintenance and refueling. The largest nuclear plant in the US, Palo Verde Nuclear Generating Station has between its three reactors a nameplate capacity of 3,942 MW. In 2010 its annual generation was 31,200,000 MWh, leading to a capacity factor of:\n\nEach of Palo Verde’s three reactors is refueled every 18 months, with one refueling every spring and fall. In 2014, a refueling was completed in a record 28 days, compared to the 35 days of downtime that the 2010 capacity factor corresponds to.\n\nThe Danish offshore wind farm Horns Rev 2, the world's largest at its inauguration in 2009, has a nameplate capacity of 209.3 MW.\n\nSites with lower capacity factors may be deemed feasible for wind farms, for example the onshore 1 GW Fosen Vind which is under construction in Norway has a projected capacity factor of 39%.\n\nCertain onshore wind farms can reach capacity factors of over 60%, for example the 44 MW Eolo plant in Nicaragua had a net generation of 232.132 GWh in 2015, equivalent to a capacity factor of 60.2%, while U.S. annual capacity factors from 2013 through 2016 range from 32.2% to 34.7%.\n\nSince the capacity factor of a wind turbine measures actual production relative to possible production, it is unrelated to Betz's coefficient of 16/27 formula_3 59.3%, which limits production vs. energy available in the wind.\n\n the Three Gorges Dam in China is, with its nameplate capacity of 22,500 MW, the largest power generating station in the world by installed capacity. \nIn 2015 it generated 87 TWh, for a capacity factor of:\n\nHoover Dam has a nameplate capacity of 2080 MW and an annual generation averaging 4.2 TW·h. (The annual generation has varied between a high of 10.348 TW·h in 1984, and a low of 2.648 TW·h in 1956.).\nTaking the average figure for annual generation gives a capacity factor of:\n\nAt the low range of capacity factors is the photovoltaic power station, which supplies power to the electricity grid from a large-scale photovoltaic system (PV system). \nAn inherent limit to its capacity factor comes from its requirement of daylight, preferably with a sun unobstructed by clouds, smoke or smog, shade from trees and building structures. \nSince the amount of sunlight varies both with the time of the day and the seasons of the year, the capacity factor is typically computed on an annual basis. \nThe amount of available sunlight is mostly determined by the latitude of the installation and the local cloud cover.\nThe actual production is also influenced by local factors such as dust and ambient temperature, which ideally should be low. As for any power station, the maximum possible power production is the nameplate capacity times the number of hours in a year, while the actual production is the amount of electricity delivered annually to the grid.\n\nFor example, Agua Caliente Solar Project, located in Arizona near the 33rd parallel and awarded for its excellence in renewable energy has a nameplate capacity of 290 MW and an actual average annual production of 740 GWh/year.\nIts capacity factor is thus:\n\nA significantly lower capacity factor is achieved by Lauingen Energy Park located in Bavaria, near the 49th parallel. With a nameplate capacity of 25.7 MW and an actual average annual production of 26.98 GWh/year it has a capacity factor of 12.0%.\n\nThere are several reasons why a plant would have a capacity factor lower than 100%. These include technical constraints, such as availability of the plant, economic reasons, and availability of the energy resource.\n\nA plant can be out of service or operating at reduced output for part of the time due to equipment failures or routine maintenance. \nThis accounts for most of the unused capacity of base load power plants. Base load plants have the lowest costs per unit of electricity because they are designed for maximum efficiency and are operated continuously at high output. \nGeothermal plants, nuclear plants, coal-fired plants and bioenergy plants that burn solid material are almost always operated as base load plants.\n\nA plant can also have its output curtailed or intentionally left idle because the electricity is not needed or because the price of electricity is too low to make production economical.\nThis accounts for most of the unused capacity of peaking power plants and load following power plants.\nPeaking plants may operate for only a few hours per year or up to several hours per day.\nMany other power plants operate only at certain times of the day or year because of variation in loads and electricity prices.\nIf a plant is only needed during the day, for example, even if it operates at full power output from 8 am to 8 pm every day (12 hours) all year long, it would only have a 50% capacity factor.\nDue to low capacity factors, electricity from peaking power plants is relatively expensive because the limited generation have to cover the plant fixed costs.\n\nA third reason is that a plant may not have the fuel available to operate all of the time. \nThis can apply to fossil generating stations with restricted fuels supplies, but most notably applies to intermittent renewable resources.\nSolar PV and wind turbines have a capacity factor limited by the availability of their \"fuel\", sunshine and wind respectively.\nA hydroelectricity plant may have a capacity factor lower than 100% due to scarcity of water, or its output may be regulated to match the current power need, conserving its stored water for later usage.\n\nOther reasons that a power plant may not have a capacity factor of 100% include restrictions or limitations on air permits and limitations on transmission that force the plant to curtail output.\n\nFor renewable energy sources such as solar power, wind power and hydroelectricity, the main reason for reduced capacity factor is generally the availability of the energy source. \nThe plant may be capable of producing electricity, but its \"fuel\" (wind, sunlight or water) may not be available. \nA hydroelectric plant's production may also be affected by requirements to keep the water level from getting too high or low and to provide water for fish downstream. \nHowever, solar, wind and hydroelectric plants do have high availability factors, so when they have fuel available, they are almost always able to produce electricity.\n\nWhen hydroelectric plants have water available, they are also useful for load following, because of their high \"dispatchability\". A typical hydroelectric plant's operators can bring it from a stopped condition to full power in just a few minutes.\n\nWind farms are variable, due to the natural variability of the wind. \nFor a wind farm, the capacity factor is determined by the availability of wind, the swept area of the turbine and the size of the generator. \nTransmission line capacity and electricity demand also affect the capacity factor. \nTypical capacity factors of current wind farms are between 25 and 45%. In the United Kingdom during the five year period from 2011 to 2015 the annual capacity factor for wind was over 30%. \n\nSolar energy is variable because of the daily rotation of the earth, seasonal changes, and because of cloud cover. \nFor example, the Sacramento Municipal Utility District observed a 15% capacity factor in 2005.\nHowever, according to the SolarPACES programme of the International Energy Agency (IEA), solar power plants designed for solar-only generation are well matched to summer noon peak loads in areas with significant cooling demands, such as Spain or the south-western United States, although in some locations solar PV does not reduce the need for generation of network upgrades given that air conditioner peak demand often occurs in the late afternoon or early evening when solar output is reduced. \nSolarPACES states that by using thermal energy storage systems the operating periods of solar thermal power (CSP) stations can be extended to become dispatchable (load following).\n\nGeothermal has a higher capacity factor than many other power sources, and geothermal resources are generally available all the time.\n\nAccording to the US Energy Information Administration (EIA), from 2013-2017 the capacity factors of utility-scale generators were as follows:\n! colspan=\"8\"| Non-fossil fuels !! Coal !! colspan=\"4\" | Natural Gas !! colspan=\"3\" | Petroleum Liquids\n!Nuclear !! Hydro !!Wind !!Solar PV !!Solar CSP !!Landfill Gas and !!Other Biomass including Wood !! Geothermal !! !! !! !! !! !! !! !! \nHowever, these values often vary significantly by month.\n\n\nThe following figures were collected by the Department of Energy and Climate Change on the capacity factors for various types of plants in UK grid: \n\n"}
{"id": "318895", "url": "https://en.wikipedia.org/wiki?curid=318895", "title": "Deodorant", "text": "Deodorant\n\nA deodorant is a substance applied to the body to prevent body odor caused by the bacterial breakdown of perspiration in armpits, feet, and other areas of the body. A subgroup of deodorants, antiperspirants, affect odor as well as prevent sweating by affecting sweat glands. Antiperspirants are typically applied to the underarms, while deodorants may also be used on feet and other areas in the form of body sprays.\n\nIn the United States, the Food and Drug Administration classifies and regulates most deodorants as cosmetics, but classifies antiperspirants as over-the-counter drugs.\n\nThe first commercial deodorant, Mum, was introduced and patented in the late nineteenth century by an inventor in Philadelphia, Pennsylvania, Edna Murphey. The product was briefly withdrawn from the market in the U.S. The modern formulation of the antiperspirant was patented by Jules Montenier on January 28, 1941. This formulation was first found in \"Stopette\" deodorant spray, which \"Time\" magazine called \"the best-selling deodorant of the early 1950s\".\n\nThere is a popular myth that deodorant use is linked to breast cancer, but so far no such causal link has been substantiated in research.\n\nIn 1888, the first commercial deodorant, Mum, was developed and patented by a U.S. inventor in Philadelphia, Pennsylvania, Edna Murphey. The small company was bought by Bristol-Myers in 1931 and in the late 1940s, Helen Barnett Diserens developed an underarm applicator based on the newly invented ball-point pen. In 1952, the company began marketing the product under the name Ban Roll-On. The product was briefly withdrawn from the market in the U.S., but it is once again available at retailers in the U.S. under the brand Ban. In the UK it is sold under the names Mum Solid and Mum Pump Spray. Chattem acquired Ban deodorant brand in 1998 and subsequently sold it to Kao Corporation in 2000.\n\nIn 1903, the first commercial antiperspirant was Everdry. The modern formulation of the antiperspirant was patented by Jules Montenier on January 28, 1941. This patent addressed the problem of the excessive acidity of aluminum chloride and its excessive irritation of the skin, by combining it with a soluble nitrile or a similar compound. This formulation was first found in \"Stopette\" deodorant spray, which Time Magazine called \"the best-selling deodorant of the early 1950s\". \"Stopette\" gained its prominence as the first and long-time sponsor of the game show \"What's My Line?\", and was later eclipsed by many other brands as the 1941 patent expired.\n\nBetween 1942 and 1957 the market for deodorants increased 600 times to become a $70 million market. Deodorants were originally marketed primarily to women, but by 1957 the market had expanded to male users, and estimates were that 50% of men were using deodorants by that date. The Ban Roll-On product led the market in sales.\n\nIn the early 1960s, the first aerosol antiperspirant in the marketplace was Gillette's Right Guard, whose brand was later sold to Henkel in 2006. Aerosols were popular because they let the user dispense a spray without coming in contact with the underarm area. By the late 1960s, half of all the antiperspirants sold in the U.S. were aerosols, and continued to grow in all sales to 82% by the early 1970s. However, in the late 1970s two problems arose which greatly changed the popularity of these products. First, in 1977 the U.S. Food and Drug Administration banned the active ingredient used in aerosols, aluminium zirconium chemicals, due to safety concerns over long term inhalation. Second, the U.S. Environmental Protection Agency limited the use of chlorofluorocarbon (CFC) propellants used in aerosols due to awareness that these gases can contribute to depleting the ozone layer. As the popularity of aerosols slowly decreased, stick antiperspirants became more popular.\n\nHuman perspiration is largely odorless until it is fermented by bacteria that thrive in hot, humid environments. The human underarm is among the most consistently warm areas on the surface of the human body, and sweat glands provide moisture, which when excreted, has a vital cooling effect. When adult armpits are washed with alkaline pH soap, the skin loses its acid mantle (pH 4.5 - 6), raising the skin pH and disrupting the skin barrier. As many bacteria thrive in this elevated pH environment, this makes the skin susceptible to bacterial colonization. The bacteria feed on the sweat from the apocrine glands and on dead skin and hair cells, releasing trans-3-methyl-2-hexenoic acid in their waste, which is the primary cause of body odor. Underarm hair wicks the moisture away from the skin and aids in keeping the skin dry enough to prevent or diminish bacterial colonization. The hair is less susceptible to bacterial growth and therefore is ideal for preventing the bacterial odor.\nIn the United States, deodorants are classified and regulated as cosmetics by the U.S. Food and Drug Administration (FDA) and are designed to eliminate odor. Deodorants are often alcohol-based. Alcohol initially stimulates sweating, but may also temporarily kill bacteria. Other active ingredients in deodorants include sodium stearate, sodium chloride and stearyl alcohol. Deodorants can be formulated with other, more persistent antimicrobials such as triclosan that slow bacterial growth or with metal chelant compounds such as EDTA. Deodorants may contain perfume fragrances or natural essential oils intended to mask the odor of perspiration. In the past, deodorants included chemicals such as zinc oxide, acids, ammonium chloride, sodium bicarbonate and formaldehyde, but some of these ingredients were messy, irritating to the skin or even carcinogenic.\n\nOver-the-counter products labeled as \"natural deodorant crystal\" containing the chemical potassium alum, which contains aluminum, have gained new-found popularity as an alternative health product, in spite of concerns about possible contact dermatitis. A popular alternative to modern commercial deodorants is ammonium alum, which is a common type of alum, also containing aluminum, sold in crystal form and often referred to as a \"deodorant crystal\". It has been used as a deodorant throughout history in Thailand, the Far East, Mexico and other countries.\n\nVaginal deodorant, in the form of sprays, suppositories and wipes, is often used by women to take away the smell of the vagina. Vaginal deodorants can sometimes cause dermatitis.\n\nIn the United States, deodorants combined with antiperspirant agents are classified as drugs by the FDA. Antiperspirants attempt to stop or significantly reduce perspiration and thus reduce the moist climate in which bacteria thrive. Aluminium chloride, aluminium chlorohydrate, and aluminium-zirconium compounds, most notably aluminium zirconium tetrachlorohydrex gly and aluminium zirconium trichlorohydrex gly, are frequently used in antiperspirants. Aluminium chlorohydrate and aluminium-zirconium tetrachlorohydrate gly are the most frequent active ingredients in commercial antiperspirants. Aluminium-based complexes react with the electrolytes in the sweat to form a gel plug in the duct of the sweat gland. The plugs prevent the gland from excreting liquid and are removed over time by the natural sloughing of the skin. The metal salts work in another way to prevent sweat from reaching the surface of the skin: the aluminium salts interact with the keratin fibrils in the sweat ducts and form a physical plug that prevents sweat from reaching the skin’s surface. Aluminium salts also have a slight astringent effect on the pores; causing them to contract, further preventing sweat from reaching the surface of the skin. The blockage of a large number of sweat glands reduces the amount of sweat produced in the underarms, though this may vary from person to person. Methenamine in the form of cream and spray is successfully used for treatment of excessive sweating and related to it odour. Antiperspirants are usually better applied before bed.\n\nDeodorants and antiperspirants come in many forms. What is commonly used varies in different countries. In Europe, aerosol sprays are popular, as are cream and roll-on forms. In North America, solid or gel forms are dominant.\n\nAfter using a deodorant containing zirconium, the skin may develop an allergic, axillary granuloma response. Antiperspirants with propylene glycol, when applied to the axillae, can cause irritation and may promote sensitization to other ingredients in the antiperspirant. Deodorant crystals containing synthetically made potassium alum were found to be a weak irritant to the skin. Unscented deodorant is available for those with sensitive skin. Frequent use of deodorants was associated with blood concentrations of the synthetic musk galaxolide.\n\nAluminum is present most often in antiperspirants in the form of aluminum chlorohydrate. Aluminum chlorohydrate is not the same as the compound aluminum chloride, which has been established as a neurotoxin. At high doses, aluminum itself adversely affects the blood–brain barrier, is capable of causing DNA damage, and has adverse epigenetic effects.\n\nThe U.S. Food and Drug Administration, in a monograph dedicated to analysing the safety of deodorants, concluded that \"despite many investigators looking at this issue, the agency does not find data from topical and inhalation chronic exposure animal and human studies submitted to date sufficient to change the monograph status of aluminum containing antiperspirants\", therefore allowing their use and vowing to keep monitoring the scientific literature. Members of the Scientific Committee on Consumer Safety (Europe) concluded that \"due to the lack of adequate data on dermal penetration to estimate the internal dose of aluminium following cosmetic uses, risk assessment cannot be performed.\"\n\nThe myth that breast cancer is believed to be linked with deodorant use has been widely circulated, and appears to originate from a spam email sent in 1999; however, there is no evidence to support the existence of such a link. One constituent of deodorant products which has given cause for concern are parabens, a chemical additive. According to the American Cancer Society \"studies have not shown any direct link between parabens and any health problems, including breast cancer\".\n\nThe FDA has \"acknowledge[d] that small amounts of aluminium can be absorbed from the gastrointestinal tract and through the skin.\", leading to a warning \"that people with kidney disease may not be aware that the daily use of antiperspirant drug products containing aluminium may put them at a higher risk because of exposure to aluminium in the product.\" The agency warns people with kidney dysfunction to consult a doctor before using antiperspirants containing aluminum.\n\nIf aerosol deodorant is held close to the skin for long enough, it can cause an aerosol burn—a form of frostbite. In controlled tests, spray deodorants have been shown to cause temperature drops of over 60 °C in a short period of time.\n\nAluminium zirconium tetrachlorohydrex gly, a common antiperspirant, can react with sweat to create yellow stains on clothing. Underarm liners are an antiperspirant alternative that does not leave stains.\n\n\n"}
{"id": "3443916", "url": "https://en.wikipedia.org/wiki?curid=3443916", "title": "Design methods", "text": "Design methods\n\nDesign methods are procedures, techniques, aids, or tools for designing. They offer a number of different kinds of activities that the designer might use within an overall design process. Conventional procedures of design, such as drawing, can be regarded as design methods, but since the 1950s new procedures have been developed that are more usually grouped together under the name of 'Design Methods’. What these new methods have in common is that they \"are attempts to make public the hitherto private thinking of designers; to externalise the design process\".\n\n\"Design methodology\" is the broader study of method in design: the study of the principles, practices and procedures of designing.\nSocial, political and economic developments of the late 19th and first half of the 20th century put into motion modern benefits and constraints for living and working. Industrial and technological breakthroughs associated with this period created social and economic complexities for people and their environment. Disciplines such as architecture, urban planning, engineering and product development began to tackle new types of problem-solving past traditional artifact making. More informed and methodical approaches to designing were required.\n\nFrom 1958 to 1963 Horst Rittel was a pioneer in articulating the relationship between science and design, specifically the limitations of design processes based on the 19th century rational view of science, in his courses at Ulm School of Design in Germany (Hochschule für Gestaltung - HfG Ulm: 1953–1968). Rittel proposed principles for dealing with these limitations through his seminal HfG design methods courses: cybernetics, operational analysis and communication theory. In 1963 he was recruited to Berkeley to teach design methods courses and helped found the Design Methods Group (DMG) and the DMG Journal.\n\nDesign methods in England originally drew from a 1962 conference called \"The Conference on Systematic and Intuitive Methods in Engineering, Industrial Design, Architecture and Communications.\" This event was organized by John Chris Jones, and Peter Slann who, with conference invitees, were driven by concerns about how their modern industrialized world was being manifested.\n\nConference participants countered the craftsman model of design which was rooted in turning raw materials through tried and true craft-based knowledge into finished products. They believed that a single craft-based designer producing design solutions was not compatible with addressing the evolving complexity of post-industrial societies. They stressed that designers needed to work in cross-disciplinary teams where each participant brings his/her specific body of skills, language and experiences to defining and solving problems in whatever context.\n\nThe key benefit was to find a method that suits a particular design situation. Christopher Alexander went on to write his seminal books \"A Pattern Language\" and \"A Timeless Way of Building\".\n\nIn the late 1950s and early 1960s, graduates of the Ulm School of Design in Germany (Hochschule für Gestaltung - HfG Ulm: 1953–1968). began to spread Horst Rittel's approach of design methodology across Europe and the United States in context of their professional work and teaching what became known as the 'Ulm Model'.\n\nLikewise, after the 1962 conference in England, many of the participants began to publish and to define an area of research that focused on design. Three \"camps\" seemed to emerge to develop different directions in the initial work in Design Methods:\n\nThe Design Research Society was founded in 1966 by several participants from the Conference on Design Methods. The purpose of the Society is to promote \"the study of and research into the process of designing in all its many fields\" and is an interdisciplinary group with many professions represented, but all bound by the conviction of the benefits of design research.\n\nThe Environmental Design Research Association is one of the best-known entities that strive to integrate designers and social science professionals for better built environments. EDRA was founded by Henry Sanoff in 1969. Both John Chris Jones and Christopher Alexander interacted with EDRA and other camps; both seemed at a certain point to reject their interpretations. Jones and Alexander also questioned their original theses about design methods.\n\nAn interesting shift that affected design methods and design studies was a series of 1968 lectures by Herbert A. Simon, the Nobel laureate, on \"The Sciences of the Artificial\". He proposed using scientific methods to explore the world of man-made things (hence artificial). He discussed the role of analysis (observation) and synthesis (making) as a process of creating man-made responses to the world he/she interacted with. Important to Simon's contribution were his notions of \"bounded rationality\" and \"satisficing.\" Simon's concept had a profound impact on the discourse in both design methods, and the newly emerging design studies communities in two ways. It provided an entry to using scientific ideas to overlay on design, and it also created an internal debate whether design could/should be expressed and practiced as a type of science with the reduction of emphasis on intuition.\n\nNigel Cross has been prolific in articulating the issues of design methods and design research. The discussion of the ongoing debate of what is design research and design science was, and continues to be articulated by Cross. His thesis is that design is not a science, but is an area that is searching for \"intellectual independence.\" He views the original design methods discussions of the 1960s as a way to integrate objective and rational methods in practicing design. Scientific method was borrowed as one framework, and the term design science was coined in 1966 at the conference on The Design Method focusing on a systematic approach to practicing design. Cross defined the \"science of design\" as a way to create a body of work to improve the understanding of design methods — and more importantly that design does not need to be a binary choice between science and art.\n\nNigan Bayazit published an overview of the historical development of design methods. She stated that \"Design methods people were looking at rational methods of incorporating scientific techniques and knowledge into the design process to make rational decisions to adapt to the prevailing values, something that was not always easy to achieve.\"\n\nConversations about design methods and a more systematic approach to design was not isolated to Europe. America was also a magnet for practicing design professionals to codify their successes in design practice and backing into larger theories about the dynamics of design methods.\n\nAmerican designers were much more pragmatic at articulating design methods and creating an underlying language about the practice of industrial and graphic design. They were tied to economic systems that supported design practice and therefore focused on the way design could be managed as an extension of business, rather than the European approach to design methods based on transforming engineering by design.\n\nIndustrial design was the first area that made inroads into systematizing knowledge through practice. Raymond Loewy was instrumental at elevating the visibility of industrial design through cult of personality (appearing three times on front cover of \"Time Magazine\"). Henry Dreyfuss had a profound impact on the practice of industrial design by developing a systematic process used to shape environments, transportation, products and packaging. His focus on the needs of the average consumer was most celebrated in his book \"Designing for People\", an extensive exploration of ergonomics.\n\nJay Doblin one of America's foremost industrial designers, worked for Raymond Loewy and was later an employee of Unimark International, the world’s largest global design firm during the 1960s with offices in seven countries. In 1972, Doblin formed Chicago-based Jay Doblin & Associates, a firm which managed innovative programs for Xerox Corporation and General Electric. Doblin was prolific at developing a language to describe design. One of his best articles was \"A Short, Grandiose Theory of Design\", published in the 1987 Society of Typographic Arts Design Journal. In seven pages, Doblin presents a straightforward and persuasive argument for design as a systematic process. He described the emerging landscape of systematic design:\n\nDoblin and others were responding to the increased specialization of design and the complexity of managing large design programs for corporations. It was a natural process to begin to discuss how design should move upstream to be involved with the specifications of problems, not only in the traditional mode of production which design had been practiced. Particularly since 2000, design methods and its intersection with business development have been visibly championed by numerous consultancies within design industry.\n\nThe continuity of approaches to design projects by such representative firms is the generation of inputs incited by the human condition in varied contexts. These approaches utilize a sustainable methods-based mode of making that takes into account critical analytic and synthetic skills toward more informed and inspired specifications grounded in:\n\nPractitioners approached design methods from a different angle than John Christopher Jones and the group of engineers and designers who convened in 1962. Many practitioners, through actual design opportunities, began to confront the complexities of the market and clients. They began to address issues of specifications, users, distribution and innovation. Since there were no established methods, each practitioner began to develop frameworks and languages to describe a new way to design. Like any market-based model, there were many competing ideas about these new methods and their basis. Many of these designers may have been aware of the design methods movement, but many were not. Yet all their ideas were aligned to many of the basic tenets of the 1962 conference which advocated a more rigorous way of doing design. However, the social perspectives and criticisms of mediocre products of 1962 participants may not have been shared or agreed with.\n\nThere is no one way to practice design methods. John Chris Jones recognized this by stating:\n\nThe focus of most post-1962 enhancements to design methods has been on developing a series of relevant, sound, humanistic problem-solving procedures and techniques to reduce avoidable errors and oversights that can adversely affect design solutions. The key benefit is to find a method that suits a particular design situation.\n\nThe benefits of their original work has been abstracted many times over; but in today's design environment, several of their main ideas have been integrated into contemporary design methods:\n\nA large challenge for design as a discipline, its use of methods and an endeavor to create shared values, is its inherent synthetic nature as an area of study and action. This allows design to be extremely malleable in nature, borrowing ideas and concepts from a wide variety of professions to suit the ends of individual practitioners. It also makes design vulnerable since these very activities make design a discipline unextensible as a shared body of knowledge.\n\nIn 1983, Donald Schon at the Massachusetts Institute of Technology, published \"The Reflective Practitioner\". He saw traditional professions with stable knowledge bases, such as law and medicine, becoming unstable due to outdated notions of 'technical-rationality' as the grounding of professional knowledge. Practitioners were able to describe how they 'think on their feet', and how they make use of a standard set of frameworks and techniques. Schon foresaw the increasing instability of traditional knowledge and how to achieve it. This is in line with the original founders of design methods who wanted to break with an unimaginative and static technical society and unify exploration, collaboration and intuition.\n\nDesign methods has influenced design practice and design education. It has benefited the design community by helping to create introductions that would never have happened if traditional professions remained \"stable\", which did not necessarily allow collaboration due to gate keeping of areas of knowledge and expertise. Design has been by nature an interloper activity, with individuals that have crossed disciplines to question and innovate.\n\nThe challenge is to transform individual experiences, frameworks and perspectives into a shared, understandable, and, most importantly, a transmittable area of knowledge. Victor Margolin states three reasons why this will prove difficult:\n\nIn the end, \"design methods\" is a term that is widely used. Though conducive to interpretations, it is a shared belief in an exploratory and rigorous method to solve problems through design, an act which is part and parcel of what designers aim to accomplish in today's complex world.\n\n\n"}
{"id": "2154814", "url": "https://en.wikipedia.org/wiki?curid=2154814", "title": "Digital Cinema Initiatives", "text": "Digital Cinema Initiatives\n\nDigital Cinema Initiatives, LLC (DCI) is a joint venture of major motion picture studios, formed to establish a standard architecture for digital cinema systems.\n\nThe organization was formed in March 2002 by Metro-Goldwyn-Mayer, Paramount Pictures, Sony Pictures Entertainment, 20th Century Fox, Universal Studios, The Walt Disney Company and Warner Bros.\n\nThe primary purpose of DCI is to establish and document specifications for an open architecture for digital cinema that ensures a uniform and high level of technical performance, reliability and quality. By establishing a common set of content requirements, distributors, studios, exhibitors, d-cinema manufacturers and vendors can be assured of interoperability and compatibility. Because of the relationship of DCI to many of Hollywood's key studios, conformance to DCI's specifications is considered a requirement by software developers or equipment manufacturers targeting the digital cinema market.\n\nOn July 20, 2005, DCI released Version 1.0 of its \"Digital Cinema System Specification\", commonly referred to as the \"DCI Specification\". The document describes overall system requirements and specifications for digital cinema. Between March 28, 2006, and March 21, 2007, DCI issued 148 errata to Version 1.0.\n\nDCI released Version 1.1 of the DCI Specification on April 12, 2007, incorporating the previous 148 errata into the DCI Specification. On April 15, 2007, at the annual NAB Digital Cinema Summit, DCI announced the new version, as well as some future plans. They released a \"Stereoscopic Digital Cinema Addendum\" to begin to establish 3-D technical specifications in response to the popularity of 3-D stereoscopic films. It was also announced \"which studios would take over the leadership roles in DCI after the current leadership term expires at the end of September.\"\n\nSubsequently, between August 27, 2007, and February 1, 2008, DCI issued 100 errata to Version 1.1. So, DCI released Version 1.2 of the DCI Specification on March 7, 2008, again incorporating the previous 100 errata into the specification document. An additional 96 errata were issued by August 30, 2012, so a revised Version 1.2 incorporating those additional errata was approved on October 10, 2012. DCI approved DCI Specification Version 1.3 on June 27, 2018, integrating the 45 errata issued to the previous version into a new document. The previous versions are also archived on the DCI web site.\n\nBased on many SMPTE and ISO standards, such as JPEG 2000-compressed image and \"broadcast wave\" PCM/WAV sound, it explains the route to create an entire Digital Cinema Package (DCP) from a raw collection of files known as the Digital Cinema Distribution Master (DCDM), as well as the specifics of its content protection, encryption, and forensic marking.\n\nThe specification also establishes standards for the decoder requirements and the presentation environment itself, such as ambient light levels, pixel aspect and shape, image luminance, white point chromaticity, and those tolerances to be kept.\n\nEven though it specifies what kind of information is required, the DCI Specification does not include specific information about how data within a distribution package is to be formatted. Formatting of this information is defined by the Society of Motion Picture and Television Engineers (SMPTE) digital cinema standards and related documents.\n\n\n\n\nDCI has additionally published a document outlining recommended practice for High Frame Rate digital cinema. This document discloses the following proposed frame rates: 60, 96, and 120 frames per second for 2D at 2K resolution; 48 and 60 for stereoscopic 3D at 2K resolution; 48 and 60 for 2D at 4K resolution. The maximum compressed bit rate for support of all proposed frame rates should be 500 Mbit/s.\n\nThe idea for DCI was originally mooted in late 1999 by Tom McGrath, then COO of Paramount Pictures, who applied to the U.S. Department of Justice for anti-trust waivers to allow the joint cooperation of all seven major motion picture studios.\n\nUniversal Pictures made one of the first feature-length DCPs created to DCI specifications, using their film \"Serenity\". Although it was not distributed theatrically, it had one public screening on November 7, 2005, at the USC Entertainment Technology Center's Digital Cinema Laboratory in the Pacific Theatre, Hollywood. \"Inside Man\" was Universal's first DCP commercial release, and, in addition to 35mm film distribution, was delivered via hard drive to 20 theatres in the United States along with two trailers.\n\nThe Academy Film Archive houses the Digital Cinema Initiatives, LLC Collection, which includes film and digital elements from DCI’s Standard Evaluation Material (StEM), a 12-minute production shot on 35mm and 65mm film, created for vendors and standards organizations to test and evaluate image compression and digital projection technologies.\n\n"}
{"id": "1813588", "url": "https://en.wikipedia.org/wiki?curid=1813588", "title": "Digital distribution", "text": "Digital distribution\n\nDigital distribution (also referred to as content delivery, online distribution, or electronic software distribution (ESD), among others) is the delivery or distribution of digital media content such as audio, video, software and video games. The term is generally used to describe distribution over an online delivery medium, such as the Internet, thus bypassing physical distribution methods, such as paper, optical discs, and VHS videocassettes. The term online distribution is typically applied to freestanding products; downloadable add-ons for other products are more commonly known as downloadable content. With the advancement of network bandwidth capabilities, online distribution became prominent in the 21st century.\n\nContent distributed online may be streamed or downloaded, and often consists of books, films and television programs, music, software, and video games. Streaming involves downloading and using content at a user's request, or \"on-demand\", rather than allowing a user to store it permanently. In contrast, fully downloading content to a hard drive or other form of storage media may allow offline access in the future.\n\nSpecialist networks known as content delivery networks help distribute content over the Internet by ensuring both high availability and high performance. Alternative technologies for content delivery include peer-to-peer file sharing technologies. Alternatively, content delivery platforms create and syndicate content remotely, acting like hosted content management systems.\n\nHowever, the term is also used in film distribution to describe distribution of content through physical media, in opposition to distribution by analog media such as photographic film and magnetic tape (see digital cinema).\n\nA primary characteristic of online distribution is its direct nature. To make a commercially successful work, artists usually must enter their industry’s publishing chain. Publishers help artists advertise, fund and distribute their work to retail outlets. In some industries, particularly video games, artists find themselves bound to publishers, and in many cases unable to make the content they want; the publisher might not think it will profit well. This can quickly lead to the standardization of the content and to the stifling of new, potentially risky ideas.\n\nBy opting for online distribution, an artist can get their work into the public sphere of interest easily with potentially minimum business overhead. This often leads to cheaper goods for the consumer, increased profits for the artists, as well as increased artistic freedom. Online distribution platforms often contain or act as a form of digital rights management.\n\nOnline distribution also opens the door to new business models (e.g., the Open Music Model). For instance, an artist could release one track from an album or one chapter from a book at a time instead of waiting for them all to be completed. This either gives them a cash boost to help continue their projects or warns that their work is not financially viable. This is hopefully done before they have spent excessive money and time on a project deemed unviable. Video games have increased flexibility in this area, demonstrated by micropayment models. A clear result of these new models is their accessibility to smaller artists or artist teams who do not have the time, funds, or expertise to make a new product in one go.\n\nAn example of this can be found in the music industry. Indie artists may access the same distribution channels as major record labels, with potentially fewer restrictions and manufacturing costs. There is a growing collection of 'Internet labels' that offer distribution to unsigned or independent artists directly to online music stores, and in some cases marketing and promotion services. Further, many bands are able to bypass this completely, and offer their music for sale via their own independently controlled websites.\n\nAn issue is the large number of incompatible formats in which content is delivered, restricting the devices that may be used, or making data conversion necessary.\n\nThe rise of online distribution has provided controversy for the traditional business models and resulted in challenges as well as new opportunities for traditional retailers and publishers. Online distribution affects all of the traditional media markets including music, press, and broadcasting. In Britain, the iPlayer, a software application for streaming television and radio, accounts for 5% of all bandwidth used in the United Kingdom.\n\nThe move towards online distribution led to a dip in sales in the 2000s when CD sales were nearly cut in half. One such example of online distribution taking its toll on a retailer is the Canadian music chain Sam the Record Man who blamed online distribution for having to close a number of its traditional retail venues in 2007–08. One main reason that sales took such a big hit was that unlicensed downloads of digital music was very accessible. With copyright infringement affecting sales, the music industry realized it needed to change its business model to keep up with the rapidly changing technology. The step that was taken to move the music industry into the online space has been successful for several reasons. The development of lossy audio compression file formats such as MP3, allows users to compress music files into a high quality format, compressed down to usually a 3-megabyte (MB) file. The lossless FLAC format may require only a few megabytes more. In comparison, the same song might require 30–40 megabytes of storage on a CD. The smaller file size yields much greater Internet transfer speeds.\n\nThe transition into the online space has boosted sales, and profit for some artists. It has also allowed for potentially lower expenses such as lower coordination costs, lower distribution costs, as well as the possibility for redistributed total profits. These lower costs have aided new artists in breaking onto the scene and gaining recognition. In the past, some emerging artists have struggled to find a way to market themselves and compete in the various distribution channels. The Internet may give artists more control over their music in terms of ownership, rights, creative process, pricing, and more. In addition to providing global users with easier access to content, online stores allow users to choose the songs they wish instead of having to purchase an entire album from which there may only be one or two titles that the buyer enjoys.\n\nThe number of downloaded single tracks rose from 160 million in 2004 to 795 million in 2006 which accounted for a revenue boost from US$397 million to US$2 billion.\n\nMany traditional network television shows, movies and other video content is now available online, either from the content owner directly or from third party services. YouTube, Netflix, Hulu, Vudu, Amazon Prime Video, DirecTV, SlingTV and other Internet-based video services allow content owners to let users access their content on computers, smart phones, tablets or by using appliances such as video game consoles, set-top boxes or Smart TVs.\n\nMany film distributors also include a Digital Copy, also called Digital HD, with Blu-ray disc, Ultra HD Blu-ray, 3D Blu-ray or a DVD.\n\nSome companies, such as Bookmasters Distribution, which invested US$4.5 million in upgrading its equipment and operating systems, have had to direct capital toward keeping up with the changes in technology. The phenomenon of books going digital has given users the ability to access their books on handheld digital book readers. One benefit of electronic book readers is that they allow users to access additional content via hypertext links. These electronic book readers also give users portability for their books since a reader can hold multiple books depending on the size of its hard drive. Companies that are able to adapt and make changes to capitalize on the digital media market have seen sales surge. Vice President of Perseus Books Group stated that since shifting to electronic books (e-books), it saw sales rise by 68%. Independent Publishers Group experienced a sales boost of 23% in the first quarter of 2012 alone.\n\nTor Books, a major publisher of science fiction and fantasy books, started to sell e-books DRM-free by July 2012. One year later the publisher stated that they will keep this model as removing DRM was not hurting their digital distribution ebook business. Smaller e-book publishers such as O'Reilly Media, Carina Press and Baen Books had already forgone DRM previously.\n\nOnline distribution is changing the structure of the video game industry. Gabe Newell, creator of the digital distribution service Steam, formulated the advantages over physical retail distribution as such:\n\nSince the 2000s, there has been an increasing number of smaller and niche titles available and commercially successful, like e.g. remakes of classic games. The new possibility of the digital distribution stimulated also the creation of game titles of very small video game producers like Independent game developer and Modders (e.g. Garry's Mod), which were before not commercially feasible.\n\nThe years after 2004 saw the rise of many digital distribution services on the PC, such as Amazon Digital Services, Desura, GameStop, Games for Windows – Live, Impulse, Steam, Origin, Direct2Drive, GOG.com, and GamersGate. The offered properties differ significantly: while most of these digital distributors don't allow reselling of bought games, \"Green Man Gaming\" allows this. Another example is \"gog.com\" which has a strict non-DRM policy while most other services allow various (strict or less strict) forms of DRM.\n\nNext important thing is that the digital distribution is more eco-friendly than physical. Optical disc is made of polycarbonate plastic and aluminum. The creation of 30 CDs requires the use of 300 cubic feet of natural gas, 2 cups of oil and 24 gallons of water. The boxes for CDs are made from polyvinyl chloride (PVC), which increases the risk of cancer.\n\nA general issue is the large number of incompatible data formats in which content is delivered, possibly restricting the devices that may be used, or making data conversion necessary. Streaming services can have several drawbacks: requiring a constant Internet connection to use content; the restriction of some content to never be stored locally; the restriction of content from being transferred to physical media; and the enabling of greater censorship at the discretion of owners of content, infrastructure, and consumer devices.\n"}
{"id": "25487049", "url": "https://en.wikipedia.org/wiki?curid=25487049", "title": "Eating Animals", "text": "Eating Animals\n\nEating Animals is the third book by the American novelist Jonathan Safran Foer, published in 2009. It was written in close collaboration with Farm Forward, a 501(c)(3) nonprofit organization that implements innovative strategies to promote conscientious food choices, reduce farmed animal suffering, and advance sustainable agriculture. A \"New York Times\" best-seller, \"Eating Animals\" provides a dense discussion of what it means to eat animals in an industrialized world. It was adapted and extended into a 2018 documentary film with the same name, directed by Christopher Dillon Quinn and co-narrated by Foer and Natalie Portman.\n\nFoer presents the book as a way for him to decide whether or not his newborn child should eat meat. Foer’s son is representative of the generations that are entering a world of industrialized farming, in which the decision to eat meat has many more implications than taste. More often than not, putting meat on our plates comes with immense ramifications not only for the animals involved, but also for the environment, and ourselves; the animals suffer, the environment is damaged, and our health is put into question. Essentially, Foer concludes that the detriments of factory farms outweigh the benefits of taste, which is why he chooses to raise his son a vegan.\n\nThroughout the book, Foer places significant emphasis on the stories that come with food. To strengthen the emphasis, both the first and the last chapters of the book are entitled “storytelling.” In the book, Foer states that “stories about food are stories about us―our history and our values,” and establishes storytelling as the overriding theme of the whole book. For Foer, storytelling is a way of recognizing and dealing with the complexity of the subject that is eating animals, and how it is connected to identity. The stories in our plates are the stories about our relationship with the world as represented by the people we eat with, the process by which our food reaches the table, what kinds of food find their ways to our table, etc. According to Foer, the way humans cope with and understand complex phenomena is by turning their occurrences into stories about what they mean. In this sense, the suggested profundity within the phenomenon of meat eating gives Foer’s concept of storytelling a religious undertone.\n\nAs the title suggests, the particular phenomenon Foer focuses on is the consumption of meat. He discusses what eating meat has meant in the past, and what it means today. In doing so, he does not, as one might expect, make the claim that eating meat is intrinsically bad. Rather, he claims that eating meat is circumstantially bad; for example, it is bad when it entails the suffering of animals, environmental destruction, and/or a risk for human health. Today, according to the book and a number of its cited sources, eating meat overwhelmingly entails these problems, while in the past, it has not. The conclusion Foer reaches is that eating animals that come from industrial methods―such as factory farming, industrial fishing, and the like―is bad.\n\nFoer notes that most people recognize there is something bad about eating animals, but that people willingly forget this is the case. Part of what is forgotten in this process, Foer argues, is a connection to our own animality. We neglect the parts of us that makes us similar to them―like, for example, the ability to feel or be relieved of pain―and we deny their importance in the constitution of our humanity. As Foer puts it, “what we forget about animals, we begin to forget about ourselves.” What this leads to, Foer argues, is a fairly ambiguous sense of shame―the feeling of shame that arises when memory reminds us of what we have willingly forgotten.\n\nForgetfulness, the book suggests, is reinforced and perpetuated by the lack of transparency in the meat industry. Farms are generally closed to the public, and it is so difficult to get inside of one that Foer illegally sneaks into one to write about the conditions of the typical factory farm. During his operation, he witnesses the dismal conditions in which the animals live, which helps him understand why the industry seeks confidentiality. He describes this experience as a direct contrast to the marketing tactics used by factory farms. In an attempt to shine light on the meaning of such marketing claims, Foer dedicates a whole chapter to definitions of words that connect humans and food. In it, he defines some of the labels and certifications that are assigned to animal products, suggesting that many of them are misleading.\n\nUltimately, \"Eating Animals\" discusses the ethics of food. It suggests that our food choices directly reflect the ethical values we stand for. When people eat meat, Foer claims, they are implying that satisfying their desire for meat is more important than letting animals live well, or even live at all. This can be a conscious or unconscious process, but its implications, for Foer, are always real. When one supports factory farming, one is relinquishing the importance of certain moral behavior to animals, and in turn, to humans as well. For example, if one denies the importance of the suffering of an animal, one denies the importance of the ability to suffer in and of itself, so it follows that one denies the importance of suffering for humans. In a similar chain of logic, Foer connects our treatment of animals to our treatment of humans―we dichotomize between those who matter and those who do not. Consequently, each food choice an individual makes is an ethical one that profoundly impacts both human and non-human animals.\n\nA \"New York Times\" best-seller, \"Eating Animals\" has received mixed reviews from critics. A \"Washington Post\" article describes Foer's book as providing a writing style that has \"always divided his readers into love-him or hate-him camps.\" \n\nSome critics praise both the conclusions Foer reaches and how he reaches them. A \"Los Angeles Times\" article states that \"Eating Animals\" contains \"the kind of wisdom that... deserves a place at the table with our greatest philosophers.\" In a Huffington Post article, Natalie Portman claimed that the book was so powerful that she went from a twenty-year vegetarian to a vegan activist. According to a piece by the \"New Yorker\", the power of the book lies in its ability to discuss why humans can be so loving to their companion animals while simultaneously being completely indifferent to the ones they eat.\n\nOther critics have criticized the book for various reasons. In a \"New York Magazine\" review, one vegetarian critic called the book \"deeply irritating,\" as it \"settles on the safest possible non-conclusion.\"\n\nAward-winning producer Christopher Quinn and actress Natalie Portman have produced a documentary version of \"Eating Animals\". Like the book, the documentary explores the realities of contemporary animal agriculture as it is related to the complexities of food ethics. The documentary hopes to expand the reach of \"Eating Animals\" message so that more people think of the meat they eat in new ways. The film opened in select cities June 15, 2018.\n\n"}
{"id": "9127187", "url": "https://en.wikipedia.org/wiki?curid=9127187", "title": "Egon Bretscher", "text": "Egon Bretscher\n\nEgon Bretscher (1901–1973) was a Swiss-born British chemist and nuclear physicist and Head of the Nuclear Physics Division from 1948 to 1966 at the Atomic Energy Research Establishment, also known as Harwell Laboratory, in Harwell, United Kingdom. He was one of the pioneers in nuclear fission research and one of the first to foresee that plutonium could be used as an energy source. His work on nuclear physics led to his involvement in the British atomic bomb research project Tube Alloys and his membership of the British Mission to the Manhattan Project at Los Alamos, where he worked in Enrico Fermi's Advanced Development Division in the F-3 Super Experimentation group. His contributions up to 1945 are discussed by Margaret Gowing in her \"\"Britain and Atomic Energy\", 1935-1945.\"\n\nBorn near Zurich, Switzerland and educated at the Eidgenössische Technische Hochschule (ETH) there, Bretscher gained a PhD degree in organic chemistry at Edinburgh in 1926. He returned to Zurich as privat docent to Peter Debye, later moving in 1936 to work in Rutherford’s laboratory at the Cavendish in Cambridge as a Rockefeller Scholar. Here he switched to research in nuclear physics, proposing (with Norman Feather) in 1940 that the 239 isotope of element 94 could be produced from the common isotope of uranium-238 by neutron capture and that, like U-235, this should be able to sustain a nuclear chain reaction. A similar conclusion was independently arrived at by Edwin McMillan and Philip Abelson at Berkeley Radiation Laboratory. In addition, he devised theoretical chemical procedures for purifying this unknown element away from the parent uranium; this element was named Plutonium by Nicholas Kemmer.\n\nBretscher used to joke that his main contribution to physics occurred in the summer of 1930, when he was climbing in the Bergel region near Engadin with another student, Felix Bloch, in the Swiss Alps. Bloch slipped over an icy edge but was saved, as he fell, by the rope joining him to Bretscher. The latter's swift action in driving his ice axe into the ice prevented their combined demise. After raising the alarm, Bretscher returned with a guide and spent the night with Bloch discussing physics. It took guides a further three days to bring Bloch down. Bloch later won the Nobel Prize for physics for his discovery of nuclear magnetic resonance.\n\nIn 1944 he became a part of the British Mission to the Manhattan Project in Los Alamos, New Mexico led by James Chadwick, where he made the first measurements on the energy released in fusion processes. During his time in Los Alamos, he took many Kodachrome slides which appear to constitute a unique coloured record of that research site. His pictures, which are now held by the Churchill Archives Centre, include photographs of Enrico Fermi, Edward Teller and the Trinity site in New Mexico after the first atomic bomb was detonated, showing the surface light brown sand turned to a green-blue glass.\n\nIn 1947 he was invited by John Cockcroft to head the Chemistry Division at the newly established Atomic Energy Research Establishment at Harwell, Oxfordshire, England and in 1948 succeeded Otto Frisch as head of the Nuclear Physics Division there. Amongst his colleagues were Bruno Pontecorvo in the Nuclear Physics Division, and Klaus Fuchs who was the head of the Theoretical Physics Division. He was appointed a Commander of the Most Excellent Order (CBE) on retirement from Harwell.\n\nBretscher died in Switzerland in 1973. Of his two daughters and three sons, Mark Bretscher and Anthony Bretscher are cell biologists, whilst Peter Bretscher is an immunologist.\n"}
{"id": "24451977", "url": "https://en.wikipedia.org/wiki?curid=24451977", "title": "Fiberglass reinforced plastic grating", "text": "Fiberglass reinforced plastic grating\n\nFiberglass reinforced plastic grating (also known as FRP grating, glass reinforced plastic grating or fiberglass grating) is a composite material manufactured by combining a matrix of resin and fiberglass. Fiberglass grating does not corrode like steel grating and is therefore used in corrosive environments to reduce maintenance costs. It is used in a variety of applications including walkways and overhead platforms. FRP grating is a structural product that can be weight-bearing between spans.\n\nMolded FRP grating is composed of alternating directional layers of continuous glass fiber for strength, with resin to consolidate the fibers and provide the shape and corrosion-resistance. Due to its bidirectional strength, molded grating can tolerate cutouts in the panel to allow pipe or equipment penetrations without requiring additional support around the opening. Molded grating has very high impact tolerance, as well as the highest chemical resistance of any fiberglass grating. It can be made slip-resistant by adding grit to the surface.\n\nPultruded FRP grating consists of continuous glass strands encased in resin and wrapped in a surfacing veil which protects the fibers and allows resin to saturate the outermost part of the bar as well as penetrate between the glass fibers. This ensures a smooth and corrosion-resistant surface, with a higher glass/resin ratio than molded products. The individual bars are then assembled using cross bars and epoxy to mechanically join the load bars. Pultruded grating has unidirectional strength, with its much higher glass content resulting in a greater span capability than molded products. This product is usually specified in applications requiring larger spans or heavier loading.\n\nMany resin types may be used in pultrusion including polyester, polyurethane, vinylester epoxy, isophthalic polyester, Orthophthalic and phenolics.\n\nFiberglass grating is often used when there are safety concerns due to liquids or oils on the floor and more corrosive environments needing chemical resistance.\n\nMany different applications can benefit from Fiberglass Grating, such as: Walkways, Platforms, Protective Shielding, Machinery Housings, Raised Floors and Stairways\n\nIn addition, Industries that use Molded Fiberglass Grating can include bottling lines, food processing plants, lift stations, commercial aquariums, lube oil facilities, plating shops, beverage canning facilities, chemical plants and pulp and paper plants.\n\nStair tread panels can also be manufactured using FRP technology. Molded stair treads provide corrosion resistance, durability, and ease of fabrication and maintenance.\n\n"}
{"id": "4321490", "url": "https://en.wikipedia.org/wiki?curid=4321490", "title": "Fish trap", "text": "Fish trap\n\nA fish trap is a trap used for fishing. Fish traps can have the form of a fishing weir or a lobster trap. Some fishing nets are also called fish traps, for example fyke nets.\n\nA typical contemporary trap consists of a frame of thick steel wire in the shape of a heart, with chicken wire stretched around it. The mesh wraps around the frame and then tapers into the inside of the trap. When a fish swims inside through this opening, it cannot get out, as the chicken wire opening bends back into its original narrowness. Contemporary eel traps come in many shapes and sizes and are constructed of many materials. In earlier times, traps were constructed of wood and fibre. \n\nTraps are culturally almost universal and seem to have been independently invented many times. There are essentially two types of trap, a permanent or semi-permanent structure placed in a river or tidal area and bottle or pot trap that are usually, but not always baited to attract prey, and are periodically lifted out of the water.\n\nThe Mediterranean Sea, with an area of about of 2.5 million km (970,000 sq mi), is shaped according to the principle of a bottle trap. It is easy for fish from the Atlantic Ocean to swim into the Mediterranean through the narrow neck at Gibraltar, and difficult for them to find their way out. It has been described as \"the largest fish trap in the world\".\n\nThe prehistoric Yaghan people who inhabited the Tierra Del Fuego area constructed stonework in shallow inlets that would effectively confine fish at low tide levels. Some of this extant stonework survives at Bahia Wulaia at the Bahia Wulaia Dome Middens archaeological site.\n\nIn southern Italy, during the 17th century, a new fishing technique began to be used. The trabucco is an old fishing machine typical of the coast of Gargano protected as historical monuments by the homonym National Park. This giant trap, built in structural wood, is spread along the coast of southern Adriatic especially in the province of Foggia, in some areas of the Abruzzese coastlines and also in some parts of the coast of southern Tyrrhenian Sea.\n\nIndigenous Australians were, prior to European colonisation, most populous in Australia's better-watered areas such as the Murray-Darling river system of the south-east. Here, where water levels fluctuate seasonally, indigenous people constructed ingenious stone fish traps. Most have been completely or partially destroyed. The largest and best-known are those on the Barwon River at Brewarrina, New South Wales, which are at least partly preserved. The Brewarrina fish traps caught huge numbers of migratory native fish as the Barwon River rose in flood and then fell. In southern Victoria, indigenous people created an elaborate system of canals, some more than 2 km long. The purpose of these canals was to attract and catch eels, a fish of short coastal rivers (as opposed to rivers of the Murray-Darling system). The eels were caught by a variety of traps including stone walls constructed across canals with a net placed across an opening in the wall. Traps at different levels in the marsh came into operation as the water level rose and fell. Somewhat similar stone-wall traps were constructed by Native American Pit River people in north-eastern California.\n\nA technique called dam fishing is used by the Baka pygmies. This involves the construction of a temporary dam resulting in a drop in the water levels downstream— allowing fish to be easily collected.\n\nAlso used in Chile, mainly in Chiloé, which were unusually abundant (fish weir and basket fish trap).\n\nThe manner in which fish traps are used depends on local conditions and the behaviour of the local fish. For example, a fish trap might be placed in shallow water near rocks where pikes like to lie. If placed correctly, traps can be very effective. It is usually not necessary to check the trap daily, since the fish remain alive inside the trap, relatively unhurt. Because of this, the trap also allows for the release of undersized fish as per fishing regulations.\n\n\n\n"}
{"id": "3653720", "url": "https://en.wikipedia.org/wiki?curid=3653720", "title": "Flammable liquid", "text": "Flammable liquid\n\nGenerally, a flammable liquid is a combustible liquid that can easily catch fire. However, it is not the liquid itself that burns, but the vapor cloud above the liquid that will burn if the vapor's concentration in air is between the lower flammable limit (LFL) and upper flammable limit (UFL) of the liquid.\n\nA number of attempts have been made to standardise the definition of 'flammable' based on the need to classify such fluids as presenting a higher risk of ignition and therefore needing additional precautions. \n\nIn the US, a flammable liquid is defined as one with a flash point below . This definition is part of a categorisation of \"combustible liquids\" used by the National Fire Protection Association, The US Department of Transportation, the US Environmental Protection Agency, the US Occupational Safety and Health Administration and others.\n\nThese categories are further subdivided, depending on the liquid's flash point and boiling point.\n\n\n"}
{"id": "52809488", "url": "https://en.wikipedia.org/wiki?curid=52809488", "title": "FreeCodeCamp", "text": "FreeCodeCamp\n\nfreeCodeCamp (also referred to as “Free Code Camp”) is a non-profit organization that consists of an interactive learning web platform, an online community forum, chat rooms, Medium publications and local organizations that intend to make learning web development accessible to anyone. Beginning with tutorials that introduce students to HTML, CSS and JavaScript, students progress to project assignments that they must complete either alone or in pairs. Upon completion of all project tasks, students are partnered with other nonprofits to build web applications, giving the students practical development experience.\n\nfreeCodeCamp was launched in October 2014 and incorporated as Free Code Camp, Inc. The founder, Quincy Larson, is a software developer who took up programming after graduate school and created freeCodeCamp as a way to streamline a student's progress from beginner to being job-ready.\nIn a 2015 podcast interview, he summarized his motivation for creating freeCodeCamp as follows: “freeCodeCamp is my effort to correct the extremely inefficient and circuitous way I learned to code. I’m committing my career and the rest of my life towards making this process as efficient and painless as possible. […] All those things that made learning to code a nightmare to me are things that we are trying to fix with freeCodeCamp.”\n\nThe original curriculum focused on MongoDB, Express.js, AngularJS, and Node.js and was estimated to take 800 hours to complete. Many of the lessons were links to free material on other platforms, such as Codecademy, Stanford, or Code School. The course was broken up into “Waypoints” (quick, interactive tutorials), “Bonfires” (algorithm challenges), “Ziplines” (front-end projects), and “Basejumps” (full-stack projects). Completing the front-end and full-stack projects awarded the student with respective certificates.\n\nThe curriculum was updated in January 2016 to rely less on outside material, remove the unconventional section names, and switch focus from AngularJS to React.js as the front-end library of choice. There were a number of additions to the coursework, including D3.js and Sass, which brought the total time estimate to 2,080 hours and two more certificates, data visualization and back-end.\n\nAfter returning from a trip to China, Larson was inspired to launch freeCodeCamp by seeing a young boy's interest towards coding. And he decided to give people the opportunity to learn programming. After six years living in San Francisco, Larson now lives in Oklahoma City, Oklahoma.\n\nThe self-paced curriculum involves 1,200 hours of interactive coding challenges and web development projects, plus 800 hours of contributing to open-source projects for nonprofits and is constantly expanded by more challenges and projects. This translates into about one year of full-time coding. The curriculum is divided into front-end development, data visualization, back-end development, and full-stack development. Participants receive a certificate after completing each section.\n\nThe curriculum emphasizes pair programming, intended to foster a culture of collaboration and shared learning, which can overcome a student's doubts about the adequacy of their skills (popularly referred to as “impostor syndrome”).\n\nThe languages and technologies currently taught by freeCodeCamp include HTML5, CSS 3, JavaScript, jQuery, Bootstrap, Sass, React.js, Node.js, Express.js, MongoDB, and Git.\n\nAs students of freeCodeCamp finish all certificates of the curriculum, they get the opportunity to work with nonprofit organizations. Examples have been Indonesia-based nonprofit Kopernik and People Saving Animals. The organization has donated US$1,400,000 worth of development work to nonprofits as of January 2017.\n\nIn 2016, freeCodeCamp announced their \"Open Source for Good\" initiative, which extends and open sources their nonprofit work to all nonprofits and organizations to use. Within ten months of launching, the initiative has created seven open-source tools. Mail for Good is one of the projects, which helps organizations send bulk email messages at a low cost, which serves as a cheaper alternative to services such as MailChimp.\n\nfreeCodeCamp's platform is used by about 350,000 unique visitors per month, with students from over 160 countries. According to Alexa, freeCodeCamp is ranked around 2,850 globally and around 1,650 in the United States in terms of monthly traffic.\n\nfreeCodeCamp has international, community-run groups where students can interact in person. Some groups have been featured in local news, citing freeCodeCamp as an introduction to programming in order to fill the estimated vacancy in programming-related jobs in the next decade.\n"}
{"id": "21034868", "url": "https://en.wikipedia.org/wiki?curid=21034868", "title": "GMX Multi Messenger", "text": "GMX Multi Messenger\n\nGMX Multi Messenger was an instant messenger software and application launched by the German webmail provider GMX Mail in November, 2006. GMX is a subsidiary of United Internet, a company that also owns the webmailer Web.de, which offered the same instant messenger client with their own branding.\n\n\n"}
{"id": "22688727", "url": "https://en.wikipedia.org/wiki?curid=22688727", "title": "Gardon gauge", "text": "Gardon gauge\n\nA Gardon gauge or Circular-foil gauge is a heat flux sensor primarily intended for the measurement of high intensity radiation. It is a sensor that is designed to measure the radiation flux density (in watts per metre squared) from a field of view of 180 degrees. \nThe most common application of Gardon gauges is in exposure testing of sample materials for their resistance to fire and flames.\n\nWhile heat flux sensors can be made according to various designs, the sensor of a Gardon gauge consists of a foil connected to the sensor body at its external radius, and connected to a thin wire at the center, named after its originator Robert Gardon. The foil center and side are the hot- and cold joint of a thermocouple respectively. When radiation hits the sensor this generates a signal. It is typically water-cooled and does not require any power to operate.\nA so-called Schmidt-Boelter Gauge has the same outward appearance as a Gardon Gauge, but employs different sensor technology. The Schmidt-Boelter has a plated constantan wire wrapped around an insulating chip. Both are heat flux sensors. The only difference is practical; Gardon gauges can be manufactured in such a way that they withstand extremely high flux levels. The range for Schmidt-Boelter technology is more limited. On the other hand the Schmidt-Boelter technology can reach higher sensitivities at a lower response time.\nPlease note: Images on this page are of a Schmidt-Boelter gauge. While of similar appearance externally, the internal construction is not that of a Gardon gauge. Construction of both is detailed in the explanation. \n\nA high intensity radiation spectrum extends approximately from 300 to 2,800 nm. Gardon gauges usually cover that spectrum with a spectral sensitivity that is as “flat” as possible.\n\nFor a flux density or irradiance measurement it is required by definition that the response to “beam” radiation varies with the cosine of the angle of incidence; i.e. full response at when the radiation hits the sensor perpendicularly (normal to the surface, 0 degrees angle of incidence), zero response when the radiation is at the horizon (90 degrees angle of incidence, 90 degrees zenith angle), and 0.5 at 60 degrees angle of incidence. It follows from the definition that a Gardon gauge should have a so-called “directional response” or “cosine response” that is close to the ideal cosine characteristic.\n\nIn order to attain the proper directional and spectral characteristics, a Gardon gauge’s main components are:\n\n\nThe black coating on the thermopile sensor absorbs the radiation that is converted to heat. The heat flows through the sensor to the sensor housing and from the housing to the cooling water. The thermopile sensor generates a voltage output signal that is proportional to the heat flux.\n\nGardon Gauges are frequently used in fire testing. Typically installed vertically and next to the sample under testing.\nGardon- or Schmidt Boelter gauges are unprotected heat flux sensors, and that they are highly sensitive to local convection. In general users should make sure that:\n\nGardon Gauges are standardised according to the ASTM standard.\n\nCalibration is typically done relative to NIST .\n\nSpecifications, drawings and pictures courtesy of Hukseflux Thermal Sensors, www.Hukseflux.com\n"}
{"id": "23637261", "url": "https://en.wikipedia.org/wiki?curid=23637261", "title": "Gordon Foster", "text": "Gordon Foster\n\nFrederic Gordon Foster (24 February 1921 – 20 December 2010) was an Irish computational engineer, statistician, professor, and college dean who is widely known for devising, in 1965, a nine-digit code upon which the International Standard Book Number (ISBN) is based.\n\nFoster was born in Belfast, United Kingdom of Great Britain and Ireland, between 1920 enactment and 1921 implementation of the partition of Ireland. He studied at the Royal Belfast Academical Institution and began advanced study in mathematics at Queen's University Belfast. During World War II, he was recruited from Queen's by MI6 to work as a code-breaker at Bletchley Park.\n\nAfter the war he resumed studies at Magdalen College, Oxford. A lecture on feedback control by Norbert Wiener, regarded as the originator of cybernetics, proved to be a great influence on Foster's \nresearch. Upon completing his PhD at Magdalen, he accepted an offer to lecture on his research at the University of Manchester, where he met Alan Turing, a Bletchley Park veteran who became known as the father of computer science. Turing introduced him to the \"Manchester Mark I\" computer and enlisted his help working on it. In 1952 Foster joined the faculty of the London School of Economics (LSE), first as an assistant lecturer in statistics, then lecturer and reader. In 1964, he was appointed to the chair of computational methods. While at LSE he helped develop operations research as an academic discipline.\n\nFoster became professor of statistics at Trinity College Dublin in 1967. There he flourished, promoting statistical analysis and computer applications in several constituent schools and academic departments. He also developed his own department, with a strong postgraduate as well as undergraduate program, and set up the statistics and operations research laboratory, known for outreach to industry and public services. Eventually he was Dean of Engineering and System Sciences.\n\nFoster died in Dublin, 20 December 2010.\n\n"}
{"id": "1015267", "url": "https://en.wikipedia.org/wiki?curid=1015267", "title": "Harvester (forestry)", "text": "Harvester (forestry)\n\nA harvester is a type of heavy forestry vehicle employed in cut-to-length logging operations for felling, delimbing and bucking trees. A forest harvester is typically employed together with a forwarder that hauls the logs to a roadside landing.\n\nForest harvesters were mainly developed in Sweden and Finland and today do practically all of the commercial felling in these countries. The first fully mobile timber \"harvester\", the PIKA model 75, was introduced in 1973 by Finnish systems engineer Sakari Pinomäki and his company PIKA Forest Machines. The first single grip harvester head was introduced in the early 1980s by Swedish company SP Maskiner. Their use has become widespread throughout the rest of Northern Europe, particularly in the harvesting of plantation forests.\n\nBefore modern harvesters were developed in Finland and Sweden, two inventors from Texas developed a crude tracked unit that sheared off trees at the base up to in diameter was developed in the US called The Mammoth Tree Shears. After shearing off the tree, the operator could use his controls to cause the tree to fall either to the right or left. Unlike a harvester, it did not delimb the tree after felling it.\n\nHarvesters are employed effectively in level to moderately steep terrain for clearcutting areas of forest. For very steep hills or for removing individual trees, humans working with chain saws are still preferred in some countries. In northern Europe small and manoeuvrable harvesters are used for thinning operations, manual felling is typically only used in extreme conditions, where tree size exceeds the capacity of the harvester head or by small woodlot owners.\n\nThe principle aimed for in mechanised logging is \"no feet on the forest floor\", and the harvester and forwarder allow this to be achieved. Keeping humans inside the driving cab of the machine provides a safer and more comfortable working environment for industrial scale logging.\n\nHarvesters are built on a robust all terrain vehicle, either wheeled, tracked or on Walking Excavator. The vehicle may be articulated to provide tight turning capability around obstacles. A diesel engine provides power for both the vehicle and the harvesting mechanism through hydraulic drive. An extensible, articulated boom, similar to that on an excavator, reaches out from the vehicle to carry the harvester head. Some harvesters are adaptations of excavators with a new harvester head, while others are purpose-built vehicles.\n\n\"Combi\" machines are available which combine the felling capability of a harvester with the load-carrying capability of a forwarder, allowing a single operator and machine to fell, process and transport trees. These novel type of vehicles are only competitive in operations with short distances to the landing.\n\nA typical harvester head consists of (from bottom to top, with head in vertical position)\n\nOne operator in the vehicle's cab can control all of these functions. A control computer can simplify mechanical movements and can keep records of the length and diameter of trees cut. Length is computed by either counting the rotations of the gripping wheels or, more commonly, using the measuring wheel. Diameter is computed from the pivot angle of the gripping wheels or delimbing knives when hugging the tree. Length measurement also can be used for automated cutting of the tree into predefined lengths. Computer software can predict the volume of each stem based on analysing stems harvested previously. This information when used in conjunction with price lists for each specific log specification enables the optimisation of log recovery from the stem.\n\nHarvesters are routinely available for cutting trees up to in diameter, built on vehicles weighing up to , with a boom reaching up to radius. Larger, heavier vehicles do more damage to the forest floor, but a longer reach helps by allowing harvesting of more trees with fewer vehicle movements.\n\nThe approximate equivalent type of vehicle in full-tree logging systems are feller-bunchers.\n\n\n"}
{"id": "3931027", "url": "https://en.wikipedia.org/wiki?curid=3931027", "title": "Herman Lukoff", "text": "Herman Lukoff\n\nHerman Lukoff (May 2, 1923 – September 24, 1979) was a computer pioneer and fellow of the IEEE.\n\nLukoff was born in Philadelphia, Pennsylvania to Aaron and Anna (Slemovitz) Lukoff. He graduated from the Moore School of Electrical Engineering at the University of Pennsylvania in 1943. While at the Moore School, he helped develop the ENIAC and EDVAC computers. He followed ENIAC co-inventors J. Presper Eckert and John W. Mauchly to their newly formed Electronic Control Company, which became Eckert-Mauchly Computer Corporation, then became part of Remington Rand in 1950 and Sperry Corporation in 1955. Lukoff assisted Eckert and Mauchly with the development of the UNIVAC computer. He stayed with the company until his death.\n\nLukoff died of leukemia on September 24, 1979. At the time of his death, he lived in Fort Washington, Pennsylvania. He was survived by his wife, Shirley Rosner Lukoff; his three sons, Arthur, Barry, and Andrew; and his daughter, Carol.\n\nLukoff's memoir, \"From Dits to Bits\", details his experiences as a first-hand observer of the birth of the computer industry.\n\n"}
{"id": "3543062", "url": "https://en.wikipedia.org/wiki?curid=3543062", "title": "Hydrodesulfurization", "text": "Hydrodesulfurization\n\nHydrodesulfurization (HDS) is a catalytic chemical process widely used to remove sulfur (S) from natural gas and from refined petroleum products, such as gasoline or petrol, jet fuel, kerosene, diesel fuel, and fuel oils. The purpose of removing the sulfur, and creating products such as ultra-low-sulfur diesel, is to reduce the sulfur dioxide () emissions that result from using those fuels in automotive vehicles, aircraft, railroad locomotives, ships, gas or oil burning power plants, residential and industrial furnaces, and other forms of fuel combustion.\n\nAnother important reason for removing sulfur from the naphtha streams within a petroleum refinery is that sulfur, even in extremely low concentrations, poisons the noble metal catalysts (platinum and rhenium) in the catalytic reforming units that are subsequently used to upgrade the octane rating of the naphtha streams.\n\nThe industrial hydrodesulfurization processes include facilities for the capture and removal of the resulting hydrogen sulfide () gas. In petroleum refineries, the hydrogen sulfide gas is then subsequently converted into byproduct elemental sulfur or sulfuric acid (). In fact, the vast majority of the 64,000,000 metric tons of sulfur produced worldwide in 2005 was byproduct sulfur from refineries and other hydrocarbon processing plants.\n\nAn HDS unit in the petroleum refining industry is also often referred to as a hydrotreater.\n\nAlthough some reactions involving catalytic hydrogenation of organic substances were already known, the property of finely divided nickel to catalyze the fixation of hydrogen on hydrocarbon (ethylene, benzene) double bonds was discovered by the French chemist Paul Sabatier in 1897. \nThrough this work, he found that unsaturated hydrocarbons in the vapor phase could be converted into saturated hydrocarbons by using hydrogen and a catalytic metal, laying the foundation of the modern catalytic hydrogenation process.\n\nSoon after Sabatier's work, a German chemist, Wilhelm Normann, found that catalytic hydrogenation could be used to convert unsaturated fatty acids or glycerides in the liquid phase into saturated ones. He was awarded a patent in Germany in 1902 and in Britain in 1903, which was the beginning of what is now a worldwide industry.\n\nIn the mid-1950s, the first noble metal catalytic reforming process (the Platformer process) was commercialized. At the same time, the catalytic hydrodesulfurization of the naphtha feed to such reformers was also commercialized. In the decades that followed, various proprietary catalytic hydrodesulfurization processes, such as the one depicted in the flow diagram below, have been commercialized. Currently, virtually all of the petroleum refineries worldwide have one or more HDS units.\n\nBy 2006, miniature microfluidic HDS units had been implemented for treating JP-8 jet fuel to produce clean feed stock for a fuel cell hydrogen reformer. By 2007, this had been integrated into an operating 5 kW fuel cell generation system.\n\nHydrogenation is a class of chemical reactions in which the net result is the addition of hydrogen (H). Hydrogenolysis is a type of hydrogenation and results in the cleavage of the C-X chemical bond, where C is a carbon atom and X is a sulfur (S), nitrogen (N) or oxygen (O) atom. The net result of a hydrogenolysis reaction is the formation of C-H and H-X chemical bonds. Thus, hydrodesulfurization is a hydrogenolysis reaction. Using ethanethiol (), a sulfur compound present in some petroleum products, as an example, the hydrodesulfurization reaction can be simply expressed as \n\n\\overset{Ethane}{C2H6} + \\overset{Hydrogen\\ sulfide}{H2S}</chem>\nFor the mechanistic aspects of, and the catalysts used in this reaction see the section catalysts and mechanisms.\n\nIn an industrial hydrodesulfurization unit, such as in a refinery, the hydrodesulfurization reaction takes place in a fixed-bed reactor at elevated temperatures ranging from 300 to 400 °C and elevated pressures ranging from 30 to 130 atmospheres of absolute pressure, typically in the presence of a catalyst consisting of an alumina base impregnated with cobalt and molybdenum (usually called a CoMo catalyst). Occasionally, a combination of nickel and molybdenum (called NiMo) is used, in addition to the CoMo catalyst, for specific difficult-to-treat feed stocks, such as those containing a high level of chemically bound nitrogen.\n\nThe image below is a schematic depiction of the equipment and the process flow streams in a typical refinery HDS unit.\nThe liquid feed (at the bottom left in the diagram) is pumped up to the required elevated pressure and is joined by a stream of hydrogen-rich recycle gas. The resulting liquid-gas mixture is preheated by flowing through a heat exchanger. The preheated feed then flows through a fired heater where the feed mixture is totally vaporized and heated to the required elevated temperature before entering the reactor and flowing through a fixed-bed of catalyst where the hydrodesulfurization reaction takes place.\n\nThe hot reaction products are partially cooled by flowing through the heat exchanger where the reactor feed was preheated and then flows through a water-cooled heat exchanger before it flows through the pressure controller (PC) and undergoes a pressure reduction down to about 3 to 5 atmospheres. The resulting mixture of liquid and gas enters the gas separator vessel at about 35 °C and 3 to 5 atmospheres of absolute pressure.\n\nMost of the hydrogen-rich gas from the gas separator vessel is recycle gas, which is routed through an amine contactor for removal of the reaction product that it contains. The -free hydrogen-rich gas is then recycled back for reuse in the reactor section. Any excess gas from the gas separator vessel joins the sour gas from the stripping of the reaction product liquid.\n\nThe liquid from the gas separator vessel is routed through a reboiled stripper distillation tower. The bottoms product from the stripper is the final desulfurized liquid product from hydrodesulfurization unit.\n\nThe overhead sour gas from the stripper contains hydrogen, methane, ethane, hydrogen sulfide, propane, and, perhaps, some butane and heavier components. That sour gas is sent to the refinery's central gas processing plant for removal of the hydrogen sulfide in the refinery's main amine gas treating unit and through a series of distillation towers for recovery of propane, butane and pentane or heavier components. The residual hydrogen, methane, ethane, and some propane is used as refinery fuel gas. The hydrogen sulfide removed and recovered by the amine gas treating unit is subsequently converted to elemental sulfur in a Claus process unit or to sulfuric acid in a wet sulfuric acid process or in the conventional Contact Process.\n\nNote that the above description assumes that the HDS unit feed contains no olefins. If the feed does contain olefins (for example, the feed is a naphtha derived from a refinery fluid catalytic cracker (FCC) unit), then the overhead gas from the HDS stripper may also contain some ethene, propene, butenes and pentenes, or heavier components.\n\nIt should also be noted that the amine solution to and from the recycle gas contactor comes from and is returned to the refinery's main amine gas treating unit.\n\nThe refinery HDS feedstocks (naphtha, kerosene, diesel oil, and heavier oils) contain a wide range of organic sulfur compounds, including thiols, thiophenes, organic sulfides and disulfides, and many others. These organic sulfur compounds are products of the degradation of sulfur containing biological components, present during the natural formation of the fossil fuel, petroleum crude oil.\n\nWhen the HDS process is used to desulfurize a refinery naphtha, it is necessary to remove the total sulfur down to the parts per million range or lower in order to prevent poisoning the noble metal catalysts in the subsequent catalytic reforming of the naphthas.\n\nWhen the process is used for desulfurizing diesel oils, the latest environmental regulations in the United States and Europe, requiring what is referred to as \"ultra-low-sulfur diesel\" (ULSD), in turn requires that very deep hydrodesulfurization is needed. In the very early 2000s, the governmental regulatory limits for highway vehicle diesel was within the range of 300 to 500 ppm by weight of total sulfur. As of 2006, the total sulfur limit for highway diesel is in the range of 15 to 30 ppm by weight.\n\nA family of substrates that are particularly common in petroleum are the aromatic sulfur-containing heterocycles called thiophenes. Many kinds of thiophenes occur in petroleum ranging from thiophene itself to more condensed derivatives called benzothiophenes and dibenzothiophenes. Thiophene itself and its alkyl derivatives are easier to hydrogenolyse, whereas dibenzothiophene, especially its 4,6-disubstituted derivatives, are considered the most challenging substrates. Benzothiophenes are midway between the simple thiophenes and dibenzothiophenes in their susceptibility to HDS.\n\nThe main HDS catalysts are based on molybdenum disulfide () together with smaller amounts of other metals. The nature of the sites of catalytic activity remains an active area of investigation, but it is generally assumed basal planes of the structure are not relevant to catalysis, rather the edges or rims of these sheet. At the edges of the crystallites, the molybdenum centre can stabilize a coordinatively unsaturated site (CUS), also known as an anion vacancy. Substrates, such as thiophene, bind to this site and undergo a series of reactions that result in both C-S scission and C=C hydrogenation. Thus, the hydrogen serves multiple roles—generation of anion vacancy by removal of sulfide, hydrogenation, and hydrogenolysis. A simplified diagram for the cycle is shown:\nMost metals catalyse HDS, but it is those at the middle of the transition metal series that are most active. Ruthenium disulfide appears to be the single most active catalyst, but binary combinations of cobalt and molybdenum are also highly active. Aside from the basic cobalt-modified MoS catalyst, nickel and tungsten are also used, depending on the nature of the feed. For example, Ni-W catalysts are more effective for hydrodenitrogenation.\n\nMetal sulfides are \"supported\" on materials with high surface areas. A typical support for HDS catalyst is γ-alumina. The support allows the more expensive catalyst to be more widely distributed, giving rise to a larger fraction of the that is catalytically active. The interaction between the support and the catalyst is an area of intense interest, since the support is often not fully inert but participates in the catalysis.\n\nThe basic hydrogenolysis reaction has a number of uses other than hydrodesulfurization.\n\nThe hydrogenolysis reaction is also used to reduce the nitrogen content of a petroleum stream in a process referred to as hydrodenitrogenation (HDN). The process flow is the same as that for an HDS unit.\n\nUsing pyridine (), a nitrogen compound present in some petroleum fractionation products, as an example, the hydrodenitrogenation reaction has been postulated as occurring in three steps:\n\nand the overall reaction may be simply expressed as:\n\nMany HDS units for desulfurizing naphthas within petroleum refineries are actually simultaneously denitrogenating to some extent as well.\n\nThe hydrogenolysis reaction may also be used to saturate or convert olefins (alkenes) into paraffins (alkanes). The process used is the same as for an HDS unit.\n\nAs an example, the saturation of the olefin pentene can be simply expressed as:\nSome hydrogenolysis units within a petroleum refinery or a petrochemical plant may be used solely for the saturation of olefins or they may be used for simultaneously desulfurizing as well as denitrogenating and saturating olefins to some extent.\n\nThe food industry uses hydrogenation to completely or partially saturate the unsaturated fatty acids in liquid vegetable fats and oils to convert them into solid or semi-solid fats, such as those in margarine and shortening.\n\n\n"}
{"id": "17545130", "url": "https://en.wikipedia.org/wiki?curid=17545130", "title": "Instruments used in pathology", "text": "Instruments used in pathology\n\nInstruments used specially in pathology are as follows:\n"}
{"id": "13347912", "url": "https://en.wikipedia.org/wiki?curid=13347912", "title": "Intelligent Munitions System", "text": "Intelligent Munitions System\n\nXMX1100 Scorpion: Intelligent Munitions System (IMS) is a smart system being developed by Textron and the U.S. Army TACOM-ARDEC Picatinny Arsenal.\n\nThe IMS allows for controlled and safe use of munitions in the battlefield and prevents unwanted munitions from being buried and forgotten. It is remotely operated and can be left in the field on an automatic mode or turned off to allow friendly vehicles to pass through. Each module contains four anti-vehicle smart munitions that are fired into the air and release a guided warhead that will descend vertically on the target.\n"}
{"id": "29421171", "url": "https://en.wikipedia.org/wiki?curid=29421171", "title": "James May's Man Lab", "text": "James May's Man Lab\n\nJames May's Man Lab is a British television series presented by former Top Gear presenter James May. The first, three-part series was aired on BBC Two between 31 October and 14 November 2010. The second, five-part series was aired between 25 October and 18 December 2011. Repeats of Series 2 continued on late night BBC One with signing for the deaf throughout January 2012.\n\nSeries 3 began broadcasting in March 2013, after James May's other co-hosted TV show (\"Top Gear\") finished Series 19. Series One was released on DVD on 7 November 2011 by Acorn Media UK, followed by Series Two on 8 October 2012.\n\nThe series explores traditional skills that are being lost by the modern man, and shows how to stop them from being lost forever. Each episode has a variety of themed tasks, including construction, seduction, destruction and more. If science, geometry, maths, logic and explosives can be used in these tasks, so much the better. Tasks include sending a dead pet's ashes into space using a homemade hydrogen balloon, creating one's own smelting furnace, constructing a pool table, felling a tree using explosives, escaping from Dartmoor prison whilst avoiding detection from expert trackers and making a magnetic ceiling panel to throw your keys at so you don't lose them.\n\nThe first series also had a celebrity man task, where a celebrity attempted to beat a personal best at a certain 'man task', such as changing a tyre; however, this aspect was not continued into series two. The theme tune was written by May himself and is often played live over the end credits by a variety of different performers, including barber shop quartet, bagpipes and more.\n\nOn 9 July 2012, May announced on his Facebook page that filming had started on the third series. In January 2013, May announced via his Twitter that Man Lab was due to air in March, after the next series of \"Top Gear\" had finished. The series began at 8pm on 28 March 2013.\n\nJames May's Man Lab was broadcast on BBC America in the U.S., where it is periodically rebroadcast. It has also been broadcast in Australia on SBS One. However, SBS stopped showing series three after just two episodes in June 2013. In New Zealand it used to air on TV3 for first 2 seasons. From series 3 it will now air on Prime.\n\nSeries One was released on DVD by Acorn Media UK on 7 November 2011. Series Two was released on 8 October 2012.\n\n"}
{"id": "35280714", "url": "https://en.wikipedia.org/wiki?curid=35280714", "title": "Lahore Composting Facility", "text": "Lahore Composting Facility\n\nThe Danish Carbon Fund's (DCF) Lahore Composting Facility project is the first of its kind in Pakistan. It is bringing composting technology to a country where the common practice is by open dumping of waste, as there are no landfills. This project is the first public-private partnership project in Pakistan on a large scale in the area of Municipal Solid Waste Management (MSW). \nIt will contribute towards the sustainable development of the municipality of Lahore, as well as significantly reduce health hazards for the local communities. It was set up at Mehmood Booti under an agreement with the City District Government of Lahore (CDGL). The project is expected to generate over 310,000 tons of CO2e by 2018.\n\nThe rapid increase of population, high rates of migration towards cities, as well as the introduction of disposable items, such as plastic bags and bottles have created serious environmental problems including: inadequate solid and liquid waste management, lack of safe water and minimal pollution control. Similar to other big cities of Pakistan, Lahore is witnessing a rapid growth in its population due to rural-urban migration from the surrounding areas and other parts of the country. Lahore has a population of around 10 million and is considered to be one of the 30 largest cities in the world. \nThe increase in population has exerted immense pressure on the social and physical infrastructure of the city, leading to various socio-economic and environmental problems. Inadequate solid waste management is one of the most visible and pressing problems in the city, contributing to an unattractive environment, poor sanitation, disease, pollution of water bodies and general environmental degradation. Due to its high population growth and the lack of resources, waste management has become a challenge for the city. This project addresses the need to dispose of solid waste in economically beneficial ways without putting an extra burden on the infrastructure of Lahore city .\n\nThe country has no scientifically designed landfill and the common practice is for the open dumping of waste. LCL has leased the Mehmood Booti dumping site from CDGL as well as receiving a concession from it to process 1,000 tons per day (tpd) of MSW collected from residential areas, as well as from the city's fruit and vegetable markets. LCL has imported Belgian machinery and composting technology. LCL has created local compost management know-how through the practice of learning by doing, which did not exist previously. Composting in a scientifically designed plant will improve the local environment, reducing the health hazards created by the present practices of dumping waste in open sites. It will also sequester the emission of methane gas generated in the process of anaerobic decomposition of the bio-degradable matter. the project will also lengthen the usable life of the dump site in a city where land is running short and its costs are at a premium. The compost that is produced will be used as a soil conditioner/fertilizer for improving the quality of the soil in and around Lahore.\nDeclining land yields and soil erosion are big large problems in Pakistan, especially for an agriculturally based country. This is exacerbated by the fact that Pakistan has been estimated to be one of the 12 hardest hit countries by climate change, the highest impact being on agriculture and floodings. The Lahore Compost project is not only a pilot for better a MSW in the country, but also represents a potential solution for badly degraded agricultural lands. Compost application pilots are already showing positive effects in reducing land salinity, which is a big problem in Pakistan. In addition, land yields and the size of agricultural products produced have been shown to increase dramatically with the first application of compost to degraded soils. In addition to selling Certified Emission Reduction (CER), the project must also create a market for compost in the country.\n\nIn addition to mitigating climate change, LCL has created jobs through this project. It has implemented free vaccinations for its workers, showers where they can wash after work, introduced safety measures like the mandatory use of safety boots and gloves and it is working on creating an education program through its project. Forecast CDM revenues are expected to encourage the development of a market for compost in Pakistan, where it has not yet been operating on a larger scale. Lahore Compost will demonstrate the viability of such an initiative and encourage the private sectors participation in similar MSW projects, which will help contribute towards the sustainable development of other municipalities in Pakistan. \nFurther details can be viewed on this link.\n"}
{"id": "2331971", "url": "https://en.wikipedia.org/wiki?curid=2331971", "title": "Limbo (weapon)", "text": "Limbo (weapon)\n\nLimbo, or Anti Submarine Mortar Mark 10 (A/S Mk.10), was the final British development of a forward-throwing anti-submarine weapon originally designed during the Second World War. Limbo, a three-barreled mortar similar to the earlier Hedgehog and Squid which it superseded, was developed by the Admiralty Underwater Weapons Establishment in the 1950s. Squid was loaded manually, which was difficult on a pitching deck in heavy seas with no protection from the elements; in contrast Limbo was loaded and fired automatically, with all the crew under cover. It was widely fitted on the quarterdeck of Royal Navy escort ships on a mounting stabilised for pitch and roll from 1955 to the mid–1980s. Australian built versions of the destroyer all carried Limbo as did the Australian . Limbo was also widely employed by the Royal Canadian Navy, being incorporated into all destroyer designs from the late 1950s to the early 1970s, including the , , , and classes.\n\nThe firing distance of the mortars was controlled by opening gas vents; rounds could be fired from . The weapon was linked to the sonar system of the ship, firing on command when the target was in range. The rounds were projected so that they fell in a triangular pattern around the target. Limbo could fire in any direction around the ship and is reported to have been very accurate. The weapon was used in the 1982 Falklands War, and remained in service in the Royal Navy and Commonwealth navies until the 1990s. A surviving system is preserved at Explosion! Museum of Naval Firepower in Gosport, Hampshire.\n\nThe firing of the Mortar Mk 10 was controlled by the Type 170 (and later the 502) attack sonar from the Sonar Control Room (SCR), which was generally located next to the operations room in the warship.\n\nThe 170 sonar had 3 operators who maintained sonar contact with the target and effectively aimed the weapon in bearing, range and depth. The operators were controlled by the SC (Sonar Controller) who was in charge of the SCR.\n\nWhen a contact had been confirmed as a hostile submarine, the SC manually fired the Mortar Mk 10 from the SCR upon receiving the order from the captain in the operations room. The firing was done by means of a pistol grip and trigger mounted to the deckhead immediately behind the operators.\n\n\n"}
{"id": "22436359", "url": "https://en.wikipedia.org/wiki?curid=22436359", "title": "List of firsts in aviation", "text": "List of firsts in aviation\n\nThis is a list of firsts in aviation.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "31217397", "url": "https://en.wikipedia.org/wiki?curid=31217397", "title": "Lists of computers", "text": "Lists of computers\n\nList of computers may refer to:\n\n\n\n"}
{"id": "15150250", "url": "https://en.wikipedia.org/wiki?curid=15150250", "title": "Monique Frize", "text": "Monique Frize\n\nMonique Frize, , née Aubry (born 7 January 1942) is a Canadian biomedical engineer and professor, knowledgeable in medical instruments and decision support systems. Notably, her scientific research and outreach efforts led her to receive the prestigious distinction of Officer of the Order of Canada.\n\nBorn in Montreal, Quebec, Frize received a Bachelor of Applied Science (B.A.Sc.) degree in Electrical Engineering from the University of Ottawa in 1966 - the first Canadian woman to graduate from this program at the university. From 1967 to 1969, Frize was an Athlone Fellow as she completed her Master of Philosophy (M.Phil.) degree in Engineering in Medicine from Imperial College of Science and Technology in London. In 1986, she received a Master's in Business Administration (MBA) degree from the Université de Moncton. She received her Ph.D. degree from Erasmus Universiteit in Rotterdam in 1989.\n\nMonique Frize worked as a clinical engineer for 18 years, starting at Hopital Notre-Dame in Montreal, Quebec (1971-1979) before becoming the Director of the Regional Clinical Engineering Service in Moncton, New Brunswick. While in Moncton, she became the first Chair of the Division of Clinical Engineering for the International Federation of Medical and Biological Engineering, a position she continues to hold. \n\nIn 1989, Frize was appointed the first holder of the Nortel-NSERC Women in Engineering Chair at the University of New Brunswick and a professor of Electrical Engineering. In 1997, she was appointed Professor in the Department of Systems and Computer Engineering at Carleton University and Professor in the School of Information Technology and Engineering at the University of Ottawa. She is currently a Distinguished Research Professor and Professor Emeritus. She is also a founding member of the International Network of Women Engineers and Scientists (INWES) and was President from 2002 to 2008 as well as being President of the Education and Research Institute (ERI) from 2007. In 2018, in collaboration with Library and Archives Canada and the University of Ottawa Library - Archives and Special Collections, as a member of INWES-ERI, she led an initiative to develop a centre of expertise to document the history of women who have contributed to science, technology, engineering and mathematics (STEM) in Canada.\n\nFrize's research interests include medical imaging, medical decision support systems, medical technology management issues (clinical engineering) and technical services for hospitals in developing countries.\n\nMonique Frize has received numerous awards and honors throughout her career. In 1992, she was made a Fellow of the Canadian Academy of Engineering. In 1993, she was inducted into the Order of Canada, in recognition of being \"well-known in the field of biomedical engineering\" and for being \"a role model and an inspiration for women seeking careers in science\". She has received several honorary degrees from the University of Ottawa, York University, Lakehead University and from Mount St-Vincent University. She received the Gold Medal in 2010 from Professional Engineers Ontario and the Ontario Society of Professional Engineers and she became Fellow of Engineers Canada in 2010. In 2013, she was awarded the honour of Fellow of the Canadian Medical and Biological Engineering Society.\n\n"}
{"id": "44062145", "url": "https://en.wikipedia.org/wiki?curid=44062145", "title": "Morgan Solar Inc", "text": "Morgan Solar Inc\n\nMorgan Solar, Inc. is a Canadian solar power and optical technology company based in Toronto, Ontario. The company's two main products are the Sun Simba concentrated photovoltaic (CPV) solar panel and the Savanna Dual-Axis Tracker.\n\nThe Sun Simba uses a customized \"Light-guide Solar Optic\" to focus light collected over a large surface area on to a relatively small, highly efficient solar cell for conversion into electricity. Multiple generations of the Sun Simba have been developed and deployed at test sites in Ontario and California. To date, the highest reported active area efficiency of the Sun Simba is 29%, as compared to the experimental world record efficiency of 44.7% in 2013.\n\nThe Savanna PV tracker is designed both to compliment the Sun Simba CPV panel, and to be used as an augmentation to improve the output of standard PV solar panels. Solar panels generate the most power when they are directly facing the sun. By using a tracking system to keep panels properly aligned, output power production can be significantly increased. The Savanna distinguishes itself from many other trackers on the market in that it is much easier to install, requiring no heavy equipment or specialized infrastructure to be assembled ahead of time.\n\nMorgan Solar was founded in 2007 by brothers John Paul Morgan and Nicolas Morgan as an optical technology company with the goal of achieving solar power generation at costs rivalling traditional energy sources. Since its founding, Morgan Solar has grown to employ over 50 scientists, engineers and business administrators in the GTA as well as a number of manufacturing experts, test site managers and business developers outside of Canada.\n\nIn 2010 Asif Ansari, former founder and CEO of eSolar, joined Morgan Solar as Chief Executive Officer. Ansari's corporate portfolio includes leadership positions with eSolar and Suntrough Energy.\n\nIn 2016 Mike Andrade, former President of the new products division of Celestica, took over as Chief Executive Officer. Andrade's corporate portfolio includes leadership positions with Celestica as part of the founding management team as well as building Celestica's PV business.\n\nMorgan Solar has raised over USD $38 million in investments as of the end of 2011. In addition to monetary investments Morgan Solar has formed strategic partnerships with several companies, including Iberdrola, Enbridge, inc. and Enertech.\n\n\n"}
{"id": "12198246", "url": "https://en.wikipedia.org/wiki?curid=12198246", "title": "Nathan Sivin", "text": "Nathan Sivin\n\nNathan Sivin (born 11 May 1931), also known as Xiwen (), is an American author, scholar, sinologist, historian, essayist, and currently professor emeritus at the University of Pennsylvania. He resides in Philadelphia with his wife, the artist Carole Sivin.\n\nThe major areas of study and focus in Nathan Sivin's career and written publications have been in the history of science and technology in China, Traditional Chinese medicine, Chinese philosophy, and Chinese religious beliefs. He speaks four different foreign languages and has traveled abroad to many countries, on four continents, numerous times since the 1960s. In his written works he has also collaborated with many other prominent scholars, such as G.E.R. Lloyd, A.C. Graham and Joseph Needham.\n\nFrom 1954 until 1956, Sivin was enrolled in an 18-month language program for Chinese at the U.S. Army Language School. He then went on to receive his Bachelor of Science degree in humanities with a chemistry minor at the Massachusetts Institute of Technology in 1958. He received his A.M. in the history of science at Harvard University in 1960, and his Ph.D. in the history of science at Harvard University in 1966. He received an honorary M.A. at the University of Pennsylvania.\n\nIn 1966, at MIT, Nathan Sivin served as an assistant professor of humanities, associate professor in 1969, and professor from 1972 until 1977, where he then moved to the University of Pennsylvania as a professor of Chinese culture and history of science.\n\nNathan Sivin has studied abroad on many occasions. From October 1961 to August 1962 he studied Chinese language and philosophy at Taipei in Taiwan. From August 1962 to March 1963 he studied the history of Chinese alchemy in Singapore and provided guest lectures there. From the 1960s until the 1980s he was an avid visitor to Kyoto in Japan, where he acted as a visiting professor, studied at the Research Institute of Humanistic Studies, and studied Chinese astronomy, alchemy, and medicine. From 1974 to 2000 he made numerous trips to Cambridge in order to study Chinese astronomy, visiting Gonville and Caius College, the Needham Research Institute, and St. John's College in the process. From the late 1970s until the late 1990s he traveled several times to the People's Republic of China. In September 1979 he lectured in seminars at the École Pratique des Hautes Etudes of Paris, France, and at the Sinologisches Seminar at the University of Würzburg in Germany in 1981. Nathan Sivin also speaks several foreign languages, including Mandarin, Japanese, German, and French.\n\nAlong with various responsibilities at the University of Pennsylvania, throughout his career Nathan Sivin was also an elective member of numerous societies and committees. This included the American Society for the Study of Religion, the Philomathean Society, the Académie Internationale d'Histoire des Sciences, the T'ang Studies Society, and many others.\n\nAlong with numerous book publications since the 1960s, Nathan Sivin also wrote many more essays from the 1960s onwards. He has also given over 200 lectures throughout Europe, Asia, Australia, and North America. He is currently working on several projects, including a biography on the Song Dynasty polymath scientist Shen Kuo and a translation into English of a Yuan Dynasty calendrical treatise published in 1279 AD, the \"Season-Granting\" (a hallmark of Chinese mathematical astronomy).\n\nIn a statistical overview derived from writings by and about Nathan Sivin, OCLC/WorldCat encompasses roughly 50 works in 80+ publications in 7 languages and 4,000+ library holdings.\n\n\n\n"}
{"id": "41338204", "url": "https://en.wikipedia.org/wiki?curid=41338204", "title": "Nintendo DS family", "text": "Nintendo DS family\n\nThe Nintendo DS family is a family of handheld game consoles that were developed and sold by Nintendo.\n\nInitially released as an experimental platform ancillary to the then-current Game Boy Advance family, Nintendo DS soon replaced it as Nintendo's flagship handheld device family. The DS was distinguished by its predecessor by featuring a folding \"clamshell\" form factor and two screens—the lower screen being a touchscreen enabling input with an included stylus pen. It was also Nintendo's first handheld console to have built-in wireless capabilities, supporting both local communication between other DS consoles, and internet communications via the now-discontinued Nintendo Wi-Fi Connection service. \n\nThe line comprised three models; the original Nintendo DS, the Nintendo DS Lite—a streamlined version of the original model with brighter screens, and the Nintendo DSi—an enhanced iteration of the DS Lite with updated hardware, support for internal and removable storage, cameras, and updated system software with photo and sound recording apps, as well as the ability to download and purchase additional software via the DSIWare store. The DSi was also released in a larger XL model.\n\nThroughout its lifetime, Sony's PlayStation Portable has been the main market competitor. There have been four different models of the Nintendo DS line on the market: the original Nintendo DS, the Nintendo DS Lite, as well as the Nintendo DSi and its XL variant. The Nintendo DS line has been highly successful, continuing the trend of its predecessor, the Game Boy line. With over 154 million units sold worldwide, the DS models are the best-selling handheld consoles, and second best-selling console overall behind the PlayStation 2. \n\nThe DS line was officially superseded by the Nintendo 3DS family in 2011; the 3DS line carries similarities in functionality to the DSi, but with further enhancements to its internal hardware and software, as well as some models featuring an autostereoscopic 3D display.\n\nThe Nintendo DS (abbreviated to DS) is a dual-screen handheld game console developed and released by Nintendo. The device went on sale in North America on November 21, 2004. The DS, short for \"Developers' System\" or \"Dual Screen\", introduced distinctive new features to handheld gaming: an LCD screen working in tandem with a touchscreen, a built-in microphone, and support for wireless connectivity. Both screens are encompassed within a clamshell design similar to the Game Boy Advance SP. The Nintendo DS also features the ability for multiple DS consoles to directly interact with each other over Wi-Fi within a short range without the need to connect to an existing wireless network. Alternatively, they could interact online using the now-discontinued Nintendo Wi-Fi Connection service.\n\nPrior to its release, the Nintendo DS was marketed as a \"third pillar\" in Nintendo's console lineup, meant to complement the Game Boy Advance and GameCube. However, backward compatibility with Game Boy Advance titles and strong sales ultimately established the new handheld console as the successor to the Game Boy series.\n\nThe Nintendo DS Lite (abbreviated to DS Lite) is a dual-screen handheld game console developed and manufactured by Nintendo. It is a slimmer, brighter, and more lightweight redesign of the original Nintendo DS. It was announced on January 26, 2006, more than a month before its initial release in Japan on March 2, 2006 due to overwhelming demand for the original model. It has been released in Australia, North America, Europe, New Zealand, Singapore, and defined regions in South America, the Middle East, and East Asia. As of December 31, 2013, shipments of the DS Lite have reached 93.86 million units worldwide, according to Nintendo. It was the final handheld to have backwards compatibility with Game Boy Advance games.\n\nThe Nintendo DSi (abbreviated to DSi) is a dual-screen handheld game console released by Nintendo. The console launched in Japan on November 1, 2008, and worldwide beginning in April 2009. It is the third iteration of the Nintendo DS, and its primary market rival is Sony's PlayStation Portable (PSP). Development of the DSi began in late 2006, and the handheld was unveiled during an October 2008 Nintendo conference in Tokyo. Consumer demand convinced Nintendo to produce a slimmer handheld with larger screens than the DS Lite. Consequently, Nintendo removed the Game Boy Advance (GBA) cartridge slot to improve portability without sacrificing durability.\n\nWhile the DSi's design is similar to that of the DS Lite, it features two digital cameras, supports internal and external content storage, and connects to an online store called the Nintendo DSi Shop (which was discontinued in 2017) . Nintendo stated that families often share DS and DS Lite consoles. Its new functionality was intended to facilitate personalization, so as to encourage each member of a household to purchase a DSi. The handheld supports exclusive physical media in addition to DS games with DSi-specific features and standard DS titles. The only exception to its backward compatibility are earlier DS games that required the GBA slot. As of September 30, 2014, Nintendo had sold 41.37 million DSi and DSi XL units combined.\n\nReviews of the Nintendo DSi were generally positive; although IGN and \"bit-tech\" decried the console's lack of exclusive software and removal of the GBA cartridge slot, its added functionality caused many journalists to recommend it to those who had not purchased a previous DS model. Numerous critics were disappointed with the limited resolution of DSi's cameras, though others such as Ars Technica and GameSpot agreed they were adequate for the handheld's display. CNET and \"PCWorld\" considered the DSi Shop to be the most important buying incentive for current DS owners.\n\nThe Nintendo DSi XL (abbreviated to DSi XL) features larger screens, and a greater overall size, than the original DSi. It is the fourth DS model, the first to be available as a pure size variation. Iwata said that cost restraints had, until then, limited the screen size and multiplayer aspects of portable game consoles, and that the DSi XL offers \"an improved view angle on the screens\", which makes it the first \"portable system that can be enjoyed with people surrounding the gamer.\" He argued that this introduces a new method of playing portable video games, wherein those \"surrounding the game player can also join in one way or the other to the gameplay.\" While the original DSi was specifically designed for individual use, Iwata suggested that DSi XL buyers give the console a \"steady place on a table in the living room,\" so that it might be shared by multiple household members.\n\nThe DSi XL is the longest, widest and heaviest DS model. The console features two wide-viewing-angle LCD screens with the same resolution as the smaller model. It has improved battery life over the DSi on all brightness settings; for example, batteries last 13–17 hours at the dimmest setting. The handheld is outfitted with identical speakers contained in larger speaker enclosures, enabling them to produce louder sound. The hinges stop the screen at 120° in addition to the original DSi's position of 155° to allow easier table-top viewing. The DSi XL is bundled with two longer styli, one of which is thicker, rounded, and pen-like, and does not fit inside the unit.\n\nNintendo big stylus was made for the Nintendo DSi XL\n\n"}
{"id": "24247569", "url": "https://en.wikipedia.org/wiki?curid=24247569", "title": "Pacemaker failure", "text": "Pacemaker failure\n\nPacemaker failure is the inability of an implanted artificial pacemaker to perform its intended function of regulating the beating of the heart. A pacemaker uses electrical impulses delivered by electrodes in order to contract the heart muscles. Failure of a pacemaker is defined by the requirement of repeat surgical pacemaker-related procedures after the initial implantation. Most implanted pacemakers are dual chambered and have two leads, causing the implantation time to take longer because of this more complicated pacemaker system. These factors can contribute to an increased rate of complications which can lead to pacemaker failure.\n\nApproximately 2.25 million pacemakers were implanted in the United States between 1990 and 2002, and of those pacemakers, about 8,834 were removed from patients because of device malfunction most commonly connected to generator abnormalities. In the 1970s, results of an Oregon study indicated that 10% of implanted pacemakers failed within the first month. Another study found that more than half of pacemaker complications occurred during the first 3 months after implantation. Causes of pacemaker failure include lead related failure, unit malfunction, problems at the insertion site, failures related to exposure to high voltage electricity or high intensity microwaves, and a miscellaneous category (one patient had ventricular tachycardia when using his electric razor and another patient had persistent pacing of the diaphragm muscle). Pacemaker malfunction has the ability to cause serious injury or death, but if detected early enough, patients can continue with their needed therapy once complications are resolved.\n\n\n\n\n\n"}
{"id": "24900", "url": "https://en.wikipedia.org/wiki?curid=24900", "title": "Plastic explosive", "text": "Plastic explosive\n\nPlastic explosive is a soft and hand-moldable solid form of explosive material. Within the field of explosives engineering, plastic explosives are also known as putty explosives.\n\nPlastic explosives are especially suited for explosive demolition. Common plastic explosives include Semtex and C-4. The first discovered plastic explosive was gelignite in 1875, invented by Alfred Nobel.\n\nPlastic explosives are especially suited for explosive demolition of obstacles and fortifications by engineers, combat engineers and criminals as they can be easily formed into the best shapes for cutting structural members and have a high enough velocity of detonation and density for metal cutting work.\n\nAn early use of plastic explosives was in the warhead of the Petard demolition mortar of the British Armoured Vehicle Royal Engineers (AVRE); said mortar was used to destroy concrete fortifications encountered during Operation Overlord (D-Day). The original use of Nobel 808 supplied by the SOE was for sabotage of German installations and railways in Occupied Europe.\n\nThey are generally not used for ordinary blasting as they tend to be significantly more expensive than other materials that perform just as well in this application. A common commercial use of plastic explosives is for shock hardening high manganese percentage steel, a material typically used for train rail components and earth digging implements.\n\nSome terrorist groups have used plastic explosives. In October 2000, al-Qa'ida used C-4 to attack the USS \"Cole\", killing 17 sailors. In 1996, terrorists used C-4 to blow up the Khobar Towers U.S. military housing complex in Saudi Arabia.\n\nReactive armor in tanks uses plastic explosives sandwiched between two plates of steel. Incoming high explosive anti-tank rounds pierce the outer steel plate, then detonate the plastic explosive. This absorbs the energy from the incoming tank round and shields the tank.\n\nThe first plastic explosive was gelignite, invented by Alfred Nobel in 1875. Prior to World War I, the British explosives chemist Oswald Silberrad obtained British and U.S. patents for a series of plastic explosives called \"Nitrols\", composed of nitrated aromatics, collodion, and oxidising inorganic salts. The language of the patents indicate that at this time, Silberrad saw no need to explain to \"those versed in the art\" either what he meant by plasticity nor why it may be advantageous, as he only explains why his plastic explosive is superior to others of that type.\n\nOne of the simplest plastic explosives was Nobel's Explosive No. 808, also known as \"Nobel 808\" (often just called \"Explosive 808\" in the British Armed Forces during the Second World War), developed by the British company Nobel Chemicals Ltd well before World War II. It had the appearance of green plasticine with a distinctive smell of almonds. During World War II it was extensively used by the British Special Operations Executive (SOE) for sabotage missions. It is also the explosive used in HESH anti-tank shells and was an essential factor in the devising of the Gammon grenade. Captured SOE-supplied Nobel 808 was the explosive used in the failed 20 July plot assassination attempt on Adolf Hitler in 1944.\n\nDuring and after World War II a number of new RDX-based explosives were developed, including Compositions C, C2, and eventually C3. Together with RDX these incorporate various plasticisers to decrease sensitivity and make the composition plastic. The origin of the obsolete term \"plastique\" dates back to the Nobel 808 explosive introduced to the U.S. by the British in 1940. The samples of explosive brought to the U.S. by the Tizard Mission had already been packaged by the SOE ready for dropping via parachute container to the French Resistance and were therefore labelled in French, as \"Explosif Plastique\". It is still referred to by this name in France and also by some Americans.\n\nThe British used a plastic explosive during World War II as a demolition charge. The specific explosive, Composition C, was 88.3% RDX and 11.7% non-oily, non-explosive plasticizer. The material was plastic between 0 and 40 degrees C, but was brittle at colder temperatures and gummy at higher temperatures. Composition C was superseded by Composition C2, which used a mixture of 80% RDX and 20% plasticizer. Composition C2 had a wider temperature range at which it remained plastic, from −30 to 52 degrees C. Composition C2 was replaced by Composition C3, which was a mixture of 77% RDX and 23% explosive plasticizer. C3 was effective but proved to be too brittle in cold weather and was replaced with C4. There are three classes of C4, with varying amounts of RDX and polyisobutylene.\n\n\n"}
{"id": "3886695", "url": "https://en.wikipedia.org/wiki?curid=3886695", "title": "Pyrotol", "text": "Pyrotol\n\nPyrotol was an explosive available for a time after World War I. It was reprocessed from military surplus. Usually used in combination with dynamite, it created an incendiary blast. Since it was very inexpensive, it was often used by farmers to remove tree stumps and clear ditches. The substance was known for being used to commit the Bath School bombing in 1927 and distribution of pyrotol for farm use was discontinued in 1928 due to exhaustion of the supply of surplus explosives.\n"}
{"id": "4149041", "url": "https://en.wikipedia.org/wiki?curid=4149041", "title": "Radio repeater", "text": "Radio repeater\n\nA radio repeater is a combination of a radio receiver and a radio transmitter that receives a signal and retransmits it, so that two-way radio signals can cover longer distances. A repeater sited at a high elevation can allow two mobile stations, otherwise out of line-of-sight propagation range of each other, to communicate. Repeaters are found in professional, commercial, and government mobile radio systems and also in amateur radio.\n\nRepeater systems use two different radio frequencies; the mobiles transmit on one frequency, and the repeater station receives those transmission and transmits on a second frequency. Since the repeater must transmit at the same time as the signal is being received, and may even use the same antenna for both transmitting and receiving, frequency-selective filters are required to prevent the receiver from being overloaded by the transmitted signal. Some repeaters use two different frequency bands to provide isolation between input and output or as a convenience.\n\nIn a communications satellite, a transponder serves a similar function, but the transponder does not necessarily demodulate the relayed signals.\n\nA repeater is an automatic radio-relay station, usually located on a mountain top, tall building, or radio tower. It allows communication between two or more bases, mobile or portable stations that are unable to communicate directly with each other due to distance or obstructions between them.\n\nThe repeater receives on one radio frequency (the \"input\" frequency), demodulates the signal, and simultaneously re-transmits the information on its \"output\" frequency. All stations using the repeater transmit on the repeater's input frequency and receive on its output frequency. Since the repeater is usually located at an elevation higher than the other radios using it, their range is greatly extended.\n\nBecause the transmitter and receiver are on at the same time, isolation must exist to keep the repeater's own transmitter from degrading the repeater receiver. If the repeater transmitter and receiver are not isolated well, the repeater's own transmitter \"desensitizes\" the repeater receiver. The problem is similar to being at a rock concert and not being able to hear the weak signal of a conversation over the much stronger signal of the band.\n\nIn general, isolating the receiver from the transmitter is made easier by maximizing, as much as possible, the separation between input and output frequencies.\n\nWhen operating through a repeater, mobile stations must transmit on a different frequency than the repeater output. Although the repeater site must be capable of simultaneous reception and transmission (on two different frequencies), mobile stations can operate in one mode at a time, alternating between receiving and transmitting; so, mobile stations do not need the bulky, and costly filters required at a repeater site. Mobile stations may have an option to select a \"talk around\" mode to transmit and receive on the same frequency; this is sometimes used for local communication within range of the mobile units.\n\nThere is no set rule about spacing of input and output frequencies for all radio repeaters. Any spacing where the designer can get sufficient isolation between receiver and transmitter will work.\n\nIn some countries, under some radio services, there are agreed-on conventions or separations that are required by the system license. In the case of input and output frequencies in the United States, for example:\n\nThese are just a few examples. There are many other separations or spacings between input and output frequencies in operational systems.\n\nSame band repeaters operate with input and output frequencies in the same frequency band. For example, in US two-way radio, 30–50 MHz is one band and 150–174 MHz is another. A repeater with an input of 33.980 MHz and an output of 46.140 MHz is a same band repeater.\n\nIn same band repeaters, a central design problem is keeping the repeater's own transmitter from interfering with the receiver. Reducing the coupling between transmitter and input frequency receiver is called \"isolation\".\n\nIn same-band repeaters, isolation between transmitter and receiver can be created by using a single antenna and a device called a \"duplexer\". The device is a tuned filter connected to the antenna. In this example, consider a type of device called a \"band-pass duplexer\". It allows, or passes, a band, (or a narrow range,) of frequencies.\n\nThere are two legs to the duplexer filter, one is tuned to pass the input frequency, the other is tuned to pass the output frequency. Both legs of the filter are coupled to the antenna. The repeater receiver is connected to the receive leg while the transmitter is connected to the transmit leg. The duplexer prevents degradation of the receiver sensitivity by the transmitter in two ways. First, the receive leg greatly attenuates the transmitter's carrier at the receiver input (typically by 90-100 dB), preventing the carrier from overloading (blocking) the receiver front end. Second, the transmit leg attenuates the transmitter broadband noise on the receiver frequency, also typically by 90-100 dB. By virtue of the transmitter and receiver being on different frequencies, they can operate at the same time on a single antenna.\n\nThere is often not enough tower space to accommodate a separate antenna for each repeater at crowded equipment sites. In same-band repeaters at engineered, shared equipment sites, repeaters can be connected to shared antenna systems. These are common in trunked systems, where up to 29 repeaters for a single trunked system may be located at the same site. (Some architectures such as iDEN sites may have more than 29 repeaters.)\n\nIn a shared system, a receive antenna is usually located at the top of the antenna tower. Putting the receive antenna at the top helps to capture weaker received signals than if the receive antenna were lower of the two. By splitting the received signal from the antenna, many receivers can work satisfactorily from a single antenna. Devices called \"receiver multicouplers\" split the signal from the antenna into many receiver connections. The multicoupler amplifies the signals reaching the antenna, then feeds them to several receivers, attempting to make up for losses in the power dividers (or splitters). These operate similarly to a cable TV splitter but must be built to higher quality standards so they work in environments where strong interfering signals are present.\n\nOn the transmitter side, a second transmit antenna is installed somewhere below the receive antenna. There is an electrical relationship defined by the distance between transmit and receive antennas. A desirable null exists if the transmit antenna is located exactly below the receive antenna beyond a minimum distance. Almost the same isolation as a low-grade duplexer (about −60 decibels) can be accomplished by installing the transmit antenna below, and along the centerline of, the receive antenna. Several transmitters can be connected to the same antenna using filters called \"combiners\". Transmitters usually have directional devices installed along with the filters that block any reflected power in the event the antenna malfunctions. The antenna must have a power rating that will handle the sum of energy of all connected transmitters at the same time.\n\nTransmitter combining systems are lossy. As a rule of thumb, each leg of the combiner has a 50% (3 decibel) power loss. If two transmitters are connected to a single antenna through a combiner, half of their power will reach the combiner output. (This assumes everything is working properly.) If four transmitters are coupled to one antenna, a quarter of each transmitter's power will reach the output of the combining circuit. Part of this loss can be made up with increased antenna gain. Fifty watts of transmitter power to the antenna will make a received signal strength at a distant mobile radio that is almost identical to 100 watts.\n\nIn trunked systems with many channels, a site design may include several transmit antennas to reduce combining network losses. For example, a six-channel trunked system may have two transmit antennas with three transmitters connected to each of the two transmit antennas. Because small variations affect every antenna, each antenna will have a slightly different directional pattern. Each antenna will interact with the tower and other nearby antennas differently. If one were to measure received signal levels, this would cause a variation among channels on a single trunked system. Variations in signal strength among channels on one trunked system can also be caused by:\n\nCross-band repeaters are sometimes a part of government trunked radio systems. If one community is on a trunked system and the neighboring community is on a conventional system, a talk group or agency-fleet-subfleet may be designated to communicate with the other community. In an example where the community is on 153.755 MHz, transmitting on the trunked system talk group would repeat on 153.755 MHz. Signals received by a base station on 153.755 MHz would go over the trunked system on an assigned talk group.\n\nIn conventional government systems, cross band repeaters are sometimes used to connect two agencies who use radio systems on different bands. For example, a fire department in Colorado was on a 46 MHz channel while a police department was on a 154 MHz channel, they built a cross-band repeater to allow communication between the two agencies.\n\nIf one of the systems is simplex, the repeater must have logic preventing transmitter keying in both directions at the same time. Voting comparators with a transmitter keying matrix are sometimes used to connect incompatible base stations.\n\nIn looking at records of old systems, examples of cross-band commercial systems were found in every US radio service where regulations allowed them. In California, specific systems using cross-band repeaters have existed at least since the 1960s. Historic examples of cross-band systems include:\n\n\nIn commercial systems, manufacturers stopped making cross band mobile radio equipment with acceptable specifications for public safety systems in the early 1980s. At the time, some systems were dismantled because new radio equipment was not available. Sporadic E ionospheric ducting can make the 46MHz and below frequencies unworkable in summer.\n\nFor decades, cross-band repeaters have been used as fixed links. The links can be used for remote control of base stations at distant sites or to send audio from a diversity (voting) receiver site back to the diversity combining system (voting comparator). Some legacy links occur in the US 150–170 MHz band. US Federal Communications Commission rule changes did not allow 150 MHz links after the 1970s. Newer links are more often seen on 72–76 MHz (Mid-band), 450–470 MHz interstitial channels, or 900 MHz links. These links, known as \"fixed\" stations in US licensing, typically connect an equipment site with a dispatching office.\n\nModern amateur radios sometimes include cross-band repeat capability native to the radio transceiver.\n\nIn commercial systems, cross-band repeaters are sometimes used in vehicular repeaters. For example, a 150 MHz hand held may communicate to a vehicle-mounted low-power transceiver. The low-power radio repeats transmissions from the portable over the vehicle's high power mobile radio, which has a much longer range. In these systems, the hand-held works so long as it is within range of the low power mobile repeater. The mobile radio is usually on a different band than the hand-held to reduce the chances of the mobile radio transmitter interfering with the transmission from the hand-held to the vehicle.\n\n\nThere is a difficult engineering problem with these systems. If you get two vehicle radios at the same location, some protocol has to be established so that one portable transmitting doesn't activate two or more mobile radio transmitters. Motorola uses a hierarchy system with PAC*RT, each repeater transmits a tone when it is turned on, so the last one on site that turns on is the one that gets used. This is so several of them are not on at once.\n\nVehicular repeaters are complex but can be less expensive than designing a system that covers a large area and works with the weak signal levels of hand-held radios. Some models of radio signals suggest that the transmitters of hand-held radios create received signals at the base station one to two orders of magnitude (10 to 20 decibels or 10 to 100 times) weaker than a mobile radio with a similar transmitter output power.\n\nRadio repeaters are typically placed in locations which maximize their effectiveness for their intended purpose:\n\n\n"}
{"id": "37993695", "url": "https://en.wikipedia.org/wiki?curid=37993695", "title": "Rayman Activity Centre", "text": "Rayman Activity Centre\n\nRayman Activity Centre, also known as Rayman Activity Center, is an educational video game published by Ubi Soft for the PC released in 1999 for Windows. It was originally released in French in 1997 as Rayman Éveil. The game consists of a series of minigames or 'activities' designed to teach young children the basics of numbers and literacy. Most of the graphics and music in \"Rayman Activity Centre\" are lifted from the original game, though many unique animations are featured. Characters from the original, such as Betilla the Fairy and the Magician, play prominent roles, while it includes new characters like \"Joe the Cricket\" and \"The Clown in Love\".\n\n"}
{"id": "38666401", "url": "https://en.wikipedia.org/wiki?curid=38666401", "title": "RekenTest", "text": "RekenTest\n\nRekenTest is educational software to practice, analyze and test mental calculation skills in arithmetic operations such as addition and subtraction, the multiplication tables, decimals, money problems, percentages and fractions.\n\nSession<br>\nIn a session the software offers arithmetic problems one by one. The following types are supported:\nTask<br>\nThe software has many settings to specify the exact problems the user will get and how a session looks like. A task encapsulates all these settings and can be assigned to the session in one of the following ways:\n\nProblem sources<br>\nThe software can create arithmetic problems in one or more of the following ways:\n\nReport<br>\nAfter each session a printable report is created.\n\nNetwork<br>\nA client- and server-version of the software is available to use the software with multiple computers in a network (usually in a school). A purchased license is required to use this feature.\n\nLanguages<br>\nSupported languages are: English, Spanish, Portuguese and Dutch. The software can be translated into another language by adding a new language file.\n\nMozilla Public License v2.0\n\n\nVersion 4.4, released in July 2016<br>\nVersion 4.3, released in May 2015<br>\nVersion 4.2, released in September 2013<br>\nVersion 4.1, released in April 2012<br>\nVersion 4.0.2, released in May 2011<br>\nVersion 4.0.1.1, released in November 2010<br>\nVersion 4.0.1, released in August 2010<br>\nVersion 3.0.5.1, released in March 2010<br>\nVersion 3.0.5, released in February 2010<br>\nVersion 3.0.4, released in September 2008<br>\nVersion 3.0.3, released in March 2007<br>\nVersion 3.0.2, released in September 2006<br>\nVersion 3.0.1, released in July 2006<br>\nVersion 3, released in June 2006.\n\n"}
{"id": "47567153", "url": "https://en.wikipedia.org/wiki?curid=47567153", "title": "Robot research initiative", "text": "Robot research initiative\n\nRobot Research Initiative (RRI) is a research institute dedicated to advanced robotics research. It is an affiliated organization of Chonnam National University in Gwangju, Republic of Korea. Prof. Jong Oh Park moved from the Korea Institute of Science and Technology to Chonnam National University in early 2005 and established RRI in March 2008, where he is still actively in charge. RRI is currently a leading institute in the medical robotics field, especially in the area of biomedical micro/nano robotics. RRI is one of the largest institutions among university robotics laboratories in Korea and competes globally.\n\nThe current research focuses of RRI are biomedical micro/nano robotics, surgery robotics, cable robotics, and so on. The Korean government invests roughly 200 million USD annually in the Korean robotics industry, and almost 90% of this budget is designated for R&D. RRI has been actively involved in the government-funded R&D projects. After an over 10-year investment in personal service robotics, as well as IT–based ubiquitous robotics, the government has been strategically investing in medical robotics for the past 6 years. The biomedical micro/nano robotics field in Korea has been exclusively initiated by RRI, and the reputation and status of RRI is currently stable.\n\nThe global networking of RRI is mostly focused on biomedical micro/nano robotics, covering engineering and scientific approaches.\nProf. Park and his staff have led both government- and industry-funded R&D robotics projects. As the industry partners Samsung Electronics, Hyundai Motors, Daewoo Motors, and DSME.\n\nThe Robot Research Initiative was established at the building of engineering 1A, Chonnam National University in March 2008. As a director of the Robot Research Initiative, Professor Jong Oh Park was nominated a month later.\nIn year 2008, Robot Research Initiative signed MOU for mutual cooperation with KIST Europe, Dario Lab in Scuola Superior Sant’Anna in Italy and Sitti Lab in Carnegie Mellon University. In year 2010, Robot Research Initiative announced ‘Development of Biomedical Microrobot for Intravascular Therapy'’ to the public. Also, ‘Pioneer research center for bacteriobot’ and ‘Space Robot Research Center’ was opened. As of March 2011, Prof. Jong Oh Park signed MOU for mutual cooperation with centern for Micro-Nano Mechatronics, Nagoya University(CMM), Japan and Fondazione Instituto Italiano di Tecnologia(IIT). In year 2012, MOU for mutual cooperation was signed with Fraunhofer-Gesellschaft and Fraunhofer-IPA, and with National Science Foundation-founded Materials Research Science and Engineering Centre of Brandeis University. Also, March 2013, MOU with Yanbian University of Science&Technology(Mechanical Material Automation Engineering) was made. In June, Cooperation agreement with Fraunhofer-Gesellschaft and Fraunhofer-IPA was made as well a year later of its MOU. Recently, Robot Research Initiative signed MOU with Daewoo Shipbuilding & Marine Enhoneeting Co., LTD. in April, 2014.\n\n\nProf. Jong Oh Park, director of RRI, transferred following technologies when he worked for KIST. \n\n\n"}
{"id": "44301737", "url": "https://en.wikipedia.org/wiki?curid=44301737", "title": "Rote Liste", "text": "Rote Liste\n\nThe , full name , is a red list of threatened breeds of domestic animal published annually by the Gesellschaft zur Erhaltung alter und gefährdeter Haustierrassen, the German national association for the conservation of historic and endangered domestic animal breeds.\n\nThe GEH was founded in Witzenhausen, in Hesse, central Germany, in 1981. In 1987 it established the criteria on which the Rote Liste is based. The list is published annually, and attributes one of four categories of conservation risk to domestic breeds of cattle, dogs, goats, horses, pigs, rabbits and sheep, of chickens, ducks, geese and turkeys, and of bees; listing of domestic pigeon breeds is in preparation. Some breeds from outside Germany are listed separately. The four levels of risk are:\n\n\nThe risk level is calculated using a formula that takes into account five criteria: the number of breeding animals or breeding females; the percentage of pure-bred matings; the five-year trend in breed numbers; the number of breeders or herds; and the interval between generations of the animal.\n\nThe GEH also publishes, in conjunction with the , the German national association of poultry breeders, a separate list of the historic poultry breeds and colour varieties that were raised in Germany before 1930. The same levels of conservation risk are assigned as in the main red list.\n\nIn 2014 the breeds listed were:\n\n"}
{"id": "3294483", "url": "https://en.wikipedia.org/wiki?curid=3294483", "title": "Shell in situ conversion process", "text": "Shell in situ conversion process\n\nThe Shell's \"in situ\" conversion process (Shell ICP) is an \"in situ\" shale oil extraction technology to convert kerogen in oil shale to shale oil. It is developed by the Shell Oil Company.\n\nShell's \"in situ\" conversion process has been under development since the early 1980s. In 1997, the first small scale test was conducted on the Mahogany property test site, located west of Denver on Colorado's Western Slope in the Piceance Creek Basin. Since 2000, additional research and development activities have carried on as a part of the Mahogany Research Project. The oil shale heating at Mahogany started early 2004. From this test site, Shell has recovered of shale oil.\n\nThe process heats sections of the vast oil shale field \"in situ\", releasing the shale oil and oil shale gas from the rock so that it can be pumped to the surface and made into fuel. In this process, a freeze wall is first to be constructed to isolate the processing area from surrounding groundwater. To maximize the functionality of the freeze walls, adjacent working zones will be developed in succession. wells, eight feet apart, are drilled and filled with a circulating super-chilled liquid to cool the ground to . Water is then removed from the working zone. Heating and recovery wells are drilled at intervals within the working zone. Electrical heating elements are lowered into the heating wells and used to heat oil shale to between and over a period of approximately four years. Kerogen in oil shale is slowly converted into shale oil and gases, which are then flow to the surface through recovery wells.\n\nA RAND study in 2005 estimated that production of of oil (5.4 million tons/year) would theoretically require a dedicated power generating capacity of 1.2 gigawatts (10 billion kWh/year), assuming deposit richness of per ton, with 100% pyrolysis efficiency, and 100% extraction of pyrolysis products. If this amount of electricity were to be generated by a coal-fired power plant, it would consume five million ton of coal annually (about 2.2 million toe). \n\nIn 2006, Shell estimated that over the project life cycle, for every unit of energy consumed, three to four units would be produced. Such an \"energy returned on energy invested\" would be significantly better than that achieved in the Mahogony trials. For the 1996 trial, Shell applied 440,000 kWh (which would require about 96 toe energy input in a coal-fired plant), to generate of oil (37 toe output).\n\nShell's underground conversion process requires significant development on the surface. The separation between drilled wells is less than five meters and wells must be connected by electrical wiring and by piping to storage and processing facilities. Shell estimates that the footprint of extraction operations would be similar to that for conventional oil and gas drilling. However, the dimensions of Shell's 2005 trial indicate that a much larger footprint is required. Production of 50,000 bbl/day would require that land be developed at a rate on the order of per year.\n\nExtensive water use and the risk of groundwater pollution are the technology's greatest challenges.\n\nIn 2006, Shell received a Bureau of Land Management lease to pursue a large demonstration with a capacity of ; Shell has since dropped those plans and is planning a test based on ICP that would produce a total of minimum , together with nahcolite, over a seven-year period.\n\nIn Israel, IEI, a subsidiary of IDT Corp. is planning a shale pilot based on ICP technology. The project would produce a total of 1,500 barrels. However, IEI has also announced that any subsequent projects would not use ICP technology, but would instead utilize horizontal wells and hot gas heating methods.\n\nIn Jordan, Shell subsidiary JOSCO plans to use ICP technology to achieve commercial production by the \"late 2020s.\" In October, 2011, it was reported that JOSCO had drilled more than 100 test holes over the prior two years, apparently for the sake of testing shale samples.\n\nThe Mahogany Oil Shale Project has been abandoned by Shell in 2013 due to unfavorable project economics \n\n\n"}
{"id": "662175", "url": "https://en.wikipedia.org/wiki?curid=662175", "title": "Silicon-germanium", "text": "Silicon-germanium\n\nSiGe ( or ), or silicon-germanium, is an alloy with any molar ratio of silicon and germanium, i.e. with a molecular formula of the form SiGe. It is commonly used as a semiconductor material in integrated circuits (ICs) for heterojunction bipolar transistors or as a strain-inducing layer for CMOS transistors. IBM introduced the technology into mainstream manufacturing in 1989. This relatively new technology offers opportunities in mixed-signal circuit and analog circuit IC design and manufacture. SiGe is also used as a thermoelectric material for high temperature applications (>700 K).\n\nThe use of silicon-germanium as a semiconductor was championed by Bernie Meyerson. \nSiGe is manufactured on silicon wafers using conventional silicon processing toolsets. SiGe processes achieve costs similar to those of silicon CMOS manufacturing and are lower than those of other heterojunction technologies such as gallium arsenide. Recently, organogermanium precursors (e.g. isobutylgermane, alkylgermanium trichlorides, and dimethylaminogermanium trichloride) have been examined as less hazardous liquid alternatives to germane for MOVPE deposition of Ge-containing films such as high purity Ge, SiGe, and strained silicon.\n\nSiGe foundry services are offered by several semiconductor technology companies. AMD disclosed a joint development with IBM for a SiGe stressed-silicon technology, targeting the 65-nm process. TSMC also sells SiGe manufacturing capacity.\n\nIn July 2015, IBM announced that it had created working samples of transistors using a 7 nm silicon-germanium process, promising a quadrupling in the amount of transistors compared to a contemporary process.\n\nSiGe allows CMOS logic to be integrated with heterojunction bipolar transistors, making it suitable for mixed-signal circuits. Heterojunction bipolar transistors have higher forward gain and lower reverse gain than traditional homojunction bipolar transistors. This translates into better low current and high frequency performance. Being a heterojunction technology with an adjustable band gap, the SiGe offers the opportunity for more flexible band gap tuning than silicon-only technology.\n\nSilicon Germanium-on-insulator (SGOI) is a technology analogous to the Silicon-On-Insulator (SOI) technology currently employed in computer chips. SGOI increases the speed of the transistors inside microchips by straining the crystal lattice under the MOS transistor gate, resulting in improved electron mobility and higher drive currents. SiGe MOSFETs can also provide lower junction leakage due to the lower band gap value of SiGe. However, a major issue with SGOI MOSFETs is the inability to form stable oxides with silicon germanium using standard silicon oxidation processing.\n\nA silicon germanium thermoelectric device, MHW-RTG3, was used in the Voyager 1 and 2 spacecraft.\nSilicon germanium thermoelectric devices were also used in other MHW-RTGs and GPHS-RTGs aboard Cassini, Galileo, Ulysses, and Flight Units F-1 and F-4.\n\n\n\n"}
{"id": "1626816", "url": "https://en.wikipedia.org/wiki?curid=1626816", "title": "Size change in fiction", "text": "Size change in fiction\n\nResizing (including miniaturization, growth, shrinking, and enlargement) is a recurring theme in fiction, in particular in fairy tales, fantasy, and science fiction.\n\n\n\n\n\n\n"}
{"id": "35841670", "url": "https://en.wikipedia.org/wiki?curid=35841670", "title": "Vectored interrupt", "text": "Vectored interrupt\n\nIn computer science, a vectored interrupt is a processing technique in which the interrupting device directs the processor to the appropriate interrupt service routine. This is in contrast to a polled interrupt system, in which a single interrupt service routine must determine the source of the interrupt by checking all potential interrupt sources, a slow and relatively laborious process.\n\nVectored interrupts are achieved by assigning each interrupting device a unique code, typically four to eight bits in length. When a device interrupts, it sends its unique code over the data bus to the processor, telling the processor which interrupt service routine to execute.\n"}
{"id": "9808909", "url": "https://en.wikipedia.org/wiki?curid=9808909", "title": "Vegetable oils as alternative energy", "text": "Vegetable oils as alternative energy\n\nVegetable oils are increasingly used as a substitute for fossil fuels. Vegetable oils are the basis of biodiesel, which can be used like conventional diesel. Some vegetable oil blends are used in unmodified vehicles, but straight vegetable oil needs specially prepared vehicles which have a method of heating the oil to reduce its viscosity and surface tension. Another alternative is vegetable oil refining.\n\nThe availability of biodiesel around the world is increasing, although still tiny compared to conventional fossil fuel sources. There is significant research in algaculture methods to make biofuel from algae.\n\nConcerns have been expressed about growing crops for fuel use rather than food and the environmental impacts of large-scale agriculture and land clearing required to expand the production of vegetable oil for fuel use. These effects/impacts would need to be specifically researched and evaluated, economically and ecologically, and weighed in balance with the proposed benefits of vegetable oil fuel in relation to the use of other fuel sources.\n\nThere is a limited amount of fossil fuel inside the Earth. Since the current world energy resources and consumption is mainly fossil fuels, we are very dependent on them for both transportation and electric power generation. The Hubbert peak theory predicts that oil depletion will result in oil production dropping off in the not too distant future. As time goes on our economy will have to transition to some alternative fuels. Fossil fuels have solved two problems which could be separately solved in the future: the problem of a source of primary energy and of energy storage. Along with straight vegetable oil and biodiesel, some energy technologies that could play an important part in the future include:\n| width=\"30%\" align=\"left\" valign=\"top\" style=\"border:0\"|\n| width=\"30%\" align=\"left\" valign=\"top\" style=\"border:0\"|\n\nPlants use sunlight and photosynthesis to take carbon dioxide (CO) out of the Earth's atmosphere to make vegetable oil. The same CO is then put back after it is burned in an engine. Thus vegetable oil does not increase the CO in the atmosphere, and does not directly contribute to the problem of greenhouse gas. It is really a way of catching and storing solar energy; it is a renewable energy.\n\nHowever, as with other \"renewable\" energy sources, there may be a (relatively small) carbon footprint associated with production or distribution of vegetable oil.\n\nVegetable oil is far less toxic than other fuels such as gasoline, petroleum-based diesel, ethanol, or methanol, and has a much higher flash point (approximately 275-290 °C). The higher flash point reduces the risk of accidental ignition. Some types of vegetable oil are edible.\n\nTechnologies of hydrogen economy, batteries, compressed air energy storage, and flywheel energy storage address the energy storage problem but not the source of primary energy. Other technologies like fission power, fusion power, and solar power address the problem of a source of primary energy but not energy storage. Vegetable oil addresses both the source of primary energy and of energy storage. The cost and weight to store a given amount of energy as vegetable oil is low compared to many of the potential replacements for fossil fuels.\n\nThe list of vegetable oils article discusses which types of vegetable oil are used for fuel and where different types are grown.\n\nVegetable oil is used for transportation in four different ways:\n\nThe transition can start with biodiesel, vegetable oil refining, and vegetable oil blends, since these technologies do not require the capital outlay of converting an engine to run on vegetable oils. Because it costs to convert vegetable oil into biodiesel it is expected that vegetable oil will always be cheaper than biodiesel. After there are production cars that can use straight vegetable oil and a standard type available at gas stations, consumers will probably choose straight vegetable oil to save money. So the transition to vegetable oil can happen gradually.\n\nOther methods, like nuclear power, fusion power, wind power and solar power, may provide cheaper electricity, so vegetable oil may only be used in peaking power plants and small power plants, as diesel is limited to today. There is at least one 5 MW power plant that runs on biodiesel. MAN B&W Diesel, Wärtsilä and other companies produce engines suitable for power generation that can be fueled with pure plant oils.\n\nIn Europe, straight vegetable oil (SVO) costs 150 pence/litre at most supermarkets and somewhat less when bought in bulk direct from the manufacturers whereas diesel costs at least 130 pence per litre (in the UK ) to well over that (depends on the year, 1.4 euro is the current market price in central Europe). In the USA, diesel costs about 0,6 $ per liter and the cheapest SVO costs about the same, with more expensive oils costing more than that (up to $7 per gallon).\n\nThe availability of biodiesel around the World is increasing. It is estimated that by 2010 the market for biodiesel will be 7.5 billion litres (2 billion USgallons) in the U.S and 9.5 billion litres (2.5 billion USgallons) in Europe. Biodiesel currently has 3% of the diesel market in Germany and is the number 1 alternative fuel. The German government has a Biofuels Roadmap in which they expect to reach 10% biofuels by 2010 with the diesel 10% coming from fuel made from vegetable oil.\n\nFrom 2005 to 2007 a number of types of vegetable oil have doubled in price. The rise in vegetable oil prices is largely attributed to biofuel demand.\n\nMuch of the fuel price at the pump is due to fuel tax. If you buy vegetable oil at the grocery store it does not have such high taxes. So at times people have bought vegetable oil at the store for their cars because it was cheaper. They did this in spite of the fact that packaging by the gallon adds to the cost and it was illegal to use in a car since no fuel tax had been paid on it.\n\nSince vegetable oil (even as biodiesel) does not contribute to greenhouse gas, governments may tax it much less than gasoline as they have done with ethanol.\nThis would help them reach Kyoto protocol targets.\n\nThe World production of vegetable oil seed is forecast to be 418 million tonnes in 2008/09. After pressing this will make 131 million tonnes of vegetable oil. Much of this is from Oil Palm, and palm oil production is growing at 5% per year. At about 7.5 lb/USgal (900 g/L) this is about 38 billion USgallons (144 billion L). Currently vegetable oil is mostly used in food and some industrial uses with a small percentage used as fuel. The major fuel usage is by conversion to biodiesel with about in 2009.\n\nIn 2004 the US consumed 530 billion litres (140 billion USgal) of gasoline and 150 billion litres (40 billion USgal) of diesel. In biodiesel it says oil palm produces 5940 litres per hectare (635 USgal/acre) of palm oil each year. To make of vegetable oil each year would require or a square of land on a side.\n\n\"The gradual move from oil has begun. Over the next 15 to 20 years we may see biofuels providing a full 25 percent of the world's energy needs. While the move is good for reducing greenhouse emissions, soaring oil prices have encouraged most countries to 'go green' by switching to greater use of biofuels.\" - Alexander Müller, Assistant Director-General of Sustainable Development at the FAO.\n\nAlgaculture could potentially produce far more oil per unit area. Results from pilot algaculture projects using sterile CO from power plant smokestacks look promising.\n\nGenetic modifications to soybeans are already being used. Genetic modifications and breeding can increase vegetable oil yields. From 1979 to 2005 the soybean yield in bushels per acre more than doubled. A company has developed a variety of camelina sativa that yields 20% more oil than the standard variety.\n\nThere is concern that the current growing demand for vegetable oil is causing deforestation, with old forests being replaced with oil palms. When land is cleared it is often burned, which releases large amounts of the greenhouse gas CO. Vegetable oil production would have to increase substantially to replace gasoline and diesel. With current technology such an increase in production would have a substantial environmental impact.\n\nIn some poor countries the rising price of vegetable oil is causing problems. There are those that say using a food crop for fuel sets up competition between food in poor countries and fuel in rich countries. Some propose that fuel only be made from non-edible vegetable oils like jatropha oil. Others argue that the problem is more fundamental. The law of supply and demand predicts that if fewer farmers are producing food the price of food will rise. It may take some time, as farmers can take some time to change which things they are growing, but increasing demand for biofuels is likely to result in price increases for many kinds of food. Some have pointed out that there are poor farmers and poor countries making more money because of the higher price of vegetable oil.\n\nWith the use of non-edible vegetable oils produced by trees such as Millettia Pinnata (formerly Pongamia Pinnata) or the Moringa oleifera tree, both which grow on borderline or non-arable land, the food versus fuel debate becomes less of an either/or question. Apart from their facility of growing in non-arable and/or marginal land, these trees offer major advantages over peanut, soy-bean, sunflower, etc., in that they have long lives (up to 100 years), very low maintenance (since the intensive husbandry is limited to the first few years of their producing lives) and can provide cash-crops to rural areas, such as rural India. In the case of Millettia Pinnata and a few others, the fact that they are nitrogen-fixing legumes is another very important factor, in that they do not deplete the soil. Among other benefits of these trees is that they have root-systems that penetrate much deeper and do not compete with shallow-rooted plants, like grass (once the trees have attained a certain maturity). This means that the land can be used for multiple purposes, such as grazing for animals. Yet another benefit of using Millettia Pinnata to produce bio-diesel is that it can tolerate low rainfall (as little as 250 ml per year), far below what most food-crops require, thus reducing yet more their potential to compete. \n\nSome species of algae contain as much as 50% vegetable oil. Algae have very high growth rates compared to plants normally used to produce vegetable oil. Potentially algae could produce much more oil per area of land than current farming methods. So producing vegetable oil this way should result in less deforestation and less competition for food production land. One expert wrote: \"As demonstrated here, microalgal biodiesel is technically feasible. It is the only renewable biodiesel that can potentially completely displace liquid fuels derived from petroleum. Economics of producing microalgal biodiesel need to improve substantially to make it competitive with petrodiesel, but the level of improvement necessary appears to be attainable.\n\nWhere there is existing electricity generation using fossil fuels, there is a source of sterile CO. This makes algaculture much easier. To grow algae you need lots of CO, but if you get it from air you will also get all kinds of other organisms, some of which eat algae. Getting CO from a smokestack works out really well. Governments trying to address the external costs of coal power plants may have a carbon tax or carbon credit that provides additional motivation to use CO from smokestacks. Several commercial pilot plants are under construction.\n\nThere is substantial research and development work in this area but as of 2007 there is no commercial vegetable oil produced from algae and used as biofuel. ExxonMobil is investing $600 million and estimates they are 5 to 10 years from significant production, but could invest billions in final development and commercialization. If and when the commercialization challenges are overcome, vegetable oil production could expand very rapidly.\n\nIn 2012 President Obama supported the idea of getting oil from algae.\n\n\n"}
{"id": "9961109", "url": "https://en.wikipedia.org/wiki?curid=9961109", "title": "White torture", "text": "White torture\n\nWhite torture is a type of psychological torture that includes extreme sensory deprivation and isolation. Carrying out this type of torture makes the detainee lose personal identity through long periods of isolation.\n\nIn Iran, white torture () has been practiced on political prisoners. Most political prisoners who experience this type of torture are journalists held in the Evin prison. According to Hadi Ghaemi, carrying out such tortures in Evin are not necessarily authorized directly by the Iranian government.\n\nIt can include prolonged periods of solitary confinement, the use of continual illumination to deprive sleep (listed in Geneva Convention on Basic Human Rights, 1949) often in detention centers outside the control of the prison authorities, including Section 209 of Evin Prison.\n\nAhmed Shaheed, the UN special human rights rapporteur in Iran, mentioned in a statement that human rights activist Vahid Asghari was psychologically tortured by means of long-term detention in solitary confinement, and with threats to arrest, torture or rape his family members. He was also reportedly tortured with severe beatings for the purpose of eliciting confessions. \nAn Amnesty International report in 2004 documented evidence of \"white torture\" on Amir Abbas Fakhravar, by the revolutionary guards. According to the report, which called his case the first known example of white torture in Iran claimed that \"his cells had no windows, and the walls and his clothes were white. His meals consisted of white rice on white plates. To use the toilet, he had to put a white piece of paper under the door. He was forbidden to speak, and the guards reportedly wore shoes that muffled sound\". Upon his arrival in the US, Fakhravar confirmed this report in an interview with Christian Broadcasting Network.\n\nIn a telephone call to the Human Rights Watch in 2004, Iranian journalist Ebrahim Nabavi made the following claim regarding White torture:\n\n\"Since I left Evin, I have not been able to sleep without sleeping pills. It is terrible. The loneliness never leaves you, long after you are “free.” Every door that is closed on you ... This is why we call it “white torture.” They get what they want without having to hit you. They know enough about you to control the information that you get: they can make you believe that the president has resigned, that they have your wife, that someone you trust has told them lies about you. You begin to break. And once you break, they have control. And then you begin to confess.\"\n\nKianush Sanjari, an Iranian blogger and activist who had allegedly experienced this type of torture in 2006 claimed that:\n\n\"I feel that solitary confinement—which wages war on the soul and mind of a person—can be the most inhuman form of white torture for people like me, who are arrested solely for [defending] citizens' rights. I only hope the day comes when no one is put in solitary confinement [to punish them] for the peaceful expression of his ideas.\"\n\nJohn McGuffin's book \"The Guinea Pigs\" details the use of sensory deprivation used in Northern Ireland by the British Army until the UK conviction in 1971 of torture before the European Court in the Hague, later downgraded to \"severe maltreatment\".\nThis consisted of hooding and dressing in thick boiler suits and being made to stand against a wall on tip-toe and being subject to \"white noise\". This technique was developed largely in order to avoid accusations of torture (by not inflicting physical pain, but an absence of stimulus) while still providing an interrogation tool. The antecedents of this had been experiments carried out in Canada on volunteers, ostensibly in support of a manned space programme. These had to be discontinued due to the severity of the psychiatric symptoms induced.\nThe UK Government brought together experience of previous torture carried out in various colonial wars - Fort Morbut in Yemen, Hola Camp in Kenya and in Cyprus - in a conference held at Ashford Joint Intelligence Centre in Kent.\nPhotographs taken during the Abu-Ghraib scandal indicate similar techniques being employed by the US Army.\n\nThe United States has been accused by Amnesty International and other international human rights organizations of using \"extreme isolation and sensory deprivation ... detainees confined to windowless cells ... days without seeing daylight\" along with other torture techniques with the approval of the George W. Bush administration. The organization of European Democratic Lawyers (EDL) has explicitly accused the United States of white torture: \"Fundamental rights are violated on the part of the United States. In Guantánamo prisoners are held under sensory deprivation, ears and eyes covered, hands and feet tied, hands in thick gloves, held in cages without any privacy, always observed, light day and night: This is called white torture.\"\n\nAccording to human rights organizations and other NGOs, the Bolivarian Intelligence Service (SEBIN) of the Venezuelan government holds political prisoners in the lower levels of SEBIN's headquarters, which has been deemed by government officials \"The Tomb\". The cells are two by three meters that have a cement bed, white walls, security cameras, no windows and barred doors, with each cell aligned next to one another so there are no interactions between prisoners. Such conditions have caused prisoners to become very ill, though they are denied medical treatment. Bright lights in the cells are kept on so prisoners lose their sense of time and the temperature is below freezing, with the only sounds heard being from the nearby Metro Caracas cars. Allegations of torture in La Tumba, specifically white torture, are also common, with some prisoners attempting to commit suicide. Such conditions according to NGO Justice and Process are to force prisoners to plead guilty to crimes they are accused of.\n\n"}
{"id": "307882", "url": "https://en.wikipedia.org/wiki?curid=307882", "title": "Wireline (cabling)", "text": "Wireline (cabling)\n\nIn the oil and gas industry, the term wireline usually refers to a cabling technology used by operators of oil and gas wells to lower equipment or measurement devices into the well for the purposes of well intervention, reservoir evaluation, and pipe recovery.\n\nTools inserted into the well for both workover and logging efforts, wirelines and slicklines are very similar devices. While a slickline is a thin cable introduced into a well to deliver and retrieve tools downhole, a wireline is an electrical cable used to lower tools into and transmit data about the conditions of the wellbore called wireline logs. Usually consisting of braided cables, wirelines are used to perform wireline logging, as well.\n\nBraided line can contain an inner core of insulated wires which provide power to equipment located at the end of the cable, normally referred to as electric line, and provides a pathway for electrical telemetry for communication between the surface and equipment at the end of the cable.\n\nUsed to place and recover wellbore equipment, such as plugs, gauges and valves, slicklines are single-strand non-electric cables lowered into oil and gas wells from the surface. Slicklines can also be used to adjust valves and sleeves located downhole, as well as repair tubing within the wellbore.\n\nWrapped around a drum on the back of a truck, the slickline is raised and lowered in the well by reeling in and out the wire hydraulically.\n\nBraided line can contain an inner core of insulated wires which provide power to equipment located at the end of the cable, normally referred to as electric line, and provides a pathway for electrical telemetry for communication between the surface and equipment at the end of the cable.\nOn the other hand, wirelines are electric cables that transmit data about the well. Consisting of single strands or multi-strands, the wireline is used for both well intervention and formation evaluation operations. In other words, wirelines are useful in gathering data about the well in logging activities, as well as in workover jobs that require data transmittal.\n\nFirst developed by Conrad and Marcel Schlumberger in 1927, wireline logs measure formation properties in a well through electrical lines of wire. Different from measurement while drilling (MWD) and mud logs, wireline logs are constant downhole measurements sent through the electrical wireline used to help geologists, drillers and engineers make real-time decisions about drilling operations. Wireline logs can measure resistivity, conductivity and formation pressure, as well as sonic properties and wellbore dimensions.\n\nThe logging tool, also called a sonde, is located at the bottom of the wireline. The measurements are taken by lowering the wireline to the prescribed depth and then raising it out of the well. The measurements are taken continuously on the way up, in an effort to sustain tension on the line.\n\nWhen producing wells require remedial work to sustain, restore or enhance production, this is called workover. Many times, workover operations require production shut-in, but not always.\n\nIn workover operations, a well-servicing unit is used to winch items in and out of the wellbore. The line used to raise and lower equipment can be braided steel wireline or a single steel slickline. Workover operations conducted can include well clean-up, setting plugs, production logging and perforation through explosives.\n\nWireline tools are specially designed instruments lowered into a well bore on the end of the wireline cable. They are individually designed to provide any number of particular services, such as evaluation of the rock properties, the location of casing collars, formation pressures, information regarding the pore size or fluid identification and sample recovery. Modern wireline tools can be extremely complicated, and are often engineered to withstand very harsh conditions such as those found in many modern oil, gas, and geothermal wells. Pressures in gas wells can exceed 30,000 psi, while temperatures can exceed 500 deg Fahrenheit in some geothermal wells. Corrosive or carcinogenic gases such as hydrogen sulfide can also occur downhole.\n\nTo reduce the amount of time running in the well, several wireline tools are often joined together and run simultaneously in a tool string that can be hundreds of feet long and weigh more than 5000 lbs.\n\nNatural gamma ray tools are designed to measure gamma radiation in the Earth caused by the disintegration of naturally occurring potassium, uranium, and thorium. Unlike nuclear tools, these natural gamma ray tools emit no radiation. The tools have a radiation sensor, which is usually a scintillation crystal that emits a light pulse proportional to the strength of the gamma ray striking it. This light pulse is then converted to a current pulse by means of a photomultiplier tube (PMT). From the photomultiplier tube, the current pulse goes to the tool's electronics for further processing and ultimately to the surface system for recording. The strength of the received gamma rays is dependent on the source emitting gamma rays, the density of the formation, and the distance between the source and the tool detector. The log recorded by this tool is used to identify lithology, estimate shale content, and depth correlation of future logs.\n\nNuclear tools measure formation properties through the interaction of reservoir molecules with radiation emitted from the logging tool. The two most common properties measured by nuclear tools are formation porosity and rock density: \n\nFormation porosity is determined by installing a radiation source capable of emitting fast neutrons into the downhole environment. Any pore spaces in the rock are filled with fluid containing hydrogen atoms, which slow the neutrons down to an epithermal or thermal state. This atomic interaction creates gamma rays which are then measured in the tool through dedicated detectors, and interpreted through a calibration to a porosity. A higher number of gamma rays collected at the tool sensor would indicate a larger number of interactions with hydrogen atoms, and thus a larger porosity.\n\nMost open hole nuclear tools utilize double-encapsulated chemical sources.\n\nDensity tools use gamma ray radiation to determine the lithology and density of the rock in the downhole environment. Modern density tools utilize a Cs-137 radioactive source to generate gamma rays which interact with the rock strata. Since higher density materials absorb gamma rays much better than lower density materials, a gamma ray detector in the wire line tool is able to accurately determine formation density by measuring the number and associated energy level of returning gamma rays that have interacted with the rock matrix. Density tools usually incorporate an extendable caliper arm, which is used both to press the radioactive source and detectors against the side of the bore and also to measure the exact width of the bore in order to remove the effect of varying bore diameter on the readings.\n\nSome modern nuclear tools use an electronically powered source controlled from the surface to generate neutrons. By emitting neutrons of varying energies, the logging engineer is able to determine formation lithology in fractional percentages.\n\nIn any matrix which has some porosity, the pore spaces will be filled with a fluid of oil, gas (either hydrocarbon or otherwise) or formation water (sometimes referred to as connate water). This fluid will saturate the rock and change its electrical properties. A wireline resistivity tool direct injects current (lateralog-type tools for conductive water based muds) or induces (induction-type tools for resistive or oil based muds) an electric current into the surrounding rock and determines the resistivity via Ohm's law. The resistivity of the formation is used primarily to identify pay zones containing highly resistive hydrocarbons as opposed to those containing water, which is generally more conductive. It is also useful for determining the location of the oil-water contact in a reservoir. Most wireline tools are able to measure the resistivity at several depths of investigation into the bore hole wall, allowing log analysts to accurately predict the level of fluid invasion from the drilling mud, and thus determine a qualitative measurement of permeability.\n\nSome resistivity tools have many electrodes mounted on several articulated pads, allowing for multiple micro-resistivity measurements. These micro-resistivities have a very shallow depth of investigation, typically in the range of 0.1 to 0.8 inches, making them suitable for borehole imaging. Resistivity imagers are available which operate using induction methods for resistive mud systems (oil base), and direct current methods for conductive mud systems (water based).\n\nSonic tools, such as the Baker Hughes XMAC-F1, consist of multiple piezoelectric transducers and receivers mounted on the tool body at fixed distances. The transmitters generate a pattern of sound waves at varying operating frequencies into the down hole formation. The signal path leaves the transmitter, passes through the mud column, travels along the borehole wall and is collected at multiple receivers spaced out along the tool body. The time it takes for the sound wave to travel through the rock is dependent on a number of properties of the existing rock, including formation porosity, lithology, permeability and rock strength. Different types of pressure waves can be generated in specific axis, allowing geoscientists to determine anisotropic stress regimes. This is very important in determining hole stability and aids drilling engineers in planning for future well design.\n\nSonic tools are also used extensively to evaluate the cement bond between casing and formation in a completed well, primarily by calculating the accentuation of the signal after it as passed through the casing wall (see Cement Bond Tools below).\n\nUltrasonic tools use a rotating acoustic transducer to map a 360 degree image of the borehole as the logging tool is pulled to surface. This is especially useful for determining small scale bedding and formation dip, as well as identifying drilling artifacts such as spiraling or induced fractures.\n\nA measurement of the nuclear magnetic resonance (NMR) properties of hydrogen in the formation. There are two phases to the measurement: polarization and acquisition. First, the hydrogen atoms are aligned in the direction of a static magnetic field (B0). This polarization takes a characteristic time T1. Second, the hydrogen atoms are tipped by a short burst from an oscillating magnetic field that is designed so that they precess in resonance in a plane perpendicular to B0. The frequency of oscillation is the Larmor frequency. The precession of the hydrogen atoms induces a signal in the antenna. The decay of this signal with time is caused by transverse relaxation and is measured by the CPMG pulse sequence. The decay is the sum of different decay times, called T2. The T2 distribution is the basic output of a NMR measurement.\n\nThe NMR measurement made by both a laboratory instrument and a logging tool follow the same principles very closely. An important feature of the NMR measurement is the time needed to acquire it. In the laboratory, time presents no difficulty. In a log, there is a trade-off between the time needed for polarization and acquisition, logging speed and frequency of sampling. The longer the polarization and acquisition, the more complete the measurement. However, the longer times require either lower logging speed or less frequent sampling.\n\nA cement bond tool, or CBT, is an acoustic tool used to measure the quality of the cement behind the casing. Using a CBT, the bond between the casing and cement as well as the bond between cement and formation can be determined. Using CBT data, a company can troubleshoot problems with the cement sheath if necessary. This tool must be centralized in the well to function properly.\n\nTwo of the largest problems found in cement by CBT's are channeling and micro-annulus. A micro annulus is the formation of microscopic cracks in the cement sheath. Channeling is where large, contiguous voids in the cement sheath form, typically caused by poor centralization of the casing. Both of these situations can, if necessary, be fixed by remedial electric line work.\n\nA CBT makes its measurements by rapidly pulsing out compressional waves across the well bore and into the pipe, cement, and formation. The compressional pulse originates in a transmitter at the top of the tool, which, when powered up on surface sounds like a rapid clicking sound. The tool typically has two receivers, one three feet away from the receiver, and another at five feet from the transmitter. These receivers record the arrival time of the compressional waves. The information from these receivers are logged as traveltimes for the three and five foot receivers and as a micro-seismogram.\n\nRecent advances in logging technologies have allowed the receivers to measure 360 degrees of cement integrity and can be represented on a log as a radial cement map and as 6-8 individual sector arrival times.\n\nCasing collar locator tools, or CCL's, are among the simplest and most essential in cased hole electric line. CCL's are typically used for depth correlation and can be an indicator of line overspeed when logging in heavy fluids. \n\nA CCL operates on Faraday's Law of Induction. Two magnets are separated by a coil of copper wire. As the CCL passes by a casing joint, or collar, the difference in metal thickness across the two magnets induces a current spike in the coil. This current spike is sent uphole and logged as what's called a collar kick on the cased hole log.\n\nA cased hole gamma perforator is used to perform mechanical services, such as shooting perforations, setting downhole tubing/casing elements, dumping remedial cement, tracer surveys, etc. Typically, a gamma perforator will have some sort of explosively initiated device attached to it, such as a perforating gun, a setting tool, or a dump bailor. In certain instances, the gamma perforator is used to merely spot objects in the well, as in tubing conveyed perforating operations and tracer surveys.\n\nGamma perforators operate in much the same way as an open hole natural gamma ray tool. Gamma rays given off from naturally occurring radioactive elements bombard the scintillation detector mounted on the tool. The tool processes the gamma ray counts and sends the data uphole where it processed by a computerized acquisition system, and plotted on a log versus depth. The information is then used to ensure that the depth shown on the log is correct. After that, power can be applied through the tool to set off explosive charges for things like perforating, setting plugs or packers, dumping cement, etc.\n\nSetting tools are used to set downhole completion elements such as production packers or bridge plugs. Setting tools typically use the expanding gas energy from a slow burning explosive charge to drive a hydraulic piston assembly. The assembly is attached to the plug or packer by means of a setting mandrel and a sliding sleeve, which when \"stroked\" by the piston assembly, effectively squeezes the elastomer elements of the packing element, deforming it sufficiently to wedge it into place in the tubing or casing string. Most completion packers or plugs have a specially designed shear mechanism which release the setting tool from the element allowing it to be retrieved back to surface. The packer/plug however, remains down hole as a barrier to isolate production zones or permanently plug off a well bore.\n\nThe cable head is the upper most portion of the toolstring on any given type of wireline. The cable head is where the conductor wire is made into an electrical connection that can be connected to the rest of the toolstring. Cable heads are typically custom built by the wireline operator for every job and depend greatly on depth, pressure and the type of wellbore fluid.\n\nElectric line weakpoints are also located in the cable head. If the tool is to become stuck in the well, the weak point is where the tool would first separate from the wireline. If the wireline were severed anywhere else along the line, the tool becomes much more difficult to fish.\n\nTractors are electrical tools used to push the toolstring into hole, overcoming wireline's disadvantage of being gravity dependent. These are used for in highly deviated and horizontal wells where gravity is insufficient, even with roller stem. They push against the side of the wellbore either through the use of wheels or through a wormlike motion.\n\nA measuring head is the first piece of equipment the wireline comes into contact with off the drum. The measuring head is composed of several wheels which support the wireline on its way to the winch and they also measure crucial wireline data.\n\nA measuring head records tension, depth, and speed. Current models use optical encoders to derive the revolutions of a wheel with a known circumference, which in turn is used to figure speed and depth. A wheel with a pressure sensor is used to figure tension.\n\nFor oilfield work, the wireline resides on the surface, wound around a large (3 to 10 feet in diameter) spool. Operators may use a portable spool (on the back of a special truck) or a permanent part of the drilling rig. A motor and drive train turn the spool and raise and lower the equipment into and out of the well – the winch.\n\nThe pressure control employed during wireline operations is intended to contain pressure originating from the well bore. During open hole electric line operations, the pressure might be the result from a well kicking. During cased hole electric line, this is most likely the result of a well producing at high pressures. Pressure equipment must be rated to well over the expected well pressures. Normal ratings for wireline pressure equipment is 5,000, 10,000, and 15,000 pounds per square inch. Some wells are contained with 20,000 psi and 30,000 psi equipment is in development also.\n\nA flange attaches to the top of the Christmas tree, usually with some sort of adapter for the rest of the pressure control. A metal gasket is placed between the top of the Christmas tree and the flange to keep in well pressures.\n\nA wireline control valve, also called a wireline blow out preventer(BOP), is an enclosed device with one or more rams capable of closing over the wireline in an emergency. A dual wireline valve has two sets of rams and some have the capability of pumping grease in the space between the rams to counterbalance the well pressure.\n\nLubricator is the term used for sections of pressure tested pipe that act to seal in wireline tools during pressurization.\nAs stated it is a series of pipes that connect and it is what holds the tool string so operators can make runs in and out of the well. It has valves to bleed off pressure so that you can disconnect it from the well and work on tools, etc.\n\nPump-in subs (also known as a flow T) allow for the injection of fluid into the pressure control string. Normally these are used for wellsite pressure testing, which is typically performed between every run into the well. They can also be used to bleed off pressure from the string after a run in the well, or to pump in kill fluids to control a wild well.\nThe grease injector head is the main apparatus for controlling well pressure while running into the hole. The grease head uses a series of very small pipes, called flow tubes, to decrease the pressure head of the well. Grease is injected at high pressure into the bottom portion of the grease head to counteract the remaining well pressure.\n\nPack-off subs utilize hydraulic pressure on a two brass fittings which compress a rubber sealing element to create a seal around the wireline. Pack-offs can be hand pumped or compressed through a motorized pumping unit.\nA line wiper operates in much the same way as a pack-off sub, except that the rubber element is much softer. Hydraulic pumps exert force on the rubber element until a light pressure is exerted on the wireline, cleaning grease and well fluid off the line in the process.\n\nA quick test sub (QTS) is used when pressure testing the pressure control equipment (PCE) for repetitive operations. The PCE is pressure tested and then broke at the QTS afterwards to avoid having to retest the entire string. The PCE is then reconnected at the QTS. The QTS has two O-rings where it was disconnected that can be tested with hydraulic pressure to confirm the PCE can still hold the pressure it was tested to.\n\nIf the wireline were to become severed from the tool, a ball check valve can seal the well off from the surface. During wireline operations, a steel ball sits to the side of a confined area within the grease head while the cable runs in and out of the hole. If the wireline exits that confined area under pressure, the pressure will force the steel ball up towards the hole where the wireline had been. The ball's diameter is larger than that of the hole, so the ball effectively seals off pressure to the surface.\n\nA head catcher is a device placed at the top of the lubricator section. Should the wireline tools be forced into the top of the lubricator section, the head catcher, which looks like a small 'claw,' will clamp down on the fishing neck of the tool. This action prevents the tools from falling downhole should the line pull out of the rope socket. Pressure is bled off of the head catcher to release the tools.\n\nA tool trap has the same purpose as a head catcher in that it prevents the tools from inadvertently dropping down the hole.\n\n"}
