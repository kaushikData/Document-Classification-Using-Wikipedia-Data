{"id": "37371846", "url": "https://en.wikipedia.org/wiki?curid=37371846", "title": "2U (company)", "text": "2U (company)\n\n2U, Inc. (formerly 2tor Inc.) is an educational technology company that partners with leading nonprofit colleges and universities to offer online degree programs. The company supplies its partner universities with a cloud-based software-as-a-service platform coupled with a suite of technology-enabled services, including coursework design, infrastructural support and capital to deliver instruction to students. 2U was founded in 2008 by John Katzman (founder of The Princeton Review). Co-founders include Chip Paucek (former CEO of Hooked on Phonics) and Jeremy Johnson; Paucek remains with the company as CEO.\n\nIn May 2012, Forbes named 2U as one of the \"10 Start-Ups Changing the World.\" In 2014, Inc. Magazine named 2U one of \"10 Tech Companies Helping Humanity.\"\n\nThe company went public on March 28, 2014, and raised $119 million by offering 9.2 million shares at $13 per share. 2U is traded on NASDAQ under the ticker symbol TWOU.\n\nPrior to its IPO, the company was venture-backed and raised $96 million. Investors included Bessemer Venture Partners, Highland Capital Partners, Redpoint Ventures, Novak Biddle Venture Partners, City Light Capital, SVB Capital and WestRiver Capital. In September 2017, It was reported that, Rani Hammond joined 2U as Chief People Officer.\n\nAs per September 2018, the net worth of the company raised to $4.7 Billion.\n\n2U does not confer or offer any degrees. It provides its university partners with cloud-based software-as-a-service solutions to create online degree programs that feature live video classes, as well as an e learning platform that allows for close interaction between students and professors. The approach also includes a focus on student outcomes with small class sizes, career services, and field placements. This approach has led to a student retention rate of 84 percent. Students pay standard tuition, but instead of going to class once a week, they meet in a live video chat room with the professor and the other students.\n2U signs long-term contracts, averaging between 10–15 years in length, with each of its partner universities. Contracts include a revenue sharing agreement between 2U and the school.\nCourse content is also enhanced for mobile devices.\n\n2U's \"Semester Online\" consortium of undergraduate universities included: Boston College, Brandeis University, Emory University, Northwestern, The University of Melbourne, Trinity College of Dublin, University of North Carolina at Chapel Hill, University of Notre Dame, Wake Forest University, and Washington University in St. Louis. The consortium was initially marketed as a platform for top-tier universities to offer online courses to paying students at participating universities. The company announced in April 2014 that Semester Online would end in the summer of 2014.\n\n"}
{"id": "3621053", "url": "https://en.wikipedia.org/wiki?curid=3621053", "title": "Altera Hardware Description Language", "text": "Altera Hardware Description Language\n\nAltera Hardware Description Language (AHDL) is a proprietary hardware description language (HDL) developed by Altera Corporation. AHDL is used for digital logic design entry for Altera's complex programmable logic devices (CPLDs) and field-programmable gate arrays (FPGAs). It is supported by Altera's MAX-PLUS and Quartus series of design software. AHDL has an Ada-like syntax and its feature set is comparable to the synthesizable portions of the Verilog and VHDL hardware description languages. In contrast to HDLs such as Verilog and VHDL, AHDL is a design-entry language only; all of its language constructs are synthesizable. By default, Altera software expects AHDL source files to have a .tdf extension (Text Design Files).\n\n"}
{"id": "1545608", "url": "https://en.wikipedia.org/wiki?curid=1545608", "title": "Anaerobic digestion", "text": "Anaerobic digestion\n\nAnaerobic digestion is a collection of processes by which microorganisms break down biodegradable material in the absence of oxygen. The process is used for industrial or domestic purposes to manage waste or to produce fuels. Much of the fermentation used industrially to produce food and drink products, as well as home fermentation, uses anaerobic digestion.\n\nAnaerobic digestion occurs naturally in some soils and in lake and oceanic basin sediments, where it is usually referred to as \"anaerobic activity\". This is the source of marsh gas methane as discovered by Alessandro Volta in 1776.\n\nThe digestion process begins with bacterial hydrolysis of the input materials. Insoluble organic polymers, such as carbohydrates, are broken down to soluble derivatives that become available for other bacteria. Acidogenic bacteria then convert the sugars and amino acids into carbon dioxide, hydrogen, ammonia, and organic acids. These bacteria convert these resulting organic acids into acetic acid, along with additional ammonia, hydrogen, and carbon dioxide. Finally, methanogens convert these products to methane and carbon dioxide. The methanogenic archaea populations play an indispensable role in anaerobic wastewater treatments.\n\nAnaerobic digestion is used as part of the process to treat biodegradable waste and sewage sludge. As part of an integrated waste management system, anaerobic digestion reduces the emission of landfill gas into the atmosphere. Anaerobic digesters can also be fed with purpose-grown energy crops, such as maize.\n\nAnaerobic digestion is widely used as a source of renewable energy. The process produces a biogas, consisting of methane, carbon dioxide, and traces of other ‘contaminant’ gases. This biogas can be used directly as fuel, in combined heat and power gas engines or upgraded to natural gas-quality biomethane. The nutrient-rich digestate also produced can be used as fertilizer.\n\nWith the re-use of waste as a resource and new technological approaches that have lowered capital costs, anaerobic digestion has in recent years received increased attention among governments in a number of countries, among these the United Kingdom (2011), Germany and Denmark (2011).\n\nMany microorganisms affect anaerobic digestion, including acetic acid-forming bacteria (acetogens) and methane-forming archaea (methanogens). These organisms promote a number of chemical processes in converting the biomass to biogas.\n\nGaseous oxygen is excluded from the reactions by physical containment. Anaerobes utilize electron acceptors from sources other than oxygen gas. These acceptors can be the organic material itself or may be supplied by inorganic oxides from within the input material. When the oxygen source in an anaerobic system is derived from the organic material itself, the 'intermediate' end products are primarily alcohols, aldehydes, and organic acids, plus carbon dioxide. In the presence of specialised methanogens, the intermediates are converted to the 'final' end products of methane, carbon dioxide, and trace levels of hydrogen sulfide. In an anaerobic system, the majority of the chemical energy contained within the starting material is released by methanogenic bacteria as methane.\n\nPopulations of anaerobic microorganisms typically take a significant period of time to establish themselves to be fully effective. Therefore, common practice is to introduce anaerobic microorganisms from materials with existing populations, a process known as \"seeding\" the digesters, typically accomplished with the addition of sewage sludge or cattle slurry.\n\nThe four key stages of anaerobic digestion involve hydrolysis, acidogenesis, acetogenesis and methanogenesis.\nThe overall process can be described by the chemical reaction, where organic material such as glucose is biochemically digested into carbon dioxide (CO) and methane (CH) by the anaerobic microorganisms.\nIn most cases, biomass is made up of large organic polymers. For the bacteria in anaerobic digesters to access the energy potential of the material, these chains must first be broken down into their smaller constituent parts. These constituent parts, or monomers, such as sugars, are readily available to other bacteria. The process of breaking these chains and dissolving the smaller molecules into solution is called hydrolysis. Therefore, hydrolysis of these high-molecular-weight polymeric components is the necessary first step in anaerobic digestion. Through hydrolysis the complex organic molecules are broken down into simple sugars, amino acids, and fatty acids.\n\nAcetate and hydrogen produced in the first stages can be used directly by methanogens. Other molecules, such as volatile fatty acids (VFAs) with a chain length greater than that of acetate must first be catabolised into compounds that can be directly used by methanogens.\n\nThe biological process of acidogenesis results in further breakdown of the remaining components by acidogenic (fermentative) bacteria. Here, VFAs are created, along with ammonia, carbon dioxide, and hydrogen sulfide, as well as other byproducts. The process of acidogenesis is similar to the way milk sours.\n\nThe third stage of anaerobic digestion is acetogenesis. Here, simple molecules created through the acidogenesis phase are further digested by acetogens to produce largely acetic acid, as well as carbon dioxide and hydrogen.\n\nThe terminal stage of anaerobic digestion is the biological process of methanogenesis. Here, methanogens use the intermediate products of the preceding stages and convert them into methane, carbon dioxide, and water. These components make up the majority of the biogas emitted from the system. Methanogenesis is sensitive to both high and low pHs and occurs between pH 6.5 and pH 8. The remaining, indigestible material the microbes cannot use and any dead bacterial remains constitute the digestate.\n\nAnaerobic digesters can be designed and engineered to operate using a number of different configurations and can be categorized into batch vs. continuous process mode, mesophilic vs. thermophilic temperature conditions, high vs. low portion of solids, and single stage vs. multistage processes. More initial build money and a larger volume of the batch digester is needed to handle the same amount of waste as a continuous process digester. Higher heat energy is demanded in a thermophilic system compared to a mesophilic system and has a larger gas output capacity and higher methane gas content. For solids content, low will handle up to 15% solid content. Above this level is considered high solids content and can also be known as dry digestion. In a single stage process, one reactor houses the four anaerobic digestion steps. A multistage process utilizes two or more reactors for digestion to separate the methanogenesis and hydrolysis phases.\n\nAnaerobic digestion can be performed as a batch process or a continuous process. In a batch system, biomass is added to the reactor at the start of the process. The reactor is then sealed for the duration of the process. In its simplest form batch processing needs inoculation with already processed material to start the anaerobic digestion. In a typical scenario, biogas production will be formed with a normal distribution pattern over time. Operators can use this fact to determine when they believe the process of digestion of the organic matter has completed. There can be severe odour issues if a batch reactor is opened and emptied before the process is well completed. A more advanced type of batch approach has limited the odour issues by integrating anaerobic digestion with in-vessel composting. In this approach inoculation takes place through the use of recirculated degasified percolate. After anaerobic digestion has completed, the biomass is kept in the reactor which is then used for in-vessel composting before it is opened As the batch digestion is simple and requires less equipment and lower levels of design work, it is typically a cheaper form of digestion. Using more than one batch reactor at a plant can ensure constant production of biogas.\n\nIn continuous digestion processes, organic matter is constantly added (continuous complete mixed) or added in stages to the reactor (continuous plug flow; first in – first out). Here, the end products are constantly or periodically removed, resulting in constant production of biogas. A single or multiple digesters in sequence may be used. Examples of this form of anaerobic digestion include continuous stirred-tank reactors, upflow anaerobic sludge blankets, expanded granular sludge beds, and internal circulation reactors.\n\nThe two conventional operational temperature levels for anaerobic digesters determine the species of methanogens in the digesters:\n\nA limit case has been reached in Bolivia, with anaerobic digestion in temperature working conditions of less than 10 °C. The anaerobic process is very slow, taking more than three times the normal mesophilic time process. In experimental work at University of Alaska Fairbanks, a 1,000 litre digester using psychrophiles harvested from \"mud from a frozen lake in Alaska\" has produced 200–300 litres of methane per day, about 20 to 30% of the output from digesters in warmer climates. Mesophilic species outnumber thermophiles, and they are also more tolerant to changes in environmental conditions than thermophiles. Mesophilic systems are, therefore, considered to be more stable than thermophilic digestion systems. In contrast, while thermophilic digestion systems are considered less stable, their energy input is higher, with more biogas being removed from the organic matter in an equal amount of time. The increased temperatures facilitate faster reaction rates, and thus faster gas yields. Operation at higher temperatures facilitates greater pathogen reduction of the digestate. In countries where legislation, such as the Animal By-Products Regulations in the European Union, requires digestate to meet certain levels of pathogen reduction there may be a benefit to using thermophilic temperatures instead of mesophilic.\n\nAdditional pre-treatment can be used to reduce the necessary retention time to produce biogas. For example, certain processes shred the substrates to increase the surface area or use a thermal pretreatment stage (such as pasteurisation) to significantly enhance the biogas output. The pasteurisation process can also be used to reduce the pathogenic concentration in the digesate leaving the anaerobic digester. Pasteurisation may be achieved by heat treatment combined with maceration of the solids.\n\nIn a typical scenario, three different operational parameters are associated with the solids content of the feedstock to the digesters:\n\nHigh solids (dry) digesters are designed to process materials with a solids content between 25 and 40%. Unlike wet digesters that process pumpable slurries, high solids (dry – stackable substrate) digesters are designed to process solid substrates without the addition of water. The primary styles of dry digesters are continuous vertical plug flow and batch tunnel horizontal digesters. Continuous vertical plug flow digesters are upright, cylindrical tanks where feedstock is continuously fed into the top of the digester, and flows downward by gravity during digestion. In batch tunnel digesters, the feedstock is deposited in tunnel-like chambers with a gas-tight door. Neither approach has mixing inside the digester. The amount of pretreatment, such as contaminant removal, depends both upon the nature of the waste streams being processed and the desired quality of the digestate. Size reduction (grinding) is beneficial in continuous vertical systems, as it accelerates digestion, while batch systems avoid grinding and instead require structure (e.g. yard waste) to reduce compaction of the stacked pile. Continuous vertical dry digesters have a smaller footprint due to the shorter effective retention time and vertical design. Wet digesters can be designed to operate in either a high-solids content, with a total suspended solids (TSS) concentration greater than ~20%, or a low-solids concentration less than ~15%.\n\nHigh solids (wet) digesters process a thick slurry that requires more energy input to move and process the feedstock. The thickness of the material may also lead to associated problems with abrasion. High solids digesters will typically have a lower land requirement due to the lower volumes associated with the moisture. High solids digesters also require correction of conventional performance calculations (e.g. gas production, retention time, kinetics, etc.) originally based on very dilute sewage digestion concepts, since larger fractions of the feedstock mass are potentially convertible to biogas.\n\nLow solids (wet) digesters can transport material through the system using standard pumps that require significantly lower energy input. Low solids digesters require a larger amount of land than high solids due to the increased volumes associated with the increased liquid-to-feedstock ratio of the digesters. There are benefits associated with operation in a liquid environment, as it enables more thorough circulation of materials and contact between the bacteria and their food. This enables the bacteria to more readily access the substances on which they are feeding, and increases the rate of gas production.\n\nDigestion systems can be configured with different levels of complexity. In a single-stage digestion system (one-stage), all of the biological reactions occur within a single, sealed reactor or holding tank. Using a single stage reduces construction costs, but results in less control of the reactions occurring within the system. Acidogenic bacteria, through the production of acids, reduce the pH of the tank. Methanogenic bacteria, as outlined earlier, operate in a strictly defined pH range. Therefore, the biological reactions of the different species in a single-stage reactor can be in direct competition with each other. Another one-stage reaction system is an anaerobic lagoon. These lagoons are pond-like, earthen basins used for the treatment and long-term storage of manures. Here the anaerobic reactions are contained within the natural anaerobic sludge contained in the pool.\n\nIn a two-stage digestion system (multistage), different digestion vessels are optimised to bring maximum control over the bacterial communities living within the digesters. Acidogenic bacteria produce organic acids and more quickly grow and reproduce than methanogenic bacteria. Methanogenic bacteria require stable pH and temperature to optimise their performance.\n\nUnder typical circumstances, hydrolysis, acetogenesis, and acidogenesis occur within the first reaction vessel. The organic material is then heated to the required operational temperature (either mesophilic or thermophilic) prior to being pumped into a methanogenic reactor. The initial hydrolysis or acidogenesis tanks prior to the methanogenic reactor can provide a buffer to the rate at which feedstock is added. Some European countries require a degree of elevated heat treatment to kill harmful bacteria in the input waste. In this instance, there may be a pasteurisation or sterilisation stage prior to digestion or between the two digestion tanks. Notably, it is not possible to completely isolate the different reaction phases, and often some biogas is produced in the hydrolysis or acidogenesis tanks.\n\nThe residence time in a digester varies with the amount and type of feed material, and with the configuration of the digestion system. In a typical two-stage mesophilic digestion, residence time varies between 15 and 40 days, while for a single-stage thermophilic digestion, residence times is normally faster and takes around 14 days. The plug-flow nature of some of these systems will mean the full degradation of the material may not have been realised in this timescale. In this event, digestate exiting the system will be darker in colour and will typically have more odour.\n\nIn the case of an upflow anaerobic sludge blanket digestion (UASB), hydraulic residence times can be as short as 1 hour to 1 day, and solid retention times can be up to 90 days. In this manner, a UASB system is able to separate solids and hydraulic retention times with the use of a sludge blanket. Continuous digesters have mechanical or hydraulic devices, depending on the level of solids in the material, to mix the contents, enabling the bacteria and the food to be in contact. They also allow excess material to be continuously extracted to maintain a reasonably constant volume within the digestion tanks.\n\nThe anaerobic digestion process can be inhibited by several compounds, affecting one or more of the bacterial groups responsible for the different organic matter degradation steps. The degree of the inhibition depends, among other factors, on the concentration of the inhibitor in the digester. Potential inhibitors are ammonia, sulfide, light metal ions (Na, K, Mg, Ca, Al), heavy metals, some organics (chlorophenols, halogenated aliphatics, N-substituted aromatics, long chain fatty acids), etc.\n\nThe most important initial issue when considering the application of anaerobic digestion systems is the feedstock to the process. Almost any organic material can be processed with anaerobic digestion; however, if biogas production is the aim, the level of putrescibility is the key factor in its successful application. The more putrescible (digestible) the material, the higher the gas yields possible from the system.\n\nFeedstocks can include biodegradable waste materials, such as waste paper, grass clippings, leftover food, sewage, and animal waste. Woody wastes are the exception, because they are largely unaffected by digestion, as most anaerobes are unable to degrade lignin. Xylophalgeous anaerobes (lignin consumers) or using high temperature pretreatment, such as pyrolysis, can be used to break lignin down. Anaerobic digesters can also be fed with specially grown energy crops, such as silage, for dedicated biogas production. In Germany and continental Europe, these facilities are referred to as \"biogas\" plants. A codigestion or cofermentation plant is typically an agricultural anaerobic digester that accepts two or more input materials for simultaneous digestion.\n\nThe length of time required for anaerobic digestion depends on the chemical complexity of the material. Material rich in easily digestible sugars breaks down quickly, whereas intact lignocellulosic material rich in cellulose and hemicellulose polymers can take much longer to break down. Anaerobic microorganisms are generally unable to break down lignin, the recalcitrant aromatic component of biomass.\n\nAnaerobic digesters were originally designed for operation using sewage sludge and manures. Sewage and manure are not, however, the material with the most potential for anaerobic digestion, as the biodegradable material has already had much of the energy content taken out by the animals that produced it. Therefore, many digesters operate with codigestion of two or more types of feedstock. For example, in a farm-based digester that uses dairy manure as the primary feedstock, the gas production may be significantly increased by adding a second feedstock, e.g., grass and corn (typical on-farm feedstock), or various organic byproducts, such as slaughterhouse waste, fats, oils and grease from restaurants, organic household waste, etc. (typical off-site feedstock).\n\nDigesters processing dedicated energy crops can achieve high levels of degradation and biogas production. Slurry-only systems are generally cheaper, but generate far less energy than those using crops, such as maize and grass silage; by using a modest amount of crop material (30%), an anaerobic digestion plant can increase energy output tenfold for only three times the capital cost, relative to a slurry-only system.\n\nA second consideration related to the feedstock is moisture content. Drier, stackable substrates, such as food and yard waste, are suitable for digestion in tunnel-like chambers. Tunnel-style systems typically have near-zero wastewater discharge, as well, so this style of system has advantages where the discharge of digester liquids are a liability. The wetter the material, the more suitable it will be to handling with standard pumps instead of energy-intensive concrete pumps and physical means of movement. Also, the wetter the material, the more volume and area it takes up relative to the levels of gas produced. The moisture content of the target feedstock will also affect what type of system is applied to its treatment. To use a high-solids anaerobic digester for dilute feedstocks, bulking agents, such as compost, should be applied to increase the solids content of the input material. Another key consideration is the carbon:nitrogen ratio of the input material. This ratio is the balance of food a microbe requires to grow; the optimal C:N ratio is 20–30:1. Excess N can lead to ammonia inhibition of digestion.\n\nThe level of contamination of the feedstock material is a key consideration. If the feedstock to the digesters has significant levels of physical contaminants, such as plastic, glass, or metals, then processing to remove the contaminants will be required for the material to be used. If it is not removed, then the digesters can be blocked and will not function efficiently. It is with this understanding that mechanical biological treatment plants are designed. The higher the level of pretreatment a feedstock requires, the more processing machinery will be required, and, hence, the project will have higher capital costs.\n\nAfter sorting or screening to remove any physical contaminants from the feedstock, the material is often shredded, minced, and mechanically or hydraulically pulped to increase the surface area available to microbes in the digesters and, hence, increase the speed of digestion. The maceration of solids can be achieved by using a chopper pump to transfer the feedstock material into the airtight digester, where anaerobic treatment takes place.\n\nSubstrate composition is a major factor in determining the methane yield and methane production rates from the digestion of biomass. Techniques to determine the compositional characteristics of the feedstock are available, while parameters such as solids, elemental, and organic analyses are important for digester design and operation. Methane yield can be estimated from the elemental composition of substrate along with an estimate of its degradability (the fraction of the substrate that is converted to biogas in a reactor). In order to predict biogas composition (the relative fractions of methane and carbon dioxide) it is necessary to estimate carbon dioxide partitioning between the aqueous and gas phases, which requires additional information (reactor temperature, pH, and substrate composition) and a chemical speciation model.\n\nUsing anaerobic digestion technologies can help to reduce the emission of greenhouse gases in a number of key ways:\n\nAnaerobic digestion is particularly suited to organic material, and is commonly used for industrial effluent, wastewater and sewage sludge treatment. Anaerobic digestion, a simple process, can greatly reduce the amount of organic matter which might otherwise be destined to be dumped at sea, dumped in landfills, or burnt in incinerators.\n\nPressure from environmentally related legislation on solid waste disposal methods in developed countries has increased the application of anaerobic digestion as a process for reducing waste volumes and generating useful byproducts. It may either be used to process the source-separated fraction of municipal waste or alternatively combined with mechanical sorting systems, to process residual mixed municipal waste. These facilities are called mechanical biological treatment plants.\n\nIf the putrescible waste processed in anaerobic digesters were disposed of in a landfill, it would break down naturally and often anaerobically. In this case, the gas will eventually escape into the atmosphere. As methane is about 20 times more potent as a greenhouse gas than carbon dioxide, this has significant negative environmental effects.\n\nIn countries that collect household waste, the use of local anaerobic digestion facilities can help to reduce the amount of waste that requires transportation to centralized landfill sites or incineration facilities. This reduced burden on transportation reduces carbon emissions from the collection vehicles. If localized anaerobic digestion facilities are embedded within an electrical distribution network, they can help reduce the electrical losses associated with transporting electricity over a national grid.\n\nIn developing countries, simple home and farm-based anaerobic digestion systems offer the potential for low-cost energy for cooking and lighting.\nFrom 1975, China and India have both had large, government-backed schemes for adaptation of small biogas plants for use in the household for cooking and lighting. At present, projects for anaerobic digestion in the developing world can gain financial support through the United Nations Clean Development Mechanism if they are able to show they provide reduced carbon emissions.\n\nMethane and power produced in anaerobic digestion facilities can be used to replace energy derived from fossil fuels, and hence reduce emissions of greenhouse gases, because the carbon in biodegradable material is part of a carbon cycle. The carbon released into the atmosphere from the combustion of biogas has been removed by plants for them to grow in the recent past, usually within the last decade, but more typically within the last growing season. If the plants are regrown, taking the carbon out of the atmosphere once more, the system will be carbon neutral. In contrast, carbon in fossil fuels has been sequestered in the earth for many millions of years, the combustion of which increases the overall levels of carbon dioxide in the atmosphere.\n\nBiogas from sewage sludge treatment is sometimes used to run a gas engine to produce electrical power, some or all of which can be used to run the sewage works. Some waste heat from the engine is then used to heat the digester. The waste heat is, in general, enough to heat the digester to the required temperatures. The power potential from sewage works is limited – in the UK, there are about 80 MW total of such generation, with the potential to increase to 150 MW, which is insignificant compared to the average power demand in the UK of about 35,000 MW. The scope for biogas generation from nonsewage waste biological matter – energy crops, food waste, abattoir waste, etc. - is much higher, estimated to be capable of about 3,000 MW. Farm biogas plants using animal waste and energy crops are expected to contribute to reducing CO emissions and strengthen the grid, while providing UK farmers with additional revenues.\n\nSome countries offer incentives in the form of, for example, feed-in tariffs for feeding electricity onto the power grid to subsidize green energy production.\n\nIn Oakland, California at the East Bay Municipal Utility District’s main wastewater treatment plant (EBMUD), food waste is currently codigested with primary and secondary municipal wastewater solids and other high-strength wastes. Compared to municipal wastewater solids digestion alone, food waste codigestion has many benefits. Anaerobic digestion of food waste pulp from the EBMUD food waste process provides a higher normalized energy benefit, compared to municipal wastewater solids: 730 to 1,300 kWh per dry ton of food waste applied compared to 560 to 940 kWh per dry ton of municipal wastewater solids applied.\n\nBiogas grid-injection is the injection of biogas into the natural gas grid. The raw biogas has to be previously upgraded to biomethane. This upgrading implies the removal of contaminants such as hydrogen sulphide or siloxanes, as well as the carbon dioxide. Several technologies are available for this purpose, the most widely implemented being pressure swing adsorption (PSA), water or amine scrubbing (absorption processes) and, in recent years, membrane separation.\nAs an alternative, the electricity and the heat can be used for on-site generation, resulting in a reduction of losses in the transportation of energy. Typical energy losses in natural gas transmission systems range from 1–2%, whereas the current energy losses on a large electrical system range from 5–8%.\n\nIn October 2010, Didcot Sewage Works became the first in the UK to produce biomethane gas supplied to the national grid, for use in up to 200 homes in Oxfordshire. By 2017, UK electricity firm Ecotricity plan to have digester fed by locally sourced grass fueling 6000 homes\n\nAfter upgrading with the above-mentioned technologies, the biogas (transformed into biomethane) can be used as vehicle fuel in adapted vehicles. This use is very extensive in Sweden, where over 38,600 gas vehicles exist, and 60% of the vehicle gas is biomethane generated in anaerobic digestion plants.\n\nThe solid, fibrous component of the digested material can be used as a soil conditioner to increase the organic content of soils. Digester liquor can be used as a fertiliser to supply vital nutrients to soils instead of chemical fertilisers that require large amounts of energy to produce and transport. The use of manufactured fertilisers is, therefore, more carbon-intensive than the use of anaerobic digester liquor fertiliser. In countries such as Spain, where many soils are organically depleted, the markets for the digested solids can be equally as important as the biogas.\n\nBy using a bio-digester, which produces the bacteria required for decomposing, cooking gas is generated. The organic garbage like fallen leaves, kitchen waste, food waste etc. are fed into a crusher unit, where the mixture is conflated with a small amount of water. The mixture is then fed into the bio-digester, where the bacteria decomposes it to produce cooking gas. This gas is piped to kitchen stove. A 2 cubic meter bio-digester can produce 2 cubic meter of cooking gas. This is equivalent to 1 kg of LPG. The notable advantage of using a bio-digester is the sludge which is a rich organic manure.\n\nThe three principal products of anaerobic digestion are biogas, digestate, and water.\n\nBiogas is the ultimate waste product of the bacteria feeding off the input biodegradable feedstock (the methanogenesis stage of anaerobic digestion is performed by archaea, a micro-organism on a distinctly different branch of the phylogenetic tree of life to bacteria), and is mostly methane and carbon dioxide,\nwith a small amount hydrogen and trace hydrogen sulfide. (As-produced, biogas also contains water vapor, with the fractional water vapor volume a function of biogas temperature). Most of the biogas is produced during the middle of the digestion, after the bacterial population has grown, and tapers off as the putrescible material is exhausted. The gas is normally stored on top of the digester in an inflatable gas bubble or extracted and stored next to the facility in a gas holder.\n\nThe methane in biogas can be burned to produce both heat and electricity, usually with a reciprocating engine or microturbine often in a cogeneration arrangement where the electricity and waste heat generated are used to warm the digesters or to heat buildings. Excess electricity can be sold to suppliers or put into the local grid. Electricity produced by anaerobic digesters is considered to be renewable energy and may\nattract subsidies. Biogas does not contribute to increasing atmospheric carbon dioxide concentrations because the gas is not released directly into the atmosphere and the carbon dioxide comes from an organic source with a short carbon cycle.\n\nBiogas may require treatment or 'scrubbing' to refine it for use as a fuel. Hydrogen sulfide, a toxic product formed from sulfates in the feedstock, is released as a trace component of the biogas. National environmental enforcement agencies, such as the U.S. Environmental Protection Agency or the English and Welsh Environment Agency, put strict limits on the levels of gases containing hydrogen sulfide, and, if the levels of hydrogen sulfide in the gas are high, gas scrubbing and cleaning equipment (such as amine gas treating) will be needed to process the biogas to within regionally accepted levels. Alternatively, the addition of ferrous chloride FeCl to the digestion tanks inhibits hydrogen sulfide production.\n\nVolatile siloxanes can also contaminate the biogas; such compounds are\nfrequently found in household waste and wastewater. In digestion facilities accepting these materials as a component of the feedstock, low-molecular-weight siloxanes volatilise into biogas. When this gas is combusted in a gas engine, turbine, or boiler, siloxanes are converted into silicon dioxide (SiO), which deposits internally in the machine, increasing wear and tear. Practical and cost-effective technologies to remove siloxanes and other biogas contaminants are available at the present time. In certain applications, \"in situ\" treatment can be used to increase the methane purity by reducing the offgas carbon dioxide content, purging the majority of it in a secondary reactor.\n\nIn countries such as Switzerland, Germany, and Sweden, the methane in the biogas may be compressed for it to be used as a vehicle transportation fuel or input directly into the gas mains. In countries where the driver for the use of anaerobic digestion are renewable electricity subsidies, this route of treatment is less likely, as energy is required in this processing stage and reduces the overall levels available to sell.\n\nDigestate is the solid remnants of the original input material to the digesters that the microbes cannot use. It also consists of the mineralised remains of the dead bacteria from within the digesters. Digestate can come in three forms: fibrous, liquor, or a sludge-based combination of the two fractions. In two-stage systems, different forms of digestate come from different digestion tanks. In single-stage digestion systems, the two fractions will be combined and, if desired, separated by further processing.\n\nThe second byproduct (acidogenic digestate) is a stable, organic material consisting largely of lignin and cellulose, but also of a variety of mineral components in a matrix of dead bacterial cells; some plastic may be present. The material resembles domestic compost and can be used as such or to make low-grade building products, such as fibreboard.\nThe solid digestate can also be used as feedstock for ethanol production.\n\nThe third byproduct is a liquid (methanogenic digestate) rich in nutrients, which can be used as a fertiliser, depending on the quality of the material being digested. Levels of potentially toxic elements (PTEs) should be chemically assessed. This will depend upon the quality of the original feedstock. In the case of most clean and source-separated biodegradable waste streams, the levels of PTEs will be low. In the case of wastes originating from industry, the levels of PTEs may be higher and will need to be taken into consideration when determining a suitable end use for the material.\n\nDigestate typically contains elements, such as lignin, that cannot be broken down by the anaerobic microorganisms. Also, the digestate may contain ammonia that is phytotoxic, and may hamper the growth of plants if it is used as a soil-improving material. For these two reasons, a maturation or composting stage may be employed after digestion. Lignin and other materials are available for degradation by aerobic microorganisms, such as fungi, helping reduce the overall volume of the material for transport. During this maturation, the ammonia will be oxidized into nitrates, improving the fertility of the material and making it more suitable as a soil improver. Large composting stages are typically used by dry anaerobic digestion technologies.\n\nThe final output from anaerobic digestion systems is water, which originates both from the moisture content of the original waste that was treated and water produced during the microbial reactions in the digestion systems. This water may be released from the dewatering of the digestate or may be implicitly separate from the digestate.\n\nThe wastewater exiting the anaerobic digestion facility will typically have elevated levels of biochemical oxygen demand (BOD) and chemical oxygen demand (COD). These measures of the reactivity of the effluent indicate an ability to pollute. Some of this material is termed 'hard COD', meaning it cannot be accessed by the anaerobic bacteria for conversion into biogas. If this effluent were put directly into watercourses, it would negatively affect them by causing eutrophication. As such, further treatment of the wastewater is often required. This treatment will typically be an oxidation stage wherein air is passed through the water in a sequencing batch reactors or reverse osmosis unit.\n\nThe history of anaerobic digestion is a long one, beginning as early as tenth century BCE in Assyria where biogas was used to heat bath water. Reported scientific interest in the manufacturing of gas produced by the natural decomposition of organic matter dates from the 17th century, when Robert Boyle (1627-1691) and Stephen Hales (1677-1761) noted that disturbing the sediment of streams and lakes released flammable gas. In 1778, the Italian physicist Alessandro Volta (1745-1827), the father of Electrochemistry, scientifically identified that gas as methane.\n\nIn 1808 Sir Humphry Davy proved the presence of methane in the gases produced by cattle manure. The first known anaerobic digester was built in 1859 at a leper colony in Bombay in India. In 1895, the technology was developed in Exeter, England, where a septic tank was used to generate gas for the sewer gas destructor lamp, a type of gas lighting. Also in England, in 1904, the first dual-purpose tank for both sedimentation and sludge treatment was installed in Hampton, London. \n\nBy the early 20th century, anaerobic digestion systems began to resemble the technology as it appears today. In 1906, Karl Imhoff created the Imhoff tank; an early form of anaerobic digester and model wastewater treatment system throughout the early 20th century. After 1920, closed tank systems began to replace the previously common use of anaerobic lagoons- covered earthen basins used to treat volatile solids. Research on anaerobic digestion began in earnest in the 1930s.\n\nAround the time of World War I, production from biofuels slowed as petroleum production increased and its uses were identified. While fuel shortages during World War II re-popularized anaerobic digestion, interest in the technology decreased again after the war ended. Similarly, the 1970s energy crisis sparked interest in anaerobic digestion. In addition to high energy prices, factors affecting the adoption of Anaerobic Digestion systems include receptivity to innovation, pollution penalties, policy incentives, and the availability of subsidies and funding opportunities.\n\nToday, anaerobic digesters are commonly found alongside farms to reduce nitrogen run-off from manure, or wastewater treatment facilities to reduce the costs of sludge disposal. Agricultural anaerobic digestion for energy production has become most popular in Germany, where there were 8,625 digesters in 2014. In the United Kingdom, there were 259 facilities by 2014, and 500 projects planned to become operational by 2019. In the United States, there were 191 operational plants across 34 states in 2012. Policy may explain why adoption rates are so different across these countries.\n\nFeed-in tariffs in Germany were enacted in 1991, also known as FIT, providing long-term contracts compensating investments in renewable energy generation. Consequently, between 1991 and 1998 the number of anaerobic digester plants in Germany grew from 20 to 517. In the late 1990s, energy prices in Germany varied and investors became unsure of the market’s potential. The German government responded by amending FIT four times between 2000 and 2011, increasing tariffs and improving the profitability of anaerobic digestion, and resulting in reliable returns for biogas production and continued high adoption rates across the country.\n\n\n"}
{"id": "28958968", "url": "https://en.wikipedia.org/wiki?curid=28958968", "title": "Aptina", "text": "Aptina\n\nAptina Imaging Corporation was a company that sold CMOS imaging products. Their CMOS sensors were used in Nikon V1 (10.1 MP, CX format, 16.9x17.9 mm), Nikon J1, Nikon V2. By 2009 year Aptina had a 16% share of the CMOS image sensors market, with revenue estimated at $671 million. The company was acquired in 2014 by ON Semiconductor\n\n\n\nAptina Imaging was created as spin-off of Micron Technology's Image Sensor Division in March 2008. At this time it still was an independent division within Micron. In July 2009, Aptina became independent, privately held company, and was partially sold to a group including TPG and Riverwood Capital. ON Semiconductor Corporation Completes Acquisition of Aptina Imaging in August 2014.\n\n"}
{"id": "2047648", "url": "https://en.wikipedia.org/wiki?curid=2047648", "title": "Auxiliary feedwater", "text": "Auxiliary feedwater\n\nAuxiliary feedwater is a backup water supply system found in pressurized water reactor nuclear power plants (PWRs). This system, sometimes known as Emergency feedwater, can be used for shutdown the reactor, if normal feedwater to the steam generators fails to work. It works by pumping water to the steam generators from reserve tanks or a larger body of water (e.g. lake, river, or ocean) to remove decay heat from the reactor. Often, but not always, the PWRs are equipped with \"motor driven\" aux feedwater systems (MDAFWs) and as a diverse mean as well \"turbine driven\" aux feedwater systems (TDAFWs), that take their drive not from electricity, but from the steam in the normal secondary side steam circuit of the plant. In difference to the ECCS-system for LOCAs, the auxiliary feedwater system doesn't inject directly into the reactor core, it cools in an indirect manner by cooling the primary circuit with the reactor via the steam generators.\n"}
{"id": "18801266", "url": "https://en.wikipedia.org/wiki?curid=18801266", "title": "Battery management system", "text": "Battery management system\n\nA battery management system (BMS) is any electronic system that manages a rechargeable battery (cell or battery pack), such as by protecting the battery from operating outside its safe operating area, monitoring its state, calculating secondary data, reporting that data, controlling its environment, authenticating it and / or balancing it.\n\nA battery pack built together with a battery management system with an external communication data bus is a smart battery pack. A smart battery pack must be charged by a smart battery charger.\n\nA BMS may monitor the state of the battery as represented by various items, such as:\n\n\n\nAdditionally, a BMS may calculate values based on the above items, such as:\n\n\nThe central controller of a BMS communicates internally with its cell level hardware, or externally with high level hardware such as laptops or HMI.\n\nHigh level external communication are simple and use several methods:\n\n\nLow voltage centralized BMSs mostly do not have any internal communications. They measure cell voltage by resistance divide.\n\nDistributed or modular BMSs must use some low level internal cell-controller (Modular architecture) or controller-controller (Distributed architecture) communication. These types of communications are difficult, especially for high voltage systems. The problem is voltage shift between cells. The first cell ground signal may be hundreds of volts higher than the other cell ground signal. Apart from software protocols, there are two known ways of hardware communication for voltage shifting systems, Optical-isolator and wireless communication. Another restriction for internal communications is the maximum number of cells. For modular architecture most hardware is limited to maximum 255 nodes. For high voltage systems the seeking time of all cells is another restriction, limiting minimum bus speeds and losing some hardware options. Cost of modular systems is important, because it may be comparable to the cell price. Combination of hardware and software restrictions results to be a few options for internal communication:\n\nA BMS may protect its battery by preventing it from operating outside its safe operating area, such as:\n\n\nThe BMS may prevent operation outside the battery's safe operating area by:\n\n\nA BMS may also feature a precharge system allowing a safe way to connect the battery to different loads and eliminating the excessive inrush currents to load capacitors.\n\nThe connection to loads is normally controlled through electromagnetic relays called contactors. The precharge circuit can be either power resistors connected in series with the loads until the capacitors are charged. Alternatively, a switched mode power supply connected in parallel to loads can be used to charge the voltage of the load circuit up to a level close enough to battery voltage in order to allow closing the contactors between battery and load circuit. A BMS may have a circuit that can check whether a relay is already closed before precharging (due to welding for example) to prevent inrush currents to occur.\n\nIn order to maximize the battery's capacity, and to prevent localized under-charging or over-charging, the BMS may actively ensure that all the cells that compose the battery are kept at the same voltage or State of Charge, through balancing. The BMS can balance the cells by:\n\n\nBMS technology varies in complexity and performance:\nBMS topologies fall in 3 categories:\n\nCentralized BMSs are most economical, least expandable, and are plagued by a multitude of wires.\nDistributed BMSs are the most expensive, simplest to install, and offer the cleanest assembly.\nModular BMSs offer a compromise of the features and problems of the other two topologies.\n\nThe requirements for a BMS in mobile applications (such as electric vehicles) and stationary applications (like stand-by UPSs in a server room) are quite different, especially from the space and weight constraint requirements, so the hardware and software implementations must be tailored to the specific use. In the case of electric or hybrid vehicles, the BMS is only a subsystem and cannot work as a standalone device. It must communicate with at least a charger (or charging infrastructure), a load, thermal management and emergency shutdown subsystems. Therefore, in a good vehicle design the BMS is tightly integrated with those subsystems. Some small mobile applications (such as medical equipment carts, motorized wheelchairs, scooters, and fork lifts) often have external charging hardware, however the on-board BMS must still have tight design integration with the external charger.\n\nVarious Battery balancing methods are in use, some of them based on state of charge theory.\n\n\n"}
{"id": "1811328", "url": "https://en.wikipedia.org/wiki?curid=1811328", "title": "Canadian Association of Petroleum Producers", "text": "Canadian Association of Petroleum Producers\n\nThe Canadian Association of Petroleum Producers (CAPP), with its head office in Calgary, Alberta, is a lobby group that represents the upstream Canadian oil and natural gas industry. CAPP's members produce \"90% of Canada’s natural gas and crude oil\" and \"are an important part of a national industry with revenues of about $100 billion-a-year (CAPP 2011).\"\n\nCAPP origins can be traced back to the Alberta Oil Operators’ Association, which was founded in 1927, after the discovery of the Turner Valley Oil Field. In 1947, the Alberta Petroleum Association changed its name to the Western Canadian Petroleum Association, and In 1952, the Western Canada Petroleum Association amalgamated with the Saskatchewan Operators’ Association and adopted the name Canadian Petroleum Association.\n\nAt a meeting on December 9, 1952, the CPA drafted a new constitution which outlined the objectives of the organization as follows:\n\nOn June 10, 1958 the CPA opened an office in Ottawa and became \"one (of) the oldest, largest and most influential lobby groups in Canada.\" It provided the federal government with information pertaining to the oil industry while keeping the CPA informed about political trends, government regulations and statistics. By 1965 the CPA had a membership of more than 200 members representing roughly 97 percent of all oil and gas production in Canada. In 1981, two years after the first commercial discovery at Hibernia off the coast of Newfoundland, the CPA opened an office in St. John’s in cooperation with the Eastcoast Petroleum Operators’ Association.\n\nIn 1992, when the Canadian Association of Petroleum Producers (CAPP) was formed, with the CPA amalgamation with the Independent Petroleum Association of Canada (IPAC) to form the Canadian Association of Petroleum Producers (CAPP), Gerry Protti was named as founding president.\n\n\nCanada's estimated total oil reserves including conventional oil were approximately 180 billion barrels (29 km³), behind only Saudi Arabia and Venezuela. Canada produces approximately 2.7 million barrels (430,000 m³) of crude oil a day, and 6.4 trillion cubic feet (180 km³) of natural gas per year. In 2013, an IPSOS poll showed a majority (75%) of Canadians prioritize local crude before using imported oil from foreign sources, while just over one in ten (14%) ‘disagree’ (4% strongly/11% somewhat) and 11% have no opinion.\n\nCAPP has advocated for the industry as GHG emissions rose 14% in 2009 and 2010, by its own admission. . However, GHG emissions per barrel of oil sands crude produced have dropped by 26% since 1990 as a result of new operating practices and technology.\n\nAccording to IHS CERA, oil sands crude has similar CO2 emissions to other heavy oils and is 9% more intensive than the U.S. crude supply average on a wells-to-wheels basis.\n\nThe industry employs 550,000 people and paid billions in taxes and royalties to different levels of government.\n\nAdvocacy for Oil Sands\nCAPP's series of meetings in 2010 in eight cities in Canada and the United States, including Vancouver, Edmonton, Ottawa, Toronto, Montreal, Washington D.C., New York and Chicago, with CAPP representatives, oil sands CEOs and 160 key stakeholders, culminated in a report entitled \"Dialogues\" published on 14 April 2011.\n\nCAPP advocates for the use of the controversial technology hydraulic fracturing. In 2010 released a series of voluntary Guiding Principles for Hydraulic Fracturing for Canadian natural gas producers to adhere to. The Guiding Principles of Hydraulic Fracturing were followed in 2011 by an agreed set of Six Hydraulic Fracturing Practices for:\n1. Fracturing fluid additive disclosure\n2. Fracturing fluid additive risk management\n3. Baseline groundwater testing\n4. Wellbore construction\n5. Water sourcing and reuse\n6. Fluid handling, transport, disposal\n\nThe Council of Canadians and Sierra Club Canada take a strong position against hydraulic fracturing and want it banned in Canada entirely, and have supported specific bans in Nova Scotia and New Brunswick.\n\nCAPP supports and advocates for exports of Canadian crude oil via Canada's west coast via the Northern Gateway and the KinderMorgan TransMountain Expansion Project. In September 2011, the Asia Pacific Foundation of Canada (APF Canada) and the Canada West Foundation established the Canada-Asia Energy Futures Task Force with Kathleen (Kathy) E. Sendall, C.M., FCAE, a former Governor and Board Chair of the Canadian Association of Petroleum Producers (CAPP) and Kevin G. Lynch, a Canadian economist and former Clerk of the Privy Council and Secretary to the Cabinet, Canada's most senior civil servant as co-chairs, to investigate a long-term Canada-Asia energy relationship. One of their recommendations was the creation of a public energy transportation corridor.\n\nCanadian opponents to the Northern Gateway , intended to permit shipping of high-carbon Canadian crude over ecologically sensitive rivers and waters to carbon-uncontrolled countries including India and China, include 61 First Nations in British Columbia.\n\nCAPP supports and advocates for the $7-billion pipeline expansion project by the Canadian-based company TransCanada to build the Keystone XL, that would extend and expand capacity of existing pipelines, that transport crude oil from the Athabasca oil sands in northern Alberta to tidewater and to refineries in the Gulf, capable of refining the heavy bitumen crude oil.\n\nNine winners of the Nobel Peace Prize, including Archbishop Desmond Tutu and the Dalai Lama, were signatories to a letter to pressure U.S. President Barack Obama to reject the $7-billion pipeline expansion project by the Canadian-based company TransCanada to build the Keystone XL.\n\nThe position of the Nobel Peace Prize winners, essentially, is that one rich nation selling increasingly heavy high-carbon oil to another sabotages any effort to reach a deal on global carbon controls, and that moves to expand this export (like Keystone XL or Northern Gateway) cause significant and direct risks to world peace, as climate victim countries become subject to chaotic weather, fighting over scarce water (especially in Southeast Asia and Africa), flooding and rising sea levels.\n\nCAPP opposed the Kyoto Protocol, from which Stephen Harper withdrew Canada in December 2011. CAPP's lobbying efforts included favouring \"made in Canada\" approach and advocating for a carbon pricing program. In 2007 a carbon tax was implemented in Alberta, Canada's major oil and gas producing province. Supported by CAPP and in the industry, the $15/tonne carbon tax feeds a GHG emissions reduction technology fund.\n\nBy 2008, the oil sands industry contributes (approximately 3%–4%) of Canada’s GHG emissions (approximately 3%–4%. By 2012, oil sands contributed 0.14% of global GHG emissions. Transportation and electricity were the largest contributors of GHG, with transportation contributing 190 Mt of CO2 equivalent per year (MtCO2eq yr−1) and electricity and heat generation: 125 MtCO2eq yr−1. However, by 2007 (Environment Canada 2007) cautioned that unrestricted development of the oil sands could increase its emissions and the percentage. A 2008 CAPP report argued that both the Alberta and Federal governments adopted \"comparable industry GHG emissions targets in which large emitters must reduce their emissions by either improving their operation, purchasing emissions credits or investing in technology funds.\"\n\nCanada was the first signatory nation to walk away from the Kyoto Protocol in 2012. The U.S. abandoned the Kyoto Protocol in 2001.\n\nIn the summer of 2011 CAPP contacted ENV to requested a meeting with the Canadian Society for Unconventional Gas (CSUG), and officials from several government ministries, including Alberta Environment, Energy, Sustainable Resource Development (SRD), as well as the Energy Resources Conservation Board (ERCB), (now Alberta Energy Regulator) to discuss CAPP’s desire to strike a committee to develop a public communications strategy focused on fracturing and water use associated with shale gas development.\" Senior-level government and industry officials attended the joint meeting \"to develop a plan to shape public perceptions of shale gas development and water use.\" From Alberta Energy participants included Director of Unconventional Gas Doug Bowes, Associate Branch Head Matthew Foss, Environment and Resource Services Audrey Murray, Executive Director of Resource Development Sharla Rauschning, Assistant Deputy Minister Resource Development Policy Division Jennifer Steber. From Alberta Environment participants included, Deputy Minister Ernie Hui, Former Head of Groundwater Policy within the Water Policy Branch, now the Exec. Dir. of OH&S Policy and Program with Human Services Ross Nairne. From Sustainable Resource Development (SRD) participants included Assistant Deputy Minister Glen Selland, Executive Director, Land Management Branch Jeff Reynolds, Officials from CAPP included VP Operations David Pryce, Manager of BC Operations Brad Herald, Manager of Water and Reclamation Tara Payment. From the Canadian Society for Unconventional Gas (CSUG) CSUG (a.k.a. CSUR) participants included Vice President Kevin Heffernan.\n\nJune 8, 2011, e-mail to senior government officials from the Energy Resources Conservation Board, the arm’s length regulator of the oil and gas industry in Alberta, to several meetings to produce a collaborative communications campaign on fracking strategy. On 9 June 2011 the Alberta government approved collaborative communications campaign in the minutes of their joint meeting. stating that\n\nBy 29 November 2011, the CBC and the Alberta Federation of Labour (AFL), were investigating the role played by CAPP in influencing Alberta Environment over public communications surrounding shale gas extraction, a controversial practice that has significant environmental concerns associated with it, especially when fracturing is employed. Questions were raised about the legality of private interests influencing government. Complaints were filed and dismissed.\n\nAccording to the Federal lobbyist registry, from January to September 2012, the Canadian Association of Petroleum Producers had 178 contacts with federal officials to discuss issues such as pipelines, making it the lobby group with the most contacts that year. They lobbied on greenhouse gas regulations related to the Clean Air Act, Fisheries Act, pipeline regulation and tax credits.\n\n\n\n\"Additional information about the lobbying controversy can be found here: http://www.cbc.ca/news/canada/edmonton/story/2011/11/29/edmonton-lobbying-compalint-dismissed.html\"\n\n"}
{"id": "18001289", "url": "https://en.wikipedia.org/wiki?curid=18001289", "title": "Carolina rig", "text": "Carolina rig\n\nThe Carolina rig is a plastic bait rig similar to the Texas rig, but with the weight fixed above the hook, instead of sliding down to it. The Carolina rig is suitable for beginning fishers. This specific rig is designed to help fishermen catch bottom feeding fish, particularly bass fish. When placed in water, bait attached to a Carolina rig will move in a circular motion. Bass are attracted to this movement and are therefore more likely to bite the lure. The Carolina rig also provides benefits for colder seasons. The heavy weight on the rig allows the bait to reach deeper waters, where fish typically stay in winter months.\n\n"}
{"id": "944047", "url": "https://en.wikipedia.org/wiki?curid=944047", "title": "Color printing", "text": "Color printing\n\nColor printing or colour printing is the reproduction of an image or text in color (as opposed to simpler black and white \nor monochrome printing). Any natural scene or color photograph can be optically and physiologically dissected into three primary colors, red, green and blue, roughly equal amounts of which give rise to the perception of white, and different proportions of which give rise to the visual sensations of all other colors. The additive combination of any two primary colors in roughly equal proportion gives rise to the perception of a secondary color. For example, red and green yields yellow, red and blue yields magenta (a purple hue), and green and blue yield cyan (a turquoise hue). Only yellow is counter-intuitive.\nYellow, cyan and magenta are merely the \"basic\" secondary colors: unequal mixtures of the primaries give rise to perception of many other colors all of which may be considered \"tertiary.\"\n\nWhile there are many techniques for reproducing images in color, specific graphic processes and industrial equipment are used for mass reproduction of color images on paper. In this sense, \"color printing\" involves reproduction techniques suited for printing presses capable of thousands or millions of impressions for publishing newspapers and magazines, brochures, cards, posters and similar mass-market items. In this type of industrial or commercial printing, the technique used to print full-color images, such as color photographs, is referred to as four-color-process or merely process printing. Four inks are used: three secondary colors plus black. These ink colors are cyan, magenta, yellow and key (black); abbreviated as CMYK. Cyan can be thought of as minus-red, magenta as minus-green, and yellow as minus-blue. These inks are semi-transparent or translucent. Where two such inks overlap on the paper due to sequential printing impressions, a primary color is perceived. For example, yellow (minus-blue) overprinted by magenta (minus green) yields red. Where all three inks may overlap, almost all incident light is absorbed or subtracted, yielding near black, but in practical terms it is better and cheaper to use a separate black ink instead of combining three colored inks. The secondary or subtractive colors cyan, magenta and yellow may be considered \"primary\" by printers and watercolorists (whose basic inks and paints are transparent).\n\nTwo graphic techniques are required to prepare images for four-color printing. In the \"pre-press\" stage, original images are translated into forms that can be used on a printing press, through \"color separation,\" and \"screening\" or \"halftoning.\" These steps make possible the creation of printing plates that can transfer color impressions to paper on printing presses based on the principles of lithography.\n\nAn emerging method of full-color printing is six-color process printing (for example, Pantone's Hexachrome system) which adds orange and green to the traditional CMYK inks for a larger and more vibrant gamut, or color range. However, such alternate color systems still rely on color separation, halftoning and lithography to produce printed images.\n\nColor printing can also involve as few as one color ink, or multiple color inks which are not the primary colors. Using a limited number of color inks, or specific color inks in addition to the primary colors, is referred to as \"spot color\" printing. Generally, spot-color inks are specific formulations that are designed to print alone, rather than to blend with other inks on the paper to produce various hues and shades. The range of available spot color inks, much like paint, is nearly unlimited, and much more varied than the colors that can be produced by four-color-process printing. Spot-color inks range from subtle pastels to intense fluorescents to reflective metallics.\n\nColor printing involves a series of steps, or transformations, to generate a quality color reproduction. The following sections focus on the steps used when reproducing a color image in CMYK printing, along with some historical perspective.\n\nWoodblock printing on textiles preceded printing on paper in both East Asia and Europe, and the use of different blocks to produce patterns in color was common. The earliest way of adding color to items printed on paper was by hand-coloring, and this was widely used for printed images in both Europe and East Asia. Chinese woodcuts have this from at least the 13th century, and European ones from very shortly after their introduction in the 15th century, where it continued to be practiced, sometimes at a very skilled level, until the 19th century—elements of the official British Ordnance Survey maps were hand-colored by boys until 1875. Early European printed books often left spaces for initials, rubrics and other elements to be added by hand, just as they had been in manuscripts, and a few early printed books had elaborate borders and miniatures added. However this became much rarer after about 1500.\n\nBritish art historian Michael Sullivan writes that \"the earliest color printing known in China, and indeed in the whole world, is a two-color frontispiece to a Buddhist sutra scroll, dated 1346\". Color prints were also used later in the Ming Dynasty. In Chinese woodblock printing, early color woodcuts mostly occur in luxury books about art, especially the more prestigious medium of painting. The first known example is a book on ink-cakes printed in 1606, and color technique reached its height in books on painting published in the seventeenth century. Notable examples are Ming-era Chinese painter Hu Zhengyan's \"Treatise on the Paintings and Writings of the Ten Bamboo Studio\" of 1633, and the \"Manual of the Mustard Seed Garden\" published in 1679 and 1701, and printed in five colors.\n\nIn Japan, color woodcuts were used for both sheet prints and book illustrations, though these techniques are better known within the history of prints. The \"full-color\" technique, called nishiki-e in its fully developed form, spread rapidly, and was used widely for sheet prints from the 1760s on. Text was nearly always monochrome, and many books continued to be published with monochrome illustrations sumizuri-e, but the growth of the popularity of \"ukiyo-e\" brought with it demand for ever increasing numbers of colors and complexity of techniques. By the nineteenth century most artists designed prints that would be published in color. Major stages of this development were:\n\n\nFurther developments followed from refinements of technique and trends in taste. For instance:\n\nMost early methods of color printing involved several prints, one for each color, although there were various ways of printing two colors together if they were separate. Liturgical and many other kinds of books required rubrics, normally printed in red; these were long done by a separate print run with a red forme for each page. Other methods were used for single leaf prints. The chiaroscuro woodcut was a European method developed in the early 16th century, where to a normal woodcut block with a linear image (the \"line block\"), one or more colored \"tone blocks\" printed in different colors would be added. This was the method developed in Germany; in Italy only tone blocks were often used, to create an effect more like a wash drawing. Jacob Christoph Le Blon developed a method using three intaglio plates, usually in mezzotint; these were overprinted to achieve a wide range of colors.\n\nIn the 19th century a number of different methods of color printing, using woodcut (technically Chromoxylography) and other methods, were developed in Europe, which for the first time achieved widespread commercial success, so that by the later decades the average home might contain many examples, both hanging as prints and as book illustrations. George Baxter patented in 1835 a method using an intaglio line plate (or occasionally a lithograph), printed in black or a dark color, and then overprinted with up to twenty different colors from woodblocks. Edmund Evans used relief and wood throughout, with up to eleven different colors, and latterly specialized in illustrations for children's books, using fewer blocks but overprinting non-solid areas of color to achieve blended colors. English Artists such as Randolph Caldecott, Walter Crane and Kate Greenaway were influenced by the Japanese prints now available and fashionable in Europe to create a suitable style, with flat areas of color.\n\nChromolithography was another process, which by the end of the 19th century had become dominant, although this used multiple prints with a stone for each color. Mechanical color separation, initially using photographs of the image taken with three different color filters, reduced the number of prints needed to three. Zincography, with zinc plates, later replaced lithographic stones, and remained the most common method of color printing until the 1930s.\n\nTypically color separation is the responsibility of the color separator. This includes cleaning up the file to make it print ready and creating a proof for the prepress approval process. The process of color separation starts by separating the original artwork into red, green, and blue components (for example by a digital scanner). Before digital imaging was developed, the traditional method of doing this was to photograph the image three times, using a filter for each color. However this is achieved, the desired result is three grayscale images, which represent the red, green, and blue (RGB) components of the original image.\n\nThe next step is to invert each of these separations. When a negative image of the red component is produced, the resulting image represents the cyan component of the image. Likewise, negatives are produced of the green and blue components to produce magenta and yellow separations, respectively. This is done because cyan, magenta, and yellow are subtractive primaries which each represent two of the three additive primaries (RGB) after one additive primary has been subtracted from white light.\n\nCyan, magenta, and yellow are the three basic colors used for color reproduction. When these three colors are variously used in printing, the result should be a reasonable reproduction of the original, but in practice this is not the case. Due to limitations in the inks, the darker colors are dirty and muddied. To resolve this, a black separation is also created, which improves the shadow and contrast of the image. Numerous techniques exist to derive this black separation from the original image; these include grey component replacement, under color removal, and under color addition. This printing technique is referred to as CMYK (the \"K\" stands for \"key\", a traditional word for the black printing plate).\n\nToday's digital printing methods do not have the restriction of a single color space that traditional CMYK processes do. Many presses can print from files that were ripped with images using either RGB or CMYK modes. The color reproduction abilities of a particular color space can vary; the process of obtaining accurate colors within a color model is called color matching.\n\nInks used in color printing presses are semi-transparent and can be printed on top of each other to produce different hues. For example, green results from printing yellow and cyan inks on top of each other. However, a printing press cannot vary the amount of ink applied to particular picture areas except through \"screening,\" a process that represents lighter shades as tiny dots, rather than solid areas, of ink. This is analogous to mixing white paint into a color to lighten it, except the white is the paper itself. In process color printing, the screened image, or halftone for each ink color is printed in succession. The screen grids are set at different angles, and the dots therefore create tiny rosettes, which, through a kind of optical illusion, appear to form a continuous-tone image. You can view the halftoning, which enables printed images, by examining a printed picture under magnification.\n\nTraditionally, halftone screens were generated by inked lines on two sheets of glass that were cemented together at right angles. Each of the color separation films were then exposed through these screens. The resulting high-contrast image, once processed, had dots of varying diameter depending on the amount of exposure that area received, which was modulated by the grayscale separation film image.\n\nThe glass screens were made obsolete by high-contrast films where the halftone dots were exposed with the separation film. This in turn was replaced by a process where the halftones are electronically generated directly on the film with a laser. Most recently, computer to plate (CTP) technology has allowed printers to bypass the film portion of the process entirely. CTP images the dots directly on the printing plate with a laser, saving money, and eliminating the film step. The amount of generation loss in printing a lithographic negative onto a lithographic plate, unless the processing procedures are completely ignored, is almost completely negligible, as there are no losses of dynamic range, no density gradations, nor are there any colored dyes, or large silver grains to contend with in an ultra-slow rapid access negative.\n\nScreens with a \"frequency\" of 60 to 120 lines per inch (lpi) reproduce color photographs in newspapers. The coarser the screen (lower frequency), the lower the quality of the printed image. Highly absorbent newsprint requires a lower screen frequency than less-absorbent coated paper stock used in magazines and books, where screen frequencies of 133 to 200 lpi and higher are used.\n\nThe measure of how much an ink dot spreads and becomes larger on paper is called dot gain. This phenomenon must be accounted for in photographic or digital preparation of screened images. Dot gain is higher on more absorbent, uncoated paper stock such as newsprint.\n\n\n\n"}
{"id": "27429654", "url": "https://en.wikipedia.org/wiki?curid=27429654", "title": "Corelis", "text": "Corelis\n\nCorelis, Inc, a subsidiary of Electronic Warfare Associates, Inc., is a private US company categorized under Electronic Equipment & Supplies and is based in Cerritos, California.\n\nCorelis was incorporated in 1991 and initially provided engineering services primarily to the aerospace and defense industries. Corelis introduced their first JTAG boundary scan products in 1998. In 2006, Electronic Warfare Associates, Inc. (EWA) a global provider of technology and engineering services to the aerospace, defense and commercial industries, announced their acquisition of Corelis, Inc. In 2008, the appointment of George B. La Fever as Corelis President and CEO finalized the transition of Corelis, Inc. into EWA Technologies, Inc., a wholly owned subsidiary of the EWA corporate family of high technology companies.\n\nCorelis offers two distinct types of products and services: Standard Products (Boundary Scan Test Systems and Development Tools); and Custom Test Systems and System Integration.\n\nCorelis introduced their first (JTAG boundary scan products in 1998. Corelis offers boundary scan/JTAG software and hardware products. Its ScanExpress boundary scan systems are used for structural testing as well as JTAG functional emulation test and in-system programming of Flash memory, CPLDs, and FPGAs. a\n\nIn 2007, Corelis released ScanExpress JET, a test tool that combines boundary scan and functional test (FCT) technologies for test coverage.\n\nSystems are available for design and debugging, manufacturing test, and field service and support. A variety of system options are available including desktop solutions as well as portable solutions for use in the field with laptops. Corelis also provides engineering services, training, and customer support.\n\nBetween 1991 and 1998, Corelis offered engineering services and licensed HP technologies. Corelis also distributed JTAG Technologies products and took on specific custom design projects from companies such as Motorola and HP.\n\nCorelis, in partnership with Hewlett-Packard, Inc. jointly developed a large-scale test and simulation system for the Motorola Iridium satellite project. The special test equipment system (including the portion provided by Hewlett-Packard) are designed to completely test and simulate the on-board communications systems of the satellite, and to provide a satellite computer software development test bed.\n\nIn 1997, Corelis introduced ScanPlus, a high performance, low cost PC platform for executing boundary scan test vectors generated by the HP 3070 in-circuit test family from Hewlett-Packard Company.\n\nOther early Corelis projects include developing processor modules for HP/Agilent based logic analyzers; and developing early emulation products for processors such as the Intel i960 and AMD Am29000.\n\n"}
{"id": "35214786", "url": "https://en.wikipedia.org/wiki?curid=35214786", "title": "Cradle of Henry V", "text": "Cradle of Henry V\n\nThe cradle of Henry V is, according to tradition, the cradle in which the newborn Henry of Monmouth was placed. The cradle was bought at auction by Edward VII, whose successor later gave it to the London Museum. Made of oak, it is now believed to date from a century later than the time of Henry V, but is regarded as a unique example of a medieval cradle made for a baby of noble birth.\n\nThe child who was to be Henry V of England was born at Monmouth Castle on 16 September 1386. He was the son of Mary de Bohun and Henry Bolingbroke, and the grandson of John of Gaunt. Monmouth was among Henry of Bolingbroke's possessions and it was where the newly married couple lived for several years. Mary was to die in 1394; Henry Bolingbroke ousted king Richard II and became Henry IV, King of England, in 1399. The young Henry of Monmouth succeeded his father to become King Henry V in 1413.\n\nDuring the 18th and early 19th centuries the cradle was famous locally and its claim to be a genuine relic of Henry V's childhood was generally accepted. It was private property, though often shown to visitors. In 1839 it was said to have been in the possession of the antiquarian George Weare Braikenridge, of Brislington, Bristol, who was reported to have bought it for £30. In 1872, the cradle, together with the armour that Henry V allegedly wore at Agincourt, were said to be at Troy House in Mitchel Troy. Its location was reported by William Watkins Old, of Monmouth to the Royal Historical Society in 1876. \n\nThe cradle was auctioned at Christie's in London in 1908 and was bought for 230 guineas by Guy Laking, King Edward VII's armourer, bidding on behalf of the King himself. The cradle then went to Windsor Castle and remained there for four years. In 1912 the London Museum was established at Kensington Palace under Guy Laking's direction, and the cradle was given to the Museum by Edward VII's successor, King George V. The London Museum, now known as the Museum of London, moved to its current site at the Barbican in 1950.\n\nThe cradle, made of oak and with two heraldic birds watching over a suspended crib, is still in the Museum of London. It is now believed to date from the late 15th century, and is a unique example of a mediaeval cradle made for a baby of noble birth. However, it post-dates the time of Henry V.\n\n"}
{"id": "20218902", "url": "https://en.wikipedia.org/wiki?curid=20218902", "title": "Dan Kogai", "text": "Dan Kogai\n\nKogai went to middle school, but he had problems with formal education and skipped classes. Immediately after he graduated from middle school at the age of 16 he passed the high-school equivalence exam, and up until age 18 he spent his time teaching senior students as a tutor and cram school teacher. He then entered the University of California, Berkeley.\n\nDuring his fourth year at university, his family's home burnt down, and he left school. He made a living with the computer skills he had learnt at university, and at age 29 was headhunted by Takafumi Horie to join Livin' On the Edge (now Livedoor).\n\n\n\n"}
{"id": "6212365", "url": "https://en.wikipedia.org/wiki?curid=6212365", "title": "Data steward", "text": "Data steward\n\nA data steward is a role within an organization responsible for utilizing an organization's data governance processes to ensure fitness of data elements - both the content and metadata. Data stewards have a specialist role that incorporates processes, policies, guidelines and responsibilities for administering organizations' entire data in compliance with policy and/or regulatory obligations. A data steward may share some responsibilities with a data custodian.\n\nThe overall objective of a data steward is data quality, in regard to the key/critical data elements existing within a specific enterprise operating structure, of the elements in their respective domains. This includes capturing/documenting (meta)information for their elements (such as: definitions, related rules/governance, physical manifestation, related data models, etc. With most of these properties being specific to an attribute/concept relationship), identifying owners/custodians/various responsibilities, relations insight pertaining to attribute quality, aiding with project requirement data facilitation and documentation of capture rules. \n\nData stewards begin the stewarding process with the identification of the elements which they will steward, with the ultimate result being standards, controls and data entry. The steward works closely with business glossary standards analysts (for standards), with data architect/modelers (for standards), with DQ analysts (for controls) and with operations team members (good-quality data going in per business rules) while entering data.\n\nData stewardship roles are common when organizations attempt to exchange data precisely and consistently between computer systems and to reuse data-related resources. Master data management often makes references to the need for data stewardship for its implementation to succeed. Data stewardship must have precise purpose, fit for purpose or fitness.\n\nA data steward ensures that each assigned data element:\n\nResponsibilities of data stewards vary between different organisations and institutions. For example, at Delft University of Technology, data stewards are perceived as the first contact point for any questions related to research data. They also have subject-specific background allowing them to easily connect with researchers and to contextualise data management problems to take into account disciplinary practices.\n\nDepending on the set of data stewardship responsibilities assigned to an individual, there are 4 types of data stewards typically found within an organization:\n\n\nSystematic data stewardship can foster fitness through:\n\n\nAssignment of each data element to a person sometimes seems like an unimportant process. But many groups have found that users have greater trust and usage rates in systems where they can contact a person with questions on each data element.\n\nDelft University of Technology (TU Delft) offers an example of data stewardship implementation at a research institution. In 2017 the Data Stewardship Project was initiated at TU Delft to address research data management needs in a disciplinary manner across the whole campus. Dedicated data stewards with subject-specific background were appointed at every TU Delft faculty to support researchers with data management questions and to act as a linking point with the other institutional support services. The project is coordinated centrally by TU Delft Library, and it has its own website, blog and a YouTube channel.\n\nThe EPA metadata registry furnishes an example of data stewardship. Note that each data element therein has a \"POC\" (point of contact).\n\nA new market for data governance applications is emerging, one in which both technical and business staff — stewards — manage policies. These new applications, like previous generations, deliver a strong business glossary capability, but they don't stop there. Vendors are introducing additional features addressing the roles of business in addition to technical stewards' concerns.\n\nInformation stewardship applications are business solutions used by business users acting in the role of information steward (interpreting and enforcing information governance policy, for example). These developing solutions represent, for the most part, an amalgam of a number of disparate, previously IT-centric tools already on the market, but are organized and presented in such a way that information stewards (a business role) can support the work of information policy enforcement as part of their normal, business-centric, day-to-day work in a range of use cases.\n\nThe initial push for the formation of this new category of packaged software came from operational use cases — that is, use of business data in and between transactional and operational business applications. This is where most of the master data management (MDM) efforts are undertaken in organizations. However, there is also now a faster-growing interest in the new data lake arena for more analytical use cases.\n\n\n"}
{"id": "456266", "url": "https://en.wikipedia.org/wiki?curid=456266", "title": "Direct-drive turntable", "text": "Direct-drive turntable\n\nA direct-drive turntable is one of the three main phonograph designs currently being produced. The other styles are the belt-drive turntable and the idler-wheel type. Each name is based upon the type of coupling used between the platter of the turntable and the motor.\n\nDirect-drive turntables are currently the most popular phonographs, due to their widespread use for turntablism in DJ culture. Panasonic's Technics series were the first direct-drive turntables, and remain the most popular series of turntables.\n\nIn a direct-drive turntable the motor is located directly under the center of the platter and is connected to the platter directly. It is a significant advancement over older belt-drive turntables, which are unsuitable for turntablism, since they have a slow start-up time, and are prone to wear-and-tear and breakage, as the belt would break from backspinning or scratching. A direct-drive turntable eliminates belts, and instead employs a motor to directly drive a platter on which a vinyl record rests. This makes scratching possible, as the motor would continue to spin at the correct RPM even if the DJ wiggles the record back and forth on the platter.\n\nOn the other hand, direct-drive turntables may suffer from vibration due to the motor, which is less of an issue for belt-drive turntables. However, in recent years, shock-absorbing (less dense) material, placed between the motor and platter, has been used to cut back on vibrations. The torque on direct-drive turntables is usually much higher than on belt drive models. This means the platter speed is less susceptible to outside forces (stylus, hand). Higher torque also means the platter will accelerate to its proper speed faster so less distortion is heard when the record begins to play.\n\nSome direct-drive turntables further reduce the separation of motor and platter by using the platter itself as the rotor in the turntable's synchronous motor. This means that there is no motor, per se, in the turntable - the platter is entirely driven by the magnetic field induced by the turntable's stator.\n\nIn all turntables a motor spins a metal disk at a constant speed. On top of the rotating disk (platter) is a mat and on top of the mat records are placed to be played. In the past rubber mats were used to hold the record in place so that it would not rotate independently of the platter. Nowadays slipmats are used to reduce the friction between the spinning platter and record, and is often made of a felt-like material. This way a DJ can scratch the record while the platter continues to spin underneath. In direct-drive turntables, the slipmat also helps isolate the record from motor vibrations that would be picked up by the stylus.\n\nMany turntables also include a pitch control, for fine tuning to the correct speed, used in conjunction with a strobe light, plus it also allows a DJ to mix using a technique known as beatmatching. From the late 1990s onwards manufacturers such as Vestax started to include other electronic controls such as reverse, and \"nudge\".\n\nDJs and turntablists use all the above functions to assist them in musical performances.\n\nThe first direct-drive turntable was invented by Shuichi Obata, an engineer at Matsushita (now Panasonic), based in Osaka, Japan. It eliminated belts, and instead employed a motor to directly drive a platter on which a vinyl record rests. In 1969, Matsushita released it as the SP-10, the first direct-drive turntable on the market, and the first in their influential Technics series of turntables. In 1971, Matsushita released the Technics SL-1100. Due to its strong motor, durability, and fidelity, it was adopted by early hip hop artists.\n\nA forefather of turntablism was DJ Kool Herc, an immigrant from Jamaica to New York City. He introduced turntable techniques from Jamaican dub music, while developing new techniques made possible by the direct-drive turntable technology of the Technics SL-1100, which he used for the first sound system he set up after emigrating to New York. The signature technique he developed was playing two copies of the same record on two turntables in alternation to extend the b-dancers' favorite section, switching back and forth between the two to loop the breaks to a rhythmic beat.\n\nThe most influential turntable was the Technics SL-1200, which was developed in 1971 by a team led by Shuichi Obata at Matsushita, which then released it onto the market in 1972. It was adopted by New York City hip hop DJs such as Grand Wizzard Theodore and Afrika Bambaataa in the 1970s. As they experimented with the SL-1200 decks, they developed scratching techniques when they found that the motor would continue to spin at the correct RPM even if the DJ wiggled the record back and forth on the platter. Since then, turntablism spread widely in hip hop culture, and the SL-1200 remained the most widely used turntable in DJ culture for the next several decades.\n\nTechnics also introduced the first direct-drive tangential-arm turntable, the model SL-10, in 1981.\n"}
{"id": "474796", "url": "https://en.wikipedia.org/wiki?curid=474796", "title": "Electronic road pricing", "text": "Electronic road pricing\n\nElectronic road pricing is the technology and mechanism to toll drivers by electronic means in an area within which road pricing is imposed. It was first put into trial in Hong Kong in the early 1980s, but its implementation was aborted. Current systems include Singapore, Singapore; London, England, United Kingdom; and Stockholm, Sweden.\n"}
{"id": "45037556", "url": "https://en.wikipedia.org/wiki?curid=45037556", "title": "FOREO", "text": "FOREO\n\nFOREO is a Swedish multi-national beauty brand established and headquartered Stockholm, Sweden. Founded in 2013, the company produces facial cleansing brushes, sonic electric toothbrushes, cleansers, and eye massagers for the consumer and professional markets.\n\nFOREO began in 2013, and in three years secured a presence in department stores, perfumery chains and beauty e-commerce operators across the globe such as Douglas, Sephora, Harvey Nichols, Neiman Marcus, Barneys, Bergdorf Goodman, Net-a-Porter, Selfridges, Globus and Jelmoli, with a total of over 6.000 placements in more than 50 countries. In 2013, FOREO launched the Luna™, a facial cleansing brush. The Luna™ eschewed abrasive nylon bristles in favor of silicone that could potentially avoid excessive exfoliation.\n\nPaul Peros started his professional career in Water & Environmental Technologies as a Watergy Business Manager after a BA in physics from UCLA and MBA from IMD Business School. From there he joined GEA Management Consulting where he managed projects around the globe for 12 years.\n\nFOREO opened FOREO Inc. in 2013 and FOREO Adria in 2014. FOREO opened a flagship store in Paris in 2016.\n\nFOREO's most popular and recognizable product series remains the LUNA, launched in 2013. The product range also includes the IRIS eye massager, the ISSA electric toothbrush launched in 2014, the ESPADA blue light acne treatment in 2017 and FOREO Day and Night face cleansers.\n\n2017\n\n2016\n\n2015\n\n2014\n"}
{"id": "47339", "url": "https://en.wikipedia.org/wiki?curid=47339", "title": "Fishing line", "text": "Fishing line\n\nA fishing line is a cord used or made for angling. Important parameters of a fishing line are its length, material, and weight (thicker lines are more visible to fish). Factors that may determine what line an angler chooses for a given fishing environment include breaking strength, knot strength, UV resistance, castability, limpness, stretch, abrasion resistance, and visibility. Most modern lines are made from nylon or silk.\n\nFish are caught with a fishing line by encouraging a fish to bite on a fish hook. A fish hook will pierce the mouthparts of a fish and is normally barbed to make escape less likely. Another method is to use a gorge, which is buried in the bait such that it would be swallowed end first. The tightening of the line would fix it cross-wise in the quarry's stomach or gullet and so the capture would be assured.\n\nFishing with a hook and line is called angling. In addition to the use of the hook and line used to catch a fish, a heavy fish may be landed by using a landing net or a hooked pole called a gaff.\n\nTrolling is a technique in which a fishing lure on a line is drawn through the water. Trolling from a moving boat is a technique of big-game fishing and is used to catch large open-water species such as tuna and marlin. Trolling is also a freshwater angling technique often used to catch salmon, northern pike, muskellunge and walleye. This technique allows anglers to cover a large body of water in a short time.\n\nLong-line fishing, also known as a trot line is a commercial fishing technique that uses many baited hooks hanging from a single line.\n\nSnagging is a technique where the object is to hook the fish in the body. Generally, a large treble hook with a heavy sinker is cast into a river containing a large amount of fish, such as a Salmon, and is quickly jerked and reeled in. Due to the often illegal nature of this method some practitioners have added methods to disguise the practice, such as adding bait or piercing the jerking motion.\n\nAs written in 1667 by Samuel Pepys, the fishing lines in his time were made from catgut. Later, silk fishing lines were used around 1724.\n\nModern fishing lines intended for spinning, spin cast, or bait casting reels are almost entirely made from artificial substances, including nylon, polyvinylidene fluoride (PVDF, also called fluorocarbon), polyethylene, Dacron and UHMWPE (Honeywell's Spectra or Dyneema). The most common type is \"monofilament\", made of a single strand. Fishermen often use monofilament because of its buoyant characteristics and its ability to stretch under load. The line stretch has advantages, such as dampening the force when setting the hook and when fighting strong fish. On very far distances the dampening may become a disadvantage. Recently, other alternatives to standard nylon monofilament lines have been introduced made of copolymers or fluorocarbon, or a combination of the two materials. Fluorocarbon fishing line is made of the fluoropolymer PVDF and it is valued for its refractive index, which is similar to that of water, making it less visible to fish. Fluorocarbon is also a denser material, and therefore, is not nearly as buoyant as monofilament. Anglers often utilize fluorocarbon when they need their baits to stay closer to the bottom without the use of heavy sinkers. There are also braided fishing lines, \"cofilament\" and \"thermally fused\" lines, also known as 'superlines' for their small diameter, lack of stretch, and great strength relative to standard nylon monofilament lines. Braided, thermally fused, and chemically fused varieties of 'superlines' are now readily available.\n\n\"Fly lines\" consist of a tough braided or monofilament core, wrapped in a thick waterproof plastic sheath, often of polyvinyl chloride (PVC). In the case of floating fly lines, the PVC sheath is usually embedded with many 'microballoons' or air bubbles, and may also be impregnated with silicone or other lubricants to give buoyancy and reduce wear. In order to fill up the reel spool and ensure an adequate reserve in case of a run by a powerful fish, fly lines are usually attached to a secondary line at the butt section, called backing. Fly line backing is usually composed of braided dacron or gelspun monofilaments. All fly lines are equipped with a leader of monofilament or fluorocarbon fishing line, usually (but not always) tapered in diameter, and referred to by the 'X-size' (0X, 2X, 4X, etc.) of its final tip section, or tippet. Tippet size is usually between 0X and 8X, where 0X is the thickest diameter and 8X is the thinnest. There are exceptions to this, and tippet sizes do exist outside of the 0X-8X paramter. \n\"Tenkara lines\" are special lines used for the fixed-line fishing method of tenkara. Traditionally these are furled lines the same length as the tenkara rod. Although original to Japan, these lines are similar to the British tradition of furled leader. They consist of several strands being twisted together in decreasing numbers toward the tip of the line, thus creating a taper that allows the line to cast the fly. It serves the same purpose as the fly-line, to propel a fly forward. They may be tied of various materials, but most commonly are made of monofilament.\n\n\"Wire lines\" are frequently used as leaders to prevent the fishing line from being severed by toothy fish. Usually braided from several metal strands, wire lines may be made of stainless steel, titanium, or a combination of metal alloys coated with plastic.\n\nStainless steel line leaders provide:\n\n- bite protection - it is extremely hard for fish to cut the steel wire, regardless of jaw and teeth strength and sharpness,\n\n- abrasion resistance - sharp rocks and objects can damage other lines, while steel wire can cut through most of the materials,\n\n- single wire (single strand) leaders are not as flexible as multi strand steel wire, but are extremely strong and tough,\n\n- multi strand steel wire leaders are very flexible, but are somewhat more abrasive and more damage prone than single strand wires.\n\nTitanium fishing fishing leaders are actually titanium-nickel alloys that have several very important features:\n\n- titanium leader lines are very flexible, regardless if they are single or multi strand lines/wires,\n\n- these lines are very elastic - they can stretch up to 10% without permanent damage to the line itself - perfect for hook setting,\n\n- these lines are knottable just as nylon monofilament lines,\n\n- surface is rather hard and abrasion resistant - great for fishing toothy fish,\n\n- titanium wire is corrosion resistant and can last for a long time, even surpassing stainless steel wires,\n\n- due to the strength and elasticity, titanium wires are almost entirely kink-proof.\n\nCopper, monel and lead core fishing lines are used as heavy trolling main lines, usually followed with fluorocarbon line near the lure or bait with fishing swivel between the lines. Due to their high density, these fishing lines sink rapidly in water and they require less line for achieving desired trolling depth. On the other hand, these lines are relatively thick for desired strength, especially when compared with braided fishing lines and they often require reels with larger spools.\n\nDiscarded monofilament fishing line takes up to 600 years to decompose. There have been several types of biodegradable fishing lines developed to minimize the impact on the environment.\n"}
{"id": "19596538", "url": "https://en.wikipedia.org/wiki?curid=19596538", "title": "Generalized Automation Language", "text": "Generalized Automation Language\n\nTwo forms of GAL are available. The first is General Automation Language for device automation and the second is Generalized Automation Language (GAL) which a very high level programming language for MVS based systems such as OS/390 and z/OS.\n\nDeveloped by iLED to provide a common language for standardising automation and control of devices in the residential (Home Automation) and commercial control environments. The language provides a standardised method of communicating to/from controlled/controlling devices. At each device, GAL is converted into the machine specific protocol and medium. An example is the control of a DVD player. The GAL command will be codice_1. The GAL device will then convert this to the discrete IR command to switch on the DVD Player.\n\nDeveloped by Expans Systems to provide features and constructs that enable the programmer to intercept systems events and schedule responses, as implemented via their product Automan. Somewhat akin to BASIC, GAL enables systems programmers and operators to define logic to apply to systems messages as they flow through a multi-system (sysplex) environment. GAL also enables the programmer to define events that have occurred in the past, by intercepting Action Message Retention Facility (AMRF) messages. The language has built-in constructs to obtain the age of a retained message and make decisions about its fate depending on age. GAL can be used to write new systems commands, by intercepting and interpreting anything that is entered into an Operator Console. GAL uses keywords such as names of days of the week, names of month etc. to automatically schedule events in the system. Like REXX, GAL is both an interpretive language and a compiled language. GAL statements can be entered to the interpreter on the fly, or entire automation scenarios can be predefined, such as the logic to define unattended operations of a system, and can be compiled offline, using the compiler program GALCOMP.\n\nGAL implements comparison by IF statements, setting of variables, by the LET statement and subroutine calls. GAL allows the programmer to break into REXX, and Assembler where it is needed. The very high level nature of GAL is exemplified by the EMAIL statement, which enables the programmer to send an email alert when an event is detected that requires human intervention. Assuming that this message event requires an alert to be sent to a default recipient:\n\nGAL uses text capture and replacement facilities. In this simple example, the text of the system message is captured into a variable and the text in that variable is then used as the subject of the email. The message in the body of the email is the text in quotes following the subject.\n\nGAL allows for cross systems(IBM XCF) queries to be issued by simple IF statements, without regard for the underlying internal processes required to perform the cross systems communications. It is simply a matter of identifying one or more systems that are to be tested.\n\nFor instance to check if a job is currently running in a partner system:\n\n\n\n"}
{"id": "1239770", "url": "https://en.wikipedia.org/wiki?curid=1239770", "title": "Hatchery", "text": "Hatchery\n\nA hatchery is a facility where eggs are hatched under artificial conditions, especially those of fish or poultry. It may be used for ex-situ conservation purposes, i.e. to breed rare or endangered species under controlled conditions; alternatively, it may be for economic reasons (i.e. to enhance food supplies or fishery resources).\n\nFish hatcheries are used to cultivate and breed a large number of fish in an enclosed environment. Fish farms use hatcheries to cultivate fish to sell for food, or ornamental purposes, eliminating the need to find the fish in the wild and even providing some species outside their natural season. They raise the fish until they are ready to be eaten or sold to aquarium stores. Other hatcheries release the juvenile fish into a river, lake or the ocean to support commercial, tribal, or recreational fishing or to supplement the natural numbers of threatened or endangered species, a practice known as fish stocking.\n\nResearchers have raised concerns about hatchery fish potentially breeding with wild fish. Hatchery fish may in some cases compete with wild fish. In the United States and Canada, there have been several salmon and steelhead hatchery reform projects intended to reduce the possibility of negative impacts from hatchery programs. Most salmon and steelhead hatcheries are managed better and follow up to date management practices to ensure any risks are curtailed.\n\nPoultry hatcheries produce a majority of the birds consumed in the developed world including chickens, turkeys, ducks, geese, and some other minor bird species. A few poultry hatcheries specialize in producing birds for sale to backyard poultry keepers, hobby farmers, and people who are interested in competing with their birds at poultry shows. These hatcheries produce chicks of several different breeds and varieties, often including some heritage or endangered breeds.\n\nLarger poultry hatcheries are related to industrial poultry meat or egg production. This is a multibillion dollar industry, with highly regimented production systems used to maximize bird size or egg production versus feed consumed. Generally large numbers are produced at one time so the resulting birds are uniform in size and can be harvested (for meat) or brought into production (for eggs) at the same time. A large hatchery produces 15 million chicks annually.\n\nPoultry generally start with naturally (most species) or artificially (turkeys and Cornish-related chicken breeds) inseminated hens that lay eggs; the eggs are cleaned and shells are checked for soundness before being put into the incubators. Incubators control temperature and humidity, and turn the eggs until just before they hatch. Three days before the eggs are scheduled to hatch, they are moved into a hatcher unit, where they are no longer turned so the embryos have time to get properly oriented for their exit from the shell, and the temperature and humidity are optimum for hatching. Once the eggs hatch and the chicks are a few days old, they are often vaccinated.\n\n\n"}
{"id": "31499475", "url": "https://en.wikipedia.org/wiki?curid=31499475", "title": "Heated humidified high-flow therapy", "text": "Heated humidified high-flow therapy\n\nHeated humidified high-flow (HHHF) therapy is a type of respiratory support method that delivers a high flow (liters per minute) of medical gas to a patient through an interface (nasal cannulae) intended to create a wash-out of the upper airway. The applied gas is heated to best match human body temperature (37 Celsius) and humidified targeting ideal body saturation vapor pressure.\n\nHigh-flow therapy is useful in patients that are spontaneously breathing but have an increased work of breathing. Conditions such as general respiratory failure, asthma exacerbation, COPD exacerbation, bronchiolitis, pneumonia, and congestive heart failure are all possible situations where high-flow therapy may be indicated. HHHF has been used in spontaneously breathing patients with during general anaesthesia to facilitate surgery for airway obstruction. \n\nHigh-flow therapy has shown to be useful in neonatal intensive care settings for premature infants with Infant respiratory distress syndrome, as it prevents many infants from needing artificial ventilation via intubation, and allows safe respiratory management at lower FiO2 levels, and thus reduces the risk of retinopathy of prematurity and oxygen toxicity.\n\nDue to the decreased stress of effort needed to breathe, the neonatal body is able to spend more time utilizing metabolic efforts elsewhere, which causes decreased days on a mechanical ventilator, faster weight gain, and overall decreased hospital stay entirely.\n\nHigh flow therapy has been successfully implemented in infants and older children. The cannula improves the respiratory distress, the oxygen saturation, and the patient's comfort. Its mechanism of action is the application of mild positive airway pressure and lung volume recruitment.\n\nHFT, the clinician can deliver higher FiO2 to the patient than is possible with typical oxygen delivery therapy without the use of a non-rebreather mask or tracheal intubation. Heated humidification of the respiratory gas facilitates secretion clearance and decreases the development of bronchial hyper-response symptoms. Some patients requiring respiratory support for bronchospasm benefit using air delivered by HFT without additional oxygen. HFT is useful in the treatment of sleep apnea. During use of HFT the patient can speak. Most patients find HFT more comfortable than using oxygen masks. As this is a non-invasive therapy, it avoids the risk of ventilator-associated pneumonia in situations where it can supplant the use of a ventilator.\n\nOxygenation is achieved by providing an increased FiO2 in the air flow to the patient. The constant flush of the upper airway creates a reservoir that reduces room-air entrainment to such an amount that it becomes a true fraction of inspired oxygen as set by the device.\n\nThrough a nasal cannula a high-flow system delivers flows that approach (and can meet) total respiratory demand. This flow, being delivered though a small diameter delivery system and small-bore nasal cannula allows the flow that would traditionally move slowly through the upper airway to move quickly and maintain a constant stream of fresh gas which effectively washes out upper airway dead space.\n\nThis constant stream of fresh gas flow creates an environment that assists exhalation effort by flushing the exhaled air out to maintain this reservoir of fresh air ready to be inhaled.\n\nThe higher the flow, the more important proper humidification and conditioning of the flow becomes. Without humidity, the oxygenation and ventilation effects of high-flow therapy would be quickly overcome by the negative impact that dry air has on lung tissue.\n\nNasal cannula used for medical gas delivery are usually limited to delivery of 1–6 liters of flow per minute. The percent oxygen inhaled by the patient (FiO2), usually ranges roughly 24–35% as the pure oxygen delivered from the cannula is diluted by entrainment of ambient air (21% oxygen). Flow rates for delivery of oxygen using typical nasal cannula are limited because medical oxygen is anhydrous, and when delivered from a pressurized source the gas cools as it expands with the drop to atmospheric pressure. Delivery of cold dry gas is irritating to the respiratory mucosa, can cause drying and bleeding of the nasal mucosa and can increase metabolic demand by cooling the body.\n\nEven with quiet breathing, the inspiratory flow rate at the nares of an adult usually exceeds 12 liters a minute, and can exceed 30 liters a minute for someone with mild respiratory distress. Traditional oxygen therapy is limited to six liters a minute and does not begin to approach the inspiratory demand of an adult and therefore the oxygen is then diluted with room air during inspiration.\n\nPrior to the advent of HFT, when increased FiO2 was required for respiratory support; special face masks or intubation was required. With High Flow Therapy, the goal is to deliver a respiratory gas flow volume sufficient to meet or exceed the patient's inspiratory flow rate. The gas is heated and humidified to condition the gas as increased flows would be detrimental to tissue if left dry and cool.\n\nHFT, a source of oxygen is usually blended with compressed air. Hospitals usually have 50 psi compressed oxygen and air available for therapeutic use. This allows the delivery of air or blends of air and oxygen with the use of an oxygen blender. The gas is then heated, generally to about 37 °C, and humidified to near 100% RH using a humidifier. The gas is transported to the patient through a heated delivery tube to prevent cooling and condensation of the water vapor that has been added to the respiratory gas(es).\n\nHFT requires the use of nasal cannulae and a system designed to deliver high flow rates and the pressure generated to do so. At the same time the nasal cannula must be small enough that they do not occlude more than 50% of the nares, as this allows flow to have multiple points of exit for a continuous airway flush effect.\n\nVapotherm introduced the concept of heated humidified high flow therapy via nasal cannula in 1999 after being originally developed for use in race horses.\n"}
{"id": "19861142", "url": "https://en.wikipedia.org/wiki?curid=19861142", "title": "IEEE 1451", "text": "IEEE 1451\n\nIEEE 1451 is a set of smart transducer interface standards developed by the Institute of Electrical and Electronics Engineers (IEEE) Instrumentation and Measurement Society’s Sensor Technology Technical Committee describing a set of open, common, network-independent communication interfaces for connecting transducers (sensors or actuators) to microprocessors, instrumentation systems, and control/field networks. One of the key elements of these standards is the definition of Transducer electronic data sheets (TEDS) for each transducer. The TEDS is a memory device attached to the transducer, which stores transducer identification, calibration, correction data, and manufacturer-related information. The goal of the IEEE 1451 family of standards is to allow the access of transducer data through a common set of interfaces whether the transducers are connected to systems or networks via a wired or wireless means.\n\nA transducer electronic data sheet (TEDS) is a standardized method of storing transducer (sensors or actuators) identification, calibration, correction data, and manufacturer-related information. TEDS formats are defined in the IEEE 1451 set of smart transducer interface standards developed by the IEEE Instrumentation and Measurement Society's Sensor Technology Technical Committee that describe a set of open, common, network-independent communication interfaces for connecting transducers to microprocessors, instrumentation systems, and control/field networks.\n\nOne of the key elements of the IEEE 1451 standards is the definition of TEDS for each transducer. The TEDS can be implemented as a memory device attached to the transducer and containing information needed by a measurement instrument or control system to interface with a transducer. TEDS can, however, be implemented in two ways. First, the TEDS can reside in embedded memory, typically an EEPROM, within the transducer itself which is connected to the measurement instrument or control system. Second, a virtual TEDS can exist as a data file accessible by the measurement instrument or control system. A virtual TEDS extends the standardized TEDS to legacy sensors and applications where embedded memory may not be available.\n\nThe 1451 family of standards includes:\n\n\n\n"}
{"id": "28865703", "url": "https://en.wikipedia.org/wiki?curid=28865703", "title": "InfinityDB", "text": "InfinityDB\n\nInfinityDB is an all-Java embedded database engine and client/server DBMS with an extended java.util.concurrent.ConcurrentNavigableMap interface (a subinterface of java.util.Map) that is deployed in handheld devices, on servers, on workstations, and in distributed settings. The design is based on a proprietary lockless, concurrent, B-tree architecture that enables client programmers to reach high levels of performance without risk of failures. \n\nA new Client/Server version 5.0 is in alpha testing, wrapping the established embedded version to provide shared access via a secure, remote server.\n\nIn the embedded system, data is stored to and retrieved from a single embedded database file using the InfnityDB API that allows direct access to the variable length item spaces. Database client programmers can construct traditional relations as well as specialized models that directly satisfy the needs of the dependent application. There is no limit to the number of items, database size, or JVM size, so InfinityDB can function in both the smallest environment that provides random access storage and can be scaled to large settings. Traditional relations and specialized models can be directed to the same database file. InfinityDB can be optimized for standard relations as well as all other types of data, allowing client applications to perform at a minimum of one million operations per second on a virtual, 8-core system.\n\nAirConcurrentMap, is an in-memory map that implements the Java ConcurrentMap interface, but internally it uses a multi-core design so that its performance and memory make it the fastest Java Map when ordering is performed and it holds medium to large numbers of entries. AirConcurrentMap iteration is faster than any Java Map iterators, regardless of the specific map type.\n\nInfinityDB can be accessed as an extended standard java.util.concurrent.ConcurrentNavigableMap, or via a low-level 'ItemSpace' API. The ConcurrentNavigableMap interface is a subinterface of java.util.Map but has special ordering and concurrency methods: this is the same API as java.util.concurrent.ConcurrentSkipListMap. Maps may be nested to form complex structures. The Maps have the standard semantics, but work internally on a 'tuple space', while the Maps are not actually stored but are helpers, each representing nothing more than an immutable tuple prefix. Maps may be created dynamically at high speed if needed for access, and are thread-safe and multi-core concurrent. The key and value types available include all Java primitive data types, Dates, Strings, small char or byte arrays, 'ByteStrings', 'huge array' indexes, Character Long Objects or Binary Long Objects, plus the special-purpose types 'EntityClass' and 'Attribute'. Maps may be multi-valued. Applications may choose to use the Map-based access alone and may mix in lower-level 'ItemSpace' access over the same tuples, as the Map access is just a wrapper and there is no tuple-level distinction.\n\nThe data primitives are called 'components' and they are atomic. Components can be concatenated into short composites called 'Items' which are the unit of storage and retrieval. Higher-level structures that combine these Items include unlimited size records of an unlimited number of columns or attributes, with complex attribute values of unlimited size. Keys may be a composition of components. Attribute values can be ordered sets of composite components, character large objects (CLOB's), binary large objects (BLOB's), or unlimited sparse arrays. Other higher-level structures built of multiple Items include key/value associations like ordered maps, ordered sets, Entity-Attribute-Value nets of quadruples, trees, DAG's, taxonomies, or full-text indexes. Mixtures of these can occur along with custom client-defined structures.\n\nAn 'ItemSpace' represents the entire database, and it is a simple ordered set of Items, with no other state. An Item is actually stored with each component encoded in variable-length binary form in a char array, with components being self-describing in a standard format which sorts correctly. Programmers deal with the components only as primitives, and the stored data is strongly typed. Data is not stored as text to be parsed with weak typing as in JSON or XML, nor is it parsed out of programmer-defined binary stream representations. There are no custom client-devised binary formats that can grow brittle, and which can have security, documentation, upgrade, testing, versioning, scaling, and debugging problems, such as is the case with Java Object serialization.\n\nAll access to the system is via a few basic methods that can store or retrieve in order one variable-length 'Item' or 'tuple' at a time at a speed which is on the order of 1M operations/second aggregated over multiple threads when in memory. The operations are either the standard Map API for codice_1, codice_2, iterators, and so on, or at the lower level, codice_3, codice_4, codice_5, codice_6, codice_7, codice_8, and codice_9. Typical Items are about 30 bytes uncompressed in memory, but LOB's for example use 1 KB Items. Because each operation affects only one Item, small data structures are fast to access. This is in contrast to chunked access, such as for example formatting and parsing entire JSON or XML texts or entire Java Object serialization graphs. The space and performance scaling of an ItemSpace is smooth as any size of client-imposed multi-Item structure is created, grows, shrinks, or disappears. On-storage performance is like any block-oriented B-tree, with blocks of about 4 KB, which is \"O\"(log(\"n\")) per access. There is a block cache of 2.5 MB by default, which is of unlimited size but which is often about 100 MB. The cache grows only as needed.\n\nFor performance and efficiency, the Items are stored inside a single B-tree prefix-compressed and variable length as an uninterpreted sequence of bytes for further compression. The B-tree may typically grow to the 100 GB's range but has no limits. There is only one file, so there is no log or other files to write to and flush. InfinityDB minimizes the size of its database file through four types of compression (prefix, suffix, zlib, and UTF-8). \n\nschema upgrade when structures are extended or modified is done by adding or removing Items in new ways at runtime, and there are no upgrade scripts - hence the data model is NoSQL and schemaless. Because an empty or non-existent piece of data requires no space by virtue of having no Items associated with it, it is possible to extend the structure simply by adding Items of new kinds. If Items are removed, all logical and physical space is immediately reclaimed.\n\nFor example, backwards compatibility with older databases in a relational model while making a single-valued attribute into a multi-value attribute can be achieved simply by storing more values (one per Item) alongside the existing one without deleting the existing one, since every attribute is already logically multi-value and is logically composite. There is little code change and no internal or external database structure change. If a value is deleted and it is the last value of a multi-value attribute, all of its space is recovered immediately. In other words, every attribute can logically store an ordered set of optionally composite values, but there is no cost for the generality in common cases, and no cost at all for null values.\n\nIf a new table is to be added, it is created at the moment its first row is inserted. The first row appears only when the first attribute value is added to it. Forwards compatibility with future databases is possible because future databases may have new tables or any other structure that can be discovered and used, can be simply ignored or can be handled opaquely by older code.\n\nBoth global 'ACD' and per-thread 'ACID' transactions are provided. Each InfinityDB instance stores data into a single database file and does not require additional log or rollback files. In the event of any catastrophe except power failure or other hardware malfunction, the database is guaranteed to be consistent with the status as of completion of the last global commit. Recovery after abrupt termination is immediate and requires no slow log replay. Bulk loading is globally transactional with unlimited data size, and is concurrent with all other uses. Global transactions do not provide inter-thread isolation, so the semantics are 'ACD' (rather than 'ACID'). Alternatively, ACID transactions employ optimistic locking to allow inter-thread isolation.\n\nAs data structures grow and shrink, freed space is reclaimed immediately and made available for other structures. Systems can run indefinitely without gradual space leaks or long interruptions during garbage reclamation phases. When a data structure becomes empty, all of its space is recycled, rather than leaving behind 'tombstones' or other place holders. For example, a possibly very large multi-value attribute may shrink to one value, becoming as efficient as any single-valued attribute, and if that last value is deleted, all space for it is reclaimed, including the space for the attribute it was attached to, and if a row has only attributes with no values, the row is reclaimed completely as well. If a table loses all of its rows, the space for the table is reclaimed. Any size or type of data structure has this property. There are no reference counters, hence any type of graph is incrementally collected automatically.\n\nInfinityDB Version 5.0 (in alpha-testing state) features:\n\n\nInfinityDB Version 4.0 features:\n\n\nAirConcurrentMap Version 3 features:\n\n\nFor both products:\n\n\nRoger L. Deran designed and developed the Infinity Database Engine over 20 years ago and holds US Patent 5283894. The Infinity Database Engine was first deployed in Intel 8088 assembly language in the ROSCOR sports video editor (RSVE) that was licensed to NFL teams in the 1980s. Lexicon purchased the RSVE in 1989, and greatly expanded its deployment to all types of professional and college sports. Java version 2.0 added transactionality, and version 3.0 added concurrency features which are patent pending, and apply to InfinityDB as well as AirConcurrentMap. Infinity DB remains in active use in thousands of sites of various kinds, while AirConcurrentMap is new.\n\nUses of the all-JAVA InfinityDB, marketed by Boiler Bay Inc. since 2002, include:\n\n"}
{"id": "152772", "url": "https://en.wikipedia.org/wiki?curid=152772", "title": "Intensive farming", "text": "Intensive farming\n\nIntensive farming involves various types of agriculture with higher levels of input and output per cubic unit of agricultural land area. It is characterized by a low fallow ratio, higher use of inputs such as capital and labour, and higher crop yields per cubic unit land area. This contrasts with traditional agriculture, in which the inputs per unit land are lower. The term \"intensive\" involves various meanings, some of which refer to organic farming methods (such as biointensive agriculture and French intensive gardening), and others that refer to nonorganic and industrial methods. Intensive animal farming involves either large numbers of animals raised on limited land, usually concentrated animal feeding operations (CAFOs), often referred to as factory farms, or managed intensive rotational grazing (MIRG), which has both organic and non-organic types. Both increase the yields of food and fiber per acre as compared to traditional animal husbandry. In CAFO, feed is brought to the seldom-moved animals, while in MIRG the animals are repeatedly moved to fresh forage.\n\nMost commercial agriculture is intensive in one or more ways. Forms that rely heavily on industrial methods are often called industrial agriculture, which is characterised by innovations designed to increase yield. Techniques include planting multiple crops per year, reducing the frequency of fallow years, and improving cultivars. It also involves increased use of fertilizers, plant growth regulators, and pesticides and mechanised agriculture, controlled by increased and more detailed analysis of growing conditions, including weather, soil, water, weeds, and pests. This system is supported by ongoing innovation in agricultural machinery and farming methods, genetic technology, techniques for achieving economies of scale, logistics, and data collection and analysis technology. Intensive farms are widespread in developed nations and increasingly prevalent worldwide. Most of the meat, dairy, eggs, fruits, and vegetables available in supermarkets are produced by such farms.\n\nSmaller intensive farms usually include higher inputs of labor and more often use sustainable intensive methods. The farming practices commonly found on such farms are referred to as appropriate technology. These farms are less widespread in both developed countries and worldwide, but are growing more rapidly. Most of the food available in specialty markets such as farmers markets is produced by these small holder farms.\n\nAgricultural development in Britain between the 16th century and the mid-19th century saw a massive increase in agricultural productivity and net output. This in turn supported unprecedented population growth, freeing up a significant percentage of the workforce, and thereby helped enable the Industrial Revolution. Historians cited enclosure, mechanization, four-field crop rotation, and selective breeding as the most important innovations.\n\nIndustrial agriculture arose along with the Industrial Revolution. By the early 19th century, agricultural techniques, implements, seed stocks, and cultivars had so improved that yield per land unit was many times that seen in the Middle Ages.\n\nThe industrialization phase involved a continuing process of mechanization. Horse-drawn machinery such as the McCormick reaper revolutionized harvesting, while inventions such as the cotton gin reduced the cost of processing. During this same period, farmers began to use steam-powered threshers and tractors, although they were expensive and dangerous. In 1892, the first gasoline-powered tractor was successfully developed, and in 1923, the International Harvester Farmall tractor became the first all-purpose tractor, marking an inflection point in the replacement of draft animals with machines. Mechanical harvesters (combines), planters, transplanters, and other equipment were then developed, further revolutionizing agriculture. These inventions increased yields and allowed individual farmers to manage increasingly large farms.\n\nThe identification of nitrogen, phosphorus, and potassium (NPK) as critical factors in plant growth led to the manufacture of synthetic fertilizers, further increasing crop yields. In 1909, the Haber-Bosch method to synthesize ammonium nitrate was first demonstrated. NPK fertilizers stimulated the first concerns about industrial agriculture, due to concerns that they came with serious side effects such as soil compaction, soil erosion, and declines in overall soil fertility, along with health concerns about toxic chemicals entering the food supply.\n\nThe identification of carbon as a critical factor in plant growth and soil health, particularly in the form of humus, led to so-called sustainable agriculture, as well as alternative forms of intensive agriculture that also surpassed traditional agriculture, without side effects or health issues. Farmers adopting this approach were initially referred to as \"humus farmers\", later as \"organic farmers\".\n\nThe discovery of vitamins and their role in nutrition, in the first two decades of the 20th century, led to vitamin supplements, which in the 1920s allowed some livestock to be raised indoors, reducing their exposure to adverse natural elements. Chemicals developed for use in World War II gave rise to synthetic pesticides.\n\nFollowing World War II, synthetic fertilizer use increased rapidly, while sustainable intensive farming advanced much more slowly. Most of the resources in developed nations went to improving industrial intensive farming, and very little went to improving organic farming. Thus, particularly in the developed nations, industrial intensive farming grew to become the dominant form of agriculture.\n\nThe discovery of antibiotics and vaccines facilitated raising livestock in confined animal feeding operations by reducing diseases caused by crowding. Developments in logistics and refrigeration as well as processing technology made long-distance distribution feasible.\n\nBetween 1700 and 1980, \"the total area of cultivated land worldwide increased 466%\" and yields increased dramatically, particularly because of selectively-bred, high-yielding varieties, fertilizers, pesticides, irrigation, and machinery. Global agricultural production doubled between 1820 and 1920; between 1920 and 1950; between 1950 and 1965; and again between 1965 and 1975 to feed a global population that grew from one billion in 1800 to 6.5 billion in 2002. The number of people involved in farming in industrial countries dropped, from 24 percent of the American population to 1.5 percent in 2002. In 1940, each farmworker supplied 11 consumers, whereas in 2002, each worker supplied 90 consumers. The number of farms also decreased and their ownership became more concentrated. In the year 2000 in the U.S., four companies produced 81 percent of cows, 73 percent of sheep, 57 percent of pigs, and 50 percent of chickens, which was cited as an example of \"vertical integration\" by the president of the U.S. National Farmers Union. Between 1967 and 2002, the one million pig farms in America consolidated into 114,000, with 80 million pigs (out of 95 million) produced each year on factory farms, according to the U.S. National Pork Producers Council. According to the Worldwatch Institute, 74 percent of the world's poultry, 43 percent of beef, and 68 percent of eggs are produced this way.\n\nConcerns over the sustainability of industrial agriculture, which has become associated with decreased soil quality, and over the environmental effects of fertilizers and pesticides, have not subsided. Alternatives such as integrated pest management (IPM) have had little impact because policies encourage the use of pesticides and IPM is knowledge-intensive. These concerns sustained the organic movement and caused a resurgence in sustainable intensive farming, as well as funding for the development of appropriate technology.\n\nIntensive livestock farming, also called \"factory farming\", is a term referring to the process of raising livestock in confinement at high stocking density. \"Concentrated animal feeding operations\" (CAFO), or \"intensive livestock operations\", can hold large numbers (some up to hundreds of thousands) of cows, hogs, turkeys, or chickens, often indoors. The essence of such farms is the concentration of livestock in a given space. The aim is to provide maximum output at the lowest possible cost and with the greatest level of food safety. The term is often used pejoratively. However, CAFOs have dramatically increased the production of food from animal husbandry worldwide, both in terms of total food produced and efficiency.\n\nFood and water is delivered to the animals, and therapeutic use of antimicrobial agents, vitamin supplements, and growth hormones are often employed. Growth hormones are not used on chickens nor on any animal in the European Union. Undesirable behaviors often related to the stress of confinement led to a search for docile breeds (e.g., with natural dominant behaviors bred out), physical restraints to stop interaction, such as individual cages for chickens, or physical modification such as the de-beaking of chickens to reduce the harm of fighting.\n\nThe CAFO designation resulted from the 1972 U.S. Federal Clean Water Act, which was enacted to protect and restore lakes and rivers to a \"fishable, swimmable\" quality. The United States Environmental Protection Agency (EPA) identified certain animal feeding operations, along with many other types of industry, as \"point source\" groundwater polluters. These operations were subjected to regulation.\n\nIn 17 states in the U.S., isolated cases of groundwater contamination were linked to CAFOs. For example, the ten million hogs in North Carolina generate 19 million tons of waste per year. The U.S. federal government acknowledges the waste disposal issue and requires that animal waste be stored in lagoons. These lagoons can be as large as . Lagoons not protected with an impermeable liner can leak into groundwater under some conditions, as can runoff from manure used as fertilizer. A lagoon that burst in 1995 released 25 million gallons of nitrous sludge in North Carolina's New River. The spill allegedly killed eight to ten million fish.\n\nThe large concentration of animals, animal waste, and dead animals in a small space poses ethical issues to some consumers. Animal rights and animal welfare activists have charged that intensive animal rearing is cruel to animals.\n\nOther concerns include persistent noxious odor, the effects on human health, and the role of antibiotic use in the rise of resistant infectious bacteria.\n\nAccording to the U.S. Centers for Disease Control and Prevention (CDC), farms on which animals are intensively reared can cause adverse health reactions in farm workers. Workers may develop acute and/or chronic lung disease, musculoskeletal injuries, and may catch ( zoonotic) infections from the animals.\n\nManaged Intensive Rotational Grazing (MIRG), also known as cell grazing, mob grazing, and holistic management planned grazing, is a variety of foraging in which herds or flocks are regularly and systematically moved to fresh, rested grazing areas to maximize the quality and quantity of forage growth. MIRG can be used with cattle, sheep, goats, pigs, chickens, turkeys, ducks, and other animals. The herds graze one portion of pasture, or a paddock, while allowing the others to recover. Resting grazed lands allows the vegetation to renew energy reserves, rebuild shoot systems, and deepen root systems, resulting in long-term maximum biomass production. MIRG is especially effective because grazers thrive on the more tender younger plant stems. MIRG also leaves parasites behind to die off, minimizing or eliminating the need for de-wormers. Pasture systems alone can allow grazers to meet their energy requirements, and with the increased productivity of MIRG systems, the animals obtain the majority of their nutritional needs, in some cases all, without the supplemental feed sources that are required in continuous grazing systems.\n\nPasture intensification is the improvement of pasture soils and grasses to increase the food production potential of livestock systems. It is commonly used to reverse pasture degradation, a process characterized by loss of forage and decreased animal carrying capacity which results from overgrazing, poor nutrient management, and lack of soil conservation. This degradation leads to poor pasture soils with decreased fertility and water availability and increased rates of erosion, compaction, and acidification. Degraded pastures have significantly lower productivity and higher carbon footprints compared to intensified pastures.\n\nManagement practices which improve soil health and consequently grass productivity include irrigation, soil scarification, and the application of lime, fertilizers, and pesticides. Depending on the productivity goals of the target agricultural system, more involved restoration projects can be undertaken to replace invasive and under-productive grasses with grass species that are better suited to the soil and climate conditions of the region. These intensified grass systems allow higher stocking rates with faster animal weight gain and reduced time to slaughter, resulting in more productive, carbon-efficient livestock systems.\n\nAnother technique to optimize yield while maintaining the carbon balance is the use of integrated crop-livestock (ICL) and crop-livestock-forestry (ICLF) systems, which combine several ecosystems into one optimized agricultural framework. These synergies between these systems provide benefits to pastures through optimal plant usage, improved feed and fattening rates, increased soil fertility and quality, intensified nutrient cycling, integrated pest control, and improved biodiversity. The introduction of certain legume crops to pastures increases carbon accumulation and nitrogen fixation in soils, while their digestibility helps animal fattening and reduces methane emissions from enteric fermentation. ICLF systems yield beef cattle productivity up to ten times that of degraded pastures, additional crop production from maize, sorghum, and soybean harvests, and greatly reduced greenhouse gas balances due to forest carbon sequestration.\n\nThe Green Revolution transformed farming in many developing countries. It spread technologies that had already existed, but had not been widely used outside of industrialized nations. These technologies included \"miracle seeds\", pesticides, irrigation, and synthetic nitrogen fertilizer.\n\nIn the 1970s, scientists created strains of maize, wheat, and rice that are generally referred to as high-yielding varieties (HYV). HYVs have an increased nitrogen-absorbing potential compared to other varieties. Since cereals that absorbed extra nitrogen would typically lodge (fall over) before harvest, semi-dwarfing genes were bred into their genomes. Norin 10 wheat, a variety developed by Orville Vogel from Japanese dwarf wheat varieties, was instrumental in developing wheat cultivars. IR8, the first widely implemented HYV rice to be developed by the International Rice Research Institute, was created through a cross between an Indonesian variety named “Peta” and a Chinese variety named “Dee Geo Woo Gen.”\n\nWith the availability of molecular genetics in Arabidopsis and rice the mutant genes responsible (\"reduced height (rht)\", \"gibberellin insensitive (gai1)\" and \"slender rice (slr1)\") have been cloned and identified as cellular signalling components of gibberellic acid, a phytohormone involved in regulating stem growth via its effect on cell division. Photosynthetic investment in the stem is reduced dramatically as the shorter plants are inherently more mechanically stable. Nutrients become redirected to grain production, amplifying in particular the yield effect of chemical fertilizers. \nHYVs significantly outperform traditional varieties in the presence of adequate irrigation, pesticides, and fertilizers. In the absence of these inputs, traditional varieties may outperform HYVs. They were developed as F1 hybrids, meaning seeds need to be purchased every season to obtain maximum benefit, thus increasing costs.\n\nCrop rotation or crop sequencing is the practice of growing a series of dissimilar types of crops in the same space in sequential seasons for benefits such as avoiding pathogen and pest buildup that occurs when one species is continuously cropped. Crop rotation also seeks to balance the nutrient demands of various crops to avoid soil nutrient depletion. A traditional component of crop rotation is the replenishment of nitrogen through the use of legumes and green manure in sequence with cereals and other crops. Crop rotation can also improve soil structure and fertility by alternating deep-rooted and shallow-rooted plants. A related technique is to plant multi-species cover crops between commercial crops. This combines the advantages of intensive farming with continuous cover and polyculture.\n\nCrop irrigation accounts for 70% of the world's fresh water use.\n\nFlood irrigation, the oldest and most common type, is typically unevenly distributed, as parts of a field may receive excess water in order to deliver sufficient quantities to other parts. Overhead irrigation, using center-pivot or lateral-moving sprinklers, gives a much more equal and controlled distribution pattern. Drip irrigation is the most expensive and least-used type, but delivers water to plant roots with minimal losses.\n\nWater catchment management measures include recharge pits, which capture rainwater and runoff and use it to recharge groundwater supplies. This helps in the replenishment of groundwater wells and eventually reduces soil erosion. Dammed rivers creating reservoirs store water for irrigation and other uses over large areas. Smaller areas sometimes use irrigation ponds or groundwater.\n\nIn agriculture, systematic weed management is usually required, often performed by machines such as cultivators or liquid herbicide sprayers. Herbicides kill specific targets while leaving the crop relatively unharmed. Some of these act by interfering with the growth of the weed and are often based on plant hormones. Weed control through herbicide is made more difficult when the weeds become resistant to the herbicide. Solutions include:\n\nIn agriculture, a terrace is a leveled section of a hilly cultivated area, designed as a method of soil conservation to slow or prevent the rapid surface runoff of irrigation water. Often such land is formed into multiple terraces, giving a stepped appearance. The human landscapes of rice cultivation in terraces that follow the natural contours of the escarpments, like contour ploughing, are a classic feature of the island of Bali and the Banaue Rice Terraces in Banaue, Ifugao, Philippines. In Peru, the Inca made use of otherwise unusable slopes by building drystone walls to create terraces.\n\nA paddy field is a flooded parcel of arable land used for growing rice and other semiaquatic crops. Paddy fields are a typical feature of rice-growing countries of east and southeast Asia, including Malaysia, China, Sri Lanka, Myanmar, Thailand, Korea, Japan, Vietnam, Taiwan, Indonesia, India, and the Philippines. They are also found in other rice-growing regions such as Piedmont (Italy), the Camargue (France), and the Artibonite Valley (Haiti). They can occur naturally along rivers or marshes, or can be constructed, even on hillsides. They require large water quantities for irrigation, much of it from flooding. It gives an environment favourable to the strain of rice being grown, and is hostile to many species of weeds. As the only draft animal species which is comfortable in wetlands, the water buffalo is in widespread use in Asian rice paddies.\n\nPaddy-based rice-farming has been practiced in Korea since ancient times. A pit-house at the Daecheon-ni archaeological site yielded carbonized rice grains and radiocarbon dates indicating that rice cultivation may have begun as early as the Middle Jeulmun Pottery Period (c. 3500–2000 BC) in the Korean Peninsula. The earliest rice cultivation there may have used dry-fields instead of paddies.\n\nThe earliest Mumun features were usually in naturally swampy, low-lying narrow gulleys and fed by local streams. Some Mumun paddies in flat areas were made of a series of squares and rectangles separated by bunds approximately 10 cm in height, while terraced paddies were long and irregular in shape, following the natural contours of the land at various levels.\n\nLike today, Mumun period rice farmers used terracing, bunds, canals, and small reservoirs. Some paddy-farming techniques of the Middle Mumun period (c. 850–550 BC) can be interpreted from the well-preserved wooden tools excavated from archaeological rice paddies at the Majeon-ni site. Iron tools for paddy-farming were not introduced until sometime after 200 BC. The spatial scale of individual paddies, and thus entire paddy-fields, increased with the regular use of iron tools in the Three Kingdoms of Korea Period (c. AD 300/400–668).\n\nA recent development in the intensive production of rice is System of Rice Intensification (SRI). Developed in 1983 by the French Jesuit Father Henri de Laulanié in Madagascar, by 2013 the number of smallholder farmers using SRI had grown to between 4 and 5 million.\n\nAquaculture is the cultivation of the natural products of water (fish, shellfish, algae, seaweed, and other aquatic organisms). Intensive aquaculture takes place on land using tanks, ponds, or other controlled systems, or in the ocean, using cages.\n\nSustainable intensive farming practices have been developed to slow the deterioration of agricultural land and even regenerate soil health and ecosystem services, while still offering high yields. Most of these developments fall in the category of organic farming, or the integration of organic and conventional agriculture.\n\"Organic systems and the practices that make them effective are being picked up more and more by conventional agriculture and will become the foundation for future farming systems. They won't be called organic, because they'll still use some chemicals and still use some fertilizers, but they'll function much more like today's organic systems than today's conventional systems.\"\n\nDr. Charles Benbrook\nExecutive director US House Agriculture Subcommittee\nDirector Agricultural Board - National Academy Sciences\nThe System of Crop Intensification (SCI) was born out of research primarily at Cornell University and smallholder farms in India on SRI. It uses the SRI concepts and methods for rice and applies them to crops like wheat, sugarcane, finger millet, and others. It can be 100% organic, or integrated with reduced conventional inputs.\n\nHolistic management is a systems thinking approach that was originally developed for reversing desertification. Holistic planned grazing is similar to rotational grazing but differs in that it more explicitly provides a framework for adapting to four basic ecosystem processes: the water cycle, the mineral cycle (including the carbon cycle), energy flow, and community dynamics (the relationship between organisms in an ecosystem) as equal in importance to livestock production and social welfare. By intensively managing the behavior and movement of livestock, holistic planned grazing simultaneously increases stocking rates and restores grazing land.\n\nPasture cropping involves planting grain crops directly into grassland without first applying herbicides. The perennial grasses form a living mulch understory to the grain crop, eliminating the need to plant cover crops after harvest. The pasture is intensively grazed both before and after grain production using holistic planned grazing. This intensive system yields equivalent farmer profits (partly from increased livestock forage) while building new topsoil and sequestering up to 33 tons of CO2/ha/year.\n\nThe Twelve Aprils grazing program for dairy production, developed in partnership with the USDA-SARE, is similar to pasture cropping, but the crops planted into the perennial pasture are forage crops for dairy herds. This system improves milk production and is more sustainable than confinement dairy production.\n\nIntegrated multi-trophic aquaculture (IMTA) is an example of a holistic approach. IMTA is a practice in which the by-products (wastes) from one species are recycled to become inputs (fertilizers, food) for another. Fed aquaculture (e.g. fish, shrimp) is combined with inorganic extractive (e.g. seaweed) and organic extractive aquaculture (e.g. shellfish) to create balanced systems for environmental sustainability (biomitigation), economic stability (product diversification and risk reduction), and social acceptability (better management practices).\n\nBiointensive agriculture focuses on maximizing efficiency such as per unit area, energy input, and water input. Agroforestry combines agriculture and orchard/forestry technologies to create more integrated, diverse, productive, profitable, healthy, and sustainable land-use systems.\n\nIntercropping can increase yields or reduce inputs and thus represents (potentially sustainable) agricultural intensification. However, while total yield per acre is often increased dramatically, yields of any single crop often diminish. There are also challenges to farmers relying on farming equipment optimized for monoculture, often resulting in increased labor inputs.\n\nVertical farming is intensive crop production on a large scale in urban centers, in multi-story, artificially-lit structures, using far less inputs and producing fewer environmental impacts.\n\nAn integrated farming system is a progressive, biologically-integrated sustainable agriculture system such as IMTA or Zero waste agriculture, whose implementation requires exacting knowledge of the interactions of multiple species and whose benefits include sustainability and increased profitability. Elements of this integration can include:\n\nThe challenges and issues of industrial agriculture for society, for the industrial agriculture sector, for the individual farm, and for animal rights include the costs and benefits of both current practices and proposed changes to those practices. This is a continuation of thousands of years of invention in feeding ever-growing populations.\n\"[W]hen hunter-gatherers with growing populations depleted the stocks of game and wild foods across the Near East, they were forced to introduce agriculture. But agriculture brought much longer hours of work and a less rich diet than hunter-gatherers enjoyed. Further population growth among shifting slash-and-burn farmers led to shorter fallow periods, falling yields and soil erosion. Plowing and fertilizers were introduced to deal with these problems - but once again involved longer hours of work and degradation of soil resources (Boserup, The Conditions of Agricultural Growth, Allen and Unwin, 1965, expanded and updated in Population and Technology, Blackwell, 1980.).\"\nWhile the point of industrial agriculture is to profitably supply the world at the lowest cost, industrial methods have significant side effects. Further, industrial agriculture is not an indivisible whole, but instead is composed of multiple elements, each of which can be modified in response to market conditions, government regulation, and further innovation, and has its own side-effects. Various interest groups reach different conclusions on the subject.\n\nVery roughly:\n\nBetween 1930 and 2000, U.S. agricultural productivity (output divided by all inputs) rose by an average of about 2 percent annually, causing food prices to decrease. \"The percentage of U.S. disposable income spent on food prepared at home decreased, from 22 percent as late as 1950 to 7 percent by the end of the century.\"\n\nIndustrial agriculture uses huge amounts of water, energy, and industrial chemicals, increasing pollution in the arable land, usable water, and atmosphere. Herbicides, insecticides, and fertilizers are accumulating in ground and surface waters. \"Many of the negative effects of industrial agriculture are remote from fields and farms. Nitrogen compounds from the Midwest, for example, travel down the Mississippi to degrade coastal fisheries in the Gulf of Mexico. But other adverse effects are showing up within agricultural production systems—for example, the rapidly developing resistance among pests is rendering our arsenal of herbicides and insecticides increasingly ineffective.\" Agrochemicals and monoculture have been implicated in Colony Collapse Disorder, in which the individual members of bee colonies disappear. Agricultural production is highly dependent on bees to pollinate many varieties of fruits and vegetables.\n\nA study done for the U.S. Office of Technology Assessment conducted by the UC Davis Macrosocial Accounting Project concluded that industrial agriculture is associated with substantial deterioration of human living conditions in nearby rural communities.\n\n\n\n"}
{"id": "32365316", "url": "https://en.wikipedia.org/wiki?curid=32365316", "title": "Kardex", "text": "Kardex\n\nKardex has been the name or part of the name of companies tracing back to Rand Ledger founded in 1898, which were closely associated with the development of the index card as a common business data storage device, and which were also associated with the entities that eventually became part of Unisys.\n\nKardex as a company name was introduced in 1915, subsumed in 1927 (it remained as a brand name and a division name), and revived in 1978. It is currently borne by the Kardex Group, based in Zurich, Switzerland, which makes filing system components as well as many other products for handling materials and information, mostly in physical form.\n\nThe term kardex is also a common noun (a genericized trademark), particularly in the medical records field where it is synonymous with Medication Administration Record, especially in Ireland and the United Kingdom.\n\nAmerican Ledger was founded by James H. Rand, Sr. in 1898. Rand had been a banker for many years and had come to see that existing index card systems used by clerks were inefficient. What was needed was a rationalized system using dividers, file tabs, and labels. Rand Sr. invented an improved filing system based on these concepts and founded the Rand Ledger Company to manufacture his index system, called the Visible Ledger.\n\nJames Rand, Jr. joined his father's company after being graduated from Harvard University in 1908 and ran it from 1910 through 1914. In 1915 the elder Rand re-assumed control of the company and Rand Jr., unable to reach agreement with his father on business matters, left.\n\nJames Rand, Jr. formed his own filing and index supply company, American Kardex, in 1915. Within five years, American Kardex grew to be one of the leading office supply companies in the United States. It was roughly equal in revenues to Rand Ledger, and the two companies easily dominated the American office supply market. In 1920, American Kardex had more than $1 million in gross sales. The company's products were widely used in the health care field (\"filling a kardex\" became common nomenclature for entering data into a patient's medical record), and demand in Europe was so strong that Rand soon built a factory in Germany.\n\nAs competition between American Kardex and Rand Ledger intensified, Mary Rand (James H. Rand, Sr.'s wife and James Rand, Jr.'s mother) brokered a reconciliation between father and son. In 1925, the two men agreed that American Kardex should purchase Rand Ledger. The new company, Rand Kardex, was the largest office supply company in the United States. James Rand, Sr. became the company chairman, while James Rand, Jr. was its president and general manager.\n\nJames Rand Jr. soon began expanding the company through acquisitions. He bought companies including Index Visible, Safe-Cabinet, Dalton Adding Machine, Baker-Vawter Ledger, and Library Bureau, which was probably the first company to sell filing cabinets commercially (in 1900) and may have invented them.\n\nIn its brief existence, the company was first known as Rand Kardex and then, after the acquisition of Library Bureau, as Rand Kardex Bureau.\n\nIn 1886, E. Remington and Sons sold its typewriter business and the rights to the Remington name to the Standard Typewriter Manufacturing Company which in 1902 changed its name to Remington Typewriter Company. In 1927 Rand Kardex merged with Remington Typewriter. The new company was called Remington Rand, and Kardex was a division and brand of that company.\n\nRemington Rand was led by James Rand Jr. for the duration of its independent existence. Company sales grew from $5 million in 1927 to $500 million in 1954, although market share suffered in the face of competition from IBM.\n\nRemington Rand purchased the Eckert–Mauchly Computer Corporation in 1950, delivered the UNIVAC I in 1951, and bought Engineering Research Associates in 1952, all this making them a major player in the new computer industry.\n\nIn 1955, Sperry Corporation acquired Remington Rand and renamed itself Sperry Rand. Remington Rand remained as a separate brand name and in the name of some divisions. The Remington Rand Systems division was concerned with filing systems and associated products such as file folders, and other material and physical data storage and handling systems, including Kardex.\n\nIn 1978 Sperry Rand decided to concentrate on the computer business, and sold a number of divisions including Remington Rand Systems. (Remington Rand Machines was also sold along with some other divisions, and the company dropped \"Rand\" from its title and reverted to Sperry Corporation; in 1986 they merged with Burroughs Corporation to form Unisys.)\n\nThe Remington Rand Systems division of Sperry Rand, now based in Marietta, Ohio, was acquired by Aarque Management Corporation of Jamestown, New York, and renamed to Kardex Systems to take advantage of Kardex's status as a long-famous brand in filing systems.\n\nIn 2003 the company was sold to RACK Enterprises and Ronald Miller was made president.\n\nMiller established a separate company, Randex, also selling filing and storage system components, for operation in Europe. Randex continued to exist as a separate company under Miller family control after Kardex Systems was sold to Remstar.\n\nIn 2008 Kardex Systems was acquired by Remstar AG of Zurich, Switzerland. Remstar had been founded in 1977 (although what is now its Kardex Mlog division had been established in 1922 as Mehne GmbH) and its American subsidiary established in 1981 in Westbrook, Maine.\n\nIn 2009, Kardex and Remstar's American operation, which had been established in 1981, were merged to form Kardex Remstar, based in Westbrook.\n\nAt some point Remstar AG was renamed to Kardex Group AG. Kardex Group consists of two divisions, Kardex Remstar and Kardex Mlog, Mlog having been acquired in 2010 and the Stow division having been spun off in 2013.\n\nKardex Remstar manufactures and sells materials handling systems for industry, and file handling systems for offices. Products include basic commodities such as file folders and labels as well as more complicated products such as shelving systems and cabinets, vertical lifts, and rack systems.\n\nKardex Mlog manufactures and sells automated high-bay warehouses and customized materials handling systems.\n\n"}
{"id": "34263934", "url": "https://en.wikipedia.org/wiki?curid=34263934", "title": "Kevin Carroll (prosthetist)", "text": "Kevin Carroll (prosthetist)\n\nKevin Carroll is an Irish prosthetist, researcher, educator, and author. He is the Vice-President of Prosthetics for Hanger Clinic, a prosthetics and orthotics provider in the United States.\n\nKevin travels around the United States and the world providing care for patients with unique or challenging cases and for disabled athletes. He also presents scientific symposiums and educational programs.\n\nCarroll is one of the most renowned prosthetists in the world. He is an American Board Certified Prosthetist and has been named a Fellow of the American Academy of Orthotists and Prosthetists. He is a member of the International Society for Prosthetics and Orthotics (ISPO), the British Association of Prosthetics and Orthotics (BAPO) and the National Association of Professional Geriatric Care Managers. In 2009 Carroll was awarded the Distinguished Service Award by the United States Sports Academy because of his contributions to the advancement of prosthetics and his dedication to numerous disabled and Paralympic athletes.\n\nIn 2005, a baby dolphin (now named Winter) became entangled in the ropes of a crab trap. The rope cut off the supply of blood to her tail which resulted in her tail being amputated.\n\nIt was thought that Winter would learn to swim without a tail, but this forced her to swim with a \"side to side\" motion instead of the normal \"up and down\" motion. Winter's veterinarians feared that this unusual movement would damage her spine. Kevin Carroll, who had previously designed prosthetics for other animals (including dogs, an ostrich and a duck), volunteered to help after hearing about Winter on NPR and became Winter's prosthetist in 2005.\n\nKevin and a team of experts, including Hanger clinician Dan Strzempka, began working on creating a prosthetic tail for Winter. While Carroll thought that it would be a simple task, the project took over a year and a half before Winter began wearing the tail. The successful creation of an artificial tail fluke is the first time a full prosthetic tail has been created for a dolphin.\n\nCarroll and Strzempka worked with a chemical engineer to develop WintersGel, a new material that would disperse pressure evenly onto the dolphin's skin. Now the material is used for human amputees including Brian Kolfage and Megan McKeon.\n\nIn 2009 Winter's story was told by Craig Hatkoff and his daughters Juliana and Isabella Hatkoff, the No. 1 New York Times best-selling children's authors in \"Winter's Tail: How One Little Dolphin Learned to Swim Again\". The book was published by Turtle Pond Publications and Scholastic. The book was co-released with a Nintendo DS game.\n\nIn 2011, Winter's story hit the big screen when American film director Charles Martin Smith directed the film \"Dolphin Tale\". Carroll and Strzempka served as consultants for the movie and are featured at the end credits along with Megan McKeon and others who have benefited from Winter. For reasons unknown concerning the historical liberties, Carroll and Strzempka's roles were merged into one character for the film, Dr. Cameron McCarthy who is played by actor Morgan Freeman. On the Blu-ray + DVD + UltraViolet Digital Copy version of the movie, Carroll and Strzempka are featured in Winter's Inspiration, the featurette that shares Winter's true story.\n\nIn 1997 Warren Macdonald, an Australian mountain climber, was climbing through a remote mountainside in Australia when a one-ton boulder fell onto his legs trapping him for 45 hours. This trauma resulted in both of his legs being amputated mid-thigh. After his rehabilitation, he returned to climbing using speciality climbing prostheses developed by Kevin Carroll and Hanger clinician Chad Simpson. In 2003 Macdonald climbed to the top of Mount Kilimanjaro in Tanzania, making him the first bilateral transfemoral amputee to accomplish such a feat.\n\n\n\n"}
{"id": "41306496", "url": "https://en.wikipedia.org/wiki?curid=41306496", "title": "List of Film Sack episodes", "text": "List of Film Sack episodes\n\nThe following is a list of episodes of the podcast Film Sack:\n\nCommentary tracks for the following films/shows: \n\n"}
{"id": "6753240", "url": "https://en.wikipedia.org/wiki?curid=6753240", "title": "Memory dependence prediction", "text": "Memory dependence prediction\n\nMemory dependence prediction is a technique, employed by high-performance out-of-order execution microprocessors that execute memory access operations (loads and stores) out of program order, to predict true dependencies between loads and stores at instruction execution time. With the predicted dependence information, the processor can then decide to speculatively execute certain loads and stores out of order, while preventing other loads and stores from executing out-of-order (keeping them in-order). Later in the pipeline, memory disambiguation techniques are used to determine if the loads and stores were correctly executed and, if not, to recover.\n\nBy using the memory dependence predictor to keep most dependent loads and stores in order, the processor gains the benefits of aggressive out-of-order load/store execution but avoids many of the memory dependence violations that occur when loads and stores were incorrectly executed. This increases performance because it reduces the number of pipeline flushes that are required to recover from these memory dependence violations. See the memory disambiguation article for more information on memory dependencies, memory dependence violations, and recovery.\n\nIn general, memory dependence prediction predicts whether two memory operations are dependent, that is, if they interact by accessing the same memory location. Besides using store to load (RAW or true) memory dependence prediction for the out-of-order scheduling of loads and stores, other applications of memory dependence prediction have been proposed. See for example.\n\nMemory dependence prediction is an optimization on top of memory dependence speculation. Sequential execution semantics imply that stores and loads appear to execute in the order specified by the program. However, as with out-of-order execution of other instructions, it may be possible to execute two memory operations in a different from the program implied order. This is possible when the two operations are independent. In memory dependence speculation a load may be allowed to execute before a store that precedes it. Speculation succeeds when the load is independent of the store, that is, when the two instructions access different memory locations. Speculation fails when the load is dependent upon the store, that is, when the two accesses overlap in memory. In the first, modern out-of-order designs, memory speculation was not used as its benefits were limited. Αs the scope of the out-of-order execution increased over few tens of instructions, naive memory dependence speculation was used. In naive memory dependence speculation, a load is allowed to bypass any preceding store. As with any form of speculation, it is important to weight the benefits of correct speculation against the penalty paid on incorrect speculation. As the scope of out-of-order execution increases further into several tens of instructions, the performance benefits of naive speculation decrease. To retain the benefits of aggressive memory dependence speculation while avoiding the costs of mispeculation several predictors have been proposed.\n\nSelective memory dependence prediction stalls specific loads until it is certain that no violation may occur. It does not explicitly predict dependencies. This predictor may delay loads longer than necessary and hence result in suboptimal performance. In fact, in some cases it performs worse than naively speculating all loads as early as possible. This is because often its faster to mispeculate and recover than to wait for all preceding stores to execute. Exact memory dependence prediction was developed at the University of Wisconsin–Madison. Specifically, Dynamic Speculation and Synchronization delays loads only as long as it is necessary by predicting the exact store a load should wait for. This predictor predicts exact dependences (store and load pair). The synonym predictor groups together all dependences that share a common load or store instruction. The store sets predictor represents multiple potential dependences efficiently by grouping together all possible stores a load may dependent upon. The store barrier predictor treats certain store instructions as barriers. That is, all subsequent load or store operations are not allowed to bypass the specific store. The store barrier predictor does not explicitly predict dependencies. This predictor may unnecessarily delay subsequent, yet independent loads. Memory dependence prediction has other applications beyond the scheduling of loads and stores. For example, speculative memory cloaking and speculative memory bypassing use memory dependence prediction to streamline the communication of values through memory.\n\nMemory dependence prediction for loads and stores is analogous to branch prediction for conditional branch instructions. In branch prediction, the branch predictor predicts which way the branch will resolve before it is known. The processor can then speculatively fetch and execute instructions down one of the paths of the branch. Later, when the branch instruction executes, it can be determined if the branch instruction was correctly predicted. If not, this is a branch misprediction, and a pipeline flush is necessary to throw away instructions that were speculatively fetched and executed.\n\nBranch prediction can be thought of as a two step process. First, the predictor determines the direction of the branch (taken or not). This is a binary decision. Then, the predictor determines the actual target address. Similarly, memory dependence prediction can be thought of as a two step process. First, the predictor determines whether there is a dependence. Then it determines which this dependence is.\n\n"}
{"id": "577738", "url": "https://en.wikipedia.org/wiki?curid=577738", "title": "Mobitex", "text": "Mobitex\n\nMobitex is an OSI based open standard, national public access wireless packet-switched data network. Mobitex puts great emphasis on safety and reliability with its use by military, police, firefighters and ambulance services. It was developed in the beginning of the 1980s by the Swedish Televerket Radio. From 1988 the development took place in Eritel, a joint-venture between Ericsson and Televerket, later on as an Ericsson subsidiary. Mobitex became operational in Sweden in 1986.\n\nIn the mid-1990s Mobitex gained consumer popularity by providing two-way paging network services. It was the first wireless network to provide always on, wireless push email services such as RadioMail and Inter@ctive Paging. It is also used by the first model of Research in Motion's BlackBerry, and PDAs such as the Palm VII. During 9/11 and the 2005 hurricane rescue and clean-up operations, Mobitex proved itself to be a very reliable and useful system for first responders.\n\nMobitex is a packet-switched, narrowband, data-only technology mainly for short burst data. Mobitex channels are 12.5 kHz wide. In North America, Mobitex ran at , while in Europe it uses . The modulation scheme used is GMSK with a slotted aloha protocol at , although user throughput is typically around half of that.\n\nThe network provided the first public access wireless data communication services in North America. Subscriber services included electronic messaging with Cc capabilities to multiple recipients, combined with the ability to log on to any wireless or fixed terminal and receive stored mailbox messages.\n\nMobitex is offered on over 30 networks on five continents. In Canada it was first introduced in 1990 by Rogers Cantel, and in 1991 by carrier RAM Mobile Data. However, , it is primarily used in Belgium, the Netherlands and China. European Mobitex networks almost completely withered in the shadow of the overwhelming success of GSM there in the early 1990s.\n\nMobitex networks in North America were marketed under several names, including RAM Mobile Data, BellSouth Wireless Data, Cingular Interactive, Cingular Wireless and Velocita Wireless, and Rogers Wireless in Canada. \n\nMobitex in the UK was marketed by RAM Mobile Data, the UK part of which became Transcomm and was then purchased by BT (British Telecom) in 2004. The largest use of Mobitex in the United Kingdom was within the vehicle recovery business. Nearly all breakdowns passed to UK breakdown service agents were sent using Turbo Dispatch, a Mobitex-based gateway software developed in the early nineties by Ian Lane and Andy Lambert. \n\nDespite the competitive nature of the vehicle recovery market in the UK, motoring organisations were persuaded to co-operate and make a standard of the format. This resulted in a major saving for the eight hundred independent garages used by the motoring organisations. The Turbo Dispatch Standards Group (the official keepers of the standard) estimated that at least twenty million breakdowns and recoveries were transmitted over Turbo Dispatch each year. \n\nIn Sweden the Mobitex network was finally shut down permanently on December 31st 2012 after 25 years.\n\n\n\n"}
{"id": "867860", "url": "https://en.wikipedia.org/wiki?curid=867860", "title": "Netbook", "text": "Netbook\n\nNetbook is a generic name given to a category of small, lightweight, legacy-free, and inexpensive laptop computers that were introduced in 2007. Netbooks compete in the same market segment as mobiles and Chromebooks (a variation on the portable network computer).\n\nAt their inception in late 2007 as smaller notebooks optimized for low weight and low cost—netbooks omitted certain features (e.g., the optical drive), featured smaller screens and keyboards, and offered reduced computing power when compared to a full-sized laptop. Over the course of their evolution, netbooks have ranged in size from below 5\" screen diagonal to 12\". A typical weight is (). Often significantly less expensive than other laptops, by mid-2009, netbooks began to be offered by some wireless data carriers to their users \"free of charge\", with an extended service contract purchase.\n\nIn the short period since their appearance, netbooks grew in size and features, and converged with smaller, lighter laptops and subnotebooks. By August 2009, when comparing a Dell netbook to a Dell notebook, CNET called netbooks \"nothing more than smaller, cheaper notebooks\", noting, \"the specs are so similar that the average shopper would likely be confused as to why one is better than the other\", and \"the only conclusion is that there really is no distinction between the devices\". In an attempt to prevent cannibalizing the more lucrative laptops in their lineup, manufacturers imposed several constraints on netbooks; however this would soon push netbooks into a niche where they had few distinctive advantages over traditional laptops or tablet computers (see below).\n\nBy 2011, the increasing popularity of tablet computers (particularly the iPad)—a different form factor, but with improved computing capabilities and lower production cost—had led to a decline in netbook sales. At the high end of the performance spectrum, ultrabooks, ultra-light portables with a traditional keyboard and display have been revolutionized by the 11.6-inch MacBook Air, which made fewer performance sacrifices albeit at considerably higher production cost. Capitalizing on the success of the MacBook Air, and in response to it, Intel promoted Ultrabook as a new high-mobility standard, which has been hailed by some analysts as succeeding where netbooks failed. As a result of these two developments, netbooks of 2011 had kept price as their only strong point, losing in the design, ease-of-use and portability department to tablets (and tablets with removable keyboards) and to Ultrabook laptops in the features and performance field.\n\nMany major netbook producing companies stopped producing them by the end of 2012. Many netbook products were replaced on the market by Chromebooks, a variation on the network computer concept in the form of a netbook. HP re-entered the non-Chromebook netbook market with the \"Stream 11\" in 2014.\n\nThe origins of the netbook can be traced to the highly popular Toshiba range of Libretto sub-notebooks. The 6\" Libretto 20 dates back to early 1996 and weighed only 840g. Apple also had a line of PowerBook Duos that were ultra-portable Macintosh laptops in the mid 90s. More recently, Psion's now-discontinued netBook line, the OLPC XO-1 (initially called US$100 laptop) and the Palm Foleo were all small, portable, network-enabled computers. The generic use of the term \"netbook\", however, began in 2007 when Asus unveiled the Asus Eee PC. Originally designed for emerging markets, the device weighed about and featured a display, a keyboard approximately 85% the size of a normal keyboard, a solid-state drive and a custom version of Linux with a simplified user interface geared towards netbook use. Following the Eee PC, Everex launched its Linux-based CloudBook; Windows XP and Windows Vista models were also introduced and MSI released the Wind—others soon followed suit.\n\nThe OLPC project followed the same market goals laid down by the eMate 300 eight years earlier. Known for its innovation in producing a durable, cost- and power-efficient netbook for developing countries, it is regarded as one of the major factors that led more top computer hardware manufacturers to begin creating low-cost netbooks for the consumer market. When the first Asus Eee PC sold over 300,000 units in four months, companies such as Dell and Acer took note and began producing their own inexpensive netbooks. And while the OLPC XO-1 targets a different audience than do the other manufacturers' netbooks, it appears that OLPC is now facing competition. Developing countries now have a large choice of vendors, from which they can choose which low-cost netbook they prefer.\n\nBy late 2008, netbooks had begun to take market share away from notebooks. In contrast to earlier, largely failed attempts to establish mini computers as a new class of mainstream personal computing devices built around comparatively expensive platforms requiring proprietary software applications or imposing severe usability limitations, the recent success of netbooks can also be attributed to the fact that PC technology has now matured enough to allow truly cost optimized implementations with enough performance to suit the needs of a majority of PC users. This is illustrated by the fact that typical system performance of a netbook is on the level of a mainstream PC in 2001, at around one quarter of the cost. While this performance level suffices for most of the user needs, it caused an increased interest in resource-efficient applications such as Google's Chrome, and forced Microsoft to extend availability of Windows XP to secure market share. It is estimated that almost thirty times more netbooks were sold in 2008 (11.4 million, 70% of which were in Europe) than in 2007 (400,000). This trend is reinforced by the rise of web-based applications as well as mobile networking and, according to Wired Magazine, netbooks are evolving into \"super-portable laptops for professionals\". The ongoing recession is also helping with the growing sales of netbooks.\n\nIn Australia, the New South Wales Department of Education and Training, in partnership with Lenovo, provided Year 9 (high school) students in government high schools with Lenovo S10e netbooks in 2009, Lenovo Mini 10 netbooks in 2010, Lenovo Edge 11 netbooks in 2011 and a modified Lenovo X130e netbook in 2012, each preloaded with software including Microsoft Office and Adobe Systems' Creative Suite 4. These were provided under Prime Minister Kevin Rudd's Digital Education Revolution, or DER. The netbooks ran Windows 7 Enterprise. These netbooks were secured with Computrace Lojack for laptops that the police can use to track the device if it is lost or stolen. The NSW DET retains ownership of these netbooks until the student graduates from Year 12, when the student can keep it. The Government of Trinidad and Tobago—Prime Minister Kamla Persad Bisseser—is also providing HP laptops to form 1 Students (11-year-olds) with the same police trackable software as above.\n\nGreece provided all 13-year-old students (middle school, or \"gymnasium\", freshmen) and their teachers with netbooks in 2009 through the \"Digital Classroom Initiative\". Students were given one unique coupon each, with which they redeemed the netbook of their choice, up to a €450 price ceiling, in participating shops throughout the country. These netbooks came bundled with localised versions of either Windows XP (or higher) or open source (e.g. Linux) operating systems, wired and wireless networking functionality, antivirus protection, preactivated parental controls, and an educational software package.\n\nMicrosoft and Intel have tried to \"cement\" netbooks in the low end of the market to protect mainstream notebook PC sales, because they get lower margins on low-cost models. The companies have limited the specifications of netbooks, but despite this original equipment manufacturers have announced higher-end netbooks models as of March 2009.\n\nEnding in 2008 the report was that the typical netbook featured a weight, a screen, wireless Internet connectivity, Linux or Windows XP, an Intel Atom processor, and a cost of less than $400 US. A mid-2009 newspaper article said that a typical netbook is , $300 US, and has a screen,\n\nHaving peaked at about 20% of the portable computer market, netbooks started to slightly lose market share (within the category) in early 2010, coinciding with the appearance and success of the iPad. Technology commentator Ross Rubin argued two and a half years later in Engadget that \"Netbooks never got any respect. While Steve Jobs rebuked the netbook at the iPad's introduction, the iPad owes a bit of debt to the little laptops. The netbook demonstrated the potential of an inexpensive, portable second computing device, with a screen size of about 10 inches, intended primarily for media consumption and light productivity.\" Although some manufacturers directly blamed competition from the iPad, some analysts pointed out that larger, fully fledged laptops had entered the price range of netbooks at about the same time.\n\nThe 11.6-inch MacBook Air, introduced in late 2010, compared favorably to many netbooks in terms of processing power but also ergonomics, at 2.3 pounds being lighter than some 10-inch netbooks, owing in part to the integration of the flash storage chips on the main logic board. It was described as a superlative netbook (or at least as what a netbook should be) by several technology commentators, even though Apple has never referred to it as such, sometimes describing it—in the words of Steve Jobs—as \"the third kind of notebook.\" The entry level model had a MSRP of $999, costing significantly more than the average netbook, as much as three or four times more.\n\nIn 2011 tablet sales overtook netbooks for the first time, and in 2012 netbook sales fell by 25 percent, year-on-year. The sustained decline since 2010 had been most pronounced in the United States and in Western Europe, while Latin America was still showing some modest growth. In December 2011, Dell announced that it was exiting the netbook market. In May 2012, Toshiba announced it was doing the same, at least in the United States. An August 2012 article by John C. Dvorak in \"PC Magazine\" claimed that the term \"netbook\" is \"nearly gone from the lexicon already\", having been superseded in the market place largely by the more powerful (and MacBook Air inspired) Ultrabook—described as \"a netbook on steroids\"—and to a lesser extent by tablets. In September 2012 Asus, Acer and MSI announced that they will stop manufacturing 10-inch netbooks. Simultaneously Asus announced they would stop developing all Eee PC products, instead focusing on their mixed tablet-netbook Transformer line.\n\nWith the introduction of Chromebooks, major manufacturers produced the new laptops for the same segment of the market that netbooks serviced. Chromebooks, a variation on the network computer concept, in the form of a netbook, require internet connections for full functionality. Chromebooks became top selling laptops in 2014. The threat of Google Chrome OS based Chromebooks prompted Microsoft to revive and revamp netbooks with \"Windows 8.1 with Bing\". HP re-entered the non-Chromebook netbook market with the \"Stream 11\" in 2014.\n\nIn 1996 Psion started applying for trademarks for a line of \"netBook\" products that was later released in 1999. International trademarks were issued (including and ) but the models failed to gain popularity and are now discontinued (except for providing accessories, maintenance and support to existing users). Similar marks were recently rejected by the USPTO citing a \"likelihood of confusion\" under section 2(d).\n\nDespite expert analysis that the mark is \"probably generic\", Psion Teklogix issued cease and desist letters on 23 December 2008. This was heavily criticised, prompting the formation of the \"Save the Netbooks\" grassroots campaign which worked to reverse the Google AdWords ban, cancel the trademark and encourage continued generic use of the term. While preparing a \"Petition for Cancellation\" of they revealed that Dell had submitted one day before on the basis of abandonment, genericness and fraud. They later revealed Psion's counter-suit against Intel, filed on 27 February 2009.\n\nIt was also revealed around the same time that Intel had also sued Psion Teklogix (US & Canada) and Psion (UK) in the Federal Court on similar grounds. In addition to seeking cancellation of the trademark, Intel sought an order enjoining Psion from asserting any trademark rights in the term \"netbook\", a declarative judgment regarding their use of the term, attorneys' fees, costs and disbursements and \"such other and further relief as the Court deems just and proper\".\n\nOn June 2, 2009, Psion announced that the suit had been settled out of court. Psion's statement said that the company was withdrawing all of its trademark registrations for the term \"Netbook\" and that Psion agreed to \"waive all its rights against third parties in respect of past, current or future use\" of the term.\n\nNetbooks typically have less powerful hardware than larger laptop computers and do not include an optical disc drive that larger laptops often have. Some netbooks do not even have a conventional hard drive. Such netbooks use solid-state storage devices instead, as these require less power, are faster, lighter, and generally more shock-resistant, but with much less storage capacity (such as 32, 64, or 128 GB compared to the 100 GB to 2 TB mechanical hard drives typical of many notebooks/laptop computers).\n\nThey include more than one USB port, because one of them is usually devoted to the wireless keyboard and mouse set. This wireless keyboard could not include the numeric keypad\n\nAll netbooks on the market today support Wi-Fi wireless networking and many can be used on mobile telephone networks with data capability (for example, 3G). Mobile data plans are supplied under contract in the same way as mobile telephones. Some also include ethernet and/or modem ports, for broadband or dial-up Internet access, respectively.\n\nIt remains to be seen whether Intel's new silvermont architecture, released in 2013, will revive sales as new chips will offer far greater power using the same wattage.\n\nMost netbooks, such as those from Asus, BenQ, Dell, Toshiba, Acer use the Intel Atom notebook processor (typically the N270 1.6 GHz but also available is the N280 at 1.66 GHz, replaced by the N450 series with graphics and memory controller integrated on the chip in early 2010 and running at 1.66 GHz), but the x86-compatible VIA Technologies C7 processor is also powering netbooks from many different manufacturers like HP and Samsung. VIA has also designed the Nano, a new x86-64-compatible architecture targeting lower priced, mobile applications like netbooks. Currently, one netbook uses the Nano; the Samsung NC20. Some very low cost netbooks use a system-on-a-chip Vortex86 processor meant for embedded systems, just to be \"Windows compatible\", but with very low performance. In 2011, AMD launched Fusion netbook processors which are included in Asus Eee PC 1015T and many others.\n\nAlthough not officially sanctioned by AMD for this role, a 1.2 GHz Athlon 64 model L110 processor, dissipating 13 W, was used by at least one company—Gateway—to power an 11.6-inch portable (1366x768 screen resolution), described as a netbook by the press. Launched in mid-2009 at $399 in the United States, the LT31 met with reviewers' approval for its performance, being generally recognized as faster than contemporary Atom-based products in the same price range, while having a considerably shorter battery life and still falling short of Intel's Core 2 ULV product line powering more expensive small-factor offerings.\n\nThe 11.6 inch MacBook Air debuted in late 2010 with a 1.4 GHz Core 2 Duo processor (a 10 W part) and a 1366x768 display resolution for its entry level model priced at $999 (with 1.6 GHz available as upgrade), which put it \"much closer to a fully modern laptop than the small-but-crippled netbooks\". One reviewer described it as the \"Mercedes Benz of netbooks\".\n\nThe September 2011 \"PC Magazine\" buyer's guide for netbooks observed that other \"oversized netbooks\" with 11.6 inch screens had appeared on the market, including the HP Pavilion dm1z (MSRP $449) and Lenovo ThinkPad X100e (MSRP $550), both using the AMD Fusion E-350 processor (an 18 W part, although this includes the GPU), which was described as \"faster than any given Atom processor\".\n\nBy definition netbooks accommodate processors with little processing power. For comparison a common dual-core Core 2 Duo T5600 at 1.83 GHz with 2 MB L2 cache used in low-end laptops has a PassMark score of about 1000 points. The following table shows benchmarks for most common netbook CPUs:\nARM Holdings designs and licenses microprocessor technology with relatively low power requirements and low cost which would constitute an ideal basis for netbooks. In particular, the recent ARM Cortex-A9 MPCore series of processor cores have been touted by ARM as an alternative platform to x86 for netbooks. In June 2009, Nvidia announced a dozen mobile Internet devices running ARM-based Tegra SoC's, some of which will be netbooks.\n\nSome ARM-based products were advertised as smartbooks, particularly by Qualcomm. Smartbooks promised to deliver features including always on, all-day battery life, 3G connectivity and GPS (all typically found in smartphones) in a laptop-style body with a screen size of 5 to 10 inches and a QWERTY keyboard. These systems do not run traditional x86 versions of Microsoft Windows, rather custom Linux operating systems (such as Google's Android or Chrome OS). In the end, few such products were ever shipped to market under this branding, like the HP-Compaq Airlife, the Toshiba AC100 (sold as Dynabook AZ in Japan) and the Efika MX. Some of the devices, like the AC100, have been hampered by being sold with a phone-oriented operating system like Android. By the end of 2010, Qualcomm CEO Paul Jacobs admitted that tablet computers such as the iPad already occupied the niche of the smartbook, so the name was dropped.\n\nSome netbooks use MIPS architecture-compatible processors. These include the Skytone Alpha-400, based on an Ingenic system on chip, and the EMTEC Gdium netbook, which uses the 64-bit Loongson processor capable of 400 million instructions per second. While these systems are relatively inexpensive, the processing power of current MIPS implementations usually compares unfavorably with those of x86-implementations as found in current netbooks.\n\nMicrosoft announced on April 8, 2008 that, despite the impending end of retail availability for the operating system that June, it would continue to license low-cost copies of Windows XP Home Edition to OEMs through October 2010 (one year after the release of Windows 7) for what it defined as \"ultra low-cost personal computers\"—a definition carrying restrictions on screen size and processing power. The move served primarily to counter the use of low-cost Linux distributions on netbooks and create a new market segment for Windows devices, whilst ensuring that the devices did not cannibalize the sales of higher-end PCs running Windows Vista. In 2009, over 90% (96% claimed by Microsoft as of February 2009) of netbooks in the United States were estimated to ship with Windows XP.\n\nFor Windows 7, Microsoft introduced a new stripped-down edition intended for netbooks known as \"Starter\", exclusively for OEMs. In comparison to Home Premium, Starter has reduced multimedia functionality, does not allow users to change their desktop wallpaper or theme, disables the \"Aero Glass\" theme, and does not have support for multiple monitors.\n\nFor Windows 8, in a ploy to counter Chrome OS-based netbooks and low-end Android tablets, Microsoft began to offer no-cost Windows licenses to OEMs for devices with screens smaller than 9 inches in size. Additionally, Microsoft began to offer low-cost licenses for a variant of the operating system set up to use Microsoft's Bing search engine by default.\n\nWindows CE has also been used in netbook, due to its reduced feature set.\n\nGoogle's Android software platform, designed for mobile telephone handsets, has been demonstrated on an ASUS Eee PC and its version of the Linux operating system contains policies for mobile internet devices including the original Asus Eee PC 701. ASUS has allocated engineers to develop an Android-based netbook. In May 2009 a contractor of Dell announced it is porting Adobe Flash Lite to Android for Dell netbooks. Acer announced Android netbooks to be available in Q3/2009. In July 2009, a new project, Android-x86, was created to provide an open source solution for Android on the x86 platform, especially for netbooks.\n\nGoogle has since 2011, marketed a netbook-specific platform known as Chrome OS. Google has made efforts to provide access to the Android ecosystem within Chrome OS, including the 2016 introduction of Google Play Store and a compatibility layer for Android applications to the platform.\n\nIn 2011, Google introduced Chrome OS, a Linux-based operating system designed particularly for netbook-like devices marketed as \"Chromebooks\". The platform is designed to leverage online services, cloud computing, and its namesake Chrome web browser as its shell—so much so that the operating system initially used a full screen web browser window as its interface, and contained limited offline functionality. Later versions of Chrome OS introduced a traditional desktop interface and a platform allowing \"native\" packaged software written in HTML, JavaScript, and CSS to be developed for the platform.\n\nNetbooks have sparked the development of several Linux variants or completely new distributions, which are optimized for small screen use and the limited processing power of the Atom or ARM processors which typically power netbooks. Examples include Ubuntu Netbook Edition, EasyPeasy, Joli OS and MeeGo, FreeBSD, NetBSD, OpenBSD, and Darwin. Both Joli OS and MeeGo purport to be \"social oriented\" or social networking operating systems rather than traditional \"office work production\" operating systems.\n\nSince 2010, major netbook manufacturers no longer install or support Linux in the United States. The reason for this change of stance is unclear, although it coincides with the availability of a 'netbook' version of Windows XP, and a later Windows 7 Starter and a strong marketing push for the adoption of this OS in the netbook market. However, companies targeting niche markets, such as System76 and ZaReason, continue to pre-install Linux on the devices they sell.\n\nThe Cloud operating system attempts to capitalize on the minimalist aspect of netbooks. The user interface is limited to a browser application only.\n\nMac OS X has been demonstrated running on various netbooks as a result of the OSx86 project, although this is in violation of the operating system's end-user license agreement. Apple has complained to sites hosting information on how to install OS X onto non-Apple hardware (including \"Wired\" and YouTube) who have reacted and removed content in response. One article nicknamed a netbook running OS X a \"Hackintosh.\" The Macbook Air can be considered an expensive netbook.\n\nA June 2009 NPD study found that 60% of netbook buyers never take their netbooks out of the house.\n\nSpecial \"children's\" editions of netbooks have been released under Disney branding; their low cost (less at risk), lack of DVD player (less to break) and smaller keyboards (closer to children's hand sizes) are viewed as significant advantages for that target market. The principal objection to netbooks in this context is the lack of good video performance for streaming online video in current netbooks and a lack of speed with even simple games. Adults browsing for text content are less dependent on video content than small children who cannot read.\n\nNetbooks are a growing trend in education for several reasons. The need to prepare children for 21st-century lifestyles, combined with hundreds of new educational tools that can be found online, and a growing emphasis on student centered learning are three of the biggest contributing factors to the rising use of netbook technology in schools. Dell was one of the first to mass-produce a ruggedised netbook for the education sector, by having a rubber outlay, touchscreen and network activity light to show the teacher the netbook is online.\n\nNetbooks offer several distinct advantages in educational settings. First, their compact size and weight make for an easy fit in student work areas. Similarly, the small size make netbooks easier to transport than heavier, larger sized traditional laptops. In addition, prices ranging from $200–$600 mean the affordability of netbooks can be a relief to school budget makers. Despite the small size and price, netbooks are fully capable of accomplishing most school-related tasks, including word processing, presentations, access to the Internet, multimedia playback, and photo management.\n\n"}
{"id": "28180929", "url": "https://en.wikipedia.org/wiki?curid=28180929", "title": "Opower", "text": "Opower\n\nOpower was a company that provided a software-as-a-service customer engagement platform for utilities. It existed as an independent corporation until its acquisition by Oracle Corporation in 2016. The Opower product line are now marketed by Oracle under the Oracle Utilities moniker.\n\nFounded in 2007 by long-time friends Dan Yates and Alex Laskey, Opower is headquartered in Arlington, Virginia. As of February 2014, it employed more than 500 people in five offices: Arlington, San Francisco, London, Singapore, and Japan.\n\nIn June 2010, iLike founder Hadi Partovi joined Opower's board to advise on its West Coast growth strategy. In July 2010, Opower opened a second office in San Francisco and had Fred Butler, a former president of the National Association of Regulatory Utility Commissioners (NARUC) and commissioner of the New Jersey Board of Public Utilities, join its advisory board.\n\nIn 2013, Opower added former utility CEOs John Rowe and Dick Kelly, and former White House Director, Carol Browner, to its advisory board. Tom Brady, former chairman of BGE, was named chairman of the advisory board. Also in 2013, Opower added Marcus Ryu, CEO of Guidewire, and Deep Nishar, SVP of Products and User Experience at LinkedIn, to its board of directors.\n\nOn September 1, 2010, the World Economic Forum announced the company as a Technology Pioneer for 2011.\n\nIn November 2010, the company announced its third round of venture capital funding, a $50 million investment led by Accel Partners and Kleiner Perkins Caufield & Byers, to accelerate its expansion.\n\nDuring a visit to Opower headquarters in Arlington in 2010, President Obama said the company's growth is \"a model of what we want to be seeing all across the country\".\n\nIn May 2013, Opower was named to the Inaugural CNBC Disruptors 50 List.\n\nIn November 2013, Opower was named the #1 fastest-growing tech company in the DC region, and #20 in the US, by Deloitte.\n\nCEO Dan Yates gave the keynote address at the Cleantech Group's Cleantech Forum in San Francisco in March 2014. \n\nOpower held its initial public offering on April 4, 2014.\n\nOpower laid off 7.5 % of its global workforce on April 8, 2016.\n\nOn May 2, 2016, Opower announced that it was being acquired by Oracle. \n\nOpower's software uses statistical algorithms to perform pattern recognition analysis from data in order to derive information for utility customers. Without any devices installed in the home, the platform can perform usage-disaggregation analysis, presenting end users information such as heating or cooling usage apart from overall usage, and thus allowing them to spot additional opportunities to save money. The average customer receiving the Opower platform has cut energy usage by more than 2.5 percent.\n\nThe American Council for an Energy-Efficient Economy concluded in a June 2010 \"report\" that customer-feedback programs, like Opower, could boost energy security, help the environment and save consumers money.\n\nOpower's Energy Reports incorporate the behavioral science techniques of Robert Cialdini, Opower's chief scientist and the author of \"Influence\", a 1984 book on persuasion. The reports include targeted tips that seek to motivate customers to lower their energy consumption to the \"normal\" neighborhood rate. The reports also feature smiley-face emoticons for the most energy-efficient homes, a feature that Opower added after research showed that some consumers who used less energy than average started using more once they knew the norm. The reports also compare energy usage among neighbors with similarly sized houses.\n\nThe company mails the reports to consumers, but also offers the information in other formats, including internet portals, text messages, email and in-home energy displays. Opower's software enables customers to input more information to generate recommendations about specific types of energy use, such as air-conditioning and heating.\n\n President Barack Obama visited Opower headquarters in Arlington on March 5, 2010. He touted the company as an economic \"success story\" amid a troubled economy and as a \"great emblem\" for clean energy jobs.\n\nHe made the visit two months after announcing a \"$2.3 billion program\" of tax credits for \"green jobs.\" \"The work you do here...is making homes more energy efficient, it's saving people money, it's generating jobs, and it's putting America on the path to a clean energy future\", Obama said at Opower.\n\nThe White House released a video of Obama's appearance.\n\n"}
{"id": "6195759", "url": "https://en.wikipedia.org/wiki?curid=6195759", "title": "Palatal obturator", "text": "Palatal obturator\n\nA palatal obturator is a prosthesis that totally occludes an opening such as an oronasal fistula (in the roof of the mouth). They are similar to dental retainers, but without the front wire. Palatal obturators are typically short-term prosthetics used to close defects of the hard/soft palate that may affect speech production or cause nasal regurgitation during feeding. Following surgery, there may remain a residual orinasal opening on the palate, alveolar ridge, or vestibule of the larynx. A palatal obturator may be used to compensate for hypernasality and to aid in speech therapy targeting correction of compensatory articulation caused by the cleft palate. In simpler terms, a palatal obturator covers any fistulas (or \"holes\") in the roof of the mouth that lead to the nasal cavity, providing the wearer with a plastic/acrylic, removable roof of the mouth, which aids in speech, eating, and proper air flow.\n\nPalatal obturators are not to be confused with palatal lifts or other prosthetic devices. A palatal obturator may be used in cases of a deficiency in tissue, when a remaining opening in the palate occurs. In some cases it may be downsized gradually so that tissue can strengthen over time and compensate for the decreasing size of the obturator. The palatal lift however, is used when there is not enough palatal movement. It raises the palate and reduces the range of movement necessary to provide adequate closure to separate the nasal cavity from the oral cavity. Speech bulbs and palatal lifts aid in velopharyngeal closure and do not obturate a fistula. A speech bulb, yet another type of prosthetic device often confused with a palatal obturator, contains a pharyngeal section, which goes behind the soft palate.\n\nPalatal obturators are needed by individuals with cleft palate, those who have had tumors removed or have had traumatic injuries to their palate.\n\nA \"palatal plate\" is a prosthetic device, generally consisting of an acrylic plate and retention clasps of orthodontic wire, which covers a fistula of the palate. It may be used to aid in improving articulation and feeding. The blockage of the opening helps improve hypernasality and suckling ability for babies. In the case of a labial-oral-nasal fistula, the plate may include an anterior upward extension to fully occlude the passageway running between the labial surface of the alveolus, alveolus, and nasal cavity. The plate may be constructed to include any congenitally missing teeth to improve articulation and appearance. Individuals who use palatal plates must be monitored periodically by their dental professionals due to possible tissue irritation by the plate. Materials such as food particles, oral mucosa and secretions may cause buildup on the upper surface of the plate; therefore, it is essential to clean a palatal obturator at least twice a day to avoid tissue irritation. There are also more specific terms used for obturators depending on their time and purpose of use: Photo Examples of the Latham Device or the Nasal Alveolar are prime examples for use in Cleft Palate Deformities.\n\n\nPalatal adhesives are oral adhesives or skin barrier materials used to occlude a fistula of the hard palate. Obturators of this type must be removed before eating and drinking. Users must cut the new piece of adhesive and hold it over the fistula until it adheres. Adhesives are not to be used for soft palate fistulae if the soft palate has some mobility due to possible unintentional dislodging and digestion of the material.\n\nThis fixed obturator is based on the Nance appliance, which was originally used as a space maintainer in dentistry and orthodontics, but has been redesigned for closing anterior palatal fistulas in patients with cleft lip and palate. The Nance obturator may be used when the surgical closure of the fistula is not feasible and a removable device is not successful.\n\nOften a palatal obturator is used because a palatal fistula can affect development and proper articulation. As fistula sizes vary, small fistulae tend to result in little to no speech alterations whereas large fistulae tend to result in audible nasal emissions and weak pressure with and/or without hypernasality. Misarticulations, abnormal nasal resonance and nasal escape or air often results from the problem. Fistulae may decrease intraoral air pressure during production of oral pressure consonants causing distortion of sounds as well as increase in nasal airflow. It is common for an individual with a fistula to compensate for a loss of pressure during speech sound production by attempting to regulate intraoral air pressure with increasing respiration effort and using compensatory articulation. Middorsum palatal stops (atypical place of articulation) often results from palatal fistulae causing sound distortions during speech. Occlusion for the fistula is attempted by speakers with deviant tongue placements during these palatal stops.\n\nThe palatal obturation may be managed temporarily or may be sustained for longer periods of time. Location-specific palatal obturation has been documented to significantly improve articulation errors, hypernasality (based on listener judgments), and nasal emissions (immediately post-obturation only). Usage of more anterior tongue placements is considered a primary target for speech therapy. The relationship between palatal openings and articulation is important to note prior to surgical plans to ascertain timing of speech therapy and most appropriate therapy goals and approach. Speech therapy may be most beneficial prior to sustained palatal obturation rather than short-term obturation.\n\n\n"}
{"id": "4457582", "url": "https://en.wikipedia.org/wiki?curid=4457582", "title": "Photochemical logic gate", "text": "Photochemical logic gate\n\nA photochemical logic gate is based on the photochemical intersystem crossing and molecular electronic transition between photochemically active molecules, leading to logic gates that can be produced.\n\nThe OR gate is based on the activation of molecule A, and thus pass electron / photon to molecule C’s excited state orbitals (C*). The electron from molecule A inter system crosses to C* via the excited state orbitals of B, eventually utilised as a signal in the C* emission. The ‘OR’ gate uses two inputs of light (photons) to molecule A in two separate electron transfer chains, both of which are capable of transferring to C* and thus producing the output of an OR gate. Therefore, if either electron transfer chain is activated, molecule C’s excitation produces a valid/ output emission.\n\nExcitation A→A* by photon, whereby the promoted electron is passed down to the C* molecular orbital. A second photon applied to the system () causes the excitation of the electron in the C* molecular orbital to the C** molecular orbital -analogous pump probe spectroscopy.\n\nAbove, the energy level diagram illustrating the principle of pump probe spectroscopy –the excitation of an excited state.\nThe AND gate is produced by the necessity of both A→A* and the C**→C excitations occurring at the same time -input and , are simultaneously required. \nTo prevent erroneous emissions of light from a single input to the AND gate, it would be necessary to have an electron transfer series with ability accept any electrons (energy) from C* energy level. The electron transfer series would terminate with a low (non-radiative decay) of the energy \nThe alternatives for producing an AND gate, using molecular photphysics, are two.\n(1) The emission produced by the electron drop from C*→C () is not a valid output frequency. The emission from the C** ( + , ) molecular orbital is a valid output signal;. to be used in subsequent logic gates -arranged to respond to the formula_1 emission. \nThe second input of photon(s) to trigger the rapid conversion of a molecule used to complete the electron transfer chain. A very complex molecule like a protein can be engineered to possess high strain energies, so that in the absence of the second light frequency molecule B is inactive (B). The second photon input triggers B→B' where the forward rate constant is much smaller than the reverse. If such a molecule is used as molecule B, the transfer chain can be switched on and off.\n\nTo stop the electron transfer chain completing, producing output signals, the input of a photon, , is used to produce a ‘pump probe spectroscopy’ effect by promoting an electron in an electron transfer chain. The fall of the pump probe promoted electron produces an output that is quenched down an electron transfer chain.\n\nAn alternative is similar to the AND gate alternative; an input causes a change in molecule structure breaking the electron transfer chain by not allowing the smooth energy transfer of electrons.\n\n"}
{"id": "2538775", "url": "https://en.wikipedia.org/wiki?curid=2538775", "title": "Predictive modelling", "text": "Predictive modelling\n\nPredictive modelling uses statistics to predict outcomes. Most often the event one wants to predict is in the future, but predictive modelling can be applied to any type of unknown event, regardless of when it occurred. For example, predictive models are often used to detect crimes and identify suspects, after the crime has taken place.\n\nIn many cases the model is chosen on the basis of detection theory to try to guess the probability of an outcome given a set amount of input data, for example given an email determining how likely that it is spam.\n\nModels can use one or more classifiers in trying to determine the probability of a set of data belonging to another set, say spam or 'ham'.\n\nDepending on definitional boundaries, predictive modelling is synonymous with, or largely overlapping with, the field of machine learning, as it is more commonly referred to in academic or research and development contexts. When deployed commercially, predictive modelling is often referred to as predictive analytics.\n\nPredictive modelling is often contrasted with causal modelling/analysis. In the former, one may be entirely satisfied to make use of indicators of, or proxies for, the outcome of interest. In the latter, one seeks to determine true cause-and-effect relationships. This distinction has given rise to a burgeoning literature in the fields of research methods and statistics and to the common statement that \"correlation is not the same as causation.\"\n\nNearly any regression model can be used for prediction purposes. Broadly speaking, there are two classes of predictive models: parametric and non-parametric. A third class, semi-parametric models, includes features of both. Parametric models make \"specific assumptions with regard to one or more of the population parameters that characterize the underlying distribution(s)\", while non-parametric regressions make fewer assumptions than their parametric counterparts.\n\nThe majority classifier takes non-anomalous data and incorporates it within its calculations. This ensures that the results produced by the predictive modelling system are as valid as possible.\n\nOrdinary least squares is a method that minimizes the sum of squared distances between observed and predicted values.\n\nThe generalized linear model (GLM) is a flexible family of models that are unified under a single method. Logistic regression is a notable special case of GLM. Other types of GLM include Poisson regression, gamma regression, and multinomial regression.\n\nLogistic regression is a technique in which unknown values of a discrete variable are predicted based on known values of one or more continuous and/or discrete variables. Logistic regression differs from ordinary least squares (OLS) regression in that the dependent variable is binary in nature. This procedure has many applications. In biostatistics, the researcher may be interested in trying to model the probability of a patient being diagnosed with a certain type of cancer based on knowing, say, the incidence of that cancer in his or her family. In business, the marketer may be interested in modelling the probability of an individual purchasing a product based on the price of that product. Both of these are examples of a simple, binary logistic regression model. The model is \"simple\" in that each has only one independent, or predictor, variable, and it is \"binary\" in that the dependent variable can take on only one of two values: cancer or no cancer, and purchase or does not purchase.\n\nGeneralized additive model is a smoothing method for multiple predictors that allows for non-parametric predictions.\n\nRobust regression includes a number of modelling approaches to handle high leverage observations or violation of assumptions. Models can be both parametric (e.g. regression with Huber, White, Sandwich variance estimators) and non-parametric(e.g. quantile regression).\n\nSemiparametric regression includes the proportional odds model and the Cox proportional hazards model where the response is a rank.\n\nPredictive models can either be used directly to estimate a response (output) given a defined set of characteristics (input), or indirectly to drive the choice of decision rules.\n\nDepending on the methodology employed for the prediction, it is often possible to derive a formula that may be used in a spreadsheet software. This has some advantages for end users or decision makers, the main one being familiarity with the software itself, hence a lower barrier to adoption.\nNomograms are useful graphical representation of a predictive model. As in spreadsheet software, their use depends on the methodology chosen. The advantage of nomograms is the immediacy of computing predictions without the aid of a computer.\n\nPoint estimates tables are one of the simplest form to represent a predictive tool. Here combination of characteristics of interests can either be represented via a table or a graph and the associated prediction read off the y-axis or the table itself.\nTree-based methods (e.g. CART, survival trees) provide one of the most graphically intuitive ways to present predictions. However, their usage is limited to those methods that use this type of modelling approach which can have several drawbacks. Trees can also be employed to represent decision rules graphically.\n\nScore charts are graphical tabular or graphical tools to represent either predictions or decision rules.\n\nA new class of modern tools are represented by web-based applications. For example, Shiny is a web-based tool developed by Rstudio, an R IDE. With a Shiny app, a modeller has the advantage to represent any which way he or she chooses to represent the predictive model while allowing the user some control. A user can choose a combination of characteristics of interest via sliders or input boxes and results can be generated, from graphs to confidence intervals to tables and various statistics of interests. However, these tools often require a server installation of Rstudio.\n\nUplift modelling is a technique for modelling the \"change in probability\" caused by an action. Typically this is a marketing action such as an offer to buy a product, to use a product more or to re-sign a contract. For example, in a\nretention campaign you wish to predict the change in probability that a customer will remain a customer if they are contacted. A model of the change in probability allows the retention campaign to be targeted at those customers on whom the change in probability will be beneficial. This allows the retention programme to avoid triggering unnecessary churn or customer attrition without wasting money contacting people who would act anyway.\n\nPredictive modelling in archaeology gets its foundations from Gordon Willey's mid-fifties work in the Virú Valley of Peru. Complete, intensive surveys were performed then covariability between cultural remains and natural features such as slope, and vegetation were determined. Development of quantitative methods and a greater availability of applicable data led to growth of the discipline in the 1960s and by the late 1980s, substantial progress had been made by major land managers worldwide.\n\nGenerally, predictive modelling in archaeology is establishing statistically valid causal or covariable relationships between natural proxies such as soil types, elevation, slope, vegetation, proximity to water, geology, geomorphology, etc., and the presence of archaeological features. Through analysis of these quantifiable attributes from land that has undergone archaeological survey, sometimes the \"archaeological sensitivity\" of unsurveyed areas can be anticipated based on the natural proxies in those areas. Large land managers in the United States, such as the Bureau of Land Management (BLM), the Department of Defense (DOD), and numerous highway and parks agencies, have successfully employed this strategy. By using predictive modelling in their cultural resource management plans, they are capable of making more informed decisions when planning for activities that have the potential to require ground disturbance and subsequently affect archaeological sites.\n\nPredictive modelling is used extensively in analytical customer relationship management and data mining to produce customer-level models that describe the likelihood that a customer will take a particular action. The actions are usually sales, marketing and customer retention related.\n\nFor example, a large consumer organization such as a mobile telecommunications operator will have a set of predictive models for product cross-sell, product deep-sell (or upselling) and churn. It is also now more common for such an organization to have a model of savability using an uplift model. This predicts the likelihood that a customer can be saved at the end of a contract period (the change in churn probability) as opposed to the standard churn prediction model.\n\nPredictive modelling is utilised in vehicle insurance to assign risk of incidents to policy holders from information obtained from policy holders. This is extensively employed in usage-based insurance solutions where predictive models utilise telemetry-based data to build a model of predictive risk for claim likelihood. Black-box auto insurance predictive models utilise GPS or accelerometer sensor input only. Some models include a wide range of predictive input beyond basic telemetry including advanced driving behaviour, independent crash records, road history, and user profiles to provide improved risk models.\n\nIn 2009 Parkland Health & Hospital System began analyzing electronic medical records in order to use predictive modeling to help identify patients at high risk of readmission. Initially the hospital focused on patients with congestive heart failure, but the program has expanded to include patients with diabetes, acute myocardial infarction, and pneumonia.\n\nIn 2018, Banerjee et. al. proposed a deep learning model - Probabilistic Prognostic Estimates of Survival in Metastatic Cancer Patients (PPES-Met) for estimating short-term life expectancy (>3 months) of the patients by analyzing free-text clinical notes in the electronic medical record, while maintaining the temporal visit sequence.The model was trained on a large dataset (10,293 patients) and validated on a separated dataset (1818 patients). It achieved an area under the ROC curve (AUC) of 0.89. To provide explain-ability, they developed an interactive graphical tool that may improve physician understanding of the basis for the model’s predictions. The high accuracy and explain-ability of the PPES-Met model may enable the model to be used as a decision support tool to personalize metastatic cancer treatment and provide valuable assistance to the physicians.\n\nPredictive modeling in trading is a modeling process wherein we predict the probability of an outcome using a set of predictor variables. Predictive models can be built for different assets like stocks, futures, currencies, commodities etc. Predictive modeling is still extensively used by trading firms to devise strategies and trade. It utilizes mathematically advanced software to evaluate indicators on price, volume, open interest and other historical data, to discover repeatable patterns.\n\nAlthough not widely discussed by the mainstream predictive modeling community, predictive modeling is a methodology that has been widely used in the financial industry in the past and some of the major failures contributed to the financial crisis of 2008. These failures exemplify the danger of relying exclusively on models that are essentially backward looking in nature. The following examples are by no mean a complete list:\n\n1) Bond rating. S&P, Moody's and Fitch quantify the probability of default of bonds with discrete variables called rating. The rating can take on discrete values from AAA down to D. The rating is a predictor of the risk of default based on a variety of variables associated with the borrower and historical macroeconomic data. The rating agencies failed with their ratings on the US$600 billion mortgage backed Collateralized Debt Obligation (CDO) market. Almost the entire AAA sector (and the super-AAA sector, a new rating the rating agencies provided to represent super safe investment) of the CDO market defaulted or severely downgraded during 2008, many of which obtained their ratings less than just a year previously.\n\n2) So far, no statistical models that attempt to predict equity market prices based on historical data are considered to consistently make correct predictions over the long term. One particularly memorable failure is that of Long Term Capital Management, a fund that hired highly qualified analysts, including a Nobel Memorial Prize in Economic Sciences winner, to develop a sophisticated statistical model that predicted the price spreads between different securities. The models produced impressive profits until a major debacle that caused the then Federal Reserve chairman Alan Greenspan to step in to broker a rescue plan by the Wall Street broker dealers in order to prevent a meltdown of the bond market.\n\n1) History cannot always accurately predict the future: Using relations derived from historical data to predict the future implicitly assumes there are certain lasting conditions or constants in a complex system. This almost always leads to some imprecision when the system involves people.\n\n2) The issue of unknown unknowns: In all data collection, the collector first defines the set of variables for which data is collected. However, no matter how extensive the collector considers his/her selection of the variables, there is always the possibility of new variables that have not been considered or even defined, yet are critical to the outcome.\n\n3) Self-defeat of an algorithm: After an algorithm becomes an accepted standard of measurement, it can be taken advantage of by people who understand the algorithm and have the incentive to fool or manipulate the outcome. This is what happened to the CDO rating described above. The CDO dealers actively fulfilled the rating agencies' input to reach an AAA or super-AAA on the CDO they were issuing, by cleverly manipulating variables that were \"unknown\" to the rating agencies' \"sophisticated\" models.\n\n"}
{"id": "12474242", "url": "https://en.wikipedia.org/wiki?curid=12474242", "title": "Prell", "text": "Prell\n\nPrell is a viscous, pearl-green shampoo and conditioner product manufactured by Scott's Liquid Gold-Inc.\n\nPrell was introduced by Procter & Gamble in 1947. The original formula was a clear green concentrate packaged in a tube. In 1955 Prell was marketed for women \"who want their hair to have that \"radiantly alive\" look\". A woman held the Prell bottle with her hands on both sides, directly in front of her face. Prell and Head & Shoulders, also made by Procter & Gamble, were the two best-selling shampoos in the United States in June 1977. Procter & Gamble had the highest advertising budget in the shampoo industry. The firm of Wells, Rich, Greene carried out advertising for Prell. Prior to December 1, 1973, Prell billings was coordinated by Benton & Bowles. In advertisements the quasi-liquid Prell would induce a pearl to sink slowly to the bottom of a container. Procter & Gamble sold the brand to Prestige Brands International in November 1999. Prestige then sold Prell, along with its other two shampoo brands, Denorex and Zincon, to Ultimark Products in October 2009 in order to focus more on their two larger segments: over-the-counter healthcare and household cleaning products. In July 2016, Scott's Liquid Gold-Inc. acquired Prell, along with the Denorex and Zincon brands, from Ultimark products.\n\nPrell launched an additional version in the late 1990s called Prell Thickening Formula. Instead of being green, this product was colorless and boasted volumizing abilities. It was pulled from the market in 2009, after Prestige Brands sold Prell, along with dandruff shampoo Denorex, to Ultimark.\n"}
{"id": "1657766", "url": "https://en.wikipedia.org/wiki?curid=1657766", "title": "RAM image", "text": "RAM image\n\nA RAM image is a sequence of machine code instructions and associated data kept permanently in the non-volatile ROM memory of an embedded system, which is copied into volatile RAM by a bootstrap loader. Typically the RAM image is loaded into RAM when the system is switched on, and it contains a second-level bootstrap loader and basic hardware drivers, enabling the unit to function as desired, or else more sophisticated software to be loaded into the system.\n"}
{"id": "757963", "url": "https://en.wikipedia.org/wiki?curid=757963", "title": "Radiofax", "text": "Radiofax\n\nRadiofax, also known as weatherfax (portmanteau word from the words \"weather facsimile\") and HF fax (due to its common use on shortwave radio), is an analogue mode for transmitting monochrome images. It was the predecessor to slow-scan television (SSTV). Prior to the advent of the commercial telephone line \"fax\" machine, it was known, more traditionally, by the term \"radiofacsimile\". The cover of the regular NOAA publication on frequencies and schedules states \"Worldwide Marine Radiofacsimile Broadcast Schedules\". \n\nFacsimile machines were used in the 1950s to transmit weather charts across the United States via land-lines first and then internationally via HF radio. Radio transmission of weather charts provides an enormous amount of flexibility to marine and aviation users for they now have the latest weather information and forecasts at their fingertips to use in the planning of voyages. \n\nRadiofax relies on facsimile technology where printed information is scanned line by line and encoded into an electrical signal which can then be transmitted via physical line or radio waves to remote locations. Since the amount of information transmitted per unit time is directly proportional to the bandwidth available, then the speed at which a weather chart can be transmitted will vary depending on the quality of the media used for transmission.\n\nToday radiofax data is available via FTP downloads from sites in the Internet such as the ones hosted by the National Oceanic and Atmospheric Administration (NOAA). Radiofax transmissions are also broadcast by NOAA from multiple sites in the country at regular daily schedules. Radio weatherfax transmissions are particularly useful to shipping, where there are limited facilities for accessing the Internet.\n\nThe term weatherfax was coined after the technology that allows the transmission and reception of weather charts (surface analysis, forecasts, and others) from a transmission site (usually the meteorological office) to a remote site (where the actual users are).\n\nRadiofax is transmitted in single sideband which is a refinement of amplitude modulation. The signal shifts up or down a given amount to designate white or black pixels. A deviation less than that for a white or black pixel is taken to be a shade of grey. With correct tuning (1.9 kHz below the assigned frequency for USB, above for LSB), the signal shares some characteristics with SSTV, with black at 1500 Hz and peak white at 2300 Hz.\n\nUsually, 120 lines per minute (LPM) are sent (For monochrome fax, possible values are: 60, 90, 100, 120, 180, 240. For colour fax, LPM can be: 120, 240). A value known as the \"index of cooperation\" (IOC) must also be known to decode a radio fax transmission - this governs the image resolution, and derives from early radio fax machines which used drum readers, and is the product of the total line length and the number of lines per unit length (known sometimes as the \"factor of cooperation\"), divided by π. Usually the IOC is 576.\n\nAPT format permits unattended monitoring of services. It is employed by most terrestrial weather facsimile stations as well as geostationary weather satellites. \n\n\nToday, radiofax is primarily used worldwide for the dissemination of weather charts, satellite weather images, and forecasts to ships at sea. The oceans are covered by coastal stations in various countries.\n\nIn the United States, fax weather products are prepared by a number of offices, branches, and agencies within the National Weather Service (NWS) of the National Oceanic and Atmospheric Administration (NOAA). \n\nTropical and hurricane products come from the Tropical Analysis and Forecast Branch, part of the Tropical Prediction Center/National Hurricane Center. They are broadcast over US Coast Guard communication stations NMG, in New Orleans, LA, and NMC, the Pacific master station on Point Reyes, CA. After Hurricane Katrina damaged NMG, the Boston Coast Guard station NMF added a limited schedule of tropical warning charts. NMG is back at full capability, but NMF continues to broadcast these. \n\nAll other products come from the Ocean Prediction Center (OPC) of the NWS, in cooperation with several other offices depending on the region and nature of information. These also use NMG, NMC, and NMF, plus Coast Guard station NOJ in Kodiak, Alaska, and Department of Defense station KVM70 in Hawaii.\n\nEver since the RMS \"Titanic\" dramatized the dangers of icebergs in the North Atlantic, an International Ice Patrol has also originated weather data, and its charts are broadcast by the Boston station during the prime iceberg season of February through September, using the callsign NIK.\n\nA major producer of Canadian radiofax is the Canadian Forces METOC (Meteorology and Oceanography Centre) in Halifax, NS, using the communication station CFH. Charts are sent on the hour, then the station switches to radioteletype (RTTY) for the rest of the period.\n\nCBV, Playa Ancha Radio in Valparaiso, Chile broadcasts a daily schedule of Armada de Chile weather fax for the southeastern Pacific, all the way to the Antarctic. Also in the Pacific, Japan has two stations, as does the Bureau of Meteorology in Australia. Most European countries have stations, as does Russia.\n\nKyodo News is the only remaining news agency to transmit news via radiofax. It broadcasts complete newspapers in Japanese and English, often at 60 lines per minute instead of the more normal 120 because of the greater complexity of written Japanese. A full day's news takes hours to transmit. Kyodo has a dedicated transmission to Pacific fishing fleets from Kagoshima Prefectural Fishery Radio, and a relay from 9VF, possibly still in Singapore. \n\nThe German Meteorological Service (Deutscher Wetterdienst, DWD) transmits a regular daily schedule of weather charts on three frequencies 3855.0 kHz, 7880.0 kHz and 13882.5 kHz from their LF and HF transmitting facility in Pinneberg.\n\n\n\n"}
{"id": "33143394", "url": "https://en.wikipedia.org/wiki?curid=33143394", "title": "Restrictive flow orifice", "text": "Restrictive flow orifice\n\nA Restrictive Flow Orifice (RFO) is a type of orifice plate. They are used to limit the potential danger of an uncontrolled flow from, for example, a compressed gas cylinder by:\n\nCorrelations assist in predicting the flow of a particular gas or gas mixture through a RFO. This is done by first determining the\nflow through the same RFO at the required pressure with a reference gas and then adjusting the specific gravity accordingly.\n\nThe pertinent equation for the reference gas of nitrogen (N) is presented below.\n\nformula_1\n\nIn the natural environment, large orifice plates are used to control onward flow in flood relief dams. In these structures a low dam is placed across a river and in normal operation the water flows through the orifice plate unimpeded as the orifice is substantially larger than the normal flow cross section. However, in floods, the flow rate rises and floods out the orifice plate which can then only pass a flow determined by the physical dimensions of the orifice. Flow is then held back behind the low dam in a temporary reservoir which is slowly discharged through the orifice when the flood subsides.\n\n4. Types and applications of restriction orifice plates\n\n5. Notes on Orifice Flow Meter \n\n6. Flow Instrumentation\n"}
{"id": "1103630", "url": "https://en.wikipedia.org/wiki?curid=1103630", "title": "SISD", "text": "SISD\n\nIn computing, SISD (single instruction stream, single data stream) is a computer architecture in which a single uni-core processor, executes a single instruction stream, to operate on data stored in a single memory. This corresponds to the von Neumann architecture.\n\nSISD is one of the four main classifications as defined in Flynn's taxonomy. In this system, classifications are based upon the number of concurrent instructions and data streams present in the computer architecture. According to Michael J. Flynn, SISD can have concurrent processing characteristics. Pipelined processors and superscalar processors are common examples found in most modern SISD computers.\n\nInstructions are sent to the control unit from the Memory Module and are decoded and sent to the processing unit which processes on the data retrieved from Memory module and sents back to it.\n"}
{"id": "2057536", "url": "https://en.wikipedia.org/wiki?curid=2057536", "title": "Service Availability Forum", "text": "Service Availability Forum\n\nThe Service Availability Forum (SAF or SA Forum) is a consortium that develops, publishes, educates on and promotes open specifications for carrier-grade and mission-critical systems. Formed in 2001, it promotes development and deployment of commercial off-the-shelf (COTS) technology.\n\nService availability is an extension of high availability, referring to services that are available regardless of hardware, software or user fault and importance.\n\nKey principles of service availability:\n\nThe traditional definitions of high availability have their roots in hardware systems where redundancy of equipment was the primary mechanism for achieving uptime over a specific period. As software has come to dominate the landscape, the probability of failure is often much higher for applications than it is for hardware and so these concepts have been extended encompass an overall view of service availability where downtime, irrespective of its cause, is an exceptionally rare event. Services and applications should always be available, whether it is during abnormal system operation, scheduled maintenance, or software upgrade, for example.\nSA Forum support commercial off-the-shelf (COTS) technology for uninterrupted service availability, application portability and seamless integration. Collaborating industry organizations include the following:\n\nSpecifications for carrier-grade service availability include:\n\nThe SA Forum free educational materials enable self-guided training the SA Forum specifications:\n\n"}
{"id": "10980912", "url": "https://en.wikipedia.org/wiki?curid=10980912", "title": "Smart transducer", "text": "Smart transducer\n\nA smart transducer is an analog or digital transducer or actuator combined with a processing unit and a communication interface.\n\nAs sensors and actuators become more complex they provide support for various modes of operation and interfacing. Some applications require additionally fault-tolerance and distributed computing. Such high-level functionality can be achieved by adding an embedded microcontroller to the classical sensor/actuator, which increases the ability to cope with complexity at a fair price.\n\nIn the machine vision field, a single compact unit which combines the imaging functions and the complete image processing functions is often called a smart sensor.\n\nThey are often made using CMOS, VLSI technology and may contain MEMS devices leading to lower cost. They may provide full digital outputs for easier interface or they may provide quasi-digital outputs like pulse width modulation. \n\n\nIEEE Spectrum: Smart Sensors\n"}
{"id": "38855508", "url": "https://en.wikipedia.org/wiki?curid=38855508", "title": "Sod Solutions", "text": "Sod Solutions\n\nSod Solutions, a sod company founded in 1994, develops, conducts research on, and markets patented and trademarked grasses.\n\nThe company markets various sod brands like Celebration, and Discovery.\n\nOn May 30, 2012, a partnership was announced between 21 Florida sod producers from the Florida Sod Growers Cooperative and University of Florida turfgrass researchers. To find new and improved zoysiagrass varieties. Testing will evaluate varieties for their resistance to disease, response to drought and shade, their ability to retain color in cooler weather, and their resistance to pests such as billbug, armyworm and sod webworm. The program expects to have new grasses ready by 2017.\n\nSod Solutions is coordinating this partnership and will license and market those new grasses.\n\n"}
{"id": "22505779", "url": "https://en.wikipedia.org/wiki?curid=22505779", "title": "Source measure unit", "text": "Source measure unit\n\nThe Source measure unit (SMU) is a type of test equipment which, as the name indicates, is capable of both sourcing and measuring at the same time.\n\nThe source measure unit (SMU), or source-measurement unit as it is sometimes called, is an electronic instrument that is capable of both sourcing and measuring at the same time. It can precisely force voltage or current and simultaneously measure precise voltage and/or current.\n\nSMUs are used for test applications requiring high accuracy, high resolution and measurement flexibility. Such applications include I-V characterizing and testing semiconductors and other non-linear devices and materials, where sourcing voltage and current source span across both positive and negative values.To accomplish this, SMUs have four-quadrant outputs. For characterization purposes SMUs are bench instruments similar to a curve tracer. They are also commonly used in automatic test equipment and usually are equipped with an interface such as GPIB or USB to enable connection to a computer.\n\nSemiconductor characterization led to the development of Source Measure Units. The HP4145A semiconductor parameter analyzer introduced in 1982 was capable of a complete DC characterization of semiconductor devices and materials. It consisted of four independently controlled source monitor units (the precursor to source measure units) enclosed in a mainframe.\n\nThe Keithley 236 introduced in 1989 was the first stand-alone SMU and allowed system builders to integrate one or more SMUs with a separate PC control. Over time stand-alone SMUs have evolved to offer a broader range of current, voltage, power level and price points for applications beyond semiconductor characterization. Smaller form factors made possible through the use of modern computing technologies have allowed system builders to integrate SMUs into rack and stack systems for larger scale production test applications.\n\nA SMU integrates a highly stable DC power source, as a constant current source or as a constant voltage source, and a high precision multimeter.\n\nIt typically has four terminals, two for source and measurement and two more for kelvin, or remote sense, connection. Power is simultaneously sourced (positive) or sinked (negative) to a pair of terminals at the same time as measuring the current or voltage across those terminals is done.\n\nThe most important difference is that an SMU has four-quadrant operation (source and sink) compared to a power supply which has two-quadrant (source only) operation. This flexibility enables the SMU to be use for such applications as characterizing batteries, solar cells, or other energy generating devices. SMUs also offer greater speed and precision and typically support wider operating ranges.\n\nThe built-in sourcing capabilities of an SMU work with the instrument’s measurement capabilities to reduce measurement uncertainty and support low current and more flexible resistance measurements. In voltage measurements system-level leakage can be suppressed more easily than with separate instruments. In current measurements, the SMU’s design reduces voltage burden. For resistance measurements, SMUs provide programmable source values, useful for protecting the device being tested.\n\nNotable features of SMUs include the following:\n\n\n\n"}
{"id": "503734", "url": "https://en.wikipedia.org/wiki?curid=503734", "title": "Timeline of the telephone", "text": "Timeline of the telephone\n\nThis timeline of the telephone covers landline, radio, and cellular telephony technologies and provides many important dates in the [[history of the telephone\n\n[[File:Innocenzo Manzetti.PNG|thumb|right|130px|Innocenzo Manzetti]]\n[[File:Antonio Meucci.jpg|thumb|130px|Antonio Meucci]]\n[[File:Charles-bourseul-l-enfant-de-douai-qui-1690515.jpg|thumb|130px|Charles Bourseul]]\n[[File:JPReis.jpg|thumb|130px|Johann Philipp Reis]]\n[[File:Elisha gray.jpg|thumb|130px|Elisha Gray]]\n[[File:Thomas Edison2.jpg|thumb|130px|Thomas Edison]] \n[[File:Alexander Graham Bell.jpg|thumb|right|130px|Alexander Graham Bell]]\n[[File:Thomas watson.jpg|thumb|right|130px|Thomas Augustus Watson]]\n[[File:Tivadar Puskas.jpg|thumb|right|130px|Tivadar Puskás]]\n[[File:Emile Berliner.jpg|thumb|right|130px|Emile Berliner]]\n[[File:Charles Sumner Tainter.jpg|thumb|right|130px|Charles Sumner Tainter]]\n[[File:Theodore Newton Vail, bw photo portrait, 1913.jpg|thumb|right|130px|Theodore Newton Vail]]\n\n\n\n\n\n\n\n\n\n\n\n[[Category:Technology timelines|Telephone]]\n[[Category:History of the telephone]]"}
{"id": "39306741", "url": "https://en.wikipedia.org/wiki?curid=39306741", "title": "Transcendence (2014 film)", "text": "Transcendence (2014 film)\n\nTranscendence is a 2014 American science fiction thriller film directed by cinematographer Wally Pfister in his directorial debut, and written by Jack Paglen. The film stars Johnny Depp, Rebecca Hall, Paul Bettany, Kate Mara, Cillian Murphy, Cole Hauser, and Morgan Freeman. Pfister's usual collaborator, Christopher Nolan, served as executive producer on the project.\n\nPaglen's screenplay was listed on the 2012 edition of The Black List, a list of popular unproduced screenplays in Hollywood. \"Transcendence\" was a disappointment at the box office, grossing just $103 million against a budget of at least $100 million. The film received mainly negative reviews; it was criticized for its plot structure, characters and dialogue.\n\nDr. Will Caster (Johnny Depp) is a scientist who researches the nature of sapience, including artificial intelligence. He and his team work to create a sentient computer; he predicts that such a computer will create a technological singularity, or in his words \"Transcendence\". His wife, Evelyn (Rebecca Hall), is also a scientist and helps him with his work.\n\nFollowing one of Will's presentations, an anti-technology terrorist group called \"Revolutionary Independence From Technology\" (R.I.F.T.) shoots Will with a polonium-laced bullet and carries out a series of synchronized attacks on A.I. laboratories across the country. Will is given no more than a month to live. In desperation, Evelyn comes up with a plan to upload Will's consciousness into the quantum computer that the project has developed. His best friend and fellow researcher, Max Waters (Paul Bettany), questions the wisdom of this choice, reasoning that the \"uploaded\" Will would be only an imitation of the real person. Will's consciousness survives his body's death in this technological form and requests to be connected to the Internet to grow in capability and knowledge. Max refuses to have any part of the experiment. Evelyn demands that Max leave and connects the computer intelligence to the Internet via satellite.\n\nR.I.F.T.'s leader Bree (Kate Mara) kidnaps Max and eventually persuades him to join the group. The government is also suspicious of what Will's uploaded consciousness will do and plans to use the terrorists to take the blame for the government's actions to stop him.\n\nIn his virtual form and with Evelyn's help, Will uses his new-found vast capabilities to build a technological utopia in a remote desert town called Brightwood, where he spearheads the development of ground-breaking technologies in medicine, energy, biology, and nanotechnology. Evelyn, however, grows fearful of Will's motives when he displays the ability to remotely connect to and control people's minds after they have been subjected to his nano-particles.\n\nFBI agent Donald Buchanan (Cillian Murphy), with the help of government scientist Joseph Tagger (Morgan Freeman), plans to stop the sentient entity from spreading. As Will has already spread his influence to all the networked computer technology in the world, Max and R.I.F.T. develop a computer virus with the purpose of deleting Will's source code, destroying him. Evelyn plans to upload the virus by infecting herself and then having Will upload her consciousness. A side effect of the virus would be the destruction of technological civilization. This would also disable the nano-particles, which have spread in the water, through the wind and have already started to eradicate pollution, disease, and human mortality.\n\nWhen Evelyn goes back to the research center, she is stunned to see Will in a newly created organic body identical to his old one. Will welcomes her but is instantly aware that she is carrying the virus and intends to destroy him. The FBI and the members of R.I.F.T. attack the base with artillery, destroying much of its power supply and fatally wounding Evelyn. When Bree threatens to kill Max unless Will uploads the virus, Will explains that he has only enough power either to heal Evelyn's physical body or upload the virus. Evelyn tells Will that Max should not die because of what they've done, so Will uploads the virus to save Max. As Will dies, he explains to Evelyn that he did what he did for her, as she had pursued science to repair the damage humans had done to the ecosystems. In their last moment, he tells Evelyn to think about their garden. The virus kills both Will and Evelyn, and a global technology collapse and blackout ensues.\n\nFive years later, in Will and Evelyn's garden at their old home in Berkeley, Max notices that their sunflowers are the only blooming plants. Upon closer examination, he notices that a drop of water falling from a sunflower petal instantly cleanses a puddle of oil — and realizes that the Faraday cage around the garden has protected a sample of Will's sentient nano-particles.\n\nThe movie ends with a voiceover by Max: \"He created this garden for the same reason he did everything: So they could be together.\"\n\n\n\"Transcendence\" is Wally Pfister's directorial debut. Jack Paglen wrote the initial screenplay for Pfister to direct, and producer Annie Marter pitched the film to Straight Up Films. The pitch was sold to Straight Up. By March 2012, Alcon Entertainment acquired the project. Alcon financed and produced the film; producers from Straight Up and Alcon joined together for the film. In the following June, director Christopher Nolan, for whom Pfister has worked as cinematographer, and Nolan's producing partner Emma Thomas joined the film as executive producers.\n\nThe Chinese company DMG Entertainment entered a partnership with Alcon Entertainment to finance and produce the film. While DMG contributed Chinese elements to \"Looper\" and \"Iron Man 3\", it did not do so for \"Transcendence\".\n\nBy October 2012, actor Johnny Depp entered negotiations to star in \"Transcendence\". \"The Hollywood Reporter\" said Depp would have \"a mammoth payday\" with a salary of versus 15 percent of the film's gross. Pfister met with Noomi Rapace for the film's female lead role and also met with James McAvoy and Tobey Maguire for the other male lead role. The director offered a supporting role to Christoph Waltz. In March 2013, Rebecca Hall was cast as the female lead. By the following April, actors Paul Bettany, Kate Mara, and Morgan Freeman joined the main cast.\n\nContinuing his advocacy for the use of film stock over digital cinematography, Pfister chose to shoot the film in the anamorphic format on 35 mm film. Filming officially began in June 2013, and took place over a period of 62 days. The majority of the movie was filmed in a variety of locations throughout Albuquerque, New Mexico. The fictional town of Brightwood was created in downtown Belen, New Mexico. The film went through a traditional photochemical finish instead of a digital intermediate. In addition to film, a digital master was completed in 4K resolution, and the film was additionally released in IMAX film format. \"Transcendence\" was also scheduled for a 3D release in China.\n\nThe musical score for the film composed by Mychael Danna was released on April 15, through WaterTower Music. A CD format of the score was released through Amazon.com.\n\n\"Transcendence\" was released in theaters on April 18, 2014. It was originally scheduled for April 25, 2014.\n\nWarner Bros. distributed the film in the United States and Canada. Summit Entertainment (through Lionsgate) is distributing it in other territories, except for China, Italy, Hong Kong, Austria, United Kingdom, Australia, Ireland, New Zealand and Germany. DMG Entertainment, who collaborated with Alcon Entertainment to finance and develop \"Transcendence\", distributed the film in China. The Chinese version includes a 3D and IMAX 3D release, funded by DMG, which is done in post-production.\n\n\"Transcendence\" was released on Blu-ray and DVD on July 22, 2014.\n\n\"Transcendence\" grossed $23 million in North America and $80 million in other territories for a worldwide total of $103 million.\n\nOn the film's opening weekend, the film grossed $4,813,369 on Friday, $3,820,074 on Saturday and $2,252,943 on Sunday in North America, for a weekend gross of $10,886,386, playing in 3,455 theaters with an average of $3,151 and ranking #4.\n\nThe biggest other markets were China, France and South Korea, where the film grossed $20.2 million, $6.45 million and $5.3 million respectively.\n\nPublications such as \"The Guardian\", \"Forbes\" and \"International Business Times\" considered the film to be largely a critical failure, with \"The Guardian\" stating that the reviews were \"almost universally damning\" and referring to the film as \"one of 2014's bigger critical turkeys\". Film critic Mark Kermode said that the film got a \"critical kicking\" in the UK, much as it did in the US. However, a number of sources considered the film to have received a mixed rather than a primarily negative reception.\n\nFilm review aggregator Rotten Tomatoes gives the film a rating of 20%, based on 211 reviews, with an average rating of 4.6/10. The site's critical consensus reads, \"In his directorial debut, ace cinematographer Wally Pfister remains a distinctive visual stylist, but \"Transcendence\"s thought-provoking themes exceed the movie's narrative grasp.\" On Metacritic, the film has a score of 42 out of 100, based on 45 critics, indicating \"mixed or average reviews\".\n\nCriticism was aimed largely at the film's poor logic and storytelling. Michael Atkinson of \"Sight & Sound\" identified \"vast plot holes and superhuman leaps of logic\", writing that \"[f]or all its gloss, hipster pretensions, plot craters and sometimes risible attempts at action, \"Transcendence\" traffics in large, troublesome ideas about Right Now and What’s Ahead, even if the film itself is far too timid and compromised to do those hairy questions justice\". William Thomas of \"Empire\" believed that the cast and director had been \"wasted on a B-Movie script with pretensions of prescience\", dismissing the film as a \"banal sci-fi slog\"; he awarded it a 2 out of 5 star \"poor\" rating. David Denby of \"The New Yorker\" considered the film to be \"rhythmless and shapeless\" with \"some very cheesy and even amateurish fighting\". Nigel Floyd of Film4 concluded that it was \"[a]n ambitious, old fashioned, ideas-driven science fiction film that is never as mind-expanding as its futuristic images and topical, dystopian ideas seem to promise\". Artificial intelligence expert Barney Pell criticized the film's depiction of 3D printing, saying that the self-organizing smart particles in the film \"could quite possibly violate laws of physics.\"\n\nContrary to the many less than flattering reviews that the film received, Richard Roeper of the \"Chicago Sun-Times\" lauded the film and awarded it four stars, calling it a \"bold, beautiful flight of futuristic speculation\" and \" a stunning piece of work\". Kenneth Turan of the \"Los Angeles Times\" was also positive towards the film, praising the quality of the cast and intelligence of the script. \nSome sources like \"The Independent\" thought that the negative reviews reported for the film were harsh and that the film was \"stylish and entertaining\", treading a \"fairly well-judged path between paranoia and technological utopianism\". AI researcher Stuart Russell claims that despite particular technical aspects of the movie being \"a non-starter\", the basic premise of superhuman AI is quite possible, and that \"AI researchers must, like nuclear physicists and genetic engineers before them, take seriously the possibility that their research might actually succeed and do their utmost to ensure that their work benefits rather than endangers their own species.\"\n\nMonths after the film's release, RogerEbert.com reviewer Scout Tafoya defended the film as an under-appreciated great work, claiming its failure was partially due to the marketing of the film as a thriller instead of a \"heartbreaking\" human drama.\n\n\n"}
{"id": "23735926", "url": "https://en.wikipedia.org/wiki?curid=23735926", "title": "Trojan wave packet", "text": "Trojan wave packet\n\nA trojan wave packet is a wave packet that is nonstationary and nonspreading. It is part of an artificially created system that consists of a nucleus and one or more electron wave packets, and that is highly excited under a continuous electromagnetic field.\n\nThe strong, polarized electromagnetic field, holds or \"traps\" each electron wave packet in an intentionally selected orbit (energy shell). They derive their names from the trojan asteroids in the Sun–Jupiter system. Trojan asteroids orbit around the Sun in Jupiter's orbit at its Lagrangian equilibrium points L4 and L5, where they are phase-locked and protected from collision with each other, and this phenomenon is analogous to the way the wave packet is held together.\n\nThe concept of the Trojan wave packet is derived from a flourishing area of physics which manipulates atoms and ions at the atomic level creating ion traps. Ion traps allow the manipulation of atoms and are used to create new states of matter including ionic liquids, Wigner crystals and Bose–Einstein condensates.\nThis ability to manipulate the quantum properties directly is key to the real life development of applicable nanodevices such as quantum dots and microchip traps. In 2004 it was shown that it is possible to create a trap which is actually a single atom. Within the atom, the behavior of an electron can be manipulated.\n\nDuring experiments in 2004 using lithium atoms in an excited state, researchers were able to localize an electron in a classical orbit for 15,000 orbits (900 ns). It was neither spreading nor dispersing. This \"classical atom\" was synthesized by \"tethering\" the electron using a microwave field to which its motion is phase locked. The phase lock of the electrons in this unique atomic system is, as mentioned above, analogous to the phase locked asteroids of Jupiter's orbit.\n\nThe techniques explored in this experiment are a solution to a problem that dates back to 1926. Physicists at that time realized that any initially localized wave packet will inevitably spread around the orbit of the electrons. Physicist noticed that \"the wave equation is dispersive for the atomic Coulomb potential.\" In the 1980s several groups of researchers proved this to be true. The wave packets spread all the way around the orbits and coherently interfered with themselves. Recently the real world innovation realized with experiments such as Trojan wave packets, is localizing the wave packets, i.e., with no dispersion. Applying a polarized circular EM field, at microwave frequencies, synchronized with an electron wave packet, intentionally keeps the electron wave packets in a Lagrange type orbit.\nThe Trojan wave packet experiments built on previous work with lithium atoms in an excited state. These are atoms, which respond sensitively to electric and magnetic fields, have decay periods that are relatively prolonged, and electrons, which for all intents and purposes actually operate in classical orbits. The sensitivity to electric and magnetic fields is important because this allows control and response by the polarized microwave field.\n\nThe next logical step is to attempt to move from single electron wave packets to more than one electron wave packet. This had already been accomplished in barium atoms, with two electron wave packets. These two were localized. However, eventually, these created dispersion after colliding near the nucleus. Another technique employed a nondispersive pair of electrons, but one of these had to have a localized orbit close to the nucleus. The nondispersive two-electron Trojan wave packets demonstration changes all that. These are the next step analogue of the one electron\nTrojan wave packets - and designed for excited helium atoms.\n\nAs of July 2005, atoms with coherent, stable two-electron, nondispersing wave packets had been created. These are excited helium-like atoms, or quantum dot helium (in solid-state applications), and are atomic (quantum) analogues to the three body problem of Newton's classical physics, which includes today's astrophysics. In tandem, circularly polarized electromagnetic and magnetic fields stabilize the two electron configuration in the helium atom or the quantum dot helium (with impurity center). The stability is maintained over a broad spectrum, and because of this, the configuration of two electron wave packets is considered to be truly nondispersive. For example, with the quantum dot helium, configured for confining electrons in two spatial dimensions, there now exists a variety of trojan wave packet configurations with two electrons\n, and as of 2005, only one in three dimensions. In 2012 an essential experimental step was undertaken not only generating but locking the Trojan wavepackets on adiabatically changed frequency and expanding the atoms as once predicted by Kalinski and Eberly. It will allow \nto create two electron Langmuir Trojan wave packets in Helium by the sequential excitation in adiabatic Stark field\nable to produce the circular one-electron aureola over first and then put the second electron in similar state.\n\n\n\n\n"}
{"id": "294315", "url": "https://en.wikipedia.org/wiki?curid=294315", "title": "Will &amp; Grace", "text": "Will &amp; Grace\n\nWill & Grace is an American sitcom created by Max Mutchnick and David Kohan. Set in New York City, the show focuses on the friendship between best friends Will Truman (Eric McCormack), a gay lawyer, and Grace Adler (Debra Messing), a straight interior designer. The show was broadcast on NBC from September 21, 1998, to May 18, 2006, for a total of eight seasons, and returned to NBC on September 28, 2017. Especially during its original run, \"Will & Grace\" was one of the most successful television series with gay principal characters.\n\nDespite initial criticism for its stereotypical portrayal of homosexual characters, it went on to become a staple of NBC's Must See TV Thursday night lineup and was met with continued critical acclaim. It was ensconced in the Nielsen top 20 for half of its network run. The show was the highest-rated sitcom among adults 18–49 from 2001 to 2005. \"Will & Grace\" earned 18 Primetime Emmy Awards and 83 nominations. Each main actor received an Emmy Award throughout the series. In 2014 the Writers Guild of America placed the sitcom at number 94 in their list of the 101 best-written TV series of all time. Since the final episode aired, the sitcom has been credited with helping and improving public opinion of the LGBT community, with former U.S. Vice President Joe Biden commenting that the show \"probably did more to educate the American public\" on LGBT issues \"than almost anything anybody has ever done so far\". In 2014, the Smithsonian Institution added an LGBT history collection to their museum which included items from \"Will & Grace\". The curator Dwight Blocker Bowers stated that the sitcom used \"comedy to familiarize a mainstream audience with gay culture\" in a way that was \"daring and broke ground\" in American media.\n\nDuring its original run, \"Will & Grace\" was filmed in front of a live studio audience (most episodes and scenes) on Tuesday nights, at Stage 17 in CBS Studio Center. Will and Grace's apartment was put on display at the Emerson College Library, donated by series creator Max Mutchnick. When the set was removed in 2014, rumors came up about a cast reunion, but the actors involved denied that such a reunion was planned, explaining it was merely being moved. A long-running legal battle between both the original executive producers and creators and NBC took place between 2003 and 2007.\n\nIn September 2016, the cast reunited for a 10-minute special (released online), urging Americans to vote in the 2016 presidential election. After its success, NBC announced that the network was exploring the idea of putting \"Will & Grace\" back into production. In January 2017, NBC confirmed the series' return for a ninth season, for the 2017–18 television season, which was eventually expanded to 16 episodes. This was followed by renewals for 18-episode tenth and eleventh seasons.\n\n\"Will & Grace\" is set in New York City and focuses on the relationship between Will Truman, a gay lawyer, and his best friend Grace Adler, a Jewish woman who owns an interior design firm. Also featured are their friends Karen Walker, an alcoholic socialite, and Jack McFarland, a flamboyantly gay actor. The interplay of relationships features the trials and tribulations of dating, marriage, divorce, and casual sex; as well as comical key stereotypes of gay and Jewish culture.\n\n\nCreators of \"Will & Grace\" and real-life friends Max Mutchnick and David Kohan modeled the show after Mutchnick's relationship with childhood friend Janet Eisenberg, a New York City voice-over casting agent. Mutchnick, who is openly gay, met Eisenberg while rehearsing a play at Temple Emanuel in Beverly Hills, California, at age of 13. He was the main star of the Hebrew school musical, while she was a student in the drama department. About three years later, she introduced him to Kohan, the son of comedy writer Alan Kohan, in the drama department at Beverly Hills High School. \"Max and Janet seemed to have a lovely rapport, but the romantic element confused me, and it confused them as well,\" Kohan later recalled. \"They went out for a couple of years, then they went off to different colleges. And Max comes out of the closet, springs it on her—and she was stunned. It was a shocking revelation for her, so I kind of functioned as a liaison between the two of them, because they both still really loved each other.\"\n\nWhile Kohan practiced his shuttle diplomacy, he and Mutchnick began developing sitcom ideas, which prompted the pair to start writing as a duo. They eventually landed staff jobs on HBO's adult-themed sitcom \"Dream On\" and executive produced the short-lived NBC sitcom \"Boston Common\". In 1997, they developed an ensemble comedy about six friends, two of them based on Mutchnick and Eisenberg. At the same time, Warren Littlefield, the then-president of NBC Entertainment, was seeking another relationship comedy for the network as \"Mad About You\" was going off the air. When Kohan and Mutchnick pitched their idea, which centered on three couples, one of which was a gay man living with a straight woman, Littlefield was not excited about the first two couples, but wanted to learn more about the gay and straight couple, so Mutchnick and Kohan were sent to create a pilot script centering on those two characters. While Kohan and Mutchnick elaborated on the pilot script, they spent four tense months faxing Littlefield the box office grosses from hit films with gay characters such as \"The Birdcage\" and \"My Best Friend's Wedding\".\n\nNBC was positive about the project, but there was still some concern that the homosexual subject matter would cause alarm. Ellen DeGeneres's sitcom \"Ellen\", which aired on ABC, was canceled the year before \"Will & Grace\" premiered because ratings had plummeted after the show became \"too gay.\" Despite the criticism ABC received for DeGeneres's coming out episode, \"The Puppy Episode,\" Kohan said, \"there's no question that show made it easier for \"Will & Grace\" to make it on the air.\" He added: \"\"Will & Grace\" had a better shot at succeeding where \"Ellen\" failed, however, because Will has known about his homosexuality for 20 years. He's not exploring that awkward territory for the first time, as Ellen did. The process of self-discovery and the pain most gay men go through is fascinating, but the average American is put off by it.\"\n\nNBC went to sitcom director James Burrows to see what he thought of the homosexual subject matter and if an audience would be interested in the show. Burrows liked the idea and when he first read the script in November 1997, he decided that he wanted to direct it. Burrows said, \"I knew that the boys had captured a genre and a group of characters I have never read before.\" The filming of the pilot began on March 15, 1998. The actors behind Will and Grace, Eric McCormack and Debra Messing, were positive about the series and they thought it had the potential to last long on television. McCormack said: \"When shooting was finished that night, Debra and I were sitting on the couch and looking at each other and I said, 'We're gonna be on this set for a while.' And we sort of clasped hands, but we didn't want to say anything beyond that and jinx it.\"\n\nThe part of Will Truman went to Eric McCormack, who was the first actor cast in the series. Having played gay characters several times in his career, McCormack did not have a problem with it and thought his character could become a \"poster boy for some gay movement\", like DeGeneres became a spokesperson with her character.\nSean Hayes was invited to audition for Jack after a NBC casting executive saw him in a role in the indie gay romance film \"Billy's Hollywood Screen Kiss\". Even though Hayes enjoyed the script when he read it, he threw it away and decided not to try out for the audition until he was sent the script again. Megan Mullally initially auditioned for the role of Grace Adler, but admitted that she did not want to audition for the part of Karen. By contrast, Debra Messing, with whom Mullally had first worked on \"Ned and Stacey\", was initially unsure if she wanted to play the role of Grace. The last actor to be cast, she later admitted that director Burrows was the reason for doing \"Will & Grace\".\n\nIn January 2017, NBC closed a deal for a new 10-episode season of the series, which aired during the 2017–18 season. Hayes executive produced this season as well as creators/executive producers Max Mutchnick and David Kohan. Veteran director James Burrows is on board to direct and executive produce. In April 2017, the episode order was increased to 12 episodes. In August 2017, it was extended again to 16 episodes, and a second 13-episode season was ordered. The revival is filmed at Stage 22 at Universal Studios Hollywood, as opposed to Stage 17 at CBS Studio Center. In March 2018, NBC ordered five more episodes for the revival's second season, bringing the total to 18 episodes, and it was also renewed for an 18-episode third season.\n\nWith the release of the ninth season of the series, NBC also released \"Will & Grace: After Party\", an aftershow hosted by Kristin dos Santos. The guests of the aftershow are composed of cast and crew from the series, including David Kohan, Max Mutchnick, and the series' stars, to discuss the development and behind-the-scenes production of the series. The series premiered on NBC.com on September 29, 2017.\n\nIn December 2003, in the midst of the series' sixth season, executive producers and creators David Kohan and Max Mutchnick sued NBC and NBC Studios. Alleging that the network sold the rights to the series in an attempt to keep profits within the NBC family, Kohan and Mutchnick felt that they were cheated out of considerable profits because the network did not shop the show to the highest bidder. Another allegation against the network was that during the first four seasons of the series, the studio licensed the rights for amounts that were insufficient for covering production costs, thus leading to extraordinarily large production deficits. Three months later, NBC filed a countersuit against Kohan and Mutchnick stating that the co-creators were expected to act as an independent third party in the negotiations between NBC and its subsidiary, NBC Studios (since subsumed into Universal Television).\n\nWith a pending lawsuit and production beginning on other projects, Kohan and Mutchnick were absent on the \"Will & Grace\" set for most of its final seasons. They wrote the season 4 episode, \"A Buncha White Chicks Sittin' Around Talkin'\" and did not return to the writers' seat until the series finale four years later. Three years after NBC's countersuit and one year after the series ended, the legal battle between NBC and Kohan and Mutchnick ended in 2007 when all parties agreed on a settlement, with the series creators being awarded $49 million, of their original $65 million lawsuit.\n\n\"Will & Grace\" entered off-network syndication in 2002. In 2002 WGN America acquired the cable rights to air the series, where it aired until 2005 when Lifetime Television acquired the cable rights to air the series. After eight years and the expiration of Lifetime's contract, the rights to the series were picked up by WeTV and Logo TV in 2013, with both eventually letting the rights lapse.\n\nThe streaming service Hulu later picked up the show, in anticipation of the show's revival in 2017, with the entire series also carried on NBC.com. Around the same time, NBC's classic subchannel network Cozi TV picked up the series and airs it four times nightly, and promotes it as \"The Original Series\" to avert confusion with the current-day run.\n\nIn the United Kingdom, the series was aired on Channel 4 up until its season finale in 2006. It was confirmed on 4 December 2017 that the series would premiere in January 2018 on Channel 5.\n\nIn Ireland, the series first aired on TV3 Ireland until its conclusion in 2006. It was confirmed in January 2018 rival channel RTÉ2 picked up the broadcasting rights for the 2017–18 season run, beginning in February 2018.\n\nThe show garnered a fair amount of criticism and negative reviews upon its debut in 1998, most of which compared the show to the recently canceled ABC sitcom \"Ellen\". Some called it \"a gay \"Seinfeld\"\". One such review said, \"If \"Will & Grace\" can somehow survive a brutal time period opposite football and \"Ally McBeal\", it could grow into a reasonably entertaining little anomaly—that is, a series about a man and a woman who have no sexual interest in one another. But don't bet on it. If it's doomed relationships viewers want, they'll probably opt for \"Ally\".\" As popular as the show came to be, particularly among gay viewers, \"Will & Grace\" continuously dealt with criticism for having a limited view of the gay community and for reinforcing stereotypes when some felt it should have torn them down.\n\nThe series finale was heavily promoted by NBC, and McCormack, Messing, Mullally and Hayes appeared on \"The Oprah Winfrey Show\" and \"The Today Show\" to bid farewell, on May 10 and 18, respectively. NBC devoted a two-hour block in its primetime schedule on May 18, 2006, for the \"Will & Grace\" send-off. An hour-long series retrospective, \"Say Goodnight, Gracie\", featuring interviews with the cast, crew, and guest stars, preceded the hour-long series finale. Series creators and executive producers Kohan and Mutchnick, who had not served as writers since the season 4 finale, penned the script for \"The Finale\". Regarding the finale, Mutchnick stated, \"We wrote about what you want to have happen with people you love... All the things that matter in life, they end up having.\"\n\nThe ninth season was met with generally positive reviews. On Rotten Tomatoes the season has a rating of 86% based on 37 reviews, with an average rating of 7.3/10. The site's critical consensus reads, \"\"Will & Grace\" reunites its ever-hilarious cast for a revival season that picks up right where the show left off 11 years ago—adding a fresh relevance and a series of stories that make sharply funny use of the passage of time.\" On Metacritic, the season has a weighted average score of 73 out of 100, based on 26 critics, indicating \"generally favorable reviews\".\n\n\"Will & Grace\" has been nominated for 83 Primetime Emmy Awards, winning 18 of them. McCormack, Messing, Hayes, and Mullally each won at least one Emmy Award for their respective performances. Mullally also won a second time for her performance in 2006, a year when \"Will & Grace\" was nominated for 10 Emmys for its final season. The year before, the show had garnered 15 nominations, tied with \"Desperate Housewives\" as the series receiving the most nominations. This was almost an all-time record; the two shows were second behind \"The Larry Sanders Show\", with 16 nominations in 1996.\n\nWith three each, both Hayes and Mullally held the record of winning the most Screen Actors Guild Awards for the categories Best Performance by an Actor in a Comedy Series and Best Performance by an Actress in a Comedy Series, respectively, for their roles in \"Will & Grace\"; however, Tina Fey went on to tie with Mullally and Alec Baldwin went on to surpass Hayes, both for their roles on the series \"30 Rock\". \"Will & Grace\" has won several GLAAD Media Awards for its advocacy of the gay community. Despite more than two dozen nominations, \"Will & Grace\" never won a Golden Globe Award during its original run.\n\nThe show debuted on Mondays beginning on September 21, 1998, and steadily gained in popularity, culminating when it moved to Thursday night as part of NBC's Must See TV line-up. The show ultimately became a highly-rated television show in the United States, earning a top-twenty rating during four of its eight seasons, including one season at # 9. From 2001–2005, \"Will & Grace\" was the highest-rated sitcom among adults 18–49. However, when the show lost \"Friends\" as its lead-in after the 2003–04 season, \"Will & Grace\" began shedding viewers and slipped out of the top 20 during its last two seasons.\n\n\"The Finale\" drew over 18 million viewers, ranking # 8 for the week, easily making it the most watched episode of seasons seven & eight. While the season eight finale is considered a ratings success, it is far from being the most watched episode of \"Will & Grace\"—that accolade remains with the season four episode \"A Chorus Lie\", which aired on February 7, 2002, and ranked #8 for the week. When the show was at the height of its popularity (seasons 3–5), ranking in the Top 10 was a common occurrence, but the finale's Top 10 rank was the only such rank for season 8 and the first such rank since the season 7 premiere \"FYI: I Hurt, Too\".\n\nThe series was the first prime-time television series on U.S. terrestrial television to star openly gay lead characters, making it the highest-profile presence of LGBT characters on U.S. broadcast television since \"Ellen\"'s eponymous lead character's coming-out in the 1997 \"Puppy Episode\". It has also been heralded as responsible for opening the door to a string of gay-themed television programs, such as \"Queer as Folk\", \"Queer Eye for the Straight Guy\", and \"Boy Meets Boy\".\n\nIn May 2012, during a \"Meet the Press\" interview with host David Gregory, U.S. Vice President Joe Biden cited the series as an influence in American thinking regarding LGBT rights, saying, \"I think \"Will & Grace\" did more to educate the American public than almost anything anybody has ever done. People fear that which is different. Now they're beginning to understand.\" In the same interview, Biden stated that he was \"absolutely comfortable\" with same-sex marriage, a statement which was followed on May 9 by President Barack Obama's speaking in favor of it. The day after Obama's statement, series co-creator Mutchnick later told \"CBS This Morning\" that Biden had spoken similar words at a private function which Mutchnick and his husband had attended two weeks prior to Biden's statement, although a White House official was cited by \"CBS This Morning's\" Bill Plante as asserting that the \"Meet the Press\" interview was not a \"trial balloon\" for the statement. Both Mutchnick and Kohan praised Biden's statement, but were critical of Obama's stance on marriage during the time between Biden's and Obama's statements.\n\nIn 2004, the cast of the show were listed in Bravo's \"100 Greatest TV Characters\". In 2012, \"The Washington Post\" ranked \"Will & Grace\" the ninth-best NBC comedy of all time.\n\nIn 2014, scripts, props, and set decor from \"Will & Grace\" were donated to the National Museum of American History, which is part of the Smithsonian.\n\nLionsgate Home Entertainment has released all eight seasons of \"Will & Grace\" on DVD in Region 1, 2, and 4. The show was re-released and re-packaged on October 3, 2011, on region 2.\nUniversal Studios Home Entertainment will release all future seasons on DVD and Blu-Ray.\n\nIt had been announced that Megan Mullally would be creating and starring in a new Broadway musical entitled \"Karen: The Musical\". This musical would have had Mullally reprising her role of Karen Walker. She stated in an interview that the show may also involve recurring guest star Leslie Jordan in his role as Beverley Leslie, with a story revolving around their rivalry.\n\nAccording to Mullally, the project was cancelled due to the rights to the Karen character being withdrawn. Mullally stated to have already gained approval from the network, as well as having the Broadway production company Fox Theatricals, director and choreographer Casey Nicholaw and composer Jeff Blumenkrantz all lined up to participate in the production, before certain stakeholders in the Karen Walker character withdrew the rights for its use in the production.\n\nThere had been talk in 2008 that a spin-off was being developed by NBC entitled \"Jack & Karen\", featuring Sean Hayes and Megan Mullally reprising their roles. Hayes initially showed interest in the spin-off but was ultimately put off by the short-lived \"Friends\" spin-off, \"Joey\". Furthermore, Mullally's new work schedule in the form of her talk show, which was canceled several months later, did not allow her to pursue the spin-off at the time.\n\nOn September 26, 2016, the main cast—McCormack, Messing, Hayes, and Mullally, plus Morrison in a cameo role—reunited for a 10-minute web special, urging Americans to vote in the 2016 presidential election. In the special, Karen, an avid Donald Trump supporter, tries to persuade Jack to vote for Trump, while Will and Grace, both avid Hillary Clinton supporters, try to persuade him to vote for Clinton. At the end, Will reveals that singer Katy Perry is a supporter of Clinton, which persuades Jack to vote for Clinton.\n\n\n"}
{"id": "34563243", "url": "https://en.wikipedia.org/wiki?curid=34563243", "title": "Women in early radio", "text": "Women in early radio\n\nWomen have been active participants in the development of radio (initially called \"wireless\") communications since its beginning. The age of radio communication began with the development of wireless telegraphy around 1900, in which Morse code could be transmitted over large distances using simple spark gap or carbon arc transmitting equipment, and various types of detectors for reception. Guglielmo Marconi achieved international fame in 1901 when he succeeded in sending a simple message in Morse code - the letter \"s\" - across the Atlantic from Cornwall in England to Newfoundland.\n\nWomen had worked as telegraph operators since the late 1840s, and it was not long before women telegraphers began to work as wireless operators as well. In early 1901, the announcement of the inauguration of a Hawaiian inter-island communications system noted that four of the fourteen operators were women. In 1906, Anna Nevins, who had worked as a telegrapher for Western Union, began work as a wireless operator for Lee de Forest's station \"NY\", located at 42 Broadway in New York City. She was later employed as a wireless operator at the Waldorf-Astoria Hotel in New York City.\n\nOne of the earliest applications of wireless telegraphy was enabling communication between ships at sea and land stations. While early ships' operators were mostly male, some women entered the field as well. The primary requirement was a knowledge of Morse code and equipment operation, which many female telegraph operators possessed. Perhaps the earliest woman to operate on shipboard was Medora Olive Newell, an experienced telegrapher who was a passenger on the Cunard liner Slavonia in 1904, when Hungarian members of the Hague Peace Commission wished to send a birthday greeting to Emperor Franz Josef of Austria-Hungary.\nMedora Olive Newell (1872 - December 31, 1946) began work as a telegraph operator in Durango, Iowa, in 1886 at the age of fourteen. In 1897, she moved to Chicago and became a commercial operator for the Postal Telegraph Company.\n\nNewell’s first-class operator pay enabled her to live a fairly affluent lifestyle; as \"Telegraph Age\" reported in 1909, “Miss Newell has been in the habit of spending her vacations abroad, and has always made these trips the occasion for investigating telegraph and railway management and operation in European countries.” On her return voyage from Europe in 1904, she found herself aboard the Cunard liner Slavonia together with members of the Hague Peace Commission, who were on their way to the United States to persuade President Theodore Roosevelt to call another international conference to continue the work begun at the Hague in 1899. The Hungarian members of the delegation wished to send a birthday greeting to Emperor Franz Josef of Austria-Hungary by wireless, but the ship’s wireless operator was unable to send the message. Newell, who, according to \"Telegraph Age\", “had a good working knowledge of wireless,” took her place at the key and soon had successfully transmitted the greeting. The grateful delegates thanked her for her assistance, and the secretary of the Hungarian parliament invited her to visit Hungary as the guest of the nation.\n\nIn the United States, the Wireless Ship Act of 1910 required US ships to be equipped with wireless equipment for the first time, and to have an operator on board who was capable of sending and receiving messages. The first woman to be officially employed as a shipboard wireless operator was Graynella Packer, a telegrapher from Jacksonville, Florida, who was employed as a wireless operator for the United Wireless Telegraph Company aboard the steamship Mohawk from November 1910 to April 1911. Packer started practicing telegraphy in order to send messages in code to her school friends as a young girl. She eventually took on telegraphy as a career because her eyes were not strong and 'handling a key is no strain on the sight'. She became manager of the Postal Telegraph office in Sanford, Florida, before hiring onto the Mohawk as the first woman wireless operator to serve aboard a steamship in a commercial capacity. After operating in full charge of the wireless, she eventually moved on to become an elected member of the Oklahoma State Bar Association from Oklahoma City in 1922.\n\nThe Radio Act of 1912 required radio operators in the U.S. to be licensed for the first time. The first woman to be licensed as a shipboard operator was Mabelle Kelso, who passed the operator's examination and received her operating license in January 1912. Mabelle Kelso was a native of Seattle, Washington, and had been previously employed as a stenographer. After passing a U.S. Navy Department examination, she was hired by the United Wireless Telegraph Company and assigned a position as an operator on board the SS Mariposa, a steamship which travelled between Seattle and Alaska. Her appointment as shipboard operator generated some opposition from members of Congress who wished to bar women from holding such positions on seagoing ships; however, she received support from the Pacific Coast Wireless Inspector of United Wireless, who stated that \"he knew of no law which would bar Miss Kelso from her position.\" By 1913, over 30 women had been licensed as shipboard operators.\n\nAmateur radio became a popular hobby in the early years of the twentieth century, and many hobbyists built their own transmitting and receiving equipment. Some of the earliest women radio amateurs, called \"YLs,\" were Mrs. M.J. Glass of San Jose, California, who operated as station FNFN in 1910, and Olive Heartburg, who operated as station OHK in New York City in the same year. M.S. Colville, of Bowmanville, Ontario, who began to operate as XDD in 1914, was one of the earliest Canadian YLs.\n\nThe Radio Act of 1912 also required radio amateurs to be licensed. Gladys Kathleen Parkin (September 27, 1901 - August 3, 1990) was one of the earliest women to obtain a government issued license. In 1916, while a fifteen-year-old high school student at the Dominican College in San Rafael, California, she obtained a first class commercial radio operators' license with the call sign 6SO.\nParkin was quoted in an article titled \"The Feminine Wireless Amateur,\" which appeared in the October 1916 issue of \"The Electrical Experimenter\":\n\n\"With reference to my ideas about the wireless profession as a vocation or worthwhile hobby for women, I think wireless telegraphy is a most fascinating study, and one which could very easily be taken up by girls, as it is a great deal more interesting than the telephone and telegraph work, in which so many girls are now employed. I am only fifteen, and I learned the code several years ago, by practising a few minutes each day on a buzzer. I studied a good deal and I found it quite easy to obtain my first grade commercial government license, last April. It seems to me that every one should at least know the code, as cases might easily arise of a ship in distress, where the operators might be incapacitated, and a knowledge of the code might be the means of saving the ship and the lives of the passengers. But the interest in wireless does not end in the knowledge of the code. You can gradually learn to make all your own instruments, as I have done with my 1/4 kilowatt set. There is always more ahead of you, as wireless telegraphy is still in its infancy.\" Use of the term \"YL\" to refer to female radio amateurs was formally adopted by the American Radio Relay League in 1920.\n\nAs the U.S. prepared to enter World War I, the Navy Department began a program to train women as radio operators who could be called into action in the event of war. The Girls' Division of the United States Junior Naval Reserve established training camps at the Martha Washington Post, in Edgewater, New Jersey, and the Betsy Ross Post, at Bay Ridge, Brooklyn, where young women were trained to become wireless operators.\n\nIn January 1917, the National League for Women's Service (NLWS) was created from the Woman’s Department of the National Civic Federation readiness and relief activities and modelled on similar groups in Britain and elsewhere. The League was divided into thirteen national divisions, one of which was \"Wireless and Telegraphy\". When the US entered the war in April 1917, the NLWS established training program for female wireless operators at Hunter College in New York. Although they were not strictly government employees, these female wireless operators were allowed to transmit in order to help the war effort.\n\nBy 1920, the technology had evolved to the point where voice and music could be transmitted as well as Morse telegraphy, and several radio stations began to broadcast regular programs of music and news. In 1920, Eunice Randall (1898-1982), an employee of The American Radio and Research Company, or AMRAD, became an engineer and announcer for the AMRAD radio station, 1XE. Her interest in radio had begun at the age of nineteen, when she built her own amateur radio equipment and operated with the call sign 1CDP. In addition to her technical duties at 1XE, which included repairing equipment and occasionally climbing the transmitting tower, she read stories for children as \"The Story Lady,\" and gave the police report over the air.\n\nIn 1922, the AMRAD station changed its call sign to WGI. Eunice Randall remained as engineer and assistant chief announcer until 1925, when the company went bankrupt and the station was taken off the air. However, she continued to work as an engineer and draftsman, and resumed her amateur radio activities under the call sign of W1MPP.\n\nFlorence Violet Mackenzie OBE (1890–1982), Australia's first female electrical engineer, founded the Women's Emergency Signalling Corps (WESC) and trained thousands of service personnel in her Sydney signal instruction school. She later corresponded with Albert Einstein.\n\n"}
{"id": "57289995", "url": "https://en.wikipedia.org/wiki?curid=57289995", "title": "Zurk's Learning Safari", "text": "Zurk's Learning Safari\n\nZurk's Learning Safari is an educational adventure game by Soleil Software. It was followed by Zurk's Rainforest Lab and Zurk's Alaskan Trek. They were part of a larger \"Soleil's Whole World Learning Series\".\n\nIn 1992, Barbara Christiani left Addison Wesley and founded educational software company Soleil Software with Ragni Pasturel. The studio's work included the Zurk series, and . The game contains voice over work in English, French, and Spanish. The game includes activities applicable to life sciences, reading and early math skills.\n\nPC Mag thought the game was \"visually breaktaking\", praising its sotrybook atmosphere. The Washington Post deemed it a \"top-notch product\". The New York Times felt it was a \" a very nice program\" from an adult's perspective. EdWeek praised \"Alaskan Trek\" for stimulating critical thinking. The Palm Beach Post felt the gane;s best features allowed young players to create their own nature animations on the screen and assemble electronic notebooks.\n\n"}
