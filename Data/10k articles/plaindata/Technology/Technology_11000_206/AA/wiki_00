{"id": "5865902", "url": "https://en.wikipedia.org/wiki?curid=5865902", "title": "AN/SPQ-11", "text": "AN/SPQ-11\n\nThe AN/SPQ-11 Cobra Judy was a PESA radar found on the missile range instrumentation ship.\n\nIt was used for space tracking, ballistic missiles tracking and other instrumentation. Cobra Judy was the sea component of the COBRA program for monitoring missile launches and outer space. Cobra Judy was replaced by the Cobra Judy Replacement (CJR) in April 2014.\n\nThe Cobra Judy Platform, was taken out of service and stricken from the Naval Vessel Register March 31, 2014. On 31 March 2014, the Cobra Judy Replacement program, aboard reached initial operational capability (IOC). According to the Naval Sea Systems Command (NAVSEA), the U.S. Air Force also assumed operational and sustainment responsibilities for the ship.\n\n\n"}
{"id": "37445523", "url": "https://en.wikipedia.org/wiki?curid=37445523", "title": "Asus Transformer Pad Infinity", "text": "Asus Transformer Pad Infinity\n\nAsus Transformer Pad Infinity (also known as the TF700T) - is a tablet computer made by Asus, successor to the Asus Transformer Prime. The manufacturer announced it at CES 2012, less than a month after the original product launch, to launch Q2 2012.\n\nThis new model includes a Tegra 3 T33 processor clocked at 1.6 GHz (as opposed to the Prime's T30), and an upgraded 1,920×1,200-pixel-resolution screen, more than doubling the pixel count of the prior model. The display was upgraded to a Super IPS+ panel for ultra bright outdoor readability with 178° wide viewing angles. In response to the signal problems it includes a new back-panel design with the upper part made of plastic to enhance Wi-Fi, Bluetooth, and GPS performance. The front camera was boosted from 1.2 megapixels to 2 megapixels. It has improved graphics performance with a 12-core GPU.\n\nIn November 2013 a successor was released by the name of Asus Transformer Pad Infinity TF701.\nThe new Transformer Pad was upgraded with a Tegra 4 CPU, a 2560 x 1600 resolution display with 300 ppi, 2GB RAM and other upgrades over its predecessor.\n\nCyanogenMod 11 or newer is supported on the TF700T tablet.\n"}
{"id": "1683043", "url": "https://en.wikipedia.org/wiki?curid=1683043", "title": "Behavior-shaping constraint", "text": "Behavior-shaping constraint\n\nA behavior-shaping constraint, also sometimes referred to as a forcing function or poka-yoke, is a technique used in error-tolerant design to prevent the user from making common errors or mistakes. One example is the reverse lockout on the transmission of a moving automobile.\n\nThe microwave oven provides another example of a forcing function. In all modern microwave ovens, it is impossible to start the microwave while the door is still open. Likewise, the microwave will shut off automatically if the door is opened by the user. By forcing the user to close the microwave door while it is in use, it becomes impossible for the user to err by leaving the door open. Forcing functions are very effective in safety critical situations such as this, but can cause confusion in more complex systems that do not inform the user of the error that has been made.\n\nWhen automobiles first started shipping with on-board GPS systems, it was not uncommon to use a forcing function which prevented the user from interacting with the GPS (such as entering in a destination) while the car was in motion. This ensures that the driver's attention is not distracted by the GPS. However, many drivers found this feature irksome, and the forcing function has largely been abandoned. This reinforces the idea that forcing functions are not always the best approach to shaping behavior.\n\nThese forcing functions are being used in the service industry as well. Call centers concerned with credit card fraud and friendly fraud are using agent-assisted automation to prevent the agent from seeing or hearing the credit card information so that it cannot be stolen. The customer punches the information into their phone keypad, the tones are masked to the agent and are not visible in the customer relationship management software.\n\n"}
{"id": "2287649", "url": "https://en.wikipedia.org/wiki?curid=2287649", "title": "Brent Hoberman", "text": "Brent Hoberman\n\nBrent Shawzin Hoberman CBE (born 25 November 1968) is a British entrepreneur. Together with Martha Lane Fox, he founded Lastminute.com in 1998, an online travel and gift business. As CEO, Hoberman successively floated and later sold Lastminute.com Ltd to Sabre who purchased the company's equity and bond debt for £577 million (including additional gross debt of approximately £79 million and estimated cash at bank in hand of approximately £72 million). In 2006, Hoberman handed over the CEO position to Ian McCaig, although he remained as chairman and chief strategic officer until 2007.\n\nHoberman remained CEO of lastminute.com throughout 2005, until April 2006 when he handed over the CEO position to Ian McCaig and took the position of chairman and chief strategy officer. He stepped down as chief strategy officer and chairman in January 2007.\n\nHoberman founded VC-backed internet startup, mydeco.com, an online furniture and interior design site providing 3D technology for consumers to design their own rooms online in 2007 and the website launched in February 2008.\n\nHoberman is a governor of the University of the Arts London and a non-executive board director of Guardian Media Group. In 2007, Hoberman took on the role of non-executive chairman of wayn.com – a travel and leisure social network. In December 2009, he stepped down from the board of wayn.com. He is also an angel investor in several internet companies, including Viagogo, erepublik, wayn.com and academia.edu. Hoberman joined The Business Council for Britain in July 2009.\n\nIn 2009, Hoberman was selected as one of the World Economic Forum's Global Young Leaders for the UK. In July 2009, Hoberman co-founded PROfounders Capital with Michael Birch, Peter Dubens, Jonnie Goodwin, Rogan Angelini-Hurll and Sean Seton-Rogers. The fund invests in early stage internet investors.\n\nHoberman was co-founder and chairman of Made.com, a joint venture between mydeco.com and founder Ning Li. On 29 December 2015 it was reported that Hoberman was leaving PROFounders.\n\nHoberman was appointed Commander of the Order of the British Empire (CBE) in the 2015 New Year Honours for services to entrepreneurship.\n\nHoberman is also a co-founder of FoundersForum, which is a private network for digital and technology entrepreneurs. Founders Forum has been hosting invite-only events in London since 2006 and has since 2011 expanded to events in other countries such as the US, Brazil, India, Turkey and China. Speakers at the events have included prominent figures from the technology sector like Eric Schmidt, Reid Hoffman and Arianna Huffington. The events are by invitation only.\n\nHoberman was born on 25 November 1968. He was educated at the Dragon School in Oxford, then Eton College and subsequently at New College, Oxford. He is married with two daughters and one son.\n\nHoberman says his mentors have been his father and South African grandfather Leonard Shawzin, who built an empire of over 650 clothes shops from a single store. He credits his entrepreneurial drive to the business successes of his father and grandfather, and says the catalyst for him starting his own business was being fired from his first job in investment banking for being \"a prima donna\".\n\nHoberman has stated that he believes what makes a business successful is passion; in an interview with BBC 'The Bottom Line', he states his belief that the most successful small businesses are those where the founders remain passionate about their business, and are themselves part of their target market. He believes that by building a business to offer a solution to a problem you have yourself – as he did with lastminute.com and mydeco.com – founders stand a greater chance of remaining passionate and being successful.\n\nHoberman is a member of the New Enterprise Council, a group of entrepreneurs who advise the Conservative Party (UK) on policies related to the needs of business.\n\nHoberman is Jewish.\n"}
{"id": "41555832", "url": "https://en.wikipedia.org/wiki?curid=41555832", "title": "Bugsense", "text": "Bugsense\n\nBugsense is a crash reporter for mobile phones. It collects and analyzes crash reports, performance and quality of applications on mobiles, which forwards them to the creators of those applications to act on them. Supported platforms include Android, iOS, Windows Phone 7, Windows Phone, Windows 8 and HTML 5.\n\nThe company was founded in 2011 by Jon Vlachogiannis and Panayiotis Papadopoulos. Initial funds ($100.000) were acquired by Silicon Valley Greek Seed Funding Group. Bugsense expanded to supporting more than 30000 developers and companies, including Samsung, Soundcloud, and Trulia and has a market share of 3.43% of all apps. In September 2013, Bugsense was acquired by Splunk.\n"}
{"id": "247787", "url": "https://en.wikipedia.org/wiki?curid=247787", "title": "CSI: Crime Scene Investigation", "text": "CSI: Crime Scene Investigation\n\nCSI: Crime Scene Investigation, also referred to as CSI and CSI: Las Vegas, is an American procedural forensics crime drama television series which ran on CBS from October 6, 2000, to September 27, 2015, spanning 15 seasons. The series, starring William Petersen, Marg Helgenberger, George Eads, Ted Danson, Laurence Fishburne, Elisabeth Shue, and Jorja Fox, is the first in the \"CSI\" franchise. The series concluded with a feature-length finale titled \"\".\n\nMixing deduction and character-driven drama, \"CSI: Crime Scene Investigation\" follows a team of crime-scene investigators, employed by the Las Vegas Police Department, as they use physical evidence to solve murders. The team is originally led by Gil Grissom (Petersen), a socially awkward forensic entomologist and career criminalist who is promoted to CSI supervisor following the death of a trainee investigator. Grissom's second-in-command, Catherine Willows (Marg Helgenberger), is a single mother with a cop's instinct. Born and raised in Las Vegas, Catherine was a stripper before being recruited into law enforcement and training as a blood-spatter specialist. Following Grissom's departure during the ninth season of the series, Catherine is promoted to supervisor. After overseeing the training of new investigator Raymond Langston (Fishburne), Willows is replaced by D.B. Russell (Danson), and recruited to the FBI shortly thereafter. Russell is a family man, a keen forensic botanist, and a veteran of the Seattle Crime Lab. In the series' 12th season, Russell is reunited with his former partner Julie Finlay (Elisabeth Shue), who like Catherine, is a blood-spatter expert with an extensive knowledge of criminal psychology. With the rest of the team, they work to tackle Las Vegas' growing crime rate and are on the job 24/7, scouring the scene, collecting the evidence, and finding the missing pieces that will solve the mystery.\n\nDuring the 1990s, Anthony Zuiker caught producer Jerry Bruckheimer's attention after writing his first movie script. Zuiker was convinced that a series was in the concept; Bruckheimer agreed and began developing the series with Touchstone Pictures. The studio's head at the time liked the spec script and presented it to ABC, NBC, and Fox executives, who decided to pass. The head of drama development at CBS saw potential in the script, and the network had a pay-or-play contract with actor William Petersen, who said he wanted to do the \"CSI\" pilot. The network's executives liked the pilot so much, they decided to include it in their 2000 schedule immediately, airing on Fridays after \"The Fugitive\". After CBS picked up the show, the Disney owned Touchstone decided to pull out of the project, since they didn't want to spend so much money producing a show for another network (ABC is also owned by Disney). Instead of the intended effect of making CBS cancel the show (since it no longer had a producer), Bruckheimer was able to convince Alliance Atlantis to step in as a producer, saving the show and adding CBS as another producer. Initially, \"CSI\" was thought to benefit from \"The Fugitive\" (a remake of the 1960s series), which was expected to be a hit, but by the end of 2000, \"CSI\" had a much larger audience.\n\n\"CSI: Crime Scene Investigation\" was produced by Jerry Bruckheimer Television and CBS Productions, which became CBS Paramount Television in the fall of 2006 and CBS Television Studios three years later. Formerly a co-production with the now-defunct Alliance Atlantis Communications, that company's interest was later bought by the investment firm GS Capital Partners, an affiliate of Goldman Sachs. CBS acquired AAC's international distribution rights to the program, though the non-US DVD distribution rights did not change (for example, Momentum Pictures continues to own UK DVD rights). The series is currently in syndication, and reruns are broadcast in the U.S. on Oxygen, Syfy and the USA Network on cable, with Ion Television holding the broadcast syndication rights. The show has aired in reruns on the USA Network since January 14, 2011. The CSI catalog has been exclusive to the whole NBC Universal portfolio since September 2014, after several years with Viacom Media Networks' Spike and TV Land.\n\n\"CSI\" was shot at Rye Canyon, a corporate campus owned by Lockheed Martin situated in the Valencia area of Santa Clarita, California, but after episode 11, filming shifted to the Santa Clarita Studios, originally chosen for its similarity to the outskirts of Las Vegas. Occasionally, the cast still shot on location in Las Vegas (the season-four DVD set revealed that the episode \"Suckers\" was mostly shot during December 2003 in Las Vegas, where they filmed a Gothic club scene on location for rent, and in January 2004, some scenes were filmed at Caesars Palace), although primarily Las Vegas was used solely for second unit photography such as exterior shots of streets. Other California locations include Verdugo Hills High School, UCLA's Royce Hall, Pasadena City Hall, and California State University, Los Angeles. While shooting took place primarily at Universal Studios in Universal City, California, Santa Clarita's surroundings had proven so versatile, \"CSI\" still shot some outdoor scenes there.\n\n\"CSI\"'s theme song was, since the last episode of season one, \"Who Are You\", written by Pete Townshend with vocals by lead singer Roger Daltrey of The Who. Daltrey made a special appearance in the episode \"Living Legend\", which also contained many musical references such as the words \"Who's next\" on a dry-erase board in the episode's opening sequence. In certain countries, to avoid music licensing fees, a unique theme was used, instead.\n\nThroughout the series, music played an important role; artists such as Ozzy Osbourne, The Wallflowers, John Mayer, and Akon (with Obie Trice) performed onscreen in the episodes \"Skin in the Game\", \"The Accused Is Entitled\", \"Built To Kill, Part 1\", and \"Poppin' Tags\", respectively. Mogwai was often heard during scenes showing forensic tests in progress, as were Radiohead and Cocteau Twins, but several other artists lent their music to \"CSI\", including Rammstein and Linkin Park—used heavily in Lady Heather's story arc. Sigur Rós can be heard playing in the background in the episode \"Slaves of Las Vegas\", The Turtles in \"Grave Danger\", and Marilyn Manson in \"Suckers\". A cover of the Tears for Fears song \"Mad World\", arranged by Michael Andrews and featuring vocals by Gary Jules, was used in the pilot episode and during three episodes of season six (\"Room Service\", \"Killer\", and \"Way to Go\"). Industrial rock band Nine Inch Nails was also featured multiple times throughout the three series. One episode started with The Velvet Underground's excited rendition of \"Sweet Jane\" and ended with the downbeat version of Cowboy Junkies' revision of the song. Character David Hodges' good luck has, on occasion, been accompanied by Electric Light Orchestra's \"Mr. Blue Sky\". This song was first used in the season-seven episode \"Lab Rats\", and last used during season 10's \"Field Mice\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom \"CSI\", CBS produced a franchise starting in 2002 with a spin-off entitled \"\". Set in Miami, Florida, and starring David Caruso and Emily Procter, \"Miami\" later launched \"\" in 2004. Starring Gary Sinise, Sela Ward, and Melina Kanakaredes, \"NY\" was set in New York City and was based upon the idea that \"Everything is Connected\". In 2015, a fourth \"CSI\" series, entitled \"\", starring Patricia Arquette and Ted Danson, was created. It focuses on the FBI's elite Cyber Crime Division. The \"CSI\" series exists within the same fictional \"universe\" as fellow CBS police dramas \"Without a Trace\" and \"Cold Case\". A number of comic books, video games, and novels based on the series have been made.\n\nIn 2006, the Fort Worth Museum of Science and History developed a traveling museum exhibit called \"CSI: The Experience\". On May 25, 2007, Chicago's Museum of Science and Industry was the first museum to host the exhibit, and the exhibit's opening featured stars from the TV series. Also a supporting website designed for the benefit of people who cannot visit the exhibit was developed, designed by Rice University's Center for Technology in Teaching and Learning and Left Brain Media.\n\"CSI: The Experience\" also has an interactive attraction at the MGM Grand Las Vegas in Las Vegas, and the Mall of America in Minneapolis, Minnesota.\n\nDuring its 15 years in production, \"CSI\" secured an estimated world audience of over 73.8 million viewers (in 2009), commanded, as of the fall of 2008, an average cost of $262,600 for a 30-second commercial, and reached milestone episodes including the 100th (\"Ch-Ch-Changes\"), the 200th (\"Mascara\") and the 300th (\"Frame by Frame\"). \"CSI\" spawned three spin-off series, a book series, several video games, and an exhibit at Chicago's Museum of Science and Industry. At the time of its cancellation, \"CSI\" was the seventh-longest running scripted U.S. primetime TV series overall and had been recognized as the most popular dramatic series internationally by the Festival de Télévision de Monte-Carlo, which awarded the series the International Television Audience Award (Best Television Drama Series) three times. \"CSI\" became the second-most watched show on American television by 2002, finally taking the top position for the 2002-2003 season. It was later named the most-watched show in the world for the sixth time in 2016, making it the most-watched show for more years than any other show.\n\nCritical reception to the show has been positive, with an IMDB score of 7.8/10, while early reviews showed a mixed to favorable review of the opening season. The \"Hollywood Reporter \"noted of the pilot \"...the charismatic William Petersen and the exquisite Marg Helgenberger, lend credibility to the portrayals that might be indistinct in lesser hands. There's also a compelling, pulsating edge at the outset of CSI that commands instant attention, thanks in part to dynamic work from director Danny Cannon.\". \"Entertainment Weekly\" gave the opening two seasons \"B+\" and \"A-\" ratings, respectively, noting: \"The reason for CSI’s success is that it combines a few time-tested TV elements in a fresh way. Each episode presents a murder case and a group of lovable heroes armed with cool, high-tech gadgets who do the sleuthing and wrap things up in an hour.\" The show has won six Primetime Emmy awards (out of 39 nominations) and four People's Choice awards (out of six nominations) and was nominated for six Golden Globe Awards, among other awards.\n\n\"CSI\" was often criticized for its level and explicitness of graphic violence, images, and sexual content. The \"CSI\" series and its spin-off shows have been accused of pushing the boundary of what is considered acceptable viewing for primetime network television. The series had numerous episodes on sexual fetishism and other forms of sexual pleasure (notably the recurring character of Lady Heather, a professional dominatrix). \"CSI\" was ranked among the worst primetime shows by the Parents Television Council from its second through sixth seasons, being ranked the worst show for family prime-time viewing after the 2002–2003 and 2005–2006 seasons. The PTC also targeted certain \"CSI\" episodes for its weekly \"Worst TV Show of the Week\" feature. In addition, the episode \"\" that aired in February 2005, which the PTC named the most offensive TV show of the week, also led the PTC to start a campaign to file complaints with the FCC with the episode; to date, nearly 13,000 PTC members complained to the Federal Communications Commission about the episode. The PTC also asked Clorox to pull their advertisements from \"CSI\" and \"CSI: Miami\" because of the graphically violent content on those programs.\n\nA grassroots campaign started on August 2007, upon rumors of Jorja Fox leaving the show, organized by the online forum Your Tax Dollars At Work. Many of its 19,000 members donated to the cause, collecting over $8,000 for gifts and stunts targeted at CBS executives and \"CSI\"'s producers and writers. The stunts included a wedding cake delivery to Carol Mendelsohn, 192 chocolate-covered insects with the message \"CSI Without Sara Bugs Us\" to Naren Shankar, and a plane flying several times over the Universal Studios of Los Angeles with a \"Follow the evidence keep Jorja Fox on CSI\" banner. Other protests included mailing the show's producers a dollar, to save Fox's contract \"one dollar at a time\". By October 16, 2007, according to the site's tally, more than 20,000 letters with money or flyers had been mailed to the Universal Studios and to CBS headquarters in New York from 49 different countries since the campaign started on September 29, 2007. Fox and Mendelsohn chose to donate the money to Court Appointed Special Advocate, a national association that supports and promotes court-appointed advocates for abused or neglected children.\n\nOn September 27, 2007, after \"CSI\"'s season eight premiered, a miniature model of character Gil Grissom's office (which he was seen building during ) was put up on eBay. The auction ended October 7, with the prop being sold for $15,600; CBS donated the proceeds to the National Court Appointed Special Advocate Association.\n\nReal-life crime scene investigators and forensic scientists warn that popular television shows like \"CSI\" (often specifically citing \"CSI\") do not give a realistic picture of the work, wildly distorting the nature of crime-scene investigators' work, and exaggerating the ease, speed, effectiveness, drama, glamour, influence, scope, and comfort level of their jobs, which they describe as far more mundane, tedious, limited, and boring, and very commonly failing to solve a crime.\n\nAnother criticism of the show is the depiction of police procedure, which some consider to be decidedly lacking in realism. For instance, the show's characters not only investigate (\"process\") crime scenes, but they also conduct raids, engage in suspect pursuit and arrest, interrogate suspects, and solve cases, all of which falls under the responsibility of uniformed officers and detectives, not CSI personnel. Although 'some' detectives are also registered CSIs, this is exceedingly rare in actual life. It is considered an inappropriate and improbable practice to allow CSI personnel to be involved in detective work, as it would compromise the impartiality of scientific evidence and would be impracticably time-consuming. Additionally, it is inappropriate for the CSIs who process a crime scene to be involved in the examination and testing of any evidence collected from that scene. \"CSI\" shares this characteristic with similar British drama series \"Silent Witness\".\n\nHowever, not all law enforcement agencies have been as critical; many CSIs have responded positively to the show's influence and enjoy their new reputation. In the UK, scenes of crime officers now commonly refer to themselves as CSIs. Some constabularies, such as Norfolk, have even gone so far as to change the name of the unit to Crime Scene Investigation. Also, recruitment and training programs have seen a massive increase in applicants, with a far wider range of people now interested in something previously regarded as a scientific backwater.\n\nThe \"\"CSI\" effect\" is a reference to the alleged phenomenon of \"CSI\" raising crime victims' and jury members' real-world expectations of forensic science, especially crime-scene investigation and DNA testing. This is said to have changed the way many trials are presented today, in that prosecutors are pressured to deliver more forensic evidence in court. Victims and their families are coming to expect instant answers from showcased techniques such as DNA analysis and fingerprinting, when actual forensic processing often takes days or weeks, with no guarantee of revealing a \"smoking gun\" for the prosecution's case. District attorneys state that the conviction rate in cases with little physical evidence has decreased, largely due to the influence of \"CSI\" on jury members. Some police and district attorneys have criticized the show for giving members of the public an inaccurate perception of how police solve crimes.\n\nIn 2006, the evidence cited in support of the supposed effect was mainly anecdotes from law enforcement personnel and prosecutors, and allegedly little empirical examination of the effect had been done, and the one study published by then suggested the phenomenon may be an urban myth.\n\nHowever, more recent research suggests that these modern TV shows do have an influence on public perceptions and expectations, and juror behavior.\n\nCiting the \"\"CSI\" effect\", at least one researcher has suggested screening jurors for their level of influence from such TV programs.\n\nThe show ranked number three in DVR playback (3.07 million viewers), according to Nielsen prime DVR lift data from September 22 to November 23, 2008.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe U.S. box sets are released by CBS DVD (distributed by Paramount), while the Canadian box sets are released by Alliance Atlantis (distributed by Universal Studios). The first season DVD release differs from all subsequent seasons in that it is available only in 1.33:1 or 4:3 full frame, rather than the subsequent aspect ratio of 1.78:1 or 16:9 widescreen, which is the HDTV standard aspect ratio.\n\nThe first season is also the only DVD release of the series not to feature Dolby Digital 5.1 surround audio, instead offering Dolby Digital stereo sound.\n\nThe Blu-ray disc release of Season One is 7.1 DTS sound and 1.78:1 widescreen.\n\nRegions 2 releases have followed a pattern whereby each season is progressively released in two parts (each of 11 or 12 episodes [except for Season 8, in which part 1 contained 8 episodes and the \"Without a Trace\" crossover and part 2 contained the remaining 9 episodes] with special features split up) before finally being sold as a single box set. After having been almost 12 months behind region 2 releases after the first four series, region 4 releases are speeding up, with distributors simply releasing season five as a complete box set.\n\n<nowiki>*</nowiki> = Re-released in slimline full-season packaging. Seasons 1–8 were released in 2 parts between 2003 and 2009.\n\nnone\n\nCBS Home Entertainment (distributed by Paramount) released the first season on High Definition Blu-ray disc on\nMay 12, 2009.\nUnlike its DVD counterpart , this release is in its original 16:9 widescreen format and feature 7.1 surround sound. Features on the Season 1 BR set are also in High Def.\n\nSeason 10 was released on November 18, 2011, in Region B. Like the Season 1 Blu-ray release, it features a 16:9 widescreen transfer, but it only has DTS-HD 5.1 sound.\n\nSeason 9 was released on September 1, 2009. Like the Season 1 Blu-ray release, it features a 16:9 widescreen transfer with DTS-HD Master Audio 7.1 surround sound. Extras include commentaries, featurettes and BD-Live functionality.\n\nSeason 8 was released on Blu-ray on May 29, 2009, in Region B.\n\n\"CSI\" has also been released as a series of mobile games. In Fall 2007, CBS teamed up with game developer Gameloft to bring \"CSI\" to mobile phones. The first of the series to be published was \"\". The game features actual cast members such as Alexx Woods and Calleigh Duquesne who are trying to solve a murder in South Beach with the player's assistance. The game is also available for download on various iPod devices.\n\nIn spring 2008, Gameloft and CBS released \"CSI: Crime Scene Investigation – The Mobile Game\" which is based on the original series in Las Vegas. This game introduces the unique ability to receive calls during the game to provide tips and clues about crime scenes and evidence. As for the storyline, the game developers collaborated with Anthony E. Zuiker (the series creator) to ensure that the plot and dialogue were aligned with the show's style.\n\n\n\n\"CSI\" airs on the Nine Network and TVHits (formerly TV1) in Australia, on Channel 5 in United Kingdom, on CTV in Canada, on Italia 1 in Italy, on Prime in New Zealand, on RTÉ2 in Ireland, on TF1 in France, AXN in Asia and Latin America, Skai TV in Greece, on HOT Zone in Israel, on TV3 in Estonia and Latvia and on Kanal 5 in Sweden and Denmark.\n\nThe use of forensic pathology in the investigation of crime has been the central theme of several other TV mystery-suspense dramas, including:\n\n\n"}
{"id": "46332968", "url": "https://en.wikipedia.org/wiki?curid=46332968", "title": "Caroline Ghosn", "text": "Caroline Ghosn\n\nCaroline Ghosn (born January 29, 1987) is the eldest child of Nissan and Renault CEO, Carlos Ghosn. In 2011, she founded Levo (formerly Levo League), a professional network dedicated to helping millennials navigate the workplace, and has been managing its development since then.\n\nGhosn is an active member of the World Economic Forum’s Global Shapers community and attended and spoke at the annual World Economic Forum in Davos in 2014.\n\nGhosn was recognized by Fast Company as one of the Most Creative People in Business in 2013. Ghosn was presented as a debutante at the Bal des débutantes in Paris in 2006.\n\nGhosn graduated with Honors from Stanford University in 2008 with a B.A. in International Political Economy and Environmental economics. She has lived in six countries and speaks four languages.\n\nAfter graduating, Ghosn went to work for strategic consultancy McKinsey and Company as a Business Analyst and then Sustainability and Resource Productivity Fellow, focused on clean-tech and the environment.\n\nIn 2011 Ghosn left her position to found Levo to help millennials achieve success in the workplace.\n\nGhosn currently serves as the CEO and Chairman of Levo's Board of Directors. Levo has an audience of over 9 million members globally, and 30 local Levo chapters around the world. The company has raised more than $9 million in angel investment with prominent investors including Gina Bianchini, Fran Hauser, Susan Lyne, and Lubna Olayan. Former Facebook COO, Sheryl Sandberg, is also an investor in Levo and a personal mentor to Ghosn.\n\nAs part of Levo's “Office Hours” series, Ghosn has interviewed leading businessmen and women, and influencers, including Warren Buffett, New York Senator Kirsten Gillibrand, actor Kevin Spacey, journalist Soledad O’Brien and designer Nanette Lepore.\n\nGhosn has spoken at a number of international conferences, including the 2014 World Economic Forum, Bloomberg's The Next Big Thing Summit, AMEX's CEO Bootcamp, and Cosmo’s Fun Fearless Life conference with Joanna Coles.\n\nSince launching Levo, Ghosn has been named to Fast Company’s list of the Most Creative People in Business in 2013.\n\nGhosn is the eldest child of Nissan and Renault CEO, Carlos Ghosn.\n\nShe spends her time in New York City and San Francisco, where Levo’s offices are located.\n"}
{"id": "1227223", "url": "https://en.wikipedia.org/wiki?curid=1227223", "title": "Clearstream", "text": "Clearstream\n\nClearstream is a post-trade services provider owned by Deutsche Börse AG. It provides settlement and custody as well as other related services for securities across all asset classes. It is one of two European International Central Securities Depositories (Euroclear being the other).\n\nClearstream operates securities settlement systems based in both Luxembourg and Germany, which allow for the holding and transfer of securities.\n\nClearstream operates its International Central Securities Depository (ICSD) from Luxembourg. It is also a joint partner in the Luxembourgish Central Securities Depository (CSD), LuxCSD, together with the Central Bank of Luxembourg. In Germany, Clearstream operates the German CSD, Clearstream Banking AG. Clearstream has links to over 50 domestic markets worldwide, and also issues and safekeeps Eurobonds.\n\nIn 2014, the value of assets under custody on behalf of customers averaged 12.2 trillion euros; furthermore, Clearstream processed 43.65 million ICSD and 82.68 million CSD settlement transactions.\n\nClearstream has around 2,500 customers in 110 countries. Clearstream accepts central banks and AML-regulated credit institutions (such as regulated banks) as customers. Clearstream does not accept natural persons as customers and no account is opened in the name of a natural person. Clearstream has therefore been described as a \"bank for banks\" (\"Plumbers and Visionaries, a history of settlement and custody in Europe\", Peter Norman).\n\nClearstream was founded as \"Cedel\" (Centrale de Livraison de Valeurs Mobilières) in September 1970 by 66 of the world's major financial institutions as a clearing organisation whose objective was to minimise risk in the settlement of cross-border securities trading, particularly in the growing Eurobond market.\n\nIn 1995, a new corporate structure was introduced, establishing a parent company - Cedel International - with a subsidiary company, Cedelbank; two years later, a new subsidiary - Cedel Global Services - was established.\n\nClearstream was formed in January 2000 through the merger of Cedel International and Deutsche Börse Clearing. The full integration of Clearstream was completed in July 2002.\n\nToday Clearstream has operational centres in Cork, Luxembourg, Prague and Singapore. It also maintains representative offices in London, Hong Kong, Tokyo, Dubai, New York and Zurich.\n\nIn July 2010, Clearstream founded LuxCSD together with the Luxembourg Central Bank, to act as a CSD for Luxembourg.\n\nIn December 2010, Clearstream co-founded REGIS-TR, a joint venture with the Spanish Central Securities Depository Iberclear, to act as a trade repository for derivatives transactions, helping participants meet their regulatory reporting obligations brought about by the introduction of the European Market Infrastructure Regulation (EMIR).\n\nBased in Luxembourg and Germany, Clearstream is subject to the supervision of the regulatory authorities of these two countries. In Luxembourg, the Commission de Surveillance du Secteur Financier (CSSF) is the prudential regulator with authority over all banks and financial service providers. In Germany, Clearstream is regulated as a bank according to the German Banking Act (\"Kreditwesengesetz\") and is therefore subject to the prudential supervision of the German Federal Financial Supervisory Authority (German: \"Bundesanstalt für Finanzdienstleistungsaufsicht\" - BaFin). Moreover, Clearstream is regulated in each market where it has operational centres, for example by the Monetary Authority of Singapore.\n\nAs the operator of securities settlement systems in both Luxembourg and Germany, Clearstream is additionally regulated by the central banks of these two countries, namely the Banque centrale du Luxembourg and the Deutsche Bundesbank.\n\nClearstream is responsible for the management, safekeeping/custody and administration of securities (assets) under custody. Services include income and redemption payments, corporate actions as well as tax and proxy voting.\n\nThe majority of securities safekept by Clearstream are immobilised. Securities are reflected in book-entry form in the accounts of customers at Clearstream regardless of whether they are held in physical or dematerialised form. This means that they are no longer represented by physical certificates, but instead by data entered into the Clearstream systems.\n\nSecurities financing is the ability to borrow or lend cash or securities against collateral. In securities financing, collateral comprises assets given as a guarantee by a borrower to secure a securities loan and subject to seizure in the event of default. Collateral management refers to the handling of all tasks related to the monitoring of collateral posted by a borrower to meet a financial obligation (optimisation, substitution, top-up, withdrawal, settlement instruction, reporting, processing of margin calls and returns, notification of corporate events, etc.).\n\nClearstream's collateral management, securities lending and borrowing services are gathered under the Global Liquidity Hub. It provides a pool of liquidity through links to agent banks, trading platforms, clearing houses and other market infrastructures.\n\nClearstream is a member of the Liquidity Alliance, which was established in January 2013 as a platform for CSDs to collaborate on collateral management. It was founded by ASX, Cetip, Clearstream, Iberclear and Strate.\n\nClearstream has developed the world's largest cross-border fund processing platform, called Vestima. It handles all types of funds, from mutual funds to exchange-traded funds (ETFs) and hedge funds.\n\nIn addition to providing access to all fund types, Vestima supports their cross-border distribution. Services offered by Vestima include order routing, centralised delivery versus payment (DVP) settlement, safekeeping, asset servicing and collateral management.\n\nICSDs and CSDs are often used to bring a security issue to the market, as they possess the necessary infrastructure for distributing the securities to the investors as well as for settlement and safekeeping. Clearstream's ICSD also provides custody services, which means that the security can be serviced throughout its entire lifecycle. Clearstream provides the infrastructure which enables issuers to reach investors worldwide.\n\nClearstream's services include eligibility assessments, issuance and distribution of domestic, foreign and international (i.e. Eurobonds) new issues of global and domestic instruments: certificates of deposit, depository receipts, treasury bills, commercial papers, short-term and medium-term notes, bonds, equities, warrants, equity-linked notes and investment fund shares.\n\nClearstream's ICSD settles trades in international securities and in domestic securities traded across borders. The CSDs settle domestic transactions in the German and Luxembourgish markets. Transactions between the two ICSDs Clearstream and Euroclear are settled via an electronic communications platform, called the Bridge.\n\nClearstream operates a delivery versus payment settlement system, ensuring simultaneous settlement of securities and cash transfers on a gross (trade-by-trade) basis. This helps to minimise the risk associated with the settlement of securities.\n\nBetween 2001 and 2002, two books were published by co-authors Denis Robert and Ernest Backes, entitled \"Révélation$\" and \"La Boîte Noire\", accusing Clearstream of being a money laundering organisation and the worldwide centre for international financial crime committed by major banks, shell companies and organised crime all over the world.\nAn investigation opened by the Luxembourg authorities in 2001 found no evidence to support the authors' allegations and the case was dismissed in 2004.\n\nIn 2004, a list leaked to a French judge indicated that Clearstream was a money laundering organisation covering illegal activities of French businessmen who were involved in the sale of warships by French defence giant Thales (known as Thomson-CSF before 2000) to Taiwan and related bribes paid to French officials and businessmen.\nAn investigation that included the search of Clearstream's premises and an examination of accounts in 2004 found no evidence supporting the accusations.\n\nIn the same year, additional anonymous letters and CD-Roms sent to French judges accused Clearstream of running secret accounts for criminals and senior French politicians. In late 2005, French investigating authorities officially declared the documents as forgeries and dismissed the case.\n\nThe controversy inspired the 2014 film \"The Clearstream Affair\".\n\nIn 2008, several groups of plaintiffs commenced enforcement proceedings in the US to satisfy judgments which they had obtained against Iran by restraining certain positions held in a Clearstream securities account with its intermediary bank in the US, and asking for handover of the assets. Clearstream challenged the restraints and the handover.\n\nIn 2011, the plaintiffs filed additional claims, this time directly against Clearstream, for damages of USD 250 million in connection with the purported wrongful conveyance of some of the restrained positions. Clearstream entered into a settlement agreement with the plaintiffs that became effective in 2013. The settlement provided for the dismissal of the direct claims against Clearstream, and that the plaintiffs will not further sue Clearstream for damages and in return, Clearstream agreed to not further appeal an order directing the turnover of the restrained customer's assets to the plaintiffs. \nIn 2013, a number of US plaintiffs from the 2011 case, as well as other US plaintiffs, filed a complaint targeting certain blocked assets that Clearstream holds as a custodian in Luxembourg. In 2015, the US court issued a decision dismissing the lawsuit.\n\nArising from the above, the U.S. Treasury Department Office of Foreign Assets Control (OFAC) investigated certain securities' transfers in 2008 within Clearstream's settlement systems regarding US Iran sanctions regulations. These 2008 transfers had been undertaken as part of the decision taken by Clearstream in 2007 to close its Iranian customers' accounts. In 2013 Clearstream entered into settlement talks with OFAC. The matter was resolved in 2014 through a settlement and payment of USD 151.9 million that does not constitute a final determination that a violation of Iran sanctions had occurred.\n\n\n"}
{"id": "56765008", "url": "https://en.wikipedia.org/wiki?curid=56765008", "title": "Compressed air dryer", "text": "Compressed air dryer\n\nCompressed air dryers are special types of filter systems that are specifically designed to remove the water that is inherent in compressed air. The process of compressing air raises its temperature and concentrates atmospheric contaminants, primarily water vapor. Consequently, the compressed air is generally at an elevated temperature and 100% relative humidity. As the compressed air cools, water vapor condenses into the tank(s), pipes, hoses and tools that are downstream from the compressor.\n\nExcessive liquid and condensing water in the air stream can be extremely damaging to equipment, tools and processes that rely on compressed air. The water can cause corrosion in the tank(s) and piping, wash out lubricating oils from pneumatic tools, emulsify with the grease used in cylinders, clump blasting media and fog painted surfaces. Therefore, it is desirable to remove condensing moisture from the air stream to prevent damage to equipment, air tools and processes. The function of removing this unwanted water is the purview of the compressed air dryer.\n\nThere are various types of compressed air dryers. These dryers generally fall into two different categories, Primary and Secondary. Their performance characteristics are typically defined by flow rate in Standard Cubic Feet per Minute (SCFM) and dew point expressed as a temperature, (sometimes referred to as Pressure Dew Point.)\n\nPrimary Dryers: Coalescing, Refrigerated and Deliquescent.\n\nSecondary Dryers: Desiccant, Absorption and Membrane.\n\nWater contamination is an inherent byproduct of compressing atmospheric air. Because of this phenomenon, compressed air systems usually benefit from the addition of a compressed air dryer located on the output of the air compressor and/or at various locations throughout the distribution system. In most cases, the output of the compressor is processed through a primary dryer or system dryer. In cases where higher quality air is required the output of the primary dryer is further processed through a secondary dryer or polishing dryer.\n\nCoalescing filters aren’t actually filters, rather they operate more as a consolidation element. The idea behind these devices is that the compressed air is forced through gaps or porosity within an otherwise solid\n\nelement. These gaps and/or porosity are microscopic and small enough that water vapor wets to the internal surfaces. The liquid water that forms during the wetting process is forced through the media and drips down into a trap. The dry air travels up to and out of the discharge port.\n\nThere are two basic types of coalescing elements. The first type utilizes a cast material that is dominated with an internal microscopic lattice. The air is forced to flow through the lattice which, in turn, allows the water vapor to wet to the internal surfaces. The second type is generally referred to as a stacked plate element. In this case, fine discs are stacked with microscopic gaps between them. The air is forced to flow through the gaps which, in turn, allows the water vapor to wet to the internal surfaces.\n\nIn principle, a coalescing filter is an ideal way to separate water from the compressed air stream. Practically speaking, this is not the case. Coalescing elements are extremely sensitive to oil and particulate contamination and therefore would be better placed as a second stage dryer. However, the dew point performance of a coalescing filter places it in the primary category of compressed air dryers. In order to use coalescing filters as primary dryers, they are typically set-up in pairs. The first filter has an element with larger gaps that are designed to remove oil from the stream. The second filter uses a finer element that is intended to remove water vapor. Because of the sensitivity of the elements, coalescing filters are not particularly common. One area where these filters have found acceptance is with dental compressors. The way that dental compressors are designed and used makes a two stage coalescing filter an almost ideal solution for water contamination in these systems.\n\nRefrigerated dryers are the most common type of compressed air dryer. They remove water from the air stream by cooling the air to approximately 38 °F and effectively condensing out the moisture in a \n\ncontrolled environment. 38 °F is the realistic lower limit for a refrigerated dryer because a lower temperature runs the risk of freezing the separated water. They are typically specified as primary dryers and generally produce air quality that is appropriate for approximately 95% of all compressed air applications. Refrigerated dryers are generally manufactured in one of two different ways, Freon based units and Joule-Thomson based units.\n\nThese dryers derive their cooling from a closed cycle refrigeration system based around one of three commercial refrigerants, R-22, R-134a or R410a. The refrigeration system these dryers use is similar to home and commercial air conditioning systems. The schematic shown to the right illustrates a typical Freon based refrigerated compressed air dryer.\n\nFreon based refrigerated compressed air dryers generally consist of a heat exchanger that is similar to a water cooled after cooler. Instead of using water as the coolant, liquid CFC fills the shell of the heat exchanger. The liquid CFC is maintained at a pressure that allows it to boil at 38 °F. After the CFC boils, the vapor is drawn through the suction line into a compressor, which compresses the CFC to a high pressure and high temperature. The high pressure/temperature CFC is cooled in the condenser and relaxes into its liquid state. The liquid is reintroduced into the heat exchanger via the metering device and a closed refrigeration cycle is formed. When the compressed air passes through the heat exchanger, it is cooled to the temperature of the boiling CFC. As the compressed air is cooled, it loses its ability to retain moisture and the water vapor condenses onto the inside of the exchanger tube.\n\nSee Also: \"Vapor-compression refrigeration\"\n\nVariations on this basic design include units equipped with reheating exchangers, which are intended to improve efficiency. In these cases, the cooled compressed air is reheated by the incoming air.\n\nHigh temperature dryers are equipped with an additional pre-cooler that removes excess heat via a forced air system. These units are designed to allow excessively hot compressed air to be effectively dried. Compressed air temperatures in excess of 100 °F are very common in Southern climates, mining operations, steel mills, shipboard, etc. In areas and applications that demand operations in elevated ambient temperatures, high temperature dryers are a necessity.\n\nCycling dryers (also known as thermal mass dryers) utilize a thermal mass, usually a tank of water, to store the energy produced by the refrigeration system. The temperature of the water controls the refrigeration system through a thermostat. The compressed air passes through the thermal mass via a water cooled heat exchanger. The value of this type of configuration is that they normally produce more consistent cooling results.\n\nJT type dryers are units that utilize the compressed air stream as their refrigeration element. High pressure compressed air (150 ~ 175 PSI) is fed into a pressure reducing valve on top of the dryer. The output of this valve (90 ~120 PSI) is directed into an expansion chamber which is surrounded with porous walls. As the air expands to a lower pressure, it becomes cold (based on the Joule-Thomson Effect) and its ability to retain moisture is reduced. The moisture is released from the air in the form of fog. The fog laden air then passes through the porous walls of the chamber. The micro-droplets of water that make up the fog wet to the porous material and collect until they form droplets that can be affected by gravity. The water then drops into a trap and the dried air travels up to and out of the discharge port. The drawback of the JT Dryer is that it can only be used with two-stage compressors. This is because a two-stage compressor derives its efficiency by pumping to a high pressure (150 ~ 175 PSI.) This pressure is inappropriate for the shop floor and must be dropped to (90 ~ 120 PSI.) The JT Dryer takes advantage of this pressure drop to remove moisture from the compressed air stream through the inherent refrigeration based on the Joule-Thompson effect of the expanding air. Leveraging this pressure drop allows a JT dryer to produce the same relative dew points that Freon based dryers produce.\n\nDeliquescent dryers typically consist of a pressure vessel filled with a hygroscopic medium that has a high affinity for water vapor. Practically speaking, these dryers are\n\ntypically a large pressure vessel that is filled with salt crystals. When water vapor comes in contact with the salt, it attaches and dissolves the media, or deliquesces. As liquid water builds on the salt crystals, brine is formed, which drains down and collects in the bottom of the vessel. Periodically, the brine must be drained and similarly, the media must be refilled. Typically, deliquescent dryers will produce a dew point suppression of 18 to 25 °F.\n\nOn the plus side, these dryers are very simple, have no moving parts and do not require electrical power. However, they do not perform well with high temperature air streams and/or in high ambient temperatures. They are disproportionately large units which are filled with a corrosive substance. Their size and corrosive nature can present problems with any system that uses them. Because of this, these dryers are typically used only in specialty applications.\n\nDesiccant dryers, sometimes referred to as absorption dryers, operate by absorbing water vapor into a porous media with a high affinity for water.\nThese types of dryers are also referred to as absorption systems or getters. Because these dryers get and hold the water, they are minimally effective as a first stage dryer. If a desiccant is used in this role, the media quickly becomes saturated and the effectiveness of the dryer is negated. Desiccant dryers are best applied in a second stage or polishing role. They are usually used down-stream from a refrigerated dryer or some other primary dryer. When applied as a second stage dryer, they can easily and reliably produce dew points in the sub zero range.\n\nDesiccant dryers are typically supplied in two patterns, “Single Canister” and “Twin Tower” units. Single canister units have the outward appearance of a filter housing. However, they are filled with a granular media that must be periodically replaced. The media can be regenerated by baking it at a high temperature in accordance with the manufacturers’ recommendations. Single canister desiccant dryers are typically installed in point-of-use applications. When applied as a second stage dryer, they can easily and reliably produce dew points in the sub-zero range.\n\nA variation on the single canister desiccant dryer is the Toilet Paper filter. These types of filters provide the same basic function as a desiccant dryer except they use an ordinary roll of toilet paper as their absorption media. When the toilet paper becomes saturated, it is removed and replaced with a fresh roll. The popularity of these filters is primarily based around their low cost, convenience and effectiveness. Surprisingly enough, these types of filters are very effective in point-of-use roles.\n\nTwin Tower, or regenerative desiccant dryers, have two vertical tanks filled with media. When the media in the first tank becomes saturated, the air stream is automatically redirected through the second tank. The first tank is then heated while a portion of the dried air, referred to as the purge air, is back flowed through the tank and vented to atmosphere. This process dries, or regenerates, the media in the first tank and makes it ready for the next redirect. One of the most significant drawbacks of twin tower desiccant dryers is their use of the purge air. Typically, a twin tower desiccant dryer uses some 15 to 20% of its capacity to regenerate the opposite tank, making these dryers inefficient and costly to operate.\n\n\"See also: Membrane gas separation\"\n\nMembrane dryers operate on the principle of migration. The compressed air to be dried is passed over a membrane that has a high affinity for water vapor. The water\n\nvapor builds on the membrane and migrates through to the opposite or low pressure, side. A dry cover gas is flowed across the low pressure side and absorbs the water on the membrane. After absorbing the water, the cover gas is discharged to the atmosphere. The cover gas is generally taken from the output of the dryer. The membrane is typically a series of small tubes collected in a bundle within an outer housing.\n\nIf set-up and operated properly, membrane dryers can produce extremely low dew points. For this reason they are very common in laboratories, medical facilities and specialty manufacturing environments where limited amounts of high quality compressed air is required. They are usually set up as a point-of-use dryer and provide the best service when used in a second or third stage role. The delicate nature of the equipment and how it is used makes them generally unsuitable for more mainstream or industrial applications.\n\n"}
{"id": "19957639", "url": "https://en.wikipedia.org/wiki?curid=19957639", "title": "Dancing Dots", "text": "Dancing Dots\n\nDancing Dots Braille Music Technology, L.P., is an American company based in Philadelphia that was founded in 1992 to develop and adapt music technology for the blind. Its founder, Bill McCann, is a blind musician. Among the products it offers are several programs that produce a musical version of Braille by converting print musical notation, allowing blind musicians access to the scores used by their sighted counterparts. The company also offers programs that aid blind musicians in transcribing their compositions to Braille. Dancing Dots created the latter products to help speed the process of Braille transcription for blind composers, who might otherwise have to wait between two weeks and six months to have their compositions transcribed by one of the less than one hundred certified Braille music transcribers in the United States. Dancing Dots has developed more products to help blind musicians in the areas of MIDI and digital audio production and musical instruction.\n\nThe company was founded in 1992 by Bill McCann, a blind trumpeter. It struggled financially in its early years in the long lead between developing technology and releasing its first product in 1997, a difficult period assisted by federal contracts beginning in 1994. In 1997, the company released its GOODFEEL Braille Music Translator to positive reviews. The product was well received, and its company was a success. In 1999, the company, which was a recipient of a Small Business Innovation Research Grant, was part of a display of assistive technology at the White House. In 2000, Dancing Dots released CakeTalking for SONAR, JAWS scripts and tutorials that provide access to Cakewalk Sonar, a digital audio workstation, for blind or visually impaired users.\n\nDancing Dots maintains a website at which it markets its products, as well as related and complementary products by other companies. Dancing Dots has customers throughout the U.S. and twenty-six other countries.\n\nWith GOODFEEL combined with a few mainstream products, sighted musicians can prepare a Braille score with no knowledge of braille. Music scanning software can be used to speed data entry. Blind users can make sound recordings and print and Braille editions of their compositions. The company provides customers who may not need to purchase their own copy of GOODFEEL a transcription service for individual scores.\n\nDancing Dots is also the publisher of several courses to assist blind musicians, including \"An Introduction to Music for the Blind Student: A Course in Braille Music Reading\" and \"An Introduction to the Piano for the Blind Student\".\n\n"}
{"id": "36724181", "url": "https://en.wikipedia.org/wiki?curid=36724181", "title": "Digital commons (economics)", "text": "Digital commons (economics)\n\nThe digital commons are a form of commons involving the distribution and communal ownership of informational resources and technology. Resources are typically designed to be used by the community by which they are created. Examples of the digital commons include wikis, open-source software, and open-source licensing. The distinction between digital commons and other digital resources is that the community of people building them can intervene in the governing of their interaction processes and of their shared resources.\n\nThe digital commons provides the community with free and easy access to information. Typically, information created in the digital commons is designed to stay in the digital commons by using various forms of licensing, including the GNU General Public License and various Creative Commons licenses.\n\nOne of the first examples of digital commons is the Free Software movement, founded in the 1980s by Richard Stallman as an organized attempt to create a digital software commons. Inspired by the 70s programmer culture of improving software through mutual help, Stallman's movement was designed to encourage the use and distribution of free software.\n\nTo prevent the misuse of software created by the movement, Stallman founded the GNU General Public License. Free software released under this license, even if it is improved or modified, must also be released under the same license, ensuring the software stays in the digital commons, free to use.\n\nToday the digital commons takes the form of the Internet. With the internet come radical new ways to share information and software, enabling the rapid growth of the digital commons to the level enjoyed today. People and organisations can share their software, photos, general information, and ideas extremely easily due to the digital commons.\n\nMayo Fuster Morell proposed a definition of digital commons as \"information and knowledge resources that are collectively created and owned or shared between or among a community and that tend to be non-exclusive, that is, be (generally freely) available to third parties. Thus, they are oriented to favor use and reuse, rather than to exchange as a commodity. Additionally, the community of people building them can intervene in the governing of their interaction processes and of their shared resources\".\n\nThe Foundation for P2P Alternatives explicitly aims to \"creates a new public domain, an information commons, which should be protected and extended, especially in the domain of common knowledge creation\" and actively promotes extending Creative Commons Licenses.\n\nIn light of the Zero Marginal Cost Society, the Commons Transition and the transition to a digital commons in particular, appears to be inevitable. Critics observe the tension between contributing to the digital Commons, which is fundamentally abundant, and making a living, which is based on scarcity.\n\nCreative Commons (CC) is a non-profit organization that provides many free copyright licenses with which contributors to the digital commons can license their work. Creative Commons is focused on the expansion of flexible copyright. For example, popular image sharing sites like Flickr and Pixabay, provide access to hundreds of millions of Creative Commons licensed images, freely available within the digital commons.\n\nCreators of content in the digital commons can choose the type of Creative Commons license to apply to their works, which specifies the types of rights available to other users. Typically, Creative Commons licenses are used to restrict the work to non-commercial use.\n\nWikis are a huge contribution to the digital commons, serving information while allowing members of the community to create and edit content. Through wikis, knowledge can be pooled and compiled, generating a wealth of information from which the community can draw.\n\nFollowing in the spirit of the Free Software movement, public software repositories are a system in which communities can work together on open-source software projects, typically through version control systems such as Git and Subversion. Public software repositories allow for individuals make contributions to the same project, allowing the project to grow bigger than the sum of its parts. A popular platform hosting public and open source software repositories is GitHub.\n\nTop Level Domains or TLDs are Internet resources that facilitate finding the numbered computers on the Internet. The largest and most familiar TLD is .com. Beginning in 2012, ICANN, the California not-for-profit controlling access to the Domain Name System, began issuing names to cities. More than 30 cities applied for their TLDs, with .paris, .london, .nyc, .tokyo having been issued as of May 2015. A detailing of some commons names within the .nyc TLD includes neighborhood names, voter related names, and civic names.\n\n\n"}
{"id": "333942", "url": "https://en.wikipedia.org/wiki?curid=333942", "title": "Digital reproduction", "text": "Digital reproduction\n\nDigital reproduction is one form of data reproduction which is based on the digital data model. The advantage of digital reproduction of data over analogue reproduction is its lossless quality. Reproducing analogue data is inherently lossy, because every (electronic) component involved in any analogue reproduction process can introduce noise to the final result.\n\nCompare analogue processing to digital processing.\n"}
{"id": "13789881", "url": "https://en.wikipedia.org/wiki?curid=13789881", "title": "Electro-optical MASINT", "text": "Electro-optical MASINT\n\nElectro-optical MASINT is a subdiscipline of Measurement and Signature Intelligence, (MASINT) and refers to intelligence gathering activities which bring together disparate elements that do not fit within the definitions of Signals Intelligence (SIGINT), Imagery Intelligence (IMINT), or Human Intelligence (HUMINT).\n\nElectro-optical MASINT has similarities to IMINT, but is distinct from it. IMINT's primary goal is to create a picture, composed of visual elements understandable to a trained user. Electro-optical MASINT helps validate that picture, so that, for example, the analyst can tell if an area of green is vegetation or camouflage paint. Electro-optical MASINT also generates information on phenomena that emit, absorb, or reflect electromagnetic energy in the infrared, visible light, or ultraviolet spectra, phenomena where a \"picture\" is less important than the amount or type of energy reported. For example, a class of satellites, originally intended to give early warning of rocket launches based on the heat of their exhaust, reports energy wavelengths and strength as a function of location(s). There would be no value, in this specific context, to seeing a photograph of the flames coming out of the rocket.\n\nSubsequently, when the geometry between the rocket exhaust and the sensor permits a clear view of the exhaust, IMINT would give a visual or infrared picture of its shape, while electro-optical MASINT would give, either as a list of coordinates with characteristics, or a \"false-color\" image, the temperature distribution, and spectroscopic information on its composition.\n\nIn other words, MASINT may give warning before characteristics visible to IMINT are clear, or it may help validate or understand the pictures taken by IMINT.\n\nMASINT techniques are not limited to the United States, but the U.S. distinguishes MASINT sensors from others more than do other nations. According to the United States Department of Defense, MASINT is technically derived intelligence (excluding traditional imagery IMINT and signals intelligence SIGINT) that – when collected, processed, and analyzed by dedicated MASINT systems – results in intelligence that detects, tracks, identifies, or describes the signatures (distinctive characteristics) of fixed or dynamic target sources. MASINT was recognized as a formal intelligence discipline in 1986. Another way to describe MASINT is \"a 'non-literal' discipline. It feeds on a target's unintended emissive byproducts, the 'trails' of thermal energy, chemical or radio frequency emission that an object leaves in its wake. These trails form distinct signatures, which can be exploited as reliable discriminators to characterize specific events or disclose hidden targets\".\n\nAs with many branches of MASINT, specific techniques may overlap with the six major conceptual disciplines of MASINT defined by the Center for MASINT Studies and Research, which divides MASINT into Electro-optical, Nuclear, Geophysical, Radar, Materials, and Radiofrequency disciplines.\n\nMASINT collection technologies in this area use radar, lasers, staring arrays in the infrared and visual, to point sensors at the information of interest. As opposed to IMINT, MASINT electro-optical sensors do not create pictures. Instead, they would indicate the coordinates, intensity, and spectral characteristics of a light source, such as a rocket engine, or a missile reentry vehicle. Electro-optical MASINT involves obtaining information from emitted or reflected energy, across the wavelengths of infrared, visible, and ultraviolet light. Electro-optical techniques include measurement of the radiant intensities, dynamic motion, and the materials composition of a target. These measurements put the target in spectral and spatial contexts. Sensors used in electro-optical MASINT include radiometers, spectrometers, non-literal imaging systems, lasers, or laser radar (LIDAR).\n\nObservation of foreign missile tests, for example, make extensive use of MASINT along with other disciplines. For example, electro-optical and radar tracking establish trajectory, speed, and other flight characteristics that can be used to validate the TELINT telemetry intelligence being received by SIGINT sensors. Electro-optical sensors, which guide radars, operate on aircraft, ground stations, and ships.\n\nU.S. RC-135S COBRA BALL aircraft have MASINT sensors that are \"…two linked electro-optical sensors—the Real Time Optics System (RTOS) and the Large Aperture Tracker System (LATS). RTOS consists of an array of staring sensors encompassing a wide field of regard for target acquisition. LATS serves as an adjunct tracker. Due to its large aperture, it has significantly greater sensitivity and resolving power than the RTOS, but is otherwise similar.\n\nCOBRA BALL cues the COBRA DANE ground radar and the COBRA JUDY ship-based radar. See Radar MASINT\n\nBoth electro-optical and radar sensors have been coupled with acoustic sensors in modern counter-artillery systems. Electro-optical sensors are directional and precise, so need to be cued by acoustic or other omnidirectional sensors. The original Canadian sensors, in the First World War, used electro-optical flash as well as geophysical sound sensors.\n\nComplementing counter-mortar radar is the Israeli Purple Hawk mast-mounted electro-optical sensor, which detects mortars and provides perimeter security. The device, remotely operated via fiber optics or microwave, is intended to have a laser designator.\n\nA newer U.S. system couples an electro-optical and an acoustic system to produce the Rocket Artillery Launch Spotter (RLS). RLS combines components from two existing systems, the Tactical Aircraft Directed Infra-Red Countermeasures (TADIRCM) and the UTAMS . The two-color infrared sensors were originally designed to detect surface-to-air missiles for TADIRCM. Other TADIRCM components also have been adapted to RLS, including the computer processors, inertial navigation units (INU), and detection and tracking algorithms.\n\nIt is an excellent example of automatic cueing of one sensor by another. Depending on the application, the sensitive but less selective sensor is either acoustic or non-imaging electro-optical. The selective sensor is forward looking infrared (FLIR).\n\nRLS uses two TADIRCM sensors, an INU, and a smaller field-of-view single-color (FLIR) camera on each tower. The INU, which contains a GPS receiver, allows the electro-optical sensors to align to the azimuth and elevation of any detected threat signature.\n\nThe basic system mode is for rocket detection, since a rocket launch gives a bright flare. In basic operation, RLS has electro-optical systems on three towers, separated by 2 to 3 kilometers, to give omnidirectional coverage. The tower equipment connects to the control stations using a wireless network.\n\nWhen a sensor measures a potential threat, the control station determines if it correlates with another measurement to give a threat signature. When a threat is recognized, RLS triangulates the optical signal and presents the Point of Origin (POO) on a map display. The nearest tower FLIR camera then is cued to the threat signature, giving the operator real-time video within 2 seconds of detection. When not in RLS mode, the FLIR cameras are available to the operator as surveillance cameras.\n\nMortar launches do not produce as strong an electro-optical signature as does a rocket, so RLS relies on acoustic signature cueing from an Unattended Transient Acoustic Measurement and Signal Intelligence System (UTAMS). There is an UTAMS array at the top of each of the three RLS towers. The tower heads can be rotated remotely.\n\nEach array consists of four microphones and processing equipment. Analyzing the time delays between an acoustic wavefront's interaction with each microphone in the array UTAMS provides an azimuth of origin. The azimuth from each tower is reported to the UTAMS processor at the control station, and a POO is triangulated and displayed. The UTAMS subsystem can also detect and locate the point of impact (POI), but, due to the difference between the speeds of sound and light, it may take UTAMS as long as 30 seconds to determine the POO for a rocket launch 13 km away. This means UTAMS may detect a rocket POI prior to the POO, providing very little if any warning time. but the electro-optical component of RLS will detect the rocket POO earlier.\n\nWhile infrared IMINT and MASINT operate in the same wavelengths, MASINT does not \"take pictures\" in the conventional sense, but it can validate IMINT pictures. Where an \"IR IMINT\" sensor would take a picture that fills a frame, the \"IR MASINT\" sensor gives a list, by coordinate, of IR wavelengths and energy. A classic example of validation would be analyzing the detailed optical spectrum of a green area in a photograph: is the green from natural plant life, or is it camouflage paint?\n\nThe Army's AN/GSQ-187 Improved Remote Battlefield Sensor System (I-REMBASS) contains a Passive Infrared Sensor, DT-565/GSQ, which \"detects tracked or wheeled vehicles and personnel. It also provides information on which to base a count of objects passing through its detection zone and reports their direction of travel relative to its location. The monitor uses two different [magnetic and passive infrared] sensors and their identification codes to determine direction of travel.\n\nShallow-water operations require generalizing IR imaging to include a non-developmental Thermal Imaging Sensor System (TISS) to surface ships with a day/night, high-resolution, infrared (IR) and visual imaging, and laser range-finder capability to augment existing optical and radar sensors, especially against small boats and floating mines. Similar systems are now available in Army helicopters and armored fighting vehicles.\n\nThere are several distinctive characteristics, in the range of visible light, from nuclear explosions. One of these is a characteristic \"dual flash\" measured by a bhangmeter. This went into routine use on the advanced Vela nuclear detection satellites, first launched in 1967. The earlier Velas only detected X-rays, gamma rays, and neutrons.\n\nThe bhangmeter technique was used earlier, in 1961, aboard a modified US KC-135B aircraft monitoring the preannounced Soviet test of Tsar Bomba, the largest nuclear explosion ever detonated. The US test monitoring, which carried both broadband electromagnetic and optical sensors including a bhangmeter, was named SPEEDLIGHT.\n\nAs part of Operation BURNING LIGHT, one MASINT system photographed the nuclear clouds of French atmospheric nuclear tests to measure their density and opacity. This operation is borderline with Nuclear MASINT.\n\nBhangmeters on Advanced Vela satellites detected what is variously called the Vela Incident or South Atlantic Incident, on 22 September 1979. Different reports have claimed that it was, or was not, a nuclear test, and, if it was, probably involved South Africa and possibly Israel. France and Taiwan have also been suggested. Only one bhangmeter detected the characteristic double-flash, although US Navy hydrophones suggest a low-yield blast. Other sensors were negative or equivocal, and no definitive explanation has yet been made public.\n\nSchlieren Photography can be used to detect Stealth aircraft, UAV, and missile flights even after engine cutoff. Schlieren analysis is based on the principle that any disturbances to the surrounding air may be detected (the Schlieren effect), like the shadow cast by the sun through the steam and hot air from a hot coffee, or even the Mirage wave effect caused by the hot air on pavement on a summer day. It is essentially the reverse of Adaptive optics, rather than minimizing the effect of atmospheric disturbance, Schlieren detection capitalizes on that effect. This form of MASINT is both optical and geophysical because of the optical detection of a geophysical (atmospheric) effect. Schlieren photography may be used to provide an early warning of an imminent threat or impending attack, and if sufficiently advanced, may be used in the elimination of stealth targets.\n\nThis discipline includes both measuring the performance of lasers of interest, and using lasers as part of MASINT sensors. With respect to foreign lasers, focus of the collection is on laser detection, laser threat warning, and precise measurement of the frequencies, power levels, wave propagation, determination of power source, and other technical and operating characteristics associated with laser systems strategic and tactical weapons, range finders, and illuminators.\n\nIn addition to passive measurements of other lasers, the MASINT system can use active lasers (LIDAR) for distance measurements, but also for destructive remote sensing that provides energized material for spectroscopy. Close-in lasers could do chemical (i.e., materials MASINT) analysis of samples vaporized by lasers.\n\nLaser systems are largely at a proof of concept level. One promising area is a synthetic imaging system that would be able to create images through forest canopy, but the current capability is much less than existing SAR or EO systems.\n\nA more promising approach would image through obscurations such as dust, cloud, and haze, particularly in urban environments. The laser illuminator would send a pulse, and the receiver would capture only the first photons to return, minimizing scattering and blooming.\n\nUse of LIDAR for precision elevation and mapping is much closer, and again chiefly in urban situations.\n\nSpectroscopy can be applied either to targets that are already excited, such as an engine exhaust, or stimulated with a laser or other energy source. It is not an imaging technique, although it can be used to extract greater information from images.\n\nWhere an \"IMINT\" sensor would take a picture that fills a frame, the \"Spectroscopic MASINT\" sensor gives a list, by coordinate, of wavelengths and energy. Multispectral IMINT is likely to discriminate more wavelengths, especially if it extends into the IR or UV, than a human being, even with an excellent color sense, could discriminate.\n\nThe results plot energy versus frequency. A spectral plot represents radiant intensity versus wavelength at an instant in time. The number of spectral bands in a sensor system determines the amount of detail that can be obtained about the source of the object being viewed. Sensor systems range from\n\nMore bands provide more discrete information, or greater resolution. The characteristic emission and absorption spectra serve to fingerprint or define the makeup of the feature that was observed. A radiometric plot represents the radiant intensity versus time; there can be plots at multiple bands or wavelengths. For each point along a time-intensity radiometric plot, a spectral plot can be generated based on the number of spectral bands in the collector, such as the radiant intensity plot of a missile exhaust plume as the missile is in flight. The intensity or brightness of the object is a function of several conditions including its temperature, surface properties or material, and how fast it is moving. Remember that additional, non-electro-optical sensors, such as ionizing radiation detectors, can correlate with these bands.\n\nAdvancing optical spectroscopy was identified as a high priority by a National Science Foundation workshop in supporting counterterrorism and general intelligence community needs. These needs were seen as most critical in the WMD context. The highest priority was increasing the sensitivity of spectroscopic scanners, since, if an attack has not actually taken place, the threat needs to be analyzed remotely. In the real world of attempting early warning, expecting to get a signature of something, which is clearly a weapon, is unrealistic. Consider that the worst chemical poisoning in history was an industrial accident, the Bhopal disaster. The participants suggested that the \"intelligence community must exploit signatures of feedstock materials, precursors, by-products of testing or production, and other inadvertent or unavoidable signatures.\" False positives are inevitable, and other techniques need to screen them out.\n\nSecond to detectability, as a priority was rejecting noise and background. It is especially difficult for biowarfare agents, which are the greatest WMD challenge to detect by remote sensing rather than laboratory analysis of a sample. Methods may need to depend on signal enhancement, by clandestine dispersion of reagents in the area of interest, which variously could emit or absorb particular spectra. Fluorescent reactions are well known in the laboratory; could they be done remotely and secretly? Other approaches could pump the sample with an appropriately tuned laser, perhaps at several wavelengths. The participants stressed that the need to miniaturize sensors, which might enter the area in question using unmanned sensors, including miniaturized aerial, surface, and even subsurface vehicles.\n\nElectro-optical spectroscopy is one means of chemical detection, especially using non-dispersive infrared spectroscopy is one MASINT technology that lends itself to early warning of deliberate or actual releases. In general, however, chemical sensors tend to use a combination of gas chromatography and mass spectrometry, which are more associated with materials MASINT. See Chemical Warfare and Improvised Chemical Devices.\n\nLaser excitation with multispectral return analysis is a promising chemical and possibly biological analysis method.\n\nSYERS 2, on the high-altitude U-2 reconnaissance aircraft, is the only operational airborne military multi-spectral sensor, providing 7 bands of visual and infrared imagery at high resolution.\n\nHyperspectral MASINT involves the synthesis of images as seen by visible and near infrared light. US MASINT in this area is coordinated by the Hyperspectral MASINT Support to Military Operations (HYMSMO) project. This MASINT technology differs from IMINT in that it attempts to understand the physical characteristics of what is seen, not just what it looks like.\n\nHyperspectral imaging typically needs multiple imaging modalitiesd, such as whiskbroom, pushbroom, tomographic, intelligent filters, and time series.\n\nSome of the major issues in visible and infrared hyperspectral processing include atmospheric correction, for the visible and short wave infrared. (0.4–2.5 micrometer) dictate sensor radiances need to be converted to surface reflectances. This dictates a need for measuring, and connecting for:\n\nHyperspectral, as opposed to multispectral, processing gives the potential of improved spectral signature measurement from airborne and spaceborne sensor platforms. Sensors on these platforms, however, must compensate for atmospheric effects. Such compensation is easiest with high contrast targets sensed through well-behaved atmosphere with even, reliable illumination, the real world will not always be so cooperative. For more complicated situations, one can not simply compensate for the atmospheric and illumination conditions by taking them out. The Invariant Algorithm for target detection was designed to find many possible combinations of these conditions for the image.\n\nMultiple organizations, with several reference sensors, are collecting libraries of hyperspectral signatures, starting with undisturbed areas such as deserts, forests, cities, etc.\n\nUnder the HYMSMO program, there have been a number of studies to build hyperspectral imaging signatures in various kinds of terrain. Signatures of undisturbed forest, desert, island and urban areas are being recorded with sensors including COMPASS, HYDICE and SPIRITT. Many of these areas are also being analyzed with complementary sensors including synthetic aperture radar (SAR).\n\nA representative test range, with and without buried metal, is the Steel Crater Test Area at the Yuma Proving Grounds. This was developedfor radar measurements, but is comparable to other signature development areas for other sensors and may be used for hyperspectral sensing of buried objects.\n\nIn applications of intelligence interest, the Johns Hopkins University Applied Physics Laboratory (JHU/APL) has demonstrated that hyperspectral sensing allows discrimination of refined signatures, based on a large number of narrow frequency bands across a wide spectrum. These techniques can identify include military vehicle paints, characteristic of particular countries' signatures. They can differentiate camouflage from real vegetation. By detecting disturbances in earth, they can detect a wide variety of both excavation and buried materials. Roads and surfaces that have been lightly or heavily trafficked will produce different measurements than the reference signatures.\nIt can detect specific types of foliage supporting drug-crop identification; disturbed soil supporting the identification of mass graves, minefields, caches, underground facilities or cut foliage; and variances in soil, foliage, and hydrologic features often supporting NBC contaminant detection. This was done previously with false-color infrared photographic film, but electronics are faster and more flexible.\n\nJHU/APL target-detection algorithms have been applied to the Army Wide Area Airborne Minefield Detection (WAAMD) program's desert and forest. By using the COMPASS and AHI hyperspectral sensors, robust detection of both surface and buried minefields is achieved with very low false alarm rates.\n\nHyperspectral imaging can detect disturbed earth and foliage. In concert with other methods such as coherent change detection radar, which can precisely measure changes in the height of the ground surface. Together, these can detect underground construction.\n\nWhile still at a research level, Gravitimetric MASINT can, with these other MASINT sensors, give precise location information for deeply buried command centers, WMD facilities, and other critical target. It remains a truism that once a target can be located, it can be killed. \"Bunker-buster\" nuclear weapons are not needed when multiple precision guided bombs can successively deepen a hole until the no-longer-protected structure is reached.\n\nUsing data collected over US cities by the Army COMPASS and Air Force SPIRITT sensors, JHU/APL target detection algorithms are being applied to urban hyperspectral signatures. The ability to robustly detect unique spectral targets in urban areas denied for ground inspection, with limited ancillary information will assist in the development and deployment of future operational hyperspectral systems overseas.\n\nPeace operations and war crimes investigation may require the detection of often-clandestine mass graves. Clandestinity makes it difficult to get witness testimony, or use technologies that require direct access to the suspected grave site (e.g., ground penetrating radar). Hyperspectral imaging from aircraft or satellites can provide remotely sensed reflectance spectra to help detect such graves. Imaging of an experimental mass grave and a real-world mass grave show that hyperspectral remote imaging is a powerful method for finding mass graves in real time, or, in some cases, retrospectively.\n\nJHU/APL target detection algorithms have been applied to the HYMSMO desert and forest libraries, and can reveal camouflage, concealment and deception protecting ground military equipment. Other algorithms have been demonstrated, using HYDICE data, that they can identify lines of communication based on the disturbance of roads and other ground surfaces.\n\nKnowing the fractions of vegetation and soil is of helps estimate the biomass. Biomass is not extremely important for military operations, but gives information for national-level economic and environmental intelligence. Detailed hyperspectral imagery such as the leaf chemical content (nitrogen, proteins, lignin and water) can be relevant to counterdrug surveillance.\n\nThe US, in 1970, launched the first of a series of space-based staring array sensors that detected and located infrared heat signatures, typically from rocket motors but also from other intense heat sources. Such signatures, which are associated with measurement of energy and location, are not pictures in the IMINT sense. Currently called the Satellite Early Warning System (SEWS), the program is the descendant of several generations of Defense Support Program (DSP) spacecraft. The USSR/Russian US-KMO spacecraft has been described, by US sources, as having similar capabilities to DSP.\n\nOriginally intended to detect the intense heat of an ICBM launch, this system proved useful at a theater level in 1990-1991. It detected the launch of Iraqi Scud missiles in time to give early warning to potential targets.\n\nSeveral new technologies will be needed for shallow-water naval operations. Since acoustic sensors (i.e., passive hydrophones and active sonar) perform less effectively in shallow waters than in the open seas, there is a strong pressure to develop additional sensors.\n\nOne family of techniques, which will require electro-optical sensors to detect, is bioluminescence: light generated by the movement of a vessel through plankton and other marine life. Another family, which may be solved with electro-optical methods, radar, or a combination, is detecting wakes of surface vessels, as well as effects on the water surface caused by underwater vessels and weapons.\n"}
{"id": "30126422", "url": "https://en.wikipedia.org/wiki?curid=30126422", "title": "European Conference on Computer Vision", "text": "European Conference on Computer Vision\n\nECCV, the European Conference on Computer Vision, is a biennial research conference with the proceedings published by Springer Science+Business Media. Similar to ICCV in scope and quality, it is held those years which ICCV is not. Like ICCV and CVPR, it is considered an important conference in computer vision, with an 'A' rating from the Australian Ranking of ICT Conferences and an 'A1' rating from the Brazilian ministry of education. The acceptance rate for ECCV 2010 was 24.4% posters and 3.3% oral presentations.\n\nLike other top computer vision conferences, ECCV has tutorial talks, technical sessions, and poster sessions. The conference is usually spread over five to six days with the main technical program occupying three days in the middle, and tutorial and workshops, focussed on specific topics, being held in the beginning and at the end.\n"}
{"id": "199645", "url": "https://en.wikipedia.org/wiki?curid=199645", "title": "European Geostationary Navigation Overlay Service", "text": "European Geostationary Navigation Overlay Service\n\nThe European Geostationary Navigation Overlay Service (EGNOS) is a satellite based augmentation system (SBAS) developed by the European Space Agency and EUROCONTROL on behalf of the European Commission. It supplements the GPS, GLONASS and Galileo by reporting on the reliability and accuracy of their positioning data and sending out corrections. \n\nEGNOS consists of a network of about 40 ground stations and 3 geostationary satellites. Ground stations determine accuracy data of the satellite navigation systems and transfer it to the geostationary satellites; users may freely obtain this data from those satellites using an EGNOS-enabled receiver, or over the internet. One main use of the system is in aviation.\n\nAccording to specifications, horizontal position accuracy when using EGNOS-provided corrections should be better than seven metres. In practice, the horizontal position accuracy is at the metre level. \n\nSimilar service is provided in North America by the Wide Area Augmentation System (WAAS), and in Asia, by Japan's Multi-functional Satellite Augmentation System (MSAS) and India's GPS Aided GEO Augmented Navigation (GAGAN).\n\nThe system started its initial operations in July 2005, with accuracy better than two metres and availability above 99%. As of July 2005, EGNOS has been broadcasting a continuous signal, and at the end of July 2005 the system was again used to track cyclists in the Tour de France road race.\n\nIn 2009, the European Commission announced it had signed a contract with the company European Satellite Services Provider to run EGNOS. The official start of operations was announced by the European Commission on 1 October 2009. The system was certified for use in safety of life applications in March 2011. An EGNOS Data Access Service became available in July 2012.\n\nInitial work to extend EGNOS coverage to the Southern Africa region is being done under a project called ESESA - EGNOS Service Extension to South Africa.\n\nThe European Commission is defining the roadmap for the evolution of the EGNOS mission. This roadmap should cope with legacy and new missions:\n\nSimilar to WAAS, EGNOS is mostly designed for aviation users who enjoy unperturbed reception of direct signals from geostationary satellites up to very high latitudes. The use of EGNOS on the ground, especially in urban areas, is limited due to relatively low elevation of geostationary satellites: about 30° above horizon in central Europe and much less in the North of Europe. To address this problem, ESA released in 2002 SISNeT, an Internet service designed for continuous delivery of EGNOS signals to ground users. The first experimental SISNeT receiver was created by the Finnish Geodetic Institute. The commercial SISNeT receivers have been developed by Septentrio. PRN #136 was placed into the Operational Platform from 23/08/2018 at 10:00 UTC and PRN #120 was placed into Test Platform from 30/08/2018 at 13:00 UTC.\n\nMore than 40 ground stations are linked together to create EGNOS network which consists:\n\n\nIn March 2011, the EGNOS Safety-of-Life Service was deemed acceptable for use in aviation. This allows pilots throughout Europe to use the EGNOS system as a form of positioning during an approach, and allows pilots to land the aircraft in IMC using a GPS approach. \n\nAs of September 2014 LPV (Localizer performance with vertical guidance) landing procedures, which are EGNOS-enabled, were available at more than 114 airports across Europe.\n\n"}
{"id": "53144771", "url": "https://en.wikipedia.org/wiki?curid=53144771", "title": "Flowmaster Ltd.", "text": "Flowmaster Ltd.\n\nFlowmaster Ltd. was a leading British Engineering Simulation Software company based in Towcester, UK. Its flagship 1D CFD product, also named ‘Flowmaster’, was first released commercially in 1987 although initial versions went back to the early 1980s having originated from BHRA, the not-for-profit British Hydromechanics Research Association, later to become the BHR Group.\n\nFlowmaster 1D thermo-fluid systems simulation software employed a matrix type solver and was the first tool of its type onto the market. It was initially sold and marketed by Amazon Computers Ltd based in Milton Keynes, UK. The software covered many different industries such as aerospace, automotive, marine, oil & gas, power generation, process, rail and water.\n\nFlowmaster software itself was based on extensive experimental validation data from D. S. Miller’s internationally respected scientific textbook, ‘Internal Flow Systems’, first published in 1978.\n\nA company called Flowmaster International (that later became Flowmaster Ltd) was created in 1992 when Richard Tickle was appointed CEO. During the next ten years Flowmaster experienced significant growth opening international sales offices near Frankfurt, Germany and Chicago, USA. Richard was followed as CEO by Alan Berry who led the company up to its acquisition in 2012 by Mentor Graphics Corporation where it became part of Mentor’s Mechanical Analysis Division.\n\nThe Flowmaster product has subsequently been renamed as FloMASTER by Mentor Graphics in 2016 for its V8 release.\n\nDevelopment work first commenced on a commercial computer code that would become ‘Flowmaster’ in the period between 1980 and 1984 based on the extensive BHRA research data from Don Miller’s ‘Internal Flow Systems’ for pipework and ducting. Flowmaster V1.0 was subsequently released commercially in 1987. It was the first general purpose 1D CFD flow solver, having ‘pick and drop’ components and an interactive user interface with “joystick” control; all innovations for their time. Its first customer was Vickers Shipbuilding and Engineering Ltd. in the UK. By 2002, the 1,000th commercial license of Flowmaster was purchased by Tokyo Gas Company Ltd.\n\nFlowmaster V1 was released in 1987.\n\nFlowmaster V2 was released in 1990.\n\nFlowmaster V3 was released in 1993.\n\nFlowmaster V4 was released in 1995.\n\nFlowmaster V5 was released in 1997.\n\nFlowmaster V6 was released in 1999.\n\nFlowmaster V7 was released in 2007.\n\nFloMASTER (Flowmaster) V8 was released in 2016.\n\nFlowmaster helped to pioneer software for virtual hybrid electric vehicles with its integration into the USA-based NREL ‘ADVISOR’ program that sought to simulate a complete vehicle cooling system in 2000. This idea has evolved into the concept of “digital twins” which today is a major driving force in many industries.\n\nFlowmaster has been added to over the years including the application of the Method of Characteristics to compressible pipe flow systems, a new approach to the modeling of AC components, the first commercial co-simulation link between 1D and 3D CFD software and control models were added in 1999. Industry specific variants of Flowmaster for Automotive, Aerospace, Power and Energy and Gas Turbines and were released from 2007 – 2016.\n\nAfter Flowmaster Ltd was acquired by Mentor Graphics Corporation in 2012 the software has been enhanced numerically and upgraded such that FloMASTER V8 was launched in December 2016 with Organic Rankine Cycle enhancements for Waste Heat Recovery predictions, a new user interface ‘Launchpad’ experience, plus a new integrated 1D-3D CFD workflow that allows for accelerated Simulation Based Characterization (SBC) of multi-arm 3D components inside the 1D systems simulation environment.\nFlowmaster is a general purpose 1D computational fluid dynamics (CFD) code for modeling and analysis of fluid mechanics in complex piping and ducting systems of any scale. It is a thermo-fluid systems engineering package that has the ability to simulate flows of gases and liquids, heat and mass transfer, moving bodies, two-phase flows and fluid-structure interaction (FSI) through computational modeling algorithms. It uses a Method of Characteristics solver that allows it to handle accurately fluid transients in complex systems and systems-of-systems such as pressure surge and ‘water hammer’. It also has an advanced gas turbine secondary air modeling capability.\n\nFloMASTER simulates 1D thermo-fluid systems and complex systems-of-systems through:\n\nEmpirical Data: The code is underpinned by D.S. Miller’s 'Internal Flow Systems' component and fluids data plus Wylie and Streeter’s 'Fluid Transients in Systems' that both allow for simulations to be initiated before detailed design information exists. The 1D model can then evolve according to the user’s requirements.\n\nSolver: Steady state analysis for pressure drop, flow and heat balance calculations to the same model can then be used for dynamic simulations such as pressure surge and water hammer behavior. The solver has embedded ‘parametrics’ for design iterations, and a SQL relational database to store and track all models, results, and performance data.\n\nIntegration: Open APIs allow for the direct integration with product development tools and systems, including optimization codes, MATLAB and other CAE codes.\n\nSoftware Development Kit (SDK): A suite of tools that enable FloMASTER to be integrated with product development tools and systems, including optimization tools such as Optimus, Control Modeling tools such as Simulink and other CAE tools. Integration is facilitated through an open API which supports COM and .NET as well as through an embedded script editor which allows creation of native macros, scripts and plug-ins.\n\n1D-3D CFD: Model-Based Design (MBD) and Simulation-Based Characterization (SBC) features that allow for modeling of components for which data is either difficult to obtain, or non-existent in a 1D sense, to be created. Examples include multi-arm junctions and non-standard ducts and bends. The SBC approach leads to a virtual ‘flow test bench’ inside the Mentor Graphics FloEFD 3D CFD tool to extend the FloMASTER component catalog. \nInternal Flow Systems\n\nDon Miller\n\nWaste Heat Recovery\n\nModel-Based Design\n\nComputational Fluid Dynamics\n"}
{"id": "8965629", "url": "https://en.wikipedia.org/wiki?curid=8965629", "title": "Glory hole (petroleum production)", "text": "Glory hole (petroleum production)\n\nA glory hole in the context of the offshore petroleum industry is an excavation into the sea floor designed to protect the wellhead equipment installed at the surface of a petroleum well from icebergs or pack ice. An economically attractive alternative for exploiting offshore petroleum resources is a floating platform; however, ice can pose a serious hazard to this solution. While floating platforms can be built to withstand ice loading up to a design threshold, for the largest icebergs or the thickest pack ice the only sensible alternative is to move out of the way. Floating platforms can be disconnected from the wellheads in order to allow them to be moved away from threatening ice, but the wellhead equipment is fixed in place and hence vulnerable.\n\nThe keel of an iceberg or pack ice can extend far below the surface of the water. If this keel extends deep enough to make contact with the sea floor, it will scour the sea floor as the ice moves with the current. To protect the wellhead equipment from possible scouring, a glory hole is excavated into the sea floor. This excavation must be deep enough to allow adequate clearance between the top of the wellhead equipment and the surrounding sea floor. The resulting glory hole can be either open or cased. A cased glory hole utilizes steel casing as a retaining wall while an open glory hole is simply an excavation.\n\nDue to the cost of excavating individual glory holes, typically each glory hole will contain several wellheads. Locating multiple wellheads within a single glory hole is made possible by the use of directional drilling.\n\nThe usage of the term glory hole in this context almost certainly is taken from its historical usage in the mining industry to refer to excavations.\n\n\n"}
{"id": "31505928", "url": "https://en.wikipedia.org/wiki?curid=31505928", "title": "Green bridge (filtration system)", "text": "Green bridge (filtration system)\n\nGreen bridges are an ecotechnological in-situ horizontal filtration system. Their different physical and biological filters work in combination to remove suspended and dissolved impurities of water. Green bridge filters help in reducing the suspended solids by filtration process, reducing Chemical Oxygen Demand (COD)/Biochemical Oxygen Demand (BOD) by aerobic degradation. Green Bridges also help in the restoration of ecological food chain.\n\nNatural streams, rivers and lakes have their own in-built purification system which consists of natural slopes, stones for biological growth and complex food web help in the purification process. This food web is nothing but utilization of one's waste by another as its own food. Nature has her own living machinery of detritivorous microbes and other living species to consume wastes. These principles have been harnessed in the treatment of polluted streams.\n\nGreen bridges are developed using fibrous material with stones. All the floatable and suspended solids are trapped in this biological bridge and the turbidity of flowing water is reduced. Green plants on the bridges increase the DO level in water, which in turn facilitates the growth of aerobic organisms, which degrade organic pollutants. Sandeep Joshi, director, SERI (Shrishti Eco-Research Institute) has developed this technology and has received a patent for it.\n\n\n\nOther than the changes in the water quality mentioned above a multifold change in population of avifauna, terrestrial plants along the riverbanks has been noticed. There is an overall odour and mosquito reduction and improvement of river aesthetics. Increase in health status of aquatic life in lentic-lotic system by reduction in ecotoxicity of pollutants.\n\n"}
{"id": "32024161", "url": "https://en.wikipedia.org/wiki?curid=32024161", "title": "Harry Julius Emeléus", "text": "Harry Julius Emeléus\n\nHarry Julius Emeléus CBE, FRS (22 June 1903 – 2 December 1993) was a leading British inorganic chemist and a professor in the department of chemistry, Cambridge University.\n\nEmeléus was born in Poplar, London on 22 June 1903, the son of Karl Henry Emeléus (1869–1948), a pharmacist who was born in Vaasa, Finland. The family moved to the Old Pharmacy in Battle, Sussex shortly after Emeléus was born. His elder brother Karl George Emeléus (1901–1989), went on to become professor of physics at the Queen's University of Belfast.\n\nEmeléus was educated at St Leonards Collegiate School, Hastings, and Hastings grammar school followed by the Royal College of Science, Imperial College, London, graduating in 1923. He gained his PhD in 1926 and a DSc three years later. During his post-graduate studies he spent time at the University of Karlsruhe as a student of Alfred Stock and two years at Princeton University with Professor Hugh Stott Taylor. Among his many students, notable is Norman Greenwood.\n\nEmeléus served as president of the inorganic chemistry division of the International Union of Pure and Applied Chemistry (1955–60). He was also president of the Chemical Society (1958–60) and of the Royal Institute of Chemistry (1963–5).\n\n\nEmeléus died of heart failure at Addenbrooke's Hospital, Cambridge, on 2 December 1993. He was survived by his four children, his wife having predeceased him in January 1991.\n\n"}
{"id": "42426709", "url": "https://en.wikipedia.org/wiki?curid=42426709", "title": "IPassMe", "text": "IPassMe\n\niPassMe is a mobile technology firm based in Italy, using the digital wallet technology to open up a new 1-to-1 dynamic communication channel between businesses and consumers and enhance customer relationship.\n\niPassMe is a web platform allowing SME, Agencies and Developers to create and manage mobile passes (store cards, coupons, event tickets, & more..) with no prior technical knowledge, no upfront investment or complex setup required.\niPassMe provides B2B service integrated with Apple Passbook technology and not only, creating new business communication opportunities between customers and retailers, and bridging the gap between consumers and companies who want to innovate by taking advantage of the potential of mobile digital wallet.\n\niPassMe makes the world of mobile commerce accessible to all businesses with a broad range of software and hardware including iBeacons technology, to provide in-store mobile communication.\n\nIn 2013, iPassMe was founded by Davide Starnone and Luigi Castiglione. The company received 30k€ in Seed funding in October 2013 from the Italian Incubator ComoNExT.\n"}
{"id": "10081264", "url": "https://en.wikipedia.org/wiki?curid=10081264", "title": "Independent Lubricant Manufacturer Association", "text": "Independent Lubricant Manufacturer Association\n\nThe Independent Lubricant Manufacturer Association (ILMA) was established in 1948 it is a members only trade organization that represents the interests of lubricant manufacturers. The word \"independent\" in the name indicates that Integrated Oil Companies (Big Oil) are not members however most of them are ILMA sponsors.\n\n"}
{"id": "35573025", "url": "https://en.wikipedia.org/wiki?curid=35573025", "title": "Institute of Agricultural Biodiversity and Rural Development", "text": "Institute of Agricultural Biodiversity and Rural Development\n\nThe Agricultural Biodiversity and Rural Development Institute - IBADER is a research university joint institute, created in 2001, with headquarters in the city of Lugo. This institution belongs to the University of Santiago de Compostela in its Campus from Lugo. Nowadays is formed by the University of Santiago de Compostela, Xunta de Galicia and Deputación Provincial de Lugo.\n\nThe main aims of this institute are: \n\nThere are 6 research groups of the la University of Santiago de Compostela assigned to the Institute of Agricultural Biodiversity and Rural Development, having more than 70 researchers.\n"}
{"id": "34857133", "url": "https://en.wikipedia.org/wiki?curid=34857133", "title": "Integromics", "text": "Integromics\n\nIntegromics is a global bioinformatics company headquartered in Granada, Spain, with a second office in Madrid and subsidiaries in the US and UK and distributors in 10 countries. Integromics S.L. provides bioinformatics software for data management and data analysis in genomics and proteomics. The company provides a line of products that serve the gene expression, sequencing and proteomics markets. Customers include genomic research centers, pharmaceutical companies, academic institutions, clinical research organizations and biotechnology companies.\n\nIntegromics was acquired by PerkinElmer to become part of its global informatics solution, in 2017.\n\nIntegromics operates in the global Life Science market and has an established a network of collaborations with international technology providers such as Applied Biosystems, Ingenuity and Spotfire, pharmaceutical companies, and academic institutions.\nIntegromics has key scientific collaborations with main research institutions and companies. \n\n\n\nSeqSolve® is a software solution for the tertiary analysis of Next Generation Sequencing (NGS) data.\n\nRealTime StatMiner® is a Step-by-Step Guide for RT-qPCR data analysis. RealTime StatMiner is available as a standalone as well as a TIBCO Spotfire compatible application. Co-developed with Applied Biosystems.\n\nIntegromics Biomarker Discovery® (IBD) for microarray gene expression data analysis guides the user throughout a step-by-step workflow.\n\nOmicsHub® Proteomics is a platform for the central management and analysis of data in proteomics labs.\n\nClick And Go® technology enables the automatic processing of the data along with a quality control system to prevent experimental biases.\n\n"}
{"id": "39327336", "url": "https://en.wikipedia.org/wiki?curid=39327336", "title": "Jackson Barnett No. 11 Oil Well", "text": "Jackson Barnett No. 11 Oil Well\n\nThe Jackson Barnett No. 11 Oil Well was the most productive oil well in the Cushing Oil Field of northeastern Oklahoma, to the south of Drumright. The well was drilled in 1916 in the Shamrock Dome section of the Cushing field by the Gypsy Oil Company, striking oil on February 17. The well was on the land of Jackson Barnett, a Creek landowner who subsequently became known as the \"world's richest Indian.\"\n\nBarnett owned in trust Creek County, which began producing oil in 1912. Jackson's 12.5 percent royalty on the oil from his land earned him between $3 million and $4 million during his lifetime, but only received a few hundred dollars per year at first. Until 1920 his fortune was managed as a trust by Creek County courts and the Department of the Interior, on the pretext that Jackson was illiterate and legally incompetent due to a head injury. Barnett's money became the subject of extensive litigation and eventual Congressional hearings. In 1920 he was pursued by Anna Laura Lowe, whom he married after one meeting. When a court gave part of the trust's money to Anna to administer, the couple moved to Los Angeles and bought a mansion.\nThe well was drilled into the Tucker sand layer at a depth between and Production on the well's first day was 4,000 barrels, rising to 10,000 barrels a day and peaking at 18,000 barrels per day. Average production was 10,000 barrels a day for most of its life. It was the first well in the Cushing field to produce 1 million barrels of oil and established the Oklahoma record for production from a single well. The well was capped in the mid-1960s. The site includes concrete foundations, the capped well casing, and a sign. In 1980 eight neighboring wells continued to produce oil.\n\nThe Jackson Barnett well was placed on the National Register of Historic Places on July 27, 1982.\n"}
{"id": "41726116", "url": "https://en.wikipedia.org/wiki?curid=41726116", "title": "LinkedIn Learning", "text": "LinkedIn Learning\n\nLinkedIn Learning (formerly lynda.com) is an American massive open online course website offering video courses taught by industry experts in software, creative, and business skills. It is a subsidiary of LinkedIn.\n\nIt was founded in 1995 by Lynda Weinman as \"lynda.com\" before being acquired by LinkedIn in 2015. Microsoft acquired lynda.com's parent company Linkedin in December 2016.\n\nLinkedIn Learning was founded as lynda.com was founded in 1995 in Ojai, California, as online support for the books and classes of Lynda Weinman, a special effects animator and multimedia professor who founded a digital arts school with her husband, artist Bruce Heavin.\n\nIn 2002, the company began offering courses online. By 2004, there were 100 courses, and in 2008, the company began producing and publishing documentaries on creative leaders, artists, and entrepreneurs.\n\nIn 2013, LinkedIn Learning (then lynda) received its first outside investment, raising $103 million in growth equity from Accel Partners and Spectrum Equity, with additional contributions from Meritech Capital Partners.\n\nOn January 14, 2015, LinkedIn Learning (then lynda.com) announced it had raised $186 million in financing, led by investment group TPG Capital.\n\nOn April 9, 2015, LinkedIn announced its intention to buy lynda.com in a deal valued at $1.5 billion, which officially closed on May 14, 2015. The site re-branded to \"Lynda.com® From LinkedIn\".\n\nIn 2016, LinkedIn Learning (then lynda.com) began to broadcast courses on their Apple TV application.\n\nOn June 13, 2016, Microsoft announced that it would acquire lynda.com's parent company LinkedIn for $26.2 billion. The acquisition was completed on December 8, 2016.\n\nIn October 2017, lynda was merged and renamed as \"LinkedIn Learning\".\n\nIn February 2013, LinkedIn Learning (then lynda.com) acquired video2brain, an Austrian-based provider of online classes in web design and programming, available in German, French, Spanish, and English.\n\nOn April 7, 2014, LinkedIn Learning (then lynda.com) purchased Canadian startup Compilr, provider of an online editor and sandbox.\n"}
{"id": "3502906", "url": "https://en.wikipedia.org/wiki?curid=3502906", "title": "Mannheim process", "text": "Mannheim process\n\nThe Mannheim process is an industrial process for the production of hydrogen chloride and sodium sulfate.\n\nThe Mannheim process is used to produce sodium sulfate. Sulfuric acid is reacted with sodium chloride within the Mannheim furnace. The furnace is a large cast iron kiln where the sodium chloride and sulfuric acid are first fed onto a stationary reaction plate where an initial reaction takes place. The stationary plate is up to 6 m in diameter. Rotating rabble arms constantly turn over the mixture and move the intermediate product to a lower plate. The kiln portion of the furnace is constructed with bricks that have high\nresistance to direct flame, temperature, and acid. The other parts of the furnace are heat and acid resistant. Hot flue gas passes up over the plates carrying out liberated Hydrogen Chloride gas. The intermediate product reacts with more sodium chloride in the lower, hotter section of the kiln producing sodium sulfate. This exits the furnace and passes through cooling drums before being\nmilled, screened and sent to product storage facilities. \nThe process involves two chemical reactions. In the first step sodium chloride and sulfuric acid are combined to produce sodium bisulfate and hydrochloric acid, an exothermic reaction that can occur at room temperature. The second step of the process involves an endothermic reaction, requiring energy input. A temperature of 600° to 700°C is required and maintained within the furnace to convert additional sodium chloride and the intermediate product to produce sodium sulfate and more hydrochloric acid. Based on public information about Mannheim Furnace production, about 80% of the production cost results from purchasing chemicals and 10% of the\ncost is for fuel (energy) and sulfuric acid.\n\nThe reactants in the chemical reaction are sulfuric acid (HSO) and sodium chloride (NaCl, table salt). The reaction yields sodium sulfate (NaSO) and gaseous hydrogen chloride (HCl).\n\n2 NaCl + HSO → NaSO + 2 HCl \n"}
{"id": "53936853", "url": "https://en.wikipedia.org/wiki?curid=53936853", "title": "Matias Recchia", "text": "Matias Recchia\n\nMatias Recchia is an entrepreneur based in Argentina. He is the co-founder and Chief Executive Officer of IguanaFix, an online marketplace for home improvement and car repair services.\n\nRecchia was born in Argentina in 1981 and raised in Venezuela. He had an interest in business as a child and graduated high school at age 15. Recchia attended Universidad Metropoitana and London School of Economics. Then he worked at Procter & Gamble for two years before going to Harvard and earning an MBA in his early 20s.\n\nRecchia was working as a business consultant at McKinsey & Company before he joined a social gaming startup called Vostu in 2010. Recchia was one of the first employees. He was appointed as the Chief Executive Officer of Vostu in 2012 in order to facilitate a restructuring and downsizing. He laid off 80 percent of the workforce (400 employees) in what Recchia said was \"one of the worst days of my life.\"\n\nRecchia left Vostu in 2013 forming IguanaFix with co-founder Andres Bersnasconi, who Recchia worked with at Vostu. The idea for IgunanaFix was conceived of after Recchia grew frustrated looking for painters and electricians at his new home after returning to Argentina. Recchia's work growing IguanaFix became the subject of a Harvard case study.\n"}
{"id": "31669191", "url": "https://en.wikipedia.org/wiki?curid=31669191", "title": "McQ Inc", "text": "McQ Inc\n\nMcQ Inc is a defense and electronics company in Fredericksburg, Virginia, that specializes in remote monitoring and surveillance equipment and systems for government and industry. McQ Inc designed and produces the OmniSense unattended ground sensor system equipment in use as part of currently deployed Unattended Ground Sensors (UGS).\n\nMcQ was founded in 1985 and began work mainly as contract support. In 1990 McQ began its transition to an electronics R&D company and has designed and produced many systems and products since. In 2010, McQ consolidated its production facilities into a new building on its Fredericksburg campus, expanding their manufacturing capabilities to meet demand.\n\nIn 2005, the Army Research Laboratory’s OmniSense system was commercialized and fielded by McQ Inc.\n\nIn 2006, McQ's OmniSense UGS system was selected as one of the Army's 10 greatest inventions for that year.\n\n"}
{"id": "12442563", "url": "https://en.wikipedia.org/wiki?curid=12442563", "title": "Minister for Further Education, Higher Education and Science", "text": "Minister for Further Education, Higher Education and Science\n\nThe Minister for Further Education, Higher Education and Science is a Junior ministerial post in the Education Department of the Scottish Government. As a result, the Minister does not attend the Scottish Cabinet. The post was created in May 2007 after the appointment of the Scottish National Party minority administration and the Minister reports to the Cabinet Secretary for Education and Skills, who has overall responsibility for the portfolio, and is a member of cabinet. The Minister has specific responsibility for further education and colleges, higher education and universities, science and STEM (science, technology, engineering and mathematics, student funding, youth work, and widening access to education.\n\nThe post is one of two reporting to the Cabinet Secretary for Education and Skills, alongside the Minister for Children and Young People.\n\nThe responsibilities of the junior ministerial reporting to the Education Secretary have changed considerably over time. The post was originally focused on schools: from 1999 to 2000, responsibility for schools rested with the Minister for Children and Education, which became the Minister for Education, Europe and External Affairs in the McLeish Government of 2000 to 2001. From 2001 to 2007, the schools portfolio rested with the Minister for Education and Young People. The skills brief also rested with the Minister for Enterprise and Lifelong Learning between 1999 and 2007.\n\nThe Salmond government, elected following the Scottish Parliament election, 2007, created the junior post of the Minister for Schools and Skills who assisted the Cabinet Secretary for Education and Lifelong Learning, in the Scottish Executive Education Department. This was then retitled on 7 December 2011 following the creation of the Minister of Youth Employment post which took over the responsibility for skills and training. A further change occurred in May 2016, following Nicola Sturgeon's reshuffle. The post was rebranded as Minister for Further Education, Higher Education and Science.\n\nThe post was vacant for a period over as the summer of 2018, as First Minister Nicola Sturgeon's original choice, Gillian Martin, was revealed to have posted comments of a transphobic nature in a blog in 2007. Ms Martin's name was omitted from the list of appointments to be approved by Parliament on 28 June 2018 following a reshuffle. Richard Lochhead was later appointed to the role.\n\n\n"}
{"id": "21488", "url": "https://en.wikipedia.org/wiki?curid=21488", "title": "Nanotechnology", "text": "Nanotechnology\n\nNanotechnology (\"nanotech\") is manipulation of matter on an atomic, molecular, and supramolecular scale. The earliest, widespread description of nanotechnology referred to the particular technological goal of precisely manipulating atoms and molecules for fabrication of macroscale products, also now referred to as molecular nanotechnology. A more generalized description of nanotechnology was subsequently established by the National Nanotechnology Initiative, which defines nanotechnology as the manipulation of matter with at least one dimension sized from 1 to 100 nanometers. This definition reflects the fact that quantum mechanical effects are important at this quantum-realm scale, and so the definition shifted from a particular technological goal to a research category inclusive of all types of research and technologies that deal with the special properties of matter which occur below the given size threshold. It is therefore common to see the plural form \"nanotechnologies\" as well as \"nanoscale technologies\" to refer to the broad range of research and applications whose common trait is size. Because of the variety of potential applications (including industrial and military), governments have invested billions of dollars in nanotechnology research. Through 2012, the USA has invested $3.7 billion using its National Nanotechnology Initiative, the European Union has invested $1.2 billion, and Japan has invested $750 million.\n\nNanotechnology as defined by size is naturally very broad, including fields of science as diverse as surface science, organic chemistry, molecular biology, semiconductor physics, energy storage, microfabrication, molecular engineering, etc. The associated research and applications are equally diverse, ranging from extensions of conventional device physics to completely new approaches based upon molecular self-assembly, from developing new materials with dimensions on the nanoscale to direct control of matter on the atomic scale.\n\nScientists currently debate the future implications of nanotechnology. Nanotechnology may be able to create many new materials and devices with a vast range of applications, such as in nanomedicine, nanoelectronics, biomaterials energy production, and consumer products. On the other hand, nanotechnology raises many of the same issues as any new technology, including concerns about the toxicity and environmental impact of nanomaterials, and their potential effects on global economics, as well as speculation about various doomsday scenarios. These concerns have led to a debate among advocacy groups and governments on whether special regulation of nanotechnology is warranted.\n\nThe concepts that seeded nanotechnology were first discussed in 1959 by renowned physicist Richard Feynman in his talk \"There's Plenty of Room at the Bottom\", in which he described the possibility of synthesis via direct manipulation of atoms. The term \"nano-technology\" was first used by Norio Taniguchi in 1974, though it was not widely known.\n\nInspired by Feynman's concepts, K. Eric Drexler used the term \"nanotechnology\" in his 1986 book \"Engines of Creation: The Coming Era of Nanotechnology\", which proposed the idea of a nanoscale \"assembler\" which would be able to build a copy of itself and of other items of arbitrary complexity with atomic control. Also in 1986, Drexler co-founded The Foresight Institute (with which he is no longer affiliated) to help increase public awareness and understanding of nanotechnology concepts and implications.\n\nThus, emergence of nanotechnology as a field in the 1980s occurred through convergence of Drexler's theoretical and public work, which developed and popularized a conceptual framework for nanotechnology, and high-visibility experimental advances that drew additional wide-scale attention to the prospects of atomic control of matter. Since the popularity spike in the 1980s, most of nanotechnology has involved investigation of several approaches to making mechanical devices out of a small number of atoms.\n\nIn the 1980s, two major breakthroughs sparked the growth of nanotechnology in modern era. First, the invention of the scanning tunneling microscope in 1981 which provided unprecedented visualization of individual atoms and bonds, and was successfully used to manipulate individual atoms in 1989. The microscope's developers Gerd Binnig and Heinrich Rohrer at IBM Zurich Research Laboratory received a Nobel Prize in Physics in 1986. Binnig, Quate and Gerber also invented the analogous atomic force microscope that year.\n\nSecond, Fullerenes were discovered in 1985 by Harry Kroto, Richard Smalley, and Robert Curl, who together won the 1996 Nobel Prize in Chemistry. C was not initially described as nanotechnology; the term was used regarding subsequent work with related graphene tubes (called carbon nanotubes and sometimes called Bucky tubes) which suggested potential applications for nanoscale electronics and devices.\n\nIn the early 2000s, the field garnered increased scientific, political, and commercial attention that led to both controversy and progress. Controversies emerged regarding the definitions and potential implications of nanotechnologies, exemplified by the Royal Society's report on nanotechnology. Challenges were raised regarding the feasibility of applications envisioned by advocates of molecular nanotechnology, which culminated in a public debate between Drexler and Smalley in 2001 and 2003.\n\nMeanwhile, commercialization of products based on advancements in nanoscale technologies began emerging. These products are limited to bulk applications of nanomaterials and do not involve atomic control of matter. Some examples include the Silver Nano platform for using silver nanoparticles as an antibacterial agent, nanoparticle-based transparent sunscreens, carbon fiber strengthening using silica nanoparticles, and carbon nanotubes for stain-resistant textiles.\n\nGovernments moved to promote and fund research into nanotechnology, such as in the U.S. with the National Nanotechnology Initiative, which formalized a size-based definition of nanotechnology and established funding for research on the nanoscale, and in Europe via the European Framework Programmes for Research and Technological Development.\n\nBy the mid-2000s new and serious scientific attention began to flourish. Projects emerged to produce nanotechnology roadmaps which center on atomically precise manipulation of matter and discuss existing and projected capabilities, goals, and applications.\n\nNanotechnology is the engineering of functional systems at the molecular scale. This covers both current work and concepts that are more advanced. In its original sense, nanotechnology refers to the projected ability to construct items from the bottom up, using techniques and tools being developed today to make complete, high performance products.\n\nOne nanometer (nm) is one billionth, or 10, of a meter. By comparison, typical carbon-carbon bond lengths, or the spacing between these atoms in a molecule, are in the range , and a DNA double-helix has a diameter around 2 nm. On the other hand, the smallest cellular life-forms, the bacteria of the genus \"Mycoplasma\", are around 200 nm in length. By convention, nanotechnology is taken as the scale range following the definition used by the National Nanotechnology Initiative in the US. The lower limit is set by the size of atoms (hydrogen has the smallest atoms, which are approximately a quarter of a nm kinetic diameter) since nanotechnology must build its devices from atoms and molecules. The upper limit is more or less arbitrary but is around the size below which phenomena not observed in larger structures start to become apparent and can be made use of in the nano device. These new phenomena make nanotechnology distinct from devices which are merely miniaturised versions of an equivalent macroscopic device; such devices are on a larger scale and come under the description of microtechnology.\n\nTo put that scale in another context, the comparative size of a nanometer to a meter is the same as that of a marble to the size of the earth. Or another way of putting it: a nanometer is the amount an average man's beard grows in the time it takes him to raise the razor to his face.\n\nTwo main approaches are used in nanotechnology. In the \"bottom-up\" approach, materials and devices are built from molecular components which assemble themselves chemically by principles of molecular recognition. In the \"top-down\" approach, nano-objects are constructed from larger entities without atomic-level control.\n\nAreas of physics such as nanoelectronics, nanomechanics, nanophotonics and nanoionics have evolved during the last few decades to provide a basic scientific foundation of nanotechnology.\n\nSeveral phenomena become pronounced as the size of the system decreases. These include statistical mechanical effects, as well as quantum mechanical effects, for example the \"quantum size effect\" where the electronic properties of solids are altered with great reductions in particle size. This effect does not come into play by going from macro to micro dimensions. However, quantum effects can become significant when the nanometer size range is reached, typically at distances of 100 nanometers or less, the so-called quantum realm. Additionally, a number of physical (mechanical, electrical, optical, etc.) properties change when compared to macroscopic systems. One example is the increase in surface area to volume ratio altering mechanical, thermal and catalytic properties of materials. Diffusion and reactions at nanoscale, nanostructures materials and nanodevices with fast ion transport are generally referred to nanoionics. \"Mechanical\" properties of nanosystems are of interest in the nanomechanics research. The catalytic activity of nanomaterials also opens potential risks in their interaction with biomaterials.\n\nMaterials reduced to the nanoscale can show different properties compared to what they exhibit on a macroscale, enabling unique applications. For instance, opaque substances can become transparent (copper); stable materials can turn combustible (aluminium); insoluble materials may become soluble (gold). A material such as gold, which is chemically inert at normal scales, can serve as a potent chemical catalyst at nanoscales. Much of the fascination with nanotechnology stems from these quantum and surface phenomena that matter exhibits at the nanoscale.\n\nModern synthetic chemistry has reached the point where it is possible to prepare small molecules to almost any structure. These methods are used today to manufacture a wide variety of useful chemicals such as pharmaceuticals or commercial polymers. This ability raises the question of extending this kind of control to the next-larger level, seeking methods to assemble these single molecules into supramolecular assemblies consisting of many molecules arranged in a well defined manner.\n\nThese approaches utilize the concepts of molecular self-assembly and/or supramolecular chemistry to automatically arrange themselves into some useful conformation through a bottom-up approach. The concept of molecular recognition is especially important: molecules can be designed so that a specific configuration or arrangement is favored due to non-covalent intermolecular forces. The Watson–Crick basepairing rules are a direct result of this, as is the specificity of an enzyme being targeted to a single substrate, or the specific folding of the protein itself. Thus, two or more components can be designed to be complementary and mutually attractive so that they make a more complex and useful whole.\n\nSuch bottom-up approaches should be capable of producing devices in parallel and be much cheaper than top-down methods, but could potentially be overwhelmed as the size and complexity of the desired assembly increases. Most useful structures require complex and thermodynamically unlikely arrangements of atoms. Nevertheless, there are many examples of self-assembly based on molecular recognition in biology, most notably Watson–Crick basepairing and enzyme-substrate interactions. The challenge for nanotechnology is whether these principles can be used to engineer new constructs in addition to natural ones.\n\nMolecular nanotechnology, sometimes called molecular manufacturing, describes engineered nanosystems (nanoscale machines) operating on the molecular scale. Molecular nanotechnology is especially associated with the molecular assembler, a machine that can produce a desired structure or device atom-by-atom using the principles of mechanosynthesis. Manufacturing in the context of productive nanosystems is not related to, and should be clearly distinguished from, the conventional technologies used to manufacture nanomaterials such as carbon nanotubes and nanoparticles.\n\nWhen the term \"nanotechnology\" was independently coined and popularized by Eric Drexler (who at the time was unaware of an earlier usage by Norio Taniguchi) it referred to a future manufacturing technology based on molecular machine systems. The premise was that molecular scale biological analogies of traditional machine components demonstrated molecular machines were possible: by the countless examples found in biology, it is known that sophisticated, stochastically optimised biological machines can be produced.\n\nIt is hoped that developments in nanotechnology will make possible their construction by some other means, perhaps using biomimetic principles. However, Drexler and other researchers have proposed that advanced nanotechnology, although perhaps initially implemented by biomimetic means, ultimately could be based on mechanical engineering principles, namely, a manufacturing technology based on the mechanical functionality of these components (such as gears, bearings, motors, and structural members) that would enable programmable, positional assembly to atomic specification. The physics and engineering performance of exemplar designs were analyzed in Drexler's book \"Nanosystems\".\n\nIn general it is very difficult to assemble devices on the atomic scale, as one has to position atoms on other atoms of comparable size and stickiness. Another view, put forth by Carlo Montemagno, is that future nanosystems will be hybrids of silicon technology and biological molecular machines. Richard Smalley argued that mechanosynthesis are impossible due to the difficulties in mechanically manipulating individual molecules.\n\nThis led to an exchange of letters in the ACS publication Chemical & Engineering News in 2003. Though biology clearly demonstrates that molecular machine systems are possible, non-biological molecular machines are today only in their infancy. Leaders in research on non-biological molecular machines are Dr. Alex Zettl and his colleagues at Lawrence Berkeley Laboratories and UC Berkeley. They have constructed at least three distinct molecular devices whose motion is controlled from the desktop with changing voltage: a nanotube nanomotor, a molecular actuator, and a nanoelectromechanical relaxation oscillator. See nanotube nanomotor for more examples.\n\nAn experiment indicating that positional molecular assembly is possible was performed by Ho and Lee at Cornell University in 1999. They used a scanning tunneling microscope to move an individual carbon monoxide molecule (CO) to an individual iron atom (Fe) sitting on a flat silver crystal, and chemically bound the CO to the Fe by applying a voltage.\n\nThe nanomaterials field includes subfields which develop or study materials having unique properties arising from their nanoscale dimensions.\n\nThese seek to arrange smaller components into more complex assemblies.\n\nThese seek to create smaller devices by using larger ones to direct their assembly.\n\nThese seek to develop components of a desired functionality without regard to how they might be assembled.\n\n\nThese subfields seek to anticipate what inventions nanotechnology might yield, or attempt to propose an agenda along which inquiry might progress. These often take a big-picture view of nanotechnology, with more emphasis on its societal implications than the details of how such inventions could actually be created.\n\nNanomaterials can be classified in 0D, 1D, 2D and 3D nanomaterials. The dimensionality play a major role in determining the characteristic of nanomaterials including physical, chemical and biological characteristics. With the decrease in dimensionality, an increase in surface-to-volume ratio is observed. This indicate that smaller dimensional nanomaterials have higher surface area compared to 3D nanomaterials. Recently, two dimensional (2D) nanomaterials are extensively investigated for electronic, biomedical, drug delivery and biosensor applications.\n\nThere are several important modern developments. The atomic force microscope (AFM) and the Scanning Tunneling Microscope (STM) are two early versions of scanning probes that launched nanotechnology. There are other types of scanning probe microscopy. Although conceptually similar to the scanning confocal microscope developed by Marvin Minsky in 1961 and the scanning acoustic microscope (SAM) developed by Calvin Quate and coworkers in the 1970s, newer scanning probe microscopes have much higher resolution, since they are not limited by the wavelength of sound or light.\n\nThe tip of a scanning probe can also be used to manipulate nanostructures (a process called positional assembly). Feature-oriented scanning methodology may be a promising way to implement these nanomanipulations in automatic mode. However, this is still a slow process because of low scanning velocity of the microscope.\n\nVarious techniques of nanolithography such as optical lithography, X-ray lithography, dip pen nanolithography, electron beam lithography or nanoimprint lithography were also developed. Lithography is a top-down fabrication technique where a bulk material is reduced in size to nanoscale pattern.\n\nAnother group of nanotechnological techniques include those used for fabrication of nanotubes and nanowires, those used in semiconductor fabrication such as deep ultraviolet lithography, electron beam lithography, focused ion beam machining, nanoimprint lithography, atomic layer deposition, and molecular vapor deposition, and further including molecular self-assembly techniques such as those employing di-block copolymers. The precursors of these techniques preceded the nanotech era, and are extensions in the development of scientific advancements rather than techniques which were devised with the sole purpose of creating nanotechnology and which were results of nanotechnology research.\n\nThe top-down approach anticipates nanodevices that must be built piece by piece in stages, much as manufactured items are made. Scanning probe microscopy is an important technique both for characterization and synthesis of nanomaterials. Atomic force microscopes and scanning tunneling microscopes can be used to look at surfaces and to move atoms around. By designing different tips for these microscopes, they can be used for carving out structures on surfaces and to help guide self-assembling structures. By using, for example, feature-oriented scanning approach, atoms or molecules can be moved around on a surface with scanning probe microscopy techniques. At present, it is expensive and time-consuming for mass production but very suitable for laboratory experimentation.\n\nIn contrast, bottom-up techniques build or grow larger structures atom by atom or molecule by molecule. These techniques include chemical synthesis, self-assembly and positional assembly. Dual polarisation interferometry is one tool suitable for characterisation of self assembled thin films. Another variation of the bottom-up approach is molecular beam epitaxy or MBE. Researchers at Bell Telephone Laboratories like John R. Arthur. Alfred Y. Cho, and Art C. Gossard developed and implemented MBE as a research tool in the late 1960s and 1970s. Samples made by MBE were key to the discovery of the fractional quantum Hall effect for which the 1998 Nobel Prize in Physics was awarded. MBE allows scientists to lay down atomically precise layers of atoms and, in the process, build up complex structures. Important for research on semiconductors, MBE is also widely used to make samples and devices for the newly emerging field of spintronics.\n\nHowever, new therapeutic products, based on responsive nanomaterials, such as the ultradeformable, stress-sensitive Transfersome vesicles, are under development and already approved for human use in some countries.\n\nAs of August 21, 2008, the Project on Emerging Nanotechnologies estimates that over 800 manufacturer-identified nanotech products are publicly available, with new ones hitting the market at a pace of 3–4 per week. The project lists all of the products in a publicly accessible online database. Most applications are limited to the use of \"first generation\" passive nanomaterials which includes titanium dioxide in sunscreen, cosmetics, surface coatings, and some food products; Carbon allotropes used to produce gecko tape; silver in food packaging, clothing, disinfectants and household appliances; zinc oxide in sunscreens and cosmetics, surface coatings, paints and outdoor furniture varnishes; and cerium oxide as a fuel catalyst.\n\nFurther applications allow tennis balls to last longer, golf balls to fly straighter, and even bowling balls to become more durable and have a harder surface. Trousers and socks have been infused with nanotechnology so that they will last longer and keep people cool in the summer. Bandages are being infused with silver nanoparticles to heal cuts faster. Video game consoles and personal computers may become cheaper, faster, and contain more memory thanks to nanotechnology. Also, to build structures for on chip computing with light, for example on chip optical quantum information processing, and picosecond transmission of information.\n\nNanotechnology may have the ability to make existing medical applications cheaper and easier to use in places like the general practitioner's office and at home. Cars are being manufactured with nanomaterials so they may need fewer metals and less fuel to operate in the future.\n\nScientists are now turning to nanotechnology in an attempt to develop diesel engines with cleaner exhaust fumes. Platinum is currently used as the diesel engine catalyst in these engines. The catalyst is what cleans the exhaust fume particles. First a reduction catalyst is employed to take nitrogen atoms from NOx molecules in order to free oxygen. Next the oxidation catalyst oxidizes the hydrocarbons and carbon monoxide to form carbon dioxide and water. Platinum is used in both the reduction and the oxidation catalysts. Using platinum though, is inefficient in that it is expensive and unsustainable. Danish company InnovationsFonden invested DKK 15 million in a search for new catalyst substitutes using nanotechnology. The goal of the project, launched in the autumn of 2014, is to maximize surface area and minimize the amount of material required. Objects tend to minimize their surface energy; two drops of water, for example, will join to form one drop and decrease surface area. If the catalyst's surface area that is exposed to the exhaust fumes is maximized, efficiency of the catalyst is maximized. The team working on this project aims to create nanoparticles that will not merge. Every time the surface is optimized, material is saved. Thus, creating these nanoparticles will increase the effectiveness of the resulting diesel engine catalyst—in turn leading to cleaner exhaust fumes—and will decrease cost. If successful, the team hopes to reduce platinum use by 25%.\n\nNanotechnology also has a prominent role in the fast developing field of Tissue Engineering. When designing scaffolds, researchers attempt to the mimic the nanoscale features of a Cell's microenvironment to direct its differentiation down a suitable lineage. For example, when creating scaffolds to support the growth of bone, researchers may mimic osteoclast resorption pits.\n\nResearchers have successfully used DNA origami-based nanobots capable of carrying out logic functions to achieve targeted drug delivery in cockroaches. It is said that the computational power of these nanobots can be scaled up to that of a Commodore 64.\n\nAn area of concern is the effect that industrial-scale manufacturing and use of nanomaterials would have on human health and the environment, as suggested by nanotoxicology research. For these reasons, some groups advocate that nanotechnology be regulated by governments. Others counter that overregulation would stifle scientific research and the development of beneficial innovations. Public health research agencies, such as the National Institute for Occupational Safety and Health are actively conducting research on potential health effects stemming from exposures to nanoparticles.\n\nSome nanoparticle products may have unintended consequences. Researchers have discovered that bacteriostatic silver nanoparticles used in socks to reduce foot odor are being released in the wash. These particles are then flushed into the waste water stream and may destroy bacteria which are critical components of natural ecosystems, farms, and waste treatment processes.\n\nPublic deliberations on risk perception in the US and UK carried out by the Center for Nanotechnology in Society found that participants were more positive about nanotechnologies for energy applications than for health applications, with health applications raising moral and ethical dilemmas such as cost and availability.\n\nExperts, including director of the Woodrow Wilson Center's Project on Emerging Nanotechnologies David Rejeski, have testified that successful commercialization depends on adequate oversight, risk research strategy, and public engagement. Berkeley, California is currently the only city in the United States to regulate nanotechnology; Cambridge, Massachusetts in 2008 considered enacting a similar law, but ultimately rejected it. Relevant for both research on and application of nanotechnologies, the insurability of nanotechnology is contested. Without state regulation of nanotechnology, the availability of private insurance for potential damages is seen as necessary to ensure that burdens are not socialised implicitly. Over the next several decades, applications of nanotechnology will likely include much higher-capacity computers, active materials of various kinds, and cellular-scale biomedical devices.\n\nNanofibers are used in several areas and in different products, in everything from aircraft wings to tennis rackets. Inhaling airborne nanoparticles and nanofibers may lead to a number of pulmonary diseases, e.g. fibrosis. Researchers have found that when rats breathed in nanoparticles, the particles settled in the brain and lungs, which led to significant increases in biomarkers for inflammation and stress response and that nanoparticles induce skin aging through oxidative stress in hairless mice.\n\nA two-year study at UCLA's School of Public Health found lab mice consuming nano-titanium dioxide showed DNA and chromosome damage to a degree \"linked to all the big killers of man, namely cancer, heart disease, neurological disease and aging\".\n\nA major study published more recently in Nature Nanotechnology suggests some forms of carbon nanotubes – a poster child for the \"nanotechnology revolution\" – could be as harmful as asbestos if inhaled in sufficient quantities. Anthony Seaton of the Institute of Occupational Medicine in Edinburgh, Scotland, who contributed to the article on carbon nanotubes said \"We know that some of them probably have the potential to cause mesothelioma. So those sorts of materials need to be handled very carefully.\" In the absence of specific regulation forthcoming from governments, Paull and Lyons (2008) have called for an exclusion of engineered nanoparticles in food. A newspaper article reports that workers in a paint factory developed serious lung disease and nanoparticles were found in their lungs.\n\nCalls for tighter regulation of nanotechnology have occurred alongside a growing debate related to the human health and safety risks of nanotechnology. There is significant debate about who is responsible for the regulation of nanotechnology. Some regulatory agencies currently cover some nanotechnology products and processes (to varying degrees) – by \"bolting on\" nanotechnology to existing regulations – there are clear gaps in these regimes. Davies (2008) has proposed a regulatory road map describing steps to deal with these shortcomings.\n\nStakeholders concerned by the lack of a regulatory framework to assess and control risks associated with the release of nanoparticles and nanotubes have drawn parallels with bovine spongiform encephalopathy (\"mad cow\" disease), thalidomide, genetically modified food, nuclear energy, reproductive technologies, biotechnology, and asbestosis. Dr. Andrew Maynard, chief science advisor to the Woodrow Wilson Center's Project on Emerging Nanotechnologies, concludes that there is insufficient funding for human health and safety research, and as a result there is currently limited understanding of the human health and safety risks associated with nanotechnology. As a result, some academics have called for stricter application of the precautionary principle, with delayed marketing approval, enhanced labelling and additional safety data development requirements in relation to certain forms of nanotechnology.\n\nThe Royal Society report identified a risk of nanoparticles or nanotubes being released during disposal, destruction and recycling, and recommended that \"manufacturers of products that fall under extended producer responsibility regimes such as end-of-life regulations publish procedures outlining how these materials will be managed to minimize possible human and environmental exposure\" (p. xiii).\n\nThe Center for Nanotechnology in Society has found that people respond to nanotechnologies differently, depending on application – with participants in public deliberations more positive about nanotechnologies for energy than health applications – suggesting that any public calls for nano regulations may differ by technology sector.\n\n"}
{"id": "50331310", "url": "https://en.wikipedia.org/wiki?curid=50331310", "title": "Open Connectivity Foundation", "text": "Open Connectivity Foundation\n\nThe Open Connectivity Foundation (OCF) is an industry group whose stated mission is to develop specification standards, promote a set of interoperability guidelines, and provide a certification program for devices involved in the Internet of Things (IoT). It has become one of the biggest industrial connectivity standards organizations for IoT, including in its membership Samsung Electronics, Intel, Microsoft, Qualcomm and Electrolux. Currently, there are more than 300 member companies.\n\nThe OCF tries to realize the Internet of Things also called Network of Everything. The IoT requires easy discovery, and trusted and reliable connectivity between things. The OCF delivers a framework that enables these requirements via a specification, a reference implementation and a certification program. IoTivity, the open source reference implementation of the specifications, is actively developed by different members of the OCF.\n\nThe Open Interconnect Consortium (OIC) began as an industry group whose stated mission was to develop standards and certification for devices involved in the Internet of Things (IoT) based around Constrained Application Protocol (CoAP). OIC was created in July 2014 by Intel, Broadcom, and Samsung Electronics. Broadcom left the consortium shortly after it was established, due to a disagreement on how to handle intellectual property. \n\nIn September 2015 a release candidate of the specification in version 1.0 for the core framework, smart home device, resource type, security and remote access capabilities was released to the public, accessible also for non members without registration.\n\nOn February 19, 2016 the OIC changed its name to the Open Connectivity Foundation and added Microsoft, Qualcomm and Electrolux to its membership. \n\nCurrently, there are more than 300 other member partners, including OCF \"Diamond Members\" companies; Cisco Systems, Qualcomm, Intel, Microsoft, CableLabs, Electrolux, LG, Haier, Canon, and Samsung.\n\n"}
{"id": "873932", "url": "https://en.wikipedia.org/wiki?curid=873932", "title": "PCI-X", "text": "PCI-X\n\nPCI-X, short for Peripheral Component Interconnect eXtended, is a computer bus and expansion card standard that enhances the 32-bit PCI local bus for higher bandwidth demanded mostly by servers and workstations. It uses a modified protocol to support higher clock speeds (up to 133 MHz), but is otherwise similar in electrical implementation. PCI-X 2.0 added speeds up to 533 MHz, with a reduction in electrical signal levels.\n\nThe slot is physically a 3.3 V PCI slot, with exactly the same size, location and pin assignments. The electrical specifications are compatible, but stricter. However, while most conventional PCI slots are the 85 mm long 32-bit version, most PCI-X devices use the 130 mm long 64-bit slot, to the point that 64-bit PCI connectors and PCI-X support are seen as synonymous.\n\nPCI-X is in fact fully specified for both 32- and 64-bit PCI connectors, and PCI-X 2.0 added a 16-bit variant for embedded applications.\n\nIt has been replaced in modern designs by the similar-sounding PCI Express (officially abbreviated as PCIe), with a completely different connector and a very different electrical design, having one or more narrow but fast serial connection lanes instead of a number of slower connections in parallel.\n\nIn PCI, a transaction that cannot be completed immediately is postponed by either the target or the initiator issuing retry-cycles, during which no other agents can use the PCI bus. Since PCI lacks a split-response mechanism to permit the target to return data at a later time, the bus remains occupied by the target issuing retry-cycles until the read data is ready. In PCI-X, after the master issues the request, it disconnects from the PCI bus, allowing other agents to use the bus. The split-response containing the requested data is generated only when the target is ready to return all of the requested data. Split-responses increase bus efficiency by eliminating retry-cycles, during which no data can be transferred across the bus.\n\nPCI also suffered from the relative scarcity of unique interrupt lines. With only 4 interrupt lines (INTA/B/C/D), systems with many PCI devices require multiple functions to share an interrupt line, complicating host-side interrupt-handling. PCI-X added Message Signaled Interrupts, an interrupt system using writes to host-memory. In MSI-mode, the function's interrupt is not signaled by asserting an INTx line. Instead, the function performs a memory-write to a system-configured region in host-memory. Since the content and address are configured on a per-function basis, MSI-mode interrupts are dedicated instead of shared. A PCI-X system allows both MSI-mode interrupts and legacy INTx interrupts to be used simultaneously (though not by the same function.)\n\nThe lack of registered I/Os limited PCI to a maximum frequency of 66 MHz. PCI-X I/Os are registered to the PCI clock, usually through means of a PLL to actively control I/O delay the bus pins. The improvement in setup time allows an increase in frequency to 133 MHz.\n\nSome devices, most notably Gigabit Ethernet cards, SCSI controllers (Fibre Channel and Ultra320), and cluster interconnects could by themselves saturate the PCI bus's 133 MB/s bandwidth. Ports using a bus speed doubled to 66 MHz and a bus width doubled to 64 bits (with the pin count increased to 184 from 124), in combination or not, have been implemented. These extensions were loosely supported as optional parts of the PCI 2.x standards, but device compatibility beyond the basic 133 MB/s continued to be difficult.\n\nDevelopers eventually used the combined 64-bit and 66-MHz extension as a foundation, and, anticipating future needs, established 66-MHz and 133-MHz variants with a maximum bandwidth of 532 MB/s and 1064 MB/s respectively. The joint result was submitted as PCI-X to the PCI Special Interest Group (Special Interest Group of the Association for Computing Machinery). Subsequent approval made it an open standard adoptable by all computer developers. The PCI SIG controls technical support, training, and compliance testing for PCI-X. IBM, Intel, Microelectronics, and Mylex were to develop supporting chipsets. 3Com and Adaptec were to develop compatible peripherals. To accelerate PCI-X adoption by the industry, Compaq offered PCI-X development tools at their Web site.\n\nThe PCI-X standard was developed jointly by IBM, HP, and Compaq and submitted for approval in 1998. It was an effort to codify proprietary server extensions to the PCI local bus to address several shortcomings in PCI, and increase performance of high bandwidth devices, such as Gigabit Ethernet, Fibre Channel, and Ultra3 SCSI cards, and allow processors to be interconnected in clusters.\n\nIntel gave only a qualified welcome to PCI-X, stressing that the next generation bus would have to be a \"fundamentally new architecture\". According to Cary Snyder, a senior analyst with the \"Microprocessor Report\", \"PCI-X took two years to take off\" because of a \"falling-out between the PCI SIG and a key Intel interconnect designer who spearheaded development on the Accelerated Graphics Port caused Intel to pull out of the initial PCI-X effort\".\n\nThe first PCI-X products only hit the market after Intel had already announced their next-generation PCI Express at the 2001 Intel Developer Forum. When more details of PCI Express were released in August 2001, PCI SIG chairman Roger Tipley expressed his belief that \"PCI-X is going to be in servers forever because it serves a certain level of functionality, and it may not be compelling to switch to 3GIO [PCI Express] for that functionality. We learned that from not being able to get rid of ISA. ISA hung around because of all of these systems that weren't high-volume parts.\" Tipley also announced that (at the time) the PCI SIG was planning to fold PCI Express and PCI-X 2.0 into a single work tentatively called PCI 3.0, but that name was eventually used for a relatively minor revision of conventional PCI.\n\nIn 2003, the PCI SIG ratified PCI-X 2.0. It adds 266-MHz and 533-MHz variants, yielding roughly 2,132 MB/s and 4,266 MB/s throughput, respectively. PCI-X 2.0 makes additional protocol revisions that are designed to help system reliability and add Error-correcting codes to the bus to avoid re-sends. To deal with one of the most common complaints of the PCI-X form factor, the 184-pin connector, 16-bit ports were developed to allow PCI-X to be used in devices with tight space constraints. Similar to PCI-Express, PtP functions were added to allow for devices on the bus to talk to each other without burdening the CPU or bus controller.\n\nDespite the various theoretical advantages of PCI-X 2.0 and its backward compatibility with PCI-X and PCI devices, it has not been implemented on a large scale (). This lack of implementation primarily is because hardware vendors have chosen to integrate PCI Express instead.\n\nIBM was one of the (few) vendors which provided PCI-X 2.0 (266 MHz) support in their System i5 Model 515, 520 and 525; IBM advertised these slots as suitable for 10 Gigabit Ethernet adapters, which they also provided. HP offered PCI-X 2.0 in some ProLiant servers and offered dual-port 4Gbit/s Fibre Channel adapters, also operating at 266 MHz. AMD supported PCI-X 2.0 (266 MHz) via its 8132 Hypertransport to PCI-X 2.0 tunnel chip. ServerWorks was a vocal supporter of PCI-X 2.0 (to the detriment of the first generation PCI Express) particularly through its chief Raju Vegesna, who was however fired soon thereafter for roadmap disagreements with the Broadcom leadership.\n\nIn 2003, Dell announced it would skip PCI-X 2.0 in favor of more rapid adoption of PCI Express solutions. As reported by PC Magazine, Intel began to sideline PCI-X in their 2004 roadmap, in favor of PCI Express, arguing that the latter had substantial advantages in terms of system latency and power consumption, more dramatically stated as avoiding \"the 1,000-pin apocalypse\" for their Tumwater chipset.\n\nPCI-X revised the conventional PCI standard by doubling the maximum clock speed (from 66 MHz to 133 MHz) and hence the amount of data exchanged between the computer processor and peripherals. Conventional PCI supports up to 64 bits at 66 MHz (though anything above 32 bits at 33 MHz is seen only in high-end systems). The theoretical maximum amount of data exchanged between the processor and peripherals with PCI-X is 1.06 GB/s, compared to 133 MB/s with standard PCI. PCI-X also improves the fault tolerance of PCI, allowing, for example, faulty cards to be reinitialized or taken offline.\n\nPCI-X is backward compatible to PCI in the sense that the entire bus falls back to PCI if any card on the bus does not support PCI-X.\n\nThe two most fundamental changes are:\n\nEssentially all PCI-X cards or slots have a 64-bit implementation and vary as follows:\n\nMost 32-bit PCI cards will function properly in 64-bit PCI-X slots, but the bus speed will be limited to the clock frequency of the slowest card, an inherent limitation of PCI's shared bus topology. For example, when a PCI 2.3, if a 66-MHz peripheral is installed into a PCI-X bus capable of 133 MHz, the entire bus backplane will be limited to 66 MHz. To get around this limitation, many motherboards have multiple PCI/PCI-X buses, with one bus intended for use with high-speed PCI-X peripherals, and the other bus intended for general-purpose peripherals.\n\nMany 64-bit PCI-X cards are designed to work in 32-bit mode if inserted in shorter 32-bit connectors, with some loss of speed. An example of this is the Adaptec 29160 64-bit SCSI interface card. However some 64-bit PCI-X cards do not work in standard 32-bit PCI slots. Even if it would work, installing a 64-bit PCI-X card in a 32-bit slot will leave the 64-bit portion of the card edge connector not connected and overhanging, which requires that there be no motherboard components positioned so as to mechanically obstruct the overhanging portion of the card edge connector.\n\nPCI-X is often confused by name with similar-sounding PCI Express, commonly abbreviated as PCI-E or PCIe, although the cards themselves are totally incompatible and look different. While they are both high-speed computer buses for internal peripherals, they differ in many ways. The first is that PCI-X is a 64-bit parallel interface that is backward compatible with 32-bit PCI devices. PCIe is a serial point-to-point connection with a different physical interface that was designed to supersede both PCI and PCI-X.\n\nPCI-X and standard PCI buses may run on a PCIe bridge, similar to the way ISA buses ran on standard PCI buses in some computers. PCIe also matches PCI-X and even PCI-X 2.0 in maximum bandwidth. PCIe 1.0 x1 offers 250 MB/s in each direction (lane), and up to 16 lanes (x16) are currently supported each direction, in full-duplex, giving a maximum of 4 GB/s bandwidth in each direction. PCI-X 2.0 offers (at its maximum 64-bit 533-MHz variant) a maximum bandwidth of 4,266 MB/s (~4.3 GB/s), although only in half-duplex.\n\nPCI-X has technological and economical disadvantages compared to PCI Express. The 64-bit parallel interface requires difficult trace routing, because, as with all parallel interfaces, the signals from the bus must arrive simultaneously or within a very short window, and noise from adjacent slots may cause interference. The serial interface of PCIe suffers fewer such problems and therefore does not require such complex and expensive designs. PCI-X buses, like standard PCI, are half-duplex bidirectional, whereas PCIe buses are full-duplex bidirectional. PCI-X buses run only as fast as the slowest device, whereas PCIe devices are able to independently negotiate the bus speed. Also, PCI-X slots are longer than PCIe 1x through PCIe 16x, which makes it impossible to make short cards for PCI-X. PCI-X slots take quite a bit of space on motherboards, which can be a problem for ATX and smaller form factors.\n\n\n\n"}
{"id": "23183485", "url": "https://en.wikipedia.org/wiki?curid=23183485", "title": "Peli Lens", "text": "Peli Lens\n\nThe Peli Lens is a mobility aid for people with homonymous hemianopia. It is also known as “EP” or Expansion Prism concept and was developed by Dr. Eli Peli of Schepens Eye Research Institute in 1999. It expands the visual field by 20 degrees. He tested this concept on several patients in his private practice with great success using 40Δ Fresnel press-on prisms (Peli 2000). Development of the lens and clinical trials were funded by NEI-NIH Grant EY014723 awarded to Chadwick Optical. The results of the multi-center clinical trials were published in 2008 reporting a 74% patient acceptance rate. Under this grant Chadwick Optical also designed and produced a cosmetically acceptable permanent version of this concept in a prescription lens.\n\nAn improved version of the Peli Lens expanding the visual field by 30 degrees is available. Clinical trial results are pending.\n"}
{"id": "1212721", "url": "https://en.wikipedia.org/wiki?curid=1212721", "title": "Release notes", "text": "Release notes\n\nRelease notes are documents that are distributed with software products, sometimes when the product is still in the development or test state (e.g., a beta release). For products that have already been in use by clients, the release note is delivered to the customer when an update is released.\n\nRelease notes are documents that are shared with end users, customers and clients of an organization. The definition of the terms 'End Users', 'Clients' and 'Customers' are very relative in nature and might have various interpretations based on the specific context. For instance, Quality Assurance group within a software development organization can be interpreted as an internal customer. They detail the corrections, changes or enhancements made to the service or product the company provides. This document is usually circulated only after the product or service is thoroughly tested and approved against the specification provided by the development team. However this document might also be provided as an artifact accompanying the deliverables for System Testing and System Integration Testing and other managed environments especially with reference to an information technology organization.\n\nRelease notes can also contain test results and information about the test procedure. This kind of information gives readers of the release note more confidence in the fix/change done; this information also enables implementer of the change to conduct rudimentary acceptance tests.\n\nThere is no standard format for release notes that is followed throughout different organizations. Organizations normally adopt their own formatting styles based on the requirement and type of the information to be circulated. The content of release notes also vary according to the release type. For products that are at testing stage and that are newly released, the content is usually more descriptive compared to release notes for bug fixes and feature enhancements, which are usually brief. \nRelease notes may include the following sections:\n\nA release note is usually a terse summary of recent changes, enhancements and bug fixes in a particular software release. It is not a substitute for user guides. Release notes are frequently written in the present tense and provide information that is clear, correct, and complete.\n\n\n"}
{"id": "56443718", "url": "https://en.wikipedia.org/wiki?curid=56443718", "title": "Rudolf Bailovic", "text": "Rudolf Bailovic\n\nRudolf Bailovic (born 1885 in Sarajevo) was a Serbo-Croatian interpreter and cryptographer, of Austrian descent, who was head of the Balkan Referat of General der Nachrichtenaufklärung during World War II. Bailovic held the civil service rank of () and became Oberregierungsrat in 1944. Bailovic was considered an anti-Nazi, who held anti-Nazi views, and refused to wear German decorations, when in uniform. Bailovic was also a Turkish interpreter and spent significant time in evaluation, providing intelligence.\n\nBailovic was an Austrian cryptanalyst. A colonel in the Austro-Hungarian Army, who served as an officer in Trieste and was director of the Austrian cipher bureau during World War I. He was considered a middle ranking official of the Austrian cipher section, by Fenner, before and during World War II. During the Anschluss, Bailovic refused to surrender the keys of his department to the Nazis when Austria was subsumed. Subsequently, he was relegated to a minor position in the Austrian civil service. General Erich Fellgiebel and Fritz Thiele, recognising his potential, ordered Wilhelm Fenner to Vienna, to bring Bailovic, along with seven of his colleagues back to Germany, to be employed as cryptanalysts and evaluators. In the final tally, only 4 people came back with Bailovic, that included Joseph Seifert, the then current director of the Austrian cipher bureau. Upon their landing in Germany, a Forschungsamt official met the party at the airport where the Forschungsamt (abbr. FA) official offered money to Bailovic to work for them, which Fenner found disturbing.\n\nBailovic initially worked for the FA, which was the Luftwaffe's chief Hermann Göring private cipher bureau, specifically for the Nazi Party. Bailovic worked at the FA unit, for several months, when he quit and was known to be employed by Inspectorate 7/VI by Autumn 1941, when he ran the Balkan section. During this period, the results from solving both codes and cyphers in the Balkan section were generally forwarded to KONA 4, the Signals unit assigned to the Balkans theatre.\n\nBailovic ran the \"Bailovic Party\" inside In 7/VI, that was an anti-nazi clique. After the 20 July plot, the Grupperleiter Major Lechner, who replaced Major Metting as commander of In 7/VI, was a fanatical Nazi. The Bailovic Party, held the most able personnel, but dwindling Balkan traffic meant the unit was superfluous. With a new Nazi leader in Major Lechnet, and the plot to kill Hitler occurring, the group was to be disbanded. Bailovic was removed from the unit, along with several others, with Lechner being posted to the west to become commander of KONA 6.\n\nIn Oct 1944 Bailovic moved to the OKW/Chi, also as head of a section dealing with Balkan traffic. Towards the end of the war, he became an administrator, as Balkan message traffic dwindled by its nature, and became progressively rarer, as the war reached its conclusion.\n"}
{"id": "9482791", "url": "https://en.wikipedia.org/wiki?curid=9482791", "title": "Semiconductor Industry Association", "text": "Semiconductor Industry Association\n\nThe Semiconductor Industry Association (SIA) is a trade association and lobbying group founded in 1977 that represents the United States semiconductor industry. It is located in Washington, D.C.\n\nOne of the main achievements of the SIA was the creation of the first National Technology Roadmap for Semiconductors, in the early 1990s. A chart from this roadmap, reproduced below, outlined the progress of the semiconductor industry over the next 15 years.\n\nThe Semiconductor Industry Association (SIA) positions itself as the voice of the U.S. semiconductor industry. This is one of America's top export industries and a driver of American economic strength, national security and global competitiveness. Founded in 1977 by five microelectronics pioneers Wilfred Corrigan of Fairchild Semiconductor, Robert Noyce of Intel Corporation, Jerry Sanders of Advanced Micro Devices, Charles Sporck of National Semiconductor Corporation and John Welty of Motorola, SIA unites companies that account for 80 percent of America’s semiconductor production. Through this coalition, SIA seeks to strengthen US leadership of semiconductor design and manufacturing by working with Congress, the Administration and other key industry stakeholders to encourage policies and regulations that fuel innovation, propel business and drive international competition. \n\nThe SIA maintains that a robust semiconductor industry is the only way to ensure that America remains the global technology leader, and works towards this goal through outreach to members of Congress, their staff, executive branch officials, foreign governments, member companies and trade associations.\n\nSemiconductors – microchips that control all modern electronics – have a major impact on modern life. They enable the technologies that people use to work, communicate, travel, entertain, harness energy, treat illness and make new scientific discoveries.\n\n"}
{"id": "49975980", "url": "https://en.wikipedia.org/wiki?curid=49975980", "title": "Seven Solutions", "text": "Seven Solutions\n\nSeven Solutions is a Spanish hardware technology company headquartered in Granada, Spain, that developed the first white rabbit element on The White Rabbit Project which it was the White Rabbit Switch to use the Precision Time Protocol (PTP) in real application as networking. Seven Solutions got involved on it with the design, manufacture, testing and support.\n\nThis project was financed by The government of Spain and CERN. Through this project Seven Solution demonstrated a high performance enhanced PTP switch with sub-ns accuracy.\n"}
{"id": "38717352", "url": "https://en.wikipedia.org/wiki?curid=38717352", "title": "SlideWiki", "text": "SlideWiki\n\nSlideWiki is an open web-based OpenCourseWare authoring system. It supports learning content authoring and management (including SCORM 2004 compliance) and tools for collaboration/crowd-sourcing, translation, communication, evaluation and assessment.\n\nSlideWiki is a Web application facilitating the collaboration around educational content. With SlideWiki users can create and collaborate on slides and arrange slides in presentations. Presentations can be organized hierarchically, so as to structure them reasonably according to their content. Currently large-scale collaboration (also referred to as crowd-sourcing) around educational content (other than texts) is supported only in a very limited way. Slides, presentations, diagrams, assessment tests etc. are mainly created by tutors, teachers, lecturers and professors individually or in very small groups. The resulting content can be shared online (e.g. using Slideshare, OpenStudy, Google Docs). However, proper community collaboration, authoring, versioning, branching, reuse and re-purposing of educational content similarly as we know it from the open-source software community is currently not supported.\n\nSlideWiki is a platform, where potentially large communities of teachers, lecturers, academics are empowered to create sophisticated educational content in a collaborative way. For newly emerging research fields, for example, a collaboration facility such as SlideWiki allows disseminating content and educating PhD students and peer-researchers more rapidly, since the burden of creating and structuring the new field can be distributed among a large community. Specialists for individual aspects of the new field can focus on creating educational content in their particular area of expertise and still this content can be easily integrated with other content, re-structured and re-purposed. A particular aspect, which is facilitated by SlideWiki is multi-linguality. Since all content is versioned and richly structured, it is easy to semi-automatically translate content and to keep track of changes in various multi-lingual versions of the same content object.\n\nSlideWiki particularly focusses on the crowd-sourced authoring, translation support and enrichment of educational content with self-assessment questions. Features include:\n\n\nSlideWikis future development has been established with the beginning of the year 2016 as an EU funded project. It was reengineered on a new technology stack and released on the official SlideWiki.org platform in 2017. 17 partner institutes from all over Europe and also South America cooperate in order to improve SlideWiki and implement many new features and enhance the user experience in order to become a central hub for open educational resources, especially presentations.\n\n\n"}
{"id": "1145374", "url": "https://en.wikipedia.org/wiki?curid=1145374", "title": "Snubber", "text": "Snubber\n\nA snubber is a device used to suppress (\"snub\") a phenomenon such as voltage transients in electrical systems, pressure transients in fluid systems (caused by for example water hammer) or excess force or rapid movement in mechanical systems.\n\nSnubbers are frequently used in electrical systems with an inductive load where the sudden interruption of current flow leads to a sharp rise in voltage across the current switching device (\"inductive kick\"), in accordance with Faraday's law. This transient can be a source of electromagnetic interference (EMI) in other circuits. Additionally, if the voltage generated across the device is beyond what the device is intended to tolerate, it may damage or destroy it. The snubber provides a short-term alternative current path around the current switching device so that the inductive element may be safely discharged. Inductive elements are often unintentional, but arise from the current loops implied by physical circuitry. While current switching is everywhere, snubbers will generally only be required where a major current path is switched, such as in power supplies. Snubbers are also often used to prevent arcing across the contacts of relays and switches and the electrical interference and welding or sticking of the contacts that can occur (see also arc suppression).\n\nA simple RC snubber uses a small resistor (R) in series with a small capacitor (C). This combination can be used to suppress the rapid rise in voltage across a thyristor, preventing the erroneous turn-on of the thyristor; it does this by limiting the rate of rise in voltage (dV/dt) across the thyristor to a value which will not trigger it. An appropriately-designed RC snubber can be used with either DC or AC loads. This sort of snubber is commonly used with inductive loads such as electric motors. The voltage across a capacitor cannot change instantaneously, so a decreasing transient current will flow through it for a small fraction of a second, allowing the voltage across the switch to increase more slowly when the switch is opened. Determination of voltage rating can be difficult owing to the nature of transient waveforms, and may be defined simply by the power rating of the snubber components and the application. RC snubbers can be made discretely and are also built as a single component (see also Boucherot cell).\n\nWhen the current flowing is DC, a simple rectifier diode is often employed as a snubber. The snubber diode is wired in parallel with an inductive load (such as a relay coil or electric motor). The diode is installed so that it does not conduct under normal conditions. When the external driving current is interrupted, the inductor current flows instead through the diode. The stored energy of the inductor is then gradually dissipated by the diode voltage drop and the resistance of the inductor itself. One disadvantage of using a simple rectifier diode as a snubber is that the diode allows current to continue flowing for some time, causing the inductor to remain active for slightly longer than desired. When such a snubber is utilized in a relay, this effect may cause a significant delay in the \"drop out\", or disengagement, of the actuator.\n\nThe diode must immediately enter into forward conduction mode as the driving current is interrupted. Most ordinary diodes, even \"slow\" power silicon diodes, are able to turn on very quickly, in contrast to their slow reverse recovery time. These are sufficient for snubbing electromechanical devices such as relays and motors.\n\nIn high-speed cases, where the switching is faster than 10 nanoseconds, such as in certain switching power regulators, \"fast\", \"ultrafast\", or Schottky diodes may be required.\n\nMore sophisticated designs use a diode with an RC network.\n\nIn some DC circuits, a varistor or two inverse-series Zener diodes (collectively called a Transil or Transorb) may be used instead of the simple diode. Because these devices dissipate significant power, the relay may drop-out faster than it would with a simple rectifier diode. An advantage to using a transorb over just one diode is that it will protect against over voltage with both polarities, if connected to ground, forcing the voltage to stay between the confines of the breakdown voltages of the Zener diodes. A Zener diode connected to ground will protect against positive transients that go over the Zener's breakdown voltage, and will protect against negative transients greater than a normal forward diode drop.\n\nIn AC circuits a rectifier diode snubber cannot be used; if a simple RC snubber is not adequate a more complex bidirectional snubber design must be used.\n\nSnubbers for pipes and equipment are used to control movement during abnormal conditions such as earthquakes, turbine trips, safety valve closure, relief valve closure, or hydraulic fuse closure. Snubbers allow for free thermal movement of a component during regular conditions, but restrain the component in irregular conditions. A hydraulic snubber allows for pipe deflection under normal operating conditions. When subjected to an impulse load, the snubber becomes activated and acts as a restraint in order to restrict pipe movement. A mechanical snubber uses mechanical means to provide the restraint force.\n\n\n\n"}
{"id": "1730328", "url": "https://en.wikipedia.org/wiki?curid=1730328", "title": "Superconducting quantum computing", "text": "Superconducting quantum computing\n\nSuperconducting quantum computing is an implementation of a quantum computer in superconducting electronic circuits. Research in superconducting quantum computing is conducted by Google, Microsoft, IBM, Rigetti, and Intel. Up to nine fully controllable qubits are demonstrated in a 1D array, up to sixteen in a 2D architecture.\n\nMore than two thousand superconducting qubits are in a commercial product by D-Wave Systems, however these qubits implement Quantum annealing instead of a universal model of quantum computation.\n\nClassical computation models rely on physical implementations consistent with the laws of classical mechanics. It is known, however, that the classical description is only accurate for specific cases, while the more general description of nature is given by the quantum mechanics. Quantum computation studies the application of quantum phenomena, that are beyond the scope of classical approximation, for information processing and communication. Various models of quantum computation exist, however the most popular models incorporate the concepts of qubits and quantum gates. A qubit is a generalization of a bit - a system with two possible states, that can be in a quantum superposition of both. A quantum gate is a generalization of a logic gate: it describes the transformation that one or more qubits will experience after the gate is applied on them, given their initial state. The physical implementation of qubits and gates is difficult, for the same reasons that quantum phenomena are hard to observe in everyday life. One approach is to implement the quantum computers in superconductors, where the quantum effects become macroscopic, though at a price of extremely low operation temperatures.\n\nIn a superconductor, the basic charge carriers are pairs of electrons (known as Cooper pairs), rather than the single electrons in a normal conductor. The total spin of a Cooper pair is an integer number, thus the Cooper pairs are bosons (while the single electrons in the normal conductor are fermions). Cooled bosons, contrary to cooled fermions, are allowed to occupy a single quantum energy level, in an effect known as the Bose-Einstein condensate. In a classical interpretation it would correspond to multiple particles occupying the same position in space and having an equal momentum, effectively behaving as a single particle.\n\nAt every point of a superconducting electronic circuit (that is a network of electrical elements), the condensate wave function describing the charge flow is well-defined by a specific complex probability amplitude. In a normal conductor electrical circuit, the same quantum description is true for individual charge carriers, however the various wave functions are averaged in the macroscopic analysis, making it impossible to observe quantum effects. The condensate wave function allows designing and measuring macroscopic quantum effects. For example, only a discrete number of magnetic flux quanta penetrates a superconducting loop, similarly to the discrete atomic energy levels in the Bohr model. In both cases, the quantization is a result of the complex amplitude continuity. Differing from the microscopic quantum systems (such as atoms or photons) used for implementations of quantum computers, the parameters of the superconducting circuits may be designed by setting the (classical) values of the electrical elements that compose them, e.g. adjusting the capacitance or inductance.\n\nIn order to obtain a quantum mechanical description of an electrical circuit a few steps are required. First, all the electrical elements are described with the condensate wave function amplitude and phase, rather than with the closely related macroscopic current and voltage description used for classical circuits. For example, a square of the wave function amplitude at some point in space is the probability of finding a charge carrier there, hence the square of the amplitude corresponds to the classical charge distribution. Second, generalized Kirchhoff's circuit laws are applied at every node of the circuit network to obtain the equations of motion. Finally, the equations of motion are reformulated to Lagrangian mechanics and a quantum Hamiltonian is derived.\n\nThe devices are typically designed in the radio-frequency spectrum, cooled down in dilution refrigerators below 100mK and addressed with conventional electronic instruments, e.g. frequency synthesizers and spectrum analyzers. Typical dimensions on the scale of micrometers, with sub-micrometer resolution, allow a convenient design of a quantum Hamiltonian with the well-established integrated circuit technology.\n\nA distinguishing feature of superconducting quantum circuits is the usage of a Josephson junction - an electrical element non existent in normal conductors. A junction is a weak connection between two leads of a superconducting wire, usually implemented as a thin layer of insulator with a shadow evaporation technique. The condensate wave functions on the two sides of the junction are weakly correlated - they are allowed to have different superconducting phases, contrary to the case of a continuous superconducting wire, where the superconducting wave function must be continuous. The current through the junction occurs by quantum tunneling. This is used to create a non-linear inductance which is essential for qubit design, as it allows a design of anharmonic oscillators. A quantum harmonic oscillator cannot be used as a qubit, as there is no way to address only two of its states.\n\nThe three superconducting qubit archetypes are the phase, charge and flux qubits, though many hybridizations exist (Fluxonium, Transmon, Xmon, Quantronium). For any qubit implementation, the logical quantum states formula_1\n\nof frequency formula_2, a driven qubit Hamiltonian in a rotating wave approximation is\n\nformula_3,\n\nwhere formula_4 is the qubit resonance and formula_5 are Pauli matrices.\n\nIn order to implement a rotation about the formula_6 axis, one can set formula_7 and apply the microwave pulse at frequency formula_8 for time formula_9. The resulting transformation is\n\nformula_10,\n\nthat is exactly the rotation operator formula_11 by angle formula_12 about the formula_6 axis in the Bloch sphere. An arbitrary rotation about the formula_14 axis can be implemented in a similar way. Showing the two rotation operators is sufficient for universality, as every single qubit unitary operator formula_15 may be presented as formula_16 (up to a global phase, that is physically unimportant) by a procedure known as the formula_17 decomposition.\n\nFor example, setting formula_18 results with a transformation\n\nformula_19,\n\nthat is known as the NOT gate (up to the global phase formula_20).\n\nCoupling qubits is essential for implementing 2-qubit gates. Coupling two qubits may be achieved by connecting them to an intermediate electrical coupling circuit. The circuit might be a fixed element, such as a capacitor, or controllable, such as a DC-SQUID. In the first case, decoupling the qubits (during the time the gate is off) is achieved by tuning the qubits out of resonance one from another, i.e. making the energy gaps between their computational states different. This approach is inherently limited to allow nearest-neighbor coupling only, as a physical electrical circuit is to be lay out in between the connected qubits. Notably, D-Wave Systems' nearest-neighbor coupling achieves a highly connected unit cell of 8 qubits in the Chimera graph configuration. Generally, quantum algorithms require coupling between arbitrary qubits, therefore the connectivity limitation is likely to require multiple swap operations, limiting the length of the possible quantum computation before the processor decoherence.\n\nAnother method of coupling two or more qubits is by coupling them to an intermediate quantum bus. The quantum bus is often implemented as a microwave cavity, modeled by a quantum harmonic oscillator. Coupled qubits may be brought in and out of resonance with the bus and one with the other, hence eliminating the nearest-neighbor limitation. The formalism used to describe this coupling is cavity quantum electrodynamics, where qubits are analogous to atoms interacting with optical photon cavity, with the difference of GHz rather than THz regime of the electromagnetic radiation.\n\nOne popular gating mechanism includes two qubits and a bus, all tuned to different energy level separations. Applying microwave excitation to the first qubit, with a frequency resonant with the second qubit, causes a formula_21 rotation of the second qubit. The rotation direction depends on the state of the first qubit, allowing a controlled phase gate construction.\nMore formally, following the notation of, the drive Hamiltonian describing the system excited through the first qubit driving line is\n\nformula_22,\n\nwhere formula_23 is the shape of the microwave pulse in time, formula_24 is the resonance frequency of the second qubit, formula_25 are the Pauli matrices, formula_26 is the coupling coefficient between the two qubits via the resonator, formula_27 is the qubit detuning, formula_28 is the stray (unwanted) coupling between qubits and formula_29 is Planck constant divided by formula_30. The time integral over formula_23 determines the angle of rotation. Unwanted rotations due to the first and third terms of the Hamiltonian can be compensated with single qubit operations. The remaining part is exactly the controlled-X gate.\n\nArchitecture-specific readout (measurement) mechanisms exist. The readout of a phase qubit is explained in the qubit archetypes table above. A state of the flux qubit is often read by an adjust DC-SQUID magnetometer. A more general readout scheme includes a coupling to a microwave resonator, where the resonance frequency of the resonator is shifted by the qubit state.\n\nThe list of DiVincenzo's criteria for a physical system to implement a logical qubit is satisfied by the superconducting implementation. The challenges currently faced by the superconducting approach are mostly in the field of microwave engineering.\n\n\n"}
{"id": "55829056", "url": "https://en.wikipedia.org/wiki?curid=55829056", "title": "Tawni Cranz", "text": "Tawni Cranz\n\nTawni Cranz is an American information technology executive, formerly serving as the Chief Talent Officer of Netflix, a position she had held from October 2012 until April 2017. \n\nCranz went to the University of California, Santa Barbara and graduated with a BA in Psychology. She then attended graduate school at Claremont University's Peter F. Drucker and Masatoshi Ito Graduate School of Management and received an Executive MBA.\n\nCranz worked in Human Resources at Bausch & Lomb and FedEx. She joined Netflix in 2007. In October 2012 she became Chief Talent Officer. \n\nIn an interview with The Alumni Society, she said, \"I wanted to work in different industries to see how HR makes a meaningful impact in a variety of industries. in each setting. I’ve never worked in the same industry twice. Until now with the move from Cruise to Waymo. I’ve worked in start-ups and in large, established organizations. That variety helps me bring a better perspective, and more unique and innovative approaches to each new role I take.\"\n\nWhile at Netflix, she implemented a policy allowing parental leave during the first year of a child's life. The policy allows a parent, regardless of gender, to take time off or work part time while receiving full salary and benefits. Parents are not required to file for disability or other state coverage to qualify.\n"}
{"id": "31302951", "url": "https://en.wikipedia.org/wiki?curid=31302951", "title": "Trim-Slice", "text": "Trim-Slice\n\nThe Trim-Slice is a small, fanless nettop computer manufactured by the Israeli company CompuLab. Trim-Slice is the first commercially available desktop computer based on the NVIDIA Tegra 2. It was announced in January 2011 and began shipping in late April 2011.\n\nIn July 2013 CompuLab announced its successor, the Utilite computer, a single to quad core computer based on the Freescale i.MX6 SoC which has since then become one of the most popular fanless computers worldwide.\n\n"}
{"id": "12722882", "url": "https://en.wikipedia.org/wiki?curid=12722882", "title": "TutorVista", "text": "TutorVista\n\nTutorVista Global Private Limited (acquired by BYJUs) is an educational organisation that specializes in online tutoring majorly in the US. The website and its content are provided mainly in English. It relies on a combination of voice over Internet Protocol (VoIP) telephony, session recording, instant messaging, file sharing and an interactive whiteboard. The company mainly provides academic help by paid subscription to students in subjects such as English, Mathematics, Statistics and Science. \n\nTutorVista’s operations are entirely located in India, but most of its market is abroad. TutorVista’s online tutorial platform connects students with about 2,000 teachers across India, United States, UK, Australia, China, Middle East and South East Asia. TutorVista is reachable through computers, tablets, and mobile phones.\n\nTutorVista was founded in the year 2005 by the duo Krishnan Ganesh and Meena Ganesh with over years of experience in e-learning. Other formation contributors included Srinivas Anumolu, Louise Kumar, and Ravi Kannan. In 2006, Sequoia Capital India and Light speed Venture Partners invested more than $10 million in TutorVista.\n\nThe company expanded with the acquisition of Edurite, an educational firm in 2007 and a partnership with American Book Company in 2009. In 2011, Pearson increased a smaller stake to a 76% majority stake in TutorVista for $127 million and fully acquired it by buying the remaining 20% stake in 2013. At this point, Krishnan Ganesh and Meena Ganesh stepped down from all management roles in the Bangalore-based company and Srikanth Iyer was appointed the new CEO.\n\n\n\"The Economist\" has suggested that building trust for an unknown Indian brand was the biggest difficulty faced by the company. There have also been several protests in the United States against those organizations that do not allow face-to-face and personalized tutoring. Officials and parents from both local and national bodies have mixed reactions regarding the ethical and practical implications of this trend. In the UK it is estimated that 1 in 4 students is now tutored outside school, with offshore tutoring companies increasingly entering the market.\n\n"}
{"id": "7736795", "url": "https://en.wikipedia.org/wiki?curid=7736795", "title": "VX (videocassette format)", "text": "VX (videocassette format)\n\nVX was a short-lived and unsuccessful consumer analog recording videocassette format developed by Matsushita and launched in 1975 in Japan. In the United States it was sold using the Quasar brand and marketed under the name \"The Great Time Machine\" to exhibit its time-shifting capabilities, since VX machines had a companion electro-mechanical clock timer for timed recording of television programs. In Japan, the VX-100 model was launched in 1975, with the VX-2000 following in 1976. The first and only model sold in North America was the Quasar VR-1000 (based on the Panasonic VX-2000), with the VT-100 timer.\n\nThe VX cassette itself had both reels of magnetic tape stacked on top of each other in a coaxial fashion (much like the earlier Philips \"VCR\" and Cartrivision formats) in the bottom half of the tape, with a circular opening on the underside of the top half of the cassette, where the video head drum would enter. The tape in this opening was pre-formed in a loop to go around the head drum, eliminating the need for the tape to be pulled out of the cassette and threaded around the drum (as with later videocassette formats such as VHS and Betamax). The opening was protected by a cylindrical plastic plug to protect the tape, which was unscrewed from the opening when the tape was loaded in the machine (by means of moving a horizontal lever on the front of the machine), and inserted back into the tape when ejected (by moving the lever back).\n\nThe video head drum itself of a VX machine also has the unique distinction of being completely removable and replaceable without any special tools or equipment. The drum has a knurled nut on top, which can be adjusted by hand, allowing the drum to be removed from the deck for cleaning or replacement. To this date, the VX machines have been the only VCR designed with this feature.\n\n\n"}
{"id": "34301725", "url": "https://en.wikipedia.org/wiki?curid=34301725", "title": "Vegetarian and non-vegetarian marks", "text": "Vegetarian and non-vegetarian marks\n\nPackaged food products sold in India are required to be labelled with a mandatory mark in order to be distinguished between lacto-vegetarian and non-lacto-vegetarian. The symbol is in effect following the Food Safety and Standards (Packaging and Labelling) Act of 2006, and got a mandatory status after the framing of the respective regulations (Food Safety and Standards [Packaging and Labelling] Regulation) in 2011. According to the law, vegetarian food should be identified by a green symbol and non-vegetarian food with a brown symbol.\n\nRestaurants use voluntary Vegan Friendly mark to denote availability of vegan options. Packaged food manufacturers also use a variation of Vegan Friendly mark for their vegan offerings.\n\n\n"}
{"id": "47839262", "url": "https://en.wikipedia.org/wiki?curid=47839262", "title": "Wahoo Fitness", "text": "Wahoo Fitness\n\nWahoo Fitness is a fitness technology company based in Atlanta, Georgia. Its CEO is Chip Hawkins.\n\nAs of August 2014 Wahoo Fitness had 35 employees.\n\nWahoo is the official indoor trainer of Team Sky, with athletes including Chris Froome as users of their KICKR trainer.\nWahoo's most popular product is the GPS bike computer Elemnt Bolt. It is the successor of the Elemnt, having the same functionality but the Elemnt has a bigger screen size.\nWahoo has several types of bike trainers, called Kickr. Their heart rates monitors are Tickr and Tickr X.\n"}
{"id": "35525745", "url": "https://en.wikipedia.org/wiki?curid=35525745", "title": "WaterML", "text": "WaterML\n\nWaterML is a technical standard and information model used to represent hydrological time series structures. The current version is WaterML 2.0, released an open standard of the Open Geospatial Consortium (OGC).\n\nVersion 1.0 of WaterML was published in 2009 by the Consortium of Universities for the Advancement of Hydrologic Science. WaterML 1.0 (and 1.1) is an XML exchange format developed for use specifically in the United States.\n\nWaterML 2.0 is an open standard of the OGC. Version 2.0 marks a harmonisation with different formats from various organisations and countries, including the Australian Water Data Transfer Format, WaterML 1.0 from the United States, XHydro from Germany, and with existing OGC formats. WaterML 2.0 was adopted as an official standard by the OGC in September 2012, endorsed by the US Federal Geographic Data Committee, and has been proposed for adoption by the World Meteorological Organisation (WMO).\n\nExample uses include: exchange of data for operational hydrological monitoring programs; supporting operation of infrastructure (e.g. dams, supply systems); cross-border exchange of observational data; release of data for public dissemination; enhancing disaster management through data exchange; and exchange in support of national reporting. The standard was developed through a harmonisation process by members of the joint OGC-WMO Hydrology Domain Working Group.\n\nWaterML 2.0 makes use of existing OGC standards, primarily Observations and Measurements (O&M) and the Geography Markup Language (GML). This enhances consistency and interoperability with other standards and web services. Through use of the O&M standard, WaterML 2.0 defines types allowing for standard definition of the core properties relating to hydrological time series, including:\n\n\nThe core information model is defined using the Unified Modelling Language, allowing for flexibility in creating implementation-specific encodings. The standard defines a GML-conformant XML encoding allowing for use with OGC Web Services.\n\n\n"}
{"id": "45513855", "url": "https://en.wikipedia.org/wiki?curid=45513855", "title": "William W. Havens Jr.", "text": "William W. Havens Jr.\n\nWilliam Westerfield Havens Jr. (March 31, 1920June 29, 2004) was an American physicist.\n\nA graduate of City College of New York and Columbia University, Havens worked with James Rainwater on the construction of a neutron spectrometer, which became the subject of his doctoral thesis. During World War II he worked on the Manhattan Project, the effort to create the first atomic bombs, in its Substitute Alloy Materials (SAM) Laboratories.\n\nHavens was awarded his doctorate in 1946 after his thesis was declassified. He spent the rest of his career at Columbia University, where he became a full professor in 1955, and was its director of nuclear science and engineering from 1961 to 1977. He was part of the US delegation at the United Nations Atoms for Peace Conferences in Geneva in July and August 1955 and in September 1958, and became a consultant at the Los Alamos National Laboratory in 1962. He retired from Columbia in 1985, and from then until 1990 was the first full-time CEO of the American Physical Society.\n\nHavens was born in the Bronx on March 31, 1920, the son of William Havens Sr., a civil engineer, and Elsie S. Nedle, a schoolteacher. He had an older sister, Marjorie, who became a lawyer. He was a descendant of Jonathan Nicoll Havens, a politician who served in the United States Congress from 1795 to 1799. He was educated at Evander Childs High School, from which he graduated in 1935 at the age of 15. His father wanted him to enter Columbia University, his own \"alma mater\", but Columbia would not accept his on account of his age. He therefore entered the City College of New York, which he attended on a scholarship. While there he was on the swimming team, and served in the ROTC, reaching the rank of cadet captain. During the summer of 1938 he was a lifeguard at the ROTC camp. The following year he worked as a lifeguard at Jones Beach Island. He graduated with his Bachelor of Arts degree in 1939, majoring in mathematics and chemistry.\n\nIn 1939, Havens entered Columbia University, where he studied physics, taking classes on mechanics with George B. Pegram, atomic physics with Isidor Isaac Rabi and electromagnetism with Shirley L. Quimby. He was awarded his Master of Arts degree in 1941. He then began working on his doctorate under the supervision of John R. Dunning. Fellow graduate students in physics at the time included James Rainwater, Herbert L. Anderson and George Weil. He worked with Rainwater on the construction of a neutron spectrometer. His thesis, on \"Slow neutron cross sections of indium, gold, silver, antimony, lithium and mercury as measured with a neutron beam spectrometer\", was classified.\n\nThe physics faculty at Columbia were drawn into what became the Manhattan Project, the effort to create the first atomic bombs, which accelerated after the United States entered World War II in December 1941. Fermi and Anderson carried out studies of neutrons emitted by fission, while Dunning began investigating isotope separation. Havens and Rainwater attempted to measure the time it took for fission to occur. That found it was less than a microsecond, which was the smallest time that they could measure. They analyzed samples of uranium that Robert R. Wilson's team at Princeton University had attempted to separate the isotopes using a device called the \"isotron\". They reported that the degree of enrichment was slight, and the process was eventually abandoned. Wilson's group was transferred to the Los Alamos Laboratory.\n\nFermi's reactor group also left Columbia, as Arthur Compton consolidated the Manhattan Project's reactor project at the Metallurgical Laboratory. Those that remained at Columbia became the Substitute Alloy Materials (SAM) Laboratories, under the direction of Harold Urey, which was mainly concerned with isotope separation for uranium enrichment. Havens and Rainwater also devised a means of measuring residual hydrogen in fluorocarbons, a subject of great interest because uranium hexafluoride gas was being considered for use in isotope separation processes. Havens, Rainwater and Chien-Shiung Wu worked on the development of radiation detector instrumentation, and studies of neutron cross sections using the neutron spectrometer. This included work with plutonium. After the war ended in August 1945, a dozen papers by Dunning, Havens, Rainwater and Wu would be declassified and published. This included his PhD thesis, and he was awarded his doctorate after it was published, along with Rainwater's, in the Physical Review in 1946.\n\nHavens spent the rest of his career at Columbia University, where he became a full professor in 1955, and was its director of nuclear science and engineering from 1961 to 1977. In the immediate post-war period, Columbia built a powerful 400 MeV synchrotron, which became operational in 1950, at the Nevis Laboratories, on an estate on the Hudson River at Irvington, New York, willed to Columbia University by the DuPont family. To learn about cyclotrons, Havens spent some time with Emilio Segrè's group at the University of California, Berkeley. Havens used the Nevis synchrotron to produce neutrons as part of an Atomic Energy Commission project to see if plutonium could be produced without a nuclear reactor, as it was believed at the time that uranium was scarce. The project eventually evolved into the Materials Test Accelerator at the Lawrence Livermore Laboratory; but it never served its intended purpose, as uranium was found to not be a scarce as first thought.\n\nFrom 1948 until the 1970s, Havens served with the AEC Neutron Cross-sections Advisory Group. This led to his being a part of the US delegation at the United Nations Atoms for Peace Conferences in Geneva in July and August 1955 and in September 1958. Havens pressed the AEC to release more information on neutron cross-sections, but was thwarted by its chairman, Lewis Strauss, who explained, \"We’ve got to keep something secret\".\n\nThe work with neutrons led to Havens becoming a consultant (later called \"visiting staff member\") at Los Alamos in 1962. He would attend occasional meetings at Los Alamos during the academic year, and spend a couple of weeks there during the summer. At the time there was considerable interest in neutron weapons. In 1985, Havens retired from Columbia and became the first full-time CEO of the American Physical Society (APS), the largest professional physicists' body in the world. He retired from this in 1990. He oversaw the APS a period of considerable growth, and worked with Vera Kistiakowsky to promote the role of women in science.\n\nHavens died from complications related to leukaemia in Memorial Sloan-Kettering Hospital on June 29, 2004. He was survived by his wife Aldine, daughters Nancy and Cynthia, and sister, Marjorie.\n\n"}
