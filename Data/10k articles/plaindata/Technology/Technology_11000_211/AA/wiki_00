{"id": "35938155", "url": "https://en.wikipedia.org/wiki?curid=35938155", "title": "Air-gap flash", "text": "Air-gap flash\n\nAn air-gap flash is a photographic light source capable of producing sub-microsecond light flashes, allowing for (ultra) high-speed photography. This is achieved by a high-voltage (20 kV typically) electric discharge between two electrodes over the surface of a quartz (or glass) tube. The distance between the electrodes is such that a spontaneous discharge does not occur. To start the discharge a high-voltage pulse is applied on an electrode inside the quartz tube.\n\nThe flash can be triggered electronically by being synchronised with an electronic detection device such as a microphone or an interrupted laser beam in order to illuminate a fast event. A sub-microsecond flash is fast enough to photographically capture a supersonic bullet in flight without noticeable motion blur.\n\nThe person credited with popularising the flash is Harold Eugene Edgerton, though earlier scientists such as Ernst Mach also used a spark gap as a fast photographic lighting system. William Henry Fox Talbot is said to have created the first spark-based flash photo, using a Leyden jar, the original form of the capacitor. Edgerton was one of the founders of EG&G company who sold an air-gap flash under the name Microflash 549. There are several commercial flashes available today.\n\nThe aim of a high-speed flash is to be very fast and yet bright enough for adequate exposure. An air-gap flash system typically consists of a capacitor that is discharged through a gas (air in this case). The speed of a flash is mainly determined by the time it takes to discharge the capacitor through the gas. This time is proportional to\n\nformula_1,\n\nin which L is the inductance and C the capacitance of the system. To be fast, both L and C must be kept small.\n\nThe brightness of the flash is proportional to the energy stored in the capacitor:\n\n<math>E = \n"}
{"id": "23957650", "url": "https://en.wikipedia.org/wiki?curid=23957650", "title": "Belt filter", "text": "Belt filter\n\nThe belt filter (sometimes called a belt press filter, or belt filter press) is an industrial machine, used for solid/liquid separation processes, particularly the dewatering of sludges in the chemical industry, mining and water treatment. Belt filter presses are also used in the production of apple juice, cider and winemaking. The process of filtration is primarily obtained by passing a pair of filtering cloths and belts through a system of rollers. The system takes a sludge or slurry as a feed, and separates it into a filtrate and a solid cake.\n\nThe belt filter is mainly used for dewatering of sludge and slurry and juice extraction from apples, pears and other fruits, as well as grapes for winemaking, etc. Belt filters are used both municipally and industrially in a range of areas including urban sewage and wastewater treatment, metallurgy and mining, steel plants, coal plants, breweries, dyeing, tanneries, as well as chemical and paper factories.\n\nThe applications of a belt filter are only limited to the sludges, slurry or mashed fruit that it can process. The sludges from municipal use include raw, anaerobically digested and aerobically digested sludges, alum sludge, lime softening sludge and river water silt. In industry, any sludge or slurry is sourced from food processing wastes, pulp and paper wastes, chemical sludges, pharmaceutical wastes, industrial waste processing sludges, and petrochemical wastes. These wastes can include mixed sludge, mineral slurry, dust sediment, selected coal washing mud, biological sludge, primary sludge, and straw, wood or waste paper pulp.\n\nSome dewatering objectives include reducing the volume to reduce the transport and storage costs, removing liquids before landfill disposal, reducing fuel requirements before further drying or incineration, producing adequate material for composting, avoiding runoff and pooling when used for land applications, and optimizing other drying processes. Belt filters are specifically designed for each of these particular applications and feeds.\n\nThere are many physical separation processes similar to the belt filter press used for dewatering, including centrifuges, vacuum-disc filters, and the plate and frame filter press. When compared to other compression filters, belt filters use relatively lower pressures. Although centrifuges have lower moisture content, lower costs and simpler operations in coal tails processing, belt filters tend to make less noise and have much quicker startup and shutdown times.\n\nBelt filters are considered simple and reliable, with good availability, low staffing, easy maintenance and a long life. The belt filter is most advantageous when installed such that it is open and viewable at floor level for easier adjustment and monitoring. This is of course subject to what lighting and ventilation will allow.\n\nThe belt filter press is often used in wastewater treatment, and thus the odour of the feed sludge, volatile emissions and the chemicals used in treatment, may become a problem. One control method is to use odour-neutralizing chemicals such as potassium permanganate. However this only neutralizes odours and doesn’t affect any gases or chemicals involved. Although all problems can be controlled by enclosing the filter, the enclosure reduces essential visibility and easy access to the machine for maintenance and repairs, leading to the expensive automation of the process.\n\nA belt filter press is also known for its high capacity throughput, as it is designed to handle excess capacity. It has low initial costs and low energy running costs, however, if throughput is less than 4 million gallons per day, the belt filter press may be less cost efficient than liquid transport, hiring a processing facility, or utilizing non-mechanical dewatering methods such as drying beds or reed beds.\n\nBelt filters are less effective at processing some feeds. Unless the feed is well mixed from a digester, the use of belt filters will be more costly when processing a feed with varying solids content as this requires more operator attention, raising staffing costs. Feeds with high grease and oil content can lower the solids percentage in the cake by blinding the belt filter and all feeds need to be screened to protect the belt from being damaged by sharp objects. The type of feed may also affect the washing process. The belt filter needs to be washed frequently which consumes large amount of water and time. Water and time wastage, as well as the associated costs can be reduced by automating the washing system and using effluents.\n\nBelt filter designs are crafted using manufacturer design and performance data, operating installations, pilot testing, surveys of similar plants and testing of the wastewater solids to obtain a desired dewatered solid percentage from the sludge or slurry to be processed.\n\nBelt press filters have 4 main zones: preconditioning zone, gravity drainage zone, linear compression (low-pressure) zone and roller compression (high-pressure) zone. Preconditioned slurry, which is flocculated and/or coagulated depending on the feed and process, is thickened in the gravity drainage zone. The gravity drainage zone is a flat or inclined belt where gravity drainage of free water occurs The gravity drainage area is sized according to feed solid concentrations. The standard size can be used for solids concentrations of 1.5 percent or greater, but a setup with a longer drainage area or extended size should be used for 1.5 to 2.5 percent feed solids for more free water drainage before compression. For dilute sludge with feed solids of less than 1.5 percent, an independent gravity drainage belt can be used. This belt is used only in the gravity drainage area, not in the pressure zones. The pressure or wedge zones use two belts, upper and lower, to sandwich the feed together, but an independent gravity zone has its own separate belt, making the belt filter a three-belt system. Depending on the required conditions of the cake, belt filters can have added washing stages and, infrared, hot gas or even microwave drying stages.\n\nBelt filters are very versatile and are made to suit the sludges, slurries or mashed fruit to be processed. For a feed or treatment process which produces unpleasant odours, volatile emissions, pathogens and hazardous gases like hydrogen sulphide the belt filter can include fume hoods or even be completely enclosed in a gas tight housing. Due to the reduced visibility and increased corrosion associated with enclosure, the belt filter process can also be automated. Large filtration areas, additional rollers and a variable belt speed can be found in advanced belt press filter designs.\n\nBelt press filters are designed for solids capacity, by weight or volume, rather than wastewater flow. Solids concentration must be determined based on the concentration of primary solids in the feed and further solids that may precipitate during treatment. Solids concentration for a process will vary, thus the design must have the capacity to deal with varying feed solids concentration.\n\nThe feed to a belt press filter depends on the type of solids, desired product and filter design. For most sludge types the feed dry solids concentration is typically in the range of 1-10%. The resulting dewatered sludge (or cake) dry solids concentration typically falls in the range of 12-50%. Dilute feed solids concentration results in a cake of higher moisture content whilst a higher feed solids concentration yields an improved solids filtration rate and drier end product.\n\nThe input to a belt press filter is generally measured as the rate of dry solids loading (mass of dry solids per time per belt width). Again, the input solids loading is dependent on the sludge type and filter media, thus there is great variation in the dry solids loading rates of operating belt press filters. Typically, lower range solids loading rates fall in the range of 40–230 kg/h/m belt width and high range solids loading rates fall in the range of 300–910 kg/h/m belt width. Whilst loading is important for measuring production rate, it is also important to consider the thickness of the cake that forms in the gravity drainage section. Cake thickness affects the permeability of the filtration media and the filtration rate. Testing for the particular sludge type must be conducted to determine the optimum cake thickness. In some cases where filtrate recovery is important, it may be necessary to introduce a cake washing step.\n\nThe primary objective of a belt press filter is to dewater process sludge and much of this dewatering occurs in the gravity drainage zone. The gravity drainage zone can achieve a 5 to 10 percent increase in solids concentration. The degree of dewatering in the gravity drainage zone is greatly dependent on the type of solids, the filter media and the sludge conditioning. The dewatering achieved in the gravity drainage zone is adversely affected if the sludge is poorly spread across the belt or the residence time is insufficient. Sludge conditioning is the addition of chemicals to promote flocculation of particles to form a thickened sludge and to promote dewatering. Dewatering can be promoted by the addition of surfactant and flocculation is achieved via the addition of high molecular weight polymer. Flocculation is improved with optimum polymer dosage, polymer dilution and mixing. The pH of the feed slurry must also be monitored and controlled as low pH decreases flocculation. It is important to find the optimum value for each conditioning parameter as too much polymer or mixing can have a negative impact on flocculation and greatly increase operating expenses. The effects of sludge conditioning are most apparent in the gravity drainage zone which can be easily replicated on a laboratory scale where the optimum conditioning strategy can be determined. For a belt press filter to be industrially viable it must be economically efficient and thus maximum throughput is desired. Without sufficient conditioning, the gravity drainage is generally the limiting process step, but with optimum dilution the limiting process step can be shifted to the compression zone.\n\nIn the compression zone of a belt press filter, the filter cake is compressed between the two belts and passed over rollers to exert pressure on the cake. There is an optimum number of rollers above which a drier product is not necessarily the result. Drier product is obtained from reduced belt speed rather than increased pressing time.\n\nThe overall performance of a belt press filter is improved where variations in parameters such as sludge type, feed solids concentration and conditioning are minimised.\n\nThe efficiency of a belt press filter is often assessed based on the dry solids content of the product cake, solids recovery and lateral migration of sludge on the belt. Solids recovery is the percentage of dry solids recovered from the feed sludge. Solids recovery is dependent on the filter media which must be selected for good permeability to promote dewatering but with pore diameter sufficiently small so that solids recovery is not greatly decreased. It is important that the belt press filter has an effective belt washing section so that blinding does not decrease the permeability of the belt. Solids recovery is directly related to filtrate quality and thus the filter media and process arrangement must satisfy the desired cake and filtrate qualities. Dry solids content is a measure of the degree of dewatering. The degree of dewatering is increased when the belt speed is decreased. Lowering the belt speed reduces the capacity of the process. The following correlation relates input mass flow rate to belt speed:\n\nformula_1\n\nWhere Q = mass flow rate (kg/s), m = mass loading (kg/m), s = belt speed (m/s) and L = initial width of sludge across the belt (m). Thus to maintain industrial scale economic throughput at lower belt speed, the mass loading and width of sludge across the belt must be increased. It has been found that increasing the solids loading slightly decreases the dry solids concentration of the cake while significantly increasing the potential for sludge to overflow the belt. Lateral migration of sludge on the belt is a measure of how the sludge spreads across the width of the belt. Increased lateral sludge migration means that sludge is escaping the edge of the belt and overflowing into the filtrate. Therefore, increased lateral sludge migration negatively impacts filtrate quality and dry solids recovery.\n\nGenerally, the minimum design discharge cake thickness is 3–5 mm. This ensures that the cake is thick enough to discharge and is easily removed from the belt.\n\nIn order of increasing cost and decreasing product moisture the most common dewatering options are a thickener, deep bed thickening, belt presses and membrane filter presses. In general centrifuges and other competing technologies do not show a significant cost advantage compared to the belt press filter, for the same cake dryness. The cost of flocculant is often a major operating cost of dewatering equipment. Belt press filters in general have the lowest flocculant consumption for any of the listed processes excluding membrane filter presses and centrifuges.\n\nIncreasing the feed solid concentration increases the solid filtration rate, minimises cake moisture content and produces a more homogeneous cake all of which are desirable outcomes. If increasing the feed solid concentration is not practical the addition of flocculants in a pre-treatment step has a similar result. The optimum dosage level of flocculant can be found by monitoring the viscosity of the slurry.\n\nHomogeneous cakes are desirable as if the feed slurry is too dilute the filter cake will contain higher moisture content as a result of stratification. The minimum feed concentration that results in a homogeneous cake is determined by observing a sample of the slurry. If rapid settling occurs the filter cake formed will not be homogeneous and the filtration rate is decreased.\n\nThe minimum cake discharge thickness for horizontal belt press filters is in the region of 5 mm.\n\nThe choice of belt is critical to the function of the belt press filter and a wide variety of materials and weaves are available. The filter cloth for a belt press filter should be as open as possible while maintaining the desired filtrate clarity or, if precoat is used, to prevent the loss of precoat. Lighter cloths produce a clearer filtrate and do not block as rapidly however their durability and life span is significantly shorter than heavier cloths. Both seamless and seamed belts are available. Seamed belts wear faster at the seam and cause wear at the rollers and the doctor blade. Zipper-type and clipper-type seamed belts are also available with the zipper-type having a longer life span as they provide less discontinuity. Seamless belts have the longest life span but are more expensive. Also it should be ensured that the belt press is compatible with a seamless belt.\n\nIncreasing the temperature of the feed slurry decreases the viscosity of the liquid phase. This is beneficial as it increases the filtration rate and decreases the cake moisture. The same advantages can be obtained by other drying methods such as passing dry steam through the deliquored cake to raise the temperature of the remaining moisture, or other drying methods can be utilised.\n\nCake thickness may have to be controlled or restricted when cake washing is required or the final cake moisture is a critical parameter. When cake washing time is a dominating factor the maximum filtration rate will occur when the minimum cake thickness for discharge is achieved. The time required for washing is increased by the square of the ratio of cake thicknesses. For example, if the thickness of the cake is doubled the washing time will increase roughly by a factor of 4.\n\nCompletely clear filtrate cannot be obtained using belt press filters except in rare circumstances. Thus further treatment may be required for the filtrate before it is reused or discharged as waste. If the filter is downstream of a clarifier or thickener the filtrate (and wash water) can be recycled back into the clarifier to reduce the required filtrate clarity and allows for the use of more durable cloths. If recycling or reuse is not an option the filtrate should be discharged subject to legislation and license requirements. Further treatment of clarified water (filtration or chemical treatment) may be required before discharge.\n\nThe filter cake usually has a high enough solid concentration to allow for all types of disposal methods without further treatment including recycling back into the process, landfill/composting and incineration. The polymer content makes filter cake from a belt press filter more suited to the aforementioned disposal methods than a cake conditioned with ferric chloride and lime which can occur with other dewatering processes.\n\nSignificant developments in belt press filter technology include: cloth developments, using three belts and, the V-fold belt. Cloth developments include the double weave which incorporates different yarn types to combine the specific advantages of each. A double weave woven wire belt is also available which has a better life span and durability than a conventional wire belt.\n\nA belt press filter using three belts can achieve independent speeds and have different belt types for the pressure and gravity zones. This allows the filter system to accommodate higher hydraulic loadings occurring with dilute feed sludge (feed solid concentration below 1.5%). The three belt system is more efficient with both a higher production rate and cake solid concentration at the expense of mechanical complexity.\n\nThe V-fold belt is similar to the belt filter press with the main difference being that only a single belt, folded along the centreline is used. The technology has not been widely proven. A final dry weight solids content of 9-13% can usually be achieved; this is smaller than competing technologies. Currently this technology is suited to small-scale applications (up to approximately 3000 L of slurry per hour as the maximum belt size is 0.75 m). V-fold belts have a small footprint, low energy and wash water consumption and low capital and operating costs. They are self-tracking and can process sludge of varying composition, reducing operator involvement.\n\n"}
{"id": "36939928", "url": "https://en.wikipedia.org/wiki?curid=36939928", "title": "Berry-picking rake", "text": "Berry-picking rake\n\nA berry-picking rake or berry picker is a tool for collecting berries. Berry-picking rakes can be used to collect lingonberries, bilberries, currants, and other berries. The rake may damage softer berries, and introduces some detritus, requiring cleaning of the berries afterwards. If misused, it may also damage or uproot the plants, reducing next year's yields. Despite these drawbacks, a rake has much greater efficiency than picking by hand, and is thus used in all commercial berry picking - though modern large-scale farms use mechanical harvesters.\n"}
{"id": "44467727", "url": "https://en.wikipedia.org/wiki?curid=44467727", "title": "Blerp", "text": "Blerp\n\nBlerp was a web annotation system, launched in 2009 by RocketOn, which allowed users to overlay text, images, videos and other widgets onto web pages for other Blerp users visiting the same page to view. This would allow users to start spontaneous discussions about websites. Like a number of other web annotation startups, it was unsuccessful.\n\n"}
{"id": "923209", "url": "https://en.wikipedia.org/wiki?curid=923209", "title": "CSU/DSU", "text": "CSU/DSU\n\nA CSU/DSU (channel service unit/data service unit) is a digital-interface device used to connect data terminal equipment (DTE), such as a router, to a digital circuit, such as a Digital Signal 1 (DS1) T1 line. The CSU/DSU implements two different functions. The channel service unit (CSU) is responsible for the connection to the telecommunication network, while the data service unit (DSU) is responsible for managing the interface with the DTE. \n\nDSL and cable modems are CSU/DSUs because they convert from one type of digital signal to another. A CSU/DSU is the equivalent of the modem for an entire LAN.\n\nThe WAN Interface Card (WIC) may contain an integrated CSU/DSU that can be inserted into a router slot. An example of a WIC is the 1-port 56/64-kbit/s DSU/CSU WIC (WIC-1DSU-56K4) by Cisco Systems.\n"}
{"id": "22536920", "url": "https://en.wikipedia.org/wiki?curid=22536920", "title": "Camping food", "text": "Camping food\n\nBackcountry camping food includes ingredients used to prepare food suitable for backcountry camping and backpacking. The foods differ substantially from the ingredients found in a typical home kitchen. The primary differences relate to campers' and backpackers' special needs for foods that have appropriate cooking time, perishability, weight, and nutritional content.\n\nTo address these needs, camping food is often made up of either freeze-dried, precooked or dehydrated ingredients. Many campers use a combination of these foods.\n\nDue to the difficulty of carrying large amounts of cooking fuel, campers often require their meals to cook in a short amount of time (5–20 minutes). Many campers prefer a ‘just add boiling water’ method of cooking, while others enjoy a more involved, and therefore often higher quality meal. The amount of cooking time can be disregarded if campers are able to cook over a campfire, however, due to the possibility of a burn-ban being in place, campers do not often rely on this option.\n\nCamping foods are often shelf-stable, that is, they require no refrigeration. Campers may be outdoors for days or weeks at a time, and will often pack food for the entire trip. Campers will sometimes take fresh food that can be consumed in the first day or two of a hike but will usually not risk carrying perishable food beyond that timeframe. Campers hiking in the snow or other cold conditions or campers with access to a cold water source may be able to store perishable food in the snow or secured in a bag and kept in the cold water to act as a refrigeration source.\n\nBackpackers must carry everything with them so they require all of their gear and food to be as lightweight as possible. Campers often turn to freeze-dried and dehydrated meals and ingredients for this reason, but they will also sometimes take a pouch of tuna or some other ingredient with a high water content with them as a treat, providing that the item has nutritional value.\n\nBackpackers, canoeists, climbers and other outdoor enthusiasts often cover many miles everyday, consuming thousands of calories to keep their energy level high. Backpackers require an average of 480 calories per hour as well as higher sodium levels. Because of the high levels of nutritional burn and emphasis on weight, backpackers monitor the ratio of calories-to-ounce that their food provides. To ensure their bodies are properly nourished, campers must pay close attention to their meal plans.\n\nTo prepare meals that work well outdoors, campers employ a variety of techniques. All campers are advised to prepare meals that are made of easy to prepare ingredients.\n\nFreeze-drying requires the use of heavy machinery and is not something that most campers are able to do on their own. Freeze-dried ingredients are often considered superior to dehydrated ingredients however, because they rehydrate at camp faster and retain more flavor than their dehydrated counterparts. Freeze-dried ingredients take so little time to rehydrate that they can often be eaten without cooking them first and have a texture similar to a crunchy chip.\n\nSmall amounts of freeze-dried ingredients are sometimes available for sale from emergency supply outlets or from stores specific to camping. Freeze-dried ingredients that have not been combined into a meal are often hard to find, however, and are often sought out by campers.\n\nDehydration can reduce the weight of the food by sixty to ninety percent by removing water through evaporation. Some foods dehydrate well, such as onions, peppers, and tomatoes. Dehydration often produces a more compact, albeit slightly heavier, end result than freeze-drying.\n\nFull meals or individual ingredients may be dehydrated. Dehydration of individual ingredients allows the flexibility to cook different meals based on available ingredients, while precooked and dehydrated meals offer greater convenience. Several cookbooks and online grocery stores specialize in dehydrated foods. A Fork in the Trail and Another Fork in the Trail are both backcountry cookbooks that focus on dehydrating full meals.\n\nSurplus military Meals, Meals, Ready-to-Eat (MREs) are sometimes used by campers. These meals contain precooked foods in retort pouches. A retort pouch is a plastic and metal foil laminate pouch that is used as an alternative to traditional industrial canning methods.\n\nThe final type of ingredients available to campers are those that are typically found in the grocery store. Some examples of these types of food are polenta, grits, quick-cooking pasta (such as angel hair pasta), ramen, instant potatoes, dried soups, jerky and pouch meats such as tuna, SPAM or salmon.\n\nWhen using these common ingredients, campers often repackage them to reduce packaging or combine them into a meal-ready package, therefore reducing prep-time at camp. The main requirement that campers look for in these types of ingredients is the cook-time with 20 minutes being the longest amount of cook-time that most campers will tolerate.\n\nThere is a large variety of camping stoves on the market ranging in specialty from being extremely lightweight to focusing on using very little fuel. The majority of campers rely on a stove for their cooking needs as they boast several advantages over cooking over a campfire. Since most camping stoves have an adjustable heat source, they can be much easier to use than a campfire. The ability to quickly adjust the flame to reduce your pot from a boil to a simmer, for example, is considered invaluable to many campers. Campfires can take a long time to start and get to a point where they are suitable for cooking over. Since a cook-stove can be ready in minutes, this is an advantage for many campers. Many types of cookware exist for outdoor cooking.\n\nWilderness areas can often have a burn-ban, prohibiting people from starting a fire. If a camper were to rely on the campfire method as their only source for cooking heat, they could find themselves in an unlucky situation. Cooking over a campfire can lead to pots and pans darkened with soot. Soot can be extremely difficult to remove and, if left on the pan, can easily rub off onto clothing or the inside of the backpack. Campers have discovered methods of preventing this problem, such as coating the pans with cooking oil, to make the soot easier to remove.\n\nCampers relying on the use of a campfire do not have to carry the extra weight of a cook stove and may rely on a campfire to reduce their pack weight. Campfires provide a great amount of warmth while cook stoves provide none. On cold days, a campfire is often welcome. Leave No Trace discourages the use of a campfire as a source of heat. Campers making a campfire in the same location time after time can deplete the available wood in the area, which impacts the natural habitat of the animals. Campers are also more likely to inadvertently leave food scraps around the fire pit, which could attract animals.\n\nSome camping food is ready to eat and may be warmed using chemical heaters, such as the flameless heaters used in MREs, or a self-contained chemical heater built into the food packaging itself.\n\nSolar cooking provides clean and safe alternative to campfire. Using solar cookers is easy and inexpensive since they do not require fuel to work.\nMost solar cookers also provide minimum required temperature during cloudy days to prepare the food. Despite many advantages that solar cooking provides it is unusable during the nighttime and it will not provide heat and protection against wild animals like campfire does.\n\n\n"}
{"id": "1741240", "url": "https://en.wikipedia.org/wiki?curid=1741240", "title": "Channel 3/4 output", "text": "Channel 3/4 output\n\nA channel 3/4 output was a common output selection for consumer audiovisual devices sold in North America that were intended to be connected to a TV using a radio frequency (RF) signal. This channel option was provided because it was rare to have broadcast channels 3 and 4 used in the same market, or even just channel 3 itself. The choice allowed the user to select the unused channel in their area so that the connected device would be able to provide video and audio on an RF feed to the television without excessive interference from a broadcast signal.\n\nRF modulation was common on equipment deployed in North America. Other countries had the RF output for video equipment on different groups of frequencies. For example, equipment sold in Europe, South Africa and Hong Kong used UHF channels 30–39 for this purpose. Equipment sold in Japan used channel 1 or 2 (Channel 13–16 is for cable converters). With other channels being used for RF modulation function in other regions, channel 3/4 output is a misnomer for those regions.\n\nIn Australia, some manufacturers used the channel 3/4 output, while other manufacturers would use the UHF channels 30–36 for this purpose. It become common for VCRs sold in that market since the mid-1990s to use this output specification as well. The other common output frequency set in Australia was channel 0 and 1 on the VHF band throughout the 1980s. This was because most televisions that were deployed on the Australian market before model-year 1980 could not receive the UHF band.For years, the RF output for video recorders marketed for Australia used channel 0 or 1 on VHF bandwidth and these two channels were not usual on TV sets marketed for Asia/ Middle East,except for Sony Trinitron KV-2062E ( Asia model )which had the capability of receiving any frequency below channel E2. The S-VHS video recorder,HR-S6800EA made by JVC in 1992,was a model made for Australia and in terms of RF output it used VHF channel 0 or 1. In New Zealand,video recorders had RF output on either VHF channel 2 or 3 ( PAL B ).\n\nSince 1982, VHS video recorders and VHS players ( 1984 onwards ) marketed typically for South and South-East Asia, featured RF output on PAL system, standard B, using VHF channel 3 or 4. Some models of course, featured output on PAL G using UHF channels 36 to 38.Usually these models were not recommended for use in Australia. VHS recorders made for Asia/ Middle East normally featured RF output on UHF channel E36 or E37 or even E38,and some decks offered selection between PAL and SECAM systems.\n\nIt also is common to have this type of RF output on video cassette recorders (VCRs), early DVD players and video game consoles.\n\nThe VCR's ubiquity was responsible for making consumers familiar with RF modulation, which could explain its lasting popularity and use in all-digital media like DVD and video game consoles. A better explanation might be its familiarity and ease of use, along with legacy televisions, which could not readily support newer technologies like composite or S-video.\n\nDVD's relatively later entrance to the marketplace means that most players after the early generations do not natively support RF modulation, instead relying on converters, which could also amplify the signal.\n\nFirst- through early fifth-generation video game consoles commonly used this method to connect to the television, which was used as the audio and video device for the game, with some variations (the Atari 2600, for example, used a Channel 2/3 output switch). In many cases, an RF modulator was used to take the composite output from the game and modulate it before sending the signal to the television. Late fifth- through current eighth-generation video game consoles retain the ability to output through RF modulators, usually through a separately purchased adapter. Recently, some RF demodulators have been marketed on obscure online markets to allow channel 3 inputs from legacy devices to work on RCA composite in.\n"}
{"id": "13676918", "url": "https://en.wikipedia.org/wiki?curid=13676918", "title": "Chemical looping combustion", "text": "Chemical looping combustion\n\nChemical looping combustion (CLC) is a technological process typically employing a dual fluidized bed system. The CLC operated with an interconnected moving bed with a fluidized bed system has also been employed as a technology process. In CLC, a metal oxide is employed as a bed material providing the oxygen for combustion in the fuel reactor. The reduced metal is then transferred to the second bed (air reactor) and re-oxidized before being reintroduced back to the fuel reactor completing the loop. Fig 1 shows a simplified diagram of the CLC process. Fig 2 shows an example of a dual fluidized bed circulating reactor system and a moving bed-fluidized bed circulating reactor system.\n\nIsolation of the fuel from air simplifies the number of chemical reactions in combustion. Employing oxygen without nitrogen and the trace gases found in air eliminates the primary source for the formation of nitrogen oxide (), produces a flue gas composed primarily of carbon dioxide and water vapor; other trace pollutants depend on the fuel selected.\n\nChemical looping combustion (CLC) uses two or more reactions to perform the oxidation of hydrocarbon based fuels. In its simplest form, an oxygen carrying species (normally a metal) is first oxidized in air forming an oxide. This oxide is then reduced using a hydrocarbon as reducer in a second reaction. As an example, a iron based system burning pure carbon would involve the two redox reactions:\n\nIf () and () are added together, the reaction set reduces to straight carbon oxidation i.e.:\nCLC was first studied as a way to produce from fossil fuels, using two interconnected fluidized beds. Later it was proposed as a system for increasing power station efficiency. The gain in efficiency is possible due to the enhanced reversibility of the two redox reactions; in traditional single stage combustion, the release of a fuel’s energy occurs in a highly irreversible manner - departing considerably from equilibrium. In CLC, if an appropriate oxygen carrier is chosen, both redox reactions can be made to occur almost reversibly and at relatively low temperatures. Theoretically, this allows a power station using CLC to approach the ideal work output for an internal combustion engine without exposing components to excessive working temperatures.\n\nFig 3 illustrates the energy exchanges in a CLC system graphically, and shows a Sankey diagram of the energy fluxes occurring in a reversible CLC based engine. Studying Fig 1, a heat engine is arranged to receive heat at high temperature from the exothermic oxidation reaction. After converting part of this energy to work, the heat engine rejects the remaining energy as heat. Almost all of this heat rejection can be absorbed by the endothermic reduction reaction occurring in the reducer. This arrangement requires the redox reactions to be exothermic and endothermic respectively, but this is normally the case for most metals. Some additional heat exchange with the environment is required to satisfy the second law; theoretically, for a reversible process, the heat exchange is related to the standard state entropy change, ΔS, of the primary hydrocarbon oxidation reaction as follows:\n\nHowever, for most hydrocarbons, ΔS is a small value and, as a result, an engine of high overall efficiency is theoretically possible.\n\nAlthough proposed as a means of increasing efficiency, in recent years, interest has been shown in CLC as a carbon capture technique. Carbon capture is facilitated by CLC because the two redox reactions generate two intrinsically separated flue gas streams: a stream from the air reactor, consisting of atmospheric and residual , but sensibly free of ; and a stream from the fuel reactor predominately containing and with very little diluent nitrogen. The air reactor flue gas can be discharged to the atmosphere causing minimal pollution. The reducer exit gas contains almost all of the generated by the system and CLC therefore can be said to exhibit 'inherent carbon capture', as water vapor can easily be removed from the second flue gas via condensation, leading to a stream of almost pure . This gives CLC clear benefits when compared with competing carbon capture technologies, as the latter generally involve a significant energy penalty associated with either post combustion scrubbing systems or the work input required for air separation plants. This has led to CLC being proposed as an energy efficient carbon capture technology, able to capture nearly all of the CO, for example, from a Coal Direct Chemical Looping (CDCL) plant. A continuous 200-hour demonstration results of a 25 kW CDCL sub-pilot unit indicated nearly 100% coal conversion to CO with no carbon carryover to the air reactor.\n\nFirst operation of chemical-looping combustion with gaseous fuels was demonstrated in 2003, and later with solid fuels in 2006. Total operational experience in 34 pilots of 0.3 to 3 MW is more than 9000 h. Oxygen carrier materials used in operation include monometallic oxides of nickel, copper, manganese and iron, as well as various combined oxides including manganese oxides.combined with calcium, iron and silica. Also natural ores have been in use, especially for solid fuels, including iron ores, manganese ores and ilmenite.\n\nA detailed technology assessment of chemical-looping combustion of solid fuel, i.e. coal, for a 1000 MW power plant shows that the added CLC reactor costs as compared to a normal circulating fluidized bed boiler are small, because of the similarities of the technologies. Major costs are instead CO compression, needed in all CO capture technologies, and oxygen production. Molecular oxygen production may also be needed in certain CLC configuration for polishing the product gas from the fuel reactor. In all the added costs were estimated to 20 €/tonne of CO whereas the energy penalty was 4%.\n\nA variant of CLC is Chemical-Looping Combustion with Oxygen Uncoupling (CLOU) where an oxygen carrier is used that releases gas-phase oxygen in the fuel reactor, e.g. CuO/O. This is helpful for achieving high gas conversion, and especially when using solid fuels, where slow steam gasification of char can be avoided. CLOU operation with solid fuels shows high performance\n\nChemical Looping can also be used to produce hydrogen in Chemical-Looping Reforming (CLR) processes. In one configuration of the CLR process, hydrogen is produced from coal and/or natural gas using a moving bed fuel reactor integrated with a steam reactor and a fluidized bed air reactor. This configuration of CLR can produce greater than 99% purity H without the need for CO separation.\n\nComprehensive overviews of the field are given in recent reviews on chemical looping technologies.\n\nIn summary CLC can achieve both an increase in power station efficiency simultaneously with low energy penalty carbon capture. Challenges with CLC include operation of dual fluidized bed (maintaining carrier fluidization while avoiding crushing and attrition), and maintaining carrier stability over many cycles.\n\n\n"}
{"id": "2314900", "url": "https://en.wikipedia.org/wiki?curid=2314900", "title": "Constellation 3D", "text": "Constellation 3D\n\nConstellation 3D (C3D or CDDD) was a company developing a new optical medium, the Fluorescent Multilayer Disc and Card (FMD/C). \n\nThe company was shut down following a failure to receive a committed investment by a Swiss based investor (see Press Releases of November 20, 2001, January 3, 2002, February 19, 2002 and April 19, 2002) and the unavailability of other financial sources. The company stopped activities and later on filed for bankruptcy. A company called D Data Inc. was formed which acquired the entire patent portfolio of Constellation 3D in 2003, and reintroduced the technology under the new name of Digital Multilayer Disk (DMD).\n\nAccording to Dr. Ingolf Sander, General Manager of Products until 2001, the reason for the company shutting down \"could have stemmed from their inability to overcome stability problems\" in the fluorescent materials used to make FMDs. This claim stands in sharp contrast with both the decision of the Board of Directors to stop activities due to the lack of financial resources, and the completion by D-Data of working product versions based on the same materials.\n\n"}
{"id": "43536245", "url": "https://en.wikipedia.org/wiki?curid=43536245", "title": "Coull (Technology Company)", "text": "Coull (Technology Company)\n\nCoull is a technology company that specializes in online video advertising. Its technology platform analyzes video content and viewer engagement to deliver targeted video advertising to digital audiences.\n\nCoull's technology is said to allow the publishers and advertisers tag individual products within videos and making it possible for the customers(viewers) to click through and buy products right from the video.\n\nThe company was founded in Bristol, UK, in 2008, with additional teams in London and San Francisco\n"}
{"id": "18584426", "url": "https://en.wikipedia.org/wiki?curid=18584426", "title": "Data model (GIS)", "text": "Data model (GIS)\n\nA data model in geographic information systems is a mathematical construct for representing geographic objects or surfaces as data. For example, the vector data model represents geography as collections of points, lines, and polygons; the raster data model represent geography as cell matrices that store numeric values; and the TIN data model represents geography as sets of contiguous, nonoverlapping triangles.\n\nThere are two approaches for representing three-dimensional map information, and for managing it in the data model. \nVector-based stack-unit maps depict the vertical succession of geologic units to a specified depth (here, the base of the block diagram). This mapping approach characterizes the vertical variations of physical properties in each 3-D map unit. In this example, an alluvial deposit (unit “a”) overlies glacial till (unit “t”), and the stack-unit labeled “a/t” indicates that relationship, whereas the unit “t” indicates that glacial till extends down to the specified depth. In a manner similar to that shown in figure 11, the stack-unit’s occurrence (the map unit’s outcrop), geometry (the map unit’s boundaries), and descriptors (the physical properties of the geologic units included in the stack-unit) are managed as they are for a typical 2-D geologic map.\n\nRaster-based stacked surfaces depict the surface of each buried geologic unit, and can accommodate data on lateral variations of physical properties. In this example from Soller and others (1999), the upper surface of each buried geologic unit was represented in raster format as an ArcInfo Grid file. The middle grid is the uppermost surface of an economically important aquifer, the Mahomet Sand, which fills a pre- and inter-glacial valley carved into the bedrock surface. Each geologic unit in raster format can be managed in the data model, in a manner not dissimilar from that shown for the stack-unit map. The Mahomet Sand is continuous in this area, and represents one occurrence of this unit in the data model. Each raster, or pixel, on the Mahomet Sand surface has a set of map coordinates that are recorded in a GIS (in the data model bin that is labeled “Pixel coordinates”, which is the raster corollary of the “Geometry” bin for vector map data). Each pixel can have a unique set of descriptive information, such as surface elevation, unit thickness, lithology, transmissivity, etc.).\n\n\n"}
{"id": "13518387", "url": "https://en.wikipedia.org/wiki?curid=13518387", "title": "E-agriculture", "text": "E-agriculture\n\nE-agriculture (sometimes written eagriculture or referred to as ICT in agriculture) is a relatively recent term in the field of agriculture and rural development practices. Consistency in the use of this term began to materialize with the dissemination of results from a global survey carried out by the United Nations (UN). This survey conducted in late 2006 by the Food and Agriculture Organization of the United Nations (FAO) found that half of those who replied identified \"e agriculture\" with information dissemination, access and exchange, communication and participation processes improvements around rural development. In contrast, less than a third highlighted the importance of technical hardware and technological tools.\n\nE-agriculture, therefore, describes an emerging field focused on the enhancement of agricultural and rural development through improved information and communication processes. More specifically, e-agriculture involves the conceptualization, design, development, evaluation and application of innovative ways to use information and communication technologies (ICTs) in the rural domain, with a primary focus on agriculture.\n\nIn 2008, the United Nations referred to e-agriculture as \"an emerging field\", with the expectation that its scope would change and evolve as our understanding of the area grows.\n\nMany ICT in agriculture or e-agriculture interventions have been developed and tested around the world, with varied degrees of success, to help agriculturists improve their livelihoods through increased agricultural productivity and incomes, and reduction in risks. Some useful resources for learning about e-agriculture in practice are the World Bank’s e-sourcebook ICT in agriculture – connecting smallholder farmers to knowledge, networks and institutions (2011), ICT uses for inclusive value chains (2013), ICT uses for inclusive value chains (2013) and Success stories on information and communication technologies for agriculture and rural development have documented many cases of use of ICT in agriculture.\n\nThe FAO-ITU E-agriculture Strategy Guide provides a framework to holistically address the ICT opportunities and challenges for the agricultural sector in a more efficient manner while generating new revenue streams and improve the livelihoods of the rural community as well as ensure the goals of the national agriculture master plan are achieved.The existence of e-agriculture strategy and its alignment with other government plans will prevent e-agriculture projects and services from being implemented in isolation.\n\nThe FAO-ITU E-agriculture Strategy Guide was developed by the Food and Agriculture Organization (FAO) and the International Telecommunication Union (ITU) with support from partners including the Technical Centre for Agricultural and Rural Cooperation (CTA) as a framework for countries in developing their national e-agriculture strategy/masterplan.\n\nSome of the countries who are using the FAO-ITU E-agriculture Strategy Guide to develop their national e-agriculture strategy are Bhutan, Sri Lanka, Papua New Guinea, Philippines, Fiji and Vanuatu. The guide provides a framework to engage a broader stakeholders in the development of national e-agriculture strategy.\n\nIn August 2003, the Overseas Development Institute (ODI), the UK Department for International Development (DFID) and the United Nations Food and Agricultural Organization (FAO) joined together in a collaborative research project to look at bringing together livelihoods thinking with concepts from information and communication for development, in order to improve understanding of the role and importance of information and communication in support of rural livelihoods.\n\nThe policy recommendations included:\n\nThe importance of ICT is also recognized in the 8th Millennium Development Goal, with the target to \"...make available the benefits of new technologies, especially information and communications technologies (ICTs)\" to the fight against poverty.\n\nE-agriculture is one of the action lines identified in the declaration and plan of action (2003) of the World Summit on the Information Society (WSIS). The \"Tunis Agenda for the Information Society\", published on 18 November 2005, emphasizes the leading facilitating roles that UN agencies need to play in the implementation of the Geneva Plan of Action.\n\nFAO hosted the first e-agriculture workshop in June 2006, bringing together representatives of leading development organizations involved in agriculture. The meeting served to initiate development of an effective process to engage as wide a range of stakeholders involved in e-agriculture, and resulted in the formation of the e-Agriculture Community, a community of practice. The e-Agriculture Community's Founding Partners include: Consultative Group on International Agricultural Research (CGIAR); Technical Centre for Agriculture and Rural Development (CTA); FAO; Global Alliance for Information and Communication Technologies and Development (GAID); Global Forum on Agricultural Research (GFAR); Global Knowledge Partnership (GKP); Gesellschaft fur Technische Zusammenarbeit (now called Deutsche Gesellschaft für Internationale Zusammenarbeit, GIZ); International Association of Agricultural Information Specialists (IAALD); Inter-American Institute for Cooperation on Agriculture (IICA); International Fund for Agricultural Development (IFAD); International Centre for Communication for Development (IICD); United States National Agricultural Library (NAL); United Nations Department of Economic and Social Affairs (UNDESA); the World Bank.\n\n"}
{"id": "51669100", "url": "https://en.wikipedia.org/wiki?curid=51669100", "title": "Electric-pump-fed engine", "text": "Electric-pump-fed engine\n\nThe electric-pump-fed engine is a bipropellant rocket engine in which the fuel pumps are electrically powered, and so all of the input propellant is directly burned in the main combustion chamber, and none is diverted to drive the pumps. This differs from traditional rocket engine designs, in which the pumps are driven by a portion of the input propellants.\n\nAn electric cycle engine uses electric pumps to pressurize the propellants from a low-pressure fuel tank to high-pressure combustion chamber levels, generally from to . The pumps are powered by an electric motor, with electricity from a battery bank.\n\nAs of January 2018, the only rocket engines to use electric propellant pump systems are the Rutherford engine, nine of which power the Electron rocket, and the electric pump-fed rocket engine used in sounding rockets developed by Ventions. On 21 January 2018, Electron was the first electric pump-fed rocket to reach orbit.\n\nIn comparison to turbo-pumped rocket cycles such as staged combustion and gas generator, an electric cycle engine has potentially worse performance due to the added mass of batteries, but may have lower development and manufacturing costs due its mechanical simplicity, its lack of high temperature turbomachinery, and its easier controllability. Conversely, an electric cycle engine may have significantly better performance than pressure-fed rocket engines and solid propellant rocket motors.\n\n"}
{"id": "3069854", "url": "https://en.wikipedia.org/wiki?curid=3069854", "title": "Electrolyte–insulator–semiconductor sensor", "text": "Electrolyte–insulator–semiconductor sensor\n\nAn Electrolyte–insulator–semiconductor (EIS) sensor is a sensor that is made of these three components:\n\n\nThe EIS sensor can be used in combination with other structures, for example to construct a light-addressable potentiometric sensor (LAPS).\n"}
{"id": "9372609", "url": "https://en.wikipedia.org/wiki?curid=9372609", "title": "Environmental testing", "text": "Environmental testing\n\nEnvironmental testing is the measurement of the performance of equipment under specified environmental conditions, such as:\n\n\nSuch tests are most commonly performed on equipment used in military, maritime, aeronautical and space applications. See Environmental test chambers for more information about environmental testing equipment.\n\nEnvironmental test standards include\n\n"}
{"id": "2978833", "url": "https://en.wikipedia.org/wiki?curid=2978833", "title": "Facial toning", "text": "Facial toning\n\nFacial toning, or facial exercise is a type of cosmetic procedure or physical therapy tool which promises to alter facial contours by means of increasing muscle tone, and facial volume by promoting muscular hypertrophy, and preventing muscle loss due to aging or facial paralysis. Facial toning and exercise is therefore in part a technique to achieve facial rejuvenation by reducing wrinkles, sagging and expression marks on the face and skin. As a physical therapy, facial toning is used for victims of stroke and forms of facial paralysis such as Bell’s palsy. Facial toning achieves this by performing facial muscle exercising. There are two types of facial toning exercises: active and passive face exercises.\n\nFace exercises involves repeated voluntary contractions of certain facial muscle groups. The effectiveness of these facial toning techniques in improving appearance is scientifically unproven.\n\nPassive exercising by direct skeletal muscle electrostimulation. In this, flat metal electrodes with a conductive gel are affixed to certain points in the face and electrostimulation causes facial muscle contractions.\n"}
{"id": "3428197", "url": "https://en.wikipedia.org/wiki?curid=3428197", "title": "Flammability limit", "text": "Flammability limit\n\nMixtures of dispersed combustible materials (such as gaseous or vaporised fuels, and some dusts) and air will burn only if the fuel concentration lies within well-defined lower and upper bounds determined experimentally, referred to as flammability limits or explosive limits. Combustion can range in violence from deflagration through detonation.\n\nLimits vary with temperature and pressure, but are normally expressed in terms of volume percentage at 25 °C and atmospheric pressure. These limits are relevant both to producing and optimising explosion or combustion, as in an engine, or to preventing it, as in uncontrolled explosions of build-ups of combustible gas or dust. Attaining the best combustible or explosive mixture of a fuel and air (the stoichiometric proportion) is important in internal combustion engines such as gasoline or diesel engines.\n\nThe standard reference work is that by Zabetakis using an apparatus developed by the United States Bureau of Mines.\n\nCombustion can vary in degree of violence. A deflagration is a propagation of a combustion zone at a velocity less than the speed of sound in the unreacted medium. A detonation is a propagation of a combustion zone at a velocity greater than the speed of sound in the unreacted medium. An explosion is the bursting or rupture of an enclosure or container due to the development of internal pressure from a deflagration or detonation as defined in NFPA 69.\n\nLower flammability limit (LFL): The lowest concentration (percentage) of a gas or a vapor in air capable of producing a flash of fire in presence of an ignition source (arc, flame, heat). The term is considered by many safety professionals to be the same as the lower explosive level (LEL). At a concentration in air lower than the LFL, gas mixtures are \"too lean\" to burn. \nMethane gas has an LFL of 5.0%. If the atmosphere has less than 5.0% methane, an explosion cannot occur even if a source of ignition is present. From the health and safety perspective, the LEL concentration is considered to be Immediately Dangerous to Life or Health (IDLH), where a more stringent exposure limit does not exist for the flammable gas.\n\nPercentage reading on combustible air monitors should not be confused with the LFL concentrations. Explosimeters designed and calibrated to a specific gas may show the relative concentration of the atmosphere to the LFL—the LFL being 100%. A 5% displayed LFL reading for methane, for example, would be equivalent to 5% multiplied by 5.0%, or approximately 0.25% methane by volume at 20 degrees C. Control of the explosion hazard is usually achieved by sufficient natural or mechanical ventilation, to limit the concentration of flammable gases or vapors to a maximum level of 25% of their \"lower explosive or flammable limit\".\n\nUpper flammability limit (UFL): Highest concentration (percentage) of a gas or a vapor in air capable of producing a flash of fire in presence of an ignition source (arc, flame, heat). Concentrations higher than UFL or UEL are \"too rich\" to burn. Operating above the UFL is usually avoided for safety because air leaking in can bring the mixture into combustibility range.\n\nFlammability limits of mixtures of several combustible gases can be calculated using Le Chatelier's mixing rule for combustible volume fractions formula_1:\n\nand similar for UFL.\n\nTemperature, pressure, and the concentration of the oxidizer also influences flammability limits. Higher temperature or pressure, as well as higher concentration of the oxidizer (primarily oxygen in air), results in lower LFL and higher UFL, hence the gas mixture will be easier to explode. The effect of pressure is very small at pressures below 10 millibar and difficult to predict, since it has only been studied in internal combustion engines with a turbocharger.\n\nUsually atmospheric air supplies the oxygen for combustion, and limits assume the normal concentration of oxygen in air. Oxygen-enriched atmospheres enhance combustion, lowering the LFL and increasing the UFL, and vice versa; an atmosphere devoid of an oxidizer is neither flammable nor explosive for any fuel concentration. Significantly increasing the fraction of inert gases in an air mixture, at the expense of oxygen, increases the LFL and decreases the UFL.\n\nControlling gas and vapor concentrations outside the flammable limits is a major consideration in occupational safety and health. Methods used to control the concentration of a potentially explosive gas or vapor include use of sweep gas, an unreactive gas such as nitrogen or argon to dilute the explosive gas before coming in contact with air. Use of scrubbers or adsorption resins to remove explosive gases before release are also common. Gases can also be maintained safely at concentrations above the UEL, although a breach in the storage container can lead to explosive conditions or intense fires.\n\nDusts also have upper and lower explosion limits, though the upper limits are hard to measure and of little practical importance. Lower flammability limits for many organic materials are in the range of 10–50 g/m³, which is much higher than the limits set for health reasons, as is the case for the LEL of many gases and vapours. Dust clouds of this concentration are hard to see through for more than a short distance, and normally only exist inside process equipment.\n\nFlammability limits also depend on the particle size of the dust involved, and are not intrinsic properties of the material. In addition, a concentration above the LEL can be created suddenly from settled dust accumulations, so management by routine monitoring, as is done with gases and vapours, is of no value. The preferred method of managing combustible dust is by preventing accumulations of settled dust through process enclosure, ventilation, and surface cleaning. However, lower flammability limits may be relevant to plant design.\n\nSituations caused by evaporation of flammable liquids into the air-filled void volume of a container may be limited by flexible container volume or by using an immicsible fluid to fill the void volume. Hydraulic tankers use displacement of water when filling a tank with petroleum.\n\nThe flammable/explosive limits of some gases and vapors are given below. Concentrations are given in percent by volume of air.\n\n\n"}
{"id": "24909759", "url": "https://en.wikipedia.org/wiki?curid=24909759", "title": "Friedrich Grillo", "text": "Friedrich Grillo\n\nFriedrich Grillo (20 December 1825, Essen – 16 April 1888, Düsseldorf) was a prominent industrialist in the Ruhr area of Germany, particularly in Essen and Gelsenkirchen.\n\nBorn the son of an Essen merchant, into a Protestant family of Italian origin (his ancestors came from Valtellina, from which they fled due to religious persecutions), he took over his father's enterprise and expanded the range of trade, becoming an influential businessman. Grillo was a board member and founder of several mining companies.\n\nFriedrich Grillo and, after his death, his widow Wilhelmine Grillo donated the property and two thirds of the construction costs of the Grillo-Theater which opened in Essen in 1892.\n"}
{"id": "5907861", "url": "https://en.wikipedia.org/wiki?curid=5907861", "title": "Future Electronics", "text": "Future Electronics\n\nFuture Electronics Inc. is a distributor of electronic and electro-mechanical components headquartered in Pointe-Claire, Quebec. Founded in 1968 by Canadian billionaire Robert Miller, the company is one of Quebec's largest privately owned companies and is currently the fourth largest electronics distributor in the world. It operates in 169 locations in 44 countries in the Americas, Europe, Asia and Africa.\n\nIn 2014 its revenues were $5 billion. In September 2018, Future Electronics sponsored NXP's free Technology Day in Boston.\n\nIn 1999, the offices of Future Electronics in Montreal were raided by the Royal Canadian Mounted Police following an investigation into allegations that the company had overcharged some customers. Future denied all charges and the founder and CEO of the company Robert Miller called the raid an \"unfounded assault on our integrity\". A year after the raid, in December 2000, the Quebec Appeal Court ruled that the RCMP had no basis for issuing the search warrant executed at Future’s offices. The Quebec Appeal Court ordered the RCMP to return all the documents which were seized from the company.\nIn April 2002, the Office of the US Attorney in Dallas,Texas announced that the investigation had been closed and that no charges would be filed.\n\nFuture’s North American Distribution Center is located in the Memphis Area.\n\n"}
{"id": "44098241", "url": "https://en.wikipedia.org/wiki?curid=44098241", "title": "Future Soldier 2030 Initiative", "text": "Future Soldier 2030 Initiative\n\nFuture Soldier 2030 Initiative was a US Army program that was launched in 2009 with the mission to research and develop future soldiers' equipments, weapons and body armors. The program investigates various futuristic technologies, including mind boosting drugs, powered exoskeletons and artificially intelligent assistants.\n\nIn late 2015, the Future soldier initiative was largely cancelled and shelved by its backers.\n\n"}
{"id": "58664", "url": "https://en.wikipedia.org/wiki?curid=58664", "title": "Gas turbine", "text": "Gas turbine\n\nA gas turbine, also called a combustion turbine, is a type of continuous combustion, internal combustion engine. There are three main components:\nA fourth component is often used to increase efficiency (turboprop, turbofan), to convert power into mechanical or electric form (turboshaft, electric generator), or to achieve greater power to mass/volume ratio (afterburner).\n\nThe basic operation of the gas turbine is a Brayton cycle with air as the working fluid. Fresh atmospheric air flows through the compressor that brings it to higher pressure. Energy is then added by spraying fuel into the air and igniting it so the combustion generates a high-temperature flow. This high-temperature high-pressure gas enters a turbine, where it expands down to the exhaust pressure, producing a shaft work output in the process. The turbine shaft work is used to drive the compressor; the energy that is not used for shaft work comes out in the exhaust gases that produce thrust. The purpose of the gas turbine determines the design so that the most desirable split of energy between the thrust and the shaft work is achieved. The fourth step of the Brayton cycle (cooling of the working fluid) is omitted, as gas turbines are open systems that do not use the same air again.\n\nGas turbines are used to power aircraft, trains, ships, electrical generators, pumps, gas compressors, and tanks.\n\n\nIn an ideal gas turbine, gases undergo four thermodynamic processes: an isentropic compression, an isobaric (constant pressure) combustion, an isentropic expansion and heat rejection. Together, these make up the Brayton cycle.\n\nIn a real gas turbine, mechanical energy is changed irreversibly (due to internal friction and turbulence) into pressure and thermal energy when the gas is compressed (in either a centrifugal or axial compressor). Heat is added in the combustion chamber and the specific volume of the gas increases, accompanied by a slight loss in pressure. During expansion through the stator and rotor passages in the turbine, irreversible energy transformation once again occurs. Fresh air is taken in, in place of the heat rejection.\n\nIf the engine has a power turbine added to drive an industrial generator or a helicopter rotor, the exit pressure will be as close to the entry pressure as possible with only enough energy left to overcome the pressure losses in the exhaust ducting and expel the exhaust. For a turboprop engine there will be a particular balance between propeller power and jet thrust which gives the most economical operation. In a jet engine only enough pressure and energy is extracted from the flow to drive the compressor and other components. The remaining high-pressure gases are accelerated to provide a jet to propel an aircraft.\n\nThe smaller the engine, the higher the rotation rate of the shaft(s) must be to attain the required blade tip speed. Blade-tip speed determines the maximum pressure ratios that can be obtained by the turbine and the compressor. This, in turn, limits the maximum power and efficiency that can be obtained by the engine. In order for tip speed to remain constant, if the diameter of a rotor is reduced by half, the rotational speed must double. For example, large jet engines operate around 10,000 rpm, while micro turbines spin as fast as 500,000 rpm.\n\nMechanically, gas turbines \"can\" be considerably less complex than internal combustion piston engines. Simple turbines might have one main moving part, the compressor/shaft/turbine rotor assembly (see image above), with other moving parts in the fuel system. This, in turn, can translate into price. For instance, costing for materials, the Jumo 004 proved cheaper than the Junkers 213 piston engine, which was , and needed only 375 hours of lower-skill labor to complete (including manufacture, assembly, and shipping), compared to 1,400 for the BMW 801. This, however, also translate into poor efficiency and reliability. More advanced gas turbines (such as those found in modern jet engines or combined cycle power plants) may have 2 or 3 shafts (spools), hundreds of compressor and turbine blades, movable stator blades, and extensive external tubing for fuel, oil and air systems; they use temperature resistant alloys, and are made with tight specifications requiring precision manufacture. All this often makes the construction of a simple gas turbine more complicated than a piston engine.\n\nMoreover, to reach optimum performance in modern gas turbine power plants the gas needs to be prepared to exact fuel specifications. Fuel gas conditioning systems treat the natural gas to reach the exact fuel specification prior to entering the turbine in terms of pressure, temperature, gas composition, and the related wobbe-index.\n\nThrust bearings and journal bearings are a critical part of a design. They are hydrodynamic oil bearings or oil-cooled rolling-element bearings. Foil bearings are used in some small machines such as micro turbines and also have strong potential for use in small gas turbines/auxiliary power units\n\nA major challenge facing turbine design is reducing the creep that is induced by the high temperatures. Because of the stresses of operation, turbine materials become damaged through these mechanisms. As temperatures are increased in an effort to improve turbine efficiency, creep becomes more significant. To limit creep, thermal coatings and superalloys with solid-solution strengthening and grain boundary strengthening are used in blade designs. Protective coatings are used to reduce the thermal damage and to limit oxidation. These coatings are often stabilized zirconium dioxide-based ceramics. Using a thermal protective coating limits the temperature exposure of the nickel superalloy. This reduces the creep mechanisms experienced in the blade. Oxidation coatings limit efficiency losses caused by a buildup on the outside of the blades, which is especially important in the high-temperature environment. The nickel-based blades are alloyed with aluminum and titanium to improve strength and creep resistance. The microstructure of these alloys is composed of different regions of the composition. A uniform dispersion of the gamma-prime phase – a combination of nickel, aluminum, and titanium – promotes the strength and creep resistance of the blade due to the microstructure. Refractory elements such as rhenium and ruthenium can be added to the alloy to improve creep strength. The addition of these elements reduces the diffusion of the gamma prime phase, thus preserving the fatigue resistance, strength, and creep resistance.\n\nAirbreathing jet engines are gas turbines optimized to produce thrust from the exhaust gases, or from ducted fans connected to the gas turbines. Jet engines that produce thrust from the direct impulse of exhaust gases are often called turbojets, whereas those that generate thrust with the addition of a ducted fan are often called turbofans or (rarely) fan-jets.\n\nGas turbines are also used in many liquid fuel rockets, where gas turbines are used to power a turbopump to permit the use of lightweight, low-pressure tanks, reducing the empty weight of the rocket.\n\nA turboprop engine is a turbine engine that drives an aircraft propeller using a reduction gear. Turboprop engines are used on small aircraft such as the general-aviation Cessna 208 Caravan and Embraer EMB 312 Tucano military trainer, medium-sized commuter aircraft such as the Bombardier Dash 8 and large aircraft such as the Airbus A400M transport and the 60 year-old Tupolev Tu-95 strategic bomber.\n\nAeroderivatives are also used in electrical power generation due to their ability to be shut down and handle load changes more quickly than industrial machines. They are also used in the marine industry to reduce weight. The General Electric LM2500, General Electric LM6000, Rolls-Royce RB211 and Rolls-Royce Avon are common models of this type of machine.\n\nIncreasing numbers of gas turbines are being used or even constructed by amateurs.\n\nIn its most straightforward form, these are commercial turbines acquired through military surplus or scrapyard sales, then operated for display as part of the hobby of engine collecting. In its most extreme form, amateurs have even rebuilt engines beyond professional repair and then used them to compete for the Land Speed Record.\n\nThe simplest form of self-constructed gas turbine employs an automotive turbocharger as the core component. A combustion chamber is fabricated and plumbed between the compressor and turbine sections.\n\nMore sophisticated turbojets are also built, where their thrust and light weight are sufficient to power large model aircraft. The Schreckling design constructs the entire engine from raw materials, including the fabrication of a centrifugal compressor wheel from plywood, epoxy and wrapped carbon fibre strands.\n\nSeveral small companies now manufacture small turbines and parts for the amateur. Most turbojet-powered model aircraft are now using these commercial and semi-commercial microturbines, rather than a Schreckling-like home-build.\n\nAPUs are small gas turbines designed to supply auxiliary power to larger, mobile, machines such as an aircraft. They supply:\n\nIndustrial gas turbines differ from aeronautical designs in that the frames, bearings, and blading are of heavier construction. They are also much more closely integrated with the devices they power— often an electric generator—and the secondary-energy equipment that is used to recover residual energy (largely heat).\n\nThey range in size from portable mobile plants to large, complex systems weighing more than a hundred tonnes housed in purpose-built buildings. When the gas turbine is used solely for shaft power, its thermal efficiency is about 30%. However, it may be cheaper to buy electricity than to generate it. Therefore, many engines are used in CHP (Combined Heat and Power) configurations that can be small enough to be integrated into portable container configurations.\n\nGas turbines can be particularly efficient when waste heat from the turbine is recovered by a heat recovery steam generator to power a conventional steam turbine in a combined cycle configuration. The 605 MW General Electric 9HA achieved a 62.22% efficiency rate with temperatures as high as .\nFor 2018, GE offers its 826 MW HA at over 64% efficiency in combined cycle due to advances in additive manufacturing and combustion breakthroughs, up from 63.7% in 2017 orders and on track to achieve 65% by the early 2020s.\n\nAeroderivative gas turbines can also be used in combined cycles, leading to a higher efficiency, but it will not be as high as a specifically designed industrial gas turbine. They can also be run in a cogeneration configuration: the exhaust is used for space or water heating, or drives an absorption chiller for cooling the inlet air and increase the power output, technology known as Turbine Inlet Air Cooling.\n\nAnother significant advantage is their ability to be turned on and off within minutes, supplying power during peak, or unscheduled, demand. Since single cycle (gas turbine only) power plants are less efficient than combined cycle plants, they are usually used as peaking power plants, which operate anywhere from several hours per day to a few dozen hours per year—depending on the electricity demand and the generating capacity of the region. In areas with a shortage of base-load and load following power plant capacity or with low fuel costs, a gas turbine powerplant may regularly operate most hours of the day. A large single-cycle gas turbine typically produces 100 to 400 megawatts of electric power and has 35–40% thermal efficiency.\n\nIndustrial gas turbines that are used solely for mechanical drive or used in collaboration with a recovery steam generator differ from power generating sets in that they are often smaller and feature a dual shaft design as opposed to a single shaft. The power range varies from 1 megawatt up to 50 megawatts. These engines are connected directly or via a gearbox to either a pump or compressor assembly. The majority of installations are used within the oil and gas industries. Mechanical drive applications increase efficiency by around 2%.\n\nOil and Gas platforms require these engines to drive compressors to inject gas into the wells to force oil up via another bore, or to compress the gas for transportation. They're also often used to provide power for the platform. These platforms don't need to use the engine in collaboration with a CHP system due to getting the gas at an extremely reduced cost (often free from burn off gas). The same companies use pump sets to drive the fluids to land and across pipelines in various intervals.\n\nOne modern development seeks to improve efficiency in another way, by separating the compressor and the turbine with a compressed air store. In a conventional turbine, up to half the generated power is used driving the compressor. In a compressed air energy storage configuration, power, perhaps from a wind farm or bought on the open market at a time of low demand and low price, is used to drive the compressor, and the compressed air released to operate the turbine when required.\n\nTurboshaft engines are often used to drive compression trains (for example in gas pumping stations or natural gas liquefaction plants) and are used to power almost all modern helicopters. The primary shaft bears the compressor and the high-speed turbine (often referred to as the \"Gas Generator\"), while a second shaft bears the low-speed turbine (a \"power turbine\" or \"free-wheeling turbine\" on helicopters, especially, because the gas generator turbine spins separately from the power turbine). In effect the separation of the gas generator, by a fluid coupling (the hot energy-rich combustion gases), from the power turbine is analogous to an automotive transmission's fluid coupling. This arrangement is used to increase power-output flexibility with associated highly-reliable control mechanisms.\n\nIn 1963, Jan Mowill initiated the development at Kongsberg Våpenfabrikk in Norway. Various successors have made good progress in the refinement of this mechanism. Owing to a configuration that keeps heat away from certain bearings the durability of the machine is improved while the radial turbine is well matched in speed requirement.\n\nAlso known as miniature gas turbines or micro-jets.\n\nWith this in mind the pioneer of modern Micro-Jets, Kurt Schreckling, produced one of the world's first Micro-Turbines, the FD3/67. This engine can produce up to 22 newtons of thrust, and can be built by most mechanically minded people with basic engineering tools, such as a metal lathe.\n\nEvolved from piston engine turbochargers, aircraft APUs or small jet engines, microturbines are 25 to 500 kilowatt turbines the size of a refrigerator.\nMicroturbines have around 15% efficiencies without a recuperator, 20 to 30% with one and they can reach 85% combined thermal-electrical efficiency in cogeneration.\n\nMost gas turbines are internal combustion engines but it is also possible to manufacture an external combustion gas turbine which is, effectively, a turbine version of a hot air engine.\nThose systems are usually indicated as EFGT (Externally Fired Gas Turbine) or IFGT (Indirectly Fired Gas Turbine).\n\nExternal combustion has been used for the purpose of using pulverized coal or finely ground biomass (such as sawdust) as a fuel. In the indirect system, a heat exchanger is used and only clean air with no combustion products travels through the power turbine. The thermal efficiency is lower in the indirect type of external combustion; however, the turbine blades are not subjected to combustion products and much lower quality (and therefore cheaper) fuels are able to be used.\n\nWhen external combustion is used, it is possible to use exhaust air from the turbine as the primary combustion air. This effectively reduces global heat losses, although heat losses associated with the combustion exhaust remain inevitable.\n\nClosed-cycle gas turbines based on helium or supercritical carbon dioxide also hold promise for use with future high temperature solar and nuclear power generation.\n\nGas turbines are often used on ships, locomotives, helicopters, tanks, and to a lesser extent, on cars, buses, and motorcycles.\n\nA key advantage of jets and turboprops for airplane propulsion - their superior performance at high altitude compared to piston engines, particularly naturally aspirated ones - is irrelevant in most automobile applications. Their power-to-weight advantage, though less critical than for aircraft, is still important.\n\nGas turbines offer a high-powered engine in a very small and light package. However, they are not as responsive and efficient as small piston engines over the wide range of RPMs and powers needed in vehicle applications. In series hybrid vehicles, as the driving electric motors are mechanically detached from the electricity generating engine, the responsiveness, poor performance at low speed and low efficiency at low output problems are much less important. The turbine can be run at optimum speed for its power output, and batteries and ultracapacitors can supply power as needed, with the engine cycled on and off to run it only at high efficiency. The emergence of the continuously variable transmission may also alleviate the responsiveness problem.\n\nTurbines have historically been more expensive to produce than piston engines, though this is partly because piston engines have been mass-produced in huge quantities for decades, while small gas turbine engines are rarities; however, turbines are mass-produced in the closely related form of the turbocharger.\n\nThe turbocharger is basically a compact and simple free shaft radial gas turbine which is driven by the piston engine's exhaust gas. The centripetal turbine wheel drives a centrifugal compressor wheel through a common rotating shaft. This wheel supercharges the engine air intake to a degree that can be controlled by means of a wastegate or by dynamically modifying the turbine housing's geometry (as in a VGT turbocharger).\nIt mainly serves as a power recovery device which converts a great deal of otherwise wasted thermal and kinetic energy into engine boost.\n\nTurbo-compound engines (actually employed on some trucks) are fitted with blow down turbines which are similar in design and appearance to a turbocharger\nexcept for the turbine shaft being mechanically or hydraulically connected to the engine's crankshaft instead of to a centrifugal compressor, thus providing additional power instead of boost.\nWhile the turbocharger is a pressure turbine, a power recovery turbine is a velocity one.\n\nA number of experiments have been conducted with gas turbine powered automobiles, the largest by Chrysler. More recently, there has been some interest in the use of turbine engines for hybrid electric cars. For instance, a consortium led by micro gas turbine company Bladon Jets has secured investment from the Technology Strategy Board to develop an Ultra Lightweight Range Extender (ULRE) for next-generation electric vehicles. The objective of the consortium, which includes luxury car maker Jaguar Land Rover and leading electrical machine company SR Drives, is to produce the world’s first commercially viable - and environmentally friendly - gas turbine generator designed specifically for automotive applications.\n\nThe common turbocharger for gasoline or diesel engines is also a turbine derivative.\n\nThe first serious investigation of using a gas turbine in cars was in 1946 when two engineers, Robert Kafka and Robert Engerstein of Carney Associates, a New York engineering firm, came up with the concept where a unique compact turbine engine design would provide power for a rear wheel drive car. After an article appeared in \"Popular Science\", there was no further work, beyond the paper stage.\n\nIn 1950, designer F.R. Bell and Chief Engineer Maurice Wilks from British car manufacturers Rover unveiled the first car powered with a gas turbine engine. The two-seater JET1 had the engine positioned behind the seats, air intake grilles on either side of the car, and exhaust outlets on the top of the tail. During tests, the car reached top speeds of , at a turbine speed of 50,000 rpm. The car ran on petrol, paraffin (kerosene) or diesel oil, but fuel consumption problems proved insurmountable for a production car. It is on display at the London Science Museum.\n\nA French turbine powered car, the Socema-Gregoire, was displayed at the October 1952 Paris Auto Show. It was designed by the French engineer Jean-Albert Grégoire.\n\nThe first turbine-powered car built in the US was the GM Firebird I which began evaluations in 1953. While photos of the Firebird I may suggest that the jet turbine's thrust propelled the car like an aircraft, the turbine actually drove the rear wheels. The Firebird 1 was never meant as a commercial passenger car and was solely built for testing & evaluation as well as public relation purposes.\nStarting in 1954 with a modified Plymouth, the American car manufacturer Chrysler demonstrated several prototype gas turbine-powered cars from the early 1950s through the early 1980s. Chrysler built fifty Chrysler Turbine Cars in 1963 and conducted the only consumer trial of gas turbine-powered cars. Each of their turbines employed a unique rotating recuperator, referred to as a regenerator that increased efficiency.\n\nIn 1954 FIAT unveiled a concept car with a turbine engine, called Fiat Turbina. This vehicle, looking like an aircraft with wheels, used a unique combination of both jet thrust and the engine driving the wheels. Speeds of were claimed.\n\nThe original General Motors Firebird was a series of concept cars developed for the 1953, 1956 and 1959 Motorama auto shows, powered by gas turbines.\n\nAs a result of the U.S. Clean Air Act Amendments of 1970, research was funded to developing automotive gas turbine technology. Design concepts and vehicles were conducted by Chrysler, General Motors, Ford (in collaboration with AiResearch), and American Motors (in conjunction with Williams Research). Long-term tests were conducted evaluate comparable cost efficiency. Several AMC Hornets were powered by a small Williams regenerative gas turbines weighing and producing at 4450 rpm.\n\nToyota demonstrated several gas turbine powered concept cars, such as the Century gas turbine hybrid in 1975, the Sports 800 Gas Turbine Hybrid in 1979 and the GTV in 1985. No production vehicles were made. The GT24 engine was exhibited in 1977 without a vehicle.\n\nIn the early 1990s, Volvo introduced the Volvo Environmental Concept Car(ECC) which was a gas turbine powered hybrid car.\n\nIn 1993 General Motors introduced the first commercial gas turbine powered hybrid vehicle—as a limited production run of the EV-1 series hybrid. A Williams International 40 kW turbine drove an alternator which powered the battery-electric powertrain. The turbine design included a recuperator. In 2006, GM went into the EcoJet concept car project with Jay Leno.\n\nAt the 2010 Paris Motor Show Jaguar demonstrated its Jaguar C-X75 concept car. This electrically powered supercar has a top speed of and can go from in 3.4 seconds. It uses Lithium-ion batteries to power four electric motors which combine to produce 780 bhp. It will travel on a single charge of the batteries, and uses a pair of Bladon Micro Gas Turbines to re-charge the batteries extending the range to .\n\nThe first race car (in concept only) fitted with a turbine was in 1955 by a US Air Force group as a hobby project with a turbine loaned them by Boeing and a race car owned by Firestone Tire & Rubber company. The first race car fitted with a turbine for the goal of actual racing was by Rover and the BRM Formula One team joined forces to produce the Rover-BRM, a gas turbine powered coupe, which entered the 1963 24 Hours of Le Mans, driven by Graham Hill and Richie Ginther. It averaged and had a top speed of . American Ray Heppenstall joined Howmet Corporation and McKee Engineering together to develop their own gas turbine sports car in 1968, the Howmet TX, which ran several American and European events, including two wins, and also participated in the 1968 24 Hours of Le Mans. The cars used Continental gas turbines, which eventually set six FIA land speed records for turbine-powered cars.\n\nFor open wheel racing, 1967's revolutionary STP-Paxton Turbocar fielded by racing and entrepreneurial legend Andy Granatelli and driven by Parnelli Jones nearly won the Indianapolis 500; the Pratt & Whitney ST6B-62 powered turbine car was almost a lap ahead of the second place car when a gearbox bearing failed just three laps from the finish line. The next year the STP Lotus 56 turbine car won the Indianapolis 500 pole position even though new rules restricted the air intake dramatically. In 1971 Lotus principal Colin Chapman introduced the Lotus 56B F1 car, powered by a Pratt & Whitney STN 6/76 gas turbine. Chapman had a reputation of building radical championship-winning cars, but had to abandon the project because there were too many problems with turbo lag.\n\nThe arrival of the Capstone Microturbine has led to several hybrid bus designs, starting with HEV-1 by AVS of Chattanooga, Tennessee in 1999, and closely followed by Ebus and ISE Research in California, and DesignLine Corporation in New Zealand (and later the United States). AVS turbine hybrids were plagued with reliability and quality control problems, resulting in liquidation of AVS in 2003. The most successful design by Designline is now operated in 5 cities in 6 countries, with over 30 buses in operation worldwide, and order for several hundred being delivered to Baltimore, and New York City.\n\nBrescia Italy is using serial hybrid buses powered by microturbines on routes through the historical sections of the city.\n\nThe MTT Turbine Superbike appeared in 2000 (hence the designation of Y2K Superbike by MTT) and is the first production motorcycle powered by a turbine engine - specifically, a Rolls-Royce Allison model 250 turboshaft engine, producing about 283 kW (380 bhp). Speed-tested to 365 km/h or 227 mph (according to some stories, the testing team ran out of road during the test), it holds the Guinness World Record for most powerful production motorcycle and most expensive production motorcycle, with a price tag of US$185,000.\n\nSeveral locomotive classes have been powered by gas turbines, the most recent incarnation being Bombardier's JetTrain.\n\nThe Third Reich \"Wehrmacht Heer\"'s development division, the Heereswaffenamt (Army Ordnance Board), studied a number of gas turbine engine designs for use in tanks starting in mid-1944. The first gas turbine engine design intended for use in armored fighting vehicle propulsion, the BMW 003-based GT 101, was meant for installation in the Panther tank.\n\nThe second use of a gas turbine in an armored fighting vehicle was in 1954 when a unit, PU2979, specifically developed for tanks by C. A. Parsons & Co., was installed and trialled in a British Conqueror tank. The Stridsvagn 103 was developed in the 1950s and was the first mass-produced main battle tank to use a turbine engine. Since then, gas turbine engines have been used as APUs in some tanks and as main powerplants in Soviet/Russian T-80s and U.S. M1 Abrams tanks, among others. They are lighter and smaller than diesels at the same sustained power output but the models installed to date are less fuel efficient than the equivalent diesel, especially at idle, requiring more fuel to achieve the same combat range. Successive models of M1 have addressed this problem with battery packs or secondary generators to power the tank's systems while stationary, saving fuel by reducing the need to idle the main turbine. T-80s can mount three large external fuel drums to extend their range. Russia has stopped production of the T-80 in favor of the diesel-powered T-90 (based on the T-72), while Ukraine has developed the diesel-powered T-80UD and T-84 with nearly the power of the gas-turbine tank. The French Leclerc MBT's diesel powerplant features the \"Hyperbar\" hybrid supercharging system, where the engine's turbocharger is completely replaced with a small gas turbine which also works as an assisted diesel exhaust turbocharger, enabling engine RPM-independent boost level control and a higher peak boost pressure to be reached (than with ordinary turbochargers). This system allows a smaller displacement and lighter engine to be used as the tank's powerplant and effectively removes turbo lag. This special gas turbine/turbocharger can also work independently from the main engine as an ordinary APU.\n\nA turbine is theoretically more reliable and easier to maintain than a piston engine since it has a simpler construction with fewer moving parts, but in practice, turbine parts experience a higher wear rate due to their higher working speeds. The turbine blades are highly sensitive to dust and fine sand so that in desert operations air filters have to be fitted and changed several times daily. An improperly fitted filter, or a bullet or shell fragment that punctures the filter, can damage the engine. Piston engines (especially if turbocharged) also need well-maintained filters, but they are more resilient if the filter does fail.\n\nLike most modern diesel engines used in tanks, gas turbines are usually multi-fuel engines.\n\nGas turbines are used in many naval vessels, where they are valued for their high power-to-weight ratio and their ships' resulting acceleration and ability to get underway quickly.\n\nThe first gas-turbine-powered naval vessel was the Royal Navy's Motor Gun Boat \"MGB 2009\" (formerly \"MGB 509\") converted in 1947. Metropolitan-Vickers fitted their F2/3 jet engine with a power turbine. The Steam Gun Boat \"Grey Goose\" was converted to Rolls-Royce gas turbines in 1952 and operated as such from 1953. The Bold class Fast Patrol Boats \"Bold Pioneer\" and \"Bold Pathfinder\" built in 1953 were the first ships created specifically for gas turbine propulsion.\n\nThe first large-scale, partially gas-turbine powered ships were the Royal Navy's Type 81 (Tribal class) frigates with combined steam and gas powerplants. The first, was commissioned in 1961.\n\nThe German Navy launched the first in 1961 with 2 Brown, Boveri & Cie gas turbines in the world's first combined diesel and gas propulsion system.\n\nThe Danish Navy had 6 \"Søløven\"-class torpedo boats (the export version of the British Brave class fast patrol boat) in service from 1965 to 1990, which had 3 Bristol Proteus (later RR Proteus) Marine Gas Turbines rated at combined, plus two General Motors Diesel engines, rated at , for better fuel economy at slower speeds. And they also produced 10 Willemoes Class Torpedo / Guided Missile boats (in service from 1974 to 2000) which had 3 Rolls Royce Marine Proteus Gas Turbines also rated at , same as the Søløven-class boats, and 2 General Motors Diesel Engines, rated at , also for improved fuel economy at slow speeds.\n\nThe Swedish Navy produced 6 Spica-class torpedo boats between 1966 and 1967 powered by 3 Bristol Siddeley Proteus 1282 turbines, each delivering . They were later joined by 12 upgraded Norrköping class ships, still with the same engines. With their aft torpedo tubes replaced by antishipping missiles they served as missile boats until the last was retired in 2005.\n\nThe Finnish Navy commissioned two corvettes, \"Turunmaa\" and \"Karjala\", in 1968. They were equipped with one Rolls-Royce Olympus TM1 gas turbine and three Wärtsilä marine diesels for slower speeds. They were the fastest vessels in the Finnish Navy; they regularly achieved speeds of 35 knots, and 37.3 knots during sea trials. The \"Turunmaa\"s were paid off in 2002. \"Karjala\" is today a museum ship in Turku, and \"Turunmaa\" serves as a floating machine shop and training ship for Satakunta Polytechnical College.\n\nThe next series of major naval vessels were the four Canadian helicopter carrying destroyers first commissioned in 1972. They used 2 ft-4 main propulsion engines, 2 ft-12 cruise engines and 3 Solar Saturn 750 kW generators.\nThe first U.S. gas-turbine powered ship was the U.S. Coast Guard's , a cutter commissioned in 1961 that was powered by two turbines utilizing controllable-pitch propellers. The larger High Endurance Cutters, was the first class of larger cutters to utilize gas turbines, the first of which () was commissioned in 1967. Since then, they have powered the U.S. Navy's s, and s, and guided missile cruisers. , a modified , is to be the Navy's first amphibious assault ship powered by gas turbines.\nThe marine gas turbine operates in a more corrosive atmosphere due to the presence of sea salt in air and fuel and use of cheaper fuels.\n\nUp to the late 1940s, much of the progress on marine gas turbines all over the world took place in design offices and engine builder's workshops and development work was led by the British Royal Navy and other Navies. While interest in the gas turbine for marine purposes, both naval and mercantile, continued to increase, the lack of availability of the results of operating experience on early gas turbine projects limited the number of new ventures on seagoing commercial vessels being embarked upon. In 1951, the Diesel-electric oil tanker \"Auris\", 12,290 Deadweight tonnage (DWT) was used to obtain operating experience with a main propulsion gas turbine under service conditions at sea and so became the first ocean-going merchant ship to be powered by a gas turbine. Built by Hawthorn Leslie at Hebburn-on-Tyne, UK, in accordance with plans and specifications drawn up by the Anglo-Saxon Petroleum Company and launched on the UK's Princess Elizabeth's 21st birthday in 1947, the ship was designed with an engine room layout that would allow for the experimental use of heavy fuel in one of its high-speed engines, as well as the future substitution of one of its diesel engines by a gas turbine. The \"Auris\" operated commercially as a tanker for three-and-a-half years with a diesel-electric propulsion unit as originally commissioned, but in 1951 one of its four diesel engines – which were known as \"Faith\", \"Hope\", \"Charity\" and \"Prudence\" - was replaced by the world’s first marine gas turbine engine, a open-cycle gas turbo-alternator built by British Thompson-Houston Company in Rugby. Following successful sea trials off the Northumbrian coast, the \"Auris\" set sail from Hebburn-on-Tyne in October 1951 bound for Port Arthur in the US and then Curacao in the southern Caribbean returning to Avonmouth after 44 days at sea, successfully completing her historic trans-Atlantic crossing. During this time at sea the gas turbine burnt diesel fuel and operated without an involuntary stop or mechanical difficulty of any kind. She subsequently visited Swansea, Hull, Rotterdam, Oslo and Southampton covering a total of 13,211 nautical miles. The \"Auris\" then had all of its power plants replaced with a directly coupled gas turbine to become the first civilian ship to operate solely on gas turbine power.\n\nDespite the success of this early experimental voyage the gas turbine was not to replace the diesel engine as the propulsion plant for large merchant ships. At constant cruising speeds the diesel engine simply had no peer in the vital area of fuel economy. The gas turbine did have more success in Royal Navy ships and the other naval fleets of the world where sudden and rapid changes of speed are required by warships in action.\n\nThe United States Maritime Commission were looking for options to update WWII Liberty ships, and heavy-duty gas turbines were one of those selected. In 1956 the \"John Sergeant\" was lengthened and equipped with a General Electric HD gas turbine with exhaust-gas regeneration, reduction gearing and a variable-pitch propeller. It operated for 9,700 hours using residual fuel(Bunker C) for 7,000 hours. Fuel efficiency was on a par with steam propulsion at per hour, and power output was higher than expected at due to the ambient temperature of the North Sea route being lower than the design temperature of the gas turbine. This gave the ship a speed capability of 18 knots, up from 11 knots with the original power plant, and well in excess of the 15 knot targeted. The ship made its first transatlantic crossing with an average speed of 16.8 knots, in spite of some rough weather along the way. Suitable Bunker C fuel was only available at limited ports because the quality of the fuel was of a critical nature. The fuel oil also had to be treated on board to reduce contaminants and this was a labor-intensive process that was not suitable for automation at the time. Ultimately, the variable-pitch propeller, which was of a new and untested design, ended the trial, as three consecutive annual inspections revealed stress-cracking. This did not reflect poorly on the marine-propulsion gas-turbine concept though, and the trial was a success overall. The success of this trial opened the way for more development by GE on the use of HD gas turbines for marine use with heavy fuels. The \"John Sergeant\" was scrapped in 1972 at Portsmouth PA.\nBoeing launched its first passenger-carrying waterjet-propelled hydrofoil Boeing 929, in April 1974. Those ships were powered by two Allison 501-KF gas turbines.\n\nBetween 1971 and 1981, Seatrain Lines operated a scheduled container service between ports on the eastern seaboard of the United States and ports in northwest Europe across the North Atlantic with four container ships of 26,000 tonnes DWT. Those ships were powered by twin Pratt & Whitney gas turbines of the FT 4 series. The four ships in the class were named \"Euroliner\", \"Eurofreighter\", \"Asialiner\" and \"Asiafreighter\". Following the dramatic Organization of the Petroleum Exporting Countries (OPEC) price increases of the mid-1970s, operations were constrained by rising fuel costs. Some modification of the engine systems on those ships was undertaken to permit the burning of a lower grade of fuel (i.e., marine diesel). Reduction of fuel costs was successful using a different untested fuel in a marine gas turbine but maintenance costs increased with the fuel change. After 1981 the ships were sold and refitted with, what at the time, was more economical diesel-fueled engines but the increased engine size reduced cargo space.\n\nThe first passenger ferry to use a gas turbine was the GTS \"Finnjet\", built in 1977 and powered by two Pratt & Whitney FT 4C-1 DLF turbines, generating and propelling the ship to a speed of 31 knots. However, the Finnjet also illustrated the shortcomings of gas turbine propulsion in commercial craft, as high fuel prices made operating her unprofitable. After four years of service, additional diesel engines were installed on the ship to reduce running costs during the off-season. The Finnjet was also the first ship with a Combined diesel-electric and gas propulsion. Another example of commercial use of gas turbines in a passenger ship is Stena Line's HSS class fastcraft ferries. HSS 1500-class \"Stena Explorer\", \"Stena Voyager\" and \"Stena Discovery\" vessels use combined gas and gas setups of twin GE LM2500 plus GE LM1600 power for a total of . The slightly smaller HSS 900-class \"Stena Carisma\", uses twin ABB–STAL GT35 turbines rated at gross. The \"Stena Discovery\" was withdrawn from service in 2007, another victim of too high fuel costs.\n\nIn July 2000 the \"Millennium\" became the first cruise ship to be propelled by gas turbines, in a combined diesel and gas configuration. The liner RMS Queen Mary 2 uses a combined diesel and gas configuration.\n\nIn marine racing applications the 2010 C5000 Mystic catamaran Miss GEICO uses two Lycoming T-55 turbines for its power system.\n\nGas turbine technology has steadily advanced since its inception and continues to evolve. Development is actively producing both smaller gas turbines and more powerful and efficient engines. Aiding in these advances are computer-based design (specifically CFD and finite element analysis) and the development of advanced materials: Base materials with superior high-temperature strength (e.g., single-crystal superalloys that exhibit yield strength anomaly) or thermal barrier coatings that protect the structural material from ever-higher temperatures. These advances allow higher compression ratios and turbine inlet temperatures, more efficient combustion and better cooling of engine parts.\n\nComputational Fluid Dynamics (CFD) has contributed to substantial improvements in the performance and efficiency of Gas Turbine engine components through enhanced understanding of the complex viscous flow and heat transfer phenomena involved. For this reason, CFD is one of the key computational tool used in Design & development of gas turbine engines.\n\nThe simple-cycle efficiencies of early gas turbines were practically doubled by incorporating inter-cooling, regeneration (or recuperation), and reheating. These improvements, of course, come at the expense of increased initial and operation costs, and they cannot be justified unless the decrease in fuel costs offsets the increase in other costs. The relatively low fuel prices, the general desire in the industry to minimize installation costs, and the tremendous increase in the simple-cycle efficiency to about 40 percent left little desire for opting for these modifications.\n\nOn the emissions side, the challenge is to increase turbine inlet temperatures while at the same time reducing peak flame temperature in order to achieve lower NOx emissions and meet the latest emission regulations. In May 2011, Mitsubishi Heavy Industries achieved a turbine inlet temperature of 1,600 °C on a 320 megawatt gas turbine, and 460 MW in gas turbine combined-cycle power generation applications in which gross thermal efficiency exceeds 60%.\n\nCompliant foil bearings were commercially introduced to gas turbines in the 1990s. These can withstand over a hundred thousand start/stop cycles and have eliminated the need for an oil system. The application of microelectronics and power switching technology have enabled the development of commercially viable electricity generation by microturbines for distribution and vehicle propulsion.\n\nThe following are advantages and disadvantages of gas-turbine engines:\n\n\n\nBritish, German, other national and international test codes are used to standardize the procedures and definitions used to test gas turbines. Selection of the test code to be used is an agreement between the purchaser and the manufacturer, and has some significance to the design of the turbine and associated systems. In the United States, ASME has produced several performance test codes on gas turbines. This includes ASME PTC 22-2014. These ASME performance test codes have gained international recognition and acceptance for testing gas turbines. The single most important and differentiating characteristic of ASME performance test codes, including PTC 22, is that the test uncertainty of the measurement indicates the quality of the test and is not to be used as a commercial tolerance.\n\n\n"}
{"id": "3404684", "url": "https://en.wikipedia.org/wiki?curid=3404684", "title": "Gravitational interaction of antimatter", "text": "Gravitational interaction of antimatter\n\nThe gravitational interaction of antimatter with matter or antimatter has not been conclusively observed by physicists. While the consensus among physicists is that gravity will attract both matter and antimatter at the same rate that matter attracts matter, there is a strong desire to confirm this experimentally.\n\nAntimatter's rarity and tendency to annihilate when brought into contact with matter makes its study a technically demanding task. Most methods for the creation of antimatter (specifically antihydrogen) result in high-energy particles and atoms of high kinetic energy, which are unsuitable for gravity-related study. In recent years, first ALPHA and then ATRAP have trapped antihydrogen atoms at CERN; in 2012 ALPHA used such atoms to set the first free-fall loose bounds on the gravitational interaction of antimatter with matter, measured to within ±7500% of ordinary gravity, not enough for a clear scientific statement about the sign of gravity acting on antimatter. Future experiments need to be performed with higher precision, either with beams of antihydrogen (AEGIS) or with trapped antihydrogen (ALPHA or GBAR).\n\nIn addition to uncertainty regarding whether antimatter is gravitationally attracted or repulsed from other matter, it is also unknown whether the magnitude of the gravitational force is the same. Difficulties in creating quantum gravity theories have led to the idea that antimatter may react with a slightly different magnitude.\n\nWhen antimatter was first discovered in 1932, physicists wondered about how it would react to gravity. Initial analysis focused on whether antimatter should react the same as matter or react oppositely. Several theoretical arguments arose which convinced physicists that antimatter would react exactly the same as normal matter. They inferred that a gravitational repulsion between matter and antimatter was implausible as it would violate CPT invariance, conservation of energy, result in vacuum instability, and result in CP violation. It was also theorized that it would be inconsistent with the results of the Eötvös test of the weak equivalence principle. Many of these early theoretical objections were later overturned.\n\nThe equivalence principle predicts that the gravitational acceleration of antimatter is the same as that of ordinary matter. A matter-antimatter gravitational repulsion is thus excluded from this point of view. Furthermore, photons, which are their own antiparticles in the framework of the Standard Model, have in a large number of astronomical tests (gravitational redshift and gravitational lensing, for example) been observed to interact with the gravitational field of ordinary matter exactly as predicted by the general theory of relativity. This is a feature that has to be explained by any theory predicting that matter and antimatter repel.\n\nThe CPT theorem implies that the difference between the properties of a matter particle and those of its antimatter counterpart is \"completely\" described by C-inversion. Since this C-inversion doesn't affect gravitational mass, the CPT theorem predicts that the gravitational mass of antimatter is the same as that of ordinary matter. A repulsive gravity is then excluded, since that would imply a difference in sign between the observable gravitational mass of matter and antimatter.\n\nIn 1958, Philip Morrison argued that antigravity would violate conservation of energy. If matter and antimatter responded oppositely to a gravitational field, then it would take no energy to change the height of a particle-antiparticle pair. However, when moving through a gravitational potential, the frequency and energy of light is shifted. Morrison argued that energy would be created by producing matter and antimatter at one height and then annihilating it higher up, since the photons used in production would have less energy than the photons yielded from annihilation. However, it was later found that antigravity would still not violate the second law of thermodynamics.\n\nLater in 1958, L. Schiff used quantum field theory to argue that antigravity would be inconsistent with the results of the Eötvös experiment. However, the renormalization technique used in Schiff's analysis is heavily criticized, and his work is seen as inconclusive. In 2014 the argument was redone by Marcoen Cabbolet, who concluded however that it merely demonstrates the incompatibility of the Standard Model and gravitational repulsion.\n\nIn 1961, Myron L. Good argued that antigravity would result in the observation of an unacceptably high amount of CP violation in the anomalous regeneration of kaons. At the time, CP violation had not yet been observed. However, Good's argument is criticized for being expressed in terms of absolute potentials. By rephrasing the argument in terms of relative potentials, Gabriel Chardin found that it resulted in an amount of kaon regeneration which agrees with observation. He argues that antigravity is in fact a potential explanation for CP violation based on his models on K mesons. His results date back to 1992. Since then however, studies on CP violation mechanisms in the B mesons systems have fundamentally invalidated these explanations.\n\nAccording to Gerard 't Hooft, every physicist recognizes immediately what is wrong with the idea of gravitational repulsion: if a ball is thrown high up in the air so that it falls back, then its motion is symmetric under time-reversal; and therefore, the ball falls also down in opposite time-direction. Since a matter particle in opposite time-direction is an antiparticle, this proves according to 't Hooft that antimatter falls down on earth just like \"normal\" matter.\nHowever, Cabbolet replied that 't Hooft's argument is false, and only proves that an anti-ball falls down on an anti-earth – which is not disputed.\n\nAs long as repulsive gravity has not been refuted experimentally, one can speculate about physical principles that would bring about such a repulsion. Thus far, three radically different theories have been published.\n\nThe first theory of repulsive gravity was a quantum theory published by Mark Kowitt. In this modified Dirac theory, Kowitt postulated that the positron is not a hole in the sea of electrons-with-negative-energy as in usual Dirac hole theory, but instead is a hole in the sea of electrons-with-negative-energy-and-positive-gravitational-mass: this yields a modified C-inversion, by which the positron has positive energy but negative gravitational mass. Repulsive gravity is then described by adding extra terms (\"m\"\"Φ\" and \"m\"\"A\") to the wave equation. The idea is that the wave function of a positron moving in the gravitational field of a matter particle evolves such that in time it becomes more probable to find the positron further away from the matter particle.\n\nClassical theories of repulsive gravity have been published by Ruggero Santilli and Massimo Villata. Both theories are extensions of general relativity, and are experimentally indistinguishable. The general idea remains that gravity is the deflection of a continuous particle trajectory due to the curvature of spacetime, but antiparticles now 'live' in an inverted spacetime. The equation of motion for antiparticles is then obtained from the equation of motion of ordinary particles by applying the C, P, and T-operators (Villata) or by applying \"isodual maps\" (Santilli), which amounts to the same thing: the equation of motion for antiparticles then predicts a repulsion of matter and antimatter. It has to be taken that the \"observed\" trajectories of antiparticles are projections on \"our\" spacetime of the true trajectories in the inverted spacetime. However, it has been argued on methodological and ontological grounds that the area of application of Villata’s theory cannot be extended to include the microcosmos. These objections were subsequently dismissed by Villata.\n\nThe first non-classical, non-quantum physical principles underlying a matter-antimatter gravitational repulsion have been published by Marcoen Cabbolet. He introduces the Elementary Process Theory, which uses a new language for physics, i.e. a new mathematical formalism and new physical concepts, and which is incompatible with both quantum mechanics and general relativity. The core idea is that nonzero rest mass particles such as electrons, protons, neutrons and their antimatter counterparts exhibit stepwise motion as they alternate between a particlelike state of rest and a wavelike state of motion. Gravitation then takes place in a wavelike state, and the theory allows, for example, that the wavelike states of protons and antiprotons interact differently with the earth’s gravitational field.\n\nFurther authors have used a matter-antimatter gravitational repulsion to explain cosmological observations, but these publications do not address the physical principles of gravitational repulsion.\n\nOne source of experimental evidence in favor of normal gravity was the observation of neutrinos from Supernova 1987A. In 1987, three neutrino detectors around the world simultaneously observed a cascade of neutrinos emanating from a supernova in the Large Magellanic Cloud. Although the supernova happened about 164,000 light years away, both neutrinos and antineutrinos seem to have been detected virtually simultaneously. If both were actually observed, then any difference in the gravitational interaction would have to be very small. However, neutrino detectors cannot distinguish perfectly between neutrinos and antineutrinos; in fact, the two may be identical. Some physicists conservatively estimate that there is less than a 10% chance that no regular neutrinos were observed at all. Others estimate even lower probabilities, some as low as 1%. Unfortunately, this accuracy is unlikely to be improved by duplicating the experiment any time soon. The last known supernova to occur at such a close range prior to Supernova 1987A was around 1867.\n\nPhysicist William Fairbank attempted a laboratory experiment to directly measure the gravitational acceleration of electrons, with hopes of attempting the same method for positrons. However, their charge-to-mass ratio is so large that electromagnetic effects overwhelmed attempts to measure the impact of gravity on electrons. Fairbank was never able to attempt the experiment with positrons.\n\nIt is difficult to directly observe gravitational forces at the particle level. For charged particles, the electromagnetic force overwhelms the much weaker gravitational interaction. Even antiparticles in neutral antimatter, such as antihydrogen, must be kept separate from their counterparts in the matter that forms the experimental equipment, which requires strong electromagnetic fields. These fields, e.g. in the form of atomic traps, exert forces on these antiparticles which easily overwhelm the gravitational force of Earth and nearby test masses. Since all production methods for antiparticles result in high-energy antimatter particles, the necessary cooling for observation of gravitational effects in a laboratory environment requires very elaborate experimental techniques and very careful control of the trapping fields.\n\nSince 2010 the production of cold antihydrogen has become possible at the Antiproton Decelerator at CERN. Antihydrogen, which is electrically neutral, should make it possible to directly measure the gravitational attraction of antimatter particles to the matter Earth. In 2013, experiments on antihydrogen atoms released from the ALPHA trap set direct, i.e. freefall, coarse limits on antimatter gravity. These limits were coarse, with a relative precision of ± 100%, thus, far from a clear statement even for the sign of gravity acting on antimatter. Future experiments at CERN with beams of antihydrogen, such as AEGIS, or with trapped antihydrogen, such as ALPHA and GBAR, have to improve the sensitivity to make a clear, scientific statement about gravity on antimatter.\n\n"}
{"id": "33981534", "url": "https://en.wikipedia.org/wiki?curid=33981534", "title": "Hammond Typewriter", "text": "Hammond Typewriter\n\nThe Hammond Typewriter was invented by James Bartlett Hammond and first manufactured in 1881. The typeface used by the typewriter was also available as foundry type from the Inland Type Foundry. The Hammond typewriter serial number database is at: www.HammondTypewriter.com\n\n"}
{"id": "1980416", "url": "https://en.wikipedia.org/wiki?curid=1980416", "title": "History of information technology auditing", "text": "History of information technology auditing\n\nInformation Technology Auditing (IT auditing) began as Electronic Data Process (EDP) Auditing and developed largely as a result of the rise in technology in accounting systems, the need for IT control, and the impact of computers on the ability to perform attestation services. The last few years have been an exciting time in the world of IT auditing as a result of the accounting scandals and increased regulation. IT auditing has had a relatively short yet rich history when compared to auditing as a whole and remains an ever-changing field.\n\nThe introduction of computer technology into accounting systems changed the way data was stored, retrieved and controlled. It is believed that the first use of a computerized accounting system was at General Electric in 1954. During the time period of 1954 to the mid-1960s, the auditing profession was still auditing around the computer. At this time only mainframe computers were used and few people had the skills and abilities to program computers. This began to change in the mid-1960s with the introduction of new, smaller and less expensive machines. This increased the use of computers in businesses and with it came the need for auditors to become familiar with EDP concepts in business. Along with the increase in computer use, came the rise of different types of accounting systems. The industry soon realized that they needed to develop their own software and the first of the generalized audit software (GAS) was developed. In 1968, the American Institute of Certified Public Accountants (AICPA) had the Big Eight (now the Big Four) accounting firms participate in the development of EDP auditing. The result of this was the release of \"Auditing & EDP\". The book included how to document EDP audits and examples of how to process internal control reviews.\n\nAround this time EDP auditors formed the Electronic Data Processing Auditors Association (EDPAA). The goal of the association was to produce guidelines, procedures and standards for EDP audits. In 1977, the first edition of \"Control Objectives\" was published. This publication is now known as Control Objectives for Information and related Technology (CobiT). CobiT is the set of generally accepted IT control objectives for IT auditors. In 1994, EDPAA changed its name to Information Systems Audit and Control Association (ISACA). The period from the late 1960s through today has seen rapid changes in technology from the microcomputer and networking to the internet and with these changes came some major events that change IT auditing forever.\n\nThe formation and rise in popularity of the Internet and E-commerce have had significant influences on the growth of IT audit. The Internet influences the lives of most of the world and is a place of increased business, entertainment and crime. IT auditing helps organizations and individuals on the Internet find security while helping commerce and communications to flourish.\n\nThere are five major events in U.S. history which have had significant impact on the growth of IT auditing. These are the Equity Funding scandal, the development of the Internet and E-commerce, the 1998 IT failure at AT&T Corporation, the Enron and Arthur Andersen LLP scandal, and the September 11, 2001 Attacks.\n\nThese events have not only heightened the need for more reliable, accurate, and secure systems but have brought a much needed focus to the importance of the accounting profession. Accountants certify the accuracy of public company financial statements and add confidence to financial markets. The heightened focus on the industry has brought improved control and higher standards for all working in accounting, especially those involved in IT auditing.\n\nThe first known case of misuse of information technology occurred at Equity Funding Corporation of America. Beginning in 1964 and continuing on until 1973, managers for the company booked false insurance policies to show greater profits, thus boosting the price of the stock of the company. If it wasn't for a whistle blower, the fraud may have never been caught. After the fraud was discovered, it took the auditing firm Touche Ross two years to confirm that the insurance policies were not real. This was one of the first cases where auditors had to audit through the computer rather than around the computer.\n\nIn 1998 AT&T suffered an IT failure that impacted worldwide commerce and communication. A major switch failed due to software and procedural errors and left many credit card users unable to access funds for upwards this brought to the forefront our reliance in IT services and reminds us of the need for assurance in our computer systems.\n\nThe Enron and Arthur Andersen LLP scandal led to the demise of a foremost Accounting firm, an investor loss of more than 60 billion dollars and the largest bankruptcy in U.S. history. Arthur Andersen was recently found guilty of obstruction of justice for their role in the collapse of the energy giant. This scandal had a significant impact on the Sarbanes-Oxley Act and was a major self-regulation violation.\n\n\n\n"}
{"id": "19151448", "url": "https://en.wikipedia.org/wiki?curid=19151448", "title": "Identity correlation", "text": "Identity correlation\n\nIn information systems, identity correlation is a process that reconciles and validates the proper ownership of disparate user account login IDs (user names) that reside on systems and applications throughout an organization and can permanently link ownership of those user account login IDs to particular individuals by assigning a unique identifier (also called primary or common keys) to all validated account login IDs.\n\nThe process of identity correlation validates that individuals only have account login IDs for the appropriate systems and applications a user should have access to according to the organization’s business policies, access control policies and various application requirements.\n\nA unique identifier, in the context of identity correlation, is any identifier which is guaranteed to be unique among all identifiers used for a group of individuals and for a specific purpose. There are three main types of unique identifiers, each corresponding to a different generation strategy:\n\n\nFor the purposes of identity correlation, a unique identifier is typically a serial number or random number selected from a number space much larger than the maximum number of individuals who will be identified. A unique identifier, in this context, is typically represented as an additional attribute in the directory associated with each particular data source. However, adding an attribute to each system-specific directory may affect application requirements or specific business requirements, depending on the requirements of the organization. Under these circumstances, unique identifiers may not be an acceptable addition to an organization.\n\nIdentity Correlation involves several factors:\n\n1. Linking Disparate Account IDs Across Multiple Systems or Applications\n\nMany organizations must find a method to comply with audits that require it to link disparate application user identities with the actual people who are associated with those user identities.\n\nSome individuals may have a fairly common first and/or last name, which makes it difficult to link the right individual to the appropriate account login ID, especially when those account login IDs are not linked to enough specific identity data to remain unique.\n\nA typical construct of the login ID, for example, can be the 1st character of givenname + next 7 of sn, with incremental uniqueness. This would produce login IDs like jsmith12, jsmith 13, jsmith14, etc. for users John Smith, James Smith and Jack Smith, respectively.\n\nConversely, one individual might undergo a name change either formally or informally, which can cause new account login IDs that the individual appropriates to appear drastically different in nomenclature to the account login IDs that individual acquired prior to any change.\n\nFor example, a woman could get married and decide to use her new surname professionally. If her name was originally Mary Jones but she is now Mary Smith, she could call HR and ask them to update her contact information and email address with her new surname. This request would update her Microsoft Exchange login ID to mary.smith to reflect that surname change, but it might not actually update her information or login credentials in any other system she has access to. In this example, she could still be mjones in Active Directory and mj5678 in RACF.\n\nIdentity correlation should link the appropriate system account login IDs to individuals who might be indistinguishable, as well as to those individuals who might appear to be drastically different from a system-by-system standpoint, but should be associated with the same individual.\n\nFor more details on this topic, please see: The Second Wave: Linking Identities to Contexts\n\n2. Discovering Intentional and Unintentional Inconsistencies in Identity Data\n\nInconsistencies in identity data typically develop over time in organizations as applications are added, removed or changed and as individuals attain or retain an ever-changing stream of access rights as they matriculate into and out of the organization.\n\nApplication user login IDs do not always have a consistent syntax across different applications or systems and many user login IDs are not specific enough to directly correlate it back to one particular individual within an organization.\n\nUser data inconsistencies can also occur due to simple manual input errors, non-standard nomenclature, or name changes that might not be identically updated across all systems.\n\nThe identity correlation process should take these inconsistencies into account to link up identity data that might seem to be unrelated upon initial investigation.\n\n3. Identifying Orphan or Defunct Account Login IDs\n\nOrganizations can expand and consolidate from mergers and acquisitions, which increases the complexity of business processes, policies and procedures as a result.\n\nAs an outcome of these events, users are subject to moving to different parts of the organization, attaining a new position within the organization, or matriculating out of the organization altogether. At the same time, each new application that is added has the potential to produce a new completely unique user ID.\n\nSome identities may become redundant, others may be in violation of application-specific or more widespread departmental policies, others could be related to non-human or system account IDs, and still others may simply no longer be applicable for a particular user environment.\n\nProjects that span different parts of the organization or focus on more than one application become difficult to implement because user identities are often not properly organized or recognized as being defunct due to changes in the business process.\n\nAn identity correlation process must identify all orphan or defunct account identities that no longer belong from such drastic shifts in an organization’s infrastructure.\n\n4. Validating Individuals to their Appropriate Account IDs\n\nUnder such regulations as Sarbanes-Oxley and Gramm-Leach-Bliley Act, it is required for organizations to ensure the integrity of each user across all systems and account for all access a user has to various back-end systems and applications in an organization.\n\nIf implemented correctly, identity correlation will expose compliance issues. Auditors frequently ask organizations to account for who has access to what resources. For companies that have not already fully implemented an enterprise identity management solution, identity correlation and validation is required to adequately attest to the true state of an organization’s user base.\n\nThis validation process typically requires interaction with individuals within an organization who are familiar with the organization’s user base from an enterprise-wide perspective, as well as those individuals who are responsible and knowledgeable of each individual system and/or application-specific user base.\n\nIn addition, much of the validation process might ultimately involve direct communication with the individual in question to confirm particular identity data that is associated with that specific individual.\n\n5. Assigning a unique primary or common key for every system or application Account ID that is attached to each individual\n\nIn response to various compliance pressures, organizations have an option to introduce unique identifiers for its entire user base to validate that each user belongs in each specific system or application in which he/she has login capabilities.\n\nIn order to effectuate such a policy, various individuals familiar with the organization’s entire user base, as well as each system-specific user-base, must be responsible for validating that certain identities should be linked together and other identities should be disassociated from each other.\n\nOnce the validation process is complete, a unique identifier can be assigned to that individual and his or her associated system-specific account login IDs.\n\nAs mentioned above, in many organizations, users may sign into different systems and applications using different login IDs. There are many reasons to link these into ``enterprise-wide\" user profiles.\n\nThere are a number of basic strategies to perform this correlation, or \"ID Mapping:\"\n\n\n1. Privacy Concerns\n\nOften, any process that requires an in-depth look into identity data brings up a concern for privacy and disclosure issues. Part of the identity correlation process infers that each particular data source will need to be compared against an authoritative data source to ensure consistency and validity against relevant corporate policies and access controls.\n\nAny such comparison that involves an exposure of enterprise-wide, authoritative, HR-related identity data will require various non-disclosure agreements either internally or externally, depending on how an organization decides to undergo an identity correlation exercise.\n\nBecause authoritative data is frequently highly confidential and restricted, such concerns may bar the way from performing an identity correlation activity thoroughly and sufficiently.\n\n2. Extensive Time and Effort Requirements\n\nMost organizations experience difficulties understanding the inconsistencies and complexities that lie within their identity data across all of their data sources. Typically, the process can not be completed accurately or sufficiently by undergoing a manual comparison of two lists of identity data or even executing simple scripts to find matches between two different data sets. Even if an organization can dedicate full-time individuals to such an effort, the methodologies themselves usually do not expose an adequate enough percentage of defunct identities, validate an adequate enough percentage of matched identities, or identify system (non-person) account IDs to pass the typical requirements of an identity-related audit.\n\n\nManual efforts to accomplish identity correlation require a great deal of time and people effort, and do not guarantee that the effort will be completed successfully or in a compliant fashion.\n\nBecause of this, automated identity correlation solutions have recently entered the marketplace to provide more effortless ways of handling identity correlation exercises.\n\nTypical automated identity correlation solution functionality includes the following characteristics:\n\n\nIdentity correlation solutions can be implemented under three distinct delivery models. These delivery methodologies are designed to offer a solution that is flexible enough to correspond to various budget and staffing requirements, as well as meet both short and/or long-term project goals and initiatives.\n\nSoftware Purchase – This is the classic Software Purchase model where an organization purchases a software license and runs the software within its own hardware infrastructure.\n\n\nIdentity Correlation as a Service (ICAS) – ICAS is a subscription-based service where a client connects to a secure infrastructure to load and run correlation activities. This offering provides full functionality offered by the identity correlation solution without owning and maintaining hardware and related support staff.\n\nTurn-Key Identity Correlation – A Turn-key methodology requires a client to contract with and provide data to a solutions vendor to perform the required identity correlation activities. Once completed, the solutions vendor will return correlated data, identify mismatches, and provide data integrity reports.\n\nValidation activities will still require some direct feedback from individuals within the organization who understand the state of the organizational user base from an enterprise-wide viewpoint, as well as those individuals within the organization who are familiar with each system-specific user base. In addition, some validation activities might require direct feedback from individuals within the user base itself.\n\nA Turn-Key solution can be performed as a single one-time activity or monthly, quarterly, or even as part of an organization’s annual validation activities. Additional services are available, such as:\n\n\nRelated or associated topics which fall under the category of identity correlation may include:\n\nCompliance Regulations / Audits\n\n\n\nAccess control\n\n\nDirectory services\n\n\nOther categories\n\n"}
{"id": "23546891", "url": "https://en.wikipedia.org/wiki?curid=23546891", "title": "List of human-based units of measure", "text": "List of human-based units of measure\n\nThis is a list of units of measure based on human body parts or the attributes or abilities of humans. It does not include derived units further unless they are also themselves human-based. These units are thus considered to be human scale and anthropocentric.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "33444293", "url": "https://en.wikipedia.org/wiki?curid=33444293", "title": "Lowepro", "text": "Lowepro\n\nLowepro is a brand of carrying bags for cameras, laptops, imaging accessories and portable consumer electronics with corporate headquarters in Petaluma, California. It is part of DayMen Canada Acquisition ULC, owned by private equity firm Brockway Moran & Partners.\n\nLowepro camera bags began as an offshoot of Lowe Alpine Systems, an outdoor equipment manufacturer founded by Mike, Greg and Jeff Lowe in suburban Denver (Colorado) in 1967. In the 1960s, the Lowe brothers were known in outdoor circles for their climbing, skiing and photography. In 1967, Greg Lowe invented the world’s first close-fitting internal-frame backpack. He used the same design principles to create the Lowepro line of protective camera bags as he often carried photographic equipment on hikes and climbs in the backcountry of the Western United States. The first bag marketed for cameras was manufactured in 1972.\n\nIn 1981, Uwe Mummenhoff and DayMen Photo Marketing Inc. gained exclusive rights to produce Lowepro products under license in Canada. In 1989, Lowepro U.S.A. became a wholly owned subsidiary of DayMen Photo Marketing and was established as a corporation; the Lowepro registered trademark was bought in 2002. Brockway Moran & Partners invested in DayMen in partnership with management in October 2010.\n\nLowepro produces various ranges of bags. Originally producing bags for traditional cameras, the brand has evolved to create bags which house compact digital cameras, smartphones, and even drones and quadcopters.\n\nThe Slingshot range is aimed at professional news and sports photographers, providing easy access to cameras for rapid shooting; CNet found the SlingShot 300 AW Camera Bag offered good protection and easy access. The FastPack is a rucksack-style range, which CNet found less refined.\n\n"}
{"id": "566385", "url": "https://en.wikipedia.org/wiki?curid=566385", "title": "Makeover", "text": "Makeover\n\nA makeover is a radical change in appearance. When the word is used to describe a change in human physical appearance, it may imply a change in clothing, haircut, or cosmetics. A personal makeover might also include cosmetic surgery, dental veneers, or contact lenses. Sometimes a makeover is used to refer to non-physical things, such as a makeover of character, personality or attitude. It can also refer to a dramatic change in construction, such as when a building is renovated or is refurbished. Makeovers are usually referred to in a positive manner, as a way to start fresh or improve your life.\n\nMakeovers are often popular television subjects. Long a staple subject of daytime talk shows, they have recently moved into the limelight in television shows such as \"Queer Eye\". Other popular makeover shows include \"What Not to Wear\", \"How to Look Good Naked\", \"Extreme Makeover\", \"MADE\", \"Ambush Makeover\" and \"Pimp My Ride\". There is also a category of reality TV based on giving makeovers to homes, such as , 60 Minute Makeover and Property Brothers.\n\nSee makeover reality television series.\n\nComputer software and online tools can also be used for performing what are known as Virtual Makeovers. Using a photograph of a human face, software can apply cosmetics, hairstyles, and various eye wear such as contact lenses and sunglasses in order to allow users to visualize different looks without physically trying them on. Today, virtual makeup works in real-time using phone camera tracking, and examples are Visage Technologies's MakeApp, L'Oreal's Makeup Genius, and Oriflame's Makeup Wizard.\n\nIn movies there is a common trope of a character, usually a girl, undergoing a dramatic makeover in appearance or personality. Here are some examples of movies with this trope.\n\n\n\n\n\n\n\n\n\nThere is also a series of books, aimed at teenage girls, called \"The Makeover Series\", written by Suzanne Weyn. There are several experts who perform the art of makeovers. Usually makeover artists specialize in hair styling, make-up or clothing.\n\n\"The Makeover Guy\" is a registered trademark for author and makeover expert Christopher Hopkins who is known for his television head-to-toe makeovers. He has a book called \"Staging Your Comeback: A Complete Beauty Revival for Women Over 45\".\n\n\n"}
{"id": "40226076", "url": "https://en.wikipedia.org/wiki?curid=40226076", "title": "MasterChef Junior", "text": "MasterChef Junior\n\nMasterChef Junior is an American cooking competition involving children and teenagers from age of 8–13 and airs on Fox. It premiered on Friday, September 27, 2013. It is based on the format of the U.K. series \"Junior MasterChef\".\n\nOn March 5, 2014, \"MasterChef Junior\" was renewed for a third season before production on season two began. The second season premiered on November 4, 2014. The third season premiered on January 6, 2015. The fourth season premiered on November 6, 2015. The fifth season premiered on February 9, 2017. The sixth season premiered on March 2, 2018. On November 26, 2018, the show's official YouTube page posted a video announcing a seventh season to premiere sometime in February 2019, with the judges being Gordon Ramsay, Christina Tosi, and Aarón Sanchez.\n\nAny child or teenager between ages eight and thirteen can apply to become a contestant on the series by applying online or by going to an open casting call. Twenty-four applicants are chosen to audition. At the auditions, the applicants are split into three groups. Twelve of the applicants become contestants for the next round and all twenty-four receive one of the \"MasterChef\" franchise's signature aprons that they get to keep, regardless of their progress.\n\nAfter the audition round, two contestants are sent home per episode. The winner receives a prize of $100,000 and the \"MasterChef Junior\" trophy.\n\nIn seasons 2–4, the contestants get their apron and the competition begins; starting from season 5, the cooks compete making dishes to get a spot in the competition and a white apron.\n\n<onlyinclude></onlyinclude>\n\nLike its adult counterpart, at its inception \"MasterChef Junior\" was judged by Gordon Ramsay, Joe Bastianich and Graham Elliot. Bastianich did not return for the show's fourth season, being replaced by prominent pastry chef Christina Tosi. Elliot did not return for the show's fifth season. For the sixth season, Bastianich returned to his judging position.\n\nFox placed casting calls for participants in January 2013. Fox officially ordered the series (then under the name \"Junior MasterChef\") on May 10, 2013. The name was later changed to \"MasterChef Junior\". There were concerns that Ramsay's style of cursing at the contestants on his other competition shows (most notably \"Hell's Kitchen\" and the adult \"MasterChef\") would carry over to \"MasterChef Junior\". It did not. One contestant (named Gavin) said that Ramsay had only cursed twice during the production of the series and never at the contestants. In the final editing, he cursed once in front of (but not at) the contestants.\n\nThe first season premiered on September 27, 2013, with chefs Gordon Ramsay, Graham Elliot and Joe Bastianich acting as the series' judges.\n\nThe winner of \"MasterChef Junior\" season 1 was Alexander Weiss, a thirteen-year-old from New York City.\n\nSemifinalist Troy Glass is now an actor, appearing on \"Kids React\" and other cooking shows, and making cameos on shows like \"Agents of SHIELD\".\n\nThe second season premiered on November 4, 2014, with chefs Gordon Ramsay, Graham Elliot and Joe Bastianich again acting as the series' judges.\n\nThe winner of \"MasterChef Junior\" season 2 was Logan Guleff, an eleven-year-old from Memphis, Tennessee.\n\nThe third season premiered on January 6, 2015, with chefs Gordon Ramsay, Graham Elliot and Joe Bastianich once again acting as the series' judges.\n\nThe winner of \"MasterChef Junior\" season 3 was Nathan Odom, a twelve-year-old from San Diego, California.\n\nThe fourth season premiered on November 6, 2015, with chefs Gordon Ramsay, Graham Elliot and Christina Tosi acting as the series' judges.\n\nThe winner of \"MasterChef Junior\" season 4 was Addison Smith, a nine-year-old from River Forest, Illinois.\n\nThe fifth season premiered on February 9, 2017, with chefs Gordon Ramsay and Christina Tosi acting as the series' judges, along with numerous guest judges including Julie Bowen and Mayim Bialik.\n\nThe winner of \"MasterChef Junior\" season 5 was Jasmine Stewart, an eleven-year-old from Milton, Georgia. She is the first previously eliminated contestant to win the competition.\n\nThe sixth season premiered on March 2, 2018, with returning chefs Gordon Ramsay and Christina Tosi acting as the judges, along with returning judge Joe Bastianich as the third judge.\n\nThe winner of \"MasterChef Junior\" season 6 was Beni Cwiakala, a nine-year-old from Chicago, Illinois.\n\nSeasonal rankings (based on average total viewers per episode) of \"MasterChef Junior\" on Fox.\n\n\n"}
{"id": "34244297", "url": "https://en.wikipedia.org/wiki?curid=34244297", "title": "New Mexico Department of Energy, Minerals, and Natural Resources", "text": "New Mexico Department of Energy, Minerals, and Natural Resources\n\nThe New Mexico Energy, Minerals and Natural Resources Department (EMNRD) is a Cabinet Department under the Governor of New Mexico. EMNRD oversees and protects the natural and energy resources of New Mexico. EMNRD provides Statewide leadership in the protection, management, and conservation of all such resources.\n\nThe Department is led by the Cabinet Secretary of Energy, Minerals and Natural Resources. The Cabinet Secretary is appointed by the Governor, with the approval of the New Mexico State Senate, to serve at his/her pleasure. The current Cabinet Secretary is Dave Martin.\n\nThe New Mexico Energy, Minerals and Natural Resources Department was created by the enactment of the \"Energy, Minerals and Natural Resources Department Act\". The Act merged the Energy and Minerals Department and the Natural Resources Department into a single unified entity.\n\nThe mission of EMNRD is to protect, manage, conserve and oversee the responsible use of the state’s natural\nresources. EMNRD is charged with:\n\nThe EMNRD has six main missions: \n\nFor administrative support purposes only, the New Mexico Department of Game and Fish is attached to EMNRD.\n\nThe Energy Conservation and Management Division provides the Renewable Energy and Energy Efficiency Program of EMNRD. The Division oversee Statewide energy conservation efforts including the use of alternative fuels. These fuels include solar, wind, geothermal, and biomass resources. This is accomplished via partnerships with private businesses, higher education universities, and research laboratories to invest in clean energy.\n\nThe Division serves as the United States Department of Energy state administering agency for federal energy grants.\n\nThe Healthy Forests Program is responsible for the protection of all State Forests. The Program is responsible for statewide fire management and suppression activities as well as overall tree health. The Program also oversee all forest restoration efforts.\n\nThe State Parks Program oversees the operations of State Parks across the State. These State Parks are designed to protect and preserve the State's natural environment.\n\nThe Mining and Minerals Division operates the Department's Mine Reclamation Program. The Division oversees all energy and non-energy mining operations in the State. This is accomplished by issuing permits to mining companies, inspecting mining operations, reclaiming abandoned mines, and education members of the public about mining. New Mexico heavily benefits from mined natural resources such as oil, copper, coal, petroleum, potash, molybdenum, uranium, gold, silver, and lead.\n\nThe Oil Conservation Division oversees all oil, gas, and geothermal drilling operations in the State. This is accomplished by issuing permits to drilling companies, inspecting drilling operations, protecting mineral rights, and preventing fresh water contamination.\n\nThe Division is composed of an Administrative and Records Bureau, a Fiscal Bureau, an Engineering and Geological Services Bureau, and Environmental Bureau, a Legal Bureau, and four District Offices.\n\nThe Program Support Program provides centralized administrative and management support to all other aspects of the Department.\n\nThe head of EMNRD is the Cabinet Secretary of Energy, Minerals and Natural Resources. The Cabinet Secretary is appointed by the Governor of New Mexico, with the approval of the New Mexico State Senate, and serves as a member of the Governor's Cabinet. \n\nThe Cabinet Secretary is assisted by a Deputy Secretary and six Division Directors. Each of the Division Directors is appointed by the Cabinet Secretary with the approval of the Governor.\n\n"}
{"id": "28910089", "url": "https://en.wikipedia.org/wiki?curid=28910089", "title": "Pascalization", "text": "Pascalization\n\nPascalization, bridgmanization, high pressure processing (HPP) or high hydrostatic pressure (HHP) processing is a method of preserving and sterilizing food, in which a product is processed under very high pressure, leading to the inactivation of certain microorganisms and enzymes in the food. HPP has a limited effect on covalent bonds within the food product, thus maintaining both the sensory and nutritional aspects of the product. The technique was named after Blaise Pascal, a French scientist of the 17th century whose work included detailing the effects of pressure on fluids. During pascalization, more than 50,000 pounds per square inch (340 MPa, 3.4 kbar) may be applied for around fifteen minutes, leading to the inactivation of yeast, mold, and bacteria. Pascalization is also known as bridgmanization, named for physicist Percy Williams Bridgman.\n\nSpoilage microorganisms and some enzymes can be deactivated by HPP, which can extend the shelf life while preserving the sensory and nutritional characteristics of the product . Pathogenic microorganisms such as \"Listeria, E. coli, Salmonella,\" and \"Vibrio\" are also sensitive to pressures of 400-1000 MPa used during HPP. Thus, HPP can pasteurize food products with decreased processing time, reduced energy usage, and less waste . The treatment occurs at low temperatures and does not include the use of food additives. From 1990, some juices, jellies, and jams have been preserved using pascalization in Japan. The technique is now used there to preserve fish and meats, salad dressing, rice cakes, and yogurts. HPP is now being used to preserve fruit and vegetable smoothies and other products such as meat for sale in the UK. An early use of pascalization in the United States was to treat guacamole. It did not change the guacamole's taste, texture, or color, but the shelf life of the product increased to thirty days, from three days without the treatment. However, some treated foods still require cold storage because pascalization does not stop all enzyme activity caused by proteins, some of which affects shelf life. In recent years, HPP has also been used in the processing of raw pet food. Most commercial frozen and freeze-dried raw diets now go through post-packaging HPP treatment to destroy potential bacteria and viruses contaminants, with salmonella being one of the biggest concerns.\n\nExperiments into the effects of pressure on microorganisms have been recorded as early as 1884, and successful experiments since 1897. In 1899, B. H. Hite was the first to conclusively demonstrate the inactivation of microorganisms by pressure. After he reported the effects of high pressure on microorganisms, reports on the effects of pressure on foods quickly followed. Hite tried to prevent milk from spoiling, and his work showed that microorganisms can be deactivated by subjecting it to high pressure. He also mentioned some advantages of pressure-treating foods, such as the lack of antiseptics and no change in taste.\n\nHite said that, since 1897, a chemist at the West Virginia Agricultural Experimental Station had been studying the relationship between pressure and the preservation of meats, juices, and milk. Early experiments involved inserting a large screw into a cylinder and keeping it there for several days, but this did not have any effect in stopping the milk from spoiling. Later, a more powerful apparatus was able to subject the milk to higher pressures, and the treated milk was reported to stay sweeter for 24–60 hours longer than untreated milk. When of pressure was applied to samples of milk for one hour, they stayed sweet for one week. Unfortunately, the device used to induce pressure was later damaged when researchers tried to test its effects on other products.\n\nExperiments were also performed with anthrax, typhoid, and tuberculosis, which was a potential health risk for the researchers. Indeed, before the process was improved, one employee of the Experimental Station became ill with typhoid fever.\n\nThe process that Hite reported on was not feasible for widespread use and did not always completely sterilize the milk. While more extensive investigations followed, the original study into milk was largely discontinued due to concerns over its effectiveness. Hite mentioned \"certain slow changes in the milk\" related to \"enzymes that the pressure could not destroy\".<ref name=\"Hendrickx 14/15\"></ref>\n\nHite et al. released a more detailed report on pressure sterilization in 1914, which included the number of microorganisms that remained in a product after treatment. Experiments were conducted on various other foods, including fruits, fruit juices, and some vegetables. They were met with mixed success, similar to the results obtained from the earlier tests on milk. While some foods were preserved, others were not, possibly due to bacterial spores that had not been killed.\n\nHite's 1914 investigation led to other studies into the effect of pressure on microorganisms. In 1918, a study published by W. P. Larson et al. was intended to help advance vaccines. This report showed that bacterial spores were not always inactivated by pressure, while vegetative bacteria were usually killed. Larson et al.'s investigation also focused on the use of carbon dioxide, hydrogen, and nitrogen gas pressures. Carbon dioxide was found to be the most effective of the three at inactivating microorganisms.\n\nAround 1970, researchers renewed their efforts in studying bacterial spores after it was discovered that using moderate pressures was more effective than using higher pressures. These spores, which caused a lack of preservation in the earlier experiments, were inactivated faster by moderate pressure, but in a manner different from what occurred with vegetative microbes. When subjected to moderate pressures, bacterial spores germinate, and the resulting spores are easily killed using pressure, heat, or ionizing radiation. If the amount of initial pressure is increased, conditions are not ideal for germination, so the original spores must be killed instead. However, using moderate pressure does not always work, as some bacterial spores are more resistant to germination under pressure and a small portion of them will survive. A preservation method using both pressure and another treatment (such as heat) to kill spores has not yet been reliably achieved. Such a technique would allow for wider use of pressure on food and other potential advancements in food preservation.\n\nResearch into the effects of high pressures on microorganisms was largely focused on deep-sea organisms until the 1980s, when advancements in ceramic processing were made. This resulted in the production of machinery that allowed for processing foods at high pressures at a large scale, and generated some interest in the technique, especially in Japan. Although commercial products preserved by pascalization first emerged in 1990, the technology behind pascalization is still being perfected for widespread use. There is now higher demand for minimally processed products than in previous years, and products preserved by pascalization have seen commercial success despite being priced significantly higher than products treated with standard methods.\n\nIn the early 21st century, it was discovered that pascalization can separate the meat of shellfish from their shells. Lobsters, shrimp, crabs, etc. may be pascalized, and afterwards their raw meat will simply and easily slide whole right out of the cracked shell.\n\nIn pascalization, food products are sealed and placed into a steel compartment containing a liquid, often water, and pumps are used to create pressure. The pumps may apply pressure constantly or intermittently. The application of high hydrostatic pressures (HHP) on a food product will kill many microorganisms, but the spores are not destroyed. Some bacterial spores may need to be separately treated with acid to prevent their reproduction. Pascalization works especially well on acidic foods, such as yogurts and fruits, because pressure-tolerant spores are not able to live in environments with low pH levels.<ref name=\"Adams 94/95\"></ref> The treatment works equally well for both solid and liquid products.\n\nDuring pascalization, the food's proteins are denatured, hydrogen bonds are fortified, and noncovalent bonds in the food are disrupted, while the product's main structure remains intact. Because pascalization is not heat-based, covalent bonds are not affected, causing no change in the food's taste. This retention of intramolecular bonds means that HPP does not destroy vitamins, maintaining the nutritional value of the food. High hydrostatic pressure can affect muscle tissues by increasing the rate of lipid oxidation, which in turn leads to poor flavor and decreased health benefits.Additionally, there are some compounds present in foods that are subject to change during the treatment process. For example, carbohydrates are gelatinized by an increase in pressure instead of increasing the temperature during the treatment process.\n\nBecause hydrostatic pressure is able to act quickly and evenly on food, neither the size of a product's container nor its thickness play a role in the effectiveness of pascalization. There are several side effects of the process, including a slight increase in a product's sweetness, but pascalization does not greatly affect the nutritional value, taste, texture, and appearance. As a result, high pressure treatment of foods is regarded as a \"natural\" preservation method, as it does not use chemical preservatives.\n\nAnurag Sharma, a geochemist, James Scott, a microbiologist, and others at the Carnegie Institution of Washington directly observed microbial activity at pressures in excess of 1 gigapascal. The experiments were performed up to 1.6 GPa (232,000 psi) of pressure, which is more than 16,000 times normal air pressure, or about 14 times the pressure in the deepest ocean trench.\n\nThe experiment began by depositing an \"Escherichia coli\" and \"Shewanella oneidensis\" film in a Diamond Anvil Cell (DAC). The pressure was then raised to 1.6 GPa. When raised to this pressure and kept there for 30 hours, at least 1% of the bacteria survived. The experimenters then monitored formate metabolism using in-situ Raman spectroscopy and showed that formate metabolism continued in the bacterial sample.\n\nMoreover, 1.6 GPa is such great pressure that during the experiment the DAC turned the solution into ice-IV, a room-temperature ice. When the bacteria broke down the formate in the ice, liquid pockets would form because of the chemical reaction.\n\nThere was some skepticism of this experiment. According to Art Yayanos, an oceanographer at the Scripps Institute of Oceanography, an organism should only be considered living if it can reproduce. Another issue with the DAC experiment is that when high pressures occur, there are usually high temperatures present as well, but in this experiment there were not. This experiment was performed at room-temperature. However, the intentional lack of high temperature in the experiments isolated the actual effects of pressure on life and results clearly indicated life to be largely pressure insensitive.\n\nNewer results from independent research groups have confirmed Sharma et al. (2002). This is a significant step that reiterates the need for a new approach to the old problem of studying environmental extremes through experiments. There is practically no debate whether microbial life can survive pressures up to 600 MPa, which has been shown over the last decade or so to be valid through a number of scattered publications.\n\nIn the consumer studies of Hightech Europe consumers mentioned more positive than negative associations descriptions for this technology showing that these products are well accepted.\n\n\n"}
{"id": "23851721", "url": "https://en.wikipedia.org/wiki?curid=23851721", "title": "Petoscope", "text": "Petoscope\n\nA Petoscope is an optoelectronic device for detecting small, distant objects such as flying aircraft. The design, as described in 1936, consisted of an instrument with two parallel light paths. In each path was a collimating objective lens, a screen marked with many small, alternating opaque and transparent squares in a chequerboard pattern, and a second concentrating lens focused on a photocell. The two screens were inverted with respect to each other. This caused a small object in the instrument's field of view to produce differing signals in the two photocells, while a large object affected both light paths equally. The difference between the two signals was amplified and used to raise an alarm. At the beginning of World War II, the device was adapted for use in proximity fuses for bombs.\n\nThe inventor was Alan S. Fitzgerald of Wynnewood, Pennsylvania, U.S.A., a Research Associate in Electrical Engineering at Swarthmore College.\n"}
{"id": "53366", "url": "https://en.wikipedia.org/wiki?curid=53366", "title": "Reconnaissance satellite", "text": "Reconnaissance satellite\n\nA reconnaissance satellite (commonly, although unofficially, referred to as a spy satellite) is an Earth observation satellite or communications satellite deployed for military or intelligence applications.\n\nThe first generation type (i.e., Corona\nand Zenit) took photographs, then ejected canisters of photographic film which would descend to earth. Corona capsules were retrieved in mid-air as they floated down on parachutes. Later, spacecraft had digital imaging systems and downloaded the images via encrypted radio links.\n\nIn the United States, most information available is on programs that existed up to 1972, as this information has been declassified due to its age. Some information about programs prior to that time is still classified, and a small amount of information is available on subsequent missions.\n\nA few up-to-date reconnaissance satellite images have been declassified on occasion, or leaked, as in the case of KH-11 photographs which were sent to \"Jane's Defence Weekly\" in 1984.\n\nOn 16 March 1955, the United States Air Force officially ordered the development of an advanced reconnaissance satellite to provide continuous surveillance of \"preselected areas of the Earth\" in order \"to determine the status of a potential enemy’s war-making capability\".\n\nThere are several major types of reconnaissance satellite.\n\n\n\n\n\n\nExamples of reconnaissance satellite missions:\n\nOn 28 August 2013, it was thought that \"a $1-billion high-powered spy satellite capable of snapping pictures detailed enough to distinguish the make and model of an automobile hundreds of miles below\" was launched from California's Vandenberg Air Force Base using a Delta IV Heavy launcher, America's highest-payload space launch vehicle.\n\nOn 17 February 2014, a Russian Kosmos-1220 originally launched in 1980 and used for naval missile targeting until 1982, made an uncontrolled atmospheric entry.\n\nReconnaissance satellites have been used to enforce human rights, through the Satellite Sentinel Project, which monitors atrocities in Sudan and South Sudan.\n\nDuring his 1980 State of the Union Address, President Jimmy Carter explained how all of humanity benefited from the presence of American spy satellites:\n\nAdditionally, companies such as GeoEye and DigitalGlobe have provided commercial satellite imagery in support of natural disaster response and humanitarian missions.\n\nDuring the 1950s, a Soviet hoax had led to American fears of a bomber gap. In 1968, after gaining satellite photography, the United States' intelligence agencies were able to state with certainty that \"No new ICBM complexes have been established in the USSR during the past year.\" President Lyndon B. Johnson told a gathering in 1967:\n\nSpy satellites are commonly seen in spy fiction and military fiction. Some works of fiction that focus specifically on spy satellites include:\n\n\n\n"}
{"id": "645397", "url": "https://en.wikipedia.org/wiki?curid=645397", "title": "Rockland Community College", "text": "Rockland Community College\n\nRockland Community College is a community college in Ramapo, New York. It is part of the State University of New York. The college, established in 1959, became the 18th community college to join the SUNY system. The college offers 51 programs and offers associate degrees and certificates. Additionally, students can earn other degrees, including Bachelor of Arts, Bachelor of Science, and Master of Arts in the arts and sciences, Doctoral Program in Executive Leadership (EdD), technology, and health professions while attending classes at Rockland through articulation programs with four-year schools. The current enrollment is about 7,500 full and part-time students which includes about 125 International students from more than 50 foreign countries and approximately 800 high school enroll in credit courses.\n\nThe main campus is in Suffern, New York, but instructions are also offered at an extension sites in Haverstraw and Orangeburg. The Spring Valley satellite campus has been discontinued. It was located in the historic North Main Street School.\n\nThe college has more than 525 full - and part-time faculty members, including several Fulbright Scholars, SUNY Chancellor's Award winners, and published authors and artists. The faculty-student ratio is 22:1. Rockland has the third highest transfer rate in the SUNY system and has a Continuing Education programs which served about 3,500 each year.\n\nAn institution called Rockland College, chartered by the state Board of Regents in 1878, existed for sixteen years in Nyack, New York.\n\nRockland Junior College, supported by federal funds disbursed through New York State, and sponsored by Nyack High School was established in 1932 as one of several depression-era two-year schools. New York University and Syracuse University accepted two years of credit from the college. Rockland Junior College shut down in 1935.\n\nRockland Community College came eighteen years and later was organized to be an affordable, two-year college in location convenient for county residents; it was planned that it would raise taxes by only $4 a year. At the time, Rockland County, the state's smallest in geographic area outside of New York City, was growing exponentially in population and in demand for a skilled, educated work force. Between 1956 and 1970, Rockland's population was one of the fastest growing in the state, expected to double from 107,000 to 215,000 and the number of high school graduates was projected to rise from 700 to 2,463.\n\nLarge local industries like Avon Products in Suffern and Lederle Laboratories in Pearl River required more skilled workers, and the growth of hospitals such as Nyack Hospital and Good Samaritan Hospital in Suffern warranted the creation of a nursing program.\n\nSome 69 percent of parents polled expressed interest in their children attending a community college in Rockland, and 183 high school juniors indicated a strong interest in and an ability to attend a community college in Rockland.\n\n\nOn June 11, 1961, the college's first commencement exercises honored 39 graduates—22 men, 17 women—who had finished the journey begun by 139 full-time students two years before. In 1962 there were 60 graduates, and in 1963, 115, including the first 24 from the school's nursing program.\n\nOn May 23, 2010, the college's 50th commencement exercises honored 350 graduates.\n\nOn October 3, 2013, the United States Department of Veterans Affairs hosted the first induction of RCC students who are veterans into SALUTE - Veterans National Honors Society. SALUTE, established in 2008 is headquartered out of Colorado State University has over 90 chapters in colleges and universities across the country. RCC is the first community college to have a chapter in this prestigious organization.\n\nIn the 1970, concerts and big-ticket events were held in the Eugene Levy Field house. Several major artist included, Billy Joel, Earth, Wind & Fire, Genesis, Meat Loaf, Styx and The Monkees.\n\nThe main campus and main entrance on Almshouse Road is located on the crest of a sloping rise in a former farm community known as Mechanicsville, renamed Viola when a post office was established in 1882. The original property included:\n\n\nThe current campus evolved with these milestones:\n\n\n\nA cemetery was established on the property for almshouse residents, and shortly after the property was designated for the college's campus, the county designated a tract to serve as a veterans cemetery. The Korean War Monument is large granite rock at the Gary Onderdonk Rockland Veterans Cemetery on Rockland Community College's campus with a plaque bearing the names of all 27 Rocklanders killed in action during the Korean War, with the inscription: \"They gave their today for your tomorrow.\"\n\nThe aesthetics of the campus have been enhanced by more than a dozen sculptures that came to the College through the Model to Monuments Outdoor Sculpture Program developed by world-class sculptor, Greg Wyatt, Sculptor-in-Residence at the Cathedral of Saint John the Divine, NYC. Greg, in collaboration with the Art Students League, received a substantial grant to work with aspiring sculptors to create models to enhance NYC parks. After being displayed in parks in NYC for two years, the sculptures now have a permanent home at Rockland Community College. The College also installed, through the Art in Public Places program, a neon sculpture, TRILOGY, created by the extraordinary Stephen Antonakos.\n\nRCC is sponsored by the County of Rockland and operating and administered by a ten-member Rockland Community College Board of Trustees that is appointed pursuant to New York State Education Law § 6306. Nine of the Trustees serve seven-year terms, with five of those appointed by the County and four by the Governor. The tenth trustee is a voting, student representative. The board in turn appoints a President who hires and supervises the staff. RCC is a community college unit of the State University of New York and is also subject to regulation and visitation by the Regents of the University of the State of New York.\n\nMiddle States Commission on Higher Education \n\n\nRockland Community College offers 40 associate degrees and 11 one-year certificate programs.\n\nIn 2015 Rockland Community College was ranked as the eighth best community colleges in the state of New York by College.com.\n\nIn 2016 Rockland Community College was ranked as the tenth best community colleges among the 121 two year community colleges in the state of New York by EDsmart.com.\n\nin 2017 Rockland Community College partnered with College Steps, a non-profit group which provides hands-on and personalized college support for students with learning and social challenges, building a foundation for success in high school, college, preparing students for meaningful careers and life.\n\nIn 2017 Rockland Community College and Rockland BOCES merged their adult education programs making Rockland County the first in the Lower Hudson Valley to merge its adult programming with a SUNY school through a new partnership named RACE (Rockland Adult and Community Education), which aims to create a one-stop shop for those looking to pursue training, continuing education or professional certification.\n\nIn 2017 Rockland Community College was ranked the best community college in New York for adult learners in Washington Monthly’s recent annual ranking of American colleges and universities. Rockland Community College rated twenty-eighth nationwide out of nearly 1,500 two-year colleges.\n\nIn 2018 Rockland Community College was ranked the seventh best community college in New York by BestColleges.com \n\nIn 2018 Rockland Community College was name the fifth Campus Pride’s 2018 Best of the Best LGBTQ-Friendly Colleges & Universities \n\nAlthough Rockland Community College does not confer four-year degrees, it cooperates with 4-year institutions:\n\n\n\n\nStudents can obtain their master's in Early Childhood Education and Early Childhood Special Education at SUNY Rockland through a program co-sponsored by The College of New Rochelle (CNR).\n\nSince 2016, students are registering for graduate degrees from LIU Hudson by enrolling in evening and weekend courses at the Rockland Community College main campus.\n\nIn September 2014 a Doctoral Program in Executive Leadership (EdD), offered through St. John Fisher College was the first doctoral program at SUNY Rockland\n\nRockland Community College continuing education programs include courses for\n\nCPA Review,\nClinical Medical Assistant with clinical externship,\nDental assistant,\nEKG Technician Certification,\nMedical Administrative Assistant,\nMedical billing,\nMedical classification,\nNYS Life, Accident & Health Insurance Pre-Licensing Course\nOphthalmic Assistant,\nPharmacy technician Phlebotomy,\nTax Update and Review\n\nThe Rockland Community College High School Program began in 2010. Rockland Community College academic departments have oversight of the curriculum, textbooks and student assessments offered at the high schools. High school teachers who are college adjuncts teach the courses at the high schools. Rockland Community College participates in the New York Concurrent Enrollment Partnerships (NYCEP), which enables the College to network and share strategies of concurrent enrollment programs offered throughout SUNY community colleges. The program continues to grow yearly.\n\nThe Samuel Draper Mentored/Talented Students and Management Development programs are nationally acclaimed, rigorous academic programs for liberal arts and business students seeking to transfer to premier colleges. Graduates transfer to such universities as \nColumbia,\nCornell,\nDuke,\nFordham,\nGeorgetown, \nHavard, \nUniversity of Pennsylvania, \nNYU, \nStanford,\nWesleyan,\nYale and\nSmith College.\nThe program was awarded a FIPSE (Fund for the Improvement of Postsecondary Education) grant from the United States Department of Education to serve as a model for community college honors programs throughout the country. In 2012 Rockland Community College was ranked as the Second Best Community College in the USA by the website Community College Transfer Student.\n\nStudents must be at least 18 years of age and meet the requirement of a minimum 3.0 GPA and two faculty recommendations when classes begin at Cambridge University in July or August. Students can be from any college but must apply through the RCC Sam Draper M/TS Honors Program Office in Spring to be eligible.\n\nThames Valley University offers a range of undergraduate degree programs that are validated by the Council for National Academic Awards (CNAA)\n\nRockland Community College as a member in the College Consortium for International Studies (CCIS), offers students the opportunity to spend a semester in over 25 countries including Argentina, Australia, Chile, China, Costa Rica, England, France, Greece, Hungary, Ireland, Israel, Italy, Mexico, Morocco, Northern Ireland, Portugal, Scotland, and Spain. A student need not be fluent in a foreign language to participate as most instructions are in English.\n\nRockland Community College in recent years has participated in study abroad tours in China, England, France, Greece and Italy.\n\nRockland Community College has articulation (transfer) agreements with over four dozen colleges including these and some previously mentioned.\nThe Academic Success Center (ASC) formerly known as Centers for Academic Progress and Success (CAPS) includes Reading & Writing, Tutoring, Testing and Science Learning centers which provide academic assistance free to all Rockland Community College students.\n\nRockland Community College is an Authorized American Heart Association (AHA) Training Center.\n\nAT&T's special program grant to the STEM Exploratory Research Fund through the Rockland Community Foundation brings High School Students to learn about STEM (Science, Technology, Engineering and Mathematics) Education and Careers.\n\nThe CETL program provides outstanding professional programs and services to faculty and staff as well as one of America's premier psychologists, Dr. Edmund W. Gordon, as scholar-in-residence. Professional development workshops on a wide variety of topics are held throughout the academic year with a full program of workshops offered during January and May.\n\nRockland Community College offers a continuing Education program which trains students as a medical assistant and prepares student for a career in health care. Students who successfully complete the program are eligible to sit for the National Healthcareer Association (NHA) Certified Clinical Medical Assistant (CCMA) examination.\n\nRockland Community College in 2018 offered a New Construction Management Course which met the Light Construction science general education requirements.\n\nOn March 8, 2017, Rockland Community College was designated as a National Center of Academic Excellence in Cyber Defense Two Year Education (CAE2Y) by the National Security Agency (NSA) and Department of Homeland Security (DHS) becoming just the second SUNY Community College to earn this prestigious designation, Mohawk Valley Community College was certified in 2016. RCC’s certificate will be awarded during the 9th Annual Cyber Summit, June 6–8 in Huntsville, Alabama.\n\nRockland Community College recently signed an affiliation agreement with the University at Buffalo to establish the accelerated program where students can now obtain a Doctor of Pharmacy (PharmD)degree.\n\nRockland Community College students can now obtain a certification by CompTIA, the industry standard for computer support technicians. \n\nOur Homeland Security and Domestic Preparedness is part of The Hudson Valley Educational Consortium which is a collaboration with Dutchess, Orange, Ulster, Sullivan and Westchester Community Colleges providing broader access to academic programs and workforce training throughout the four county region.\n\nRockland Community College became first New York Community College to Receive Information Assurance Course Validation from the Committee on National Security Systems during the June 2008 CNSS Awards Ceremony held at the 12th Colloquium for Information Systems Security Education.\n\nMore than 125 international students are enrolled at Rockland Community College yearly.\n\nRockland Community College is ranked number one in the country among two-year institutions for sending students on international study programs.\n\nSUNY Rockland Community College offers nine \"living\" languages which have been approved by the State University of New York (SUNY) – the most of any community college in New York State, which meet the Foreign Language General Education requirement and are mandatory for students seeking a bachelor's degree from a SUNY school. These include;\n\nRockland Community College now offers the nation’s first Maritime Studies Program works with a US Coast Guard-approved partner featuring online training with hands-on classroom instruction at the College’s campuses. Through the RCC/Learn America program, Mariners seeking work on board any vessel are required to complete a Coast Guard-approved Basic Training course. Students also are able to sign up for any of the individual elements of the Basic Training course. This unique program can lead to a career on the high seas—or in a variety of other maritime settings. Maritime officials say the U.S. will need thousands of new merchant marine workers.\n\nRockland Community College is offerS a 90-hour program to train students to collect blood specimens from clients for the purpose of laboratory analysis. Once students complete the course, graduates will have the opportunity to pursue the National Healthcareer Association Phlebotomy Technician Exam and become phlebotomy technicians.\n\nThe course provides the opportunity to begin preparation for a career as a local police officer prior to being hired by a law enforcement agency. A Civil service exam which students will be required to pass in order to be hired by a law enforcement agency, will be given upon completion of the course work at RCC. A candidate from the civil service list who has completed the coursework is more likely to be chosen by an agency, as he or she will not need to attend a twenty-three-week academy.\n\nRockland Community College is a participating campus of the SUNY COIL Center's Nodal Network.\n\nSenior Adult Audits – Seniors over the age of 60 are eligible to take credit courses free of charge on a space available basis. The seniors are responsible for program fees, are required to meet pre-requisites and cannot enroll in contract courses.\n\nInstitute for Senior Education (I.S.E.) – The College's Institute for Senior Education (ISE) offers courses during the College's regular semesters at modest prices. These informal groups meet once a week and stress independent study within a curriculum adjusted to the needs and interests of the group's members. All ISE courses are open to everyone over 50 years old.\n\nIn 2016 Rockland Community College opened The 211 Connection Center which connects students to off-campus resources regarding health and human services such as housing, childcare, food, utilities and legal because \"life happens\".\n\n\nRockland Community College is also the training camp site for the Rockland Boulders, a member of the Canadian American Association of Professional Baseball who play their home games at the new 4,350-seat/16 suite Provident Bank Park in Pomona, New York.\n\nNestled in the hills of Rockland County/Facing the Eastern sky/Rockland Community College/Stands loyal, firm and true./No matter where we roam/No matter where we be/We will always remember RCC/We will always remember RCC!\n\nThe presidential chain of office of Rockland Community College showing the academic programs of Liberal Arts, Allied Health Sciences, Engineering, Performing Arts, Nursing and Natural Sciences in front going left to right is approximately forty-eight inches long and is made of sterling silver with a medallion of sterling silver, partly gold-plated and green acrylic featuring ten frames, each illustrating one of the academic programs, alternating with round links made up of concentric initials \"R.C.C\" along with a display of the Map of Rockland County on the right side of the circle.\n\nThe ceremonial mace, a gift from the Rockland Community College Foundation is a mace that serves as the official symbol of the institution of higher education. At its crown is a bronze medallion displaying the Seal of Rockland County on one side and the Seal of New York on the obverse side. Beneath the crown is a sterling silver globe with the inscription \"Rockland Community College, 1959\" in thick metal letters around it. The wood staff is sculpted from decorative Purple Heart wood in a non=glossy oil finish and the mace's counterbalance is made of copper and bronze in a dark patina.\n\n\n"}
{"id": "26068547", "url": "https://en.wikipedia.org/wiki?curid=26068547", "title": "RunCore", "text": "RunCore\n\nRuncore (\"RunCore Innovation Technology Co., Ltd. \") is a solid state drive research, development, and production company founded in 1999. in China. The company has subsidiaries in Shanghai and California (both named \"Runcore Co., Ltd.\") \n\n"}
{"id": "28005288", "url": "https://en.wikipedia.org/wiki?curid=28005288", "title": "SHAPE Services", "text": "SHAPE Services\n\nSHAPE.AG (formerly SHAPE Services) is a cross-platform independent software vendor and web-based services provider. The company develops instant messaging, social networking, productivity, entertainment, games, media and location-based applications for Apple iPhone, iPod Touch and iPad, BlackBerry, Windows Phone/Windows Mobile, Android, Symbian S60, UIQ, J2ME, and HP/Palm webOS mobile platforms.\n\nSHAPE.AG (with AG standing for Apps&Games )is a worldwide operating company headquartered in Stuttgart, Germany and offices in Germany and Ukraine. The company was founded in 2002 and as of 2011 had more than 60 employees.\nIn May 2008 SHAPE acquired Warelex LLC, the US developer of multimedia applications and technologies for mobile devices. \nIn July 2011 SHAPE has agreed to acquire Crisp App, the Hong Kong-based developer of the fone app for iOS. \nIn 2011 the company added location-based instant messaging service Neighbors into IM+ application. \nIn 2012 SHAPE raised $10 million from Russian investment firm Finam.\nIn March 2012 SHAPE officially changed its name from SHAPE Services to SHAPE.AG (AG stands for Apps&Games ).\n\nThe company's best-known software products include:\n\n\n"}
{"id": "16374548", "url": "https://en.wikipedia.org/wiki?curid=16374548", "title": "School website", "text": "School website\n\nA school website is any website built, designed, and maintained by or for a school. Many school websites share certain characteristics, and some educators have developed guidelines to help schools create the best and most useful websites they can.\n\nPossible functions of a school website include:\n\nThere are various ways in which a school can approach the task of creating a website. Because they are educational institutions, the creation of portions of the website can be incorporated into the curriculum. Such innovation is gathering momentum in schools as educators become themselves more familiar with the technologies involved.\n\n\nSchool web sites, especially in the public sector, are generally under-developed. Lack of expertise among non-teaching staff has been a contributing factor, and many institutions have not acted to make their web presence a priority; these schools perceive themselves as 'stretched' with respect to budgets and time. Requisite skills are sometimes available in the teaching staff, but the investment of their time is not adequately compensated, as stated above.\nThis general lack of commitment by schools to the internet reflects a limited perception of its capacity to provide educational outcomes for the school community. The 'school website' is viewed as a public face for the school - a more sophisticated newsletter or advertisement. In that regard it is not prioritized. There is now burgeoning interest in the tangible learning benefits a website can generate for a school, and the capacity a website affords for the streamlining of information access within and between faculties, schools and educational departments. Traditionally isolated teachers/faculties are able to network ideas and resources in ways that afford real professional development.\n\nA school website is a communication tool between the school and community; however, its reach can extend beyond country, state lines. A school website may be used by many demographic groups such as staff/administration, parents, students, community, and potential, as well as, former students and alumni. Even across state lines a school website can connect groups of its demographics such as the alumni now living cross country looking for information on the next reunion.\n\nE-safety should be addressed in the classroom, both directly within computing lessons and when it comes up using information technology in other subjects. Try to reinforce safe online behaviour on an ongoing basis. The ubiquity of information technology means that messages about its use need to be frequent to have an impact.\n\nAll pupils are different. Attitudes at home towards e-safety will vary hugely within a class, as will the experiences of children among their peers. It’s therefore important to start by finding out what pupils know, what they do online, and what is allowed at home. This can be a useful place for starting a discussion about what happens online.\n\nWherever they’re starting from, children need to learn certain key skills. How to filter their own online activity so as to not give away personal information, filter out certain types of incoming messages, and report inappropriate or illegal behaviour are vital skills for children going online. Recognising and responding to inappropriate behaviour is about more than inappropriate approaches from adults. Social media and online gaming, while potentially incredible outlets for creativity and self-expression, also contain some subcultures mired in bullying and prejudice. Learning to deal with this safely is a valuable life skill.\n"}
{"id": "3561981", "url": "https://en.wikipedia.org/wiki?curid=3561981", "title": "Scottish inventions and discoveries", "text": "Scottish inventions and discoveries\n\nScottish inventions and discoveries are objects, processes or techniques either partially or entirely invented, innovated or discovered by a person born in or descended from Scotland. In some cases, an invention's Scottishness is determined by the fact that it came into existence in Scotland (e.g., animal cloning), by non-Scots working in the country. Often, things that are discovered for the first time are also called \"inventions\" and in many cases there is no clear line between the two.\n\nThe Scots take enormous pride in the history of Scottish invention and discovery. There are many books devoted solely to the subject, as well as scores of websites listing Scottish inventions and discoveries with varying degrees of science.\n\nEven before the Industrial Revolution, Scots have been at the forefront of innovation and discovery across a wide range of spheres. Some of the most significant products of Scottish ingenuity include James Watt's steam engine, improving on that of Thomas Newcomen, the bicycle, macadamisation (not to be confused with tarmac or tarmacadam), Alexander Graham Bell's invention of the first practical telephone, John Logie Baird's invention of television, Alexander Fleming's discovery of penicillin, and insulin.\n\nThe following is a list of inventions, innovations or discoveries that are known or generally recognised as being Scottish.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first positive displacement liquid flowmeter, the reciprocating piston meter by Thomas Kennedy Snr.\n\nScots have been instrumental in the invention and early development of several sports:\n\n\n\npaintball gun\n\n\n\n\n\n"}
{"id": "1804451", "url": "https://en.wikipedia.org/wiki?curid=1804451", "title": "Sir Frank Whittle Medal", "text": "Sir Frank Whittle Medal\n\nThe Sir Frank Whittle Medal is awarded annually by the Royal Academy of Engineering to an engineer,\nnormally resident in the United Kingdom, for outstanding and sustained achievement which has contributed to the well-being of the nation. The field of activity in which the medal is awarded changes annually.\n\nNamed after Sir Frank Whittle, the award was instituted in 2001.\n\nPrevious winners:\n"}
{"id": "32721695", "url": "https://en.wikipedia.org/wiki?curid=32721695", "title": "Spacecraft thermal control", "text": "Spacecraft thermal control\n\nIn spacecraft design, the function of the thermal control system (TCS) is to keep all the spacecraft's component systems within acceptable temperature ranges during all mission phases. It must cope with the external environment, which can vary in a wide range as the spacecraft is exposed to deep space or to solar or planetary flux, and with ejecting to space the internal heat generated by the operation of the spacecraft itself.\n\nThermal control is essential to guarantee the optimum performance and success of the mission because if a component is subjected to temperatures which are too high or too low, it could be damaged or its performance could be severely affected. Thermal control is also necessary to keep specific components (such as optical sensors, atomic clocks, etc.) within a specified temperature stability requirement, to ensure that they perform as efficiently as possible. \nThe thermal control subsystem can be composed both of passive and of active items and works in two ways:\nPassive Thermal Control System (PTCS) components include:\nActive Thermal Control System (ATCS) components include:\n\n\nFor a spacecraft the main environmental interactions are the energy coming from the sun and the heat radiated to deep space. Other parameters also influence the thermal control system design such as the spacecraft’s altitude, orbit, attitude stabilization, and spacecraft shape. Different types of orbit, such as low earth orbit and geostationary orbit, also affect the design of the thermal control system. \n\nThe temperature requirements of the instruments and equipment on board are the main factors in the design of the thermal control system. The goal of the TCS is to keep all the instruments working within their allowable temperature range. All of the electronic instruments on board the spacecraft, such as cameras, data collection devices, batteries, etc., have a fixed operating temperature range. Keeping these instruments in their optimal operational temperature range is crucial for every mission. Some examples of temperature ranges include\n\nCoatings are the simplest and least expensive of the TCS techniques. A coating may be paint or a more sophisticated chemical applied to the surfaces of the spacecraft to lower or increase heat transfer. The characteristics of the type of coating depends on their absorptivity, emissivity, transparency, and reflectivity. The main disadvantage of coating is that it degrades quickly due to the operating environment.\n\nMultilayer insulation (MLI) is the most common passive thermal control element used on spacecrafts. MLI prevent both heat losses to the environment and excessive heating from the environment. Spacecraft components such as propellant tanks, propellant lines, batteries, and solid rocket motors are also covered in MLI blankets to maintain ideal operating temperature. MLI consist of an outer cover layer, interior layer, and an inner cover layer. The outer cover layer needs to be opaque to sunlight, generate a low amount of particulate contaminates, and be able to survive in the environment and temperature to which the spacecraft will be exposed. Some common materials used for the outer layer are fiberglass woven cloth impregnated with PTFE Teflon, PVF reinforced with Nomex bonded with polyester adhesive, and FEP Teflon. The general requirement for the interior layer is that it needs to have a low emittance. The most commonly used material for this layer is Mylar that is aluminized on one or both sides. The interior layers are usually thin compared to the outer layer to save weight and are perforated to aid in venting trapped air during launch. The inner cover faces the spacecraft hardware and is used to protect the thin interior layers. Inner covers are often not aluminized in order to prevent electrical shorts. Some materials used for the inner covers are Dacron and Nomex Netting. Mylar is not used because of flammability concerns. MLI blankets are an important element of the thermal control system.\n\nLouvers are active thermal control elements that are used in many different forms. Most commonly they are placed over external radiators, louvers can also be used to control heat transfer between internal spacecraft surfaces, or be placed on openings on the spacecraft walls. A louver in its fully open state can reject six times as much heat as it does in its fully closed state, with no power required to operate it. The most commonly used louver is the bimetallic, spring-actuated, rectangular blade louver also known as venetian-blind louver. Louver radiator assemblies consist of five main elements: baseplate, blades, actuators, sensing elements, and structural elements.\n\nHeaters are used in thermal control design to protect components under cold-case environmental conditions or to make up for heat that is not dissipated. Heaters are used with thermostats or solid-state controllers to provide exact temperature control of a particular component. Another common use for heaters is to warm up components to their minimum operating temperatures before the components are turned on. \n\nExcess waste heat created on the spacecraft is rejected to space by the use of radiators. Radiators come in several different forms, such as spacecraft structural panels, flat-plate radiators mounted to the side of the spacecraft, and panels deployed after the spacecraft is on orbit. Whatever the configuration, all radiators reject heat by infrared (IR) radiation from their surfaces. The radiating power depends on the surface's emittance and temperature. The radiator must reject both the spacecraft waste heat and any radiant-heat loads from the environment. Most radiators are therefore given surface finishes with high IR emittance to maximize heat rejection and low solar absorptance to limit heat from the sun. Most spacecraft radiators reject between 100 and 350 W of internally generated electronics waste heat per square meter. Radiators weight typically varies from almost nothing, if an existing structural panel is used as a radiator, to around 12 kg/m for a heavy deployable radiator and its support structure.\n\nThe radiators of the International Space Station are clearly visible as arrays of white square panels attached to the main truss.\n\nHeat pipes use a closed two-phase liquid-flow cycle with an evaporator and a condenser to transport relatively large quantities of heat from one location to another without electrical power.\n\n\nA major event in the field of space thermal control is the International Conference on Environmental Systems, organized every year by AIAA.\n\nIn spacecraft design, a sun shield restricts or reduces heat caused by sunlight hitting a spacecraft. An example of use of a thermal shield is on the Infrared Space Observatory. The ISO sunshield helped protect the cryostat from sunlight, and it was also covered with solar panels.\n\nNot to be confused with concept of a global scale sun shield in geoengineering, often called a Space sunshade or \"sun shield\", in that case the spacecraft itself is used to block sunlight on a planet, not as part the spacecraft's thermal design.\n\nAn example of a sunshield in spacecraft design is the Sunshield (JWST) on the planned James Webb Space Telescope.\n\n\n"}
{"id": "1267179", "url": "https://en.wikipedia.org/wiki?curid=1267179", "title": "Spindizzy", "text": "Spindizzy\n\nThe Dillon-Wagoner Graviton Polarity Generator, known colloquially as the spindizzy, is a fictitious anti-gravity device imagined by James Blish for his series \"Cities in Flight\". This device grows more efficient with the amount of mass being lifted, which was used as the hook for the stories—it was more effective to lift an entire city than it was to lift something smaller, such as a classic spaceship. This is taken to extremes in the final stories, where an entire planet is used to cross the galaxy in a matter of hours using the spindizzy drive.\n\nAccording to the stories, the spindizzy is based on principles contained in an equation coined by P.M.S. Blackett, a British physicist of the mid-20th century. Several other Blish stories involving novel space drives contain the same assertion. Blackett's original formula was an attempt to correlate the known magnetic fields of large rotating bodies, such as the Sun, Earth, and a star in Cygnus whose field had been measured indirectly. It was unusual in that it brought Isaac Newton's gravitational constant and Coulomb's constant together, the one governing forces between masses, the other governing forces between electric charges. However, it was later disproved by more accurate measurements, and by new discoveries such as magnetic field reversals on Earth and the Sun, and the lack of a magnetic field on bodies such as Mars, despite its rotation being similar to Earth's.\n\nBlish's extrapolation was that if rotation combined with mass produces magnetism via gravity, then rotation and magnetism could produce anti-gravity. The field created by a spindizzy is described as altering the magnetic moment of any atom within its influence.\n\nThe spindizzy was also used in at least two novels by Jesse Franklin Bone, \"The Lani People\" and \"Confederation Matador\" and appears as the nickname for fictional Heim theory devices in Ken MacLeod's \"The Execution Channel\".\n\n\n"}
{"id": "4237693", "url": "https://en.wikipedia.org/wiki?curid=4237693", "title": "Spurline", "text": "Spurline\n\nThe spurline is a type of radio-frequency and microwave distributed element filter with band-stop (notch) characteristics, most commonly used with microstrip transmission lines. Spurlines usually exhibit moderate to narrow-band rejection, at about 10% around the central frequency.\n\nSpurline filters are very convenient for dense integrated circuits because of their inherently compact design and ease of integration: they occupy surface that corresponds only to a quarter-wavelength transmission line.\n\nIt consists of a normal microstrip line breaking into a pair of smaller coupled lines that rejoin after a quarter-wavelength distance. Only one of the input ports of the coupled lines is connected to the feed microstrip, as shown in the figure below. The orange area of the illustration is the microstrip transmission line conductor and the gray color the exposed dielectric.\n\nWhere formula_1 is the wavelength corresponding to the central rejection frequency of the bandstop filter, measured - of course - in the microstrip line material. This is the most important parameter of the filter that sets the rejection band.\n\nThe distance between the two coupled lines can be selected appropriately to fine-tune the filter. The smaller the distance, the narrower the stop-band in terms of rejection. Of course that is limited by the circuit-board printing resolution, and it is usually considered at about 10% of the input microstrip width.\n\nThe gap between the input microstrip line and the one open-circuited line of the coupler has a negligible effect on the frequency response of the filter. Therefore, it is considered approximately equal to the distance of the two coupled lines.\n\nSpurlines can also be used in printed antennae such as the planar inverted-F antenna. The additional resonances can be designed to widen the antenna bandwidth or to create multiple bands, for instance, for a tri-band mobile phone.\n\nA spurline filter was first proposed by Schiffman and Matthaei in stripline form in 1964. Bates adapted the design for microstrip in 1977. Nguyen and Hsieh improved the analysis for microstrip implementations in 1983.\n\n\n"}
{"id": "30229492", "url": "https://en.wikipedia.org/wiki?curid=30229492", "title": "StudyPoint", "text": "StudyPoint\n\nStudyPoint is a for-profit American company that provides academic and test preparation tutoring to students in grades K-12. It offers preparation for standardized aptitude tests such as the SAT, ACT, PSAT, and SAT Subject Tests, as well as tutoring for academic subjects such as math, biology, physics, Spanish, and writing. Headquartered in Stoneham, Massachusetts, StudyPoint employs 650 tutors and operates in 23 metropolitan areas, including New York City, Chicago, Washington, DC, Philadelphia, San Francisco, and Los Angeles. \n\nStudyPoint uses a one-to-one, in-home tutoring model that incorporates individual needs assessment and online homework tools. The company developed its own customer relationship management (CRM) and dashboard software to track sales data and student-progress metrics such as SAT score improvement and homework completion.\n\nStudyPoint (originally StudySmart, Inc.) was founded in Boston in 1999 by two Cornell University graduates, Richard Enos and Greg Zumas. In 2007 StudyPoint was profiled in a New York Times article about small-business performance in a slowing economy. It has been named seven times by Inc. magazine as one of the 5,000 fastest-growing companies in the United States.\n\n"}
{"id": "6990805", "url": "https://en.wikipedia.org/wiki?curid=6990805", "title": "Topwater fishing lure", "text": "Topwater fishing lure\n\nA Topwater fishing lure is a type of fishing lure, usually floating, that may be moved about the surface of water in order to attract and cause fish to attempt to strike the lure. Non-floating versions may be retrieved at sufficient speed to cause them to travel at the water's surface.\n\nSuch lures are often designed to resemble smaller creatures that would normally be considered as food for the target fish species. (They are painted to look like the prey of the target species; usually smaller fish, frogs, or insects.) One of the key features of the topwater lure is the \"action\" that it imparts as it travels along the water's surface. The more effective lures have an action that closely resembles that of the actual living creature. The lure is typically fitted with one of more fish hooks (usually treble hooks) to hook the target fish as it strikes the lure. Variations exist that include internal rattles to generate sound that might be similar to the sounds created by the actual, live creature being emulated. some also include small light sources such as LEDs that might be battery powered. There are also jointed bodies, moving eyes, holographic finishes, etc. all of which are incorporated to encourage the target species to strike the lure. The lure is normally attached to the end of a fishing line that is attached to a fishing rod and reel and is cast into areas where the target species might be found and \"worked\" skillfully within that area to encourage strikes. This type of fishing is considered by many to be one of the more exciting methods used to catch fish. A frequent mistake when fishing topwater lures is to initiate the hookset immediately upon seeing the fish strike the lure. In many species, especially bass, it is important to wait a few seconds before initiating the hookset to ensure that the lure is in the best position in the fish's mouth to optimize the chances of a successful hooking. Black bass, spotted seatrout, ladyfish, redfish, bluefish, tarpon, bonefish, barracuda, & pickerel are examples of fish that might be taken by the topwater approach.\n"}
{"id": "3294549", "url": "https://en.wikipedia.org/wiki?curid=3294549", "title": "Traverse board", "text": "Traverse board\n\nThe traverse board is a memory aid formerly used in dead reckoning navigation to easily record the speeds and directions sailed during a watch. Even crew members who could not read or write could use the traverse board.\n\nAs the mathematician William Bourne remarked in 1571, “I have known within these 20 years that them that were ancient masters of shippes hathe derided and mocked them that have occupied their cards and plattes and also the observation of the Altitude of the Pole saying; that they care not for their sheepskinne for he could keepe a better account upon a board.” \n\nBourne’s ‘old salt’ is talking about a traverse board, a wooden board with a compass rose drawn on it linked by pegs and cords to a series of peg holes beneath it. It allowed a helmsman to keep a rough check of the time sailed on each rhumb of the wind.\n\nThe traverse board is a wooden board with peg-holes and attached pegs. It is divided into two parts, upper and lower.\n\nThe top part is for recording direction sailed. It has a representation of the compass rose with its 32 compass points, just as on the face of the ship's compass. Eight concentric rings are inscribed on the compass rose. Each ring has one peg hole at each point of the compass. Eight pegs are attached to the centre of the compass rose with strings.\n\nThe bottom part is for recording speed. It has four rows of holes. Each column represents a certain speed, measured in knots. Three more columns to the right give fractional knots: , , and . Eight pegs are attached to this part of the board.\n\nEach half-hour during the watch, a crew member inserted a peg in the top part of the board to represent the heading sailed during that half-hour, as shown on the ship's compass. The innermost ring of peg-holes is used for the first half-hour, and each succeeding measurement was made in the next ring out, until all eight rings were used.\n\nEach hour during the watch, a crew member inserted a peg in the bottom portion of the board to represent the speed sailed during the hour. The speed would have been measured using a knot log. If the speed for the first hour of the watch was knots, the crew member would count over 10 holes in the first row and place one peg, then place another peg in the column marked \"\". In the second hour of the watch, the crew member would use the second row of pegs, and so on until all 4 rows were used.\n\nAt the end of the watch, the navigator collected the information about the speeds and directions sailed during the watch into the logbook, cleared the pegs from the board, and used the information to figure the vessel's dead reckoning track. Meanwhile, the helm of the new watch would begin recording the new sailing headings and speeds on the traverse board.\n\n"}
{"id": "12301565", "url": "https://en.wikipedia.org/wiki?curid=12301565", "title": "Tube (structure)", "text": "Tube (structure)\n\nIn structural engineering, the tube is a system where, to resist lateral loads (wind, seismic, impact), a building is designed to act like a hollow cylinder, cantilevered perpendicular to the ground. This system was introduced by Fazlur Rahman Khan while at the architectural firm Skidmore, Owings & Merrill (SOM), in their Chicago office. The first example of the tube’s use is the 43-story Khan-designed DeWitt-Chestnut Apartment Building, since renamed Plaza on DeWitt, in Chicago, Illinois, finished in 1966.\n\nThe system can be built using steel, concrete, or composite construction (the discrete use of both steel and concrete). It can be used for office, apartment, and mixed-use buildings. Most buildings of over 40 stories built since the 1960s are of this structural type.\n\nThe tube system concept is based on the idea that a building can be designed to resist lateral loads by designing it as a hollow cantilever perpendicular to the ground. In the simplest incarnation of the tube, the perimeter of the exterior consists of closely spaced columns that are tied together with deep spandrel beams through moment connections. This assembly of columns and beams forms a rigid frame that amounts to a dense and strong structural wall along the exterior of the building.\n\nThis exterior framing is designed sufficiently strong to resist all lateral loads on the building, thereby allowing the interior of the building to be simply framed for gravity loads. Interior columns are comparatively few and located at the core. The distance between the exterior and the core frames is spanned with beams or trusses and can be column-free. This maximizes the effectiveness of the perimeter tube by transferring some of the gravity loads within the structure to it, and increases its ability to resist overturning via lateral loads.\n\nBy 1963, a new structural system of framed tubes had appeared in skyscraper design and construction. Fazlur Rahman Khan defined the framed tube structure as \"a three dimensional space structure composed of three, four, or possibly more frames, braced frames, or shear walls, joined at or near their edges to form a vertical tube-like structural system capable of resisting lateral forces in any direction by cantilevering from the foundation.\" Closely spaced interconnected exterior columns form the tube. Lateral or horizontal loads (wind, seismic, impact) are supported by the structure as a whole. About half the exterior surface is available for windows. Framed tubes allow fewer interior columns, and so allow more usable floor space. Where larger openings like garage doors are needed, the tube frame must be interrupted, with transfer girders used to maintain structural integrity.\n\nThe first building to apply the tube-frame construction was the DeWitt-Chestnut Apartment Building which Khan designed and which was finished in Chicago by 1963. This laid the foundations for the tube structural design of many later skyscrapers, including his own John Hancock Center and Willis Tower, and the construction of the World Trade Center, Petronas Towers, Jin Mao Building, and most other supertall skyscrapers since the 1960s, including the world's tallest building , the Burj Khalifa.\n\nFrom its conception, the tube has been varied to suit different structural needs.\n\nThis is the simplest incarnation of the tube. It can appear in a variety of floor plan shapes, including square, rectangular, circular, and freeform. This design was first used in Chicago's DeWitt-Chestnut Apartment Building, designed by Khan and finished in 1965, but the most notable examples are the Aon Center and the original World Trade Center towers.\n\nThe trussed tube, also termed \"braced tube\", is similar to the simple tube but with comparatively fewer and farther-spaced exterior columns. Steel bracings or concrete shear walls are introduced along the exterior walls to compensate for the fewer columns by tying them together. The most notable examples incorporating steel bracing are the John Hancock Center, the Citigroup Center, and the Bank of China Tower.\n\nAlso known as \"hull and core\", these structures have a core tube inside the structure, holding the elevator and other services, and another tube around the exterior. Most of the gravity and lateral loads are normally taken by the outer tube because of its greater strength. The \"780 Third Avenue\" 50-story concrete frame office building in Manhattan uses concrete shear walls for bracing and an off-center core to allow column-free interiors.\n\nInstead of one tube, a building consists of several tubes tied together to resist lateral forces. Such buildings have interior columns along the perimeters of the tubes when they fall within the building envelope. Notable examples include Willis Tower, One Magnificent Mile, and the Newport Tower.\n\nBeside being efficient structurally and economically, the bundled tube was \"innovative in its potential for versatile formulation of architectural space. Efficient towers no longer had to be box-like; the tube-units could take on various shapes and could be bundled together in different sorts of groupings.\" The bundled tube structure meant that \"buildings no longer need be boxlike in appearance: they could become sculpture.\"\n\nHybrids include a varied category of structures where the basic concept of tube is used, and supplemented by other structural support(s). This method is used where a building is so thin that one system cannot provide adequate strength or stiffness.\n\nSome lattice towers consist of steel tube elements. They can be used for guyed and for free-standing lattice structures. An example for the first type was Warsaw Radio Mast, an example for latter one are the Russian 3803 KM-towers.\n"}
{"id": "34504702", "url": "https://en.wikipedia.org/wiki?curid=34504702", "title": "Ucode system", "text": "Ucode system\n\nThe ucode system (written in lower case \"ucode\") is an identification number system that can be used to identify things in the real world uniquely. Digital information can be associated with objects and places, and the associated information can be retrieved by using ucode.\n\nUnique identification system for real world objects is considered an essential enabler for the realisation of Internet of Things and therefore ucode system is seen as a building block for Internet of Things.\nThe ucode system uses 128 bit code for unique naming of things so there are 340282366920938463463374607431768211455 or 3.4 x 10^38 different codes. If more codes are needed, they can be added in chunks of 128 bits. Ucode is application and technology agnostic. Uniqueness means that each ucode is unique, there can – or at least should - not be another ucode with exactly same number. Ucode is not tied to any specific application or business domain, neither is it committed to any specific technology for containing the ucode number, e.g. RFID, barcode or matrix code. ucode is supported by the uID center, which is a non-profit organisation based in Tokyo, Japan. The Chairman of the uID Center is Professor Ken Sakamura who is also the person behind ucode.\n\nUbiquitous ID system consists of five components: (1) ucode, (2) ucode tag, (3) ubiquitous communicator, (4) ucode resolution server and (5) ucode information server. The resolution process goes as follows. First, the ucode from an ucode tag using e.g. a mobile phone is read. The camera of the phone can be used to read a matrix code containing the ucode. Then, the mobile phone inquires the ucode resolution server – via internet connection - about the code. The ucode resolution server returns the source of the provided ucode information based on the ucode read. Finally, the ubiquitous communicator connects to the information provision source and acquires contents and services.\nThe ucode server architecture is similar to the familiar Internet DNS resolution service. Like DNS, the ucode resolution mechanism consists of hierarchical levels. The ucode resolution mechanism is three tiered as follows:\nThe root server is maintained by uID Center in Tokyo. TLD servers are in place in Japan, other Asian countries and in Europe (Oulu, Finland). The number of TLD and SLD servers is not limited.\n\nUcode tags can take various forms. They can be \nPrint tags can be matrix codes, e.g. QR codes or barcodes. A special sub-section of RFID tags are NFC tags, which can contain ucode. UID Center has certified a 46 differenrf ucode tags, the first ones in 2003 were barcodes made by Sato corporation, Toppan Forms Inc. and Dai Nippon Printing Co., later on two dimensional matrix codes were introduced, followed by hologram implementation and several RFID tags, often compliant with ISO/IEC15693 standard and using frequency band 13.56 MHz.\n\nThe ucode solution has been used in a number of trial cases related to tourist guides, geospatial information applications, housing and real estate as well as food and drug traceability. \nJapanese organisation Center for Better Living is using ucode for labelling construction material and components in a unique and traceable manner. The label is a sign of approved quality and entitles the buyer into a two to ten year repair warranty, depending on the nature of the material or component.\n\nEPCGlobal is an identification system aimed for supply chain information management. The EPC (Electronic Product Code) relies on RFID tags for object identification. EPCGlobal is a successor of MIT Auto ID Center which developed the technology used by EPCGlobal.\n[EAN] International Article Numbering system and its sister system UPC Universal Product Code are the familiar barcodes seen in all retail merchandise. ucode differs from them in identifying individual objects, not just the product type.\n\n"}
{"id": "25114583", "url": "https://en.wikipedia.org/wiki?curid=25114583", "title": "WeoGeo", "text": "WeoGeo\n\nWeoGeo was acquired by Trimble Navigation in 2014 and is now known as \"Trimble Data Marketplace.\" It allows users to discover, transform and download geospatial data. WeoGeo launched at the 2007 Where 2.0 Conference in San Jose, CA. WeoGeo and Safe Software announced a partnership in 2008 to bring FME Server to the cloud on Amazon Web Services.\n\nWeoGeo was co-founded by W. Paul Bissett and Dave Kohler.\n\nWeoGeo was established in 2006, and spun out of the Florida Environmental Research Institute, where the founders had worked together for nine years. It was founded to enable greater productivity in the geospatial industry by removing the vertical barriers to geo-content creation and sharing. In 2007, WeoGeo was a finalist in the Amazon Web Services StartUp Challenge. WeoGeo released their Library Appliance in the July, 2008 concurrent with their Market service. They released their Library, a monthly, Software-as-a-Service version of their Appliance, in March, 2009. WeoGeo left private beta on May 6, 2010 opening the service to the public. In the fall of 2012, WeoGeo became more focused on marketplace offerings.\n\nWeoGeo's main product is the WeoGeo Market. It can handle millions of individual geo-content files and maps, ranging in file size from megabytes to terabytes. It is an internet B2B marketplace that enables geospatial professionals to search, discover, customize, and acquire professional geo-content within minutes of entry to the marketplace.\n\nWeoGeo is located in Portland, OR at the Portland State Business Accelerator\n\n"}
{"id": "51153846", "url": "https://en.wikipedia.org/wiki?curid=51153846", "title": "Women in Tech", "text": "Women in Tech\n\nWomen In Tech: Take Your Career to the Next Level with Practical Advice and Inspiring Stories is a 2016 anthology written by Tarah Wheeler, an entrepreneur, keynote speaker and scientist, and published by Sasquatch Books. The book began as a Kickstarter project, with 772 backers and $32,226 in funding.\n\nIt includes advice for women in developing careers skills such as salary negotiations, networking and finding work-life balance, as well as personal stories from female tech professionals.\n\nThe book contains a foreword by Esther Dyson. In addition to Wheeler's writing, the anthology includes chapters by Kamilah Taylor, Miah Johnson, Kristin Toth Smith, Keren Elazari, Brianna Wu, Katie Cunningham, and Angie Chang\n\n\"Library Journal\" called \"Women in Tech\" \"The essential handbook for women in technology -- engaging, practical, and inspirational.\"\n"}
{"id": "22379062", "url": "https://en.wikipedia.org/wiki?curid=22379062", "title": "X-Vision", "text": "X-Vision\n\nX.Vision (Persian \"ایکس.ویژن\") is an Iranian consumer electronics manufacturer and a subsidiary of Maadiran Group.\n\nX.Visions' market is focused on the Middle East, primarily Iran. The company launched its products in a highly publicized campaign on April 2005, where the Iranian distribution company Maadiran Group became responsible for all aspects of the X-Vision brand in Iran.\n\n"}
