{"id": "60254", "url": "https://en.wikipedia.org/wiki?curid=60254", "title": "88open", "text": "88open\n\nThe 88open Consortium Ltd. was an industry standards group set up by Motorola in 1988 to standardize Unix systems on their Motorola 88000 RISC processor systems. At its peak, it had a staff of 30 people and over 50 supporters. The effort was largely a failure, at least in terms of attracting attention to the 88000 platform, and the group folded as soon as Motorola turned their attention to the PowerPC.\n\nMotorola provided 50% of the financial support for the consortium. Early members were Data General, Convergent and Tektronics. By May 1988, there were 28 companies who had joined with 7 not releasing their names.\n\n\n"}
{"id": "17291331", "url": "https://en.wikipedia.org/wiki?curid=17291331", "title": "API well number", "text": "API well number\n\nAn API well number or API number is a \"unique, permanent, numeric identifier\" assigned to each \nwell drilled for oil and gas in the United States. The API number is one of many industry standards established by the American Petroleum Institute. Custody of the API Number standard was transferred in 2010 to the PPDM Association.\n\nOil and gas wells have a name which normally consists of three parts: an operator name, a well number, and a lease name. All three parts of the name are subject to change, especially in the case of a producing well. When an oil or gas field is sold, the operator name will change. If a field is unitized for enhanced oil recovery, the well number and lease name will change. Almost three million wells have been drilled for oil and gas in the United States \n\nAPI well numbers grew out of an internal numbering scheme developed by Petroleum Information for its WHCS (Well History Control System) product. WHCS was first offered to the oil and gas industry at the 1956 annual meeting of the AAPG in Los Angeles. Several companies agreed to underwrite the WHCS project, and the first files were delivered in 1962.\n\nThe API Subcommittee on Well Data Retrieval Systems was formed in 1962 to standardize well identification numbers. The first recommendations of the subcommittee were published in 1966 as Appendix A of API Bulletin D12 (Well Data Glossary). In April 1968, API published Bulletin D12A, which dealt solely with well numbering systems. This publication was modified slightly in December 1970 and January 1979, and the 1979 edition (reissued in 1982 and 1985) is the most current version. In 1995, the Subcommittee drafted some modifications to the API numbering scheme, but the group was disbanded before the recommendations were published. The 1979 standards are still in effect, but the publication is no longer available from API . In 2013, the PPDM Association issued an updated standard for the API Number (now called the US Well Number) which includes a requirement to identify all wellbores within each well. The 1979 standard and the 2013 standard, along with supporting materials, are available at PPDM Association's website for well identification. \n\nOne of the Subcommittee's original recommendations was that the unique numbers should be assigned by regulatory agencies as part of the oil and gas well permitting process. In general, Petroleum Information assigned API well numbers before January 1, 1967. After that date, most of the numbers were assigned by the appropriate regulatory bodies.\n\nAn API well number can have up to 14 digits divided by dashes as follows:\n\nExample: 42-501-20130-03-00\n\nThe \"42\" means that this well is located in \"State Code\" 42 which is Texas. The \"501\" means that this well is located in \"County Code\" 501 which is Yoakum County. The \"20130\" is a \"Unique Well Identifier\" within the county. The \"03\" is the \"Directional Sidetrack Code\" for wells that have been sidetracked. The \"00\" is the \"Event Sequence Code\" to indicate how many operations have taken place.\n\nMost public databases that use API numbers are maintained by the individual oil and gas commissions, Therefore, they only require the \"County Code\" and \"Unique Well Identifier.\" For an example, consult the Wyoming Oil and Gas Commission website .\n\nThe first two digits (42 in the example above) of the API number represent the state where the surface location of the well is located. The state codes are based on standard state codes proposed by IBM in 1952 for accounting applications. The states are numbered from 01 (Alabama) to 49 (Wyoming), with the District of Columbia being number 08. After this scheme was devised, Alaska (50) and Hawaii (51) joined the United States. Numbers 52 to 54 are reserved for future states, including perhaps Puerto Rico.\n\nThere are additional \"State Codes\" that are reserved for \"pseudo-states.\" The pseudo-state numeric codes for offshore federal waters are Alaska Offshore (55), Pacific Coast Offshore (56), Northern Gulf of Mexico (60), and Atlantic Coast Offshore (61). A list of all the state codes is available from the Society of Petrophysicists and Well Log Analysts.\n\nThe state codes used in an API number are different from another standard which is the Federal Information Processing Standard state code established in 1987 by NIST.\n\nThe third through fifth digits (501 in the example above) of the API number represent the county where the surface location of the well is located. The \"pseudo-states\" like Atlantic Coast Offshore have \"pseudo-counties.\"\n\nThe \"County Codes\" are normally odd numbers. Delaware has the fewest counties (3) of any state, so its \"County Codes\" are Kent County (07-001), New Castle County (07-003), and Sussex County (07-005). Texas has the most counties of any state (254), so the \"County Codes\" range from Anderson County (42-001) to Zavala County (42-507).\n\nThe odd numbers were chosen for \"County Codes\" to allow expansion of the database. New counties were added in Arizona and New Mexico, and these were assigned even numbers. These include La Paz County, Arizona (02-012), Cibola County, New Mexico (30-006), and Los Alamos County, New Mexico (30-028).\n\nKern County, California has a well population that has exceeded the digits available for the \"Unique Well Identifier\" so 029 is used for the first 999 wells, and 030 is used for any additional wells . A list of counties and a downloadable Excel list that includes counties and \"pseudo counties\" is available from SPWLA for SPWLA members only.\n\nThe sixth through tenth digits (20130 in the example above) of the API number is a unique number within the county where the well was drilled. In states with very few wells, the unique number may be based on the permit number, and may be unique within the state rather than the county. This is an unusual situation.\n\nThere are four types of unique well identifiers, and these are called historical, current, reserved, and exempt.\n\nHistorical API numbers were assigned by Petroleum Information or other service companies and cooperative groups. In most states, wells drilled before 1967 have historical numbers. These numbers range in value from 00001 to 20000, so 42-501-05095 would be a typical historical number.\n\nCurrent API numbers are assigned by regulatory agencies, usually the oil and gas commission for the state where the well was drilled. These numbers are assigned as part of the well permitting process, and they may be the same as the well permit number. Current numbers are numbered sequentially beginning from 20001-60000 with some exceptions. In the example above, 42-501-20130 is a current number.\n\nSome states have deviated from this recommended system because of their own needs or previously established systems. Illinois and North Dakota have no break between their historical and current well numbers. Arkansas started its current numbers at 10001, while Texas started at 30001. Colorado, Michigan, and Utah have special numbering systems. The wells in the Federal waters of offshore Texas and Louisiana started at 40001.\n\nReserved API numbers were assigned by Petroleum Information to various wells drilled after 1967. These were for wells that for some reason were not assigned a number by the regulatory agency. If PI deemed the well deemed \"information important,\" it received a number between 60001 and 95000. \nThere are eight types of wells that may have received a reserved number. These are stratigraphic or core tests, water supply wells, water disposal wells, water or gas injection wells, sulfur wells, underground storage wells, geothermal wells, or prospect tests.\n\nExempt numbers range in value from 95001 to 99999. These numbers are proprietary, and may not be assigned by any regulatory agency or data vendor like Petroleum Information. This allows the oil company to include information on any wells that are \"information important\" but confidential.\n\nThe sidetrack code is the eleventh and twelfth digits (03 in the example above) of the API number. The original vertical well is normally 00. The first directional sidetrack would then be 01. In some states, the regulatory agency assigns the sidetrack codes, while other regulatory agencies do not. This means that the sidetrack code is useful in some places, but not used in others.\n\nThe thirteenth and fourteenth digits (00 in the example above) are to distinguish between separate operations in a single bore hole. In 1995, the API Subcommittee on Well Data Retrieval Systems proposed adding the event sequence code to deal with re-entries, recompletions, and hole deepenings. However, because of industry conditions (low oil prices), the subcommittee was disbanded before the recommendations were published and adopted by API\n\nIHS Energy , the successor to Petroleum Information, adopted these unpublished recommendations in January 1999. Data in the WHCS well completion and the Active well database have this event sequence code. However, this event sequence code is assigned by IHS Energy, and is not found in most oil and gas databases.\n\nThere are primary and secondary sources for API numbers. Petroleum Information (now IHS Energy ) assigned API numbers for most wells drilled before January 1, 1967. After that date, most numbers were assigned by the various state oil and gas regulatory bodies. The state commissions are therefore the primary, authoritative source for API numbers. However, API numbers can be purchased (along with other well information) from IHS.\n\nMost oil and gas commissions make API numbers (and well header data) available on-line and free of charge. The ability to download the data varies from state to state. Below is a list of the 50 states, along with their state code, and the name and internet address of their oil and gas regulatory agency where available. This list is partially based on information available from the Railroad Commission of Texas.\n\nIHS is now a secondary source for API numbers for most states, since they do not have assigning authority. Other secondary sources include commercial vendors of oil and gas data. API numbers are also used in many private corporate databases.\n\n"}
{"id": "17952882", "url": "https://en.wikipedia.org/wiki?curid=17952882", "title": "Allen Scythe", "text": "Allen Scythe\n\nThe Allen Scythe, sometimes called the Allen Power Scythe, is a petrol-powered finger-bar mower. It was made from 1935 until 1973 by John Allen and Sons in Cowley, Oxfordshire. The company, formerly the Eddison and Nodding Company, was bought in 1897 by John Allen, who renamed it the Oxford Steam Plough Company, and then renamed it to John Allen and Sons.\n\nThe Allen Scythe does not resemble a hand scythe but serves the same purpose. The engine drives a or or 4ft wide toothed blade sliding back and forth horizontally across stationary teeth to produce a scissor action, and also drives two large wheels for forward travel. There are handles to allow the machine to be controlled by an operator walking behind it, controls comprise throttle and clutch. Separate ratchets allow either one or both wheels to be driven. In normal use only one is engaged so that turning is easier. The machines are extremely robust and many from the 1950s are still in regular use. They can be dangerous, as the clutch system only disengages the wheel drive from the engine: the blades cut all the time the engine runs.\n\nLater models were available with different attachments, including rotating brushes that replaced the cutting head, air or water pumps and saws or hedge trimmers that drove from the starting pulley side of the engine.\n\nJohn Allen and Sons used a number of engine types, the most common being a Villiers two-stroke with magneto ignition and rope start. The engines may fail to start because of ignition problems. The spark should occur when the piston is 3/16 of an inch before top dead centre (TDC) on all Villiers engines except for the Villiers Mk. 11C & 25C which should spark 5/32 of an inch before TDC.\n\nEpisode 6 of the BBC Two reality television series \"Wartime Farm\" featured an Allen Scythe. Cast members Alex Langlands and Peter Ginn used it to harvest hay in a churchyard.\n\n"}
{"id": "31548452", "url": "https://en.wikipedia.org/wiki?curid=31548452", "title": "Axess Vision Technology", "text": "Axess Vision Technology\n\nAxess Vision Technology is a start up company and manufacturer of medical devices, mainly endoscopes. It is headquartered in Saint Pierre des Corps,France.It is competing with Ambu and Single Use Surgical Ltd to develop the world first generation of disposable endoscopes. These companies are responding to a market need of getting rid of cross contamination from flexible endoscopes, the latter ranked third in the ECRI 2011 top 10 Health Hazards. Even when endoscopes are thoroughly cleaned, there remains a risk of infection of 1%. Therefore, these endoscopes are designed to be used on one patient only before being incinerated.\n\n\n"}
{"id": "50425630", "url": "https://en.wikipedia.org/wiki?curid=50425630", "title": "Azimo", "text": "Azimo\n\nAzimo is an online remittance service. with headquarters in London and offices in Kraków. Launched in October 2012 with the mission to make international money transfer cheaper, faster, simpler and easier to understand, Azimo was voted Best Money-saving App by the Guardian. The company was co-founded by Michael Kent, Marta Krupinska, Ricky Knox and Marek Wawro.\n\nCustomers are able to transfer money via the Azimo website or smartphone app to more than 190 countries in over 80 different currencies. The company has half a million customers connected to its platform and offers more than 270,000 cash pick-up locations globally. In 2016, an updated version of its app was launched with features including in-app chat and biometric security.\n\nIn 2017, Azimo co-founder and general manager Marta Krupinska hosted the Duke and Duchess of Cambridge on their tour to Poland at a prestigious event held by the British Embassy in Poland, showcasing Britain's vibrant Polish diaspora and young Polish entrepreneurs. Azimo was chosen to host as an example of a successful and innovative cross-border FinTech business.\n\nAzimo Ltd is authorised by the Financial Conduct Authority under the Electronic Money Regulations 2011 (FRN 900220) for the issuing of electronic money. Azimo is licensed and regulated by HMRC as a Money Services Business - Licence Number 12676497. In July 2018, Azimo added ten new countries to its customer services in Nordic countries.\n\nJan 2012: founded\n<br>\nSep 2012: First transaction sent\n<br>\nOct 2012: First external investment\n<br>\nNov 2012: First employee hired\n<br>\nDec 2013: Launch of Kraków office\n\nCEO Michael Kent previously founded the Small World Financial Services Group in 2005, growing the company to become Europe’s largest offline money transfer provider. He is also co-founder of digital bank Tandem and has invested in and advised multiple UK FinTech businesses, including CorporatePay (now WEX Europe), Yoyo Wallet, Curve, ComplyAdvantage and Logical Glue. In 2013, he initiated the industry and NGO World Money Transfer Day, which has since been adopted by the World Bank Group and United Nations. He also blogs on global remittance reality for the World Bank.\n\nGeneral Manager Marta Krupinska previously founded Travelnity, a social media website for travellers and expats. In 2016, she was recognised by Forbes as one of Europe’s 30 Under 30 for Finance and rated ninth in Business Insider’s list of the 40 Coolest People in FinTech\n\nAs of early 2016, Azimo had raised $31 million in Series A and B funding from investors including Frog Capital, Greycroft, MCL Partners, e.ventures and Quona Capital. In May 2016, Japanese e-commerce company Rakuten invested in Azimo to accelerate the company’s expansion into Asia.\n\nAzimo offers all customers the chance to donate a portion of the money they save to charity. The company is also committed to donating 10% of its annual profits to its chosen charities, Operation Smile and ActionAid, and every member of staff is given a free day every quarter to allow them to get involved with work in their local community.\n\n2013: Best Money-saving App, The Guardian\n<br>\n2013: Best Startup, PayExpo\n<br>\n2015: Recognised by the UN as one of the most comprehensive and best value money transfer companies in UK and Germany\n<br>\n2016: Best Low Cost Remittance Service, Kalahari Awards\n"}
{"id": "1138406", "url": "https://en.wikipedia.org/wiki?curid=1138406", "title": "Bindi (decoration)", "text": "Bindi (decoration)\n\nA bindi (, from Sanskrit बिन्दु \"bindú\", meaning \"point, drop, dot or small particle\") is a coloured dot worn on the centre of the forehead, originally by Hindus and Jains. The word \"bindu\" dates back to the \"hymn of creation\" known as Nasadiya Sukta in the Rigveda. \"Bindu\" is considered the point at which creation begins and may become unity. It is also described as \"the sacred symbol of the cosmos in its unmanifested state\".\n\nA bindi is a bright dot of some colour applied in the centre of the forehead close to the eyebrows worn in the Indian subcontinent (particularly amongst Hindus in India, Pakistan, Bangladesh, Nepal, Bhutan and Sri Lanka) and Southeast Asia among Balinese, Javanese, Malaysian, Singaporean and Burmese Hindus. A similar marking is also worn by babies and children in China and, like in the Indian subcontinent and Southeast Asia, represents the opening of the third eye. Bindi in Hinduism, Buddhism, and Jainism is associated with ajna chakra, and Bindu is known as the third eye chakra. Bindu is the point or dot around which the mandala is created, representing the universe. The bindi holds a significant historical and cultural presence in the Indian cultural sphere.\n\nTraditionally, the area between the eyebrows (where the bindi is placed) is said to be the sixth chakra, \"ajna\", the seat of \"concealed wisdom\". The bindi is said to retain energy and strengthen concentration. The bindi also represents the third eye. \nThe Nasadiya Sukta of the Rig Veda, the earliest known Sanskrit text, mentions the word \"Bindu\".\n\nThe Ajna is symbolised by a sacred lotus with two petals, and corresponds to the colours violet, indigo or deep blue, though it is traditionally described as white. It is at this point that the two sides Nadi Ida (yoga) and Pingala are said to terminate and merge with the central channel Sushumna, signifying the end of duality, the characteristic of being dual (e.g. \"light\" and \"dark\", or \"male\" and \"female\"). The seed syllable for this chakra is the syllable OM, and the presiding deity is Ardhanarishvara, who is a half male, half female Shiva/Shakti. The Shakti goddess of Ajna is called Hakini. In metaphysics, Bindu is considered the dot or point at which creation begins and may become unity. It is also described as \"the sacred symbol of the cosmos in its unmanifested state\". Bindu is the point around which the mandala is created, representing the universe. Ajna (along with Bindu), is known as the third eye chakra and is linked to the pineal gland which may inform a model of its envisioning. The pineal gland is a light sensitive gland that produces the hormone melatonin which regulates sleep and waking up, and is also postulated to be the production site of the psychedelic dimethyltryptamine, the only known hallucinogen endogenous to the human body. Ajna's key issues involve balancing the higher and lower selves and trusting inner guidance. Ajna's inner aspect relates to the access of intuition. Mentally, Ajna deals with visual consciousness. Emotionally, Ajna deals with clarity on an intuitive level.\n\nIn Hinduism, Buddhism, and Jainism, bindi is associated with Ajna Chakra and Bindu. Divinities in these religions are typically depicted with \"Bhrumadhya Bindu\", in meditative pose with their eyes nearly closed show the gaze focused between eyebrows, other spot being the tip of the nose—Naasikagra. The very spot between the eyebrows known as Bhrumadhya is where one focuses his/her sight, so that it helps concentration. In South Asia, bindi is worn by women of all religious dispositions and is not restricted to religion or region. However, the Islamic Research Foundation, located in India, says \"wearing a bindi or mangalsutra is a sign of Hindu women. The traditional bindi still represents and preserves the symbolic significance that is integrated into Indian mythology in many parts of India.\"\n\nThe red bindi has multiple meanings which are all simultaneously valid:\n\nA traditional bindi is red or maroon in colour. A pinch of vermilion powder is applied skilfully with a ring-finger to make a perfect red dot. It takes considerable practice to achieve the perfect round shape by hand. A small annular disc aids application for beginners. First, they apply a sticky wax paste through the empty centre of the disc. This is then covered with kumkum or vermilion and then the disc is removed to get a perfect round bindi. Various materials such as lac, sandal, 'aguru', mica, 'kasturi', kumkum (made of red turmeric) and sindoor colour the dot. Saffron ground together with 'kusumba' flower can also work. Traditionally they are green in colour with red dot in the middle. The bindi is no longer restricted in colour or shape.\n\nHistorically, the ornamental bindi spangle consists of a small piece of lac over which is smeared vermilion, while above it a piece of mica or thin glass is fixed for ornament. Women wore large spangles set in gold with a border of jewels if they could afford it. The bindi was made and sold by lac workers known as Lakhera. In Hinduism, it's part of the \"Suhāg\" or \"lucky trousseau\" at marriages and is affixed to the girl's forehead on her wedding and thereafter always worn. Unmarried girls optionally wore small ornamental spangles on their foreheads. A widow was not allowed to wear bindi or any ornamentation associated with married women. In modern times, self-adhesive bindis are available in various materials, usually made of felt or thin metal and adhesive on the other side. These are simple to apply, disposable substitutes for older lac tikli bindis. Sticker bindis come in many colours, designs, materials, and sizes.\n\nThere are different regional variations of the bindi. In Maharashtra a large crescent moon shaped bindi is worn with a smaller black dot underneath or above, associated with \"Chandrabindu\" and \"Bindu chakra\" represented by crescent moon, they are commonly known as \"Chandrakor\" in this region, outside Maharashtra they are popularly known as \"Marathi bindi\". In Bengal region a large round red bindi is worn, brides in this region are often decorated with \"Alpana\" design on forehead and cheeks, along with bindi. In southern India a smaller red bindi is worn with a white tilak at the bottom, another common type is a red tilak shaped bindi. In Rajasthan the bindi is often worn round, long tilak shaped bindi are also common, as well as the crescent moon on some occasions. Decorative bindis have become popular among women in South Asia, regardless of religious background. Bindis are staple and symbolic for women in the Indian subcontinent.\n\nIn addition to the bindi, in India, a vermilion mark in the parting of the hair just above the forehead is worn by married women as commitment to long-life and well-being of their husbands. During all Hindu marriage ceremonies, the groom applies sindoor in the part in the bride's hair.\n\nApart from their cosmetic use, bindis have found a modern medical application in India. Iodine patch bindis have often been used among women in north-west Maharashtra to battle iodine deficiency.\n\nIn Southeast Asia, bindis are worn by Balinese and Javanese Hindus of Indonesia. Historically, it was worn by many Indianized kingdoms in Southeast Asia. Bindis are also decorated on wedding brides and grooms of Java and other parts of Indonesia, even worn by non-Hindus. It is worn for cultural purposes because Indonesia was once ruled by Indianized Hindu kingdoms, thus the culture still preserves until today. Bindis in Indonesia are usually white, rather than red as in India.\n\nBindis are popular outside the Indian subcontinent and Southeast Asia as well. They are sometimes worn purely for decorative purpose or style statement without any religious or cultural affiliation. Decorative and ornamental bindis were introduced to other parts of the world by immigrants from the Indian subcontinent. International celebrities such as Gwen Stefani, Julia Roberts, Madonna, Selena Gomez and many others have been seen wearing bindis. The appropriateness of such uses has been disputed. Reacting to Gomez wearing a bindi while singing her song \"Come and Get It\", Hindu leader Rajan Zed said that the bindi has religious significance and should not be used as a fashion accessory, but Indian actress Priyanka Chopra praised Gomez's choice as \"an embrace of Indian culture\".\n\nA bindi can be called:\n\n\n"}
{"id": "12905808", "url": "https://en.wikipedia.org/wiki?curid=12905808", "title": "Biomax Informatics AG", "text": "Biomax Informatics AG\n\nBiomax Informatics is a Munich-based software company specializing in research software for bioinformatics. Biomax was founded in 1997 and has its roots in the Munich Information Center for Protein Sequences (MIPS). The company's customer base consists of companies and research organizations in the areas of drug discovery, diagnostics, fine chemicals, food and plant production. In addition to exclusive software tools, Biomax Informatics provides services and curated knowledge bases.\n\nIn September 2007, Biomax Informatics acquired the Viscovery software business of the Austrian data mining specialist Eudaptics Software.\n\nBiomax Informatics and Sophic Systems Alliance Inc. (USA) participate in the Cancer Gene Data Curation Project with the National Cancer Institute (USA). This project maintains a public data set of cancer-related genes and drugs. This data set has been integrated with the NCI's caBIO (cancer Bioinformatics Infrastructure Objects) domain model which is part of the CaBIG Integrative Cancer Research (ICR) workspace. This Cancer Gene Index can be obtained separately from an NCI web site.\n\n\n\n\n"}
{"id": "5074933", "url": "https://en.wikipedia.org/wiki?curid=5074933", "title": "Bivalent (engine)", "text": "Bivalent (engine)\n\nA bivalent engine is an engine that can use two different types of fuel. Examples are petroleum/CNG and petroleum/LPG engines, which are widely available in the European passenger vehicle aftermarket. \n\nEngines that can use either alcohol (often produced as a biofuel) or standard gasoline are variants of flex fuel vehicles. Such vehicles are in production and commonly available for sale in the United States and other countries.\n\nCompressed natural gas (CNG) is made by compressing methane to store it high pressures. Liquefied natural gas (LNG) is made and stored cryogenically, much like liquid hydrogen. The physical properties of natural gas require the compression ratio of the engine to be higher than in normal internal combustion engines, and the higher compression makes for greater efficiency. Natural gas also has a higher octane rating, so it can be burned at a higher temperature, reducing engine knock, and the fuel can be produced without complicated refinement processes. Since little unburned carbon is produced in the combustion of natural gas, the engine and oil are kept much cleaner than would be the case if gasoline alone was being burned, and the engine's life is thus increased. Aftermarket kits are available to convert vehicles to run on LNG or CNG and gasoline. In the United States, natural gas is cheaper than gasoline, but CNG at typical pressures requires more frequent refueling, because it contains only a quarter of the energy per unit volume of gasoline, whereas LNG contains only 80%. Although natural gas is a finite resource and its reserves can be depleted, it is unique among current fuels in having a net positive EROEI (energy returned on energy invested), while petroleum and other fuels are net energy sinks.\n\nLiquefied petroleum gas (LPG) is a mixture of several hydrocarbons, mainly propane, butane and ethane. The gas mixes readily with air, allowing for more complete combustion. The fuel costs less than regular gasoline, but LPG has lower energy per unit volume, so its fuel economy and efficiency are lower. LPG gives a longer engine life due to its clean burning characteristics. The main difference between these vehicles and others is in their fuel storage systems. LPG is a gas at room temperature, but a liquid when pressurized (the required pressure varies according to the composition of the mixture). It is usually stored at around 10 bar. One drawback is that LPG fuel tanks are much heavier than conventional ones, hence two tanks would be needed, which would increase the vehicle's weight. Many automobile manufacturers make vehicles that run on LPG and gasoline. Some say that LPG is the least environmentally friendly alternative fuel, because it is derived from fossil fuels, so that greenhouse gases will inevitably be released into the atmosphere.\n\nBivalent engines can also use hydrogen fuel, as demonstrated by the BMW Hydrogen 7 using a bivalent V12 H7 Series engine. The engine itself is similar to a regular gasoline combustion engine, except for the fuel injection system. When a BMW Hydrogen 7 is running in gasoline mode, the fuel is injected directly into the cylinders, but when the vehicle is running on hydrogen, the fuel is injected into the intake manifold. BMW claims the Hydrogen 7 is the “world’s first production-ready hydrogen vehicle”, although only 100 total vehicles have been produced, and no more are planned to be produced.\n\nReduction of greenhouse gas emissions and preservation of natural resources are becoming increasingly important to people around the world. Many countries have regulations on the fuel economy of newly manufactured vehicles, and many governments offer tax breaks for vehicle manufactures that use clean-burning fuels. Vehicle manufacturers are thus motivated to develop new internal combustion engine technologies. The bivalent engine allows for an easier transition from fossil fuels to alternative fuels. The technology is advancing and there is increasing demand for more efficient and cleaner burning engines.\n\nHydrogen is being researched as a fuel for vehicles because it produces no carbon dioxide emissions. If hydrogen fuel becomes more popular, it has the potential to be less expensive than other fuels, if low-cost production via electrolysis can be implemented. Hydrogen can be used and created in fuel cells to power electric motors or burnt directly in combustion engines. BMW has developed a bivalent internal combustion engine that can switch between petroleum and liquid hydrogen fuels. In gaseous form, hydrogen is difficult to store and has a low volumetric energy density. It can be produced in many different ways, but many of the methods produce carbon dioxide. The most promising method is electrolysis. Safety of the hydrogen storage tanks in the event of an accident has been investigated, and various tests show that they do not present any problems. When used as a fuel, hydrogen has a wide range of flammability and low ignition energy. These properties allow hydrogen to be burned using a wide range of air-fuel mixtures, but problems arise with premature ignition. Crankcase ventilation is very important when burning hydrogen, because of the low ignition energy. Proper ventilation is needed to prevent ignition in the crankcase and the formation of water in the engine oil. Hydrogen and natural gas are very similar as fuels, so the differences between the components needed to burn them are trivial, and interoperable systems are easily made.\n\nThe volumetric energy density can be increased if hydrogen is stored as a cryogenic liquid. Liquid hydrogen provides almost a third as much energy per unit volume as gasoline. \n\nLiquid hydrogen fuel has some disadvantages. The technology is very new and the infrastructure for liquid hydrogen filling stations is currently very limited. Many of the processes commonly used in creating the fuel give off greenhouse gases, and the hydrogen produced is usually derived from finite resources. The storage of the liquid hydrogen is a major problem. Since the boiling point of liquid hydrogen is very low (-252.88 °C), it is difficult to keep the fuel cold enough to maintain its liquid form. When it warms, it evaporates. The pressure in the fuel tank then increases, and some gas must be released. Release valves are installed in these vehicles so that the pressure in the tank does not get too high, but a small amount of fuel is lost.\n\n\n"}
{"id": "2239715", "url": "https://en.wikipedia.org/wiki?curid=2239715", "title": "Bushcraft", "text": "Bushcraft\n\nBushcraft is a popular term for wilderness survival skills. The term was popularized in the Southern Hemisphere by Les Hiddins (the Bush Tucker Man) as well as in the Northern Hemisphere by Mors Kochanski and recently gained considerable currency in the United Kingdom due to the popularity of Ray Mears and his bushcraft and survival television programs. It is also becoming popular in urban areas where the average person is separated from nature, as a way to get back in tune with their rural roots. The origin of the phrase \"bushcraft\" comes from skills used in the bush country of Australia. Often the phrase 'wilderness skills' is used as it describes skills used all over the world.\n\nBushcraft is about thriving in the natural environment, and the acquisition of the skills and knowledge to do so. Bushcraft skills include firecraft, tracking, hunting, fishing, shelter-building, navigation by natural means, the use of tools such as knives and axes, foraging, water sourcing, hand-carving wood, container construction from natural materials, and rope and twine-making, among others.\n\nThe \"Oxford English Dictionary\" definition of bushcraft is \"skill in matters pertaining to life in the bush\".\n\nThe word has been used in its current sense in Australia and South Africa at least as far back as the 1800s. Bush in this sense is probably a direct adoption of the Dutch 'bosch', (now 'bos') originally used in Dutch colonies for woodland and country covered with natural wood, but extended to usage in British colonies, applied to the uncleared or un-farmed districts, still in a state of nature. Later this was used by extension for the country as opposed to the town. In Southern Africa, we get Bushman from the Dutch 'boschjesman' applied by the Dutch colonists to the natives living in the bush. In North America (where there was also considerable colonisation by the Dutch) you have the word 'bushwacker' which is close to the Dutch 'bosch-wachter' (now 'boswachter') meaning 'forest-keeper' or 'forest ranger'.\n\nHistorically, the term has been spotted in the following books (amongst others):\n\n\nThe word bushcraft was trademarked by Bushcraft USA LLC. The application was submitted July 30, 2012 and issued November 12, 2013. This trademark is a service mark, for the general use of the word bushcraft and is not limited to electronic forms of communication or commerce. However, the validity of this TM is in question since the Mark was used in commerce, by Mors Kochanski in 1988, 24 years prior to Bushcraft USA making claim to the Mark. \n\nThe Irish-born Australian writer Richard Graves titled his outdoor manuals \"The 10 bushcraft books\".\n\nCanadian wilderness instructor Mors Kochanski published the \"Northern Bushcraft\" book (later retitled \"Bushcraft\") in 1988. He has stated on numerous occasions that book title was an explicit reference to Graves' work.\n\nThe term has enjoyed a recent popularity largely thanks to Ray Mears, Cody Lundin, Les Stroud, Dave Canterbury, Jos Brech and their television programs.\n\n"}
{"id": "3873144", "url": "https://en.wikipedia.org/wiki?curid=3873144", "title": "Chapman–Jouguet condition", "text": "Chapman–Jouguet condition\n\nThe Chapman–Jouguet condition holds approximately in detonation waves in high explosives. It states that the detonation propagates at a velocity at which the reacting gases just reach sonic velocity (in the frame of the leading shock wave) as the reaction ceases.\n\nDavid Chapman and Émile Jouguet originally (c. 1900) stated the condition for an infinitesimally thin detonation. A physical interpretation of the condition is usually based on the later modelling (c. 1943) by Yakov Borisovich Zel'dovich, John von Neumann, and Werner Döring (the so-called ZND detonation model).\n\nIn more detail (in the ZND model) in the frame of the leading shock of the detonation wave, gases enter at supersonic velocity and are compressed through the shock to a high-density, subsonic flow. This sudden change in pressure initiates the chemical (or sometimes, as in steam explosions, physical) energy release. The energy release re-accelerates the flow back to the local speed of sound. It can be shown fairly simply, from the one-dimensional gas equations for steady flow, that the reaction must cease at the sonic (\"CJ\") plane, or there would be discontinuously large pressure gradients at that point.\n\nThe sonic plane forms a so-called choke point that enables the lead shock, and reaction zone, to travel at a constant velocity, undisturbed by the expansion of gases in the rarefaction region beyond the CJ plane.\n\nThis simple one-dimensional model is quite successful in explaining detonations. However, observations of the structure of real chemical detonations show a complex three-dimensional structure, with parts of the wave traveling faster than average, and others slower. Indeed, such waves are quenched as their structure is destroyed. The Wood-Kirkwood detonation theory can correct for some of these limitations.\n\nThe Rayleigh line equation and the Hugoniot curve equation obtained from the Rankine–Hugoniot relations for an ideal gas, with the assumption of constant specific heat and constant molecular weight, respectively are\n\nwhere formula_2 is the specific heat ratio and\n\nHere the subscript 1 and 2 identifies flow properties (pressure formula_4, density formula_5) upstream and downstream of the wave and formula_6 is the constant mass flux and formula_7 is the heat released in the wave. The slopes of Rayleigh line and Hugoniot curve are\n\nAt the Chapman-Jouguet point, both slopes are equal, leading the condition that\n\nSubstituting this back into the Rayleigh equation, we find \n\nUsing the definition of mass flux formula_11, where formula_12 denotes the flow velocity, we find\n\nwhere formula_14 is the Mach number and formula_15 is the speed of sound, in other words, downstream flow is sonic with respect to the Chapman-Jouguet wave. Explicit expression for the variables can be derived,\n\nThe upper sign applies for the Upper Chapman-Jouguet point (detonation) and the lower sign applies for the Lower Chapman-Jouguet point (deflagration). Similarly, the upstream Mach number can be found from\n\nand the temperature ratio formula_18 can be found from the relation formula_19.\n\n"}
{"id": "5590830", "url": "https://en.wikipedia.org/wiki?curid=5590830", "title": "Cole Palen", "text": "Cole Palen\n\nCole Palen (December 28, 1925 – December 8, 1993) was the founder of the Old Rhinebeck Aerodrome, a \"living\" museum of vintage aircraft from 1900-1937 located in Red Hook, New York. Palen's aerodrome boasts one of the finest collections of antique aircraft in the world, including an original Bleriot XI (civil registration N60094), the oldest flying aircraft in the United States and the second oldest in the world.\n\nJames Henry \"Cole\" Palen Jr. grew up in upstate New York outside the town of Poughkeepsie. As a child, he developed an early fascination with aviation and delighted in building free-flight model airplanes. In later life, he was recognized for his work in the preservation of early aviation history.\n\nAfter graduating from high school in 1944, Palen joined the United States Army just in time for the Battle of the Bulge. On returning to the United States, he enrolled in the Roosevelt Aviation School at Roosevelt Field, Long Island, to train as a mechanic. Here he was thrilled to find that one of the hangars contained a small museum of World War I aircraft. He dreamed of one day owning his own unique airfield and flying the early aircraft as he felt they should be flown.\n\nIn 1951, Roosevelt Field closed and plans were laid for a vast shopping center to be built on the site. Accordingly, the World War I aircraft were put up for sale. The Smithsonian had already acquired three of the aircraft so Cole quickly bid his life savings for the remainder. Soon thereafter he found himself the proud owner of a SPAD XIII, Avro 504K, Curtiss Jenny, Standard J-l, Aeromarine 39B and Sopwith Snipe. He was given just thirty days to remove the aircraft from Roosevelt Field, which required nine 200-mile round trips to the family home where they were stored in disused chicken coops.\n\nIn 1959, Palen found a farm for sale near Rhinebeck, New York. This property included a small farmhouse in which an unsolved murder had taken place. Around this time, Palen earned money through the rental of some of his aircraft to a company in California that was filming the World War I movie \"Lafayette Escadrille\", starring Tab Hunter. Between his savings from his employment at Texaco as a mechanic and earnings from the film deal, he was able to purchase the property by paying the back taxes owed on it. He cleared a runway and built makeshift hangars from scrapped materials with his bare hands and the Old Rhinebeck Aerodrome was born.\n\nPalen collected aircraft spanning the period of the birth of aviation up to the start of World War II. He restored them and flew them regularly, and where surviving examples of early original aircraft did not exist, accurate reproductions powered by authentic, vintage-era engines were built. A sizable collection of veteran and vintage vehicles was also collected, nearly all in working order.\n\nIn 1960, Palen opened the Old Rhinebeck Aerodrome to the public, the first known American example of a living museum dedicated to preserving pre-World War II aviation history and technology. The first air show took place in 1960 to an audience of approximately 25 people. Gradually word spread and shows were held regularly on the last Sunday of the summer months. As demand grew this was changed to the present format of a show every Saturday and Sunday from mid-June through mid-October.\n\nPalen had a strict philosophy regarding his aircraft; he believed that a plane was not truly a plane unless it could fly. By putting this philosophy into action, Palen made the Old Rhinebeck Aerodrome one of the few places in the world where the public could see aircraft from the dawn of aviation actually fly. Taking this a step further, Palen made both his original and reproduction aircraft as authentic as possible. Original drawings would be used for restoration and creation of accurate reproduction airframes, as well as the installation of original parts and engines into the machines so they would look, sound, and fly the same way they did for the daredevils of early aviation. Because of this, Palen and his aerodrome became the focus of countless newspaper and magazine articles, documentaries, websites, and books.\n\nIn April 1965, Cole Palen flew his 1912 Thomas Pusher from Rhinebeck to New York City and after a three-day trip, appeared on the television game show \"I've Got a Secret\".\n\nOn March 17, 1967, Palen married Rita Weidner. Rita took over management of the Old Rhinebeck Aerodrome and brought some order to the administrative side of things.\n\nPalen was associated with several movies, most notably in 1983, when he worked as a stunt double for Woody Allen in the film \"Zelig\". Reference IMDb in External Links below.\n\nEarly in 1993, Palen suffered a stroke. Looking to the future, he decided to form the Rhinebeck Aerodrome Museum Foundation. The Foundation came into being during the course of the year under a board of directors and a special new foundation building for the static display of more valuable aircraft was erected opposite the Pioneer, W.W.I and Lindbergh era buildings.\n\nFollowing the end of the 1993 season, Cole and Rita made their annual pilgrimage to their winter home in Florida, where Palen also maintained a workshop. Early that December, Rita suffered a slight stroke and was admitted to the hospital. It was at this time that Cole died in his sleep. Rita made an excellent recovery and continued with her husband's legacy until her death on August 12, 2002.\n\nIn 1995, the Experimental Aircraft Association recognized Palen for his work by posthumously naming him to the EAA's Vintage Aircraft Association Hall of Fame.\n\nSeveral of the original World War I aircraft that Cole acquired and restored to airworthy condition are now on display in museums such as the United States Air Force Museum, Canada Aviation Museum, and the National Air and Space Museum.\n\nAn excellent biography of Cole and the aerodrome was written by E. Gordon Bainbridge and published in 1977, \"The Old Rhinebeck Aerodrome: The story of Cole Palen and his 'living' aviation museum\" ().\n\n"}
{"id": "15695872", "url": "https://en.wikipedia.org/wiki?curid=15695872", "title": "Consumability", "text": "Consumability\n\nA concept recently championed by International Business Machines (IBM), consumability is a description of customers' end-to-end experience with technology solutions (although the concept could easily apply to almost anything). The tasks associated with consumability start before the consumer purchases a product and continue until the customer stops using the product. By improving the consumability of the product, the value of that product to the client can be increased.\n\nUnderstanding product consumability requires an in-depth understanding of how clients are actually trying to use the product, which is why consumability is so closely aligned with the user experience and Outside-in software development. While usability addresses a client's ability to use a product, consumability is a higher-level concept that incorporates all the other aspects of the customer’s experience with the product.\n\nKey consumability aspects of the user experience include:\n\n\nHow efficiently and effectively clients can complete these tasks affects the value they get from the product. Missteps anywhere along this path can have direct impacts on the customer's ability to complete the task they set out to do. By focusing on consumability, developers can smooth the path, allowing technology solution consumers to focus on the needs of their business, improving their perception and satisfaction with the product or solution.\n\n\n"}
{"id": "21156199", "url": "https://en.wikipedia.org/wiki?curid=21156199", "title": "Copper slag", "text": "Copper slag\n\nCopper slag is a by-product of copper extraction by smelting. During smelting, impurities become slag which floats on the molten metal. Slag that is quenched in water produces angular granules which are disposed of as waste or utilized as discussed below.\n\nSlag from ores that are mechanically concentrated before smelting contain mostly iron oxides and silicon oxides.\n\nCopper slag is mainly used for surface blast-cleaning. Abrasive blasting is used to clean and shape the surface of metal, stone, concrete and other materials. In this process, a stream of abrasive grains called grit are propelled toward the workpiece. Copper slag is just one of many different materials that may be used as abrasive grit. Rate of grit consumption, amount of dust generated, and surface finish quality are some of the variables affected by the choice of grit material.\n\nInternationally the described media is manufactured in compliance with ISO 11126-3\n\nThe blasting media manufactured from copper slag brings less harm to people and environment than sand. The product meets the most rigid health and ecological standards.\n\nCopper slag can be used in concrete production as a partial replacement for sand. Copper slag is used as a building material, formed into blocks. Such use was common in areas where smelting was done, including St Helens and Cornwall in England. In Sweden (Skellefteå region) fumed and settled granulated copper slag from the Boliden copper smelter is used as road-construction material. The granulated slag (<3 mm size fraction) has both insulating and drainage properties which are usable to avoid ground frost in winter which in turn prevents pavement cracks. The usage of this slag reduces the usage of primary materials as well as reduces the construction depth which in turn reduces energy demand in building. Due to the same reasons the granulated slag is usable as a filler and insulating material in house foundations in a cold climate. Numerous houses in the same region are built with a slag insulated foundation.\n\n"}
{"id": "3296562", "url": "https://en.wikipedia.org/wiki?curid=3296562", "title": "DO-254", "text": "DO-254\n\nRTCA/DO-254, Design Assurance Guidance for Airborne Electronic Hardware is a document providing guidance for the development of airborne electronic hardware, published by RTCA, Incorporated.\n\nThe DO-254 standard was formally recognized by the FAA in 2005 via AC 20-152 as a means of compliance for the design of complex electronic hardware in airborne systems. Complex electronic hardware includes devices like Field Programmable Gate Arrays (FPGAs), Programmable Logic Devices (PLDs), and Application Specific Integrated Circuits (ASICs). The DO-254 standard is the counterpart to the well-established software standard RTCA DO-178B/EUROCAE ED-12B. With DO-254, the FAA has indicated that avionics equipment contains both hardware and software, and each is critical to safe operation of aircraft. There are five levels of compliance, A through E, which depend on the effect a failure of the hardware will have on the operation of the aircraft. Level A is the most stringent, defined as \"catastrophic\" (e.g. loss of the aircraft), while a failure of Level E hardware will not affect the safety of the aircraft. Meeting Level A compliance for complex electronic hardware requires a much higher level of verification and validation than Level E compliance.\n\nThe main regulations which must be followed are requirements capturing and tracking throughout the design and verification process. The following items of substantiation are required to be provided to the FAA, or the Designated Engineering Representative (DER) representing the FAA:\n\nThe hardware design and hardware verification need to be done independently. The hardware designer works to ensure the design of the hardware will meet the defined requirements. Meanwhile, the verification engineer will generate a verification plan which will allow for testing the hardware to verify that it meets all of its derived requirements.\n\nThe planning process is the first step where the design authority (the company who develops the H/W and implements the COTS into its design) declares its approach towards the certification. At this point the PHAC (Plan for H/W Aspects of Certification) is presented to the authorities (EASA, FAA...). In this plan, the developer presents its approach and how DO-254 is implemented. The PHAC is submitted as part of the authorities 1st stage of involvement (SOI#1).\n\n\nThe validation process provides assurance that the hardware item derived requirements are correct and complete with respect to system requirements allocated to the hardware item.\n\nThe verification process provides assurance that the hardware item implementation meets all of the hardware requirements, including derived requirements.\n\nA widely used industry mnemonic for the difference is:\n\n\n\n\n\n\n"}
{"id": "4556039", "url": "https://en.wikipedia.org/wiki?curid=4556039", "title": "Electronic notetaking", "text": "Electronic notetaking\n\nElectronic notetaking (ENT), also known as computer-assisted notetaking (CAN), is a system that provides virtually simultaneous access to spoken information to people who are deaf and hard of hearing, facilitating equal participation with their hearing colleagues, coworkers, and classmates. This method is most often used in educational or training sessions, but it is also used at health care appointments, meetings, or interviews.\n\nUsing a software program, an operator types a summary of the spoken information into a computer at a minimum typing speed of 60 words per minute. The text is then projected on a screen or transmitted to a second computer.\n\nThe text also provides a written record of sessions, which is particularly useful for deaf and hard of hearing attendees.\n\nElectronic notetaking began in the 1990s, when the disability legislation changed, such as the Disability Discrimination Act (DDA) in the UK which provided more support. \n\nThe operators may work freelance either for an agency or as part of a professional team providing communication support.\n\n\n"}
{"id": "18070660", "url": "https://en.wikipedia.org/wiki?curid=18070660", "title": "External ventricular drain", "text": "External ventricular drain\n\nAn external ventricular drain (EVD), also known as a ventriculostomy or extraventricular drain, is a device used in neurosurgery to treat hydrocephalus and relieve elevated intracranial pressure when the normal flow of cerebrospinal fluid (CSF) inside the brain is obstructed. An EVD is a flexible plastic catheter placed by a neurosurgeon or neurointensivist and managed by intensive care unit (ICU) physicians and nurses. The purpose of external ventricular drainage is to divert fluid from the ventricles of the brain and allow for monitoring of intracranial pressure. An EVD must be placed in a center with full neurosurgical capabilities, because immediate neurosurgical intervention can be needed if a complication of EVD placement, such as bleeding, is encountered.\n\nEVDs are a short-term solution to hydrocephalus, and if the underlying hydrocephalus does not eventually resolve, it may be necessary to convert the EVD to a cerebral shunt, which is a fully internalized, long-term treatment for hydrocephalus.\n\nThe EVD catheter is most frequently placed by way of a twist-drill craniostomy placed at Kocher's point, a location in the frontal bone of the skull, with the goal of placing the catheter tip in the frontal horn of the lateral ventricle or in the third ventricle. The catheter is typically inserted on the right side of the brain, but in some cases a left-sided approach is used, and in other situations catheters are needed on both sides. EVDs can be used to monitor intracranial pressure in patients with traumatic brain injury (TBI), subarachnoid hemorrhage (SAH), intracerebral hemorrhage (ICH), or other brain abnormalities that lead to increased CSF build-up. In draining the ventricle, the EVD can also remove blood products from the ventricular spaces. This is important because blood is an irritant to brain tissue and can cause complications such as vasospasm.\n\nThe EVD is leveled to a common reference point that corresponds to the skull base, usually the tragus or external auditory meatus. The EVD is set to drain into a closed, graduated burette at a height corresponding to a particular pressure level, as prescribed by a healthcare professional, usually a neurosurgeon or neurointensivist. Leveling the EVD to a set pressure level is the basis for cerebrospinal fluid (CSF) drainage; hydrostatic pressure dictates CSF drainage. The fluid column pressure must be greater than the weight of the CSF in the system before drainage occurs. It is therefore important that family members and visitors understand the patient's head of bed position cannot be changed without assistance.\n\nAn example of a healthcare provider order regarding an EVD is: set EVD open to drain to 15 cmH20 above tragus, check and record cerebrospinal fluid drainage and intracranial pressure every hour.\n\nThe cerebral perfusion pressure (CPP) can be calculated from data obtained from the EVD and systemic blood pressure. In order to calculate the CPP the intracranial pressure and mean arterial pressure (MAP) must be available.\n\nformula_1\n\nEVD placement is an invasive procedure. It is associated with several potential complications:\n\nBleeding can occur along the EVD insertion tract or in the several layers of the meninges that prohibit passage into the brain. If drilling or dural puncture is not successful, the surgeon may dissect away dura and create a secondary bleed known as an epidural or subdural hemorrhage. Bleeding from EVD placement can be life-threatening and can require neurosurgical intervention in some cases. The risk of hemorrhage with EVD placement is increased if the patient suffers from coagulopathy.\n\nMechanical complications from EVD placement can be categorized into:\nIf the EVD is not placed with the tip of the catheter in the lateral or third ventricle, it is considered malplaced. If the catheter crosses critical brain regions such as the internal capsule or the upper brainstem, malplacement can be symptomatic. \nObstruction/occlusion of EVD commonly due to fibrinous/clot like material or kinking of the tube. The brain can swell due to pressure build up in the ventricles and permanent brain damage can occur. Physicians or nurses may have to adjust or flush these small diameter catheters to manage medical tube obstructions and occlusions at the intensive-care bedside.\n\nAfter EVD placement, the drain is tunneled subcutaneously and secured with surgical sutures and/or surgical staples. However, it is possible for the EVD to dislodge or migrate. This will cause the tip of the drain migrated away from its intended position and provide inaccurate ICP measurement or lead to occlusion of the drain.\n\nThe EVD is a foreign body inserted into the brain, and as such it represents a potential portal for serious infection. Historically, the rate of infections associated with EVDs has been very high, ranging from 5% to >20%. Infections associated with EVDs can progress to become a severe form of brain infection known as ventriculitis. Protocols designed to reduce the rate of EVD infections have been successful, applying infection control 'bundle' approaches to reduce the rate of infection to well less than 1%.\n\nAlthough neurological deficits from passing the EVD catheter across the brain are uncommon, there can be an association of a patient's poor neurological status with EVD malplacement. In this report, the EVD was inserted too deep into the fourth ventricle, and the authors hypothesized that the patient's coma was due to irritation of the recticular activating system. The patient's consciousness improved after the EVD was adjusted.\n"}
{"id": "40875066", "url": "https://en.wikipedia.org/wiki?curid=40875066", "title": "Food Factory", "text": "Food Factory\n\nFood Factory is a Canadian television series produced by Cineflix airing in that country on the Food Network, and in the United States on FYI. The show features the industrial production lines of major food companies, mostly in Canada, but also in the United States, and occasionally in other countries. It is co-narrated by Colleen Rusholme and Todd Schick.\n\nBetween the original program's third and fourth seasons, the first season of Food Factory USA was produced for FYI and featured only U.S. factories. The style of the show, including the theme music, text graphics, and two narrators, is identical to the original three seasons of \"Food Factory\". However, the format is somewhat different, catering to the demands of American programmers by eliminating one of the four segments to make room for more TV commercials, and putting those commercials in the middle of each of the three remaining segments instead of between them. In addition, each break is preceded by a trivia question related to the segment, whose answer is given following the break (similar to other series such as \"Pawn Stars\"). In the spring of 2015, a second season began airing on May 23, two at a time each week as with season four of \"Food Factory\". The only noticeable difference is the use of graphical text in various colors (consistent within each episode), instead of the silvery grey used in all four previous seasons of the two series.\n\nIn the FYI telecasts, \"Food Factory USA\" also uses only English units, instead of the metric system measurements used in the original three \"Food Factory\" seasons. Those seasons, as seen on Food Network Canada (as well in the United States as FYI and its predecessor, Bio), used metric measurements in the narration, with the FYI broadcasts also including metric with English conversions in the graphics. As aired in the U.S., the fourth season of \"Food Factory\" has no metric units of any kind (narration, graphics, or captions), but the closed captioning still uses Canadian spelling.\n\nAlso in May 2015, a true spinoff began airing in the U.S. on FYI. \"Home Factory\" is nearly identical to the original series, except that its products are non-food items found in and around the home, ranging from towels and brooms to rubber ducks and lawn flamingos.\n\n\n"}
{"id": "14453240", "url": "https://en.wikipedia.org/wiki?curid=14453240", "title": "Governo", "text": "Governo\n\nGoverno is a winemaking technique reportedly invented in Tuscany in the 14th century to help complete fermentation and stabilize the wine. The technique involves saving a batch of harvested grapes and allowing them to partially dry. If fermentation of the main batch starts to slow or appears to be nearing stuck fermentation, the half dried grapes are added to the must which then gives the yeast cells a new source of sugar to enliven the batch. From there, the must can be fermented dry or stopped with the wine having a higher level of residual sugar. The process was widely used in the Chianti zones until the advent of temperature controlled fermentation tanks. From Tuscany the technique spread to Marche and Umbria where it is sometimes used today. In the Marche the technique is most often used on wines made from the Verdicchio grape to counteract the grape's natural bitterness and to add some sweetness and \"frizzante\" qualities.\n\nThe benefits of Governo is that it encourages not only fully completed primary fermentation but can also aid in the developing of malolactic fermentation which can help stabilize the wine. With very acidic grapes like Sangiovese this process will temper some of the harshness and volatility in the wine. A by-product of this technique is an increase in carbon dioxide or \"fizziness\" in the wine as well as increased alcohol content due to the added sugar that the yeast will convert into alcohol.\n\n"}
{"id": "36287660", "url": "https://en.wikipedia.org/wiki?curid=36287660", "title": "Grape stomping", "text": "Grape stomping\n\nGrape-stomping (also known as pigeage) is part of method of maceration used in traditional winemaking. Rather than using a wine press or other mechanized method, grapes were crushed by having barefoot participants repeatedly stomp on them in vats to release their juices and begin fermentation. Stomping was widespread in the history of winemaking, but with the introduction of industrial methods, it now survives mostly as a recreational or competitive activity at cultural festivals.\n\nOne of the earliest extant visual representations of the practice appears on a Roman Empire sarcophagus from the 3rd century AD, which depicts an idealized pastoral scene with a group of Erotes harvesting and stomping grapes at Vindemia, a rural festival.\n\nMany contemporary wineries hold grape-stomping contests to attract visitors. The practice is also the subject of many depictions in contemporary media, including the 1974 Mel Tillis song \"Stomp Them Grapes,\" the \"I Love Lucy\" episode \"Lucy's Italian Movie,\" and \"The Littlest Grape Stomper\", a children's book by Alan Madison.\n\n\n"}
{"id": "1041429", "url": "https://en.wikipedia.org/wiki?curid=1041429", "title": "Harry Daghlian", "text": "Harry Daghlian\n\nHaroutune Krikor \"Harry\" Daghlian Jr. (May 4, 1921 – September 15, 1945) was a physicist with the Manhattan Project which designed and produced the atomic bombs that were used in World War II. He accidentally irradiated himself on August 21, 1945, during a critical mass experiment at the remote Omega Site of the Los Alamos Laboratory in New Mexico, resulting in his death 25 days later.\n\nDaghlian was irradiated as a result of a criticality accident that occurred when he accidentally dropped a tungsten carbide brick onto a 6.2 kg plutonium–gallium alloy bomb core. This core, subsequently nicknamed the \"demon core\", was later involved in the death of another physicist, Louis Slotin.\n\nHaroutune Krikor Daghlian Jr., of Armenian-American descent, was born in Waterbury, Connecticut, on May 4, 1921, one of three children of Margaret Rose (\"née\" Currie) and Haroutune Krikor Daghlian. He had a sister, Helen, and a brother, Edward. Soon after his birth the family moved across state to the coastal town of New London, Connecticut. He was educated at Harbor Elementary School, where he played violin in the school orchestra, and at Bulkeley High School. In 1938, at the age of 17, he entered the Massachusetts Institute of Technology, intending to study mathematics, but became interested in physics, particularly particle physics, then emerging as an exciting new field. This interest led him to transfer to the West Lafayette, Indiana, campus of Purdue University, where he graduated with a bachelor of Science degree in 1942 . He then commenced work on his doctorate, assisting Marshall Holloway with the cyclotrons. In 1944, while still a graduate student, he joined Otto Frisch's Critical Assembly Group at the Los Alamos Laboratory of the Manhattan Project.\n\nDuring an experiment on August 21, 1945, Daghlian was attempting to build a neutron reflector manually by stacking a set of tungsten carbide bricks in an incremental fashion around a plutonium core. The purpose of the neutron reflector was to reduce the mass required for the plutonium core to attain criticality. He was moving the final brick over the assembly, but neutron counters alerted Daghlian to the fact that the addition of that brick would render the system supercritical. As he withdrew his hand, he inadvertently dropped the brick onto the center of the assembly. Since the assembly was nearly in the critical state, the accidental addition of that brick caused the reaction to go immediately into the prompt critical region of neutronic behavior. This resulted in a criticality accident.\n\nDaghlian reacted immediately after dropping the brick and attempted to knock the brick off the assembly without success. He was forced to disassemble part of the tungsten-carbide pile in order to halt the reaction.\n\nDaghlian was estimated to have received a dose of 510 rem (5.1 Sv) of neutron radiation, from a yield of 10 fissions. Despite intensive medical care, he developed symptoms of severe radiation poisoning and his mother and sister were flown out to care for him (his father had died in 1943). He fell into a coma, and died 25 days after the accident. He was the first known fatality caused by a criticality accident. His body was returned to New London, where he was buried in Cedar Grove Cemetery.\nAs a result of the incident, safety regulations for the project were scrutinized and revised. A special committee was established to review any similar experiments and recommend appropriate safety procedures. This change of procedures included needing a minimum of two people involved in such an experiment, using at least two instruments monitoring neutron intensities with audible alerts, and preparing a plan for operating methods and any contingencies that might occur during similar experiments. Additionally, discussions and designs for remote-controlled test devices were initiated, eventually leading to the creation of the Godiva device.\n\nThese changes did not prevent another criticality accident from happening at Los Alamos the following year. Louis Slotin, a colleague of Daghlian's, was killed in 1946 while performing criticality tests on the same plutonium core. After these two incidents it became known as the \"demon core\", and all similar criticality experiments were halted until remote-controlled assembly devices were more fully developed and available.\n\nDaghlian was memorialized on May 20, 2000, by the city of New London, with the erection of a memorial stone and flagpole in Calkins Park, which was unveiled by his brother and sister. It read: \"though not in uniform, he died in service to his country.\"\n\n"}
{"id": "9879153", "url": "https://en.wikipedia.org/wiki?curid=9879153", "title": "Haute Qualité Environnementale", "text": "Haute Qualité Environnementale\n\nThe Haute Qualité Environnementale or HQE (High Quality Environmental standard) is a standard for green building in France, based on the principles of sustainable development first set out at the 1992 Earth Summit. The standard is controlled by the Paris-based \"Association pour la Haute Qualité Environnementale (ASSOHQE).\"\n\nThe standard specifies criteria for the following:\n\nManaging the impacts on the outdoor environment\n\nCreating a pleasant indoor environment\n\nOn 16 June 2009, it was announced that the CSTB \"(Centre Scientifique et Technique du Batiment)\" and its subsidiary CertiVéA had signed a memorandum of understanding to work together with the global arm of the United Kingdom's Building Research Establishment (BRE) to develop a pan-European building environmental assessment method. The BRE developed and markets (BREEAM (the BRE Environmental Assessment Method), which has similarities to the French HQE. Unfortunately, BREEAM and HQE are still disseminating their own standards round the world, leaving little doubt that no pan-European method will emerge in the near future, at least stemming from these two organisations.\n\nSince 2013, the HQE brand is now available for buildings and districts worldwide.\nAs of 2016 HQE is present in 24 countries.\n\n\n"}
{"id": "49840256", "url": "https://en.wikipedia.org/wiki?curid=49840256", "title": "Have I Been Pwned?", "text": "Have I Been Pwned?\n\nHave I Been Pwned? (HIBP) is a website that allows internet users to check if their personal data has been compromised by data breaches. The service collects and analyzes hundreds of database dumps and pastes containing information about billions of leaked accounts, and allows users to search for their own information by entering their username or email address. Users can also sign up to be notified if their email address appears in future dumps. The site has been widely touted as a valuable resource for internet users wishing to protect their own security and privacy. Have I Been Pwned? was created by security expert Troy Hunt on 4 December 2013.\n\nAs of November 2017, Have I Been Pwned? receives around sixty thousand daily visitors, the site has over 1.7 million active email subscribers and contains records of over 4.8 billion accounts from over 251 data breaches.\n\nThe primary function of Have I Been Pwned? since it was launched is to provide the general public a means to check if their private information has been leaked or compromised. Visitors to the website can enter an email address, and see a list of all known data breaches with records tied to that email address. The website also provides details about each data breach, such as the backstory of the breach and what specific types of data were included in it.\n\nHave I Been Pwned? also offers a \"Notify me\" service that allows visitors to subscribe to notifications about future breaches. Once someone signs up with this notification mailing service, they will receive an email message any time their personal information is found in a new data breach.\n\nIn September 2014, Hunt added functionality that enabled new data breaches to be automatically added to HIBP's database. The new feature used Dump Monitor, a Twitter bot which detects and broadcasts likely password dumps found on pastebin pastes, to automatically add new potential breaches in real-time. Data breaches often show up on pastebins before they are widely reported on; thus, monitoring this source allows consumers to be notified sooner if they've been compromised.\n\nAlong with detailing which data breach events the email account has been affected by, the website also points those who appear in their database search to install a password manager, namely: 1password, which Troy Hunt has recently endorsed. An online explanation on his website explains his motives and maintains that monetary gain is not the goal of this partnership.\n\nIn August 2017, Hunt made public 306 million passwords which could be accessed via a web search or downloadable in bulk.\n\nIn February 2018 British computer scientist, Junade Ali, created a communication protocol (using K-Anonymity and cryptographic hashing) to anonymously verify if a password was leaked without fully disclosing the searched password. This protocol was implemented as a public API in Hunt's service and is now consumed by multiple websites and services including password managers and browser extensions.\n\nIn late 2013, web security expert Troy Hunt was analyzing data breaches for trends and patterns. He realized breaches could greatly impact users who might not even be aware their data was compromised, and as a result, began developing HIBP. \"Probably the main catalyst was Adobe,\" said Hunt of his motivation for starting the site, referring to the Adobe Systems security breach that affected 153 million accounts in October 2013.\n\nHunt launched Have I Been Pwned? on 4 December 2013 with an announcement on his blog. At this time, the site had just five data breaches indexed: Adobe Systems, Stratfor, Gawker, Yahoo! Voices, and Sony Pictures. However, the site now had the functionality to easily add future breaches as soon as they were made public:\n\nSince its launch, the primary development focus of HIBP has been to add new data breaches as quickly as possible after they are leaked to the public.\n\nIn July 2015, online dating service Ashley Madison, known for encouraging users to have an extramarital affair, suffered a data breach, and the identities of more than 30 million users of the service were leaked to the public. The data breach received wide media coverage, presumably due to the large number of impacted users and the perceived shame of having an affair. According to Hunt, the breach's publicity resulted in a 57,000% increase in traffic to HIBP. Following this breach, Hunt added functionality to HIBP by which breaches considered \"sensitive\" would not be publicly searchable, and would only be revealed to subscribers of the email notification system. This functionality was enabled for the Ashley Madison data, as well as for data from other potentially scandalous sites, such as Adult FriendFinder.\n\nIn October 2015, Hunt was contacted by an anonymous source who provided him with a dump of 13.5 million users' email addresses and plaintext passwords, claiming it came from 000webhost, a free web hosting provider. Working with Thomas Fox-Brewster of \"Forbes\", he verified that the dump was most likely legitimate by testing email addresses from it and by confirming sensitive information with several 000webhost customers. Hunt and Fox-Brewster attempted many times to contact 000webhost to further confirm the authenticity of the breach, but were unable to get a response. On 29 October 2015, following a reset of all passwords and the publication of Fox-Brewster's article about the breach, 000webhost announced the data breach via their Facebook page.\n\nIn early November 2015, two breaches of gambling payment providers Neteller and Skrill were confirmed to be legitimate by the Paysafe Group, the parent company of both providers. The data included 3.6 million records from Neteller obtained in 2009 using an exploit in Joomla, and 4.2 million records from Skrill (then known as Moneybookers) that leaked in 2010 after a virtual private network was compromised. The combined 7.8 million records were added to HIBP's database.\n\nLater that month, electronic toy maker VTech was hacked, and an anonymous source privately provided a database containing nearly five million parents' records to HIBP. According to Hunt, this was the fourth largest consumer privacy breach to date.\n\nIn May 2016, an unprecedented series of very large data breaches that dated back several years were all released in a short timespan. These breaches included 360 million Myspace accounts from circa 2009, 164 million LinkedIn accounts from 2012, 65 million Tumblr accounts from early 2013, and 40 million accounts from adult dating service Fling.com. These datasets were all put up for sale by an anonymous hacker named \"peace_of_mind\", and were shortly thereafter provided to Hunt to be included in HIBP. In June 2016, an additional \"mega breach\" of 171 million accounts from Russian social network VK was added to HIBP's database.\n\nIn August 2017, BBC News featured Have I Been Pwned? on Hunt's discovery of a spamming operation that has been drawing on a list of 711.5 million email addresses.\n\nThe name \"Have I Been Pwned?\" is based on the script kiddie jargon term \"pwn\", which means \"to compromise or take control, specifically of another computer or application.\"\n\nHIBP's logo includes the text codice_1, which is a common SQL injection attack string. A hacker trying to take control of a website's database might use such an attack string to manipulate a website into running malicious code. Injection attacks are one of the most common vectors by which a database breach can occur; they are the #1 most common web application vulnerability on the OWASP Top 10 list.\n\n\n"}
{"id": "3954002", "url": "https://en.wikipedia.org/wiki?curid=3954002", "title": "Iddq testing", "text": "Iddq testing\n\nIddq testing is a method for testing CMOS integrated circuits for the presence of manufacturing faults. It relies on measuring the supply current (Idd) in the quiescent state (when the circuit is not switching and inputs are held at static values). The current consumed in the state is commonly called Iddq for Idd (quiescent) and hence the name.\n\nIddq testing uses the principle that in a correctly operating quiescent CMOS digital circuit, there is no static current path between the power supply and ground, except for a small amount of leakage. Many common semiconductor manufacturing faults will cause the current to increase by orders of magnitude, which can be easily detected. This has the advantage of checking the chip for many possible faults with one measurement. Another advantage is that it may catch faults that are not found by conventional stuck-at fault test vectors.\n\nIddq testing is somewhat more complex than just measuring the supply current. If a line is shorted to Vdd, for example, it will still draw no extra current if the gate driving the signal is attempting to set it to '1'. However, a different input that attempts to set the signal to 0 will show a large increase in quiescent current, signalling a bad part. Typical Iddq tests may use 20 or so inputs. Note that Iddq test inputs require only controllability, and not observability. This is because the observability is through the shared power supply connection.\n\nIddq testing has many advantages:\n\nDrawback:\nCompared to scan chain testing, Iddq testing is time consuming, and thus more expensive, as is achieved by current measurements that take much more time than reading digital pins in mass production.\n\nAs device geometry shrinks, i.e transistors and gates become smaller resulting in larger and more complex processors and SOC's (see Moore's law), the leakage current becomes much higher and less predictable. This makes it difficult to tell a low leakage part with a defect from a naturally high leakage part. Also, increasing circuit size means a single fault will have a lower percentage effect, making it harder for the test to detect. However, Iddq is so useful that designers are taking steps to keep it working. One particular technique that helps is power gating, where the entire power supply to each block can be switched off using a low leakage switch. This allows each block to be tested individually or in combination, which makes the tests much easier when compared to testing the whole chip.\n\n"}
{"id": "23743068", "url": "https://en.wikipedia.org/wiki?curid=23743068", "title": "International Microwave Power Institute", "text": "International Microwave Power Institute\n\nThe International Microwave Power Institute (IMPI) is an organization devoted to microwave energy and its usage. The organization has conducted surveys as well as educated the public to dispel microwave myths\n\nFounded in Canada in 1966, it is now headquartered in Mechanicsville, Virginia. It was initially created for industrial and scientific purposes, however in 1977, IMPI's purpose was expanded to deal with the evolution of microwave oven for the home.\n\nIts professional journal is the \"Journal of Microwave Power and Electromagnetic Energy\"\n\n"}
{"id": "57091934", "url": "https://en.wikipedia.org/wiki?curid=57091934", "title": "Ion implantation-induced nanoparticle formation", "text": "Ion implantation-induced nanoparticle formation\n\nIon implantation-induced nanoparticle formation is a technique for creating nanometer-sized particles for use in electronics.\n\nIon Implantation is a technique extensively used in the field of materials science for material modification. The effect it has on nanomaterials allows manipulation of mechanical, electronic, morphological and optical properties. \n\nOne-dimensional nano-materials are an important contributor to the creation of nano-devices i.e. field effect transistors, nanogenerators and solar cells. The offer the potential of high integration density, lower power consumption, higher speed and super high frequency. \nThe effects of ion implantation varies according to multiple variables. Collision cascade may occur during implantation and this causes of interstitials and vacancies in target materials (although these defects may be mitigated through dynamic annealing). Collision modes are nuclear collision, electron collision and charge exchange. Another process is the sputtering effect, which significantly affects the morphology and shape of nano-materials.\n"}
{"id": "22949908", "url": "https://en.wikipedia.org/wiki?curid=22949908", "title": "Isolator (microwave)", "text": "Isolator (microwave)\n\nAn isolator is a two-port device that transmits microwave or radio frequency power in one direction only. It is used to shield equipment on its input side, from the effects of conditions on its output side; for example, to prevent a microwave source being detuned by a mismatched load.\n\nAn isolator is a non-reciprocal device, with a non-symmetric scattering matrix. An ideal isolator transmits all the power entering port 1 to port 2, while absorbing all the power entering port 2, so that to within a phase-factor its S-matrix is\n\nTo achieve non-reciprocity, an isolator must necessarily incorporate a non-reciprocal material. At microwave frequencies this material is invariably a ferrite which is biased by a static magnetic field. The ferrite is positioned within the isolator such that the microwave signal presents it with a rotating magnetic field, with the rotation axis aligned with the direction of the static bias field. The behaviour of the ferrite depends on the sense of rotation with respect to the bias field, and hence is different for microwave signals travelling in opposite directions. Depending on the exact operating conditions, the signal travelling in one direction may either be phase-shifted, displaced from the ferrite or absorbed.\n\nIn this type the ferrite absorbs energy from the microwave signal travelling in one direction. A suitable rotating magnetic field is found in the TE mode of rectangular waveguide. The rotating field exists away from the centre-line of the broad wall, over the full height of the guide. However, to allow heat from the absorbed power to be conducted away, the ferrite does not usually extend from one broad-wall to the other, but is limited to a shallow strip on each face. For a given bias field, resonance absorption occurs over a fairly narrow frequency band, but since in practice the bias field is not perfectly uniform throughout the ferrite, the isolator functions over a somewhat wider band.\n\nThis type is superficially very similar to a resonance absorption isolator, but the magnetic biasing differs, and the energy from the backward travelling signal is absorbed in a resistive film or card on one face of the ferrite block rather than within the ferrite itself.\n\nThe bias field is weaker than that necessary to cause resonance at the operating frequency, but is instead designed to give the ferrite near-zero permeability for one sense of rotation of the microwave signal field. The bias polarity is such that this special condition arises for the forward signal; the backward signal sees the ferrite as an ordinary dielectric material (with little permeability, as the ferrite is already saturated by the bias field). Consequently, for the electromagnetic field of the forward signal, the ferrite has very low characteristic wave impedance, and the field tends to be excluded from the ferrite. This results in a null of the electric field of the forward signal on the surface of the ferrite where the resistive film is placed. Conversely for the backward signal, the electric field is strong over this surface and so its energy is dissipated in driving current through the film.\n\nIn rectangular waveguide the ferrite block will typically occupy the full height from one broad-wall to the other, with the resistive film on the side facing the centre-line of the guide.\n\nA circulator is a non-reciprocal three- or four-port device, in which power entering any port is transmitted to the next port in rotation (only). So to within a phase-factor, the scattering matrix for a three-port circulator is\n\nA two-port isolator is obtained simply by terminating one of the three ports with a matched load, which absorbs all the power entering it. The biased ferrite is part of the circulator and causes a differential phase-shift for signals travelling in different directions. The bias field is lower than that needed for resonance absorption, and so this type of isolator does not require such a heavy permanent magnet. Because the power is absorbed in an external load, cooling is less of a problem than with a resonance absorption isolator.\n\n\n"}
{"id": "40131684", "url": "https://en.wikipedia.org/wiki?curid=40131684", "title": "Liquipel", "text": "Liquipel\n\nLiquipel is a manufacturer of nanotechnology products that claims to provide super-hydrophobic protection and corrosion resistance to a variety of products, including smartphones, ear buds and headphones, tablets and other mobile electronics.\n\nThe company was established in 2009 as Gadget Evolutions by Kevin Bacon and Daniel “Danny” McPhail. It was launched as Liquipel at the 2012 International CES (Consumer Electronics Show) in Las Vegas, Nev. In 2013, the company was reformed as a limited liability corporation (LLC).\n\nThe company is headquartered in Santa Ana, Calif., and has affiliated, licensed operations in Australia, Canada, Hong Kong, Malaysia, Saudi Arabia, South Korea, Turkey and the United Arab Emirates.\n\nDevices are placed in a machine that creates a vacuum and then injects super-hydrophobic particles onto the outside and inside of the devices. Plasma then binds the formulation onto the devices’ surfaces and circuits, creating a microscopic gap of air between the object and liquid. The entire process, from start to finish, takes approximately 30 minutes.\n\nSince its debut at CES in 2012, Liquipel has won several awards. In April 2012, it was awarded a Silver Edison Award in the Material Science category for Enhanced Functionality.\n\nIn May 2012, Liquipel was included in Entrepreneur Magazine’s yearly 100 Brilliant Companies list, described as a “look at the brightest ideas, the hottest industries and the most insightful innovators”, in the Inventions category.\n\nIn November 2012, Popular Science Magazine named Liquipel its Grand Award Winner in the Gadgets category in its 25th annual Best of What's New Issue.\n"}
{"id": "406171", "url": "https://en.wikipedia.org/wiki?curid=406171", "title": "Lists of rockets", "text": "Lists of rockets\n\nThere are several different types of rockets. The following articles contain lists of rockets by type:\n\n\n"}
{"id": "44054581", "url": "https://en.wikipedia.org/wiki?curid=44054581", "title": "Loudclass", "text": "Loudclass\n\nLoudclass is a tablet-based classroom management platform developed by Chungdahm Learning in Seoul, South Korea. It consists of 3 interconnected elements: a desktop PC-based authoring client, an Android tablet-based classroom management app, and a cloud-based administrative web console bridging them. With over 28,000 individually paying users, it is the largest solution of its kind in Korea.\n\nIn 2013, Chungdahm launched its tablet-based Chungdahm 3.o curriculum. To support the initiative, the company developed in-house the Android-based classroom management system Chungdahm Smart Learning Platform (CSLP) and the PC-based authoring tool authoring tool CREO Author .\n\nThe 3.o curriculum rolled out over the course of 18 months to Chungdahm's 90 locations and 28,000 students in South Korea, during which time CSLP and CREO were packaged for global distribution and rebranded as Loudclass.\n\nLoudclass pilot programs have been announced for schools in Kyrgyzstan and Thailand.\n\nLoudclass runs in an instructor-led, 1:1 tablet-based classroom environment with optional internet access . Each student is given their own tablet with Loudclass installed. Tablet sharing is supported between classes (e.g. the tablets are assigned to a single classroom, with a single tablet used by several different students over the course of different classes) but it is recommended that within a given class, each student have their own tablet.\n\nThe instructor tablet running a teacher-specific version of the app, with specialized functionality to manage and support the class. A Wi-Fi access point is required to support communication between the student and teacher tablets throughout the class. While the Wi-Fi is mandatory, it is not necessary for it to be connected to the internet since all processing occurs directly on the teacher's tablet.\n\nAn optional but recommended feature is that the classroom have a projector or monitor to display the teacher's tablet screen using mirroring technology such as Miracast.\n\nLoudclass consists of 3 standalone components: Loudclass Author for creating instructional content, Loudclass Cloud for web-based administration, and Loudclass Interactive for real-time classroom management.\n\nLoudclass Author is used prior to class to create instructional content. It runs as a standalone desktop app installed on the author's PC. A new \"book\" is created by loading either static images, a PDF, or PPT file, which forms the background images of each page. From there, instructional content is overlaid on the image, elements including problem-solving activities such as multiple choice and matching questions, free text entry, and multimedia support of embedded image, audio, and video files. In addition, the developer can leave instructional 'hints,' cuing the teacher to conduct certain exercises, surveys, or discussions at various points in the textbook.\n\nTextbook authoring can be conducted either by designated content developers, or directly by teachers.\n\nLoudclass Cloud serves as an administrative portal for managing rosters of students, teachers, textbooks and classes . The administration process is typically run once at the start of the term, and only revisited when an incremental change occurs to one of the above rosters. Behind the scenes, Loudclass Cloud serves as an online storage and delivery mechanism of textbooks to the tablet.\n\nThe client is accessible via web browser.\n\nLoudclass Interactive is the Android-tablet based classroom management tool and flagship of the Loudclass platform. There are different versions of the app for students and teachers, with one instance of the teacher app serving as a hub for multiple students connected over the same Wi-Fi network.\n\nA consequence of Wi-Fi-based connectivity is that a class running Loudclass Interactive does not require internet access. All processing takes place on the teacher's tablet, with communication taking place over the local Wi-Fi network.\n\n"}
{"id": "29161878", "url": "https://en.wikipedia.org/wiki?curid=29161878", "title": "Micro-combustion", "text": "Micro-combustion\n\nMicro-combustion is the sequence of exothermic chemical reaction between a fuel and an oxidant accompanied by the production of heat and conversion of chemical species at micro level. The release of heat can result in the production of light in the form of either glowing or a flame. Fuels of interest often include organic compounds (especially hydrocarbons) in the gas, liquid or solid phase. The major problem of micro-combustion is the high surface to volume ratio. As the surface to volume ratio increases heat loss to walls of combustor increases which leads to flame quenching.\n\nThe development of miniaturized products such as microrobots, notebook computers, micro-aerial vehicles and other small scale devices is becoming increasingly important in our daily life. There is a growing interest in developing small scale combustors to power these micro-devices due to their inherent advantages of higher energy density, higher heat and mass transfer coefficients and shorter recharge times compared to electrochemical batteries. The energy density of hydrocarbon fuels is 20-50 times higher than the most advanced Li-ion concept based electrochemical batteries. The concept of the micro-heat engine was proposed by Epstein and Senturia in 1997. Since then, substantial amount of work has been done towards the development and application of such small scale devices to generate power through the combustion of hydrocarbon fuels. Micro-combustors are an attractive alternate to batteries as they have large surface area to volume ratio, due to which, significant amount of heat is transferred through the walls which leads to flame quenching. However, the increased rate of heat transfer through solid walls is advantageous in the case of steam reformers used for hydrogen production.\n\nB. Khandelwal et al. have experimentally studied the flame stability limits and other characteristics in a two staged micro combustor. They found out that staged combustor leads to higher flame stability limits, in addition to that they also offer higher temperature profiles which would be helpful in utilizing the heat produced by combustion. Maruta et al. have experimentally studied the flame propagation characteristics of premixed methane air mixtures in a 2.0 mm diameter straight quartz channel with a positive wall temperature gradient along the flow direction. This was a simple one-dimensional configuration to study flame stabilization characteristics in microchannels. Other researchers have studied the flame stabilization behavior and combustion performance in a Swiss roll combustor, micro-gas turbine engines, a micro-thermo-photovoltaic system, a free piston knock engine, a micro-tube combustor, radial channel combustors, and in various other types of micro-combustor.\n"}
{"id": "27901844", "url": "https://en.wikipedia.org/wiki?curid=27901844", "title": "Ministry of Energy (Iran)", "text": "Ministry of Energy (Iran)\n\nMinistry of Energy ( \"Vezârat-e Niru\"), is the main organ of the Government in charge of the regulation and implementation of policies applicable to energy, electricity, water and wastewater services.\n\nBeing founded on 17 October 1936, the ministry was established to provide electricity in Tehran. On 20 May 1943, its portfolio was expanded to include water management in the country. It was later renamed as the ministry of water and electricity on 17 March 1964. Later on 17 February 1975, after the parliamentary approval it became ministry of energy. On 10 May 1978, the ministry's function was expanded to contain the construction and operation of nuclear power plants in the country. \n\nAfter the Iranian revolution in 1979 some parliamentary changes took place for the duties of the ministry. On 12 July 1980, some of the functions of the ministry were given to the ministry of agriculture. On 7 March 1983, water management, and fair distribution of water resources were made part of the ministry of energy.\n\nMinistry of Energy is responsible for management of supply and demand of water, electricity, energy, and wastewater services and also promoting the training, research and technology, and bedding for goods and services market in water and electrical industry. It also plays a major role in preservation of natural resources, environment protection, public health promotion, welfare and self-sufficiency for sustainable development of the country. \n\nThe ministry has been on the sanction list of the European Union since 16 October 2012.\n\nThe ministry consists of five deputies as follows: \n\nThe energy ministers of Iran after the 1979 Iranian revolution:\n\n\n"}
{"id": "47090691", "url": "https://en.wikipedia.org/wiki?curid=47090691", "title": "Ministry of Science and Technology (Myanmar)", "text": "Ministry of Science and Technology (Myanmar)\n\nThe Ministry of Science and Technology (; abbreviated MOST) administers Burma's science and technology research and development affairs. MOST was established on 2 October 1996 under Order No. 30/96.\n\nThe latest Minister of the Ministry of Science and Technology is Khin San Yee, who was appointed by President Thein Sein in December 2015 after the death of Ko Ko Oo.\n\nThe Ministry of Science and Technology is organized under the Ministry of Education as Ministry of Education (Science and Technology) in April 2016 by the Government of Myanmar, led by Htin Kyaw. There are 57 Universities, Colleges and Technical Institutes under the MOE-ST.\n\n\n"}
{"id": "23515462", "url": "https://en.wikipedia.org/wiki?curid=23515462", "title": "Nevada–Texas–Utah retort", "text": "Nevada–Texas–Utah retort\n\nThe Nevada–Texas–Utah retort process (also known as NTU, Dundas–Howes or Rexco process) was an above-ground shale oil extraction technology to produce shale oil, a type of synthetic crude oil. It heated oil shale in a sealed vessel (retort) causing its decomposition into shale oil, oil shale gas and spent residue. The process was developed in the 1920s and used for shale oil production in the United States and in Australia. The process was simple to operate; however, it was ceased from the operation because of a small capacity and labor extensiveness.\n\nThe NTU retort was a successor of the 19th-century coal gasification retorts and it is considered as a predecessor of the gas combustion retort and the Paraho processes. It was invented and patented by Roy C. Dundas and Raymond T. Howes in 1923. The process was improved by David Davis and George Wightman Wallace, a consulting engineer of the NTU Company. In 1925, the NTU Company built a test plant at Sherman Cut near Casmalia, California. \nIn 1925–1929, the process was tested by the United States Bureau of Mines in the Oil Shale Experiment Station at Anvil Point in Rifle, Colorado. Retorting was carried out from 17 January to 28 June 1927. The plant was dismantled when work was terminated in June 1929. One of the leading technologist involved in this stage was Lewis Cass Karrick, an inventor of the Karrick process. In 1946–1951, two pilot plants with nominal capacities of 40 tons of raw oil shale were located at the same location. More than 12,000 barrels of shale oil was produced during this period. During the World War II, three NTU retorts were operated at Marangaroo, New South Wales, Australia. Almost 500,000 barrels of shale oil was produced by these retorts by retorting local torbanite.\n\nThe NTU retort was a vertical downdraft retort, which used internal combustion to generate heat for an oil shale pyrolysis (chemical decomposition). The retort was designed as a steel cylinder, lined with fire bricks. At the top it was equipped with an air supply pipe and at the bottom it was equipped with an exhaust pipe. The batch of crushed oil shale was loaded from the top; after that the retort was sealed. To start the pyrolysis process the fuel gas was ignited at the top of retort, and air injection into the retort started. The supply of fuel gas stopped after the upper quarter of the oil shale batch started to burn. At the same time the air injection continued bringing temperature in the burning part to about .\n\nThe heated gas caused a pyrolysis on the lower part of oil shale and produced shale oil and oil-shale gas are escaped from the retort through exhaust pipe at the bottom of retort. The pyrolysis occurred at the temperature about . By time-being, the combustion zones moved downward, and the char (semi-coke) produced as a solid residue of pyrolysis, ignited to burn as additional fuel for combustion. This caused the pyrolysis zone to move downward to the lower parts of retort. After combustion zone reached to the bottom of retort, air injection was stopped to stop combustion. After burning of char, the shale oil production ceased and only spent oil shale ash remained in the retort. The bottom of retort could opened for removal of the oil shale ash after retorting process. Operating the NTU retort with nominal capacity of 40 ton of raw oil shale, the full process cycle took about 40 hours. The shale oil yield varied from 80% to 85% of Fischer assay.\n\nThe advantage of the NTU retort process was simple design, simple operation, and limited need for external fuel. It was suitable for processing of wide variety of oil shales. The disadvantage of this process was a batch mode of operation not allowing continuous retorting, and therefore having small capacity being labor extensive at the same time. The process also had a relatively low oil yield and it required cooling water.\n\n"}
{"id": "39993709", "url": "https://en.wikipedia.org/wiki?curid=39993709", "title": "Nozh (explosive reactive armour)", "text": "Nozh (explosive reactive armour)\n\nNozh or Nizh (“Knife”, in Russian and Ukrainian, respectively) is a brand of explosive reactive armour designed by the Kharkiv Morozov Machine Building Design Bureau and manufactured in Ukraine by the state enterprise Fundamental Center of Crucial Technologies (FCCT-Microtek). Nozh modules have been provided by the government of Ukraine for the upgrade of Pakistani Al-Khalid tank.\n\nNozh modules are fitted to the exterior of a tank or other armored vehicle. As with all ERA modules, they are designed to explode when impacted by a weapon. Nozh modules differ from other ERA modules in that they are specifically designed to eliminate or minimize damage to adjacent modules, thus allowing for a 200% to 300% increased effectiveness against multiple weapon impacts, compared to other ERA module designs.\n"}
{"id": "43176498", "url": "https://en.wikipedia.org/wiki?curid=43176498", "title": "PIRCH (company)", "text": "PIRCH (company)\n\nPIRCH is a fixture and appliance retailer for kitchen, bath and outdoor products based in San Diego, California. Founded in 2009, the company expanded to ten metropolitan markets throughout the United States before pulling back to its four California stores in 2017. The stores feature an experiential showroom that allows consumers to test living appliances and bathroom plumbing fixtures as they would in their homes, while being advised by sales consultants.\n\nThe company was founded in San Diego in 2009 by a team of businessmen including Phil Roxworthy, James Fikes, and Tom Cavallo, under Rox Design DBA Fixtures Kitchen Bath Outdoor. The team shared the point of view that the appliance retail industry had a need for the re-invention of the complete in-store, shopping and delivery/installation experience, as there was \"a void in the marketplace, left by industries that had remained virtually unchanged for decades.”\nOn February 26, 2013,the private equity firm Catterton Partners announced its investment in Pirch as a minority shareholder. \nAs of 2017 Pirch has four stores in California, more than any other state, located in San Diego, Rancho Mirage, Costa Mesa, and Glendale. The company's fifth store, its first outside California, opened in a part of a shuttered Bloomingdale's home store at Oakbrook Center in Oak Brook, Illinois on March 8, 2014. A sixth opened on August 23, 2014 at the NorthPark Center in Dallas, Texas, in part of a former Barneys New York. On December 13, 2014, a seventh store opened at the Lenox Marketplace in Atlanta, Georgia, as part of the company's plan to expand nationwide. In March 2015, an eighth location was introduced at the Garden State Plaza in Paramus, New Jersey. A three-story location opened in the SoHo neighborhood of New York City in May 2016 as the ninth store. In May 2017, a tenth store was opened in Austin, Texas at the DOMAIN Northside.\n\nIn September 2017 the company announced a change of focus, closing all its stores outside of California.\n"}
{"id": "25668599", "url": "https://en.wikipedia.org/wiki?curid=25668599", "title": "Pentazenium", "text": "Pentazenium\n\nThe pentazenium cation (also known as pentanitrogen) is a positively charged polynitrogen ion of the chemical formula . Together with dinitrogen, solid nitrogen polymers and the azide anion, it is one of only three polynitrogen species obtained in bulk quantities.\n\nWithin the High Energy Density Matter research program, run by the U.S. Air Force since 1986, systematic attempts to approach polynitrogen compounds began in 1998, when Air Force Research Laboratory at Edwards AFB became interested in researching alternatives to the highly toxic hydrazine-based rocket fuel and simultaneously funded several such proposals. Karl O. Christe, then, a senior investigator at AFRL, chose to attempt building linear out of and , based on the proposed bond structure:\nThe reaction succeeded, and was created in sufficient quantities to be fully characterized by NMR, IR and Raman spectroscopy in 1999. The salt was highly explosive, but when was replaced by , a stronger Lewis acid, much more stable was produced, shock-resistant and thermally stable up to 60–70 °C. This made bulk quantities, easy handling, and X-ray crystal structure analysis possible.\n\nReaction of and in dry at −78 °C is the only known method so far:\n\n is capable of oxidizing water, NO, and , but not or ; its electron affinity is 10.44 eV (1018.4 kJ/mol). For this reason, must be prepared and handled in a dry environment:\n\nDue to stability of the fluoroantimonate, it is used as the precursor for all other known salts, typically accomplished by metathesis reactions in non-aqueous solvents such as HF, , , or , where suitable hexafluoroantimonates are insoluble:\n\nThe most stable salts of decompose when heated to 50–60 °C: , , and , while the most unstable salts that were obtained and studied, and were extremely shock and temperature sensitive, exploding in solutions as dilute as 0.5 mmol. A number of salts, such as fluoride, azide, nitrate, or perchlorate, cannot be formed.\n\nIn valence bond theory, pentazenium can be described by six resonance structures:\nwhere the last three pictured have smaller contributions to the overall structure because they have less favorable formal charge states than the first three.\n\nAccording to both \"ab initio\" calculations and the experimental X-ray structure, the cation is planar, symmetric, and approximately V-shaped, with bond angles 111° at the central atom (angle N2–N3–N4) and 168° at the second and fourth atoms (angles N1–N2–N3 and N3–N4–N5). The bond lengths for N1–N2 and N4–N5 are 1.10 Å and the bond lengths N2–N3 and N3–N4 are 1.30 Å.\n\n"}
{"id": "385509", "url": "https://en.wikipedia.org/wiki?curid=385509", "title": "Pin compatibility", "text": "Pin compatibility\n\nIn electronics, pin-compatible devices are electronic components, generally integrated circuits or expansion cards, sharing a common footprint and with the same functions assigned or usable on the same pins. Pin compatibility is a property desired by systems integrators as it allows a product to be updated without redesigning printed circuit boards, which can reduce costs and decrease time to market.\n\nAlthough devices which are pin-compatible share a common footprint, they are not necessarily electrically or thermally compatible. As a result, manufacturers often specify devices as being either \"pin-to-pin\" or \"drop-in\" compatible. Pin-compatible devices are generally produced to allow upgrading within a single product line, to allow end-of-life devices to be replaced with newer equivalents, or to compete with the equivalent products of other manufacturers.\n\n\"Pin-to-pin compatible\" devices share an assignment of functions to pins, but may have differing electrical characteristics (supply voltages, or oscillator frequencies) or thermal characteristics (TDPs, reflow curves, or temperature tolerances). As a result, their use in a system may require that portions of the system, such as its power delivery subsystem, be adapted to fit the new component.\n\nA common example of pin-to-pin compatible devices which may not be electrically compatible are the 7400 series integrated circuits. The 7400 series devices have been produced on a number of different manufacturing processes, but have retained the same pinouts throughout. For example, all 7405 devices provide six NOT gates (or inverters) but may have incompatible supply voltage tolerances.\n\n\nIn other cases, particularly with computers, devices may be pin-to-pin compatible but made otherwise incompatible as a result of market segmentation. For example, Intel Skylake desktop-class Core and Xeon E3v5 processors both use the LGA 1151 socket, but motherboards using C230-series chipsets will only be compatible with Xeon-branded processors, and will not work with Core-branded processors.\n\nA \"drop-in compatible\" device is a device which may be swapped with another without need to make compensating alterations to the system the device was a part of. The device will have the same functions available on the same pins, and will be electrically and thermally compatible. Such devices may not be an exact match to the devices they can replace. For example, they may have a wider range of supply voltage or temperature tolerances.\n\n\"Software-compatible\" devices are devices which are able to run the same software to produce the same results without the software having to be modified first.\n\nMicrocontrollers, FPGAs, and other programmable devices may be pin-to-pin compatible from the perspective of the program on the device, but incompatible in terms of hardware. For example, the device may take the signal on pin X, negate it, and output the result on pin Y. If the method of configuring a pin remains the same but the package of the device (such as TSSOP or QFN) changes, the program will continue to function but the physical locations of the pins the program works with may change.\n\nA device may also be pin-compatible while being software-incompatible. This may occur when the device uses a different instruction set, or if the device has a multiplexer attached to a pin (which, for example, may allow the switching of the pin between being driven as GPIO or by an A/D) and that multiplexer selects, by default, a different input source than is selected on the device being replaced.\n\nTo ease the use of software-incompatible devices, manufacturers often provide hardware abstraction layers. Examples of these include, CMSIS for ARM Cortex-M processors and the now-deprecated HAL subsystem for UNIX-like operating systems.\n\n\n"}
{"id": "48118447", "url": "https://en.wikipedia.org/wiki?curid=48118447", "title": "SICAL", "text": "SICAL\n\nSICAL is a Portuguese coffee brand company under the Nestlé portfolio since 1987.\n"}
{"id": "31855679", "url": "https://en.wikipedia.org/wiki?curid=31855679", "title": "SVTC Technologies", "text": "SVTC Technologies\n\nSVTC Technologies was a technology services company that provided development and commercialization services for semiconductor process-based technologies and products. SVTC operated from 2004 to October 2012.\n\nIt operated facilities in San Jose, California and Austin, Texas. SVTC provided development and production services for MEMS, microfluidics, high voltage, and Through-silicon via (TSV) technologies. These technologies are used in a variety of industries including semiconductor, life sciences, aerospace and defense, consumer-mobility and clean energy.\n\nSVTC Technologies' fabrication facilities included CMOS and MEMS process tools as well as microscopy (Scanning electron microscope, Transmission electron microscopy, Atomic Force Microscope, Focused ion beam) and failure analysis equipment.\n\nSVTC Technologies began operations as a subsidiary of Cypress Semiconductor in 2004. At that time, it was named the Silicon Valley Technology Center. It spun out of Cypress Semiconductor's Fab 1 facility. It became a privately owned, independent company in 2007. SVTC is funded by Oakhill Capital Partners and Tallwood Venture Capital.\nIn December 2007, SVTC acquired the Advanced Technology Development Facility (ATDF), a subsidiary of SEMATECH. At that time, the name was changed to SVTC Technologies.\n\nIn May 2011, SVTC attained ISO 13485 certification which is targeted for next generation life science products. SVTC is also ITAR registered. ITAR is required for many defense applications.\n\nSVTC closed down in October 2012. Per an e-mail from the company, SVTC \"ceased operations and executed a general assignment for the benefit of creditors as of October 15, 2012\".\n\nOn April 10, 2011, the United States Department of Energy (DOE) awarded SVTC Solar, a subsidiary of SVTC Technologies, $25 million in funding as a part of the DOE's SunShot Initiative. This grant supports the start-up of the first photovoltaics (PV) manufacturing development facility (MDF) in the U.S. to reduce the costs and development time for the PV industry.\n\nOn August 3, 2011, it was announced that the grant from the Department of Energy was increased to $30M. In addition, another $55M of equipment, materials and funding was committed by industry partners and customers bringing the total funding to $85M.\n\nThe SunShot Initiative was launched to make large-scale photovoltaic solar energy systems competitive with other forms of energy and eliminate the industry's reliance on subsidies by the end of the decade. The initiative's goal of reducing the costs of PV solar systems to roughly six cents per kilowatt-hour will allow solar energy systems to be broadly deployed across the country. \"The SunShot Initiative will not only keep the United States at the forefront in solar energy research and development, but will help us win the worldwide race to build a solar manufacturing industry that produces solar systems that are cost competitive with fossil fuels,\" said U.S. Secretary of Energy, Steven Chu.\n\nThe DOE program provides developers two access models: a consortium model patterned after the Semiconductor Manufacturing Technology (SEMATECH) effort of the 1980s, and a commercial model based on SVTC's Technology Development Process (TDP) — a process that provides customers access to leading-edge technology and enables accelerated commercial development while protecting their valuable IP. The SVTC MDF will provide solar energy innovators access to PV technology: multi- and monocrystalline silicon PV, selective-emitter cells, back-contact cells, emitter wrap-through cells, PERC cells, PERL cells, thin-silicon, 3D cells and hybrid PV technologies.\n\nMEMS and nanotechnology development for the Aerospace & Defense industry is on-going at SVTC Technologies. Advanced military and space systems utilize MEMS technology in the areas of infrared (IR) imaging devices, optical and electrical switching, displays, munitions, inertial navigation, laser communications, sensors, and night vision systems. Nanotechnology is used to integrate transistors from compound semiconductor technologies together with transistors on a wafer fabricated using a standard silicon CMOS foundry process. The compound semiconductor technologies provide much higher speed-breakdown voltages than CMOS transistors, while CMOS technology provides higher integration density for computation functions. The result is a 10× better power-to-performance improvement versus the equivalent silicon devices.\n\n"}
{"id": "2489342", "url": "https://en.wikipedia.org/wiki?curid=2489342", "title": "Sausage making", "text": "Sausage making\n\nThe origins of meat preservation are lost to the ages but probably began when humans began to realize the preservative value of salt. \nSausage making originally developed as a means to preserve and transport meat. Primitive societies learned that dried berries and spices could be added to dried meat. By 600-500 BC there is mention of sausages from China, Rome and Greece.\nThe procedure of stuffing meat into casings remains basically the same today, but sausage recipes have been greatly refined and sausage making has become a highly respected culinary art.\n\nSausages come in two main types: fresh and cured. Cured sausages may be either cooked or dried. Most cured sausages are smoked, but this is not mandatory. The curing process itself changes the meat and imparts its own flavors. An example is the difference in taste between a pork roast and a ham.\n\nAll smoked sausages are cured. The reason is the threat of botulism. The bacterium responsible, \"Clostridium botulinum\", is ubiquitous in the environment, grows in the anaerobic conditions created in the interior of the sausage, and thrives in the to temperature range common in the smoke house and subsequent ambient storage. Thus, for safety reasons, sausages are cured before smoking.\n\nFresh sausages are simply seasoned ground meats that are cooked before serving. Fresh sausages normally do not use cure (Prague powder #1) although cure can be used if desired. In addition fresh sausages typically do not use smoke flavors, although liquid smoke can be used. Fresh sausages are never smoked in a cold smoker because of the danger of botulism.\n\nThe primary seasoning agents in fresh sausages are salt and sugar along with various savory herbs and spices, and often vegetables, including onion and garlic.\n\nA British fresh sausage typically contains around 10% butcher's rusk, 10% water, 2.5% seasoning, and 77.5% meat. At the point of sale, British sausages will often be labelled as \"actual meat content X%\". As meat can be fatty or lean, the X% is calculated using reference tables with the intention to give a fairer representation of the \"visual lean\" meat content.\n\nCured sausages differ from fresh sausages by including 2 teaspoons of cure (Prague powder #1) per 10 pounds of finished product. This is usually interpreted per 10 pounds of meat. This works out to 4 ounces of cure for 100 pounds of sausage.\n\nNext the product is typically hot smoked. However, similar effects can be achieved by incorporating liquid smoke in the recipe. Smoking temperatures vary and are typically less than . At a temperature of 152 °F (67 °C) these sausages are fully cooked.\n\nIn some cases cold smoke is used. If so, then the sausage may have been previously cooked in a water bath held at the proper temperature. An example of this process is the preparation of Braunschweiger. In this style of sausage, after stuffing into to hog buns or fiberous casings, the sausage is submerged in water for 2 to 2½ hours until the internal temperature reaches . At this point the sausage should be chilled in ice water, then cold smoked at a temperature of for 2–3 hours.\n\nCured dry sausages are prepared in a fashion similar to cured cooked sausages. The major difference is that Prague powder #2 will be used in place of Prague powder #1. In addition, certified meats must be used. Since these products are never heated to a temperature that can kill trichinella parasites, it is necessary to accomplish this by other methods. The usual method is via freezing. Pork may be rendered acceptable for use in dry sausages by freezing it using the following guidelines:\n\n\nThe specific regulations are quite complex. They depend on the thickness of the cuts of meat, the packaging method, and other factors. In addition there are very specific requirements as to the times in the drying rooms and the temperatures in the smoke rooms.\n\nWhile it is quite feasible for the small sausage kitchen or hobbyist to produce excellent cured dry sausages, a great deal of technical information is required. Alternatively, certified pork can be simply purchased.\n\nEquipment depends on scale, a small home grinder and some basic measuring tools may be all that is required. In a larger scale commercial operation, more high volume equipment will be required.\n\nRegarded as the three most important pieces of equipment, regardless of the amount of sausage being made are an accurate thermometer, a calibrated scale, and a meat grinder. Smoked or smoke/cooked sausages require a smoker (small batches) or a commercial smokehouse. Emulsion-type cooked sausages, such as frankfurters or bologna, use a bowl chopper to make finely ground meat batter that is put into casings and cooked or smoked.\n\nA variety of fresh meats may be used for making sausage, the most common are from beef, pork, lamb, chicken, turkey, and game. Meat should be fresh, high quality, have the proper lean-to-fat ratio and good binding qualities. The meat should not be contaminated with bacteria or other microorganisms. Combining spices and seasonings in amounts that complement each other is important.\n\nMaking dry sausages involves curing salts, which contain sodium nitrite and sodium nitrate. Nitrites are used for all types of sausages and are the most common. Nitrates are used only in the preparation of the cured dry style of sausages. Over a period of time the nitrates are converted into nitrites by endogenous or added bacteria.\n\nThe human digestive system manufactures nitrites, which is thought to be what prevents botulism, which would thrive in the anaerobic conditions and temperature range of the digestive system (gut).\n\nCured meat products typically contain less than 40 ppm nitrites.\n\nPotassium nitrite and potassium nitrate additions allow the production of sausages with lower levels of sodium. When using the potassium form, it is necessary to include other ingredients to mask the bitter flavours it imparts. Old recipes use saltpetre which is not recommended. The primary reason is that often these old recipes contain many times more curing ingredients than are appropriate. Modern techniques are readily available and do a much better job.\n\nIn the sausage industry the nitrites and nitrates are pre-formulated into products called Prague powder #1 and Prague powder #2. Prague powder #1 contains 6.25% sodium nitrite and 93.75% sodium chloride and is used for the preparation of all cured meats and sausages other than the dry type. Prague powder #2 contains 1 ounce of sodium nitrite (6.25%) and 0.64 ounces sodium nitrate (4.0%) per pound of finished product (the remaining 14.36 ounces is sodium chloride) and is used for the preparation of cured dry sausages. Prague powder #2 should never be used on any product that will be fried at high temperature (e.g. bacon) because of the resulting formation of nitrosamines.\n\nWhen using cure, it is very important to never exceed the recommended amount of 2.5 grams of Prague powder #1 in 1 kilogram of meat (4 ounces/100 pounds). Equivalently this is 10 mL for 4.5 kg (2 teaspoons for 10 pounds). Note that the maximum allowable amount of sodium nitrite and potassium nitrite is governed by regulations and is limited to 7 grams per 45 kg (0.25 ounces per 100 pounds) of chopped meat. Since Prague powder #1 is a 1:15 dilution (in 0.45 kg of Prague powder #1 30 grams is sodium nitrite and 425 grams are common table salt), we get the proper amount at a rate of 114 grams added to of meat.\n\nSodium nitrate and potassium nitrate are limited to 1.7 gram per kilogram (2.75 ounces per 100 pounds).\n\nSodium and potassium nitrite are quite toxic to humans with the lethal dose being about 4 grams. As little as 22 mg/kg of body weight can cause death. This is about 2.2 grams for a body mass of 100 kg. Thus, there is enough sodium nitrite in 2 ounces of Prague powder #1 to kill a person.\n\nMorton's Tenderquick is the brand name of another formulation of sodium nitrite, with salt and sugars added. It is not the same concentration as either \"Prague powder #1 or #2\". Since certainty about the amount of nitrite present in a recipe is essential for safety, one cannot take a recipe designed for Prague powder and simply substitute like amounts of such products as Morton's Tenderquick. To do so would invite the risk of botulism poisoning. Similarly, one cannot just substitute Prague powder #1 in place of Morton's Tenderquick. For any such substitutions, one must calculate the exact amount of nitrite required and make the proper adjustments.\nNote: The volume-to-weight ratio applies to the herbs and spices only. This in no way indicates the particular amount for a given recipe.\n\n\n"}
{"id": "43908163", "url": "https://en.wikipedia.org/wiki?curid=43908163", "title": "Sciothericum telescopicum", "text": "Sciothericum telescopicum\n\nSciothericum telescopicum was a sundial (sciothericum) that incorporated a telescope (telescopicum) for greater accuracy in determining exactly when noon occurred. It was invented by William Molyneux in Ireland in 1686. The device used a telescopic site to determine the position of the center of the sun relative to a double gnomon and could thus determine the time of noon to within 15 seconds. The improved accuracy was important for geography, navigation and astronomy calculations.\n"}
{"id": "2909793", "url": "https://en.wikipedia.org/wiki?curid=2909793", "title": "Smith Corona", "text": "Smith Corona\n\nSmith Corona is a US manufacturer of thermal labels, direct thermal labels, and thermal ribbons used in warehouses for primarily barcode labels. Once a large U.S. typewriter and mechanical calculator manufacturer, it expanded aggressively during the 1960s to become a broad-based industrial conglomerate whose products extended to paints, foods, and paper. The mechanical calculator sector was wiped out in the early 1970s by the production of cheap electronic calculators, and the typewriter business collapsed in the mid-1980s due to the introduction of PC-based word processing.\n\nSmith Corona did address this by manufacturing word processing typewriters such as PWP 1400 model. Its competitors were Brother, Olivetti, Adler, Olympia and IBM. In late 2010, Smith Corona entered the industrial ribbon and label market.\n\nThe company no longer manufacturers typewriters or calculators, but does manufacture large quantities of barcode and shipping labels and thermal ribbons used in thermal transfer printers. Their facility is in Cleveland, Ohio. Smith Corona now competes with distributors of Zebra Technologies supplies, packaging companies like Uline and various other private companies.\n\nThe company originated in 1886, when the Smith Premier Typewriter Company was established by the brothers Lyman Cornelius Smith, Wilbert Smith, Monroe C. Smith and Hurlburt Smith. The typewriter was the first to use a double keyboard, but it was not the first typewriter that typed both upper and lower case characters; that honor belonged to the Remington #2 that was introduced in 1877-78, a decade before the first model of the Smith Premier was placed on the market. The advertisements \"cunningly boasted\" that there was \"a key for every character!\"\n\nIn 1889, the \"Smith-Premier\", the first typewriter to bear the Smith name, was manufactured in Lyman C. Smith's gun factory on South Clinton Street in Syracuse, New York. Alexander T. Brown, an employee, invented the machine, and Wilbert Smith financed the construction of the prototype.\n\nLyman Smith and Wilbert Smith owned a gun factory in Syracuse and after they hired Brown he sparked Wilbert's interest in financing and improving typewriters. The parts of a typewriter are surprisingly similar to those of a shotgun \"so producing the typewriter at the gun factory was logical and easy.\"\n\nBusiness was going so well, in 1888, the Smith brothers discontinued shotgun production and strictly produced typewriters with the help of their younger brothers, Monroe Smith and Hurlburt Smith.\n\nThe Smith Premier Typewriter Company was established in 1886 by brothers; Lyman Cornelius Smith, Wilbert Smith, Monroe C. Smith and Hurlbut Smith who were born in Lisle, New York.\n\nEldest brother, Leroy Smith, invented the Peerless typewriter in 1891, which \"greatly resembled\" the \"Smith-Premier\". It had the same double keyboard with 76 characters and also had \"blind type\" so the typist could not see what was being printed.\n\nDuring 1893, Smith joined with the Union Typewriter Company, a trust in Syracuse which included rival firms Remington, Caligraph, Densmore and Yost.\n\nNot long after, Union took action and blocked the Smith Premier Typewriter Company from using the new front strike design, which allowed typists to see the paper as they typed. As a result, the Smith brothers quit in 1903 and founded L. C. Smith & Bros. Typewriter Company. The new company soon released the \"L.C. Smith & Bros. Model No. 2\", which was an odd beginning because, a full year later, they released the \"L.C. Smith & Bros. Model No. 1.\" Carl Gabrielson invented both models.\n\nIn 1906, the Rose Typewriter Company of New York City marketed the first successful portable typewriter. They were bought out by Smith in 1909, renamed Standard Typewriter Company, and moved upstate to Groton, New York.\n\nTo promote usage of the typewriter, the company began by offering typing services at the company headquarters located at the corner of East Genesee and Washington streets in Syracuse. An advertisement on December 27, 1904, for \"Smith Premier\" typewriters, touted the \"Employee Department\" which offered services such as finding a \"competent stenographer (male or female) to operate any make of machine.\" The company advertised they could provide the services promptly, saving clients time and trouble and \"examining\" all applicants. Operators could perform duties such as stenographer, typewriter, telegrapher and bookkeeper.\n\nWith the success of their Corona model in 1914, Standard Typewriter Company was renamed again and became the Corona Typewriter Company. \"Smith Corona\" was created when L. C. Smith & Bros. united with Corona Typewriter in 1926, with L. C. Smith & Bros. making office typewriters and Corona Typewriter making portables.\n\nProduction shifted from typewriters to various military weapons and parts during World War II. In October 1942, Smith-Corona Typewriter Company began producing M1903A3 Springfield rifles at its plant in Syracuse, with assistance from Remington Arms and High Standard Manufacturing Company. Subcontractor barrels give unusual collector value to some of these 234,580 Springfield rifles. Serial numbers 3608000 to 3707999 and 4708000 to 4992000 carry the Smith-Corona name on the receiver ring. While many M1903A3 rifles manufactured by Remington have 2-groove barrels, most rifles assembled by Smith Corona used 4-groove barrels manufactured by High Standard, and approximately five thousand of the barrels finished by High Standard were from 6-groove barrel blanks made by Savage Arms.\n\nBolts on Remington M1903A3 rifles have a parkerized finish and are stamped with the letter R at the root of the handle; while Smith Corona bolts are blued and usually stamped with a letter X on top of the handle, although some are unmarked. Some extractors on Smith Corona rifles are stamped with a letter S on the bottom. Stamped steel stock fittings were generally blued, although some were parkerized in late production. Butt plates of the Smith Corona rifles were checkered with 10 or 11 lines per inch, while Remington used 16 lines per inch. Rifle production ceased on February 19, 1944, when supplies of standard M1 Garand rifles were considered adequate. Some of the rifles were never issued, while others were reconditioned in government armories after service use. Reconditioned rifles often have substituted parts from Remington or Springfield manufacture. Most rifles were stored after the war until many were sold through the Civilian Marksmanship Program in the early 1960s.\n\nAfter the war, the company concentrated on making its typewriters more convenient and efficient for use in business offices. Typewriter sales peaked after World War II; in response to a demand for typewriters capable of faster output, \"Smith Corona\" introduced electric typewriters in 1955. Electric portables, intended for traveling writers and business people, but later widely purchased for general home use, were introduced in 1957. The new portable electric typewriters would become an essential tool for generations of U.S. high school and college students.\n\nIn a diversification move into the wider office technology sector, \"Smith Corona\" purchased the Kleinschmidt Corporation in 1956 and Marchant Calculator in 1958, changing its corporate name to Smith-Corona Marchant Inc. Also in 1958, \"Smith Corona\" acquired British Typewriters, Ltd. of West Bromwich, England, a company that made small portable typewriters. The company invented the typewriter power carriage return in 1960, the same year it moved from Syracuse to Cortland, New York and opened new corporate headquarters on Park Avenue in New York City. 1960 also saw the company's first foray into the photocopier business with the Vivicopy range of machines, also the accounting machinery market with a range of punch card and tape products manufactured for it in Germany by Kienzle. Still on the acquisition trail, SCM acquired the St. Louis Microstatic Company in 1961. This merger gave rise to the Model 33 Electrostatic Copier, which went on sale in April 1962.\n\nThus by the mid-1960s SCM had become a major supplier to the office equipment market, offering photocopiers, typewriters and calculating machines.\n\nIn 1962, \"Smith Corona\" changed its corporate name to SCM Corporation and adopted the tribar SCM logo. In 1967, SCM purchased the Allied Paper Corporation for $33 million. The new paper-making division was named SCM Allied Paper. In the same year, SCM also merged with The Glidden paint company. Glidden was reorganized as the Glidden-Durkee division of SCM. One reason for this merger was that Glidden saw SCM's bid as a \"White Knight\" bid in preference to an alternative offer from Greatamerica Corporation in Dallas, Texas and General Anniline & Film of New York. In its turn, the acquisition put the (now much larger) SCM itself beyond the reach of any potential hostile bidders of the time. It was also hoped that Glidden's research into paper coatings would be useful in SCM's copier business.\n\nThe \"Letterpack\" product of 1967 was a handset on which personal voice messages could be recorded on small tape cartridges which could be mailed to the recipient (who needed another handset to replay it). The cartridges lasted 3, 6 or 10 minutes, and a pair of handsets cost $7.\n\nIn 1965, SCM was instrumental in developing smaller computers for the business market. The basic computer consisted of an electric typewriter, plug boards, card readers, paper and mag tape readers. The client would purchase a computer and programs specifically designed for their operation. This data processing division was eventually sold to Control Data Corporation in the early '70s.\n\nIn 1966, SCM bought the consumer product company Proctor Silex, manufacturers of toasters and can-openers.\n\nIn 1973, a new typewriter manufacturing facility, employing 1,300 people, was erected in Singapore.\n\nIn 1973, SCM introduced a cartridge ribbon which eliminated the long-standing problem of getting ink-stained fingers from hand-threading a replacement spool of inked ribbon.\n\nThe calculator market was devastated by cheap electronic pocket calculators in the mid-1970s. The typewriter market too was being undermined by cheap imports from the Far East, this being a contributing factor in the closure of the West Bromwich, England, plant in 1981. By 1985, personal computers were being widely used for word processing, and SCM launched their first portable word processor, along with the first portable typewriter that included an electronic spelling function. But these products were insufficient to counter the diminishing size of the typewriter market. This, and the corporate bloat associated with being a conglomerate whose many different operating divisions had no inherent business logic, rendered it vulnerable to takeover. Thus, in 1986, SCM was taken over by Hanson Plc, a UK-based vulture capitalist. Hanson immediately disposed of some SCM divisions, and the headquarters building in New York City, for a significant profit.\n\nThe company moved its remaining typewriter manufacturing operations from Cortland to Mexico in 1995 and announced it was cutting 750 jobs as a result of continuing sales declines. Shortly thereafter, the company declared bankruptcy. Since 1995, the company concentrated on sales of portable electronic typewriters, as well as typewriter and word processor supplies. The company's then current electronic models featured LCD displays, built-in dictionaries, and spell check and grammar check features.\n\nAfter being acquired by a private company during its second bankruptcy in 2000, Smith Corona briefly moved all typewriter manufacturing and typewriter supplies manufacturing to Cleveland, Ohio. Within five years Smith Corona quit manufacturing all typewriters. As the typewriter supply business continued to decline, Smith Corona decided to leverage its expertise in ribbons and thermal technologies it had previously used in the typewriter business in the growing thermal label market.\n\n\n"}
{"id": "914987", "url": "https://en.wikipedia.org/wiki?curid=914987", "title": "Soroban", "text": "Soroban\n\nThe is an abacus developed in Japan. It is derived from the ancient Chinese suanpan, imported to Japan in the 14th century. Like the suanpan, the soroban is still used today, despite the proliferation of practical and affordable pocket electronic calculators.\n\nThe soroban is composed of an odd number of columns or rods, each having beads: one bead having a value of five, called and four beads each having a value of one, called . Each set of beads of each rod is divided by a bar known as a reckoning bar. The number and size of beads in each rod make a standard-sized 13-rod soroban much less bulky than a standard-sized suanpan of similar expressive power.\n\nThe number of rods in a soroban is always odd and never fewer than nine. Basic models usually have thirteen rods, but the number of rods on practical or standard models often increases to 21, 23, 27 or even 31, thus allowing calculation of more digits or representations of several different numbers at the same time. Each rod represents a digit, and a larger number of rods allows the representation of more digits, either in singular form or during operations.\n\nThe beads and rods are made of a variety of different materials. Most soroban made in Japan are made of wood and have wood, metal, rattan, or bamboo rods for the beads to slide on. The beads themselves are usually biconal (shaped like a double-cone). They are normally made of wood, although the beads of some soroban, especially those made outside Japan, can be marble, stone, or even plastic. The cost of a soroban is commensurate with the materials used in its construction.\n\nOne unique feature that sets the soroban apart from its Chinese cousin is a dot marking every third rod in a soroban. These are \"unit rods\" and any one of them is designated to denote the last digit of the whole number part of the calculation answer. Any number that is represented on rods to the right of this designated rod is part of the decimal part of the answer, unless the number is part of a division or multiplication calculation. Unit rods to the left of the designated one also aid in place value by denoting the groups in the number (such as thousands, millions, etc.). Suanpan usually do not have this feature.\n\nThe soroban uses a decimal system, where each of the rods can represent a single digit from 0 to 9. By moving beads towards the reckoning bar, they are put in the \"on\" position; i.e., they assume value. For the \"five bead\" this means it is moved downwards, while \"one beads\" are moved upwards. In this manner, all digits from 0 to 9 can be represented by different configurations of beads, as shown below:\n\nThese digits can subsequently be used to represent multiple-digit numbers. This is done in the same way as in Western, decimal notation: the rightmost digit represents units, the one to the left of it represents tens, etc. The number \"8036\", for instance, is represented by the following configuration:\n\nThe soroban user is free to choose which rod is used for the units; typically this will be one of the rods marked with a dot (see the 6 in the example above). Any digits to the right of the units represent decimals: tenths, hundredths, etc. In order to change \"8036\" into \"80.36\", for instance, the user places the digits in such a way that the 0 falls on a rod marked with a dot:\n\nThe methods of addition and subtraction on a soroban are basically the same as the equivalent operations on a suanpan, with basic addition and subtraction making use of a complementary number to add or subtract ten in carrying over.\n\nThere are many methods to perform both multiplication and division on a soroban, especially Chinese methods that came with the importation of the suanpan. The authority in Japan on the soroban, the Japan Abacus Committee, has recommended so-called standard methods for both multiplication and division which require only the use of the multiplication table. These methods were chosen for efficiency and speed in calculation.\n\nBecause the soroban developed through a reduction in the number of beads from seven, to six, and then to the present five, these methods can be used on the suanpan as well as on soroban produced before the 1930s, which have five \"one\" beads and one \"five\" bead.\n\nThe Japanese abacus has been taught in school for over 500 years, deeply rooted in the value of learning the fundamentals as a form of art. However, the introduction of the West during the Meiji period and then again after World War II has gradually altered the Japanese education system. Now, the strive is for speed and turning out deliverables rather than understanding the subtle intricacies of the concepts behind the product. Calculators replace sorobans and elementrary schools are no longer required to teach the abacus. If they do, it is by choice. The growing popularity of calculators within the context of Japanese modernization has driven the study of soroban from public schools to private after school classrooms. Where once it was an institutionally required subject in school for children grades 2 to 6, current laws have made keeping this art form and perspective on math practiced amongst the younger generations more lenient. Today, it shifted from a given to a game where one can take The Japanese Chamber of Commerce and Industry's examination in order to obtain a certificate and license. \n\nThere are six levels of mastery, starting from sixth-grade (very skilled) all the way up to first-grade (for those who have completely mastered the use of the soroban). Those obtaining at least a third-grade certificate/license are qualified to work in public corporations.\n\nThe soroban is still taught in some primary schools as a way to visualize and grapple with mathematical concepts. The practice of soroban includes the teacher reciting a string of numbers (addition, subtraction, multiplication, and division) in a song-like manner where at the end, the answer is given by the teacher. This helps train the ability to follow the tempo given by the teacher while remaining calm and accurate. In this way, it reflects on a fundamental aspect of Japanese culture of practicing meditative repetition in every aspect of life. Primary school students often bring two soroban to class, one with the modern configuration and the one having the older configuration of one heavenly bead and five earth beads.\n\nShortly after the beginning of one’s soroban studies, drills to enhance mental calculation, known as anzan(暗算, \"blind calculation\") in Japanese are incorporated. Students are asked to mentally by visualizing the soroban and working out the problem by moving the beads theoretically in one’s mind. The mastery of anzan is one reason why, despite the access to handheld calculators, some parents still send their children to private tutors to learn the soroban.\n\nThe soroban is also the basis for two kinds of abaci developed for the use of blind people. One is the toggle-type abacus wherein flip switches are used instead of beads. The second is the Cranmer abacus which has circular beads, longer rods, and a leather backcover so the beads do not slide around when in use.\n\nThe soroban's physical resemblance is derived from the suanpan but the number of beads is identical to the Roman abacus, which had four beads below and one at the top.\n\nMost historians on the soroban agree that it has its roots on the suanpan's importation to Japan via the Korean peninsula around the 14th century. When the suanpan first became native to Japan as the soroban (with its beads modified for ease of use), it had two heavenly beads and five earth beads. But the soroban was not widely used until the 17th century, although it was in use by Japanese merchants since its introduction. Once the soroban became popularly known, several Japanese mathematicians, including Seki Kōwa, studied it extensively. These studies became evident on the improvements on the soroban itself and the operations used on it.\n\nIn the construction of the soroban itself, the number of beads had begun to decrease, especially at a time when the basis for Japanese currency was shifted from hexadecimal to decimal. In around 1850, one heavenly bead was removed from the suanpan configuration of two heavenly beads and five earth beads. This new Japanese configuration existed concurrently with the suanpan until the start of the Meiji era, after which the suanpan fell completely out of use. In 1891, Irie Garyū further removed one earth bead, forming the modern configuration of one heavenly bead and four earth beads. This configuration was later reintroduced in 1930 and became popular in the 1940s.\n\nAlso, when the suanpan was imported to Japan, it came along with its division table. The method of using the table was called in Japanese, while the table itself was called the . The division table used along with the suanpan was more popular because of the original hexadecimal configuration of Japanese currency. But because using the division table was complicated and it should be remembered along with the multiplication table, it soon fell out in 1935 (soon after the soroban's present form was reintroduced in 1930), with a so-called standard method replacing the use of the division table. This standard method of division, recommended today by the Japan Abacus Committee, is in fact an old method which used counting rods, first suggested by mathematician Momokawa Chubei in 1645, and therefore had to compete with the division table during the latter's heyday.\nOn November 12, 1946, a contest was held in Tokyo between the Japanese soroban, used by Kiyoshi Matsuzaki, and an electric calculator, operated by US Army Private Thomas Nathan Wood. The basis for scoring in the contest was speed and accuracy of results in all four basic arithmetic operations and a problem which combines all four. The soroban won 4 to 1, with the electric calculator prevailing in multiplication.\n\nAbout the event, the \"Nippon Times\" newspaper reported that \"Civilization ... tottered\" that day, while the \"Stars and Stripes\" newspaper described the soroban's \"decisive\" victory as an event in which \"the machine age took a step backward...\".\n\nThe breakdown of results is as follows:\nEven with the improvement of technology involving calculators, this event has yet to be replicated officially.\n\n\n"}
{"id": "414738", "url": "https://en.wikipedia.org/wiki?curid=414738", "title": "Surge protector", "text": "Surge protector\n\nA surge protector (or spike suppressor, or surge suppressor, or surge diverter) is an appliance or device designed to protect electrical devices from voltage spikes.\n\nA voltage spike is a transient event, typically lasting 1 to 30 microseconds, that may reach over 1,000 volts. Lightning that hits a power line can give many thousands, sometimes 100,000 or more volts. A motor when switched off can generate a spike of 1,000 or more volts. Spikes can degrade wiring insulation and destroy electronic devices like battery chargers, modems and TVs.\n\nSpikes can also occur on telephone and data lines when AC (alternating current) main lines accidentally connect to them or lightning hits them or the telephone and data lines travel near lines with a spike and the voltage is induced.\n\nA long term surge, lasting seconds, minutes, or hours, caused by power transformer failures such as a lost neutral or other power company error, are not protected by transient protectors. Long term surges can destroy the protectors in an entire building or area. Even tens of milliseconds can be longer than a protector can handle. Long term surges may or may not be handled by fuses and over voltage relays.\n\nA transient surge protector attempts to limit the voltage supplied to an electric device by either blocking or shorting current to reduce the voltage below a safe threshold. Blocking is done by using inductors which inhibit a sudden change in current. Shorting is done by spark gaps, discharge tubes, zener-type semiconductors, and MOVs (Metal Oxide Varistors), all of which begin to conduct current once a certain voltage threshold is reached, or by capacitors which inhibit a sudden change in voltage. Some surge protectors use multiple elements.\n\nThe most common and effective way is the shorting method in which the electrical lines are temporarily shorted together until the voltage is reduced by the resistance in the power lines. The spike's energy is dissipated in the power lines (and/or the ground), converted to heat. Since a spike lasts only 10s of microseconds, the temperature rise is minimal. However, if the spike is large enough, like a nearby hit by lightning, there might not be enough power line or ground resistance and the MOV (or other protection element) can be destroyed and power lines melted.\n\nSurge protectors for homes can be in power strips used inside, or a device outside at the power panel. A modern house has three wires, Line, Neutral, and Ground (L,N, and G). Many protectors will connect to all three, in pairs, L-N,L-G,and N-G, since there are conditions, like lightning, where both L and N have high voltage spikes that need to be shorted to ground. \n\nThe terms \"surge protection device\" (\"SPD\") and \"transient voltage surge suppressor\" (\"TVSS\") are used to describe electrical devices typically installed in power distribution panels, process control systems, communications systems, and other heavy-duty industrial systems, for the purpose of protecting against electrical surges and spikes, including those caused by lightning. Scaled-down versions of these devices are sometimes installed in residential service entrance electrical panels, to protect equipment in a household from similar hazards.\n\nMany power strips have basic surge protection built in; these are typically clearly labeled as such. However, in unregulated countries there are power strips labelled as \"surge\" or \"spike\" protectors that only have a capacitor or RFI circuit (or nothing) that do \"not\" provide true (or any) spike protection.\n\nThese are some of the most prominently featured specifications which define a surge protector for AC mains, as well as for some data communications protection applications.\n\nAlso known as the let-through voltage, this specifies what spike voltage will cause the protective components inside a surge protector to short or clamp. A lower clamping voltage indicates better protection, but can sometimes result in a shorter life expectancy for the overall protective system. The lowest three levels of protection defined in the UL rating are 330 V, 400 V and 500 V. The standard let-through voltage for 120 V AC devices is 330 volts.\n\nUnderwriters Laboratories (UL), a global independent safety science company, defines how a protector may be used safely. UL 1449 became compliance mandatory with the 3rd edition in September 2009 to increase safety compared to products conforming to the 2nd edition. A measured limiting voltage test, using six times higher current (and energy), defines a voltage protection rating (VPR). For a specific protector, this voltage may be higher compared to a Suppressed Voltage Ratings (SVR) in previous editions that measured let-through voltage with less current. Due to non-linear characteristics of protectors, let-through voltages defined by 2nd edition and 3rd edition testing are not comparable.\n\nA protector may be larger to obtain a same let-through voltage during 3rd edition testing. Therefore, a 3rd edition or later protector should provide superior safety with increased life expectancy.\n\nA protector with a higher let-through voltage, e.g.400v vs 330v, will pass a higher voltage to the connected device. The design of the connected device determines whether this pass-thorough spike will cause damage. Motors and mechanical devices are usually not affected. Some (especially older) electronic parts, like chargers, LED or CFL bulbs and computerized appliances are sensitive and can be compromised and have their life reduced.\n\nThe Joule rating number defines how much energy a MOV-based surge protector can theoretically absorb in a single event, without failure. Better protectors exceed ratings of 1,000 joules and 40,000 amperes. Since the actual duration of a spike is only about 10 microseconds, the actual power dissipated in the MOV is only 1 to 20 watts. Any more than that and the MOV will fuse, or sometimes short and melt, hopefully blowing a fuse, disconnecting itself from the circuit.\n\nThe MOV (or other shorting device) requires resistance in the supply line in order to limit the voltage. If you have big, low resistance, power lines you need a bigger, larger joule rated MOV. Inside a house, with smaller wires that have more resistance, you can use lower ratings.\n\nEvery time a MOV shorts, its internal structure is changed and its threshold voltage reduced slightly. After many spikes the threshold voltage can reduce enough to be near the line voltage, i.e. 120 vac or 240 vac. At this point the MOV will partially conduct and heat up and eventually fail, sometimes in a dramatic meltdown or even a fire. Most modern surge protectors have circuit breakers and temperature fuses to prevent serious consequences. Many also have a LED light to indicate if the MOVs are still functioning.\nThe joule rating is commonly quoted for comparing MOV-based surge protectors. An average surge (spike) is of short duration, lasting for nanoseconds to microseconds, and experimentally modeled surge energy can be less than 100 joules. Well-designed surge protectors consider the resistance of the lines that supply the power, the chance of lightning or other seriously energetic spike, and specify the MOVs accordingly. A little battery charger might include a MOV of only 1 watt, whereas a surge strip will have a 20 watt MOV or several of them in parallel. A house protector will have a large block-type MOV.\n\nSome manufacturers commonly design higher joule-rated surge protectors by connecting multiple MOVs in parallel and this can produce a misleading rating. Since individual MOVs have slightly different voltage thresholds and non-linear responses when exposed to the same voltage curve, any given MOV might be more sensitive than others. This can cause one MOV in a group to conduct more (a phenomenon called current hogging), leading to possible overuse and eventual premature failure of that component. However the other MOVs in the group do help a little as they start to conduct as the voltage continues to rise as it does since a MOV does not have a sharp threshold. It may start to short at 270 volts but not reach full short until 450 or more volts. A second MOV might start at 290 volts and another at 320 volts so they all can help clamp the voltage, and at full current there is a series ballast effect that improves current sharing, but stating the actual joule rating as the sum of all the individual MOVs does not accurately reflect the total clamping ability. The first MOV may bear more of the burden and fail earlier. One MOV manufacturer recommends using fewer but bigger MOVs (e.g.60mm vs 40mm diameter) if they can fit in the device and to match them and derate them. In some cases it may take four 40 mm MOVs to be equivalent to one 60 mm MOV. \n\nA further problem is that if a single inline fuse is placed in series with a group of paralleled MOVs as a disconnect safety feature, it will open and disconnect all remaining working MOVs.\nThe \"effective\" surge energy absorption capacity of the entire system is dependent on the MOV matching so derating by 20% or more is usually required. This limitation can be managed by using carefully \"matched sets\" of MOVs, matched according to manufacturer's specification.\n\nAccording to industry testing standards, based on IEEE and ANSI assumptions, power line surges inside a building can be up to 6,000 volts and 3,000 amperes, and deliver up to 90 joules of energy, including surges from external sources not including lightning strikes.\n\nThe common assumptions regarding lightning specifically, based ANSI/IEEE C62.41 and UL 1449 (3rd Edition) at time of this writing, are that minimum lightning-based power line surges inside a building are typically 10,000 amperes or 10 kiloamperes (kA). This is based on 20 kA striking a power line, the imparted current then traveling equally in both directions on the power line with the resulting 10 kA traveling into the building or home. These assumptions are based on an average approximation for testing minimum standards. While 10 kA is typically good enough for minimum protection against lightning strikes it is possible for a lightning strike to impart up to 200 kA to a power line with 100 kA traveling in each direction.\n\nLightning and other high-energy transient voltage surges can be suppressed with pole mounted supressors by the utility, or with an owner supplied whole house surge protector. A whole house product is more expensive than simple single-outlet surge protectors and often needs professional installation on the incoming electrical power feed; however, they prevent power line spikes from entering the house. Damage from direct lightning strikes via other paths must be controlled separately.\n\nSurge protectors don't operate instantaneously; a slight delay exists, some few nanoseconds. The longer the response time and depending on system impedance, the connected equipment maybe exposed to some of the surge. However, surges typically are much slower and take around a few microseconds to reach their peak voltage, and a surge protector with a nanosecond response time would kick in fast enough to suppress the most damaging portion of the spike.\n\nThus response time under standard testing is not a useful measure of a surge protector's ability when comparing MOV devices. All MOVs have response times measured in nanoseconds, while test waveforms usually used to design and calibrate surge protectors are all based on modeled waveforms of surges measured in microseconds. As a result, MOV-based protectors have no trouble producing impressive response-time specs.\n\nSlower-responding technologies (notably, GDTs) may have difficulty protecting against fast spikes. Therefore, good designs incorporating slower but otherwise useful technologies usually combine them with faster-acting components, to provide more comprehensive protection.\n\nSome frequently listed standards include:\n\nEach standard defines different protector characteristics, test vectors, or operational purpose.\n\nThe 3rd Edition of UL Standard 1449 for SPDs was a major rewrite of previous editions, and was also accepted as an ANSI standard for the first time. A subsequent revision in 2015 included the addition of low-voltage circuits for USB charging ports and associated batteries.\nEN 62305 and ANSI/IEEE C62.xx define what spikes a protector might be expected to divert. EN 61643-11 and 61643-21 specify both the product's performance and safety requirements. In contrast, the IEC only writes standards and does not certify any particular product as meeting those standards. IEC Standards are used by members of the CB Scheme of international agreements to test and certify products for safety compliance.\n\nNone of those standards guarantee that a protector will provide proper protection in a given application. Each standard defines what a protector should do or might accomplish, based on standardized tests that may or may not correlate to conditions present in a particular real-world situation. A specialized engineering analysis may be needed to provide sufficient protection, especially in situations of high lightning risk.\n\nSystems used to reduce or limit high-voltage surges can include one or more of the following types of electronic components. Some surge suppression systems use multiple technologies, since each method has its strong and weak points.\nThe first six methods listed operate primarily by diverting unwanted surge energy away from the protected load, through a protective component connected in a \"parallel\" (or shunted) topology. The last two methods also block unwanted energy by using a protective component connected in \"series\" with the power feed to the protected load, and additionally may shunt the unwanted energy like the earlier systems.\n\nA metal oxide varistor (MOV) consists of a bulk semiconductor material (typically sintered granular zinc oxide) that can conduct large currents (effectively short-circuits) when presented with a voltage above its rated voltage.\nMOVs typically limit voltages to about 3 to 4 times the normal circuit voltage by diverting surge current elsewhere than the protected load. MOVs may be connected in parallel to increase current capability and life expectancy, providing they are \"matched sets\". (Unmatched MOVs have a tolerance of approximately ±10% on voltage ratings, which may not be sufficient .) For more details on the effectiveness of parallel-connected MOVs, see the section on Joules rating elsewhere in this article.\n\nMOVs have finite life expectancy and \"degrade\" when exposed to a few large transients, or many small transients.. Every time a MOV activates (shorts,) its threshold voltage reduces slightly. After many spikes the threshold voltage can reduce enough to be near the protection voltage, either mains or data. At this point the MOV conducts more and more often, heats up and finally fails. In data circuits, the data channel becomes shorted and non-functional. In a power circuit, you may get a dramatic meltdown or even a fire if not protected by a fuse of some kind. .\n\nMost modern surge strips and house protectors have circuit breakers and temperature fuses to prevent serious consequences. A thermal fuse disconnects the MOV when it gets too hot. Only the MOV is disconnected leaving the rest of the circuit working but not protected. Often there is a LED light to indicate if the MOVs are still functioning. Older surge strips had no thermal fuse and relied on a 10 or 15 amp circuit breaker which usually blew only after the MOVs had smoked, burned, popped, melted and permanently shorted. \n\nA failing MOV is a fire risk, which is a reason for the National Fire Protection Association's (NFPA) UL1449 in 1986 and subsequent revisions in 1998, 2009 and 2015. NFPA's primary concern is protection from fire.\n\nTherefore, all MOV-based protectors intended for long-term use should have an indicator that the protective components have failed, and this indication must be checked on a regular basis to ensure that protection is still functioning. \n\nBecause of their good price/performance ratio, MOVs are the most common protector component in low-cost basic AC power protectors.\n\nA TVS diode is a type of Zener diode, also called an avalanche diode or silicon avalanche diode (SAD), which can limit voltage spikes. These components provide the fastest limiting action of protective components (theoretically in picoseconds), but have a relatively low energy-absorbing capability. Voltages can be clamped to less than twice the normal operation voltage. If current impulses remain within the device ratings, life expectancy is exceptionally long. If component ratings are exceeded, the diode may fail as a permanent short circuit; in such cases, protection may remain but normal circuit operation is terminated in the case of low-power signal lines. Due to their relatively limited current capacity, TVS diodes are often restricted to circuits with smaller current spikes. TVS diodes are also used where spikes occur significantly more often than once a year, since this component will not degrade when used within its ratings. A unique type of TVS diode (trade names Transzorb or Transil) contains reversed paired \"series\" avalanche diodes for bi-polar operation.\n\nTVS diodes are often used in high-speed but low-power circuits, such as occur in data communications. These devices can be paired in \"series\" with another diode to provide low capacitance as required in communication circuits.\n\nA Trisil is a type of thyristor surge protection device (TSPD), a specialized solid-state electronic device used in crowbar circuits to protect against overvoltage conditions. A SIDACtor is another thyristor type device used for similar protective purposes.\n\nThese thyristor-family devices can be viewed as having characteristics much like a spark gap or a GDT, but can operate much faster. They are related to TVS diodes, but can \"break over\" to a low clamping voltage analogous to an ionized and conducting spark gap. After triggering, the low clamping voltage allows large current surges while limiting heat dissipation in the device.\n\nA gas discharge tube (GDT) is a sealed glass-enclosed device containing a special gas mixture trapped between two electrodes, which conducts electric current after becoming ionized by a high voltage spike. GDTs can conduct more current for their size than other components. Like MOVs, GDTs have a finite life expectancy, and can handle a few very large transients or a greater number of smaller transients. The typical failure mode occurs when the triggering voltage rises so high that the device becomes ineffective, although lightning surges can occasionally cause a dead short.\n\nGDTs take a relatively long time to trigger, permitting a higher voltage spike to pass through before the GDT conducts significant current. It is not uncommon for a GDT to let through pulses of 500 V or more of 100 ns in duration. In some cases, additional protective components are necessary to prevent damage to a protected load, caused by high-speed let-through voltage which occurs before the GDT begins to operate.\n\nGDTs create an effective short circuit when triggered, so that if any electrical energy (spike, signal, or power) is present, the GDT will short this. Once triggered, a GDT will continue conducting (called follow-on current) until all electric current sufficiently diminishes, and the gas discharge quenches. Unlike other shunt protector devices, a GDT once triggered will continue to conduct at a voltage \"less than\" the high voltage that initially ionized the gas; this behavior is called negative resistance. Additional auxiliary circuitry may be needed in DC (and some AC) applications to suppress follow-on current, to prevent it from destroying the GDT after the initiating spike has dissipated. Some GDTs are designed to deliberately short out to a grounded terminal when overheated, thereby triggering an external fuse or circuit breaker.\n\nMany GDTs are light-sensitive, in that exposure to light lowers their triggering voltage. Therefore, GDTs should be shielded from light exposure, or opaque versions that are insensitive to light should be used.\n\nThe CG2 SN series of surge arrestors, formerly produced by C P Clare, are advertised as being non-radioactive, and the datasheet for that series states that some members of the CG/CG2 series (75-470V) are radioactive.\nDue to their exceptionally low capacitance, GDTs are commonly used on high frequency lines, such as those used in telecommunications equipment. Because of their high current-handling capability, GDTs can also be used to protect power lines, but the follow-on current problem must be controlled.\n\nAn \"overvoltage clamping\" bulk semiconductor similar to an MOV, though it does not clamp as well. However, it usually has a longer life than an MOV. It is used mostly in high-energy DC circuits, like the exciter field of an alternator. It can dissipate power continuously, and it retains its clamping characteristics throughout the surge event, if properly sized.\n\nA spark gap is one of the oldest protective electrical technologies still found in telephone circuits, having been developed in the nineteenth century. A carbon rod electrode is held with an insulator at a specific distance from a second electrode. The gap dimension determines the voltage at which a spark will jump between the two parts and short to ground. The typical spacing for telephone applications in North America is (0.003 inches). Carbon block suppressors are similar to gas arrestors (GDTs) but with the two electrodes exposed to the air, so their behavior is affected by the surrounding atmosphere, especially the humidity. Since their operation produces an open spark, these devices should \"never\" be installed where an explosive atmosphere may develop.\n\nUsed in RF signal transmission paths, this technology features a tuned quarter-wavelength short-circuit stub that allows it to pass a bandwidth of frequencies, but presents a short to any other signals, especially down towards DC. The passbands can be narrowband (about ±5% to ±10% bandwidth) or wideband (above ±25% to ±50% bandwidth). Quarter-wave coax surge arrestors have coaxial terminals, compatible with common coax cable connectors (especially N or 7-16 types). They provide the most rugged available protection for RF signals above ; at these frequencies they can perform much better than the gas discharge cells typically used in the universal/broadband coax surge arrestors. Quarter-wave arrestors are useful for telecommunications applications, such as Wi-Fi at 2.4 or but less useful for TV/CATV frequencies. Since a quarter-wave arrestor shorts out the line for low frequencies, it is not compatible with systems which send DC power for a LNB up the coaxial downlink.\n\nThese devices are not rated in joules because they operate differently from the earlier suppressors, and they do not depend on materials that inherently wear out during repeated surges. SM suppressors are primarily used to control transient voltage surges on electrical power feeds to protected devices. They are essentially heavy-duty low-pass filters connected so that they allow 50 or 60 Hz line voltages through to the load, while blocking and diverting higher frequencies. This type of suppressor differs from others by using banks of inductors, capacitors and resistors that suppress voltage surges and inrush current to the neutral wire, whereas other designs shunt to the ground wire. Surges are not diverted but actually suppressed. The inductors slow down the energy. Since the inductor in series with the circuit path slows the current spike, the peak surge energy is spread out in the time domain and harmlessly absorbed and slowly released from a capacitor bank.\n\nExperimental results show that most surge energies occur at under 100 joules, so exceeding the SM design parameters is unlikely. SM suppressors do not present a fire risk should the absorbed energy exceed design limits of the dielectric material of the components because the surge energy is also limited via arc-over to ground during lightning strikes, leaving a surge remnant that often does not exceed a theoretical maximum (such as 6000 V at 3000 A with a modeled shape of 8 × 20 microsecond waveform specified by IEEE/ANSI C62.41). Because SMs work on both the current rise and the voltage rise, they can safely operate in the worst surge environments.\n\nSM suppression focuses its protective philosophy on a \"power supply input\", but offers nothing to protect against surges appearing between the input of an SM device and \"data lines\", such as antennae, telephone or LAN connections, or multiple such devices cascaded and linked to the primary devices. This is because they do not divert surge energy to the ground line. Data transmission requires the ground line to be clean in order to be used as a reference point. In this design philosophy, such events are already protected against by the SM device before the power supply. NIST reports that \"Sending them [surges]\ndown the drain of a grounding conductor only makes them reappear within a microsecond about 200 meters away on some other conductor.\" So having protection on a data transmission line is only required if surges are diverted to the ground line.\n\nIn comparison to devices relying on 10-cent components that operate only briefly (such as MOVs or GDTs), SM devices tend to be bulkier and heavier than those simpler spike shunting components. The initial costs of SM filters are higher, typically and up, but a long service life can be expected if they are used properly. In-field installation costs can be higher, since SM devices are installed in \"series\" with the power feed, requiring the feed to be cut and reconnected.\n\n\n"}
{"id": "53543792", "url": "https://en.wikipedia.org/wiki?curid=53543792", "title": "Switching Kalman filter", "text": "Switching Kalman filter\n\nThe switching Kalman filtering (SKF) method is a variant of the Kalman filter. In its generalised form, it is often attributed to Kevin P. Murphy, but related switching state-space models have been in use.\n\nApplications of the switching Kalman filter include:\nbrain-computer interfaces and neural decoding, real-time decoding for continuous neural-prosthetic control.\nIt also has application in econometrics, signal processing, tracking, computer vision, etc. It is an alternative to the Kalman filter when the system's state has a discrete component. For example, when an industrial plant has \"multiple discrete modes of behaviour, each of which having a linear (Gaussian) dynamics\".\n\nThere are several variants of SKF discussed in.\n\nIn the simpler case, switching state-space models are defined based on a switching variable which evolves independent of the hidden variable. The probabilistic model of such variant of SKF is as the following:\n\n[This section is badly written: It does not explain the notation used below.]\n\nThe hidden variables include not only the continuous formula_2, but also a discrete *switch* (or switching) variable formula_3. The dynamics of the switch variable are defined by the term formula_4. The probability model of formula_2 and formula_6 can depend on formula_3.\n\nThe switch variable can take its values from a set formula_8. This changes the joint distribution formula_9 which is a separate multivariate Gaussian distribution in case of each value of formula_3.\n\nIn more generalised variants, the switch variable affects the dynamics of formula_11, e.g. through formula_12.\nThe filtering and smoothing procedure for general cases is discussed in.\n"}
{"id": "44144744", "url": "https://en.wikipedia.org/wiki?curid=44144744", "title": "TV gateway", "text": "TV gateway\n\nA TV gateway (also called network TV tuner) is a television headend to a network UPnP router that receives live digital video broadcast (DVB) MPEG transport streams (channels) from terrestrial aerials, satellite dishes, or cable feeds and converts them into IP streams for distribution over an IP network. \n\nTV gateways allow users to stream broadcast live TV content to connected devices on the IP network, including tablets, smartphones, computers, gaming consoles and smart tvs. They also allow multiple users to watch and record different channels at the same time.\n\nThe device offers multi-platform, multi-screen broadcast television with rich live TV content and high quality HD channels.\n\nMost TV gateways support free-to-air (FTA) television services found in many countries. These include services such as Freeview and Freesat in the United Kingdom, TNT in France and TDT in Spain, and basic cable packages in Germany, Switzerland, Austria, and others.\n\nA few TV gateways also support third party conditional access modules (CAMs) for premium pay TV channels, which are transmitted by using a CAM CI card provided by the broadcasters or by third party manufacturers to access their TV service.\nWhile many first-generation TV gateways support only one channel or a limited number of channels, modern TV gateways provide multiple TV tuners that can process several channels simultaneously. The more channels a TV gateway provides the more users it can service at the same time. Modern TV gateways also allow users to record TV programs to a USB flash drive, or external hard disk and in some cases, shared folders or network attached storage (NAS).\n\nAn electronic program guide (EPG) is like a traditional TV listing magazine but available online or on a TV service like aerial, satellite or cable. It allows viewers to find out what shows will air and search for programs they’d like to watch. EPG's also allow users to set reminders and record shows automatically.\n\nMost TV gateways with PVR functionality offer EPG data. This can be free of charge for data processed from the broadcaster TV stream (according to standard DVB EN 300 468 and technical specification TS 101 211) or via a paid service provided by a third party online EPG provider.\n\nWhole house HD-digital video recorder allows users to record programs on a centralized TV gateway DVR and then watch them on any device connected to their home network.\n\nTV gateways with whole house HD-DVR require storage to record live TV programs or schedule future recordings using the EPG.\n\nWhole house DVR TV gateways use a number of storage mediums to store recordings:\n\nUnicast (HTTP) protocols are mainly used in consumer grade TV gateways to provide a small number of simultaneous users with the flexibility to view multiple channels.\n\nMulticast (UDP), is mainly used in professional-grade TV gateways to enable efficient broadcast of a preset number of channels to a large number of simultaneous viewers. Multicast TV gateways are used primarily by IPTV broadcasters, hotels, hospitals, and digital signage applications.\n\nUniversal plug and play (UPnP) is a set of networking protocols that permits network devices, to seamlessly discover each other's presence on the network and establish functional network services for data sharing, communications, and entertainment.\n\nM3U - The m3u8 file format is a de facto standard playlist format suitable for carrying lists of media file URL. Advanced TV gateways use M3U in addition to UPnP to offer better application support, allowing for faster channel zapping and provide a preset channel list.\n"}
{"id": "7982051", "url": "https://en.wikipedia.org/wiki?curid=7982051", "title": "Tarus Balog", "text": "Tarus Balog\n\nTarus Balog (born 1966) is an American open source advocate, software developer, commentator, and blogger. He is lead maintainer of the OpenNMS project.\n\nBalog was born in Pittsburgh, Pennsylvania to parents of Hungarian extraction. He spent his childhood in Asheboro, North Carolina. In 1984 he graduated from high school at the North Carolina School of Science and Mathematics in Durham.\n\nIn September 2001 Balog joined Oculan, which was developing the open-source OpenNMS network management software framework under the leadership of the project's founder and original Chief Technologist Steve Giles. His role was to build a business around support and services for OpenNMS, as a complement to Oculan's prior channel-only approach to selling its line of OpenNMS-powered network management appliances.\n\nIn 2002 Oculan discontinued work on the OpenNMS open-source project, turning its focus solely to its appliance business. Concerned that the project would die without a caretaker, Balog requested to become the new project maintainer. Giles agreed, on the condition that Balog leave Oculan the same week.\n\nBalog further developed the software project, gained customers, and recruited a group of volunteer core developers while operating as Sortova Consulting Company. Eventually he transitioned his employment and the OpenNMS assets to a local Internet service provider, where he continued to work on the project until 2004.\n\nIn 2004, CEO Balog founded The OpenNMS Group along with President David Hustace and Chief Technology Officer Matt Brozowski. With the exception of a five-month period in 2013, Balog has served as company CEO since its founding. The company has its headquarters in Apex, North Carolina, plus satellite offices in Georgia, US; Ontario, Canada; and Germany.\n\nBalog frequently speaks at conferences concerned with open-source software and network monitoring and management.\n\nHe has spoken at the Southern California Linux Expo in 2007, 2010 (as keynote speaker), and 2015. In 2009 and 2014 he delivered talks at the Open Source Monitoring Conference in Nuremberg, Germany. He has spoken at SouthEast LinuxFest in 2011, 2013 (delivered keynote), and 2015. Balog spoke at Indiana LinuxFest in 2011 and at Ohio LinuxFest in 2011, 2012, and as keynote speaker in 2017.\n\n, Balog has written 14 articles for Opensource.com. In a 2009 blog post he popularized the term \"fauxpen source\" as a satire of the open-core business model.\n\nBalog's characterization of the open-core business model has drawn criticism from some quarters. Some question the practicality of sustaining a profitable business built around an open-source project that fully conforms to all tenets of the Free Software Definition and Open Source Definition.\n\n"}
{"id": "10867713", "url": "https://en.wikipedia.org/wiki?curid=10867713", "title": "Workover", "text": "Workover\n\nThe term workover is used to refer to any kind of oil well intervention involving invasive techniques, such as wireline, coiled tubing or snubbing. More specifically though, it will refer to the expensive process of pulling and replacing a completion.\n\nWorkovers rank among the most complex, difficult and expensive types of wellwork. They are only performed if the completion of a well is terminally unsuitable for the job at hand. The production tubing may have become damaged due to operational factors like corrosion to the point where well integrity is threatened. Downhole components such as tubing, retrievable downhole safety valves, or electrical submersible pumps may have malfunctioned, needing replacement.\n\nIn other circumstances, the reason for a workover may not be that the completion itself is in a bad condition, but that changing reservoir conditions make the former completion unsuitable. For example, a high productivity well may have been completed with 5½\" tubing to allow high flow rates (a narrower tubing would have unnecessarily choked the flow). Some years on, declining productivity means the reservoir can no longer support stable flow through this wide bore. This may lead to a workover to replace the 5½\" tubing with 4½\" tubing. The narrower bore makes for a more stable flow.\n\nBefore any workover, the well must first be killed. Since workovers are long planned in advance, there would be much time to plan the well kill and so the reverse circulation would be common. The intense nature of this operation often requires no less than the capabilities of a drilling rig.\n\nThe workover begins by killing the well then removing the wellhead and possibly the flow line, then installing a B.O.P commonly known as a blow out preventer, then lifting the tubing hanger from the casing head, thus beginning to pull the completion out of the well. The string will almost always be fixed in place by at least one production packer. If the packer is retrievable it can be released easily enough and pulled out with the completion string. If it is permanent, then it is common to cut the tubing just above it and pull out the upper portion of the string. If necessary, the packer and the tubing left in hole can be milled out, though more commonly, the new completion will make use of it by setting a new packer just above it and running new tubing down to the top of the old.\n\nAlthough less exposed to wellbore fluids, casing strings too have been known to lose integrity. On occasion, it may be deemed economical to pull and replace it. Because casing strings are cemented in place, this is significantly more difficult and expensive than replacing the completion string. If in some instances the casing cannot be removed from the well, it may be necessary to sidetrack the offending area and recomplete, also an expensive process. For all but the most productive well, replacing casing would never be economical.\n\n\n"}
{"id": "12470526", "url": "https://en.wikipedia.org/wiki?curid=12470526", "title": "World Alliance for Decentralized Energy", "text": "World Alliance for Decentralized Energy\n\nThe World Alliance for Decentralized Energy (WADE) was founded in 1997 with the original name of the \"International Cogeneration Alliance\". It was originally formed in response to the UNFCCC meetings in Kyoto to raise the profile of cogeneration on the agenda. In 2002 the organization changed its name to WADE and broadened its scope to include all forms of decentralized energy or distributed generation including renewable technologies such as solar PV and small scale wind power.\n\n\n"}
{"id": "52353307", "url": "https://en.wikipedia.org/wiki?curid=52353307", "title": "X2 transceiver", "text": "X2 transceiver\n\nThe X2 transceiver format is a 10 gigabit per second modular fiber optic interface intended for use in routers, switches and optical transport platforms. It is an early generation 10 gigabit interface related to the similar XENPAK and XPAK formats. X2 may be used with 10 gigabit ethernet or OC-192/STM-64 speed SDH/SONET equipment.\n\nX2 modules are smaller and consume less power than first generation XENPAK modules, but larger and consume more energy than the newer XFP transceiver standard and SFP+ standards.\n\nAs of 2016 this format is relatively uncommon and has been replaced by 10Gbit/s SFP+ in most new equipment.\n"}
