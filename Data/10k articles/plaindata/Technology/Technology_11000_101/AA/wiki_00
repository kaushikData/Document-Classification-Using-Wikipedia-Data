{"id": "44173797", "url": "https://en.wikipedia.org/wiki?curid=44173797", "title": ".design", "text": ".design\n\n.design is a top-level domain name. It was proposed in ICANN's new generic top-level domain (gTLD) program, and became available to the general public on May 12, 2015. Top Level Design is the domain name registry for the string.\n\nIn September 2014, Portland, Oregon-based Top Level Design (TLD) won the right to operate the .design top-level domain after beating out six other applicants in a private auction. According to TLD's CEO Ray King, winning the auction was \"very important\" and one of the company's top priorities, evidenced by its name. He told Domain Name Wire, \"Think of all the things that require design. Design permeates all aspects of culture.\". design domain registrations became available to the general public on May 12, 2015. According to The Domains, more than 5,200 .design domains were registered on the first day of general availability.\n\nCentralNic provides backend services through an exclusive distribution agreement and shares in the global revenues from .design domain names. Ben Crawford, CentralNic's CEO, said of the top-level domain, \"It has impressive commercial potential, and it will be adopted more quickly than many other TLDs as it caters, among many other groups, to one of the best-informed professions on new Internet developments – website designers\".\n\nAhead of .design's launch, King said of the gTLD:\n"}
{"id": "27117572", "url": "https://en.wikipedia.org/wiki?curid=27117572", "title": "AdExtent", "text": "AdExtent\n\nAdExtent (formally Semantinet) is an Israeli Startup Company dedicated to developing RTB Display technologies for various ad platforms.\n\nThe brainchild of Tal Muskal, who initially intended to create a development platform for the seamless integration of data, Semantinet was incorporated as a company in December 2006, after receiving a preseed investment from Yossi Vardi.\nThe company completed hiring its development team in January 2008, and by April the same year the team completed development of the company's initial semantic web engine. Three months later, in July 2008, Semantinet released its first product, the Headup client, as a private Alpha. In October the client was deemed ready for Beta and in February 2009 it was approved for distribution via Mozilla's Firefox Addon directory.\n\nIn March 2009 Semantinet began the transition from client to server based solutions and in July it started a pilot of a semantic web WordPress plugin for bloggers. The product was made publicly available via the WordPress plugin directory in October 2009. In January 2010 the server based Headup solution was made available as a Joomla extension and in March it became available as a Drupal module.\n\nSemantinet has released to date two applications to Beta. The company's first application, the Headup Semantic Web Firefox extension enables users to highlight terms appearing in web content in order to discover related content provided in the extension's overlay window user interface.\nThe company's second application is the Headup website and blog extension, which automatically identifies and highlights topics appearing in a publisher's web content. Users interacting with the plugin's highlights are provided related articles, images, videos, Tweets, etcetera, via one of three configurable interfaces.\n\nSemantinet was founded and is currently managed by Tal Keinan and Tal Muskal, who, like many of Israel's startup community members, served in the IDF in information technology related roles. Keinan, who's assumed the role of CEO and serves on the company's board, worked at Morgan Stanley's Risk Management Department prior to founding Semantinet, while Muskal, who invented Semantinet's core technology and now serves as its CTO, worked as an R&D engineer at Go Networks, where he made his first pitch about the technology to Oz Leave, Go Networks' CEO, who now serves as the Chairman of Semantinet's Board of Directors.\n\nSemantinet's Board of Directors is composed of one member besides Tal Keinan.\nEyal Niv of Giza Venture Capital. Eyal Niv funded XtremIO, which was acquired by EMC, and became the fastest growing business ever for EMC.\n\nSemantinet is backed by Giza Venture Capital and a number of Angel Investors including Yossi Vardi, Jeff Pulver and Sir Ronald Cohen. To date the company has raised US$4,400,000.\n\n"}
{"id": "50664621", "url": "https://en.wikipedia.org/wiki?curid=50664621", "title": "Batter board", "text": "Batter board\n\nBatter boards (or \"battre boards\", Sometimes mispronounced as \"battle boads\") are temporary frames, set beyond the corners of a planned foundation at precise elevations. These batter boards are then used to hold layout lines (construction twine) to indicate the limits (edges and corners) of the foundation.\n"}
{"id": "26555225", "url": "https://en.wikipedia.org/wiki?curid=26555225", "title": "Benzvalene", "text": "Benzvalene\n\nBenzvalene is an organic compound and one of several isomers of benzene. It was first synthesized in 1971 by Thomas J. Katz et al.\n\nThe 1971 synthesis consisted of treating cyclopentadiene with methyllithium in dimethyl ether and then with dichloromethane and methyllithium in diethyl ether at −45 °C. It can also be formed in low yield (along with fulvene and Dewar benzene) by irradiation of benzene at 237 to 254 nm. The hydrocarbon in solution was described as having an extremely foul odor. Due to the high steric strain present in benzvalene, the pure compound (~71 kcal/mol higher in energy than benzene) easily detonates, for example by scratching.\n\nThe compound converts to benzene with a chemical half-life of approximately 10 days. This symmetry-forbidden transition is believed to take place through a diradical intermediate.\n\nBenzvalene can be polymerized in a ROMP process to polybenzvalene. This polymer contains highly strained bicyclobutane rings which again makes it a sensitive material. The rings can be isomerized to 1,3-dienes and for this reason polybenzvalene has been investigated as a precursor to polyacetylene.\n"}
{"id": "15987733", "url": "https://en.wikipedia.org/wiki?curid=15987733", "title": "Biosearch Technologies", "text": "Biosearch Technologies\n\nBiosearch Technologies, Inc. is a closely held biotechnology company headquartered in Petaluma, California, United States. This company is vertically integrated and specializes in custom synthesized oligonucleotides for qPCR, cGMP oligos for molecular diagnostics, and DNA/RNA synthesis reagents. Their GMP (Good Manufacturing Practice) manufacturing facility is located in Novato, California. In 2015, the company LGC \"agreed to acquire\" Biosearch Technologies based on the two companies' complimentary product lines.\n\nIn 2000, Biosearch Technologies developed a dark quencher known as the Black Hole Quencher (BHQ) dyes, which has become an industry standard product and is currently licensed out to a number of other oligonucleotide manufacturers, biotechnology and molecular diagnostic manufacturers. The series of Black Hole Quencher dyes have no native fluorescence, high signal-to-noise ratios providing greater sensitivity, and exceptional coupling efficiency. BHQ dyes are able to quench the entire visible spectrum and near IR spectrum, allowing for a broader range of fluorogenic reporter dyes, which makes the Black Hole Quenchers ideal for multiplexing assays.\n\nThe research team at Biosearch Technologies also developed other fluorescent dyes such as the CAL Fluor, Quasar, and Pulsar series of dyes, which emit fluorescence from 500-700 nanometers. Equipped with the various technology in fluorescent dyes and quenchers, much of Biosearch Technologies' business comes from the design and manufacture of custom probes and primers, which are commonly used for genomic applications such as quantitative PCR and SNP genotyping.\n\nDuring the 2009 H1N1 Pandemic, Biosearch Technologies became the first to license from the CDC the H1N1 Influenza and Influenza A sub-typing panel signatures. They would later obtain a license for the “pdm” H1N1 signatures as well. With these licenses from the CDC, Biosearch Technologies manufactured probes and primers to discriminate the various flu signatures as part of their ValuPanel Reagents product line.\n\nIn 2013, Biosearch responded to the Avian Influenza A (H7N9) outbreak in China by providing an H7N9 detection panel.\n\nIn 2010, Biosearch Technologies acquired an exclusive license to the single molecule fish technology developed by scientists at University of Medicine and Dentistry of New Jersey. The technology is now branded as Stellaris FISH, and is a method of detecting and quantifying mRNA and other long RNA molecules in cell culture or tissue samples. The technology is a refinement of earlier RNA ISH technologies, and uses multiple single labeled DNA oligonucleotide probes to increase sensitivity and specificity.\n\nBiosearch Technologies offers a free, web-based design service known as RealTimeDesign (RTD) equipped with comprehensive algorithms that allow the software to model and propose quantitative PCR and SNP genotyping assays. RealTimeDesign is meant to help scientists craft custom oligonucleotides, averaging 99% in amplification efficiency through a series of different features offered by the software. Some of those features include a direct link to NCBI and BLAST databases as well as a selection of user-modifiable parameters.\n\nAlthough Biosearch Technologies was founded in 1993, its roots can be traced back to 1979 when it was preceded by Dr. Ronald Cook's first company, Biosearch, Inc. Biosearch, Inc. experienced 9 years of DNA synthesis instrumentation and chemistry by playing a key role in engineering and manufacturing one of the first automated solid-phase DNA synthesis instruments, the SAM I. As time progressed, Biosearch was also able to bring other DNA synthesizers to market such as the Biosearch 8700, Biosearch 8800 Prep, and the Cyclone.\n\nIn 1987, Biosearch was acquired by New Brunswick Scientific, who sold the Biosearch unit to Millipore Corporation in 1988. By 1989, Biosearch was renamed Milligen-Biosearch. Due to the loss of several corporate officers, Milligen-Biosearch was subsequently acquired by PerSeptive Biosystems which in turn was acquired by Applied Biosystems who eventually retired the Biosearch name and products.\n\nAfter taking a short hiatus, Dr. Cook decided to return to the oligonucleotide industry and founded what is currently known as Biosearch Technologies, Inc. In 2013, Biosearch acquired the oligonucleotide manufacturing arm of DNA Technology and the entirety of VitraBio, a maker of porous glass.\n\nWhen Kary Mullis received the Nobel Prize in 1993 and gave his Nobel Lecture concerning his invention of the polymerase chain reaction (PCR) method, he gratefully acknowledged Biosearch and Dr. Cook's role in providing him one of the first SAM I DNA synthesizers which was used in support of Kary Mullis' PCR research.\n\n\n"}
{"id": "35955101", "url": "https://en.wikipedia.org/wiki?curid=35955101", "title": "Bloch–Grüneisen temperature", "text": "Bloch–Grüneisen temperature\n\nFor typical three-dimensional metals, the temperature-dependence of the electrical resistivity \"ρ(T)\" due to the scattering of electrons by acoustic phonons changes from a high-temperature regime in which \"ρ ∝ T\" to a low-temperature regime in which \"ρ ∝ T\" at a characteristic temperature known as the Debye temperature. For low density electron systems, however, the Fermi surface can be substantially smaller than the size of the Brillouin zone, and only a small fraction of acoustic phonons can scatter off electrons. This results in a new characteristic temperature known as the Bloch–Grüneisen temperature that is lower than the Debye temperature. The Bloch–Grüneisen temperature is defined as \"2ħvk/k\", where \"ħ\" is the Planck constant, \"v\" is the velocity of sound, \"ħk\" is the Fermi momentum, and \"k\" is the Boltzmann constant.\n\nWhen the temperature is lower than the Bloch–Grüneisen temperature, the most energetic thermal phonons have a typical momentum of \"kT/v\" which is smaller than \"ħk\", the momentum of the conducting electrons at the Fermi surface. This means that the electrons will only scatter in small angles when they absorb or emit a phonon.\nIn contrast when the temperature is higher than the Bloch–Grüneisen temperature, there are thermal phonons of all momenta and in this case electrons will also experience large angle scattering events when they absorb or emit a phonon.\n\nThe Bloch–Grüneisen temperature has been observed experimentally in a two-dimensional electron gas and in graphene.\n"}
{"id": "6935781", "url": "https://en.wikipedia.org/wiki?curid=6935781", "title": "CTX (explosive-detection device)", "text": "CTX (explosive-detection device)\n\nThe CTX (Computer Tomography X-ray) is an explosive detection device, a family of x-ray devices developed by InVision Technologies in 1990 that uses CAT scans and sophisticated image processing software to automatically screen checked baggage for explosives.\n\nIn 1994, the CTX-5000 became the first computed tomography explosive detection system certified by the US Federal Aviation Administration (FAA). The certification of the CTX-5000 followed nine years of development. During that time the FAA invested $90 million in explosives detection and nearly $8.6 million in the specific technology. From 1995 to 1997, the CTX-5000 was tested to solve the challenges involved in integrating an explosives detection system into a baggage system and to validate the estimated costs of wide-scale deployment of the systems.\n\nThe CTX-5000 SP scanning system, an improved version of the CTX-5000 for checked baggage, was delivered to the FAA in 1997 and placed at several of the US's busiest and largest airports. From 1997 to 2000, more than 100 of the systems have been purchased by the FAA to install in US airports, according to InVision.\n\nThe CTX-5500DS is an automated explosives detection system that uses computed tomography to characterize materials in checked bags and automatically identify objects that could be improvised explosive devices. The CTX-5500DS is the most widely used, FAA-certified Explosives Detection System in the world. It can be used for either standalone applications or in an integrated manner with airport baggage handling systems. It can also be configured to detect other types of contraband material. The CTX-5500DS has an FAA-certified throughput of 384 bags per hour. Its Dynamic Screening (DS) capability offers flexibility by allowing manual or automatic switching between various screening modes.\n\nThe CTX-2500 is a small-sized explosives detection system that is half the length of earlier CTX models. The CTX 2500 utilizes a single rotating X-ray source to acquire positioning images and CT-slice images, thus achieving its smaller size. The CTX 2500 system is the first FAA-certified Explosives Detection System (EDS) mounted on a truck for easy mobility and access to cargo. One of the units costs approximately US$700,000.\n\nThe CTX-9000 DSi system is the world's fastest FAA-certified (Certification moved to TSA Transportation Security Lab in 2002) Explosives Detection System, handling 542 bags per hour. It features alternate operational modes yielding even higher throughputs. The CTX-9000 DSi is designed for integrated airport installations. Its 1-metre wide conveyor coordinates with standard airport baggage handling systems. The system's architecture utilizes modular components, helping to ease scanner upgrading and servicing. The scanner contains 4 active radiation-shielding curtains. In addition, the gantry rotates at 120 RPM, enabling a slice image to be generated within half a second. A high-speed RF data link connects the rotating gantry to the stationary part of the unit. An air-conditioning unit ensures high performance and reliability in hot, dusty and humid airport environments.\n\nIn the late 1990s, L-3 Communications developed a competing computerized tomography system that also met FAA approval, however it was not TSA Qualified until late 2002. In November 1999, the FAA awarded a contract worth up to US$75 million to L-3 to purchase up to 60 of its explosive detection systems. The eXaminer 3DX 6000 explosive detection system developed by L-3 operates similarly to the CTX system.\n\nRecent research has evaluated the use of computer vision based algorithms that operate on the volumetric data used collected as CT-slice images by these and other manufacturers computed tomography (CT) baggage scanner machines for the automatic detection of other threat types (e.g. guns, knives, liquid containers) using 3D object classification.\n\n"}
{"id": "7656250", "url": "https://en.wikipedia.org/wiki?curid=7656250", "title": "Calculated Carbon Aromaticity Index", "text": "Calculated Carbon Aromaticity Index\n\nThe calculated carbon aromaticity index (CCAI) is an index of the ignition quality of residual fuel oil.\n\nThe running of all internal combustion engines is dependent on the ignition quality of the fuel. For spark-ignition engines the fuel has an octane rating. For diesel engines it depends on the type of fuel, for distillate fuels the cetane numbers are used. Cetane numbers are tested using a special test engine and the existing engine was not made for residual fuels. For residual fuel oil two other empirical indexes are used: CCAI and Calculated Ignition Index (CII). Both CCAI and CII are calculated from the density and kinematic viscosity of the fuel.\n\nFormula for CCAI:\n\nformula_1\n\nwhich is equivalent to:\n\nformula_2\n\nWhere:\nD= density at 15°C (kg/m)\nV= viscosity (cST)\nt = viscosity temperature (°C)\n\nThis will normally give a value somewhere between 800 and 880. The lower the value is the better the ignition quality. Fuels with a CCAI higher than 880 are often problematic or even unusable in a diesel engine. CCAI are often calculated under testing of marine fuel.\nIn case of high CCAI, the manufacturers recommendations and guidance limits should be consulted to ensure that the fuel falls within the permissible range for the engine type. Attention should be given to the combustion profile, peak pressures and exhaust temperatures on the Engine.\n\nAs the name suggests, CCAI is a calculation based on the density and viscosity of a given fuel. The\nformula is rather complex but in general, the higher the CCAI, the poorer the ignition quality of the fuel\nis considered to be. Once the CCAI goes above 860, it is an indication that some combustion problems\nmay occur.\n\nStudies carried out by engine manufacturers indicate that combustion related problems caused by fuels with\nhigh CCAI can be reduced by avoiding running the engine at part load. It is therefore suggested that\nwherever possible, the engine load should be maintained above 50% and the chief engineer should listen\nfor indications of poor combustion (i.e. knocking). Should any such problems be noted then it is\nrecommended that an alternative fuel is used whilst further investigations are carried out on samples from this fuel\n"}
{"id": "53059018", "url": "https://en.wikipedia.org/wiki?curid=53059018", "title": "Calutron Girls", "text": "Calutron Girls\n\nThe Calutron Girls were a group of young women, mostly high school graduates who joined the World War II efforts in Oak Ridge, Tennessee in 1945.\n\nAlthough they were not allowed to know at the time, they were monitoring dials and watching meters for a calutron, a mass spectrometer that separates uranium isotopes. The enriched uranium was used to make the first atomic bomb.\n\nCalutron Girls were trained and employed at the Y-12 National Security Complex. Wartime labor shortages forced the Tennessee Eastman Corporation to hire women to work at the Y-12 plant.\n\nAccording to Gladys Owens, one of the few Calutron Girls, a manager at the facility once told them: \"We can train you how to do what is needed, but cannot tell you what you are doing. I can only tell you that if our enemies beat us to it, God have mercy on us!\"\n\n"}
{"id": "91300", "url": "https://en.wikipedia.org/wiki?curid=91300", "title": "Climbing protection", "text": "Climbing protection\n\nClimbing protection is any of a variety of devices employed to reduce risk and protect others while climbing rock and ice. It includes such items as nylon webbing and metal nuts, cams, bolts, and pitons.\n\nDifferent forms of climbing draw on varying forms of protection and the systems that are created from its elements.\n\nThere are a number of ways to \"protect\" a climb, varying according to the type of climbing:\n\nA lead climber places protection (temporary or permanent anchors) in the rock, snow, or ice establishing a climbing route. The rope is clipped through carabiners (often joined by a short length of webbing into a pair known as a quickdraw) which are in turn connected to the protection. The belayer pays out rope during the ascent, and manually arrests the climber's fall by locking the rope, typically with some form of belay device.\n\nAid climbing involves standing on or pulling oneself up via devices attached to fixed or placed protection is used to make upward progress. In contrast to free climbing protection, which can sustain the force of sometimes long falls, some aid protection is only designed to hold one's body weight.\n\nTop roping involves either placing an anchor at the top of a route before climbing or utilizing a fixed one, then running a rope through it to a belayer on the ground. Unlike in lead climbing, the belayer takes in rope as the climber advances and slack is practically eliminated from the rope, minimizing both the drop and shock load on the rope and protection system should the climber fall.\n\nSolo climbing involves climbing without a partner. Soloing can be done with or without protection. A solo climber may place protection and clip in with a short tether for safety during a difficult move, then remove the protection and continue the ascent. Or they may employ some form of self-locking device, such as a Silent Partner, in lieu of a belayer, allowing a soloist to climb without a partner. Additionally, soloing can also be done using a top rope.\n\nBouldering involves climbing routes low to the ground without rope. The chief form of protection from injury used is a bouldering mat, a padded foam-cell mat placed on the ground below a climber. In bouldering, one can also utilize a \"spotter\". A spotter is someone who stands near the bouldering mat and guides the climber to the mat in the event of a fall.\n\nThe gear used to protect climbs includes:\n\nFixed protection usually consists of permanently anchored bolts fitted with hangers, a chain, and ring, or pitons left \"in situ\".\n\nThere are two major standards for climbing equipment safety and reliability worldwide:\n\nIn recent years, the CEN has become an important standards organization, mainly in Europe since any products sold in Europe must by law be third-party certified to the relevant standards. There is no such requirement in most other countries, although most manufacturers voluntarily follow UIAA or CEN standards (much like electrical equipment in the US is almost always privately certified by Underwriters Laboratories).\n\nIn Europe, equipment used by climbers has to meet the requirements of the Personal and Protective Equipment (PPE) Directive. Essentially, the equipment must be manufactured using a carefully controlled process and samples must pass various tests. Equipment meeting the regulations is marked with the CE Mark. Various standards are used to specify how equipment is to be tested:\n\n\nThere are many more, most of them appearing in ICS code 97.220.40 and having \"Mountaineering\" in the title.\n\nThe UIAA Safety Commission develops and maintains safety standards for climbing equipment. These standards are implemented world-wide by the manufacturers who also participate in annual Safety Commission meetings. The Commission works with nearly 60 manufacturers world-wide and has 1861 products certified.\n\nIn the mid-nineties, CEN adopted the UIAA Safety Standards. Both commissions in CEN and UIAA share similar members.\n\n"}
{"id": "53103457", "url": "https://en.wikipedia.org/wiki?curid=53103457", "title": "Configurable mixed-signal IC", "text": "Configurable mixed-signal IC\n\nConfigurable Mixed-signal IC (abbreviated as CMIC) is a category of ICs comprising a matrix of analog and digital blocks which are configurable through programmable (OTP) non-volatile memory. The technology, in combination with its design software and development kits, allows immediate prototyping of custom mixed-signal circuits, as well as the integration of multiple discrete components into a single IC to reduce PCB cost, size and assembly issues.\n"}
{"id": "26615494", "url": "https://en.wikipedia.org/wiki?curid=26615494", "title": "Controlled traffic farming", "text": "Controlled traffic farming\n\nControlled traffic farming (CTF) is a management tool which is used to reduce the damage to soils caused by heavy or repeated agricultural machinery passes on the land. This damage and its negative consequences have been well documented and include increased fuel use, poor seedbeds, reduced crop yields and poor soil function in terms of water infiltration, drainage and greenhouse gas mitigation due to soil compaction.\n\nControlled traffic farming is a system which confines all machinery loads to the least possible area of permanent traffic lanes. Current farming systems allow machines to run at random over the land, compacting around 75% of the area within one season and at least the whole area by the second season. Soils don’t recover quickly, taking as much as a few years (e.g., >5 years, particularly in soils without swelling-shrinking properties). A proper CTF system on the other hand can reduce tracking to just 15% and this is always in the same place. CTF is a tool; it does not include a prescription for tillage although most growers adopting CTF use little or none because soil structure does not need to be repaired. The permanent traffic lanes are normally parallel to each other and this is the most efficient way of achieving CTF, but the definition does not preclude tracking at an angle. The permanent traffic lanes may be cropped or non-cropped depending on a wide range of variables and local constraints.\n\nControlled traffic farming can be achieved on any scale but to get tracked areas to the minimum possible, there are three requirements:\n\nMatching implement widths is a case of forward planning, or making sure that if anything doesn’t match now, its replacement will. It’s also the case that the wider the implements are, the less will be the tracked area. Growers often find they can use wider machines because without soil damage, they are easier to pull and in the case of cultivators, most likely don’t need to work anywhere near as deep.\n\nMatching track widths is more difficult because grain harvesters often come with a track of 3 m or more and matching to these would make all machines very wide and often impractical to use on a daily basis. Matching to the harvester track works well in Australia where properties are often remote, there is plenty of space and road travel may not be too extensive. In other parts to the world, including most of Europe, alternatives have been found. These may not be quite as efficient in minimising the tracked area, but achieving less than 20% is still perfectly feasible. \n\nIn controlled traffic farming systems for vegetable cropping, track widths vary between 1.83 m (the 72” imperial standard) to customised systems of perhaps 3.2 m or more. Road transport with these wide systems can be difficult but their use in the Netherlands and Denmark is increasing and growers accept the constraints because of the advantages that the systems bring.\n\nKeeping machines in exactly the same place is most easily achieved with a satellite guidance system based on an RTK correction signal and auto-steer. Only the RTK system can guarantee to keep vehicles in the same place year in year out and it also achieves the highest pass to pass accuracy of around ± 2 cm. \nGuidance systems have many other advantages and once a system of this nature has been adopted, the natural progression is to move to controlled traffic farming. \n\nThe generic advantages of guidance include much reduced overlap between passes of machines, particularly of wide cultivators which may overlap by around 10%. Although planters and drills use physical markers to match up between one pass and the next, cumulative errors can be large and non-cropped tracks created by these machines (tramlines) for chemical applications can often have an overlap of 5%. The implications of this are significant because tramlines are used for all chemical applications. A 5% overlap wastes the equivalent in increasingly expensive materials and through overdosing, damages crops and could lead to extra run-off and diffuse pollution.\n\nThe main advantages are:\n\nMore discipline is required in the field, and can increase journey times when removing large volumes of material, such as sugar beet or potatoes.\n\nThere is greater reliance on technology in the form of satellite guidance and auto-steer. If a machine is in a state of disrepair, an exact replacement with a matching track or implement width may not be available.\n\nThe cutting width of grain harvesters for example, seldom match up to cultivator or drill widths. More demand and time could solve this issue.\n\nOnce a field has been laid out with a controlled traffic farming system, it is not advantageous to change it. However, it is not impossible because only around 20% of the field may be compacted and the position of these strips is known.\n\nAlthough controlled traffic farming is still in its infancy as far as adoption is concerned (partially because the enabling technology of satellite guidance is still relatively new), there is a better engineering solution that would reduce tracked areas to less than 10%. This is not a recent concept, having been pioneered by Alexander Halkett in the 1850s and David Dowler in the 1970s, but the concept of a wide span vehicle is becoming increasingly attractive because of the other advantages it brings.\n\nThe concept is described in detail at controlled traffic farming Europe's wide span page and achieves the low tracked area by virtue of using one of the same wheel tracks on adjacent passes. The system also reduces reliance on satellite technology because a guiding wheel track for the next pass is automatically laid down. With implements mostly contained within its wheel track, part width operations, for example ploughing or root crop harvesting, are perfectly feasible, whereas with existing tractor systems they are not. \n\n"}
{"id": "44078957", "url": "https://en.wikipedia.org/wiki?curid=44078957", "title": "Cord-cutting", "text": "Cord-cutting\n\nIn broadcast television, cord-cutting refers to the pattern of viewers, referred to as cord-cutters, cancelling their subscriptions to multichannel subscription television services available over cable, dropping pay television channels or reducing the number of hours of subscription TV viewed in response to competition from rival media available over the Internet such as Amazon Prime, Sling TV, Crunchyroll, Hulu, Netflix and YouTube Premium. This Internet content is either free or significantly cheaper than the same content provided via cable.\n\nAs a market trend, a growing number of \"cord cutters\" do not pay for subscription television in favour of some combination of broadband Internet and IPTV, digital video recorders, digital terrestrial television and/or free-to-air satellite television broadcasts. A related group, the cord-nevers, have never used commercial cable for television service, relying on internet sources from the start. A number of purely internet television services, part of the wider IPTV concept, have emerged to cater to these groups.\n\nParks Associates estimated that in 2008, about 0.9 million American households relied entirely on the Internet for television viewing; by 2017, this figure had increased to 22.2 million. Leichtman Research Group found that six percent of Americans watched at least one show online each week in 2008, a figure that grew to eight percent in 2009. The number of Americans subscribing to cable service increased two percent in 2008, but the growth had slowed. Sanford C. Bernstein & Co. found that in the fourth quarter of 2008, the increase was seven-tenths of one percent, or 220,000 homes, the lowest ever recorded. A Centris report showed that 8% of Americans expected to cancel their pay television service by the third quarter of 2009. About half of Americans tried to get a better deal from a provider other than the one they were subscribed to. Amazon Video, Hulu, iTunes, Netflix, Sling TV and YouTube, made cancelling service possible for those who would be unable to see their favorite programs over the air. Sports programming was a big reason for not cancelling pay television service, although online options existed for many events. Another problem was the inability to watch many programs live, or at least soon enough in the case of a television series.\n\n2010 was the first year that pay television saw quarterly subscriber declines. In the second quarter of 2012, Sanford Bernstein determined that losses took place in five quarters. Leichtman found that the decrease in pay subscriptions was not happening in large numbers. One reason was that some sports events, as well as other types of television (such as series airing on cable-originated networks), could not be seen online. Sanford Bernstein said the number of pay television subscribers increased by 677,000 during the first quarter of 2010, and a poll conducted by \"The New York Times\" and CBS News showed that 88% of people surveyed had such a service, and only 15% had considered going exclusively to web services. People under the age of 45, the survey said, were four times more likely to use the Internet only. To combat the trend, pay television providers were allowing people to stream television programs on desktop, laptop and tablet computers. Craig Moffett of Sanford C. Bernstein still stated that high prices and other methods would eventually drive customers away, calling cord cutting \"perhaps the most overhyped and overanticipated phenomenon in tech history.\"\n\nComcast reported a loss of 275,000 subscribers in the third quarter of 2010, bringing the total for the calendar year to 625,000. The company said most of these losses were not from people leaving for another service. Moffett pointed out that cable companies needed to offer lower-cost packages, but a survey by Strategy Analytics revealed financial considerations were not the primary reason. People were not satisfied with what they could get, and online sources had a wider array of content. The survey showed that 13% of cable subscribers intended to cancel service in the next year. Slightly more than half were under the age of 40, and nearly all had a high school education. Two-thirds had or planned further schooling, and just over half earned at least $50,000 a year.\n\nIn second quarter 2011, Comcast lost 238,000 television customers, compared to 265,000 a year earlier, though the company was making up for these losses with increases in other services such as Internet. Moffett said the slowing rate indicated that online sources were not making people drop cable as quickly. On the other hand, Time Warner Cable and Charter Communications lost more customers in the quarter than in 2010. Time Warner Cable lost 130,000, while Dish Network lost 135,000; by comparison, DirecTV gained 26,000 subscribers, compared to 100,000 the previous year. Nielsen Media Research estimated that the number of households with at least one television set had decreased from 115.9 million to 114.7 million, while also estimating an increase in program viewing by computer, tablets or smartphones. Services such as U-verse were increasing their subscriber numbers by offering special features: U-verse's \"My Multiview\" option allowed people to watch four channels at once, while Cablevision's \"iO TV Quick Views\" allowed the display of up to nine channels at once.\n\nA Nielsen report showed that during the fourth quarter of 2011, the number of people paying for television had dropped by 15 million people (a rate of 1.5 percent), and the number of cable subscribers dropped by 2.9 million. A 2012 Deloitte report said 9% of television households dropped cable service during 2011 and an additional 11% planned to cancel their service. Sanford Bernstein estimates 400,000 dropped pay video services during the second quarter of 2012, up from 340,000 in 2011. One reason for the drop was college students' returning home for the summer, while the companies made up for the loss in other quarters. However, the number of new homes paying for television service is less than the total number of new homes. Another possible reason is services, such as time shifting and live recording capabilities, that were once exclusive to pay television services, are now being offered to cord cutters. Although the number of subscribers usually increases in the third quarter, in 2012 only 30,000 people added pay television service, according to a study by the International Strategy & Investment Group. Cable lost 340,000 subscribers (with Time Warner Cable accounting for 140,000 of that number) and satellite gained only 50,000; telephone companies added 320 subscribers. Throughout 2012, pay television added only 46,000 new subscribers, out of 974,000 new households overall, according to SNL Kagan. 84.7 percent of households subscribed, compared to 87.3 percent in early 2010.\n\nAnother category of cord-cutters was labeled by Nielsen in March 2013 as \"Zero TV\". In 2007, two million households had neither subscribed to a pay television service or received television programming via antenna. By 2013, this number had increased to five million. Most people in this category were younger and did not have children in the household. People could still view shows via online streaming through services such as Netflix. At the 2013 National Association of Broadcasters Show, the solution for broadcasters was stated to be mobile television. A 2013 Leichtman survey showed that the 13 largest MVPD companies, covering 94 percent of the country, experienced their first year-to-year subscriber losses. 80,000 subscribers dropped their service in the year ending March 31, 2013. 1.5 million cable customers dropped their service, with Time Warner Cable losing 553,000 and Comcast losing 359,000 subscribers. AT&T and Verizon added 1.32 million subscribers; DirecTV and Dish added 160,000 subscribers, compared to 439,000 the previous year. Before 2013, only quarter-to-quarter losses had been recorded industrywide. Internet video and switching to receiving television programming by antenna were reasons. Bruce Leichtman described the subscription television industry as \"saturated\". A TDG study showed nearly 101 million U.S. households subscribed to television at the industry's peak in 2011, but the number would fall below 95 million in 2017. In 2013, the number of total subscribers to pay TV services fell by a quarter of a million. This was the first decline from one year to the next.\n\nIn Q1 2017 the pay TV industry experience subscriber loss that was five times greater than Q1 of 2016. According to a Statisica survey in 2015, 31.64% of people who cut the cord said they did so because of price increase. Yet cable companies increased their rates 3.5%. \n\nSome broadcasters have elected to embrace the concept of cord-cutting by establishing subscription-based over-the-top content offerings of their own, such as HBO Now. Alongside the 2014 launch of CBS All Access, Les Moonves stated that there was a \"very strong possibility\" Showtime would also offer an OTT service—a plan which would be realized in June 2015. On March 31, 2016, Canadian sports channel Sportsnet (owned by media and telecom conglomerate Rogers Communications) announced an OTT service offering its four regional feeds and two main national channels.\n\nA TiVo survey showed that 19.8 percent of those without pay TV service had dropped it in the previous year.\n\nOn November 28, 2011, a report by Credit Suisse media analyst Stefan Anninger said that young people who grew up accustomed to watching shows online would be less likely to subscribe to pay television services, terming these people as \"cord-nevers\". Anninger predicted that by the end of 2012, the industry's subscriber count would drop by 200,000 to 100.5 million; Anninger's report also stated that consumers were not likely to return to paying for television. In the case of land-line telephones, people had believed younger people would eventually get them, but now numerous subscribers only have mobile phones. Anninger predicted that the same would hold true for pay television, and that providers would need to offer lower-priced packages with fewer channels in order to reverse the trend. Also using the term \"cord-nevers\" was Richard Schneider, whose company Antennas Direct was selling antennas through the Internet. After a decade in business, the company was selling 600,000 antennas a year. However, Schneider said some people only knew of the Internet and services such as Netflix and were not even aware broadcast television even existed. In a speech on November 16, 2012, Time Warner CEO Jeff Bewkes said \"cord nevers\" did not see anything worth paying for.\n\nIn 2013, specific channels were losing more subscribers than pay TV providers. This was because of what came to be known as \"cord shaving\", switching to a cheaper package of channels.\n\nIn an effort to entice cord cutters and cord nevers, some cable television providers have begun offering Internet-only streaming services. Cablevision began to offer \"Cord Cutter\" packages that include a free digital antenna and access to its Optimum WiFi network, as well as the option to add HBO Now to the service, making it the first ever cable provider to do so. In 2015, Comcast and Time Warner Cable (TWC) began to trial television services delivered via their managed internet infrastructures; Comcast's \"Stream\" service offered access to broadcast networks, HBO, Xfinity StreamPix, and their respective TV Everywhere services. Outside of TVE apps, the service can only be accessed via Comcast home internet on supported devices. In October 2015, TWC began to trial a service under which subscribers are given a Roku 3 digital media player to access their service via the supplied TWC app, rather than a traditional set-top box. A TWC spokesperson emphasized that this offering would provide \"the same TV and same packages delivered to the home today\", but delivered over TWC-managed internet rather than a cable line. This service has since been transferred to the current Spectrum service after Time Warner Cable's merger with Charter, with an equivalent Apple TV app forthcoming.\n\nAt the end of 2017, most new televisions had Internet capability. This contributed to the cord-cutting trend.\n"}
{"id": "58520836", "url": "https://en.wikipedia.org/wiki?curid=58520836", "title": "Cylinder Head Temperature gauge", "text": "Cylinder Head Temperature gauge\n\nA Cylinder Head Temperature gauge (CHT) measures the cylinder head temperature of an engine. Commonly used on air-cooled engines, the head temperature gauge displays the work that the engine is performing more quickly than an oil or water temperature gauge. As the engine works at high speed or uphill, head temperature will increase quickly. The meter can be digital or analog.\n\nA air-cooled engine requires a steady flow of air for cooling. Unlike water cooled engine, air cooled engines have no thermostat. Over temperature can cause engine failure. Air-cooled engine are used in aircraft engine control and other air-cooled engines as in cars and air-cooled motorcycles.\n\nThe CHT senders usually has a K-type thermocouple that is mounted under the spark plug. The K-type thermocouple is pair of two dissimilar metals that produce a small voltage signal when heated. The metal closest to the spark plug is called the hot junction and the other close to the head cold junction. The ring under the spark plug is used to transfer the heat from the plug to the thermocouple. The gauge and cold junction are usually calibrated at room temperature . Because the thermocouple is calibrated for room temperature, the gauge readings will only be 100% accurate at that engine compartment temperature. If the engine compartment temperature is colder the CHT temperature will display higher. If the engine compartment temperature is higher the reading will be lower. The error can be fixed with a cold-junction compensating thermistor, which measures the temperature at the cold junction so the gauge can adjust the reading. Low budget gauges do not have this compensating thermistor.\n\n\n"}
{"id": "36691790", "url": "https://en.wikipedia.org/wiki?curid=36691790", "title": "Decapoint", "text": "Decapoint\n\nDecapoint, or \"raphigraphy\", was a tactile form of the Latin script invented by Louis Braille as a system that could be used by both the blind and sighted. It was published in 1839. Letters retained their linear form, and so were legible without training to the sighted, but the lines were composed of embossed dots like those used in braille. Each letter contained ten dots in the height and different dots in the width to produce the graphic form of print.\n\nThe reason for the development of this writing was that relatives of the students could not read braille.\n\nThese letters were not easy for the blind to write because of their height of ten dots despite grid. It therefore did not take long for the blind friend of Louis Braille Pierre-François-Victor Foucault in 1841 to build the first apparatus, the Raphigraph, which could push all the points of one column of characters at the same time into the paper. This font was now named (Raphigrafie or Raphigraphie).\n\nWhen the first typewriters were invented, they repressed quickly the complicated Raphigraphy or decapoint, despite the impossibility for the blind to read the writing of typewriters. And so the Decapoint or Raphigraphy fell into oblivion, but it was the first digital font of Latin letters ever.\n\n"}
{"id": "18266777", "url": "https://en.wikipedia.org/wiki?curid=18266777", "title": "Decoupled debit card", "text": "Decoupled debit card\n\nA decoupled debit card is a debit card in the US that is not issued by, and not tied to, any particular retail financial institution, such as a bank or credit union. This is based on the ability in the US ACH Network payment system to make an electronic payment from any bank or credit union without needing to use a card issued by the bank or credit union. A third party, such as a retailer, can create a decoupled debit card which will use this system to make a payment from the customer's checking account. They may do this as part of a loyalty scheme or to reduce their own debit card processing costs.\n\nIn May 2007, Capital One began a one-year decoupled debit card experiment. This card was novel in that prior to this launch, a debit card was always tied to a traditional financial institution. Capital One's MasterCard-branded decoupled card did not require an account be opened with a retail financial institution, and was made in partnership with the Ukrops grocery chain, based in Capital One's hometown of Richmond, Virginia. The card was also tied to a reward program offered by Ukrops. The one-year experiment ended in May 2008, and was followed by a national rollout of its own version of a decoupled debit card tied to its own reward program.\n\nDecoupled Debit gained the attention of the financial services industry in May 2007 when Capital One announced they were going to add a decoupled debit card product. At the time Capital One did not offer checking accounts—only credit cards and this was a strategy to offer a debit card without actually owning the checking account relationship. The one year pilot ended without Capital One citing specific reasons as why, leaving the industry with only speculation as to why.\n\nToday, decoupled debit cards are typically issued and branded by retailers without any association to a national network such as Visa, MasterCard or Amex. The retailer has complete control over the issuing of the cards to their customers and the processing of the payment. By definition, decoupled debit cards are processed via the Federal Reserve ACH System as the mechanism for reaching a consumer’s checking account as a debit to their account for their purchase. The payment side of the product operates like an electronic check, but the product is more than just a payment card.\n\nWhen the Durbin Amendment became law in October 2011, there was new speculation that this was the end of decoupled debit. At the time there were only two main companies offering decoupled debit: Tempo and National Payment Card Association. Tempo did in fact close their doors shortly after the roll-out of the Durbin Amendment—National Payment Card Association (NPCA) did not and is still thriving today in 2013.\n\nWhy one survived Durbin and the other did not is a function of their differences in business models and program structures. Tempo had a product model tied to Discover and accepted signature debit which resulted in a pricing model and risk profile that would no longer be attractive to retailers under Durbin.\n\nNational Payment Card Association has a different pricing model – generally a fixed price per transaction of $.15 that retailers pay in processing fees, which is still very competitive even under Durbin regulation. The company also has a well articulated and easily understood value proposition for retailers:\n\nRetailers reap swipe fee savings when they have a National Payment Card program when compared to what they pay in swipe fees for accepting Visa, MasterCard, AMEX, etc. The savings are used to fund rewards to the retailer’s customers which provides the incentive to the consumer to carry another payment card in their wallet.\n\nThe Target Red Card program is a great example of how the value proposition works. Consumers enroll with Target, provide their checking account to be debited and receive 5% discount at the register when they use the Red Card to pay. Target saves the fees they would have otherwise paid to the payment networks and banks and gives the savings back to their customers.\n\nNPCA on the other hand has developed their niche in c-store petroleum. The programs offer consumers an instant price rollback at the pump of anywhere between 3 cents and 10 cents off per gallon of fuel purchased. \nThe programs offer another benefit to retailers and that is in building a customer data base. Since the consumer enrolls (generally online) into the program, the retailer gains baseline information about their cardholders (name, address, phone number, email addresses, etc.) and they gain the ability to track spending. This is information banks typically have when a customer uses a bank-issued payment card. Now the retailer has the information and can mine the data for use in targeted marketing to their customers.\n\nOn a going forward basis, the real payoff for decoupled debit might very well end up to be mobile payments. The most noted program in this early life cycle of mobile payment using decoupled debit is Cumberland Farms SmartPay. In this program, consumers can have a mobile payment app, a traditional mag stripe card, or both. The program tracks lifetime savings within the app and coupons and promotions can be delivered to the app while the Cumberland Farms customers are fueling.\n\nIn October 2013 payments technology firm FIS announced a new private label, decoupled debit product called \"InterPayment\". InterPayment utilizes proprietary FIS technology and payment processing platforms.\n\n\n"}
{"id": "17813566", "url": "https://en.wikipedia.org/wiki?curid=17813566", "title": "Diffuser (heat)", "text": "Diffuser (heat)\n\nA heat diffuser is a cooking utensil that is placed on top of a ring on a cooktop in order to separate the pan/pot from the direct source of heat.\n"}
{"id": "4077375", "url": "https://en.wikipedia.org/wiki?curid=4077375", "title": "Directivity", "text": "Directivity\n\nIn electromagnetics, directivity is a parameter of an antenna or optical system which measures the degree to which the radiation emitted is concentrated in a single direction. It measures the power density the antenna radiates in the direction of its strongest emission, versus the power density radiated by an ideal isotropic radiator (which emits uniformly in all directions) radiating the same total power.\n\nAn antenna's directivity is a component of its gain; the other component is its (electrical) efficiency. Directivity is an important measure because many antennas and optical systems are designed to radiate electromagnetic waves in a single direction or over a narrow angle. Directivity is also defined for an antenna receiving electromagnetic waves, and its directivity when receiving is equal to its directivity when transmitting.\n\nThe directivity of an actual antenna can vary from 1.76 dBi for a short dipole, to as much as 50 dBi for a large dish antenna.\n\nThe \"directivity\", formula_1, of an antenna is the maximal value of its \"directive gain\". Directive gain is represented as formula_2and compares the radiant intensity (power per unit solid angle) formula_3 that an antenna creates in a particular direction against the average value over all directions:\n\nHere formula_5 and formula_6 are the zenith angle and azimuth angle respectively in the standard spherical coordinate angles; formula_3 is the radiation intensity, which is the power per unit solid angle; and formula_8is the total radiated power. The quantities formula_9 and formula_8 satisfy the relation\n\nthat is, the total radiated power formula_8 is the power per unit solid angle formula_3 integrated over a spherical surface. Since there are 4π steradians on the surface of a sphere, the quantity formula_14 represents the \"average\" power per unit solid angle.\n\nIn other words, directive gain is the radiation intensity of an antenna at a particular formula_15 coordinate combination divided by what the radiation intensity would have been had the antenna been an isotropic antenna radiating the same amount of total power into space.\n\n\"Directivity\", then, is the maximal directive gain value found among all possible solid angles:\n\nThe word \"directivity\" is also sometimes used as a synonym for directive gain. This usage is readily understood, as the direction will be specified, or directional dependence implied. Later editions of the IEEE Dictionary specifically endorse this usage; nevertheless it has yet to be universally adopted.\n\nIn an antenna array (a set of multiple identical antennas which work together as a single antenna), the directivity of the entire array is the multiplicative sum of the individual antenna's directivity function with a mathematical expression known as the array factor formula_17, which typically depends on the location, the excitation and the phase of each antenna element. The overall directivity function is given by\n\nwhere formula_19 is the directivity of a single element. This single element term is sometimes referred to as the \"element pattern\".\n\nThe beam solid angle, represented as formula_20, is defined as the solid angle which all power would flow through if the antenna radiation intensity were constant at its maximal value. If the beam solid angle is known, then directivity can be calculated as\n\nwhich simply calculates the ratio of the beam solid angle to the solid angle of a sphere.\n\nThe beam solid angle can be approximated for antennas with one narrow major lobe and very negligible minor lobes by simply multiplying the half-power beamwidths (in radians) in two perpendicular planes. The half-power beamwidth is simply the angle in which the radiation intensity is at least half of the peak radiation intensity.\n\nThe same calculations can be performed in degrees rather than in radians:\n\nwhere formula_23 is the half-power beamwidth in one plane (in degrees) and formula_24 is the half-power beamwidth in a plane at a right angle to the other (in degrees).\n\nIn planar arrays, a better approximation is\n\nFor an antenna with a \"conical\" (or approximately conical) beam with a half-power beamwidth of formula_5 degrees, then elementary integral calculus yields an expression for the directivity as\n\nThe directivity is rarely expressed as the unitless number formula_1 but rather as a decibel comparison to a reference antenna:\n\nThe reference antenna is usually the theoretical perfect isotropic radiator, which radiates uniformly in all directions and hence has a directivity of 1. The calculation is therefore simplified to\n\nAnother common reference antenna is the theoretical perfect half-wave dipole, which radiates perpendicular to itself with a directivity of 1.64:\n\nWhen polarization is taken under consideration, three additional measures can be calculated:\n\nPartial directive gain is the power density in a particular direction and \"for a particular component of the polarization\", divided by the average power density for all directions and \"all polarizations\". For any pair of orthogonal polarizations (such as left-hand-circular and right-hand-circular), the individual power densities simply add to give the total power density. Thus, if expressed as dimensionless ratios rather than in dB, the total directive gain is equal to the sum of the two partial directive gains.\n\nPartial directivity is calculated in the same manner as the partial directive gain, but without consideration of antenna efficiency (i.e. assuming a lossless antenna). It is similarly additive for orthogonal polarizations.\n\nPartial gain is calculated in the same manner as gain, but considering only a certain polarization. It is similarly additive for orthogonal polarizations.\nThe term directivity is also used with other systems.\n\nWith directional couplers, directivity is a measure of the difference in dB of the power output at a coupled port, when power is transmitted in the desired direction, to the power output at the same coupled port when the same amount of power is transmitted in the opposite direction.\n\nIn acoustics, it is used as a measure of the radiation pattern from a source indicating how much of the total energy from the source is radiating in a particular direction. In electro-acoustics, these patterns commonly include omnidirectional, cardioid and hyper-cardioid microphone polar patterns. A loudspeaker with a high degree of directivity (narrow dispersion pattern) can be said to have a high \"Q\".\n"}
{"id": "1451167", "url": "https://en.wikipedia.org/wiki?curid=1451167", "title": "Disability robot", "text": "Disability robot\n\nA disability robot is a robot designed to help people who have physical disabilities that impede with daily tasks. The field of expertise that creates such robots is called \"disability robotics\".\n\nDisability robot has been proven to assist people who are recovering from strokes and people who have abstained injuries that effect their daily tasks. \n\nIn 1988 the National Institute of Disability and Rehabilitation Research, NIDRR, awarded Gaulladet University a grant for the project “Robotic finger spelling hand for communication and access to text by deaf-blind persons.” Researchers at the University developed and tested a robotic hand. Although it was never commercialized the concept is relevant for current and future research. \n\nSince this grant, many others have been written. NIDRR funded research appears to be moving from the fabrication of robotic arms that can be used by disabled persons to perform daily activities, to developing robotics that assist with therapy in the hopes of achieving long-term performance gains. If there is success in development of robotics, these mass-marketed products could assist tomorrow’s longer-living elderly individuals enough to postpone nursing home stays. \"Jim Osborn, executive director of the Quality of Life Technology Center, recently told a gathering of long-term care providers that if such advances could delay all nursing home admissions by a month, societal savings could be $1 billion monthly.\" Shortage of both paid personal assistants and available family members makes artificial assistance a necessity.\n\nChildren with severe disabilities can develop learned helplessness, which makes them lose interest in their environment. Robotic arms are used to provide an alternative method to engage in joint play activities. These robotic arms allows children to manipulate real objects in the context of play activities.\n\nDisability robotics is a broad category that includes wheelchairs, robotic arms, and other robotic devices that assist disabled persons of all ability levels. This section will provide examples of the many types of robotic devices used to assist disabled persons.\n\nPersons with severe disabilities may be assisted with robotic wheelchairs when manual control is not possible. These devices can deter loss of residual skills and frustration. Traditionally wheelchairs either gave control to the person or robot depending on disability level.\n\nBodyweight-supported treadmill training (BWSTT) are used to enhance walking ability of people with neurological injury. These machines are therapist-assisted devices that are used in the clinical setting, but is limited by the personnel and labor requirements placed on physical therapists. The BWSTT device, and many others like it, assist physical therapists by providing task-specific practice of walking in people following neurological injury.\n\n"}
{"id": "10705807", "url": "https://en.wikipedia.org/wiki?curid=10705807", "title": "Gillham code", "text": "Gillham code\n\nGillham code is a zero-padded 12-bit binary code using a parallel nine- to eleven-wire interface, the Gillham interface, that is used to transmit uncorrected barometric altitude between an encoding altimeter or analog air data computer and a digital transponder. It is a modified form of a Gray code and is sometimes referred to simply as a \"Gray code\" in avionics literature.\n\nThe \"Gillham interface\" and \"code\" are an outgrowth of the 12-bit IFF Mark X system, which was introduced in the 1950s. The civil transponder interrogation modes A and C were defined in air traffic control (ATC) and secondary surveillance radar (SSR) in 1960. The exact origin of the term \"Gillham code\" is unclear, but by 1962 the code was broadly recognized under this name and described in an FAA report. By the mid-1960s the code was also known as \"MOA–Gillham code\" or \"ICAO–Gillham code\". \"ARINC 572\" specified the code as well in 1968.\n\nOnce recommended by the ICAO for automatic height transmission for air traffic control purposes, it is now discouraged and has been mostly replaced by modern serial communication in newer aircraft.\n\nAn altitude encoder takes the form of a small metal box containing a pressure sensor and signal conditioning electronics. The pressure sensor is often heated, which requires a warm-up time during which height information is either unavailable or inaccurate. Older style units can have a warm-up time of up to 10 minutes; more modern units warm up in less than 2 minutes. Some of the very latest encoders incorporate unheated 'instant on' type sensors. During the warm-up of older style units the height information may gradually increase until it settles at its final value. This is not normally a problem as the power would typically be applied before the aircraft enters the runway and so it would be transmitting correct height information soon after take-off.\n\nLight aircraft electrical systems are typically 14 V or 28 V. To allow seamless integration with either, the encoder uses a number of open-collector (open-drain) transistors to interface to the transponder. The height information is represented as 11 binary digits in a parallel form using 11 separate lines designated D2 D4 A1 A2 A4 B1 B2 B4 C1 C2 C4. As a twelfth bit, the Gillham code contains a D1 bit but this is unused and consequently set to zero in practical applications.\n\nDifferent classes of altitude encoder do not use all of the available bits. All use the A, B and C bits; increasing altitude limits require more of the D bits. Up to and including 30700 ft does not require any of the D bits (9-wire interface). This is suitable for most light general aviation aircraft. Up to and including 62700 ft requires D4 (10-wire interface). Up to and including 126700 ft requires D4 and D2 (11-wire interface). D1 is never used.\n\nBits D2 (msbit) through B4 (lsbit) encode the pressure altitude in 500 ft increments (above a base altitude of −1000±250 ft) in a standard 8-bit reflected binary code (Gray code). The specification stops at code 1000000 (126500±250 ft), above which D1 would be needed as a most significant bit.\n\nBits C1, C2 and C4 use a mirrored 5-state 3-bit Gray BCD code of a Giannini Datex code type (with the first 5 states resembling O'Brien code type II) to encode the offset from the 500 ft altitude in 100 ft increments. Specifically, if the parity of the 500 ft code is even then codes 001, 011, 010, 110 and 100 encode −200, −100, 0, +100 and +200 ft relative to the 500 ft altitude. If the parity is odd, the assignments are reversed. Codes 000, 101 and 111 are not used.\n\nThe Gillham code can be decoded using various methods. Standard techniques use hardware or software solutions. The latter often uses a lookup table but an algorithmic approach can be taken.\n\n\n"}
{"id": "4624733", "url": "https://en.wikipedia.org/wiki?curid=4624733", "title": "Glue code", "text": "Glue code\n\nIn computer programming, glue code is executable code (often source code) that serves solely to \"adapt\" different parts of code that would otherwise be incompatible. Glue code does not contribute any functionality towards meeting program requirements. Instead, it often appears in code that lets existing libraries or programs interoperate, as in language bindings or foreign function interfaces such as the Java native interface, when mapping objects to a database using object-relational mapping, or when integrating two or more commercial off-the-shelf programs. Glue code may be written in the same language as the code it is gluing together, or in a separate glue language. Glue code is very efficient in rapid prototyping environments where several components are quickly put together into a single language or framework.\n\nBecause each component is independent (i.e. it is unaware of its relations and is only connected to another component through glue code), the behavior of a component and its interactions can change during the execution of the script. In addition, a different version of one of the components may behave differently, breaking the glue code.\n\nHigh-level programming languages can suffer from performance penalties because glue code must run through the language interpreter, even when connecting high-performance subsystems. If performance is crucial, using configuration scripting is often preferred to directly connecting binary interfaces of components. In object-oriented scripting languages, glue code often eliminates the need for class hierarchies and large numbers of classes.\n\n\n"}
{"id": "19589645", "url": "https://en.wikipedia.org/wiki?curid=19589645", "title": "Head mirror", "text": "Head mirror\n\nA head mirror is a simple diagnostic device, stereotypically worn by physicians, but less so in recent decades as they have become somewhat obsolete.\n\nA head mirror is mostly used for examination of the ear, nose & throat. It comprises a circular concave mirror, with a small hole in the middle, and is attached to a head band. The mirror is worn over the physician's eye of choice, with the concave mirror surface facing outwards and the hole directly over the physician's eye, providing illumination like a ring light.\n\nIn use, the patient sits and faces the physician. A bright lamp is positioned adjacent to the patient's head, pointing toward the physician's face and hence towards the head mirror. The light from the lamp reflects off the mirror, along the line of sight of the user, with the light being somewhat concentrated by the curvature of the mirror. When used properly, the head mirror thus provides excellent shadow-free illumination.\n\nBecause they were once in common use, notably by general practitioners and otorhinolaryngologists, head mirrors are often a stereotypical part of a physician's uniform by costumers and prop men e.g. in comic routines. The main drawback to head mirrors was that they require some skill to use well. They are rarely seen outside of the ENT setting, having been largely replaced by pen lights among general practitioners. They are still routinely used by otolaryngologists, particularly for examination and procedures involving the oral cavity.\n\n"}
{"id": "1932741", "url": "https://en.wikipedia.org/wiki?curid=1932741", "title": "Hydrogen station", "text": "Hydrogen station\n\nA hydrogen station is a storage or filling station for hydrogen, usually located along a road or hydrogen highway, or at home as part of the distributed generation resources concept. The stations are usually intended to power hydrogen vehicles, but can also be used to power small devices. Vehicles use hydrogen as fuel in one of several ways, including fuel cells and mixed fuels like HCNG. The hydrogen fuel dispensers dispense hydrogen gas by the kilogram.\n\nHydrogen Filling Stations global map is available.\n\n\n\n, there are more than 25 stations in Europe capable of filling 4-5 cars per day.\n\n\n\n\n\n\n\n\n\n\n\n\nA hydrogen highway is a chain of hydrogen-equipped filling stations and other infrastructure along a road or highway. Italy and Germany are collaborating to build a hydrogen highway between Mantua in northern Italy and Munich in southern Germany. Italy completed building a hydrogen filling station in Mantua on 21 September 2007.\n\nSince the turn of the millennium, filling stations offering hydrogen have been opening worldwide. However, this does not begin to replace the existing extensive gasoline fuel station infrastructure, which in the US alone numbered 168,000 retail outlets in 2004, with revenues for 2014 of US$536 billion. According to Joseph Romm in 2004 replacing these would cost a half trillion U.S. dollars. The cost of the necessary European-wide hydrogen fuelling infrastructure could be five times lower than the cost of the charging network required for battery and plug-in hybrid vehicles. When viewed as cost per station, EV stations are cheaper than the $3 million per hydrogen station. The reason that hydrogen infrastructure is less expensive than electric, even though individual station cost is much more, is quicker vehicle fueling and longer refueling intervals, thus needing far fewer hydrogen stations per million fuel cell cars than charging stations per million battery electric cars.\n\nHydrogen home stations come in different types.\n\n\n\n\n\nhttp://www.prweb.com/releases/2014/07/prweb12042788.htm\n\n"}
{"id": "34606684", "url": "https://en.wikipedia.org/wiki?curid=34606684", "title": "IA, The Journal of the Society for Industrial Archeology", "text": "IA, The Journal of the Society for Industrial Archeology\n\nIA, The Journal of the Society for Industrial Archeology is a biannual peer-reviewed academic journal published by the Society for Industrial Archeology, currently edited by Fredric L. Quivik (Michigan Technological University). \"IA\" publishes scholarly research, essays, and reviews of books published in the field of industrial archeology.\n\nThe first issue of \"IA\" was published in 1975, followed by one issue per year through volume 11 in 1985. The current biannual publication frequency began with volume 12 in 1986, although there have been several double issues.\n\n\"IA\" has published a number of issues with articles on a common theme, including Montréal's Lachine Canal (2003), the Springfield Armory (1988), the West Point Foundry (2009), and two on the Historic American Engineering Record (1997 and 1999).\n"}
{"id": "1910996", "url": "https://en.wikipedia.org/wiki?curid=1910996", "title": "Implant (medicine)", "text": "Implant (medicine)\n\nAn implant is a medical device manufactured to replace a missing biological structure, support a damaged biological structure, or enhance an existing biological structure. Medical implants are man-made devices, in contrast to a transplant, which is a transplanted biomedical tissue. The surface of implants that contact the body might be made of a biomedical material such as titanium, silicone, or apatite depending on what is the most functional. In some cases implants contain electronics e.g. artificial pacemaker and cochlear implants. Some implants are bioactive, such as subcutaneous drug delivery devices in the form of implantable pills or drug-eluting stents.\n\nImplants can roughly be categorized into groups by application.\n\nSensory and neurological implants are used for disorders affecting the major senses and the brain, as well as other neurological disorders. They are predominately used in the treatment of conditions such as cataract, glaucoma, keratoconus, and other visual impairments; otosclerosis and other hearing loss issues, as well as middle ear diseases such as otitis media; and neurological diseases such as epilepsy, Parkinson's disease, and treatment-resistant depression. Examples include the intraocular lens, intrastromal corneal ring segment, cochlear implant, tympanostomy tube, and neurostimulator.\n\nCardiovascular medical devices are implanted in cases where the heart, its valves, and the rest of the circulatory system is in disorder. They are used to treat conditions such as heart failure, cardiac arrhythmia, ventricular tachycardia, valvular heart disease, angina pectoris, and atherosclerosis. Examples include the artificial heart, artificial heart valve, implantable cardioverter-defibrillator, cardiac pacemaker, and coronary stent.\n\nOrthopaedic implants help alleviate issues with the bones and joints of the body. They're used to treat bone fractures, osteoarthritis, scoliosis, spinal stenosis, and chronic pain. Examples include a wide variety of pins, rods, screws, and plates used to anchor fractured bones while they heal.\n\nMetallic glasses based on magnesium with zinc and calcium addition are tested as the potential metallic biomaterials for biodegradable medical implants.\n\nPatient with orthopaedic implants sometimes need to be put under magnetic resonance imaging (MRI) machine for detailed musculoskeletal study. Therefore, concerns have been raised regarding the loosening and migration of implant, heating of the implant metal which could cause thermal damage to surrounding tissues, and distortion of the MRI scan that affects the imaging results. A study of orthopaedic implants in 2005 has shown that majority of the orthopaedic implants does not react with magnetic fields under the 1.0 Tesla MRI scanning machine with the exception of external fixator clamps. However, at 7.0 Tesla, several orthopaedic implants would show significant interaction with the MRI magnetic fields, such as heel and fibular implant.\n\nContraceptive implants are primarily used to prevent unintended pregnancy and treat conditions such as non-pathological forms of menorrhagia. Examples include copper- and hormone-based intrauterine devices.\n\nCosmetic implants — often prosthetics — attempt to bring some portion of the body back to an acceptable aesthetic norm. They are used as a follow-up to mastectomy due to breast cancer, for correcting some forms of disfigurement, and modifying aspects of the body (as in buttock augmentation and chin augmentation). Examples include the breast implant, nose prosthesis, ocular prosthesis, and injectable filler.\n\nOther types of organ dysfunction can occur in the systems of the body, including the gastrointestinal, respiratory, and urological systems. Implants are used in those and other locations to treat conditions such as gastroesophageal reflux disease, gastroparesis, respiratory failure, sleep apnea, urinary and fecal incontinence, and erectile dysfunction. Examples include the LINX, implantable gastric stimulator, diaphragmatic/phrenic nerve stimulator, neurostimulator, surgical mesh, and penile prosthesis.\n\nMedical devices are classified by the US Food and Drug Administration (FDA) under three different classes depending on the risks the medical device may impose on the user.According to 21CFR 860.3, Class I devices are considered to pose the least amount of risk to the user and require the least amount of control. Class I devices include simple devices such as arm slings and hand-held surgical instruments. Class II devices are considered to need more regulation than Class I devices and are required to undergo specific requirements before FDA approval. Class II devices include X-ray systems and physiological monitors. Class III devices require the most regulatory controls since the device supports or sustains human life or may not be well tested. Class III devices include replacement heart valves and implanted cerebellar stimulators. Many implants typically fall under Class II and Class III devices.\n\nUnder ideal conditions, implants should initiate the desired host response. Ideally, the implant should not cause any undesired reaction from neighboring or distant tissues. However, the interaction between the implant and the tissue surrounding the implant can lead to complications. The process of implantation of medical devices is subjected to the same complications that other invasive medical procedures can have during or after surgery. Common complications include infection, inflammation, and pain. Other complications that can occur include risk of rejection from implant-induced coagulation and allergic foreign body response. Depending on the type of implant, the complications may vary.\n\nWhen the site of an implant becomes infected during or after surgery, the surrounding tissue becomes infected by microorganisms. Three main categories of infection can occur after operation. Superficial immediate infections are caused by organisms that commonly grow near or on skin. The infection usually occurs at the surgical opening. Deep immediate infection, the second type, occurs immediately after surgery at the site of the implant. Skin-dwelling and airborne bacteria cause deep immediate infection. These bacteria enter the body by attaching to the implant’s surface prior to implantation. Though not common, deep immediate infections can also occur from dormant bacteria from previous infections of the tissue at the implantation site that have been activated from being disturbed during the surgery. The last type, late infection, occurs months to years after the implantation of the implant. Late infections are caused by dormant blood-borne bacteria attached to the implant prior to implantation. The blood-borne bacteria colonize on the implant and eventually get released from it. Depending on the type of material used to make the implant, it may be infused with antibiotics to lower the risk of infections during surgery. However, only certain types of materials can be infused with antibiotics, the use of antibiotic-infused implants runs the risk of rejection by the patient since the patient may develop a sensitivity to the antibiotic, and the antibiotic may not work on the bacteria.\n\nInflammation, a common occurrence after any surgical procedure, is the body’s response to tissue damage as a result of trauma, infection, intrusion of foreign materials, or local cell death, or as a part of an immune response. Inflammation starts with the rapid dilation of local capillaries to supply the local tissue with blood. The inflow of blood causes the tissue to become swollen and may cause cell death. The excess blood, or edema, can activate pain receptors at the tissue. The site of the inflammation becomes warm from local disturbances of fluid flow and the increased cellular activity to repair the tissue or remove debris from the site.\n\nImplant-induced coagulation is similar to the coagulation process done within the body to prevent blood loss from damaged blood vessels. However, the coagulation process is triggered from proteins that become attached to the implant surface and lose their shapes. When this occurs, the protein changes conformation and different activation sites become exposed, which may trigger an immune system response where the body attempts to attack the implant to remove the foreign material. The trigger of the immune system response can be accompanied by inflammation. The immune system response may lead to chronic inflammation where the implant is rejected and has to be removed from the body. The immune system may encapsulate the implant as an attempt to remove the foreign material from the site of the tissue by encapsulating the implant in fibrinogen and platelets. The encapsulation of the implant can lead to further complications, since the thick layers of fibrous encapsulation may prevent the implant from performing the desired functions. Bacteria may attack the fibrous encapsulation and become embedded into the fibers. Since the layers of fibers are thick, antibiotics may not be able to reach the bacteria and the bacteria may grow and infect the surrounding tissue. In order to remove the bacteria, the implant would have to be removed. Lastly, the immune system may accept the presence of the implant and repair and remodel the surrounding tissue. Similar responses occur when the body initiates an allergic foreign body response. In the case of an allergic foreign body response, the implant would have to be removed.\n\nThe many examples of implant failure include rupture of silicone breast implants, hip replacement joints, and artificial heart valves, such as the Bjork–Shiley valve, all of which have caused FDA intervention. The consequences of implant failure depend on the nature of the implant and its position in the body. Thus, heart valve failure is likely to threaten the life of the individual, while breast implant or hip joint failure is less likely to be life-threatening.\n\nDevices implanted directly in the grey matter of the brain produce the highest quality signals, but are prone to scar-tissue build-up, causing the signal to become weaker, or even non-existent, as the body reacts to a foreign object in the brain.\n\n\n"}
{"id": "49089762", "url": "https://en.wikipedia.org/wiki?curid=49089762", "title": "International Sustainable Energy Organization", "text": "International Sustainable Energy Organization\n\nThe objective of the International Sustainable Energy Organization for Renewable Energy and Energy Efficiency (ISEO) with headquarters in Geneva is to accelerate and enlarge the worldwide contribution of clean, sustainable energy to economic and equitable social development. Gustav R. Grob founded ISEO for the United Nations in 2002.\n\n\n"}
{"id": "57288756", "url": "https://en.wikipedia.org/wiki?curid=57288756", "title": "Isaac Asimov's Science Adventure", "text": "Isaac Asimov's Science Adventure\n\nIsaac Asimov's Science Adventure is an educational interactive CD-Rom. The game was later updated as Isaac Asimov's Science Adventure II. It is part of Knowledge Adventure's Adventure series.\n\nAsimov died in April 1992, and this collaboration between his works and the developers behind the program was one of the last before he passed. The game's articles were based on Isaac Asimov's Chronology of Science and Discovery. The game teaches topics ranging from roller coaster acceleration to planatary orbit to pulleys. The central hub of the program is a reference screen, which displays text panels, pictures, and timeline, and a globe. The designers described the program as an educational software toy - a sort of intellectual playground, to encourage curiosity but with no agenda. The virtual science museum has over 150 rooms, and over 1000\nillustrated, interactive, and interlinked articles by Isaac Asimov.\n\nPC Mag included it in its list of the Top 100 CD-ROMS, commenting on its \"spectacular computer graphics\" and high quality articles. The magazine recommended it as a holiday gift as much as the Knowledge Adventure titles Buzz Aldrin's Space Adventure and Sports Adventure. Compute magazine thoguht the title was well produced and impressive technically. New Scientist felt the title had excellent graphics and interface. The program was highly recommended by The New York Times.\n"}
{"id": "49523730", "url": "https://en.wikipedia.org/wiki?curid=49523730", "title": "Jane Mulemwa", "text": "Jane Mulemwa\n\nJane Nambakire Mulemwa is a Ugandan chemist and educator. She is the chairperson of the Petroleum Authority of Uganda.\n\nShe was born in the Central Region of Uganda on 18 September 1953. She attended Mount St Mary's College Namagunga. She studied chemistry and biology at Makerere University, graduating with a Bachelor of Science degree and a concurrent Diploma of Education. From 1976 until 1979, she studied for her master's degree in chemistry, also at Makerere. She attended Queen's University Belfast from 1980 until 1982, graduating with a Doctor of Philosophy in chemistry in 1982.\n\nWhile pursuing her master's degree at Makerere University from 1976 until 1979, Mulemwa taught undergraduates in the Department of Education at the university and taught high school students at the nearby Makerere College School. Between 1980 and 1982, while pursuing her doctorate degree in Belfast, she was a \"demonstrator\" to undergraduates in chemistry at Queen's University Belfast, Northern Ireland.\n\nShe returned to Uganda in 1982 to be a lecturer in chemistry and science education at Makerere University. In 1988, she served as senior lecturer in chemistry and science education at the Department of Science and Technical Education, serving in that capacity until 1998. She then joined the Education Service Commission, rising to the rank of deputy chairperson.\n\nIn 2015, she was appointed by President Yoweri Museveni to chair the newly created Petroleum Authority of Uganda, the autonomous regulator of the petroleum industry. In September 2015, Uganda's parliament approved her appointment.\n\n\n"}
{"id": "56075244", "url": "https://en.wikipedia.org/wiki?curid=56075244", "title": "Jsish", "text": "Jsish\n\nJavaScript Interpreter SHell (or Jsi for short) is a scripting language designed for use in embedded systems.\n\nIt is a language used primarily by applications written in C or C++ using GNU tool-chains.\nMinGW provides Windows compatibility.\n\nJsi code compiles as either C99 or C++, with the C++ support being native instead of \"extern C\".\n\nJsi was created as an embedded application replacement for Tcl and JimTcl.\nIts main goal is to provide embedded scripting using\na mostly standard syntax (JavaScript), as opposed to the\nidiosyncratic ones used in Tcl, Lua, and Python.\nA secondary goal is data compatibility with Web Browsers using JSON.\nSpeed is not a major goal.\n\nLife for Jsi started as a fork of the quad-wheel interpreter.\nEventually, most of the major internal features of Tcl were added.\nThe current implementation is nearly 10 times the size of the original,\nand very little of the original code remains.\n\nJsi comes with builtin support for SQLite, WebSocket.\nand a self-hosting ZIP file-systems.\nThe jsish executable is unusual in that it\nintegrates a self-mounting file system containing builtin utilities and applications.\nThese include a Debugger and a Web-Server, which is used to provide web user-interfaces for the Debugger and Sqlite.\nEach of these applications can be accessed via jsish command-line arguments.\n\nJsi is highly independent:\n\n\nScripts and extensions are unusually robust due to functions that support duck typed-parameters.\nThe implementation is mature and has a rich and resilient development environment that\nincludes integrated logging, asserts and strict mode.\n\nAs of the December 2017 2.4 release, Jsi is considered to be feature-stable.\n\nJsi implements version 5.1.\nof the ECMAScript standard,\nwith the following deviations:\n\n\nThere is one major extension: function definitions can use types and defaults.\n\n"}
{"id": "25121593", "url": "https://en.wikipedia.org/wiki?curid=25121593", "title": "KOA Corporation", "text": "KOA Corporation\n\nKOA Corporation (Japanese: コーア株式会社, \"Kōa kabushiki kaisha\"; ) is a Japanese electronic passive components supplier founded in 1940 in Tokyo and is based in Ina, Nagano, in the Nagano Prefecture.\n\nKOA is one of the largest manufacturers of SMD resistors worldwide. The range of products covers Low Temperature Co-Fired Ceramic (LTCC), Resistors, Temperature Sensors, Inductors, Fuses and Varistors.\n\nKOA employs 3,600 worldwide, with 1,100 employees in Japan. Twenty-six production facilities are located across Japan, China, Taiwan and Malaysia with distribution and sales locations in the US, Germany, China and Singapore.\n\n\n"}
{"id": "3166882", "url": "https://en.wikipedia.org/wiki?curid=3166882", "title": "Liquid-ring pump", "text": "Liquid-ring pump\n\nA liquid-ring pump is a rotating positive-displacement pump.\n\nThey are typically used as a vacuum pump, but can also be used as a gas compressor.\nThe function of a liquid-ring pump is similar to a rotary vane pump, with the difference being that the vanes are an integral part of the rotor and churn a rotating ring of liquid to form the compression-chamber seal. They are an inherently low-friction design, with the rotor being the only moving part. Sliding friction is limited to the shaft seals. Liquid-ring pumps are typically powered by an induction motor.\n\nThe liquid-ring pump compresses gas by rotating a vaned impeller located eccentrically within a cylindrical casing. Liquid (usually water) is fed into the pump and, by centrifugal acceleration, forms a moving cylindrical ring against the inside of the casing. This liquid ring creates a series of seals in the space between the impeller vanes, which form compression chambers. The eccentricity between the impeller's axis of rotation and the casing geometric axis results in a cyclic variation of the volume enclosed by the vanes and the ring.\n\nGas, often air, is drawn into the pump through an inlet port in the end of the casing. The gas is trapped in the compression chambers formed by the impeller vanes and the liquid ring. The reduction in volume caused by the impeller rotation compresses the gas, which reports to the discharge port in the end of the casing.\n\nCompressed gas on discharge of pump contains certain amount of working liquid which is usually removed in vapor–liquid separator.\n\nThe earliest liquid-ring pumps date from 1903, when a patent was granted in Germany to Siemens-Schuckert. US Patent 1,091,529, for liquid-ring vacuum pumps and compressors, was granted to Lewis H. Nash in 1914. They were manufactured by the Nash Engineering Company in Norwalk, CT. Around the same time, in Austria, Patent 69274 was granted to Siemens-Schuckertwerke for a similar liquid-ring vacuum pump.\n\nLiquid-ring systems can be single- or multistage. Typically a multistage pump will have up to two compression stages on a common shaft. In vacuum service, the attainable pressure reduction is limited by the vapour pressure of the ring-liquid. As the generated vacuum approaches the vapour pressure of the ring-liquid, the increasing volume of vapor released from the ring-liquid diminishes the remaining vacuum capacity. The efficiency of the system declines as a result.\n\nSingle-stage vacuum pumps typically produce vacuum to 35 Torr (mm Hg) or , and two-stage pumps can produce vacuum to 25 Torr, assuming air is being pumped and the ring-liquid is water at 15 °C (60 °F) or less. Dry air and 15 °C sealant-water temperature is the standard performance basis, which most manufacturers use for their performance curves.\n\nSome ring-liquid is also entrained with the discharge stream. This liquid is separated from the gas stream by other equipment external to the pump. In some systems, the discharged ring-liquid is cooled by a heat exchanger or cooling tower, then returned to the pump casing. In some recirculating systems, contaminants from the gas become trapped in the ring-liquid, depending on system configuration. These contaminants become concentrated as the liquid continues to recirculate, eventually causes damage and reduced life to the pump. In this case, filtration systems are required to ensure that contamination is kept to acceptable levels.\n\nIn non-recirculating systems, the discharged hot liquid (usually water) is treated as a waste stream. In this case fresh cool water is used to make up the loss. Environmental considerations are making such \"once-through\" systems increasingly rare.\nThese simple, but highly reliable pumps have a variety of industrial applications. They are used to maintain condenser vacuum on large steam-turbine generator sets by removing incondensable gasses, where vacuum levels are typically 30–50 mbar. They are used on paper machines to dewater the pulp slurry and to extract water from press felts. Another application is the vacuum forming of molded paper-pulp products (egg cartons and other packaging). Other applications include soil remediation, where contaminated ground water is drawn from wells by vacuum. In petroleum refining, vacuum distillation also makes use of liquid-ring vacuum pumps to provide the process vacuum. Liquid-ring compressors are often used in vapor recovery systems.\n\nLiquid-ring vacuum pumps can use any liquid compatible with the process, provided it has the appropriate vapor pressure properties, as the sealant liquid. Although the most common sealant is water, almost any liquid can be used. The second most common is oil. Since oil has a very low vapor pressure, oil-sealed liquid-ring vacuum pumps are typically air-cooled. For dry chlorine gas applications concentrated sulfuric acid is used.\n\nThe ability to use any liquid allows the liquid-ring vacuum pump to be ideally suited for solvent (vapor) recovery. If a process, such as distillation or a vacuum dryer, is generating toluene vapors, for example, then it is possible to use toluene as the sealant, provided the cooling water is cold enough to keep the vapor pressure of the sealant liquid low enough to pull the desired vacuum.\n\nIonic liquids in liquid-ring vacuum pumps can lower the vacuum pressure from about 70 mbar to below 1 mbar.\n"}
{"id": "58759507", "url": "https://en.wikipedia.org/wiki?curid=58759507", "title": "Long Range Reconnaissance Imager", "text": "Long Range Reconnaissance Imager\n\nLong Range Reconnaissance Imager (LORRI) is a telescope aboard the \"New Horizons\" spacecraft for imaging. LORRI has been used to image Jupiter and its moons and especially Pluto and its system of moons since its launched in 2006. LORRI is a reflecting telescope of Ritchey-Chrétien design, and it has a main mirror diameter of 20.8 cm (8.2 inches) across. Images are taken with a CCD capturing data with 1024 × 1024 pixels. LORRI is a telescopic panchromatic imager integrated with the \"New Horizons\" spacecraft, and it is one of seven major science instruments on the probe. LORRI does not have any moving parts and is pointed by moving the entire \"New Horizons\" spacecraft. LORRI has a narrow field of view, less than a third of one degree.\n\nLORRI was used to calculate albedos for Pluto and Charon. LORRI is also used for navigation, especially to more precisely determine the location of a flyby target. In 2018, \"New Horizons\" spacecraft utilized navigation data from LORRI for its planned flyby of Ultima Thule ((486958) 2014 MU69) flyby target in a couple months.\n\nDuring the cruise to Jupiter, LORRI data was also used to determine a value for the cosmic optical background as an alternative to other methods. At Jupiter, LORRI was used for an extensive observation campaign of Jupiter's atmosphere, rings, and moons.\n\nOn Aug. 29, 2006, the cover on LORRI was opened and it took an image in space of Messier 7 (aka Ptolemy’s Cluster) for its first light image. The following year, in 2007 when it flew by Jupiter for its gravity assist, it was used to image Jupiter and its moons. LORRI also imaged the Jovian system in 2010 as part of an Annual Checkout confirming the operation of LORRI, taking pictures from a distance of about 16 AU. \n\nIn 2015, LORRI was used to take extensive imaging of Pluto before and during the flyby.\n\nIn August 2018, LORRI was able to visually detect 2014 MU69 'Ultima Thule' at distance of about 100 million miles (about 1 AU or 161 million km).\n\nLORRi is a reflecting telescope integrated with the unmanned \"New Horizons\" spacecraft, it can take black and white digital pictures of astronomical targets.\n\nSpecifications:\n\nThe mirror is made of the material silicon carbide which helped support meeting the thermal requirements of the design. \n\nThe detector is a thinned backside-illuminated charge-coupled device, and records 1024 by 1024 pixels, with a variety of exposure settings. LORRI can take one picture per second and store the picture digitally as a 12-bit image, with either lossless or lossy compression. (See also Data compression)\n\nLORRI incorporates a field-flattening lens, with three elements.\n\nThe design can take images at very light low levels required for the mission, including light levels 1/900 those of Earth when it is at Pluto. For the 2014 MU69 encounter, the longest exposure time, which was up to ten seconds for the Pluto flyby was increased. This was accomplished after the Pluto flyby by the team, to support taking images in even lower light levels.\n\nAfter the Pluto times, exposure times of at least 30 seconds were made possible (half a minute), which was also useful for doing reconnaissance images and enabling imaging down to 21 magnitude. Magnitude in astronomy is way to measure brightness, and higher number is dimmer; two related concepts are Apparent magnitude and Absolute magnitude.\n\nLORRI is pointed by moving the entire spacecraft, which limits the exposure time. The spacecraft does not have reaction wheels and is stabilized by thrusters.\n\nWhile passing by Jupiter in February 2017, the Jovian system was observed using LORRI and other instruments.\n\nLORRI views of the Galilean moons:\n\nDue to its telescope power LORRI was able to capture images of Pluto and its moon, offering the closer and closer views as the spacecraft progressed to the dwarf planet.\n\nIn 2016 \"New Horizons\" used LORRI to observer the Kuiper belt object, 15810 Arawn ( at the time still known as 1994 JR1), depicted here as the light that is the one moving between star frames and in top left some internal reflections of LORRI's optics are visible.\n\nBecause LORRI had the highest magnification of the instruments, it captured the closet views of Pluto's terrain during the flyby. Its smaller field of view was panned across Pluto, capturing a stripe of the terrain.\n\n\n"}
{"id": "26019679", "url": "https://en.wikipedia.org/wiki?curid=26019679", "title": "MULTICUBE", "text": "MULTICUBE\n\n\"Multi-objective Design Space Exploration of MultiProcessor-SoC Architectures for Embedded Multimedia Applications\" (MULTICUBE) \nis a Seventh Framework Programme (FP7) project aimed to define innovative methods for the design optimization of computer architectures for the embedded system domain.\n\nEmbedded systems are specialized computing systems for wide domain of applications ranging from mobile phones and wearable electronics for military applications to control systems for automobile, factories and home automation.\nEven if all these domains are different, they are all characterized by their computational and programmability needs. All these applications need an underlying computing platform specially designed to cater to the application needs.\n\nThe improvements in Very Large Scale Integrations technology (VLSI) and the availability of the high computational power provided by System-on-a-Chip (SoC) has enabled development of highly sophisticated embedded applications. Today, the computer architectures are often designed in a multi-core paradigm, where more processors are integrated onto the same chip/die. This type of computer architecture can also be referred to as Chip-MultiProcessors (CMP), MultiProcessor-SoC (MPSoC) or, Network On Chip (NoC) where different processors communicate via a network infrastructure.\n\nDesigning complex systems on chip many platform parameters has to be tuned. This is done in order to maximize platform performances while minimizing non functional costs such as the power consumption. This tuning phase is called Design Space Exploration (DSE). This process can be formalized as a multiobjective optimization problem where non-commensurable objectives have to be maximized (or minimized).\n\nIn the context of MPSoC design, the problem is two fold:\n\n\nWith the aim to reduce the design time of future embedded systems, the MULTICUBE project faces the problems related to multiobjective DSE of MPSoC platforms.\nThe MULTICUBE project defines an automatic framework for DSE providing advanced methodologies for heuristic optimization and techniques for analyzing the effects of platform parameters in order to restrict the search space to the crucial ones enabling an efficient optimization.\n\nTo have a trade-off between exploration speed and solution accuracy, the MULTICUBE project proposes a multilevel modeling methodology. The underlying idea is that the expensive simulations with a detailed low-level system model are not always needed. Rather, for obtaining sufficient number of design points, approximate but faster evaluation methods are acceptable. Thus, the multilevel system modeling enables quick analysis of many design points using high level models. The final configuration is obtained by performing a more accurate low level simulations on the most promising candidates obtained from the high level approximation methods.\n\nAmong other activities, the MULTICUBE project develops open source tools for MPSoC modeling and optimization providing to the research and engineering communities the above mentioned methodologies.\n\n\n\n\n"}
{"id": "8144645", "url": "https://en.wikipedia.org/wiki?curid=8144645", "title": "Mead &amp; Conway revolution", "text": "Mead &amp; Conway revolution\n\nThe Mead & Conway revolution was a very-large-scale integration (VLSI) design revolution which resulted in a worldwide restructuring of academic education, and was paramount for the development of industries based on the application of microelectronics.\n\nImmediately after the invention and commercialization of the integrated circuit (with 100 or fewer transistors in a chip), the design was co-located with integrated circuit technology. The circuits' design capability centered in the hands of industry, with universities falling behind in their capability to design computers and systems. But, as predicted by Moore’s law, the number of transistors which fit in a chip doubled every two years, meaning that because of the high circuit complexity, the device physics experts were not qualified enough to cope. For this reason, Carver Mead, who coined the term \"Moore's law\", called for a separation of design from technology throughout the 1970s in order to establish electronic design automation (EDA) as its own discipline developing its own methodologies.\n\nIn 1978–79, when approximately 20,000 transistors could be fabricated in a single chip, Carver Mead and Lynn Conway wrote the textbook \"Introduction to VLSI Systems\" () published in 1979 which became a bestseller. It was the first VLSI design textbook for non-technologists. The authors intended the book to fill a gap in the literature and introduce electrical engineering and computer science students to integrated system architecture. This textbook triggered a breakthrough in education. Mead & Conway VLSI design courses were created in many universities. Computer science and electrical engineering professors throughout the world started teaching VLSI system design and used this textbook. Many of them also obtained a copy of Lynn Conway's notes for her famous MIT course in 1978, which included a collection of exercises.\n\nAn important milestone that followed was the Multi Project Chip (MPC) concept that allowed multiple designs to be fabricated on a single wafer, greatly reducing cost to the point that students' design exercises and prototypes could be fabbed in small numbers. The first successful run of an MPC line was demonstrated at Lynn Conway's 1978 VLSI design course at MIT. A few weeks after completion of their designs, the students had the fabricated prototypes in their hands, available for testing. Lynn Conway's improved new Xerox PARC MPC VLSI implementation system and service was operated successfully for a dozen universities by late 1979. It was then transferred to University of Southern California Information Sciences Institute, becoming the foundation for Metal Oxide Semiconductor Implementation Service (MOSIS), which has evolved since 1981 as a national infrastructure for fast-turnaround prototyping of VLSI chip designs by universities and researchers.\n\nIn 1980 Defense Advanced Research Projects Agency began the DoD's new VLSI research program to support extensions of this work, resulting in many university and industry researchers being involved in following up the Mead-Conway innovations. The Mead & Conway revolution rapidly spread around the world and many national Mead & Conway scenes were organized, like the German multi-university .\n\n"}
{"id": "10806484", "url": "https://en.wikipedia.org/wiki?curid=10806484", "title": "Medarex", "text": "Medarex\n\nMedarex (former NASDAQ symbol: MEDX ) was an American biopharmaceutical company based in Princeton, New Jersey, with manufacturing facilities in Bloomsbury and Annandale, New Jersey, and research facilities in Milpitas and Sunnyvale, California. In 2009, Medarex was purchased by Bristol Myers Squibb.\n\nMedarex developed monoclonal antibodies to CTLA-4 and PD-1, which are proteins on the surface of T cells. T cells attack cancer cells, but CTLA-4 and PD-1 act as \"brakes\" on the T cell's anti-cancer activities. The monoclonal antibodies bind to these proteins and block them, releasing the T cell to attack cancer cells.\n\nSeveral monoclonal antibodies developed by Medarex have been approved for disease therapy. In 2009, the U.S. Food and Drug Administration approved Simponi, a human monoclonal antibody to tumor necrosis factor alpha co-developed with Johnson & Johnson's Janssen Biotech, for treatment of arthritis. \nIn 2011, the U.S. FDA approved ipilimumab, a monoclonal antibody to CTLA-4, for treatment of metastatic melanoma.\nIn 2014, the U.S. FDA approved nivolumab, a monoclonal antibody to PD-1, for treatment of advanced melanoma. Its use was expanded to the treatment of squamous non-small-cell lung carcinoma in 2015.\n\nMedarex was founded in 1987 by a group of immunologists at Dartmouth Medical School—Dr. Michael W. Fanger, Dr. Paul M. Guyre, and Dr. Edward D. Ball — who partnered with Donald L. Drakeman and Charles Schaller of Essex Chemical Company, through its venture capital arm Essex Vencap. Drakeman, a Dartmouth graduate, brought the parties together and served as the company's chief executive officer. The company went public in 1991, with 2,300,000 shares of common stock at $6.10 per share and 2,250,000 Redeemable Warrants offered at its IPO.\nThe company's second president and CEO was Howard H. Pien, succeeding Drakeman in 2007.\nGenmab was founded as a European spin-off of American Biotech company Medarex in February 1999.\n\nThe company was acquired by Bristol Myers Squibb in 2009 for $2.4B, which included $300M in debt, making the payment to Medarex $2.1B.\n\n"}
{"id": "6157978", "url": "https://en.wikipedia.org/wiki?curid=6157978", "title": "Micro heat exchanger", "text": "Micro heat exchanger\n\nMicro heat exchangers, Micro-scale heat exchangers, or microstructured heat exchangers are heat exchangers in which (at least one) fluid flows in lateral confinements with typical dimensions below 1 mm. The most typical such confinement are microchannels, which are channels with a hydraulic diameter below 1 mm. Microchannel heat exchangers can be made from metal, ceramic,\n\nMicrochannel heat exchangers can be used for many applications including:\n\nInvestigation of microscale thermal devices is motivated by the single phase internal flow correlation for convective heat transfer:\nWhere formula_2 is the heat transfer coefficient, formula_3 is the Nusselt number, formula_4 is the thermal conductivity of the fluid and formula_5 is the hydraulic diameter of the channel or duct. In internal laminar flows, the Nusselt number becomes a constant. This is a result which can be arrived at analytically: For the case of a constant wall temperature, formula_6 and for the case of constant heat flux formula_7 for round tubes. The last value is increased to 140/17 = 8.23 for flat parallel plates. As Reynolds number is proportional to hydraulic diameter, fluid flow in channels of small hydraulic diameter will predominantly be laminar in character. This correlation therefore indicates that the heat transfer coefficient increases as channel diameter decreases. Should the hydraulic diameter in forced convection be on the order of tens or hundreds of micrometres, an extremely high heat transfer coefficient should result.\n\nThis hypothesis was initially investigated by Tuckerman and Pease. Their positive results led to further research ranging from classical investigations of single channel heat transfer to more applied investigations in parallel micro-channel and micro scale plate fin heat exchangers. Recent work in the field has focused on the potential of two-phase flows at the micro-scale.\n\nJust like \"conventional\" or \"macro scale\" heat exchangers, micro heat exchangers have one, two or even three fluidic flows. In the case of one fluidic flow, heat can be transferred to the fluid (each of the fluids can be a gas, a liquid, or a multiphase flow) from electrically powered heater cartridges, or removed from the fluid by electrically powered elements like Peltier chillers. In the case of two fluidic flows, micro heat exchangers are usually classified by the orientation of the fluidic flows to another as \"cross flow\" or \"counter flow\" devices. If a chemical reaction is conducted inside a micro heat exchanger, the latter is also called a microreactor.\n\n\n"}
{"id": "31998138", "url": "https://en.wikipedia.org/wiki?curid=31998138", "title": "National Center for Digitization", "text": "National Center for Digitization\n\nThe National Center for Digitization (NCD) is a consortium composed of the most important\nleading Serbian (and former Yugoslav) cultural and scientific institutions, which analyse problems of digitization of cultural and scientific heritage.\n\nThe work on the foundation of the National Center for Digitization (NCD) in Serbia started in 2002, with the idea to form a consortium consisting of leading cultural and research institutions involved in digitization of heritage. At the present state, the consortium includes: \n\n\nThe main subjects of cooperation in the NCD are the following:\n\n\nThe main activities of the NCD are:\n\n\n\n"}
{"id": "34845182", "url": "https://en.wikipedia.org/wiki?curid=34845182", "title": "New Year's glasses", "text": "New Year's glasses\n\nNew Year's glasses are novelty eyeglasses in the numerical shape of the coming year usually worn during New Year's Eve parties. They were invented and patented by Richard Sclafani and Peter Cicero in 1990, although other companies have produced similar versions. New Year's glasses' inspiration and popularity arose from the fact that the two digits in the middle of the year number (9 and 0 from the years 1990-2009) had holes suitable for looking through or mounting lenses into.\n\nNew Year's Glasses are often sold at New Year's Eve events and parties. Street vendors in NYC are known to sell the glasses with extremely high markups. In 2015, the average cost of glasses in Times Square was upwards of $25.\n\nNew Year's glasses can restrict vision due to the large frame. The large frame can also cause discomfort.\n"}
{"id": "466494", "url": "https://en.wikipedia.org/wiki?curid=466494", "title": "POKEY", "text": "POKEY\n\nThe Pot Keyboard Integrated Circuit (POKEY) is a digital I/O chip designed for the Atari 8-bit family of home computers and found in Atari arcade games of the 1980s. POKEY combines functions for sampling (ADC) potentiometers (such as game paddles) and scan matrices of switches (such as a computer keyboard) as well as sound generation. It produces four voices of distinctive square wave sound, either as clear tones or modified with a number of distortion settings. POKEY chips are used for audio in many arcade games including \"Centipede\", \"Missile Command\", \"Asteroids Deluxe\", and \"Gauntlet\". Some of Atari's arcade systems use multi-core versions with 2 or 4 POKEY chips in a single package for more sound voices. The Atari 7800 console allows a game cartridge to contain a POKEY, providing better sound than the system's audio chip. Only two licensed games make use of this: the ports of \"Ballblazer\" and \"Commando\", with some later homebrews like \"Bentley Bear and the Crystal Quest\" also making use of it.\n\nThe LSI chip has 40 pins and is identified as C012294. POKEY was designed by Atari employee Doug Neubauer, who also programmed the original \"Star Raiders\". The USPTO granted U.S. Patent 4,314,236 to Atari on February 2, 1982 for an \"Apparatus for producing a plurality of audio sound effects\". This referred to POKEY's sound generation abilities. The inventors listed are Steven T. Mayer and Ronald E. Milner.\n\nNo longer manufactured, POKEY is now emulated in software by classic arcade and Atari 8-bit emulators and also via the Atari SAP music format and associated player.\n\n\nBy part number:\n\nThe Atari 8-bit computers map POKEY to the $D2xx page and the Atari 5200 console maps it to the $E8xx page.\n\nPOKEY provides 29 Read/Write registers controlling Sound, Paddle input, keyboard input, serial input/output, and interrupts. Many POKEY register addresses have dual purposes performing different functions as a Read vs a Write register. Therefore, no code should read Hardware registers expecting to retrieve the previously written value.\n\nThis problem is solved for some registers by Operating System \"Shadow\" registers implemented in regular RAM that mirror the values of hardware registers. During the Vertical Blank the Operating System copies the Shadow registers in RAM for Write registers to the corresponding hardware register, and updates Shadow values for Read registers from the hardware accordingly. Therefore, writes to hardware registers which have corresponding shadow registers will be overwritten by the value of the Shadow registers during the next vertical blank.\n\nReading values directly from hardware at an unknown stage in the display cycle may return inconsistent results (an example: reading potentiometers). Operating System Shadow registers for Read registers would usually be the preferred source of information.\n\nSome Write hardware registers do not have corresponding Shadow registers. They can be safely written by an application without the value being overwritten during the vertical blank. If the application needs to know the last value written to the register then it is the responsibility of the application to implement its own shadow value to remember what it wrote.\n\nIn the individual register listings below the following legend applies:\nPokey contains four audio channels with separate frequency, noise and voice level controls.\n\nEach channel has an 8-bit frequency divider and an 8-bit register to select noise and volume.\n\nPOKEY's sound is distinctive: when the four channels are used independently, there is noticeable detuning of parts of the 12-tone equal temperament scale, due to lack of pitch accuracy. Channels may be paired for higher accuracy; in addition, multiple forms of distortion are available, allowing a thicker sound. The distortion is primarily used in music for bass parts.\n\nOne of the sound-engines developed for the Atari 8-bit family was called the AMP engine (Advanced Music Processor). This was used by the musician Gary Gilbertson.\n\nThe AUDF* registers control the frequency or pitch of the corresponding sound channels. The AUDF* values also control the POKEY hardware timers useful for code that must run in precise intervals more frequent than the vertical blank.\n\nEach AUDF* register is an 8-bit value providing a countdown timer or divisor for the pulses from the POKEY clock. So, smaller values permit more frequent output of pulses from POKEY, and larger values, less frequent. The values $0/0 to $FF/255 are incremented by POKEY to range from $1/1 to $100/256. The actual audible sound pitch is dependent on the POKEY clock frequency and distortion values chosen. See Audio Channel Control and Audio Control.\n\nAudio Channel 1 Frequency\n\nAudio Channel 2 Frequency\n\nAudio Channel 3 Frequency\n\nAudio Channel 4 Frequency\n\nThe Audio Channel control registers provide volume and distortion control over individual sound channels. Audio may also be generated independently of the POKEY clock by direct volume manipulation of a sound channel which is useful for playing back digital samples.\n\nAudio Channel 1 Control\n\nAudio Channel 2 Control\n\nAudio Channel 3 Control\n\nAudio Channel 4 Control\n\nBit 0-3: Control over volume level, from 0 to F.\n\nBit 4: Forced volume-only output. When this bit is set the channel ignores the AUDF timer, noise/distortion controls, and high-pass filter. Sound is produced only by setting volume bits 0:3 . This feature was used to create digital audio via pulse-code modulation.\n\nBit 5-7: Shift register settings for noises/distortion. Bit values described below:\n\nGenerating random noises is served by reading 8 bits from top of 17-bit shift register. That registers are driven by frequency 1.79 MHz for NTSC or 1.77 MHz for PAL. Its outputs can by used independently by each audio channels' divider rate.\n\nAudio Control allows the choice of clock input used for the audio channels, control over the high-pass filter feature, merging two channels together allowing 16-bit frequency accuracy, selecting a high frequency clock for specific channels, and control over the \"randomness\" of the polynomial input.\n\n\"1\" means \"on\", if not described:\n\n\nAll frequency dividers (AUDF) can be driven at the same time by 64 kHz or 15 kHz rate.\n\nFrequency dividers 1 and 3 can be alternately driven by CPU clock (1.79 MHz NTSC, 1.77 MHz PAL).\nFrequency dividers 2 and 4 can be alternately driven by output of dividers 1 and 3.\nIn this way, POKEY makes possible connecting of 8-bit channels to create sound with 16-bit accuracy.\n\nPossible channel configurations:\n\nPOKEY has eight analog to digital converter ports most commonly used for potentiometers, also known as Paddle Controllers. The analog inputs are also used for the Touch Tablet controller, and the 12-button,video game Keyboard Controllers. Each input has a drop transistor, which can be set on or off from software. The timers can also be used to support a light pen, by connecting a photodiode to the drop transistor, which captures the timer when the electron beam in the television passes by the pen. The vertical position of the pen had to be read separately.\n\nSHADOW: PADDL0 $0270\n\nPaddle Controller 0 Input\n\nSHADOW: PADDL1 $0271\n\nPaddle Controller 1 Input\n\nSHADOW: PADDL2 $0272\n\nPaddle Controller 2 Input\n\nSHADOW: PADDL3 $0273\n\nPaddle Controller 3 Input\n\nSHADOW: PADDL4 $02704\n\nPaddle Controller 4 Input\n\nSHADOW: PADDL5 $0275\n\nPaddle Controller 5 Input\n\nSHADOW: PADDL6 $0276\n\nPaddle Controller 6 Input\n\nSHADOW: PADDL7 $0277\n\nPaddle Controller 7 Input\n\nEach input has 8-bit timer, counting time when each TV line is being displayed. This had the added advantage of allowing the value read out to be fed directly into screen coordinates of objects being driven by the paddles. The Atari Paddle values range from 0 to 228, though the maximum possible is 244. The Paddle controller reads 0 when turned to its maximum clockwise position, and returns increasing values as it is turned counter-clockwise ending at its maximum value.\n\nThe Paddle reading process begins by writing to POTGO which resets the POT* values to 0, the ALLPOT value to $FF, and discharges the potentiometer read capacitors. The POT* values increment as they are being scanned until reaching the resistance value of the potentiometer. When the Paddle reading is complete the corresponding bit in ALLPOT is reset to 0.\n\nThe Paddle scanning process can take the majority of a video frame to complete. The Atari Operating System takes care of Paddle reading automatically. The Paddles are read and paddle scanning initiated during the stage 2 vertical blank. Paddle values are copied to shadow registers. (Note that Paddle triggers are actually joystick direction input read from PIA.)\n\nA faster mode of scanning the Paddles is possible by setting a bit in SKCTL. The reading sequence completes in only a couple scan lines, but the value is less accurate.\n\nPotentiometer Scanning Status\n\nEach bit corresponds to one potentiometer input (the POT* registers). When paddle scanning is started by writing to POTGO each paddle's bit in ALLPOT is set to 1. When a paddle's scan is complete the corresponding bit in ALLPOT is reset to 0 indicating the value in the associated POT* register is now valid to read.\n\nStart Potentiometer Scan\n\nWriting to POTGO initiates the potentiometer (Paddle) scanning process. This resets the POT* values to 0, the ALLPOT value to $FF, and discharges the potentiometer read capacitors. As each potentiometer scan completes the bit corresponding to the potentiometer in ALLPOT is cleared indicating the value of the associated POT* register is valid for reading.\n\nContains:\n\nPOKEY is a sort of UART. Usually one of the doubled audio channels is used as baud rate generator. The standard baud rate is 19.2 kbit/s, the maximum possible baud rate is 127 kbit/s. A byte put into the SEROUT register is automatically sent over the serial bus. The data frame contains 10 bits: 1 start bit, 8 data bits, 1 stop bit. The voltage levels are 0 V (logical 0) and +4 V (logical 1). It is possible to connect the Atari serial port with an RS-232 port by means of a simple voltage converter. \n\nEach input/output operation causes POKEY's internal shift registers to change value, so when programming for POKEY, it is necessary to re-initialise some values after each operation is carried out.\n\nReset Serial Port Status (SKSTAT).\n\nA write to this register will reset bits 5 through 7 of SKSTAT which are latches to 1. The latches flag keyboard overrun, Serial data input overrun, and Serial data input frame error.\nSerial port data output byte.\n\nThis is a parallel \"holding\" register for the eight bit (one byte) value that will be transferred to the serial shift register for output one bit at a time. When the port is ready to accept data for output a Serial Data Out interrupt informs the Operating System that it can write a byte to this output register.\n\nSerial port data input byte.\n\nLike SEROUT, also a parallel \"holding\" register. This holds the eight bit (one byte) value assembled by the serial shift register reading the data input one bit at a time. When a full byte is read a Serial Data In interrupt occurs informing the Operating System that it can read the byte from this register. \n\nSerial Port Control\n\nBit 0: Enable \"debounce\" scanning which is intended to eliminate noise or jitter from mechanical switches. A value of 1 enables POKEY to use an internal comparison register while scanning keys. A key must be detected in two simultaneous scans before it is identified as pressed, and it must be seen released for two consecutive scans to be considered released. This should be enabled to maintain normal keyboard handling with the Operating System.\n\nBit 1: Set to 1 to enable keyboard scanning. This should be enabled to maintain normal keyboard handling with the Operating System.\n\nBit 2: Set to 1 to enable fast, though less accurate Potentiometer scanning. Fast Pot scanning increments the counter on every cycle and returns a usable result within two scan lines. The Operating System uses the slow Pot Scanning which increments the counter once every 114 cycles (scan line) taking a frame (1/60th second) to produce a result. The OS reads the Pot values during the its Vertical Blank Interrupt (VBI) and copies the result to the potentiometer Shadow registers in RAM. It then resets POTGO for the next read during the next VBI. \n\nBit 3: Enable Serial port two-tone mode. When enabled, 1 and 0 bits output to the SIO bus are replaced by tones set by timers 1 and 2. This is ordinarily used for writing analog tones representing digital data to cassette tape.\n\nBit 4-6: Clock Timing Control for serial port operation. Bit values described below:\n\nBit 7: Forces a known 0 output, so that timer 2 can reset timer 1 in two-tone serial output mode.\n\nSerial Port Status\n\nSHADOW: CH $02FC\n\nKeyboard Code\nInterrupts can be set on or off from software by register IRQEN.\nIRQSTAT register contains interrupts status.\n\nSix key register of actually pushed keys (K0 K5), which contains values from 00 to 3F. Contains 2 control values. One of them acts as decoder of all 6 values. Second control values is used to decode special key values — CTRL, SHIFT and BREAK.\n\n"}
{"id": "21514226", "url": "https://en.wikipedia.org/wiki?curid=21514226", "title": "Passive daylighting", "text": "Passive daylighting\n\nPassive daylighting is a system of both collecting sunlight using static, non-moving, and non-tracking systems (such as windows, sliding glass doors, most skylights, light tubes) and reflecting the collected daylight deeper inside with elements such as light shelves. Passive daylighting systems are different from active daylighting systems in that active systems track and/or follow the sun, and rely on mechanical mechanisms to do so.\n\nCollecting devices rely on their position to most effectively capture sunlight. A building's position as well as architectural considerations are critical in the effectiveness of passive daylighting. Passive daylight systems are typically non-mechanical, and optimal daylighting efficiency is achieved by proper building and system orientation. A southern facing orientation is optimal if a building or system is located in the northern hemisphere, and a northern facing orientation is optimal if located in the southern hemisphere.\n\nReflecting elements such as light shelves, lighter wall colors, mirrored wall sections, interior walls with upper glass panels, and clear or translucent glassed hinged doors and Sliding glass doors take the captured light and passively reflect it further inside. The light can be from passive vertical windows or overhead skylights-tubes or active daylighting sources. In traditional Japanese architecture the Shōji sliding panel doors, with translucent washi screens, are an original precedent. International style, Modernist and Mid-century modern architecture were earlier innovators of this passive penetration and reflection in industrial, commercial, and residential applications. \n\nThe use of all these passive daylighting methods reduces energy consumption from artificial lighting use, creating a more sustainable architecture. They are some of the components in designing for LEED - Leadership in Energy and Environmental Design certification. Specifically, designing for daylighting applies to the IEQ credit \"Daylighting and Views\", and can contribute to the EA credit \"Optimize Energy Performance\".\n\n"}
{"id": "1645192", "url": "https://en.wikipedia.org/wiki?curid=1645192", "title": "Pigging", "text": "Pigging\n\nPigging in the context of pipelines refers to the practice of using devices known as \"pigs\" to perform various maintenance operations. This is done without stopping the flow of the product in the pipeline.\n\nThese operations include but are not limited to cleaning and inspecting the pipeline. This is accomplished by inserting the pig into a \"pig launcher\" (or \"launching station\") — an oversized section in the pipeline, reducing to the normal diameter. The launching station is then closed and the pressure-driven flow of the product in the pipeline is used to push the pig along down the pipe until it reaches the receiving trap — the \"pig catcher\" (or \"receiving station\").\n\nPigging has been used for many years to clean large diameter pipelines in the oil industry. Today, however, the use of smaller diameter pigging systems is now increasing in many continuous and batch process plants as plant operators search for increased efficiencies and reduced costs.\n\nPigging can be used for almost any section of the transfer process between, for example, blending, storage or filling systems. Pigging systems are already installed in industries handling products as diverse as lubricating oils, paints, chemicals, toiletries, cosmetics and foodstuffs.\n\nPigs are used in lube oil or paint blending to clean the pipes to avoid cross-contamination, and to empty the pipes into the product tanks (or sometimes to send a component back to its tank). Usually pigging is done at the beginning and at the end of each batch, but sometimes it is done in the midst of a batch, such as when producing a premix that will be used as an intermediate component.\n\nPigs are also used in oil and gas pipelines to clean the pipes. There are also \"smart pigs\" used to inspect pipelines for the purpose of preventing leaks, which can be explosive and dangerous to the environment. They usually do not interrupt production, though some product can be lost when the pig is extracted. They can also be used to separate different products in a multiproduct pipeline.\n\nIf the pipeline contains butterfly valves, or reduced port ball valves, the pipeline cannot be pigged. Full port (or full bore) ball valves cause no problems because the inside diameter of the ball opening is the same as that of the pipe.\n\nSome early cleaning \"pigs\" were made from straw bales wrapped in barbed wire while others used leather. Both made a squealing noise while traveling through the pipe, sounding to some like a pig squealing, which gave pigs their name. \"PIG\" is sometimes claimed as an acronym or backronym derived from the initial letters of the term \"Pipeline Inspection Gauge\" or \"Pipeline Intervention Gadget\".\n\nA major advantage for multi-product pipelines of piggable systems is the potential of product savings. At the end of each product transfer, it is possible to clear out the entire line contents with the pig, either forwards to the receipt point, or backwards to the source tank. There is no requirement for extensive line flushing.\n\nWithout the need for line flushing, pigging offers the additional advantage of much more rapid and reliable product changeover. Product sampling at the receipt point is faster with pigs, because the interface between products is very clear; the old method of checking at intervals to determine where the product is on-specification takes considerably longer.\n\nPigging can also be operated totally by a programmable logic controller (PLC).\n\nPigging has a significant role to play in reducing the environmental impact of batch operations. Traditionally, the only way that an operator of a batch process could ensure a product was completely cleared from a line was to flush the line with a cleaning agent such as water or a solvent, or even with the next product. The cleaning agent then had to be subjected to effluent treatment or solvent recovery. If a product was used to clear the line, it was necessary to downgrade or dump the contaminated portion of the product. All of these problems can now be eliminated due to the very precise interface produced by modern pigging systems.\n\nPigging systems are designed so that the pig is loaded into the launcher, which is pressured to launch the pig into the pipeline through a kicker line. In some cases, the pig is removed from the pipeline via the receiver at the end of each run. All systems must allow for the receipt of pigs at the launcher, as blockages in the pipeline may require the pigs to be pushed back to the launcher. Many systems are designed to pig the pipeline in either direction.\n\nThe pig is pushed either with a gas or a liquid; if pushed by gas, some systems can be adapted in the gas inlet in order to ensure pig's constant speed, whatever the flow pressure is. The pigs must be removed, as many pigs are rented, pigs wear and must be replaced, and cleaning (and other) pigs push contaminants from the pipeline such as wax, foreign objects, hydrates, etc., which must be removed from the pipeline. There are inherent risks in opening the barrel to atmospheric pressure so care must be taken to ensure that the barrel is depressurized prior to opening. If the barrel is not completely depressurized, the pig can be ejected from the barrel and operators have been severely injured when standing in front of an open pig door. A pig was once accidentally shot out of the end of a pipeline without a proper pig receiver and went through the side of a house 500 feet away. When the product is sour, the barrel should be evacuated to a flare system where the sour gas is burnt. Operators should wear a self-contained breathing apparatus when working on sour systems.\n\nA few pigging systems utilize a \"captive pig\", and the pipeline is only opened occasionally to check the condition of the pig. At all other times, the pig is shuttled up and down the pipeline at the end of each transfer, and the pipeline is never opened up during process operation. These systems are not common.\n\nThere are many reports of incidents in which operators have been injured or even killed while performing a pigging operation. Common causes of such events are:\n\nAll these causes are directly related to improper operation of the process valves and the closure door. A common method of avoiding these kinds of incidents is to add valve interlocks, which has been adopted by all global oil and gas operating companies.\n\nSafety during pigging relies on strict adherence to safety procedures, which give detailed description of a safe valve operating sequence. By physically blocking incorrect operations, valve interlocks enforce such sequences. Valve interlocks are permanently mounted to both manual and motor operated valves and the closure door. The interlocks block operation of a valve or door, unless the appropriate keys are inserted.\n\nThe principle of valve interlocking is the transfer of keys. Each lock is equipped with two keys: a key for the locked open position and one for the locked closed position. During an operating procedure, only one key at a time is free. This key only fits in the interlock belonging to the valve that is to be operated next in the operating procedure. All keys are uniquely coded to avoid the possibility that valves can be unlocked at an inappropriate time.\n\nNowadays intelligent interlocking solutions enable the integration of field devices like pressure or HS meters in a valve interlocking sequence. This increases safety by integrating operator procedures with DCS and SIS safety systems.\n\nA \"pig\" in the pipeline industry is a tool that is sent down a pipeline and propelled by the pressure of the product flow in the pipeline itself. There are four main uses for pigs:\n\nOne of the most common and versatile is the foam pig which is cut or poured out of open cell polyurethane foam into the shape of a bullet and is driven through pipelines for many reasons such as to prove the inner diameter of, clean, de-water, or dry out the line. There are several types of pigs for cleaning in various densities from 2 lb to 10 lb foam and in special applications up to 20 lb. Some have tungsten studs or abrasive wire mesh on the outside to cut rust, scale, or paraffin wax deposits off the inside of the pipe. Other types are fully or criss-cross coated in urethane, or there are bare polyurethane foam pigs with a urethane coating just on the rear to seal and assist in driving the pig. There are also fully molded urethane pigs used for liquid removal or batching several different products in one line.\n\nInline inspection pigs use various methods for inspecting a pipeline. A sizing pig uses one (or more) notched round metal plates as gauges. The notches allow different parts of the plate to bend when a bore restriction is encountered. More complex systems exist for inspecting various aspects of the pipeline. Intelligent pigs are used to inspect the pipeline with sensors and record the data for later analysis. These pigs use technologies such as magnetic flux leakage (MFL) and ultrasound to inspect the pipeline. Intelligent pigs may also use calipers to measure the inside geometry of the pipeline.\n\nIn 1961, the first intelligent pig was run by Shell Development. It demonstrated that a self-contained electronic instrument could traverse a pipeline while measuring and recording wall thickness. The instrument used electromagnetic fields to sense wall integrity. In 1964 Tuboscope ran the first commercial instrument. It used MFL technology to inspect the bottom portion of the pipeline. The system used a black box similar to those used on aircraft to record the information; it was basically a highly customized analog tape recorder. Until recently, tape recording (although digital) was still the preferred recording medium. As the capacity and reliability of solid-state memory improved, most recording media moved away from tape to solid-state.\n\nCapacitive sensor probes are used to detect defects in polyethylene pipe gas pipeline. These probes are attached to the pig before it is sent through the polyethylene pipe to detect any defects in the outside of the pipe wall. This is done by using a triple plate capacitive sensor in which electrostatic waves are propagated outward through the pipe's wall. Any change in dielectric material results in a change in capacitance. Testing was conducted by NETL DOE research lab at the Battelle West Jefferson’s Pipeline Simulation Facility\n(PSF) near Columbus, Ohio.\n\nModern intelligent or \"smart\" pigs are highly sophisticated instruments that include electronics and sensors that collect various forms of data during their trip through the pipeline. They vary in technology and complexity depending on the intended use and the manufacturer.\n\nThe electronics are sealed to prevent leakage of the pipeline product into the electronics since products can range from being highly basic to highly acidic and can be of extremely high pressure and temperature. Many pigs use specific materials according to the product in the pipeline. Power for the electronics is typically provided by onboard batteries which are also sealed. Data recording may be by various means ranging from analog tape, digital tape, or solid-state memory in more modern units.\n\nThe technology used varies by the service required and the design of the pig, each pigging service provider may have unique and proprietary technologies to accomplish the service. Surface pitting and corrosion, as well as cracks and weld defects in steel/ferrous pipelines are often detected using magnetic flux leakage (MFL) pigs. Other \"smart\" pigs use electromagnetic acoustic transducers to detect pipe defects. Caliper pigs can measure the roundness of the pipeline to determine areas of crushing or other deformations. Some smart pigs use a combination of technologies, such as providing MFL and caliper functions in a single tool. Trials of pigs using acoustic resonance technology have been reported.\n\nDuring the pigging run the pig is unable to directly communicate with the outside world due to the distance underground or underwater and/or the materials that the pipe is made of. For example, steel pipelines effectively prevent any significant radio communications outside the pipe. It is therefore necessary that the pig use internal means to record its own movement during the trip. This may be done by odometers, gyroscope-assisted tilt sensors and other technologies. The pig records this positional data so that the distance it moves along with any bends can be interpreted later to determine the exact path taken.\n\nLocation verification is often accomplished by surface instruments that record the pig's passage by either audible, magnetic, radio-transmission or other means. The sensors record when they detect the passage of the pig (time-of-arrival); this is then compared to the internal record for verification or adjustment. The external sensors may have Global Positioning System capability to assist in their location. A few pig passage indicators transmit the pig's passage, time and location, via satellite uplink. The pig itself cannot use GPS as the metal pipe blocks satellite signals.\n\nAfter the pigging run has been completed, the positional data from the external sensors is combined with the pipeline evaluation data (corrosion, cracks, etc.) from the pig to provide a location-specific defect map and characterization. In other words, the combined data reveals to the operator the location, type and size of each pipe defect. This information is used to judge the severity of the defect and help repair crews locate and repair the defect quickly without having to dig up excessive amounts of pipeline. By evaluating the rate of change of a particular defect over several years, proactive plans can be made to repair the pipeline before any leakage or environmental damage occurs.\n\nThe inspection results are typically archived (perhaps in Pipeline Open Data Standard format) for comparison with the results of later pigging runs on the same pipeline.\n\nA pig has been used as a plot device in three James Bond films: \"Diamonds Are Forever\", where Bond disabled a pig to escape from a pipeline; \"The Living Daylights\", where a pig was modified to secretly transport a person through the Iron Curtain; and \"The World Is Not Enough\", where a pig was used to move a nuclear weapon through a pipeline.\n\nA pig was also used as a plot device in the Tony Hillerman book \"The Sinister Pig\" where an abandoned pipeline from Mexico to the United States was used with a pig to transport illegal drugs.\n\nA pig launcher was featured in the season 2 episode \"Pipeline Fever\" of the animated show \"Archer (2009 TV series)\", wherein Sterling Archer and Lana Kane are tasked with going into a swamp and defending a pig launcher from radical environmentalist Joshua Gray.\n\n\n"}
{"id": "922638", "url": "https://en.wikipedia.org/wiki?curid=922638", "title": "Pioneer Corporation", "text": "Pioneer Corporation\n\nPioneer played a role in the development of interactive cable TV, the Laser Disc player, the first automotive Compact Disc player, the first detachable face car stereo, Supertuner technology, DVD and DVD recording, plasma display (branded as Kuro), and Organic LED display (OLED). The company works with optical disc and display technology and software products and is also a manufacturer. Sharp Corporation took a 14% stake in Pioneer in 2007, which has been reduced to 9%, but Sharp still remains the largest shareholder of Pioneer Corporation, followed by Honda Motor Co., Ltd. who owns roughly 4% of Pioneer shares following a memorandum between the two companies in 2010 to strengthen business ties.\n\nIn March 2010, Pioneer stopped producing televisions as announced on 12 February 2009. On June 25, 2009, Sharp Corporation agreed to form a joint venture on their optical business to be called Pioneer Digital Design and Manufacturing Corporation. In September 2014, Pioneer agreed to sell Pioneer Home Electronics (Home A/V) to Onkyo, and in March 2015, Pioneer sold its DJ equipment business division to KKR, which resulted in the establishment of Pioneer DJ as a separate entity, independent of Pioneer.\n\n\n\nPioneer Karaoke Channel () is a satellite television channel that features Asian music videos and karaoke 24 hours a day. Pioneer and Malaysian satellite broadcaster Astro officially launched on January 1996.\n\n\n\n\n"}
{"id": "353796", "url": "https://en.wikipedia.org/wiki?curid=353796", "title": "Sebakh", "text": "Sebakh\n\nSebakh (, less commonly transliterated as \"sebbakh\") is an Arabic word that translates to \"fertilizer\". This term is used to describe decomposed organic material that can be employed both as an agricultural fertilizer and as a fuel for fires.\n\nMost sebakh consists of ancient, deteriorated mud brick. Mud brick was a primary building material in ancient Egypt. This material is composed of ancient mud mixed with the nitrous \ncompost of the hay and stubble that the bricks were originally formulated with to give added strength before being baked in the sun.\n\nA common practice in Egypt, in the late nineteenth and early twentieth century, was for farmers to obtain government permits to remove this material from ancient mounds; such farmers were known as 'sebakhin'. Mounds indicating the location of ancient cities are also known as a \"tell\", or \"tel\".\n\nAn archaeological site could provide an excellent source of sebakh because decomposed organic debris creates a soil very rich in nitrogen. Nitrogen is an essential component in fertilizers used for plant crops.\n\nNumerous potentially valuable archaeological finds were unfortunately destroyed by farmers in this way. However, sebakh digging also led to the discovery of archaeological finds that might otherwise have gone undetected.\n\nSebakh is most commonly associated with the finding of the site of Amarna (Arabic: العمارنة al-‘amārnä). In 1887, a local inhabitant who was delving into sebakh deposits accidentally discovered more than 300 cuneiform tablets that turned out to be Pharaonic records of correspondence. These tablet letters, known as the Amarna Letters, have provided much valuable historical and chronological data, as well as information bearing on Egyptian diplomatic relations with her neighbors at that time.\n\n\n"}
{"id": "11865929", "url": "https://en.wikipedia.org/wiki?curid=11865929", "title": "Tantangara Dam", "text": "Tantangara Dam\n\nTantangara Dam is a major ungated concrete gravity dam with concrete chute spillway across the Murrumbidgee River upstream of Adaminaby in the Snowy Mountains region of New South Wales, Australia. The dam is part of the Snowy Mountains Scheme, a vast hydroelectricity and irrigation complex constructed in south-east Australia between 1949 and 1974 and now run by Snowy Hydro. The purpose of the dam includes water management and conservation, with much of the impounded headwaters diverted to Lake Eucumbene. The impounded reservoir is called Tantangara Reservoir.\n\nCommenced in 1958 and completed in 1960, the Tantangara Dam is located on the Murrumbidgee River, approximately downstream of its confluence with Gurrangorambla Creek and is wholly within the Kosciuszko National Park. Her Royal Highness Princess Alexandra of Kent visited the dam in 1959, during its construction.\n\nThe dam was constructed by Utah-Brown & Root Sudamericana on behalf of the Snowy Mountains Hydro-Electric Authority, and is now managed by Snowy Hydro Limited. The concrete gravity dam of is high with a crest length of . At 100% capacity, the dam wall holds back of water. The surface area of Tantangara Reservoir is and the catchment area is . The spillway across the Murrumbidgee River is capable of discharging .\n\nWater flows 16.6 km from Tantangara Reservoir to Lake Eucumbene via the 3.1 to 3.35m diameter Murrumbidgee-Eucumbene tunnel falling 40 m in the process. Flow is controlled by a 1.83 x 2.13 m regulating gate such that a maximum of 20 cumecs is allowed. Flow downstream into the Murrumbidgee River is controlled at the dam and comprises two by high shafts, from the outlet works tower and tapering to diameter before passing through a diameter nozzle to river diversion tunnel, with a capacity of\n\nThe Snowy Water Initiative (SWI) is an agreement for water recovery and environmental flows between the NSW, Victorian and Australian Governments, and Snowy Hydro Limited (SHL) which is set out in the Snowy Water Inquiry Outcomes Implementation Deed 2002 (SWIOID 2002). The SWI provides three main environmental water programs as part of rebalancing the impacts of the Snowy Hydro Scheme on montane rivers. These three programs are increased flows for: (i) Snowy River, (ii) River Murray, and (iii) Snowy Montane Rivers.\n\nThe Snowy Montane River Increased Flows (SMRIF) program identifies five montane rivers to receive environmental water. The water availability for SMRIF is linked to the water availability for Snowy River Increased Flows (SRIF) (Williams 2017), which is determined by the water recovery in the western rivers and the preceding climatic conditions. The SWIOID 2002 provides for SHL to forego up to 150 Giga Watt hours (GWHrs) of electricity generation to allow for environmental releases to be made to SMRIF. This value of 150 GWHrs is converted into a volumetric allocation, but the conversion factor differs depending on the location of the releases in the Snowy Mountains Scheme, and thus influence the overall volume released. In some locations water released can be re-used to generate electricity so a smaller conversion factor is applied (SWIOID 2002), however, where water is lost to the Snowy Scheme a higher conversion factor is applied.<ref name=\"Williams 2017\">\n"}
{"id": "31247", "url": "https://en.wikipedia.org/wiki?curid=31247", "title": "Teleprinter", "text": "Teleprinter\n\nA teleprinter (teletypewriter, Teletype or TTY) is an electromechanical typewriter that can be used to send and receive typed messages through various communications channels, in both point-to-point and point-to-multipoint configurations. The machines were adapted to provide a user interface to early mainframe computers and minicomputers, sending typed data to the computer and printing the response. Some models could also be used to create punched tape for data storage (either from typed input or from data received from a remote source) and to read back such tape for local printing or transmission.\n\nTeleprinters could use a variety of different communication media. These included a simple pair of wires; dedicated non-switched telephone circuits (leased lines); switched networks that operated similarly to the public telephone network (telex); and radio and microwave links (telex-on-radio, or TOR). A teleprinter attached to a modem could also communicate through standard switched public telephone lines. This latter configuration was often used to connect teleprinters to remote computers, particularly in time-sharing environments.\n\nTeleprinters have largely been replaced by fully electronic computer terminals which typically have a computer monitor instead of a printer (though the term \"TTY\" is still occasionally used to refer to them, such as in Unix systems). Teleprinters are still widely used in the aviation industry (see AFTN and airline teletype system), and variations called Telecommunications Devices for the Deaf (TDDs) are used by the hearing impaired for typed communications over ordinary telephone lines.\n\nThe teleprinter evolved through a series of inventions by a number of engineers, including Samuel Morse, Alexander Bain, Royal Earl House, David Edward Hughes, Emile Baudot, Donald Murray, Charles L. Krum, Edward Kleinschmidt and Frederick G. Creed. Teleprinters were invented in order to send and receive messages without the need for operators trained in the use of Morse code. A system of two teleprinters, with one operator trained to use a typewriter, replaced two trained Morse code operators. The teleprinter system improved message speed and delivery time, making it possible for messages to be flashed across a country with little manual intervention.\n\nIn 1835 Samuel Morse devised a recording telegraph, and Morse code was born.\n\nIn 1841 Alexander Bain devised a printing telegraph.\n\nBy 1846, the Morse telegraph service was operational between Washington, D.C., and New York. Royal Earl House patented his printing telegraph that same year. He linked two 28-key piano-style keyboards by wire. Each piano key represented a letter of the alphabet and when pressed caused the corresponding letter to print at the receiving end. A \"shift\" key gave each main key two optional values. A 56-character typewheel at the sending end was synchronised to coincide with a similar wheel at the receiving end. If the key corresponding to a particular character was pressed at the home station, it actuated the typewheel at the distant station just as the same character moved into the printing position, in a way similar to the daisy wheel printer. It was thus an example of a synchronous data transmission system. House's equipment could transmit around 40 instantly readable words per minute, but was difficult to manufacture in bulk. The printer could copy and print out up to 2,000 words per hour. This invention was first put in operation and exhibited at the Mechanics Institute in New York in 1844.\n\nLandline teleprinter operations began in 1849, when a circuit was put in service between Philadelphia and New York City.\n\nIn 1855, David Edward Hughes introduced an improved machine built on the work of Royal Earl House. In less than two years, a number of small telegraph companies, including Western Union in early stages of development, united to form one large corporation – Western Union Telegraph Co. – to carry on the business of telegraphy on the Hughes system.\n\nIn France, Émile Baudot designed in 1874 a system using a five-unit code, which began to be used extensively in that country from 1877. The British Post Office adopted the Baudot system for use on a simplex circuit between London and Paris in 1897, and subsequently made considerable use of duplex Baudot systems on their Inland Telegraph Services.\n\nDuring 1901, Baudot's code was modified by Donald Murray (1865–1945, originally from New Zealand), prompted by his development of a typewriter-like keyboard. The Murray system employed an intermediate step, a keyboard perforator, which allowed an operator to punch a paper tape, and a tape transmitter for sending the message from the punched tape. At the receiving end of the line, a printing mechanism would print on a paper tape, and/or a reperforator could be used to make a perforated copy of the message. As there was no longer a direct correlation between the operator's hand movement and the bits transmitted, there was no concern about arranging the code to minimize operator fatigue, and instead Murray designed the code to minimize wear on the machinery, assigning the code combinations with the fewest punched holes to the most frequently used characters. The Murray code also introduced what became known as \"format effectors\" or \"control characters\" – the CR (Carriage Return) and LF (Line Feed) codes. A few of Baudot's codes moved to the positions where they have stayed ever since: the NULL or BLANK and the DEL code. NULL/BLANK was used as an idle code for when no messages were being sent.\n\nIn the United States in 1902, electrical engineer Frank Pearne approached Joy Morton, head of Morton Salt, seeking a sponsor for research into the practicalities of developing a printing telegraph system. Joy Morton needed to determine whether this was worthwhile and so consulted mechanical engineer Charles L. Krum, who was vice president of the Western Cold Storage Company. Krum was interested in helping Pearne, so space was set up in a laboratory in the attic of Western Cold Storage. Frank Pearne lost interest in the project after a year and left to get involved in teaching. Krum was prepared to continue Pearne’s work, and in August, 1903 a patent was filed for a 'typebar page printer'. In 1904, Krum filed a patent for a 'type wheel printing telegraph machine' which was issued in August, 1907. In 1906 Charles Krum's son, Howard Krum, joined his father in this work. It was Howard who developed and patented the start-stop synchronizing method for code telegraph systems, which made possible the practical teleprinter.\n\nIn 1908, a working teleprinter was produced by the Morkrum Company, called the Morkrum Printing Telegraph, which was field tested with the Alton Railroad. In 1910, the Morkrum Company designed and installed the first commercial teletypewriter system on Postal Telegraph Company lines between Boston and New York City using the \"Blue Code Version\" of the Morkrum Printing Telegraph.\n\nIn 1916, Edward Kleinschmidt filed a patent application for a typebar page printer. In 1919, shortly after the Morkrum company obtained their patent for a start-stop synchronizing method for code telegraph systems, which made possible the practical teleprinter, Kleinschmidt filed an application titled \"Method of and Apparatus for Operating Printing Telegraphs\" which included an improved start-stop method. The basic start-stop procedure, however, is much older than the Kleinschmidt and Morkrum inventions. It was already proposed by D'Arlincourt in 1870.\n\nInstead of wasting time and money in patent disputes on the start-stop method, Kleinschmidt and the Morkrum Company decided to merge and form the Morkrum-Kleinschmidt Company in 1924. The new company combined the best features of both their machines into a new typewheel printer for which Kleinschmidt, Howard Krum, and Sterling Morton jointly obtained a patent.\n\nIn 1924 Britain's Creed & Company, founded by Frederick G. Creed, entered the teleprinter field with their Model 1P, a page printer, which was soon superseded by the improved Model 2P. In 1925 Creed acquired the patents for Donald Murray's Murray code, a rationalised Baudot code. The Model 3 tape printer, Creed’s first combined start-stop machine, was introduced in 1927 for the Post Office telegram service. This machine printed received messages directly on to gummed paper tape at a rate of 65 words per minute. Creed created his first keyboard perforator, which used compressed air to punch the holes. He also created a reperforator (receiving perforator) and a printer. The reperforator punched incoming Morse signals on to paper tape and the printer decoded this tape to produce alphanumeric characters on plain paper. This was the origin of the Creed High Speed Automatic Printing System, which could run at an unprecedented 200 words per minute. His system was adopted by the Daily Mail for daily transmission of the newspaper's contents. The Creed Model 7 page printing teleprinter was introduced in 1931 and was used for the inland Telex service. It worked at a speed of 50 baud, about 66 words a minute, using a code based on the Murray code.\n\nA Teletype system was installed in the Federal Aviation Administration Flight Service Station Airway Radio Stations system in 1928, carrying administrative messages, flight information and weather reports. By 1938, the FAA's Teletype network, handling weather traffic, extended over 20,000 miles, covering all 48 states except Maine, New Hampshire, and South Dakota.\n\nThere were at least five major types of teleprinter networks:\n\nMost teleprinters used the 5-bit International Telegraph Alphabet No. 2 (ITA2). This limited the character set to 32 codes (2 = 32). One had to use a \"FIGS\" (for \"figures\") shift key to type numbers and special characters. Special versions of teleprinters had FIGS characters for specific applications, such as weather symbols for weather reports. Print quality was poor by modern standards. The ITA2 code was used asynchronously with start and stop bits: the asynchronous code design was intimately linked with the start-stop electro-mechanical design of teleprinters. (Early systems had used synchronous codes, but were hard to synchronize mechanically). Other codes, such as FIELDATA and Flexowriter, were introduced but never became as popular as ITA2.\n\n\"Mark\" and \"space\" are terms describing logic levels in teleprinter circuits. The native mode of communication for a teleprinter is a simple series DC circuit that is interrupted, much as a rotary dial interrupts a telephone signal. The marking condition is when the circuit is closed (current is flowing), the spacing condition is when the circuit is open (no current is flowing). The \"idle\" condition of the circuit is a continuous marking state, with the start of a character signalled by a \"start bit\", which is always a space. Following the start bit, the character is represented by a fixed number of bits, such as 5 bits in the ITA2 code, each either a mark or a space to denote the specific character or machine function. After the character's bits, the sending machine sends one or more stop bits. The stop bits are marking, so as to be distinct from the subsequent start bit. If the sender has nothing more to send, the line simply remains in the marking state (as if a continuing series of stop bits) until a later space denotes the start of the next character. The time between characters need not be an integral multiple of a bit time, but it must be at least the minimum number of stop bits required by the receiving machine.\n\nWhen the line is broken, the continuous spacing (open circuit, no current flowing) causes a receiving teleprinter to cycle continuously, even in the absence of stop bits. It prints nothing because the characters received are all zeros, the ITA2 blank (or ASCII) null character.\n\nTeleprinter circuits were generally leased from a communications common carrier and consisted of ordinary telephone cables that extended from the teleprinter located at the customer location to the common carrier central office. These teleprinter circuits were connected to switching equipment at the central office for Telex and TWX service. Private line teleprinter circuits were not directly connected to switching equipment. Instead, these private line circuits were connected to network hubs and repeaters configured to provide point to point or point to multipoint service. More than two teleprinters could be connected to the same wire circuit by means of a current loop.\n\nEarlier teleprinters had three rows of keys and only supported upper case letters. They used the 5 bit ITA2 code and generally worked at 60 to 100 words per minute. Later teleprinters, specifically the Teletype Model 33, used ASCII code, an innovation that came into widespread use in the 1960s as computers became more widely available.\n\n\"Speed\", intended to be roughly comparable to words per minute, is the standard term introduced by Western Union for a mechanical teleprinter data transmission rate using the 5-bit ITA2 code that was popular in the 1940s and for several decades thereafter. Such a machine would send 1 start bit, 5 data bits, and 1.42 stop bits. This unusual stop bit time is actually a rest period to allow the mechanical printing mechanism to synchronize in the event that a garbled signal is received. This is true especially on High frequency radio circuits where selective fading is present. Selective fading causes the mark signal amplitude to be randomly different from the space signal amplitude. Selective fading, or Rayleigh fading can cause two carriers to randomly and independently fade to different depths. Since modern computer equipment cannot easily generate 1.42 bits for the stop period, common practice is to either approximate this with 1.5 bits, or to send 2.0 bits while accepting 1.0 bits receiving.\n\nFor example, a \"60 speed\" machine is geared at 45.5 baud (22.0 ms per bit), a \"66 speed\" machine is geared at 50.0 baud (20.0 ms per bit), a \"75 speed\" machine is geared at 56.9 baud (17.5 ms per bit), a \"100 speed\" machine is geared at 74.2 baud (13.5 ms per bit), and a \"133 speed\" machine is geared at 100.0 baud (10.0 ms per bit). 60 speed became the \"de facto\" standard for amateur radio RTTY operation because of the widespread availability of equipment at that speed and the U.S. Federal Communications Commission (FCC) restrictions to only 60 speed from 1953 to 1972. Telex, news agency wires and similar services commonly used 66 speed services. There was some migration to 75 and 100 speed as more reliable devices were introduced. However, the limitations of HF transmission such as excessive error rates due to multipath distortion and the nature of ionospheric propagation kept many users at 60 and 66 speed. Most Teletype audio recordings in existence today are of teleprinters operating at 60 words per minute, and mostly of the Teletype Model 15.\n\nAnother measure of the speed of a Teletype machine was in total \"operations per minute (OPM)\". For example, 60 speed was usually 368 OPM, 66 speed was 404 OPM, 75 speed was 460 OPM, and 100 speed was 600 OPM. Western Union Telexes were usually set at 390 OPM, with 7.0 total bits instead of the customary 7.42 bits.\n\nBoth wire-service and private teleprinters had bells to signal important incoming messages and could ring 24/7 while the power was turned on. For example, ringing 4 bells on UPI wire-service machines meant an \"Urgent\" message; 5 bells was a \"Bulletin\"; and 10 bells was a FLASH, used only for very important news, such as the assassination of John F. Kennedy.\n\nThe teleprinter circuit was often linked to a 5-bit paper tape punch (or \"reperforator\") and reader, allowing messages received to be resent on another circuit. Complex military and commercial communications networks were built using this technology. Message centers had rows of teleprinters and large racks for paper tapes awaiting transmission. Skilled operators could read the priority code from the hole pattern and might even feed a \"FLASH PRIORITY\" tape into a reader while it was still coming out of the punch. Routine traffic often had to wait hours for relay. Many teleprinters had built-in paper tape readers and punches, allowing messages to be saved in machine-readable form and edited off-line.\n\nCommunication by radio, known as \"radioteletype\" or \"RTTY\" (pronounced \"ritty\"), was also common, especially among military users. Ships, command posts (mobile, stationary, and even airborne) and logistics units took advantage of the ability of operators to send reliable and accurate information with a minimum of training. Amateur radio operators continue to use this mode of communication today, though most use computer-interface sound generators, rather than legacy hardware teleprinter equipment. Numerous modes are in use within the \"ham radio\" community, from the original ITA2 format to more modern, faster modes, which include error-checking of characters.\n\nA typewriter or electromechanical printer can print characters on paper, and execute operations such as move the carriage back to the left margin of the same line (carriage return), advance to the same column of the next line (line feed), and so on. Commands to control non-printing operations were transmitted in exactly the same way as printable characters by sending control characters with defined functions (e.g., the \"line feed\" character forced the carriage to move to the same position on the next line) to teleprinters. In modern computing and communications a few control characters, such as carriage return and line feed, have retained their original functions (although they are often implemented in software rather than activating electromechanical mechanisms to move a physical printer carriage) but many others are no longer required and are used for other purposes.\n\nSome teleprinters had a \"Here is\" key, which transmitted a fixed sequence of 20 or 22 characters, programmable by breaking tabs off a drum. This sequence could also be transmitted automatically upon receipt of an ENQ (control E) signal, if enabled. This was commonly used to identify a station; the operator could press the key to send the station identifier to the other end, or the remote station could trigger its transmission by sending the ENQ character, essentially asking \"who are you?\"\n\nCreed & Company, a British company, built teleprinters for the GPO's teleprinter service.\n\nIn 1931 Edward Kleinschmidt formed Kleinschmidt Labs to pursue a different type design of Teletype. In 1944 Kleinschmidt demonstrated their lightweight unit to the Signal Corps and in 1949 their design was adopted for the Army's portable needs. In 1956, Kleinschmidt Labs merged with Smith-Corona, which then merged with the Marchant Calculating Machine Co., forming the SCM Corporation. By 1979, the Kleinschmidt division was branching off into Electronic Data Interchange, a business in which they became very successful, and replaced the mechanical products, including teleprinters.\n\nKleinschmidt machines, with the military as their primary customer, used standard military designations for their machines. The teleprinter was identified with designations such as a TT-4/FG, while communication \"sets\" to which a teleprinter might be a part generally used the standard Army/Navy designation system such as AN/FGC-25. This includes Kleinschmidt teleprinter TT-117/FG and tape reperforator TT-179/FG.\n\nMorkrum made their first commercial installation of a printing telegraph with the Postal Telegraph Company in Boston and New York in 1910. It became popular with railroads, and the Associated Press adopted it in 1914 for their wire service. Morkrum merged with their competitor Kleinschmidt Electric Company to become Morkrum-Kleinschmidt Corporation shortly before being renamed the Teletype Corporation.\n\nItalian office equipment maker Olivetti (est. 1908) started to manufacture teleprinters in order to provide Italian post offices with modern equipment to send and receive telegrams. The first models typed on a paper ribbon, which was then cut and glued into telegram forms.\n\nSiemens & Halske, later Siemens AG, a German company, founded in 1897.\n\nThe Teletype Corporation, a part of American Telephone and Telegraph Company's Western Electric manufacturing arm since 1930, was founded in 1906 as the Morkrum Company. In 1925, a merger between Morkrum and Kleinschmidt Electric Company created the Morkrum-Kleinschmidt Company. The name was changed in December 1928 to Teletype Corporation. In 1930, Teletype Corporation was purchased by the American Telephone and Telegraph Company and became a subsidiary of Western Electric. In 1984, the divestiture of the Bell System resulted in the Teletype name and logo being replaced by the AT&T name and logo, eventually resulting in the brand being extinguished. The last vestiges of what had been the Teletype Corporation ceased in 1990, bringing to a close the dedicated teleprinter business. Despite its long-lasting trademark status, the word \"Teletype\" went into common generic usage in the news and telecommunications industries. Records of the United States Patent and Trademark Office indicate the trademark has expired and is considered dead.\n\nTeletype machines tended to be large, heavy, and extremely robust, capable of running non-stop for months at a time if properly lubricated. The Model 15 stands out as one of a few machines that remained in production for many years. It was introduced in 1930 and remained in production until 1963, a total of 33 years of continuous production. Very few complex machines can match that record. The production run was stretched somewhat by World War II—the Model 28 was scheduled to replace the Model 15 in the mid-1940s, but Teletype built so many factories to produce the Model 15 during World War II, it was more economical to continue mass production of the Model 15. The Model 15, in its receive only, no keyboard, version was the classic \"news Teletype\" for decades.\n\nSeveral different high-speed printers like the \"Ink-tronic\" etc.\n\nA global teleprinter network, called the \"Telex network\", was developed in the late 1920s, and was used through most of the 20th century for business communications. The main difference from a standard teleprinter is that Telex includes a switched routing network, originally based on pulse-telephone dialing, which in the United States was provided by Western Union. AT&T developed a competing network called \"TWX\" which initially also used rotary dialing and Baudot code, carried to the customer premises as pulses of DC on a metallic copper pair. TWX later added a second ASCII-based service using Bell 103 type modems served over lines whose physical interface was identical to regular telephone lines. In many cases, the TWX service was provided by the same telephone central office that handled voice calls, using class of service to prevent POTS customers from connecting to TWX customers. Telex is still in use in some countries for certain applications such as shipping, news, weather reporting and military command. Many business applications have moved to the Internet as most countries have discontinued telex/TWX services.\n\nIn addition to the 5-bit Baudot code and the much later seven-bit ASCII code, there was a six-bit code known as the Teletypesetter code (TTS) used by news wire services. It was first demonstrated in 1928 and began to see widespread use in the 1950s. Through the use of \"shift in\" and \"shift out\" codes, this six-bit code could represent a full set of upper and lower case characters, digits, symbols commonly used in newspapers, and typesetting instructions such as \"flush left\" or \"center\", and even \"auxiliary font\", to switch to italics or bold type, and back to roman (\"upper rail\"). \n\nThe TTS produces aligned text, taking into consideration character widths and column width, or line length.\n\nA Model 20 Teletype machine with a paper tape punch (\"reperforator\") was installed at subscriber newspaper sites. Originally these machines would simply punch paper tapes and these tapes could be read by a tape reader attached to a \"Teletypesetter operating unit\" installed on a Linotype machine. The \"operating unit\" was essentially a box full of solenoids that sat on type of the Linotype's keyboard and pressed the appropriate keys in response to the codes read from the tape, thus creating type for printing in newspapers and magazines.\n\nIn later years the incoming 6-bit current loop signal carrying the TTS code was connected to a minicomputer or mainframe for storage, editing, and eventual feed to a phototypesetting machine.\n\nComputers used teleprinters for input and output from the early days of computing. Punched card readers and fast printers replaced teleprinters for most purposes, but teleprinters continued to be used as interactive time-sharing terminals until video displays became widely available in the late 1970s.\n\nUsers typed commands after a prompt character was printed. Printing was unidirectional; if the user wanted to delete what had been typed, further characters were printed to indicate that previous text had been cancelled. When video displays first became available the user interface was initially exactly the same as for an electromechanical printer; expensive and scarce video terminals could be used interchangeably with teleprinters. This was the origin of the text terminal and the command-line interface.\nPaper tape was sometimes used to prepare input for the computer session off line and to capture computer output. The popular Teletype Model 33 used 7-bit ASCII code (with an eighth parity bit) instead of Baudot. The common modem communications settings, \"Start/Stop Bits\" and \"Parity,\" stem from the Teletype era.\n\nIn early operating systems such as Digital's RT-11, serial communication lines were often connected to teleprinters and were given device names starting with tt. This and similar conventions were adopted by many other operating systems. Unix and Unix-like operating systems use the prefix tty, for example /dev/tty13, or pty (for pseudo-tty), such as /dev/ptya0. In many computing contexts, \"TTY\" has become the name for any text terminal, such as an external console device, a user dialing into the system on a modem on a serial port device, a printing or graphical computer terminal on a computer's serial port or the RS-232 port on a USB-to-RS-232 converter attached to a computer's USB port, or even a terminal emulator application in the window system using a pseudoterminal device.\n\nTeleprinters were also used to record fault printout and other information in some TXE telephone exchanges.\n\nAlthough printing news, messages, and other text at a distance is still universal, the dedicated teleprinter tied to a pair of leased copper wires was made functionally obsolete by the fax, personal computer, inkjet printer, email, and the Internet.\n\nIn the 1980s, packet radio became the most common form of digital communications used in amateur radio. Soon, advanced multimode electronic interfaces such as the AEA PK-232 were developed, which could send and receive not only packet, but various other modulation types including Baudot. This made it possible for a home or laptop computer to replace teleprinters, saving money, complexity, space and the massive amount of paper which mechanical machines used.\n\nAs a result, by the mid-1990s, amateur use of actual Teletype machines had waned, though a core of \"purists\" still operate on equipment originally manufactured in the 1940s, 1950s, 1960s and 1970s.\n\nDespite the obsolescence of teleprinters by the 21st century, its distinctive sound continues to be played in the background of newscasts on the New York City radio station WINS, and Philadelphia's KYW, a tradition dating back to the mid-1960s.\n\n\n\n\n"}
{"id": "42906494", "url": "https://en.wikipedia.org/wiki?curid=42906494", "title": "Time-Sensitive Networking", "text": "Time-Sensitive Networking\n\nTime-Sensitive Networking (TSN) is a set of standards under development by the Time-Sensitive Networking task group of the IEEE 802.1 working group. The TSN task group was formed in November 2012 by renaming the existing Audio Video Bridging Task Group and continuing its work. The name changed as a result of extension of the working area of the standardization group. The standards define mechanisms for the time-sensitive transmission of data over Ethernet networks.\n\nThe majority of projects define extensions to the IEEE 802.1Q Virtual LANs. These extensions in particular address the transmission of very low transmission latency and high availability. Possible applications include converged networks with real time Audio/Video Streaming and real-time control streams which are used in automotive or industrial control facilities.\n\nWork is also currently being carried out in AVnu Alliances specially created Industrial group to define Compliance & Interoperability requirements for TSN networked elements. \nThe different TSN standards documents that are specified by IEEE 802.1 can be grouped into three basic key component categories that are required for a complete real-time communication solution. Each and every standard specification can be used on its own and is mostly self-sufficient. However, only when used together in a concerted way, TSN as a communication system can achieve its full potential. The three basic components are:\n\nThe name \"Time-sensitive networking\" is already quite descriptive in this regard: In contrast to standard Ethernet according to IEEE 802.3 and Ethernet bridging according to IEEE 802.1Q, time plays an important role in TSN networks. For real-time communication with hard, non-negotiable time boundaries for end-to-end transmission latencies, all devices in this network need to have a common time reference and therefore, need to synchronize their clocks among each other. This is not only true for the end devices of a communication stream, such as an industrial controller and a manufacturing robot, but also true for network components, such as Ethernet switches. Only through synchronized clocks, it is possible for all network devices to operate in unison and execute the required operation at exactly the required point in time.\n\nTime synchronization in TSN networks can be achieved with different technologies. Theoretically, it is possible to outfit every end device and network switch with a GPS clock. However, this is costly and there is no guarantee that the Radio or GPS clock has access to the radio or satellite signal at all times - for example if the network is installed in a moving car, on a factory floor or in a tunnel deep beneath the surface of the earth. Due to these constraints, time in TSN networks is usually distributed from one central time source directly through the network itself. In most cases, this is done using the IEEE 1588 Precision Time Protocol, which utilizes Ethernet frames to distribute time synchronization information. In addition to the universally applicable IEEE 1588 specification, the Time-Sensitive Task Group of the IEEE 802.1 committee has specified a profile of IEEE 1588, called IEEE 802.1AS. The idea behind this profile is to narrow the huge list of different IEEE 1588 options down to a manageable few critical options that are applicable to Home networks or networks in automotive car or industrial automation environments.\n\nScheduling and traffic shaping allows for the coexistence of different traffic classes with different priorities on the same network - each with different requirements to available bandwidth and end-to-end latency. Standard bridging according to IEEE 802.1q uses eight distinct priorities with a strict priority scheme. On the protocol level, these priorities are visible in the 802.1Q VLAN tag of a standard Ethernet frame. These priorities already allow to distinguish between more important and less important network traffic, but even with the highest of the eight priorities, no absolute guarantee for an end-to-end delivery time can be given. The reason for this are buffering effects inside the Ethernet switches. If a switch has started the transmission of an Ethernet frame on one of its ports, even the highest priority frame has to wait inside the switch buffer for this transmission to finish. With standard Ethernet switching, this non-determinism cannot be avoided. This is not an issue in environments where applications do not depend on the timely delivery of single Ethernet frames - such as office IT infrastructures. In these environments, file transfers, emails or other business applications have limited time sensitivity themselves and are usually protected by other mechanisms further up the protocol stack, such as the Transmission Control Protocol. In industrial automation and automotive car environments, however, where closed loop control or safety applications are using the Ethernet network, reliable and timely delivery is of utmost importance. For Ethernet to be used here, the strict priority scheduling of IEEE 802.1Q needs to be enhanced.\n\nTSN enhances standard Ethernet communication by adding mechanisms to ensure timely delivery with soft and hard real-time requirements. The mechanism of utilizing the eight distinct VLAN priorities is retained, to ensure complete backwards compatibility to non-TSN Ethernet. This has always been one of the design principles of the IEEE 802 group when developing Ethernet further - maintain backwards compatibility to maintain interoperability with the existing infrastructure and to allow a seamless migration towards new technologies.\n\nWith TSN, for each of the eight priorities, a user can select from different mechanisms how Ethernet frames are processed and priorities can be individually assigned to already existing methods (such as the IEEE 802.1Q strict priority scheduler) or new processing methods, such as the TSN IEEE 802.1Qbv time-aware traffic scheduler.\n\nA typical use case for TSN is the communication of a Programmable Logic Controller (PLC) with an industrial robot through an Ethernet network. To achieve transmission times with guaranteed end-to-end latency that can support the closed loop control that is operating between the PLC and the robot, one or several of the eight Ethernet priorities can be assigned to the IEEE 802.1Qbv time-aware scheduler. This scheduler is designed to separate the communication on the Ethernet network into fixed length, repeating time cycles. Within these cycles, different time slices can be configured that can be assigned to one or several of the eight Ethernet priorities. By doing this, it is possible to grant exclusive use - for a limited time - to the Ethernet transmission medium for those traffic classes that need transmission guarantees and can't be interrupted. The basic concept is a time-division multiple access (TDMA) scheme. By establishing virtual communication channels for specific time periods, time-critical communication can be separated from non-critical background traffic. By granting exclusive access to the transmission medium and devices to time-critical traffic classes, the buffering effects in the Ethernet switch transmission buffers can be avoided and time-critical traffic can be transmitted without non-deterministic interruptions. One example for an IEEE 802.1Qbv scheduler configuration is visible in figure 1:\nIn this example, each cycle consists of two time slices. Time slice 1 only allows the transmission of traffic tagged with VLAN priority 3, and time slice 2 in each cycle allows for the rest of the priorities to be sent. Since the IEEE 802.1Qbv scheduler requires all clocks on all network devices (Ethernet switches and end devices) to be synchronized and the identical schedule to be configured, all devices understand which priority can be sent to the network at any given point in time. Since time slice 2 has more than one priority assigned to it, within this time slice, the priorities are handled according to standard IEEE 802.1Q strict priority scheduling.\n\nThis separation of Ethernet transmissions into cycles and time slices can be enhanced further by the inclusion of other scheduling or traffic shaping algorithms, such as the Audio- and Video Bridging traffic shaper IEEE 802.1Qav. IEEE 802.1Qav supports soft real-time. In this particular example, IEEE 802.1Qav could be assigned to one or two of the priorities that are used in time slice two to distinguish further between audio/video traffic and background file transfers. The IEEE 802.1 Time-Sensitive Networking Task Group specifies a number of different schedulers and traffic shapers that can be combined to achieve the nonreactive coexistence of hard real-time, soft real-time and background traffic on the same Ethernet infrastructure.\n\nWhen an Ethernet interface has started the transmission of a frame to the transmission medium, this transmission has to be completely finished before another transmission can take place. This includes the transmission of the CRC32 checksum at the end of the frame to ensure a reliable, fault-free transmission. This inherent property of Ethernet networks - again- poses a challenge to the TDMA approach of the IEEE 802.1Qbv scheduler. This is visible in figure 2:\nJust before the end of time slice 2 in cycle n, a new frame transmission is started. Unfortunately, this frame is too large to fit into its time slice. Since the transmission of this frame cannot be interrupted, the frame infringes the following time slice 1 of the next cycle n+1. By partially or completely blocking a time-critical time slice, real-time frames can be delayed up to the point where they cannot meet the application requirements any longer. This is very similar to the actual buffering effects that happen in non-TSN Ethernet switches, so TSN has to specify a mechanism to prevent this from happening.\n\nThe IEEE 802.1Qbv time-aware scheduler has to ensure that the Ethernet interface is not busy with the transmission of a frame when the scheduler changes from one time slice into the next. The time-aware scheduler achieves this by putting a guard band in front of every time slice that carries time-critical traffic. During this guard band time, no new Ethernet frame transmission may be started, only already ongoing transmissions may be finished. The duration of this guard band has to be as long as it takes the maximum frame size to be safely transmitted. For an Ethernet frame according to IEEE 802.3 with a single IEEE 802.1Q VLAN tag and including interframe spacing, the total length is: 1500 byte (frame payload) + 18 byte (ethernet addresses, ethertype and CRC) + 4 byte (VLAN Tag) + 12 byte (Interframe spacing) + 8 byte (preamble and SFD) = 1542 byte.\n\nThe total time needed for sending this frame is dependant on the link speed of the Ethernet network. With Fast Ethernet and 100 Mbit/s transmission rate, the transmission duration is as follows:\n\nformula_1\n\nIn this case, the guard band has to be at least 123.36 µs long. With the guard band, the total bandwidth / time that is usable within a time slice is reduced by the length of the guard band. This is visible in figure 3:\nNote: to facilitate the presentation of the topic, the actual size of the guard band in figure 3 is not to scale, but is significantly smaller than indicated by the frame in figure 2.\n\nIn this example, the time slice 1 always contains high priority data (e.g. for motion control), while time slice 2 always contains best effort data.Therefore, a guard band needs to be placed at every transition point into time slice 1 to protect the time slice of the critical data stream(s).\n\nWhile the guard bands manage to protect the time slices with high priority, critical traffic, they also have some significant drawbacks:\nTo partially mitigate the loss of bandwidth through the guard band, the standard IEEE 802.1Qbv includes a length-aware scheduling mechanism. This mechanism is used when store-and-forward switching is utilized: after the full reception of an Ethernet frame that needs to be transmitted on a port where the guard band is in effect, the scheduler checks the overall length of the frame. If the frame can fit completely inside the guard band, without any infringement of the following high priority slice, the scheduler can send this frame, despite an active guard band, and reduce the waste of bandwidth. This mechanism, however, cannot be used when cut-through switching is enabled, since the total length of the Ethernet frame needs to be known a priori. Therefore, when cut-through switching is used to minimize end-to-end latency, the waste of bandwidth will still occur. Also, this does not help with the minimum achievable cycle time. Therefore, length-aware scheduling is an improvement, but cannot mitigate all drawbacks that are introduced by the guard band.\n\nTo further mitigate the negative effects from the guard bands, the IEEE working groups 802.1 and 802.3 have specified the frame pre-emption technology. The two working groups collaborated in this endeavour, since the technology required both changes in the Ethernet Media Access Control (MAC) scheme that is under the control of IEEE 802.3, as well as changes in the management mechanisms that are under the control of IEEE 802.1. Due to this fact, frame pre-emption is described in two different standards documents: IEEE 802.1Qbu for the bridge management component and IEEE 802.3br for the Ethernet MAC component.\nFigure 4 gives a basic example how frame pre-emption works. During the process of sending a best effort Ethernet frame, the MAC interrupts the frame transmission just before the start of the guard band. The partial frame is completed with a CRC and will be stored in the next switch to wait for the second part of the frame to arrive. After the high priority traffic in time slice 1 has passed and the cycle switches back to time slice 2, the interrupted frame transmission is resumed. Frame pre-emption always operates on a pure link-by-link basis and only fragments from one Ethernet switch to the next Ethernet switch, where the frame is reassembled. In contrast to fragmentation with the Internet Protocol (IP), no end-to-end fragmentation is supported.\n\nEach partial frame is completed by a CRC32 for error detection. In contrast to the regular Ethernet CRC32, the last 16 bit are inverted to make a partial frame distinguishable from a regular Ethernet frame. In addition, also the start of frame delimiter (SFD) is changed.\n\nThe support for frame pre-emption has to be activated on each link between devices individually. To signal the capability for frame pre-emption on a link, an Ethernet switch announces this capability through the LLDP (Link Layer Discovery Protocol). When a device receives such an LLDP announcement on a network port and supports frame pre-emption itself, it may activate the capability. There is no direct negotiation and activation of the capability on adjacent devices. Any device that receives the LLDP pre-emption announcement assumes that on the other end of the link, a device is present that can understand the changes in the frame format (changed CRC32 and SFD).\n\nFrame pre-emption allows for a significant reduction of the guard band. The length of the guard band is now dependant on the precision of the frame pre-emption mechanism: how small is the minimum size of the frame that the mechanism can still pre-empt. IEEE 802.3br specifies the best accuracy for this mechanism at 64 byte - due to the fact that this is the minimum size of a still valid Ethernet frame. In this case, the guard band can be reduced to a total of 127 byte: 64 byte (minimum frame) + 63 byte (remaining length that cannot be pre-empted). All larger frames can be pre-empted again and therefore, there is no need to protect against this size with a guard band.\n\nThis minimizes the best effort bandwidth that is lost and also allows for much shorter cycle times at slower Ethernet speeds, such as 100 Mbit/s and below. Since the pre-emption takes place in hardware in the MAC, as the frame passes through, cut-through switching can be supported as well, since the overall frame size is not needed a priori. The MAC interface just checks in regular 64 byte intervals whether the frame needs to be pre-empted or not.\n\nThe combination of time synchronization, the IEEE 802.1Qbv scheduler and frame pre-emption already constitutes an effective set of standards that can be utilized to guarantee the coexistence of different traffic categories on a network while also providing end-to-end latency guarantees. This will be enhanced further as new IEEE 802.1 specifications, such as 802.1Qch are finalized.\n\nThe TSN technology, especially the time-aware scheduler according to IEEE 802.1Qbv, have been developed for use in mission-critical network environments. In these networks, not only are timing guarantees relevant, but fault-tolerance is as well. Networks that support applications such as safety-relevant control loops or autonomous driving in vehicles have to be protected against faults in hardware or network media. The TSN task group is currently specifying the fault-tolerance protocol IEEE 802.1CB for this purpose. In addition to this protocol, existing high-availability protocols such as HSR or PRP that are specified in IEC 62439-3, can be utilized.\n\nTo register fault-tolerant communication streams across a network, Path control and reservation as specified in IEEE 802.1Qca, manual configuration or vendor-specific solutions can be used.\n\nIn the currently ongoing project IEEE 802.1Qcc, the TSN task group focuses on the definition of management interfaces and protocols to enable TSN network administration on large scale networks. Three different aspects are discussed here, both with a de-centralized approach as well as a fully centralized approach that re-uses configuration concepts from software-defined networking (SDN). The current discussion can be followed through the public document archive of IEEE 802.1.\n\nRelated projects:\n\n"}
{"id": "4496518", "url": "https://en.wikipedia.org/wiki?curid=4496518", "title": "Topological quantum computer", "text": "Topological quantum computer\n\nA topological quantum computer is a theoretical quantum computer that employs two-dimensional quasiparticles called anyons, whose world lines pass around one another to form braids in a three-dimensional spacetime (i.e., one temporal plus two spatial dimensions). These braids form the logic gates that make up the computer. The advantage of a quantum computer based on quantum braids over using trapped quantum particles is that the former is much more stable. Small, cumulative perturbations can cause quantum states to decohere and introduce errors in the computation, but such small perturbations do not change the braids' topological properties. This is like the effort required to cut a string and reattach the ends to form a different braid, as opposed to a ball (representing an ordinary quantum particle in four-dimensional spacetime) bumping into a wall. Alexei Kitaev proposed topological quantum computation in 1997. While the elements of a topological quantum computer originate in a purely mathematical realm, experiments in fractional quantum Hall systems indicate these elements may be created in the real world using semiconductors made of gallium arsenide at a temperature of near absolute zero and subjected to strong magnetic fields.\n\nAnyons are quasiparticles in a two-dimensional space. Anyons are neither fermions nor bosons, but like fermions, they cannot occupy the same state. Thus, the world lines of two anyons cannot intersect or merge, which allows their paths to form stable braids in space-time. Anyons can form from excitations in a cold, two-dimensional electron gas in a very strong magnetic field, and carry fractional units of magnetic flux. This phenomenon is called the fractional quantum Hall effect. In typical laboratory systems, the electron gas occupies a thin semiconducting layer sandwiched between layers of aluminium gallium arsenide.\n\nWhen anyons are braided, the transformation of the quantum state of the system\ndepends only on the topological class of the anyons' trajectories (which are classified\naccording to the braid group). Therefore, the quantum information which is stored in the state of the system is impervious to small errors in the trajectories.\nIn 2005, Sankar Das Sarma, Michael Freedman, and Chetan Nayak proposed a quantum Hall device which would realize a topological qubit.\nIn a key development for topological quantum computers, in 2005 Vladimir J. Goldman, Fernando E. Camino, and Wei Zhou were said to have created the first experimental evidence for using fractional quantum Hall effect to create actual anyons, although others have suggested their results could be the product of phenomena not involving anyons. It should also be noted that non-abelian anyons, a species required for topological quantum computers, have yet to be experimentally confirmed. Possible experimental evidence has been found, but the conclusions remain contested.\n\nTopological quantum computers are equivalent in computational power to other standard models of quantum computation, in particular to the quantum circuit model and to the quantum Turing machine model. That is, any of these models can efficiently simulate any of the others. Nonetheless, certain algorithms may be a more natural fit to the topological quantum computer model. For example, algorithms for evaluating the Jones polynomial were first developed in the topological model, and only later converted and extended in the standard quantum circuit model.\n\nTo live up to its name, a topological quantum computer must provide the unique computation properties promised by a conventional quantum computer design, which uses trapped quantum particles. Fortunately in 2002, Michael H. Freedman, Alexei Kitaev, Michael J. Larsen, and Zhenghan Wang proved that a topological quantum computer can, in principle, perform any computation that a conventional quantum computer can do.\n\nThey found that a conventional quantum computer device, given an error-free operation of its logic circuits, will give a solution with an absolute level of accuracy, whereas a topological quantum computing device with flawless operation will give the solution with only a finite level of accuracy. However, any level of precision for the answer can be obtained by adding more braid twists (logic circuits) to the topological quantum computer, in a simple linear relationship. In other words, a reasonable increase in elements (braid twists) can achieve a high degree of accuracy in the answer. Actual computation [gates] are done by the edge states of a fractional quantum Hall effect. This makes models of one-dimensional anyons important. In one space dimension, anyons are defined algebraically.\n\nEven though quantum braids are inherently more stable than trapped quantum particles, there is still a need to control for error inducing thermal fluctuations, which produce random stray pairs of anyons which interfere with adjoining braids. Controlling these errors is simply a matter of separating the anyons to a distance where the rate of interfering strays drops to near zero. Simulating the dynamics of a topological quantum computer may be a promising method of implementing fault-tolerant quantum computation even with a standard quantum information processing scheme. Raussendorf, Harrington, and Goyal have studied one model, with promising simulation results.\n\nOne of the prominent examples in topological quantum computing is with a system of fibonacci anyons. These anyons can be used to create generic gates for topological quantum computing. There are three main steps for creating a model:\n\nFibonacci Anyons are defined by three qualities:\nThe last ‘fusion’ rule can be extended this to a system of three anyons:\n\nThus, fusing three anyons will yield a final state of total charge formula_1 in 2 ways, or a charge of formula_2 in exactly one way. We use three states to define our basis. However, because we wish to encode these three anyon states as superpositions of 0 and 1, we need to limit the basis to a two-dimensional Hilbert Space. Thus, we consider only two states with a total charge of formula_1. This choice is purely phenomenological. In these states, we group the two leftmost anyons into a 'control group', and leave the rightmost as a 'non-computational anyon'. We classify a formula_14 state as one where the control group has total 'fused' charge of formula_2, and a state of formula_16 has a control group with a total 'fused' charge of formula_1. For a more complete description, see Nayak.\n\nFollowing the ideas above, adiabatically braiding these anyons around each-other with result in a unitary transformation. These braid operators are a result of two subclasses of operators:\nThe R matrix can be conceptually thought of as the topological phase that is imparted onto the anyons during the braid. As the anyons wind around each-other, they pick up some phase due to the Aharonov-Bohm effect.\n\nThe F matrix is a result of the physical rotations of the anyons. As they braid between each-other, it is important to realize that the bottom two anyons—the control group—will still distinguish the state of the qubit. Thus, braiding the anyons will change which anyons are in the control group, and therefore change the basis. We evaluate the anyons by always fusing the control group (the bottom anyons) together first, so exchanging which anyons these are will rotate the system. Because these anyons are non-abelian, the order of the anyons (which ones are within the control group) will matter, and as such they will transform the system.\n\nThe complete braid operator can be derived as:\n\nformula_18\n\nIn order to mathematically construct the F and R operators, we can consider permutations of these F and R operators. We know that if we sequentially change the basis that we are operating on, this will eventually lead us back to the same basis. Similarly, we know that if we braid anyons around each-other a certain number of times, this will lead back to the same state. These axioms are called the pentagonal and hexagonal axioms respectively as performing the operation can be visualized with a pentagon/hexagon of state transformations. Although mathematically difficult, these can be approached much more successfully visually.\n\nWith these braid operators, we can finally formalize the notion of braids in terms of how they act on our Hilbert space and construct arbitrary universal quantum gates.\n\nExplicit braids that perform particular quantum computations with Fibonacci anyons have been given by \n\n\n"}
