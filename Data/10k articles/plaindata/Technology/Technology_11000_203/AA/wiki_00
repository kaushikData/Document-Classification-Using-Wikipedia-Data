{"id": "54238206", "url": "https://en.wikipedia.org/wiki?curid=54238206", "title": "ACM SIGHPC", "text": "ACM SIGHPC\n\nACM SIGHPC is the Association for Computing Machinery's Special Interest Group on High Performance Computing, an international community of students, faculty, researchers, and practitioners working on research and in professional practice related to supercomputing, high-end computers, and cluster computing. The organization co-sponsors international conferences related to high performance and scientific computing, including: SC, the International Conference for High Performance Computing, Networking, Storage and Analysis; the Platform for Advanced Scientific Computing (PASC) Conference; and PPoPP, the Symposium on Principles and Practice of Parallel Programming.\n\nACM SIGHPC was founded on November 1, 2011 with the support of ACM SIGARCH. The first chair was Cherri Pancake, who was also the 1999 ACM/IEEE Supercomputing Conference chair.\n\nDuring its formation, the SIG was led by a set of volunteer officers:\n\nThese officers were replaced by the first elected slate of officers in July 2013, with subsequent elections scheduled every three years.\n\nIn addition to elected officers, the SIG is supported by a variety of appointed volunteer leaders who are responsible for membership coordination, creating the newsletter, and other duties needed to operated the SIG; the roles vary as the needs of the SIG change over time. These volunteer leaders are appointed by the SIG chair.\n\nACM SIGHPC co-sponsors the following international conferences related to high performance computing:\n\nIn addition, ACM SIGHPC supports the following conferences with in-cooperation status:\n\nSIGHPC offers a variety of travel grants to support student participation in its conferences, including a program in partnership with ACM-W that focuses specifically on participation by women. SIGHPC also sponsors an Outstanding Doctoral Dissertation Award, given each year for the best doctoral dissertation completed in high performance computing. This award is open to students studying anywhere in the world who have completed a PhD dissertation with HPC as a central research theme.\n\nThe SIG also sponsors the Emerging Woman Leader in Technical Computing Award. This award is offered once every two years and recognizes a woman in high performance and technical computing during the important middle years of their career for their work in research, education, and/or practice over the five to fifteen years after completing her last academic degree.\n\nIn 2015 SIGHPC partnered with Intel on a five year program to encourage participation by students from underrepresented groups in high performance computing, and data and computational science. The ACM SIGHPC/Intel Computational and Data Science Fellowships are part of a 5-year program to increase the diversity of students pursuing graduate degrees in data science and computational science. Specifically targeted at women or students from racial/ethnic backgrounds that have not traditionally participated in the computing field, the program is open to students pursuing degrees at institutions anywhere in the world. As of the announcement of the 2018 class, the fellowship is supporting a total of 33 students.\n\nChapters are sub-organizations within ACM SIGs that may be organized by a geographic region or by a technical topic area. Chapter activities vary, but may include physical meetings, webinars, and workshops. SIGHPC currently supports the following chapters:\n\n\n\n\n"}
{"id": "7589571", "url": "https://en.wikipedia.org/wiki?curid=7589571", "title": "APA Task Force on Deceptive and Indirect Methods of Persuasion and Control", "text": "APA Task Force on Deceptive and Indirect Methods of Persuasion and Control\n\nThe APA Task Force on Deceptive and Indirect Methods of Persuasion and Control (DIMPAC) formed at the request of the American Psychological Association (APA) in 1983. The APA asked Margaret Singer, one of the leading proponents of theories of coercive persuasion, to chair a task force to:\nBefore the task force had submitted its final report, the APA submitted an \"amicus curiae\" brief (10 February 1987) in a case pending before the California Supreme Court. The case involved issues of brainwashing and coercive persuasion. The brief stated that Singer's hypotheses \"were uninformed speculations based on skewed data.\" The APA subsequently withdrew from the brief, portraying its participation as premature in that DIMPAC had not yet submitted its report. (Scholars who had co-signed the brief did not withdraw.)\n\nThe task force completed its final report in November 1986. In May 1987 the APA Board of Social and Ethical Responsibility for Psychology (BSERP) rejected the DIMPAC final report; stating that the report \"lack[ed] the scientific rigor and evenhanded critical approach necessary for APA imprimatur\", and also stating that the BSERP did \"not believe that we have sufficient information available to guide us in taking a position on this issue\".\nThe BSERP board requested that the task-force members not distribute or publicize the report without indicating that the Board found the report unacceptable, and cautioned the members of the task force against using their past appointment to it \"to imply BSERP or APA support or approval of the positions advocated in the report\".\n\nSinger and her professional associate sociologist Richard Ofshe subsequently sued the APA in 1992 for \"defamation, frauds, aiding and abetting and conspiracy\" and lost in 1994. Subsequently, judges did not accept Singer as an expert witness in cases alleging brainwashing and mind control.\n\nThe members of the task force were:\n\n\nThe draft report of the DIMPAC task force\n(which the BSERP board requested that task-force members not distribute or publicize without indicating that the Board found the report unacceptable) included the following abstract:\nCults and large group awareness trainings have generated considerable controversy because of their widespread use of deceptive and indirect techniques of persuasion and control. These techniques can compromise individual freedom, and their use has resulted in serious harm to thousands of individuals and families. This report reviews the literature on this subject, proposes a new way of conceptualizing influence techniques, explores the ethical ramifications of deceptive and indirect techniques of persuasion and control, and makes recommendations addressing the problems described in the report.\nDraft recommendations included:\n\n\nBefore the task force had submitted its final report, the APA, together with a group of scholars, submitted on February 10, 1987, an \"amicus curiæ\" brief in a pending case, Molko v. Holy Spirit Ass'n for the Unification of World Christianity, before the California Supreme Court, involving issues of brainwashing and coercive persuasion related to the Unification Church. The brief portrayed Singer's hypotheses as uninformed speculations based on skewed data.\n\nThe brief characterized the theory of brainwashing as not scientifically proven and advanced the position that \"this commitment to advancing the appropriate use of psychological testimony in the courts carries with it the concomitant duty to be vigilant against those who would use purportedly expert testimony lacking scientific and methodological rigor\".\n\nOn March 24, 1987, the APA filed a motion to withdraw its signature from this brief, as it considered the conclusion premature, in view of the ongoing work of the DIMPAC task force. The \"amicus\" as such continued because the co-signed scholars did not withdraw their signatures. These included: Jeffrey Hadden, Eileen Barker, David Bromley and J. Gordon Melton, Joseph Bettis, Durwood Foster, William R. Garret, Richard D. Kahone, Timothy Miller, John Young, James T. Richardson, Ray L. Hart, Benton Johnson, Franklin Littell, Newton Malony, Donald E. Miller, Mel Prosen, Thomas Robbins, and Huston Smith.\n\nOn May 11, 1987, the APA \"Board of Social and Ethical Responsibility for Psychology\" (BSERP) rejected the DIMPAC report because \"In general, the report lacks the scientific rigor and evenhanded critical approach necessary for APA imprimatur.\"\n\nAlong with the rejection memo came two letters from external advisers to the APA who reviewed the report (the APA did not make its internal review public):\n\n\nThe BSERP board also cautioned the members of the task force \"against using their past appointment to imply BSERP or APA support or approval of the positions advocated in the report\", and stated that they should \"not distribute or publicize the report without indicating that the report was unacceptable to the Board.\"\n\nThe memorandum concludes with \"Finally, after much consideration, BSERP does not believe that we have sufficient information available to guide us in taking a position on this issue.\"\n\nIn August 1988 the District of Columbia Court of Appeals overturned the \"Kropinski v. World Plan Executive Council\" case, based on the lack of scientific support for the theories presented by Margaret Singer, during her testimony as an expert witness.\n\nIn 1989 the Fourth Appellate District Court of Appeal of California, in the \"Robin George v. International Society for Krishna Consciousness\" case, rejected Singer's expert testimony on the basis that the brainwashing theory of false imprisonment constituted an attempt to premise tort liability on religious practices that the plaintiff believed to be objectionable, and that such premise appeared inconsistent with the First Amendment.\n\nIn 1990 District Court Judge Lowell Jensen excluded Singer's testimony in \"United States v. Fishman\", because the Court remained unconvinced that the medical community widely accepted the application of coercive persuasion theory to religious cults and because the Court did not accept the theory of coercive persuasion in the context of cults.\n\nIn 1991, in the \"Patrick Ryan v. Maharishi Yogi\" case filed in the US District Court in Washington, DC, Judge Oliver Gasch refused to allow Singer to testify, based on the premises that Singer and Ofshe's theory did not enjoy substantial scientific approval and was therefore not admissible as the basis of expert opinion.\n\nWhen the APA's BSERP declined to accept the DIMPAC findings, Singer sued the APA and other scholars in 1992 for \"defamation, frauds, aiding and abetting and conspiracy\", under the Racketeer Influenced and Corrupt Organizations Act (RICO), and lost in 1994.\nThe lawsuit alleged that several top executives at the APA and ASA attempted to destroy careers, charging that from 1986 to 1992 they resorted to improper influence of witnesses in state court litigations, filed untrue affidavits, attempted to obstruct justice in federal litigations, deceived federal judges, and committed wire and mail fraud. Ofshe and Singer said that these actions damaged their reputations as forensic experts in the fields of psychology and sociology in the area of coercive persuasion, preventing their testimony against cults, and specified acts of collusion between several of the defendants and cult groups.\n\nThe court summons filed by Singer and Ofshe's lawyer described the rejection of the DIMPAC report by the APA's BSERP as a \"rejection of the scientific validity of the theory of coercive persuasion\".\n\nThe court dismissed the case on the basis that the claims of defamation, frauds, aiding and abetting and conspiracy constituted a dispute over the application of the First Amendment to a public debate over academic and professional matters. The court stated that one could characterize the parties as the opposing camps in a long-standing debate over certain theories in the field of psychology; and that the plaintiffs could not establish deceit with reference to representations made to other parties in the lawsuit.\n\nIn a further ruling, James R. Lamden ordered Ofshe and Singer to pay $80,000 in attorneys' fees under California's SLAPP suit law, which penalizes those who harass others for exercising their First Amendment rights. At that time, Singer and Ofshe declared their intention to sue Michael Flomenhaft, the lawyer who represented them in the case, for malpractice.\n\nAPA Division 36 (then \"Psychologists Interested in Religious Issues\" (PIRI), subsequently \"Psychology of Religion\") in its 1990 annual convention approved a resolution stating that insufficient consensually accepted research then scientifically supported the assertion that equated the use of \"techniques of influence as typically practiced\" by religious groups with \"coercive persuasion\", \"mind control\", or \"brainwashing\". The Executive Committee invited researchers to submit proposals to present their work on the topic.\n\n\n"}
{"id": "31417095", "url": "https://en.wikipedia.org/wiki?curid=31417095", "title": "Antenna array", "text": "Antenna array\n\nAn antenna array (or array antenna) is a set of multiple connected antennas which work together as a single antenna, to transmit or receive radio waves. The individual antennas (called \"elements\") are usually connected to a single receiver or transmitter by feedlines that feed the power to the elements in a specific phase relationship. The radio waves radiated by each individual antenna combine and superpose, adding together (interfering constructively) to enhance the power radiated in desired directions, and cancelling (interfering destructively) to reduce the power radiated in other directions. Similarly, when used for receiving, the separate radio frequency currents from the individual antennas combine in the receiver with the correct phase relationship to enhance signals received from the desired directions and cancel signals from undesired directions. More sophisticated array antennas may have multiple transmitter or receiver modules, each connected to a separate antenna element or group of elements. \n\nAn antenna array can achieve higher gain (directivity), that is a narrower beam of radio waves, than could be achieved by a single element. In general, the larger the number of individual antenna elements used, the higher the gain and the narrower the beam. Some antenna arrays (such as military phased array radars) are composed of thousands of individual antennas. Arrays can be used to achieve higher gain, to give path diversity (also called MIMO) which increases communication reliability, to cancel interference from specific directions, to steer the radio beam electronically to point in different directions, and for radio direction finding (RDF).\n\nThe term antenna array most commonly means a \"driven array\" consisting of multiple identical driven elements all connected to the receiver or transmitter, often half-wave dipoles fed in phase. A \"parasitic array\" consists of a single driven element connected to the feedline, and other elements which are not, called parasitic elements. It is usually another name for a Yagi-Uda antenna.\n\nA \"phased array\" usually means an \"electronically scanned array\"; a driven array antenna in which each individual element is connected to the transmitter or receiver through a phase shifter controlled by a computer. The beam of radio waves can be steered electronically to point instantly in any direction over a wide angle, without moving the antennas. However the term \"phased array\" is sometimes used to mean an ordinary array antenna.\n\nSmall antennas around one wavelength in size, such as quarter-wave monopoles and half-wave dipoles, don't have much directivity (gain); they are omnidirectional antennas which radiate radio waves over a wide angle. To create a high gain antenna, which radiates radio waves in a narrow beam, two general techniques can be used. One technique is to use large metal surfaces such as parabolic reflectors, horns or dielectric lenses which change the direction of the radio waves by reflection or refraction, to focus the radio waves from a single low gain antenna into a beam. This type is called an \"aperture antenna\". A parabolic dish is an example of this type of antenna.\n\nA second technique is to use multiple antennas which are fed from the same transmitter or receiver; this is called an array antenna, or antenna array. If the currents are fed to the antennas with the proper phase, due to the phenomenon of interference the spherical waves from the individual antennas combine (superpose) in front of the array to create plane waves, a beam of radio waves traveling in a specific direction. In directions in which the waves from the individual antennas arrive in phase, the waves add together (constructive interference) to enhance the power radiated. In directions in which the individual waves arrive out of phase, with the peak of one wave coinciding with the valley of another, the waves cancel (destructive interference) reducing the power radiated in that direction. Similarly, when receiving, the oscillating currents received by the separate antennas from radio waves received from desired directions are in phase and when combined in the receiver reinforce each other, while currents from radio waves received from other directions are out of phase and when combined in the receiver cancel each other.\n\nThe radiation pattern of such an antenna consists of a strong beam in one direction, the main lobe, plus a series of weaker beams at different angles called sidelobes, usually representing residual radiation in unwanted directions. The larger the width of the antenna and the greater the number of component antenna elements, the narrower the main lobe, and the higher the gain which can be achieved, and the smaller the sidelobes will be.\n\nArrays in which the antenna elements are fed in phase are broadside arrays; the main lobe is emitted perpendicular to the plane of the elements.\n\nThe largest array antennas are radio interferometers used in the field of radio astronomy, in which multiple radio telescopes consisting of large parabolic antennas are linked together into an antenna array, to achieve higher resolution. Using the technique called aperture synthesis such an array can have the resolution of an antenna with a diameter equal to the distance between the antennas. In the technique called Very Long Baseline Interferometry (VLBI) dishes on separate continents have been linked, creating \"array antennas\" thousands of miles in size.\n\nMost array antennas can be divided into two classes based on how the component antennas' axis is related to the direction of radiation. \nThere are also arrays (such as phased arrays) in which the direction of radiation is at some other angle to the antenna axis.\n\n\"Driven array\" - This is an array in which the individual component antennas are all \"driven\" - connected to the transmitter or receiver. The individual antennas, which are usually identical, often consist of single \"driven element\"s, such as half-wave dipoles, but may also be composite antennas such as Yagi antennas or turnstile antennas.\n\"Parasitic array\" - This is an endfire array which consist of multiple antenna elements in a line of which only one, the driven element, is connected to the transmitter or receiver, while the other elements, called parasitic elements, are not. The parasitic elements function as resonators, absorbing radio waves from the driven element and reradiating them with a different phase, to modify the radiation pattern of the antenna, increasing the power radiated in the desired direction. Since these have only one driven element they are often called \"antennas\" instead of \"arrays\".\n\n"}
{"id": "59131574", "url": "https://en.wikipedia.org/wiki?curid=59131574", "title": "Aquamog", "text": "Aquamog\n\nAn aquamog is a machine for removing weeds growing over a body of water. An aquamog was used in 2018 for removing water primrose from North Lake in San Francisco's Golden Gate Park. A newspaper photo shows a machine similar to a backhoe on very wide crawlers. \n\nThe name \"aquamog\" was derived from the multi-purpose vehicle \"Unimog\" produced by Mercedes Benz.\n"}
{"id": "31871788", "url": "https://en.wikipedia.org/wiki?curid=31871788", "title": "Brand Affinity Technologies", "text": "Brand Affinity Technologies\n\nBrand Affinity Technologies (BAT) is a technology and marketing services company. It was founded by brother entrepreneurs Ryan Steelberg and Chad Steelberg. In October 2012, Brand Affinity Technologies acquired Printroom, a leading fan photography business.\n"}
{"id": "6900318", "url": "https://en.wikipedia.org/wiki?curid=6900318", "title": "Building insulation", "text": "Building insulation\n\nBuilding insulation is any object in a building used as insulation for any purpose. While the majority of insulation in buildings is for thermal purposes, the term also applies to acoustic insulation, fire insulation, and impact insulation (e.g. for vibrations caused by industrial applications). Often an insulation material will be chosen for its ability to perform several of these functions at once.\n\nThermal insulation in buildings is an important factor to achieving thermal comfort for its occupants. Insulation reduces unwanted heat loss or gain and can decrease the energy demands of heating and cooling systems. It does not necessarily deal with issues of adequate ventilation and may or may not affect the level of sound insulation. In a narrow sense insulation can just refer to the insulation materials employed to slow heat loss, such as: cellulose, glass wool, rock wool, polystyrene, urethane foam, vermiculite, perlite, wood fibre, plant fibre (cannabis, flax, cotton, cork, etc.), recycled cotton denim, plant straw, animal fibre (sheep's wool), cement, and earth or soil, reflective insulation (also known as radiant barrier) but it can also involve a range of designs and techniques to address the main modes of heat transfer - conduction, radiation and convection materials. Many of the materials in this list deal with heat conduction and convection by the simple expedient of trapping large amounts of air (or other gas) in a way that results in a material that employs the low thermal conductivity of small pockets of gas, rather than the much higher conductivity of typical solids. (A similar gas-trapping principle is used in animal hair, down feathers, and in air-containing insulating fabrics).\n\nThe effectiveness of reflective insulation (radiant barrier) is commonly evaluated by the reflectivity (emittance) of the surface with airspace facing to the heat source.\n\nThe effectiveness of bulk insulation is commonly evaluated by its R-value, of which there are two - metric (SI) and US customary, the former being 0.176 times the latter. For attics, it is recommended that it should be at least R-38 (US customary, R-6.7 metric). However, an R-value does not take into account the quality of construction or local environmental factors for each building. Construction quality issues include inadequate vapor barriers, and problems with draft-proofing. In addition, the properties and density of the insulation material itself is critical.\n\nHow much insulation a house should have depends on building design, climate, energy costs, budget, and personal preference. Regional climates make for different requirements. Building codes often set minimum standards for fire safety and energy efficiency, which can be voluntarily exceeded within the context of sustainable architecture for green certifications such as LEED.\n\nThe insulation strategy of a building needs to be based on a careful consideration of the mode of energy transfer and the direction and intensity in which it moves. This may alter throughout the day and from season to season. It is important to choose an appropriate design, the correct combination of materials and building techniques to suit the particular situation.\n\nTo determine whether you should add insulation, you first need to find out how much insulation you already have in your home and where. A qualified home energy auditor will include an insulation check as a routine part of a whole-house energy audit. However, you can sometimes perform a self-assessment in certain areas of the home, such as attics. Here, a visual inspection, along with use of a ruler, can give you a sense of whether you may benefit from additional insulation.\n\nAn initial estimate of insulation needs in the United States can be determined by the US Department of Energy's ZIP code insulation calculator.\n\nIn Russia, the availability of abundant and cheap gas has led to poorly insulated, overheated and inefficient consumption of energy. The Russian Center for Energy Efficiency found that Russian buildings are either over- or under-heated, and often consume up to 50 percent more heat and hot water than needed. 53 percent of all carbon dioxide (CO) emissions in Russia are produced through heating and generating electricity for buildings. However, greenhouse gas emissions from the former Soviet Bloc are still below their 1990 levels.\n\nIn cold conditions, the main aim is to reduce heat flow out of the building. The components of the building envelope—windows, doors, roofs, walls, and air infiltration barriers—are all important sources of heat loss; in an otherwise well insulated home, windows will then become an important source of heat transfer. The resistance to conducted heat loss for standard glazing corresponds to an R-value of about 0.17 m⋅K⋅W (compared to 2–4 m⋅K⋅W for glass wool batts). Losses can be reduced by good weatherisation, bulk insulation, and minimising the amount of non-insulative (particularly non-solar facing) glazing. Indoor thermal radiation can also be a disadvantage with spectrally selective (low-e, low-emissivity) glazing. Some insulated glazing systems can double to triple R values.\n\nIn hot conditions, the greatest source of heat energy is solar radiation. This can enter buildings directly through windows or it can heat the building shell to a higher temperature than the ambient, increasing the heat transfer through the building envelope. The Solar Heat Gain Co-efficient (SHGC) (a measure of solar heat transmittance) of standard single glazing can be around 78-85%. Solar gain can be reduced by adequate shading from the sun, light coloured roofing, spectrally selective (heat-reflective) paints and coatings and various types of insulation for the rest of the envelope. Specially coated glazing can reduce SHGC to around 10%. Radiant barriers are highly\neffective for attic spaces in hot climates. In this application, they are much more effective in hot climates than cold climates. For downward heat flow, convection is weak and radiation dominates heat transfer across an air space. Radiant barriers must face an adequate air-gap to be effective.\n\nIf refrigerative air-conditioning is employed in a hot, humid climate, then it is particularly important to seal the building envelope. Dehumidification of humid air infiltration can waste significant energy. On the other hand, some building designs are based on effective cross-ventilation instead of refrigerative air-conditioning to provide convective cooling from prevailing breezes.\n\nOptimal placement of building elements (e.g. windows, doors, heaters) can play a significant role in insulation by considering the impact of solar radiation on the building and the prevailing breezes. Reflective laminates can help reduce passive solar heat in pole barns, garages, and metal buildings.\n\nSee insulated glass for discussion of windows.\n\nThe thermal envelope defines the conditioned or living space in a house. The attic or basement may or may not be included in this area. Reducing airflow from inside to outside can help to reduce convective heat transfer significantly.\n\nEnsuring low convective heat transfer also requires attention to building construction (weatherization) and the correct installation of insulative materials.\n\nThe less natural airflow into a building, the more mechanical ventilation will be required to support human comfort. High humidity can be a significant issue associated with lack of airflow, causing condensation, rotting construction materials, and encouraging microbial growth such as mould and bacteria. Moisture can also drastically reduce the effectiveness of insulation by creating a thermal bridge (see below). Air exchange systems can be actively or passively incorporated to address these problems.\n\nThermal bridges are points in the building envelope that allow heat conduction to occur. Since heat flows through the path of least resistance, thermal bridges can contribute to poor energy performance. A thermal bridge is created when materials create a continuous path across a temperature difference, in which the heat flow is not interrupted by thermal insulation. Common building materials that are poor insulators include glass and metal.\n\nA building design may have limited capacity for insulation in some areas of the structure. A common construction design is based on stud walls, in which thermal bridges are common in wood or steel studs and joists, which are typically fastened with metal. Notable areas that most commonly lack sufficient insulation are the corners of buildings, and areas where insulation has been removed or displaced to make room for system infrastructure, such as electrical boxes (outlets and light switches), plumbing, fire alarm equipment, etc.\n\nThermal bridges can also be created by uncoordinated construction, for example by closing off parts of external walls before they are fully insulated.\nThe existence of inaccessible voids within the wall cavity which are devoid of insulation can be a source of thermal bridging.\n\nSome forms of insulation transfer heat more readily when wet, and can therefore also form a thermal bridge in this state.\n\nThe heat conduction can be minimized by any of the following: reducing the cross sectional area of the bridges, increasing the bridge length, or decreasing the number of thermal bridges.\n\nOne method of reducing thermal bridge effects is the installation of an insulation board (e.g. foam board EPS XPS, wood fibre board, etc.) over the exterior outside wall. Another method is using insulated lumber framing for a thermal break inside the wall.\n\nInsulating buildings during construction is much easier than retrofitting, as generally the insulation is hidden, and parts of the building need to be deconstructed to reach them.\n\nThere are essentially two types of building insulation - bulk insulation and reflective insulation. Most buildings use a combination of both types to make up a total building insulation system. The type of insulation used is matched to create maximum resistance to each of the three forms of building heat transfer - conduction, convection, and radiation.\n\nBulk insulators block conductive heat transfer and convective flow either into or out of a building. The denser a material is, the better it will conduct heat. Because air has such low density, air is a very poor conductor and therefore makes a good insulator. Insulation to resist conductive heat transfer uses air spaces between fibers, inside foam or plastic bubbles and in building cavities like the attic. This is beneficial in an actively cooled or heated building, but can be a liability in a passively cooled building; adequate provisions for cooling by ventilation or radiation are needed.\n\nRadiant barriers work in conjunction with an air space to reduce radiant heat transfer across the air space. Radiant or reflective insulation reflects heat instead of either absorbing it or letting it pass through. Radiant barriers are often seen used in reducing downward heat flow, because upward heat flow tends to be dominated by convection. This means that for attics, ceilings, and roofs, they are most effective in hot climates.\nThey also have a role in reducing heat losses in cool climates. However, much greater insulation can be achieved through the addition of bulk insulators (see above).\n\nSome radiant barriers are spectrally selective and will preferentially reduce the flow of infra-red radiation in comparison to other wavelengths. For instance low-emissivity (low-e) windows will transmit light and short-wave infra-red energy into a building but reflect the long-wave infra-red radiation generated by interior furnishings. Similarly, special heat-reflective paints are able to reflect more heat than visible light, or vice versa.\n\nThermal emissivity values probably best reflect the effectiveness of radiant barriers. Some manufacturers quote an 'equivalent' R-value for these products but these figures can be difficult to interpret, or even misleading, since R-value testing measures total heat loss in a laboratory setting and does not control the type of heat loss responsible for the net result (radiation, conduction, convection).\n\nA film of dirt or moisture can alter the emissivity and hence the performance of radiant barriers.\n\nEco-friendly insulation is a term used for insulating products with limited environmental impact. The commonly accepted approach to determine whether or not an insulation products, but in fact any product or service is eco-friendly is by doing a life-cycle assessment (LCA). A number of studies compared the environmental impact of insulation materials in their application. The comparison shows that most important is the insulation value of the product meeting the technical requirements for the application. Only in a second order step a differentiation between materials becomes relevant. The report commissioned by the Belgian government to VITO is a good example of such a study. A valuable way to graphically represent such results is by a spider diagram.\n\n\n\n\n\n"}
{"id": "12637359", "url": "https://en.wikipedia.org/wiki?curid=12637359", "title": "Carbon dioxide scrubber", "text": "Carbon dioxide scrubber\n\nA carbon dioxide scrubber is a piece of equipment that absorbs carbon dioxide (CO). It is used to treat exhaust gases from industrial plants or from exhaled air in life support systems such as rebreathers or in spacecraft, submersible craft or airtight chambers. Carbon dioxide scrubbers are also used in controlled atmosphere (CA) storage. They have also been researched for carbon capture.\n\nThe primary application for CO scrubbing is for removal of from the exhaust of coal- and gas-fired power plants. Virtually the only technology being seriously evaluated involves the use of various amines, e.g. monoethanolamine. Cold solutions of these organic compounds bind CO, but the binding is reversed at higher temperatures:\n, this technology has only been lightly implemented because of capital costs of installing the facility and the operating costs of utilizing it.\n\nSeveral minerals and mineral-like materials reversibly bind CO. Most often, these minerals are oxides or hydroxides, and often the CO is bound as carbonate. Carbon dioxide reacts with quicklime (calcium oxide) to form limestone (calcium carbonate), in a process called carbonate looping. Other minerals include serpentinite, a magnesium silicate hydroxide, and olivine. Molecular sieves also function in this capacity.\n\nVarious scrubbing processes have been proposed to remove CO from the air, or from flue gases. These usually involve using a variant of the Kraft process. Scrubbing processes may be based on sodium hydroxide. The CO is absorbed into solution, sent to lime via a process called causticization and released in a kiln. With some modifications to the existing processes, mainly an oxygen-fired kiln, the end result is a concentrated stream of CO ready for storage or use in fuels. An alternative to this thermo-chemical process is an electrical one in which a nominal voltage is applied across the carbonate solution to release the CO. While simpler, this electrical process consumes more energy as it splits water at the same time. Since it depends on electricity, the electricity needs to be renewable, like PV. Otherwise the CO produced during electricity production has to be taken into account. Early incarnations of air capture used electricity as the energy source; hence, were dependent on a carbon-free source. Thermal air capture systems use heat generated on-site, which reduces the inefficiencies associated with off-site electricity production, but of course it still needs a source of (carbon-free) heat. Concentrated solar power is an example of such a source.\n\nZeman and Lackner outlined a specific method of air capture.\n\nFirst, CO is absorbed by an alkaline NaOH solution to produce dissolved sodium carbonate. The absorption reaction is a gas liquid reaction, strongly exothermic, here:\n\nCausticization is performed ubiquitously in the pulp and paper industry and readily transfers 94% of the carbonate ions from the sodium to the calcium cation. Subsequently, the calcium carbonate precipitate is filtered from solution and thermally decomposed to produce gaseous CO. The calcination reaction is the only endothermic reaction in the process and is shown here:\n\nThe thermal decomposition of calcite is performed in a lime kiln fired with oxygen in order to avoid an additional gas separation step. Hydration of the lime (CaO) completes the cycle. Lime hydration is an exothermic reaction that can be performed with water or steam. Using water, it is a liquid/solid reaction as shown here:\n\nOther strong bases such as soda lime, sodium hydroxide, potassium hydroxide, and lithium hydroxide are able to remove carbon dioxide by chemically reacting with it. In particular, lithium hydroxide was used aboard spacecraft, such as in the Apollo program, to remove carbon dioxide from the atmosphere. It reacts with carbon dioxide to form lithium carbonate. Recently lithium hydroxide absorbent technology has been adapted for use in anesthesia machines. Anesthesia machines which provide life support and inhaled agents during surgery typically employ a closed circuit necessitating the removal of carbon dioxide exhaled by the patient. Lithium hydroxide may offer some safety and convenience benefits over the older calcium based products.\n\nThe net reaction being:\n\nLithium peroxide can also be used as it absorbs more CO per unit weight with the added advantage of releasing oxygen.\n\nThe regenerative carbon dioxide removal system (RCRS) on the space shuttle orbiter used a two-bed system that provided continuous removal of carbon dioxide without expendable products. Regenerable systems allowed a shuttle mission a longer stay in space without having to replenish its sorbent canisters. Older lithium hydroxide (LiOH)-based systems, which are non-regenerable, were replaced by regenerable metal-oxide-based systems. A system based on metal oxide primarily consisted of a metal oxide sorbent canister and a regenerator assembly. It worked by removing carbon dioxide using a sorbent material and then regenerating the sorbent material. The metal-oxide sorbent canister was regenerated by pumping air at approximately through it at a standard flow rate of for 10 hours.\n\nActivated carbon can be used as a carbon dioxide scrubber. Air with high carbon dioxide content, such as air from fruit storage locations, can be blown through beds of activated carbon and the carbon dioxide will adsorb onto the activated carbon. Once the bed is saturated it must then be \"regenerated\" by blowing low carbon dioxide air, such as ambient air, through the bed. This will release the carbon dioxide from the bed, and it can then be used to scrub again, leaving the net amount of carbon dioxide in the air the same as when the process was started.\n\nMetal-organic frameworks are one of the most promising new technologies for carbon dioxide capture and sequestration via adsorption. Although no large-scale commercial technology exists nowadays, several research studies have indicated the great potential that MOFs have as a CO adsorbent. Its characteristics, such as pore structure and surface functions can be easily tuned to improve CO selectivity over other gases.\n\nA MOF could be specifically designed to act like a CO removal agent in post-combustion power plants. In this scenario, the flue gas would pass through a bed packed with a MOF material, where CO would be stripped. After saturation is reached, CO could be desorbed by doing a pressure or temperature swing. Carbon dioxide could then be compressed to supercritical conditions in order to be stored underground or utilized in enhanced oil recovery processes. However, this is not possible in large scale yet due to several difficulties, one of those being the production of MOFs in great quantities.\n\nAnother problem is the availability of metals necessary to create MOFs. In a hypothetical scenario where these materials are used to capture all CO needed to avoid global warming issues, such as maintaining a global temperature rise less than 2C above the pre-industrial average temperature, we would need more metals than are available on Earth. For example, to synthesize all MOFs that utilize vanadium, we would need 1620% of 2010 global reserves. Even if using magnesium-based MOFs, which have demonstrated a great capacity to adsorb CO, we would need 14% of 2010 global reserves, which is a considerable amount. Also, extensive mining would be necessary, leading to more potential environmental problems.\n\nIn a project sponsored by the DOE and operated by UOP LLC in collaboration with faculty from four different universities, MOFs were tested as possible carbon dioxide removal agents in post-combustion flue gas. They were able to separate 90% of the CO from the flue gas stream using a vacuum pressure swing process. Through extensive investigation, researchers found out that the best MOF to be used was Mg/DOBDC, which has a 21.7 wt% CO loading capacity. Estimations showed that, if a similar system were to be applied to a large scale power plant, the cost of energy would increase by 65%, while a NETL baseline amine based system would cause an increase of 81% (the DOE goal is 35%). Also, each ton of CO avoided would cost $57, while for the amine system this cost is estimated to be $72. The project ended in 2010,estimating that the total capital required to implement such a project in a 580 MW power plant was 354 million dollars.\n\nMany other methods and materials have been discussed for scrubbing carbon dioxide.\n\n"}
{"id": "4330211", "url": "https://en.wikipedia.org/wiki?curid=4330211", "title": "Carefree (feminine hygiene)", "text": "Carefree (feminine hygiene)\n\nCarefree is a brand of pantyliners (although originally the brand name belonged to tampons) from Johnson & Johnson. In the US Carefree brand was formerly marketed by McNeil-PPC and currently being marketed by Edgewell Personal Care (along with other US feminine hygiene brands from Johnson & Johnson).\n\"Carefree\" panty liner was introduced in 1976 (trademark registered on May 27, 1976) and by the end of 70s captured more than half of the market. It was promoted as a perfect solution for a \"fresh-dressed woman\" (tagline \"For the fresh-dressed woman\" has been developed by SSC&B advertising agency) for every day use.\n\nIn 1997 \"Carefree\" held 10% market share in the USA sanitary protection market.\n\nIn 2001 black pantyliner \"Carefree Black\" was launched.\n\nIn 2008 \"Carefree\" introduced its \"Ultra Protection\" line which has been discontinued sometime in 2012.\n\nIn 2012 the brand aired controversial TV advertising for Carefree Acti-Fresh pantyliners in New Zealand and Australia, mentioning the word \"vagina\". As soon as the ad appeared Advertising Standards Bureau received nine complaints.\n\nThe Carefree product line contains the following:\n\n\n"}
{"id": "31286534", "url": "https://en.wikipedia.org/wiki?curid=31286534", "title": "Civilité", "text": "Civilité\n\nCivilité type (in French: \"Caractères de civilité\") is a typeface invented in 1557 by the French engraver Robert Granjon. These characters imitate French cursiva letters of the Renaissance.\n\nThe first book in the new type was \"Dialogue de la vie et de la mort\", a French version of Innocenzo Ringhieri's dialogue, in the dedication of which Granjon explains his purpose in cutting the new design. He calls the typeface \"lettres françaises\" and suggests that France like other nations should have a type based on the national hand; his model was contemporary handwriting. The popular name for the type came from the titles of two early books in which it was used: Erasmus's \"La Civilité puerile\", Jean Bellère, Antwerp, 1559, and \"La Civile honesteté pour les enfans\", R. Breton, Paris, 1560. \"Civilité\" meant \"good manners\" and it was thought an advantage that children should learn to read from a book printed in a type resembling current handwriting. Between 1557 and 1562 Granjon printed some 20 books in this type. Two other Paris printers had typefaces made that were very similar and Granjon himself supplied his version to Guillaume Silvius and to Christophe Plantin at Antwerp.\n\nThey were mostly employed to print books in Flanders, Holland, England, and France. In the latter, they were used until the second half of the 19th century to print children's lesson-books teaching civility and manners from which the type got its name. Civilité type did not win great popularity in France although used occasionally at all periods. Another version of civilité were used in one book printed in 1597 by Claude Micard, and two others in two books printed by Jean de Tournes in 1581 and 1598. In the mid 19th century Louis Perrin of Lyons printed J. Soulary's \"Sonnets humouristiques\" in civilité. Granjon's experiment cannot be said to have been a success: one of the grave disadvantages was that many ligatures were required and some letters had more than one variant.\n\n"}
{"id": "4360817", "url": "https://en.wikipedia.org/wiki?curid=4360817", "title": "Common Information Model (computing)", "text": "Common Information Model (computing)\n\nThe Common Information Model (CIM) is an open standard that defines how managed elements in an IT environment are represented as a common set of objects and relationships between them. The Distributed Management Task Force maintains the CIM to allow consistent management of these managed elements, independent of their manufacturer or provider.\n\nOne way to describe CIM is to say that it allows multiple parties to exchange management information about these managed elements. However, this falls short in expressing that CIM not only represents these managed elements and the management information, but also provides means to actively control and manage these elements. By using a common model of information, management software can be written once and work with many implementations of the common model without complex and costly conversion operations or loss of information.\n\nThe CIM standard is defined and published by the Distributed Management Task Force (DMTF). A related standard is Web-Based Enterprise Management (WBEM, also defined by DMTF) which defines a particular implementation of CIM, including protocols for discovering and accessing such CIM implementations.\n\nThe CIM standard includes the CIM Infrastructure Specification and the CIM Schema:\n\n\n\nCIM is the basis for most of the other DMTF standards (e.g. WBEM or SMASH). It is also the basis for the SMI-S standard for storage management.\n\nUpdates to the CIM Schema are published regularly.\n\n\nMany vendors provide implementations of CIM in various forms:\n\n\nThere is also a growing tools market around CIM.\n\nCIM-XML is a protocol for sending CIM messages on top of HTTP. It has two message types:\n\n\nCIM-XML forms part of the WBEM protocol family, and is standardised by the DMTF.\n\nCIM-XML comprises 3 specifications:\n\n\n"}
{"id": "37838338", "url": "https://en.wikipedia.org/wiki?curid=37838338", "title": "DIN 1.0/2.3", "text": "DIN 1.0/2.3\n\nThe DIN 1.0/2.3 connector is a RF connector used for coaxial cable at microwave frequencies. They were introduced in the 1990s for telecommunication applications. They are available in 50 Ω and 75 Ω impedance and are compatible with the most widely used cable sizes. It has a push/pull lock and release feature. The DIN 1.0/2.3 is ideally suited to applications where space limitation is a factor. In broadcasting applications the 75 Ω version is used for Serial Digital Interface video data up to maximum frequency of 4 GHz. The 50 Ω connector can be used to a maximum of 10 GHz.\n\n\n"}
{"id": "25144647", "url": "https://en.wikipedia.org/wiki?curid=25144647", "title": "Embedded pavement flashing-light system", "text": "Embedded pavement flashing-light system\n\nAn embedded flashing-light system or an in-pavement flashing-light system is a type of device that is used at existing or new pedestrian crosswalks to warn drivers of oncoming pedestrian traffic. The device usually consists of LED lights that are embedded into the roadway alongside the crosswalk and are oriented to face oncoming traffic. When a pedestrian approaches the crosswalk, the system is activated and the LED lights begin to flash simultaneously. These lights are programmed to flash for a period of time that is sufficient for an average pedestrian to cross.\n\nThe concept for embedded pavement flashing light system was conceived by pilot Michael Harrison in Santa Rosa, California in 1992 after a friend was involved in a pedestrian accident. He based it on his experience with airport runway lights embedded in pavement.\n\nThere are two different types of embedded pavement flashing light systems, passive and active. These types differ on how the system is activated.\n\nWith a passive system, the pedestrian activates the device merely by walking up to the crosswalk. This is accomplished by using one of several motion detection devices. These include microwave, motion sensors, video detection, pressure plates, or a light trip beam. With an active system, the device is usually activated by a button that a pedestrian pushes in order to cross. These active systems are generally similar to lighted pedestrian signs at traffic intersections. Because many pedestrians may not realize that they need to press a button to activate the system, it is generally recommended to install a passive system.\n\nCompared with other types of warning devices, the effectiveness of the embedded pavement flashing light system seem to be high. When approaching a crosswalk with an embedded pavement flashing light system, drivers are more apt to slow down and yield to pedestrians than when drivers approach a crosswalk with another type of lighted warning device. Also, compared to a crosswalk with no warning device, drivers are more likely to slow down and yield to pedestrians when the embedded pavement flashing light system is in place.\n\n"}
{"id": "33413050", "url": "https://en.wikipedia.org/wiki?curid=33413050", "title": "Federated Millers and Mill Employees' Association of Australasia", "text": "Federated Millers and Mill Employees' Association of Australasia\n\nThe Federated Millers and Mill Employees' Association of Australasia (MEA) was an Australian trade union which existed between 1911 and 1988. The union represented workers employed in milling grain.\n\nThe Federation (originally the Federated Millers and Mill Employes' Association) was formed as a result of a meeting in Adelaide in May 1910. At the first annual meeting held in Melbourne in March 1911, the following officers were elected: President, Mr. T. Drum (N.S.W.); vice-president, Mr. W. Bain (S.A.); general secretary, Mr. G. Lewis (N.S.W.); treasurer, Mr. J. Nealer (Vic.); trustees, Frank Condon (S.A.) and J. Kebble. Apologies were received from the Western Australian branch.\n\nIn 1988 the M.E.A. amalgamated with the Manufacturing Grocers' Employees' Federation of Australia to form the Federated Millers and Manufacturing Grocers Union. This union then merged shortly after into the recently formed National Union of Workers.\n\n\n"}
{"id": "24325138", "url": "https://en.wikipedia.org/wiki?curid=24325138", "title": "Flywheel energy storage", "text": "Flywheel energy storage\n\nFlywheel energy storage (FES) works by accelerating a rotor (flywheel) to a very high speed and maintaining the energy in the system as rotational energy. When energy is extracted from the system, the flywheel's rotational speed is reduced as a consequence of the principle of conservation of energy; adding energy to the system correspondingly results in an increase in the speed of the flywheel.\n\nMost FES systems use electricity to accelerate and decelerate the flywheel, but devices that directly use mechanical energy are being developed.\n\nAdvanced FES systems have rotors made of high strength carbon-fiber composites, suspended by magnetic bearings, and spinning at speeds from 20,000 to over 50,000 rpm in a vacuum enclosure. Such flywheels can come up to speed in a matter of minutes – reaching their energy capacity much more quickly than some other forms of storage.\n\nA typical system consists of a flywheel supported by rolling-element bearing connected to a motor–generator. The flywheel and sometimes motor–generator may be enclosed in a vacuum chamber to reduce friction and reduce energy loss.\n\nFirst-generation flywheel energy-storage systems use a large steel flywheel rotating on mechanical bearings. Newer systems use carbon-fiber composite rotors that have a higher tensile strength than steel and can store much more energy for the same mass.\n\nTo reduce friction, magnetic bearings are sometimes used instead of mechanical bearings.\n\nThe expense of refrigeration led to the early dismissal of low-temperature superconductors for use in magnetic bearings. However, high-temperature superconductor (HTSC) bearings may be economical and could possibly extend the time energy could be stored economically. Hybrid bearing systems are most likely to see use first. High-temperature superconductor bearings have historically had problems providing the lifting forces necessary for the larger designs, but can easily provide a stabilizing force. Therefore, in hybrid bearings, permanent magnets support the load and high-temperature superconductors are used to stabilize it. The reason superconductors can work well stabilizing the load is because they are perfect diamagnets. If the rotor tries to drift off center, a restoring force due to flux pinning restores it. This is known as the magnetic stiffness of the bearing. Rotational axis vibration can occur due to low stiffness and damping, which are inherent problems of superconducting magnets, preventing the use of completely superconducting magnetic bearings for flywheel applications.\n\nSince flux pinning is an important factor for providing the stabilizing and lifting force, the HTSC can be made much more easily for FES than for other uses. HTSC powders can be formed into arbitrary shapes so long as flux pinning is strong. An ongoing challenge that has to be overcome before superconductors can provide the full lifting force for an FES system is finding a way to suppress the decrease of levitation force and the gradual fall of rotor during operation caused by the flux creep of the superconducting material.\n\nCompared with other ways to store electricity, FES systems have long lifetimes (lasting decades with little or no maintenance; full-cycle lifetimes quoted for flywheels range from in excess of 10, up to 10, cycles of use), high specific energy (100–130 W·h/kg, or 360–500 kJ/kg), and large maximum power output. The energy efficiency (\"ratio of energy out per energy in\") of flywheels, also known as round-trip efficiency, can be as high as 90%. Typical capacities range from 3 kWh to 133 kWh. Rapid charging of a system occurs in less than 15 minutes. The high specific energies often cited with flywheels can be a little misleading as commercial systems built have much lower specific energy, for example 11 W·h/kg, or 40 kJ/kg.\n\nHere formula_1 is the integral of the flywheel's mass, and formula_2 is the rotational speed (number of revolutions per second).\n\nThe maximal specific energy of a flywheel rotor is mainly dependent on two factors: the first being the rotor's geometry, and the second being the properties of the material being used. For single-material, isotropic rotors this relationship can be expressed as\n\nwhere\n\nThe highest possible value for the shape factor of a flywheel rotor, is formula_9, which can only be achieved by the theoretical \"constant-stress disc\" geometry. A constant-thickness disc geometry has a shape factor of formula_10, while for a rod of constant thickness the value is formula_11. A thin cylinder has a shape factor of formula_12.\n\nFor energy storage, materials with high strength and low density are desirable. For this reason, composite materials are frequently used in advanced flywheels. The strength-to-density ratio of a material can be expressed in Wh/kg (or Nm/kg); values greater than 400 Wh/kg can be achieved by certain composite materials.\n\nSeveral modern flywheel rotors are made from composite materials. Examples include the carbon-fiber composite flywheel from Beacon Power Corporation and the \"PowerThru\" flywheel from Phillips Service Industries. Alternatively, Calnetix utilizes aerospace-grade high-performance steel in their flywheel construction.\n\nFor these rotors, the relationship between material properties, geometry and energy density can be expressed by using a weighed-average approach.\n\nOne of the primary limits to flywheel design is the tensile strength of the rotor. Generally speaking, the stronger the disc, the faster it may be spun, and the more energy the system can store.\n\nWhen the tensile strength of a composite flywheel's outer binding cover is exceeded, the binding cover will fracture, and the wheel will shatter as the outer wheel compression is lost around the entire circumference, releasing all of its stored energy at once; this is commonly referred to as \"flywheel explosion\" since wheel fragments can reach kinetic energy comparable to that of a bullet. Composite materials that are wound and glued in layers tend to disintegrate quickly, first into small-diameter filaments that entangle and slow each other, and then into red-hot powder; a cast metal flywheel throws off large chunks of high-speed shrapnel.\n\nFor a cast metal flywheel, the failure limit is the binding strength of the grain boundaries of the polycrystalline molded metal. Aluminum in particular suffers from fatigue and can develop microfractures from repeated low-energy stretching. Angular forces may cause portions of a metal flywheel to bend outward and begin dragging on the outer containment vessel, or to separate completely and bounce randomly around the interior. The rest of the flywheel is now severely unbalanced, which may lead to rapid bearing failure from vibration, and sudden shock fracturing of large segments of the flywheel.\n\nTraditional flywheel systems require strong containment vessels as a safety precaution, which increases the total mass of the device. The energy release from failure can be dampened with a gelatinous or encapsulated liquid inner housing lining, which will boil and absorb the energy of destruction. Still, many customers of large-scale flywheel energy-storage systems prefer to have them embedded in the ground to halt any material that might escape the containment vessel.\n\nFlywheel energy storage systems using mechanical bearings can lose 20% to 50% of their energy in two hours. Much of the friction responsible for this energy loss results from the flywheel changing orientation due to the rotation of the earth (an effect similar to that shown by a Foucault pendulum). This change in orientation is resisted by the gyroscopic forces exerted by the flywheel's angular momentum, thus exerting a force against the mechanical bearings. This force increases friction. This can be avoided by aligning the flywheel's axis of rotation parallel to that of the earth's axis of rotation.\n\nConversely, flywheels with magnetic bearings and high vacuum can maintain 97% mechanical efficiency, and 85% round trip efficiency.\n\nWhen used in vehicles, flywheels also act as gyroscopes, since their angular momentum is typically of a similar order of magnitude as the forces acting on the moving vehicle. This property may be detrimental to the vehicle's handling characteristics while turning or driving on rough ground; driving onto the side of a sloped embankment may cause wheels to partially lift off the ground as the flywheel opposes sideways tilting forces. On the other hand, this property could be utilized to keep the car balanced so as to keep it from rolling over during sharp turns.\n\nWhen a flywheel is used entirely for its effects on the attitude of a vehicle, rather than for energy storage, it is called a reaction wheel or a control moment gyroscope.\n\nThe resistance of angular tilting can be almost completely removed by mounting the flywheel within an appropriately applied set of gimbals, allowing the flywheel to retain its original orientation without affecting the vehicle (see \"Properties\" of a gyroscope). This doesn't avoid the complication of gimbal lock, and so a compromise between the number of gimbals and the angular freedom is needed.\n\nThe center axle of the flywheel acts as a single gimbal, and if aligned vertically, allows for the 360 degrees of yaw in a horizontal plane. However, for instance driving up-hill requires a second pitch gimbal, and driving on the side of a sloped embankment requires a third roll gimbal.\n\nAlthough the flywheel itself may be of a flat ring shape, a free-movement gimbal mounting inside a vehicle requires a spherical volume for the flywheel to freely rotate within. Left to its own, a spinning flywheel in a vehicle would slowly precess following the Earth's rotation, and precess further yet in vehicles that travel long distances over the Earth's curved spherical surface.\n\nA full-motion gimbal has additional problems of how to communicate power into and out of the flywheel, since the flywheel could potentially flip completely over once a day, precessing as the Earth rotates. Full free rotation would require slip rings around each gimbal axis for power conductors, further adding to the design complexity.\n\nTo reduce space usage, the gimbal system may be of a limited-movement design, using shock absorbers to cushion sudden rapid motions within a certain number of degrees of out-of-plane angular rotation, and then gradually forcing the flywheel to adopt the vehicle's current orientation. This reduces the gimbal movement space around a ring-shaped flywheel from a full sphere, to a short thickened cylinder, encompassing for example ± 30 degrees of pitch and ± 30 degrees of roll in all directions around the flywheel.\n\nAn alternative solution to the problem is to have two joined flywheels spinning synchronously in opposite directions. They would have a total angular momentum of zero and no gyroscopic effect. A problem with this solution is that when the difference between the momentum of each flywheel is anything other than zero the housing of the two flywheels would exhibit torque. Both wheels must be maintained at the same speed to keep the angular velocity at zero. Strictly speaking, the two flywheels would exert a huge torqueing moment at the central point, trying to bend the axle. However, if the axle were sufficiently strong, no gyroscopic forces would have a net effect on the sealed container, so no torque would be noticed.\n\nTo further balance the forces and spread out strain, a single large flywheel can be balanced by two half-size flywheels on each side, or the flywheels can be reduced in size to be a series of alternating layers spinning in opposite directions. However this increases housing and bearing complexity.\n\nIn the 1950s, flywheel-powered buses, known as gyrobuses, were used in Yverdon (Switzerland) and Ghent (Belgium) and there is ongoing research to make flywheel systems that are smaller, lighter, cheaper and have a greater capacity. It is hoped that flywheel systems can replace conventional chemical batteries for mobile applications, such as for electric vehicles. Proposed flywheel systems would eliminate many of the disadvantages of existing battery power systems, such as low capacity, long charge times, heavy weight and short usable lifetimes. Flywheels may have been used in the experimental Chrysler Patriot, though that has been disputed.\n\nFlywheels have also been proposed for use in continuously variable transmissions. Punch Powertrain is currently working on such a device.\n\nDuring the 1990s, Rosen Motors developed a gas turbine powered series hybrid automotive powertrain using a 55,000 rpm flywheel to provide bursts of acceleration which the small gas turbine engine could not provide. The flywheel also stored energy through regenerative braking. The flywheel was composed of a titanium hub with a carbon fiber cylinder and was gimbal-mounted to minimize adverse gyroscopic effects on vehicle handling. The prototype vehicle was successfully road tested in 1997 but was never mass-produced.\n\nIn 2013, Volvo announced a flywheel system fitted to the rear axle of its S60 sedan. Braking action spins the flywheel at up to 60,000 rpm and stops the front-mounted engine. Flywheel energy is applied via a special transmission to partially or completely power the vehicle. The , carbon fiber flywheel spins in a vacuum to eliminate friction. When partnered with a four-cylinder engine, it offers up to a 25 percent reduction in fuel consumption versus a comparably performing turbo six-cylinder, providing an 80 hp boost and allowing it to reach in 5.5 seconds. The company did not announce specific plans to include the technology in its product line.\n\nIn July 2014 GKN acquired Williams Hybrid Power (WHP) division and intends to supply 500 carbon fiber \"Gyrodrive\" electric flywheel systems to urban bus operators over the next two years As the former developer name implies, these were originally designed for Formula one motor racing applications. In September 2014, Oxford Bus Company announced that it is introducing 14 \"Gyrodrive hybrid\" buses by Alexander Dennis on its Brookes Bus operation.\n\nFlywheel systems have been used experimentally in small electric locomotives for shunting or switching, e.g. the Sentinel-Oerlikon Gyro Locomotive. Larger electric locomotives, e.g. British Rail Class 70, have sometimes been fitted with flywheel boosters to carry them over gaps in the third rail. Advanced flywheels, such as the 133 kWh pack of the University of Texas at Austin, can take a train from a standing start up to cruising speed.\n\nThe Parry People Mover is a railcar which is powered by a flywheel. It was trialled on Sundays for 12 months on the Stourbridge Town Branch Line in the West Midlands, England during 2006 and 2007 and was intended to be introduced as a full service by the train operator London Midland in December 2008 once two units had been ordered. In January 2010, both units are in operation.\n\nFES can be used at the lineside of electrified railways to help regulate the line voltage thus improving the acceleration of unmodified electric trains and the amount of energy recovered back to the line during regenerative braking, thus lowering energy bills. Trials have taken place in London, New York, Lyon and Tokyo, and New York MTA's Long Island Rail Road is now investing $5.2m in a pilot project on LIRR's West Hempstead Branch line.\nThese trials and systems store kinetic energy in rotors consisting of a carbon-glass composite cylinder packed with neodymium-iron-boron powder that forms a permanent magnet. These spin at up to 37800rev/min, and each 100 kW unit can store 11 MJ of re-usable energy.\n\nFlywheel power storage systems in production have storage capacities comparable to batteries and faster discharge rates. They are mainly used to provide load leveling for large battery systems, such as an uninterruptible power supply for data centers as they save a considerable amount of space compared to battery systems.\n\nFlywheel maintenance in general runs about one-half the cost of traditional battery UPS systems. The only maintenance is a basic annual preventive maintenance routine and replacing the bearings every five to ten years, which takes about four hours. Newer flywheel systems completely levitate the spinning mass using maintenance-free magnetic bearings, thus eliminating mechanical bearing maintenance and failures.\n\nCosts of a fully installed flywheel UPS (including power conditioning) are (in 2009) about $330 per kilowatt (for 15 seconds full-load capacity).\n\nA long-standing niche market for flywheel power systems are facilities where circuit breakers and similar devices are tested: even a small household circuit breaker may be rated to interrupt a current of or more amperes, and larger units may have interrupting ratings of or amperes. The enormous transient loads produced by deliberately forcing such devices to demonstrate their ability to interrupt simulated short circuits would have unacceptable effects on the local grid if these tests were done directly from building power. Typically such a laboratory will have several large motor–generator sets, which can be spun up to speed over several minutes; then the motor is disconnected before a circuit breaker is tested.\n\nTokamak fusion experiments need very high currents for brief intervals (mainly to power large electromagnets for a few seconds). \n\nAlso the non-tokamak: Nimrod synchrotron at the Rutherford Appleton Laboratory had two 30 ton flywheels.\n\nThe \"Gerald R. Ford\"-class aircraft carrier will use flywheels to accumulate energy from the ship's power supply, for rapid release into the electromagnetic aircraft launch system. The shipboard power system cannot on its own supply the high power transients necessary to launch aircraft. Each of four rotors will store 121 MJ at 6400 rpm. They can store 122 MJ in 45 secs and release it in 2–3 seconds. The flywheel energy densities are 28 kJ/kg; including the stators and cases this comes down to 18.1 kJ/kg, excluding the torque frame.\n\nThis was a design funded by NASA's Glenn Research Center and intended for component testing in a laboratory environment. It used a carbon fiber rim with a titanium hub designed to spin at 60,000 rpm, mounted on magnetic bearings. Weight was limited to 250 pounds. Storage was 525 W-hr (1.89 MJ) and could be charged or discharged at 1 kW. The working model shown in the photograph at the top of the page ran at 41,000 rpm on September 2, 2004.\n\nThe Incredible Hulk roller coaster at Universal's Islands of Adventure features a rapidly accelerating uphill launch as opposed to the typical gravity drop. This is achieved through powerful traction motors that throw the car up the track. To achieve the brief very high current required to accelerate a full coaster train to full speed uphill, the park utilizes several motor generator sets with large flywheels. Without these stored energy units, the park would have to invest in a new substation or risk browning-out the local energy grid every time the ride launches.\n\nFlywheel Energy Storage Systems (FESS) are found in a variety of applications ranging from grid-connected energy management to uninterruptible power supplies. With the progress of technology, there is fast renovation involved in FESS application. Examples include high power weapons, aircraft powertrains and shipboard power systems, where the system requires a very high-power for a short period in order of a few seconds and even milliseconds.\nCompensated pulsed alternator (compulsator) is one of the most popular choices of pulsed power supplies for fusion reactors, high-power pulsed lasers, and hypervelocity electromagnetic launchers because of its high energy density and power density, which is generally designed for the FESS. \nCompulsators (low-inductance alternators) act like capacitors, they can be spun up to provide pulsed power for railguns and lasers. Instead of having a separate flywheel and generator, only the large rotor of the alternator stores energy. See also Homopolar generator.\n\nUsing a continuously variable transmission (CVT), energy is recovered from the drive train during braking and stored in a flywheel. This stored energy is then used during acceleration by altering the ratio of the CVT. In motor sports applications this energy is used to improve acceleration rather than reduce carbon dioxide emissionsalthough the same technology can be applied to road cars to improve fuel efficiency.\n\nAutomobile Club de l'Ouest, the organizer behind the annual 24 Hours of Le Mans event and the Le Mans Series, is currently \"studying specific rules for LMP1 which will be equipped with a kinetic energy recovery system.\"\n\nWilliams Hybrid Power, a subsidiary of Williams F1 Racing team, have supplied Porsche and Audi with flywheel based hybrid system for Porsche's 911 GT3 R Hybrid and Audi's R18 e-Tron Quattro. Audi's victory in 2012 24 Hours of Le Mans is the first for a hybrid(diesel-electric) vehicle.\n\nFlywheels are sometimes used as short term spinning reserve for momentary grid frequency regulation and balancing sudden changes between supply and consumption. No carbon emissions, faster response times and ability to buy power at off-peak hours are among the advantages of using flywheels instead of traditional sources of energy like natural gas turbines. Operation is very similar to batteries in the same application, their differences are primarily economic.\n\nBeacon Power opened a 5 MWh (20 MW over 15 mins) flywheel energy storage plant in Stephentown, New York in 2011 using 200 flywheels and a similar 20 MW system at Hazle Township, Pennsylvania in 2014.\n\nA 2 MW (for 15 min) flywheel storage facility in Minto, Ontario, Canada opened in 2014. The flywheel system (developed by NRStor) uses 10 spinning steel flywheels on magnetic bearings.\n\nAmber Kinetics, Inc. has an agreement with Pacific Gas and Electric (PG&E) for a 20 MW / 80 MWh flywheel energy storage facility located in Fresno, CA with a four-hour discharge duration.\n\nFlywheels may be used to store energy generated by wind turbines during off-peak periods or during high wind speeds.\n\nBeacon Power began testing of their Smart Energy 25 (Gen 4) flywheel energy storage system at a wind farm in Tehachapi, California. The system is part of a wind power/flywheel demonstration project being carried out for the California Energy Commission.\n\nFriction motors used to power many toy cars, trucks, trains, action toys and such, are simple flywheel motors.\n\nIn industry, toggle action presses are still popular. The usual arrangement involves a very strong crankshaft and a heavy duty connecting rod which drives the press. Large and heavy flywheels are driven by electric motors but the flywheels only turn the crankshaft when clutches are activated.\n\nFlywheels are not as adversely affected by temperature changes, can operate at a much wider temperature range, and are not subject to many of the common failures of chemical rechargeable batteries. They are also less potentially damaging to the environment, being largely made of inert or benign materials. Another advantage of flywheels is that by a simple measurement of the rotation speed it is possible to know the exact amount of energy stored.\n\nUnlike most batteries which only operate for a finite period (for example roughly 36 months in the case of lithium ion polymer batteries), a flywheel potentially has an indefinite working lifespan. Flywheels built as part of James Watt steam engines have been continuously working for more than two hundred years. Working examples of ancient flywheels used mainly in milling and pottery can be found in many locations in Africa, Asia, and Europe.\n\nMost modern flywheels are typically sealed devices that need minimal maintenance throughout their service lives. Magnetic bearing flywheels in vacuum enclosures, such as the NASA model depicted above, do not need any bearing maintenance and are therefore superior to batteries both in terms of total lifetime and energy storage capacity. Flywheel systems with mechanical bearings will have limited lifespans due to wear.\n\nThe physical arrangement of batteries can be designed to match a wide variety of configurations, whereas a flywheel at a minimum must occupy a fixed square surface area and volume. Where dimensions are a constraint for the application of an energy storage technology, (e.g. under the chassis of a train in a tunnel), a flywheel may not be a viable solution.\n\n\n"}
{"id": "376774", "url": "https://en.wikipedia.org/wiki?curid=376774", "title": "Gas mantle", "text": "Gas mantle\n\nAn incandescent gas mantle, gas mantle or Welsbach mantle is a device for generating bright white light when heated by a flame. The name refers to its original heat source in gas lights, which filled the streets of Europe and North America in the late 19th century, mantle referring to the way it is hung above the flame. Today it is still used in portable camping lanterns, pressure lanterns and some oil lamps.\n\nGas mantles are usually sold as fabric items, which, because of impregnation with metal nitrates, form a rigid but fragile mesh of metal oxides when heated during initial use; these metal oxides produce light from the heat of the flame whenever used. Thorium dioxide was commonly a major component; being radioactive, it has led to concerns about the safety of those involved in manufacturing mantles. Normal use, however, poses minimal health risk.\n\nThe mantle is a roughly pear-shaped fabric bag, made from silk, ramie-based artificial silk, or rayon. The fibers are impregnated with rare-earth metallic salts; when the mantle is heated in a flame, the fibers burn away, and the metallic salts convert to solid oxides, forming a brittle ceramic shell in the shape of the original fabric. A mantle glows brightly in the visible spectrum while emitting little infrared radiation. The rare-earth oxides (cerium) and actinide (thorium) in the mantle have a low emissivity in the infrared (in comparison with an ideal black body) but have high emissivity in the visible spectrum. There is also some evidence that the emission is enhanced by candoluminescence, the emission of light from the combustion products before they reach thermal equilibrium. The combination of these properties yields a mantle that, when heated by a kerosene or liquified petroleum gas flame, emits intense radiation that is mostly visible light, with relatively little energy in the unwanted infrared, increasing the luminous efficiency.\n\nThe mantle aids the combustion process by keeping the flame small and contained inside itself at higher fuel flow rates than in a simple lamp. This concentration of combustion inside the mantle improves the transfer of heat from the flame to the mantle. The mantle shrinks after all the fabric material has burned away and becomes very fragile after its first use.\n\nFor centuries, artificial light has been generated using open flames. Limelight was invented in the 1820s, but the temperature required to produce visible light through black-body radiation alone was too high to be practical for small lights. In the late 19th century several inventors tried to develop an effective alternative based on heating a material to a lower temperature but using the emission of discrete spectral lines to simulate white light.\n\nMany early attempts used platinum-iridium gauze soaked in metal nitrates, but these were not successful because of the high cost of these materials and their poor reliability. The first effective mantle was the Clamond basket in 1881, named after its inventor. This device was made from a cleverly produced matrix of magnesium oxide, which did not need to be supported by a platinum wire cage, and was exhibited in the Crystal Palace exhibition of 1883.\n\nThe modern gas mantle was one of the many inventions of Carl Auer von Welsbach, a chemist who studied rare-earth elements in the 1880s and who had been Robert Bunsen's student. Ignaz Kreidl worked with him on his early experiments to create the Welsbach mantle. His first process used a mixture of 60% magnesium oxide, 20% lanthanum oxide and 20% yttrium oxide, which he called \"Actinophor\" and patented in 1885. These original mantles gave off a green-tinted light and were not very successful. Carl Auer von Welsbach's first company established a factory in Atzgersdorf in 1887, but it failed in 1889. In 1890 he discovered that thorium was superior to magnesium, and in 1891 he perfected a new mixture of 99% thorium dioxide and 1% cerium dioxide that gave off a much whiter light and produced a stronger mantle. After introducing this new mantle commercially in 1892, it quickly spread throughout Europe. The gas mantle remained an important part of street lighting until the widespread introduction of electric lighting in the early 1900s.\n\nTo produce a mantle, cotton is woven or knit into a net bag, impregnated with soluble nitrates of the chosen metals and then heated; the cotton burns away, and the nitrates are converted to nitrites, which fuse together to form a solid mesh. As the heating continues, the nitrites finally decompose into a fragile mesh of solid oxides of very high melting point.\n\nEarly mantles were sold in the unheated cotton mesh condition, since the oxide structure was too fragile to transport easily. The mantle was converted to working form when the cotton burned away on first use. Unused mantles could not be stored for very long, since the cotton quickly rotted due to the corrosive nature of the acidic metal nitrates, a problem that was later addressed by soaking the mantle in an ammonia solution to neutralize the excess acid.\n\nLater mantles were made from guncotton (nitrocellulose) or collodion rather than ordinary cotton, since extremely fine threads of this material could be produced, but it had to be converted back to cellulose by immersion in ammonium sulfide before first use, since guncotton is highly flammable and can be explosive. Later, it was discovered that a cotton mantle could be strengthened sufficiently by dipping it in a solution of collodion, which coated it with a thin layer, which would be burned off when the mantle was first used.\n\nMantles have a binding thread to tie them to the lamp fitting. Until it was banned due to its carcinogenicity, asbestos thread was used; modern mantles use a wire or a ceramic fiber thread.\n\nThorium is radioactive and produces the radioactive gas radon-220 as one of its decay products. Moreover, when heated to incandescence, the thorium volatilizes its in-growth Radio-daughters, particularly radium-224. Despite its very short half-life, radium quickly replenishes from its radio-parent (thorium-228), and every new heating of the mantle to incandescence releases a fresh flush of radium-224 into the air. This byproduct can be inhaled if the mantle is being used indoors, and is an internal alpha-emitter radio-toxicity concern. Secondary decay products of thorium include radium and actinium. Because of this, there are concerns about the safety of thorium mantles. Some nuclear safety agencies make recommendations about their use. \n\nA study in 1981 estimated that the dose from using a thorium mantle every weekend for a year would be , tiny in comparison to the normal annual background radiation dose of around . A person actually ingesting a mantle would receive a dose of . However, the radioactivity is a major concern for people involved with the manufacture of mantles and an issue with contamination of soil around some former factory sites. \n\nOne potential cause for concern is that particles from thorium gas mantles \"fall out\" over time and get into the air, where they may be ingested in food or drink. These particles may also be inhaled and remain in the lungs or liver, causing long-term exposure. Also of concern is the release of thorium-bearing dust if the mantle shatters due to mechanical impact.\n\nAll of these issues have led to the use of alternatives in some countries, usually yttrium or sometimes zirconium, although these are usually either more expensive or less efficient. Safety concerns were the subject of a federal suit against the Coleman Company (\"Wagner v. Coleman\"), which initially agreed to place warning labels on the mantles for this concern, and subsequently switched to using yttrium.\n\nIn June 2001 the NUREG published a study about the \"Systematic Radiological Assessment of Exemptions for Source and Byproduct Materials\" stating that radioactive gas mantles are explicitly legal in the US.\n\n\n"}
{"id": "5252915", "url": "https://en.wikipedia.org/wiki?curid=5252915", "title": "Hamper", "text": "Hamper\n\nA hamper refers to one of several related basket-like items. In primarily British usage, it refers to a wicker basket, usually large, that is used for the transport of items, often food. In North America, the term generally refers to a household receptacle, often a basket, for clean (out of the dryer or off the line) or dirty clothing, regardless of its composition, i.e. \"a laundry hamper\". Typically a laundry hamper is used for storage and will be sturdier, taller and have a lid while a laundry basket is open and used mainly for transport.\n\nIn agricultural use, a hamper is a wide-mouthed container of basketwork that may often be carried on the back during the harvesting of fruit or vegetables by hand by workers in the field. The contents of the hamper may be decanted regularly into larger containers or a cart, wagon, or truck.\n\nThe open ventilation and the sturdiness offered by a hamper has made it suitable for the transport of food, hence the use of the picnic hamper.\n\nAt one time it was common for laundry services to leave a large basketwork container with a lid which is now commonly referred to as a clothes hamper. The same type of container would be used to return clean clothing, which would be put away by the laundry service and the empty container left in place of the full container for later pickup. This type of daily or bi-daily hamper service was most common with Chinese laundry services in 19th-century England and America.\n\nThere is a long tradition of community and social philanthropy and charity related to hampers, in which persons or community groups donate to needy people a hamper of food, clothing, toiletries, cleaning products, or other household necessities, to assist with their family economy.\n\nUp until the mid 20th century, in the Western tradition, the hamper was a basket that could be carried by the donor and physically handed to the recipient. This limited the size of the gift to food ingredients for at most several days, or other necessities for one to two weeks. The basket itself was a useful item around the house or farm, and any cloth wrapping for the food or lining of the basket would also be usable by the recipient family.\n\nIn more recent times, the hamper would likely be a plastic bag or acrylic fibre bag of a size that can be carried, with tinned or packaged goods. A Christmas hamper is likely to be bigger and have some party or celebratory foods, or toys. Hampers can also contain related festive foods.\n\nA Christmas hamper is a traditional gift containing small nonperishable food items, particularly seasonal favorites such as fruitcake or plum pudding, chocolate, nuts, jams, biscuits, honey, smoked or dried meats, and cheese. Some hampers containing tea, coffee, or cocoa might also include a cup and saucer, often seasonally themed or personalized. Luxury hampers may also contain high-end items such as tins of caviar or small bottles of wine. A \"fresh hamper\" contains perishable items such as fruits, baked goods, or flowers. The tradition of the Christmas hamper may be intended as a special holiday meal for people who might otherwise have no memorable meal to mark the occasion, or for people such as students or shut-ins who are unable to join their families for Christmas.\n\nIn the US, the Christmas hamper is more usually called a gift basket and does not necessarily contain food items. Non-food gift baskets are frequently themed, such as baskets containing luxury bath items including scented soaps and towels, or beauty baskets with skincare products, perfumes, or lotions. These gift baskets are also popular for occasions other than Christmas.\n\nA number of companies exist that sell ready-made food hampers or provide customers the service of compiling a custom hamper for them, which may involve company staff going shopping for requested specific items on behalf of individual customers. Such hampers are popular gift items in the UK and Ireland. Hamper companies usually link their services to certain occasions, most particularly Christmas, and provide options for payment across the year. Grocers, delis and supermarkets may also stock ready-made hampers, though mostly just on a seasonal basis, and with a selection generally limited to items stocked by the store or sourced from their own suppliers.\n\nRecently dietary hamper companies have sprung up which provide sufferers of diseases such as diabetes or those intolerant to foods such as gluten a selection of food which is suitable.\n\n"}
{"id": "1259296", "url": "https://en.wikipedia.org/wiki?curid=1259296", "title": "Head-mounted display", "text": "Head-mounted display\n\nA head-mounted display (or \"helmet-mounted display\", for aviation applications), both abbreviated HMD, is a display device, worn on the head or as part of a helmet, that has a small display optic in front of one (monocular HMD) or each eye (binocular HMD). A HMD has many uses, including in gaming, aviation, engineering, and medicine.\n\nThere is also an optical head-mounted display (OHMD), which is a wearable display that can reflect projected images and allows a user to see through it.\n\nA typical HMD has one or two small displays, with lenses and semi-transparent mirrors embedded in eyeglasses (also termed \"data glasses\"), a visor, or a helmet. The display units are miniaturised and may include cathode ray tubes (CRT), liquid crystal displays (LCDs), liquid crystal on silicon (LCos), or organic light-emitting diodes (OLED). Some vendors employ multiple micro-displays to increase total resolution and field of view.\n\nHMDs differ in whether they can display only computer-generated imagery (CGI), or only live imagery from the physical world, or combination. Most HMDs can display only a computer-generated image, sometimes referred to as virtual image. Some HMDs can allow a CGI to be superimposed on real-world view. This is sometimes referred to as augmented reality or mixed reality. Combining real-world view with CGI can be done by projecting the CGI through a partially reflective mirror and viewing the real world directly. This method is often called \"optical see-through\". Combining real-world view with CGI can also be done electronically by accepting video from a camera and mixing it electronically with CGI. This method is often called \"video see-through\".\n\nAn optical head-mounted display uses an optical mixer which is made of partly silvered mirrors. It can reflect artificial images, and let real images cross the lens, and let a user look through it.\n\nVarious methods have existed for see-through HMD's, most of which can be summarized into two main families based on \"curved mirrors\" or \"waveguides\". Curved mirrors have been used by Laster Technologies, and by Vuzix in their Star 1200 product. Various waveguide methods have existed for years. These include diffraction optics, holographic optics, polarized optics, and reflective optics.\n\nMajor HMD applications include military, government (fire, police, etc.), and civilian-commercial (medicine, video gaming, sports, etc.).\n\nIn 1962, Hughes Aircraft Company revealed the Electrocular, a compact CRT(7\" long), head-mounted monocular display that reflected a TV signal in to transparent eyepiece.\n\nRuggedized HMDs are increasingly being integrated into the cockpits of modern helicopters and fighter aircraft. These are usually fully integrated with the pilot's flying helmet and may include protective visors, night vision devices, and displays of other symbology.\n\nMilitary, police, and firefighters use HMDs to display tactical information such as maps or thermal imaging data while viewing a real scene. Recent applications have included the use of HMD for paratroopers. In 2005, the Liteye HMD was introduced for ground combat troops as a rugged, waterproof lightweight display that clips into a standard US PVS-14 military helmet mount. The self-contained color monocular organic light-emitting diode (OLED) display replaces the NVG tube and connects to a mobile computing device. The LE has see-through ability and can be used as a standard HMD or for augmented reality applications. The design is optimized to provide high definition data under all lighting conditions, in covered or see-through modes of operation. The LE has a low power consumption, operating on four AA batteries for 35 hours or receiving power via standard Universal Serial Bus (USB) connection.\n\nThe Defense Advanced Research Projects Agency (DARPA) continues to fund research in augmented reality HMDs as part of the Persistent Close Air Support (PCAS) Program. Vuzix is currently working on a system for PCAS that will use holographic waveguides to produce see-through augmented reality glasses that are only a few millimeters thick.\n\nEngineers and scientists use HMDs to provide stereoscopic views of computer-aided design (CAD) schematics. Virtual reality, when applied to engineering and design, is a key factor in integration of the human in the design. By enabling engineers to interact with their designs in full life-size scale, products can be validated for issues that may not have been visible until physical prototyping. The use of HMDs for VR is seen as supplemental to the conventional use of CAVE for VR simulation. HMDs are predominantly used for single-person interaction with the design, while CAVEs allow for more collaborative virtual reality sessions.\n\nHead Mounted Display systems are also used in the maintenance of complex systems, as they can give a technician a simulated \"x-ray vision\" by combining computer graphics such as system diagrams and imagery with the technician's natural vision (augmented or modified reality).\n\nThere are also applications in surgery, wherein a combination of radiographic data (X-ray computed tomography (CAT) scans, and magnetic resonance imaging (MRI) imaging) is combined with the surgeon's natural view of the operation, and anesthesia, where the patient vital signs are within the anesthesiologist's field of view at all times.\n\nResearch universities often use HMDs to conduct studies related to vision, balance, cognition and neuroscience. As of 2010, the use of predictive visual tracking measurement to identify mild traumatic brain injury was being studied. In visual tracking tests, a HMD unit with eye tracking ability shows an object moving in a regular pattern. People without brain injury are able to track the moving object with \"smooth pursuit eye movements\" and correct trajectory.\n\nLow cost HMD devices are available for use with 3D games and entertainment applications. One of the first commercially available HMDs was the Forte VFX1 which was announced at Consumer Electronics Show (CES) in 1994. The VFX-1 had stereoscopic displays, 3-axis head-tracking, and stereo headphones.\n\nAnother pioneer in this field was Sony, which released the Glasstron in 1997. It had as an optional accessory a positional sensor which permitted the user to view the surroundings, with the perspective moving as the head moved, providing a deep sense of immersion. One novel application of this technology was in the game \"\", which permitted users of the Sony Glasstron or Virtual I/O's iGlasses to adopt a new visual perspective from inside the cockpit of the craft, using their own eyes as visual and seeing the battlefield through their craft's own cockpit.\n\nAs of 2013, many brands of video glasses can be connected to video and DSLR cameras, making them applicable as a new age monitor. As a result of the glasses ability to block out ambient light, filmmakers and photographers are able to see clearer presentations of their live images.\n\nThe Oculus Rift is a virtual reality (VR) head-mounted display created by Palmer Luckey that the company Oculus VR is developing for virtual reality simulations and video games. The HTC Vive is a virtual reality head-mounted display. The headset is produced by a collaboration between Valve and HTC, with its defining feature being precision room-scale tracking, and high-precision motion controllers. The PlayStation VR is the only virtual reality headset for gaming consoles, for the PlayStation 4.\n\nA HMD system has been developed for Formula One drivers by Kopin Corp. and the BMW Group. According to BMW, \"The HMD is part of an advanced telemetry system approved for installation by the Formula One racing committee… to communicate to the driver wirelessly from the heart of the race pit.\" The HMD will display critical race data while allowing the driver to continue focusing on the track. Pit crews control the data and messages sent to their drivers through two-way radio.\n\nRecon Instruments released on two head mounted displays for ski goggles, MOD and MOD Live, the latter based on an Android operating system.\n\nA key application for HMDs is training and simulation, allowing to virtually place a trainee in a situation that is either too expensive or too dangerous to replicate in real-life. Training with HMDs covers a wide range of applications from driving, welding and spray painting, flight and vehicle simulators, dismounted soldier training, medical procedure training, and more. However, a number of unwanted symptoms have been caused by prolonged use of certain types of head-mounted displays, and these issues must be resolved before optimal training and simulation is feasible.\n\n\nDepth perception inside an HMD requires different images for the left and right eyes. There are multiple ways to provide these separate images:\n\nThe advantage of dual video inputs is that it provides the maximum resolution for each image and the maximum frame rate for each eye. The disadvantage of dual video inputs is that it requires separate video outputs and cables from the device generating the content.\n\nTime-based multiplexing preserves the full resolution per each image, but reduces the frame rate by half. For example, if the signal is presented at 60 Hz, each eye is receiving just 30 Hz updates. This may become an issue with accurately presenting fast-moving images.\n\nSide-by-side and top-bottom multiplexing provide full-rate updates to each eye, but reduce the resolution presented to each eye. Many 3D broadcasts, such as ESPN, chose to provide side-by-side 3D which saves the need to allocate extra transmission bandwidth and is more suitable to fast-paced sports action relative to time-based multiplexing methods.\n\nNot all HMDs provide depth perception. Some lower-end modules are essentially bi-ocular devices where both eyes are presented with the same image.\n\n3D video players sometimes allow maximum compatibility with HMDs by providing the user with a choice of the 3D format to be used.\n\n\n\n"}
{"id": "91820", "url": "https://en.wikipedia.org/wiki?curid=91820", "title": "Instructional design", "text": "Instructional design\n\nInstructional design (ID), also known as instructional systems design (ISD), is the practice of systematically designing, developing and delivering instructional products and experiences, both digital and physical, in a consistent and reliable fashion towards an efficient, effective, appealing, engaging and inspiring acquisition of knowledge. The process consists broadly of determining the state and needs of the learner, defining the end goal of instruction, and creating some \"intervention\" to assist in the transition. The outcome of this instruction may be directly observable and scientifically measured or completely hidden and assumed. There are many instructional design models but many are based on the ADDIE model with the five phases: analysis, design, development, implementation, and evaluation. \n\nAs a field, instructional design is historically and traditionally rooted in cognitive and behavioral psychology, though recently constructivism has influenced thinking in the field. This can be attributed to the way it emerged during a period when the behaviorist paradigm was dominating American psychology. There are also those who cite that, aside from behaviorist psychology, the origin of the concept could be traced back to systems engineering. The impact of each of these fields is difficult to quantify, however, it is argued that the language and the \"look and feel\" of the early forms of instructional design and their progeny were derived from this engineering discipline. Specifically, they were linked to the training development model used by the U.S. military, which were based on systems approach and was explained as \"the idea of viewing a problem or situation in its entirety with all its ramifications, with all its interior interactions, with all its exterior connections and with full cognizance of its place in its context.\" \n\nThe role of systems engineering in the early development of instructional design was demonstrated during World War II when a considerable amount of training materials for the military were developed based on the principles of instruction, learning, and human behavior. Tests for assessing a learner’s abilities were used to screen candidates for the training programs. After the success of military training, psychologists began to view training as a system and developed various analysis, design, and evaluation procedures. In 1946, Edgar Dale outlined a hierarchy of instructional methods, organized intuitively by their concreteness. The framework first migrated to the industrial sector to train workers before it finally found its way to the education field.\n\nB. F. Skinner's 1954 article “\"The Science of Learning and the Art of Teaching\"” suggested that effective instructional materials, called programmed instructional materials, should include small steps, frequent questions, and immediate feedback; and should allow self-pacing.\nRobert F. Mager popularized the use of learning objectives with his 1962 article “\"Preparing Objectives for Programmed Instruction”\". The article describes how to write objectives including desired behavior, learning condition, and assessment.\nIn 1956, a committee led by Benjamin Bloom published an influential taxonomy with three domains of learning: cognitive (what one knows or thinks), psychomotor (what one does, physically) and affective (what one feels, or what attitudes one has). These taxonomies still influence the design of instruction.\nRobert Glaser introduced “criterion-referenced measures” in 1962. In contrast to norm-referenced tests in which an individual's performance is compared to group performance, a criterion-referenced test is designed to test an individual's behavior in relation to an objective standard. It can be used to assess the learners’ entry level behavior, and to what extent learners have developed mastery through an instructional program.\n\nIn 1965, Robert Gagne (see below for more information) described three domains of learning outcomes (cognitive, affective, psychomotor), five learning outcomes (Verbal Information, Intellectual Skills, Cognitive Strategy, Attitude, Motor Skills), and nine events of instruction in “\"The Conditions of Learning\"”, which remain foundations of instructional design practices. Gagne’s work in learning hierarchies and hierarchical analysis led to an important notion in instruction – to ensure that learners acquire prerequisite skills before attempting superordinate ones.\n\nIn 1967, after analyzing the failure of training material, Michael Scriven suggested the need for formative assessment – e.g., to try out instructional materials with learners (and revise accordingly) before declaring them finalized.\n\nDuring the 1970s, the number of instructional design models greatly increased and prospered in different sectors in military, academia, and industry. Many instructional design theorists began to adopt an information-processing-based approach to the design of instruction. David Merrill for instance developed Component Display Theory (CDT), which concentrates on the means of presenting instructional materials (presentation techniques).\n\nAlthough interest in instructional design continued to be strong in business and the military, there was little evolution of ID in schools or higher education.\nHowever, educators and researchers began to consider how the personal computer could be used in a learning environment or a learning space. PLATO (Programmed Logic for Automatic Teaching Operation) is one example of how computers began to be integrated into instruction. Many of the first uses of computers in the classroom were for \"drill and skill\" exercises. There was a growing interest in how cognitive psychology could be applied to instructional design.\n\nThe influence of constructivist theory on instructional design became more prominent in the 1990s as a counterpoint to the more traditional cognitive learning theory. Constructivists believe that learning experiences should be \"authentic\" and produce real-world learning environments that allow learners to construct their own knowledge. This emphasis on the learner was a significant departure from traditional forms of instructional design.\n\nPerformance improvement was also seen as an important outcome of learning that needed to be considered during the design process. The World Wide Web emerged as an online learning tool with hypertext and hypermedia being recognized as good tools for learning. As technology advanced and constructivist theory gained popularity, technology’s use in the classroom began to evolve from mostly drill and skill exercises to more interactive activities that required more complex thinking on the part of the learner. Rapid prototyping was first seen during the 1990s. In this process, an instructional design project is prototyped quickly and then vetted through a series of try and revise cycles. This is a big departure from traditional methods of instructional design that took far longer to complete.\n\nOnline learning became common. Technology advances permitted sophisticated simulations with authentic and realistic learning experiences.\n\nIn 2008, the Association for Educational Communications and Technology (AECT) changed the definition of Educational Technology to \"the study and ethical practice of facilitating learning and improving performance by creating, using, and managing appropriate technological processes and resources\". \n\nAcademic degrees focused on integrating technology, internet, and human–computer interaction with education gained momentum with the introduction of Learning Design and Technology (LDT) majors. Universities such as Bowling Green State University, Penn State, Purdue, San Diego State University, Stanford, Harvard University of Georgia, California State University, Fullerton and Carnegie Mellon University have established undergraduate and graduate degrees in technology-centered methods of designing and delivering education.\n\nInformal learning became an area of growing importance in instructional design, particularly in the workplace. A 2014 study showed that formal training makes up only 4 percent of the 505 hours per year an average employee spends learning. It also found that the learning output of informal learning is equal to that of formal training. As a result of this and other research, more emphasis was placed on creating knowledge bases and other supports for self-directed learning.\n\nRobert Gagné's work is widely used and cited in the design of instruction, as exemplified by more than 130 citations in prominent journals in the field during the period from 1985 through 1990. Synthesizing ideas from behaviorism and cognitivism, he provided a clear template, which is easy to follow for designing instructional events. Instructional designers who follow Gagné's theory will likely have tightly focused, efficient instruction.\n\nRobert Gagné classified the types of learning outcomes by asking how learning might be demonstrated. His domains and outcomes of learning correspond to standard verbs.\n\nAccording to Gagné, learning occurs in a series of nine learning events, each of which is a condition for learning which must be accomplished before moving to the next in order. Similarly, instructional events should mirror the learning events:\n\n\nSome educators believe that Gagné's taxonomy of learning outcomes and events of instruction oversimplify the learning process by over-prescribing. However, using them as part of a complete instructional package can assist many educators in becoming more organized and staying focused on the instructional goals.\n\nRobert Gagné’s work has been the foundation of instructional design since the beginning of the 1960s when he conducted research and developed training materials for the military. Among the first to coin the term “instructional design”, Gagné developed some of the earliest instructional design models and ideas. These models have laid the groundwork for more present-day instructional design models from theorists like Dick, Carey, and Carey (The Dick and Carey Systems Approach Model), Jerold Kemp’s Instructional Design Model, and David Merrill (Merrill’s First Principle of Instruction). Each of these models are based on a core set of learning phases that include (1) activation of prior experience, (2) demonstration of skills, (3) application of skills, and (4) integration or these skills into real world activities. The figure below illustrates these five ideas.\n\nGagné's main focus for instructional design was how instruction and learning could be systematically connected to the design of instruction. He emphasized the design principles and procedures that need to take place for effective teaching and learning. His initial ideas, along with the ideas of other early instructional designers were outlined in \"Psychological Principles in Systematic Development\", written by Roberts B. Miller and edited by Gagné. Gagné believed in internal learning and motivation which paved the way for theorists like Merrill, Li, and Jones who designed the Instructional Transaction Theory, Reigeluth and Stein’s Elaboration Theory, and most notably, Keller’s ARCS Model of Motivation and Design.\n\nPrior to Robert Gagné, learning was often thought of as a single, uniform process. There was little or no distinction made between “learning to load a rifle and learning to solve a complex mathematical problem”. Gagné offered an alternative view which developed the idea that different learners required different learning strategies. Understanding and designing instruction based on a learning style defined by the individual brought about new theories and approaches to teaching.\nGagné 's understanding and theories of human learning added significantly to understanding the stages in cognitive processing and instructions. For example, Gagné argued that instructional designers must understand the characteristics and functions of short-term and long-term memory to facilitate meaningful learning. This idea encouraged instructional designers to include cognitive needs as a top-down instructional approach.\n\nGagné (1966) defines curriculum as a sequence of content units arranged in such a way that the learning of each unit may be accomplished as a single act, provided the capabilities described by specified prior units (in the sequence) have already been mastered by the learner.\n\nHis definition of curriculum has been the basis of many important initiatives in schools and other educational environments. In the late 1950s and early 1960s, Gagné had expressed and established an interest in applying theory to practice with particular interest in applications for teaching, training and learning. Increasing the effectiveness and efficiency of practice was of particular concern. His ongoing attention to practice while developing theory continues to influence education and training.\n\nGagné's work has had a significant influence on American education, and military and industrial training. Gagné was one of the early developers of the concept of instructional systems design which suggests the components of a lesson can be analyzed and should be designed to operate together as an integrated plan for instruction. In \"Educational Technology and the Learning Process\" (Educational Researcher, 1974), Gagné defined instruction as \"the set of planned external events which influence the process of learning and thus promote learning\".\n\nThe concept of learning design arrived in the literature of technology for education in the late 1990s and early 2000s with the idea that \"designers and instructors need to choose for themselves the best mixture of behaviourist and constructivist learning experiences for their online courses\". But the concept of learning design is probably as old as the concept of teaching. Learning design might be defined as \"the description of the teaching-learning process that takes place in a unit of learning (e.g., a course, a lesson or any other designed learning event)\".\n\nAs summarized by Britain, learning design may be associated with:\n\nPerhaps the most common model used for creating instructional materials is the ADDIE Model. This acronym stands for the 5 phases contained in the model (Analyze, Design, Develop, Implement, and Evaluate).\n\nBrief History of ADDIE’s Development – The ADDIE model was initially developed by Florida State University to explain “the processes involved in the formulation of an instructional systems development (ISD) program for military interservice training that will adequately train individuals to do a particular job and which can also be applied to any interservice curriculum development activity.” The model originally contained several steps under its five original phases (Analyze, Design, Develop, Implement, and [Evaluation and] Control), whose completion was expected before movement to the next phase could occur. Over the years, the steps were revised and eventually the model itself became more dynamic and interactive than its original hierarchical rendition, until its most popular version appeared in the mid-80s, as we understand it today.\n\nThe five phases are listed and explained below:\n\nAnalyze – The first phase of content development is Analysis. Analysis refers to the gathering of information about one’s audience, the tasks to be completed, how the learners will view the content, and the project’s overall goals. The instructional designer then classifies the information to make the content more applicable and successful.\n\nDesign – The second phase is the Design phase. In this phase, instructional designers begin to create their project. Information gathered from the analysis phase, in conjunction with the theories and models of instructional design, is meant to explain how the learning will be acquired. For example, the design phase begins with writing a learning objective. Tasks are then identified and broken down to be more manageable for the designer. The final step determines the kind of activities required for the audience in order to meet the goals identified in the Analyze phase.\n\nDevelop – The third phase, Development, involves the creation of the activities that will be implemented. It is in this stage that the blueprints of the design phase are assembled.\n\nImplement – After the content is developed, it is then Implemented. This stage allows the instructional designer to test all materials to determine if they are functional and appropriate for the intended audience.\n\nEvaluate – The final phase, Evaluate, ensures the materials achieved the desired goals. The evaluation phase consists of two parts: formative and summative assessment. The ADDIE model is an iterative process of instructional design, which means that at each stage the designer can assess the project's elements and revise them if necessary. This process incorporates formative assessment, while the summative assessments contain tests or evaluations created for the content being implemented. This final phase is vital for the instructional design team because it provides data used to alter and enhance the design.\n\nConnecting all phases of the model are external and reciprocal revision opportunities. As in the internal Evaluation phase, revisions should and can be made throughout the entire process.\n\nMost of the current instructional design models are variations of the ADDIE model.\n\nAn adaptation of the ADDIE model, which is used sometimes, is a practice known as rapid prototyping.\n\nProponents suggest that through an iterative process the verification of the design documents saves time and money by catching problems while they are still easy to fix. This approach is not novel to the design of instruction, but appears in many design-related domains including software design, architecture, transportation planning, product development, message design, user experience design, etc. In fact, some proponents of design prototyping assert that a sophisticated understanding of a problem is incomplete without creating and evaluating some type of prototype, regardless of the analysis rigor that may have been applied up front. In other words, up-front analysis is rarely sufficient to allow one to confidently select an instructional model. For this reason many traditional methods of instructional design are beginning to be seen as incomplete, naive, and even counter-productive.\n\nHowever, some consider rapid prototyping to be a somewhat simplistic type of model. As this argument goes, at the heart of Instructional Design is the analysis phase. After you thoroughly conduct the analysis—you can then choose a model based on your findings. That is the area where most people get snagged—they simply do not do a thorough-enough analysis. (Part of Article By Chris Bressi on LinkedIn)\n\nAnother well-known instructional design model is the Dick and Carey Systems Approach Model. The model was originally published in 1978 by Walter Dick and Lou Carey in their book entitled \"The Systematic Design of Instruction\".\n\nDick and Carey made a significant contribution to the instructional design field by championing a systems view of instruction, in contrast to defining instruction as the sum of isolated parts. The model addresses instruction as an entire system, focusing on the interrelationship between context, content, learning and instruction. According to Dick and Carey, \"Components such as the instructor, learners, materials, instructional activities, delivery system, and learning and performance environments interact with each other and work together to bring about the desired student learning outcomes\". The components of the Systems Approach Model, also known as the Dick and Carey Model, are as follows:\n\nWith this model, components are executed iteratively and in parallel, rather than linearly.\n\nThe instructional design model, Guaranteed Learning, was formerly known as the Instructional Development Learning System (IDLS). The model was originally published in 1970 by Peter J. Esseff, PhD and Mary Sullivan Esseff, PhD in their book entitled \"IDLS—Pro Trainer 1: How to Design, Develop, and Validate Instructional Materials\".\n\nPeter (1968) & Mary (1972) Esseff both received their doctorates in Educational Technology from the Catholic University of America under the mentorship of Dr. Gabriel Ofiesh, a founding father of the Military Model mentioned above. Esseff and Esseff synthesized existing theories to develop their approach to systematic design, \"Guaranteed Learning\" aka \"Instructional Development Learning System\" (IDLS). In 2015, Peter and Mary Esseff created an eLearning course to enable participants to take the GL course online under the direction of Dr. Esseff.\n\nThe components of the Guaranteed Learning Model are the following:\n\nOther useful instructional design models include: the Smith/Ragan Model, the Morrison/Ross/Kemp Model and the OAR Model of instructional design in higher education, as well as, Wiggins' theory of backward design.\n\nLearning theories also play an important role in the design of instructional materials. Theories such as behaviorism, constructivism, social learning and cognitivism help shape and define the outcome of instructional materials.\n\nAlso see: Managing Learning in High Performance Organizations, by Ruth Stiehl and Barbara Bessey, from The Learning Organization, Corvallis, Oregon. .\n\nMotivation is defined as an internal drive that activates behavior and gives it direction. The term motivation theory is concerned with the process that describe why and how human behavior is activated and directed.\n\n\"Intrinsic and Extrinsic Motivation\"\n\nJohn Keller \nhas devoted his career to researching and understanding motivation in instructional systems. These decades of work constitute a major contribution to the instructional design field. First, by applying motivation theories systematically to design theory. Second, in developing a unique problem-solving process he calls the ARCS Motivation...\n\nThe ARCS Model of Motivational Design was created by John Keller while he was researching ways to supplement the learning process with motivation. The model is based on Tolman's and Lewin's expectancy-value theory, which presumes that people are motivated to learn if there is value in the knowledge presented (i.e. it fulfills personal needs) and if there is an optimistic expectation for success. The model consists of four main areas: Attention, Relevance, Confidence, and Satisfaction.\n\nAttention and relevance according to John Keller's ARCS motivational theory are essential to learning. The first 2 of 4 key components for motivating learners, attention, and relevance can be considered the backbone of the ARCS theory, the latter components relying upon the former.\n\nThe attention mentioned in this theory refers to the interest displayed by learners in taking in the concepts/ideas being taught. This component is split into three categories: perceptual arousal, using surprise or uncertain situations; inquiry arousal, offering challenging questions and/or problems to answer/solve; and variability, using a variety of resources and methods of teaching. Within each of these categories, John Keller has provided further sub-divisions of types of stimuli to grab attention. Grabbing attention is the most important part of the model because it initiates the motivation for the learners. Once learners are interested in a topic, they are willing to invest their time, pay attention, and find out more.\n\nRelevance, according to Keller, must be established by using language and examples that the learners are familiar with. The three major strategies Keller presents are goal-oriented, motive matching, and familiarity. Like the Attention category, Keller divided the three major strategies into subcategories, which provide examples of how to make a lesson plan relevant to the learner. Learners will throw concepts to the wayside if their attention cannot be grabbed and sustained and if relevance is not conveyed.\n\nThe confidence aspect of the ARCS model focuses on establishing positive expectations for achieving success among learners. The confidence level of learners is often correlated with motivation and the amount of effort put forth in reaching a performance objective. For this reason, it's important that learning design provides students with a method for estimating their probability of success. This can be achieved in the form of a syllabus and grading policy, rubrics, or a time estimate to complete tasks. Additionally, confidence is built when positive reinforcement for personal achievements is given through timely, relevant feedback.\n\nFinally, learners must obtain some type of satisfaction or reward from a learning experience. This satisfaction can be from a sense of achievement, praise from a higher-up, or mere entertainment. Feedback and reinforcement are important elements and when learners appreciate the results, they will be motivated to learn. Satisfaction is based upon motivation, which can be intrinsic or extrinsic. To keep learners satisfied, instruction should be designed to allow them to use their newly learned skills as soon as possible in as authentic a setting as possible.\n\nAlong with the motivational components (Attention, Relevance, Confidence, and Satisfaction) the ARCS model provides a process that can address motivational problems. This process has 4 phases (Analysis, Design, Development, and Evaluation) with 10 steps within the phases:\n\nStep 1: Obtain course information\n\nIncludes reviewing the description of the course, the instructor, and way of delivery the information.\n\nStep 2: Obtain audience information\n\nIncludes collecting the current skill level, attitudes towards the course, attitudes towards the teacher, attitudes towards the school.\n\nStep 3: Analyze audience\n\nThis should help identify the motivational problem that needs to be addressed.\n\nStep 4: Analyze existing materials\n\nIdentifying positives of the current instructional material, as well as any problems.\n\nStep 5: List objectives and assessments\n\nThis allows the creation of assessment tools that align with the objectives.\n\nStep 6: List potential tactics\n\nBrainstorming possible tactics that could fill in the motivational gaps.\n\nStep 7: Select and design tactics\n\nIntegrates, enhances, and sustains tactics from the list that fit the situation.\n\nStep 8: Integrate with instruction\n\nIntegrate the tactic that was chosen from the list into the instruction.\n\nStep 9: Select and develop materials\n\nSelect materials, modify to fit the situation and develop new materials.\n\nStep 10: Evaluate and revise\n\nObtain reactions from the learner and determine satisfaction level.\n\nAlthough Keller's ARCS model currently dominates instructional design with respect to learner motivation, in 2006 Hardré and Miller proposed a need for a new design model that includes current research in human motivation, a comprehensive treatment of motivation, integrates various fields of psychology and provides designers the flexibility to be applied to a myriad of situations.\n\nHardré proposes an alternate model for designers called the Motivating Opportunities Model or MOM. Hardré's model incorporates cognitive, needs, and affective theories as well as social elements of learning to address learner motivation. MOM has seven key components spelling the acronym 'SUCCESS' – Situational, Utilization, Competence, Content, Emotional, Social, and Systemic.\n\n\"Alphabetic by last name\"\n\n\n"}
{"id": "22340844", "url": "https://en.wikipedia.org/wiki?curid=22340844", "title": "Instructor-led training", "text": "Instructor-led training\n\nInstructor-led training, or ILT, is the practice of training and learning material between an instructor and learners, either individuals or groups. Instructors can also be referred to as a facilitator, who may be knowledgeable and experienced in the learning material, but can also be used more for their facilitation skills and ability to deliver material to learners.\n\nInstructors may deliver training in a lecture or classroom format, as an interactive workshop, as a demonstration with the opportunity for learners to practice, or even virtually, using video-conferencing tools; and the instructor may have facilitation and teaching skills, in which they can use different methods to engage learners and embrace different learning styles.\n\nOther learning delivery methods include e-learning which delivers self-paced courses online, and blended learning which mixes instructor-led and e-learning elements.\n\nInstructor-led training represents overall 66% of corporate training and development; it reaches 76% in high-performing companies and 80% in high-consequence industries (healthcare industry, pharmaceutical industry, finance, utilities, etc.). It is also the most widely-used method for extended enterprise training, which trains customers and partners, with an 80% usage rate. In the UK, one in three companies who use e-learning currently deliver more than three-quarters of their training and development activities completely through face-to-face experiences.\n\nThe primary reason why human resources departments prefer instructor-led training is its high effectiveness in terms of knowledge retention: in a survey rating every training and development delivery format, in-person instructor-led classrooms are in third position, with 3.63 out of 5, whereas e-learning modules are in seventh position, with a grade of 3.05 out of 5.\n\nILT is an effective means of delivering information, as it allows for real-time feedback, questions and answers, manipulation and changeable delivery to suit the needs of learners in a real-time environment, and a learning environment can be created by the instructor's style.\n\nAlthough instructor-led training dominates the market in terms of use because of its effectiveness, it tends to be a costlier learning delivery method. Hiring an instructor, renting or maintaining facilities, providing hands-on tools, travel, food, and boarding can push companies on a tight budget. Additionally, classroom occupancy rates and resource use are not always maximized.\n\nThis is why streamlining logistics, scheduling and administration, managing resources, and optimizing the budget are key in managing instructor-led training.. Organizations such as corporate training departments, corporate university as well as training companies generally manage this through a core enterprise resource planning (ERP) such as a training management system.\n\nThe training management system optimizes ILT management by streamlining every aspect of the training process: planning (training plan and budget forecasting), logistics (scheduling and resource management), financials (cost tracking, profitability), reporting, and sales for for-profit training providers. For example, a training management system can be used to schedule instructors, venues and equipment through graphical agendas, optimize resource use, create a training plan and track remaining budgets, generate reports and share data between different teams.\n\nWhile training management systems focus on managing instructor-led training, they can complete an LMS. In this situation, an LMS will manage e-learning delivery and assessment, while a training management system will manage ILT and back-office budget planning, logistic and reporting.\n\nRecently, there have been many trends in modernizing and optimizing instructor-led training through educational technology.\n\nInstructor-led training can be delivered within a classroom or remotely through a virtual classroom, in which case it is called virtual instructor-led training. Instructor and learners are in different locations, and a classroom environment is replicated through online tools. This type of training can be delivered synchronously or asynchronously.\n\nInstructor-led training can also be combined with e-learning in a blended learning scenario to achieve a maximum effectiveness. In this case, some of the training is delivered live while online courses serve as refreshers between sessions. A growing type of blended learning is called the flipped classroom model, where students acquire information by watching lectures online and then engage in problem-solving, discussion and group activities in class.\n\nTraining within the classroom can be also enhanced through a range of technology and collaborative tools such as video software and system to access content during the class. These tools will help the instructor monitor learners level of understanding and adapt the training pace as the session unfolds. Additionally, gamification elements can be added to the training session to elicit greater attention and engagement from learners during the session .\n\nLastly, instructor-led training back office management can be optimized through dedicated software which streamline all processes (scheduling, logistics, costs and budget management, administration, reporting, etc.) such as a training management system. This allows training organizations to improve the efficiency of their ILT and optimize their training budget.\n\n"}
{"id": "45031327", "url": "https://en.wikipedia.org/wiki?curid=45031327", "title": "Integrated Water Flow Model (IWFM)", "text": "Integrated Water Flow Model (IWFM)\n\nIntegrated Water Flow Model (IWFM) is a computer program for simulating water flow through the integrated land surface, surface water and groundwater flow systems. It is a rewrite of the abandoned software IGSM, which was found to have several programing errors. The IWFM programs and source code are freely available. IWFM is written in Fortran, and can be compiled and run on Microsoft Windows, Linux and Unix operating systems. The IWFM source code is released under the GNU General Public License.\n\nGroundwater flow is simulated using the finite element method. Surface water flow can be simulated as a simple one-dimensional flow-through network or with the kinematic wave method. IWFM input data sets incorporate a time stamp, allowing users to run a model for a specified time period without editing the input files.\n\nOne of the most useful features of IWFM is the internal calculation of water demands for each land use type. IWFM simulates four land use classes: agricultural, urban, native vegetation, and riparian vegetation. Land use areas are delineated as a time series, with corresponding evapotranspiration rates and water management parameters. Each time step, the land use process applies precipitation, calculates infiltration and runoff, calculates water demands, and determines what portion of the demands are not met by soil moisture. For agricultural and urban land use classes, IWFM then applies surface water and groundwater at specified rates, and optionally adjusts surface water and groundwater to exactly meet water demands. This automatic adjustment feature is especially useful for calculating unmeasured flow components (such as groundwater withdrawals) or for simulating proposed future scenarios such as studying the impacts of potential climate change.\n\nIn IWFM, the land surface, surface water and groundwater flow domains are simulated as separate processes, compiled into individual dynamic link libraries. The processes are linked by water flow terms, maintain conservation of mass and momentum between processes, and are solved simultaneously. This allows each IWFM process to be run independently as a stand-alone model, or to be linked to other programs. This functionality has been used to create a Microsoft Excel Add-in to create workbooks from IWFM output files. The IWFM land surface process has been compiled into a stand-alone program called the IWFM Demand Calculator IDC. The groundwater process is linked to the WRIMS modeling system and used in the water resources optimization model CalSim. This feature allows other models to be easily linked with IWFM, to either enhance the capabilities of the target model (for example, by adding groundwater flow to a land surface-surface water model) or to enhance the capabilities of IWFM (for example, linking an economic model to IWFM to dynamically change the crop mix based on the depth to groundwater, as the cost of pumping increases with depth to water).\n\nNotable models developed with IWFM include the California Central Valley Groundwater-Surface Water Simulation Model (C2VSim), a model of the Walla-Walla Basin in Washington and Oregon, USA, a model of the Butte Basin, CA, USA, and several unpublished models. IWFM has also been peer reviewed.\n"}
{"id": "443212", "url": "https://en.wikipedia.org/wiki?curid=443212", "title": "Key (instrument)", "text": "Key (instrument)\n\nA key is a specific part of a musical instrument. The purpose and function of the part in question depends on the instrument.\n\nOn instruments equipped with tuning machines, violins and guitars, for example, a key is part of a tuning machine. It is a worm gear with a key shaped end used to turn a cog, which, in turn, is attached to a post which winds the string. The key is used to make pitch adjustments to a string.\n\nWith other instruments, zithers and drums, for example, a key is essentially a small wrench used to turn a tuning machine or lug.\n\nOn woodwind instruments such as a flute or saxophone, keys are finger operated levers used to open or close tone holes, thereby shortening or lengthening the resonating tube of the instrument. The keys on the keyboard of a pipe organ also open and close various valves, but the air flow is driven mechanically rather than lung powered, and the flow of air is directed through different pipes tuned for each note. The keys of an accordion direct the air flow from a manually operated bellows across various tuned vibrating reeds.\n\nOn other keyboard instruments, a key may be a lever which mechanically triggers a hammer to strike a group of strings, as on a piano, or an electric switch which energizes an audio oscillator as on an electronic organ or a synthesizer.\n\n"}
{"id": "29119695", "url": "https://en.wikipedia.org/wiki?curid=29119695", "title": "Lanedo", "text": "Lanedo\n\nLanedo is a professional open source software development consultancy based in Germany. It operates as a European company with limited liability based in Hamburg. The company has been involved with a number of mobile and embedded platforms over the years including Maemo and MeeGo. Their software development has a focus around Linux in general and targeting platforms ranging from mobile and embedded to desktop environments.\n\nThe name 'Lanedo' is based on a roughly phonetic sounding of the letters L, N, and D. These are mentioned as Linux, Networking, and Development respectively.\n\nIn 2003, Mikael Hallendal and Richard Hult were working together on a project management application called Planner. They founded Imendio.\n\nIn 2011, Lanedo joined the Document Foundation TSC (Task Steering Committee) and became much more involved in the future of LibreOffice.\n\nStarting in summer 2012, Lanedo, in cooperation with ITOMIG, supported the town of Munich in the LibreOffice maintenance of the LiMux project.\nLanedo is also actively participating in implementing the OOXML support for LibreOffice.\n\nThese projects are contributed to by Lanedians.\n\n"}
{"id": "27683037", "url": "https://en.wikipedia.org/wiki?curid=27683037", "title": "List of Israeli inventions and discoveries", "text": "List of Israeli inventions and discoveries\n\nThis is a list of inventions and discoveries by Israeli scientists and researchers, working locally or overseas. According to Joel Landau, there are over 6000 startups currently in Israel.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"Ridiculous Dietary Allowance\" S.Hickey PhD H. Roberts PhD. pp. 107–111.\n\n"}
{"id": "44417219", "url": "https://en.wikipedia.org/wiki?curid=44417219", "title": "List of Taiwanese inventions and discoveries", "text": "List of Taiwanese inventions and discoveries\n\nThis is a list of inventions by people who were born in Taiwan or current citizens of Taiwan.\n\n\n"}
{"id": "1462693", "url": "https://en.wikipedia.org/wiki?curid=1462693", "title": "M1 (missile)", "text": "M1 (missile)\n\nThe M1 MSBS was the first French submarine-launched ballistic missile. In French, MSBS is the abbreviation for \"Mer-Sol Balistique Stratégique\", or Sea-Ground Strategic Ballistic Missile. It has two stages. It was deployed on the \"Redoutable\"-class SNLEs (\"Sous-marin Nucléaire Lanceur d'Engins\") (Device-launching Nuclear Submarine) or SSBNs from 1971 to 1975. It was replaced by the M2 MSBS in 1974-75.\n"}
{"id": "9754789", "url": "https://en.wikipedia.org/wiki?curid=9754789", "title": "Magister Musicae", "text": "Magister Musicae\n\nMagister Musicae is an internet portal which hosts a digital catalogue of more than 3,000 hours of music classes, given by more than 200 of the world’s most renowned musicians.\n\nThe project is promoted by the president of the Albéniz Foundation, Paloma O'Shea. The aim is to preserve the classes that take place each day at the Queen Sofía College of Music and make them accessible to a wider audience.\n\nIn the year 2000, the Spanish Ministry for Science and Technology supported the project with a considerable donation under the aegis of the PROFIT programme. The resulting three years of research and development, which focused on the digitization, cataloguing, compression and high-quality broadcasting of the videos on the internet, gave rise to HTCMedia, a tool which manages large quantities of video information. This has been the cornerstone of the creation of Magister Musicae.\n\nThe master classes are recorded in the music schools. These lessons are scrupulously analyzed by music specialists, catalogued by work and by movement, then broken down into teaching units of no more than six minutes’ duration. This system of classification allows the visitor to the site to go directly to the section of the class that is of interest, and to be able to link with a number of classes given by various teachers.\n\nThe scale and ambition of the project led, in 2004, to six of the principal conservatoires and music schools joining Magister Musicae, through the European Harmos project. It offered access to their teachers and enhanced Magister Musicae’s international profile. \n\n\nThe project received the financial backing of the European Commission, who called it “the model project which impacts on the policies of the Union”.\n\nThe following are amongst the more notable participants in the project:\n\n\nOnly the first 5 minutes are free. Videos are accessible by paying per time periods, not per videos.\n\nVideos can only be seen using the Microsoft Internet Explorer browser. Media is stored in Microsoft Windows Media.\n\n"}
{"id": "23173129", "url": "https://en.wikipedia.org/wiki?curid=23173129", "title": "Meat and bone meal", "text": "Meat and bone meal\n\nMeat and bone meal (MBM) is a product of the rendering industry. It is typically about 48–52% protein, 33–35% ash, 8–12% fat, and 4–7% moisture. It is primarily used in the formulation of animal feed to improve the amino acid profile of the feed. Feeding of MBM to cattle is thought to have been responsible for the spread of BSE (mad cow disease). In most parts of the world, MBM is no longer allowed in feed for ruminant animals. However, it is still used to feed monogastric animals.\n\nMBM is widely used in the United States as a low-cost meat in dog food and cat food. In Europe, some MBM is used as ingredients in petfood but the vast majority is now used as a fossil-fuel replacement for renewable energy generation, as a fuel in cement kilns, landfilling or incineration. Meat and bone meal has around two thirds the energy value of fossil fuels such as coal; the UK in particular widely uses meat and bone meal for the generation of renewable electricity. This was particularly prominent after many cattle were slaughtered during the BSE crisis. Meat and bone meal is increasingly used in cement kilns as an environmentally sustainable replacement for coal.\n\nMeat meal: Powder obtained by cooking, defatting, sterilizing, grinding, and sifting by-products of terrestrial animals. This denomination includes both meat and bone meal (MBM) and meat meal in the strict sense, less rich in minerals than MBM; it is often referred to as \"processed animal protein\".\n\nBone meal: It is produced with bones (of terrestrial animals) of second quality. The other bones can be used beforehand for the manufacture of gelatin and / or treated to produce dicalcium phosphate or ossein powder; the meal is produced by heating, defatting, drying, grinding and sieving the bones of terrestrial animals.\n\nMeal from skin appendages: horns, hooves and nails (around 55,700 tons per year in France) are used for example in agricultural or garden fertilizers for their sulfur, nitrogen and phosphorus contents, or used in the composition of meat meal.\n\nBlood meal: fresh and whole blood collected from slaughterhouses, coagulated and steam dried now often referred to as \"animal protein transformed from blood\".\n\nFeather meal: (Fresh feathers from slaughterhouses, treated by thermal hydrolysis (chemical decomposition in the presence of pressurized water), dried and crushed.\n\nFishmeal \n\nIn Europe (before the 2002 EU Regulation), animal by-products were classified into two categories: \"high risk\" or \"low risk\" products.\n\nSince 2002, \"processed animal protein\" (PAP) and other animal by-products; authorized or not for various uses are categorized into three categories (by the European Regulation 2002) according to their supposed or demonstrated level of health risk. \n\n\n\n"}
{"id": "29102401", "url": "https://en.wikipedia.org/wiki?curid=29102401", "title": "Membraneless Fuel Cells", "text": "Membraneless Fuel Cells\n\nMembraneless Fuel Cells convert stored chemical energy into electrical energy without the use of a conducting membrane as with other types of fuel cells. In Laminar Flow Fuel Cells (LFFC) this is achieved by exploiting the phenomenon of non-mixing laminar flows where the interface between the two flows works as a proton/ion conductor. The interface allows for high diffusivity and eliminates the need for costly membranes. The operating principles of these cells mean that they can only be built to millimeter-scale sizes. The lack of a membrane means they are cheaper but the size limits their use to portable applications which require small amounts of power.\n\nAnother type of membraneless fuel cell is a Mixed Reactant Fuel Cell (MRFC). Unlike LFFCs, MRFCs use a mixed fuel and electrolyte, and are thus not subject to the same limitations. Without a membrane, MRFCs depend on the characteristics of the electrodes to separate the oxidation and reduction reactions. By eliminating the membrane and delivering the reactants as a mixture, MRFCs can potentially be simpler and less costly than conventional fuel cell systems.\n\nThe efficiency of these cells is generally much higher than modern electricity producing sources. For example, a fossil fuel power plant system can achieve a 40% electrical conversion efficiency while a nuclear power plant is slightly lower at 32%. Fuel cell systems are capable of reaching efficiencies in the range of 55%–70%. However, as with any process, fuel cells also experience inherent losses due to their design and manufacturing processes.\n\n A fuel cell consists of an electrolyte which is placed in between two electrodes – the cathode and the anode. In the simplest case, hydrogen gas passes over the cathode, where it is decomposed into hydrogen protons and electrons. The protons pass through the electrolyte (often NAFION – manufactured by DuPont) across to the anode to the oxygen. Meanwhile, the free electrons travel around the cell to power a given load and then combine with the oxygen and hydrogen at the anode to form water. Two common types of electrolytes are a proton exchange membrane(PEM) (also known as Polymer Electrolyte Membrane) and a ceramic or solid oxide electrolyte (often used in Solid oxide fuel cells). Although hydrogen and oxygen are very common reactants, a plethora of other reactants exist and have been proven effective.\n\nHydrogen for fuel cells can be produced in many ways. The most common method in the United States (95% of production) is via Gas reforming, specifically using methane, which produces hydrogen from fossil fuels by running them through a high temperature steam process. Since fossil fuels are primarily composed of carbon and hydrogen molecules of various sizes, various fossil fuels can be utilized. For example, methanol, ethanol, and methane can all be used in the reforming process. Electrolysis and high temperature combination cycles are also used to provide hydrogen from water whereby the heat and electricity provide sufficient energy to disassociate the hydrogen and oxygen atoms.\n\nHowever, since these methods of hydrogen production are often energy and space intensive, it is often more convenient to use the chemicals directly in the fuel cell. Direct Methanol Fuel Cells (DMFC's), for example, use methanol as the reactant instead of first using reformation to produce hydrogen. Although DMFC's are not very efficient (~25%), they are energy dense which means that they are quite suitable for portable power applications. Another advantage over gaseous fuels, as in the H-O cells, is that liquids are much easier to handle, transport, pump and often have higher specific energies allowing for greater power extraction. Generally gases need to be stored in high pressure containers or cryogenic liquid containers which is a significant disadvantage to liquid transport.\n\nThe majority of fuel cell technologies currently employed are either PEM or SOFC cells. However, the electrolyte is often costly and not always completely effective. Although hydrogen technology has significantly evolved, other fossil fuel based cells (such as DMFC's) are still plagued by the shortcomings of proton exchange membranes. For example, fuel crossover means that low concentrations need to be used which limits the available power of the cell. In solid oxide fuel cells, high temperatures are needed which require energy and can also lead to quicker degradation of materials. Membraneless fuel cells offer a solution to these problems.\n\n LFFC's overcome the problem of unwanted crossover through the manipulation of the Reynolds number, which describes the behavior of a fluid. In general, at low Reynolds numbers, flow is laminar whereas turbulence occurs at a higher Reynolds number. In laminar flow, two fluids will interact primarily through diffusion which means mixing is limited. By choosing the correct fuel and oxidizing agents in LFFC's, protons can be allowed to diffuse from the anode to the cathode across the interface of the two streams. The LFFC's are not limited to a liquid feed and in certain cases, depending on the geometry and reactants, gases can also be advantageous. Current designs inject the fuel and oxidizing agent into two separate streams which flow side by side. The interface between the fluids acts as the electrolytic membrane across which protons diffuse. Membraneless fuel cells offer a cost advantage due to the lack of the electrolytic membrane. Further, a decrease in crossover also increases fuel efficiency resulting in higher power output.\n\nDiffusion across the interface is extremely important and can severely affect fuel cell performance. The protons need to be able to diffuse across both the fuel and the oxidizing agent. The diffusion coefficient, a term which describes the ease of diffusion of an element in another medium, can be combined with Fick's laws of diffusion which addresses the effects of a concentration gradient and distance over which diffusion occurs:\n\nwhere\n\n\nIn order to increase the diffusion flux, the diffusivity and/or concentration need to be increased while the length needs to be decreased. In DMFC's for example, the thickness of the membrane determines the diffusion length while the concentration is often limited due to crossover. Thus, the diffusion flux is limited. A membraneless fuel cell is theoretically the better option since the diffusion interface across both fluids is extremely thin and using higher concentrations does not result in a drastic effect on crossover.\n\nIn most fuel cell configurations with liquid feeds, the fuel and oxidizing solutions almost always contain water which acts as a diffusion medium. In many hydrogen-oxygen fuel cells, the diffusion of oxygen at the cathode is rate limiting since the diffusivity of oxygen in water is much lower than that of hydrogen. As a result, LFFC performance can also be improved by not using aqueous oxygen carriers.\n\nThe promise of membraneless fuel cells has been offset by several problems inherent to their designs. Ancillary structures are one of the largest obstacles. For example, pumps are required to maintain laminar flow while gas separators can be needed to supply the correct fuels into the cells. For micro fuel cells, these pumps and separators need to be miniaturized and packaged into a small volume (under 1 cm). Associated with this process is a so-called \"packaging penalty\" which results in higher costs. Further, pumping power drastically increases with decreasing size (see Scaling Laws) which is disadvantageous. Efficient packaging methods and/or self-pumping cells (see Research and Development) need to be developed to make this technology viable. Also, while using high concentrations of specific fuels, such as methanol, crossover still occurs. This problem can be partially solved by using a nanoporous separator, lowering fuel concentration or choosing reactants which have a lower tendency towards crossover.\n\nDate: January 2010: Researchers developed a novel method of inducing self-pumping in a membraneless fuel cell. Using formic acid as a fuel and sulfuric acid as an oxidant, CO is produced in the reaction in the form of bubbles. The bubbles nucleate and coalesce on the anode. A check valve at the supply end prevents any fuel entering while the bubbles are growing. The check valve is not mechanical but hydrophobic in nature. By creating micro structures which form specific contact angles with water, fuel cannot be drawn backwards. As the reaction continues, more CO is formed while fuel is consumed. The bubble begins to propagate towards the outlet of the cell. However, before the outlet, a hydrophobic vent allows the carbon dioxide to escape while simultaneously ensuring other byproducts (such as water) do not clog the vent. As the carbon dioxide is being vented, fresh fuel is also drawn in at the same through the check valve and the cycle begins again. Thus, the fuel cell pumping is regulated by the reaction rate. This type of cell is not a two stream laminar flow fuel cell. Since the formation of bubbles can disrupt two separate laminar flows, a combined stream of fuel and oxidant was used. In laminar conditions, mixing will still not occur. It was found that using selective catalysts (i.e. Not platinum) or extremely low flow rates can prevent crossover.\n\nMembraneless fuel cells are currently being manufactured on the micro scale using fabrication processes found in the MEMS/NEMS area. These cell sizes are suited for the small scale due to the limit of their operating principles. The scale-up of these cells to the 2–10 Watt range has proven difficult since, at large scales, the cells cannot maintain the correct operating conditions.\n\nFor example, laminar flow is a necessary condition for these cells. Without laminar flow, crossover would occur and a physical electrolytic membrane would be needed. Maintaining laminar flow is achievable on the macro scale but maintaining a steady Reynolds number is difficult due to variations in pumping. This variation causes fluctuations at the reactant interfaces which can disrupt laminar flow and affect diffusion and crossover. However, self-pumping mechanisms can be difficult and expensive to produce on the macro-scale. In order to take advantage of hydrophobic effects, the surfaces need to be smooth to control the contact angle of water. To produce these surfaces on a large scale, the cost will significantly increase due to the close tolerances which are needed. Also, it is not evident whether using a carbon-dioxide based pumping system on the large scale is viable.\n\nMembraneless fuel cells can utilize self-pumping mechanisms but requires the use of fuel which release GHG's (greenhouse gases) and other unwanted products. To use an environmentally friendly fuel configuration (such as H-O), self pumping can be difficult. Thus, external pumps are required. However, for a rectangular channel, the pressure required increases proportional to the L, where L is a length unit of the cell. Thus, by decreasing the size of a cell from 10 cm to 1 cm, the required pressure will increase by 1000. For micro fuel cells, this pumping requirement requires high voltages. Although in some cases, Electroosmotic flow can be induced. However, for liquid mediums, high voltages are also required. Further, with decreasing size, surface tension effects also become significantly more important. For the fuel cell configuration with a carbon dioxide generating mechanism, the surface tension effects could also increase the pumping requirements drastically.\n\nThe thermodynamic potential of a fuel cell limits the amount of power that an individual cell can deliver. Therefore, in order to obtain more power, fuel cells must be connected in series or parallel (depending on whether greater current or voltage is desired). For large scale building and automobile power applications, macro fuel cells can be used because space is not necessarily the limiting constraint. However, for portable devices such as cell phones and laptops, macro fuel cells are often inefficient due to their space requirements lower run times. LFFCs however, are perfectly suited for these types of applications. The lack of a physical electrolytic membrane and energy dense fuels that can be used means that LFFC's can be produced at lower costs and smaller sizes. In most portable applications, energy density is more important than efficiency due to the low power requirements.\n"}
{"id": "18378757", "url": "https://en.wikipedia.org/wiki?curid=18378757", "title": "Milking pipeline", "text": "Milking pipeline\n\nA milking pipeline or milk pipeline is a component of a dairy farm animal-milking operation which is used to transfer milk from the animals to a cooling and storage bulk tank.\n\nIn small dairy farms with less than 100 cows, goats or sheep, the pipeline is installed above the animals' stalls and they are then are milked in sequence by moving down the row of stalls. The milking machine is a lightweight transportable hose assembly which is plugged into sealed access ports along the pipeline.\n\nIn the United States, for farmers who participate in the voluntary Dairy Herd Improvement Association, approximately once a month the milk volume from each animal is measured using additional portable metering devices inserted between the milker and the pipeline.\n\nIn large dairy farms with more than 100 animals, the pipeline is installed within a milking parlor that the animals walk through in order to be milked at fixed stations. Because the machine is stationary, it can include additional fixed equipment such as computerized milk-metering systems to measure volume, which would be cumbersome to use with portable milkers.\n\nIn both cases the pipeline is constructed out of stainless steel, which does not easily corrode and is resistant to most chemicals, though larger operations may use larger-diameter pipes in order to handle greater milk volumes.\n\nThere is usually a transition point to move the milk from the pipeline under vacuum to the bulk tank, which is at normal atmospheric pressure. This is done by having the milk flow into a receiver bowl or globe, which is a large hollow glass container with electronic liquid-detecting probes in the center. As the milk rises to a certain height in the bowl, a transfer pump is used to push it through a one-way check valve and into a pipe that transfers it to the bulk tank. When the level has dropped far enough in the bowl, the transfer pump turns off. Without the check valve, the milk in the bulk tank could be sucked back into the receiver bowl when the pump is not running.\n\nIn the event of electronics or pump failure, there is also usually a secondary bowl attached to the top of receiver bowl, which contains a float and a diaphragm valve. If the main receiver bowl overflows due to pump failure, the rising milk lifts the float in the secondary bowl, which will cut off vacuum to the entire milk pipeline and will prevent the milk or wash water from being sucked into the vacuum pump.\n\nSome milk handling systems eliminate the receiver bowl and transfer pump by having rubber seals on the bulk tank covers, to permit the entire tank to be under vacuum until milking is finished. Milk can then just flow directly by gravity from the pipeline into the bulk tank.\n\nThe pipeline and all milk handling systems are cleaned after every milking session using a washing system that first rinses out the remaining milk and then flushes cleaning solution through the piping to kill bacteria and remove \"milkstone\", a layer of scale mainly formed by cations like calcium and magnesium. The entire washing mechanism is operated very much like a household dishwasher with an automatic fill system, soap dispenser, and automatic drain opener.\n\nThe pipeline is usually set up so that the vacuum in the system that lifts milk up can also be used to drive the cleaning process. Rather than having a single line run to the bulk tank, typically a pair of lines transport milk by gravity flow to the receiver bowl and transfer pump. The high ends of these two lines are joined together to form a complete loop back to the receiver bowl.\n\nCleaning is accomplished by inserting a choke plug into one of the lines leading to the transfer pump, and sucking large volumes of water from a wash-water supply tank into the choked line. This choke plug is mounted on a rod, and is inserted into the line before cleaning, and pulled out for regular milking. Due to the choke, the water, which is sufficient to completely fill the pipe, is sucked up one side of the pipeline, over the high point joining the two pipeline sections, and then flows back to the receiver bowl and transfer pump through the unchoked line. The transfer pump is then used to move the cleaning solution from the receiver bowl back to the wash-water supply tank to restart the process.\n\nTypically, the inlet ports on the receiver globe are designed so that large slugs of wash water moving at high speed will enter on a tangent to the sides of the globe and rapidly spin around inside to assist in vigorous cleaning of the globe's interior. It is normal for wash water to overflow out the top of the globe and for some wash water to be sucked into the overflow chamber to also flush it out. During cleaning the bottom of the overflow chamber is connected to a drain channel on the receiver globe to permit water to flow out.\n\nFor the small-farm pipeline, portable milkers are inserted into this cleaning loop usually by sucking the cleaning solution out of the wash supply tank through the milker claw and outputting from the milker hoses into the choked end of the line. When the water returns to the receiver bowl, the transfer pump returns the water back to the milker's water pickup tank.\n\n\n"}
{"id": "30257635", "url": "https://en.wikipedia.org/wiki?curid=30257635", "title": "Minnesota Technology Center", "text": "Minnesota Technology Center\n\nThe Minnesota Technology Center, formerly called the BTC/Bunker and also known as the 511 Building in reference to its address, is a colocation center located in downtown Minneapolis, Minnesota, near U.S. Bank Stadium and the West Bank campus of the University of Minnesota. The 511 Building has been referred to as \"the most wired building in Minnesota\" and is a major source of fiber optic data transmission and reception. It is operated by Timeshare Systems Inc. It hosts an interchange between many major carriers, and is on the Internet Backbone.\n\nThe City of Minneapolis budgeted almost $750,000 to rent space at the center for a security command center during the 2018 Super Bowl and 2019 Final Four.\n\n"}
{"id": "54965195", "url": "https://en.wikipedia.org/wiki?curid=54965195", "title": "Miratorg", "text": "Miratorg\n\nMiratorg Agribusiness Holding is a Russian privately-held agribusiness company based in Moscow, Russia that was established in 1995. It is one of the largest producers and distributors of meat products in Russia.\n\nMiratorg is a subsidiary company of Agromir Ltd., which is based in Cyprus and \"has a 99.9% ownership interest in\" Miratorg.\n\nMiratorg and its subsidiaries are active in the processing, production and distribution of poultry, pork and beef, frozen vegetables, convenience foods, ready-to-eat meals and other foods. It is also active in crop farming.\n\n\n<br>\n"}
{"id": "34172701", "url": "https://en.wikipedia.org/wiki?curid=34172701", "title": "Museo del Hombre y la Tecnología", "text": "Museo del Hombre y la Tecnología\n\nMuseo del Hombre y la Tecnología (Museum of Man and Technology) is located in Salto Department, Uruguay. Its exhibitions are devoted to demonstrating how the evolution of technology has influenced the lives of the people of Uruguay and the rest of the world. The museum is housed in the old Central Market in a building constructed built between 1909 and 1915. Its rooms have interactive exhibits dedicated to the evolution of technology in the region. There are eleven rooms and a central area. The museum's director since 2010 is Professor Mario Trindade. The collections include agricultural machinery from the time of colonization, watches, cameras, vintage clothing, and archaeological finds from the past 2,000 years, discovered during the construction of the Salto Grande Dam, among others. A separate room has information about the dam and its operation.\n\n"}
{"id": "13646381", "url": "https://en.wikipedia.org/wiki?curid=13646381", "title": "Open Virtualization Format", "text": "Open Virtualization Format\n\nOpen Virtualization Format (OVF) is an open standard for packaging and distributing virtual appliances or, more generally, software to be run in virtual machines.\n\nThe standard describes an \"open, secure, portable, efficient and extensible format for the packaging and distribution of software to be run in virtual machines\". The OVF standard is not tied to any particular hypervisor or instruction set architecture. The unit of packaging and distribution is a so-called \"OVF Package\" which may contain one or more \"virtual systems\" each of which can be deployed to a virtual machine.\n\nIn September 2007 VMware, Dell, HP, IBM, Microsoft and XenSource submitted to the Distributed Management Task Force (DMTF) a proposal for OVF, then named \"Open Virtual Machine Format\".\n\nThe DMTF subsequently released the OVF Specification V1.0.0 as a preliminary standard in September, 2008, and V1.1.0 in January, 2010. In January 2013, DMTF released the second version of the standard, OVF 2.0 which applies to emerging cloud use cases and provides important developments from OVF 1.0 including improved network configuration support and package encryption capabilities for safe delivery.\n\nANSI has ratified OVF 1.1.0 as ANSI standard INCITS 469-2010.\n\nOVF 1.1 was adopted as an International Standard in August 2011 by the Joint Technical Committee 1 (JTC 1) of the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC)\n\nOVF 2.0 brings an enhanced set of capabilities to the packaging of virtual machines, making the standard applicable to a broader range of cloud use cases that are emerging as the industry enters the cloud era. The most significant improvements include support for network configuration along with the ability to encrypt the package to ensure safe delivery.\n\nAn OVF package consists of several files placed in one directory. An OVF package always contains exactly one OVF descriptor (a file with extension .ovf). The OVF descriptor is an XML file which describes the packaged virtual machine; it contains the metadata for the OVF package, such as name, hardware requirements, references to the other files in the OVF package and human-readable descriptions. In addition to the OVF descriptor, the OVF package will typically contain one or more disk images, and optionally certificate files and other auxiliary files.\n\nThe entire directory can be distributed as an Open Virtual Appliance (OVA\")\" package, which is a tar archive file with the OVF directory inside.\n\nOVF has generally been positively received. Several virtualization players in the industry have announced support for OVF.\n\n"}
{"id": "41148", "url": "https://en.wikipedia.org/wiki?curid=41148", "title": "Optical amplifier", "text": "Optical amplifier\n\nAn optical amplifier is a device that amplifies an optical signal directly, without the need to first convert it to an electrical signal. An optical amplifier may be thought of as a laser without an optical cavity, or one in which feedback from the cavity is suppressed. Optical amplifiers are important in optical communication and laser physics. They are used as optical repeaters in the long distance fiberoptic cables which carry much of the world's telecommunication links.\n\nThere are several different physical mechanisms that can be used to amplify a light signal, which correspond to the major types of optical amplifiers. In doped fiber amplifiers and bulk lasers, stimulated emission in the amplifier's gain medium causes amplification of incoming light. In semiconductor optical amplifiers (SOAs), electron-hole recombination occurs. In Raman amplifiers, Raman scattering of incoming light with phonons in the lattice of the gain medium produces photons coherent with the incoming photons. Parametric amplifiers use parametric amplification.\n\nAlmost any laser active gain medium can be pumped to produce gain for light at the wavelength of a laser made with the same material as its gain medium. Such amplifiers are commonly used to produce high power laser systems. Special types such as regenerative amplifiers and chirped-pulse amplifiers are used to amplify ultrashort pulses.\n\nSolid-state amplifiers are optical amplifiers that uses a wide range of doped solid-state materials (Nd:YAG, Yb:YAG, Ti:Sa) and different geometries (disk, slab, rod) to amplify optical signals. The variety of materials allows the amplification of different wavelength while the shape of the medium can distinguish between more suitable for energy of average power scaling. Beside their use in fundamental research from gravitational wave detection to high energy physics at NIF they can also be found in many today’s ultra short pulsed lasers.\n\nDoped fiber amplifiers (DFAs) are optical amplifiers that use a doped optical fiber as a gain medium to amplify an optical signal. They are related to fiber lasers. The signal to be amplified and a pump laser are multiplexed into the doped fiber, and the signal is amplified through interaction with the doping ions. The most common example is the Erbium Doped Fiber Amplifier (EDFA), where the core of a silica fiber is doped with trivalent erbium ions and can be efficiently pumped with a laser at a wavelength of 980 nm or 1,480 nm, and exhibits gain in the 1,550 nm region.\n\nAn \"erbium-doped waveguide amplifier\" (\"EDWA\") is an optical amplifier that uses a waveguide to boost an optical signal.\n\nAmplification is achieved by stimulated emission of photons from dopant ions in the doped fiber. The pump laser excites ions into a higher energy from where they can decay via stimulated emission of a photon at the signal wavelength back to a lower energy level. The excited ions can also decay spontaneously (spontaneous emission) or even through nonradiative processes involving interactions with phonons of the glass matrix. These last two decay mechanisms compete with stimulated emission reducing the efficiency of light amplification.\n\nThe \"amplification window\" of an optical amplifier is the range of optical wavelengths for which the amplifier yields a usable gain. The amplification window is determined by the spectroscopic properties of the dopant ions, the glass structure of the optical fiber, and the wavelength and power of the pump laser.\n\nAlthough the electronic transitions of an isolated ion are very well defined, broadening of the energy levels occurs when the ions are incorporated into the glass of the optical fiber and thus the amplification window is also broadened. This broadening is both homogeneous (all ions exhibit the same broadened spectrum) and inhomogeneous (different ions in different glass locations exhibit different spectra). Homogeneous broadening arises from the interactions with phonons of the glass, while inhomogeneous broadening is caused by differences in the glass sites where different ions are hosted. Different sites expose ions to different local electric fields, which shifts the energy levels via the Stark effect. In addition, the Stark effect also removes the degeneracy of energy states having the same total angular momentum (specified by the quantum number J). Thus, for example, the trivalent erbium ion (Er) has a ground state with J = 15/2, and in the presence of an electric field splits into J + 1/2 = 8 sublevels with slightly different energies. The first excited state has J = 13/2 and therefore a Stark manifold with 7 sublevels. Transitions from the J = 13/2 excited state to the J= 15/2 ground state are responsible for the gain at 1.5 µm wavelength. The gain spectrum of the EDFA has several peaks that are smeared by the above broadening mechanisms. The net result is a very broad spectrum (30 nm in silica, typically). The broad gain-bandwidth of fiber amplifiers make them particularly useful in \nwavelength-division multiplexed communications systems as a single amplifier can be utilized to amplify all signals being carried on a fiber and whose wavelengths fall within the gain window.\n\nA relatively high-powered beam of light is mixed with the input signal using a wavelength selective coupler (WSC). The input signal and the excitation light must be at significantly different wavelengths.\nThe mixed light is guided into a section of fiber with erbium ions included in the core.\nThis high-powered light beam excites the erbium ions to their higher-energy state.\nWhen the photons belonging to the signal at a different wavelength from the pump light meet the excited erbium atoms, the erbium atoms give up some of their energy to the signal and return to their lower-energy state.\nA significant point is that the erbium gives up its energy in the form of additional photons which are exactly in the same phase and direction as the signal being amplified. So the signal is amplified along its direction of travel only. This is not unusual – when an atom “lases” it always gives up its energy in the same direction and phase as the incoming light. Thus all of the additional signal power is guided in the same fiber mode as the incoming signal. There is usually an isolator placed at the output to prevent reflections returning from the attached fiber. Such reflections disrupt amplifier operation and in the extreme case can cause the amplifier to become a laser. The erbium doped amplifier is a high gain amplifier.\n\nThe principal source of noise in DFAs is Amplified Spontaneous Emission (ASE), which has a spectrum approximately the same as the gain spectrum of the amplifier. Noise figure in an ideal DFA is 3 dB, while practical amplifiers can have noise figure as large as 6–8 dB.\n\nAs well as decaying via stimulated emission, electrons in the upper energy level can also decay by spontaneous emission, which occurs at random, depending upon the glass structure and inversion level. Photons are emitted spontaneously in all directions, but a proportion of those will be emitted in a direction that falls within the numerical aperture of the fiber and are thus captured and guided by the fiber. Those photons captured may then interact with other dopant ions, and are thus amplified by stimulated emission. The initial spontaneous emission is therefore amplified in the same manner as the signals, hence the term \"Amplified Spontaneous Emission\". ASE is emitted by the amplifier in both the forward and reverse directions, but only the forward ASE is a direct concern to system performance since that noise will co-propagate with the signal to the receiver where it degrades system performance. Counter-propagating ASE can, however, lead to degradation of the amplifier's performance since the ASE can deplete the inversion level and thereby reduce the gain of the amplifier.\n\nGain is achieved in a DFA due to population inversion of the dopant ions. The inversion level of a DFA is set, primarily, by the power of the pump wavelength and the power at the amplified wavelengths. As the signal power increases, or the pump power decreases, the inversion level will reduce and thereby the gain of the amplifier will be reduced. This effect is known as gain saturation – as the signal level increases, the amplifier saturates and cannot produce any more output power, and therefore the gain reduces. Saturation is also commonly known as gain compression.\n\nTo achieve optimum noise performance DFAs are operated under a significant amount of gain compression (10 dB typically), since that reduces the rate of spontaneous emission, thereby reducing ASE. Another advantage of operating the DFA in the gain saturation region is that small fluctuations in the input signal power are reduced in the output amplified signal: smaller input signal powers experience larger (less saturated) gain, while larger input powers see less gain.\n\nThe leading edge of the pulse is amplified, until the saturation energy of the gain medium is reached. In some condition, the width (FWHM) of the pulse is reduced.\n\nDue to the inhomogeneous portion of the linewidth broadening of the dopant ions, the gain spectrum has an inhomogeneous component and gain saturation occurs, to a small extent, in an inhomogeneous manner. This effect is known as \"spectral hole burning\" because a high power signal at one wavelength can 'burn' a hole in the gain for wavelengths close to that signal by saturation of the inhomogeneously broadened ions. Spectral holes vary in width depending on the characteristics of the optical fiber in question and the power of the burning signal, but are typically less than 1 nm at the short wavelength end of the C-band, and a few nm at the long wavelength end of the C-band. The depth of the holes are very small, though, making it difficult to observe in practice.\n\nAlthough the DFA is essentially a polarization independent amplifier, a small proportion of the dopant ions interact preferentially with certain polarizations and a small dependence on the polarization of the input signal may occur (typically < 0.5 dB). This is called Polarization Dependent Gain (PDG).\nThe absorption and emission cross sections of the ions can be modeled as ellipsoids with the major axes aligned at random in all directions in different glass sites. The random distribution of the orientation of the ellipsoids in a glass produces a macroscopically isotropic medium, but a strong pump laser induces an anisotropic distribution by selectively exciting those ions that are more aligned with the optical field vector of the pump. Also, those excited ions aligned with the signal field produce more stimulated emission. The change in gain is thus dependent on the alignment of the polarizations of the pump and signal lasers – i.e. whether the two lasers are interacting with the same sub-set of dopant ions or not. \nIn an ideal doped fiber without birefringence, the PDG would be inconveniently large. Fortunately, in optical fibers small amounts of birefringence are always present and, furthermore, the fast and slow axes vary randomly along the fiber length. A typical DFA has several tens of meters, long enough to already show this randomness of the birefringence axes. These two combined effects (which in transmission fibers give rise to polarization mode dispersion) produce a misalignment of the relative polarizations of the signal and pump lasers along the fiber, thus tending to average out the PDG. The result is that PDG is very difficult to observe in a single amplifier (but is noticeable in links with several cascaded amplifiers).\n\nThe erbium-doped fiber amplifier (EDFA) is the most deployed fiber amplifier as its amplification window coincides with the third transmission window of silica-based optical fiber.\n\nTwo bands have developed in the third transmission window – the \"Conventional\", or C-band, from approximately 1525 nm – 1565 nm, and the \"Long\", or L-band, from approximately 1570 nm to 1610 nm. Both of these bands can be amplified by EDFAs, but it is normal to use two different amplifiers, each optimized for one of the bands.\n\nThe principal difference between C- and L-band amplifiers is that a longer length of doped fiber is used in L-band amplifiers. The longer length of fiber allows a lower inversion level to be used, thereby giving at longer wavelengths (due to the band-structure of Erbium in silica) while still providing a useful amount of gain.\n\nEDFAs have two commonly used pumping bands – 980 nm and 1480 nm. The 980 nm band has a higher absorption cross-section and is generally used where low-noise performance is required. The absorption band is relatively narrow and so wavelength stabilised laser sources are typically needed. The 1480 nm band has a lower, but broader, absorption cross-section and is generally used for higher power amplifiers. A combination of 980 nm and 1480 nm pumping is generally utilised in amplifiers.\n\nGain and lasing in Erbium-doped fibers were first demonstrated in 1986–87 by two groups; one including David N. Payne, R. Mears, I.M Jauncey and L. Reekie, from the University of Southampton and one from AT&T Bell Laboratories, consisting of E. Desurvire, P. Becker, and J. Simpson. The dual-stage optical amplifier which enabled Dense Wave Division Multiplexing (DWDM,) was invented by Stephen B. Alexander at Ciena Corporation.\n\nThulium doped fiber amplifiers have been used in the S-band (1450–1490 nm) and Praseodymium doped amplifiers in the 1300 nm region. However, those regions have not seen any significant commercial use so far and so those amplifiers have not been the subject of as much development as the EDFA. However, Ytterbium doped fiber lasers and amplifiers, operating near 1 micrometre wavelength, have many applications in industrial processing of materials, as these devices can be made with extremely high output power (tens of kilowatts).\n\nSemiconductor optical amplifiers (SOAs) are amplifiers which use a semiconductor to provide the gain medium. These amplifiers have a similar structure to Fabry–Pérot laser diodes but with anti-reflection design elements at the end faces. Recent designs include anti-reflective coatings and tilted wave guide and window regions which can reduce end face reflection to less than 0.001%. Since this creates a loss of power from the cavity which is greater than the gain, it prevents the amplifier from acting as a laser. Another type of SOA consists of two regions. One part has a structure of a Fabry-Pérot laser diode and the other has a tapered geometry in order to reduce the power density on the output facet.\n\nSemiconductor optical amplifiers are typically made from group III-V compound semiconductors such as GaAs/AlGaAs, InP/InGaAs, InP/InGaAsP and InP/InAlGaAs, though any direct band gap semiconductors such as II-VI could conceivably be used. Such amplifiers are often used in telecommunication systems in the form of fiber-pigtailed components, operating at signal wavelengths between 0.85 µm and 1.6 µm and generating gains of up to 30 dB.\n\nThe semiconductor optical amplifier is of small size and electrically pumped. It can be potentially less expensive than the EDFA and can be integrated with semiconductor lasers, modulators, etc. However, the performance is still not comparable with the EDFA. The SOA has higher noise, lower gain, moderate polarization dependence and high nonlinearity with fast transient time. The main advantage of SOA is that all four types of nonlinear operations (cross gain modulation, cross phase modulation, wavelength conversion and four wave mixing) can be conducted. Furthermore, SOA can be run with a low power laser.\nThis originates from the short nanosecond or less upper state lifetime, so that the gain reacts rapidly to changes of pump or signal power and the changes of gain also cause phase changes which can distort the signals.\nThis nonlinearity presents the most severe problem for optical communication applications. However it provides the possibility for gain in different wavelength regions from the EDFA. \"Linear optical amplifiers\" using gain-clamping techniques have been developed.\n\nHigh optical nonlinearity makes semiconductor amplifiers attractive for all optical signal processing like all-optical switching and wavelength conversion. There has been much research on semiconductor optical amplifiers as elements for optical signal processing, wavelength conversion, clock recovery, signal demultiplexing, and pattern recognition.\n\nA recent addition to the SOA family is the vertical-cavity SOA (VCSOA). These devices are similar in structure to, and share many features with, vertical-cavity surface-emitting lasers (VCSELs). The major difference when comparing VCSOAs and VCSELs is the reduced mirror reflectivity used in the amplifier cavity. With VCSOAs, reduced feedback is necessary to prevent the device from reaching lasing threshold. Due to the extremely short cavity length, and correspondingly thin gain medium, these devices exhibit very low single-pass gain (typically on the order of a few percent) and also a very large free spectral range (FSR). The small single-pass gain requires relatively high mirror reflectivity to boost the total signal gain. In addition to boosting the total signal gain, the use of the resonant cavity structure results in a very narrow gain bandwidth; coupled with the large FSR of the optical cavity, this effectively limits operation of the VCSOA to single-channel amplification. Thus, VCSOAs can be seen as amplifying filters.\n\nGiven their vertical-cavity geometry, VCSOAs are resonant cavity optical amplifiers that operate with the input/output signal entering/exiting normal to the wafer surface. In addition to their small size, the surface normal operation of VCSOAs leads to a number of advantages, including low power consumption, low noise figure, polarization insensitive gain, and the ability to fabricate high fill factor two-dimensional arrays on a single semiconductor chip. These devices are still in the early stages of research, though promising preamplifier results have been demonstrated. Further extensions to VCSOA technology are the demonstration of wavelength tunable devices. These MEMS-tunable vertical-cavity SOAs utilize a microelectromechanical systems (MEMS) based tuning mechanism for wide and continuous tuning of the peak gain wavelength of the amplifier. SOAs have a more rapid gain response, which is in the order of 1 to 100 ps.\n\nFor high output power and broader wavelength range, tapered amplifiers are used. These amplifiers consist of a lateral single-mode section and a section with a tapered structure, where the laser light is amplified. The tapered structure leads to a reduction of the power density at the output facet.\n\nTypical parameters:\n\nIn a Raman amplifier, the signal is intensified by Raman amplification. Unlike the EDFA and SOA the amplification effect is achieved by a nonlinear interaction between the signal and a pump laser within an optical fiber. There are two types of Raman amplifier: distributed and lumped. A distributed Raman amplifier is one in which the transmission fiber is utilised as the gain medium by multiplexing a pump wavelength with signal wavelength, while a lumped Raman amplifier utilises a dedicated, shorter length of fiber to provide amplification. In the case of a lumped Raman amplifier, a highly nonlinear fiber with a small core is utilised to increase the interaction between signal and pump wavelengths, and thereby reduce the length of fiber required.\n\nThe pump light may be coupled into the transmission fiber in the same direction as the signal (co-directional pumping), in the opposite direction (contra-directional pumping) or both. Contra-directional pumping is more common as the transfer of noise from the pump to the signal is reduced.\n\nThe pump power required for Raman amplification is higher than that required by the EDFA, with in excess of 500 mW being required to achieve useful levels of gain in a distributed amplifier. Lumped amplifiers, where the pump light can be safely contained to avoid safety implications of high optical powers, may use over 1 W of optical power.\n\nThe principal advantage of Raman amplification is its ability to provide distributed amplification within the transmission fiber, thereby increasing the length of spans between amplifier and regeneration sites. The amplification bandwidth of Raman amplifiers is defined by the pump wavelengths utilised and so amplification can be provided over wider, and different, regions than may be possible with other amplifier types which rely on dopants and device design to define the amplification 'window'.\n\nRaman amplifiers have some fundamental advantages. First, Raman gain exists in every fiber, which provides a cost-effective means of upgrading from the terminal ends. Second, the gain is nonresonant, which means that gain is available over the entire transparency region of the fiber ranging from approximately 0.3 to 2µm. A third advantage of Raman amplifiers is that the gain spectrum can be tailored by adjusting the pump wavelengths. For instance, multiple pump lines can be used to increase the optical bandwidth, and the pump distribution determines the gain flatness. Another advantage of Raman amplification is that it is a relatively broad-band amplifier with a bandwidth > 5 THz, and the gain is reasonably flat over a wide wavelength range.\n\nHowever, a number of challenges for Raman amplifiers prevented their earlier adoption. First, compared to the EDFAs, Raman amplifiers have relatively poor pumping efficiency at lower signal powers. Although a disadvantage, this lack of pump efficiency also makes gain clamping easier in Raman amplifiers. Second, Raman amplifiers require a longer gain fiber. However, this disadvantage can be mitigated by combining gain and the dispersion compensation in a single fiber. A third disadvantage of Raman amplifiers is a fast response time, which gives rise to new sources of noise, as further discussed below. Finally, there are concerns of nonlinear penalty in the amplifier for the WDM signal channels.\n\n\"Note: The text of an earlier version of this article was taken from the public domain Federal Standard 1037C.\"\n\nAn optical parametric amplifier allows the amplification of a weak signal-impulse in a noncentrosymmetric nonlinear medium (e.g. Beta barium borate (BBO)). In contrast to the previously mentioned amplifiers, which are mostly used in telecommunication environments, this type finds its main application in expanding the frequency tunability of ultrafast solid-state lasers (e.g. Ti:sapphire). By using a noncollinear interaction geometry optical parametric amplifiers are capable of extremely broad amplification bandwidths.\n\nThe adoption of high power fiber lasers as an industrial material processing tool has been ongoing for several years and is now expanding into other markets including the medical and scientific markets. One key enhancement enabling penetration into the scientific market has been the improvements in high finesse fiber amplifiers, which are now capable of delivering single frequency linewidths (<5 kHz) together with excellent beam quality and stable linearly polarized output. Systems meeting these specifications, have steadily progressed in the last few years from a few Watts of output power, initially to the 10s of Watts and now into the 100s of Watts power level. This power scaling has been achieved with developments in the fiber technology, such as the adoption of stimulated brillouin scattering (SBS) suppression/mitigation techniques within the fiber, along with improvements in the overall amplifier design. The latest generation of high finesse, high power fiber amplifiers now deliver power levels exceeding what is available from commercial solid-state single frequency sources and are opening up new scientific applications as a result of the higher power levels and stable optimized performance.\n\nThere are several simulation tools that can be used to design optical amplifiers. Popular commercial tools have been developed by Optiwave Systems and VPI Systems.\n\n\n"}
{"id": "48487131", "url": "https://en.wikipedia.org/wiki?curid=48487131", "title": "Polygone Scientifique", "text": "Polygone Scientifique\n\nThe Polygone Scientifique (en: Scientific Polygon) is a neighborhood of the city of Grenoble in France. It includes a significant number of research centers in a presque-isle between Isère and Drac.\n\nPolygon hosts in 1956 the first French Atomic Energy Commission (CEA) outside Paris and created by Professor Louis Néel. In 1962, it hosts a campus CNRS. In 1967, the was founded by CEA and became one of the world’s largest organizations for applied research in microelectronics and nanotechnology.\n\nThree international organizations are implanted between 1973 and 1988 with the Institut Laue–Langevin, the European Synchrotron Radiation Facility and one of the five branches of the European Molecular Biology Laboratory. In 2006, the complex Minatec specializing in nanotechnology opens on the Polygon and in 2007, the Institut Néel, specializing in condensed matter physics, is founded.\n\nNational Laboratory for Intense Magnetic Fields has also numerous collaborations in terms of technical and technological innovations with these institutions.\n\nIn 2008, the new innovation campus is called GIANT (Grenoble Innovation for Advanced New Technologies).\n\nIn 2012, Clinatec is founded on Polygone Scientifique by the professor Alim-Louis Benabid.\n\nThe polygon is served by Grenoble tramway.\n\n\n\n"}
{"id": "1861516", "url": "https://en.wikipedia.org/wiki?curid=1861516", "title": "Prepayment for service", "text": "Prepayment for service\n\nPrepaid refers to services paid for in advance. Examples include postage stamps, attorneys, tolls, public transit cards like the Greater London Oyster card, pay as you go cell phones, and stored-value cards such as gift cards and preloaded credit cards.\n\nPrepaid services and goods are sometimes targeted to marginal customers by retailers. Prepaid options can have substantial cost reductions over postpaid counterparts because they allow customers to monitor and budget usage in advance.\n\nUnlike postpaid or contract based services, prepaid accounts can be obtained with cash. As a result, they can be established by people who have minimal identification or poor credit ratings. Minors, immigrants, students, defaulters, and those on low incomes are typical prepaid customers.\n\nRecent statistics (OECD \"Communications Outlook\" 2005) indicate that 40% of the total mobile phone market in the OECD region consists of prepaid accounts. This service was invented by Portuguese provider TMN, while researching for a means to increase penetration of mobile technology by allowing anyone to buy a fully working (usually requiring a quick and simple activation process) mobile phone on any supermarket or electronics store. By removing the complications inherent to the contract system, this allowed the mobile communications user base to grow incredibly fast. In many countries this type of service became the predominant one, shortly after introduction, by providing both consumers and service providers with considerable advantages over the traditional method. In some countries, such as Italy or Mexico, market share of prepaid can be as high as 90%. In other countries, such as Finland or South Korea, the figure drops to about 2%.\n\nA Prepaid Call (or even Calling Card) is actually established through two calls, and the first is either through an 800 Toll Free Number, and even some from the big telephone companies have them available with international toll free numbers, or you will find many that offer calling through a local phone number, usually printed on the back that offer cheaper rates but the first call costs are passed on to you, the consumer.\n\nYou place this first call to get to the providers platform, and once connected you are authenticated via a PIN or as seen in Europe or around other parts of the world the card may have an electronic chip on it, like you may have seen on the American Express Blue card. If you use the local number provided from your home or cell phone you pay for the cost of the first call, you can see this on your bills even if you don't get connected. The reason that 800 toll free is more expensive is because the Prepaid Calling Card Company pays for the attempts and even when you just call to check your balance, you only pay when you are actually connected. Many Local Telephone Companies don't show local usage so you may not see that you are paying for a local call every time you make an attempt. This is not true when you call a toll free number from a land line but with Cell phones you do use air time. With most local phone companies you can get call detail to support this.\n\nOnce you are authenticated you are usually presented with your balance then prompted for your destination number (the second call), and usually given the time left on your card based on the number you just dialed. Then you are connected.\n\nWith smaller companies beware the hidden charges when the rates are too low to be true. They mave have hidden fees or poorly advertised fees, like 3 minute rounding, connection fees, monthly, weekly, daily, or even maintenance fees. Look around when you buy these smaller name brands to make sure you know what you are buying.\n\nThe pseudonymity enabled by prepaid services has recently become a concern with law enforcement agencies that consider it a safe haven for criminals and terrorists. As a result, a number of countries including Australia, Germany, Indonesia, Japan, Malaysia, Mozambique, Norway, Singapore, South Africa, Switzerland, and Thailand have passed laws to require that all prepaid customers register their personal information with their mobile carrier.\n\nIn some countries the law requires that customers notify their mobile carrier when transferring ownership of a prepaid phone or SIM card.\n\nIn Australia, the prepaid registration policy is part of a larger law enforcement initiative that includes the creation and maintenance of an Integrated Public Number Database.\n\nIn 1999, Texas deregulated the energy industry by allowing third-party affiliates to offer electricity services. As a result, a handful of Texas companies are offering the ability to prepay your electricity, with the balance going toward the posted kilowatt hours rate. This system is now common in China and Indonesia, as a way to forestall non-payment of bills.\n\nInsurance premiums are usually paid at or before the start of the insurance period, the period of cover, but the premium may at times be payable in instalments during the insurance period.\n\n\n"}
{"id": "36603", "url": "https://en.wikipedia.org/wiki?curid=36603", "title": "Rotaxane", "text": "Rotaxane\n\nA rotaxane is a mechanically interlocked molecular architecture consisting of a \"dumbbell shaped molecule\" which is threaded through a \"macrocycle\" (see graphical representation). The name is derived from the Latin for wheel (rota) and axle (axis). The two components of a rotaxane are kinetically trapped since the ends of the dumbbell (often called stoppers) are larger than the internal diameter of the ring and prevent dissociation (unthreading) of the components since this would require significant distortion of the covalent bonds.\n\nMuch of the research concerning rotaxanes and other mechanically interlocked molecular architectures, such as catenanes, has been focused on their efficient synthesis or their utilization as artificial molecular machines. However, examples of rotaxane substructure have been found in naturally occurring peptides, including: cystine knot peptides, cyclotides or lasso-peptides such as microcin J25.\n\nThe earliest reported synthesis of a rotaxane in 1967 relied on the statistical probability that if two halves of a dumbbell-shaped molecule were reacted in the presence of a macrocycle that some small percentage would connect through the ring. To obtain a reasonable quantity of rotaxane, the macrocycle was attached to a solid-phase support and treated with both halves of the dumbbell 70 times and then severed from the support to give a 6% yield. However, the synthesis of rotaxanes has advanced significantly and efficient yields can be obtained by preorganizing the components utilizing hydrogen bonding, metal coordination, hydrophobic forces, covalent bonds, or coulombic interactions. The three most common strategies to synthesize rotaxane are \"capping\", \"clipping\", and \"slipping\", though others do exist. Recently, Leigh and co-workers described a new pathway to mechanically interlocked architectures involving a transition-metal center that can catalyse a reaction through the cavity of a macrocycle.\n\nSynthesis via the capping method relies strongly upon a thermodynamically driven template effect; that is, the \"thread\" is held within the \"macrocycle\" by non-covalent interactions, for example rotaxinations with cyclodextrin macrocycles involve exploitation of the hydrophobic effect. This dynamic complex or pseudorotaxane is then converted to the rotaxane by reacting the ends of the threaded guest with large groups, preventing disassociation.\n\nThe clipping method is similar to the capping reaction except that in this case the dumbbell shaped molecule is complete and is bound to a partial macrocycle. The partial macrocycle then undergoes a ring closing reaction around the dumbbell-shaped molecule, forming the rotaxane.\n\nThe method of slipping is one which exploits the thermodynamic stability of the rotaxane. If the end groups of the dumbbell are an appropriate size it will be able to reversibly thread through the macrocycle at higher temperatures. By cooling the dynamic complex, it becomes kinetically trapped as a rotaxane at the lower temperature.\n\nLeigh and co-workers recently began to explore a strategy in which template ions could also play an active role in promoting the crucial final covalent bond forming reaction that captures the interlocked structure (i.e., the metal has a dual function, acting as a template for entwining the precursors and catalyzing covalent bond formation between the reactants).\n\nRotaxane-based molecular machines have been of initial interest for their potential use in molecular electronics as logic molecular switching elements and as molecular shuttles. These molecular machines are usually based on the movement of the macrocycle on the dumbbell. The macrocycle can rotate around the axis of the dumbbell like a wheel and axle or it can slide along its axis from one site to another. Controlling the position of the macrocycle allows the rotaxane to function as a molecular switch, with each possible location of the macrocycle corresponding to a different state. These rotaxane machines can be manipulated both by chemical and photochemical inputs. Rotaxane based systems have also been shown to function as molecular muscles. In 2009, there was a report of a \"domino effect\" from one extremity to the other in a Glycorotaxane Molecular Machine. In this case, the \"C\" or \"C\" chair-like conformation of the mannopyranoside stopper can be controlled, depending on the localization of the macrocycle. In 2012, unique pseudo-macrocycles consisting of double-lasso molecular machines (also called rotamacrocycles) were reported in Chem. Sci. These structures can be tightened or loosened depending on pH. A controllable jump rope movement was also observed in these new molecular machines.\n\nPotential application as long-lasting dyes is based on the enhanced stability of the inner portion of the dumbbell-shaped molecule. Studies with cyclodextrin-protected rotaxane azo dyes established this characteristic. More reactive squaraine dyes have also been shown to have enhanced stability by preventing nucleophilic attack of the inner squaraine moiety. The enhanced stability of rotaxane dyes is attributed to the insulating effect of the macrocycle, which is able to block interactions with other molecules.\n\nIn a nanorecording application, a certain rotaxane is deposited as a Langmuir–Blodgett film on ITO-coated glass. When a positive voltage is applied with the tip of a scanning tunneling microscope probe, the rotaxane rings in the tip area switch to a different part of the dumbbell and the resulting new conformation makes the molecules stick out 0.3 nanometer from the surface. This height difference is sufficient for a memory dot. It is not yet known how to erase such a nanorecording film.\n\nAccepted nomenclature is to designate the number of components of the rotaxane in brackets as a prefix. Therefore, the a rotaxane consisting of a single dumbbell-shaped axial molecule with a single macrocycle around its shaft is called a [2]rotaxane, and two cyanostar molecules around the central phosphate group of dialkylphosphate is a [3]rotaxane.\n\n"}
{"id": "4875198", "url": "https://en.wikipedia.org/wiki?curid=4875198", "title": "Sector (instrument)", "text": "Sector (instrument)\n\nThe sector, also known as a proportional compass or military compass, was a major calculating instrument in use from the end of the sixteenth century until the nineteenth century. It is an instrument consisting of two rulers of equal length joined by a hinge. A number of scales are inscribed upon the instrument which facilitate various mathematical calculations. It was used for solving problems in proportion, trigonometry, multiplication and division, and for various functions, such as squares and cube roots. Its several scales permitted easy and direct solutions of problems in gunnery, surveying and navigation. The sector derives its name from the fourth proposition of the sixth book of Euclid, where it is demonstrated that similar triangles have their like sides proportional. It has four parts, two legs with a pivot (the articulation), a quadrant and a clamp (the curved part at the end of the leg) that enables the compass to function as a gunner's quadrant.\n\nThe sector was invented, essentially simultaneously and independently, by a number of different people prior to the start of the 17th century.\n\nFabrizio Mordente (1532 – ca 1608) was an Italian mathematician who is best known for his invention of the \"proportional eight-pointed compass\" which has two arms with cursors that allow the solution of problems in measuring the circumference, area and angles of a circle. In 1567 he published a single sheet treatise in Venice showing illustrations of his device. In 1585 Giordano Bruno used Mordente's compass to refute Aristotle's hypothesis on the incommensurability of infinitesimals, thus confirming the existence of the \"minimum\" which laid the basis of his own atomic theory.\n\nCredit for the invention is often given to either Thomas Hood, a British mathematician, or to the Italian mathematician and astronomer Galileo Galilei. Galileo, with the help of his personal instrument maker Marc'Antonio Mazzoleni, created more than 100 copies of his military compass design and trained students in its use between 1595 and 1598. Of the credited inventors, Galileo is certainly the most famous, and earlier studies usually attributed its invention to him.\n\nThe following is a description of the instrument as it was constructed by Galileo, and for which he wrote a popular manual. The terminating values are arbitrary and varied from manufacturer to manufacturer.\n\nThe innermost scales of the instrument are called the Arithmetic Lines from their division in arithmetical progression, that is, by equal additions which proceed out to the number 250. It is a linear scale generated by the function formula_1, where \"n\" is an integer between 1 and 250, inclusive, and \"L\" is the length at mark 250.\n\nThe next scales are called the Geometric Lines and are divided in geometric progression out to 50. The lengths on the geometric lines vary as the square root of the labeled values. If \"L\" represents the length at 50, then the generating function is: formula_2, where \"n\" is a positive integer less than or equal to 50.\n\nThe Stereometric Lines are so called because their divisions are according to the ratios of solid bodies, out to 148. One of this scale's applications is to calculate, when given one side of any solid body, the side of a similar one that has a given volume ratio to the first. If \"L\" is the scale length at 148, then the scale-generating function is: formula_3, where \"n\" is a positive integer less than or equal to 148.\n\nThese lines have divisions on which appeared these symbols: Au, Pb, Ag, Cu, Fe, Sn, Mar, Sto, (gold, lead, silver, copper, iron, tin, marble, and stone). From these you can get the ratios and differences of specific weight found between the materials. With the instrument set at any opening, the intervals between any correspondingly marked pair of points will give the diameters of balls (or sides of other solid bodies) similar to one another and equal in weight.\n\nFrom the given information, the side length and the number of sides, the Polygraphic lines yield the radius of the circle that will contain the required regular polygon. If the polygon required has \"n\" sides, then the central angle opposite one side will be 360/\"n\".\n\nTetragonic Lines are so called from their principal use, which is to square all regular areas and the circle as well. The divisions of this scale use the function: formula_4, between the values of 3 and 13.\n\nThese Added Lines are marked with two series of numbers, of which the outer series begins at a certain mark called \"D\" followed by the numbers 1, 2, 3, 4, and so on out to 18. The inner series begins from this mark \"D\", going on then to 1, 2, 3, 4, and so on, also out to 18. They were used in conjunction with the other scales for a number of complex calculations.\n\nThe instrument can be used to graphically solve questions of proportion, and relies on the principle of similar triangles. Its vital feature is a pair of jointed legs, which carry paired geometrical scales. In use, problems are set up using a pair of dividers to determine the appropriate opening of the jointed legs and the answer is taken off directly as a dimension using the dividers. Specialised scales for area, volume and trigonometrical calculations, as well as simpler arithmetical problems were quickly added to the basic design.\n\nDifferent versions of the instrument also took different forms and adopted additional features. The type publicised by Hood was intended for use as a surveying instrument, and included not only sights and a mounting socket for attaching the instrument to a pole or post, but also an arc scale and an additional sliding leg. Galileo's earliest examples were intended to be used as gunner's levels as well as calculating devices.\n\n\n"}
{"id": "54101072", "url": "https://en.wikipedia.org/wiki?curid=54101072", "title": "Separation of content and presentation", "text": "Separation of content and presentation\n\nSeparation of content and presentation (or separation of content and style) is the separation of concerns design principle as applied to the authoring and presentation of content. Under this principle, visual and design aspects (presentation and style) are separated from the core material and structure (content) of a document. A typical analogy used to explain this principle is the distinction between the human skeleton (as the structural component) and human flesh (as the visual component) which makes up the body's appearance. Common applications of this principle are seen in Web design (HTML and CSS) and markup language (see LaTeX).\n\nThis principle is not a rigid guideline, but serves more as best practice for keeping appearance and structure separate. In many cases, the design and development aspects of a project are performed by different people, so keeping both aspects separated ensures both initial production accountability and later maintenance simplification, as in the \"don't repeat yourself\" (DRY) principle.\n\nLaTeX is a document markup language that focuses primarily on the content and structure of a document. With this methodology, academic writings and publication can be structured and styled with minimal effort by the creator, and can be quickly reformatted for different purposes.\n\n"}
{"id": "27311331", "url": "https://en.wikipedia.org/wiki?curid=27311331", "title": "Six factor formula", "text": "Six factor formula\n\nThe six-factor formula is used in nuclear engineering to determine the multiplication of a nuclear chain reaction in a non-infinite medium.\n\nThe symbols are defined as:\n\nThe multiplication factor, , is defined as (see Nuclear chain reaction): \n\n\n"}
{"id": "43454117", "url": "https://en.wikipedia.org/wiki?curid=43454117", "title": "Tefifon", "text": "Tefifon\n\nThe Tefifon is a German-developed and manufactured audio playback format that utilizes cartridges loaded with an endlessly looped reel of plastic tape (much like the later 4-track and 8-track magnetic audio tape cartridges) with grooves embossed on it, similar to the ones on a phonograph record. The grooves were embossed in a helical fashion across the width of the tape, much similar to Dictaphone's Dictabelt format, and are read with a stylus and amplified pickup in the player's transport. A Tefifon cartridge can hold up to four hours of music; therefore, most releases for the format are usually compilations of popular hits or dance music, operas and operettas. Tefifon players were not sold by television and radio dealers in Germany, but rather sold directly by special sales outlets affiliated with Tefi (the manufacturer of the format). \n\nThe Tefifon format was developed by the German entrepreneur Dr. Karl Daniel and his \"Tefi\" company in 1936. Although a few years earlier, devices using phonographic grooved tape like the Tefifon were produced by Tefi for special purposes, mainly for the military, and were designed for voice recording. Prior to the Tefifon's introduction, Tefi introduced a device capable of recording and playback under the name \"Tefiphon\" (note the alternate spelling) and another device only capable of playback under the name \"Teficord\". Both use loose tape, unlike the cartridge-loaded tape of the Tefifon.\n\nThe first Tefifon players and cartridges for home use came available in the German market towards the end of the 1940s, but couldn't compete with the popular phonographic disc format readily already available at the time. One reason was that well-known artists were committed exclusively by contract to major record companies, most of which had no interest in offering their artists' albums on Tefifon tapes. As a result, most Tefifon releases were of relatively unknown bands and artists. In addition, the Tefifon saw competition from phonographs equipped with record changers, some of which could allow for up to three hours of music without interruption.\n\nThe Tefifon was offered in the 1950s as a standalone device, but also in combination with various types of radios, including portable and home models. The chassis of the radio sets were often purchased ready-made from third parties, with a Tefifon playback transport being added to it as a finished product. The sound quality of Tefifon tapes is much superior to 78 rpm records made of shellac, but is still less than that of 33 rpm LP records made of vinyl. In addition, the mechanical stress on the stylus and supporting cantilever is quite strong, so this wears out relatively quickly.\n\nThe last innovation of the Tefifon format was in 1961, with the introduction of stereo sound, but this was not commercially successful. Tefifon production at its main plant in Porz am Rhein was halted in 1965. Afterward, the rights to the name were acquired by the Neckermann mail-order company, which also took over the sale of existing Tefifon products. Even though the Tefifon was little-known outside of Germany, it was imported and sold in the United States for a very short time by Western Electric's Westrex division from 1963-1964 under the \"Westrex\" name.\n\n"}
{"id": "11269605", "url": "https://en.wikipedia.org/wiki?curid=11269605", "title": "The Big Bang Theory", "text": "The Big Bang Theory\n\nThe Big Bang Theory is an American television sitcom created by Chuck Lorre and Bill Prady, both of whom serve as executive producers on the series, along with Steven Molaro. All three also serve as head writers. The show premiered on CBS on September 24, 2007. The twelfth and final season which will run through 2018–19 premiered on September 24, 2018, consisting of 24 episodes.\n\nThe show originally centered on five characters living in Pasadena, California: Leonard Hofstadter and Sheldon Cooper, both physicists at Caltech, who share an apartment; Penny, a waitress and aspiring actress who lives across the hall; and Leonard and Sheldon's similarly geeky and socially awkward friends and co-workers, aerospace engineer Howard Wolowitz and astrophysicist Raj Koothrappali. Over time, supporting characters have been promoted to starring roles including: physicist Leslie Winkle, neuroscientist Amy Farrah Fowler, microbiologist Bernadette Rostenkowski, and Stuart Bloom, the cash-strapped owner of the comic book store the characters often visit.\n\nThe show is filmed in front of a live audience and is produced by Warner Bros. Television and Chuck Lorre Productions. \"The Big Bang Theory\" received mixed reviews from critics throughout its first season, but reception was more favorable in the second and third seasons. Later seasons saw a return to a lukewarm reception, with the show being criticized for a decline in comedic quality. Despite the mixed reviews, seven seasons of the show have ranked within the top ten of the final television season ratings; ultimately reaching the no. 1 spot in its eleventh season. The show was nominated for the Emmy Award for Outstanding Comedy Series from 2011 to 2014 and won the Emmy Award for Outstanding Lead Actor in a Comedy Series four times for Jim Parsons. It has so far won 7 Emmy Awards from 46 nominations. Parsons also won the Golden Globe for Best Actor in a Television Comedy Series in 2011. The series has so far won 56 awards from 216 nominations. It has also spawned a prequel series in 2017 based on Parsons' character, Sheldon Cooper, named \"Young Sheldon\", which originally aired on CBS.\n\nThe show's pilot episode premiered on September 24, 2007. This was the second pilot produced for the show. A different pilot was produced for the 2006–07 television season but never aired. The structure of the original unaired pilot was substantially different from the series' current form. The only characters retained in both pilots were Leonard (Johnny Galecki) and Sheldon (Jim Parsons), who are named after Sheldon Leonard, a longtime figure in episodic television as producer, director and actor. Althea (Vernee Watson) was a minor character that appeared in both pilots. The first pilot included two female lead characters who were not used in the second pilot. The characters were Canadian actress Amanda Walsh as Katie, \"a street-hardened, tough-as-nails woman with a vulnerable interior,\" and Iris Bahr as Gilda, a scientist colleague and friend of the male characters. Sheldon and Leonard meet Katie after she breaks up with a boyfriend and they invite her to share their apartment. Gilda is threatened by Katie's presence. Test audiences reacted negatively to Katie, but they liked Sheldon and Leonard. The original pilot used Thomas Dolby's hit \"She Blinded Me with Science\" as its theme song.\n\nAlthough the original pilot was not picked up, its creators were given an opportunity to retool it and produce a second pilot. They brought in the remaining cast and retooled the show to its final format. Katie was replaced by Penny (Kaley Cuoco). The original unaired pilot never has officially been released, but it has circulated on the Internet. On the evolution of the show, Chuck Lorre said, \"We did the 'Big Bang Pilot' about two and a half years ago, and it sucked ... but there were two remarkable things that worked perfectly, and that was Johnny and Jim. We rewrote the thing entirely and then we were blessed with Kaley and Simon and Kunal.\" As to whether the world will ever see the original pilot on a future DVD release, Lorre said, \"Wow, that would be something. We will see. Show your failures...\"\n\nThe first and second pilots of \"The Big Bang Theory\" were directed by James Burrows, who did not continue with the show. The reworked second pilot led to a 13-episode order by CBS on May 14, 2007. Prior to its airing on CBS, the pilot episode was distributed on iTunes free of charge. The show premiered on September 24, 2007, and was picked up for a full 22-episode season on October 19, 2007. The show is filmed in front of a live audience, and is produced by Warner Bros. Television and Chuck Lorre Productions. Production was halted on November 6, 2007, due to the Writers Guild of America strike. Nearly three months later, on February 4, 2008, the series was temporarily replaced by a short-lived sitcom, \"Welcome to The Captain\". The series returned on March 17, 2008, in an earlier time slot and ultimately only 17 episodes were produced for the first season.\n\nAfter the strike ended, the show was picked up for a second season, airing in the 2008–2009 season, premiering in the same time slot on September 22, 2008. With increasing ratings, the show received a two-year renewal through the 2010–11 season in 2009. In 2011, the show was picked up for three more seasons. In March 2014, the show was renewed again for three more years through the 2016–17 season. This marks the second time the series has gained a three-year renewal. In March 2017, the series was renewed for two additional seasons, bringing its total to 12, and running through the 2018–19 television season.\n\nDavid Saltzberg, a professor of physics and astronomy at the University of California, Los Angeles, checks scripts and provides dialogue, mathematics equations, and diagrams used as props. According to executive producer/co-creator Bill Prady, \"We're working on giving Sheldon an actual problem that he's going to be working on throughout the [first] season so there's actual progress to the boards ... . We worked hard to get all the science right.\"\n\nSeveral of the actors in \"The Big Bang Theory\" previously worked together on the sitcom \"Roseanne\", including Johnny Galecki, Sara Gilbert, Laurie Metcalf (who plays Sheldon's mother, Mary Cooper), and Meagen Fay (who plays Bernadette's mother). Additionally, Lorre was a writer on the series for several seasons.\n\nThe Canadian alternative rock band Barenaked Ladies wrote and recorded the show's theme song, which describes the history and formation of the universe and the Earth. Co-lead singer Ed Robertson was asked by Lorre and Prady to write a theme song for the show after the producers attended one of the band's concerts in Los Angeles. By coincidence, Robertson had recently read Simon Singh's book \"Big Bang\", and at the concert improvised a freestyle rap about the origins of the universe. Lorre and Prady phoned him shortly thereafter and asked him to write the theme song. Having been asked to write songs for other films and shows, but ending up being rejected because producers favored songs by other artists, Robertson agreed to write the theme only after learning that Lorre and Prady had not asked anyone else.\n\nOn October 9, 2007, a full-length (1 minute and 45 seconds) version of the song was released commercially. Although some unofficial pages identify the song title as \"History of Everything,\" the cover art for the single identifies the title as \"Big Bang Theory Theme.\" A music video also was released via special features on \"The Complete Fourth Season\" DVD and Blu-ray set. The theme was included on the band's greatest hits album, \"Hits from Yesterday & the Day Before\", released on September 27, 2011. In September 2015, TMZ uncovered court documents showing that Steven Page sued former bandmate Robertson over the song, alleging that he was promised 20% of the proceeds, but that Robertson has kept that money entirely for himself.\n\nFor the first three seasons, Galecki, Parsons, and Cuoco, the three main stars of the show, received at most $60,000 per episode. The salary for the three went up to $200,000 per episode for the fourth season. Their per-episode pay went up an additional $50,000 in each of the following three seasons, culminating in $350,000 per episode in the seventh season. In September 2013, Bialik and Rauch renegotiated the contracts they held since they were introduced to the series in 2010. On their old contracts, each was making $20,000–$30,000 per episode, while the new contracts doubled that, beginning at $60,000 per episode, increasing steadily to $100,000 per episode by the end of the contract, as well as adding another year for both.\n\nBy season seven, Galecki, Parsons, and Cuoco were also receiving 0.25% of the series' back-end money. Before production began on the eighth season, the three plus Helberg and Nayyar, looked to renegotiate new contracts, with Galecki, Parsons, and Cuoco seeking around $1 million per episode, as well as more back-end money. Contracts were signed in the beginning of August 2014, giving the three principal actors an estimated $1 million per episode for three years, with the possibility to extend for a fourth year. The deals also include larger pieces of the show, signing bonuses, production deals, and advances towards the back-end. Helberg and Nayyar were also able to renegotiate their contracts, giving them a per-episode pay in the \"mid-six-figure range\", up from around $100,000 per episode they each received in years prior. The duo, who were looking to have salary parity with Parsons, Galecki, and Cuoco, signed their contracts after the studio and producers threatened to write the characters out of the series if a deal could not be reached before the start of production on season eight. By season 10, Helberg and Nayyar reached the $1 million per episode parity with Parsons, Galecki, and Cuoco, due to a clause in their deals signed in 2014.\n\nIn March 2017, the main cast members (Galecki, Parsons, Cuoco, Helberg, and Nayyar) took a 10% pay cut to allow Bialik and Rauch an increase in their earnings. This put Galecki, Parsons, Cuoco, Helberg and Nayyar at $900,000 per episode, with Parsons, Galecki, and Helberg also receiving overall deals with Warner Bros. Television. By the end of April, Bialik and Rauch had signed deals to earn $500,000 per episode, each, with the deals also including a separate development component for both actors. The deal was an increase from the $175,000 – $200,000 the duo had been making per episode.\n\nThese actors are credited in all episodes of the series:\n\nThese actors were first credited as guest stars and later promoted to main cast:\n\nAs the theme of the show revolves around science, many distinguished and high-profile scientists have appeared as guest stars on the show. Famous astrophysicist and Nobel laureate George Smoot had a cameo appearance in the second season. Theoretical physicist Brian Greene appeared in the fourth season, as well as astrophysicist, science populizer, and physics outreach specialist Neil deGrasse Tyson, who also appeared in the twelfth season.\n\nCosmologist Stephen Hawking made a short guest appearance in the fifth-season episode; in the eighth season, Hawking video conferences with Sheldon and Leonard, and makes another appearance in the 200th episode. In the fifth and sixth seasons, NASA astronaut Michael J. Massimino played himself multiple times in the role of Howard's fellow astronaut. Bill Nye appeared in the seventh and twelfth seasons.\n\nMuch of the series focuses on science, particularly physics. The four main male characters are employed at Caltech and have science-related occupations, as do Bernadette and Amy. The characters frequently banter about scientific theories or news (notably around the start of the show), and make science-related jokes.\n\nScience has also interfered with the characters' romantic lives. Leslie breaks up with Leonard when he sides with Sheldon in his support for string theory rather than loop quantum gravity. When Leonard joins Sheldon, Raj, and Howard on a three-month Arctic research trip, it separates Leonard and Penny at a time when their relationship is budding. When Bernadette takes an interest in Leonard's work, it makes both Penny and Howard envious and results in Howard confronting Leonard, and Penny asking Sheldon to teach her physics. Sheldon and Amy also briefly end their relationship after an argument over which of their fields is superior.\n\nDavid Saltzberg, who has a Ph.D. in physics, has served as the science consultant for the show for six seasons and attends every taping. While Saltzberg knows physics, he sometimes needs assistance from Mayim Bialik, who has a Ph.D. in neuroscience. Saltzberg sees early versions of scripts which need scientific information added to them, and he also points out where the writers, despite their knowledge of science, have made a mistake. He is usually not needed during a taping unless a lot of science, and especially the whiteboard, is involved.\n\nThe four main male characters are all avid science fiction, fantasy, and comic book fans and memorabilia collectors.\n\n\"Star Trek\" in particular is frequently referenced and Sheldon identifies strongly with the character of Spock, so much so that when he is given a used napkin signed by Leonard Nimoy as a Christmas gift from Penny he is overwhelmed with excitement and gratitude (\"I possess the DNA of Leonard Nimoy?!\"). \"\" cast member George Takei has made a cameo, and Leonard Nimoy made a cameo as the voice of Sheldon's vintage Mr. Spock action figure (both cameos were in dream sequences). \"\" cast members Brent Spiner and LeVar Burton have had cameos as themselves, while Wil Wheaton has a recurring role as a fictionalized version of himself.\n\nThey are also fans of \"Star Wars\", \"Battlestar Galactica\", and \"Doctor Who\". In the episode \"The Ornithophobia Diffusion\", when there is a delay in watching \"Star Wars\" on Blu-ray, Howard complains, \"If we don't start soon, George Lucas is going to change it again\" (referring to Lucas' controversial alterations to the films) and in \"The Hot Troll Deviation\", Katee Sackhoff of \"Battlestar Galactica\" appeared as Howard's fantasy dream girl. The characters have different tastes in franchises with Sheldon praising \"Firefly\" but disapproving of Leonard's enjoyment of \"Babylon 5\". With regard to fantasy, the four make frequent references to \"The Lord of the Rings\" and \"Harry Potter\" novels and movies. Additionally, Howard can speak Sindarin, one of the two Elvish languages from \"The Lord of the Rings\".\n\nWednesday night is the group's designated \"comic book night\" because that is the day of the week when new comic books are released. The comic book store is run by fellow geek and recurring character Stuart. On a number of occasions, the group members have dressed up as pop culture characters, including The Flash, Aquaman, Frodo Baggins, Superman, Batman, Spock, The Doctor, Green Lantern, and Thor. As a consequence of losing a bet to Stuart and Wil Wheaton, the group members are forced to visit the comic book store dressed as Catwoman, Wonder Woman, Batgirl, and Supergirl. DC Comics announced that, to promote its comics, the company will sponsor Sheldon wearing Green Lantern T-shirts.\n\nVarious games have been featured, as well as referenced, on the series (e.g. \"World of Warcraft\", \"Halo\", \"Mario\", Donkey Kong, etc.), including fictional games like \"Mystic Warlords of Ka'a\" (which became a reality in 2011) and Rock-paper-scissors-lizard-Spock.\n\nOne of the recurring plot lines is the relationship between Leonard and Penny. Leonard becomes attracted to Penny in the pilot episode and his need to do favors for her is a frequent point of humor in the first season. Meanwhile, Penny dates a series of muscular, attractive, unintelligent, and insensitive jocks. Their first long-term relationship begins when Leonard returns from a three-month expedition to the North Pole in the season 3 premiere. However, when Leonard tells Penny that he loves her, she realizes she cannot say it back. Both Leonard and Penny go on to date other people; most notably with Leonard dating Raj's sister Priya for much of season 4. This relationship is jeopardized when Leonard comes to falsely believe that Raj has slept with Penny, and ultimately ends when Priya sleeps with a former boyfriend in \"The Good Guy Fluctuation\".\n\nPenny, who admits to missing Leonard in \"The Roommate Transmogrification\", accepts his request to renew their relationship in \"The Beta Test Initiation\". After Penny suggests having sex in \"The Launch Acceleration\", Leonard breaks the mood by proposing to her. Penny says \"no\" but does not break up with him. She stops a proposal a second time in \"The Tangible Affection Proof\". In the sixth-season episode, \"The 43 Peculiarity\", Penny finally tells Leonard that she loves him. Although they both feel jealousy when the other receives significant attention from the opposite sex, Penny is secure enough in their relationship to send him off on an exciting four-month expedition without worrying in \"The Bon Voyage Reaction\". After Leonard returns, their relationship blossoms over the seventh season. In the penultimate episode \"The Gorilla Dissolution\", Penny admits that they should marry and when Leonard realizes that she is serious, he proposes with a ring that he had been saving for years. Leonard and Penny decide to elope to Las Vegas in the season 8 finale, but beforehand, wanting no secrets, Leonard admits to kissing another woman, Mandy Chow (Melissa Tang) while on an expedition on the North Sea. Despite this, Leonard and Penny finally elope in the season 9 premiere.\n\nIn the third-season finale, Raj and Howard search for a woman compatible with Sheldon and discover neurobiologist Amy Farrah Fowler. Like him, she has a history of social ineptitude and participates in online dating only to fulfill an agreement with her mother. This spawns a storyline in which Sheldon and Amy communicate daily while insisting to Leonard and Penny that they are not romantically involved. In \"The Agreement Dissection\", Sheldon and Amy talk in her apartment after a night of dancing and she kisses him on the lips. Instead of getting annoyed, Sheldon says \"fascinating\" and later asks Amy to be his girlfriend in \"The Flaming Spittoon Acquisition\". The same night he draws up \"The Relationship Agreement\" to verify the ground rules of him as her boyfriend and vice versa (similar to his \"Roommate Agreement\" with Leonard). Amy agrees but later regrets not having had a lawyer read through it.\n\nIn the episode \"The Launch Acceleration\", Amy tries to use her \"neurobiology bag of tricks\" to increase the attraction between herself and Sheldon. In the final fifth-season episode \"The Countdown Reflection\", Sheldon takes Amy's hand as Howard is launched into space. In the sixth season first episode \"The Date Night Variable\", after a dinner in which Sheldon fails to live up to this expectation, Amy gives Sheldon an ultimatum that their relationship is over unless he tells her something from his heart. Amy accepts Sheldon's romantic speech even after learning that it is a line from the first Spider-Man movie. In \"The Cooper/Kripke Inversion\" Sheldon states that he has been working on his discomfort about physical contact and admits that \"it's a possibility\" that he could one day have sex with Amy. Amy is revealed to have similar feelings in \"The Love Spell Potential\". Sheldon explains that he never thought about intimacy with anyone before Amy.\n\n\"The Locomotive Manipulation\" is the first episode in which Sheldon initiates a kiss with Amy. Although initially done in a fit of sarcasm, he discovers that he enjoys the feeling. Consequently, Sheldon slowly starts to open up over the rest of the season, and starts a more intimate relationship with Amy. However, in the season finale, Sheldon leaves temporarily to cope with several changes and Amy becomes distraught. However, in \"The Prom Equivalency\", he hides in his room to avoid going to a mock prom reenactment with her. In the resulting stand-off, Amy is about to confess that she loves Sheldon, but he surprises her by saying that he loves her too. This prompts Amy to have a panic attack.\n\nIn the season eight finale, Sheldon and Amy get into a fight about commitment on their fifth anniversary. Amy tells Sheldon that she needs to think about the future of their relationship, unaware that Sheldon was about to propose to her. Season nine sees Sheldon harassing Amy about making up her mind until she breaks up with him. Both struggle with singlehood and trying to be friends for the next few weeks until they reunite in episode ten and have sex for the first time on Amy's birthday.\n\nIn the season eleven premiere, Sheldon proposes to Amy and she accepts. The two get married in the eleventh season finale.\n\nIn the show, the song \"Soft Kitty\" was described by Sheldon as a song sung by his mother when he was ill. Its repeated use in the series popularized the song. A scene depicting the origin of the song in Sheldon's childhood is depicted in an episode of \"Young Sheldon\", which aired on February 1, 2018. It shows Sheldon's mother Mary singing the song to her son, who is suffering with the flu.\n\nIn scenes set at Howard's home, he interacts with his rarely-seen mother (voiced by Carol Ann Susi until her death) by shouting from room to room in the house. She similarly interacts with other characters in this manner. She reflects the Jewish mother stereotype in some ways, such as being overly controlling of Howard's adult life and sometimes trying to make him feel guilty about causing her trouble. She is dependent on Howard, as she requires him to help her with her wig and makeup in the morning. Howard, in turn, is attached to his mother to the point where she still cuts his meat for him, takes him to the dentist, does his laundry and \"grounds\" him when he returns home after briefly moving out. Until Howard's marriage to Bernadette in the fifth-season finale, Howard's former living situation led Leonard's psychiatrist mother to speculate that he may suffer from some type of pathology, and Sheldon to refer to their relationship as Oedipal. In season 8, Howard's mother dies in her sleep while in Florida, which devastates Howard and Stuart, who briefly lived with Mrs. Wolowitz.\n\nIn the apartment building where Sheldon, Leonard and Penny (and later Amy) live, the elevator has been out of order throughout the first 11 seasons, forcing characters to have to use the stairs. Stairway conversations between characters occur in almost every episode, often serving as a transition between longer scenes. A season 3 episode reveals that the elevator was broken when Leonard was experimenting with rocket fuel.\n\nLike most shows created by Chuck Lorre, \"The Big Bang Theory\" ends by showing for one second a vanity card written by Lorre after the credits, followed by the Warner Bros. Television closing logo. These cards are archived on Lorre's website.\n\nInitial reception for the series was mixed. The review aggregation website Rotten Tomatoes reported a 52% approval rating for the first season based on reviews from 23 critics, with an average rating of 5.18/10. The website's critical consensus reads, \"\"The Big Bang Theory\" brings a new class of character to mainstream television, but much of the comedy feels formulaic and stiff.\" On Metacritic, the season holds a score of 57 out of 100, based on reviews from 23 critics, indicating \"mixed or average reviews\". Later seasons received more acclaim and in 2013, \"TV Guide\" ranked the series #52 on its list of the 60 Best Series of All Time.\n\n\"The Big Bang Theory\" started off slowly in the ratings, failing to make the top 50 in its first season (ranking 68th), and ranking 40th in its second season. When the third season premiered on September 21, 2009, however, \"The Big Bang Theory\" ranked as CBS's highest-rated show of that evening in the adults 18–49 demographic (4.6/10) along with a then-series-high 12.83 million viewers. After the first three seasons aired at different times on Monday nights, CBS moved the show to Thursdays at 8:00 ET for the 2010–2011 schedule, to be in direct competition with NBC's Comedy Block and Fox's \"American Idol\" (then the longest reigning leading primetime show on U.S. television from 2004 to 2011). During its fourth season, it became television's highest rated comedy, just barely beating out eight-year champ \"Two and a Half Men\". However, in the age 18–49 demographic (the show's target age range), it was the second highest rated comedy, behind ABC's \"Modern Family\". The fifth season opened with viewing figures of over 14 million.\n\nThe sixth season boasts some of the highest-rated episodes for the show so far, with a then-new series high set with \"The Bakersfield Expedition\", with 20 million viewers, a first for the series, which along with \"NCIS\", made CBS the first network to have two scripted series reach that large an audience in the same week since 2007. In the sixth season, the show became the highest rated and viewed scripted show in the 18–49 demographic, trailing only the live regular \"NBC Sunday Night Football\" coverage, and was third in total viewers, trailing \"NCIS\" and \"Sunday Night Football\". Season seven of the series opened strong, continuing the success gained in season six, with the second episode of the premiere, \"The Deception Verification\", setting the new series high in viewers with 20.44 million.\n\nShowrunner Steve Molaro, who took over from Bill Prady with the sixth season, credits some of the show's success to the sitcom's exposure in off-network syndication, particularly on TBS, while Michael Schneider of \"TV Guide\" attributes it to the timeslot move two seasons earlier. Chuck Lorre and CBS Entertainment president Nina Tassler also credit the success to the influence of Molaro, in particular the deepening exploration of the firmly established regular characters and their interpersonal relationships, such as the on-again, off-again relationship between Leonard and Penny. Throughout much of the 2012–13 season, \"The Big Bang Theory\" placed first in all of syndication ratings, receiving formidable competition from only \"Judge Judy\" and \"Wheel of Fortune\" (first-run syndication programs). By the end of the 2012–13 television season, \"The Big Bang Theory\" had dethroned \"Judge Judy\" as the ratings leader in all of syndicated programming with 7.1, \"Judy\" descending to second place for that season with a 7.0. \"The Big Bang Theory\" did not place first in syndication ratings for the 2013–14 television season, beaten out by \"Judge Judy\".\n\nThe show made its United Kingdom debut on Channel 4 on February 14, 2008. The show was also shown as a 'first-look' on Channel 4's digital offshoot E4 prior to the main channel's airing. While the show's ratings were not deemed strong enough for the main channel, they were considered the opposite for E4. For each following season, all episodes were shown first-run on E4, with episodes only aired on the main channel in a repeat capacity, usually on a weekend morning. From the third season, the show aired in two parts, being split so that it could air new episodes for longer throughout the year. This was due to rising ratings. The first part began airing on December 17, 2009, at 9:00 p.m. while the second part, containing the remaining eleven episodes, began airing in the same time period from May 6, 2010. The first half of the fourth season began airing on November 4, 2010, at 9:00 p.m., drawing 877,000 viewers, with a further 256,000 watching on the E4+1 hour service. This gave the show an overall total of 1.13 million viewers, making it E4's most-watched programme for that week. The increased ratings continued over subsequent weeks.\n\nThe fourth season's second half began on June 30, 2011. Season 5 began airing on November 3, 2011, at 8:00 p.m. as part of E4's Comedy Thursdays, acting as a lead-in to the channel's newest comedy, \"Perfect Couples\". Episode 19, the highest-viewed episode of the season, attracted 1.4 million viewers. Season 6 premiered on November 15, 2012, with 1.89 million viewers and a further 469,000 on the time shift channel, bringing the total to 2.31 million, E4's highest viewing ratings of 2012, and the highest the channel had received since June 2011. The sixth season returned in mid-2013 to finish airing the remaining episodes. Season 7 premiered on E4 on October 31, 2013 at 8:30pm and hit multiple ratings records this season. The second half of season seven aired in mid 2014. The eighth season premiered on E4 on October 23, 2014 at 8:30 p.m. During its eighth season, \"The Big Bang Theory\" shared its 8:30 p.m. time period with fellow CBS comedy, \"2 Broke Girls\". Following the airing of the first eight episodes of that show's fourth season, \"The Big Bang Theory\" returned to finish airing its eighth season on March 19, 2015.\n\nNetflix UK & Ireland announced on February 13, 2016 that seasons 1–8 would be available to stream from February 15, 2016.\n\n\"The Big Bang Theory\" started off quietly in Canada, but managed to garner major success in later seasons. \"The Big Bang Theory\" is telecast throughout Canada via the CTV Television Network in simultaneous substitution with cross-border CBS affiliates. Now immensely popular in Canada, \"The Big Bang Theory\" is also rerun daily on the Canadian cable channel The Comedy Network.\n\nThe season 4 premiere garnered an estimated 3.1 million viewers across Canada. This is the largest audience for a sitcom since the series finale of \"Friends\" (12.4 million viewers). \"The Big Bang Theory\" has pulled ahead and has now become the most-watched entertainment television show in Canada.\n\n\"The Big Bang Theory\" premiered in the United States on September 24, 2007 on CBS. The series debuted in Canada on CTV in September 2007. On February 14, 2008, the series debuted in the United Kingdom on channels E4 and Channel 4. In Australia the first seven seasons of the series began airing on the Seven Network and 7mate from October 2015 and also gained the rights to season 8 in 2016, though the Nine Network has rights to air seasons nine & ten. On January 22, 2018, it was announced that Nine had acquired the rights to Season 1–8.\n\nIn May 2010, it was reported that the show had been picked up for syndication, mainly among Fox's owned and operated stations and other local stations, with Warner Bros. Television's sister cable network TBS holding the show's cable syndication rights. Broadcast of old shows began airing in September 2011. TBS now airs the series in primetime on Tuesdays, Wednesdays, and Thursdays, with evening broadcasts on Saturdays (TBS's former local sister station in Atlanta also holds local weeknight rights to the series). Although details of the syndication deal have not been revealed, it was reported the deal \"set a record price for a cable off-network sitcom purchase\". CTV holds national broadcast syndication rights in Canada, while sister cable network The Comedy Network holds cable rights.\n\nWarner Bros. Television controls the online rights for the show. Full episodes are available at , while short clips and recently aired full episodes are available on cbs.com. In Canada, recent episode(s) and pictures are available on CTV.ca. Additionally in Canada, the first six seasons are available for streaming on Bell Media's CraveTV. After the show has aired in New Zealand the shows are available in full online at TVNZ's on demand web service.\n\nThe first and second seasons were only available on DVD at their time of release in 2008 and 2009. Starting with the release of the third season in 2010 and continuing every year with every new season, a Blu-ray disc set has also been released in conjunction with the DVD. In 2012, Warner Bros. released the first two seasons on Blu-ray, marking the first time that all episodes were available on the Blu-ray disc format.\n\nIn August 2009, the sitcom won the best comedy series TCA award and Jim Parsons (Sheldon) won the award for individual achievement in comedy. In 2010, the show won the People's Choice Award for Favorite Comedy, while Parsons won a Primetime Emmy Award for Outstanding Lead Actor in a Comedy Series. On January 16, 2011, Parsons was awarded a Golden Globe for Best Performance by an Actor in a Television Series – Comedy or Musical, an award that was presented by co-star Kaley Cuoco. On September 18, 2011, Parsons was again awarded an Emmy for Best Actor in a Comedy Series. On January 9, 2013, the show won People's Choice Award for Favorite Comedy for the second time. August 25, 2014, Jim Parsons was awarded an Emmy for Best Actor in a Comedy Series. \"The Big Bang Theory\" also won the 2016 People's Choice Awards for under \"Favorite TV Show\" and \"Favorite Network TV Comedy\" with Jim Parsons winning Favorite Comedic TV Actor. On January 20, 2016, The Big Bang Theory also won the International category at the UK's National Television Awards.\n\nOn March 16, 2014, a Lego Ideas project portraying the living room scene in Lego style with the main cast as mini-figures reached 10,000 supporters on the platform, which qualified it to be considered as an official set by the Lego Ideas review board. On November 7, 2014, Lego Ideas approved the design and began refining it. The set was released in August 2015, with an exclusive pre-sale taking place at the San Diego Comic-Con International.\n\nThrough the use of his vanity cards at the end of episodes, Lorre alleged that the program had been plagiarized by a show produced and aired in Belarus. Officially titled \"Теоретики\" (\"The Theorists\"), the show features \"clones\" of the main characters, a similar opening sequence, and what appears to be a very close Russian translation of the scripts. Lorre expressed annoyance and described his inquiry with the Warner Bros. legal department about options. The television production company and station's close relationship with the Belarus government was cited as the reason that any attempt to claim copyright infringement would be in vain because the company copying the episodes is operated by the government.\n\nHowever, no legal action was required to end production of the other show: as soon as it became known that the show was unlicensed, the actors quit and the producers canceled it. Dmitriy Tankovich (who plays Leonard's counterpart, \"Seva\") said in an interview, \"I'm upset. At first, the actors were told all legal issues were resolved. We didn't know it wasn't the case, so when the creators of \"The Big Bang Theory\" started talking about the show, I was embarrassed. I can't understand why our people first do, and then think. I consider this to be the rock bottom of my career. And I don't want to take part in a stolen show\".\n\nIn November 2016, it was reported that CBS was in negotiations to create a spin-off of \"The Big Bang Theory\" centered on Sheldon as a young boy. The prequel series, described as \"a \"Malcolm in the Middle\"-esque single-camera family comedy\" would be executive-produced by Lorre and Molaro, with Prady expected to be involved in some capacity, and intended to air in the 2017–18 season alongside \"The Big Bang Theory\". The initial idea for the series came from Parsons, who passed it along to \"The Big Bang Theory\" producers. In early March 2017, Iain Armitage was cast as the younger Sheldon, as well as Zoe Perry as his mother, Mary Cooper. Perry is the real-life daughter of Laurie Metcalf, who portrays Mary Cooper on \"The Big Bang Theory\".\n\nOn March 13, 2017, CBS ordered the spin-off \"Young Sheldon\" series. Jon Favreau directed and executive produced the pilot. Created by Lorre and Molaro, the series follows 9-year-old Sheldon Cooper as he attends high school in East Texas. Alongside Armitage as 9-year-old Sheldon Cooper and Perry as Mary Cooper, Lance Barber stars as George Cooper, Sheldon's father; Raegan Revord stars as Missy Cooper, Sheldon's twin sister; and Montana Jordan as George Cooper Jr., Sheldon's older brother. Jim Parsons reprises his role as adult Sheldon Cooper, as narrator for the series. Parsons, Lorre, Molaro and Todd Spiewak will also serve as executive producers on the series, for Chuck Lorre Productions, Inc. in association with Warner Bros. Television. The show's pilot episode premiered on September 25, 2017. Subsequent weekly episodes began airing on November 2, 2017 following the broadcast of the 237th episode of \"The Big Bang Theory\".\n\n"}
{"id": "10849093", "url": "https://en.wikipedia.org/wiki?curid=10849093", "title": "Tortilla warmer", "text": "Tortilla warmer\n\nA round shaped container which helps keep tortillas warm during a meal. Warm tortillas are placed in the warmer, which is often lined with a cloth or paper napkin. Tortilla warmers made of terra cotta, plastic and styrofoam are available and commonly used in Mexican and Mexican-influenced cultures.\n"}
{"id": "43380", "url": "https://en.wikipedia.org/wiki?curid=43380", "title": "Trebuchet", "text": "Trebuchet\n\nA trebuchet (French \"trébuchet\") is a type of catapult, a common type of siege engine which uses a swinging arm to throw a projectile.\n\nThe traction trebuchet, also referred to as a mangonel at times, first appeared in Ancient China during the 4th century BC as a siege weapon. It spread westward, probably by the Avars, and was adopted by the Byzantines in the mid 6th century AD. It uses manpower to swing the arm.\n\nThe later counterweight trebuchet, also known as the counterpoise trebuchet, uses a counterweight to swing the arm. It appeared in both Christian and Muslim lands around the Mediterranean in the 12th century, and made its way back to China via Mongol conquests in the 13th century.\n\nThe trebuchet is a compound machine that makes use of the mechanical advantage of a lever to throw a projectile. They are typically large constructions (up to in height or more) made primarily of wood, usually reinforced with metal, leather, rope, and other materials. They are usually immobile and must be assembled on-site, possibly making use of local lumber with only key parts brought with the army to the site of the siege or battle.\n\nA trebuchet consists primarily of a long beam attached by an axle suspended high above the ground by a stout frame and base, such that the beam can rotate vertically through a wide arc (typically over 180°). A sling is attached to one end of the beam to hold the projectile. The projectile is thrown when the beam is quickly rotated by applying force to the opposite end of the beam. The mechanical advantage is primarily obtained by having the projectile end of the beam much longer than the opposite end where the force is applied – usually four to six times longer.\n\nCounterweight trebuchets are powered by gravity; potential energy is stored by slowly raising an extremely heavy box (typically filled with stones, sand, or lead) attached by a hinged connection to the shorter end of the beam, and releasing it on command. Traction trebuchets are human powered; on command, men pull ropes attached to the shorter end of the trebuchet beam. The difficulties of coordinating the pull of many men together repeatedly and predictably makes counterweight trebuchets preferable for the larger machines, though they are more complicated to engineer. Further increasing their complexity is that either winches or treadwheels, aided by block and tackle, are typically required to raise the more massive counterweights. So while counterweight trebuchets require significantly fewer men to operate than traction trebuchets, they require significantly more time to reload. In a long siege, reload time may not be a critical concern.\n\nWhen the trebuchet is loosed, the force causes rotational acceleration of the beam around the axle (the fulcrum of the lever). These factors multiply the acceleration transmitted to the throwing portion of the beam and its attached sling. The sling starts rotating with the beam, but rotates farther (typically about 360°) and therefore faster, transmitting this increased speed to the projectile. The length of the sling increases the mechanical advantage, and also changes the trajectory so that, at the time of release from the sling, the projectile is traveling in the desired speed and angle to give it the range to hit the target. Adjusting the sling's release point is the primary means of fine-tuning the range, as the rest of the trebuchet's actions are difficult to adjust after construction.\n\nThe rotation speed of the throwing beam increases smoothly until it reaches maximum rotation speed. Then the arm continues to rotate, slowing, coming to rest at the end of the rotation rather smoothly as momentum is transferred to the sling and its projectile. This is unlike the violent sudden stop inherent in the action of other siege engine designs such as the onager, which loses energy thereby. This key difference also makes the trebuchet much more durable, allowing for larger and more powerful machines.\n\nA trebuchet projectile can be almost anything, even debris, corpses, or incendiaries, but is typically a large stone. Dense stone, or even metal, specially worked to be round and smooth, gives the best range and predictability. When attempting to breach enemy walls, it is important to use materials that will not shatter on impact; projectiles were sometimes brought from distant quarries to get the desired properties.\n\nThe traction trebuchet, also referred to as a mangonel in some sources, is thought to have originated in ancient China. Torsion-based siege weapons such as the ballista and onager are not known to have been used in China.\n\nThe first recorded use of traction trebuchets was in ancient China. They were probably used by the Mohists as early as 4th century BC, descriptions of which can be found in the \"Mojing\" (compiled in the 4th century BC). In Chapter 14 of the \"Mojing\", the traction trebuchet is described hurling hollowed out logs filled with burning charcoal at enemy troops. The trebuchet was carried westward by the Avars and appeared next in the eastern Mediterranean by the late 6th century AD, where it replaced torsion powered siege engines such as the ballista and onager due to its simpler design and faster rate of fire. The Byzantines adopted the traction trebuchet possibly as early as 587, the Persians in the early 7th century, and the Arabs in the second half of the 7th century. The Franks and Saxons adopted the weapon in the 8th century.\n\nWest of China, the traction trebuchet remained the primary siege weapon until the 12th century when it was replaced by the counterweight trebuchet. In China the traction trebuchet continued to be used until the counterweight trebuchet was introduced during the Mongol conquest of the Song dynasty. In 617 Li Mi (Sui dynasty) constructed 300 trebuchets for his assault on Luoyang, in 621 Li Shimin did the same at Luoyang, and onward into the Song dynasty when in 1161, trebuchets operated by Song dynasty soldiers fired bombs of lime and sulphur against the ships of the Jin dynasty navy during the Battle of Caishi.\n\nThe term \"traction trebuchet\" is a modern invention and was not used by contemporary users of the weapon. The term was created mainly to distinguish it from the \"onager\", a torsion powered artillery weapon which is often confused with another name for the traction trebuchet, the \"mangonel\", a generic medieval term for stone throwing artillery. Confusion between the onager and mangonel in terminology has led some historians today to use \"traction trebuchet\" instead. The mangonel is called \"al-manjanīq\" in Arabic. In China the traction trebuchet was called the \"pào\" (砲).\n\nThe hand-trebuchet () was a staff sling mounted on a pole using a lever mechanism to propel projectiles. Basically a one-man traction trebuchet, it was used by emperor Nikephoros II Phokas around 965 to disrupt enemy formations in the open field. It was also mentioned in the Taktika of general Nikephoros Ouranos (c. 1000), and listed in \"De obsidione toleranda\" (author anonymous) as a form of artillery.\n\nAccording to Paul E. Chevedden, a hybrid trebuchet existed that used both counterweight and human propulsion. However no illustrations or descriptions of the device exist from the time when they were supposed to have been used. The entire argument for the existence of hybrid trebuchets rests on accounts of increasingly more effective siege weapons. Peter Purton suggests that this was simply because the machines became larger. The earliest depiction of a hybrid trebuchet is dated to 1462, when trebuchets had already become obsolete due to cannons.\n\nThe earliest known description and illustration of a counterweight trebuchet comes from a commentary on the conquests of Saladin by Mardi ibn Ali al-Tarsusi in 1187.\n\nThe earliest solid reference to counterweight trebuchets in European sources dates to the siege of Castelnuovo Bocca d'Adda in 1199. They were used in Germany from around 1205, in England at least by 1217, and in Iberia shortly after 1218. By the 1230s the counterweight trebuchet was a common item in siege warfare.\n\nPaul E. Chevedden argues that counterweight trebuchets appeared even earlier in Europe based on what might have been counterweight trebuchets in earlier sources. The 12th century Byzantine historian Niketas Choniates may have been referring to a counterweight trebuchet when he described one equipped with a windlass, which is only useful to counterweight machines, at the siege of Zevgminon in 1165. At the Siege of Nicaea in 1097 the Byzantine emperor Alexios I Komnenos reportedly invented new pieces of heavy artillery which deviated from the conventional design and made a deep impression on everyone. Possible references to counterweight trebuchets also appear for the second siege of Tyre in 1124, where the crusaders reportedly made use of \"great trebuchets\". Chevedden argues that given the references to new and better trebuchets that by the 1120–30s, the counterweight trebuchet was being used in a variety of places by different peoples such as the crusader states, the Normans of Sicily and the Seljuks.\n\nCounterweight trebuchets do not appear with certainty in Chinese historical records until about 1268 when the Mongols laid siege to Fancheng and Xiangyang. After failing to take the twin cities of Fancheng and Xiangyang for several years, collectively known as the Siege of Fancheng and Xiangyang, the Mongol army brought in two Persian engineers to build hinged counterweight trebuchets. Known as the Huihui trebuchet (回回砲, where \"huihui\" is a loose slang referring to any Muslims), or Xiangyang trebuchet (襄陽砲) because they were first encountered in that battle. Ismail and Al-aud-Din arrived travelled to South China from Iraq and mangonels and trebuchets for the siege. Chinese and Muslim engineers operated artillery and siege engines for the Mongol armies. The counterweight trebuchet, known as the Muslim trebuchet in China, replaced the traction version after its introduction in the late 13th century. Its greater range was however, somewhat countered by the fact that it had to be constructed close to the site of the siege unlike traction trebuchets, which were easier to take apart and put back together again where necessary.\n\nThe counterweight trebuchet remained in use in China for roughly two centuries, at which point it was well on its way to obsolescence.\n\nThe couillard is a smaller version of a counterweight trebuchet with a single frame instead of the usual double \"A\" frames. The counterweight is split into two halves to avoid hitting the center frame.\n\nWith the introduction of gunpowder, the trebuchet began to lose its place as the siege engine of choice to the cannon. Trebuchets were still used both at the siege of Burgos (1475–1476) and siege of Rhodes (1480). One of the last recorded military uses was by Hernán Cortés, at the 1521 siege of the Aztec capital Tenochtitlán. Accounts of the attack note that its use was motivated by the limited supply of gunpowder. The attempt was reportedly unsuccessful: the first projectile landed on the trebuchet itself, destroying it.\n\nMost trebuchet use in recent centuries has been for recreational or educational, rather than military purposes. New machines have been constructed and old ones restored by living history enthusiasts, for historical re-enactments, and use in other historical celebrations. As their construction is substantially simpler than modern weapons, trebuchets also serve as the object of engineering challenges.\n\nThe trebuchet's technical constructions were lost at the beginning of the 16th century. In 1984, the French engineer Renaud Beffeyte made the first modern reconstruction of a trebuchet, based on documents from 1324.\n\nThe largest currently-functioning trebuchet in the world is the 22-tonne machine at Warwick Castle, England, constructed in 2005. Based on historical designs, it stands tall and throws missiles typically 36 kg (80 lbs) up to . The trebuchet gained significant interest from numerous news sources when in 2015 a burning missile fired from the siege engine struck and damaged a Victorian-era boathouse situated at the River Avon close by, inadvertently demonstrating the weapon's power. It is built on the design of a similar trebuchet at Middelaldercentret in Denmark. In 1989, Middelaldercentret became the first place in the modern era to have a working trebuchet.\n\nTrebuchets compete in one of the classifications of machines used to hurl pumpkins at the annual pumpkin chucking contest held in Sussex County, Delaware, U.S. The record-holder in that contest for trebuchets is the Yankee Siege II from New Hampshire, which at the 2013 WCPC Championship tossed a pumpkin 2835.8 ft (864.35 metres). The , trebuchet flings the standard pumpkins, specified for all entries in the WCPC competition.\n\nA large trebuchet has recently been tested in Belfast as part of the set for the television series \"Game of Thrones.\"\n\nAlthough rarely used as a weapon today, trebuchets maintain the interest of professional and hobbyist engineers. One modern technological development, especially for the competitive pumpkin-hurling events, is the \"floating arms\" design. Instead of using the traditional axle fixed to a frame, these devices are mounted on wheels that roll on a track parallel to the ground, with a counterweight that falls directly downward upon release, allowing for greater efficiency by increasing the proportion of energy transferred to the projectile.\n\nIn 2013, during the Syrian civil war, rebels were filmed using a trebuchet in the Battle of Aleppo. The trebuchet was used to project explosives at government troops.\n\nIn 2014, during the Hrushevskoho street riots in Ukraine, rioters used an improvised trebuchet to throw bricks and molotov cocktails at the Berkut.\n\n\n"}
{"id": "53946597", "url": "https://en.wikipedia.org/wiki?curid=53946597", "title": "Virgin Mobile Saudi Arabia", "text": "Virgin Mobile Saudi Arabia\n\nVirgin Mobile KSA is a telecommunication company operating in the Kingdom of Saudi Arabia.\n\nThe company behind Virgin Mobile Saudi Arabia was formally called the Virgin Mobile Saudi Consortium — a Saudi Arabian company that brings together local, regional and global shareholders and experts in mobile telecommunications.\n\nThe company is headquartered in Riyadh and has outlets across the kingdom and a member care center in Jeddah.\n\nVirgin Mobile Saudi Arabia was awarded a licence by the Communications and Information Technology Commission to operate as a Mobile Virtual Network Operator in April 2014.\n\nVirgin Mobile Saudi Consortium LLC was formally incorporated in June 2013, shortly after the award of Virgin Mobile’s licence by CITC.\n\nThe company is part of Virgin Mobile Middle East & Africa and has local Saudi Arabian companies as shareholders.\n\nVirgin Mobile uses the STC network for all its Saudi based services. This network operates on the following frequencies: \n\nVirgin Mobile Saudi Arabia says it focuses mainly on “fairness and simplicity” in its offerings. \n\nVirgin Mobile’s telecommunications products include:\nVirgin Mobile sells their products across Saudi Arabia.\n\n"}
{"id": "599563", "url": "https://en.wikipedia.org/wiki?curid=599563", "title": "Voltage-controlled oscillator", "text": "Voltage-controlled oscillator\n\nA voltage-controlled oscillator (VCO) is an electronic oscillator whose oscillation frequency is controlled by a voltage input. The applied input voltage determines the instantaneous oscillation frequency. Consequently, a VCO can be used for frequency modulation (FM) or phase modulation (PM) by applying a modulating signal to the control input. A VCO is also an integral part of a phase-locked loop.\n\nA voltage-to-frequency converter (VFC) is a special type of VCO designed to be very linear in frequency control over a wide range of input control voltages.\n\nVCOs can be generally categorized into two groups based on the type of waveform produced.\n\n\nA voltage-controlled capacitor is one method of making an LC oscillator vary its frequency in response to a control voltage. Any reverse-biased semiconductor diode displays a measure of voltage-dependent capacitance and can be used to change the frequency of an oscillator by varying a control voltage applied to the diode. Special-purpose variable-capacitance varactor diodes are available with well-characterized wide-ranging values of capacitance. \n\nFor low-frequency VCOs, other methods of varying the frequency (such as altering the charging rate of a capacitor by means of a voltage controlled current source) are used (see function generator).\n\nThe frequency of a ring oscillator is controlled by varying either the supply voltage, the current available to each inverter stage, or the capacitive loading on each stage.\n\nVCOs are used in analog applications such as frequency modulation and frequency-shift keying. The functional relationship between the control voltage and the output frequency for a VCO (especially those used at radio frequency) may not be linear, but over small ranges, the relationship is approximately linear, and linear control theory can be used. A voltage-to-frequency converter (VFC) is a special type of VCO designed to be very linear over a wide range of input voltages.\n\nModeling for VCOs is often not concerned with the amplitude or shape (sinewave, triangle wave, sawtooth) but rather its instantaneous phase. In effect, the focus is not on the time-domain signal but rather the argument of the sine function (the phase). Consequently, modeling is often done in the phase domain.\n\nThe instananeous frequency of a VCO is often modeled as a linear relationship with its instantaneous control voltage. The output phase of the oscillator is the integral of the instantaneous frequency.\n\nFor analyzing a control system, the Laplace transforms of the above signals are useful.\n\nTuning range, tuning gain and phase noise are the important characteristics of a VCO. Generally low phase noise is preferred in the VCO. The noise present in the control signal and the tuning gain affect the phase noise; high noise or high tuning gain imply more phase noise. Other important elements that determine the phase noise are the transistor's flicker noise (1/\"f\" noise), the output power level, and the loaded Q of the resonator. (see Leeson's equation). The low frequency flicker noise affects the phase noise because the flicker noise is heterodyned to the oscillator output frequency due to the active devices non-linear transfer function. The effect of flicker noise can be reduced with negative feedback that linearizes the transfer function (for example, emitter degeneration).\n\nLeeson's expression for single-sideband (SSB) phase noise in dBc/Hz (decibels relative to output level per Hertz) is\n\nwhere \"f\" is the output frequency, Q is the loaded Q, \"f\" is the offset from the output frequency (Hz), \"f\" is the 1/\"f\" corner frequency, \"F\" is the noise factor of the amplifier, \"k\" is Boltzmann's constant, \"T\" is absolute temperature in Kelvins, and \"P\" is the oscillator output power.\n\nCommonly used VCO circuits are the Clapp and Colpitts oscillators. The more widely used oscillator of the two is Colpitts and these oscillators are very similar in configuration.\n\nVCOs generally have the lowest Q-factor of the used oscillators, and so suffer more jitter than the other types. The jitter can be made low enough for many applications (such as driving an ASIC), in which case VCOs enjoy the advantages of having no off-chip components (expensive) or on-chip inductors (low yields on generic CMOS processes). These oscillators also have larger tuning ranges than the other kinds, which improves yield and is sometimes a feature of the end product (for instance, the dot clock on a graphics card which drives a wide range of monitors).\n\nA clock generator is an oscillator that provides a timing signal to synchronize operations in digital circuits. VCXO clock generators are used in many areas such as digital TV, modems, transmitters and computers. Design parameters for a VCXO clock generator are tuning voltage range, center frequency, frequency tuning range and the timing jitter of the output signal. Jitter is a form of phase noise that must be minimised in applications such as radio receivers, transmitters and measuring equipment.\n\nA \"voltage-controlled crystal oscillator\" (\"VCXO\") is used for fine adjustment of the operating frequency. The frequency of a voltage-controlled crystal oscillator can be varied a few tens of parts per million (ppm) over a control voltage range of typically 0 to 3 volts, because the high Q factor of the crystals allows \"pulling\" over only a small range of frequencies. When a wider selection of clock frequencies is needed the VCXO output can be passed through digital divider circuits to obtain lower frequencies or be fed to a phase-locked loop (PLL). ICs containing both a VCXO (for external crystal) and a PLL are available. A typical application is to provide clock frequencies in a range from 12 kHz to 96 kHz to an audio digital-to-analog converter.\n\nThere are two reasons for using a VCXO:\nA \"temperature-compensated VCXO\" (TCVCXO) incorporates components that partially correct the dependence on temperature of the resonant frequency of the crystal. A smaller range of voltage control then suffices to stabilize the oscillator frequency in applications where temperature varies, such as heat buildup inside a transmitter.\n\nPlacing the oscillator in a temperature-controlled \"oven\" at a constant but higher-than-ambient temperature is another way to stabilize oscillator frequency. High stability crystal oscillator references often place the crystal in an oven and use a voltage input for fine control. The temperature is selected to be the \"turnover temperature\": the temperature where small changes do not affect the resonance. The control voltage can be used to occasionally adjust the reference frequency to a NIST source. Sophisticated designs may also adjust the control voltage over time to compensate for crystal aging.\n\nVCOs are used in function generators, phase-locked loops including frequency synthesizers used in communication equipment and the production of electronic music, to generate variable tones in synthesizers.\n\nFunction generators are low-frequency oscillators which feature multiple waveforms, typically sine, square, and triangle waves. Monolithic function generators are voltage-controlled.\n\nAnalog phase-locked loops typically contain VCOs. High-frequency VCOs are usually used in phase-locked loops for radio receivers. Phase noise is the most important specification in this application.\n\nAudio frequency VCOs are used in analog music synthesizers. For these, sweep range, linearity, and distortion are often the most important specs. Audio-frequency VCOs for use in musical contexts were largely superseded in the 1980s by their digital counterparts, digitally controlled oscillators (DCOs), due to their output stability in the face of temperature changes during operation. From the 1990s on, pure software is the primary sound-generating method, but VCOs have become popular again often thanks to their imperfections.\n\nVoltage-to-frequency converters are voltage-controlled oscillators with a highly linear relation between applied voltage and frequency. They are used to convert a slow analog signal (such as from a temperature transducer) to a digital signal for transmission over a long distance, since the frequency will not drift or be affected by noise. VCOs in this application may have sine or square wave outputs.\n\n\n"}
{"id": "57816166", "url": "https://en.wikipedia.org/wiki?curid=57816166", "title": "WDR paper computer", "text": "WDR paper computer\n\nThe WDR paper computer or Know-How computer was a \"computer\" that could be easily assembled from a sheet of paper and individual matches. This would allow anyone interested to learn how to program without having an electronic computer at their disposal. Thus, this \"computer\" served as an educational aid in the field of computer science. The know-how computer was developed by Wolfgang Back and Ulrich Rohde and first presented in the television program WDR Computerclub in 1983.\n\nHe was also published in German magazines MC and .\n\nThe \"computer\" worked on paper; Matches were used as information units. Only five commands were enough to represent all mathematical functions. This exercise computer on paper was sent in over 400,000 copies and belonged at that time to the computers with the widest circulation. An implementation as a computer program is available on Wolfgang Back's website.\n\nThe method of operation is based on register machine, but is more to the approach of Shepherdson and Sturgis.\n\nA derived version of the \"Paper Computer\" is used as a \"know how computer\" in Namibian school education.\n"}
{"id": "606411", "url": "https://en.wikipedia.org/wiki?curid=606411", "title": "Walter Bradford Cannon", "text": "Walter Bradford Cannon\n\nWalter Bradford Cannon (October 19, 1871 – October 1, 1945) was an American physiologist, professor and chairman of the Department of Physiology at Harvard Medical School. He coined the term fight or flight response, and he expanded on Claude Bernard's concept of homeostasis. He popularized his theories in his book \"The Wisdom of the Body\", first published in 1932. A \"Review of General Psychology\" survey, published in 2002, ranked Cannon as the 81st most cited scholar of the 20th century in technical psychology journals, introductory psychology textbooks, and survey responses.\n\nCannon was born on October 19, 1871 in Prairie du Chien, Wisconsin, the son of Colbert Hanschett Cannon and his wife Wilma Denio.\n\nIn his autobiography \"The Way of an Investigator\", Cannon counts himself among the descendents of Jacques de Noyon, a French Canadian explorer and coureur des bois. His Calvinist family was intellectually active, including readings from James Martineau, John Fiske (philosopher), and James Freeman Clarke. Cannon's curiosity also led him to Thomas Henry Huxley, John Tyndall, George Henry Lewes, and William Kingdon Clifford. A high school teacher, Mary Jeannette Newson, became his mentor. \"Miss May\" Newson motivated and helped him take his academic skills to Harvard University.\n\nIn 1896, his first year at Harvard Medical School, he started working in Bowditch's Lab, and in 1900 he received his medical degree.\n\nAfter graduation, Cannon was hired by William Townsend Porter at Harvard as an instructor in the Department of Physiology. He was a close friend of the physicist G. W. Pierce; they founded the Wicht Club with other young instructors for social and professional purposes. In 1906 Cannon became Higginson Professor and chairman of the Department of Physiology at Harvard Medical School, a position he held until 1942. From 1914 to 1916 he was also President of the American Physiological Society.\n\nHe was married to Cornelia James Cannon, a best-selling author and feminist reformer. Although not mountaineers, during their honeymoon in Montana the couple were the first, on July 19, 1901, to reach the summit of the unclimbed southwest peak (2657 m or 8716 ft) of Goat Mountain, between Lake McDonald and Logan Pass in what is now Glacier National Park. The peak was subsequently named Mount Cannon by the United States Geological Survey The couple had five children. One son was Dr. Bradford Cannon, a military plastic surgeon and radiation researcher. The daughters are Wilma Cannon Fairbank (Mrs. John K. Fairbank), Linda Cannon Burgess, Helen Cannon Bond and Marian Cannon Schlesinger, a painter and author living in Cambridge, Massachusetts.\n\nWalter Cannon died on October 1, 1945 in Franklin, New Hampshire.\n\nWalter Cannon began his career in science as a Harvard undergraduate in the year 1892. Henry Pickering Bowditch, who had worked with Claude Bernard, directed the laboratory in physiology at Harvard. Here Cannon began his research: he used the newly discovered X rays to study the mechanism of swallowing and the motility of the stomach. He demonstrated deglutition in a goose at the APS meeting in December 1896 and published his first paper on this research in the first issue of the \"American Journal of Physiology\" in January 1898.\n\nIn 1945 Cannon summarized his career in physiology by describing his focus at different ages:\n\n\nAs per Cannon, adrenaline exerts several important effects in different body organs, all of which, from Cannon’s point of view, maintain homeostasis in fight-or-flight situations . For example, in the skeletal muscle of the limbs, adrenaline relaxes blood vessels, increasing local blood flow. Adrenaline constricts blood vessels in the skin, minimizing blood loss from physical trauma. Adrenaline also releases the key metabolic fuel, glucose, by the liver into the bloodstream, etc. \nHowever, the fact that aggressive attack and fearful escape both involve adrenaline release into the bloodstream does not imply an equivalence of “fight” with “flight” from a physiological or biochemical point of view. \n\n\n\nCannon’s proposed the existence and functional unity of the sympathoadrenal (or “sympathoadrenomedullary” or “sympathico-adrenal”) system. He theorized that the sympathetic nervous system and the adrenal gland work together as a unit to maintain homeostasis in emergencies. To identify and quantify adrenaline release during stress, beginning in about 1919 Cannon exploited an ingenious experimental setup. He would surgically excise the nerves supplying the heart of a laboratory animal such as a dog or cat. Then he would subject the animal to a stressor and record the heart rate response. With the nerves to the heart removed, he could deduce that if the heart rate increased in response to the perturbation, then the increase in heart rate must have resulted from the actions of a hormone. Finally, he would compare the results in an animal with intact adrenal glands with those in an animal from which he had removed the adrenal glands. From the difference in the heart rate between the two animals, he could infer further that the hormone responsible for the increase in heart rate came from the adrenal glands. Moreover, the amount of increase in the heart rate provided a measure of the amount of hormone released. Cannon became so convinced that the sympathetic nervous system and adrenal gland functioned as a unit that in the 1930s he formally proposed that the sympathetic nervous system uses the same chemical messenger—adrenaline—as does the adrenal gland. Cannon’s notion of a unitary sympathoadrenal system persists to this day. Researchers in the area have come to question the validity of the notion of a unitary sympathoadrenal system, although clinicians often continue to lump together the two components. \n\n\n\nCannon wrote several books and articles.\n\n\n"}
{"id": "15112", "url": "https://en.wikipedia.org/wiki?curid=15112", "title": "Wave interference", "text": "Wave interference\n\nIn physics, interference is a phenomenon in which two waves superpose to form a resultant wave of greater, lower, or the same amplitude. Interference usually refers to the interaction of waves that are correlated or coherent with each other, either because they come from the same source or because they have the same or nearly the same frequency. Interference effects can be observed with all types of waves, for example, light, radio, acoustic, surface water waves or matter waves.\n\nThe principle of superposition of waves states that when two or more propagating waves of same type are incident on the same point, the resultant amplitude at that point is equal to the vector sum of the amplitudes of the individual waves. If a crest of a wave meets a crest of another wave of the same frequency at the same point, then the amplitude is the sum of the individual amplitudes—this is constructive interference. If a crest of one wave meets a trough of another wave, then the amplitude is equal to the difference in the individual amplitudes—this is known as destructive interference.\n\nConstructive interference occurs when the phase difference between the waves is an even multiple of π (180°) , whereas destructive interference occurs when the difference is an odd multiple of π. If the difference between the phases is intermediate between these two extremes, then the magnitude of the displacement of the summed waves lies between the minimum and maximum values.\n\nConsider, for example, what happens when two identical stones are dropped into a still pool of water at different locations. Each stone generates a circular wave propagating outwards from the point where the stone was dropped. When the two waves overlap, the net displacement at a particular point is the sum of the displacements of the individual waves. At some points, these will be in phase, and will produce a maximum displacement. In other places, the waves will be in anti-phase, and there will be no net displacement at these points. Thus, parts of the surface will be stationary—these are seen in the figure above and to the right as stationary blue-green lines radiating from the centre.\n\nInterference of light is a common phenomenon that can be explained classically by the superposition of waves, however a deeper understanding of light interference requires knowledge of wave-particle duality of light which is due to quantum mechanics. Prime examples of light interference are the famous double-slit experiment, laser speckle, anti-reflective coatings and interferometers. Traditionally the classical wave model is taught as a basis for understanding optical interference, based the Huygens–Fresnel principle.\n\nThe above can be demonstrated in one dimension by deriving the formula for the sum of two waves. The equation for the amplitude of a sinusoidal wave traveling to the right along the x-axis is\nwhere formula_2 is the peak amplitude, formula_3 is the wavenumber and formula_4 is the angular frequency of the wave. Suppose a second wave of the same frequency and amplitude but with a different phase is also traveling to the right \nwhere formula_6 is the phase difference between the waves in radians. The two waves will superpose and add: the sum of the two waves is\nUsing the trigonometric identity for the sum of two cosines: formula_8, this can be written\nThis represents a wave at the original frequency, traveling to the right like the components, whose amplitude is proportional to the cosine of formula_10. \n\nA simple form of interference pattern is obtained if two plane waves of the same frequency intersect at an angle. \nInterference is essentially an energy redistribution process. The energy which is lost at the destructive interference is regained at the constructive interference.\nOne wave is travelling horizontally, and the other is travelling downwards at an angle θ to the first wave. Assuming that the two waves are in phase at the point B, then the relative phase changes along the \"x\"-axis. The phase difference at the point A is given by\n\nIt can be seen that the two waves are in phase when\n\nand are half a cycle out of phase when\n\nConstructive interference occurs when the waves are in phase, and destructive interference when they are half a cycle out of phase. Thus, an interference fringe pattern is produced, where the separation of the maxima is\n\nand is known as the fringe spacing. The fringe spacing increases with increase in wavelength, and with decreasing angle .\n\nThe fringes are observed wherever the two waves overlap and the fringe spacing is uniform throughout.\n\nA point source produces a spherical wave. If the light from two point sources overlaps, the interference pattern maps out the way in which the phase difference between the two waves varies in space. This depends on the wavelength and on the separation of the point sources. The figure to the right shows interference between two spherical waves. The wavelength increases from top to bottom, and the distance between the sources increases from left to right.\n\nWhen the plane of observation is far enough away, the fringe pattern will be a series of almost straight lines, since the waves will then be almost planar.\n\nInterference occurs when several waves are added together provided that the phase differences between them remain constant over the observation time.\n\nIt is sometimes desirable for several waves of the same frequency and amplitude to sum to zero (that is, interfere destructively, cancel). This is the principle behind, for example, 3-phase power and the diffraction grating. In both of these cases, the result is achieved by uniform spacing of the phases.\n\nIt is easy to see that a set of waves will cancel if they have the same amplitude and their phases are spaced equally in angle. Using phasors, each wave can be represented as formula_21 for formula_22 waves from formula_23 to formula_24, where\n\nTo show that\n\none merely assumes the converse, then multiplies both sides by formula_27\n\nThe Fabry–Pérot interferometer uses interference between multiple reflections.\n\nA diffraction grating can be considered to be a multiple-beam interferometer; since the peaks which it produces are generated by interference between the light transmitted by each of the elements in the grating; see interference vs. diffraction for further discussion.\n\nBecause the frequency of light waves (~10 Hz) is too high to be detected by currently available detectors, it is possible to observe only the intensity of an optical interference pattern. The intensity of the light at a given point is proportional to the square of the average amplitude of the wave. This can be expressed mathematically as follows. The displacement of the two waves at a point is:\n\nwhere represents the magnitude of the displacement, represents the phase and represents the angular frequency.\n\nThe displacement of the summed waves is\n\nThe intensity of the light at is given by\n\nThis can be expressed in terms of the intensities of the individual waves as\n\nThus, the interference pattern maps out the difference in phase between the two waves, with maxima occurring when the phase difference is a multiple of 2π. If the two beams are of equal intensity, the maxima are four times as bright as the individual beams, and the minima have zero intensity.\n\nThe two waves must have the same polarization to give rise to interference fringes since it is not possible for waves of different polarizations to cancel one another out or add together. Instead, when waves of different polarization are added together, they give rise to a wave of a different polarization state.\n\nThe discussion above assumes that the waves which interfere with one another are monochromatic, i.e. have a single frequency—this requires that they are infinite in time. This is not, however, either practical or necessary. Two identical waves of finite duration whose frequency is fixed over that period will give rise to an interference pattern while they overlap. Two identical waves which consist of a narrow spectrum of frequency waves of finite duration, will give a series of fringe patterns of slightly differing spacings, and provided the spread of spacings is significantly less than the average fringe spacing, a fringe pattern will again be observed during the time when the two waves overlap.\n\nConventional light sources emit waves of differing frequencies and at different times from different points in the source. If the light is split into two waves and then re-combined, each individual light wave may generate an interference pattern with its other half, but the individual fringe patterns generated will have different phases and spacings, and normally no overall fringe pattern will be observable. However, single-element light sources, such as sodium- or mercury-vapor lamps have emission lines with quite narrow frequency spectra. When these are spatially and colour filtered, and then split into two waves, they can be superimposed to generate interference fringes. All interferometry prior to the invention of the laser was done using such sources and had a wide range of successful applications.\n\nA laser beam generally approximates much more closely to a monochromatic source, and it is much more straightforward to generate interference fringes using a laser. The ease with which interference fringes can be observed with a laser beam can sometimes cause problems in that stray reflections may give spurious interference fringes which can result in errors.\n\nNormally, a single laser beam is used in interferometry, though interference has been observed using two independent lasers whose frequencies were sufficiently matched to satisfy the phase requirements.\nThis has also been observed for widefield interference between two incoherent laser sources.\nIt is also possible to observe interference fringes using white light. A white light fringe pattern can be considered to be made up of a 'spectrum' of fringe patterns each of slightly different spacing. If all the fringe patterns are in phase in the centre, then the fringes will increase in size as the wavelength decreases and the summed intensity will show three to four fringes of varying colour. Young describes this very elegantly in his discussion of two slit interference. Since white light fringes are obtained only when the two waves have travelled equal distances from the light source, they can be very useful in interferometry, as they allow the zero path difference fringe to be identified.\n\nTo generate interference fringes, light from the source has to be divided into two waves which have then to be re-combined. Traditionally, interferometers have been classified as either amplitude-division or wavefront-division systems.\n\nIn an amplitude-division system, a beam splitter is used to divide the light into two beams travelling in different directions, which are then superimposed to produce the interference pattern. The Michelson interferometer and the Mach-Zehnder interferometer are examples of amplitude-division systems.\n\nIn wavefront-division systems, the wave is divided in space—examples are Young's double slit interferometer and Lloyd's mirror.\n\nInterference can also be seen in everyday phenomena such as iridescence and structural coloration. For example, the colours seen in a soap bubble arise from interference of light reflecting off the front and back surfaces of the thin soap film. Depending on the thickness of the film, different colours interfere constructively and destructively.\n\nInterferometry has played an important role in the advancement of physics, and also has a wide range of applications in physical and engineering measurement.\n\nThomas Young's double slit interferometer in 1803 demonstrated interference fringes when two small holes were illuminated by light from another small hole which was illuminated by sunlight. Young was able to estimate the wavelength of different colours in the spectrum from the spacing of the fringes. The experiment played a major role in the general acceptance of the wave theory of light. \nIn quantum mechanics, this experiment is considered to demonstrate the inseparability of the wave and particle natures of light and other quantum particles (wave–particle duality). Richard Feynman was fond of saying that all of quantum mechanics can be gleaned from carefully thinking through the implications of this single experiment.\nThe results of the Michelson–Morley experiment are generally considered to be the first strong evidence against the theory of a luminiferous aether and in favor of special relativity.\n\nInterferometry has been used in defining and calibrating length standards. When the metre was defined as the distance between two marks on a platinum-iridium bar, Michelson and Benoît used interferometry to measure the wavelength of the red cadmium line in the new standard, and also showed that it could be used as a length standard. Sixty years later, in 1960, the metre in the new SI system was defined to be equal to 1,650,763.73 wavelengths of the orange-red emission line in the electromagnetic spectrum of the krypton-86 atom in a vacuum. This definition was replaced in 1983 by defining the metre as the distance travelled by light in vacuum during a specific time interval. Interferometry is still fundamental in establishing the calibration chain in length measurement.\n\nInterferometry is used in the calibration of slip gauges (called gauge blocks in the US) and in coordinate-measuring machines. It is also used in the testing of optical components.\n\nIn 1946, a technique called astronomical interferometry was developed. Astronomical radio interferometers usually consist either of arrays of parabolic dishes or two-dimensional arrays of omni-directional antennas. All of the telescopes in the array are widely separated and are usually connected together using coaxial cable, waveguide, optical fiber, or other type of transmission line. Interferometry increases the total signal collected, but its primary purpose is to vastly increase the resolution through a process called Aperture synthesis. This technique works by superposing (interfering) the signal waves from the different telescopes on the principle that waves that coincide with the same phase will add to each other while two waves that have opposite phases will cancel each other out. This creates a combined telescope that is equivalent in resolution (though not in sensitivity) to a single antenna whose diameter is equal to the spacing of the antennas furthest apart in the array.\n\nAn acoustic interferometer is an instrument for measuring the physical characteristics of sound wave in a gas or liquid. It may be used to measure velocity, wavelength, absorption, or impedance. A vibrating crystal creates the ultrasonic waves that are radiated into the medium. The waves strike a reflector placed parallel to the crystal. The waves are then reflected back to the source and measured.\n\nIf a system is in state formula_33, its wavefunction is described in Dirac or bra–ket notation as:\n\nwhere the formula_35s specify the different quantum \"alternatives\" available (technically, they form an eigenvector basis) and the formula_36 are the probability amplitude coefficients, which are complex numbers.\n\nThe probability of observing the system making a transition or quantum leap from state formula_33 to a new state formula_38 is the square of the modulus of the scalar or inner product of the two states:\n\nwhere formula_41 (as defined above) and similarly formula_42 are the coefficients of the final state of the system. * is the complex conjugate so that formula_43, etc.\n\nNow let's consider the situation classically and imagine that the system transited from formula_44 to formula_45 via an intermediate state formula_46. Then we would \"classically\" expect the probability of the two-step transition to be the sum of all the possible intermediate steps. So we would have\n\nThe classical and quantum derivations for the transition probability differ by the presence, in the quantum case, of the extra terms formula_49; these extra quantum terms represent \"interference\" between the different formula_50 intermediate \"alternatives\". These are consequently known as the \"quantum interference terms\", or \"cross terms\". This is a purely quantum effect and is a consequence of the non-additivity of the probabilities of quantum alternatives.\n\nThe interference terms vanish, via the mechanism of quantum decoherence, if the intermediate state formula_35 is measured or coupled with its environment. \n\n"}
{"id": "78027", "url": "https://en.wikipedia.org/wiki?curid=78027", "title": "Whirlwind I", "text": "Whirlwind I\n\nWhirlwind I was a Cold War-era vacuum tube computer developed by the MIT Servomechanisms Laboratory for the U.S. Navy. It was among the first digital electronic computers that operated in real-time for output, and the first that was not simply an electronic replacement of older mechanical systems.\n\nIt was one of the first computers to calculate in parallel (rather than serial), and was the first to use magnetic core memory.\n\nIts development led directly to the Whirlwind II design used as the basis for the United States Air Force SAGE air defense system, and indirectly to almost all business computers and minicomputers in the 1960s.\n\nDuring World War II, the U.S. Navy approached MIT about the possibility of creating a computer to drive a flight simulator for training bomber crews. They envisioned a fairly simple system in which the computer would continually update a simulated instrument panel based on control inputs from the pilots. Unlike older systems like the Link Trainer, the system they envisioned would have a considerably more realistic aerodynamics model that could be adapted to any type of plane. This was an important consideration at the time, when many new designs were being introduced into service.\n\nThe Servomechanisms Lab in MIT building 32 conducted a short survey that concluded such a system was possible. The Navy decided to fund development under \"Project Whirlwind\", and the lab placed Jay Forrester in charge of the project. They soon built a large analog computer for the task, but found that it was inaccurate and inflexible. Solving these problems in a general way would require a much larger system, perhaps one so large as to be impossible to construct.\n\nIn 1945, Perry Crawford, another member of the MIT team, saw a demonstration of ENIAC and suggested that a digital computer was the solution. Such a machine would allow the accuracy of the simulation to be improved with the addition of more code in the computer program, as opposed to adding parts to the machine. As long as the machine was fast enough, there was no theoretical limit to the complexity of the simulation.\n\nUntil this point, all computers constructed were dedicated to single tasks, and run in batch mode. A series of inputs were set up in advance and fed into the computer, which would work out the answers and print them. This was not appropriate for the Whirlwind system, which needed to operate continually on an ever-changing series of inputs. Speed became a major issue: whereas with other systems it simply meant waiting longer for the printout, with Whirlwind it meant seriously limiting the amount of complexity the simulation could include.\n\nBy 1947, Forrester and collaborator Robert Everett completed the design of a high-speed stored-program computer for this task. Most computers of the era operated in \"bit-serial\" mode, using single-bit arithmetic and feeding in large words, often 48 or 60 bits in size, one bit at a time. This was simply not fast enough for their purposes, so Whirlwind included sixteen such math units, operating on a complete 16-bit word every cycle in \"bit-parallel\" mode. Ignoring memory speed, Whirlwind (\"20,000 single-address operations per second\" in 1951) was essentially sixteen times as fast as other machines. Today, almost all CPUs perform arithmetic in \"bit-parallel\" mode.\n\nThe word size was selected after some deliberation. The machine worked by passing in a single address with almost every instruction, thereby reducing the number of memory accesses. For operations with two operands, adding for instance, the \"other\" operand was assumed to be the last one loaded. Whirlwind operated much like a reverse Polish notation calculator in this respect; except there was no operand stack, only an accumulator. The designers felt that 2048 words of memory would be the minimum usable amount, requiring 11 bits to represent an address, and that 16 to 32 instructions would be the minimum for another five bits — and so it was 16 bits.\n\nThe Whirlwind design incorporated a control store driven by a master clock. Each step of the clock selected one or more signal lines in a diode matrix that enabled gates and other circuits on the machine. A special switch directed signals to different parts of the matrix to implement different instructions. In the early 1950s, Whirlwind I \"would crash every 20 minutes on average.\"\n\nThe design used approximately 5,000 vacuum tubes.\n\nWhirlwind construction started in 1948, an effort that employed 175 people. including 70 engineers and technicians. In the third quarter of 1949 the computer was advanced enough to solve an equation and display its solution on an oscilloscope, and even for the first animated and interactive computer graphic game. Finally Whirlwind \"successfully accomplished digital computation of interception courses\" on April 20, 1951. The project's budget was approximately $1 million a year, which was vastly higher than the development costs of most other computers of the era. After three years the Navy had lost interest. However, during this time the Air Force had become interested in using computers to help the task of ground controlled interception, and the Whirlwind was the only machine suitable to the task. They took up development under \"Project Claude\".\n\nWhirlwind weighed .\n\nThe original machine design called for 2048 (2K) words of 16 bits each of random-access storage. The only two available memory technologies in 1949 that could hold this much data were mercury delay lines and electrostatic storage.\n\nA mercury delay line consisted of a long tube filled with mercury, a mechanical transducer on one end, and a microphone on the other end, much like a spring reverb unit later used in audio processing. Pulses were sent into the mercury delay line at one end, and took a certain amount of time to reach the other end. They were detected by the microphone, amplified, reshaped into the correct pulse shape, and sent back into the delay line. Thus, the memory was said to recirculate.\n\nMercury delay lines operated at about the speed of sound, so were very slow in computer terms, even by the standards of the computers of the late 1940s and 1950s. The speed of sound in mercury was also very dependent on temperature. Since a delay line held a defined number of bits, the frequency of the clock had to change with the temperature of the mercury. If there were many delay lines and they did not all have the same temperature at all times, the memory data could easily become corrupted.\n\nThe Whirlwind designers quickly discarded the delay line as a possible memory—it was both too slow for the envisioned flight simulator, and too unreliable for a reproducible production system, for which Whirlwind was intended to be a functional prototype.\n\nThe alternative form of memory was known as \"electrostatic\". This was a cathode ray tube memory, similar in many aspects to an early TV picture tube or oscilloscope tube. An electron gun sent a beam of electrons to the far end of the tube, where they impacted a screen. The beam would be deflected to land at a particular spot on the screen. The beam could then build up a negative charge at that point, or change a charge that was already there. By measuring the beam current it could be determined whether the spot was originally a zero or a one, and a new value could be stored by the beam.\n\nThere were several forms of electrostatic memory tubes in existence in 1949. The best known today is the Williams tube, developed in England, but there were a number of others that had been developed independently by various research labs. The Whirlwind engineers considered the Williams tube, but determined that the dynamic nature of the storage and the need for frequent refresh cycles was incompatible with the design goals for Whirlwind I. Instead, they settled on a design that was being developed at the MIT Radiation Laboratory. This was a dual-gun electron tube. One gun produced a sharply-focused beam to read or write individual bits. The other gun was a \"flood gun\" that sprayed the entire screen with low-energy electrons. As a result of the design, this tube was more of a static RAM that did not require refresh cycles, unlike the dynamic RAM Williams tube.\n\nIn the end the choice of this tube was unfortunate. The Williams tube was considerably better developed, and despite the need for refresh could easily hold 1024 bits per tube, and was quite reliable when operated correctly. The MIT tube was still in development, and while the goal was to hold 1024 bits per tube, this goal was never reached, even several years after the plan had called for full-size functional tubes. Also, the specifications had called for an access time of six microseconds, but the actual access time was around 30 microseconds. Since the basic cycle time of the Whirlwind I processor was determined by the memory access time, the entire processor was slower than designed.\n\nJay Forrester was desperate to find a suitable memory replacement for his computer. Initially the computer only had 32 words of storage, and 27 of these words were read-only registers made of toggle switches. The remaining five registers were flip-flop storage, with each of the five registers being made from more than 30 vacuum tubes. This \"test storage\", as it was known, was intended to allow checkout of the processing elements while the main memory was not ready. Main memory was so late that the first experiments of tracking airplanes with live radar data were done using a program manually set into test storage. Forrester came across an advertisement for a new magnetic material being produced by a company. Recognizing that this had the potential to be a data storage medium, Forrester obtained a workbench in the corner of the lab, and got several samples of the material to experiment with. Then for several months he spent as much time in the lab as he did in the office managing the entire project. \n\nAt the end of those months he had invented the basics of magnetic core memory and demonstrated that it was likely to be feasible. His demonstration consisted of a small core plane of 32 cores, each three-eighths of an inch in diameter. Having demonstrated that the concept was practical, it needed only to be reduced to a workable design. In the fall of 1949, Forrester enlisted graduate student William N. Papian to test dozens of individual cores, to determine those with the best properties. Papian's work was bolstered when Forrester asked student Dudley Allen Buck to work on the material and assigned him to the workbench, while Forrester went back to full-time project management. (Buck would go on to invent the cryotron and content-addressable memory at the lab.)\n\nAfter approximately two years of further research and development, they were able to demonstrate a core plane that was made of 32 by 32, or 1024 cores, holding 1024 bits of data. Thus, they had reached the originally intended storage size of an electrostatic tube, a goal that had not yet been reached by the tubes themselves, only holding 512 bits per tube in the latest design generation. Very quickly a 1024-word core memory was fabricated, replacing the electrostatic memory. The electrostatic memory design and production was summarily canceled, saving a good deal of money to be reallocated to other research areas. Two additional core memory units were later fabricated, increasing the total memory size available.\n\nAfter connection to the experimental Microwave Early Warning (MEW) radar at Hanscom Field using Jack Harrington's equipment and commercial phone lines, aircraft were tracked by Whirlwind I. The Cape Cod System subsequently demonstrated computerized air defence covering southern New England. Signals from three long range (AN/FPS-3) radars, eleven gap-filler radars, and three height-finding radars were transmitted over telephone lines to the Whirlwind I computer in Cambridge, Massachusetts. The Whirlwind II design for a larger and faster machine (never completed) was the basis for the SAGE air defense system IBM AN/FSQ-7 Combat Direction Central.\n\nThe Whirlwind used approximately 5,000 vacuum tubes. An effort was also started to convert the Whirlwind design to a transistorized form, led by Ken Olsen and known as the TX-0. TX-0 was very successful and plans were made to make an even larger version known as TX-1. However this project was far too ambitious and had to be scaled back to a smaller version known as TX-2. Even this version proved troublesome, and Olsen left in mid-project to start Digital Equipment Corporation (DEC). DEC's PDP-1 was essentially a collection of TX-0 and TX-2 concepts in a smaller package.\n\nAfter supporting SAGE, Whirlwind I was rented ($1/yr) from June 30, 1959, until 1974 by project member, Bill Wolf. \n\nKen Olsen and Robert Everett saved the machine, which became the basis for the Boston Computer Museum in 1979. It is now in the collection of the Computer History Museum in Mountain View, California.\n\nAs of February 2009, a core memory unit is displayed at the Charles River Museum of Industry & Innovation in Waltham, Massachusetts. One plane, on loan from the Computer History Museum, is on shown as part of the Historic Computer Science displays at the Gates Computer Science Building, Stanford.\n\nThe building which housed Whirlwind was until recently home to MIT's campus-wide IT department, Information Services & Technology and in 1997–1998, it was restored to its original exterior design.\n\n\n"}
