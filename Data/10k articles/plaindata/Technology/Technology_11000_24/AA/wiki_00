{"id": "25595255", "url": "https://en.wikipedia.org/wiki?curid=25595255", "title": "AOC International", "text": "AOC International\n\nAOC International (trading as AOC, formerly Admiral Overseas Corporation) is a multinational electronics company headquartered in Taipei, Taiwan and a subsidiary of TPV Technology. It designs and produces a full range of IPS and TFT monitors as well as LCD TVs and formerly CRT monitors for PCs which are sold worldwide under the AOC brand.\n\nAdmiral Overseas Corporation (AOC) was founded in Chicago, Illinois, by Ross Siragusa as the Asian arm of his Admiral Corporation, and later established in Taiwan in 1967 as the first manufacturer of color televisions for export. In 1978, Admiral Overseas Corporation was renamed AOC International. Direct Marketing under the AOC brand name began in 1979. From 1988 to 1997, AOC established its sales offices in United States, China, Europe, and Brazil. AOC was launched in India and Mexico in 2005 and 2006 respectively. Today AOC products including CRT & LCD monitors, LCD TVs, All-in-One units, and Android Tablets, are available in more than 40 countries globally.\n\nAOC History:\n\n\nIn 2006, AOC was selected in Info 200 Company – top 200 in technology, Brazil (from 82nd in 2005 to 59th position in 2006). AOC ranked Top 3 in monitor category in the \"2007 Best Brands Survey\" by \"PC World\" in Brazil. The same year AOC received the \"Fast-growing company in LCD monitors market\" award from CCID Consulting in China. AOC was ranked 33 of Global Top 50 CE Brands by IDG in CES Daily magazine in 2008. AOC received the Consumers' Brand of Choice from the China Computer Users Association in 2008. The same year AOC received Best in Hardware IT Monitors (Top 100 IT & Telecom, IDG ComputerWorld Magazine) in Brazil. In 2009, AOC was awarded as Power Brand Malaysia 2009 (Global CEO Magazine). Over the years AOC products have won several design awards including the Red Dot Design Award in 2008. In 2007 and 2008 several awards were conferred to AOC products in India by \"Digit\" & \"Chip\" magazines.\n\n"}
{"id": "1876356", "url": "https://en.wikipedia.org/wiki?curid=1876356", "title": "Architectural lighting design", "text": "Architectural lighting design\n\nArchitectural lighting design is a field within architecture, interior design and electrical engineering that is concerned with the design of lighting systems, including natural light, electric light, or both, to serve human needs.\n\nThe design process takes account of:\n\nThe objective of lighting design is the human response, to see clearly and without discomfort. The objective of architectural lighting design is to further the design of architecture or the experience of buildings and other physical structures.\n\nGas lighting was economical enough to light streets in major cities starting in the early 1800s, and was also used in some commercial buildings and in the homes of wealthy people. The gas mantle boosted the luminosity of utility lighting and of kerosene lamps. The next major drop in price came about with the incandescent light bulb powered by electricity.\n\nArchitectural lighting design focuses on three fundamental aspects of the illumination of buildings or spaces. The first is the aesthetic appeal of a building, an aspect particularly important in the illumination of retail environments. Secondly, the ergonomic aspect: the measure of how much of a function the lighting plays. Thirdly is the energy efficiency issue to ensure that light is not wasted by over illumination, either by illuminating vacant spaces unnecessarily or by providing more light than needed for the aesthetics or the task. Cultural factors also need to be considered; for example, bright lights was a mark of wealth through much of Chinese history.\n\nAs the Sun crosses the sky, it may appear to be red, orange, yellow or white depending on its position. The changing color of the Sun over the course of the day is mainly a result of scattering of light and is not due to changes in black-body radiation. The blue color of the sky is caused by Rayleigh scattering of the sunlight from the atmosphere, which tends to scatter blue light more than red light.\n\nFor colors based on black-body theory, blue occurs at higher temperatures, while red occurs at lower, cooler, temperatures. This is the opposite of the cultural associations attributed to colors, in which red represents hot, and blue cold.\n\nLighting fixtures come in a wide variety of styles for various functions. The most important functions are as a holder for the light source, to provide directed light and to avoid visual glare. Some are very plain and functional, while some are pieces of art in themselves. Nearly any material can be used, so long as it can tolerate the excess heat and is in keeping with safety codes.\n\nAn important property of light fixtures is the luminous efficacy or wall-plug efficiency, meaning the amount of usable light emanating from the fixture per used energy, usually measured in lumen per watt. A fixture using replaceable light sources can also have its efficiency quoted as the percentage of light passed from the \"bulb\" to the surroundings. The more transparent the lighting fixture is, the higher efficacy. Shading the light will normally decrease efficiency but increase the directionality and the visual comfort probability.\n\nThe PH-lamps are a series of light fixtures designed by Danish designer and writer Poul Henningsen from 1926 onwards. The lamp is designed with multiple concentric shades to eliminate visual glare, only emitting reflected light, obscuring the light source.\n\nPhotometric studies (also sometimes referred to as \"layouts\" or \"point by points\") are often used to simulate lighting designs for projects before they are built or renovated. This enables architects, lighting designers, and engineers to determine whether a proposed lighting setup will deliver the amount of light intended. They will also be able to determine the contrast ratio between light and dark areas. In many cases these studies are referenced against IESNA or CIBSE recommended lighting practices for the type of application. Depending on the type of area, different design aspects may be emphasized for safety or practicality (i.e. such as maintaining uniform light levels, avoiding glare or highlighting certain areas). Specialized software is often used to create these, which typically combine the use of two-dimensional digital CAD drawings and lighting simulation software.\n\nColor temperature for white light sources also affects their use for certain applications. The color temperature of a white light source is the temperature in kelvins of a theoretical black body emitter that most closely matches the spectral characteristics of the lamp. An incandescent bulb has a color temperature around 2800 to 3000 kelvins; daylight is around 6400 kelvins. Lower color temperature lamps have relatively more energy in the yellow and red part of the visible spectrum, while high color temperatures correspond to lamps with more of a blue-white appearance. For critical inspection or color matching tasks, or for retail displays of food and clothing, the color temperature of the lamps will be selected for the best overall lighting effect. Color may also be used for functional reasons. For example, blue light makes it difficult to see veins and thus may be used to discourage drug use.\n\nThe color temperature of a light source is the temperature of an ideal black-body radiator that radiates light of comparable hue to that of the light source. Color temperature is a characteristic of visible light that has important applications in lighting, photography, videography, publishing, manufacturing, astrophysics, horticulture, and other fields. In practice, color temperature is only meaningful for light sources that do in fact correspond somewhat closely to the radiation of some black body, i.e., those on a line from reddish/orange via yellow and more or less white to blueish white; it does not make sense to speak of the color temperature of, e.g., a green or a purple light. Color temperature is conventionally stated in the unit of absolute temperature, the kelvin, having the unit symbol K.\n\nFor lighting building interiors, it is often important to take into account the color temperature of illumination. For example, a warmer (i.e., lower color temperature) light is often used in public areas to promote relaxation, while a cooler (higher color temperature) light is used to enhance concentration in offices.\n\nCCT dimming for LED technology is regarded as a difficult task, since binning, age and temperature drift effects of LEDs change the actual color value output. Here feedback loop systems are used for example with color sensors, to actively monitor and control the color output of multiple color mixing LEDs.\n\nThe color temperature of the electromagnetic radiation emitted from an ideal black body is defined as its surface temperature in kelvins, or alternatively in \"mireds\" (micro-reciprocal kelvin). This permits the definition of a standard by which light sources are compared.\n\nTo the extent that a hot surface emits thermal radiation but is not an ideal black-body radiator, the color temperature of the light is not the actual temperature of the surface. An incandescent lamp's light is thermal radiation, and the bulb approximates an ideal black-body radiator, so its color temperature is essentially the temperature of the filament.\n\nMany other light sources, such as fluorescent lamps, or LEDs (light emitting diodes) emit light primarily by processes other than thermal radiation. This means that the emitted radiation does not follow the form of a black-body spectrum. These sources are assigned what is known as a correlated color temperature (CCT). CCT is the color temperature of a black-body radiator which to human color perception most closely matches the light from the lamp. Because such an approximation is not required for incandescent light, the CCT for an incandescent light is simply its unadjusted temperature, derived from the comparison to a black-body radiator.\n\nFor simple installations, hand-calculations based on tabular data can be used to provide an acceptable lighting design. More critical or optimized designs now routinely use mathematical modeling on a computer.\n\nBased on the positions and mounting heights of the fixtures, and their photometric characteristics, the proposed lighting layout can be checked for uniformity and quantity of illumination. For larger projects or those with irregular floor plans, lighting design software can be used. Each fixture has its location entered, and the reflectance of walls, ceiling, and floors can be entered. The computer program will then produce a set of contour charts overlaid on the project floor plan, showing the light level to be expected at the working height. More advanced programs can include the effect of light from windows or skylights, allowing further optimization of the operating cost of the lighting installation. The amount of daylight received in an internal space can typically be analyzed by undertaking a daylight factor calculation.\n\nThe Zonal Cavity Method is used as a basis for both hand, tabulated, and computer calculations. This method uses the reflectance coefficients of room surfaces to model the contribution to useful illumination at the working level of the room due to light reflected from the walls and the ceiling. Simplified photometric values are usually given by fixture manufacturers for use in this method.\n\nComputer modeling of outdoor flood lighting usually proceeds directly from photometric data. The total lighting power of a lamp is divided into small solid angular regions. Each region is extended to the surface which is to be lit and the area calculated, giving the light power per unit of area. Where multiple lamps are used to illuminate the same area, each one's contribution is summed. Again the tabulated light levels (in lux or foot-candles) can be presented as contour lines of constant lighting value, overlaid on the project plan drawing. Hand calculations might only be required at a few points, but computer calculations allow a better estimate of the uniformity and lighting level.\n\nThe Illuminating Engineering Society of Australia and New Zealand was established in 1930 during the Great Depression.\n\nThe International Association of Lighting Designers (IALD) was founded in 1969, and its current mission is \"to serve the IALD worldwide membership by promoting the visible success of its members in practicing lighting design.\" The organization created a new attitude towards the profession and raised the profile of architectural lighting design, one its principal goals.\n\nThe Professional Lighting Designers Association (PLDA) was formed in 1993 as the European Lighting Designers' Association (ELDA, later ELDA+). Until it was dissolved in 2014, it was with the IALD one of the main authorities regarding lighting design in architecture.\n\nThe Illuminating Engineering Society of North America (IESNA) seeks to improve the lighted environment by bringing together those with lighting knowledge and by translating that knowledge into actions that benefit the public.\n\nThe National Council on Qualifications for the Lighting Professions (NCQLP) is a non-profit organization founded in 1991 to serve and protect the well-being of the public through effective and efficient lighting practice. Through a peer-review process, the NCQLP establishes the education, experience and examination requirements for baseline certification across the lighting professions. The NCQLP has established a certification process by which practitioners in lighting and related fields, through testing, demonstrate their knowledge and experience across the lighting professions. Those who successfully complete the NCQLP Lighting Certification Examination are entitled to use the appellation LC (Lighting Certified) after their name for professional purposes.\n\nThe International Commission on Illumination (CIE) is an organization \"devoted to international cooperation and exchange of information among its member countries on all matters relating to the science and art of lighting.\" CIE works globally to develop and publish lighting design standardization and best-practice documents.\n\nThe Professional Lighting & Sound Association (PLASA) represents the interests of many lighting designers and manufacturers, several of which are involved in the Architectural lighting market. PLASA is UK orientated, but does represent companies on a European and International level.\n\nThere are many more nationally-based organizations such as the Schweizerische Licht Gesellschaft (SLG) in Switzerland, the Association des Concepteurs Lumière et Éclairagistes (ACE) in France, the Hellenic Illumination Committee (HIC) in Greece and the Associazione Professionisti dell'Illuminazione (APIL) in Italy.\n\n\n\n\n\n\nWith the increase in global focus on green design and energy codes, lighting design and its role in sustainability have become more well known, resulting in a number of lighting-specific trade publications and an increase in coverage in architectural publications.\n\n\nTypes of electric lighting include:\n\nDifferent types of lights have vastly differing efficiencies and color of light. \n\nThe most efficient source of electric light is the low-pressure sodium lamp. It produces, for all practical purposes, a monochromatic orange/yellow light, which gives a similarly monochromatic perceprtion of any illuminated scene. For this reason, it is generally reserved for outdoor public lighting usages. Low-pressure sodium lights are favoured for public lighting by astronomers, since the light pollution that they generate can be easily filtered, contrary to broadband or continuous spectra.\n\nThe modern incandescent light bulb, with a coiled filament of tungsten, was commercialized in the 1920s developed from the carbon filament lamp introduced in about 1880. As well as bulbs for normal illumination, there is a very wide range, including low voltage, low-power types often used as components in equipment, but now largely displaced by LEDs\n\nThere is currently interest in banning some types of filament lamp in some countries, such as Australia planning to ban standard incandescent light bulbs by 2010, because they are inefficient at converting electricity to light. Sri Lanka has already banned importing filament bulbs because of high use of electricity and less light. Less than 3% of the input energy is converted into usable light. Nearly all of the input energy ends up as heat that, in warm climates, must then be removed from the building by ventilation or air conditioning, often resulting in more energy consumption. In colder climates where heating and lighting is required during the cold and dark winter months, the heat byproduct has at least some value.\n\n \nHalogen lamps are usually much smaller than standard incandescents, because for successful operation a bulb temperature over 200 °C is generally necessary. For this reason, most have a bulb of fused silica (quartz), but sometimes aluminosilicate glass. This is often sealed inside an additional layer of glass. The outer glass is a safety precaution, reducing UV emission and because halogen bulbs can occasionally explode during operation. One reason is if the quartz bulb has oily residue from fingerprints. The risk of burns or fire is also greater with bare bulbs, leading to their prohibition in some places unless enclosed by the luminaire.\n\nFluorescent lamps consist of a glass tube that contains mercury vapour or argon under low pressure. Electricity flowing through the tube causes the gases to give off ultraviolet energy. The inside of the tubes are coated with phosphors that give off visible light when struck by ultraviolet energy. have much higher efficiency than Incandescent lamps. For the same amount of light generated, they typically use around one-quarter to one-third the power of an incandescent.\n\nSolid state light-emitting diodes (LEDs) have been popular as indicator lights since the 1970s. In recent years, efficacy and output have risen to the point where LEDs are now being used in niche lighting applications.\n\nIndicator LEDs are known for their extremely long life, up to 100,000 hours, but lighting LEDs are operated much less conservatively (due to high LED cost per watt), and consequently have much shorter lives.\n\nDue to the relatively high cost per watt, LED lighting is most useful at very low powers, typically for lamp assemblies of under 10 W. LEDs are currently most useful and cost-effective in low power applications, such as nightlights and flashlights. Colored LEDs can also be used for accent lighting, such as for glass objects, and even in fake ice cubes for drinks at parties. They are also being increasingly used as holiday lighting.\n\nLED efficiencies vary over a very wide range. Some have lower efficiency than filament lamps, and some significantly higher. LED performance in this respect is prone to being misinterpreted, as the inherent directionality of LEDs gives them a much higher light intensity in one direction per given total light output.\n\nSingle color LEDs are well developed technology, but white LEDs at time of writing still have some unresolved issues:\n\nLED technology is useful for lighting designers because of its low power consumption, low heat generation, instantaneous on-and-off control, and in the case of single color LEDs, continuity of color throughout the life of the diode and relatively low cost of manufacture.\n\nIn the last few years, software has been developed to merge lighting and video by enabling lighting designers to stream video content to their LED fixtures, creating low resolution video walls.\n\n"}
{"id": "6307651", "url": "https://en.wikipedia.org/wiki?curid=6307651", "title": "Association of Biomolecular Resource Facilities", "text": "Association of Biomolecular Resource Facilities\n\nThe Association of Biomolecular Resource Facilities (ABRF) is dedicated to advancing core and research biotechnology laboratories through research, communication, and education. ABRF members include over 900 scientists representing 340 different core laboratories in 41 countries, including those in industry, government, academic and research institutions.\n\nIn 1986 a Research Resource Facility Satellite Meeting was held in conjunction with the Sixth International Conference on Methods in Protein Sequence Analysis. The next year protein sequencing and amino acid samples were sent to survey 103 core facilities. By 1989 the ABRF was formally organized and incorporated. Each year an annual meeting was held as a satellite meeting of the Protein Society until 1996 when separate meetings began.\n\nResearch Groups are established to fulfill two of the purposes of the Association of Biomolecular Resource Facilities. First, to provide mechanisms for the self-evaluation and improvement of procedural and operational accuracy, precision and efficiency in resource facilities and research laboratories. Second, to contribute to the education of resource facility and research laboratory staff, users, administrators, and interested members of the scientific community. The results of ABRF Research Group studies have been published in scientific papers and the data reused in subsequent works.\n\nMembers of ABRF are involved in a broad spectrum of biomolecular technologies that are implemented in core facility settings: \n\nEvery year the Association of Biomolecular Resource Facilities annual conference is held during the spring in a varying North American city. This international conference is used to expose members to new and emerging biotechnology through lectures, roundtables, Research Group presentations, poster sessions, workshops and technical exhibits.\n\nThe ABRF Award is presented at the annual ABRF meeting for outstanding contributions to Biomolecular Technologies.\nPast Award Winners:\n\nThe ABRF is the publisher of the Journal of Biomolecular Techniques. The journal is peer-reviewed and is published quarterly. The major focus of the journal is to publish scientific reviews and articles related to biomolecular resource facilities. The Research Group published reports include annual surveys. News and events, as well as an article watch focused on techniques used in typical core facility environments are also included.\n\n\n"}
{"id": "44047145", "url": "https://en.wikipedia.org/wiki?curid=44047145", "title": "Asus Transformer Pad TF701T", "text": "Asus Transformer Pad TF701T\n\nThe Asus Transformer Pad TF701T is an Android tablet computer made by Asus, successor to the Asus Transformer Pad Infinity. The Transformer design includes a docking keyboard. The Asus Transformer Pad TF701T was released on the market in the UK in October 2013 and in the U.S. in November 2013.\n\nThis new model includes a Tegra 4 T114 processor clocked at 1.9 GHz, and an upgraded 2560×1600 pixel resolution screen, increasing the pixel density to 300 PPI.\n\nCyanogenMod version 11 XNG3C (based on Android KitKat 4.4.4 with security fixes) up to version 12.1 can be installed on the TF701T tablet.\n"}
{"id": "33075099", "url": "https://en.wikipedia.org/wiki?curid=33075099", "title": "Boiler design", "text": "Boiler design\n\nBoiler design is the process of designing boilers used for various purposes. The main function of a boiler is to heat water to generate steam. Steam produced in a boiler can be used for a variety of purposes including space heating, sterilisation, drying, humidification and power generation. The temperature or condition of steam required for these applications is different, so boiler designs vary accordingly.\n\nModern boiler design offers several benefits. In the past, improper design of boilers has caused explosions which led to loss of life and property. Modern designs attempt to avoid such mishaps. Further, mathematical modeling can determine how much space a boiler will need and the type of materials to be used. When the design specifications of a boiler are determined, design engineers can estimate a cost and time schedule for the construction.\n\nBoiler design may be based upon:\n\nAccessories and mountings are devices which form an integral part of boiler but are not mounted on it. They include economizers, superheaters, feed pumps and air pre-heaters. Accessories help in controlling and running the boiler efficiently. Certain common mountings (specifically those required by the Indian Boiler Act) include:\n\n"}
{"id": "57863698", "url": "https://en.wikipedia.org/wiki?curid=57863698", "title": "Bridge plate (mechanism)", "text": "Bridge plate (mechanism)\n\nA bridge plate, or bridgeplate, is a mechanical, movable form of wheelchair ramp that is used on some low-floor light rail vehicles (LRVs) to provide for wheelchair access. The bridge plate extends from the vehicle to the platform, which must be raised to close to the level of the floor of the vehicle so that the wheelchair need not travel over an excessively steep ramp (in the United States, the Americans with Disabilities Act specifies that the slope must be no more than 1 inch of rise for every 12 inches of length). Some low-floor buses also use bridge plates (in this case, extending to the curb) to provide for wheelchair access, but many low-floor buses instead use a ramp that normally serves as part of the floor but can be flipped out through the door (using a hinge at the door) onto the curb or street; in this case the ramp is long enough that it can serve as a true wheelchair ramp rather than a bridge without being excessively steep.\n\nBridge plates can be manually deployed (by the vehicle operator or other crew person) or powered, rectractable ramps. The first passenger rail cars in North America to be equipped with retractable bridge plates were TriMet's (Portland, Oregon) Siemens SD660 LRVs, the first of which were completed in 1996. Earlier, in 1987, the newly opened Sacramento RT Light Rail system used non-powered, station-platform-mounted bridge plates to bridge the gap between a high-platform section at each station and the floor of an LRV.\n\n"}
{"id": "5871200", "url": "https://en.wikipedia.org/wiki?curid=5871200", "title": "Color motion picture film", "text": "Color motion picture film\n\nColor motion picture film refers both to unexposed color photographic film in a format suitable for use in a motion picture camera, and to finished motion picture film, ready for use in a projector, which bears images in color.\n\nThe first color cinematography was by additive color systems such as the one patented by Edward Raymond Turner in 1899 and tested in 1902. A simplified additive system was successfully commercialised in 1909 as Kinemacolor. These early systems used black-and-white film to photograph and project two or more component images through different color filters.\n\nAround 1920, the first practical subtractive color processes were introduced. These also used black-and-white film to photograph multiple color-filtered source images, but the final product was a multicolored print that did not require special projection equipment. Before 1932, when three-strip Technicolor was introduced, commercialized subtractive processes used only two color components and could reproduce only a limited range of color.\n\nIn 1935, Kodachrome was introduced, followed by Agfacolor in 1936. They were intended primarily for amateur home movies and \"slides\". These were the first films of the \"integral tripack\" type, coated with three layers of differently color-sensitive emulsion, which is usually what is meant by the words \"color film\" as commonly used. The few color films still being made in the 2010s are of this type. The first color negative films and corresponding print films were modified versions of these films. They were introduced around 1940 but only came into wide use for commercial motion picture production in the early 1950s. In the US, Eastman Kodak's Eastmancolor was the usual choice, but it was often re-branded with another trade name, such as \"WarnerColor\", by the studio or the film processor.\n\nLater color films were standardized into two distinct processes: Eastman Color Negative 2 chemistry (camera negative stocks, duplicating interpositive and internegative stocks) and Eastman Color Positive 2 chemistry (positive prints for direct projection), usually abbreviated as ECN-2 and ECP-2. Fuji's products are compatible with ECN-2 and ECP-2.\n\nFilm was the dominant form of cinematography until the 2010s, when it was largely replaced by digital cinematography.\n\nThe first motion pictures were photographed using a simple homogeneous photographic emulsion that yielded a black-and-white image—that is, an image in shades of gray, ranging from black to white, corresponding to the luminous intensity of each point on the photographed subject. Light, shade, form and movement were captured, but not color.\n\nWith color motion picture film, information about the color of the light at each image point is also captured. This is done by analyzing the visible spectrum of color into several regions (normally three, commonly referred to by their dominant colors: red, green and blue) and recording each region separately.\n\nCurrent color films do this with three layers of differently color-sensitive photographic emulsion coated on one strip of film base. Early processes used color filters to photograph the color components as completely separate images (e.g., three-strip Technicolor) or adjacent microscopic image fragments (e.g., Dufaycolor) in a one-layer black-and-white emulsion.\n\nEach photographed color component, initially just a colorless record of the luminous intensities in the part of the spectrum that it captured, is processed to produce a transparent dye image in the color complementary to the color of the light that it recorded. The superimposed dye images combine to synthesize the original colors by the subtractive color method. In some early color processes (e.g., Kinemacolor), the component images remained in black-and-white form and were projected through color filters to synthesize the original colors by the additive color method.\n\nThe earliest motion picture stocks were orthochromatic, and recorded blue and green light, but not red. Recording all three spectral regions required making film stock panchromatic to some degree. Since orthochromatic film stock hindered color photography in its beginnings, the first films with color in them used aniline dyes to create artificial color. Hand-colored films appeared in 1895 with Thomas Edison's hand-painted \"Annabelle's Dance\" for his Kinetoscope viewers.\n\nMany early filmmakers from the first ten years of film also used this method to some degree. George Méliès offered hand-painted prints of his own films at an additional cost over the black-and-white versions, including the visual-effects pioneering \"A Trip to the Moon\" (1902). The film had various parts of the film painted frame-by-frame by twenty-one women in Montreuil in a production-line method.\n\nThe first commercially successful stencil color process was introduced in 1905 by Pathé Frères. \"Pathé Color\", renamed Pathéchrome in 1929, became one of the most accurate and reliable stencil coloring systems. It incorporated an original print of a film with sections cut by pantograph in the appropriate areas for up to six colors by a coloring machine with dye-soaked, velvet rollers. After a stencil had been made for the whole film, it was placed into contact with the print to be colored and run at high speed (60 feet per minute) through the coloring (staining) machine. The process was repeated for each set of stencils corresponding to a different color. By 1910, Pathé had over 400 women employed as stencilers in their Vincennes factory. Pathéchrome continued production through the 1930s.\n\nA more common technique emerged in the early 1910s known as film tinting, a process in which either the emulsion or the film base is dyed, giving the image a uniform monochromatic color. This process was popular during the silent era, with specific colors employed for certain narrative effects (red for scenes with fire or firelight, blue for night, etc.).\n\nA complementary process, called toning, replaces the silver particles in the film with metallic salts or mordanted dyes. This creates a color effect in which the dark parts of the image are replaced with a color (e.g., blue and white rather than black and white). Tinting and toning were sometimes applied together.\n\nIn the United States, St. Louis engraver Max Handschiegl and cinematographer Alvin Wyckoff created the Handschiegl Color Process, a dye-transfer equivalent of the stencil process, first used in \"Joan the Woman\" (1917) directed by Cecil B. DeMille, and used in special effects sequences for films such as \"The Phantom of the Opera\" (1925).\n\nEastman Kodak introduced its own system of pre-tinted black-and-white film stocks called Sonochrome in 1929. The Sonochrome line featured films tinted in seventeen different colors including Peachblow, Inferno, Candle Flame, Sunshine, Purple Haze, Firelight, Azure, Nocturne, Verdante, Aquagreen, Caprice, Fleur de Lis, Rose Doree, and the neutral-density Argent, which kept the screen from becoming excessively bright when switching to a black-and-white scene.\n\nTinting and toning continued to be used well into the sound era. In the 1930s and 1940s, some western films were processed in a sepia-toning solution to evoke the feeling of old photographs of the day. Tinting was used as late as 1951 for Sam Newfield's sci-fi film \"Lost Continent\" for the green lost-world sequences. Alfred Hitchcock used a form of hand-coloring for the orange-red gun-blast at the audience in \"Spellbound\" (1945). Kodak's Sonochrome and similar pre-tinted stocks were still in production until the 1970s and were used commonly for custom theatrical trailers and snipes.\n\nIn the last half of the 20th century, Norman McLaren, who was one of the pioneers in animated movies, made several animated films in which he directly hand-painted the images, and in some cases, also the soundtrack, on each frame of the film. This approach was previously employed in the early years of movies, late 19th and early 20th century. One of the precursors in color hand painting frame by frame was the Aragonese Segundo de Chomon, that worked with Melies.\n\nTinting was gradually replaced by natural color techniques.\n\nThe principles on which color photography is based were first proposed by Scottish physicist James Clerk Maxwell in 1855 and presented at the Royal Society in London in 1861. By that time, it was known that light comprises a spectrum of different wavelengths that are perceived as different colors as they are absorbed and reflected by natural objects. Maxwell discovered that all natural colors in this spectrum as perceived by the human eye may be reproduced with additive combinations of three primary colors—red, green, and blue—which, when mixed equally, produce white light.\n\nBetween 1900 and 1935, dozens of natural color systems were introduced, although only a few were successful.\n\nThe first color systems that appeared in motion pictures were additive color systems. Additive color was practical because no special color stock was necessary. Black-and-white film could be processed and used in both filming and projection. The various additive systems entailed the use of color filters on both the movie camera and projector. Additive color adds lights of the primary colors in various proportions to the projected image. Because of the limited amount of space to record images on film, and later because the lack of a camera that could record more than two strips of film at once, most early motion-picture color systems consisted of two colors, often red and green or red and blue.\n\nA pioneering three-color additive system was patented in England by Edward Raymond Turner in 1899. It used a rotating set of red, green and blue filters to photograph the three color components one after the other on three successive frames of panchromatic black-and-white film. The finished film was projected through similar filters to reconstitute the color. In 1902, Turner shot test footage to demonstrate his system, but projecting it proved problematic because of the accurate registration (alignment) of the three separate color elements required for acceptable results. Turner died a year later without having satisfactorily projected the footage. In 2012, curators at the National Media Museum in Bradford, UK, had the original custom-format nitrate film copied to black-and-white 35 mm film, which was then scanned into a digital video format by telecine. Finally, digital image processing was used to align and combine each group of three frames into one color image. The result is that the whole world can now view brief motion pictures from 1902 in full color.\n\nPractical color in the motion picture business began with Kinemacolor, first demonstrated in 1906. This was a two-color system created in England by George Albert Smith, and promoted by film pioneer Charles Urban's The Charles Urban Trading Company in 1908. It was used for a series of films including the documentary \"With Our King and Queen Through India\", depicting the Delhi Durbar (also known as \"The Durbar at Delhi\", 1912), which was filmed in December 1911. The Kinemacolor process consisted of alternating frames of specially sensitized black-and-white film exposed at 32 frames per second through a rotating filter with alternating red and green areas. The printed film was projected through similar alternating red and green filters at the same speed. A perceived range of colors resulted from the blending of the separate red and green alternating images by the viewer's persistence of vision.\n\nWilliam Friese-Greene invented another additive color system called Biocolour, which was developed by his son Claude Friese-Greene after William's death in 1921. William sued George Albert Smith, alleging that the Kinemacolor process infringed on the patents for his Bioschemes, Ltd.; as a result, Smith's patent was revoked in 1914. Both Kinemacolor and Biocolour had problems with \"fringing\" or \"haloing\" of the image, due to the separate red and green images not fully matching up.\n\nBy their nature, these additive systems were very wasteful of light. Absorption by the color filters involved meant that only a minor fraction of the projection light actually reached the screen, resulting in an image that was dimmer than a typical black-and-white image. The larger the screen, the dimmer the picture. For this and other case-by-case reasons, the use of additive processes for theatrical motion pictures had been almost completely abandoned by the early 1940s, though additive color methods are employed by all the color video and computer display systems in common use today.\n\nThe first practical subtractive color process was introduced by Kodak as \"Kodachrome\", a name recycled twenty years later for a very different and far better-known product. Filter-photographed red and blue-green records were printed onto the front and back of one strip of black-and-white duplitized film. After development, the resulting silver images were bleached away and replaced with color dyes, red on one side and cyan on the other. The pairs of superimposed dye images reproduced a useful but limited range of color. Kodak's first narrative film with the process was a short subject entitled \"Concerning $1000\" (1916). Though their duplitized film provided the basis for several commercialized two-color printing processes, the image origination and color-toning methods constituting Kodak's own process were little-used.\n\nThe first truly successful subtractive color process was William van Doren Kelley's Prizma, an early color process that was first introduced at the American Museum of Natural History in New York City on 8 February 1917. Prizma began in 1916 as an additive system similar to Kinemacolor.\n\nHowever, after 1917, Kelley reinvented the process as a subtractive one with several years of short films and travelogues, such as \"Everywhere With Prizma\" (1919) and \"A Prizma Color Visit to Catalina\" (1919) before releasing features such as the documentary \"Bali the Unknown\" (1921), \"The Glorious Adventure\" (1922), and \"Venus of the South Seas\" (1924). A Prizma promotional short filmed for Del Monte Foods titled \"Sunshine Gatherers\" (1921) is available on DVD in Treasures 5 The West 1898–1938 by the National Film Preservation Foundation.\n\nThe invention of Prizma led to a series of similarly printed color processes. This bipack color system used two strips of film running through the camera, one recording red, and one recording blue-green light. With the black-and-white negatives being printed onto duplitized film, the color images were then toned red and blue, effectively creating a subtractive color print.\n\nLeon Forrest Douglass (1869–1940), a founder of Victor Records, developed a system he called Naturalcolor, and first showed a short test film made in the process on 15 May 1917 at his home in San Rafael, California. The only feature film known to have been made in this process, \"Cupid Angling\" (1918) — starring Ruth Roland and with cameo appearances by Mary Pickford and Douglas Fairbanks — was filmed in the Lake Lagunitas area of Marin County, California.\n\nAfter experimenting with additive systems (including a camera with two apertures, one with a red filter, one with a green filter) from 1915 to 1921, Dr. Herbert Kalmus, Dr. Daniel Comstock, and mechanic W. Burton Wescott developed a subtractive color system for Technicolor. The system used a beam splitter in a specially modified camera to send red and green light to adjacent frames of one strip of black-and-white film. From this negative, skip-printing was used to print each color's frames contiguously onto film stock with half the normal base thickness. The two prints were chemically toned to roughly complementary hues of red and green, then cemented together, back to back, into a single strip of film. The first film to use this process was \"The Toll of the Sea\" (1922) starring Anna May Wong. Perhaps the most ambitious film to use it was \"The Black Pirate\" (1926), starring and produced by Douglas Fairbanks.\n\nThe process was later refined through the incorporation of dye imbibition, which allowed for the transferring of dyes from both color matrices into a single print, avoiding several problems that had become evident with the cemented prints and allowing multiple prints to be created from a single pair of matrices.\n\nTechnicolor's system was very popular for a number of years, but it was a very expensive process: shooting cost three times that of black-and-white photography and printing costs were no cheaper. By 1932, color photography in general had nearly been abandoned by major studios, until Technicolor developed a new advancement to record all three primary colors. Utilizing a special dichroic beam splitter equipped with two 45-degree prisms in the form of a cube, light from the lens was deflected by the prisms and split into two paths to expose each one of three black-and-white negatives (one each to record the densities for red, green, and blue).\n\nThe three negatives were then printed to gelatin matrices, which also completely bleached the image, washing out the silver and leaving only the gelatin record of the image. A receiver print, consisting of a 50% density print of the black-and-white negative for the green record strip, and including the soundtrack, was struck and treated with dye mordants to aid in the imbibition process (this \"black\" layer was discontinued in the early 1940s). The matrices for each strip were coated with their complementary dye (yellow, cyan, or magenta), and then each successively brought into high-pressure contact with the receiver, which imbibed and held the dyes, which collectively rendered a wider spectrum of color than previous technologies. The first animation film with the three-color (also called three-strip) system was Walt Disney's \"Flowers and Trees\" (1932), the first short live-action film was \"La Cucaracha\" (1934), and the first feature was \"Becky Sharp\" (1935).\n\nGasparcolor, a single-strip 3-color system, was developed in 1933 by the Hungarian chemist Dr. Bela Gaspar.\n\nThe real push for color films and the nearly immediate changeover from black-and-white production to nearly all color film were pushed forward by the prevalence of television in the early 1950s. In 1947, only 12 percent of American films were made in color. By 1954, that number rose to over 50 percent. The rise in color films was also aided by the breakup of Technicolor's near monopoly on the medium.\n\nIn 1947, the United States Justice Department filed an antitrust suit against Technicolor for monopolization of color cinematography (even though rival processes such as Cinecolor and Trucolor were in general use). In 1950, a federal court ordered Technicolor to allot a number of its three-strip cameras for use by independent studios and filmmakers. Although this certainly affected Technicolor, its real undoing was the invention of Eastmancolor that same year.\n\nIn the field of motion pictures, the many-layered type of color film normally called an \"integral tripack\" in broader contexts has long been known by the less tongue-twisting term \"monopack\". For many years, Monopack (capitalized) was a proprietary product of Technicolor Corp, whereas monopack (not capitalized) generically referred to any of several single-strip color film products, certainly including various Eastman Kodak products. It appears that Technicolor made no attempt to register Monopack as a trademark with the US Patent and Trademark Office, although it certainly asserted that term as if it were a registered trademark, and it had the force of a legal agreement between it and Eastman Kodak to back up that assertion. It was a solely-sourced product, too, as Eastman Kodak was legally prevented from marketing any color motion picture film products wider than 16mm, 35mm specifically, until the expiration of the so-called \"Monopack Agreement\" in 1950. This, notwithstanding the facts that Technicolor never had the capability to manufacture sensitized motion picture films of any kind, nor single-strip color films based upon its so-called \"Troland Patent\" (which patent Technicolor maintained covered all monopack-type films in general, but monopack-type motion picture films in particular, and which Eastman Kodak elected not to contest as Technicolor was then one of its largest customers, if not its largest customer). After 1950, Eastman Kodak was free to make and market color films of any kind, particularly including monopack color motion picture films in 65/70mm, 35mm, 16mm and 8mm. The \"Monopack Agreement\" had no effect on color still films.\n\nMonopack color films are based on the subtractive color system, which filters colors from white light by using superimposed cyan, magenta and yellow dye images. Those images are created from records of the amounts of red, green and blue light present at each point of the image formed by the camera lens. A subtractive primary color (cyan, magenta, yellow) is what remains when one of the additive primary colors (red, green, blue) has been removed from the spectrum. Eastman Kodak's monopack color films incorporated three separate layers of differently color sensitive emulsion into one strip of film. Each layer recorded one of the additive primaries and was processed to produce a dye image in the complementary subtractive primary.\n\nKodachrome was the first commercially successful application of monopack multilayer film, introduced in 1935. For professional motion picture photography, Kodachrome Commercial, on a 35mm BH-perforated base, was available exclusively from Technicolor, as its so-called \"Technicolor Monopack\" product. Similarly, for sub-professional motion picture photography, Kodachrome Commercial, on a 16mm base, was available exclusively from Eastman Kodak. In both cases, Eastman Kodak was the sole manufacturer and the sole processor. In the 35mm case, Technicolor dye-transfer printing was a \"tie-in\" product. In the 16mm case, there were Eastman Kodak duplicating and printing stocks and associated chemistry, not the same as a \"tie-in\" product. In exceptional cases, Technicolor offered 16mm dye-transfer printing, but this necessitated the exceptionally wasteful process of printing on a 35mm base, only thereafter to be re-perforated and re-slit to 16mm, thereby discarding slightly more than one-half of the end product.\n\nA late modification to the \"Monopack Agreement\", the \"Imbibition Agreement\", finally allowed Technicolor to economically manufacture 16mm dye-transfer prints as so-called \"double-rank\" 35/32mm prints (two 16mm prints on a 35mm base that was originally perforated at the 16mm specification for both halves, and was later re-slit into two 16mm wide prints without the need for re-perforation). This modification also facilitated the early experiments by Eastman Kodak with its negative-positive monopack film, which eventually became Eastmancolor. Essentially, the \"Imbibition Agreement\" lifted a portion of the \"Monopack Agreement's\" restrictions on Technicolor (which prevented it from making motion picture products less than 35mm wide) and somewhat related restrictions on Eastman Kodak (which prevented it from experimenting and developing monopack products greater than 16mm wide).\n\nEastmancolor, introduced in 1950, was Kodak's first economical, single-strip 35 mm negative-positive process incorporated into one strip of film. This rendered Three-Strip color photography relatively obsolete, even though, for the first few years of Eastmancolor, Technicolor continued to offer Three-Strip origination combined with dye-transfer printing (150 titles produced in 1953, 100 titles produced in 1954 and 50 titles produced in 1955, the very last year for Three-Strip). The first commercial feature film to use Eastmancolor was the documentary \"Royal Journey\", released in December 1951. Hollywood studios waited until an improved version of Eastmancolor negative came out in 1952 before using it; \"This is Cinerama\" was an early film which employed three separate and interlocked strips of Eastmancolor negative. \"This is Cinerama\" was initially printed on Eastmancolor positive, but its significant success eventually resulted in it being reprinted by Technicolor, using dye-transfer.\n\nBy 1953, and especially with the introduction of anamorphic wide screen CinemaScope, Eastmancolor became a marketing imperative as CinemaScope was incompatible with Technicolor's Three-Strip camera and lenses. Indeed, Technicolor Corp became one of the best, if not the best, processor of Eastmancolor negative, especially for so-called \"wide gauge\" negatives (5-perf 65mm, 6-perf 35mm), yet it far preferred its own 35mm dye-transfer printing process for Eastmancolor-originated films with a print run that exceeded 500 prints, not withstanding the significant \"loss of register\" that occurred in such prints that were expanded by CinemaScope's 2X horizontal factor, and, to a lesser extent, with so-called \"flat wide screen\" (variously 1.66:1 or 1.85:1, but spherical and not anamorphic). This nearly fatal flaw was not corrected until 1955 and caused numerous features initially printed by Technicolor to be scrapped and reprinted by DeLuxe Labs. (These features are often billed as \"Color by Technicolor-DeLuxe\".) Indeed, some Eastmancolor-originated films billed as \"Color by Technicolor\" were never actually printed using the dye-transfer process, due in part to the throughput limitations of Technicolor's dye-transfer printing process, and competitor DeLuxe's superior throughput. Incredibly, DeLuxe once had a license to install a Technicolor-type dye-transfer printing line, but as the \"loss of register\" problems became apparent in Fox's CinemaScope features that were printed by Technicolor, after Fox had become an all-CinemaScope producer, Fox-owned DeLuxe Labs abandoned its plans for dye-transfer printing and became, and remained, an all-Eastmancolor shop, as Technicolor itself later became.\n\nTechnicolor continued to offer its proprietary imbibition dye-transfer printing process for projection prints until 1975, and even briefly revived it in 1998. As an archival format, Technicolor prints are one of the most stable color print processes yet created, and prints properly cared for are estimated to retain their color for centuries. With the introduction of Eastmancolor low-fade positive print (LPP) films, properly stored (at 45 °F or 7 °C and 25 percent relative humidity) monopack color film is expected to last, with no fading, a comparative amount of time. Improperly stored monopack color film from before 1983 can incur a 30 percent image loss in as little as 25 years.\n\nA color film is made up of many different layers that work together to create the color image. Color negative films provide three main color layers: the blue record, green record, and red record; each made up of two separate layers containing silver halide crystals and dye-couplers. A cross-sectional representation of a piece of developed color negative film is shown in the figure at the right. Each layer of the film is so thin that the composite of all layers, in addition to the triacetate base and antihalation backing, is less than 0.0003\" (8 µm) thick.\n\nThe three color records are stacked as shown at right, with a UV filter on top to keep the non-visible ultraviolet radiation from exposing the silver-halide crystals, which are naturally sensitive to UV light. Next are the fast and slow blue-sensitive layers, which, when developed, form the latent image. When the exposed silver-halide crystal is developed, it is coupled with a dye grain of its complementary color. This forms a dye \"cloud\" (like a drop of water on a paper towel) and is limited in its growth by development-inhibitor-releasing (DIR) couplers, which also serve to refine the sharpness of the processed image by limiting the size of the dye clouds. The dye clouds formed in the blue layer are actually yellow (the opposite or complementary color to blue). There are two layers to each color; a \"fast\" and a \"slow.\" The fast layer features larger grains that are more sensitive to light than the slow layer, which has finer grain and is less sensitive to light. Silver-halide crystals are naturally sensitive to blue light, so the blue layers are on the top of the film and they are followed immediately by a yellow filter, which stops any more blue light from passing through to the green and red layers and biasing those crystals with extra blue exposure. Next are the red-sensitive record (which forms cyan dyes when developed); and, at the bottom, the green-sensitive record, which forms magenta dyes when developed. Each color is separated by a gelatin layer that prevents silver development in one record from causing unwanted dye formation in another. On the back of the film base is an anti-halation layer that absorbs light which would otherwise be weakly reflected back through the film by that surface and create halos of light around bright features in the image. In color film, this backing is \"rem-jet\", a black-pigmented, non-gelatin layer which is removed in the developing process.\n\nEastman Kodak manufactures film in 54-inch (1,372 mm) wide rolls. These rolls are then slit into various sizes (70 mm, 65 mm, 35 mm, 16 mm) as needed.\n\nMotion picture film, primarily because of the rem-jet backing, requires different processing than standard C-41 process color film. The process necessary is ECN-2, which has an initial step using an alkaline bath to remove the backing layer. There are also minor differences in the remainder of the process. If motion picture negative is run through a standard C-41 color film developer bath, the rem-jet backing partially dissolves and destroys the integrity of the developer and, potentially, ruins the film.\n\nIn the late 1980s, Kodak introduced the T-Grain emulsion, a technological advancement in the shape and make-up of silver halide grains in their films. T-Grain is a tabular silver halide grain that allows for greater overall surface area, resulting in greater light sensitivity with a relatively small grain and a more uniform shape that results in a less overall graininess to the film. This made for sharper and more sensitive films. The T-Grain technology was first employed in Kodak's EXR line of motion picture color negative stocks. This was further refined in 1996 with the Vision line of emulsions, followed by Vision2 in the early 2000s and Vision3 in 2007.\n\nFuji films also integrate tabular grains in their SUFG (Super Unified Fine Grain) films. In their case, the SUFG grain is not only tabular, it is hexagonal and consistent in shape throughout the emulsion layers. Like the T-grain, it has a larger surface area in a smaller grain (about one-third the size of traditional grain) for the same light sensitivity. In 2005, Fuji unveiled their Eterna 500T stock, the first in a new line of advanced emulsions, with Super Nano-structure Σ Grain Technology.\n\n\n"}
{"id": "35933999", "url": "https://en.wikipedia.org/wiki?curid=35933999", "title": "Composite lumber", "text": "Composite lumber\n\nComposite lumber is a material that is a mixture of wood fiber, plastic, and some type of binding agent. These ingredients are put together to form a material that is denser, stronger, and heavier than wood alone, a wood-plastic composite.\n\nUntil the 1990s, wood was the material of choice for deck construction. However, new products, composites, began to emerge at this time. These new products offered the look and workability of wood, but they were more water resistant and required less maintenance. Over time, these lower maintenance decking options increased in popularity. Although the majority of decks are still built of pressure treated pine, redwood, cedar or mahogany, use of composite woods has increased as outdoor decks and living areas have become popular as home features.\n\nWorking with composite lumber is similar to working with wood. However, composite lumber has the added benefit of being less likely to split or delaminate. Some composite lumber is also engineered to be lighter weight for easier handling. Composite lumber is also more stain, scratch, and mold resistant, and is therefore supposed to have a longer life than wood lumber.\n\nComposite lumber comes from the manufacturer as a finished product. There is no need to stain, sand, or paint the material. Composite materials usually cost more than lumber, but their long life and low-maintenance requirements could make them more economical in the long run. Many composites are often made partially out of recycled plastics and waste wood, which makes them an environmentally friendly, efficient use of resources.\n\nComposite lumber is usually more costly than normal or treated lumber. Composites may last longer, but the initial investment is likely to be higher. Many composites are formulated to be fade, scratch, and stain resistant, but no lumber is immune to the elements. Although composite lumber may resist these marring effects better than other materials, it will still show signs of wear over time. Composite lumber often has a plastic-like or synthetic appearance. Although manufacturers do mold the product with a wood grain or brush stroke pattern, some consumers simply do not like the artificial sheen.\n\nThe growth of wood-alternative products continues to fuel a debate about their environmental value when compared to wood, treated wood, metal and other alternatives. Many suggest that wood decking is made from a more sustainable ingredient and that it carries a smaller manufacturing carbon footprint. Others have claimed that the sawing of wood-alternative products during construction creates dust that will not biodegrade and that the wood-alternative deck boards, when they have outlived their useful lives, will not biodegrade in landfills.\n\nCapped composite decking is composite decking, with a thin, approximately 1/16th inch, PVC-like veneer, or cap. This cap provides protection for the composite underneath. The cap is also formulated differently, in order to provide increased fade, stain, and scratch resistance. Capped composite decking is more expensive than both normal composite decking and wood decking because of the more involved manufacturing process in adding the second, co extruded layer to the board. Capped composites also lack the feel of real wood. Although manufacturers form the product with a realistic wood grain or brush stroke, some contractors and deck owners will not accept the artificial sheen. Capped composites, although formulated to resist fading, stains, and scratches, will show some wear over time, even if it is less than a normal composite or real wood product.\n\nComposite Deck Boards are sold in either grooved or solid sided versions. The grooved composite board is fastened with hidden deck fasteners or clips, while the solid board is typically face-screwed. Most Composite Deck Board manufacturers produce 12', 16', or 20' deck boards, 5-1/2\" wide, x 3/4\" high.\n\nManufacturers of capped composite boards will often leave one side uncapped to prevent the material from expanding and mushrooming out of the corners of the boards. This is usually the bottom side and allows the material within to expand and contract naturally during varying weather conditions, without causing lasting damage.\n\n\n"}
{"id": "2579709", "url": "https://en.wikipedia.org/wiki?curid=2579709", "title": "Content re-appropriation", "text": "Content re-appropriation\n\nFundamental to modern information architectures, and driven by semantic Web technologies, content re-appropriation is the act of searching, filtering, gathering, grouping, and aggregation which allows information to be related, classified and identified. This is achieved by applying syntactic or semantic meaning though intelligent tagging or artificial interpretation of fragmented content (see Resource Description Framework). Hence, all information becomes valuable and interpretable.\n\nSince the domain of Content applies to areas of software applications, documents, and media, these can be processed though a pipeline of generation, aggregation, transform-many, and serialization (see XML Pipeline). The output of this can viewed in a medium most effect for decision making.\n\nThe desired outcomes of content re-appropriation are:\n\n\nEssentially to make \"information\" disparities transparent to the user - getting to the bottom line … quickly.\n\nContent re-appropriation is effective across the Content-Tier, that is places where Content exists:\n\n\n"}
{"id": "3464288", "url": "https://en.wikipedia.org/wiki?curid=3464288", "title": "Defence Science and Technology Laboratory", "text": "Defence Science and Technology Laboratory\n\nThe Defence Science and Technology Laboratory (Dstl) is an executive agency of the Ministry of Defence of the United Kingdom. Its stated purpose is \"to maximise the impact of science and technology for the defence and security of the UK.\" The agency is headed by Gary Aitkenhead as its Chief Executive, with the board being chaired by Sir David Pepper.. Ministerial responsibility lies with the Minister for Defence Procurement.\n\nDstl was formed from the July 2001 split of the Defence Evaluation and Research Agency (DERA). Dstl was established to carry out and retain the science and technology work that is best done within government, while the majority of DERA's activities and that suitable for industry was transferred to Qinetiq, a wholly owned government company before being floated on the stock exchange. \n\nDstl absorbed the Home Office's Centre for Applied Science and Technology (CAST) in April 2018, taking on CAST's role to apply science and technology to support the Home Office's operations and frontline delivery, provision of evidence to support policy, and provide certain regulatory functions. \n\nDstl existed as a Trading Fund of the MOD from its formation in 2001 until 2016, when it became an Executive Agency of the MOD.\n\nDstl is an executive agency sponsored by the Ministry of Defence (MOD). Most funding comes from the MOD, while a small portion comes from other government departments and commercial sources. According to 2016/17 figures, 91% of Dstl's £587m income came from the MOD.\n\nIn 2015 Dstl completed a major reorganisation, merging twelve operating departments into five divisions on 1 April 2015. The motivation behind this change was to enable more coherent and productive delivery to customers and simplify access routes for suppliers.\n\nDstl has had four permanent Chief Executives:\n\nDstl carries out a broad range of work from high-level analysis to support Ministry of Defence policy and procurement decisions, to technical research in defence areas such as biomedical science and electronics, alongside operational work such as forensic analysis of explosives and providing paid volunteer scientists to Iraq and Afghanistan to provide rapid scientific advice to British forces. It has done work for around 40 government departments and agencies including the Home Office and Department for Transport. It undertakes research with both industry and academia to achieve its role.\n\nFollowing a review and consultation process initiated by MOD's Chief Scientific Advisor (CSA), it become responsible for the formulation and commission of MOD's non-nuclear research programme from 1 Apr 2010, under the responsibility of the Dstl Programme Office. Within the Programme Office were 16 domains with some established as Science and Technology Centres, including Armour and Protection, Cyber and Influence, Counter Terrorism, and CBR (Chemical, Biological and Radiological). These centres funded research via the Centre for Defence Enterprise, also part of the Programme Office. \n\nA subsequent MOD CSA-led review in 2015 into MOD's science and technology capability recommended that the commissioning of science and technology should be independent of the delivery. Following this, the commissioning role was moved to Defence Science and Technology (DST) within MOD Head Office, with Dstl focusing on delivery.\n\nWithin the Strategic Defence and Security Review 2015 was a proposal to create \"a government-backed service designed to help small and medium-sized businesses bring new ideas to market more quickly\". In 2016, it was announced by Defence Secretary Michael Fallon that this 'Defence and Security Accelerator' would have access to an £800m innovation fund and build on the 'Centre for Defence Enterprise' model, operating within Dstl.\n\nIn 2017, Dstl began a five-year programme of innovation in space science.\n\nCurrent sites include:\n\nThe functions of the two former CAST sites – Sandridge and Langhurst – will be transferred to Dstl's core sites of Porton Down and Portsdown West by 2020.\n\nSections of 150-millimetre-thick (5.9 in) pre-atomic steel plate uncontaminated with radionuclides, recovered from HMS \"Vanguard\", were used for the shielding of the whole body monitor at the Radiobiological Research Laboratory (now Dstl) at Alverstoke, Gosport, Hampshire.\n\nIn April 2005 the technology transfer company Ploughshare Innovations Ltd was formed to manage and exploit intellectual property within Dstl. Dstl and Ploughshare Innovations have successfully spun-out several new companies including Alaska Food Diagnostics and P2i Ltd.\n\n\n"}
{"id": "24225786", "url": "https://en.wikipedia.org/wiki?curid=24225786", "title": "Draw-works", "text": "Draw-works\n\nA draw-works is the primary hoisting machinery that is a component of a rotary drilling rig. Its main function is to provide a means of raising and lowering the traveling blocks. The wire-rope drilling line winds on the drawworks drum and extends to the crown block and traveling blocks, allowing the drill string to be moved up and down as the drum turns. The segment of drilling line from the draw-works to the crown block is called the \"fast line\". The drilling line then enters the sheaves of the crown block and it makes several passes between the crown block and traveling block pulleys for mechanical advantage. The line then exits the last sheave on the crown block and is fastened to a derrick leg on the other side of the rig floor. This section of drilling line is called the \"dead line\".\n\nA modern draw-works consists of five main parts: the drum, the motor(s), the reduction gear, the brake, and the auxiliary brake. The motors can be AC or DC-motors, or the draw-works may be connected directly to diesel engines using metal chain-like belts. The number of gears could be one, two or three speed combinations. The main brake, usually operated manually by a long handle, may be friction band brake, a disc brake or a modified clutch. It serves as a parking brake when no motion is desired. The auxiliary brake is connected to the drum, and absorbs the energy released as heavy loads are lowered. This brake may use eddy current rotors or water-turbine-like apparatus to convert the kinetic energy of the moving load to heat and dissipate it. \n\nPower catheads (winches) located on each side provide the means of actuating the tongs used to couple and uncouple threaded pipe members. Outboard catheads can be used manually with ropes for various small hoisting jobs around the rig.\n\nThe drawworks often has a pulley drive arrangement on the front side to provide turning power to the rotary table, although on many rigs the rotary table is independently powered. \n\nDrawworks can be used to hoist or lower several hundred thousand pounds of weight and can come in AC, DC or mechanical power units. Horsepower ratings for drawworks can also have a wide range, often ranging from 1000 HP to over 3000 HP.\n\n"}
{"id": "30201605", "url": "https://en.wikipedia.org/wiki?curid=30201605", "title": "E-commerce credit card payment system", "text": "E-commerce credit card payment system\n\nElectronic commerce, commonly known as e-commerce or eCommerce, or e-business consists of the buying and selling of products or services over electronic systems such as the Internet and other computer networks. The amount of trade conducted electronically has grown extraordinarily with widespread Internet usage. The use of commerce is conducted in this way, spurring and drawing on innovations in electronic funds transfer, supply chain management, Internet marketing, online transaction processing, electronic data interchange (EDI), inventory management systems, and automated data collection systems. Modern electronic commerce typically uses the World Wide Web at least at some point in the transaction's lifecycle, although it can encompass a wider range of technologies such as e-mail as well. \n\nA large percentage of electronic commerce is conducted entirely electronically for virtual items such as access to premium content on a website, but most electronic commerce involves the transportation of physical items in some way. Online retailers are sometimes known as e-tailers and online retail is sometimes known as e-tail. Almost all big retailers have electronic commerce presence on the World Wide Web. \n\nElectronic commerce that is conducted between businesses is referred to as business-to-business or B2B. B2B can be open to all interested parties (e.g. commodity market) or limited to specific, pre-qualified participants (private electronic market). Electronic commerce that is conducted between businesses and consumers, on the other hand, is referred to as business-to-consumer or B2C. This is the type of electronic commerce conducted by companies such as Amazon.com. Online shopping is a form of electronic commerce where the buyer is connected directly online to the seller's computer usually via the Internet. There is no specific intermediary service. The sale and purchase transaction is completed electronically and interactively in real-time, such as when buying a new book on Amazon.com. If an intermediary is present, then the sale and purchase transaction is called consumer-to-consumer, such as an online auction conducted on eBay.com. \n\nThis payment system has been widely accepted by consumers and merchants throughout the world, and is by far the most popular method of payments especially in the retail markets. Some of the most important advantages over the traditional modes of payment are: privacy, integrity, compatibility, good transaction efficiency, acceptability, convenience, mobility, low financial risk and anonymity.\n\nThis flow of ecommerce payment system can be better understood from the flow of the system below.\n\nFigure: Online Credit Card (VISA) Transaction Process\n\n"}
{"id": "14679287", "url": "https://en.wikipedia.org/wiki?curid=14679287", "title": "Expansion spring", "text": "Expansion spring\n\nExpansion springs are used as electrical connectors in some children's electronics kits. They are easy to use with bare fingers, they accept multiple wires, they require no learning or expertise to use them, and the cost is low.\n\nKits employing these connectors also have the secondary advantage that many additional projects can be made if the components are removed from the springs, and components strung between the springs instead of wires. For even more projects, these kits usually have enough space to add further springs.\n\nTheir major limitation is that spring connectors can not accept integrated circuit chips.\n\n"}
{"id": "1080186", "url": "https://en.wikipedia.org/wiki?curid=1080186", "title": "FLEX (protocol)", "text": "FLEX (protocol)\n\nFLEX is a communications protocol developed by Motorola and used in many pagers. FLEX provides one-way communication only (from the provider to the pager device), but a related protocol called ReFLEX provides two-way messaging.\n\nTransmission of message data occurs in one of four modes: 1600/2, 3200/2, 3200/4, or 6400/4. All modes use FSK modulation. At 1600/2 this is on a 2 level FSK signal transmitted at 1600 bits per second. At 3200/2, this is a 2 level FSK signal transmitted at 3200 bits per second. At 3200/4, this is a 4 level FSK signal transmitted at 1600 symbols per second. Each 4 level symbol represents two bits for a bit rate of 3200 bits per second. At 6400/4, this is a 4 level FSK signal transmitted at 3200 symbols per second or 6400 bits per second.\n\nData is transmitted in a set of 128 frames that takes 4 minutes to complete. Each frame contains a sync followed by 11 data blocks. The data blocks contain 256, 512 or 1024 bits for 1600, 3200 or 6400 bits per second respectively.\n\nA BCH type ECC is used to improve the integrity of the data. The standard has been designed to allow the pager's receiver to be turned off for a high percentage of the time and therefore save on battery usage.\n\nSince data transmission over FLEX is unencrypted and insecure, transmitting private information over it should be done with caution. There have been reported instances of individuals actively listening to pager traffic (private investigators, news organizations, etc.).\n\nIn The Netherlands the emergency services use the Flex-protocol in the nationwide P2000 network for pagers. The traffic on this network can be monitored via several websites.\n\nIn South Australia the State's SAGRN network for the Emergency Services paging system (CFS, SES, MFS and SAAS) is run on the FLEX 1600 protocol, and can be monitored through several websites.\n\n\n"}
{"id": "37507423", "url": "https://en.wikipedia.org/wiki?curid=37507423", "title": "Furrow profilometer", "text": "Furrow profilometer\n\nA furrow profilometer is used for the measurement of the cross-sectional geometry of furrows and corrugations, and is important in furrow assessments. For each furrow, the cross-sectional geometry should be measured at two to three locations before and after the irrigation. A profilometer for determining the cross-sections of furrows is shown in Figure. Individual scales located on the horizontal rod of the profilometer provide data to plot furrow depth as a function of the lateral distance and the data can be numerically integrated. This gives geometric relationships such as area verses depth, wetted perimeter versus depth and top-width verses depth.\n"}
{"id": "17123894", "url": "https://en.wikipedia.org/wiki?curid=17123894", "title": "Garbage Warrior", "text": "Garbage Warrior\n\nGarbage Warrior is a 2007 film about architect Mike Reynolds, inventor of the Earthship style of building, directed by Oliver Hodge. It follows Reynolds and how he developed the Earthship style of building and his struggle with the laws of Taos, New Mexico, the location of his experimental Earthship community, in order to be allowed to build homes that do not match the structures of local building codes. The film concludes with a postscript showing Reynolds and his team of builders travelling to the Andaman Islands in the aftermath of the Boxing Day tsunami to assist the locals with disaster recovery and teaching them how to construct extremely low-cost earthships.\n"}
{"id": "3271673", "url": "https://en.wikipedia.org/wiki?curid=3271673", "title": "Ghost in the Shell (manga)", "text": "Ghost in the Shell (manga)\n\nGhost in the Shell, known in Japan as , is a \"seinen\" manga series written and illustrated by Masamune Shirow, which spawned the media franchise of the same name. The manga, first serialized in 1989 under the subtitle of \"The Ghost in the Shell\", and later published as its own \"tankōbon\" volumes by Kodansha, told the story of the fictional counter-cyberterrorist organization Public Security Section 9, led by protagonist Major Motoko Kusanagi, in the mid 21st century of Japan. \"Ghost in the Shell 2: Man-Machine Interface\" was the sequel work which follows the story of Motoko after merging with the Puppeteer. The last volume, \"Ghost in the Shell 1.5: Human-Error Processor\", contains four separate cases.\n\nThe books contain Shirow's thoughts on design and philosophy, including sociological issues, the consequences of technological advances and themes on the nature of consciousness and identity. Several artbooks have been released to detail the concept art and the world of \"Ghost in the Shell\". All three volumes have received mainly positive reviews.\n\nPrimarily set in the mid-twenty-first century in the fictional Japanese city of , otherwise known as , the manga and the many anime adaptations follow the members of Public Security Section 9, a special-operations task-force made up of former military officers and police detectives. Political intrigue and counter-terrorism operations are standard fare for Section 9, but the various actions of corrupt officials, companies, and cyber-criminals in each scenario are unique and require the diverse skills of Section 9's staff to prevent a series of incidents from escalating.\n\nIn this cyberpunk iteration of a possible future, computer technology has advanced to the point that many members of the public possess cyberbrains, technology that allows them to interface their biological brain with various networks. The level of cyberization varies from simple minimal interfaces to almost complete replacement of the brain with cybernetic parts, in cases of severe trauma. This can also be combined with various levels of prostheses, with a fully prosthetic body enabling a person to become a cyborg. The heroine of \"Ghost in the Shell\", Major Motoko Kusanagi, is such a cyborg, having had a terrible accident befall her as a child that ultimately required that she use a full-body prosthesis to house her cyberbrain. This high level of cyberization, however, opens the brain up to attacks from highly skilled hackers, with the most dangerous being those who will hack a person to bend to their whims.\n\n begins in 2029, and features Section 9, led by Chief Daisuke Aramaki and Major Motoko Kusanagi, as they investigate the Puppeteer, a cyber-criminal wanted for committing a large number of crimes by proxy through \"ghost hacking\" humans with cyberbrains. As the investigation continues, Section 9 discovers that the Puppet Master is actually an advanced artificial intelligence created by a department of the Japanese government, taking up residence in a robot body. After destroying the latest host of the Puppeteer, Section 9 believes all is well, until the Major discovers the Puppet Master in her own mind. After hearing the Puppeteer's wishes to reach its next step in evolution, Kusanagi allows it to become one with her own ghost. After this event, the Major leaves Section 9 to work as a private contractor, with the remaining members of the unit, Batou, Togusa, Ishikawa, Saito, Paz, Borma and Azuma, continuing their work as covert operatives, occasionally meeting up with the Major in her various guises. These stories were later collected under the name . In 2035, the Major, now known as Motoko Aramaki, works as a security expert for Poseidon Industrial, now an entity composed of multiple identities that she controls via the network in other prosthetic bodies that attack industrial spies, assassins, and cyber-hackers, solving various crimes, while still at her day job. However, a psychic investigator finds something dangerous emerging as the teachings of a professor of artificial intelligence fall into the wrong hands and attempt to intermingle with the Major's current evolving sense of self. These stories are collected under the title .\n\nWhile writing the manga, Masamune Shirow struggled to make it neither too complex nor too simple. Two official names exist for the works, the first is and the second is \"\"Ghost in the Shell\". Masamune Shirow originally wanted to use the name \"Ghost in the Shell\"\" for the publication, as an homage to Arthur Koestler's \"The Ghost in the Machine\", from which he drew inspiration. Kōichi Yuri, First Coordinator at \"Young Magazine\", requested a \"more flashy\" name and Shirow came up with . Shirow requested that \"Ghost in the Shell\" be included on the title even if it was in small print. Yuri believes that \"Kōkaku Kidōtai\" is the mainstream title while \"Ghost in the Shell\" is the theme. While most Japanese publications use both names, the original publication in \"Young Magazine\" used \"Kōkaku Kidōtai\".\n\nWhen developing \"Ghost in the Shell 2: Man-Machine Interface\", Shirow initially wanted to use a new title by changing the last kanji character meaning , to the homophonic kanji for so that it would literally translate , but eventually he decided not to do so. The production of \"Ghost in the Shell 2: Man-Machine Interface\" manga was done digitally, which was difficult for Shirow because of troubles including a hard disk failure which resulted in the loss of 16 gigabytes of data, USB hardware troubles and reading manuals related to new application upgrades. Shirow considers the manga a completely different kind of work and not a true sequel of \"Ghost in the Shell\". The original manga revolved around Public Security Section 9 and \"Ghost in the Shell 2: Man-Machine Interface\" follows what happens to the Major after she merges with the Puppeteer. Shirow drew the color pages on computer, in which he states was difficult to due to technical issues with his computer. In the \"short-cut\" version of the manga, Masamune Shirow made the color darker and softer, but used more contrasting colors in the \"standard\" version.\n\nShirow's thoughts and work on \"Ghost in the Shell\" contain numerous footnotes and detailed explanations about scenes to give readers an understanding of the sociological differences or technological advances and philosophical discussion of the material. Examples include concepts like the future of hacking techniques, in which a cyberbrain can be hacked to copy information without being detected. Shirow explains instances of spirit channeling in cyborgs with kiko energy. Shirow even wrote that this phenomenon may be related to the \"hearing voices\" in individuals that suffer from mental disabilities like schizophrenia. This belief is represented in Motoko's reasons for head hunting Togusa for Section 9. Shirow also notes that he believes these channelers do not speak with a human-like god, but instead tap into a phase of the universe which synchronizes with the channeler's functions. Other philosophical stances are represented such as Shirow's personal beliefs regarding death sentences and crime and punishment.\n\nShirow explains numerous facets of the daily life throughout his notes in \"Ghost in the Shell\". Cyborgs are shown consuming food, but Shirow noted that early in the development would have been pills or paste substance that would have both psychological and physical functions. The Fuchikoma robots also must consume in a sense, requiring replenishment of fluid for their neurochips every two months, but Fuchikoma are not entirely bio-robots. Shirow discussed in his notes how the family of Yano received notification of his death and what would be disclosed, but also notes strategic use and premature notifications exist for various purposes. The advancement of technology in Shirow's vision of the future is rapid, but the advancements are at least partially related to then-current technology. The concepts of a 3-D viewing room was based on \"crude\" golf simulator technology.\n\nOther personal beliefs of Shirow are represented in the scenes and author's commentary, such metaphysics, religious references, and other philosophical stances that enter a range of topics including his thoughts on a rotating universe.\n\nThe removal of a two-page sex scene in Studio Proteus's localization of \"Ghost in the Shell\" was not well received, with readers reacting negatively to the removal of the previously uncensored content that was included in the original Dark Horse release. Toren Smith commented on Studio Proteus's actions claiming that requirement of the \"Mature Readers Only\" would translate into a 40% loss in sales and likely have caused the immediate cancellation of the series. Shirow, who grew tired of \"taking flak\" over the pages, opted to remove them and reworked the previous page as necessary.\n\nThe original \"Ghost in the Shell\" ran from April 1989 to November 1990 in Kodansha's manga anthology \"Young Magazine\", and was released in \"tankōbon\" format on October 2, 1991. Dark Horse initially published it in English monthly into eight comic issues from March 1, 1995, to October 1, 1995, with the translation of Studio Proteus.<ref name=\"GITS-1/8\"></ref><ref name=\"GITS-8/8\"></ref> It was later collected into a single volume in trade paperback format on early December 1, 1995. An uncensored version was later released by Dark Horse Comics on October 6, 2004. The censored version of the Dark Horse manga was later republished by Kodansha Comics on October 13, 2009.\n\nThe sequel \"Ghost in the Shell 2: Man-Machine Interface\" was penned by Shirow later. The manga series ran in \"Young Magazine\" from September 1991 to August 1997 and was originally released in hardcover format along with the original manga in a limited edition box set titled on December 1, 2000. The box set also contained a booklet titled \"ManMachine Interface Inactive Module\", a poster and a Fuchikoma robot action figure. Kodansha later released the standard edition in \"tankōbon\" format on June 26, 2001. The \"SOLID BOX\" version added over 140 pages of new content and more changes were added to the \"tankōbon\" version, such as 24 color pages and large modifications to over 20 pages. However, 200 pages from the original version that ran in \"Young Magazine\" were not included in either the \"SOLID BOX\" or the \"tankōbon\" version. The manga was then distributed in English by Dark Horse Comics into 11 comic issues from January 29, 2003, to December 31, 2003.<ref name=\"GITS2-1/11\"></ref><ref name=\"GITS2-11/11\"></ref> Masamune Shirow manually redrew the manga for the English version so that it could be read from left to right. It was later collected into a single volume in trade paperback format on January 12, 2005. The manga was later republished by Kodansha Comics on August 10, 2010.\n\nFour chapters that were not released in \"tankōbon\" format from previous releases were later collected into a single volume titled \"Ghost in the Shell 1.5: Human-Error Processor\". The manga was published in July 23, 2003, by Kodansha. It contained a booklet and a CD-ROM featuring the full stories, adding music to the manga scenes, and a screen saver. Dark Horse Comics announced an English version at the 2005 San Diego Comic-Con. The series was released as eight individual comic issues from November 1, 2006, to June 6, 2007, and was the first of the \"Ghost in the Shell\" manga released in the United States to read right-to-left.<ref name=\"GITS1.5-1/8\"></ref><ref name=\"GITS1.5-8/8\"></ref> The four original titles were each split into two each, to make up the 8 in this series. It was later collected in a single volume in trade paperback format on October 10, 2007. The manga was later republished by Kodansha Comics on September 25, 2012.\n\nA number of artbooks detailing the concept art and world of \"Ghost in the Shell\" have been released. A box set titled was released on July 8, 1997. The box set contains a collection of posters illustrated by Masamune Shirow, a booklet and a puzzle. A guidebook titled was published by Kodansha and released on January 16, 1998. An art book titled was released by Kodansha on July 24, 2000. The book contains several different artwork and paper cut out figures of the Fuchikoma.\n\nThe \"Ghost in the Shell\" video game was developed by Exact and released for the PlayStation on July 17, 1997, in Japan by Sony Computer Entertainment. It is a third-person shooter featuring an original storyline where the character plays a rookie member of Section 9. The video game's soundtrack \"Megatech Body\" features various electronica artists.\n\nAnimation studio Production I.G has produced several different anime adaptations of \"Ghost in the Shell\", starting with the 1995 film of the same name, telling the story of Section 9's investigation of the Puppet Master. The film was followed by a sequel titled \"\", released in 2004. Meanwhile, a television series release began in 2002 under the title \"\", telling an alternate story from the manga and first film, featuring Section 9's investigations of government corruption in the Laughing Man and Individual Eleven incidents. The series ran for two seasons of 26 episodes each, with the second season titled \"Ghost in the Shell: S.A.C. 2nd GIG\". In 2006 a sequel film to the S.A.C. series was produced as \"\". 2013 saw the start of the \"\" OVA series with a plot set before the events of the original manga and consisting of four parts released through mid-2014. The series was recompiled in early 2015 as a television series titled \"\", airing with an additional two episodes (one part). An animated feature film produced by most of the \"Arise\" staff, titled \"\", was released on June 20, 2015.\n\nA live-action Hollywood adaptation starring Scarlett Johansson as The Major was released in the US on March 31, 2017.\n\n\"Ghost in the Shell\" had received mainly positive reviews. \"Publishers Weekly\" praised the manga for its artwork: \"Masamune's b&w drawings are dynamic and beautifully gestural; he vividly renders the awesome urban landscape of a futuristic, supertechnological Japan.\" Leroy Douresseaux of the website ComicBookBin gave the manga an A stating: \"It is visually potent and often inscrutable, but its sense of wonder and exploration makes its ideas still seem fresh two decades after its debut.\" Peter Gutiérrez of the website Teenreads praised the manga, writing: \"In short, \"Ghost in the Shell\" is hard sci-fi of the best possible sort: the type that’s so full of both undiluted artfulness and philosophy that it’s arguably a must-read even for those who don’t usually take to the genre.\" Greg McElhatton of Read About Comics praised the artwork, however criticized the manga for its story pacing and collection of short adventures stating, \"I'm glad I got to experience Shirow's artistic view of the future and am a little interested in the idea of his \"Intron Depot\" art books, but on the whole \"Ghost in the Shell\" was a massive shell game: flashy and fascinating from a glance, but ultimately empty when you decide to dive in.\"\n\n\"Ghost in the Shell 2: Man-Machine Interface\" had sold over 100,000 copies from its initial printing in Japan. Diamond Comic Distributors ranked the manga #7 in its Top Performing Manga list of 2005. Mike Crandol of Anime News Network criticized the manga for being too complex and overwhelming stating it is \"too technical for its own good\" but praised the new artwork, stating that Shirow's \"canny drawing skills are supplemented by an innovative use of CGI graphics that represent the series' boldest artistic endeavor.\" \"Publishers Weekly\" praised the artwork as \"the color and b&w graphics are stunning, brilliantly evoking the nonvisual world of data transmission\" but stated that the story can be confusing.\n\n\"Ghost in the Shell 1.5: Human-Error Processor\" was ranked #10 in \"The New York Times\" Manga Best Seller List on October 19, 2012. Scott Green of Ain't It Cool News praised the manga for its footnotes that \"alone are worth the price of admission. The degree to which he apparently takes every aspect seriously and the amount of information he'd like to convey verges on a disorder.\"\n\n"}
{"id": "55633178", "url": "https://en.wikipedia.org/wiki?curid=55633178", "title": "Google Pay", "text": "Google Pay\n\nGoogle Pay stylized G Pay (formerly Pay with Google and Android Pay) is a digital wallet platform and online payment system developed by Google to power in-app and tap-to-pay purchases on mobile devices, enabling users to make payments with Android phones, tablets or watches. \n\nAs of January 8, 2018, the old Android Pay and Google Wallet have unified into a single pay system called Google Pay. Android Pay was rebranded and renamed as Google Pay. It also took over the branding of Google Chrome's autofill feature. Google Pay adopts the features of both Android Pay and Google Wallet through its in-store, peer-to-peer, and online payments services.\n\nThe rebranded service provided a new API that allows merchants to add the payment service to websites, apps, Stripe, Braintree, and Google Assistant. The service allows users to use the payment cards they have on file with Google Play.\n\nThe Google Pay app also added support for boarding passes and event tickets in May 2018.\n\nGoogle Pay uses near field communication (NFC) to transmit card information facilitating funds transfer to the retailer. It replaces the credit or debit card chip and PIN or magnetic stripe transaction at point-of-sale terminals by allowing the user to upload these in the Google Pay wallet. It is similar to contactless payments already used in many countries, with the addition of two-factor authentication. The service lets Android devices wirelessly communicate with point of sale systems using a near field communication (NFC) antenna, host-based card emulation (HCE), and Android's security.\n\nGoogle Pay takes advantage of physical authentications such as fingerprint ID where available. On devices without fingerprint ID, Google Pay is activated with a passcode. When the user makes a payment to a merchant, Google Pay does not send the credit or debit card number with the payment. Instead it generates a virtual account number representing the user's account information. This service keeps customer payment information private, sending a one-time security code instead of the card or user details.\n\nGoogle Pay requires that a screen lock be set on the phone. It has no card limit.\n\nUsers can add payment cards to the service by taking a photo of the card, or by entering the card information manually. To pay at points of sale, users hold their authenticated device to the point of sale system. The service has smart-authentication, allowing the system to detect when the device is considered secure (for instance if unlocked in the last five minutes) and challenge if necessary for unlock information. Spring CEO Alan Tisch said Google Pay improves mobile shopping business by supporting a \"buy button\" powered by Google Pay integrated within vendor's creative design.\n\nOriginally launched as Android Pay, the service was released at Google I/O 2015. Android Pay was a successor to and built on the base established by Google Wallet which was released in 2011. It also used technology from the carrier-backed Softcard—Google had acquired its intellectual property in February 2015. At launch, the service was compatible with 70% of Android devices, and was accepted at over 700,000 merchants. Google Wallet still powered web-based Play Store purchases and some app-based peer-to-peer payments, for instance in Gmail.\n\n, it is currently available in the United States, Canada, Brazil, UK, Ireland, Spain, Belgium, Poland, Czech Republic, Ukraine, Russia, Singapore, Hong Kong, Taiwan, Japan, Australia and New Zealand. Upon its UK launch Android Pay supported MasterCard and Visa credit and debit cards from many of the UK’s major financial institutions — including Bank of Scotland, First Direct, Halifax, HSBC, Lloyds Bank, M&S Bank, MBNA and Nationwide Building Society — \"with new banks being added all the time\" according to Google. Natwest, RBS and Ulster Bank will launch on September 14. On September 8, 2016 it was reported that UK banks TSB and Santander will participate \"over the coming weeks\". Android Pay was launched in Singapore on June 28, 2016, and in Australia on July 14, 2016. Android Pay launched in the Republic of Ireland on December 7, 2016 and is initially available to customers of AIB and KBC. The service works with both credit and debit cards.\n\nIn 2016, Google began a public trial in Silicon Valley of a related mobile app called Hands Free. In this system, the customer does not need to present a phone or card. Instead, a customer announces they wish to \"pay with Google\" and give their initials to the cashier, who verifies their identity with a photo previously uploaded to the system. The customer's phone will only authorize payment if its geographic location system indicates it is near a participating store.\n\nOn September 18, 2017, Google launched an UPI-based app Tez in India. On August 28, 2018, Google rebranded Tez to Google Pay.\n\nOn January 8, 2018, Google announced that Google Wallet would be merged into Android Pay, with the service as a whole rebranded as Google Pay. This merger extends the platform into web-based payments integrated into other Google and third-party services. The rebranding began to roll out as an update to the Android Pay app on February 20, 2018; the app was given an updated design, and now displays a personalized list of nearby stores which support Google Pay.\n\nThose on light yellow background: originally released as Android Pay.\n\n\n\n"}
{"id": "7405398", "url": "https://en.wikipedia.org/wiki?curid=7405398", "title": "Grid (spatial index)", "text": "Grid (spatial index)\n\nIn the context of a spatial index, a grid or mesh is a regular tessellation of a manifold or 2-D surface that divides it into a series of contiguous cells, which can then be assigned unique identifiers and used for spatial indexing purposes. A wide variety of such grids have been proposed or are currently in use, including grids based on \"square\" or \"rectangular\" cells, triangular grids or meshes, hexagonal grids and grids based on diamond-shaped cells.also \"global grid\" if it covers the entire surface of the globe)\n\nSquare or rectangular grids are frequently used for purposes such as translating spatial information expressed in Cartesian coordinates (latitude and longitude) into and out of the grid system. Such grids may or may not be aligned with the grid lines of latitude and longitude; for example, Marsden Squares, World Meteorological Organization squares, c-squares and others are aligned, while Universal Transverse Mercator coordinate system and various national grid based systems such as the British national grid reference system are not. In general, these grids fall into two classes, those that are \"\"equal angle\", that have cell sizes that are constant in degrees of latitude and longitude but are unequal in area (particularly with varying latitude), or those that are \"equal area\"\", that have cell sizes that are constant in distance on the ground (e.g. 100 km, 10 km) but not in degrees of longitude, in particular.\n\nA commonly used triangular grid is the \"Quaternary Triangular Mesh\" (QTM), which was developed by Geoffrey Dutton in the early 1980s. It eventually resulted in a thesis entitled \"A Hierarchical Coordinate System for Geoprocessing and Cartography\" that was published in 1999. This grid was also employed as the basis of the rotatable globe that forms part of the Microsoft Encarta product.\n\nHexagonal grids may also be used. In general, triangular and hexagonal grids are constructed so as to better approach the goals of equal-area (or nearly so) plus more seamless coverage across the poles, which tends to be a problem area for square or rectangular grids since in these cases, the cell width diminishes to nothing at the pole and those cells adjacent to the pole then become 3- rather than 4-sided. Criteria for optimal discrete global gridding have been proposed by both Goodchild and Kimerling in which equal area cells are deemed of prime importance.\n\nQuadtrees are a specialised form of grid in which the resolution of the grid is varied according to the nature and complexity of the data to be fitted, across the 2-d space. Polar grids utilize the polar coordinate system, using circles of a prescribed radius that are divided into sectors of a certain angle. Coordinates are given as the radius and angle from the center of the grid.\n\nIn practice, construction of grid-based spatial indices entails allocation of relevant objects to their position or positions in the grid, then creating an index of object identifiers vs. grid cell identifiers for rapid access. This is an example of a \"space-driven\" or data independent method, as opposed to \"data-driven\" or data dependent method, as discussed further in Rigaux et al. (2002)). A grid-based spatial index has the advantage that the structure of the index can be created first, and data added on an ongoing basis without requiring any change to the index structure; indeed, if a common grid is used by disparate data collecting and indexing activities, such indices can easily be merged from a variety of sources. On the other hand, data driven structures such as R-trees can be more efficient for data storage and speed at search execution time, though they are generally tied to the internal structure of a given data storage system.\n\nThe use of such spatial indices is not limited to digital data; the \"index\" section of any global or street atlas commonly contains a list of named features (towns, streets, etc.) with associated grid square identifiers, and may be considered a perfectly acceptable example of a spatial index (in this case, typically organised by feature name, though the reverse is conceptually also possible).\n\nThe individual cells of a grid system can also be useful as units of aggregation, for example as a precursor to data analysis, presentation, mapping, etc. For some applications (e.g., statistical analysis), equal-area cells may be preferred, although for others this may not be a prime consideration.\n\nIn computer science, one often needs to find out all cells a ray is passing through in a grid (for raytracing or collision detection); this is called \"grid traversal\".\n\n\n\n"}
{"id": "30559230", "url": "https://en.wikipedia.org/wiki?curid=30559230", "title": "Harry Clarke – Darkness in Light", "text": "Harry Clarke – Darkness in Light\n\nHarry ClarkeDarkness in Light is a documentary film originally released in 2003 (Irish-language version titled Harry Clarke - Dorchadas i Solas).\n\nFilmmaker John J Doherty traces the life and work of the Irish artist, book illustrator and stained glass artist Harry Clarke (1889–1931) with major contributions from his biographer Nicola Gordon Bowe as well as many stained glass artists, poets and historians. The film takes the artist's work in stained glass, which was mainly religious an ethereal, and in book illustration, which was mainly dark & fantastical, as the basis for its title and tells a story of talent, struggle, success and the censorship of his final masterpiece 'the Geneva Window'. Harry Clarke brought his expertise in working in fine decorative detail in glass to his book illustrations, most notably in the tales of Hans Christian Andersen and Edgar Allan Poe where he is compared to Aubrey Beardsley and which are featured in the film and paralleled with German Expressionist cinema of the time. The film was made in conjunction with the Irish Film Board and national broadcaster TG4.\n\nThis film won the Best Arts Documentary award at the 2004 Celtic International Film Festival.\nThis film won the Best Documentary award at the 2005 Worldwide International Fantasy Film Festival - Toronto\n\n\n"}
{"id": "637453", "url": "https://en.wikipedia.org/wiki?curid=637453", "title": "Hedgehog (weapon)", "text": "Hedgehog (weapon)\n\nThe Hedgehog (also known as an \"Anti-Submarine Projector\") was a forward-throwing anti-submarine weapon that was used during the Battle of the Atlantic in the Second World War. The device, which was developed by the Royal Navy, fired up to 24 spigot mortars ahead of a ship when attacking a U-boat. It was deployed on convoy escort warships such as destroyers and corvettes to supplement the depth charges.\n\nAs the mortar projectiles employed contact fuzes rather than time or barometric (depth) fuzes, detonation occurred directly against a hard surface such as the hull of a submarine making it more deadly than depth charges, which relied on damage caused by hydrostatic shockwaves. Statistics show that during WWII out of 5,174 British depth charge attacks there were 85.5 kills: a ratio of 60.5 to 1. In comparison, the Hedgehog made 268 attacks for 47 kills: a ratio of 5.7 to 1.\n\nThe \"Hedgehog\", so named because the empty rows of its launcher spigots resembled the spines of a hedgehog, was a replacement for the unsuccessful Fairlie Mortar that was trialled aboard in 1941. Although a failure, the Fairlie was designed to fire depth charges ahead of a ship when attacking a submarine. This principle of forward-firing projectiles was considered viable. This secret research by the Directorate of Miscellaneous Weapons Development (DMWD) led to the development of the Hedgehog.\n\nThe weapon was a multiple 'spigot mortar' or spigot discharger, a type of weapon developed between the wars by Lieutenant Colonel Stewart Blacker, RA. The spigot mortar was based on early infantry trench mortars. The spigot design allowed a single device to fire warheads of varying size. The propelling charge was part of the main weapon and worked against a rod (the spigot) set in the baseplate which fitted inside a tubular tail of the 'bomb'. This principle was first used on the Blacker Bombard 29 mm Spigot Mortar and the later PIAT anti-tank weapon.\n\nThe adaptation of the bombard for naval use was made in partnership with MIR(c) under Major Millis Jefferis who had taken Blacker's design and brought it into use with Army. The weapon fires a salvo of 24 bombs in an arc, aimed to land in a circular or elliptical area about in diameter at a fixed point about directly ahead of the attacking ship. The mounting initially was fixed but was later replaced by a gyro-stabilised one to allow for the rolling and pitching of the attacking ship.\n\nThe system was developed to solve the problem of the target submarine disappearing from the attacking ship's ASDIC when the ship came within the sonar's minimum range. Due to the speed of sound in water, the time taken for the 'ping' echo to return to the attacking ship from the target submarine became too short to allow the human operator to distinguish the returning audible echo from that of the initial sound pulse emitted by the sonar – the so-called \"instantaneous echo\", where the output sound pulse and returning echo merge. This \"blind spot\" allowed the submarine to make evasive manoeuvres undetected while the ship was out of range for depth charge attack. Hence, the submarine was effectively invisible to the sonar as the ship came within the sonar's minimum range. The solution was a weapon mounted on the foredeck that discharged the projectiles up and over that carrying ship's bow, to land in the water some distance in front of the ship while the submarine was still outside the sonar's minimum range.\n\nThe Hedgehog entered service in 1942. Carrying a Torpex charge weighing , each mortar had a diameter of and weighed about . The projectiles were angled so they would land in a circular shape with a diameter of about ahead of a stationary ship. The projectiles would then sink at about . They would reach a submerged U-boat, for example at in under 9 seconds. Sympathetic detonation of projectiles near those contacting hard surfaces was a possibility, but the number of explosions counted was usually fewer than the number of projectiles launched.\n\nThe prototype launcher was tested aboard in 1941, but there were no submarine kills until November 1942, after it had been installed aboard one hundred ships. Initial success rates – of about 5% – were only slightly better than depth charges. Swells and spray frequently covered the launcher during heavy North Atlantic weather, and subsequent attempts to launch often revealed firing circuit problems launching an incomplete pattern. The disappointment of a quiet miss discouraged crews who might otherwise assume depth charge explosions had damaged their target or at least frightened the enemy. The Royal Navy launched Hedgehog so seldom in early 1943 that a directive was issued ordering captains of ships equipped with Hedgehog to report why they had \"not\" used Hedgehog on an underwater contact. The results were blamed on crew inexperience and low confidence in the weapon. However, after an officer from the DMWD was sent to Londonderry Port, where the convoy crews were based, with better training and shipwide talks on examples of successful Hedgehog attacks, the kill rate improved considerably. By the end of the war, statistics showed that on average, one in every five attacks made by Hedgehog resulted in a kill (compared to less than one in 80 with depth charges).\n\nIn response to this new deadly threat to its U-boats, the \"Kriegsmarine\" brought forward its programme of acoustic torpedoes in 1943, beginning with the Falke. These new \"homing\" torpedoes could be employed effectively without the use of a periscope, providing submarines a better chance to remain undetected and evade counterattack.\n\nIn the Pacific Theater, sank six Japanese submarines in a matter of days with Hedgehog in May 1944.\n\nIn 1946, was destroyed after a crewmen accidentally dropped a Hedgehog charge near one of her main turret ammunition rooms, triggering three subsequent and devastating explosions.\n\nThe launcher had four \"cradles\", each with six launcher spigots. The firing sequence was staggered so all the bombs would land at about the same time. This had the added advantage of minimising the stress on the weapon's mounting, so that deck reinforcement was not needed, and the weapon could easily be retrofitted to any convenient place on a ship. Reloading took about three minutes.\n\nThe Hedgehog had four key advantages over the depth charge:\n\n\nIn late 1943 the Royal Navy introduced Squid. This was a three-tubed mortar that launched depth charges. Initially it was used as a single weapon, but when this failed to be successful, it was upgraded to the \"double squid\" that consisted of two launchers placed in parallel. In 1955 this system was upgraded to the three-barreled Limbo that launched Minol charges.\n\nThe United States produced a rocket version of Hedgehog called Mousetrap, then Weapon Alpha as a replacement for both. Still, Hedgehog remained in service with the United States Navy into the Cold War until both Hedgehog and the less satisfactory Weapon Alpha were replaced by ASROC.\n\nThree \"Hedgerow\" flotillas of specialized Landing Craft Assault boats carrying the Hedgehog instead of troops were used during the Normandy landings. An addition of impact fuse extensions in the projectile noses enabled detonating the warheads above ground. The bombs were used to clear 100-yard-wide paths through mines and barbed wire obstacles on the beach.\n\nThe Australian Army adapted the marine Hedgehog into a land-based seven-shot launcher that could be mounted on the back of Matilda tanks.\n\nFrom 1949, a copy of Hedgehog was produced in the USSR as MBU-200, developed in 1956 into MBU-600 (also known as RBU-6000) with increased range of .\n\nWeapons derived from the Hedgehog have been largely phased out from Western navies in favor of homing torpedoes. MBU-600 and its derivatives remain an important part of the Russian Navy's (as well as Russia's allies, such as India) anti-submarine arsenal to this day.\n\n\n\"For a single bomb\"\n\n\n"}
{"id": "1521015", "url": "https://en.wikipedia.org/wiki?curid=1521015", "title": "HomeRF", "text": "HomeRF\n\nHomeRF was a wireless networking specification for home devices. It was developed in 1998 by the Home Radio Frequency Working Group, a consortium of mobile wireless companies that included Proxim Wireless, Intel, Siemens AG, Motorola, Philips and more than 100 other companies.\n\nThe group was disbanded in January 2003 after other wireless networks became accessible to home users and Microsoft began including support for them in its Windows operating systems. As a result, HomeRF fell into obsolescence.\n\nInitially called Shared Wireless Access Protocol (SWAP) and later just HomeRF, this open specification allowed PCs, peripherals, cordless phones and other consumer devices to share and communicate voice and data in and around the home without the complication and expense of running new wires. HomeRF combined several wireless technologies in the 2.4 GHz ISM band, including IEEE 802.11 FH (the frequency-hopping version of wireless data networking) and DECT (the most prevalent digital cordless telephony standard in the world) to meet the unique home networking requirements for security, quality of service (QoS) and interference immunity—issues that still plagued Wi-Fi (802.11b and g).\n\nHomeRF used frequency hopping spread spectrum (FHSS) in the 2.4 GHz frequency band and in theory could achieve a maximum of 10 Mbit/s throughput; its nodes could travel within a 50-meter range of a wireless access point while remaining connected to the personal area network (PAN). Several standards and working groups focused on wireless networking technology in radio frequency (RF). Other standards include the popular IEEE 802.11 family, IEEE 802.16, and Bluetooth.\n\nProxim Wireless was the only supplier of HomeRF chipsets, and since Proxim also made end products, other manufacturers complained that they had to buy components from their competitor. The fact that our group didn't address that conflict led to the eventual downfall of HomeRF, which occurred during an economic recession when companies already struggled to justify duplicate engineering and marketing efforts - for HomeRF, 802.11 and Bluetooth. The fact that HomeRF was developed by a consortium and not an official standards body also put it at a disadvantage against Wi-Fi and its IEEE 802.11 standard.\n\nAT&T joined the group because HomeRF was designed for high-speed broadband services and the need to support PCs, phones, stereos and televisions; but last-mile deployment occurred more slowly than expected and with slower speeds. So it was natural that the home networking market focused more on multi-PC households sharing Internet connections for email and browsing than on integrating phone and entertainment services into a broadband service bundle. As a result, the original promoter companies gradually started pulling out of the group rather than supporting multiple standards. They included IBM, Hewlett-Packard, Compaq, Microsoft, and lastly Intel. That left only companies like Motorola, National Semiconductor, Proxim, and Siemens. Even Proxim started pulling away when negative media surrounding HomeRF started affecting its core data networking business, and that left Siemens to do the work of integrating voice, data and video. Siemens was willing to go it alone with HomeRF technology but was concerned by growing uncertainties in the cordless phone market, including mobile phone as home phone, VoIP over Wi-Fi, and 5 GHz vs. 2.4 GHz. When Siemens eventually got out of the cordless phone market, it was the final nail in the HomeRF coffin.\n\nHomeRF received some success because of its low cost and ease of installation.\nBy September 2000, some confusion came from the \"home\" in the name, leading some to associate HomeRF with home networks, using other technologies such as IEEE 802.11b for businesses.\nA digital media receiver for audio was marketed under the name \"Motorola SimpleFi\" that used HomeRF.\nIn March 2001, Intel announced they would not support further development of HomeRF technology for its Anypoint line.\nThe group promoting 802.11 technology, the Wireless Ethernet Compatibility Alliance (WECA) changed their name to the Wi-Fi Alliance in 2002, as the Wi-Fi brand became popular.\n\nThe fact that WECA members lobbied the FCC for two years, which was effective in delaying the approval of wideband frequency-hopping, helped 802.11b catch up and gain an insurmountable lead in the market, which was then extended with 802.11g. The use of OFDM in 802.11a and .11g solved many of the RF interference problems of .11b. WPA and 802.11x also improved security over WEP encryption, which was especially important in the corporate world.\n\nBy January 2003 the Home Radio Frequency Working Group had disbanded.\nArchives of the HomeRF Working Group are maintained by Palo Wireless and Wayne Caswell.\n\n\n"}
{"id": "25850552", "url": "https://en.wikipedia.org/wiki?curid=25850552", "title": "ISS ECLSS", "text": "ISS ECLSS\n\nThe International Space Station Environmental Control and Life Support System (ECLSS) is a life support system that provides or controls atmospheric pressure, fire detection and suppression, oxygen levels, waste management and water supply. The highest priority for the ECLSS is the ISS atmosphere, but the system also collects, processes, and stores waste and water produced and used by the crew—a process that recycles fluid from the sink, shower, toilet, and condensation from the air. The Elektron system aboard \"Zvezda\" and a similar system in \"Destiny\" generate oxygen aboard the station.\nThe crew has a backup option in the form of bottled oxygen and Solid Fuel Oxygen Generation (SFOG) canisters.\nCarbon dioxide is removed from the air by the Russian \"Vozdukh\" system in \"Zvezda,\" one Carbon Dioxide Removal Assembly (CDRA) located in the U.S. Lab module, and one CDRA in the U.S. Node 3 module. Other by-products of human metabolism, such as methane from the intestines and ammonia from sweat, are removed by activated charcoal filters or by the Trace Contaminant Control System (TCCS).\n\nThe ISS has two water recovery systems. \"Zvezda\" contains a water recovery system that processes water vapor from the atmosphere that could be used for drinking in an emergency but is normally fed to the Elektron system to produce oxygen. The American segment has a Water Recovery System installed during STS-126 that can process water vapour collected from the atmosphere and urine into water that is intended for drinking. The Water Recovery System was installed initially in \"Destiny\" on a temporary basis in November 2008 and moved into \"Tranquility\" (Node 3) in February 2010.\nThe Water Recovery System consists of a Urine Processor Assembly and a Water Processor Assembly, housed in two of the three ECLSS racks.\n\nThe Urine Processor Assembly uses a low pressure vacuum distillation process that uses a centrifuge to compensate for the lack of gravity and thus aid in separating liquids and gasses. The Urine Processor Assembly is designed to handle a load of 9 kg/day, corresponding to the needs of a 6-person crew. Although the design called for recovery of 85% of the water content, subsequent experience with calcium sulfate precipitation (in the free-fall conditions present on the ISS, calcium levels in urine are elevated due to bone density loss) has led to a revised operational level of recovering 70% of the water content.\n\nWater from the Urine Processor Assembly and from waste water sources are combined to feed the Water Processor Assembly that filters out gasses and solid materials before passing through filter beds and then a high-temperature catalytic reactor assembly. The water is then tested by onboard sensors and unacceptable water is cycled back through the water processor assembly.\n\nThe Volatile Removal Assembly flew on STS-89 in January 1998 to demonstrate the Water Processor Assembly's catalytic reactor in microgravity. A Vapour Compression Distillation Flight Experiment flew, but was destroyed, in STS-107.\n\nThe distillation assembly of the Urine Processor Assembly failed on November 21, 2008, one day after the initial installation. One of the three centrifuge speed sensors was reporting anomalous speeds, and high centrifuge motor current was observed. This was corrected by re-mounting the distillation assembly without several rubber vibration isolators. The distillation assembly failed again on December 28, 2008 due to high motor current and was replaced on March 20, 2009. Ultimately, during post-failure testing, one centrifuge speed sensor was found to be out of alignment and a compressor bearing had failed.\n\nSeveral systems are currently used on board the ISS to maintain the spacecraft's atmosphere, which is similar to the Earth's. Normal air pressure on the ISS is 101.3 kPa (14.7 psi); the same as at sea level on Earth. \"While members of the ISS crew could stay healthy even with the pressure at a lower level, the equipment on the Station is very sensitive to pressure. If the pressure were to drop too far, it could cause problems with the Station equipment.\".\n\nCarbon dioxide and trace contaminants are removed by the Air Revitalization System. This is a NASA rack, placed in \"Tranquility\", designed to provide a Carbon Dioxide Removal Assembly (CDRA), a Trace Contaminant Control Subassembly (TCCS) to remove hazardous trace contamination from the atmosphere and a Major Constituent Analyser (MCA) to monitor nitrogen, oxygen, carbon dioxide, methane, hydrogen, and water vapour. The Air Revitalization System was flown to the station aboard STS-128 and was temporarily installed in the Japanese Experiment Module pressurised module. The system was scheduled to be transferred to \"Tranquility\" after it arrived and was installed during Space Shuttle \"Endeavour\" mission STS-130.\n\nThe Oxygen Generating System (OGS) is a NASA rack designed to electrolyse water from the Water Recovery System to produce oxygen and hydrogen. The oxygen is delivered to the cabin atmosphere. The unit is installed in the \"Destiny\" module. During one of the spacewalks conducted by STS-117 astronauts, a hydrogen vent valve required to begin using the system was installed. The system was delivered in 2006 by STS-121, and became operational on 12 July 2007. From 2001, the US orbital segment had used oxygen in a pressurized storage tank on the Quest airlock module, or from the Russian service module. Prior to the activation of the Sabatier System in October 2010 hydrogen and carbon dioxide extracted from the cabin was vented overboard. \n\nIn 2011, American news outlet CBS news and news magazine spaceflightnow reported \"The OGA over the past six months has not been running well because the water that's been fed to it is just slightly too acidic,\" said station Flight Director Chris Edelen. \"For the past several months, the station crew has been using oxygen brought up aboard visiting Progress supply ships, a European cargo craft and the Russian Elektron oxygen generator while awaiting delivery of the OGA repair equipment. The OGA, like the Elektron, uses electricity to split water molecules into hydrogen and oxygen. \" \n\nThe Sabatier system closes the loop in the ECLSS by combining waste hydrogen from the Oxygen generating system and carbon dioxide from the station atmosphere using the Sabatier reaction and preserving these valuable chemicals. The outputs of this reaction are water, and methane. The water is recycled to reduce the total amount of water that must be carried to the station from Earth, and the methane is vented overboard by the now shared hydrogen vent line installed for the Oxygen generating system.\n\nElektron is a Russian Electrolytic Oxygen Generator, which was also used on Mir. It uses electrolysis to produce oxygen. This process splits water molecules reclaimed from other uses on board the station into oxygen and hydrogen via electrolysis. The oxygen is vented into the cabin and the hydrogen is vented into space. The three Russian Elektron oxygen generators on board the International Space Station have been plagued with problems, frequently forcing the crew to use backup sources (either bottled oxygen or the Vika system discussed below). To support a crew of six, NASA added the oxygen generating system discussed above.\n\nIn 2004, the Elektron unit shut down due to (initially) unknown causes. Two weeks of troubleshooting resulted in the unit starting up again, then immediately shutting down. The cause was eventually traced to gas bubbles in the unit, which remained non-functional until a Progress resupply mission in October 2004. In 2005 ISS personnel tapped into the oxygen supply of the recently arrived Progress resupply ship, when the Elektron unit failed. In 2006 fumes from a malfunctioning Elektron unit prompted NASA flight engineers to declare a \"spacecraft emergency\". A burning smell led the ISS crew to suspect another Elektron fire, but the unit was only \"very hot\". A leak of corrosive, odorless potassium hydroxide forced the ISS crew to don gloves and face masks. It has been conjectured that the smell came from overheated rubber seals. The incident occurred shortly after STS-115 left and just before arrival of a resupply mission (including space tourist Anousheh Ansari). The Elektron did not come back online until November 2006, after new valves and cables arrived on the October 2006 Progress resupply vessel. The ERPTC (Electrical Recovery Processing Terminal Current) was inserted into the ISS to prevent harm to the systems.\n\nThe Vika or TGK oxygen generator, also known as Solid Fuel Oxygen Generation (SFOG) when used on the ISS, is a chemical oxygen generator originally developed by Roscosmos for Mir, and it provides an alternate oxygen generating system. It uses canisters of solid lithium perchlorate, which are burned to create gaseous oxygen. Each canister can supply the oxygen needs of one crewmember for one day. \n\nAnother Russian system, Vozdukh (Russian \"Воздух\", meaning \"air\"), removes carbon dioxide from the air based on the use of regenerable absorbers of carbon dioxide gas.\n\nTemperature and Humidity Control (THC) is the subsystem of the ISS ECLSS concerned with the maintenance of a steady air temperature and the control of the moisture in the station's air supply. Thermal Control System (TCS) is a component part of the THC system and subdivides into the Active Thermal Control System (ATCS, PDF document) and Passive Thermal Control System (PTCS). Controlling humidity is possible through lowering or raising the temperature and through adding moisture to the air.\n\nFire Detection and Suppression (FDS) is the subsystem devoted to identifying that there has been a fire and taking steps to fight \nit.\n\n"}
{"id": "13766164", "url": "https://en.wikipedia.org/wiki?curid=13766164", "title": "Infantile esotropia", "text": "Infantile esotropia\n\nInfantile esotropia is an ocular condition of early onset in which one or either eye turns inward. It is a specific sub-type of esotropia and has been a subject of much debate amongst ophthalmologists with regard to its naming, diagnostic features, and treatment.\n\nHistorically the term 'congenital strabismus' was used to describe constant esotropias with onset between birth and six months of age. However, this term was felt to be an inadequate classification as it covered a variety of esotropias with different causes, features and prognoses.\n\nIn 1988, American ophthalmologist Gunter K. Von Noorden discussed what he described as 'Essential Infantile Esotropia'. He described the condition as: \n\"early acquired, not... congenital ..., although congenital factors may favor its development between the ages of 3 and 6 months\"\n\n\nThe same condition had also previously been described by other ophthalmologists, notably Cianca (1962) who named it Cianca's Syndrome and noted the presence of manifest latent nystagmus, and Lang (1968) who called it Congenital Esotropia Syndrome and noted the presence of abnormal head postures. In both cases, however, the essential characteristics were the same, but with emphasis placed on different elements of the condition.\n\nHelveston (1993) further clarified and expanded upon von Noorden's work, and incorporated the work of both Lang and Cianca into his summary of the characteristics of the condition:\n\n\nThe expressions \"congenital esotropia\", \"infantile esoptropia\", \"idiopathic infantile esotropia\" and \"essential infantile esotropia\" are often used interchangeably.\n\n\"Cross-fixation congenital esotropia\", also called \"Cianci's syndrome\" is a particular type of large-angle infantile esotropia associated with tight medius rectus muscles. With the tight muscles, which hinder adduction, there is a constant inward eye turn. The patient cross-fixates, that is, to fixate objects on the left, the patient looks across the nose with the right eye, and vice versa. The patient tends to adopt a head turn, turning the head to the right to better see objects in the left visual field and turning the head to the left to see those in the right visual field. Binasal occlusion can be used to discourage cross-fixation. However, the management of cross-fixation congenital esotropia usually involves surgery.\n\nThis remains undetermined at the present time. A recent study by Major et al. reports that:\n\"Prematurity, family history or secondary ocular history, perinatal or gestational complications, systemic disorders, use of supplemental oxygen as a neonate, use of systemic medications, and male sex were found to be significant risk factors for infantile esotropia.\"\nFurther recent evidence indicates that a cause for \"infantile strabismus\" may lie with the input that is provided to the visual cortex. In particular, neonates who suffer injuries that, directly or indirectly, perturb binocular inputs into the primary visual cortex (V1) have a far higher risk of developing strabismus than other infants.\n\nA paper published by Eltern für Impfaufklärung, a German Anti-Vaccination activist group, cites a study by The Robert Koch Institute (RKI), claiming significant correlation between children who received Vaccinations and the onset of cause of Spine, Face & Eye Asymmetry.\n\nClinically Infantile esotropia must be distinguished from:\n\n\nAccording to a Cochrane review of 2012, controversies remain regarding type of surgery, non-surgical intervention and age of intervention.\n\nThe aims of treatment are as follows:\n\nThe elimination of any amblyopia\nA cosmetically acceptable ocular alignment\nlong term stability of eye position\nbinocular cooperation.\n\nIt is essential that a child with strabismus is presented to the ophthalmologist as early as possible for diagnosis and treatment in order to allow best possible monocular and binocular vision to develop. Initially, the patient will have a full eye examination to identify any associated pathology, and any glasses required to optimise acuity will be prescribed – although infantile esotropia is not typically associated with refractive error. Studies have found that approximately 15% of infantile esotropia patients have accommodative esotropia. For these patients, antiaccommodative therapy (with spectacles) is indicated before any surgery as antiaccommodative therapy fully corrects their esotropia in many cases and significantly decreases their deviation angle in others.\n\nAmblyopia will be treated via occlusion treatment (using patching or atropine drops) of the non-squinting eye with the aim of achieving full alternation of fixation. Management thereafter will be surgical. As alternative to surgery, also botulinum toxin therapy has been used in children with infantile esotropia. Furthermore, as accompaniment to ophthalmologic treatment, craniosacral therapy may be performed in order to relieve tension (\"see also:\" Management of strabismus).\n\nControversy has arisen regarding the selection and planning of surgical procedures, the timing of surgery and about what constitutes a favourable outcome.\n\n1. Selection and planning\n\nSome ophthalmologists, notably Ing and Helveston, favour a prescribed approach often involving multiple surgical episodes whereas others prefer to aim for full alignment of the eyes in one procedure and let the number of muscles operated upon during this procedure be determined by the size of the squint.\n\n2. Timing and outcome\n\nThis debate relates to the technical anatomical difficulties of operating on the very young versus the possibility of an increased potential for binocularity associated with early surgery. Infants are often operated upon at the age of six to nine months of age and in some cases even earlier at three or four months of age. Some emphasize the importance of intervening early such as to keep the duration of the patient's abnormal visual experience to a minimum. Advocates of early surgery believe that those who have their surgery before the age of one are more likely to be able to use both eyes together post-operatively.\n\nA Dutch study (ELISSS) compared early with late surgery in a prospective, controlled, non-randomized, multicenter trial and reported that:\n\"Children operated early had better gross stereopsis at age six as compared to children operated late. They had been operated more frequently, however, and a substantial number of children in both [originally-recruited] groups had not been operated at all.\"\nOther studies also report better results with early surgery, notably Birch and Stager and Murray et al. but do not comment on the number of operations undertaken. A recent study on 38 children concluded that surgery for infantile esotropia is most likely to result in measureable stereopsis if patient age at alignment is not more than 16 months. \nAnother study found that for children with infantile esotropia early surgery decreases the risk of dissociated vertical deviation developing after surgery.\n\nAside the strabismus itself, there are other aspects or conditions that appear to improve after surgery or botulinum toxin eye alignment. Study outcomes have indicated that after surgery the child catches up in development of fine-motor skills (such as grasping a toy and handling a bottle) and of large-muscle skills (such as sitting, standing, and walking) in case a developmental delay was present before. Evidence also indicates that as of the age of six, strabismic children become less accepted by their peers, leaving them potentially exposed to social exclusion starting at this age unless their eye positioning is corrected by this time (\"see also:\" Psychosocial effects of strabismus).\n\n"}
{"id": "43675194", "url": "https://en.wikipedia.org/wiki?curid=43675194", "title": "International Flame Research Foundation", "text": "International Flame Research Foundation\n\nThe International Flame Research Foundation – IFRF is a non-profit research association and network created in 1948 in IJmuiden (Netherlands), established in Livorno (Italy) between 2005 and 2016 \"(Fondazione Internazionale per la Ricerca Sulla Combustione – ONLUS)\", and in Sheffield (UK) since 2017. Meredith Thring was one of the founders.\n\nThe IFRF Membership Network unites some 1000 combustion researchers from 130 industrial companies and academic institutions worldwide, around a common interest in efficient and environmentally responsible industrial combustion, with a focus on flame studies.\n\nThe IFRF can be traced to a proposal written in 1948 by Meredith Thring, head of the Physics Department in the newly formed British Iron and Steel Research Association (BISRA). Entitled \"Proposals for the Establishment of an International Research Project on Luminous Radiation\", the document resulted in the formation of the \"International Flame Radiation Research Committee\" with representatives of the steel, fuel and appliance making industries in France, Holland and England - specifically the British Iron and Steel Research Association (BISRA), the Iron and Steel Research Association of France (IRSID) and the Royal Dutch Iron and Steel Company (KNHS).\n\nThe IFRF is the publisher of technical reports and regular publications:\nTheses publications are freely available on-line.\n\nThe IFRF is organized in 8 national committee plus the Associate Member Group (AMG) where no national committee exists.\n\n\nThe IFRF in managed by a Council and an Executive Committee.\n\nFrom 1948 to 2005 the IFRF facilities were located in the CORUS R&D centre at IJmuiden (Netherlands). In 2005, the research station was relocated at ENEL facilities in Livorno (Italy), the measurement program was restarted November 27, 2006. In 2015 a relocation of the IFRF headquarters process was initiated. Leading to the designation of University of Sheffield and its PACT laboratory as the new IFRF location from 2017.\n\n"}
{"id": "42030947", "url": "https://en.wikipedia.org/wiki?curid=42030947", "title": "List of pickled foods", "text": "List of pickled foods\n\nThis is a list of pickled foods. Many various types of foods are pickled to preserve them and add flavor. Some of these foods also qualify as .\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "8357409", "url": "https://en.wikipedia.org/wiki?curid=8357409", "title": "List of temperature sensors", "text": "List of temperature sensors\n\n\n\nNon-exhaustive list of products classified by manufacturer.\n\n\nContents are extract from the manufacturer datasheet. Each manufacturer may have different method with different accuracy processes, so it can be difficult to compare these self-reported values directly.\n\n"}
{"id": "44362658", "url": "https://en.wikipedia.org/wiki?curid=44362658", "title": "Machine industry", "text": "Machine industry\n\nThe machine industry or machinery industry is a subsector of the industry, that produces and maintains machines for consumers, the industry, and most other companies in the economy.\n\nThis machine industry traditionally belongs to the heavy industry. Nowadays, many smaller companies in this branch are considered part of the light industry. Most manufacturers in the machinery industry are called machine factories.\n\nThe machine industry is a subsector of the industry that produces a range of products from power tools, different types of machines, and domestic technology to factory equipment etc. On the one hand the machine industry provides:\nThese means of production are called capital goods, because a certain amount of capital is invested. Much of those production machines require regular maintenance, which becomes supplied specialized companies in the machine industry.\n\nOn the other end the machinery industry supplies consumer goods, including kitchen appliances, refrigerators, washers, dryers and a like. Production of radio and television, however, is generally considered belonging to the electrical equipment industry. The machinery industry itself is a major customer of the steel industry.\n\nThe production of the machinery industry varies widely from single-unit production and series production to mass production. Single-unit production is about constructing unique products, which are specified in specific customer requirements. Due to modular design such devices and machines can often be manufactured in small series, which significantly reduces the costs. From a certain stage in the production, the specific customer requirements are build in, and the unique product is created.\n\nThe machinery industry came into existence during the Industrial Revolution. Companies in this emerging field grew out of iron foundries, shipyards, forges and repair shops. Often companies were a combination of machine factory and shipyard. Early in the 20th century several motorcycle and automobile manufacturers began their own machine factories.\n\nPrior to the industrial revolution a variety of machines existed such as clocks, weapons and running gear for mills (watermill, windmill, horse mill etc.) Production of these machines were on much smaller scale in artisan workshops mostly for the local or regional market. With the advent of the industrial revolution manufacturing began of composite tools with more complex construction, such as steam engines and steam generators for the evolving industry and transport. In addition, the emerging machine factories started making machines for production machines as textile machinery, compressors, agricultural machinery, and engines for ships.\n\nDuring the first decades of the industrial revolution in England, from 1750, there was a concentration of labor usually in not yet mechanized factories. There were all kinds of new machines invented, which were initially made by the inventors themselves. Early in the 18th century, the first steam engines, the Newcomen engine, came into use throughout Britain and Europe, principally to pump water out of mines.\n\nIn the 1770s James Watt significantly improved this design. He introduced a steam engine easy employable to supply a large amounts of energy, which set the mechanization of factories underway. In England certain cities concentrated on making specific products, such as specific types of textiles or pottery. Around these cities specialized machinery industry arose in order to enable the mechanization of the plants. Hereby late in the 18th century arose the first machinery industry in the UK and also in Germany and Belgium.\n\nThe Industrial Revolution received a further boost with the upcoming railways. These arose at the beginning of the 19th century in England as innovation in the mining industry. The work in coal mines was hard and dangerous, and so there was a great need for tools to ease this work. In 1804, Richard Trevithick placed the first steam engine on rails, and was in 1825 the Stockton and Darlington Railway was opened, intended to transport coals from the mine to the port. In 1835 the first train drove in continental Europe between Mechelen and Brussels, and in the Netherlands in 1839 the first train drove between Amsterdam and Haarlem. For the machinery industry this brought all sorts of new work with new machinery for metallurgy, machine tool for metalworking, production of steam engines for trains with all its necessities etc.\n\nIn time the market for the machine industry became wider, specialized products were manufactured for a greater national and often international market. For example, it was not uncommon in the second half of the 19th century that American steelmakers ordered their production in England, where new steelmaking techniques were more advanced. In the far east Japan would import these product until the early 1930s, the creation of an own machinery industry got underway. .\n\nThe term \"machinery industry\" came into existence later in the 19th century. One of the first times this branch of industry was recognized as such, and was investigated, was in a production statistics of 1907 created by the British Ministry of Trade and Industry. In this statistic the output of the engineering industry, was divided into forty different categories, including for example, agricultural machinery, machinery for the textile industry and equipment, and parts for train and tram.\n\nThe inventions of new propulsion techniques based on electric motors, internal combustion engines and gas turbines brought a new generation of machines in the 20th century from cars to household appliances. Not only the product range of the machinery industry increased considerably, especially smaller machines could delivered products in much greater numbers fabricated in mass production. With the rise of mass production in other parts of the industry, there was also a high demand for manufacturing and production systems, to increase the entire production.\n\nShortage of labor in agriculture and industry at the beginning of the second half of the 20th century, raised the need for further mechanization of production, which required for more specific machines. The rise of the computer made further automation of production possible, which in turn set new demands on the machinery industry.\n\nThe machinery industry produces different kind of products, for example engines, pumps, logistics equipment; for different kind of markets from the agriculture industry, food & beverage industry, manufacturing industry, health industry, and amusement industry till different branches of the consumer market. As such companies in the machine industry can be classified by product of market.\n\nIn the world of today all kinds of Industry classifications exists. Some classifications recognize the machine industry as a specific class, and offer a subdivision for this field. For example, the Dutch Standard Industrial Classification of 1993, developed by the Statistics Netherlands, give the following breakdown of the machinery industry:\n\nThis composition of the machinery industry has been significantly altered with latest revision of the Dutch Standard Industrial Classification of 1993. The Standard Industrial Classification of 1974 broke down the machinery industry into nine sectors:\nIt may be clear that classification is by markets, and the more recent classification is by product.\n\nThe machine industry makes a very diverse range of products. A selection:\n\nIn Germany, in 2011 about 900,000 people were employed in the machine industry and an estimated of 300,000 abroad. The combined turnover of the sector was €130 billion, of which 60% came from export. There were about 6,600 active companies, and 95% of those companies employed less than 500 people. Each employee generated an average of 148,000 Euro. Some of the largest companies in Germany are DMG Mori Seiki AG, GEA Group, Siemens AG, and ThyssenKrupp.\n\nIn the French machinery industry in 2009 about 650,000 people were employed, and the sector generated a turnover of 98 billion euros. Because of the crisis, the turnover of the sector had fallen by 15 percent. Due to stronger consumer spending and continuing demand from the energy sector and transport sector, the damage of the crisus was still limited.\n\nIn the Netherlands in 1996, a total of some 93,000 workers were employed in the machinery industry, with approximately 2,500 companies present. In 1000 of these companies there were working 20 or more employees. In the Netherlands, according to the Chamber of Commerce in this subsector of the industry in 2011 some 15,000 companies were active. Some of the largest companies in the Netherlands are Lely (company), Philips and Stork B.V..\n\nU.S. machinery industries had total domestic and foreign sales of $413.7 billion in 2011. The United States is the world’s largest market for machinery, as well as the third-largest supplier. American manufacturers held a 58.5 percent share of the U.S. domestic market.\n\n\n"}
{"id": "4986640", "url": "https://en.wikipedia.org/wiki?curid=4986640", "title": "Marissa Mayer", "text": "Marissa Mayer\n\nMarissa Ann Mayer (; born May 30, 1975) is an American information technology executive, formerly serving as the president and chief executive officer of Yahoo!, a position she had held starting July 2012. It was announced in January 2017 that she would step down from the company's board upon the sale of Yahoo!'s operating business to Verizon Communications for $4.8 billion. She would not join the newly combined company, now called Oath, and announced her resignation on June 13, 2017. She is a graduate of Stanford University and was a long-time executive, usability leader, and key spokeswoman for Google (employee #20).\n\nMayer was born in Wausau, Wisconsin, the daughter of Margaret Mayer, an art teacher of Finnish descent, and Michael Mayer, an environmental engineer who worked for water companies. Her grandfather, Clem Mayer, had polio when he was 7 and served as mayor of Jackson, Wisconsin, for 32 years. She has a younger brother. She would later describe herself as having been \"painfully shy\" as a child and teenager. She \"never had fewer than one after-school activity per day,\" participating in ballet, ice-skating, piano, swimming, debate, and Brownies. During middle school and high school, she took piano and ballet lessons, the latter of which taught her \"criticism and discipline, poise, and confidence\". At an early age, she showed an interest in math and science.\n\nWhen she was attending Wausau West High School, Mayer was on the curling team and the precision dance team. She excelled in chemistry, calculus, biology, and physics. She took part in extracurricular activities, becoming president of her high school's Spanish club, treasurer of the Key Club, captain of the debate team, and captain of the pom-pom squad. Her high school debate team won the Wisconsin state championship and the pom-pom squad was the state runner-up. During high school, she worked as a grocery clerk. After graduating from high school in 1993, Mayer was selected by Tommy Thompson, then the Governor of Wisconsin, as one of the state's two delegates to attend the National Youth Science Camp in West Virginia.\n\nIntending to become a pediatric neurosurgeon, Mayer took pre-med classes at Stanford University. She later switched her major from pediatric neuroscience to symbolic systems, a major which combined philosophy, cognitive psychology, linguistics, and computer science. At Stanford, she danced in the university ballet's \"Nutcracker\", was a member of parliamentary debate, volunteered at children's hospitals, and helped bring computer science education to Bermuda's schools. During her junior year, she taught a class in symbolic systems, with Eric S. Roberts as her supervisor. The class was so well received by students that Roberts asked Mayer to teach another class over the summer. Mayer went on to graduate with honors from Stanford with a BS in symbolic systems in 1997 and an MS in computer science in 1999. For both degrees, her specialization was in artificial intelligence. For her undergraduate thesis, she built travel-recommendation software that advised users in natural-sounding human language. In 2009, the Illinois Institute of Technology granted Mayer an honoris causa doctorate degree in recognition of her work in the field of search.\n\nMayer interned at SRI International in Menlo Park, California, and Ubilab, UBS's research lab based in Zurich, Switzerland. She holds several patents in artificial intelligence and interface design.\n\nAfter graduating from Stanford, Mayer received 14 job offers, including a teaching job at Carnegie Mellon University and a consulting job at McKinsey & Company. She joined Google in 1999 as employee number 20.<nowiki> </nowiki>She started out writing code and overseeing small teams of engineers, developing and designing Google's search offerings. She became known for her attention to detail, which helped land her a promotion to product manager, and later she became director of consumer web products. She oversaw the layout of Google's well-known, unadorned search homepage. She was also on the three-person team responsible for Google AdWords, which is an advertising platform that allows businesses to show their product to relevant potential customers based on their search terms. AdWords helped deliver 96% of the company's revenue in the first quarter of 2011.\n\nIn 2002, Mayer started the Associate Product Manager (APM) program, a Google mentorship initiative to recruit new talents and cultivate them for leadership roles. Each year, Mayer selected a number of junior employees for the two-year program, where they took on extracurricular assignments and intensive evening classes. Notable graduates of the program include Bret Taylor and Justin Rosenstein. In 2005, Mayer became Vice President of Search Products and User Experience. Mayer held key roles in Google Search, Google Images, Google News, Google Maps, Google Books, Google Product Search, Google Toolbar, iGoogle, and Gmail.\n\nMayer was the vice president of Google Search Products and User Experience until the end of 2010, when she was asked by then-CEO Eric Schmidt to head the Local, Maps, and Location Services. In 2011, she secured Google's acquisition of survey site Zagat for $125 million. While Mayer was working at Google, she taught introductory computer programming at Stanford and mentored students at the East Palo Alto Charter School. She was awarded the Centennial Teaching Award and the Forsythe Award from Stanford.\n\nOn July 16, 2012, Mayer was appointed president and CEO of Yahoo!, effective the following day. She is also a member of the company's board of directors. To simplify the bureaucratic process and \"make the culture the best version of itself\", Mayer launched a new online program called PB&J. It collects employee complaints, as well as their votes on problems in the office; if a problem generates at least 50 votes, online management automatically investigates the matter. In February 2013, Mayer oversaw a major personnel policy change at Yahoo! that required all remote-working employees to convert to in-office roles. Having worked from home toward the end of her pregnancy, Mayer returned to work after giving birth to a boy, and built a mother's room next to her office suite—Mayer was consequently criticized for the telecommuting ban. In April 2013, Mayer changed Yahoo!'s maternity leave policy, lengthening its time allowance and providing a cash bonus to parents. CNN noted this was in line with other Silicon Valley companies, such as Facebook and Google. Mayer has been criticized for many of her management decisions in pieces by \"The New York Times\" and \"The New Yorker\".\n\nOn May 20, 2013, Mayer led Yahoo! to acquire Tumblr in a $1.1 billion acquisition. In February 2016, Yahoo! acknowledged that the value of Tumblr had fallen by $230 million since it was acquired. In July 2013, Yahoo! reported a fall in revenues, but a rise in profits compared with the same period in the previous year. Reaction on Wall Street was muted, with shares falling 1.7%. In September 2013, it was reported that the stock price of Yahoo! had doubled over the 14 months since Mayer's appointment. However, much of this growth may be attributed to Yahoo!'s stake in the Chinese e-commerce company Alibaba Group, which was acquired before Mayer's tenure.\n\nIn November 2013, Mayer instituted a performance review system based on a bell curve ranking of employees, suggesting that managers rank their employees on a bell curve, with those at the low end being fired. Employees complained that some managers were viewing the process as mandatory. In February 2016, a former Yahoo! employee filed a lawsuit against the company claiming that Yahoo's firing practices have violated both California and federal labor laws.\n\nIn 2014, Mayer was ranked sixth on \"Fortune\"s 40 under 40 list, and was ranked the 16th most-powerful businesswoman in the world that year according to the same publication. In March 2016 \"Fortune\" would name Mayer as one of the world's most disappointing leaders. Yahoo! stocks continued to fall by more than 30% throughout 2015, while 12 key executives left the company.\n\nIn December 2015, the New York-based hedge fund SpringOwl, a shareholder in Yahoo Inc., released a statement arguing that Mayer be replaced as CEO. Starboard Value, an activist investing firm that owns a stake in Yahoo, likewise wrote a scathing letter regarding Mayer's performance at Yahoo. By January 2016, it was further estimated that Yahoo!'s core business has been worth less than zero dollars for the past few quarters. In February 2016, Mayer confirmed that Yahoo! was considering the possibility of selling its core business. In March 2017, it was reported that Mayer could receive a $23 million termination package upon the sale of Yahoo! to Verizon.\n\nMayer announced her resignation on June 13, 2017. In spite of large losses in advertising revenue at Yahoo! and a 50% reduction in staff during her 5 years as CEO, Mayer was paid a total of $239 million over that time, mainly in stock and stock options. On the day of her resignation, Mayer publicly highlighted many of the company's achievements during her tenure, including: creating $43B in market capitalization, tripling Yahoo stock, growing mobile users to over 650 million, building a $1.5B mobile ad business, and transforming Yahoo's culture.\n\nOn 8 November 2017, along with several other present and former corporate CEOs; Mayer testified before the United States Senate Committee on Commerce, Science, and Transportation regarding major security breaches at Yahoo during 2013 and 2014.\n\nScott Ard, a prominent editorial director, fired from Yahoo! in 2015, filed a lawsuit alleging that \"Mayer encouraged and fostered the use of (an employee performance-rating system) to accommodate management’s subjective biases and personal opinions, to the detriment of Yahoo!’s male employees.\" He claimed that, prior to his firing, he had received \"fully satisfactory\" performance reviews since starting at the company in 2011 as head of editorial programming for Yahoo!'s home page; however, he was relieved of his role, which was given to a woman who had been recently hired.\n\nAn earlier lawsuit was filed by Gregory Anderson, who was fired in 2014, alleging the company’s performance management system was arbitrary and unfair and disguised layoffs as terminations for the purpose of evading state and federal WARN Acts, making it the first WARN Act and gender discrimination lawsuit Yahoo! and Mayer faced in 2016.\n\nAfter leaving her position at Yahoo in 2017, Mayer has started her new project, Lumi Labs with an old colleague Enrique Munoz Torres. The company is based in Palo Alto and will be focusing on artificial intelligence and consumer media.\n\nAs well as sitting on the boards of directors of Walmart and Jawbone, Mayer also sits on several non-profit boards, such as Cooper–Hewitt, National Design Museum, New York City Ballet, San Francisco Ballet, and San Francisco Museum of Modern Art.\n\nMayer actively invests in technology companies, including crowd-sourced design retailer Minted, live video platform Airtime.com, wireless power startup uBeam, online DIY community/e-commerce company Brit + Co., mobile payments processor Square, home décor site One Kings Lane, genetic testing company Natera, and nootropics and biohacking company Nootrobox.\n\nMayer was named to \"Fortune\" magazine's annual list of America's \"50 Most Powerful Women in Business\" in 2008, 2009, 2010, 2011, 2012, 2013, and 2014 with ranks at 50, 44, 42, 38, 14, 8 and 16 respectively. In 2008, at age 33, she was the youngest woman ever listed.\n\nMayer was named one of \"Glamour Magazine\"s Women of the Year in 2009. She was listed in \"Forbes\" Magazine's List of The World's 100 Most Powerful Women in 2012, 2013 and 2014, with ranks of 20, 32 and 18 respectively.\n\nIn September 2013, Mayer became the first CEO of a Fortune 500 company to be featured in a \"Vogue\" magazine spread.\n\nIn 2013, she was also named in the \"Time\" 100, becoming the first woman listed as number one on \"Fortune\" magazine's annual list of the top 40 business stars under 40 years old.\n\nMayer made \"Fortune\" magazine history in 2013, as the only person to feature in all three of its annual lists during the same year: Businessperson of the Year (No. 10), Most Powerful Women (at No. 8), and 40 Under 40 (No. 1) at the same time. In March 2016, \"Fortune\" then named Mayer as one of the world's most disappointing leaders.\n\nOn 24 December 2015, Mayer was listed by UK-based company Richtopia at number 14 in the list of 500 Most Influential CEOs.\n\nMayer appeared on the List of women CEOs of Fortune 500 companies in 2017, having ranked 498 of the top 500 Fortune 500 company CEOs.\n\nMayer married lawyer and investor Zachary Bogue on December 12, 2009. On the day Yahoo! announced her hiring, Mayer revealed that she was pregnant; she gave birth to a baby boy on September 30, 2012. Although she asked for baby name suggestions via social media, she eventually chose the name \"Macallister\" from an existing list. On December 10, 2015, Mayer announced that she had given birth to identical twin girls, Marielle and Sylvana.\n\nMayer is Lutheran, but she has said—referencing Vince Lombardi's \"Your God, your family and the Green Bay Packers\"—that her priorities are \"God, family and Yahoo!, except I'm not that religious, so it's really family and Yahoo!.\"\n\n\n"}
{"id": "625341", "url": "https://en.wikipedia.org/wiki?curid=625341", "title": "Nanopore", "text": "Nanopore\n\nA nanopore is a pore of nanometer size. It may, for example, be created by a pore-forming protein or as a hole in synthetic materials such as silicon or graphene.\n\nWhen a nanopore is present in an electrically insulating membrane, it can be used as a single-molecule detector. It can be a biological protein channel in a high electrical resistance lipid bilayer, a pore in a solid-state membrane or a hybrid of these – a protein channel set in a synthetic membrane. The detection principle is based on monitoring the ionic current passing through the nanopore as a voltage is applied across the membrane. When the nanopore is of molecular dimensions, passage of molecules (e.g., DNA) cause interruptions of the \"open\" current level, leading to a \"translocation event\" signal. The passage of RNA or single-stranded DNA molecules through the membrane-embedded alpha-hemolysin channel (1.5 nm diameter), for example, causes a ~90% blockage of the current (measured at 1 M KCl solution).\n\nIt may be considered a Coulter counter for much smaller particles.\n\nNanopores may be formed by pore-forming proteins, typically a hollow core passing through a mushroom-shaped protein molecule. Examples of pore-forming proteins are alpha hemolysin and MspA porin. In typical laboratory nanopore experiments, a single protein nanopore is inserted into a lipid bilayer membrane and single-channel electrophysiology measurements are taken.\n\nSolid-state nanopores are generally made in silicon compound membranes, one of the most common being silicon nitride. Solid-state nanopores can be manufactured with several techniques including ion-beam sculpting and electron beams.\n\nMore recently, the use of graphene as a material for solid-state nanopore sensing has been explored. Another example of solid-state nanopores is a box-shaped graphene (BSG) nanostructure. The BSG nanostructure is a multilayer system of parallel hollow nanochannels located along the surface and having quadrangular cross-section. The thickness of the channel walls is approximately equal to 1 nm. The typical width of channel facets makes about 25 nm.\n\nSince the discovery of track-etched technology in the late 1960s, filter membranes with needed diameter have found application potential in various fields including food safety, environmental pollution, biology, medicine, fuel cell, and chemistry. These track-etched membranes are typically made in polymer membrane through track-etching procedure, during which the polymer membrane is first irradiated by heavy ion beam to form tracks and then cylindrical pores or asymmetric pores are created along the track after wet etching.\n\nAs important as fabrication of the filter membranes with proper diameters, characterizations and measurements of these materials are of the same paramount. Until now, a few of methods have been developed, which can be classified into the following categories according to the physical mechanisms they exploited: imaging methods such as scanning electron microscopy (SEM), transmission electron microscopy (TEM), atomic force microscopy (AFM); fluid transport such as bubble point and gas transport; fluid adsorptions such as nitrogen adsorption/desorption (BEH), mercury porosimetry, liquid-vapor equilibrium (BJH), gas-liquid equilibrium (permoporometry) and liquid-solid equilibrium (thermoporometry); electronic conductance; ultrasonic spectroscopy; and molecular transport.\n\nMore recently, the use of light transmission technique as a method for nanopore size measurement has been proposed.\n\nIon current rectification (ICR) is an important phenomenon for nanopore. Ion current rectification can also be used as a drug sensor and be employed to investigate charge status in the polymer membrane.\n\nThe observation that a passing strand of DNA containing different bases results in different blocking levels has led to the nanopore sequencing hypothesis. Oxford Nanopore Technologies and Professor Hagan Bayley's laboratories have shown identification of individual nucleotides including methylated cytosine as they pass through a modified hemolysin nanopore.\n\nApart from rapid DNA sequencing, other applications include separation of single stranded and double stranded DNA in solution, and the determination of length of polymers. At this stage, nanopores are making contributions to the understanding of polymer biophysics, as well as to single-molecule analysis of DNA-protein interactions.\n\nSize-tunable elastomeric nanopores have been fabricated, allowing accurate measurement of nanoparticles as they occlude the flow of ionic current.This measurement methodology can be used to measure a wide range of particle types. In contrast to the limitations of solid-state pores, they allow for the optimisation of the resistance pulse magnitude relative to the background current by matching the pore-size closely to the particle-size. As detection occurs on a particle by particle basis, the true average and polydispersity distribution can be determined. Using this principle, the world's only commercial tunable nanopore-based particle detection system has been developed by Izon Science Ltd. The box-shaped graphene (BSG) nanostructure can be used as a basis for building devices with changeable pore sizes.\n\nThese can be about 20 nm in a diameter. They are integrated into artificially constructed encapsulated cells of silicon wafers. These pores allow small molecules like oxygen, glucose and insulin to pass however they prevent large immune system molecules like immunoglobins from passing. As an example, rat pancreatic cells are microencapsulated, they receive nutrients and release insulin through nanopores being totally isolated from their neighboring environment i.e. foreign cells. This knowledge can help to replace nonfunctional islets of Langerhans cells in the pancreas (responsible for producing insulin), by harvested piglet cells. They can be implanted underneath the human skin without the need of immunosuppressants which put diabetic patients at a risk of infection.\n\n\n\n"}
{"id": "245564", "url": "https://en.wikipedia.org/wiki?curid=245564", "title": "Nitromethane", "text": "Nitromethane\n\nNitromethane is an organic compound with the chemical formula . It is the simplest organic nitro compound. It is a polar liquid commonly used as a solvent in a variety of industrial applications such as in extractions, as a reaction medium, and as a cleaning solvent. As an intermediate in organic synthesis, it is used widely in the manufacture of pharmaceuticals, pesticides, explosives, fibers, and coatings. Nitromethane is used as a fuel additive in various motorsports and hobbies, e.g. Top Fuel drag racing and miniature internal combustion engines in radio control, control line and free flight model aircraft.\n\nNitromethane is produced industrially by combining propane and nitric acid in the gas phase at 350–450 °C (662–842 °F). This exothermic reaction produces the four industrially significant nitroalkanes: nitromethane, nitroethane, 1-nitropropane, and 2-nitropropane. The reaction involves free radicals, including the alkoxyl radicals of the type CHCHCHO, which arise via homolysis of the corresponding nitrite ester. These alkoxy radicals are susceptible to C—C fragmentation reactions, which explains the formation of a mixture of products.\n\nIt can be prepared in other methods that are of instructional value. The reaction of sodium chloroacetate with sodium nitrite in aqueous solution produces this compound:\n\nThe principal use of nitromethane is as a stabilizer for chlorinated solvents, which are used in dry cleaning, semiconductor processing, and degreasing. It is also used most effectively as a solvent or dissolving agent for acrylate monomers, such as cyanoacrylates (more commonly known as \"super-glue\"). It is also used as a fuel in some forms of racing.\n\nNitromethane is a relatively acidic carbon acid. It has a pK of 17.2 in DMSO solution. This value indicates an aqueous pK of about 11. It is slow to deprotonate. Protonation of the conjugate base O2NCH2-, which is nearly isosteric with nitrate, occurs initially at oxygen.\n\nIn organic synthesis nitromethane is employed as a one carbon building block. Its acidity allows it to undergo deprotonation, enabling condensation reactions analogous to those of carbonyl compounds. Thus, under base catalysis, nitromethane adds to aldehydes in 1,2-addition in the nitroaldol reaction. Some important derivatives include the pesticides chloropicrin (ClCNO), beta-nitrostyrene, and tris(hydroxymethyl)nitromethane, ((HOCH)CNO). Reduction of the latter gives tris(hydroxymethyl)aminomethane, (HOCH)CNH, better known as tris, a widely used buffer. In more specialized organic synthesis, nitromethane serves as a Michael donor, adding to α,β-unsaturated carbonyl compounds via 1,4-addition in the Michael reaction.\n\nNitromethane is used as a fuel in motor racing, particularly drag racing, as well as for radio-controlled models (such as cars, planes and helicopters). In this context, nitromethane is commonly referred to as \"nitro\", and is the principal ingredient for fuel used in the \"Top Fuel\" category of drag racing.\n\nThe oxygen content of nitromethane enables it to burn with much less atmospheric oxygen.\nThe amount of air required to burn of gasoline is , but only of air is required for 1 kg of nitromethane. Since an engine's cylinder can only contain a limited amount of air on each stroke, 8.6 times more nitromethane than gasoline can be burned in one stroke. Nitromethane, however, has a lower specific energy: gasoline provides about 42–44 MJ/kg, whereas nitromethane provides only 11.3 MJ/kg. This analysis indicates that nitromethane generates about 2.3 times the power of gasoline when combined with a given amount of oxygen.\n\nNitromethane can also be used as a monopropellant, i.e., a fuel that burns without added oxygen. The following equation describes this process:\nNitromethane has a laminar combustion velocity of approximately 0.5 m/s, somewhat higher than gasoline, thus making it suitable for high-speed engines. It also has a somewhat higher flame temperature of about . The high heat of vaporization of 0.56 MJ/kg together with the high fuel flow provides significant cooling of the incoming charge (about twice that of methanol), resulting in reasonably low temperatures\n\nNitromethane is usually used with rich air–fuel mixtures because it provides power even in the absence of atmospheric oxygen. When rich air–fuel mixtures are used, hydrogen and carbon monoxide are two of the combustion products. These gases often ignite, sometimes spectacularly, as the normally very rich mixtures of the still burning fuel exits the exhaust ports. Very rich mixtures are necessary to reduce the temperature of combustion chamber hot parts in order to control pre-ignition and subsequent detonation. Operational details depend on the particular mixture and engine characteristics.\n\nA small amount of hydrazine blended in nitromethane can increase the power output even further. With nitromethane, hydrazine forms an explosive salt that is again a monopropellant. This unstable mixture poses a severe safety hazard and the Academy of Model Aeronautics does not permit its use in competitions.\n\nIn model aircraft and car glow fuel, the primary ingredient is generally methanol with some nitromethane (0% to 65%, but rarely over 30%, and 10–20% lubricants (usually castor oil and/or synthetic oil). Even moderate amounts of nitromethane tend to increase the power created by the engine (as the limiting factor is often the air intake), making the engine easier to tune (adjust for the proper air/fuel ratio).\n\nNitromethane was not known to be a high explosive until a railroad tanker car loaded with it exploded on . After much testing, it was realized that nitromethane was a more energetic high explosive than TNT, although TNT has a higher velocity of detonation (VoD) and brisance. Both of these explosives are oxygen-poor, and some benefits are gained from mixing with an oxidizer, such as ammonium nitrate. Pure nitromethane is an insensitive explosive with a VoD of approximately , but even so inhibitors may be used to reduce the hazards. The tank car explosion was speculated to be due to adiabatic compression, a hazard common to all liquid explosives. This is when small entrained air bubbles compress and superheat with rapid rises in pressure. It was thought that an operator rapidly snapped shut a valve creating a \"hammer-lock\" pressure surge.\n\nNitromethane can also be mixed with ammonium nitrate, which is used as an oxidizer, to form an explosive mixture known as ANNM. One graphic example of this was the use of nitromethane and ammonium nitrate in the Oklahoma City bombing.\n\nExhaust gas from an internal combustion engine whose fuel includes nitromethane will contain nitric acid vapour, which is corrosive, and when inhaled causes a muscular reaction making it impossible to breathe. The condensed nitric acid-based residue left over in a glow-fueled model engine after a model-flight session can also corrode their internal components, usually mandating use of a combination of kerosene to neutralize the residual nitric acid, and an \"after-run oil\" (often the lower-viscosity \"air tool oil\" variety of a popular preservative oil) for lubrication to safeguard against such damage, when such an engine is placed into storage.\n\nNitromethane is a popular solvent in organic and electroanalytical chemistry. It can be purified by cooling below its freezing point, washing the solid with cold diethyl ether, followed by distillation.\n\n\n"}
{"id": "33727695", "url": "https://en.wikipedia.org/wiki?curid=33727695", "title": "Organization for Machine Automation and Control", "text": "Organization for Machine Automation and Control\n\nThe Organization for Machine Automation and Control (OMAC) is a global organization that supports the machine automation and operational needs of manufacturing. OMAC, has in conjunction with ISA, created the PackML industry standard for describing the state and transitions of packaging machines. OMAC was formed by General Motors in the 1980s under the name Open Modular Architecture Controls to address the problem of each machine having different controls and/or software implementations. In the late 1990s OMAC expanded into the packaging automation industry.\n"}
{"id": "48678774", "url": "https://en.wikipedia.org/wiki?curid=48678774", "title": "Project NOAH (Philippines)", "text": "Project NOAH (Philippines)\n\nProject NOAH (Nationwide Operational Assessment of Hazard) is the Philippines' primary disaster risk reduction and management program. It was initially administered by the Department of Science and Technology (DOST) from 2012 to 2017, but is now managed by the University of the Philippines.\n\nProject NOAH was a response to President Benigno Aquino III's call on a better disaster prevention and mitigation system in the Philippines in the aftermath of the destructive Tropical Storm Sendong in December 2011. It was publicly launched by President Aquino, project head Mahar Lagmay, and \nother government officials in Marikina on July 6, 2012. The program combines science and technology for disaster risk reduction and management. It is also a responsive program that aims to provide a 6-hour lead-time warning to agencies involved in disaster prevention and mitigation. The project also uses advanced technologies to enhance current geo-hazard vulnerability maps. It is also being developed with the help of the National Institute of Geological Sciences and the College of Engineering of the University of the Philippines; the Philippine Atmospheric, Geophysical and Astronomical Services Administration (PAGASA); the Philippine Institute of Volcanology and Seismology (PHIVOLCS); the Advanced Science and Technology Institute (ASTI), and the Science and Technology Information Institute (STII).The project is now composed of twenty-one institutions from the local and private sectors, including media and telecommunication companies.\n\nThe program involves seven major components described as follows: \n\nThe program has been dubbed as the country's flagship disaster prevention and mitigation program.\n\nIn January 2017 however, the Philippine government announced that Project NOAH would be shut down effective March 1, citing the lack of funds; it was supposed to remain in operation until February 28, 2017 only. On February 23, 2017, the University of the Philippines decided to adopt Project NOAH and continue its operations upon the termination of its administration by the Department of Science and Technology (DOST) on February 28.\n\nThe last component completed before the end of its DOST-administered era was the Integrated Scenario-based Assessment of Impacts and Hazards (ISAIAH), which sought to translate hazards mapped by the project into municipal-level risk assessments that detail the level of exposure and vulnerability of a community. The component allowed citizens to contribute ground-level risk information through the use of OpenStreetMap. The component resulted to completion of 16 provinces mapped with 2.2 million structures added to the database.\n\nOn June 20, 2017, the University of the Philippines relaunched the UP Resilience Institute with Project NOAH, now called NOAH Center, as its flagship program. \n\nThe official mobile version of project NOAH was launched by then-DOST Secretary Mario Montejo and Smart Communications on 17 October 2012. The app was developed by Rolly Rulete together with Pablito Veroy and Jay Albano. The mobile application prototype was originally written in HTML5.\n\n"}
{"id": "24054571", "url": "https://en.wikipedia.org/wiki?curid=24054571", "title": "Radiall", "text": "Radiall\n\nRadiall is a company that designs, develops, and manufactures connectors and associated components for use in electronic applications. The company offers interconnect components, including radio frequency/coaxial connectors and cable assemblies, antennas, fiber optic connectors and cable assemblies, microwave components and cable assemblies, microwave switches, and multipin connectors. \nIt serves the aerospace, defense, industrial, instrumentation, telecom, space and medical markets. \nThe company markets its products worldwide through a network of agents and distributors.\n\nCo-founded by Yvon and Lucien Gattaz in 1952, it is headquartered in Paris, France. The Radiall Corporate entity is currently listed on Eurolist by NYSE Euronext, but 89% of Radiall's shares are owned by its founders (The Gattaz Family). In 2014, Radiall's revenue was 280 million euros with a growth of 21%. Total net profit was 33.9 million euros with a growth of 83%, compared to 2013. Radiall invested approximately 7% in Research and Development, which is significantly above the industry standard.\n\nIn the spring of 1952 two brothers, Yvon and Lucien Gattaz, founded Radiall. Yvon Gattaz was the entrepreneur who took care of the commercial and financial aspect of the business. Lucien was the inventor. He was more interested by the technical aspects of research and production. In 1953, they bought their first workshop in Paris, France. A slogan had already been found: \"Speed & Quality\".\n\nAt that time, France intended to be a world leader in television by using higher frequencies through its SECAM standard. The antenna had to be linked to the TV monitor with a coaxial cable. Yvon and Lucien worked hard to invent a cheap and easy-to-use interconnect that would fit the application. By 1954, Radiall was producing 100,000 connectors per month.\n\nIn November 1963, Yvon and Lucien inaugurated their first factory at Voiron in Isere, France, of 2,500 m2 (27,000 sqft). That same year, the company launched the coaxial connector Mini Quick. Five years later, in 1968, Radiall built its headquarters at Rosny-Sous-Bois. This 4,000 m2 (43,000 sqft) two story building (the third floor came after), is made only of glass and foil.\n\nBy 1967, opening itself to Europe was already in Radiall's mind, so they created a subsidiary in Staines, England: Radiall Microwave Components Ltd. In 1969, they opened another office in Germany (at Buchslag, close to Frankfurt): RGmbH with the goal of developing a European sales network.\n\nIn 1972, Lucien Gattaz had identified the company Sogie as a competitor to Radiall in the connector business. It was for sale and showing a deficit, so Radiall made its first acquisition and it only took a few years to make it profitable. Buying Sogie showed Radiall's desire to continue specializing itself in Multipins Connectors.\n\nThat same year, the company also started getting interested in Aerospace and Fiber Optics. A new facility was built in 1977 in Voreppe, France. A year later, a new factory is created, to produce hyper frequency components for military use, at l'Isle d'Abeau, which was visited by France's Prime Minister on June 6, 1980.\n\n1982 was the year in which Radiall got its first Computer-Assisted-Design equipment. \nDuring the 80s, Radiall expands worldwide with offices in the United States (Stratford, Connecticut), Brazil (Rio de Janeiro) and Asia (from Hong Kong)\nIn 1984, arrived the long-awaited first qualification by Boeing for connectors that comply to ARINC 600. This is also a period of inventions, as 15 patents were obtained from 1984 to 1986. Radiall also entered the French stock market in 1989 selling 20% of its capital to the public.\n\nIn 1992, Pierre Gattaz, son of co-founder of Radiall Yvon Gattaz, becomes the new CEO of the company.\nIn order to become a major US player in the Multipins Connectors business (concerning mostly the Aerospace industry), Radiall acquired Jerrik in May 1995, located in Tempe, Arizona. Then, Radiall continued its expansion in the USA with the acquisition of Larsen - a company based in Portland, Oregon, which was specialized in antennas for military purposes. \nIn 1994 the company continues to expand in Asia with Radiall Protectron, located in Bangalore, India. Later on, Radiall creates a presence in Japan when it opened Radiall Nihon. In 2005, Radiall acquired Applied Engineering Products (AEP) located in New Haven, CT and the AEP product line was integrated into the Radiall portfolio. Finally, in July 1997, Radiall Shangai is inaugurated, which officially creates a Radiall entity in China. \nRadiall's goal was to reach sales of one billion francs by the year 2000, a goal that was actually reached three years before, in 1997. \n2001 was a tough year for the company, due to the telecommunication crisis and the September 11 aftermath, forcing the company to reduce its manpower by 30% in two years, due to a 40% activity loss.\n\nOn January 8, Radiall announced an increase in its shareholdings of the start-up D-Lightsys to 95 percent.\n\nOn June 1, Radiall AEP Inc and Radiall USA Inc were consolidated into one company Radiall USA, Inc. doing business as Radiall and covering the North American territory where Radiall currently has 3 sites: Tempe, AZ; New Haven, CT, and Obregon, Sonora, Mexico. The total number of employees is about 470.\n\n2012 saw the creation of the automobile subsidiary \"RAYDIALL\"\n\nOn July 29, 2015, Radiall announced the acquisition of VAN-SYSTEMS, an Italian company that designs and manufactures circular electrical connectors for Industrial applications.\n\nRadiall is governed by two boards, a Supervisory Board and an Executive Board. The Supervisory board is composed of 7 people, who oversee the operation of the organization. The Executive Board, includes 3 members and obtains assistance from the Supervisory Board in creating the organizations strategy and overall management of the company.\n\nRadiall is headquartered in France with expertise centers and manufacturing locations on 3 continents and 13 countries. Radiall's 9 manufacturing plants are located in the United States, Mexico, France, China and India. More than 87% of sales are generated outside France.\n\n"}
{"id": "2306852", "url": "https://en.wikipedia.org/wiki?curid=2306852", "title": "Registration pin", "text": "Registration pin\n\nA registration pin is a device intended to hold a piece of film, paper or other material in place during photographic exposure, copying or drawing.\n\nRegistration pins are used in offset printing and cartography, in order to accurately position the different films or plates for multi-color work.\n\nIn traditional, hand-drawn animation, the registration pins are often called pegs, and are attached to a peg bar.\n\nAlso, in traditional, hand-taped printed circuit board artwork, usually at two or four times actual size. Sometimes on a single transparent base, usually mylar, with Layer 1 being on the front and Layer 2 being on the back, in red and green, respectively, for later \"separation\" into component parts using a process camera.\n\nIn motion picture cameras, the pin(s) hold the film immovable during exposure.\n\nIn certain \"professional\" motion picture cameras and \"step\" printers, there may be \"two\" registration pins: one is called the \"big pin\" and it is employed for primary (axial and lateral) registration while the other one is called the \"little pin\" and it is employed for secondary (axial) registration. With the \"big pin\"/\"little pin\" concept, it is not required to employ side pressure or other means to guide the film through the intermittent movement with \"absolute\" precision as the \"big pin\" is fully fitting in the perforation (the \"little pin\" \"is not\" fully fitting in width, but \"is\" fully fitting in height; this difference accommodates slight changes in the dimensions of the film media due to changes in relative humidity and possibly other factors such as media age).\n\nThis system is employed primarily in high-end \"professional\" cameras in the West. In the East (the former Soviet Union and its former Satellites), a single registration pin, corresponding to the \"big pin\", is employed along with side pressure.\n\nAdditionally, Western \"professional\" cameras \"always\" employ Bell and Howell (BH) pins whereas Eastern \"professional\" cameras \"generally\" employ Kodak Standard (KS) pins, which standard was originally recommended by the Western standards organizations, but was soundly rejected by Western studios and camera equipment manufacturers. Western \"professional\" cameras provided to the East during World War II's Lend-Lease program were \"generally\" converted to KS pins by the receiving country.\n\nTo further improve upon registration accuracy, the perforations which are utilized for registration are \"never\" used for film advancement (i.e., for pull-down).\n\nThe above description applies to \"professional\" applications, which is generally taken to mean film gauges larger than 16mm (i.e., 35mm and 65/70mm).\n\nFor 16mm, only, a modified strategy is \"generally\" employed, at least for \"step\" printers which utilize 1R (single-row) perforations.\n\nThe lower pin, the \"big pin\", will be fully fitting in the axial \"and\" lateral dimensions but the upper pin, the \"little pin\", will be fully fitting in the lateral dimension \"only\", for the same reason that the \"professional's\" \"little pin\" is fully fitting in the axial dimension \"only\".\n\nThis, then, also accomplishes \"absolute\" precision, but within the context of \"sub-professional\" film gauges.\n\nFor practical reasons, the 1R 16mm \"little pin\" is usually spaced two perforations above the 16mm \"big pin\".\n\nAgain for 16mm, only, certain cameras and \"step\" printers which utilize 2R (two-row) perforations may employ the same strategy as for \"professional\" applications, but 2R is seldom utilized except for certain high-speed photography and almost never for duplication or prints.\n"}
{"id": "41655", "url": "https://en.wikipedia.org/wiki?curid=41655", "title": "Repeater", "text": "Repeater\n\nIn telecommunications, a repeater is an electronic device that receives a signal and retransmits it. Repeaters are used to extend transmissions so that the signal can cover longer distances or be received on the other side of an obstruction.\n\nSome types of repeaters broadcast an identical signal, but alter its method of transmission, for example, on another frequency or baud rate.\n\nThere are several different types of repeaters; a telephone repeater is an amplifier in a telephone line, an optical repeater is an optoelectronic circuit that amplifies the light beam in an optical fiber cable; and a radio repeater is a radio receiver and transmitter that retransmits a radio signal.\n\nA broadcast relay station is a repeater used in broadcast radio and television.\n\nWhen an information-bearing signal passes through a communication channel, it is progressively degraded due to loss of power. For example, when a telephone call passes through a wire telephone line, some of the power in the electric current which represents the audio signal is dissipated as heat in the resistance of the copper wire. The longer the wire is, the more power is lost, and the smaller the amplitude of the signal at the far end. So with a long enough wire the call will not be audible at the other end. Similarly, the farther from a radio station a receiver is, the weaker the radio signal, and the poorer the reception. A repeater is an electronic device in a communication channel that increases the power of a signal and retransmits it, allowing it to travel further. Since it amplifies the signal, it requires a source of electric power.\n\nThe term \"repeater\" originated with telegraphy in the 19th century, and referred to an electromechanical device (a relay) used to regenerate telegraph signals. Use of the term has continued in telephony and data communications.\n\nIn computer networking, because repeaters work with the actual physical signal, and do not attempt to interpret the data being transmitted, they operate on the physical layer, the first layer of the OSI model.\n\nThis is used to increase the range of telephone signals in a telephone line.\n\nThey are most frequently used in trunklines that carry long distance calls. In an analog telephone line consisting of a pair of wires, it consists of an amplifier circuit made of transistors which use power from a DC current source to increase the power of the alternating current audio signal on the line. Since the telephone is a duplex (bidirectional) communication system, the wire pair carries two audio signals, one going in each direction. So telephone repeaters have to be bilateral, amplifying the signal in both directions without causing feedback, which complicates their design considerably. Telephone repeaters were the first type of repeater and were some of the first applications of amplification. The development of telephone repeaters between 1900 and 1915 made long distance phone service possible. Now, most telecommunications cables are fiber optic cables which use optical repeaters (below).\n\nBefore the invention of electronic amplifiers, mechanically coupled carbon microphones were used as amplifiers in telephone repeaters. After the turn of the 20th century it was found that negative resistance mercury lamps could amplify, and they were used. The invention of audion tube repeaters around 1916 made transcontinental telephony practical. In the 1930s vacuum tube repeaters using hybrid coils became commonplace, allowing the use of thinner wires. In the 1950s negative impedance gain devices were more popular, and a transistorized version called the E6 repeater was the final major type used in the Bell System before the low cost of digital transmission made all voiceband repeaters obsolete. Frequency frogging repeaters were commonplace in frequency-division multiplexing systems from the middle to late 20th century.\n\nThis is a type of telephone repeater used in underwater submarine telecommunications cables.\n\nThis is used to increase the range of signals in a fiber optic cable. Digital information travels through a fiber optic cable in the form of short pulses of light. The light is made up of particles called photons, which can be absorbed or scattered in the fiber. An optical communications repeater usually consists of a phototransistor which converts the light pulses to an electrical signal, an amplifier to increase the power of the signal, an electronic filter which reshapes the pulses, and a laser which converts the electrical signal to light again and sends it out the other fiber. However, optical amplifiers are being developed for repeaters to amplify the light itself without the need of converting it to an electric signal first.\n\n \nThis is used to extend the range of coverage of a radio signal. The history of radio relay repeaters began in 1898 from the publication by Johann Mattausch in Austrian Journal Zeitschrift für Electrotechnik (v. 16,\n35 - 36). But his proposal \"Translator\" was primitive and not suitable for use. The first relay system with radio repeaters, which really functioned, was that invented in 1899 by Emile Guarini-Foresio.\n\nA radio repeater usually consists of a radio receiver connected to a radio transmitter. The received signal is amplified and retransmitted, often on another frequency, to provide coverage beyond the obstruction. Usage of a duplexer can allow the repeater to use one antenna for both receive and transmit at the same time.\n\nRadio repeaters improve communication coverage in systems using frequencies that typically have line-of-sight propagation. Without a repeater, these systems are limited in range by the curvature of the Earth and the blocking effect of terrain or high buildings. A repeater on a hilltop or tall building can allow stations that are out of each other's line-of-sight range to communicate reliably.\n\nRadio repeaters may also allow translation from one set of radio frequencies to another, for example to allow two different public service agencies to interoperate (say, police and fire services of a city, or neighboring police departments). They may provide links to the public switched telephone network as well. \n\nTypically a repeater station listens on one frequency, A, and transmits on a second, B. All mobile stations listen for signals on channel B and transmit on channel A. The difference between the two frequencies may be relatively small compared to the frequency of operation, say 1%. Often the repeater station will use the same antenna for transmission and reception; highly selective filters called \"duplexers\" separate the faint incoming received signal from the billions of times more powerful outbound transmitted signal. Sometimes separate transmitting and receiving locations are used, connected by a wire line or a radio link. While the repeater station is designed for simultaneous reception and transmission, mobile units need not be equipped with the bulky and costly duplexers, as they only transmit or receive at any time.\n\nMobile units in a repeater system may be provided with a \"talkaround\" channel that allows direct mobile-to-mobile operation on a single channel. This may be used if out of reach of the repeater system, or for communications not requiring the attention of all mobiles. The \"talkaround\" channel may be the repeater output frequency; the repeater will not retransmit any signals on its output frequency.\n\nAn engineered radio communication system designer will analyze the coverage area desired and select repeater locations, elevations, antennas, operating frequencies and power levels to permit a predictable level of reliable communication over the designed coverage area.\n\nRepeaters can be divided into two types depending on the type of data they handle:\n\nThis type is used in channels that transmit data in the form of an analog signal in which the voltage or current is proportional to the amplitude of the signal, as in an audio signal. They are also used in trunklines that transmit multiple signals using frequency division multiplexing (FDM). Analog repeaters are composed of a linear amplifier, and may include electronic filters to compensate for frequency and phase distortion in the line.\n\nThe digipeater is used in channels that transmit data by binary digital signals, in which the data is in the form of pulses with only two possible values, representing the binary digits 1 and 0. A digital repeater amplifies the signal, and it also may retime, resynchronize, and reshape the pulses. A repeater that performs the retiming or resynchronizing functions may be called a regenerator.\n\n\n"}
{"id": "541538", "url": "https://en.wikipedia.org/wiki?curid=541538", "title": "SPOT (satellite)", "text": "SPOT (satellite)\n\nSPOT (, lit. \"Satellite for observation of Earth\") is a commercial high-resolution optical imaging Earth observation satellite system operating from space. It is run by Spot Image, based in Toulouse, France. It was initiated by the CNES (\"Centre national d'études spatiales\" – the French space agency) in the 1970s and was developed in association with the SSTC (Belgian scientific, technical and cultural services) and the Swedish National Space Board (SNSB). It has been designed to improve the knowledge and management of the Earth by exploring the Earth's resources, detecting and forecasting phenomena involving climatology and oceanography, and monitoring human activities and natural phenomena. The SPOT system includes a series of satellites and ground control resources for satellite control and programming, image production, and distribution. Earlier satellites were launched using the European Space Agency's Ariane 2, 3, and 4 rockets, while SPOT 6 and SPOT 7 were launched by the Indian PSLV.\n\nSPOT Image is marketing the high-resolution images, which SPOT can take from every corner of the Earth.\n\nThe SPOT orbit is polar, circular, sun-synchronous, and phased. The inclination of the orbital plane combined with the rotation of the Earth around the polar axis allows the satellite to fly over any point on Earth within 26 days. The orbit has an altitude of 832 kilometers, an inclination of 98.7°, and completing 14 + 5/26 revolutions per day.\n\nSince 1986 the SPOT family of satellites has been orbiting the Earth and has already taken more than 10 million high quality images. SPOT 1 was launched with the last Ariane 1 rocket on February 22, 1986. Two days later, the 1800 kg SPOT 1 transmitted its first image with a spatial resolution of 10 or 20 meters. SPOT 2 joined SPOT 1 in orbit on January 22, 1990, on the Ariane 4 maiden flight, and SPOT 3 followed on September 26, 1993, also on an Ariane 4.\n\nThe satellite loads were identical, each including two identical HRV (High Resolution Visible) imaging instruments that were able to operate in two modes, either simultaneously or individually. The two spectral modes are panchromatic and multispectral. The panchromatic band has a resolution of 10 meters, and the three multispectral bands (G, R, NIR) have resolutions of 20 metres. They have a scene size of 3600 km and a revisit interval of one to four days, depending on the latitude.\n\nBecause the orbit of SPOT 1 was lowered in 2003, it will gradually lose altitude and break up naturally in the atmosphere. Deorbiting of SPOT 2, in accordance with IADC (Inter-Agency Space Debris Coordination Committee), commenced in mid-July 2009 for a period of two weeks, with a final burn on 29 July 2009. SPOT 3 is no longer functioning, due to problems with its stabilization system.\n\nSPOT 4 launched March 24, 1998 and stopped functioning July, 2013. In 2013, CNES lowered the altitude of SPOT 4 by 2.5 km to put it on a phased orbit with a five-day repeat cycle. On this orbit, SPOT4 was programmed to acquire a time-lapse series of images over 42 sites with a five days revisit period from February to end of May 2013. The data set it produced is aimed at helping future users of the Sentinel-2 mission to learn working with time-lapse series. The time-lapse series provided by SPOT4 (Take5) have the same repetitiveness as those that will be delivered by the Sentinel-2 satellites, starting in 2015 and 2016.\n\nSPOT 5 was launched on May 4, 2002 and has the goal to ensure continuity of services for customers and to improve the quality of data and images by anticipating changes in market requirements.\n\nSPOT 5 has two high resolution geometrical (HRG) instruments that were deduced from the HRVIR of SPOT 4. They offer a higher resolution of 2.5 to 5 meters in panchromatic mode and 10 meters in multispectral mode (20 metre on short wave infrared 1.58 – 1.75 µm). SPOT 5 also features an HRS imaging instrument operating in panchromatic mode. HRS points forward and backward of the satellite. Thus, it is able to take stereopair images almost simultaneously to map relief.\n\nSPOT 6 was launched by India's Polar Satellite Launch Vehicle on flight C21 at 04:23 UTC on 9 September 2012, while SPOT 7 was launched on PSLV flight C23 at 04:42 UTC on 30 June 2014. They form a constellation of Earth-imaging satellites designed to provide continuity of high-resolution, wide-swath data up to 2024. EADS Astrium took the decision to build this constellation in 2009 on the basis of a perceived government need for this kind of data. Spot Image, a subsidiary of Astrium, funded the satellites alone and owned the system (satellites and ground segments) at time of launch. In December 2014, SPOT 7 was sold to Azerbaijan's space agency Azercosmos, who renamed it \"Azersky\".\n\n\n"}
{"id": "46823545", "url": "https://en.wikipedia.org/wiki?curid=46823545", "title": "Salon des arts ménagers", "text": "Salon des arts ménagers\n\nThe Salon des arts ménagers (SAM; Household Arts Show) was an annual exhibition in Paris of domestic appliances, furniture and home designs. It was first held as the Salon des appareils ménagers (Home Appliances Fair) in 1923, with 100,000 visitors. By the 1950s each exhibition attracted up to 1.4 million visitors.\n\nThe SAM was run by government agencies and served an educational purpose, introducing consumers to new types of appliance and new materials. It also provided a showplace for new commercial products. The exhibition introduced modern concepts of home layout, with the kitchen moved from the back of the apartment to a central position near the entrance and near the dining and living room(s). Ergonomically designed kitchens and labor-saving devices minimized the effort required of housewives. The \"Salon\" became less relevant as department stores and specialty outlets began to offer broader ranges of products. The last exhibition was held in 1983.\n\nThe \"Salon\" was created by Jules-Louis Breton, who had been Undersecretary of State for Inventions during World War I (1914–18) and then Minister of Health and Social Welfare.\nHe was the first director of the National Board of Scientific and Industrial Research and Inventions (ORNI: \"Office national des recherches scientifiques et industrielles et des Inventions\"), created on 29 December 1922 and dissolved on 24 May 1938, predecessor of the Centre national de la recherche scientifique (CNRS).\nThe show was financed and supervised by the ORNI, and then from 1938 by the CNRS, which received the profits.\nBreton's son Paul was \"commissaire\" of the \"Salon\" from 1929 to 1976, and Paul's brother André was director of the \"Arts Ménagers\" publication.\n\nThe first \"Salon des Appareils Ménagers\" (Home Appliances Fair) was held between 18 October 1923 and 4 November 1923 in of the \"Foire de Paris\" on the Champ de Mars.\nThe first show was held in a simple hut.\nThe show gave prizes and medals to the inventors of the best domestic appliances, judged by ORNI and the Ministry of Public Education.\nThe show was a great success, with 100,000 visitors and 200 exhibitors.\nThe second \"Salon des Appareils Ménagers\" was held on the Champ de Mars on 21 October 1924 – 9 November 1924.\nThere was no \"Salon\" in 1925, but the government decided that year to make the show an annual event.\nIn 1926 the SAM was held in the Grand Palais on the Champs-Élysées, now called the \"Salon des Arts Ménagers\" (SAM; Domestic Arts Show), again showing domestic appliances but now also showing products and materials needed for their installation and organization in the house.\nThe SAM would remain at the Grand Palais until 1961.\nThe 1926 show had 145,600 visitors and 328 exhibitors.\nThe SAM began to be involved in experiments with new architecture.\nIn 1927 an official monthly magazine was launched, \"L’Art Ménager\". An annual reader's competition was organized to select the \"most beautiful cover\". \nPrizes were given at the show for a cooking competition and for the best housekeeper. There were more competitions in the \"Salons\" that followed, and more activities associated with the show including cooking lessons, concerts and conferences on furniture and decoration.\n\nIn 1930 Francis Bernard portrayed \"Marie Mécanique\" on posters for the \"Salon\", a robot housemaid who became the symbol of the \"Salon\".\nThere were 269,000 visitors in 1932.\nIn 1932 organization of the SAM was given to the \"Comité Français des Expositions\" (CFE).\nIn 1933 the SAM hosted new events, such as the Fish Fortnight.\nArchitects and decorators had formed the \"Union des Artistes Modernes\" (UAM) in 1929 to promote their views on decorative art, which sought purer harmony in place of ornamentation.\nThe UAM published a manifesto in 1934 and asked to participate in the \"Salon\".\nThe resulting home design exhibit in the \"Salon\" was organized with the help of the magazine \"L’Architecture d’aujourd’hui\" (Today's Architecture).\nThe first prize for a house for a family of two parents and three children was won by A. and E. Novello. A model was built on the ground floor.\nThe 1936 \"Salon\" presented new housing materials and their applications, such as stainless steel and fiber cement.\n\nThe numbers of visitors to the SAM rose steadily, with 410,000 in 1935, 428,000 in 1936, 487,000 in 1937 and 535,000 in 1938.\nIn 1939 there were 608,000 visitors, and the SAM covered .\nThe goals were defined as simplifying the work of housewives through invention of devices such as dishwashers, presenting new products and materials, and teaching women to use the devices. \nGas and electricity companies demonstrated how their products could replace wood and then coal, providing comfort and efficiency while saving time, effort and money.\nThe new products would transform the lives of housewives.\nMajor brands at the show included Aspiron, Auer, Berger, Calor, De Dietrich, Electrolux, Frigidaire, Hoover, Jex, Johnson and Spontex.\n\nThe SAM was suspended during World War II (1939–45). and did not resume until 1948. \nReconstruction and redevelopment of old buildings were important post-war themes, as were the search for a hygienic and comfortable lifestyle and exploration of new materials such as metal or plastic. The wide availability of gas and electricity inspired new appliances including vacuum cleaners, pressure cookers, irons, washing machines, heaters, refrigerators and hairdryers.\nHowever, many of the visitors in the first decade after the war were living in old, cramped accommodations, often without running water, gas or electricity.\nThey preferred to delay purchase of major appliances until they could move into a more modern house or apartment. They visited the \"Salon\" to learn what was available and to decide on what would be most important. A gas water heater above the sink could be a more practical aspiration than a washing machine.\n\nThe public status of the SAM helped it maintain its mission as one of social utility and education.\nTraditionally the Parisian apartment had a kitchen at the back overlooking a service yard, with food carried through a corridor to the room where the family ate. Experiment in the inter-war period led to new post-war designs in which the kitchen was moved near to the apartment's entrance, close to the living and dining room. The back stairs disappeared. Water, electricity, gas and sewage were now integrated into the design of buildings. The kitchen was relatively small, with a layout designed to let the housewife perform different tasks without moving. The SAM played an important role in introducing these innovations. In Marcel Gascoin's 8-piece \"Logis 1949\" display the kitchen played a central role and followed the ergonomic principles spelled out by Paulette Bernège in the inter-war period.\n\nThe first exhibition after the war, the 17th \"Salon\", opened on 26 February 1948.\nThe economy had not yet recovered and the goods on display could only be ordered with delays of two to six months.\nThere were 795,113 visitors.\nWhen the 18th \"Salon\" opened on 25 February 1949 products were more readily available, but prices were too high for most of the visitors.\nThe 1949 \"Salon\" had new sections such as \"Woman and Child\" and \"Gastronomy\", There were 951,139 visitors.\nA cinema news item filmed at the 1949 \"Salon\" showed the Minister of Education Yvon Delbos during his inaugural visit and gave close-ups of washing machines, dishwashers and a refrigerator. The narrative was addressed to the ladies (\"mesdames\") who would benefit from the labor-saving devices.\n\nThe 1950 exhibition had sections on Antique Arts in Modern Life, Today's Home, the \"Cité\" 50, Rural Domestic Arts, Kitchen Furnishings, Collective Living, Food, Wine, Furniture, The Room of the Woman and Child, Cleaning Products and Home Appliances.\nDuring the 1950 exhibition there were twenty-eight conferences, including \"If Women Designed Home Appliances\" by Paulette Bernège and \"Joy and Comfort through Color and Light.\"\nThe \"Formes utiles\" design group, created by the UAM in 1949, began to exhibit objects at the \"Salon\" that met the criteria of being everyday objects, handcrafted or made industrially, with good quality, pleasing forms and reasonable prices.\nThe Salon had a restaurant that featured different regional specialties each night.\n\nIn 1950 the housing section reappeared, sponsored by the Ministry of Reconstruction and Urban Development.\nThe Exposition on Habitation in the gardens included an exhibit on Le Corbusier's new apartment complex in Marseille.\nIn February 1955 a home completely built from plastic drew more than 200,000 visitors. The design was conceived by Ionel Schein, Y. Magnant and R.A. Coulon and was financed by the \"Charbonnages de France\" and \"Houillères du Nord\" coal companies, who foresaw huge possibilities in making plastics from coal.\n\nThe number of visitors to the SAM in 1953 was down from the previous year, but sales were up, particularly refrigerators and washing machines.\nVisits then rose from 1.2 million in 1953 to 1.4 million in 1956. With more than 1,200 exhibitors, a visitor could get lost in the maze.\nBy the mid-1950s consumers were more prosperous and more likely to buy, often taking advantage of newly-available credit arrangements.\nAlthough the SAM supported commerce, by arranging products by type rather than by brand it helped visitors compare features and prices. The \"Salon\" organizers tried to impose some degree of uniformity on the display stands, but there was naturally competition by the vendors to attract attention through taller and more original stands. To counteract this, all displays had to meet standards and be approved by a committee. There was some social segregation. The displays of inexpensive small items in the basement from 1956 tended to be fairly chaotic, with visitors able to pick up and examine the products, while the booths upstairs showing the larger and more expensive items were calmer and more elegant.\nConsumer attitudes changed during the 1950s, and appliances such as refrigerators and washing machines came to be seen as necessities rather than luxuries.\nThe SAM organizers played a role in the organization that set standards for appliances, defining essential, desirable and optional features, and tried to ensure that the products exhibited met these standards.\nAppliance designs in the early 1950s were often austere and functional, but from 1955 they became more colorful and diverse, so the housewife could demonstrate her good taste in selecting items that would make the kitchen more welcoming and that would showcase the family's prosperity. By the late 1950s there was a return to cleaner designs without decorative moldings, with line and angle replacing rounded forms and a return to white or neutral colors.\n\nThe volume of sales seems to have peaked in the 1955–57 period. After this the SAM became more a place where consumers or professionals came to gain information. Purchases would be made at department stores or specialty stores.\nThe \"Salon\" continued to be the key annual marketing event for consumer appliance manufacturers, dictating the rhythm of new product introduction and advertising. The Prefect of the Saine noted in 1958 that the SAM generated far more business than was transacted at the event. Many consumers bought products from retailers that they had selected at the \"Salon\".\nThe retailer would provide delivery, installation and repair services.\n\nThe 1960 \"Salon\" included exhibits of complete areas such as the kitchen, bathroom and living room.\nIn 1961 the SAM moved into larger and more modern premises in the \"Centre des nouvelles industries et technologies\" (CNIT) in La Défense.\nIt remained at the CNIT from 1961 until it was dissolved in 1983.\nIn March 1965 \"Le Figaro\" noted that dishwashers seemed to have finally become accepted after fifteen years of false starts, with about thirty brands shown at the SAM. They were still expensive items, but according to the newspaper they were much more hygienic than hand washing, more economical of water and soap and produced brighter results.\n\nThe \"Salon des Arts Ménagers\" celebrated its 50th anniversary in 1973.\nFrom 1976 it was run by the Secretary of State for Universities.\nIt was dissolved and its staff dismissed on 31 December 1983.\nIt was replaced by the PROMODO domestic appliance show, organized by and for professionals at the Parc des Expositions de Villepinte.\n\nThe \"Salon des Arts Ménagers\" published several titles including:\n\n"}
{"id": "4088800", "url": "https://en.wikipedia.org/wiki?curid=4088800", "title": "Saudi Payments Network", "text": "Saudi Payments Network\n\nThe Saudi Payments Network or mada (formerly SPAN) is the only and major payment system in the kingdom of Saudi Arabia. It connects all ATM and point of sale (POS) terminals throughout the country to a central payment switch which in turn re-routes the financial transactions to the card issuer (local bank, Visa, American Express or MasterCard). All banks in Saudi Arabia are required by the Saudi Arabian Monetary Agency (SAMA) to issue ATM cards fully compatible with the network. All services are provided to the customer free of charge, regardless of the ATM used, its operator, or the customer's card issuer.\n\nThere are now more than 6,000 ATMs and 110,000 POSs connected to the network and in 2006 more than 340 million transactions went through SPAN, with a total value of 160 billion SR (42.5 billion USD).\n\nThe network has recently been upgraded to SPAN2 which is compliant with EMV standards and implements a higher capacity infrastructure and therefore less processing time, especially at POS terminals, resolving a major problem of the first generation SPAN system.\n\n\n\n"}
{"id": "26957755", "url": "https://en.wikipedia.org/wiki?curid=26957755", "title": "Space travel using constant acceleration", "text": "Space travel using constant acceleration\n\nConstant acceleration is a proposed aspect of most future forms of space travel. It entails that the propulsion system of whatever kind operate continuously with a steady acceleration, rather than the brief impulsive thrusts used by chemical rockets—for the first half of the journey it constantly pushes the spacecraft towards its destination, and for the last half of the journey it constantly uses backthrust, so that the spaceship arrives at the destination at a standstill.\n\nConstant acceleration is notable for several reasons:\n\n\nHowever, constant acceleration is an inefficient use of fuel and energy, and is not used in existing spaceflight systems. \n\nConstant-thrust and constant-acceleration trajectories involve the spacecraft firing its engine in a prolonged constant burn. In the limiting case where the vehicle acceleration is high compared to the local gravitational acceleration, the orbit approaches a straight line. The spacecraft points straight toward the target (accounting for target motion), and remains accelerating constantly under high thrust until it reaches its target. If it is required that the spacecraft rendezvous with the target, rather than performing a flyby, then the spacecraft must flip its orientation halfway through the journey, and decelerate the rest of the way.\n\nIn the constant-thrust trajectory, the vehicle's acceleration increases during thrusting period, since the fuel use means the vehicle mass decreases. If, instead of constant thrust, the vehicle has constant acceleration, the engine thrust must decrease during the trajectory.\n\nOver interstellar distances a spaceship using significant constant acceleration will approach the speed of light, so special relativity effects (like the difference in time flow between ship time and planetary time) become important.\n\nHow far one travels, experiencing constant acceleration, from the point of view of Earth as a function of the traveler's time is expressed by the coordinate distance \"x\" as a function of proper time \"τ\" at constant proper acceleration \"a\". It is given by:\nwhere \"c\" is the speed of light.\n\nUnder the same circumstances, the time elapsed on Earth (the coordinate time) as a function of the traveler's time is given by:\n\nA major limiting factor for constant acceleration drives is having enough fuel. Imagine a horse strong enough to pull a wagon carrying enough hay to feed it on a journey from New York City to Los Angeles. Constant acceleration won't be feasible unless the specific impulse for fuel (the fuel's fuel efficiency) becomes much higher.\n\nThere are two broad categories for ways to solve this problem: one is higher efficiency fuel (the motor ship approach) and the other is drawing propulsion energy from the environment as the ship passes through it (the sailing ship approach). Two possibilities for the motor ship approach are nuclear and matter–antimatter based fuels. One possibility for the sailing ship approach is discovering something equivalent to the parallelogram of force between wind and water which allows sails to propel a sailing ship.\n\nPicking up fuel along the way—the ramjet approach—will lose efficiency as the space craft's speed increases relative to the planetary reference. This happens because the fuel must be accelerated to the spaceship's velocity before its energy can be extracted and that will cut the fuel efficiency dramatically.\n\nA related issue is drag. If the near light-speed space craft is interacting with matter or energy that is moving slowly in the planetary reference frame—solar wind, magnetic fields, cosmic microwave background radiation—this will cause drag which will bleed off a portion of the engine's acceleration.\n\nA second big issue facing ships using constant acceleration for interstellar travel is colliding with matter and radiation while en route. In mid-journey any matter the ship strikes will be impacting at near light speed, so the impact will be dramatic.\n\nIf a space ship is using constant acceleration over interstellar distances, it will approach the speed of light for the middle part of its journey when viewed from the planetary frame of reference. This means that the interesting effects of relativity will become important. The most important effect is that time will appear to pass at different rates in the ship frame and the planetary frame, and this means that the ship's speed and journey time will appear different in the two frames.\n\nFrom the planetary frame of reference, the ship's speed will appear to be limited by the speed of light—it can approach the speed of light, but never reach it. If a ship is using 1 \"g\" constant acceleration, it will appear to get near the speed of light in about a year, and have traveled about half a light year in distance. For the middle of the journey the ship's speed will be roughly the speed of light, and it will slow down again to zero over a year at the end of the journey.\n\nAs a rule of thumb, for a constant acceleration at one \"g\" (Earth gravity), the ship journey time will be the distance in light years to the destination, plus one year. This rule of thumb will give answers that are slightly shorter than the exact calculated answer, but reasonably accurate.\n\nFrom the frame of reference of those on the ship the acceleration will not change as the journey goes on. Instead the planetary reference frame will look more and more relativistic. This means that for voyagers on the ship the journey will appear to be much shorter than what planetary observers see.\n\nAt a constant acceleration of 1 \"g\", a rocket could travel the diameter of our galaxy in about 12 years ship time, and about 113,000 years planetary time. If the last half of the trip involves deceleration at 1 \"g\", the trip would take about 24 years. If the trip is merely to the nearest star, with deceleration the last half of the way, it would take 3.6 years.\n\nThis is a half-myth because it depends on the frame of reference. It is true for those watching from the planetary reference frame. For those experiencing the journey (in the ship's reference frame) it is not true. For both the planetary frame and the ship's reference frame, the ship will change speed in a Newtonian way—push it a little and it speeds up a little, push it a lot and it speeds up a lot. However, in the planetary frame the acceleration will reduce, due to the speed of light being the maximum speed of material objects.\n\nFrom the ship's frame, the acceleration would continue at the same rate. However, due to Lorentz contraction, the galaxy around the ship would appear to become squashed in the direction of travel, and a destination many light years away would appear to become much closer. Traveling to this destination at subluminal speeds would become practical for the onboard travellers. Ultimately, from the ship's frame, it would be possible to reach anywhere in the observable universe, without the ship ever accelerating to light speed.\n\n\"Tau Zero\", a hard science fiction novel by Poul Anderson, has a spaceship using a constant acceleration drive.\n\nThe spacecraft of George O. Smith's Venus Equilateral stories are all constant acceleration ships. Normal acceleration is 1 \"g\", but in \"The External Triangle\" it is mentioned that accelerations of up to 5 \"g\" are possible if the crew is drugged with gravanol to counteract the effects of the \"g\"-load.\n\nSpacecraft in Joe Haldeman's novel \"The Forever War\" make extensive use of constant acceleration; they require elaborate safety equipment to keep their occupants alive at high acceleration (up to 25 \"g\"), and accelerate at 1 \"g\" even when \"at rest\" to provide humans with a comfortable level of gravity.\n\nIn the \"Known Space\" Universe, constructed by Larry Niven, Earth uses constant acceleration drives in the form of Bussard ramjets to help colonize the nearest planetary systems. In the non-Known Space novel \"A World Out of Time\", Jerome Branch Corbell (for himself), 'takes' a ramjet to the Galactic Center and back in 150 years ships time (most of it in cold sleep), but 3 million years passes on Earth.\n\nIn \"The Sparrow\", by Mary Doria Russell, interstellar travel is achieved by converting a small asteroid into a constant acceleration spacecraft. Force is applied by ion engines fed with material mined from the asteroid itself.\n\nIn the Revelation Space series by Alastair Reynolds, interstellar commerce depends upon \"lighthugger\" starships which can accelerate indefinitely at 1 \"g\". The effects of relativistic travel are an important plot point in several stories, informing the psychologies and politics of the lighthuggers' \"ultranaut\" crews for example.\n\nIn the novel \"\" by Arthur C. Clarke, the spaceship \"Universe\", using a muon-catalyzed fusion rocket, is capable of constant acceleration at 0.2 \"g\" under full thrust.\n\nThe UET and Hidden Worlds spaceships of F.M. Busby's Rissa Kerguelen Saga utilize a constant acceleration drive that can accelerate at 1 \"g\" or even a little more.\n\nShips in the Expanse series by James S. A. Corey make use of constant acceleration drives, which also provide artificial gravity for the occupants.\n"}
{"id": "9072691", "url": "https://en.wikipedia.org/wiki?curid=9072691", "title": "Spatial decision support system", "text": "Spatial decision support system\n\nA spatial decision support system (SDSS) is an interactive, computer-based system designed to assist in decision making while solving a semi-structured spatial problem. It is designed to assist the spatial planner with guidance in making land use decisions. A system which models decisions could be used to help identify the most effective decision path.\n\nAn SDSS is sometimes referred to as a policy support system, and comprises a decision support system (DSS) and a geographic information system (GIS). This entails use of a database management system (DBMS), which holds and handles the geographical data; a library of potential models that can be used to forecast the possible outcomes of decisions; and an interface to aid the users interaction with the computer system and to assist in analysis of outcomes.\n\nAn SDSS usually exists in the form of a computer model or collection of interlinked computer models, including a land use model. Although various techniques are available to simulate land use dynamics, two types are particularly suitable for SDSS. These are cellular automata (CA) based models and Agent based models (ABM).\n\nAn SDSS typically uses a variety of spatial and nonspatial information, like data on land use, transportation, water management, demographics, agriculture, climate, epidemiology, resource management or employment. By using two or more known points in history the models can be calibrated and then projections into the future can be made to analyze different spatial policy options. Using these techniques spatial planners can investigate the effects of different scenarios, and provide information to make informed decisions. To allow the user to easily adapt the system to deal with possible intervention possibilities an interface allows for simple modification to be made.\n\n"}
{"id": "34731548", "url": "https://en.wikipedia.org/wiki?curid=34731548", "title": "TRVL", "text": "TRVL\n\nTRVL is a peer-to-peer travel booking platform that gives travelers access to the commissions and discounts of the travel industry world. It aims to facilitate and reward travelers for helping each other researching and booking trips. \n\nTRVL's slogan is \"Power to the traveler\".\n\nCo-founded in the Netherlands by entrepreneur Jochem Wijnands and design specialist Michel Elings, TRVL first started as a digital publication exclusively for iPad in 2010; it was the first iPad-specific magazine in the world. \n\nOut of frustration with Adobe's publishing software, the TRVL team developed Prss, a mobile-first publishing platform. The Prss team and IP was acquired by Apple in 2014 to help create Apple News and the Apple News Format.\n\nIn 2015 Wijnands left Apple to turn TRVL into a travel booking platform where anyone can help anyone with travel recommendations while earning money. \n\nThe platform takes a sharing-economy approach to booking travel. The traveler does not pay extra to use the service.\n\n"}
{"id": "51348175", "url": "https://en.wikipedia.org/wiki?curid=51348175", "title": "Technology Development Board", "text": "Technology Development Board\n\nTechnology Development Board (TDB), is a statutory body established by Government of India under Technology Development Board Act, 1995, to promote development and commercialisation of indigenous technology and adaptation of imported technology for wider application. The board consists of 11 Board members. The Government reconstituted the Board in March 2000. The Board plays a pro-active role by encouraging enterprises to take up technology oriented products. Provides equity capital or loans to industrial concerns and financial assistance to research and development institutions. The loan carries a simple interest rate of 5% per annum.\n\nIn order to stimulate private sector's investment in R&D, TDB established Global Innovation & Technology Alliance (GITA) as a Section 25 company in a joint venture between CII and TDB with equity contribution of 51:49 respectively.\n\nThe GITA will assist DST in implementing industrial research and development programme with different countries under bilateral and multilateral science and technology cooperation agreements. In these country-specific programmes, if one industry from India and one industry from another country proposed jointly to do R&D for developing a marketable product, both the governments will provide financial support up to 50% of project cost to their respective industries.\n\nThe GITA has been envisaged as an industry-driven body for supporting competitive innovation clusters which in future can be entrusted with administering of Innovation Fund under a PPP model, IP acquisition by the government for non-exclusive licensing for public and social good, sectors of R&D and promotion of innovation culture in centres of excellence.\n\nIn India UTI was the first company to start venture scheme under the name of India Technology Venture Scheme in 1997. In 1999 UTI's executive trustee met the then chairperson, TDB and secretary, TDB and requested TDB's participation in venture capital fund. The board in its 13th meeting dated 19 November 1999 approved TDB's participation in UTI-India Technology Venture Unit Scheme (ITVUS) with commitment of Rs. 25 Cr. This was first commitment by TDB in any venture capital fund, and the agreement for this was signed on 6 July 2000. Thereafter on a case-by-case basis TDB' board took decision to participate in the different venture funds (as given in the list as annexure) as limited liability partner.\n\nSo far TDB has been participating in venture funds which are mainly concentrated in technology orientation, early stage projects and also investment in state-level funds where TDB's presence was inadequate. The initiative of TDB has also given confidence to venture capitalist/private equity funds to come up in big way to support the technology based projects with a pronounced emphasis on sectors witch which are growth drivers of Indian economy.\n\nThe TDB has so far supported 12 venture capital funds with total commitment of Rs. 310 Cr. leveraging total funds aggregating to Rs. 2713 Cr. from other investors.\n\nThe board instituted a national award for successful commercialisation of indigenous technology by an industrial concern. The national award of Rs. 10 Lakhs is shared equally between the industrial concern that has successfully commercialised the indigenous technology and the developer/provider of such technology.\n\nIn August 2000, TDB introduced a cash award of Rs. 2 lakh and a trophy to a SSI unit that has successfully commercialised a technology-based product. The first SSI award was given on 11 May 2001 and thereafter it has been decided to give the Award for SSI Unit every year on the Occasion of Technology Day, i.e. 11 May. The cash awards were later revised to Rs. 5 lakh in 2011–2012.\n"}
{"id": "55370978", "url": "https://en.wikipedia.org/wiki?curid=55370978", "title": "Tianchi basin", "text": "Tianchi basin\n\nAs precipitation was for important agriculture and food production, the Song Chinese mathematician and inventor Qin Jiushao developed a precipitation gauge that was widely used in 1247 during the Southern Song dynasty to gather meteorological data. Qin Jiushao later records application of rainfall measurements in the mathematical treatise \"Mathematical Treatise in Nine Sections\". The book also discusses problems using large snow gauges made from bamboo situated in mountain passes and uplands which are speculated to be first referenced to snow measurement. \nTianchi basins were installed at provincial and district capitals and bamboo snow gauges were situated in mountain passes. The rain gauges were conical or barrel-shaped with one being installed at each provincial and district capital in China. In the treatise, Qin Jiushao also discusses how point measurements were converted to real averages. These averages were important as they postulated indicators of natural disasters such as flooding, since river flooding has always been a problem in China.\n"}
{"id": "23623196", "url": "https://en.wikipedia.org/wiki?curid=23623196", "title": "VMQ", "text": "VMQ\n\nThe Virtual Machine Queue (VMQ) is a hardware virtualization technology for the efficient transfer of network traffic (such as TCP/IP, iSCSI or FCoE) to a virtualized host OS. VMQ technology was patented in 2010 by Daniel Baumberger of Intel Corp. A VMQ capable NIC can use DMA to transfer all incoming frames that should be routed to a receive queue to the receive buffers that are allocated for that queue. The miniport driver can indicate all of the frames that are in a receive queue in one receive indication call.\n\nThe VMQ interface supports:\n\n\nThe NDIS virtual machine queue (VMQ) architecture provides advantages for virtualization such as:\n\n\nSome networks recommend disabling VMQ. They state this option is prone to misconfiguration and can cause reduced network performance when enabled.\n"}
{"id": "1022998", "url": "https://en.wikipedia.org/wiki?curid=1022998", "title": "View-Master", "text": "View-Master\n\nView-Master is the trademark name of a line of special-format stereoscopes and corresponding View-Master \"reels\", which are thin cardboard disks containing seven stereoscopic 3-D pairs of small transparent color photographs on film. It was manufactured and sold by Sawyer's.\n\nThe View-Master system was introduced in 1939, four years after the advent of Kodachrome color film made the use of small high-quality photographic color images practical. Tourist attraction and travel views predominated in View-Master's early lists of available reels, most of which were meant to be interesting to users of all ages. Most current View-Master reels are intended for children.\n\nEdwin Eugene Mayer worked as a pharmacist at Owl Drug store in downtown Portland, Oregon, after serving in the U.S. Army in World War I. He built up a photo-finishing business there, and bought into Sawyer's Photo Finishing Service in 1919 with the help of his father August Mayer, his fiancée Eva McAnulty, and her sister Vi McAnulty.\n\nEdwin described how he started the business in a letter dated April 1, 1954: \"Suffice to say that in 1919, what little it was, was purchased with borrowed ($3,500) money from Dad, aided by about $1,600 in insurance money Eva got when her father died and which was left in permanently, and $1,600 borrowed from Vi and repaid, along with Dad's notes, within a few years.\"\n\nAs the business grew, Ed Mayer incorporated in about 1926, taking on partners Harold and Beulth F. Graves, Thomas and Pauline Meyer, and Augusta and Raymond F. Kelly, renaming the business Sawyer Service, Inc. The company relocated to a large two-story building at 181 Ella St., near Morrison Street in Portland, Oregon.\n\nThe company was producing photographic postcards and album sets as souvenirs by 1926, when Harold Graves joined Sawyer's. Graves handled marketing for the products while Mayer ran the business. Later, photographic greeting cards were added to the Sawyer's product line, marketed to major department stores. Sawyer's was the nation's largest producer of scenic postcards in the 1920s and the future View-Master viewer eventually became an extension of the two-dimensional cards.\nThe company took the first steps towards developing the View-Master after Edwin Mayer and Graves met with William Gruber, an organ maker of German origin trained by Welte & Sons and an avid photographer living in Portland. Mayer and Gruber had both developed devices for viewing stereo images, but Gruber had made up a stereo imaging rig out of two Kodak Bantam Specials mounted together on a tripod. He designed a machine that mounted the tiny pieces of Kodachrome color transparency film into reels made from heavy paper stock. A special viewer was also designed and produced. He had the idea of updating the old-fashioned stereoscope by using the new Kodachrome 16-mm color film, which recently had become available.\n\nA View-Master reel holds 14 film transparencies in seven pairs, making up the seven stereoscopic images. The components of each pair are viewed simultaneously, one by each eye, thus simulating binocular depth perception.\n\nAccording to a 1960 court document, the Gruber-Sawyer partner venture began from that first meeting in 1938. Thereafter, Ed Mayer negotiated with Gruber while production methods and some marketing were developed. After three years, a formal agreement was entered into on February 24, 1942, between Gruber and Sawyer partners, doing business as Sawyer's.\n\nEd Mayer and people within the Sawyer’s organization were uncertain what to call their new product, but they eventually came up with the name \"View-Master.\" The View-Master brand name eventually came to be recognized by 65 percent of the world’s population, but William Gruber disliked the name which Mayer gave it, thinking that it sounded too much like Toast-Master, Mix-Master, or some other kitchen appliance.\n\nThe View-Master was introduced at the 1939 New York World's Fair, marked \"Patent Applied For\". It was intended as an alternative to the scenic postcard, and was originally sold at photography shops, stationery stores, and scenic-attraction gift shops. The main subjects of View-Master reels were Carlsbad Caverns and the Grand Canyon.\n\nThe View-Master was marketed through Ed Mayer's photo-finishing, postcard, and greeting card company Sawyer's Service, Inc., known eventually as Sawyer's, Inc. The partnership led to the retail sales of View-Master viewers and reels. The patent on the viewing device was issued in 1940, on what came to be called the Model A viewer. Within a very short time, the View-Master took over the postcard business at Sawyer's.\n\nEd Mayer gave details of the company's expansion in a letter dated April 1, 1954:\n\nIn 1939, 20 years after starting the business, we had, by dint of hard work and long hours and frugal living, accumulated a business (Sawyer's) worth about $58,000.00 and Western Photo Supply Co. owning the buildings, worth about $30,000.00. The above figures were for the total business and buildings owned by the Kellys, Graves, Mayers and Meyers. In 1946, we had already grown a lot from 1939, and Sawyer's made a lease with Western Photo Supply Co., they to build and lease two new buildings to Sawyer's, in addition to the two we already had. At this point, Sawyer's also decided to change its structure from a partnership to a corporation, for various good reasons, one of which was to permit our children to participate in the stock ownership.\n\nIn the 1940s, the United States military recognized the potential for using View-Master products for personnel training, purchasing 100,000 viewers and nearly six million reels from 1942 to the end of World War II in 1945.\n\nAfter the development of the View-Master, Sawyer's, Inc. moved into a new building at 735 S.W. 20th Place in downtown Portland. The company also occupied a building next door at 740 S.W. 21st Avenue. Years later, Edwin Mayer and his Sawyer's partners purchased land in Washington County near Progress, Oregon, west of Beaverton, and built a large plant there in about 1951. The plant has since been removed and developed into a shopping center. After moving to the new plant, Mayer leased the old Sawyer's building on 20th Place to Oregon Television, Inc., KPTV, Channel 12.\n\nIn 1951, Sawyer's purchased Tru-Vue, the main competitor of View-Master. The take-over eliminated the main rival and also gained Tru-Vue's licensing rights to Walt Disney Studios. Sawyer's capitalized on the opportunity and produced numerous reels featuring Disney characters. The takeover paid off further in 1955 with reels of the newly opened Disneyland.\nIn 1952, Sawyer's began its View-Master Personal line, which included the View-Master Personal Stereo Camera for its users to make their own View-Master reels. It was successful at first, but the line was discontinued in ten years. This line spawned the Model D viewer, View-Master's highest-quality viewer which was available until the early 1970s, and the Stereomatic 500, View-Master's only 3-D projector. The other projectors were 2-D and used only one of the images.\n\nThe Model E was introduced in 1955 with a more modern design, big ivory buttons on the picture changer levers, and a large \"V\" slot on top for easier reel insertion. It was released in brown and black in the United States, and some other colors elsewhere. It was about 4 inches high, 5 inches wide, and 4 inches deep.\n\nThe Model F was introduced in 1958. It used C-cell batteries to power an internal lighting source. Industrial designer Charles \"Chuck\" Harrison led the team designing the Model F View-Master. Fifty years later in 2008, Harrison won the Cooper-Hewitt Lifetime Achievement Award.\n\nIn 1962, the Bakelite models were replaced with lighter plastic versions, the first of which was the Model G. This change was driven by Sawyer's new president, Bob Brost, who took over in 1959. The View-Master had been constructed originally from Kodak Tenite plastic and then Bakelite, a hard, sturdy, somewhat heavy plastic. The lightweight thermoplastic became the material of choice under Brost.\n\nIn 1966, Sawyer's was acquired by the General Aniline & Film (GAF) Corporation, and became a wholly owned subsidiary. Under GAF's ownership, View-Master reels began to feature fewer scenic and more child-friendly subjects, such as toys and cartoons. Television series were featured on View-Master reels, such as \"Doctor Who\", \"Rowan & Martin's Laugh-In\", \"\", \"The Man from U.N.C.L.E.\", \"Family Affair\", \"Here's Lucy\", and \"The Beverly Hillbillies\". Actor Henry Fonda appeared in a series of TV commercials for the GAF View-Master.\n\nFrom 1970 to around 1997, there were versions of \"Talking View-Masters\", which included audio technology with the reels with three major designs with increasing sophistication. In the early 1970s, GAF introduced the View-Master Rear Screen projector, a table-top projector that displayed images from picture wheels.\n\nIn 1980, View-Master released the Show Beam Projector, a toy that combined the company’s stereoscopic images and flashlight technology to produce a portable hand-held projector. The Show Beam used small film cartridges that were plugged into the side of the toy. Each cartridge contained 30 full-color 2D images.\n\nIn 1981, GAF sold View-Master to a group of investors headed by Arnold Thaler, and the company was reconstituted as the View-Master International Group.\nVMI acquired the Ideal Toy Company in 1984 and became known as the View-Master Ideal Group; V-M Ideal in turn was purchased by Tyco Toys in 1989.\nTyco, including the View-Master Ideal Group, merged with Mattel Inc. in 1997. V-M was placed organizationally in Mattel's pre-school division and is now marketed under the Fisher-Price imprint, who continues emphasis on juvenile content.\n\nIn 1998, during the purchase of the Tyco Toys-owned plant by Mattel, EPA investigations began on View-Master factory supply well for the toxic chemical trichloroethylene (TCE). The plant was shut down in 2001.\n\nIn March 2009, the Fisher-Price division of toy maker Mattel announced that they had stopped production in December 2008 of the scenic reels depicting tourist attractions. These reels of picturesque scenes and landscape scenery were descendants of the first View-Master reels sold in 1939. Fisher-Price announced they would continue to produce reels of animated characters. In late 2009, Alpha-cine announced it would take-up the scenic reel production under an agreement with Fisher-Price.\n\nIn February 2015, Mattel announced a collaboration with Google to produce a new version of View-Master called the View-Master Virtual Realty Viewer, based on virtual reality using smartphones. The new View-Master is an implementation of the Google Cardboard VR platform, and is accompanied by a mobile app that was built using its SDK. Content is displayed on a smartphone screen; the phone itself is inserted into the back of the unit. Instead of being inserted directly into the View-Master, reels are scanned using an augmented reality interface which enables access to content from the reel, such as 360-degree panoramas, 3D models, and minigames. A second iteration, the View-Master Viewer DLX was released in fall, 2016.\n\nThere have been some 25 viewer models, thousands of titles, and 1.5 billion copies of reels. The basic design remained intact for reels and internal mechanisms, despite its long history and many changes in models and materials, ensuring that every reel will work in every model.\n\nView-Master is part of the National Toy Hall of Fame of the United States.\n\nReels have been produced for Disneyland, many TV shows (such as \"The Flying Nun\", \"Lost In Space\", and \"The Munsters\", blockbuster movies (such as \"The Poseidon Adventure\", \"E.T. the Extra-Terrestrial\", and \"Jurassic Park\"), and the U.S. military (for airplane and ship identification and range estimation).\n\nDavid L. Bassett, an expert on anatomy and dissection, collaborated with Gruber to create a 25-volume atlas of human anatomy using the View-Master system.\n\nView-Master produced custom reels for commercial customers to show 3-D images of products and services to potential clients. For example, in the early 1990s, Canadian restaurant chain East Side Mario's used a View-Master reel as their dessert menu.\n\nAmong the newest View-Master products are a Discovery Channel View-Master, the new Virtual Viewer, the Discovery Channel View-Master Projector and Telescope, and the View-Master 3-D Pocket Viewer, which feature images of popular performers in concert and backstage.\n\n"}
{"id": "4595925", "url": "https://en.wikipedia.org/wiki?curid=4595925", "title": "W49", "text": "W49\n\nThe W49 was an American thermonuclear warhead, used on the Thor, Atlas, Jupiter, and Titan I ballistic missile systems. W49 warheads were manufactured starting in 1958 and were in service until 1963, with a few warheads being retained until 1975.\n\nThe W49 has been described as a design derivative of the B28 nuclear bomb nuclear warhead design. It was in diameter and long depending on model, weighing , and had a design yield of 1.44 megatons.\n\n\n"}
{"id": "51020709", "url": "https://en.wikipedia.org/wiki?curid=51020709", "title": "Wilhelm Fenner", "text": "Wilhelm Fenner\n\nWilhelm Fenner (* 14 April 1891 in Saint Petersburg † after 1946) was a German cryptanalyst, before and during the time of World War II in the OKW/Chi, the Cipher Department of the High Command of the Wehrmacht, working within the main cryptanalysis group, and entrusted with deciphering enemy message traffic (Cryptography). Wilhelm Fenner was considered an excellent organizer, an anti-Nazi, an anti-Bolshevik and a confirmed Protestant and was known by colleagues as someone who was keen to continue working in cryptology after World War II. To quote military historian David Alvarez:\n\nWilhelm was born on April 14, 1891 in Saint Petersburg. He was the sixth of seven children of Heinrich Gottlieb Fenner and Charlotte Georgine Fenner, born Michaelsen. His father was the chief editor of the St. Petersburgische Zeitung, a German language daily newspaper published in Saint Petersburg, then the capital of the Russian Empire. The sixth of seven children, he was home schooled for two years before he attended the Evangelical Lutheran Anne School in St. Petersburg from 1899, and completed his final examination with distinction in May 1909. In the autumn of 1910 he matriculated at the Royal Institute of Technology in Berlin (TH) in Berlin-Charlottenburg and studied construction engineering, chemistry and metallurgy. In the summer of 1914, he passed his final examination.\n\nWilhelm Fenner was married on 11 January 1922 to Elise Sophie Katharine von Blanckensee, a daughter of the former Prussian Major General . They had two children, a son, Siegwart Heinrich (born January 28, 1923), who served as a lieutenant in World War II, and was killed on February 19, 1945, and a daughter, Ilse Fredericki (born July 24, 1928).\n\nWith the outbreak of the First World War, he had to drop out of college, and worked for a short time at Siemens as an engineer, working to develop electrical systems for warships. He was drafted into military service on December 1, 1914, joining the 5th Guards Grenadiers, seeing service in Russia, France and Serbia and eventually joined the staff of the Tenth Army. After the armistice in November 1918, he remained under arms as a member of one of the \"Free Corps\" battalions of demobilized soldiers who offered their services to political groups jockeying for political position in Germany in the first months of peace. After the war, now with the rank of lieutenant, he remained in the military until February 9, 1920. When Fenner returned to civilian life, he took a job as a publicist with an èmigré assistance organization, but the position provided little interest and within the year, he took a position as an editor in a new press agency, which was founded by Konstantin von Krusenstern, who was a former colonel in the Imperial Russian Army. The fledgling news agency collapsed, with Krusenstern relocating to Paris, but before he left, Fenner's career took a decisive turn when the Russian colonel introduced him to Peter Novopaschenny in the spring of 1921.\n\nPeter Novopashenny was a former Russian Navy captain of the tsarist Marine and professor of Applied Tactics. Novopaschenny asked Fenner to help him to move to Berlin and confided in Fenner that he had worked during the war as director of the Russian cryptanalytic service working to break the ciphers of the German Baltic Fleet, and that he intended to make his experience available to the German General Staff, under the terms of the Treaty of Versailles. In the same year, Fenner provided him with contacts, eventually becoming an acquaintance of an officer called Erich Buschenhagen, who would later become a general in the Wehrmacht. In 1919, Buschenhagen had set up \"Volunteer Evaluation Post\" which had been absorbed into Germany's postwar military establishment as a cipher bureau of the army's Troops Department, which was allowed under the terms of the peace agreement.\n\nAs Buschenhagen's cipher bureau was primary working on Russian ciphers, he jumped at the change of using Novopaschenny, but less so Fenner, who knew little of Russian ciphers. Fenner became interested in the Russian telegrams that Novopaschenny was working on, and with his interest piqued, began the process of discovering the field of cryptanalysis. He was now working under the guidance of his \"teacher\", Novopaschenny, a fruitful relationship, and together they were successful in breaking Russian military ciphers. Fenner's excellent command of the German and Russian languages worked to his advantage, while Novopaschenny, although an excellent cryptanalyst, spoke hardly any German. Buschenhagen was particularly impressed when the pair decrypted a Russian cipher that was beyond the skill of the cipher bureau's own 4 person Russian desk. In the autumn of 1922, he and Novopaschenny were not only officially taken into the employment of the cryptanalysis group of the Reichswehr (Chi-point) by Buschenhagen, but Fenner was also appointed as head of the cryptanalysis section and assigned a staff of eleven. Fenner was initially unimpressed by the cipher bureau personnel, output was modest with too narrow scope of operation, and cryptanalysts happy to decrypt three or four messages a day combined with lax work habits. Within the next years Fenner changed the operation of his group significantly. From a chaotic assembly of creative \"geniuses\", he formed an analytical, systematic and disciplined unit of more, and more experienced, code breakers. In this process, Fenner introduced a uniform, clear technical terminology into the field of cryptanalysis, laying the groundwork for further successes by his new employer. His first action was introducing formal training, conducted personal lectures describing various types of cryptographic systems. His actions bore fruit, and the number of successfully decrypted messages increased steadily. The group also grew in numbers, and Fenner took the opportunity to train newcomers himself and transfer his own, now greatly developed, knowledge of cryptology to them. However, his growing leadership role caused him to start losing contact with the actual cryptanalytic work, and gained a reputation as a fearsome, arrogant pedant, which he dismissed as evidence the unit was being shaken out of its lethargy. By the mid 1920s, he had introduced a 90 training course for senior analysts which reviewed current systems and general principles. By the 1930s a fast track training course was introduced for talented newcomers that met twice a week and covered a two part course in cryptologic principles and practice.\n\nAs part of his process to professionalize the unit, he sought to gain recognition and establish career level status for his staff, as he was convinced that an staff member would serve the state loyally, if their well-being was undertaken by the state. As the arcane skills of his staff did not fit in any of the German career hierarchy, they were not considered career civil servants and so had no career progression, pensions nor job security. The process was long and arduous, beginning in the early 1920s and was fully completed until late 1939, with the outbreak of World War II and shortly after establishment of the Supreme Command of the Armed Forces the Chiffrierstelle was renamed the \"Cipher Department of the High Command of the Wehrmacht\" (OKW/Chi), that the plan was approved by the chief of the Supreme Command of the Wehrmacht, Wilhelm Keitel. Professionalism for Fenner was always a means to an end. The whole process was about improved production and an expansion of operations.\n\nOn April 4, 1927 he was appointed a Government Councillor (\"Regierungsrat\"). In addition to his management duties he increased contact and cooperation with friendly foreign groups in Austria, Hungary and Finland, and later Italy, Spain and Estonia He provided training, and wrote two treatises on cryptanalysis, namely \"Grundlagen der Entzifferung\" (fundamentals of code-breaking) and \"Beitrag zur Theorie der Schieber\" (contribution to the theory of the strip cipher). He also worked on the Enigma machine, then already in trial use by the Reichswehr, pointing out cryptographic weaknesses, and making proposals for its improvement. By 1924, Fenner had identified in the intercept traffic, 85 codes and ciphers, of which 22 has been attached and 16 had been cryptanalysed successfully. In June 1927, Fenner went to Finland to deliver a course of instruction to the members of the new fledgling service of the Finish cipher bureau.\n\nPers Z S was the civilian Signal Intelligence Agency of the German Foreign Office. Pers Z S believed it has a monopoly on diplomatic communication traffic and was also a rival to Reichswehr Cipher bureau. It dealt specifically with diplomatic ciphers and relied on the German Post Office for diplomatic intercepts, who transmitted and received the encrypted communications from embassies located in Germany, and across the world. The post service routinely retained copies of all embassy messages and passed the messages to Per Z S cryptanalysts for study and the Post Office only dealt with the Foreign Ministry. Fenner argued that OKW/Chi needed diplomatic traffic as they included information of military interest and military analysts were more likely to identify such items in messages and understand their significance. He convinced his opposite number in the foreign office ministry Kurt Selchow, that cooperation would benefit both units. Specifically, he promised Selchow that as the OKW/Chi expanded and improved its radio monitoring network, the foreign ministry cryptanalysts would have access to all diplomatic messages intercepted. Fenner also offered access to share other OKW/Chi materials and results. The result of the inter-agency collaboration was considered rare in a normally hostile and competitive German intelligence community.\n\nWithin the Reichswehr bureau, a small circle of Nazi sympathizers became increasingly vocal. Fenner had little patience with these people, who talked politics during extended coffee breaks and who flaunted their loyalties by smoking Nazi Party cigarettes, or occasionally missed work to participate in a \"Jew raid\". After the seizure of power by the Nazis in January 1933, the times became increasingly restless, and the situation worsened for the cipher bureau, which now felt the competition from newly established rival institutions. In 1933, for example, Goering's new Luftwaffe created its own intelligence agency, the Forschungsamt. During this period Fenner worried about the professionalism of the agency as the Nazi Party tried to extend their control over the intelligence and security agencies of the state.\n\nMany capable employees of the cipher bureau joined the new organisation, where they hoped for better career prospects. Among these were cryptanalysts whom Fenner considered excellent, their leader Baron Emil Freiherr von Reznicek, an ardent Nazi and cryptanalyst assigned to the Italian desk and Herr Weachter. Reznicek was particularly annoyed that in the caste conscious world of German bureaucracy, he was a mere \"employee\" of the Bureau, while Fenner was a Government Councillor with a pensioner. Gottfried Schapper, a radio intelligence operator from World War I, who ran a unit in the Bureau that was concerned with the location and construction of fixed intercept stations. Hans Schimpf was another. Fenner may have been glad to see the malcontents go, but he quickly discovered he was not entirely rid of them. From either a desire to revenge themselves on their former employer or to expand their influence, Schimpf and Schapper informed Fenner that the Forschungsamt was now solely responsible for all diplomatic cryptanalysis and that the Bureau should abandon all such work. This was of course a repeat of the last bureaucratic battle with the Pers Z S. However, Fenner found an ally in Kurt Selchow, who realized that if successfully prosecuted, the Forschungsamt's supposed monopoly on diplomatic cryptanalysis would mean the end of the Foreign Ministry. Even though Fenner managed to see off the threat, the Forschungsamt staff started a campaign to have him relieved of duty, tapped his office phone, planted an informant in his office and started spreading rumours that he was Jewish, and that he had ridiculed and criticized Hermann Göring in private conversations. Fenner was forced to fill the resulting gaps with newcomers, necessarily having to forgo experience in favour of attitude in selection of candidates. Fenner as a rule always recruited people who had not left the church and were not members of the NSDAP. \nDuring this time, Fenner's Bureau found it increasingly difficult to fend of competitors, with the German navies organization B-Dienst and the German armies Intercept Control Station both cipher agencies. Fenner actively opposed the army bureau, but could not block its creation. OKW/Chi was explicitly ordered to cease work on Russian military ciphers, which Fenner disobeyed ordering a small covert party to continue working on Russian and French military systems. In the summer of 1933, he was promoted to Senior Councillor (). By the start of World War II, he was a Director of OKW/CHI () he could counteract the harmful poaching of staff, ensure suitable staff had a secure future, and place unsuitable people at the disposal of others.\n\nThe staff strength of the Chiffrierstelle (OKW/Chi) had increased in 1939 to include over 200 employees, whereas only two years earlier there were about 40 personnel. In the ensuing war years until 1944 it quadrupled to 800. The wartime for Fenner and his staff in the OKW/Chi was initially relatively peaceful and successful. The raw material, in the form of intercepted radio messages, was prolific; they concentrated on several hundred messages a day on the most important projects and had to ignore relatively unimportant sources. They had important decoding success (), for example, against France, in 1940, which aided the quick victory of the German Armed Forces in the West—\"Case Red\". Also Polish, Russian and Yugoslav messages could be deciphered.\n\nWhile his section chief, Colonel Kettler, and the head of the main group A, Major Mettig, and also one of its best people, the head of the group IV, Dr Erich Hüttenhain, relocated to the north, Fenner fled south with a selection of employees. On April 23, 1945, OKW/Chi was officially disbanded and the staff of the General of Nachrichtenaufklärung (gDNA) assumed their responsibilities. Just before the American army reached their position (about 40 km south of Salzburg), documents were burned or thrown into the Salzach River. With the surrender of the Wehrmacht on 8 May 1945, Fenner moved to Landshut and found employment as a bicycle and car mechanic in neighboring Straubing.\n\nIn July 1946, Fenner was charged as a witness for the Nuremberg war crimes tribunal and in August transferred to the \"Haus Alaska\", a cover name for HQ 7707 European Command Intelligence Center Camp King, the US Army's interrogation centre, Oberursel (near Frankfurt), and interned with other high-ranking Germans. Fenner was intensively interrogated by the Army Security Agency (ASA) and has written a number of reports about his life and work, including an autobiographical essay \"The Career of William Fenner\"—its English translation bearing the words TOP SECRET being filed in TICOM Archive: DF-187. These documents were made publicly available only a few years ago.\n\n"}
{"id": "27628152", "url": "https://en.wikipedia.org/wiki?curid=27628152", "title": "Zhongguancun Administrative Committee", "text": "Zhongguancun Administrative Committee\n\nThe Zhongguancun Administrative Committee is a committee which oversees the affairs of Zhongguancun, Beijing.\n\n"}
