{"id": "7664801", "url": "https://en.wikipedia.org/wiki?curid=7664801", "title": "3M", "text": "3M\n\nThe 3M Company, formerly known as the Minnesota Mining and Manufacturing Company, is an American multinational conglomerate corporation operating in the fields of industry, health care, and consumer goods. The company produces a variety of products, including adhesives, abrasives, laminates, passive fire protection, personal protective equipment, dental and orthodontic products, electronic materials, medical products, car-care products, electronic circuits, healthcare software and optical films. It is based in Maplewood, Minnesota, a suburb of St. Paul.\n\nIn 2017, 3M made $31.7 billion in total sales, and the company ranked No. 97 in the 2018 Fortune 500 list of the largest United States corporations by total revenue. The company has 91,000 employees and has operations in more than 70 countries.\n\nFive businessmen founded 3M in Two Harbors, Minnesota, in 1902. Originally a mining venture, the goal was to mine corundum, but this failed because the mine's mineral holdings were anorthosite, which had no commercial value. Co-founder John Dwan solicited funds in exchange for stock and Edgar Ober and Lucius Ordway took over the company in 1905. The company moved to Duluth and began researching and producing sandpaper products. William L. McKnight, later a key executive, joined the company in 1907, and A. G. Bush joined in 1909. 3M finally became financially stable in 1916 and was able to pay dividends.\n\nThe company moved to St. Paul in 1910, where it remained for 52 years before outgrowing the campus and moving to its current headquarters at 3M Center in Maplewood, Minnesota in 1962.\n\nThe company began by mining stone from quarries for use in grinding wheels. Struggling with quality and marketing of its products, management supported its workers to innovate and develop new products, which became its core business. Twelve years after its inception, 3M developed its first exclusive product: Three-M-ite cloth. Other innovations in this era included masking tape, waterproof sandpaper, and Scotch-brand tapes. By 1929, 3M had made its first moves toward international expansion by forming Durex to conduct business in Europe. The same year, the company's stock was first traded over the counter and in 1946 listed on the New York Stock Exchange (NYSE). The company is currently a component of the Dow Jones Industrial Average and of the S&P 500.\n\nThe founders original plan was to sell the mineral corundum to manufacturers in the East for making grinding wheels. After selling one load, on June 13, 1902, the five went to the Two Harbors office of company secretary John Dwan, which was on the shore of Lake Superior and is now part of the 3M National Museum, and signed papers making Minnesota Mining and Manufacturing a corporation. In reality, however, Dwan and his associates were not selling what they thought; they were really selling the worthless mineral anorthosite.\n\nFailing to make sandpaper with the anorthosite, the founders decided to import minerals like Spanish garnet, after which sale of sandpapers grew. In 1914, customers complained that the garnet was falling off the paper. The founders discovered that the stones had traveled across the Atlantic Ocean packed near olive oil, and the oil had penetrated the stones. Unable to take the loss of selling expensive inventory, they roasted the stones over fire to remove the olive oil; this was the first instance of research and development at 3M.\n\nThe company's late innovations include waterproof sandpaper (1921) and masking tape (1925), as well as cellophane \"Scotch Tape\" and sound-deadening materials for cars.\n\nIn 1947, 3M began producing perfluorooctanoic acid (PFOA) by electrochemical fluorination. During the 1950s, the company expanded worldwide with operations in Canada, Mexico, France, Germany, Australia, and the United Kingdom in large part by Clarence Sampair.\n\nIn 1951, DuPont started purchasing PFOA from then-Minnesota Mining and Manufacturing Company for use in the manufacturing of teflon, a product that brought DuPont a billion-dollar-a-year profit by the 1990s. DuPont referred to PFOA as C8. In 1951, international sales were approximately $20 million. 3M's achievements were recognized by the American Institute of Management naming the company \"one of the five best-managed companies in the United States\" and included it among the top 12 growth stocks (3M).\n\nIn the late 1960s and early 1970s, 3M published a line of board games, largely under the \"3M bookshelf game series\" brand. These games were marketed to adults and sold through department stores, with easily learned simple rules but complex game play and depth and with uniformly high-quality components. As such, they are the ancestors of the German \"Eurogames\". The games covered a variety of topics, from business and sports simulations to word and abstract strategy games. They were a major publisher at the time for influential U.S. designers Sid Sackson and Alex Randolph. In the mid-1970s, the game line was taken over by Avalon Hill.\n\n3M's Mincom division introduced several models of magnetic tape recorders for instrumentation use and for studio sound recording. An example of the latter is the model M79 recorder, which still has a following today. 3M Mincom was also involved in designing and manufacturing video production equipment for the television and video post-production industries in the 1970s and 1980s, with such items as character generators and several different models of video switchers, from models of audio and video routers to video mixers for studio production work.\n\n3M Mincom was involved in some of the first digital audio recordings of the late 1970s to see commercial release when a prototype machine was brought to the Sound 80 studios in Minneapolis. After drawing on the experience of that prototype recorder, 3M later introduced in 1979 a commercially available digital audio recording system called the \"3M Digital Audio Mastering System\", which consisted of a 32-track digital audio recorder and a companion 4-track digital recorder for mixdown & final mastering. 3M later designed and manufactured several other commercially available models of digital audio recorders used throughout the early to mid-1980s.\n\n3M launched \"Press 'n Peel\" in stores in four cities in 1977, but results were disappointing. A year later 3M instead issued free samples directly to consumers in Boise, Idaho, with 94 percent of those who tried them indicating they would buy the product. The product was sold as \"Post-its\" in 1979 when the rollout introduction began, and was sold across the United States from April 6, 1980. The following year they were launched in Canada and Europe.\n\nIn 1996, the company's data storage and imaging divisions were spun off as Imation Corporation. In doing so 3M shed 20% of its sales, employees and product lines at a cost of only 5% of its profits and immediately looked much improved in the estimation of Wall Street analysts. These businesses, with annual sales of over $2 billion had generated handsome profits for 3M which funded R&D and development of many new business lines but were largely in \"sunset\" industries: printing products, photographic film, and removable storage media. Imation shortly sold its imaging and photographic film businesses largely to Kodak in order to concentrate on storage. Imation was purchased by a hedge fund in 2016 and ceased to exist as an independent business. What is left is now called Glassbridge Enterprises, an American holding company.\n\nAs of 2012, 3M was one of the 30 companies included in the Dow Jones Industrial Average, added on August 9, 1976, and was 97 on the 2011 Fortune 500 list. The company has 132 plants and over 67,000 employees worldwide, with sales offices in over 200 countries. The vast majority of the company's employees are local nationals, with few employees residing outside their home country. Its worldwide sales are over $20 billion, with international sales 58% of that total.\n\nIn 2002, 3M changed its legal name to 3M Company; it had been popularly known as \"3M\" for much of its history. Also in 2002, the company agreed to acquire AiT Advanced Information Technologies Corp. for about $37.4 million in cash, after AiT had strongly hinted it had put itself on the auction block.\n\nOn December 20, 2005, 3M announced a major partnership with Roush Fenway Racing, one of NASCAR's premier organizations. In 2008, the company sponsored Greg Biffle in the NASCAR Sprint Cup Series as he drove the No. 16 Ford Fusion. In addition, on February 19, 2006, 3M announced that it would become the title sponsor of the 3M Performance 400 at Michigan International Speedway until 2011.\n\nOn April 4, 2006, 3M announced its intention to sell its pharmaceutical non-core business. The pharmaceuticals businesses were sold off in three deals, in Europe, the Americas, and the remainder of the world. Another division of the Health Care business, Drug Delivery Systems, remains with 3M. The Drug Delivery System division continues to contract manufacture inhalants and transdermal drug-delivery systems, and has now taken on manufacture of the products whose licenses were sold during the divestiture of the pharmaceuticals business. On September 8, 2008, 3M announced an agreement to acquire Meguiar's, a car-care products company that was family-owned for over a century.\n\nOn August 30, 2010, 3M announced that they had acquired Cogent Systems for $943 million.\n\nOn October 13, 2010, 3M completed acquisition of Arizant Inc. In December 2011, 3M completed the acquisition of the Winterthur Technology Group, a bonded abrasives company.\n\n3M follows a business model based on \"the ability to not only develop unique products, but also to manufacture them efficiently and consistently around the world\".\n\nOn January 3, 2012, it was announced that the Office and Consumer Products Division of Avery Dennison was being bought by 3M for $550 million. The transaction was canceled by 3M in September 2012 amid antitrust concerns.\n\nIn May 2013, 3M announced that it was selling Scientific Anglers and Ross Reels to Orvis. Orvis said it planned to continue running the company as an independent entity at its Midland, Michigan headquarters and that the Ross Reels would also continue independent operations at its headquarters in Montrose County, Colorado. While under its ownership, 3M had chemists and other scientists develop improved fly lines that were easier to cast, floated at a higher level in the water, and dried faster for its fishing brands. Ross Reels had been acquired by 3M in 2010.\n\nIn March 2017, it was announced that 3M was purchasing Johnson Control International Plc's safety gear business, Scott Safety, for $2 billion.\n\nIn 2017, 3M had net sales for the year of $31.657 billion, up from $30.109 billion the year before. In 2018, it was reported that the company would pay $850 million to end the Minnesota water pollution case concerning perfluorochemicals.\n\nOn May 25, 2018, Michael F. Roman was appointed CEO by the board of directors. As of August 2018, 3M India Ltd. was the only listed 3M Company subsidiary.\n\nFor the fiscal year 2018, 3M reported earnings of US$4.858 billion, with an annual revenue of US$31.657 billion, an increase of 5.1% over the previous fiscal cycle. 3M's shares traded at over $200 per share, and its market capitalization was valued at over US$107.7 billion in October 2018.\nIn 1999 the U.S. Environmental Protection Agency began investigating perfluorinated chemicals after receiving data on the global distribution and toxicity of perfluorooctanesulfonic acid (PFOS). 3M, the former primary producer of PFOS from the U.S., announced the phase-out of PFOS, perfluorooctanoic acid, and PFOS-related product production in May 2000. Perfluorinated compounds produced by 3M were used in non-stick cookware and stain-resistant fabrics. The Cottage Grove facility manufactured PFCs from the 1940s to 2002. In response to PFC contamination of the Mississippi River and surrounding area, 3M stated the area will be \"cleaned through a combination of groundwater pump-out wells and soil sediment excavation\". The restoration plan was based on an analysis of the company property and surrounding lands. The on-site water treatment facility that handled the plant's post-production water was not capable of removing the PFCs, which were released into the nearby Mississippi River. The clean-up cost estimate was $50 to $56 million, funded from a $147 million environmental reserve set aside in 2006.\n\nIn 1983, the Oakdale Dump in Oakdale, Minnesota, was listed as an EPA Superfund site after significant groundwater and soil contamination by VOCs and heavy metals was uncovered. The Oakdale Dump was a 3M dumping site utilized through the 1940s and 1950s.\n\nIn 2008, 3M created the Renewable Energy Division within 3M's Industrial and Transportation Business to focus on Energy Generation and Energy Management.\n\nIn late 2010, the state of Minnesota sued 3M for 5 billion in punitive damages, claiming they released PFCs, a very toxic chemical according to the EPA but unknown at the time of release, into local waterways. After many long delays, a settlement for $850 million was reached in February 2018.\n\n3M's general offices, corporate research laboratories, and some division laboratories in the US are in St. Paul, Minnesota. In the United States, 3M operates 80 manufacturing facilities in 29 states, and 125 manufacturing and converting facilities in 37 countries outside the US (in 2017). \n\nIn March 2016, 3M completed a research-and-development building that cost $150 million on its Maplewood campus. Seven hundred scientists from various divisions occupy the building. They were previously scattered across the campus. 3M hopes concentrating its research and development in this manner will improve collaboration. 3M received $9.6 million in local tax increment financing and relief from state sales taxes in order to assist with development of the building.\n\n3M owns almost all of the real estate it occupies. Because 3M is a global enterprise characterized by substantial intersegment cooperation, properties are often used by multiple business segments.\n\nSelected factory detail information:\n\n\n"}
{"id": "36801303", "url": "https://en.wikipedia.org/wiki?curid=36801303", "title": "Altarpiece of Pellegrino II", "text": "Altarpiece of Pellegrino II\n\nThe altarpiece of Pellegrino II is a medieval altarpiece in the cathedral of Cividale, Italy. The silver relief was endowed by Pellegrino II, the patriarch of Aquileia, around 1200 and adorns today the main altar of the church \"Santa Maria Assunta\". It shows Mary and the Child Jesus surrounded by archangels and groups of saints. The piece is notable for its rich ornamentation and its early typographic inscription.\n\nThe altarpiece is located in the cathedral Santa Maria Assunta in the town of Cividale in Friuli. The rectangular relief plate which measures about one meter high and two meters wide is made of partly gilded silver. Protected by a modern glass case, it towers above the principal altar in the choir. It was consecrated by Pellegrino II who was patriarch of Aquileia between 1195 and 1204.\n\nThe altarpiece is divided into four parts: the center consists of a triptych which shows Mary as the Mother of God (Latin \"mater dei\") with the Child Jesus in her lap. From left and right the archangels Michael and Gabriel rush to the seated mother and her child. The whole scene takes place under a three-arched arcade. The triptych is flanked by two departments which display, in three horizontal lines each, a total of 25 male and female saints standing next to one other. All figures except the Child Jesus are identifiable by name. A frame displaying a series of head medallions without inscriptions runs all around the triptych and the two lateral sections. In the upper horizontal frame piece Christ and John the Baptist as well as the four Evangelists are depicted. In its lower counterpart Pellegrino II, kneeling at the feet of Mary, can be identified by an accompanying inscription as the donator of the altarpiece. On the inside of the two frame boards a votive inscription composed of ten Leonine verses runs horizontally across the altarpiece.\n\nAll inscriptions of the altarpiece were written in Latin. The font of the dedicatory inscription is overall classified as Gothic capitalis. Modern commentators agree that the inscriptions were produced by hammering individual letter punches one by one into the silver plate. Evidence for this typographic method can be derived from the observation that the letterforms comply with the criterion of type identity according to which every letter imprint must come from one and the same letter punch. The type identity is \"inter alia\" evident in the repeated occurrence of the faulty letter \"R\" throughout the text that indicates a damaged letter punch. The high relief letters stand proud in rectangular recesses created by the bases of the low relief punches; the fine edges between these recesses are a further indication of the sequential use of individual letter punches. A number of letters which are not properly aligned to their baseline provide additional evidence that the artisan worked with separate punches.\n\nA total of about forty types, which come in equal parts in a smaller and larger font size, were employed for creating the inscriptions. The smaller ones were used for the names of the saints and the donor's inscription of the patriarch, while the names of the archangels, of the Mother of God, the abbreviations of sanctus/sancta (\"holy\") and the two-line inscription were printed with the larger set. The latter text was punched into eight silver strips which were strung together and nailed onto the wooden substructure of the altarpiece.\n\nAccording to the art historian Angelo Lipinsky, the altarpiece may have been inspired by Byzantine reliquaries which were decorated by the same typographic technique in the 10th to 12th centuries. However, a random check with the which dates from this period showed that the inscription was engraved directly into the metal.\n\nThe Prüfening dedicatory inscription of 1119 is another early example of typographic text production in the Latin West. It differs in some technical details though: its plate is made of clay, not silver, and the inscription was created in low relief with (wooden) stamps, not metal punches.\n\n\n"}
{"id": "24617101", "url": "https://en.wikipedia.org/wiki?curid=24617101", "title": "Archibald Glenn", "text": "Archibald Glenn\n\nSir Joseph Robert Archibald Glenn, OBE (24 May 19114 January 2012) was an Australian industrialist and founding Chancellor of La Trobe University.\n\nGlenn was born in 1911 and raised near Sale, Victoria. He was educated at Scotch College, Melbourne, where he moved after Sale High School found itself without a mathematics teacher. He studied engineering at the University of Melbourne and later in London. He joined ICI (Imperial Chemical Industries Australia, now known as Orica) where he became managing director for 25 years,retiring in 1973, and where he was Chairman for 10 years. During World War II he worked alongside Essington Lewis, who was in charge of war-related industry as Director-General of the Department of Munitions.\n\nHe was Chairman of the Scotch College Council from 1963 to 1981, and Ormond College Council from 1976 to 1981.\n\nHe was appointed an Officer of the Order of the British Empire (OBE) in 1965 and was knighted in 1966. He died on 4 January 2012, aged 100.\n\nHis first wife was Elizabeth \"Betty\" née Balderstone, whom he married in 1939. After Betty died in 1988, he married Sue Debenham. His four children included Di Gribble, editor, publisher and deputy chair of the ABC; she predeceased him by three months.\n\n"}
{"id": "2124982", "url": "https://en.wikipedia.org/wiki?curid=2124982", "title": "Association for Information and Image Management", "text": "Association for Information and Image Management\n\nThe Association for Information and Image Management or AIIM (pronounced aim) is a non-profit membership organization. AIIM provides education, market research, and certification for information professionals.\n\nAIIM was founded in 1943 as the National Microfilm Association. It became the \"Association for Information and Image Management\" in 1982. The name would change again in 2018 to the \"Association for Intelligent Information Management,\" maintaining the AIIM acronym.\n\nAIIM was founded in 1943 as the National Microfilm Association. As microfilm fell into disuse, the association changed its name to the Association for Information and Image Management, or \"AIIM\" for short, in 1982. The name change demonstrated a broadening of focus for the association. \nIn 2000, AIIM created the term Enterprise Content Management (ECM), which referred to the strategies, methods, and tools used throughout the lifecycle of an organization's documents, and other content, that relate to the organization's processes. \n\nAt the AIIM Conference 2017, AIIM agreed with a blog post published by Gartner titled \"The Death of ECM and Birth of Content Services\" that \"ECM is now dead (kaput, finite, an ex-market name).\" AIIM announced that the term Enterprise Content Management would be replaced by the term Intelligent Information Management (IIM). This new term would encompass the management of data and content, not just content by itself.\n\nIn 2006, AIIM launched its first training course titled ECM Master Class. Today, AIIM offers training on a variety of information management technologies, including:\n\n\nThese courses are available online and in an in-person classroom setting. At the AIIM Conference 2017, AIIM announced they had trained over 60,000 students.\n\nAIIM set out to create a certification for information professionals as a means of standardizing on the body of knowledge necessary to be successful in this profession. AIIM worked with industry experts and focus groups to define this body of knowledge and launched a certification and test based on it in 2011.\n\nIn July 2016, the Certified Information Professional exam was updated to reflect changes in information management technology and behavior. \n\nBy 2017, more than 1,500 information professionals had earned the CIP.\n\nThe AIIM Conference was officially launched in 2012. The annual event brings together over 600 attendees, consisting mostly of information management end-users, from over 25 countries to see presentations and take part in discussions on the best practices, current trends, and top issues facing the information management industry.\n\nThe AIIM Executive Leadership Council has two delegations, the Americas and European. Each delegation meets twice a year for a think tank summit where participants discuss Information Management-related topics such as the impact of these technologies on the workplace and future organizational transformations.\n\nFindings from the summits are published as AIIM Trendscape reports which forecast the changes the information management industry will incur over the next 24 months and set priorities to take advantage of these changes.\n"}
{"id": "4934", "url": "https://en.wikipedia.org/wiki?curid=4934", "title": "Basic English", "text": "Basic English\n\nBasic English is an English-based controlled language created by linguist and philosopher Charles Kay Ogden as an international auxiliary language, and as an aid for teaching English as a second language. Basic English is, in essence, a simplified subset of regular English. It was presented in Ogden's book \"Basic English: A General Introduction with Rules and Grammar\" (1930).\n\nOgden's Basic, and the concept of a simplified English, gained its greatest publicity just after the Allied victory in World War II as a means for world peace. Although Basic English was not built into a program, similar simplifications have been devised for various international uses. Ogden's associate I. A. Richards promoted its use in schools in China. More recently, it has influenced the creation of Voice of America's Special English for news broadcasting, and Simplified Technical English, another English-based controlled language designed to write technical manuals.\n\nWhat survives today of Ogden's Basic English is the basic 850-word list used as the beginner's vocabulary of the English language taught worldwide, especially in Asia.\n\nOgden tried to simplify English while keeping it normal for native speakers, by specifying grammar restrictions and a controlled small vocabulary which makes an extensive use of paraphrasing. Most notably, Ogden allowed only 18 verbs, which he called \"operators\". His \"General Introduction\" says \"There are no 'verbs' in Basic English\", with the underlying assumption that, as noun use in English is very straightforward but verb use/conjugation is not, the elimination of verbs would be a welcome simplification.\n\nOgden's word lists include only word roots, which in practice are extended with the defined set of affixes and the full set of forms allowed for any available word (noun, pronoun, or the limited set of verbs). The 850 core words of Basic English are found in Wiktionary's Basic English word list. This core is theoretically enough for everyday life. However, Ogden prescribed that any student should learn an additional 150-word list for everyday work in some particular field, by adding a list of 100 words particularly useful in a general field (e.g., science, verse, business, etc.), along with a 50-word list from a more specialised subset of that general field, to make a basic 1000-word vocabulary for everyday work and life.\n\nMoreover, Ogden assumed that any student already should be familiar with (and thus may only review) a core subset of around 200 \"international\" words. Therefore, a first-level student should graduate with a core vocabulary of around 1200 words. A realistic general core vocabulary could contain 2000 words (the core 850 words, plus 200 international words, and 1000 words for the general fields of trade, economics, and science). It is enough for a \"standard\" English level. This 2000 word vocabulary represents \"what any learner should know\". At this level students could start to move on their own.\n\nOgden's and Voice of America's serve as dictionaries for the Simple English Wikipedia.\n\nThe word use of Basic English is similar to full English, but the rules are much simpler, and there are fewer exceptions. Not all meanings of each word are allowed.\n\nOgden's rules of grammar for Basic English help people use the 850 words to talk about things and events in a normal way.\n\n\nLike all international auxiliary languages (or IALs), Basic English may be criticised as inevitably based on personal preferences, and thus, paradoxically, inherently divisive. Moreover, like all natural language based IALs, Basic is subject to criticism as unfairly biased towards the native speaker community.\n\nAs a teaching aid for English as a Second Language, Basic English has been criticised for the choice of the core vocabulary and for its grammatical constraints.\n\nIn 1944, readability expert Rudolf Flesch published an article in \"Harper's Magazine\", \"How Basic is Basic English?\" in which he claimed, \"It's not basic, and it's not English.\" The essence of his complaint is that the vocabulary is too restricted, and, as a result, the text ends up being awkward and more difficult than necessary. He also argues that the words in the Basic vocabulary were arbitrarily selected, and notes that there had been no empirical studies showing that it made language simpler.\n\nIn the novel \"The Shape of Things to Come\", published in 1933, H. G. Wells depicted Basic English as the lingua franca of a new elite that after a prolonged struggle succeeds in uniting the world and establishing a totalitarian world government. In the future world of Wells' vision, virtually all members of humanity know this language.\n\nFrom 1942 to 1944 George Orwell was a proponent of Basic English, but in 1945 he became critical of universal languages. Basic English later inspired his use of Newspeak in \"Nineteen Eighty-Four\".\n\nEvelyn Waugh criticized his own 1945 novel \"Brideshead Revisited\", which he had previously called his magnum opus, in the preface of the 1959 reprint: \"It <nowiki>[</nowiki>World War II<nowiki>]</nowiki> was a bleak period of present privation and threatening disaster—the period of soya beans and Basic English—and in consequence the book is infused with a kind of gluttony, for food and wine, for the splendours of the recent past, and for rhetorical and ornamental language that now, with a full stomach, I find distasteful.\"\n\nIn his story \"Gulf\", science fiction writer Robert A. Heinlein used a constructed language called Speedtalk, in which every Basic English word is replaced with a single phoneme, as an appropriate means of communication for a race of genius supermen.\n\n\n\n"}
{"id": "37974009", "url": "https://en.wikipedia.org/wiki?curid=37974009", "title": "Bebionic", "text": "Bebionic\n\nBeBionic is a commercial prosthetic hand designed to enable amputees to perform everyday activities, such as eating, drinking, writing, typing, turning a key in a lock and picking up small objects. \n\nThe first version of the bebionic hand was launched at the World Congress and Orthopädie & Reha-Technik, Trade Show, Leipzig, Germany in May 2010.\n\nDesigned in the United Kingdom, the bebionic hand is manufactured by RSL Steeper and is available worldwide. Since February 2nd, 2017 BeBionic is owned by Otto Bock\n\nThe bionic hand is said to receive instructions from sensors that detect the movement of the muscles in the patient’s arm. These instructions are processed, which are then directed to the 337 mechanical parts, which are present within this bionic hand that eventually, mimic natural human movements. \n\nIn September 2011, the bebionic second-generation prosthetic hand was updated with improvements to speed, accuracy, grip and durability. The updated version also saw the bebionic hand available two sizes - medium and large. The device was upgraded with new, higher capacity 2200mAh split cell internal batteries for increased usage time.\n\nDuring 2008, Jonathan Metz, from West Hartford, Connecticut, got his arm wedged in his basement furnace. Trapped in his own basement for three days, he had no alternative to self-amputate his own arm. Since using the prosthetic hand, in 2010, his life has dramatically improved. \n\nIn 2012, Kingston upon Hull man, Mike Swainger, was the first person to receive bionic hand on the NHS.\n\nIn 2015, a 26-year old London-based woman, Ashwell who was born without a right hand received Bebionic's prosthetic hand.\n\nMargarita Gracheva, from the Moscow Serpukhov, had her hands cut off by her husband. After six months of rehabilitation, dozens of concerned viewers of the Program live with Andrey Malakhov of Russia 1 TV channel, helped to make it super-modern .\n\nIn the world of science fiction, the bebionic hand has been compared to the artificial hands of fictional characters such as The Terminator and Luke Skywalker from Star Wars.\n\n"}
{"id": "35031932", "url": "https://en.wikipedia.org/wiki?curid=35031932", "title": "BeetleCam", "text": "BeetleCam\n\nThe BeetleCam is a remote controlled buggy with a DSLR or mirrorless camera mounted on top which can be used to film and photograph wildlife at very close range.\n\nCreated by Will Burrard-Lucas, its first shots were released in 2010 in a series called \"The Adventures of BeetleCam.\" It filmed African wildlife in the Ruaha and Katavi National Parks in Tanzania. One of the cameras was destroyed in an encounter with a lion.\n\nWill Burrard-Lucas and his brother, Matt, returned to Africa in 2011 with two improved BeetleCams, with the aim of focusing on lions. During this project they created a set of pictures of feeding lions and playful cubs. This series was first released in 2012 in an article called \"BeetleCam vs the Lions of the Masai Mara\". BeetleCam Mark II used a Canon EOS-1Ds Mark III.\n\nIn 2012, Burrard-Lucas moved to Zambia and used a new version of the BeetleCam to photograph leopards and other animals, primarily in South Luangwa National Park.\n\nIn 2013, Will Burrard-Lucas founded Camtraptions Ltd. and started producing BeetleCams for other wildlife photographers and filmmakers.\n\n"}
{"id": "25906658", "url": "https://en.wikipedia.org/wiki?curid=25906658", "title": "C. K. G. Billings", "text": "C. K. G. Billings\n\nCornelius Kingsley Garrison Billings (September 17, 1861 in Saratoga, New York – May 6, 1937 in Santa Barbara, California) was a wealthy industrialist tycoon, philanthropist, art collector, and a noted horseman and horse breeder. An eccentric man, Billings invested much of his time and money promoting the sport of trotting, also known as \"harness racing\" or \"matinee racing\".\n\nBillings was born in Saratoga, New York on September 17, 1861, the son of Albert M. Bilings, a resident of Vermont, and Augusta S. Billings née Farnsworth. He was raised in Chicago, Illinois from the age of three, attended schools in Chicago, and then Racine College in Racine, Wisconsin. When he finished college at 17 years old in 1879, he joined the Peoples Gas Light and Coke Company – of which his father was a principal investor and president – beginning as a laborer. When he became the firm's president in 1887, he brought about the mergers from 1895 to 1910 of 12 gas companies into Peoples Gas. He became chairman of the board of the company in 1901, a position he held until 1911.\n\nIn 1885, Billings married Blanche E. MacLeish, whose father, Andrew MacLeish, was one of the founders of the Chicago department store Carson, Pirie, Scott and Company. They had a son, Albert Merritt Billings, who died in 1926; Billings endowed the Billings Memorial Hospital in Chicago as tribute to him. They also had a daughter, who married Halstead Van der Poel.\n\nDuring his years in Chicago, Billings was the founder and a charter member of the Chicago Athletic Club, served on the West Park Commission, and on the board of the 1893 World's Columbian Exposition.\n\nIn 1901, at the age of 40, Billings, who had inherited a controlling interest in Peoples Gas, but had retired from the day-to-day running of the company, moved to Manhattan, New York City, where he and his family lived in a townhouse on Fifth Avenue at 53rd Street.\n\nBillings, who owned 75 racing or trotting horses, would later build an extensive estate in Upper Manhattan, on the site of what is now Fort Tryon Park, but first built a stable there, at the cost of $200,000. It was completed in 1903. The stable, which was long and wide and two stories tall \"with numerous towers and cupolas\", had 22 box stalls and 9 straight stalls, a outdoor training ring, a -by- sleigh room, feed rooms, a hayloft, and a 5,000 bushel zinc-lined granary. It also had a gymnasium, a blacksmith shop with forge, a trophy room to display Billings' awards from the amateur races he won, and two five-room suites of living quarters. The interior was designed in oak and Georgia pine. The stable had steam heat, electric light, and hot water, all provided by its dynamo room. About twenty-five men were employed.\n\nNearby the stable was a 14-room -by- lodge for guests, which featured an -tall observation tower.\n\nThe site was conveniently near to the Harlem Speedway, built in 1894-89 for the exclusive use of riders on horseback and horse-drawn carriages. It ran from West 155th Street to Dyckman Street. Rich New Yorkers used the Speedway to train their horses and size up those of their friends and competitors. The Speedway was eventually paved and became the beginning of the Harlem River Drive.\n\nBillings wished to celebrate the completion of his trotting stable, and his selection to be the head of the New York Equestrian Club, by giving a dinner for 36 of his male horse-riding friends in the stable on March 29, 1903. He engaged the noted restaurateur Louis Sherry to cater the event, but to avoid reporters who staked out the estate after news of the dinner had spread, Billings changed the venue at Sherry's suggestion to the grand ballroom of Sherry's restaurant at Fifth Avenue and 44th Street. The ballroom had been decorated to look like an English country estate, complete with imitation brooks. Wanting he and his guests to be mounted on horses while they ate, the floor was covered with turf. The horses, which were docile animals rented from nearby riding academies, were brought to the fourth-floor ballroom via the freight elevator. The participants, arranged in a circle, ate from specially built silver trays which were attached to their saddles and drank through rubber tubes connected to iced bottles of champagne in their saddlebags. The waiters, one for each diner, served the numerous courses dressed as if grooms at a fox hunt. The 32 docile horses were each attended by an elaborately dressed groom, and near the end of the evening elaborate troughs filled with oats were brought in for the horses to eat from.\n\nThe evening concluded with a vaudeville show.\n\nThe $50,000 bill for the dinner (equivalent to $ in ) included the cost of a photographer from the celebrated Byron Company to document the event.\n\nTwo days later, Billings officially opened his new stable with a luncheon for members of the Equestrian Club and other wealthy horsemen and dignitaries from around the country. Some rode there on horseback, but most traveled by elevated train to the 155th Street station located at the Harlem Speedway, and were conveyed to the stable by automobiles.\n\nIn 1903, Billings, who was a prominent member of the Jockey Club and was part-owner of the Jamaica Race Course in Jamaica, Queens, was regarded as a \"Grand Marshal\" of harness racing (\"trotting\" or \"matinee racing\"), but in November 1905, just two years after his stable was completed, he sold his stock of horses at Madison Square Garden, saying that he proposed to go abroad for a few years. Billings held back only three horses from the sale, plus one that was withdrawn because it was lame. The sale of 18 horses brought in $46,270, with the top seller bringing in $10,500.\n\nThe Billings' mansion at West 196th Street and Fort Washington Road was designed by Guy Lowell, who enlarged the lodge which had been built as part of the stables. It was organized around a central courtyard with a fountain. Landscape architect Charles Downing Lay designed the grounds. Billings called it \"Tryon Hall\", after Fort Tryon, which had been located there, itself named after Sir William Tryon, who was the last Governor of the English colony of New York. The mansion stood on one of the highest points in Manhattan, overlooking the Hudson River to the west, and the Broadway Valley to the east.\n\nBy 1907, Billings, his wife, two children, and 23 servants had moved from their Manhattan townhouse into the massive Louis XIV-style chateau, now their full-time residence. It included an observatory tower topped by an octagonal room, which had a 360-degree unobstructed view. The estate had a casino with a swimming pool, squash court and bowling alley for entertaining, as well as Billings' extensive stables and an area to exercise his horses.\n\nThe entrance to the estate was originally at the top of the hill, approached via Riverside Drive and West 181st Street, to Fort Washington Road, but the upper part of Riverside Drive was completed at about the same time as Billings' mansion, and he wanted a driveway which connected the mansion directly to that section of the roadway. Unfortunately, there was a steep cliff between the road and the mansion. Billings' hired the firm of Buchman & Fox to find a solution, which they did: granite was removed from the cliff to allow a passage for a zig-zagging driveway, and the stone was then used both as a retaining wall, and for the construction of an arched viaduct that the driveway initially passed through. The arched passage became known as the \"Billings Arcade\". The entrance to the driveway had granite pillars that were tall, which supported tall gates which were wide.\nThe entire driveway project took a year for over a hundred workers to complete, at the cost of $250,000. It raised the overall cost of the estate to more than $2 million. The Billings Arcade still remains as part of Fort Tryon Park, as does part of the driveway, now used as a pedestrian path. Another remnant is a gardener's cottage, originally a gatehouse for the estate's upper entrance, now used for park offices. The gateposts of the driveway entrance to the estate were refurbished in 2017. The driveway no longer connects to the roadway which was once Riverside Drive and is now the northbound side of the Henry Hudson Parkway.\n\nIn the nearby Hudson River, Billings kept his yacht, \"Vanadis\" which was built in 1908.\n\nBillings sold his Tryon Hall estate in 1917 to John D. Rockefeller, Jr.. His family had already moved into an apartment on Fifth Avenue and 63rd Street, which has 21 rooms, and for which he paid $20,000 a year in rent. Rockefeller was assembling parcels for the creation of a park designed by the Olmsted Brothers which he planned to develop and then give to the city – this eventually became Fort Tryon Park. As part of Rockefeller's plan, he was going to tear down Tryon Hall but was held back by popular sentiment. During World War I, he offered use of the house to the U.S. government as a hospital, and was prepared to outlay $500,000 for the conversion, but this did not happen. After that, the mansion was rented to Nicolas C. Partos of the Partola Manufacturing Company, at first for the summer of 1918, but then extending for years. Partos and his family was still in residence when the building burned down on March 7, 1926.\n\nAfter leaving Tryon Hall, Billings moved to another grand estate he had built, this one called \"Farnsworth\", named for his mother's family, and located in Locust Valley, New York, on Long Island. It was again designed by Guy Lowell, this time in the Georgian Revival style, with the landscaping of the extensive grounds designed by Andrew Robeson Sargent of Boston. The buildings alone on the estate cost $1,550,000 in 1915. Although Georgian in style, Lowell designed it to be similar to an Italian villa, in that the mansion was built around a patio at its center. The house featured 11 master bedrooms with 9 baths and 19 servants bedrooms with 4 baths. The appointments were expensive and luxurious.\n\nDespite the grandeur of the surrounding, Billings did not stay in Farnsworth any longer than he had in Tryon Hall. With World War 1 raging, and his health failing, Farnsworth began to sell off his East Coast properties in preparation for moving to California.\n\nBillings moored his yacht \"Vanadis\" in the nearby waters. In 1916, he had sold the original \"Vanadis\" to Morton F. Plant in return for the smaller yacht \"Kanawha\", after the \"Vanadis\" struck the steamship \"Bunker Hill\", killing two people. In 1924, Billings ordered a second, larger yacht – long – which he also named \"Vanadis\". This ship is now anchored at Riddarholmen in Stockholm, and is now the hotel Mälardrottningen, with the ship rechristened as \"Lady Hutton\", after the actress Barbara Hutton.\n\nAt various times, Billings also owned a estate on the James River in Virginia called Curles Neck Farm, which he bought in 1913 and developed into one of the country's prime horse-breeding facilities. He had another estate in Colorado Springs, Colorado and a summer home in Lake Geneva, Wisconsin.\n\nWhen he moved to Santa Barbara, Billings had a mansion built in the hills of that city, which he called \"Asombrosa\". It was damaged by an earthquake in the mid-1930s, and he had another, smaller, house built nearby.\n\nIn 1911, Billings became the Chairman of the Board of Union Carbide and Carbon Company – a company he helped to found – a position he held until his death in 1937. His mother died in 1913, leaving him $450,000; at that time his net worth was estimated to be $30 million, equivalent to $ million in . At one time he was reported to be one of the five richest men in the United States.\n\nAround 1915, Billings – a member of the Turf and Field Club at Belmont Park – was said to be the owner of the fastest stallion, mare, and gelding in the world. He was also part-owner of the Kentucky Derby-winning Omar Khayyam. He was also the principal investor in the Billings Parks race track in Memphis, Tennessee, which eventually closed because of anti-betting laws passed by that state. At one time he bought a controlling interest in the Kentucky Breeder's Association, which prevented that organization from going under. The association was reorganized, and Billings later donated his stock to the group.\n\nBillings moved to Santa Barbara, California in 1917, but maintained ownership of \"Farnsworth\" on Long Island, where he kept some of his horses. Others were kept at the Glenville Race Track in Cleveland, Ohio.\n\nIn 1926, Billings sold his art collection, which included works by Jean-Charles Cazin, Jean-Baptiste-Camille Corot, John Crome, Charles-François Daubigny, Jules Dupré, Charles Jacque, Jean-François Millet, Théodore Rousseau, Constant Troyon, and Félix Ziem for $401,300, in 1928 he realized $4 million for the sale of the Johnson Building, located on Exchange Street from Broad Street to New Street. He was also part of a group of investors who built the Pierre Hotel in Manhattan, which opened in 1930.\n\nAfter being in bad health for ten years, Billings was reported to be seriously ill on May 3, 1937, and he died from pneumonia on his estate at Billings Park, near Santa Barbara, on May 6. At the time of his death, he was still the chairman of the board of the Union Carbide Carbon Company, and was described as \"one of America's wealthiest men\" and \"Santa Barbara's wealthiest and most philanthropic citizen.\"\n\nThe funeral for Billings was held in Santa Barbara on May 8, and he was later buried in Graceland Cemetery in Chicago.\n\nDespite the reputation which has attached to Billings in the aftermath of the horseback dinner he was memorialized quite differently: Personally Mr. Billings was a man of retiring, modest, nature, who shunned the limelight except when driving or riding one of his horses upon the race course, always dressed very quietly, and in every way made himself as inconspicuous as possible. He was happiest when surrounded by the small circle of intimate friends that he best-loved ... He was the loyalest of friends and when he had once given his good will to a man it was never withdrawn unless it had been abused. His benefactions and gifts were boundless and in them, he took the greatest pleasure. In all social relations he was the reverse of pompous, arrogant or domineering, was democratic and genial and, that rarest of all things—always the same admirable and wonderful character in every spot and place, at all times and seasons and under all circumstances.\n\n\n\n"}
{"id": "6629973", "url": "https://en.wikipedia.org/wiki?curid=6629973", "title": "Canadian Academy of Engineering", "text": "Canadian Academy of Engineering\n\nThe Canadian Academy of Engineering () is a national academy of distinguished professional engineers in all fields of engineering, who are elected on the basis of \"their distinguished service and contribution to society, to the country and to the profession\".\n\nThe Academy was founded in 1987. It has published several monographs and research studies.\n\n\n"}
{"id": "10601507", "url": "https://en.wikipedia.org/wiki?curid=10601507", "title": "Car analogy", "text": "Car analogy\n\nThe car analogy is a common technique used by engineering textbooks to ease the understanding of abstract concepts in which a car, its composite parts, and common circumstances surrounding it are used as analogs for elements of the conceptual systems.\n\nThe efficiency of car analogies reside on their capacity to explain difficult concepts (usually due to their high abstraction level) on more mundane terms with which the target audience is comfortable, and with which many also have a special interest. Due to that, car analogies appear more often on works related to applied sciences and technology.\n\nIn order to work, car analogies translate agents of action as the car driver, the seller, or police officers; likewise, elements of a system are referred as car pieces, such as wheels, motor, or ignition keys. Resources tend to appear as gas, speed, or as the money that can be spent on better accessories/vehicles.\n\nFor example, in the paragraph:\n\n\"Zener diodes regulate voltage by acting as complementary loads, drawing more or less current as necessary to ensure a constant voltage drop across the load. This is analogous to regulating the speed of an automobile by braking rather than by varying the throttle position.\"\n\nCurrent (resource) is depicted as the car's speed, while the role of the brakes is performed by the Zener diodes (element of the system).\n\nCar analogies are also typically used to explain the quality differences between two similar tools or hardware pieces; in that case, the best one is usually described as a mid-engined Italian supercar or a high-end luxury vehicle.\n"}
{"id": "13078034", "url": "https://en.wikipedia.org/wiki?curid=13078034", "title": "Chip creep", "text": "Chip creep\n\nChip creep refers to the problem of an integrated circuit (chip) working its way out of its socket over time. This was mainly an issue in early PCs.\n\nChip creep occurs due to thermal expansion, which is expansion and contraction as the system heats up and cools down. It can also occur due to vibration. While chip creep was most common with older memory modules, it was also a problem with CPUs and other main chips that were inserted into sockets. A example is the Apple III, where it's CPU would be dislodged and the user would need to reseat the chips.\n\nTo fix chip creep, users of older systems would often have to remove the case cover and push the loose chip back into the socket. Today's computer systems are not as affected by chip creep, since chips are more securely held, either by various types of retainer clips or by being soldered into place, and since system cooling has improved.\n"}
{"id": "6083924", "url": "https://en.wikipedia.org/wiki?curid=6083924", "title": "Clock drift", "text": "Clock drift\n\nClock drift refers to several related phenomena where a clock does not run at exactly the same rate as a reference clock. That is, after some time the clock \"drifts apart\" or gradually desynchronizes from the other clock i.e. the crystal-based clocks used in computers, like any\nother clocks, subject to clock drift, which means that they count time at different rates and so they diverge. This phenomenon is used, for instance, in computers to build random number generators. On the negative side, clock drift can be exploited by timing attacks.\n\nEveryday clocks such as wristwatches have finite precision. Eventually they require correction to remain accurate. The rate of drift depends on the clock's quality, sometimes the stability of the power source, the ambient temperature, and other subtle environmental variables. Thus the same clock can have different drift rates at different occasions.\n\nMore advanced clocks and old mechanical clocks often have some kind of speed trimmer where one can adjust the speed of the clock and thus correct for clock drift. For instance, in pendulum clocks the clock drift can be manipulated by slightly changing the length of the pendulum.\n\nA quartz oscillator is less subject to drift due to manufacturing variances than the pendulum in a mechanical clock. Hence most everyday quartz clocks do not have an adjustable drift correction.\n\nAtomic clocks are very precise and have nearly no clock drift. Even the Earth's rotation rate has more drift and variation in drift than an atomic clock due to tidal acceleration and other effects. The principle behind the atomic clock has enabled scientists to re-define the SI unit second in terms of exactly 9 192 631 770 oscillations of the caesium atom. The precision of these oscillations allows atomic clocks to drift roughly only one second in a hundred million years; currently, the most accurate of which loses one second every 15 billion years. The International Atomic Time (TAI) time standard and its derivatives (such as the Coordinated Universal Time (UTC)) are based on weighted averages of atomic clocks worldwide.\n\nAs Einstein predicted, relativistic effects can also cause clock drift due to time dilation. This is because there is no fixed universal time, time being relative to the observer. Special relativity describes how two clocks held by observers in different inertial frames (i.e. moving with respect to each other but not accelerating or decelerating) will each appear to either observer to tick at different rates.\n\nIn addition to this, general relativity gives us gravitational time dilation. Briefly, a clock in a stronger gravitational field (e.g. closer to a planet) will appear to tick more slowly. People holding these clocks (i.e. those inside and outside the stronger field) would all agree on which clocks appear to be going faster.\n\nNote that it is time itself rather than the function of the clock which is affected. Both effects have been experimentally observed.\n\nTime dilation is of practical importance. For instance, the clocks in GPS satellites experience this effect due to the reduced gravity they experience (making their clocks appear to run more quickly than those on Earth) and must therefore incorporate relativistically corrected calculations when reporting locations to users. If general relativity were not accounted for, a navigational fix based on the GPS satellites would be false after only 2 minutes, and errors in global positions would continue to accumulate at a rate of about 10 kilometers each day.\n\nComputer programs often need high quality random numbers, especially for cryptography. There are several similar ways clock drift can be used to build random number generators (RNGs).\n\nOne way to build a hardware random number generator is to use two independent clock crystals, one that for instance ticks 100 times per second and one that ticks 1 million times per second. On average the faster crystal will then tick 10,000 times for each time the slower one ticks. But since clock crystals are not precise, the exact number of ticks will vary. That variation can be used to create random bits. For instance, if the number of fast ticks is even, a 0 is chosen, and if the number of ticks is odd, a 1 is chosen. Thus such a 100/1000000 RNG circuit can produce 100 somewhat random bits per second.\nTypically such a system is biased—it might for instance produce more zeros than ones—and so hundreds of somewhat-random bits are \"whitened\" to produce a few unbiased bits.\n\nThere is also a similar way to build a kind of \"software random number generator\". This involves comparing the timer tick of the operating system (the tick that usually is 100–1000 times per second) and the speed of the CPU. If the OS timer and the CPU run on two independent clock crystals the situation is ideal and more or less the same as the previous example. But even if they both use the same clock crystal the process/program that does the clock drift measurement is \"disturbed\" by many more or less unpredictable events in the CPU such as interrupts and other processes and programs that runs at the same time. Thus the measurement will still produce fairly good random numbers. Some argue they are then not true random numbers but they seem to be good enough for most needs.\n\nMost hardware random number generators such as the ones described above are fairly slow. Therefore, most programs only use them to create a good seed that they then feed to a pseudorandom number generator or a cryptographically secure pseudorandom number generator to produce many random numbers fast.\n\nIn 2006, a side channel attack was published that exploited clock skew based on CPU heating. The attacker causes heavy CPU load on a pseudonymous server (Tor hidden service), causing CPU heating. CPU heating is correlated with clock skew, which can be detected by observing timestamps (under the server's real identity).\n\n"}
{"id": "1782378", "url": "https://en.wikipedia.org/wiki?curid=1782378", "title": "Clostridium acetobutylicum", "text": "Clostridium acetobutylicum\n\nClostridium acetobutylicum, ATCC 824, is a commercially valuable bacterium sometimes called the \"Weizmann Organism\", after Jewish-Russian-born Chaim Weizmann. A senior lecturer at the University of Manchester, England, he used them in 1916 as a bio-chemical tool to produce at the same time, jointly, acetone, ethanol, and butanol from starch. The method has been described since as the ABE process, (Acetone Butanol Ethanol fermentation process), yielding 3 parts of acetone, 6 of butanol, and 1 of ethanol. Acetone was used in the important wartime task of casting cordite. The alcohols were used to produce vehicle fuels and synthetic rubber.\n\nUnlike yeast, which can digest only sugar into alcohol and carbon dioxide, \"C. acetobutylicum\" and other Clostridia can digest whey, sugar, starch, cellulose and perhaps certain types of lignin, yielding butanol, propionic acid, ether, and glycerin.\n\nIn 2008, a strain of \"Escherichia coli\" was genetically engineered to synthesize butanol; the genes were derived from \"Clostridium acetobutylicum\". In 2013, the first microbial production of short-chain alkanes was reported - which is a considerable step toward the production of gasoline. One of the crucial enzymes - a fatty acyl-CoA reductase - came from \"Clostridium acetobutylicum\".\n\n\n\n"}
{"id": "6863064", "url": "https://en.wikipedia.org/wiki?curid=6863064", "title": "Contactless payment", "text": "Contactless payment\n\nContactless payment systems are credit cards and debit cards, key fobs, smart cards, or other devices, including smartphones and other mobile devices, that use radio-frequency identification (RFID) or near field communication (NFC, e.g. Samsung Pay, Apple Pay, Google Pay, Fitbit Pay, or any bank mobile application that support Contactless) for making secure payments. The embedded chip and antenna enable consumers to wave their card, fob, or handheld device over a reader at the point of sale terminal. Contactless payments are made in close physical proximity, unlike mobile payments which use broad-area cellular or WiFi networks and do not involve close physical proximity.\n\nSome suppliers claim that transactions can be almost twice as fast as a conventional cash, credit, or debit card purchase. Because no signature or PIN verification is typically required, contactless purchases are typically limited to small value sales. Lack of authentication provides a window during which fraudulent purchases can be made while the card owner is unaware of the card's loss.\n\nIn 2012, MasterCard Advisors wrote that consumers are likely to spend more money using their cards due to the ease of small transactions. MasterCard Canada says it has seen \"about 25 percent\" higher spending by users of its Mastercard Contactless-brand RFID credit cards.\nEMV is a common standard used by major credit card and smartphone companies for use in general commerce. Contactless smart cards that function as stored-value cards are becoming popular for use as transit system farecards, such as the Oyster card or RioCard. These can often store non-currency value (such as monthly passes) in additional to fare value purchased with cash or electronic payment.\n\nMobil was one of the most notable early adopters of a similar technology, and offered their \"Speedpass\" contactless payment system for participating Mobil gas stations as early as 1997. Although Mobil has since merged with Exxon, the service is still offered at many of ExxonMobil's stations. Freedompay also had early wins in the contactless space with Bank of America and McDonald's.\n\nMcDonald's, KFC, Burger King, Boots, Eat, Heron Foods, Pret a Manger, Stagecoach Group, Subway, AMT Coffee, Tesco, Asda and Lidl are among the retailers offering contactless payments to their customers in the UK. In March 2008, Eat became the first restaurant chain to adopt contactless.\n\nMajor financial entities now offering contactless payment systems include MasterCard, Citibank, JPMorgan Chase, American Express, KeyBank, Barclays, Barclaycard, HSBC, Lloyds Banking Group, FreedomPay, The Co-operative Bank, Nationwide Building Society and The Royal Bank of Scotland Group. Visa payWave, American Express Expresspay, and MasterCard Contactless are examples of contactless credit cards which have become widespread in the U.S. and UK.\n\nThe first contactless cards in the UK were issued by Barclaycard in 2007. , there are approximately 58 million contactless-enabled cards in use, in the UK, and over 147,000 terminals in use though this is growing in numbers and percentages of adoption.\n\nTelecom operators are starting to get involved in contactless payments via the use of near field communication phones. Belgacom's Pingping, for example, has a stored value account and via a partnership with Alcatel-Lucent's Touchatag provides contactless payment functionalities. In January 2010, Barclaycard partnered with mobile phone firm Orange, to launch a contactless credit card in the UK. Orange and Barclaycard also announced in 2009 that they would be launching a mobile phone with contactless technology.\n\nIn October 2011, the first mobile phones with MasterCard PayPass and/or Visa payWave certification appeared. A PayPass or payWave account can be assigned to the embedded secure element and/or SIM card within the phones. Google Pay is an application for devices running Google's Android OS, which allows users to make purchases using NFC, which initially required a physical secure element but this was replaced by host card emulation which was introduced in Android 4.4 (KitKat). Softcard (formerly known as Isis mobile wallet), Cityzi and Quick Tap wallets for example, use a secure SIM card to store encrypted personal information. Contactless payments with enabled mobile phones still occur on a small scale, but every month an increasing number of mobile phones are certified.\n\nIn February 2014, MasterCard announced that it would partner with Weve, which is a joint venture between EE, Telefónica UK, and Vodafone UK, to focus on mobile payments. The partnership will promote the development of \"contactless mobile payment systems\" by creating a universal platform in Europe for it.\n\nIn September 2014, Transport for London's Tube began accepting contactless payment. The number of completed contactless journeys has now exceeded 300m. On Friday 18 December, the busiest single day in 2015, a record 1.24m journeys were completed by over 500k unique contactless cards.\n\nIn 2016 Erste Group launched an NFC only debit card implemented as a sticker in Austria. It can be used at any NFC supporting terminal for transactions of unlimited amount however for transactions over the floor limit of 25 EUR a PIN is required to confirm the transaction.\n\nIn 2016, contactless payments start to become even broader with wearable technology devices also offering this payment feature.\n\nIn 2018, the Westpac Banking Corporation in Australia revealed contactless payment statistics from 2017 and claimed in the report that contactless payments approached saturation point by being used in over 90% of purchases. The Australian St.George Bank reported 94.6% usage for the same period.\n\nDepending on the economic space, there may be a payment limit on single transactions, and some contactless cards can only be used a certain number of times before customers are asked for their PIN. Contactless debit and credit transactions use the same chip and PIN network as older cards and are protected by the same fraud guarantees. Where PIN is supported, the contactless part of the card remains non-functional until a standard chip and PIN transaction has been executed. This provides some verification that the card was delivered to the actual cardholder.\n\nUnder fraud guarantee standards U.S. banks are liable for any fraudulent transactions charged to the contactless cards.\n\nBecause no signature or PIN verification is typically required, contactless purchases are typically limited to a set maximum amount per transaction, known as a \"floor limit\". Limits may vary between banks.\n\n"}
{"id": "7403045", "url": "https://en.wikipedia.org/wiki?curid=7403045", "title": "D2-MAC", "text": "D2-MAC\n\nD2-MAC is a satellite television transmission standard, a member of Multiplexed Analogue Components family. It was created to solve D-MAC's bandwidth problem on European cable systems.\n\nMAC transmits luminance and chrominance data separately in time rather than separately in frequency (as other analog television formats do, such as composite video).\n\nAudio and Scrambling (selective access)\n\nMAC was developed by the UK's Independent Broadcasting Authority (IBA) and in 1982 was adopted as the transmission format for the UK's forthcoming direct broadcast satellite (DBS) television services (eventually provided by British Satellite Broadcasting). The following year MAC was adopted by the European Broadcasting Union (EBU) as the standard for all DBS.\n\nBy 1986, despite there being two standards, D-MAC and D2-MAC, favoured by different countries in Europe, an EU Directive imposed MAC on the national DBS broadcasters, to provide a stepping stone from analogue PAL and SECAM formats to the eventual high definition and digital television of the future, with European TV manufacturers in a privileged position to provide the equipment required.\n\nHowever, the Astra satellite system was also starting up at this time (the first satellite, Astra 1A was launched in 1989) and that operated outside of the EU’s MAC requirements, due to being a non-DBS satellite. Despite further pressure from the EU (including a further Directive originally intended to make MAC provision compulsory in TV sets, and a subsidy to broadcasters to use the MAC format), most broadcasters outside Scandinavia preferred the lower cost of PAL transmission and receiving equipment.\n\nIn the 2000s, the use of D-MAC and D2-MAC ceased when the satellite broadcasts of the channels concerned changed to DVB-S format.\n\n\n"}
{"id": "946264", "url": "https://en.wikipedia.org/wiki?curid=946264", "title": "EPIA", "text": "EPIA\n\nVIA EPIA (\"VIA Embedded Platform Innovative Architecture\") is a series of mini-ITX, em-ITX, nano-ITX, pico-ITX and pico-ITXe motherboards with integrated VIA processors. They are small in size and consume less power than computers of comparable capabilities.\n\nThere are several types of VIA EPIA motherboard: Nano-ITX versions, based on the VIA CoreFusion processor series, and Mini-ITX form factor featuring VIA CPUs and northbridges in separate packaging. They were originally built for industrial applications but came to have a wide range of applications in the consumer market, such as carputers, firewalls, HTPCs, and Small File servers..\n\nThe VIA EPIA motherboards have the following designators:\n"}
{"id": "2848481", "url": "https://en.wikipedia.org/wiki?curid=2848481", "title": "Evernote", "text": "Evernote\n\nEvernote is a mobile app designed for note taking, organizing, task lists, and archiving. It is developed by the Evernote Corporation, headquartered in Redwood City, California. The app allows users to create notes, which can be formatted text, web pages or web page excerpts, photographs, voice memos, or handwritten \"ink\" notes. Notes can also have file attachments. Notes can be sorted into notebooks, tagged, annotated, edited, given comments, searched, and exported.\n\nEvernote is cross-platform, including support for iOS, Android, Microsoft Windows and macOS. Evernote is free to use with monthly usage limits, and offers paid plans for expanded or lifted limits.\n\nFounded by Stepan Pachikov, the Evernote Web service launched into open beta on June 24, 2008 and reached 11 million users in July 2011. In October 2010, under former CEO Phil Libin, the company raised a US$20 million funding round led by DoCoMo Capital with participation from Morgenthaler Ventures and Sequoia Capital. Since then, the company raised an additional $50 million in funding led by Sequoia Capital and Morgenthaler Ventures, and another $70 million in funding led by Meritech Capital and CBC Capital. On November 30, 2012, Evernote raised another $85 million in funding led by AGC Equity Partners/m8 Capital and Valiant Capital Partners.\nOn November 9, 2014, Evernote raised an additional $20 million in funding from Nikkei, Inc.\n\nOn May 7, 2013, TechCrunch reported that Evernote launched Yinxiang Biji Business into the Chinese market at the Global Mobile Internet Conference.\n\nLinda Kozlowski was named the Chief Operating Officer of Evernote in June 2015, after more than two years with the company, but left before the end of the year.\n\nLibin stepped down as CEO in July 2015 and was replaced by former Google Glass executive Chris O'Neill. In October 2015, the Evernote Corp. announced that the company was laying off 18 percent of its workforce and would be closing three out of 10 global offices. In February 2017, CEO O'Neill stated in a blog post that the business was now cash-flow positive. Sequoia Capital, one of Evernote's equity owners, said, \"It's great when a company starts to raise non-dilutive capital every day, which is called revenue.\"\n\nIn August 2018, Chief Technical Officer Anirban Kundu, Chief Financial Officer Vincent Toolan, Chief Product Officer Erik Wrobel, and head of HR Michelle Wagner left the company. Wrobel and Wagner both joined in 2016. On September 18, 2018, 54 employees—about 15 percent of the workforce—were laid off. In a blog post, O'Neill said, \"After a successful 2017, I set incredibly aggressive goals for Evernote in 2018. Though we have steadily grown, we committed too many resources too quickly. We built up areas of our business in ways that have proven to be inefficient. Going forward, we are streamlining certain functions, like sales, so we can continue to speed up and scale others, like product development and engineering.\" \n\nOn October 29, 2018, Evernote announced that Ian Small, former CEO of TokBox, would replace O'Neill as CEO of Evernote.\n\nIn 2010, the coding language for the suite was changed from C# for version 3.5 to C++ in version 4.0 to improve performance.\n\nAs well as the keyboard entry of typed notes, Evernote supports image capture from cameras on supported devices, and the recording of voice notes. In some situations, text that appears in captured images can be recognized using OCR and annotated. Evernote also supports touch and tablet screens with handwriting recognition. Evernote web-clipping plugins are available for the most popular Internet browsers that allow marked sections of webpages to be captured and clipped to Evernote. If no section of a webpage has been highlighted, Evernote can clip the full page. Evernote also supports the ability to e-mail notes to the service, allowing for automated note entry via e-mail rules or filters.\n\nWhere suitable hardware is available, Evernote can automatically add geolocation tags to notes. \n\nAs of November 2018, Evernote Pro integrates directly with Google Drive, Microsoft Outlook, Microsoft Teams, and Slack, and Evernote Pro adds an integration with Salesforce. All versions of Evernote also support integrations through IFTTT and Zapier. In 2013, Evernote deprecated its direct integration with Twitter in favor of these third-party services.\n\nOn supported operating systems, Evernote allows users to store and edit notes on their local machine, using a SQLite database in Windows.\n\nUsers with Internet access and an Evernote account can also have their notes automatically synchronized with a master copy held on Evernote's servers. This approach lets a user access and edit their data across multiple machines and operating system platforms, but still view, input and edit data when an Internet connection is not available. However, notes stored on Evernote servers are not encrypted.\n\nWhere Evernote client software is not available, online account-holders can access their note archive via a web interface or through a media device. The service also allows selected files to be shared for viewing and editing by other users. \n\nThe Evernote software can be downloaded and used as \"stand-alone\" software without using the online portion of an Evernote account (online registration is required for initial setup, however), but it will not be able to upload files to the Evernote server, or use the server to synchronize or share files between different Evernote installations. Also, no image or Image-PDF (Premium only) recognition and indexing will take place if the software is used entirely offline.\n\nEvernote is a free online service that allows users to upgrade to Plus*, Premium, or a Business account. Free, Plus* and Premium Evernote accounts have a maximum limit of 100,000 notes and 250 notebooks.\n\nBasic customers can upload 60 MB of data each month. Plus* customers get a 1 GB upload limit, offline notes on mobile devices, as well as passcode lock for mobile devices. Emails can also be sent to their Evernote account.\n\nPremium subscribers are granted 10 GB of new uploaded data every month, faster word recognition in images, heightened security, PDF annotation, Context, where notes and news articles can be seen, which are related to the open note and the ability to search text within PDF documents. They also receive additional options for notebook sharing. Each of free, Plus* and Premium account types allow notebook sharing with other Evernote users; however, the accounts are distinguished by editing capabilities. In regards to shared notebooks, editing permissions to non-paid account holders may only be granted to premium Evernote subscribers. The free service does not make files available offline on iOS and Android devices; while sometimes they are available from cache, editing these files can cause conflicts when synchronizing.\n\nWith the full version of Evernote Business, users sync and view work documents through a variety of platforms, such as Mac, iPhone and iPads, Web, Windows and Android Devices. Files that can be uploaded include spreadsheets, presentations, notes and design mock ups. In addition, administrators can monitor company progress and individual employees through the admin console.\n\nIn June 2016, Evernote announced the limitation for users of its free Basic account to two devices per year and raised prices for its premium service tiers. Non-paying Evernote users are able to sync notes between two devices. Plus* lets users store notes offline and upload up to 1GB files, while Premium adds document-parsing features and 9GB of additional storage.\n\nFrom early April 2018, Evernote Plus was no longer available for purchase, however, users who currently have the Plus subscription can maintain it as long as their subscription is still active.\n\nEvernote clients are available for Microsoft Windows, macOS, Android, iOS (iPhone, iPad, iPod Touch), Windows Mobile, Windows Phone, WebOS, Maemo, BlackBerry (including BlackBerry Playbook), and Google Wave platforms as well as a beta for Symbian S60 5th Edition. There are portable versions of Evernote available for flash drives and U3 drives. There is currently no officially supported native client for Linux or BSD, but the company provides an API for external Linux clients.\n\nThere is substantial variation in supported features on different platforms: For example, it is possible to edit Rich Text Format and sketches on Windows; on Mac it is possible to edit rich text, but only view sketches; and on the iPad only plain text could be edited prior to version 4.1.0 (August 2011).\n\nWeb clipping support is installed by default on the Internet Explorer and Safari browsers when the Evernote software is installed under Windows or macOS. Evernote web-clipping plugins are also available for the Yandex Browser, Firefox, Opera, and Google Chrome browsers, and need to be downloaded and installed separately from the respective browser.\n\nThe Evernote email-clipper is automatically installed in Microsoft Office Outlook if the desktop version is installed on the same computer. There is a Thunderbird email plugin, which must be installed separately from the Thunderbird client.\n\nScannable captures paper quickly, transforming it into high-quality scans ready to save or share.\n\nSkitch is a free screenshot editing and sharing utility for OS X, iOS, Windows, and Android. The app permits the user to add shapes and text to an image, and then share it online. Images can also be exported to various image formats. Originally developed by Plasq, Skitch was acquired by Evernote on August 18, 2011. On December 17, 2015, Evernote announced that it will be ending support for Skitch for Windows, Windows Touch, iOS, and Android on January 22, 2016. Evernote said it will continue to offer Skitch for Mac.\n\nEvernote Web Clipper is a simple extension for your web browser that lets you capture full-page articles, images, selected text, important emails, and any web page.\n\nOn March 25, 2013, Evernote announced a partnership with Deutsche Telekom to provide German customers with free access to Evernote Premium for one year. In January 2014 the partnership was expanded to additional European markets.\n\nIn August 2012 Moleskine partnered with Evernote to produce a digital-friendly notebook with specially designed pages and stickers for smartphone syncing.\n\nAll Samsung Galaxy Note 3 phablets included a free one-year subscription to Evernote Premium.\n\nOn August 13, 2013, \"The New York Times\" reported that Telefónica Digital and Evernote entered into a global partnership agreement, giving Brazilian customers free access to Evernote Premium for one year. Under this global deal Telefónica users in Costa Rica, Guatemala, Panama, the UK and Spain were also offered the promotion.\n\nThe service has experienced several cases of losing customer data.\n\nOn June 11, 2014, Evernote suffered a crippling distributed denial-of-service attack that prevented customers from accessing their information. The attackers demanded a ransom, which Evernote refused to pay. A denial-of-service attack on August 8, 2014 resulted in a brief period of downtime for evernote.com. Service was quickly restored.\n\nOn March 2, 2013, Evernote revealed that hackers had gained access to their network and been able to access user information, including usernames, email addresses, and hashed passwords. All users were asked to reset their passwords.\nIn the wake of this, Evernote accelerated plans to implement an optional two-factor authentication for all users.\n\nIn December 2016, Evernote announced its privacy policy would be changing in January 2017, leading to claims the policy allowed employees of the firm to access users' content in some situations. In response to the concerns, Evernote apologised and announced the policy would now not be implemented, and that its employees would not have access to users' content unless users opted in.\n\n"}
{"id": "528080", "url": "https://en.wikipedia.org/wiki?curid=528080", "title": "FADEC", "text": "FADEC\n\nA full authority digital engine (or electronics) control (FADEC) is a system consisting of a digital computer, called an \"electronic engine controller\" (EEC) or \"engine control unit\" (ECU), and its related accessories that control all aspects of aircraft engine performance. FADECs have been produced for both piston engines and jet engines.\n\nThe goal of any engine control system is to allow the engine to perform at maximum efficiency for a given condition. Originally, engine control systems consisted of simple mechanical linkages connected physically to the engine. By moving these levers the pilot or the flight engineer could control fuel flow, power output, and many other engine parameters. The mechanical/hydraulic engine control unit for Germany's BMW 801 piston aviation radial engine of World War II was just one notable example of this in its later stages of development. This mechanical engine control was progressively replaced first by analog electronic engine control and, later, digital engine control. \n\nAnalog electronic control varies an electrical signal to communicate the desired engine settings. The system was an evident improvement over mechanical control but had its drawbacks, including common electronic noise interference and reliability issues. Full authority analogue control was used in the 1960s and introduced as a component of the Rolls-Royce/Snecma Olympus 593 engine of the supersonic transport aircraft Concorde. However, the more critical inlet control was digital on the production aircraft.\n\nDigital electronic control followed. In 1968 Rolls-Royce and Elliott Automation, in conjunction with the National Gas Turbine Establishment, worked on a digital engine control system that completed several hundred hours of operation on a Rolls-Royce Olympus Mk 320. In the 1970s, NASA and Pratt and Whitney experimented with their first experimental FADEC, first flown on an F-111 fitted with a highly modified Pratt & Whitney TF30 left engine. The experiments led to Pratt & Whitney F100 and Pratt & Whitney PW2000 being the first military and civil engines, respectively, fitted with FADEC, and later the Pratt & Whitney PW4000 as the first commercial \"dual FADEC\" engine. The first FADEC in service was the Rolls-Royce Pegasus engine developed for the Harrier II by Dowty and Smiths Industries Controls.\n\nTrue full authority digital engine controls have no form of manual override available, placing full authority over the operating parameters of the engine in the hands of the computer. If a total FADEC failure occurs, the engine fails. If the engine is controlled digitally and electronically but allows for manual override, it is considered solely an EEC or ECU. An EEC, though a component of a FADEC, is not by itself FADEC. When standing alone, the EEC makes all of the decisions until the pilot wishes to intervene.\nFADEC works by receiving multiple input variables of the current flight condition including air density, throttle lever position, engine temperatures, engine pressures, and many other parameters. The inputs are received by the EEC and analyzed up to 70 times per second. Engine operating parameters such as fuel flow, stator vane position, air bleed valve position, and others are computed from this data and applied as appropriate. FADEC also controls engine starting and restarting. The FADEC's basic purpose is to provide optimum engine efficiency for a given flight condition.\n\nFADEC not only provides for efficient engine operation, it also allows the manufacturer to program engine limitations and receive engine health and maintenance reports. For example, to avoid exceeding a certain engine temperature, the FADEC can be programmed to automatically take the necessary measures without pilot intervention.\n\nWith the operation of the engines so heavily relying on automation, safety is a great concern. Redundancy is provided in the form of two or more separate but identical digital channels. Each channel may provide all engine functions without restriction. FADEC also monitors a variety of data coming from the engine subsystems and related aircraft systems, providing for fault tolerant engine control.\n\nEngine control problems simultaneously causing loss of thrust on up to three engines have been cited as causal in the crash of an Airbus A400M aircraft at Seville Spain on 9 May 2015. Airbus Chief Strategy Officer Marwan Lahoud confirmed on 29 May that incorrectly installed engine control software caused the fatal crash. \"There are no structural defects [with the aircraft], but we have a serious quality problem in the final assembly.\"\n\nA typical civilian transport aircraft flight may illustrate the function of a FADEC. The flight crew first enters flight data such as wind conditions, runway length, or cruise altitude, into the flight management system (FMS). The FMS uses this data to calculate power settings for different phases of the flight. At takeoff, the flight crew advances the throttle to a predetermined setting, or opts for an auto-throttle takeoff if available. The FADECs now apply the calculated takeoff thrust setting by sending an electronic signal to the engines; there is no direct linkage to open fuel flow. This procedure can be repeated for any other phase of flight.\n\nIn flight, small changes in operation are constantly made to maintain efficiency. Maximum thrust is available for emergency situations if the throttle is advanced to full, but limitations can’t be exceeded; the flight crew has no means of manually overriding the FADEC.\n\n\n\n\nNASA has analyzed a distributed FADEC architecture rather than the current centralized, specifically for helicopters. Greater flexibility and lower life cycle costs are likely advantages of distribution.\n\n\n\n"}
{"id": "14719154", "url": "https://en.wikipedia.org/wiki?curid=14719154", "title": "Float voltage", "text": "Float voltage\n\nFloat voltage is the voltage at which a battery is maintained after being fully charged to maintain that capacity by compensating for self-discharge of the battery. The voltage could be held constant for the entire duration of the cell's operation (such as in an automotive battery) or could be held for a particular phase of charging by the charger. The appropriate float voltage varies significantly with the chemistry and construction of the battery, and ambient temperature.\n\nWith the appropriate voltage for the battery type and with proper temperature compensation, a float charger may be kept connected indefinitely without damaging the battery.\n\nHowever, it should be understood that the concept of a float voltage does not apply equally to all battery chemistries. For instance, lithium ion cells have to be float charged with extra care because if they are float charged at just a little over optimum voltage, which is generally the full output voltage of the lithium cell, the chemical system within the cell will be damaged to some extent. Some lithium ion variants are less tolerant than others, but generally overheating, which shortens cell life is likely, and fire and explosion possible other outcomes. It is important to make certain that the battery cell involved can be safely float charged, and that the charger circuit goes into float charge status when full charge is achieved.\nAccepted average float voltages for lead-acid batteries at 25 °C can be found in following table:\n\nCompensation per cell of approximately −3.9 mV/°C (−2.17 mV/°F) of temperature rise is necessary.\nA 12 V (6-cell) battery at 30 °C (86 °F) (+5 °C change):<br>\n<br>\nA 12 V (6-cell) battery at 20 °C (68 °F) (−5 °C change):<br>\n<br>\nNot compensating for temperature will shorten battery life by over- or undercharging.\n\n"}
{"id": "31336130", "url": "https://en.wikipedia.org/wiki?curid=31336130", "title": "Glass frit bonding", "text": "Glass frit bonding\n\nGlass frit bonding, also referred to as glass soldering or seal glass bonding, describes a wafer bonding technique with an intermediate glass layer. It is a widely used encapsulation technology for surface micro-machined structures, e.g., accelerometers or gyroscopes. This technique utilizes low melting glass (\"glass solder\") and therefore provides various advantages including that viscosity of glass decreases with an increase of temperature. The viscous flow of glass has effects to compensate and planarize surface irregularities, convenient for bonding wafers with a high roughness due to plasma etching or deposition. A low viscosity promotes hermetically sealed encapsulation of structures based on a better adaption of the structured shapes. Further, the coefficient of thermal expansion (CTE) of the glass material is adapted to silicon. This results in low stress in the bonded wafer pair.\n\nGlass frit bonding can be used for many surface materials, e.g., silicon with hydrophobic and hydrophilic surface, silicon dioxide, silicon nitride, aluminium, titanium or glass, as long as the CTE are in the same range. This bonding procedure also allows the realization of metallic feedthroughs to contact active structures in the hermetically sealed cavity. Glass frit as a dielectric material does not need additional passivation for preventing leakage currents at process temperatures up to 125 °C.\n\nFollowing advantages resulting in using glass frit bonding procedure:\n\nThe glass frit bond procedure is used for the encapsulation and mounting of components. The coating of glass frit layers is applied by spin coating for thickness of 5 to 30 µm or commonly by screen printing for thickness of 10 to 30 µm.\n\nTo achieve process temperatures beneath 450 °C leaded glass or lead silicate glass is used. The glass frit is a paste consisting glass powder, organic binder, inorganic fillers and solvents. This low melting glass paste is milled into powder (grain size < 15 µm) and mixed with organic binder forming a printable viscous paste. Inorganic fillers, i.e. cordierite particles (e.g. MgAl [AlSiO]) or barium silicate, are added to the melted glass paste to influence properties, i.e. lowering the mismatch of thermal expansion coefficients between silicon and glass frit. The solvents are used to adjust the viscosity of the organic binder. Several glass frit pastes are commercially available, e.g. FERRO FX-11-0366, and every single one need individual handling after deposition. The choice of the paste depends on various factors, i.e. deposition method, substrate material and process temperatures.\n\nThe glass used for MEMS applications consists of particles and lead oxide. Latter lowers the glass transition temperature below 400 °C. The reduction of lead oxide by the silicon leads to the formation of lead precipitations at the silicon-glass interface. Those precipitations decrease the strength of the bond and are reliability risks that have to be considered for the lifetime predictions of the devices.\n\nThe printed glass frit structures are heated to form compact glass. The heating process is necessary to drive out the solvents and binder. This results in a subsequent particle fusion of the glass powder. Using mechanical pressure the wafers are bonded at elevated temperatures.\n\nThe procedural steps of glass frit bonding are divided into the following:\n\n\nScreen printing, as a commonly used deposition method, provides a technique of structuring for the glass frit material. This method has the advantage of material deposition on structured cap wafers without any additional processes, i.e. photolithography.\n\nScreen printing enables the possibility of selective bonding. So only in areas where bonding is required the glass frit is deposited.\n\nThe risk of glass frit flowing into the structures can be prevented by optimization of the screen printing process. Under high positioning precision the sizes of the structures in the range of 190 µm with a minimum spacing of < 100 µm are achievable. The exact positioning of the screen print structures to the cap wafer are required to ensure an accurate bond. The bonded structures are, dependent on the wettability of the printed surface, 10 to 20% wider than the designed screen.\n\nTo ensure a uniform glass thickness, all structures should have the same width. The printed glass frit high is about 30 µm and provides a gap of 5 to 10 µm between the bonded wafers after bonding (compare to cross sectional SEM images). A bond surface activation is not necessary to promote a higher bonding strength.\n\nThermal conditioning transforms the glass paste into glass layer and is important to prevent voids inside the glass frit layer. The conditioning process consists of:\n\n\nThe initial step comprises drying for 5 to 7 minutes at 100 to 120 °C in order to diffuse solvents out of the interface. This starts the polymerization of the organic binder. The binder molecules are linked to long-chain polymers what solidifies the paste.\n\nThe organic binder of the glass paste has to be burned with heating up to a specific temperature (325 to 350 °C) where the glass is not fully melted for 10 to 20 minutes. This so-called glazing ensures the outgassing of the organic additives.\n\nFurther, a pre-melting or sealing step heats the material to the process temperature between 410 and 459 °C for 5 to 10 min. The material fully melts and forms a compact glass without any inclusions. The inorganic fillers are melted down and the properties of the bond glass are fixed. The melting of the glass starts at the silicon-glass interface directed to the glass surface. During the melting process the porosity of the glass eliminates and based on the compression of the intermediate layer the thickness of the glass decreases significantly.\n\nThe glass frit bonding, starting with alignment of the wafers, is a thermo-compressive process that takes place in the bonding chamber at specific pressure. Under bonding pressure wafers are heated up to the process temperature around 430 °C for a few minutes. On the one hand a short bonding time causes the glass frit to spread insufficiently, on the other hand a longer bonding time causes the glass frit to be overflown subsequently leaving voids.\n\nThe alignment has to be very precise and stable to prevent shifting. This can be realized using clamps or special pressure plates. Shifting can occur through temporarily staggered pressure, not precise vertical pressure based on misalignment of the bonding tools or the difference of thermal expansion between the bonding tools.\n\nDuring bonding a supporting tool pressure is applied to improve the thermal input into the bonding glass and equal wafer geometry inadmissibility (i.e. bow and warp) supporting wettability. Based on the sufficiently high viscosity of the glass, bonding can take place nearly without pressure.\n\nThe bonding temperature needs to be high enough to reduce the viscosity of the glass material and ensures a good wetting of the bond surface, but also low enough to prevent overspreading of the glass frit material. The heating up over 410 °C enables the wetting of the bond surface. A good wetting is indicated by a low edge angle. The atomic wafer surface layers are fused into the glass at an atomic level. This forms a thin glass mixture at the interface which forms the strong bond between the glass and the wafer.\n\nDuring cooling down under pressure a mechanically strong and hermetically sealed wafer bond is formed. The cooling process leads especially at higher temperatures to thermal stress in the glass frit layer that has to be considered in the lifetime analysis of the bond frame. The wafer pair is removed from the bond chamber at lower temperatures to prevent thermal cracking of the wafers or the bond interface by thermal shocks.\n\nThe bonding strength is mainly dependent on the density, the spreading area of the glass frit layer and the surface layer of the bonding interface. It is high enough, around 20 MPa, for most applications and comparable to those achieved with anodic bonding. The hermeticity ensures the correct function and a sufficient reliability of the bond and therefore the product. Further, the bonding yield of glass frit bonded wafers is very high, normally > 90 %.\n\nGlass frit bonding is used to encapsulate surface micro-machined sensors, i.e. gyroscopes and accelerometers. Other applications are the sealing of absolute pressure sensor cavities, the mounting of optical windows and the capping of thermally active devices.\n"}
{"id": "6986061", "url": "https://en.wikipedia.org/wiki?curid=6986061", "title": "Gym floor cover", "text": "Gym floor cover\n\nGym floor covers can either be a carpet-based protection system or is a large plastic tarp, similar to a painters tarp, usually divided into equal sections wide each to cover up the entire gym floor. Gym floor covers are available in roll or tile systems.These covers are often used in large venues designed for athletic events when non-athletic events are being held there such as receptions, award ceremonies, indoor track meets, gymnastic events and graduations. \n\nGym floor covers are employed to prevent floor damage and improve safety, allowing for sports venues to be used for other purposes. Covers are used to prevent slip and fall accidents, while protecting underlying hardwood floor from foot traffic damage and heavy furniture scratches and dents. The newer carpet topcloth systems also improve acoustics in the gymnsium.\n\nThe first plastic tarp gym floor cover was invented and manufactured by Covermaster Inc. in 1969 with Seattle-based Floor Guardian Inc. following up by manufacturing a recycled content, removable, portable carpet based protection system in the late 1970s to answer the demand for a product that was less of a trip hazard, environmentally sound and classier looking than what many felt was essentially a painter's tarp. Since then, the quality of the materials has greatly evolved. Most manufacturers have switched from vinyl and polyethylene materials to more durable polyester fabric. In 2013, Protex Matting Inc. answered environmental concerns by introducing the newest 100% recycled polyester top cloth gym floor covers made completely from recycled plastic water bottles, reusing approximately 7000 water bottles for every 10,000 sq. ft produced . The best plastic tarp gym floor covers today are made of single-layer woven polyester and the newest polymer materials made via the knife-coating process, which enables dyes to be deposited deep into the core of the PVC. material and prevent peeling of the layers - a side effect of extrusion coating or lamination process..\n\nModern gym floor covers are manufactured with a variety of colors, anti-slip surfaces, and weights ranging from . Custom sizing is available with most products. The following technical characteristics are used to describe and classify the covers: filament size, weave count, total weight, core weight, tear strength, tensile strength, adhesion, coefficient of friction, slip resistance, hydrostatic resistance, fire resistance and others.\n\nThere are several common ways to install floor coverings. For the tarp method, each section is laid down to slightly overlap adjacent sections and then secured with special adhesive tape. This fastens the individual segments together into a cohesive unit. The carpet method, by contrast, affixes the segments to the floor via reusable velcro straps. Newer tile systems allow more flexibility in installation and do not require adhesive or velcro tape, but are much more labor intensive for larger floors.\n\nInstalling and deploying large covers can be labour-intensive task. The most efficient way involves rolling and unrolling each section separately, and storing them on the gym floor cover storage system (rack) in a rolled up state to prevent creases and dust accumulation. Advances in the newer Protex GymPro products allow for smaller storage racks, while the newer tile systems are stacked on compact platform trucks for storage and transport. Dual cleaning brush assemblies (to remove debris) and electric power winders (to reduce manual labor during cover removal) are normally used to efficiently maintain the covers. Carpet based gym covers are best cleaned using more traditional carpet cleaning techniques.\n\nPlastic tarp gym floor covers have a typical lifetime of 10–15 years. Carpet-based covers have a lifetime of 15–20+ years.\n\n"}
{"id": "58179215", "url": "https://en.wikipedia.org/wiki?curid=58179215", "title": "Hot Particulate Ingestion Rig", "text": "Hot Particulate Ingestion Rig\n\nThe Hot Particulate Ingestion Rig (HPIR) is a gas burner that can shoot sand into a hot gas flow and onto a target material to test how that material’s thermal barrier coating is impacted by the molten sand. It was developed by the U.S. Army Research Laboratory (ARL) to experiment with new coating materials for gas turbine engines used in military aircraft.\n\nThe HPIR uses standard military fuel and dry compressed air to produce combusted gas flows that can range from 400 °C to 1650 °C that travels as fast as 1060 meters per second or 0.8 Mach. A LabVIEW interface is used to monitor and control all the operations of the HPIR parameters and pneumatic table. Monitoring is also performed by Williamson PRO series single/dual wavelength pyrometers, S-type thermocouples, and a FLIR SC6700 mid-wave infrared (IR) camera in order to determine the emissivity of each sample.\n\nSamples are placed in a steel holder in front of the rig at a 10 degree incident angle so that heats up the surface in a uniform manner. A pneumatic table moves the sample into the flame and an S-type thermocouple is used to monitor the flame’s temperature. During testing, the sample is initially exposed to a hot gas flow at 0.28 Mach at a flame temperature of 815 °C until the pyrometer detects that the surface temperature of the target has reached 540 °C. Then, the sample goes through several cycles of heating and cooling as an initial survivability check before it can be exposed to even higher temperatures. Short-term durability testing consists of three of these cycles with the heating stage reaching engine-relevant temperatures and the cooling stage set at ambient conditions.\n\nIn 2016, the HPIR was modified to ingest sand and salt into the combustion chamber at 1 to 200 grams per minute.\n\nIn 2015, researchers at ARL were tasked with finding a way to prevent flying, micron-sized sand and dust particles from entering the gas turbine engines of military aircraft and damaging the internal machinery.\n\nWhile modern engines have particle separators that can filter out large particles, fine, powder-like sand particles that are smaller than 100 micrometers in size have consistently managed to pass through the engine’s combustors and attach to the blades and vanes. As the rotor blades experienced cycles of heating and cooling during operation, the particles melted due the extreme temperatures and then subsequently hardened onto the turbine blades. As a result, the micron-sized sand particles have frequently destroyed the engine’s internal coating, which has led to severe sand glazing, blade tip wear, calcia-magnesia-alumina-silicate (SMAS) attack, oxidation, plugged cooling holes, and, ultimately, engine loss. This problem has recently worsened due to the fact that more recent, state-of-the-art turbine engines operate at much higher temperatures than past generation turbomachinery, ranging from 1400 °C to 1500 °C.\n\nAccording to ARL scientists, the damage caused by these tiny sand particles have reduced the lifespan of a typical T-700 engine from 6000 hours to 400 hours, and replacing the rotors can cost more than $30,000. They estimate that one third of fielded engines used by the military have been affected by this sand ingestion problem.\n\nAs part of a collaborative research effort with the U.S. Army Aviation and Missile Research, Development, and Engineering Center (AMRDEC), the U.S. Navy Naval Air Systems Command (NAVAIR) and the National Aeronautics and Space Administration (NASA), ARL modified the HPIR so that it can model how sand particles adhere, melt, and glassify on thermal barrier coatings.\n\nAccording to ARL researchers, the HPIR is the first system to confirm how the sand particles damage the turbine blades at temperatures similar to that of a turbine engine out on the field. Using high-speed imaging technology, ARL scientists were able to film how sand particles experience a phase change from solid to liquid before being deposited onto turbine blade material targets and vaporizing. In 2018, the team used the HPIR to test different coating materials and develop what they call “sandphobic coatings,” which will be designed so that the sand particles flake off the rotor blades instead of attaching to them.\n"}
{"id": "22904524", "url": "https://en.wikipedia.org/wiki?curid=22904524", "title": "Human-in-the-loop", "text": "Human-in-the-loop\n\nHuman-in-the-loop or HITL is defined as a model that requires human interaction. HITL is associated with modeling and simulation (M&S) in the live, virtual, and constructive taxonomy. HITL models may conform to human factors requirements as in the case of a mockup. In this type of simulation a human is always part of the simulation and consequently influences the outcome in such a way that is difficult if not impossible to reproduce exactly. HITL also readily allows for the identification of problems and requirements that may not be easily identified by other means of simulation.\n\nHITL is often referred to as interactive simulation, which is a special kind of physical simulation in which physical simulations include human operators, such as in a flight or a driving simulator.\n\nHuman-in-the-loop allows the user to change the outcome of an event or process. HITL is extremely effective for the purposes of training because it allows the trainee to immerse themselves in the event or process . The immersion effectively contributes to a positive transfer of acquired skills into the real world. This can be demonstrated by trainees utilizing flight simulators in preparation to become pilots.\n\nHITL also allows for the acquisition of knowledge regarding how a new process may affect a particular event. Utilizing HITL allows participants to interact with realistic models and attempt to perform as they would in an actual scenario. HITL simulations bring to the surface issues that would not otherwise be apparent until after a new process has been deployed. A real-world example of HITL simulation as an evaluation tool is its usage by the Federal Aviation Administration (FAA) to allow air traffic controllers to test new automation procedures by directing the activities of simulated air traffic while monitoring the effect of the newly implemented procedures.\n\nAs with most processes, there is always the possibility of human error, which can only be reproduced using HITL simulation. Although much can be done to automate systems, humans typically still need to take the information provided by a system to determine the next course of action based on their judgment and experience. Intelligent systems can only go so far in certain circumstances to automate a process; only humans in the simulation can accurately judge the final design. Tabletop simulation may be useful in the very early stages of project development for the purpose of collecting data to set broad parameters, but the important decisions require human-in-the-loop simulation.\n\nVirtual simulations inject HITL in a central role by exercising motor control skills (e.g. flying an airplane), decision making skills (e.g. committing fire control resources to action), or communication skills (e.g. as members of a C4I team).\n\n\nAlthough human-in-the-loop simulation can include a computer simulation in the form of a synthetic environment, computer simulation is not necessarily a form of human-in-the-loop simulation, and is often considered as human-out-of-the loop simulation. In this particular case, a computer model’s behavior is modified according to a set of initial parameters. The results of the model differ from the results stemming from a true human-in-the-loop simulation because the results can easily be replicated time and time again, by simply providing identical parameters.\n\n"}
{"id": "40000349", "url": "https://en.wikipedia.org/wiki?curid=40000349", "title": "IcCube", "text": "IcCube\n\nicCube is a company founded in Switzerland that provides business intelligence software of the same name. The solution can be fully embedded as an integrated solution, can be hosted in a managed environment or installed locally, on premises.\n\nThe BI tool allows end-users to create or edit dashboards themselves and is capable of processing data from multiple sources in real-time. The solution distinguishes itself by making the dashboards, the dashboard builder, the schema/cube builder and the server monitoring application accessible from a browser only. No software has to be installed at the device of the end-user.\n\nNext to the browser-based dashboard builder, data can be accessed by running queries directly on the OLAP cube using MDX, SQL or R.\n\nicCube sells an online analytical processing (OLAP) server.\n\nStarting in June 2010 with its first public community version (0.9.2). Since then, the company has released new versions multiple times per year.\n\nicCube is implemented in Java and follows J2EE standards. For the latter, it embeds both an HTTP server (Jetty) and a servlet container to handle all the communication tasks.\n\nBeing an in-memory OLAP server, the icCube server does not need to source its data from a RDBMS; any data source that exposes its data in a tabular form can be used; several plugins exists for accessing files, HTTP stream, etc. Accessing datasource that expose JSON objects is also supported (e.g., MongoDB). icCube is then taking care of possibly complex relations (e.g., many-2-many) implied by the JSON structure.\n\nAccessing icCube (cube modeling, server monitoring, MDX queries, Web reporting and dashboards) is performed through a unique Web interface.\n\nThe icCube OLAP server does not use any caching or pre-aggregation mechanism.\n\nicCube uses Multidimensional Expressions (MDX) as its query language and several extensions to the original language : function declarations, vector (even at measures level), matrix, objects, Java and R interactions.\nicCube patented an MDX debugger.\nicCube supports a standard interface and a proprietary one.\nThe XML for Analysis (XMLA protocol can connect to any XMLA compatible reporting tool.\n\nicCube supports its own proprietary protocol called GVI. HTTP based, it can be extended.\nThis protocol leverages the Google Visualization wire protocol. Javascript is the primary implementation language and a Java mapping library is also available.\n\n\n"}
{"id": "24614319", "url": "https://en.wikipedia.org/wiki?curid=24614319", "title": "Industrial Arts Curriculum Project", "text": "Industrial Arts Curriculum Project\n\nIndustrial Arts Curriculum Project (IACP) was established by Donald G. Lux and Willis Ray, the IACP project coordinators, to established an industrial arts curriculum concerned with the instructional representation of the structure of knowledge. They placed their work within the classification of praxiological knowledge, asserting that their quest was nothing less than to identify the knowledge base that underlies all practical and vocational arts subject areas.\n\nThis joint project between Ohio State University and the University of Illinois was initiated in June, 1965.\n\nThe IACP was designed for 7th and 8th grade junior high grade levels (before the middle school transition), and had two year-long courses, \"The World of Construction\" and \"The World of Manufacturing\" (McKnight and McKnight Publishers, 1970). The concept was based on the idea that there is virtually nothing that people use on a daily basis that is not either constructed or manufactured, and that children should be taught the basis of those technologies at an early age to prevent \"technological illiteracy.\"\n\nThe World of Construction, commonly the 7th grade curriculum, taught students about how residential and commercial buildings are built. Students got a taste of woodworking, concrete, electrical and other construction trades. The final big project was to build a cross-section of a house in the traditional wood shop in the school.\n\nIn 8th grade, \"The World of Manufacturing\" was taught in the traditional metal shop of the school. Students learned about metal working crafts and trades, and as a culminating activity, set up an assembly line to create a number of items. (One of the items was a screwdriver, and whenever the students took them home, most of the screws in the school had been removed by the end of the day, causing maintenance headaches for caretakers.)\n\nFull text of the IACP teacher guides are here:\nSource: personal experience of the author at The Ohio State University\n"}
{"id": "16948598", "url": "https://en.wikipedia.org/wiki?curid=16948598", "title": "InfoInterActive", "text": "InfoInterActive\n\nInfoInterActive is a telecommunications company based in the Canada. It was acquired by AOL on May 19, 2001.\n\nInfoInterActive was merged into AOLbyPhone.\n"}
{"id": "8688920", "url": "https://en.wikipedia.org/wiki?curid=8688920", "title": "Joan Hodges Queneau Award", "text": "Joan Hodges Queneau Award\n\nThe Joan Hodges Queneau Award is an American engineering award for the field of environmental conservation.\n\nIt has been given annually since 1976 for an \"outstanding contribution by an engineer in behalf of environmental conservation\". The award is administered by the National Audubon Society, and made jointly with the American Association of Engineering Societies. The award includes a citation, the \"Palladium Medal\", and a bronze statue.\n\n\n\n"}
{"id": "7901440", "url": "https://en.wikipedia.org/wiki?curid=7901440", "title": "Kibla", "text": "Kibla\n\nKibla or KIBLA is the first presentation and production institution in Slovenia dealing with multimedia and intermedia art and a yearlong cultural programme. Kibla incorporates pure classical (»excluded«) media but in a different context, using various media to support historical continuities in visual arts and a larger aesthetization and integration of electronic media. Kibla supports principles that lead to complex systems of multimedia presentation. Kibla also produces and coproduces cultural artistic projects and publishs in various media formats, printed and digital.\nMultimedia Centre CyberSRCeLab – MMC KIBLA was founded on the 4th of July 1996 as a project of Narodni dom Maribor and Open Society Institute Slovenia with the aim of training computer users in the Maribor area, with free Internet access, information and advisory services on usage and data bases, and education consultation. Since the 16th of June 1998 MMC KIBLA staff has also had legal status as a Slovenian Association for Culture and Education (ACE KIBLA).\n\nKibla presents, distributes and promotes the work of the Multimedia Centres Network of Slovenia (M3C, established in 2004 )in sixteen multimedia centres in different regions. Kibla operates in the field of visual and intermedia Slavic culture and art and is part of the Slavic Culture Forum.\n\nCyber offers a free internet access and free courses about the use of internet through website structures, programme and hardware equipment, together with audio and projectional infrastructure for musical, online, video and other presentations.\nSRCe – Student Resource Centre offers information about study possibilities, national and foreign scholarship foundations, publications and mediates requests with the help of the large database on the internet. SRCe is incorporated into the Slovenian Resource Network.\n\nKiBela, space for visual and intermedia art; ambiental presentations of contemporary multimedia art, KiBela is also a multipurpose place suitable for seminars, presentations, lectures, round tables, press conferences and rich cultural programme.\n\nHidden Notes (Skrite note) - is a musical programme that introduces electroacoustic music, concerts, projections, lectures, workshops …\n\nIT@K – IT at Kibla –informational technology as a social event\nGraphic studio for the making of printed material, CGP…\nOnline and multimedia lab for the execution of websites, CD-ROMs, video, audio, internet transmissions in real time life, etc.\n\nVidela – digital video processing, educations, courses, presentations and workshops.\n\nZa:misel (For:thought) bookstore for sociology and humanistic studies with regular programme of book presentations and literary evenings; with a reading room equipped with national and foreign literature, newspapers and periodics.\n\nMimogrede (By The Way) – bimonthly with information about studying abroad and in Slovenia.\n\nLED display for all information about MMC KIBLA and programmes;\n\nTOX magazine – time-table through 3000, magazine (from 1995) that grew into the KIBLA publishing edition (from 1998), made several catalogues and books, e.g. Eduardo Kac: Telepresence, Biotelematics, Transgenic Art, Vili Ravnjak: The Amber way, Aleksandra Kostič’s edited essays on Levitation, catalogues for Marko Jakše, Marko Črtanec, Mitjja Ficko, Theo Botscuijver, Shuzo Azuchi Gulliver, several CDs (Nino Mureškič, Vasko Atanasovski, Siti hlapci and CD-ROMs (for EU project European Multimedia Accelerator- EMMA) and DVDs for EU project txoOm.\n\nCommunication-information point KIT in Maribor City Hall (Rotovž), Glavni trg 14, where thera are eight computer terminals and a multimedia classroom with ten computers and additional IT equipment.\n\n\n\nKIBLA is based on openness and collaboration with other institutions and individuals. KIBLA has (co)organised several events and festivals and (co)produced various events e.g. International Festival of Computer Arts (with Art Gallery Maribor and MKC), Days of Curiosity – How to become, festival of edutainment (in collaboration with or with support from Land Museum, Land Archive, Cinema Maribor, Pekarna, First, Second and Third High School, High School for Tourism etc.), Internet radio MARŠ (with Maribor Student Radio), Festival of visual communications Magdalena (with Pekarna), Linux Install Fest (with Linux User Group of Slovenia), International Peace School (with OSI, Pekarna, MARŠ, Maribor High school association, University student association, International Peace organisations), World congress of cyclist Velo-City (with Maribor, Slovene and European cyclist association), Festival Lent (with Narodni dom Maribor). Frequently KIBLA collaborates with the Association of Architects and host their exhibitions, lectures, debates, and workshops. KIBLA prepared the artistic programme for the Hermes SoftLab Company 1998 New Year’s reception, and for the opening ceremonies of fairs, e.g. Electronics, Informatics. KIBLA collaborated on the Virtual museum project on the Ljubljana Castle, first Slovenian virtual environment, together with the City of Ljubljana, Festival Ljubljana, City Museum Ljubljana, Tobačna Ljubljana and Pristop.\n\nIn 2003, for the 7th anniversary of the MMC KIBLA and 5th anniversary of the ACE KIBLA KIBLA accomplished KIPNIK, the first time ever cyber picnic in Slovenia. On the mountain Pohorje KIBLA connected computers to the Internet with wireless (radio) link and performed a multimedia programme with real-time live transmission from the Lent Festival.\n\nProject MMC KIBLA, located in Narodni dom Maribor, Ul. Kneza Koclja 9, is non-profit making. Our visitors include students, high-school kids, the deaf and dumb (who visit the Internet for the first time through us), unemployed (who search the Internet for job possibilities), refugees, Small People Association, Handicapped People Association (certificied handicapped friendly institution), groups of teachers have workshops, whole classes of pupils use our infrastructure – from computers to lectures and art programs – as an extra learning opportunity. We’ve hosted Walfdorf teachers, Student Cultural Association, faculties, universities, The American College of Management and Technology.\n\nKibla is connected on inter-city and inter-state levels with Ljubljana, Celje, Slovenj Gradec, Koper, Murska Sobota, Tolmin, Graz, Vienna, Linz, Laafeld, Belgrade, Zagreb, Sarajevo, Skoplje, Prague, Usti nad Labem, Bratislava, Trnava, Athens, Budapest, Gyor, Brussels, Rome, London, Chicago, San Francisco, New York, Avignon, São Paulo… In cooperation with Ljubljana and Koper we carried out international project hEXPO – festival of self-organised cultural forms, which crossed whole Slovenia in a month and planted a seed for the Slovenian digital communication and information platform with the ambition to raise into the Slovene civil society network, that would join European and global network(s).\n\nIn 2004 KIBLA also received the support from the EU Structural Funds for the project of Slovene Multimedia Centres Network, named The Net, which now includes 16 Slovene multimedia centres and it’s the first of a kind in Europe and on the world and KIBLA initiated it. The M3C multimedia centres are: ZAVOD LOKALPATRIOT, Novo mesto, MLADINSKI CENTER KRŠKO, Krško, LJUDMILA at KUD FRANCE PREŠEREN and ZAVOD K 4-6 – KIBER PIPA, both Ljubljana, MLADINSKI CENTER PRLEKIJE, Ljutomer, KID PINA, Koper, KID / MMC KIBLA, Maribor, KULTURNI CENTER MOSTOVNA, Nova Gorica, ZVEZA MLADINSKIH DRUŠTEV, Tolmin, MAT KULTRA, Zagorje, MLADINSKI IN INFORMATIVNI KULTURNI KLUB MIKK, Murska Sobota, ART CENTER, Prosenjakovci, MLADINSKI CENTER SLOVENJ GRADEC, Slovenj Gradec, ZAVOD O, Škofja Loka, DRUŠTVO HIŠA KULTURE, Pivka, and MLADINSKI CENTER VELENJE.\n\nIn 1999 Kibla was covered in Leonardo (MIT Press) and Flash Art and on numerous web-zines, i.e. California On-Line and in all-important Slovenian media, from newspapers and magazines to radio and TV and web.\n\nKibla is working on the EC-Culture 2000 (txOom, TransArtDislocated, Soziale Geraeusche and Virtual Centre Media Net are finished), FP5-IST (EMMA – European Multimedia Accelerator) and FP6-IST (PATENT – Partnership for Telecommunication New Technologies) funded programmes and projects. kibla is also a part of the EUREKA multimedia umbrella and finished Leonardo da Vinci supported project Name multimedia, the Multimedia Tasks & Skills Database, researching and evaluating 26 different jobs and 96 operational multimedia tasks. NAME is presented in 9 languages, with a database of more than 650 companies from 11 countries.\n\nIn 2005 KIBLA was with other partners again successful on EU-Culture 2000 and we finished 2 new projects, e-Agora, which developed a virtual multimedia platform for innovative production and presentation of Performing Arts and will use the latest communication technologies (multi-user shared environments on the Internet) in order to interconnect European theatres, cultural centres, and enhance their co-operation, and TRG – Transient reality generators (http://fo.am/trg/), that focuses on the phenomenon of Mixed Reality (environments containing significant virtual and physical interaction possibilities, strongly intertwined) and examines the potential of synaesthetic MR experience design, in which the art-works become all-encompassing art-worlds. KIBLA was 3 years in a row (2001, 2002, 2003) the most successful Slovenian cultural institution and organisation internationally.\n\nAustralian artist Stelarc has said for KIBLA »this is the best cyber space in the world,« the great Eduardo Kac, who works at the Art Institute of Chicago, »something like this doesn't exist in United States« and Joseph Tabby, university professor and writer, »I came to visit Slovenia, because I've heard about KIBLA.« A title in Slovenian magazine Mladina: »To Maribor, because of KIBLA!« and in NeDelo (the biggest Slovenian newspaper, Sunday issue) was written: »Maribor Kibla is by its space possibilities and its conceptual structure, that combines gallery, spaces for performance, specialised bookshop and cyber café undoubtedly the most exemplary project of a kind.« In Romanian newspaper CLUJENAUI was written »KIBLA is the most important and the biggest multimedia centre in Eastern Europe.« Dr. Janez Strehovec, the leading Slovenian new media theoretician, stated, »Kibla is the main Slovenian centre for new media art (and theory).« Artist, scientist, researcher and professor on the University Caxias do Sul, Brazil, dr. Diana Domingues noted: »Kibla is a very impressive environment and you are the soul of this paradise in Slovenia.« Guru of the world advertising Dragan Sakan pointed out, »KIBLA stimulates reality, which is no longer traditional but multimedial. Also ideas are not local anymore, but global. The world needs fresh ideas. The world needs global institutions like KIBLA!« Franci Pivec, master of information sciences, claims, »KIBLA is big phenomena. For me it's one of the central institutions, which Maribor gained.«\n\nPresident and curator of ACE KIBLA Aleksandra Kostič was invited to be a judge for The Webby Awards 2000 (Internet Oscars) in San Francisco and she became a member of The International Academy for Arts and Technology including David Bowie, Laurie Anderson, Francis Ford Coppola, Matt Groening, Tina Brown etc.\n\nAdviser of ACE KIBLA Peter Tomaž Dobrila was a member of Europrix jury – the leading European Contest in e-Content creation and is a member European Academy for Digital Media (EADIM) in the year 2005 he became adviser of the Ars Electronica festival, the preeminent international intermedia event.\n\nKIBLA was invited to join the European Multimedia Forum (EMF), based in Brussels and we are actively participating on the European Multimedia Associations Convention (EMMAC) as participants of the world multimedia summit Milia 2002 and 2003 in Cannes.\n\nKibla is a member of the International Federation of Multimedia Associations / La Fédération internationale des associations de multimédia (FIAM), based in Montreal and we actively participated on the World Summit on Internet and Multimedia 2002 in Montreux.\n\nKIBLA is a partner in The World Summit Award for e-content and creativity, which is a global contest for multimedia products and applications. The Award is organized in the framework of the WSIS – The United Nations first ever World Summit on the Information Society, which takes place in December 2003 in Geneva, Swiss, and in 2005 in Tunis.\n\nKibla was invited to join the Medi@terra 2001 International Art and Technology Festival in Athens, Greece, which is included in the Cultural Olympiad 2001–2004 and is among the cultural events, which are organised in the context of the Olympic games of 2004 in Athens.]\nIn 2003 Kibla worked with Harald Szeemann, on the exhibition Blood and Honey – The Future is on the Balkans, displayed in the famous Essl Collection (Sammlung Essl) in Vienna.\n\n\n"}
{"id": "44281151", "url": "https://en.wikipedia.org/wiki?curid=44281151", "title": "Lilli Hornig", "text": "Lilli Hornig\n\nLilli Hornig (née Schwenk; March 22, 1921 – November 17, 2017) was a Czech-American scientist who worked on the Manhattan Project, as well as a feminist activist.\n\nHornig was born in Ústí nad Labem in 1921 to Erwin Schwenk, an organic chemist, and the former Rascha Shapiro, a pediatrician.\n\nIn 1929 her family moved to Berlin. Four years later she and her mother came to the United States, following her father who had moved there to escape the Nazis. As her parents were Jewish, her father was threatened with imprisonment in a concentration camp.\n\nShe obtained her BA from Bryn Mawr in 1942 and her Ph.D. from Harvard University in 1950. In 1943 she married Donald Hornig.\n\nHornig went with her husband to Los Alamos where he had obtained a job; after being originally asked to take a typing test, her scientific skills were recognized and she was given a job as a staff scientist for the Manhattan Project, in a group working with plutonium chemistry.\n\nLater it was decided that plutonium chemistry was too dangerous for women, and so she worked in high-explosive lenses instead. While at Los Alamos she signed a petition urging that the first atom bomb be used on an uninhabited island as a demonstration.\n\nHornig later became a chemistry professor at Brown University, and chairwoman of the chemistry department at Trinity College in Washington, D.C. She was appointed by President Johnson as a member of a mission to the Republic of Korea that began the founding of the Korea Institute for Science and Technology.\n\nA feminist, Hornig was the founding director of HERS (Higher Education Resource Services) under the auspices of the Committee for the Concerns of Women in New England Colleges and Universities first organized by Sheila Tobias. She served on equal opportunity committees for the National Science Foundation, the National Cancer Institute, and the American Association for the Advancement of Science. She was the research chair of the Committee for the Equality of Women at Harvard, and consulted with and participated in many studies of women's science education and careers.\n\nHornig was a Life Trustee of the Woods Hole Oceanographic Institution and was a trustee of the Wheeler School.\n\nHornig died on November 17, 2017, in Providence, Rhode Island, aged 96.\n\n\nLilli was interviewed for the documentary \"The Bomb\".\n\n"}
{"id": "30716793", "url": "https://en.wikipedia.org/wiki?curid=30716793", "title": "List of virtual communities with more than 100 million active users", "text": "List of virtual communities with more than 100 million active users\n\nThis is a list of current virtual communities with more than 100 million active users. , 46% of the world's human population (or 3.4 billion) and 4.15 billion have, at the end of December 2017, used the services of the Internet within the past year—over 100 times more people than in 1995 (see Global Internet usage).\nNote: An \"active user\" is defined as a user who has interacted with the community in the last 30 days while logged in. This metric is different from monthly unique visitors, which includes unregistered readers who are only consumers and not creators of content. See also monthly active users.\n\n"}
{"id": "2174453", "url": "https://en.wikipedia.org/wiki?curid=2174453", "title": "Lotus effect", "text": "Lotus effect\n\nThe lotus effect refers to self-cleaning properties that are a result of ultrahydrophobicity as exhibited by the leaves of \"Nelumbo\" or \"lotus flower\". Dirt particles are picked up by water droplets due to the micro- and nanoscopic architecture on the surface, which minimizes the droplet's adhesion to that surface. Ultrahydrophobicity and self-cleaning properties are also found in other plants, such as \"Tropaeolum\" (nasturtium), \"Opuntia\" (prickly pear), \"Alchemilla\", cane, and also on the wings of certain insects.\n\nThe phenomenon of ultrahydrophobicity was first studied by Dettre and Johnson in 1964 using rough hydrophobic surfaces. Their work developed a theoretical model based on experiments with glass beads coated with paraffin or PTFE telomer. The self-cleaning property of ultrahydrophobic micro-nanostructured surfaces was studied by Wilhelm Barthlott and Ehler in 1977, who described such self-cleaning and ultrahydrophobic properties for the first time as the \"lotus effect\"; perfluoroalkyl and perfluoropolyether ultrahydrophobic materials were developed by Brown in 1986 for handling chemical and biological fluids. Other biotechnical applications have emerged since the 1990s.\n\nThe high surface tension of water causes droplets to assume a nearly spherical shape, since a sphere has minimal surface area, and this shape therefore demands least solid-liquid surface energy. On contact with a surface, adhesion forces result in wetting of the surface. Either complete or incomplete wetting may occur depending on the structure of the surface and the fluid tension of the droplet.\nThe cause of self-cleaning properties is the hydrophobic water-repellent double structure of the surface. This enables the contact area and the adhesion force between surface and droplet to be significantly reduced resulting in a self-cleaning process.\nThis hierarchical double structure is formed out of a characteristic epidermis (its outermost layer called the cuticle) and the covering waxes. The epidermis of the lotus plant possesses papillae with 10 µm to 20 µm in height and 10 µm to 15 µm in width on which the so-called epicuticular waxes are imposed. These superimposed waxes are hydrophobic and form the second layer of the double structure. This system regenerates. This bio-chemical property is responsible for the functioning of the water repellency of the surface.\n\nThe hydrophobicity of a surface can be measured by its contact angle. The higher the contact angle the higher the hydrophobicity of a surface. Surfaces with a contact angle < 90° are referred to as hydrophilic and those with an angle >90° as hydrophobic. Some plants show contact angles up to 160° and are called ultrahydrophobic, meaning that only 2–3% of the surface of a droplet (of typical size) is in contact. Plants with a double structured surface like the lotus can reach a contact angle of 170°, whereby the droplet's contact area is only 0.6%. All this leads to a self-cleaning effect.\n\nDirt particles with an extremely reduced contact area are picked up by water droplets and are thus easily cleaned off the surface. If a water droplet rolls across such a contaminated surface the adhesion between the dirt particle, irrespective of its chemistry, and the droplet is higher than between the particle and the surface. As this self-cleaning effect is based on the high surface tension of water it does not work with organic solvents. Therefore, the hydrophobicity of a surface is no protection against graffiti.\n\nThis effect is of a great importance for plants as a protection against pathogens like fungi or algae growth, and also for animals like butterflies, dragonflies and other insects not able to cleanse all their body parts.\nAnother positive effect of self-cleaning is the prevention of contamination of the area of a plant surface exposed to light resulting in reduced photosynthesis.\n\nWhen it was discovered that the self-cleaning qualities of ultrahydrophobic surfaces come from physical-chemical properties at the microscopic to nanoscopic scale rather than from the specific chemical properties of the leaf surface, the discovery opened up the possibility of using this effect in manmade surfaces, by mimicking nature in a general way rather than a specific one.\n\nSome nanotechnologists have developed treatments, coatings, paints, roof tiles, fabrics and other surfaces that can stay dry and clean themselves by replicating in a technical manner the self-cleaning properties of plants, such as the lotus plant. This can usually be achieved using special fluorochemical or silicone treatments on structured surfaces or with compositions containing micro-scale particulates.\n\nIn addition to chemical surface treatments, which can be removed over time, metals have been sculpted with femtosecond pulse lasers to produce the lotus effect. The materials are uniformly black at any angle, which combined with the self-cleaning properties might produce very low maintenance solar thermal energy collectors, while the high durability of the metals could be used for self-cleaning latrines to reduce disease transmission.\n\nFurther applications have been marketed, such as self-cleaning glasses installed in the sensors of traffic control units on German autobahns developed by a cooperation partner (Ferro GmbH). \nThe Swiss companies HeiQ and Schoeller Textil have developed stain-resistant textiles under the brand names \"HeiQ Eco Dry\" and \"nanosphere\" respectively. In October 2005, tests of the Hohenstein Research Institute showed that clothes treated with NanoSphere technology allowed tomato sauce, coffee and red wine to be easily washed away even after a few washes. Another possible application is thus with self-cleaning awnings, tarpaulins and sails, which otherwise quickly become dirty and difficult to clean.\n\nSuperhydrophobic coatings applied to microwave antennas can significantly reduce rain fade and the buildup of ice and snow. \"Easy to clean\" products in ads are often mistaken in the name of the self-cleaning properties of hydrophobic or ultrahydrophobic surfaces. Patterned ultrahydrophobic surfaces also show promise for \"lab-on-a-chip\" microfluidic devices and can greatly improve surface-based bioanalysis.\n\nSuperhydrophobic or hydrophobic properties have been used in dew harvesting, or the funneling of water to a basin for use in irrigation. The Groasis Waterboxx has a lid with a microscopic pyramidal structure based on the ultrahydrophobic properties that funnel condensation and rainwater into a basin for release to a growing plant's roots.\n\nAlthough the self-cleaning phenomenon of the lotus was possibly known in Asia long before (reference to the lotus effect is found in the \"Bhagavad Gita\",) its mechanism was explained only in the early 1970s after the introduction of the scanning electron microscope. Studies were performed with leaves of \"Tropaeolum\" and lotus (\"Nelumbo\"). \"The Lotus Effect\" is a registered trademark of STO SE & CO. KGAA (US Registration No. 2613850).\n\n\n"}
{"id": "54735866", "url": "https://en.wikipedia.org/wiki?curid=54735866", "title": "MicroTCA", "text": "MicroTCA\n\nMicroTCA is an open standard embedded computing specification created by PICMG. First ratified in 2006, the specification utilizes the existing Advanced Mezzanine Card (AMCs) in a hot-swappable backplane format. It features MicroTCA Carrier Hubs (MCHs) which provide IPMI-based shelf management and switching functionality to the system. MicroTCA systems provide up to 99.9999% uptime in a smaller form factor than 3U and 6U Eurocard systems. A typical mid-sized 4HP AMC is 73.8mm x 18.96mm x 181.5mm. There are also compact-sized AMCs at 3HP wide (13.88mm) and full-sized at 6HP wide (28.95mm). Double modules have also been created, extending the height of the modules from 73.8mm to 148.8mm. The architecture was originally designed for telecom applications, but MicroTCA has gained significant popularity in other applications including military/aerospace, railway, high-energy physics, and more.\n\nMTCA.0 is the core specification, with MTCA.1 for Air Cooled Rugged MicroTCA with a rugged latching system, MTCA.3 for Hardened Conduction Cooled MicroTCA, and MTCA.4 for Rear I/O and Precision Clocking. Although MicroTCA.4 is geared towards High Energy Physics, it is also used in other applications as a way to incorporate rear I/O and multi-channel A/D and D/A conversion. MTCA.4 is used in the DESY MicroTCA Lab and as the low-level radio-frequency control system for the XFEL experiments.\n\nMicroTCA utilizes high-speed serial fabrics including Gigabit Ethernet (10GbE is ratified and 40GbE is in draft), PCIe, Serial RapidIO, etc. Clock distribution is supported by the MCH with 3 clocks defined and an additional 4 clocks and AMC.0 R2 describes four additional clocks.\n"}
{"id": "13878283", "url": "https://en.wikipedia.org/wiki?curid=13878283", "title": "Mifos Initiative", "text": "Mifos Initiative\n\nThe Mifos Initiative is a U.S.-based non-profit that exists to support and collectively lead the open source Mifos X project. Founded in October 2011, the organization encompasses a community of financial service providers, technology specialists, financial services experts and open source developers working together to grow the Mifos X open source platform for financial services. Its goal is to speed the elimination of poverty by enabling financial service providers to more effectively and efficiently deliver responsible financial services to the world's 2.5 billion poor and unbanked.\n\nDevelopment of the software began as an initiative of the Grameen Foundation in 2004. At the Grameen Foundation, James Dailey and Tapan Parikh realized that the software being used by most microfinance institutions (MFIs) was outdated, expensive or otherwise fell short. James and Tapan brainstormed something new: an open-source software that launched as Mifos in 2006. The name \"Mifos\" originally came from an acronym \"MIcro Finance Open Source\", but is now used as the brand, rather than an acronym. Mifos existed within Grameen Foundation until 2011, when it was decided that Mifos would split off to be an independent, open-source entity.\n\nOn June 1, 2011, Grameen Foundation announced that it would be ending its direct involvement with the Mifos Initiative and transitioning the project to its own organization, the Mifos Initiative, which now controls the Mifos and MIfos X projects.\n\nThe MifosX community includes developers, implementers, and users from various countries who collaborate through mailing lists, IRC, and annual conferences.\n\nEvery year, MifosX developers and users from around the world gather to help shape the vision for MifosX. The summit is open to the public and has conferences, exhibitions and other audience-oriented events. The 2012 summit was held in Bangalore and in 2013 it was held at Jaipur in the month of October. Held in Kuampala, Uganda, the 2014 Summit was a four-day event with a large focus on hands-on training for partners and users, collaborative tech sessions for contributors, & educational sessions led by external experts from the fin-tech & financial inclusion sector. It was Mifos' third annual summit, and aimed to expand and grow in the East African community.\n\nMifosX has also participated in several other independent events. In 2013, Mifos has participated in Google Summer of Code. as well as Random Hacks of Kindness during RHoK Global December 2012. Moreover, Mifos has been part of . Mifos has also been a part of the FinDEVr San Francisco 2014 event, acting as a sponsor. One of Mifos' members, James Dailey (board member and Chief Innovation Officer), took the stage to discuss the reinvention of banking. On the first of November, Mifos Initiative also took part in the Global Islamic Microfinance Forum, held in Dubai, UAE.\n\n\n\nMifos has a team of business and technical advisors which are called upon for visionary thinking, critical insight, and industry expertise. These include Steve Thomson (Financial Services Advisor), Bryan Barnett (Business Advisor), and Brian Behlendorf (Technical Advisor).\n\nIndia\n\n\n"}
{"id": "4354196", "url": "https://en.wikipedia.org/wiki?curid=4354196", "title": "Mobile banking", "text": "Mobile banking\n\nMobile banking is a service provided by a bank or other financial institution that allows its customers to conduct financial transactions remotely using a mobile device such as a smartphone or tablet. Unlike the related internet banking it uses software, usually called an app, provided by the financial institution for the purpose. Mobile banking is usually available on a 24-hour basis. Some financial institutions have restrictions on which accounts may be accessed through mobile banking, as well as a limit on the amount that can be transacted. Mobile banking is dependent on the availability of an internet or data connection to the mobile device.\nTransactions through mobile banking depend on the features of the mobile banking app provided and typically includes obtaining account balances and lists of latest transactions, electronic bill payments, remote check deposits, P2P payments, and funds transfers between a customer's or another's accounts. Some apps also enable copies of statements to be downloaded and sometimes printed at the customer's premises.\nFrom the bank's point of view, mobile banking reduces the cost of handling transactions by reducing the need for customers to visit a bank branch for non-cash withdrawal and deposit transactions. Mobile banking does not handle transactions involving cash, and a customer needs to visit an ATM or bank branch for cash withdrawals or deposits. Many apps now have a remote deposit option; using the device's camera to digitally transmit cheques to their financial institution.\nMobile banking differs from mobile payments, which involves the use of a mobile device to pay for goods or services either at the point of sale or remotely, analogously to the use of a debit or credit card to effect an EFTPOS payment.\n\nThe earliest mobile banking services used SMS, a service known as SMS banking. With the introduction of smart phones with WAP support enabling the use of the mobile web in 1999, the first European banks started to offer mobile banking on this platform to their customers.\n\nMobile banking before 2010 was most often performed via SMS or the mobile web. Apple's initial success with iPhone and the rapid growth of phones based on Google's Android (operating system) have led to increasing use of special mobile apps, downloaded to the mobile device. With that said, advancements in web technologies such as HTML5, CSS3 and JavaScript have seen more banks launching mobile web based services to complement native applications. These applications are consisted of a web application module in JSP such as J2EE and functions of another module J2ME. \n\nA recent study (May 2012) by Mapa Research suggests that over a third of banks have mobile device detection upon visiting the banks' main website. A number of things can happen on mobile detection such as redirecting to an app store, redirection to a mobile banking specific website or providing a menu of mobile banking options for the user to choose from.\n\nIn one academic model, mobile banking is defined as:\n\nMobile Banking refers to provision and availment of banking- and financial services with the help of mobile telecommunication devices.The scope of offered services may include facilities to conduct bank and stock market transactions, to administer accounts and to access customised information.\"\nAccording to this model mobile banking can be said to consist of three inter-related concepts:\n\n\nMost services in the categories designated \"accounting\" and \"brokerage\" are transaction-based. The non-transaction-based services of an informational nature are however essential for conducting transactions - for instance, balance inquiries might be needed before committing a money remittance. The accounting and brokerage services are therefore offered invariably in combination with information services. Information services, on the other hand, may be offered as an independent module.\n\nMobile banking may also be used to help in business situations as well as financial\n\nTypical mobile banking services may include:\n\n\n\n\n\n\nA report by the US Federal Reserve (March 2012) found that 21 percent of mobile phone owners had used mobile banking in the past 12 months. Based on a survey conducted by Forrester, mobile banking will be attractive mainly to the younger, more \"tech-savvy\" customer segment. A third of mobile phone users say that they may consider performing some kind of financial transaction through their mobile phone. But most of the users are interested in performing basic transactions such as querying for account balance and making bill\n\nBased on the 'International Review of Business Research Papers' from World business Institute, Australia, following are the key functional trends possible in world of Mobile Banking.\nWith the advent of technology and increasing use of smartphone and tablet based devices, the use of Mobile Banking functionality would enable customer connect across entire customer life cycle much comprehensively than before.\n\nIllustration of objective based functionality enrichment In Mobile Banking:\n\nKey challenges in developing a sophisticated mobile banking application are :\n\nThere are a large number of different mobile phone devices and it is a big challenge for banks to offer a mobile banking solution on any type of device. Some of these devices support Java ME and others support SIM Application Toolkit, a WAP browser, or only SMS.\n\nInitial interoperability issues however have been localized, with countries like India using portals like \"R-World\" to enable the limitations of low end java based phones, while focus on areas such as South Africa have defaulted to the USSD as a basis of communication achievable with any phone.\n\nThe desire for interoperability is largely dependent on the banks themselves, where installed applications(Java based or native) provide better security, are easier to use and allow development of more complex capabilities similar to those of internet banking while SMS can provide the basics but becomes difficult to operate with more complex transactions.\n\nThere is a myth that there is a challenge of interoperability between mobile banking applications due to perceived lack of common technology standards for mobile banking. In practice it is too early in the service lifecycle for interoperability to be addressed within an individual country, as very few countries have more than one mobile banking service provider. In practice, banking interfaces are well defined and money movements between banks follow the IS0-8583 standard. As mobile banking matures, money movements between service providers will naturally adopt the same standards as in the banking world.\n\nIn January 2009, Mobile Marketing Association (MMA) Banking Sub-Committee, chaired by CellTrust and VeriSign Inc., published the Mobile Banking Overview for financial institutions in which it discussed the advantages and disadvantages of Mobile Channel Platforms such as Short Message Services (SMS), Mobile Web, Mobile Client Applications, SMS with Mobile Web and Secure SMS.\n\nAs with most internet-connected devices, as well as mobile-telephony devices, cybercrime rates are escalating year-on-year. The types of cybercrimes which may affect mobile-banking might range from unauthorized use while the owner is using the mobile banking, to remote-hacking, or even jamming or interference via the internet or telephone network data streams. This is demonstrated by the malware called \"SMSZombie.A\", which infected Chinese Android devices. It was embedded in wallpaper apps and installed itself so it can exploit the weaknesses of China Mobile SMS Payment system, stealing banks credit card numbers and information linked to financial transactions. One of the most advanced malwares discovered recently was the Trojan called \"Bankbot\". It went past Google's protections in its Android app marketplace and targeted Wells Fargo, Chase, and Citibank customers on Android devices worldwide before its removal by Google in September 2017. This malicious app was activated when users opened a banking app, overlaying it so it can steal banking credentials. \nIn the banking world, currency rates may change by the millisecond.\nSecurity of financial transactions, being executed from some remote location and transmission of financial information over the air, are the most complicated challenges that need to be addressed jointly by mobile application developers, wireless network service providers and the banks' IT departments.\n\nThe following aspects need to be addressed to offer a secure infrastructure for financial transaction over wireless network :\n\n\nOne-time password (OTPs) are the latest tool used by financial and banking service providers in the fight against cyber fraud. Instead of relying on traditional memorized passwords, OTPs are requested by consumers each time they want to perform transactions using the online or mobile banking interface. When the request is received the password is sent to the consumer’s phone via SMS. The password is expired once it has been used or once its scheduled life-cycle has expired.\n\nBecause of the concerns made explicit above, it is extremely important that SMS gateway providers can provide a decent quality of service for banks and financial institutions in regards to SMS services. Therefore, the provision of service level agreements (SLAs) is a requirement for this industry; it is necessary to give the bank customer delivery guarantees of all messages, as well as measurements on the speed of delivery, throughput, etc. SLAs give the service parameters in which a messaging solution is guaranteed to perform.\n\nAnother challenge for the CIOs and CTOs of the banks is to scale-up the mobile banking infrastructure to handle exponential growth of the customer base. With mobile banking, the customer may be sitting in any part of the world (true anytime, anywhere banking) and hence banks need to ensure that the systems are up and running in a true 24 x 7 fashion. As customers will find mobile banking more and more useful, their expectations from the solution will increase. Banks unable to meet the performance and reliability expectations may lose customer confidence. There are systems such as Mobile Transaction Platform which allow quick and secure mobile enabling of various banking services. Recently in India there has been a phenomenal growth in the use of Mobile Banking applications, with leading banks adopting Mobile Transaction Platform and the Central Bank publishing guidelines for mobile banking operations.\n\nDue to the nature of the connectivity between bank and its customers, it would be impractical to expect customers to regularly visit banks or connect to a web site for regular upgrade of their mobile banking application. It will be expected that the mobile application itself check the upgrades and updates and download necessary patches (so called \"Over The Air\" updates). However, there could be many issues to implement this approach such as upgrade / synchronization of other dependent components.\n\nUser adoption\n\nIt should be noted that studies have shown that a huge concerning factor of having mobile banking more widely used, is a banking customer's unwillingness to adapt. Many consumers, whether they are misinformed or not, do not want to begin using mobile banking for several reasons. These can include the learning curve associated with new technology, having fears about possible security compromises, just simply not wanting to start using technology, etc.\n\nIt would be expected from the mobile application to support personalization such as :\n\nThis is a list of countries by mobile banking usage as measured by the percentage of people who had non-SMS mobile banking transactions in the previous three months. The data is sourced from Bain, Research Now and Bain along with GMI NPS surveys in 2012.\n\nAfrican nations such as Kenya would rank highly if SMS mobile banking were included in the above list. Kenya has 38% of the population as subscribers to M-Pesa as of 2011.\nThough as of 2016 mobile banking applications have seen a tremendous growth in kenyan banking sector who have capitalised on android play store and apple store to put their applications. Kenyan banks like Equity Bank Kenya Limited Eazzy banking application and The Co-operative Bank Mco-op cash application have proved to be a success mobile banking applications.\n\nMobile banking is used in many parts of the world with little or no infrastructure, especially remote and rural areas. This aspect of mobile commerce is also popular in countries where most of their population is unbanked. In most of these places, banks can only be found in big cities, and customers have to travel hundreds of miles to the nearest bank.\n\nIn Iran, banks such as Parsian, Tejarat, Pasargad Bank, Mellat, Saderat, Sepah, Edbi, and Bankmelli offer the service. Banco Industrial provides the service in Guatemala. Citizens of Mexico can access mobile banking with Omnilife, Bancomer and MPower Venture.\nKenya's Safaricom (part of the Vodafone Group) has the M-Pesa Service, which is mainly used to transfer limited amounts of money, but increasingly used to pay utility bills as well. In 2009, Zain launched their own mobile money transfer business, known as ZAP, in Kenya and other African countries. Several other players in Kenya such as Tangerine, MobiKash and Funtrench Limited also have network-independent mobile money transfer. In Somalia, the many telecom companies provide mobile banking, the most prominent being Hormuud Telecom and its ZAAD service.\n\nTelenor Pakistan has also launched a mobile banking solution, in coordination with Taameer Bank, under the label Easy Paisa, which was begun in Q4 2009. Eko India Financial Services, the business correspondent of State Bank of India (SBI) and ICICI Bank, provides bank accounts, deposit, withdrawal and remittance services, micro-insurance, and micro-finance facilities to its customers (nearly 80% of whom are migrants or the unbanked section of the population) through mobile banking.\n\nIn a year of 2010, mobile banking users soared over 100 percent in Kenya, China, Brazil and United States with 200 percent, 150 percent, 110 percent and 100 percent respectively.\n\nDutch Bangla Bank launched the very first mobile banking service in Bangladesh on 31 March 2011. This service is launched with 'Agent' and 'Network' support from mobile operators, Banglalink and Citycell. Sybase 365, a subsidiary of Sybase, Inc. has provided software solution with their local partner Neurosoft Technologies Ltd. There are around 160 million people in Bangladesh, of which, only 13 per cent have bank accounts. With this solution, Dutch-Bangla Bank can now reach out to the rural and unbanked population, of which, 45 per cent are mobile phone users. Under the service, any mobile handset with subscription to any of the six existing mobile operators of Bangladesh would be able to utilize the service. Under the mobile banking services, bank-nominated Banking agent performs banking activities on its behalf, like opening mobile banking accounts, providing cash services (receipts and payments) and dealing with small credits. Cash withdrawal from a mobile account can also be done from an ATM validating each transaction by 'mobile phone & PIN' instead of 'card & PIN'. Other services that are being delivered through mobile banking system are person-to-person (e.g. fund transfer), person-to-business (e.g. merchant payment, utility bill payment), business-to-person (e.g. salary/commission disbursement), government-to-person (disbursement of government allowance) transactions.\n\nIn May 2012, Laxmi Bank Limited launched the very first mobile banking in Nepal with its product Mobile Khata. Mobile Khata currently runs on a third-party platform called Hello Paisa that is interoperable with all the telecoms in Nepal viz. Nepal Telecom, NCell, Smart Tel and UTL, and is also interoperable with various banks in the country. The initial joining members to the platform after Laxmi Bank Limited were Siddartha Bank, Bank of Kathmandu, Commerz and Trust Bank Nepal and International Leasing and Finance Company. In country with roughly 30 million population, over 5 million have subscribed to mobile banking in Nepal as per the recent data from Nepal Rastra Bank, the central bank of Nepal.\n\nBarclays offers a service called Barclays Pingit, and Hello Money offering services in Africa, allowing transfer of money from the United Kingdom to many parts of the world with a mobile phone. Pingit is owned by a consortium of banks. In April 2014, the UK Payments Council launched the Paym mobile payment system, allowing mobile payments between customers of several banks and building societies using the recipient's mobile phone number.\n\nIn Nov 2017 the State Bank of India launched an integrated banking platform in India called YONO offering conventional banking functions but also payment services for things such as online shopping, travel planning, taxi booking or online education. \n\nFollowing is a list showing the share of people using mobile banking apps during the last three months in selected countries worldwide in 2014. The list is based on a survey conducted by statista.com including 82,914 respondents.\n\n\n"}
{"id": "1364003", "url": "https://en.wikipedia.org/wiki?curid=1364003", "title": "NCR 53C9x", "text": "NCR 53C9x\n\nThe NCR 53C9x is a family of application-specific integrated circuits (ASIC) produced by the former NCR Corporation and others for implementing the SCSI (small computer standard interface) bus protocol in hardware and relieving the host system of the work required to sequence the SCSI bus. The 53C9x was a low-cost solution and was therefore widely adopted by OEMs in various motherboard and peripheral device designs. The original 53C90 lacked direct memory access (DMA) capability, an omission that was addressed in the 53C90A and subsequent versions.\n\nThe 53C90(A) supported the SCSI-1 protocol, implemented the eight bit parallel SCSI bus, and eight bit host data bus transfers. The 53C94 added SCSI-2 features to those of the 53C90A, and the 53C96 added support for the high voltage differential (HVD) parallel bus. The 53CF94 and 53CF96 also implemented larger transfer sizes and 20 MB/second synchronous SCSI bus speed (the so-called ultra-SCSI protocol). All members of the 53C94/96 type support both eight and 16 bit host bus transfers via programmed input/output (PIO) and DMA.\n\nQLogic FAS216 and Emulex ESP100 chips are a drop-in replacement for the NCR 53C94. The 53C90A and 53C(F)94/96 were also produced under license by Advanced Micro Devices (AMD).\n\nA list of systems which included the 53C9x controller includes:\n\n53C94\n\n\n53C96\n\n"}
{"id": "28807559", "url": "https://en.wikipedia.org/wiki?curid=28807559", "title": "NEC Mobile Communications", "text": "NEC Mobile Communications\n\nThe company was formed as a merger between the mobile handset division of NEC and the former joint venture between Casio and Hitachi, Casio Hitachi Mobile Communications, established 1 April 2004.\n\nIn the United States and Japan, NEC Casio Mobile also manufactures phones under the \"Casio G'zOne\" brand. This device line with a small cosmetic differences and RF firmware adoption were also sold in the Republic of Korea. Then widely spread around the world (including non-official sales from the US and Korea) due to the rare IP68 protective design and close to outstanding hardware specs of some models like Commando C811 / CA-201L at the moment of the release.\n\nAs of October 1, 2014, NEC Casio Mobile Communications, announced that its official name is now NEC Mobile Communications.\nSince then, official site is now: https://web.archive.org/web/20160223105020/http://www.nec-mobilecom.com/\nOld NEC Casio site https://web.archive.org/web/20140926071701/http://www.nec-casio-mobile.com/ (and all its software/updates domains) were closed in the February 2015.\nIt seems that company simply dropped support for all the popular and not so old NEC Casio models released just in a 2013-2014.\nPreviously, in 2013 NEC planned to close smartphone production in 2014, leaving in production the conventional mobile phones and the tablets.\n\nAs of January 1, 2016, NEC Mobile Communications seems to be completely gone from the mobile phone market. Official web site is completely closed. Any support abandoned. Users left alone with their devices.\nYou can explore latest official website's Archive snapshot at the link below.\n\n"}
{"id": "603278", "url": "https://en.wikipedia.org/wiki?curid=603278", "title": "Nanosensor", "text": "Nanosensor\n\nNanosensors are sensors whose active elements include nanomaterials. There are several ways being proposed today to make nanosensors; these include top-down lithography, bottom-up assembly, and molecular self-assembly.\n\nNanomaterials-based sensors have several benefits in sensitivity and specificity over sensors made from traditional materials. Nanosensors can have increased specificity because they operate at a similar scale as natural biological processes, allowing functionalization with chemical and biological molecules, with recognition events that cause detectable physical changes. Enhancements in sensitivity stem from the high surface-to-volume ratio of nanomaterials, as well as novel physical properties of nanomaterials that can be used as the basis for detection, including nanophotonics. Nanosensors can also potentially be integrated with nanoelectronics to add native processing capability to the nanosensor.\n\nOne-dimensional nanomaterials such as nanowires and nanotubes are well suited for use in nanosensors, as compared to bulk or thin-film planar devices. They can function both as transducers and wires to transmit the signal. Their high surface area can cause large signal changes upon binding of an analyte. Their small size can enable extensive multiplexing of individually addressable sensor units in a small device. Their operation is also \"label free\" in the sense of not requiring fluorescent or radioactive labels on the analytes.\n\nThere are several challenges for nanosensors, including avoiding fouling and drift, developing reproducible calibration methods, applying preconcentration and separation methods to attain a proper analyte concentration that avoids saturation, and integrating the nanosensor with other elements of a sensor package in a reliable manufacturable manner.\n\nPotential applications for nanosensors include medicine; detection of contaminants and pathogens in the workplace, the environment, for first responders, and in products such as food; and monitoring manufacturing processes and equipment and transportation systems. Medicinal uses of nanosensors mainly revolve around the potential of nanosensors to accurately identify particular cells or places in the body in need. By measuring changes in volume, concentration, displacement and velocity, gravitational, electrical, and magnetic forces, pressure, or temperature of cells in a body, nanosensors may be able to distinguish between and recognize certain cells, most notably those of cancer, at the molecular level in order to deliver medicine or monitor development to specific places in the body.\n\nThere are many mechanisms by which a recognition event can be transduced into a measurable signal. Electrochemical nanosensors are based on detecting a resistance change in the nanomaterial upon binding of an analyte, due to changes in scattering or to the depletion or accumulation of charge carriers. One possibility is to use nanowires such as carbon nanotubes, conductive polymers, or metal oxide nanowires as gates in field-effect transistors, although as of 2009 they had not yet been demonstrated in real-world conditions. Other examples include electromagnetic or plasmonic nanosensors, spectroscopic nanosensors such as surface-enhanced Raman spectroscopy, magnetoelectronic or spintronic nanosensors, and mechanical nanosensors.\n\nOne of the first working examples of a synthetic nanosensor was built by researchers at the Georgia Institute of Technology in 1999. It involved attaching a single particle onto the end of a carbon nanotube and measuring the vibrational frequency of the nanotube both with and without the particle. The discrepancy between the two frequencies allowed the researchers to measure the mass of the attached particle.\n\nChemical sensors, too, have been built using nanotubes to detect various properties of gaseous molecules. Carbon nanotubes have been used to sense ionization of gaseous molecules while nanotubes made out of titanium have been employed to detect atmospheric concentrations of hydrogen at the molecular level. Many of these involve a system by which nanosensors are built to have a specific pocket for another molecule. .When that particular molecule, and only that specific molecule, fits into the nanosensor, and light is shone upon the nanosensor, it will reflect different wavelengths of light and, thus, be a different color. In a similar fashion, Flood et al. have shown that supramolecular host-guest chemistry offers quantitative sensing using Raman scattered light as well as SERS.\n\nPhotonic devices can also be used as nanosensors to quantify concentrations of clinically relevant samples. A principle of operation of these sensors is based on the chemical modulation of a hydrogel film volume that incorporates a Bragg grating. As the hydrogel swells or shrinks upon chemical stimulation, the Bragg grating changes color and diffracts light at different wavelengths. The diffracted light can be correlated with the concentration of a target analyte.\n\nOne example of nanosensors involves using the fluorescence properties of cadmium selenide quantum dots as sensors to uncover tumors within the body. A downside to the cadmium selenide dots, however, is that they are highly toxic to the body. As a result, researchers are working on developing alternate dots made out of a different, less toxic material while still retaining some of the fluorescence properties. In particular, they have been investigating the particular benefits of zinc sulfide quantum dots which, though they are not quite as fluorescent as cadmium selenide, can be augmented with other metals including manganese and various lanthanide elements. In addition, these newer quantum dots become more fluorescent when they bond to their target cells.\n\nThere are currently several hypothesized ways to produce nanosensors. Top-down lithography is the manner in which most integrated circuits are now made. It involves starting out with a larger block of some material and carving out the desired form. These carved out devices, notably put to use in specific microelectromechanical systems used as microsensors, generally only reach the micro size, but the most recent of these have begun to incorporate nanosized components.\n\nAnother way to produce nanosensors is through the bottom-up method, which involves assembling the sensors out of even more minuscule components, most likely individual atoms or molecules. This would involve moving atoms of a particular substance one by one into particular positions which, though it has been achieved in laboratory tests using tools such as atomic force microscopes, is still a significant difficulty, especially to do en masse, both for logistic reasons as well as economic ones. Most likely, this process would be used mainly for building starter molecules for self-assembling sensors.\n\nThe third way, which promises far faster results, involves self-assembly, or \"growing\" particular nanostructures to be used as sensors. This most often entails an already complete set of components that would automatically assemble themselves into a finished product. Accurately being able to reproduce this effect for a desired sensor in a laboratory would imply that scientists could manufacture nanosensors much more quickly and potentially far more cheaply by letting numerous molecules assemble themselves with little or no outside influence, rather than having to manually assemble each sensor.\n\n\n"}
{"id": "11648439", "url": "https://en.wikipedia.org/wiki?curid=11648439", "title": "Paolo Magrassi", "text": "Paolo Magrassi\n\nPaolo Magrassi is an Italian technologist known as one of the authors of the Supranet concept, the co-creator of the AlphaIC methodology for assessing the value of information technology expenditures, and the manager of the Pontifex project, which in the mid-1980s introduced a novel approach to complex fleet scheduling. \n\nIn the early 2000s, Magrassi also was instrumental in introducing to the industrial and business world then-emerging miniature RFID and internet of things technologies such as those proposed by the MIT's Auto-ID Center.\n\nHe also published several books, including \"The Caricature Of A Revolution\" (2012), \"Digitalmente confusi\" (2011), \"La good-enough society\" (2010), \"Difendersi dalla complessità\" (2009), \"Il filo conduttore \" (2006), \"2015 Weekend nel futuro\" (2005), \"A World Of Smart Objects\" (2002).\n\n"}
{"id": "6275959", "url": "https://en.wikipedia.org/wiki?curid=6275959", "title": "Presidential Communications Group (Philippines)", "text": "Presidential Communications Group (Philippines)\n\nThe Presidential Communications Group or simply the Communications Group, is the collective name for the newly formed offices within the Office of the President of the Philippines and refers to the following positions and offices: the Presidential Spokesperson and the Presidential Communications Operations Office (PCOO)—formerly the Office of the Press Secretary.\n\nThe Communications Group is primarily responsible for planning, programming, and coordinating the activities that will most effectively collect information about what is happening in the Executive branch of the government, and getting this information to the media. The Group's other role is to help Filipinos understand government policies and programs, and to shore up support for them.\n\nThe Presidential Spokesperson speaks on behalf of the President about matters of public interest. Considering the restricted level of access that the media has to the Chief Executive, the Spokesperson is expected to be the primary source of presidential directives in the absence of the President of the Philippines.\n\nThe Presidential Spokesperson is not subject to confirmation by the Commission on Appointments, and does not have any specific item or template in government.\n\nThe current Presidential Spokesperson is Salvador Panelo (who succeeded Harry Roque) while the Assistant to the Presidential Spokesperson is China Jocson of the Duterte Administration.\n\nThe Presidential Communications Operations Office (PCOO) is in charge of disseminating the government’s message to private media entities. It exercises supervision and control over state-owned media entities to disseminate the official messages properly and effectively in accordance with the communications plan. It is responsible for the accreditation and authentication of the credentials of foreign media correspondents in line with its primary task to cultivate relations and assist private media entities. During the administration of Benigno Aquino III, the Presidential Communications Office (PCO) was named the Presidential Communications Operations Office (PCOO).\n\nThe PCOO head holds a cabinet rank. He/she is assisted by an undersecretary for administration and finance, an undersecretary for operations, a chief of staff, an assistant secretary for legislative affairs, and an electronic data processing division chief.\n\nThe PCOO is composed of the following units and agencies:\n\nThe Presidential News Desk (PND) functions as the newsroom of Malacañang Palace. It gathers and disseminates information, such as press and photograph releases, and official statements from the Palace, on a daily basis.\n\nThe PND operates from Sundays to Saturdays, 5:00 AM to 7:00 PM. It is headed by a chief editor and is composed of a managing editor, deskmen, and reporters. Other units, which support the PND are Electronic Data Processing, Transcription and Monitoring, and Photographs.\n\nFrancisco Tatad conceived the PND. He was the Press Secretary to President Ferdinand Marcos. It was then called the Central Desk and was located in the Mabini Building. During the administration of President Corazon C. Aquino, Press Secretary Benigno Teodoro relegated the Central Desk, which became the News and Reportorial Section, as a section of the Presidential Press Staff. Under the Ramos administration, Press Secretary Rodolfo Reyes strengthened presidential coverage and set up the PND.\n\nThe Philippine Information Agency (PIA), established by Executive Order No. 100, is the main development communication arm of the government. It is under the Presidential Communications Operations Office (PCOO). \n\nThe PIA directly serves the Presidency and the executive branch of the national, regional and provincial levels through its 16 regional offices and 71 provincial information centers. \n\nIts core services include: information gathering/research, production and dissemination, and institutional development and capacity-building focusing on alliance-building, networking, communication-related training, consultancy and technical assistance. The PIA is also the advertising arm of the government.\n\nIt is currently headed by Harold E. Clavite (director general) and Gregorio Angelo C. Villar (deputy director general), assisted by staff directors in the central office, regional directors in the regional offices, and more than 500 permanent and contractual employees.\n\nIts tagline is \"Empowering Communities\". The PIA works with community stakeholders, including local government units, line agencies, private entities, schools, colleges and universities, civil society organizations, and other groups in ensuring proper dissemination of information at the grassroots level. The agency advocates responsible sharing of information and responsible use of social media. Its current management and personnel work with the mantra and belief that \"a well-informed Filipino is an empowered Filipino\".\n\nIn 2017, the PIA was tasked by PCOO to take the lead in promoting the 31st Association of Southeast Asian Nations Summit and established information kiosks in more than 90 locations in the country and conducted community fora and dialogue with community leaders and local townsfolk.\n\nThe Bureau of Communication Services is the attached agency of the PCOO that is tasked to provide materials and services related to various functions of the Presidency, PCOO and the general public.\n\nIt also produces information materials for state events such as the anniversary of the People Power Revolution, \"Araw ng Kalayaan\" celebrations, and others. Director IV John S. Manalili is the bureau's current director.\n\nCreated by President Corazon C. Aquino, the Presidential Broadcast Staff - Radio Television Malacañang is tasked to provide the necessary media services, video and audio, to the incumbent President, to document all official engagements, and to make available to the public accurate and relevant information on the activities, programs and pronouncements of the national leadership.\n\nThe agency is involved in television coverage and documentation, news and public affairs syndication of all the activities of the President, either live or delayed telecast through government or private collaborating networks.\n\nThere are two government-run television networks, which are the People's Television Network (PTV-4) and the Intercontinental Broadcasting Corporation (IBC-13). The Philippine Broadcasting Service (PBS) constitutes the government radio network.\n\nIBC-13 and RPN-9 were sequestered by the government after the fall of the Marcos dictatorship. However, plans are being made to privatize IBC-13 and RPN-9 within two years to raise money. According to the 2009 report of the Presidential Commission on Good Government (PCGG), IBC-13 is valued at PHP 3.074 billion while RPN-9 is valued at PHP 1.3 billion.\n\nThe National Printing Office (NPO) was established by Executive Order. Executive Order 285 abolished the General Services Administration and transferred its functions to other agencies. Its Government Printing Offices were merged with the printing units of the PIA. The NPO is mandated by Executive Order 285 to print forms, official ballots, public documents, and public information materials.\n\nThe News and Information Bureau's main aim is to provide services relating to the development and formulation of a domestic and foreign information program for the Government in general, and the Presidency in particular, including the development of strategies for the dissemination of information on specific government programs. It is composed of the following divisions:\n\nThe OP Web Development Office, commonly referred to as the President's New Media Team, is responsible for the establishment and management of the President's Official Website and Official Presence on social networking channels such as Facebook, Twitter, and YouTube. It is also partly responsible for the feedback mechanism of the President wherein it receives the comments, concerns and suggestions through the Contact Us section of the President's website. It is also responsible for the President's Social Media engagement and in maximizing the use of new technologies to further the President's agenda, policies and programs.\n\nThe Presidential Communications Development and Strategic Planning Office (PCDSPO) ensures that all aspects of communications are covered, to ensure that the administration’s message has been delivered successfully. This includes market research and polling. It devises the communications strategy to promote the President’s agenda throughout all media, and among the many people with whom the administration interacts. This can include, but is not limited to, the State of the Nation address, televised press conferences, statements to the press, and radio addresses. This office also works closely with cabinet-level departments and other executive agencies, in order to create a coherent strategy through which the president’s message can be disseminated.\n\nDuring the administration of former president Benigno Aquino III, the PCDSPO head held a cabinet rank. The head was supported by a deputy of undersecretary rank, a chief of staff, an assistant secretary for messaging, and an electronic data processing division chief. After Rodrigo Duterte became the President in 2016, the PCDSPO was merged with the PCOO, thus effectively abolishing the PCDSPO. The PCOO was renamed as the Presidential Communications Office before it was reverted to Presidential Communications Operations Office.\n\nPrior to June 30, 2016, the PCDSPO was composed of the following units and agencies, all of which were under the PCOO:\n\n\n\n\n\n\nThe PCOO has been criticized for various errors and lapses committed by the office, including what one lawmaker called a \"revisionist attempt to whitewash the dark years of martial law\". The Secretary of the PCOO assured Congress that it will not repeat the office's mistakes. \n\nIn March 2018, the News and Information Bureau released a transcript to reporters of an interview erroneously attributed to the President of the Philippines which was in fact an interview with an impersonator.\n\n"}
{"id": "51772853", "url": "https://en.wikipedia.org/wiki?curid=51772853", "title": "SAP BusinessObjects Lumira", "text": "SAP BusinessObjects Lumira\n\nSAP BusinessObjects Lumira also known as Lumira is a business intelligence software developed and marketed by SAP BusinessObjects. The software is used to manipulate and visualize data.\n\nLumira was initially launched as SAP Visual Intelligence in 2012. The first edition of the software could only use SAP's HANA platform as a data source. The second release expanded data sources to include CSV and Excel files. In 2013, SAP rebranded the software under the Lumira name and began offering a version of the software as a cloud computing program. In 2015, the cloud version of Lumira was absorbed into SAP's Cloud For Analytics software, while the Lumira Server and Lumira Desktop software remained separate. \n\nIn 2016, Lumira 2.x was announced. The upcoming software is said to be able to run applications from the Design Studio 1.6 data visualization software.\n\n"}
{"id": "30738857", "url": "https://en.wikipedia.org/wiki?curid=30738857", "title": "Spektr-RG", "text": "Spektr-RG\n\nSpektr-RG (Russian for \"Spectrum\" + \"Röntgen\" + \"Gamma\"; also called Spectrum-X-Gamma, SRG, SXG) is a Russian/German high-energy astrophysics observatory. it is planned to launch in 2019.\nThe primary instrument of the mission is eROSITA, built by the Max Planck Institute for Extraterrestrial Physics (MPE). It is designed to be used to conduct a 4-year X-ray survey sky, the first in a medium X-ray band up to 10 keV energies.\nThis survey will detect many new clusters of galaxies and active galactic nuclei.\nThe second instrument, ART-XC, is a Russian high energy X-ray telescope.\nThis instrument and spacecraft are being built under the leadership of the Russian Space Research Institute (IKI). \n\nThe Spektr-RG programme was revived in 2005 and the spacecraft was in final stages of assembly during 2016. Instrument launch is scheduled for early-2019..\n\nDevelopment of an early version of Spektr-RG was started in mid-1990s and was cancelled in 2002. Initial launch date was set to 1995, but later postponed as far as 2008, until it was finally cancelled in 2002. However, some of the instruments have been completed, e.g., an X-ray telescope by Leicester University (JET-X) and an ultraviolet telescope by Tel-Aviv University (TAUVEX).\n\nThe satellite would have been launched into a 51.5 degree orbit with an apogee of and a period of four days, by a Proton-K rocket with a Blok DM-2 upper stage.\n\n"}
{"id": "4249871", "url": "https://en.wikipedia.org/wiki?curid=4249871", "title": "String galvanometer", "text": "String galvanometer\n\nThe string galvanometer, also known as the Einthoven galvanometer, invented around 1901 by Dutch physician Willem Einthoven was the first practical electrocardiograph (ECG); it was one of the earliest instruments capable of detecting and recording the very small electric currents produced by the human heart and produced the first reliable electrocardiograms. The original machines achieved \"such amazing technical perfection that many modern day electrocardiographs do not attain equally reliable and undistorted recordings\". Einthoven was awarded the 1924 Nobel prize in Physiology or Medicine for his work.\n\nPrevious to the string galvanometer, scientists were using a machine called the capillary electrometer to measure the heart’s electrical activity, but this device was unable to produce results of a diagnostic level. Willem Einthoven invented the string galvanometer at Leiden University in the early 20th century, publishing the first registration of its use to record an electrocardiogram in a Festschrift book in 1902. The first human electrocardiogram was recorded in 1887, however it was not until 1901 that a quantifiable result was obtained from the string galvanometer. In 1908, the physicians Arthur MacNalty, M.D. Oxon, and Thomas Lewis teamed to become the first of their profession to apply electrocardiography in medical diagnosis. Einthoven was awarded the Nobel prize in Physiology or Medicine in 1924 for his work.\n\nEinthoven's invention consisted of a very long (several metres) silver-coated quartz filament of negligible mass that conducted the electrical currents from the heart. This filament was acted upon by powerful electromagnets positioned either side of it, which caused sideways displacement of the filament in proportion to the current carried due to the electromagnetic field. The movement in the filament was heavily magnified and projected through a thin slot onto a moving photographic plate.\n\nThe filament was originally made by drawing out a filament of glass from a crucible of molten glass. To produce a sufficiently thin and long filament an arrow was fired across the room so that it dragged the filament from the molten glass. The filament so produced was then coated with silver to provide the conductive pathway for the current. By tightening or loosening the filament it is possible to very accurately regulate the sensitivity of the galvanometer.\n\nThe original machine required water cooling for the powerful electromagnets, required 5 operators and weighed some 600 lb.\n\nPatients are seated with both arms and left leg in separate buckets of saline solution. These buckets act as electrodes to conduct the current from the skin's surface to the filament. The three points of electrode contact on these limbs produces what is known as Einthoven's triangle, a principle still used in modern-day ECG recording.\n"}
{"id": "1424847", "url": "https://en.wikipedia.org/wiki?curid=1424847", "title": "Sylphon", "text": "Sylphon\n\nA sylphon is an old name for a cylindrically symmetrical metal bellows. When made of metal, the sylphon shape was formerly created by metal spinning onto a metal mandrel (model), and now by hydrostatic forming within a mold. Because the mold contains the convolutions of the bellows, the mold must be constructed in parts so that it can be disassembled when the forming process is complete. Legendary experimental physicist John Strong makes occasional use of the term sylphon in his famous book \"Procedures in Experimental Physics\". \n\nA sylphon, or bellows, is used, among other purposes, to transfer motion through the wall of a vacuum chamber. It can be used as a squeeze piston for simple pumps. It can also be used as a flexible coupling to transfer rotary motion between shafts.\n\nThe sylphon was invented in the early 1900s by meteorologist Weston Fulton (1871–1946), who named it for the sylphs of Western mythology. Also, a trade name used by Johnson Controls for pneumatically operated valves and damper actuators utilizing a metal bellows, they were rendered obsolete in the 1930s and 40s \n"}
{"id": "30928451", "url": "https://en.wikipedia.org/wiki?curid=30928451", "title": "Synchronous Data Flow", "text": "Synchronous Data Flow\n\nSynchronous Dataflow is a restriction of Kahn process networks, nodes produce and consume a fixed number of data items per firing. Allows static scheduling.\n\n\n"}
{"id": "2240380", "url": "https://en.wikipedia.org/wiki?curid=2240380", "title": "System for Electronic Document Analysis and Retrieval", "text": "System for Electronic Document Analysis and Retrieval\n\nThe System for Electronic Document Analysis and Retrieval (SEDAR) is a mandatory document filing and retrieval system for Canadian public companies. It is similar to EDGAR, the filing system operated by the Securities and Exchange Commission for United States public companies. SEDAR is administered by the Canadian Securities Administrators, a coordinating body comprising the 13 Canadian provincial and territorial securities commissions, and operated on their behalf since 2014 by the Alberta Securities Commission.\n\nSEDAR search results are rendered in PDF format. Searches of the database can be made by company name, industry group, document type or date filed.\n\nThrough registered filing agents, public companies file documents such as prospectuses, financial statements and material change reports. In the interest of transparency and full disclosure these documents are accessible to the public. Documents filed with regulators prior to the implementation of SEDAR in 1997 may be available from the individual securities commissions but in the case of the British Columbia Securities Commission historical filings are unretrievable and may have been destroyed.\n\n"}
{"id": "47262026", "url": "https://en.wikipedia.org/wiki?curid=47262026", "title": "Ultra-high-definition television", "text": "Ultra-high-definition television\n\nUltra-high-definition television (also known as Ultra HD television, Ultra HD, UHDTV, UHD and Super Hi-Vision) today includes 4K UHD and 8K UHD, which are two digital video formats with an aspect ratio of . These were first proposed by NHK Science & Technology Research Laboratories and later defined and approved by the International Telecommunication Union (ITU).\n\nThe Consumer Electronics Association announced on October 17, 2012, that \"Ultra High Definition\", or \"Ultra HD\", would be used for displays that have an aspect ratio of 16:9 or wider and at least one digital input capable of carrying and presenting native video at a minimum resolution of 3840×2160 pixels. In 2015, the Ultra HD Forum was created to bring together the end-to-end video production ecosystem to ensure interoperability and produce industry guidelines so that adoption of Ultra-high-definition television could accelerate. From just 30 in Q3 2015, the forum published a list up to 55 commercial services available around the world offering 4K resolution.\n\nThe \"UHD Alliance\", an industry consortium of content creators, distributors, and hardware manufacturers, announced during CES 2016 press conference its \"Ultra HD Premium\" specification, which defines resolution, bit depth, color gamut, high-dynamic-range imaging (HDRI) and rendering (HDRR) required for Ultra HD (UHDTV) content and displays to carry their Ultra HD Premium logo.\n\nUltra-high-definition television is also known as Ultra HD, UHD, UHDTV and 4K. In Japan, 8K UHDTV will be known as Super Hi-Vision since Hi-Vision was the term used in Japan for HDTV. In the consumer electronics market companies had previously only used the term 4K at the 2012 CES but that had changed to \"Ultra HD\" during the 2013 CES. The \"Ultra HD\" term is an umbrella term that was selected by the Consumer Electronics Association after extensive consumer research, as the term has also been established with the introduction of \"Ultra HD Blu-ray\".\n\nTwo resolutions are defined as UHDTV:\n\n\nThe human visual system has a limited ability to discern improvements in resolution when picture elements are already small enough or distant enough from the viewer. At home-viewing distances and current TV sizes, HD resolution is near the limits of resolution for the eye and increasing resolution to 4K has little perceptual impact, as consumers are beyond the critical distance (Lechner distance) to appreciate the differences in pixel count between 4K and HD. One exception to note is that even if resolution surpasses the resolving ability of the human eye, there is still an improvement in the way the image appears due to higher resolutions reducing aliasing.\n\nIn this context, it is important to be aware of two different types of what is often referred to be aliasing, but occur because of different reasons:\n\nThe first one results in false detail/moiré/striped patterns in the displayed image due to improper filtering of high spatial frequencies contained in the original image. Thus, all the details exceeding the Nyquist frequency, which is determined by the resolution of the display like a TV or projector, will fold back into the given bandwidth, leading to distortion in the form described before. This issue is not caused by the limited resolution of the display and can principally be avoided by filtering the original image when downscaling it to the proper native display resolution.\n\nThe often witnessed pixelized stairsteps of a low resolution image is not a direct consequence of what would correctly be called aliasing as no \"false frequencies\" are present but because the pixel pattern is simply not detailed enough. When reproduced correctly with a non-pixel-based screen in theory, any image would not become pixelized with lower and lower resolution, but only less and less sharp. Which leads to the second type.\n\nThe second type is more precisely called \"imaging\", at least in the audio domain, however misleadingly often enough called aliasing as well (even in technical literature), and refers to the high frequencies introduced by the nowadays pixel-based display of images. The individual pixels which in theory are only supposed to serve as individual samples for an entirely analog reconstruction, just like with audio, by their nature, when used \"as is\", have sharp discrete edges which equal high spatial frequencies (which couldn't have been present in the analog original in the first place as they would have been filtered before the A/D-process takes place), leading to a \"pixelized\" look if inside the visible bandwidth.\n\nThis second type of aliasing (imaging) can be defeated in two ways: one can use a pixel-based resolution which, depending on the distance and eyesight of the user, is high enough so that the eyes and their optical system serve as a low-pass filter (which already happens with 2K in many environments) or secondly, one could correctly filter the high spatial frequencies (anti-imaging, analog to the audio domain again) by either using other techniques than a pixel-based screen (CRT for example) or applying an optical filter in addition between the screen and the user.\n\nIn this context, raising the display resolution as mentioned before introduces two advantages in practise, where pixel-based displays and non-perfect downscaling probably will continue to persist: when downscaling without proper filtering, the higher end-resolution of the display allows more \"headroom\", where less aliasing will occur because the false frequencies will be \"mirrored back\" into the regular bandwidth at a later point, becoming eligible or less apparent at least.\n\nSecondly, the higher the display resolution, the higher the high spatial frequencies will be as well, which are introduced by the pixel-based rasterizarion. Compared to the audio domain, it is essentially noise shaping. In the case of images and video, the \"noise\" will be shifted into frequencies beyond what the eyes are able to resolve, cleaning up the usable bandwidth which is the actual image the user is supposed to see.\n\nUHDTV, however, allows other image enhancements than pixel density. Specifically, dynamic range and color are greatly enhanced, and these impact saturation and contrast differences that are readily resolved and greatly improve the experience of 4KTV compared to HDTV. UHDTV allows the future use of the new Rec. 2020 (UHDTV) color space which can reproduce colors that cannot be shown with the current Rec. 709 (HDTV) color space.\n\nIn terms of CIE 1931 color space, the new Rec. 2020 color space covers 75.8%, compared to coverage by the DCI-P3 digital cinema reference projector color space of just 53.6%, 52.1% by Adobe RGB color space, while the Rec. 709 color space covers only 35.9%. UHDTV's increases in dynamic range allow not only brighter highlights but also increased detail in the greyscale. UHDTV also allows for frame rates up to 120 frames per second (fps).\n\nUHDTV potentially allows Rec. 2020, higher dynamic range, and higher frame rates to work on HD services without increasing resolution to 4K, providing improved quality without as high of an increase in bandwidth demand.\n\nNHK researchers built a UHDTV prototype which they demonstrated in 2003. They used an array of 16 HDTV recorders with a total capacity of almost 3.5 TB that could capture up to 18 minutes of test footage. The camera itself was built with four 2.5 inch (64 mm) CCDs, each with a resolution of only 3840×2048. Using two CCDs for green and one each for red and blue, they then used a spatial pixel offset method to bring it to 7680×4320. Subsequently, an improved and more compact system was built using CMOS image sensor technology and the CMOS image sensor system was demonstrated at Expo 2005, Aichi, Japan, the NAB 2006 and NAB 2007 conferences, Las Vegas, at IBC 2006 and IBC 2008, Amsterdam, Netherlands, and CES 2009. A review of the NAB 2006 demo was published in a Broadcast Engineering e-newsletter. Individuals at NHK and elsewhere projected that the timeframe for UHDTV to be available in domestic homes varied between 2015 and 2020 but Japan was to get it in the 2016 time frame.\n\nOn November 2, 2006, NHK demonstrated a live relay of a UHDTV program over a 260 kilometer (km) distance by a fiber-optic network. Using dense wavelength division multiplex (DWDM), 24 Gbit/s speed was achieved with a total of 16 different wavelength signals.\n\nOn December 31, 2006, NHK demonstrated a live relay of their annual Kōhaku Uta Gassen over IP from Tokyo to a 450 in (11.4 m) screen in Osaka. Using a codec developed by NHK, the video was compressed from 24 Gbit/s to 180–600 Mbit/s and the audio was compressed from 28 Mbit/s to 7–28 Mbit/s. Uncompressed, a 20-minute broadcast would require roughly 4 TB of storage.\n\nThe SMPTE first released Standard 2036 for UHDTV in 2007. UHDTV was defined as having two levels, called UHDTV1 (38402160) and UHDTV2 (76804320).\n\nIn May 2007, the NHK did an indoor demonstration at the NHK Open House in which a UHDTV signal (7680 × 4320 at 60 fps) was compressed to a 250 Mbit/s MPEG2 stream. The signal was input to a 300 MHz wide band modulator and broadcast using a 500 MHz QPSK modulation. This \"on the air\" transmission had a very limited range (less than 2 meters), but shows the feasibility of a satellite transmission in the 36,000 km orbit.\n\nIn 2008, Aptina Imaging announced the introduction of a new CMOS image sensor specifically designed for the NHK UHDTV project. During IBC 2008 Japan's NHK, Italy's RAI, BSkyB, Sony, Samsung, Panasonic Corporation, Sharp Corporation, and Toshiba (with various partners) demonstrated the first ever public live transmission of UHDTV, from London to the conference site in Amsterdam.\n\nOn September 29, 2010, the NHK partnered up and recorded The Charlatans live in the UK in the UHDTV format, before broadcasting over the internet to Japan.\n\nOn May 19, 2011, SHARP in collaboration with NHK demonstrated a direct-view LCD display capable of 7680×4320 pixels at 10 bits per pixel. It was the first direct-view Super Hi-Vision-compatible display released.\n\nBefore 2011, UHDTV allowed for frame rates of 24, 25, 50, and 60 fps. In an ITU-R meeting during 2011, an additional frame rate was added to UHDTV of 120 fps.\n\nOn February 23, 2012, NHK announced that with Shizuoka University they had developed an 8K sensor that can shoot video at 120 fps.\n\nIn April 2012, NHK (in collaboration with Panasonic) announced a display (7680×4320 at 60 fps), which has 33.2 million 0.417 mm square pixels.\n\nIn April 2012, the four major Korean terrestrial broadcasters (KBS, MBC, SBS, and EBS) announced that in the future, they would begin test broadcasts of UHDTV on channel 66 in Seoul. At the time of the announcement, the UHDTV technical details had not yet been decided. LG Electronics and Samsung are also involved in UHDTV test broadcasts.\n\nIn May 2012, NHK showed the world's first ultra-high-definition shoulder-mount camera. By reducing the size and weight of the camera, the portability had been improved, making it more maneuverable than previous prototypes, so it can be used in a wide variety of shooting situations. The single-chip sensor uses a Bayer color-filter array, where only one color component is acquired per pixel. Researchers at NHK have also developed a high-quality up-converter, which estimates the other two-color components to convert the output into full resolution video.\n\nAlso in May 2012, NHK showed the ultra-high-definition imaging system it has developed in conjunction with Shizuoka University, which outputs 33.2-megapixel video at 120 fps with a color depth of 12 bits. As ultra-high-definition broadcasts at full resolution are designed for large, wall-sized displays, there is a possibility that fast-moving subjects may not be clear when shot at 60 fps, so the option of 120 fps has been standardized for these situations. To handle the sensor output of approximately 4 billion pixels per second with a data rate as high as 51.2 Gbit/s, a faster analog-to-digital converter has been developed to process the data from the pixels, and then a high-speed output circuit distributes the resulting digital signals into 96 parallel channels. This CMOS sensor is smaller and uses less power when compared to conventional ultra-high-definition sensors, and it is also the world's first to support the full specifications of the ultra-high-definition standard.\n\nDuring the 2012 Summer Olympics in Great Britain, the format was publicly showcased by the world's largest broadcaster, the BBC, which set up 15 meter wide screens in London, Glasgow, and Bradford to allow viewers to see the Games in ultra-high definition.\n\nOn May 31, 2012, Sony released the VPL-VW1000ES 4K 3D Projector, the world's first consumer-prosumer projector using the 4K UHDTV system, with the shutter-glasses stereoscopic 3D technology priced at US$24,999.99.\n\nOn August 22, 2012, LG announced the world's first 3D UHDTV using the 4K system.\n\nOn August 23, 2012, UHDTV was officially approved as a standard by the International Telecommunication Union (ITU), standardizing both 4K and 8K resolutions for the format in ITU-R Recommendation BT.2020.\n\nOn September 15, 2012, David Wood, Deputy Director of the EBU Technology and Development Department (who chairs the ITU working group that created Rec. 2020), told The Hollywood Reporter that Korea plans to begin test broadcasts of 4K UHDTV next year. Wood also said that many broadcasters have the opinion that going from HDTV to 8K UHDTV is too much of a leap and that it would be better to start with 4K UHDTV. In the same article Masakazu Iwaki, NHK Research senior manager, said that the NHK plan to go with 8K UHDTV is for economic reasons since directly going to 8K UHDTV would avoid an additional transition from 4K UHDTV to 8K UHDTV.\n\nOn October 18, 2012, the Consumer Electronics Association (CEA) announced that it had been unanimously agreed on by a vote of the CEA's Board of Industry Leaders that the term \"Ultra High-Definition\", or \"Ultra HD\", would be used for displays that have a resolution of at least 8 megapixels with a vertical resolution of at least 2,160 pixels and a horizontal resolution of at least 3,840 pixels. The Ultra HD label also requires the display to have an aspect ratio of 16:9 or wider and to have at least one digital input that can carry and present a native video signal of 3840x2160 without having to rely on a video scaler. Sony announced they would market their 4K products as \"4K Ultra High-Definition (4K UHD)\".\n\nOn October 23, 2012, Ortus Technology Co., Ltd announced the development of the world's smallest 3840x2160 pixel LCD panel with a size of and a pixel density of 458ppi. The LCD panel is designed for medical equipment and professional video equipment.\n\nOn October 25, 2012, LG Electronics began selling the first flat panel Ultra HD display in the United States with a resolution of 3840x2160. The LG 84LM9600 is a flat panel LED-backlit LCD display with a price of US$19,999 though the retail store was selling it for US$16,999.\n\nOn November 29, 2012, Sony announced the 4K Ultra HD Video Player—a hard disk server preloaded with ten 4K movies and several 4K video clips that they planned to include with the Sony XBR-84X900. The preloaded 4K movies are \"The Amazing Spider-Man\", \"Total Recall\" (2012), \"The Karate Kid\" (2010), \"Salt\", \"\", \"The Other Guys\", \"Bad Teacher\", \"That's My Boy\", \"Taxi Driver\", and \"The Bridge on the River Kwai\". Additional 4K movies and 4K video clips will be offered for the 4K Ultra HD Video Player in the future .\n\nOn November 30, 2012, Red Digital Cinema Camera Company announced that they were taking pre-orders for the US$1,450 REDRAY 4K Cinema Player, which can output 4K resolution to a single 4K display or to four 1080p displays arranged in any configuration via four HDMI 1.4 connections. Video output can be 4K DCI (4096x2160), 4K Ultra HD, 1080p, and 720p at frame rates of up to 60 fps with a bit depth of up to 12-bits with 4:2:2 chroma subsampling. Audio output can be up to 7.1 channels. Content is distributed online using the ODEMAX video service. External storage can be connected using eSATA, Ethernet, USB, or a Secure Digital memory card.\n\nOn January 6, 2013, the NHK announced that Super Hi-Vision satellite broadcasts could begin in Japan in 2016.\n\nOn January 7, 2013, Eutelsat announced the first dedicated 4K Ultra HD channel. Ateme uplinks the H.264/MPEG-4 AVC channel to the EUTELSAT 10A satellite. The 4K Ultra HD channel has a frame rate of 50 fps and is encoded at 40 Mbit/s. The channel started transmission on January 8, 2013. On the same day Qualcomm CEO Paul Jacobs announced that mobile devices capable of playing and recording 4K Ultra HD video would be released in 2013 using the Snapdragon 800 chip.\n\nOn January 8, 2013, Broadcom announced the BCM7445, an Ultra HD decoding chip capable of decoding High Efficiency Video Coding (HEVC) at up to 4096 × 2160p at 60 fps. The BCM7445 is a 28 nm ARM architecture chip capable of 21,000 Dhrystone MIPS with volume production estimated for the middle of 2014. On the same day THX announced the \"THX 4K Certification\" program for Ultra HD displays. The certification involves up to 600 tests and the goal of the program is so that \"content viewed on a THX Certified Ultra HD display meets the most exacting video standards achievable in a consumer television today\".\n\nOn January 14, 2013, Blu-ray Disc Association president Andy Parsons stated that a task force created three months ago is studying an extension to the Blu-ray Disc specification that would add support for 4K Ultra HD video.\n\nOn January 25, 2013, the BBC announced that the BBC Natural History Unit would produce \"Survival\"—the first wildlife TV series recorded in 4K resolution. This was announced after the BBC had experimented with 8k during the London Olympics.\n\nOn January 27, 2013, Asahi Shimbun reported that 4K Ultra HD satellite broadcasts would start in Japan with the 2014 FIFA World Cup. Japan's Ministry of Internal Affairs and Communications decided on this move to stimulate demand for 4K Ultra HD TVs.\n\nOn February 21, 2013, Sony announced that the PlayStation 4 would support 4K resolution output for photos and videos but wouldn't render games at that resolution.\n\nOn March 26, 2013, the Advanced Television Systems Committee (ATSC) announced a call for proposals for the ATSC 3.0 physical layer that specifies support for 3840x2160 resolution at 60 fps.\n\nOn April 11, 2013, Bulb TV created by Canadian serial entrepreneur Evan Kosiner became the first broadcaster to provide a 4K linear channel and VOD content to cable and satellite companies in North America. The channel is licensed by the Canadian Radio-Television and Telecommunications Commission to provide educational content.\n\nOn April 19, 2013, SES announced the first Ultra HD transmission using the HEVC standard. The transmission had a resolution of 3840x2160 and a bit rate of 20 Mbit/s.\n\nOn May 9, 2013, NHK and Mitsubishi Electric announced that they had jointly developed the first HEVC encoder for 8K Ultra HD TV, which is also called Super Hi-Vision (SHV). The HEVC encoder supports the Main 10 profile at Level 6.1 allowing it to encode 10-bit video with a resolution of 7680x4320 at 60 fps. The HEVC encoder has 17 3G-SDI inputs and uses 17 boards for parallel processing with each board encoding a row of 7680x256 pixels to allow for real time video encoding. The HEVC encoder is compliant with draft 4 of the HEVC standard and has a maximum bit rate of 340 Mbit/s. The HEVC encoder was shown at the NHK Science & Technology Research Laboratories Open House 2013 that took place from 30 May to June 2. At the NHK Open House 2013 the HEVC encoder used a bit rate of 85 Mbit/s, which gives a compression ratio of 350:1.\n\nOn May 21, 2013, Microsoft announced the Xbox One, which supports 4K resolution (3840×2160) video output and 7.1 surround sound. Yusuf Mehdi, corporate vice president of marketing and strategy for Microsoft, has stated that there is no hardware restriction that would prevent Xbox One games from running at 4K resolution.\n\nOn May 30, 2013, Eye IO announced that their encoding technology was licensed by Sony Pictures Entertainment to deliver 4K Ultra HD video. Eye IO encodes their video assets at 3840x2160 and includes support for the xvYCC color space.\n\nIn mid-2013, a China television manufacturer produced the first 50-inch UHD television set costing less than $1,000.\n\nOn June 11, 2013, Comcast announced that they had demonstrated the first public U.S.-based delivery of 4K Ultra HD video at the 2013 NCTA show. The demonstration included segments from Oblivion, Defiance, and nature content sent over a DOCSIS 3.0 network.\n\nOn June 13, 2013, ESPN announced that they would end the broadcast of the ESPN 3D channel by the end of that year and would \"...experiment with things like UHDTV.\"\n\nOn June 26, 2013, Sharp announced the LC-70UD1U, which is a 4K Ultra HD TV. The LC-70UD1U is the world's first TV with THX 4K certification.\n\nOn July 2, 2013, Jimmy Kimmel Live! recorded in 4K Ultra HD a performance by musical guest Karmin, and the video clip was used as demonstration material at Sony stores.\n\nOn July 3, 2013, Sony announced the release of their 4K Ultra HD Media Player with a price of US$7.99 for rentals and US$29.99 for purchases. The 4K Ultra HD Media Player only works with Sony's 4K Ultra HD TVs.\n\nOn July 15, 2013, the CEA published CEA-861-F, a standard that applies to interfaces such as DVI, HDMI, and LVDS. CEA-861-F adds support for several Ultra HD video formats and additional color spaces.\n\nOn September 2, 2013 Acer announced the first smartphone, dubbed Liquid S2, capable of recording 4K video.\n\nOn September 4, 2013, the HDMI Forum released the HDMI 2.0 specification, which supports 4K resolution at 60 fps. On the same day, Panasonic announced the Panasonic TC-L65WT600—the first 4K TV to support 4K resolution at 60 fps. The Panasonic TC-L65WT600 has a screen, support for DisplayPort 1.2a, support for HDMI 2.0, an expected ship date of October, and a suggested retail price of US$5,999.\n\nOn September 12–17, 2013, at the 2013 IBC Conference in Amsterdam, Nagra introduced a Ultra HD User Interface called Project Ultra based on HTML5, which works with OpenTV 5.\n\nOn October 4, 2013, DigitalEurope, announced the requirements for their UHD logo in Europe. The DigitalEurope UHD logo requires that the display support a resolution of at least 3840x2160, a 16:9 aspect ratio, the Rec. 709 (HDTV) color space, 8-bit video, 24p/25p/30p/50p/60p frame rates, and 2 channel audio.\n\nOn October 29, 2013, Elemental Technologies announced support for real-time 4K Ultra HD HEVC video processing. Elemental provided live video streaming of the 2013 Osaka Marathon on October 27, 2013, in a workflow designed by K-Opticom, a telecommunications operator in Japan. Live coverage of the race in 4K Ultra HD was available to viewers at the International Exhibition Center in Osaka. This transmission of 4K Ultra HD HEVC video in real-time was an industry-first.\n\nOn November 28, 2013, Organizing Committee of the XXII Olympic Winter Games and XI Paralympic Winter Games 2014 in Sochi chief Dmitri Chernyshenko stated that the 2014 Olympic Winter Games would be shot in 8K Super Hi-Vision.\n\nOn December 25, 2013, YouTube added a \"2160p 4K\" option to its videoplayer. Previously, a visitor had to select the \"original\" setting in the video quality menu to watch a video in 4K resolution. With the new setting, YouTube users can much more easily identify and play 4K videos.\n\nOn December 30, 2013, Samsung announced availability of its 110-inch Ultra HDTV for custom orders, making this the world's largest Ultra HDTV so far.\n\nOn January 22, 2014, European Southern Observatory became the first scientific organization to deliver Ultra HD footage at regular intervals.\n\nOn May 6, 2014, France announced DVB-T2 tests in Paris for Ultra HD HEVC broadcast with objectives to replace by 2020 the current DVB-T MPEG4 HD national broadcast.\n\nOn May 26, 2014, satellite operator Eutelsat announced the launch of Europe's first Ultra HD demo channel in HEVC, broadcasting at 50 frames/second. The channel is available on the Hot Bird satellites and can be watched by viewers with 4k TVs equipped with DVB-S2 demodulators and HEVC decoders.\n\nIn June 2014, the FIFA World Cup of that year (held in Brazil) became the first shot entirely in Ultra HD, by Sony. The European Broadcasting Union (EBU) broadcast matches of the FIFA World Cup to audiences in North America, Latin America, Europe and Asia in Ultra HD via SES' NSS-7 and SES-6 satellites. Indian satellite TV provider unveils its plan to launch 4k UHD service early in 2015 and showcased live FIFA World Cup quarter final match in 4k UHD through Sony Entertainment Television Sony SIX.\n\nOn June 24, 2014, the CEA updated the guidelines for Ultra High-Definition and released guidelines for Connected Ultra High-Definition, adding support for internet video delivered with HEVC. The CEA is developing a UHD logo for voluntary use by companies that make products that meet CEA guidelines. The CEA also clarified that \"Ultra High-Definition\", \"Ultra HD\", or \"UHD\" can be used with other modifiers and gave an example with \"Ultra High-Definition TV 4K\".\n\nOn July 15, 2014, Researchers from the University of Essex both captured and delivered its graduation ceremonies in 4kUHDTV over the internet using H.264 in realtime. The 4K video stream was published at 8 Mbit/s and 14 Mbit/s for all its 11 ceremonies (till July 19), with people viewing in from countries such as Cyprus, Bulgaria, Germany, Australia, UK and others.\n\nOn September 4, 2014, Canon Inc. announced that a firmware upgrade would add Rec. 2020 color space support to their EOS C500 and EOS C500 PL camera models and their DP-V3010 4K display.\n\nOn September 4, 2014, Microsoft announced a firmware update for the Microsoft Lumia 1020, 930, Icon, and 1520 phones that adds 4k video recording. The update was later released by the individual phone carriers over the following weeks and months after the announcement.\n\nOn September 5, 2014, the Blu-ray Disc Association announced that the 4K Blu-ray Disc specification supports 4K video at 60 fps, High Efficiency Video Coding, the Rec. 2020 color space, high dynamic range, and 10-bit color depth. 4K Blu-ray Disc will have a data rate of at least 50 Mbit/s and may include support for 66/100 GB discs. 4K Blu-ray Disc began licensing in 2015, with 4K Blu-ray Disc players released late that year.\n\nOn September 5, 2014, DigitalEurope released an Ultra HD logo for companies that meet their technical requirements.\n\nOn September 11, 2014 satellite operator SES announced the first Ultra HD conditional access-protected broadcast using DVB standards at the IBC show in Amsterdam. The demonstration used a Samsung Ultra HD TV, with a standard Kudelski SmarDTV CI Plus conditional access module, to decrypt a full 3840x2160 pixel CAS-protected Ultra HD signal in HEVC broadcast via an SES Astra satellite at 19.2°E.\n\nOn November 19, 2014, rock band Linkin Park's concert at Berlin's O2 World Arena was broadcast live in Ultra HD via an Astra 19.2°E satellite. The broadcast was encoded in the UHD 4K standard with the HEVC codec (50 frames per second and a 10 bit color depth), and was a joint enterprise of satellite owner SES, SES Platform Services (now MX1) and Samsung.\n\nIndian satellite pay TV provider Tata Sky launched UHD service and UHD Set Top Box on 9 January 2015. The service is 4Kp50 and price of UHD box is ₹5900 for existing SD/HD customers and ₹6400 for new customers. The Cricket World Cup 2015 was telecast live in 4K for free to those who own Tata Sky's UHD 4K STB.\n\nIn May 2015, France Télévisions broadcast matches from Roland Garros live in Ultra HD via the EUTELSAT 5 West A satellite in the HEVC standard. The channel \"France TV Sport Ultra HD\" was available via the Fransat platform for viewers in France.\n\nIn May 2015, satellite operator SES announced that Europe's first free-to-air Ultra HD channel (from Germany's pearl.tv shopping channel) would launch in September 2015, broadcast in native Ultra HD via the Astra 19.2°E satellite position.\n\nIn June, SES launched its first Ultra HD demonstration channel for cable operators and content distributors in North America to prepare their systems and test their networks for Ultra HD delivery. The channel is broadcast from the SES-3 satellite at 103°W.\n\nIn June, SPI International previewed its \"4K FunBox UHD\" Ultra HD channel on the HOT BIRD 4K1 channel, in advance of its commercial launch on Eutelsat's HOT BIRD satellites in the autumn.\n\nIn July 2015, German HD satellite broadcaster HD+ and TV equipment manufacturer TechniSat announced an Ultra HD TV set with integrated decryption for reception of existing HD+ channels (available in the Autumn) and a new Ultra HD demonstration channel due to begin broadcasting in September.\n\nOn 2 August 2015, The FA Community Shield in England was broadcast in Ultra HD by broadcast company BT Sport, becoming the first English football game shown in the Ultra HD. The match was shown on Europe's first Ultra HD channel, BT Sport Ultra HD.\n\nFashion One 4K launched on September 2, 2015 becoming the first global Ultra HD TV channel. Reaching nearly 370 million households across the World, the fashion, lifestyle and entertainment network broadcasts via satellite from Measat at 91.5°E (for Asia Pacific, Middle East, Australia) and from SES satellites Astra 19.2°E (for Europe), SES-3 at 103°W (for North America), NSS-806 at 47.5°W (for South America).\n\nIn September 2015, Eutelsat presented new consumer research, conducted by TNS and GfK, on Ultra HD and screen sales in key TV markets. The study looked at consumer exposure to Ultra HD, perceived benefits and willingness to invest in equipment and content. GfK predicts a 200% increase in Ultra HD screen sales from June to December 2015, with sales expected to reach five million by the end of the year. GfK also forecasts that Ultra HD screens in 2020 will represent more than 70% of total sales across Europe and almost 60% in the Middle East and North Africa.\n\nOn 2 September 2015, Sony unveiled the Xperia Z5 Premium; the first smartphone with a 4K display.\n\nOn 9 September 2015, Apple Inc. announced that their new smartphone the iPhone 6s could record video in 4K.\n\nOn 6 October 2015, Microsoft unveiled the latest version of their Microsoft Surface Book laptop with a display of \"over 6 million pixels\" and their new phones the Microsoft Lumia 950 and 950 XL, which, aside from 4K video recording that their predecessors included, feature a display of \"over 5 million pixels\".\n\nOn 8 December 2015, the Roman Catholic ceremony of the opening of the Holy Door in Vatican City, which marked the beginning of the Jubilee Year of Mercy, was the first worldwide Ultra HD broadcast via satellite. The event was produced in Ultra-HD by the Vatican Television Center with the support of Eutelsat, Sony, Globecast and DBW Communication. The team did some advanced experimentation with 4K/High Dynamic Range live images and in particular using technology developed by the BBC's R&D division and Japan's public broadcaster NHK in terms of Hybrid Log Gamma (HLG) signals.\n\nThe \"UHD Alliance\", an industry consortium of content creators, distributors, and hardware manufacturers, announced Monday on the 11th of January 2016 during CES 2016 press conference its \"Ultra HD Premium\" specification, which defines resolution, bit depth, color gamut, high-dynamic-range imaging (HDRI) and rendering (HDRR) required for Ultra HD (UHDTV) content and displays to carry their Ultra HD Premium logo.\n\nOn April 2, 2016, Ultra-high-definition television demo channel UHD1 broadcast the Le Corsaire ballet in Ultra HD live from the Vienna State Opera. The programme was produced by Astra satellite owner, SES in collaboration with European culture channel ARTE, and transmitted free-to-air, available to anyone with reception of the Astra 19.2°E satellites and an ultra HD screen equipped with an HEVC decoder.\n\nAs of April 2016, The NPD Group reported that 6 million 4K UHD televisions had been sold.\n\nIn May 2016, Modern Times Group, owner of the Viasat DTH platform announced the launch of Viasat Ultra HD, the first UHD channel for the Nordic region and the first UHD Sports channel in the World. The channel features selected live sport events especially produced in Ultra HD and launch in the autumn via the SES-5 satellite at 5°E. Viasat is also launching an Ultra HD set-top box from Samsung and a TV-module that enables existing UHD TVs to display the channel. Satellite operator, SES said that the launch of Viasat Ultra HD brings the number of UHD channels (including test channels and regional versions) carried on SES satellites to 24, or 46% of all UHD channels broadcast via satellite worldwide. In August 2016, Sky announced that 4K broadcasts would begin via their new Sky Q 2TB box. The opening match of the 2016-17 Premier League between Hull City and Leicester City on Sky Sports was the first 4K transmission.\n\nOn 29 September 2017, BSAT-4a, dedicated for UHDTV programming and was also claimed \"the world's first 8K satellite\", was launched from the Guiana Space Centre aboard Ariane 5 rocket. BSAT-4a would be used for 2020 Summer Olympics held in Japan.\n\nIn December 2017, Qualcomm announced that their Snapdragon 845 chipset and Spectra 280 Image Signal Processor would be the first phone SoC to record video in UHD Premium.\n\nSatellite operator SES broadcast an 8K television signal via its satellite system for the first time in May 2018. The 8K demonstration content, with a resolution of 7680x4320 pixels, a frame rate of 60 frames per second and 10-bit colour depth, was encoded in HEVC and transmitted at a rate of 80 Mbit/s via the Astra 3B satellite during SES's Industry Days conference in Luxembourg.\n\nIn June 2018, fuboTV broadcast the 2018 FIFA World Cup live in 4K and HDR10 becoming the first OTT streaming service to do so. Quarter, Semi and Final matches were available on many popular streaming devices including Apple TV, Chromecast Ultra, Fire TV, Roku and Android TVs. Content was streamed at 60 frames per second using HLS and DASH. Video was sent in fragmented MP4 containers delivering HEVC encoded video.\n\n\n\n\n\n\nField trials using existing digital terrestrial television (DTT) transmitters have included the following.\nStandards that deal with UHDTV include:\n\nStandards approved in ITU-R:\nOther documents prepared or being prepared by ITU-R:\n\nStandards developed in ITU-T's VCEG and ISO/IEC JTC 1's MPEG that support Ultra-HD include:\n\n\nDVB approved the Standard TS 101 154 V2.1.1, published (07/2014) in the DVB Blue Book A157 \"Specification for the use of Video and Audio Coding in Broadcasting Applications based on the MPEG-2 Transport Stream\", which was published by ETSI in the following months.\n\nStandards for UHDTV in Korea have been developed by its Telecommunications Technology Association.\n\nOn August 30, 2013, the scenarios for 4K-UHDTV service were described in the Report \"TTAR 07.0011: A Study on the UHDTV Service Scenarios and its Considerations\".\n\nOn May 22, 2014, the technical report \"TTAR-07.0013: Terrestrial 4K UHDTV Broadcasting Service\" was published.\n\nOn October 13, 2014, an interim standard – \"TTAI.KO-07.0123: Transmission and Reception for Terrestrial UHDTV Broadcasting Service\" – was published based on HEVC encoding, with MPEG 2 TS, and DVB-T2 serving as the standards.\n\nOn June 24, 2016, a standard – \"TTAK.KO-07.0127: Transmission and Reception for Terrestrial UHDTV Broadcasting Service\" – was published based on HEVC encoding, with MMTP/ROUTE IP, and ATSC 3.0 serving as the standards.\n\n\n"}
{"id": "1001784", "url": "https://en.wikipedia.org/wiki?curid=1001784", "title": "United Biscuits", "text": "United Biscuits\n\nUnited Biscuits (\"UB\") is a British multinational food manufacturer, makers of McVitie's biscuits, Jacob's Cream Crackers, and Twiglets. The company was listed on the London Stock Exchange and was once a constituent of the FTSE 100 Index. In November 2014, the company was acquired by Yıldız Holding and is now part of Pladis.\n\nUnited Biscuits (UB) was formed in 1948 by a merger of two Scottish family businesses: McVitie & Price and MacFarlane Lang. In 1962, William Crawford & Sons, best known for its shortbread, was acquired by United Biscuits for £6 million. In 1965, the company also acquired William MacDonald & Sons for £2.8 million, and brought the Penguin brand to the group.\n\nIn 1972, United Biscuits acquired \"Carr's of Carlisle\", makers of Table Water biscuits, from James Goldsmith's Cavenham Foods for £2.75 million. Two years later in 1974, United Biscuits acquired the US-based Keebler Company for $53 million. It also owned the Wimpy Bar fast food restaurant chain between 1977 and 1989.\n\nUnited Biscuits acquired the frozen food company Ross Young's from Hanson in 1988 for £335 million.\n\nUnited Biscuits sold Keebler for $500 million in 1995 after giving up efforts to break into the American market.\n\nUnited Biscuits was acquired in May 2000 by Finalrealm, a consortium of financial investors, as well as Nabisco Holdings Corporation. As part of the transaction, UB acquired Nabisco's European businesses. The company sold Young's Bluecrest in 2001, in order to concentrate on the sweet biscuit sector. In September 2004, United Biscuits bought the UK portion of Groupe Danone's Jacob's Biscuit Group for £240 million, including Cream Crackers and Twiglets. In July 2006, the company sold its Southern European biscuits business to Kraft Foods, which in turn left the owner syndicate. In October 2006, MidOcean Partners sold the company to a consortium made up of the Blackstone Group and PAI Partners. The deal was completed in December 2006.\n\nIn December 2012 UB agreed the sale of its KP Snacks business to Intersnack for £500 million. On 3 November 2014, private equity funds managed by Blackstone and PAI Partners announced the sale of United Biscuits to Yıldız Holding.\n\nThe core of the business is in the United Kingdom, where it produces biscuits under a number of brand names including: McVitie's biscuits and Jacob's Cream Crackers.\n\nThe company manufactures in a number of countries across Europe, such as the Netherlands, France and Belgium. It also has a manufacturing site in India.\n\nThe company's headquarters is in Hayes in suburban West London. The company's main UK distribution centre is at Ashby-de-la-Zouch.\n\nThe United Biscuits Network, a closed-circuit radio network serving their factories across the UK which was active from 1970 to 1979, spawned the career of UK radio presenter Steve Allen and TV and radio personality Dale Winton.\n\nIn his 2009 book \"The Pleasures and Sorrows of Work\", Alain de Botton chronicles his tour and brief experiences with employees of United Biscuits as they launched the \"Moments\" biscuit line.\n\n"}
{"id": "1985147", "url": "https://en.wikipedia.org/wiki?curid=1985147", "title": "Utility computing", "text": "Utility computing\n\nUtility computing, or The Computer Utility, is a service provisioning model in which a service provider makes computing resources and infrastructure management available to the customer as needed, and charges them for specific usage rather than a flat rate. Like other types of on-demand computing (such as grid computing), the utility model seeks to maximize the efficient use of resources and/or minimize associated costs. Utility is the packaging of system resources, such as computation, storage and services, as a metered service. This model has the advantage of a low or no initial cost to acquire computer resources; instead, resources are essentially rented.\n\nThis repackaging of computing services became the foundation of the shift to \"on demand\" computing, software as a service and cloud computing models that further propagated the idea of computing, application and network as a service.\n\nThere was some initial skepticism about such a significant shift. However, the new model of computing caught on and eventually became mainstream.\n\nIBM, HP and Microsoft were early leaders in the new field of utility computing, with their business units and researchers working on the architecture, payment and development challenges of the new computing model. Google, Amazon and others started to take the lead in 2008, as they established their own utility services for computing, storage and applications.\n\nUtility computing can support grid computing which has the characteristic of very large computations or sudden peaks in demand which are supported via a large number of computers.\n\n\"Utility computing\" has usually envisioned some form of virtualization so that the amount of storage or computing power available is considerably larger than that of a single time-sharing computer. Multiple servers are used on the \"back end\" to make this possible. These might be a dedicated computer cluster specifically built for the purpose of being rented out, or even an under-utilized supercomputer. The technique of running a single calculation on multiple computers is known as distributed computing.\n\nThe term \"grid computing\" is often used to describe a particular form of distributed computing, where the supporting nodes are geographically distributed or cross administrative domains. To provide utility computing services, a company can \"bundle\" the resources of members of the public for sale, who might be paid with a portion of the revenue from clients.\n\nOne model, common among volunteer computing applications, is for a central server to dispense tasks to participating nodes, on the behest of approved end-users (in the commercial case, the paying customers). Another model, sometimes called the Virtual Organization (VO), is more decentralized, with organizations buying and selling computing resources as needed or as they go idle.\n\nThe definition of \"utility computing\" is sometimes extended to specialized tasks, such as web services.\n\nUtility computing merely means \"Pay and Use\", with regards to computing power.\nUtility computing is not a new concept, but rather has quite a long history. Among the earliest references is:\nIBM and other mainframe providers conducted this kind of business in the following two decades, often referred to as time-sharing, offering computing power and database storage to banks and other large organizations from their worldwide data centers. To facilitate this business model, mainframe operating systems evolved to include process control facilities, security, and user metering. The advent of mini computers changed this business model, by making computers affordable to almost all companies. As Intel and AMD increased the power of PC architecture servers with each new generation of processor, data centers became filled with thousands of servers.\n\nIn the late 90's utility computing re-surfaced. InsynQ, Inc. launched [on-demand] applications and desktop hosting services in 1997 using HP equipment. In 1998, HP set up the Utility Computing Division in Mountain View, CA, assigning former Bell Labs computer scientists to begin work on a computing power plant, incorporating multiple utilities to form a software stack. Services such as \"IP billing-on-tap\" were marketed. HP introduced the Utility Data Center in 2001. Sun announced the Sun Cloud service to consumers in 2000. In December 2005, Alexa launched Alexa Web Search Platform, a Web search building tool for which the underlying power is utility computing. Alexa charges users for storage, utilization, etc. There is space in the market for specific industries and applications as well as other niche applications powered by utility computing. For example, PolyServe Inc. offers a clustered file system based on commodity server and storage hardware that creates highly available utility computing environments for mission-critical applications including Oracle and Microsoft SQL Server databases, as well as workload optimized solutions specifically tuned for bulk storage, high-performance computing, vertical industries such as financial services, seismic processing, and content serving. The Database Utility and File Serving Utility enable IT organizations to independently add servers or storage as needed, retask workloads to different hardware, and maintain the environment without disruption.\n\nIn spring 2006 3tera announced its AppLogic service and later that summer Amazon launched Amazon EC2 (Elastic Compute Cloud). These services allow the operation of general purpose computing applications. Both are based on Xen virtualization software and the most commonly used operating system on the virtual computers is Linux, though Windows and Solaris are supported. Common uses include web application, SaaS, image rendering and processing but also general-purpose business applications.\n\n\nDecision support and business intelligence 8th edition page 680 \n\n"}
{"id": "14702580", "url": "https://en.wikipedia.org/wiki?curid=14702580", "title": "Vector Graphic", "text": "Vector Graphic\n\nVector Graphic was an early microcomputer company founded in 1976, the same year as Apple Computer, during the pre-IBM PC era, along with the NorthStar Horizon, IMSAI, and MITS Altair.\n\nThe first product was a memory card for the S-100 bus. A full microcomputer using the Z80 microprocessor, the Vector 1, was introduced in 1977. There were several Vector Graphic models produced. The Vector 1+ had a floppy disk drive. The Vector Graphic 3 had a fixed keyboard housed anchoring a combined screen terminal and CPU case. The Vector Graphic 4 was a transitional 8-bit and 16-bit hybrid model.\n\nAlthough primarily used with the CP/M operating system, the Vector 3 ran several others including OASIS, Micropolis Disk Operating System (MDOS), and Micropolis Z80 Operating System (MZOS).\n\nEarly Vector Graphic models used the Micropolis floppy disk controller and Micropolis floppy disk drives. Later models were designed with the integrated floppy drive-hard drive controller and used Tandon floppy drives. Almost all used unusual 100-track per inch 5 ¼-inch floppy drives and 16-sector hard sector media. Some models included 8-inch floppy drives and hard disk drives.\n\nVector Graphic sales peaked in 1982, by which time the company was publicly traded, at $36 million. It faltered soon after due to several factors. The introduction of the IBM PC in August 1981 shifted the market and smaller players lost momentum. The Vector 4 was accidentally pre-announced in April 1982, the same month that founder and chief hardware designer Robert Harp left the company after a dispute with co-founder (and wife) Lore Harp over control of the company. The early announcement of the Vector 4, which had a separate keyboard tethered to the computer (as opposed to a combined keyboard and terminal) resulted in a sharp decrease in sales of the Vector 3 as customers delayed purchases up to six months until the new product was available. In addition, the company had decided to use the CP/M operating system in the Vector 4, which they considered a superior operating system than MDOS; management recognized the nature of their gamble, as IBM would move the market in a different direction if it elected to use the DOS operating system for their competing product, the IBM 8080. The gamble did not pay off, and by the end of 1984 Lore Harp was gone and venture capital investors took over. By summer 1985 only three dozen employees remained, down from a peak of 425 workers in 1982. Ultimately, the Vector Graphic headquarters and assembly factory, across from a 17-person company (Amgen) and next to the 101 freeway, was converted into a Home Depot store. Chapter 11 bankruptcy followed in December 1985. A sought-for merger partner was not found and chapter 7 liquidation of remaining assets resulted in October 1987.\n\nVector Graphic computers had many innovations, such as the Flashwriter integrated video and keyboard controller. Vector Graphic was known for their Memorite word processing application. When combined with the Flashwriter, the Vector Graphic Memorite software gave low-cost word processing capability, which had previously only been available with dedicated word processors.\n\nAs of 2007, Vector Graphic still had a small but active user community.\n\n\n\n\nMailing List:\n"}
{"id": "38981459", "url": "https://en.wikipedia.org/wiki?curid=38981459", "title": "Yibada", "text": "Yibada\n\nYibada (Chinese: 易八達;) is a Global Chinese online media company for Chinese communities around the world. Yibada operates 35 cities' editions, including New York City, San Francisco, Los Angeles, Chicago, Hongkong, Beijing, Shanghai, Nigra, São Paulo, etc. The different cities' editions help facilitate easy access to local cities' news and useful information.\n\nLaunched in 2005, Yibada was created to provide news and information services to the local Chinese community. It is a privately held company and is still owned by its co-founder Winnie Wong. Its headquarters are in the Financial District of New York.\n\nThe company was first conceived in San Francisco, California, by Winnie Wong, a Chinese native born in Hong Kong, who came up with the idea for global Chinese community news and practical information. She found that the international Chinese around the world needed more news and local information. Online media could be a better platform.\nThe site went live in 2005, and formally incorporated the company in March 2006 as Yibada in California. The website has the large circulation among Chinese and Chinese American, and it grows quickly and has expanded to a national Network. In the last 8 years, Yibada has covered 35 cities around the world, including 17 cities in North America, 5 cities in China, and 13 cities in other regions, like Europe, South America, Middle East and Africa.\n\nYibada features news, classified information, and custom content yellow pages. It has since expanded to include other specialized sections, Travel, Food, Fashion, Wedding, Health, Education, TV, Game, and Forum. Yibada provides Internet services to the local Chinese population. In every localized city edition, there are breaking News, practical classified information and specialized Travel, Food, Fashion, Wedding, Health, Education, TV, Game, and Forum sections.\n\nYibada developed the local custom yellow pages product to provide the best practical information to the local Chinese; the content includes the detail business introduction, using custom photos, videos to display the local business information. Yibada also set up the practical classified information and specialized sections to help the local Chinese get more useful information from the Internet. Yibada also share and spread local information via the popular social media, such as Facebook, Twitter, Sina Weibo and Forum with a large number of fans from these media. Yibada Forum and Weibo provide a free and large platform for more Chinese around the world to share the fastest and useful information and communicate with each other.\n\n"}
