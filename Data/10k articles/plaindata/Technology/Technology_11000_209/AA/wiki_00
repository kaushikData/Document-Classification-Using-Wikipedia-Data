{"id": "578412", "url": "https://en.wikipedia.org/wiki?curid=578412", "title": "Acetone peroxide", "text": "Acetone peroxide\n\nAcetone peroxide is an organic peroxide and a primary high explosive. It is produced by the reaction of acetone and hydrogen peroxide to yield a mixture of linear monomer and cyclic dimer, trimer, and tetramer forms. The trimer is known as triacetone triperoxide (TATP) or tri-cyclic acetone peroxide (TCAP). The dimer is known as diacetone diperoxide (DADP). Acetone peroxide takes the form of a white crystalline powder with a distinctive bleach-like odor (when impure) or a fruit-like smell when pure and can explode if subjected to heat, friction, static electricity, concentrated sulfuric acid, strong UV radiation or shock. As a non-nitrogenous explosive, TATP has historically been more difficult to detect, and it has been used as an explosive in several terrorist attacks since 2001.\n\nAcetone peroxide (specifically, triacetone triperoxide) was discovered in 1895 by Richard Wolffenstein. Wolffenstein combined acetone and hydrogen peroxide, and then he allowed the mixture to stand for a week at room temperature, during which time a small quantity of crystals precipitated, which had a melting point of 97 °C.\n\nIn 1899 Adolf von Baeyer and Victor Villiger described the first synthesis of the dimer and described use of acids for the synthesis of both peroxides. \nBaeyer and Villiger prepared the dimer by combining potassium persulfate in diethyl ether with acetone, under cooling. After separating the ether layer, the product was purified and found to melt at 132–133 °C. They found that the trimer could be prepared by adding hydrochloric acid to a chilled mixture of acetone and hydrogen peroxide. By using the depression of freezing points to determine the molecular weights of the compounds, they also determined that the form of acetone peroxide that they had prepared via potassium persulfate was a dimer, whereas the acetone peroxide that had been prepared via hydrochloric acid was a trimer, like Wolffenstein's compound.\n\nWork on this methodology and on the various products obtained, was further investigated in the mid-20th century by Milas and Golubović.\n\nThe chemical name acetone peroxide is most commonly used to refer to the cyclic trimer, the product of a reaction between two precursors, hydrogen peroxide and acetone, in an acid-catalyzed nucleophilic addition, although various further monomeric and dimeric forms are possible.\n\nSpecifically, two dimers, one cyclic (CHO) and one open chain (CHO), as well as an open chain monomer (CHO), can also be formed; under a particular set of conditions of reagent and acid catalyst concentration, the cyclic trimer is the primary product. A tetrameric form has also been described, under different catalytic conditions. The synthesis of tetrameric aceton peroxide has been disputed. Under neutral conditions, the reaction is reported to produce the monomeric organic peroxide.\n\nThe most common route for nearly pure TATP is HO/acetone/HCl in 1:1:0.25 molar ratios, using 30% hydrogen peroxide. This product contains very little or none of DADP with some very small traces of chlorinated compounds. Product that contains large fraction of DADP can be obtained from 50% HO using high amounts of conc. sulfuric acid as catalyst or alternatively with 30% HO and massive amounts of HCl as a catalyst.\n\nThe product made by using hydrochloric acid is regarded as more stable than the one made using sulfuric acid. It is known that traces of sulfuric acid trapped inside the formed acetone peroxide crystals lead to instability. In fact, the trapped sulfuric acid can induce detonation at temperatures as low as 50 °C, this is the most likely mechanism behind accidental explosions of acetone peroxide that occur during drying on heated surfaces.\nOrganic peroxides in general are sensitive, dangerous explosives, and all forms of acetone peroxide are sensitive to initiation. TATP decomposes explosively; examination of the explosive decomposition of TATP at the very edge of detonation front predicts \"formation of acetone and ozone as the main decomposition products and not the intuitively expected oxidation products.\" Very little heat is created by the explosive decomposition of TATP at the very edge of the detonation front; the foregoing computational analysis suggests that TATP decomposition as an entropic explosion. However, this hypothesis has been challenged as not conforming to actual measurements.<ref name=\"http://www.sciencedirect.com/science/article/pii/S0040603114001439\"></ref> The claim of entropic explosion has been tied to the events just behind the detonation front. The authors of the 2004 Dubnikova et al. study confirm that a final redox reaction (combustion) of ozone, oxygen and reactive species into water, various oxides and hydrocarbons takes place within about 180 ps after the initial reaction - within about a micron of the detonation wave. Detonating crystals of TATP ultimately reach temperature of 2300 K and pressure of 80 kbar. The final energy of detonation is about 2800 kJ/kg (measured in helium) - enough to -\"briefly\"- raise the temperature of gaseous products to 2000 °C. Volume of gases at STP is 855 l/kg for TATP and 713 l/kg for DADP (measured in helium).\n\nDetonation of TATP in inert gas gives:\n\nCHO →1.30 CO + 2.44 CO + 2.61 CH + 0.63 CH + 0.23CH + 0.47 H + 0.96 HO + 0.47C\n\nTATP/DADP, as well as HMTD, belongs to the mercury fulminate group of primary explosives. This means that small, unconfined, samples (less than 2 g) tend to quickly burn (deflagration), while larger and/or properly confined samples detonate. This is sometimes demonstrated in chemistry classes. The experimenter may pour a small amount of acetone peroxide on his/her hand and light it on fire. This is strongly recommended against. The nature of deflagration to detonation transition (DDT) is probabilistic and even extremely small, unconfined samples may immediately detonate! In any case, any direct contact of primary explosive with any part of the body breaks one of the cardinal safety rules for the safe handling of primary explosives! \n\nThe tetrameric form of acetone peroxide, prepared under neutral conditions using a tin catalyst in the presence of a chelator or general inhibitor of radical chemistry, is reported to be more chemically stable, although still a very dangerous primary explosive. It's synthesis has been disputed.\n\nBoth TATP and DADP are prone to loss of mass via sublimation. DADP has lower molecular weight and higher vapor pressure. This means that DADP is more prone to sublimation than TATP.\n\nSeveral methods can be used for trace analysis of TATP, including gas chromatography/mass spectrometry (GC/MS), high performance liquid chromatography/mass spectrometry (HPLC/MS), and HPLC with post-column derivitization.\n\nAcetone peroxide is soluble in toluene, chloroform, acetone, dichloromethane and methanol. Recrystalization of primary explosives may yield large crystals that detonate spontaneously due to internal strain.\n\nKetone peroxides, including acetone peroxide and methyl ethyl ketone peroxide, find application as initiators for polymerization reactions, e.g., silicone or polyester resins, in the making of fiberglass-reinforced composites. For these uses, the peroxides are typically in the form of a dilute solution in an organic solvent; methyl ethyl ketone is more common for this purpose, as it is stable in storage.\n\nAcetone peroxide is used as a flour bleaching agent to bleach and \"mature\" flour.\n\nAcetone peroxides are unwanted by-products of some oxidation reactions such as those used in phenol syntheses. Due to their explosive nature, their presence in chemical processes and chemical samples creates potential hazardous situations. Accidental occurrence at illicit MDMA laboratories is possible. Numerous methods are used to reduce their appearance, including shifting pH to more alkaline, adjusting reaction temperature, or adding inhibitors of their production. For example, triacetone peroxide is the major contaminant found in diisopropyl ether as a result of photochemical oxidation in air.\n\nTATP, when fresh, is about as sensitive as an average primary explosive. DADP, when fresh, is slightly less sensitive than the average primary explosive. Older samples may contain larger crystals due to re-sublimation and larger crystals are generally more sensitive. However the experimental evidence behind this effect is contradictory. It is important to note that the variance of friction force between different surfaces (e.g. different kinds of paper) is often greater than the variance between the friction sensitivity of a given pair of primary explosives. This leads to different values for friction sensitivity measured at different laboratories. Wet acetone peroxide is several times less sensitive to friction than dry acetone peroxide yet its impact sensitivity is almost the same as in the case of dry sample. Mixes with synthetic oil like WD-40 are both less impact and friction sensitive than the dry sample. Water mixed with ethanol is even better. Sensitivity of mixes can not be (naively) predicted. One can expect, for example, that a mixture of TATP and ammonium nitrate is going to be less sensitive than pure TATP. The opposite is true (at certain ratios) since the sensitivity is affected by complex, mutual crystal interplay and hardness.\n\nTATP has been used in bomb and suicide attacks and in improvised explosive devices, including the London bombings on 7 July 2005, where four suicide bombers killed 52 people and injured more than 700. It was one of the explosives used by the \"shoe bomber\" Richard Reid in his 2001 failed shoe bomb attempt and was used by the suicide bombers in the November 2015 Paris attacks, 2016 Brussels bombings, Manchester Arena bombing, June 2017 Brussels attack, Parsons Green bombing and the Surabaya bombings.\n\nTATP shockwave overpressure is 70% of that for TNT, the positive phase impulse is 55% of the TNT equivalent. TATP at 0.4 g/cm has 1/3 of the brisance of TNT (1.2 g/cm) measured by the Hess test. The plate dent test gives about 70% of the TNT eq. at the same densities. The ballistic mortar test gives 62% of TNT. The Trauzl lead block test gives above 250 cm/10g, that is 80% of TNT. This means that TATP is a moderately powerful explosive. A 50 g charge detonated at a 3 m distance will generate a shockwave with an impulse of 9.65 kPa.ms (positive phase) and maximum peak pressure of 18 kPa - 20 kPa is regarder as a threshold for eardrum rapture with about 50 % eardrum rapture rate at 104 kPa. A 1.75 g TATP charge detonated at 4.2 cm will generate 0.396 atm.ms (40.1 kPa.ms) positive shockwave impulse with maximum overpessure of 28 atmospheres (2837 kPa) - that is about the minimum required for immediate amputation of a finger.\n\nOne can easily calculate moderately precise lethality distances for the resulting shockwave using Bass or Bowen equation. 1 kg of TATP should produce 50% mortality at a distance of 1.2 m and near 99% mortality at a distance of about 0.9 m. 10 kg of TATP should produce 50% lethality at 3.2 m and 99% mortality at 2.5 m (distance between upper body and the center of explosion, person facing any direction). The lethality distances for a person that is directly facing the explosion are about 20 % higher. A crowded environment reduces these distances by about 1/3. This leads to about 700 g of TATP per 1 fatality for moderately crowded buses and trains from the shockwave effects alone.\n\nThe acceleration ability of loose TATP powder (0.3 - 0.5 g/cm) is rather poor. The volume energy of 0.4 g/cm TATP is about 1120 J/cm, while the same parameter for cast TNT is about 6800 J/cm. This very roughly translates to typical fragment velocities of 0.5 – 1 km/s for loose 0.4 g/cm TATP charges and 1 – 2 km/s for charges made out of cast TNT. See Gourney equations.\n\nTATP is attractive to terrorists because it is easily prepared from readily available retail ingredients, such as hair bleach and nail polish remover. It is also able to evade detection because it is one of the few high explosives that do not contain nitrogen, and can therefore pass undetected through traditional explosive detection scanners designed to detect nitrogenous explosives. Several detection devices for TATP have however now been developed.\n\nLegislative measures to limit the sale of concentrated hydrogen peroxide (above 12% conc.) have been made in the EU and in Canada.\n\nA key disadvantage is its high susceptibility to accidental detonation (and resulting workplace accidents or \"own goals\" in bomb-making shops), which has led to TATP being referred to as the \"Mother of Satan\". TATP was found in the accidental explosion that preceded the 2017 terrorist attacks in Barcelona and surrounding areas.\n\nLarge scale TATP synthesis is often betrayed by excessive bleach-like or fruity smell. This smell can even penetrate into clothes and hair in amounts that are quite noticeable. Such a person \"smells like chemicals\". This has been reported in the 2016 Brussels bombings.\n\nTATP is a common source of injury among amateur chemists, particularly finger amputations. There is at least on the order of 3 and 10 recorded amputations in Slovakia and the US respectively, with possibly over 100 cases per year in Canada, Europe and the US combined. Most of these injuries are caused by small amounts of TATP that inadvertently detonate (due to small, unexpected areas of high friction or static electricity sparks) in close proximity of fingers, since small amounts (1 - 3 grams) are generally not powerful enough to amputate fingers from distances larger than 5 – 10 cm. Direct contact between fingers and TATP is to be avoided for this reason. \n\n"}
{"id": "25489661", "url": "https://en.wikipedia.org/wiki?curid=25489661", "title": "Amatex", "text": "Amatex\n\nAmatex is a military explosive consisting of 51% ammonium nitrate, 40% TNT, and 9% RDX.\n"}
{"id": "5131367", "url": "https://en.wikipedia.org/wiki?curid=5131367", "title": "Annulus (well)", "text": "Annulus (well)\n\nThe annulus of an oil well or water well is any void between any piping, tubing or casing and the piping, tubing, or casing immediately surrounding it. It is named after the corresponding geometric concept. The presence of an annulus gives the ability to circulate fluid in the well, provided that excess drill cuttings have not accumulated in the annulus, preventing fluid movement and possibly sticking the pipe in the borehole.\n\nFor a new well in the process of being drilled, this would be the void between the drill string and the formation being drilled. An easy way to visualise this would be to stand a straw (purple in diagram) straight up in the center of a glass of water. All of the water in between the straw and the sides of the glass would be the annulus (yellow area in diagram), with the straw itself representing the drill string and the sides of the glass representing the formation. While drilling, drilling fluid is pumped down the inside of the drill string and pushes the drill cuttings up the annulus to the surface, where the cuttings are removed from the drilling fluid (drilling mud) by the shale shakers.\n\nIn a completed well, there may be many annuli. The 'A' annulus is the void between the production tubing and the smallest casing string. The 'A' annulus can serve a number of crucial tasks, including gas lift and well kills. A normal well will also have a 'B' and frequently a 'C' annulus, between the different casing strings. These annuli do not normally have any connection to well bore fluids, but maintaining pressure in them is important in order to ensure integrity of the casing strings.\n\nThough all annuli in a completed well are expected to be isolated from the production tubing and each other, connections allowing the flow of fluids between them may sometimes occur, due to either intervention or wear and tear. There is said to be \"communication\" between these connected annuli.\n\nDuring coiled tubing interventions, the void between the coil and the production tubing can also be considered an annulus and be used for circulation.\n\n"}
{"id": "5137159", "url": "https://en.wikipedia.org/wiki?curid=5137159", "title": "Atm⁵", "text": "Atm⁵\n\natm⁵ is an interbank network in Singapore, connecting the ATMs of six of Singapore's eight qualifying full banks, QFB. , there are 230+ atm⁵ ATMs island-wide. The network was established in April 2005.\n\natm⁵ is also one of the few interbank networks that does not charge its customers for transactions via another member bank's ATM, having removed all interbank transaction charges on April 4, 2006.\n\nWhile Citibank has many ATMs, only a percentage of them are part of the atm⁵ group. Hence, banks usually recommend customers to make sure that the atm⁵ logo is present on the members' ATM before they carry out their transaction(s).\n\natm⁵ is the primary network of the following banks listed below:\n\n\n"}
{"id": "27462979", "url": "https://en.wikipedia.org/wiki?curid=27462979", "title": "Azinphos-ethyl", "text": "Azinphos-ethyl\n\nAzinphos-ethyl (also spelled azinophos-ethyl) was a broad-spectrum organophosphate insecticide.\n\nIt is very toxic to mammals with a World Health Organization hazard classification as class IB, highly hazardous. It is classified as an extremely hazardous substance in the United States as defined in Section 302 of the U.S. Emergency Planning and Community Right-to-Know Act (42 U.S.C. 11002), and is subject to strict reporting requirements by facilities which produce, store, or use it in significant quantities.\n\n"}
{"id": "1259351", "url": "https://en.wikipedia.org/wiki?curid=1259351", "title": "Bed sheet", "text": "Bed sheet\n\nA sheet is a rectangular piece of cloth used as bedding, being placed immediately below or above bed occupants. Bed sheets can generally be divided into two categories: \"top\" and \"bottom\" sheets. \n\nA bottom sheet is laid above the mattress and bed occupants lie on it. It may be either a flat sheet (extra fabric is usually tucked under the mattress), or a fitted sheet, which is sewn in a pocket-like shape to go down over the corners of the mattress, and has an elastic band around the edges of the sheet to prevent the sheet from slipping. However, this can make it more difficult to fold when not in use. \n\nBottom sheets are standard in developed countries. They are more easily washable than a mattress, and when used properly can protect the longevity of the mattress and provide better sanitation for bed occupants. The biggest problem with bed sheets is that they are often not effective. Take this common example, for example. When a person lays a bed sheet down on their bed, the surrounding environment will either cool down or heat up the single bed sheet. To combat this, a bed sheet patron must apply an additional bed sheet to the bed before use.\n\nA top sheet is a flat sheet which bed occupants lie underneath. Blankets, quilts, duvets/comforters and other bed covers are laid over the top sheet, but because of the top sheet they do not directly touch the bed occupant.\n\nTop sheets are standard in the U.S., but much of Europe prefers to use duvet covers. A duvet cover consists of two rectangles sewn together on all but one side to create a sleeve for the duvet, which can be taken off and washed. When no other blankets are lain atop the covered duvet, it can provide several advantages over a top sheet. First, respecting sanitation, with a top sheet the bed occupant could accidentally in the night remove the barrier between themself and the duvet (which is less washable); with a duvet cover this is impossible. Second, for households with children, a bed with a covered duvet is easier to make: the child need only tug the corners of the duvet back towards the corners of the bed. Tucking in a top sheet before arranging the duvet is an extra step. Third, switching a duvet cover is a cheaper way to change color schemes than buying a new comforter.\n\nHowever, a top sheet has its own advantages. First, a tucked-in top sheet can provide a comfortable feeling of snugness that duvets cannot, as duvets are not usually large enough to tuck under the mattress. Second, top sheets allow a wider range of blanket choices besides duvets, such as quilts. Top sheets are ultimately useless, as the bed sheet applicator will find no comfort of a single bed sheet unless they are in the bed, snuggling. Sheets are also much easier to replace after being washed, because duvets can be difficult to insert into a duvet cover. Using a duvet instead of a top sheet also prevents a bed user from removing blankets during the night without being completely without cover, whereas a top sheet provides basic coverage without insulating too much heat.\n\nIn some Asian countries, such as China, top sheets are not used. Flat sheets are often used in place of fitted sheets or together with them as bottom sheets. Most families prefer to use duvet covers to cover the quilts.\n\nThe term \"bed sheet\" was first used in the 15th century. Bed sheets were traditionally white and made of linen, cotton or silk, but now various colors and patterns are used.\n\nBed sheets come in two main varieties: flat and fitted. A flat sheet is simply a rectangular sheet of cloth, while a fitted sheet has its four corners, and sometimes two or four sides, fitted with elastic, to be used only as a bottom sheet. The fitted sheet may also be secured using a drawstring instead of elastic. The purpose of a fitted bottom sheet is to keep it from slipping off the mattress while the bed is in use. A particular way of folding and tucking while making the bed, known as \"hospital corners,\" is sometimes used when the bottom sheet is flat rather than fitted.\n\nUsually a flat bed sheet is overlocked around the edges to form four seams. One of the seams is wider than the other three and helps with orienting the sheet correctly on the mattress. The wider seam goes at the head end of the mattress. Sometimes the sides do not have seams, but are finished with the selvedge only. When placing a flat sheet on a bed, the manufacturer has designed the printed side to be softer, and thus it should be placed on the bed printed side \"down\". When folding back the covers, this also allows the printed side to show, for aesthetic purposes. When one makes a bed, the patterned or monogrammed side of the top sheet is placed facing down and then the top edge is folded towards the foot of the bed, exposing the design.\n\nIn the US and Canada, sheets are often sold in a four-piece set consisting of a fitted sheet, a flat sheet and two pillowcases. In China, a four-piece set consists of a duvet cover, two pillowcases and either a fitted or flat sheet. Fitted Sheets are gaining popularity due to ease of use. Use of good quality elastic make fitted sheets durable.\n\nCotton and cotton blends dominate the market. The most common blend being cotton and polyester. Cotton provides absorbency and a soft hand, while polyester adds durability and wrinkle resistance. Other common fibers used in the manufacturing of bed sheets include linen, silk, Modal and bamboo rayon, lyocell, and polypropylene. Polypropylene (olefin) is a hypoallergenic spun-bound material produced at a low cost and typically used in emergency shelters or hospitals as disposable sheeting.\n\nThe quality of bed sheets is often conveyed by the thread count—the number of threads per square inch of material. In general, the higher the thread count, the softer the sheet, but the weave and type of thread may affect the \"hand\" of the material so that a sheet with a lower thread count may actually be softer than one with a higher count. Yarn quality also plays a part in the look and feel of sheets, as finer yarns tend to create a finer sheet fabric. The ply also plays a role in how heavy the sheet feels. Ply represents how many fibers are twisted together as the sheet is being created. A 2 ply 300 thread count sheet will feel heavier than a single ply 600 thread count sheet. \nThe most common constructions are muslin, percale, sateen, flannel, and knitted jersey. In a plain weave the warp and weft cross each other one at a time, and sateen, has multiple threads (usually three or four) over, and one under.\n\n"}
{"id": "44624117", "url": "https://en.wikipedia.org/wiki?curid=44624117", "title": "CBERS-4", "text": "CBERS-4\n\nChina–Brazil Earth Resources Satellite 4 (CBERS-4), also known as Ziyuan I-04 or Ziyuan 1E, is a remote sensing satellite intended for operation as part of the China–Brazil Earth Resources Satellite programme between the China Centre for Resources Satellite Data and Application and Brazil's National Institute for Space Research. The fifth CBERS satellite to fly, it was successfully launched on 7 December 2014. It replaces CBERS-3 which was lost in a launch failure in December 2013.\n\nCBERS-4 is a spacecraft based on the Phoenix-Eye 1 satellite bus. It was developed by the China Academy of Space Technology, in partnership with Brazil, at a cost of US$125 million for each party. The spacecraft have a single solar array which provides power to its systems, generating 2,300 watts of electrical power, and have a design life of three years.\n\nThe CBERS-4 spacecraft carries four instruments: MUXCam, a multispectral camera; PanMUX, a panchromatic imager; the Infrared Medium Resolution Scanner, or IRSCAM, and WFICAM, a wide-field imaging camera. These cameras will be used to observe a swath of of landmass at a time, enabling the satellite to scan the entire surface of the planet every 26 days, with a spatial resolution of up to .\n\nCBERS-4 was initially scheduled to be launched in 2015, however after the loss of CBERS-3 at launch in December 2013, China and Brazil agreed to accelerate the production of CBERS-4 by 1 year. The satellite will restore the Brazilian government's ability to observe its own territory following a 4.5-year gap caused by the failure of CBERS-2B and CBERS-3.\n\nA Chang Zheng 4B carrier rocket was used to launch CBERS-4. The launch took place at 03:26 UTC on 7 December 2014, using Launch Complex 9 at the Taiyuan Satellite Launch Centre. The satellite was successfully placed into a sun-synchronous orbit.\n\n"}
{"id": "51179089", "url": "https://en.wikipedia.org/wiki?curid=51179089", "title": "Career Insights", "text": "Career Insights\n\nCareer Insights is an international digital technology company with a focus on offering e-work experience in Digital Project management and Digital Business Analysis. It was founded in 2014 by Keji Giwa. \n\nCareer Insights is currently located at 133, Creek Road, Greenwich, London; 53 Fountain street, Manchester; Mulliner towers, Alfred Rewane road, ikoyi, Lagos, 10a Tuluzy St., Kiev, Ukraine.\n\nKeji started Career Insights, a sister company of Digital Bananas Technology, in 2014 to create a platform where people can gain practical work experience in digital project management or digital business analysis, after being trained by the company’s team of digital project management and business analysis experts. \n\nCareer Insight’s eWorkexperience program uses eLearning, along with cloud based collaborative tools available on the web and mobile to help candidates on the platform work on live projects remotely, gaining practical work experience from the convenience of their homes as a project management or business analysis professional. \n\nCandidates build their expertise in the following roles: digital project manager, Digital business analyst and has recorded over 3000 success stories from candidates since inception in 2014. With referrals from Career Insights, candidates have been able to secure jobs in multinational companies like KPMG, Accenture, HSBC, Oracle, RBS, Capgemini and Oracle.\n"}
{"id": "1251428", "url": "https://en.wikipedia.org/wiki?curid=1251428", "title": "Cash sorter machine", "text": "Cash sorter machine\n\nA cash sorter machine is a machine used for sorting banknotes. Cash Sorters can file through a number of banknotes, and sort it into denominations. Many Cash Sorters also function as a banknote counter, and can count money as well as splitting them into groups. Some cash sorters can also detect counterfeit money using security features of the banknote. Primary users are banks, financial institutions, casinos, and large theme parks, as they all come across large amounts of cash.\n\n"}
{"id": "101684", "url": "https://en.wikipedia.org/wiki?curid=101684", "title": "Celluloid", "text": "Celluloid\n\nCelluloids are a class of compounds created from nitrocellulose and camphor, with added dyes and other agents. Generally considered the first thermoplastic, it was first created as Parkesine in 1856 and as Xylonite in 1869, before being registered as \"Celluloid\" in 1870. Celluloid is easily molded and shaped, and it was first widely used as an ivory replacement.\n\nThe main use was in movie and photography film industries, which used only celluloid film stock prior to the adoption of acetate safety film in the 1950s. Celluloid is highly flammable, difficult and expensive to produce and no longer widely used; its most common uses today are in table tennis balls, musical instruments, and guitar picks.\n\nNitrocellulose-based plastics slightly predate celluloid. Collodion, invented in 1848 and used as a wound dressing and an emulsion for photographic plates, is dried to a celluloid-like film.\n\nThe first celluloid as a bulk material for forming objects was made in 1855 in Birmingham, England, by Alexander Parkes, who was never able to see his invention reach full fruition, after his firm went bankrupt due to scale-up costs. Parkes patented his discovery as Parkesine in 1862 after realising a solid residue remained after evaporation of the solvent from photographic collodion.\n\nParkes patented it as a clothing waterproofer for woven fabrics in the same year. Later Parkes showcased Parkesine at the 1862 International Exhibition in London, where he was awarded a bronze medal for his efforts. The introduction of Parkesine is generally regarded as the birth of the plastics industry. Parkesine was made from cellulose treated with nitric acid and a solvent. It is often called synthetic ivory. The Parkesine company ceased trading in 1868. Pictures of Parkesine are held by the Plastics Historical Society of London. There is a plaque on the wall of the site of the Parkesine Works in Hackney, London.\n\nIn the 1860s, an American, John Wesley Hyatt, acquired Parkes's patent and began experimenting with cellulose nitrate with the intention of manufacturing billiard balls, which until that time were made from ivory. He used cloth, ivory dust, and shellac, and on April 6, 1869, patented a method of covering billiard balls with the addition of collodion. With assistance from Peter Kinnear and other investors, Hyatt formed the Albany Billiard Ball Company (1868–1986) in Albany, New York, to manufacture the product. In 1870, John and his brother Isaiah patented a process of making a \"horn-like material\" with the inclusion of cellulose nitrate and camphor. Alexander Parkes and Daniel Spill \"(see below)\" listed camphor during their earlier experiments, calling the resultant mix \"xylonite\", but it was the Hyatt brothers who recognized the value of camphor and its use as a plasticizer for cellulose nitrate. Isaiah Hyatt dubbed his material \"celluloid\" in 1872.\n\nEnglish inventor Daniel Spill had worked with Parkes and formed the Xylonite Co. to take over Parkes' patents, describing the new plastic products as \"Xylonite\". He took exception to the Hyatts' claims and pursued the brothers in a number of court cases between 1877 and 1884. Initially the judge found in Spill's favour, but ultimately it was judged that neither party held an exclusive claim and the true inventor of celluloid/xylonite was Alexander Parkes, due to his mention of camphor in his earlier experiments and patents. The judge ruled all manufacturing of celluloid could continue both in Spill's British Xylonite Company and Hyatts' Celluloid Manufacturing Company.\nThe name \"Celluloid\" actually began as a trademark of the Celluloid Manufacturing Company, first of Albany, NY, and later of Newark, New Jersey, which manufactured the celluloids patented by John Wesley Hyatt. Hyatt used heat and pressure to simplify the manufacture of these compounds. Over the years, celluloid became the common use term used for this type of plastic. In 1878 Hyatt was able to patent a process for injection moulding thermoplastics, although it took another fifty years before it could be realised commercially, and in later years celluloid was used as the base for photographic film.\n\nEnglish photographer John Carbutt founded the Keystone Dry Plate Works in 1879 with the intention of producing gelatin dry plates. The Celluloid Manufacturing Company was contracted for this work, which was done by thinly slicing layers out of celluloid blocks and then removing the slice marks with heated pressure plates. After this, the celluloid strips were coated with a photosensitive gelatin emulsion. It is not certain exactly how long it took for Carbutt to standardize his process, but it occurred no later than 1888. A sheet of Carbutt's film was used by William Dickson for the early Edison motion picture experiments on a cylinder drum Kinetograph. However, the celluloid film base produced by this means was still considered too stiff for the needs of motion-picture photography.\n\nBy 1889, more flexible celluloids for photographic film were developed, and both Hannibal Goodwin and the Eastman Kodak Company obtained patents for a film product. (Ansco, which purchased Goodwin's patent after he died, was eventually successful in a patent-infringement suit against Kodak). This ability to produce photographic images on a flexible material (as opposed to a glass or metal plate) was a crucial step toward making possible the advent of motion pictures.\n\nMost movie and photography films prior to the widespread move to acetate films in the 1950s were made of celluloid. Its high flammability was legendary since it self-explodes when exposed to temperatures over 150° C in front of a hot movie-projector beam. While celluloid film was standard for 35mm theatrical productions until around 1950, motion-picture film for amateur use, such as 16mm and 8mm film, were on acetate \"safety base\", at least in the US.\n\nCelluloid was useful for creating cheaper jewellery, jewellery boxes, hair accessories and many items that would earlier have been manufactured from ivory, horn or other expensive animal products. It was often referred to as \"Ivorine\" or \"French Ivory\". For this use, a form of celluloid was developed in France that had lines in it to make it resemble ivory. It was also used for dressing table sets, dolls, picture frames, charms, hat pins, buttons, buckles, stringed instrument parts, accordions, fountain pens, cutlery handles and kitchen items. The main disadvantage the material had was that it was flammable. Items made in celluloid are collectible today and increasingly rare in good condition. It was soon overtaken by Bakelite and Catalin. Table tennis balls are still made from celluloid. \"Parker Brothers...made some versions out of hollow Celluloid--which, because of its 'frictionless' properties, spun even faster than steel.\"\n\nShelf clocks and other furniture items were often covered with celluloid in a manner similar to veneer. This celluloid was printed to look like expensive woods, or materials like marble or granite. The Seth Thomas clock company called its celluloid clock material \"adamantine\". Celluloid enabled clockmakers to make the typical late Victorian style of black mantel clock in such a way that the wooden case appeared to be black marble, and the various pillars and other decorative elements of the case looked like semi-precious stone.\n\nCelluloid was also a popular material in the construction of slide rules. It was primarily used to coat wooden slide rule faces, such as in early A.W. Faber rules, as well as cursor end pieces, such as in Keuffel and Esser rules.\n\nCelluloid remains in use for musical instruments, especially accordions and guitars. Celluloid is very robust and easy to mold in difficult forms, and has great acoustic performance as cover for wooden frames since it does not block wood's natural pores. Instruments covered with celluloid can easily be recognized by the material's typical nacre-like flaming pattern. Thick celluloid panels are cooked in a bain-marie which turns them into a leather-like substance. Panels are then turned on a mold and allowed to harden for as long as three months.\n\nA typical formulation of celluloid might contain 70 to 80 parts nitrocellulose, nitrated to 11% nitrogen, 30 parts camphor, 0 to 14 parts dye, 1 to 5 parts ethyl alcohol, plus stabilizers and other agents to increase stability and reduce flammability.\n\nCelluloid is made from a mixture of chemicals such as nitrocellulose, camphor, alcohol, as well as colorants and fillers depending on the desired product. The first step is transforming raw cellulose into nitrocellulose by conducting a nitration reaction. This is achieved by exposing the cellulose fibers to an aqueous solution of nitric acid; the hydroxyl groups (-OH) will then be replaced with nitrate groups (-NO3) on the cellulose chain. The reaction can produce mixed products, depending on the degree of substitution of nitrogen, or the percent nitrogen content on each cellulose molecule; cellulose nitrate has 2.8 molecule of nitrogen per molecule of cellulose. It was determined that sulfuric acid was to be used as well in the reaction in order to first, catalyze the nitric acid groups so it can allow for the substitution onto the cellulose, and second, allow for the groups to easily and uniformly attach to the fibers, creating a better quality nitrocellulose. The product then must be rinsed to wash away any free acids that did not react with the fibers, dried, and kneaded. During this time, a solution of 50% camphor in alcohol is added, which then changes the macromolecule structure of nitrocellulose into a homogeneous gel of nitrocellulose and camphor. The chemical structure is not well understood, but it is determined that it is one molecule of camphor for each unit of glucose. After the mixing, the mass is pressed into blocks at a high pressure and then is fabricated for its specific use.\n\nNitrating cellulose is an extremely flammable process in which even factory explosions are not uncommon. Many western celluloid factories closed after hazardous explosions, and only two factories in China remain in business.\n\nMany sources of deterioration in celluloid exist, such as thermal, chemical, photochemical, and physical. The most inherent flaw is as celluloid ages, the camphor molecules are ‘squeezed’ out of the mass due to the unsustainable pressure used in the production. That pressure causes the nitrocellulose molecules to bind back to each other or crystallize, and this results in the camphor molecules being shoved out of the material. Once exposed to the environment, camphor can undergo sublimation at room temperature, leaving the plastic as brittle nitrocellulose. Also, with exposure to excess heat, the nitrate groups can break off and expose nitrogen gases, such as nitrous oxide and nitric oxide, to the air.\n\nAnother factor that can cause this is excess moisture, which can accelerate deterioration of nitrocellulose with the presence of nitrate groups, either newly fragmented from heat or still trapped as a free acid from production. Both of these sources allow the accumulation of nitric acid. Another form of deterioration, photochemical deterioration, is severe in celluloid because it absorbs ultraviolet light well. The absorbed light leads to chain-breakage and stiffening.\n\nAccordions even over 100 years old covered with thick celluloid might be in pristine condition, while on the other hand thin celluloid film stocks are often melted in an unrecognisable solid block after decades in storage.\n\n\n"}
{"id": "58097311", "url": "https://en.wikipedia.org/wiki?curid=58097311", "title": "Chirp (company)", "text": "Chirp (company)\n\nChirp (also known as Asio Limited) is a technology company based in the UK. Chirp was originally a research project from UCL and was incorporated as a UK limited company in 2012. Chirp specialise in data-over-sound SDKs, which converts data into audio signals, which can be transmitted to other devices within hearing range\n\nChirp has been involved in projects with EDF Energy , Activision Blizzard, for the game and Creata, in toys for the Netflix Original, Beat Bugs \n"}
{"id": "16780073", "url": "https://en.wikipedia.org/wiki?curid=16780073", "title": "Coffee cup sleeve", "text": "Coffee cup sleeve\n\nCoffee cup sleeves, also known as coffee sleeves, coffee clutches, coffee cozies, hot cup jackets, paper zarfs, coffee collars, coffee sleeve, and cup holders, are roughly cylindrical sleeves that fit tightly over handle-less paper coffee cups to insulate the drinker's hands from hot coffee. Coffee sleeves are typically made of textured paperboard, but can be found made of other materials. Coffee sleeves allow coffee houses, fast food restaurants, and other vendors to avoid double-cupping, the practice of using two (or more) nested paper cups for a single hot beverage. Some paper cup holders carry advertisements.\n\nThe coffee sleeve was invented in 1991 by Jay Sorensen and patented in 1995 (under the trademarked name Java Jacket), and are now commonly utilized by coffee houses and other vendors that sell hot beverages dispensed in disposable paper cups. There are a number of patents that cover various coffee sleeves and their aspects. Other people have claimed to invent the coffee sleeve. \n\nThere are a number of companies that manufacture coffee sleeves; the top five companies by volume of coffee sleeves sold in the U.S. are, in no particular order, International Paper, LBP Manufacturing, Java Jacket and Labansat & Schulz Manufacturing.\n\nThe sleeve was invented while Sorensen was driving his daughter Ryeder to school and spilt a cup of coffee in his lap. The java jacket first appeared at the Seattle Coffeefest in 1995 and Sorensen and his wife Colleen were overwhelmed with requests and orders to fill.\n\nCoffee cup sleeves can be customized by having logos or brands printed directly on the sleeves. Many coffee shops have custom printed coffee sleeves. \n\nCoffee sleeves should not be confused with fixed cup holders.\n\n"}
{"id": "36619300", "url": "https://en.wikipedia.org/wiki?curid=36619300", "title": "Commuter Challenge", "text": "Commuter Challenge\n\nCommuter Challenge is a national, week long event in Canada and is held annually during the Canadian Environment Week. Formatted as a friendly competition between workplaces and Canadian municipalities, the national and host city coordinators announce winners based on the highest percent participation. The event has a strong workplace focus where employers promote the event in-house to support their employees in leaving their cars at home in favour of more sustainable commuter modes including walking, jogging, cycling, in-line skating, public transit, carpooling and teleworking.\n\nThe first Commuter Challenge was hosted in 1991 and the Canadian Commuter Challenge tracking tool was launched in 1999 and is currently coordinated nationally by the Calgary-based environmental group, the Sustainable Alberta Association. Participants record their commutes and are ranked via Sustainable Alberta Association's web-based tracking tools. In 2014, Commuter Challenge drew participation from over 26,675 individuals and 1,803 workplaces.\n\nSmall sustainable transportation events started emerging in Canada in the early 1990s in different cities across the country; all under different names and in different formats.\n\nIn 1991 in Calgary, Andrea Main, then curator of the ERCB's Energeum (Energy Resources Conservation Board, now Alberta Energy Utilities Board), organized an interdepartmental competition to promote alternative transportation options for that year's National Environment Week. In 1992, the ERCB challenged three other energy companies in Calgary to a friendly competition to see which workplace had the highest percent of sustainable commuters. By 1995, the Calgary Challenge had mushroomed to include 25 workplaces, primarily oil and gas companies. Andrea Main drafted the original proposal for the challenge and coined the name \"Commuter Challenge\". In its inaugural year, the ERCB's Gas Department won the competition—the prize for which was the adoption of one acre of rainforest. Individual distance winners won T-shirts donated by Mountain Equipment Coop.\n\nAt the same time in 1991, cyclists from Ottawa and Hull, Quebec, organized the first commuter challenge in the National Capital Region. The annual event was founded by Mike Buckthought, a programmer-analyst and avid cyclist. During National Environment Week, participants from Environment Canada and Forestry Canada reduced emissions by an estimated 1 tonne of carbon dioxide. Reductions in emissions were estimated using Environment Canada's Mobile 4C model. In 1992, the challenge expanded to include four organizations in Ottawa-Hull (Environment Canada, Forestry Canada, Bell Canada, and Bell-Northern Research). Participants reduced greenhouse gas emissions by an estimated 8 tonnes of carbon dioxide.\n\nIn 1995, the event expanded to include the first inter-city challenge, with Ottawa-Hull competing against London, Ontario. That year, the event included participants from Ottawa-Hull and London, as well as other cities in Ontario, British Columbia, Alberta and the United States. Reductions in emissions were estimated using Environment Canada's Mobile 5C model. On May 18, 1995, participants reduced emissions by an estimated 5 tonnes of carbon dioxide.\n\nIn 1997, Calgary challenged Vancouver, Ottawa and London to an informal intercity challenge which attracted the attention of the Canadian Broadcasting Association (CBC) demonstrated the need for a national challenge.\n\nIn 1998, over 14,500 people in Ottawa-Hull, London, Calgary, Vancouver and Victoria used environment-friendly transportation to get to work during Environment Week.\n\nIn 1999, Sustainable Alberta Association received Federal Government support through the Climate Change Action Fund to launch a national event and website over three years. The project, led by Kathryn Winkler, made it possible for regions, workplaces and individuals anywhere in Canada to participate and see the impact that they have as a result of their commuter choices. The impacts that are available by region, by workplace/team and for individuals include GHG reductions, calories burned, liters of fuel saved as well as regional and workplace participation levels.\n\nBy 2000, twenty Canadian cities had signed up for the event with host organizations ranging from not-for-profit environmental groups, municipal offices and health authoriteis.\n\nIn 2001 the national program coordination was handed off to an Ottawa-based non-governmental organization, Auto Free Ottawa.\n\nIn 2002 it was passed on to Go for Green, another Ottawa-based non-governmental organization. Go for Green coordinated the Commuter Challenge from 2002 to 2008.\n\nIn 2003, a total of 48,764 people joined the Commuter Challenge. The event was launched by Environment Minister David Anderson on May 30, 2003. The Commuter Challenge was now a national event, with participation from all provinces and territories. A number of cities showed strong support for green transportation, with high numbers of participants: Ottawa-Gatineau (10,807), Winnipeg (10,058), Vancouver (9,219), Central Okanagan, BC (6,560), Waterloo Region (4,146), and Calgary (1,659).\n\nIn 2006 Justin Trudeau was the National Spokes person for the event and endorse the program.\n\nIn 2009 Sustainable Alberta Association took on the national coordination once again. Today the Commuter Challenge is delivered by a small team of dedicated volunteers and occasional contract help out of the SAA office in Calgary, Alberta, Canada. The program is primarily funded by fees paid by corporate participants in Calgary and corporate sponsorships. Other financial support includes contributions from regional coordinators, the City of Calgary, foundations and local funding sources.\n\nCommuter Challenge has had many partners over the years. Significant long-term partners include: Better Environmentally Sound Transportation (BEST), a non-profit organization that coordinates the Commuter Challenge in British Columbia; the Green Action Centre, an organization that coordinates the event in Manitoba; and Clean Nova Scotia, a foundation based in Nova Scotia.\n\nGovernment partners have included the Province of Manitoba, the Province of British Columbia, the City of Winnipeg, the City of Regina and the Region of Waterloo. Major support is received annually though collaboration with the City of Calgary, notably its transportation department.\n\nSustainable Alberta Association and the Calgary Commuter Challenge received funding from the Climate Change Action Fund (CCAF). The funding supported development of an online data collection system.\n\nIn 2011 Sustainable Alberta Association participated in the Shell \"Fuelling Change\" grant and won $100,000 to help facilitate the development of the new commuter tracking software. In 2012 Commuter Challenge partnered with Climate Change Central (C-3), another Alberta-based environmental advocacy group, to give away air miles to randomly chosen participants. ConocoPhillips has been the longest standing private sector supporter of the Calgary Commuter Challenge (since 2007) that contributes directly to the national program.\n"}
{"id": "17507715", "url": "https://en.wikipedia.org/wiki?curid=17507715", "title": "Delay-line oscillator", "text": "Delay-line oscillator\n\nA delay-line oscillator is a form of electronic oscillator that uses a delay line as its principal timing element.\n\nThe circuit is set to oscillate by inverting the output of the delay line and feeding that signal back to the input of the delay line with appropriate amplification. The simplest style of delay-line oscillator, when properly designed, will oscillate with period exactly two times the delay period of the delay line. Additional outputs that are correlated in frequency with the main output but vary in phase can be derived by using additional taps from within the delay line.\n\nThe delay line may be realized with a physical delay line (such as an LC network or a transmission line). In contrast to a Phase-shift oscillator in which LC components are lumped, the capacitances and inductances are distributed through the length of the delay line. A ring oscillator uses a delay line formed from the gate delay of a cascade of logic gates. The timing of a circuit using a physical delay line is usually much more accurate. It is also easier to get such a circuit to oscillate in the desired mode.\n\nThe delay-line oscillator may be allowed to \"free run\" or it may be gated for use in asynchronous logic.\n\nSince the optical cavity is a delay line, a laser can be regarded as a special case of the delay-line oscillator.\n\n"}
{"id": "16094284", "url": "https://en.wikipedia.org/wiki?curid=16094284", "title": "Digital history", "text": "Digital history\n\nDigital history is the use of digital media to further historical analysis, presentation, and research. It is a branch of the Digital humanities and an extension of quantitative history, cliometrics, and computing. Digital history is commonly digital public history, concerned primarily with engaging online audiences with historical content, or, digital research methods, that further academic research. Digital history outputs include: digital archives, online presentations, data visualizations, interactive maps, time-lines, audio files, and virtual worlds to make history more accessible to the user. A researcher can interact with, explore and visualise, the output more easily than with conventional historiographical material. Recent digital history projects focus on creativity, collaboration, and technical innovation, text mining, corpus linguistics, network analysis, 3D Modeling, and big data analysis. Utilising these resources the user can rapidly develop new analyses that can link to, extend, and bring to life existing histories.\n\nEarly digital history in the 1960s and 70s focused on quantitative analyses, primarily of demographic data - censuses, election returns, city directories, etc. These early computers could be programmed to conduct statistical analyses of these records, creating tallies, or seeking trends across records. This research into historical demography was rooted in the rise of social history as a field of historical interest. The historians involved in this work sought to quantify past societies, to come to new conclusions about communities and population. Computers proved capable tools for that type of work. By the late 1970s younger historians turned to cultural studies, but the outpouring of quantitive studies by established scholars continued. Since then, Quantitative history and Cliometrics have been used primarily by historically-minded economists and political scientists. In the late 1980s quantifiers founded The Association for History and Computing. This movement provided some of the impetus for the rise of digital history in the 1990s.\n\nThe more recent roots of digital history were in software rather than online networks. In 1982, the Library of Congress embarked on its Optical Disk Pilot Project, which placed text and images from its collection on to laserdiscs and CD-ROMs. The library started offering online exhibits in 1992 when it launched Selected Civil War Photographs. In 1993, Roy Rosenzweig, along with Steve Brier and Josh Brown, produced their award-winning CD-ROM \"Who Built America? From the Centennial Exposition of 1876 to the Great War of 1914\", designed for Apple, Inc. that integrated images, text, film and sound clips, displayed in a visual interface that supported a text narrative.\n\nAmong the earliest online digital history projects were The Heritage Project of the University of Kansas and medieval historian Dr. Lynn Nelson's World History Index and History Central Catalogue. Another was The Valley of the Shadow, conceived in 1991 by current University of Richmond President Edward L. Ayers, who was then at the University of Virginia. The Institute for Advanced Technology in the Humanities (IATH) at the University of Virginia adopted the Valley Project and partnered with IBM to collect and transcribe historical sources into digital files. The project collected data related to Augusta County in Virginia and Franklin County in Pennsylvania during the American Civil War. In 1996, William G. Thomas III joined Ayers on the Valley Project. Together, they produced an online article entitled \"The Differences Slavery Made: A Close Analysis of Two American Communities,\" which also appeared in the \"American Historical Review\" in 2003 . A CD-ROM also accompanied the Valley Project, published by W. W. Norton and Company in 2000.\n\nRosenzweig, who died October 11, 2007, founded the Center for History and New Media (CHNM) at George Mason University in 1994. Today, CHNM boasts several digital tools available to historians, such as Zotero, Omeka or Tropy. In 1997, Ayers and Thomas used the term \"digital history\" when they proposed and founded the Virginia Center for Digital History (VCDH) at the University of Virginia, the earliest center devoted exclusively to history. Several other institutions promoting digital history include the Center for Humane Arts, Letters, and Social Sciences Online (MATRIX) at Michigan State University, Maryland's Institute for Technology in the Humanities, and the Center for Digital Research in the Humanities at the University of Nebraska. In 2004, Emory University launched Southern Spaces, a \"peer-reviewed Internet journal and scholarly forum\" examining the history of the South.\n\nThere are many potential benefits to the use of digital history when combined with traditional historical methods. Some of these applications include:\nBy adding new research methods to existing historical method, historians can benefit greatly from the ability to work with larger amounts of data and develop new interpretations from this.\n\nThe collaborative nature of most digital history endeavors has meant that the discipline has developed primarily at institutions with the resources to sponsor content research and technical innovation. Two of the first centers, George Mason University's Center for History and New Media and the Virginia Center for Digital History at the University of Virginia have been among the leaders in the development of digital history projects and the education of digital historians.\n\nSome of the noteworthy projects emerging from these pioneering centers are The Geography of Slavery, The Texas Slavery Project, and The Countryside Transformed at VCDH and Liberty, Equality, Fraternity: Exploring the French Revolution and The Lost Museum at the CHNM. In each of these projects, mediated archives holding multiple types of sources are combined with digital tools to analyze and illuminate an historical question to a varying degree; this integration of content and tools with analysis is one of the hallmarks of digital history – projects move beyond archives or collections and into scholarly analysis and the use of digital tools to develop that analysis. The differences between the ways projects incorporate these integrations are a measure of the development of the field and point to the ongoing debates over what digital history can and should be.\n\nWhile many of the projects at VCDH, CHNM, and other university's centers have been geared towards academics and post-secondary education, the University of Victoria (British Columbia), in conjunction with the Université de Sherbrooke and the Ontario Institute for Studies in Education at the University of Toronto, has created as series of projects for all ages, \"Great Unsolved Mysteries in Canadian History.\" Laden with instructional aids, this site asks teachers to introduce students to historical research methods to help them develop analytical skills and a sense of the complexities of their national history. Issues of race, religion, and gender are addressed in carefully constructed modules that cover incidents in Canadian history from Viking exploration through the 1920s. One of the original co-creators of the project, John Lutz has also developed Victoria's Victoria with the University of Victoria and Malaspina University-College.\n\nIn addition to Ayers, Thomas, Lutz, and Rosenzweig, numerous other individual scholars work with digital history techniques and have made and/or continue to make important contributions to the field. Robert Darnton's 2000 article, \"An Early Information Society: News and the Media in Eighteenth-Century Paris\" was supplemented with electronic resources and is an early model of the discussions around digital history and its future in the humanities. One of the first major digital projects to be reviewed by the American Historical Review (AHR) was Philip Ethington's \"Los Angeles and the Problem of Urban Historical Knowledge\" —a multimedia exploration of changes to Los Angeles' physical profile over the course of several decades. Patrick Manning, Andrew W. Mellon Professor of World History at the University of Pittsburgh, developed the CD-ROM project \"Migration in Modern World History, 1500-2000.\" In the \"African Slave Demography Project,\" Manning created a demographic simulation of the slave trade to show precisely how declined in West and Central Africa between 1730 and 1850 as well as in East Africa between the years 1820 and 1890 due to slavery. Jan Reiff, of UCLA, co-edited the print and online versions of the Encyclopedia of Chicago. Andrew J. Torget, founded the Texas Slavery Project while at VCDH and continues to develop the site as he completes his PhD—likely a model for new digital scholars who will incorporate digital components into larger research agendas.\n\nAnother notable project that makes use of digital tools for historical practice is The Quilt Index. As scholars became increasingly interested in women's history, quilts became valuable to study. The Quilt Index is an online collaborative database where quilt owners can upload pictures and data about their quilts. This project was created due to the difficulty of collecting quilts. Firstly, they were in the possession of various institutions, archives, and even civilians. And secondly, they can be too fragile or bulky for physical transport.\n\nAlso in the field of women's history is \"Click! The Ongoing Feminist Revolution\". which highlights the collective action and individual achievements of women from the 1940s to the present. In the UK, a pilot project began in 2002 to create a digital library of British History. This has developed into an extensive collection of over 1,200 volumes, bringing together primary and secondary sources from libraries, archives, museums and academics. Another significant project is the Old Bailey Online,a digital collection of all proceedings between 1674 and 1913. In addition to the digitized records, the Old Bailey Online website provides historical and legal background information, research guides,and educational resources for students.\n\nThe Digital History classes at the University of Hertfordshire have learned skills in digital mapping and Python programming, which makes it more accessible and easier to analyse large quantities of source data. One project that the class worked on included taking data on weather, crime and poverty and using digital history skills to analyse the trends and patterns between them. This then allowed them to use their traditional history skills to evaluate the significance of the trends. Another project was using digital mapping to compare the differences between various groups of students who studied at Oxford derived from British History Online.\n\nAt Cal State East Bay, 12-15 history majors meet in the science building's computer lab to go over new and old software that could be used for the creation or presentation of history. Professor Kaatz specializes in early Christianity and ancient Rome, and needs to be well informed in the area of digitization. The digital history program is fairly new, and larger universities like Harvard and Stanford also have digital programs that are branches of their history programs.\n\nCalifornia State University San Marcos offers a history graduate studies program with an emphasis on digital history. The master's thesis requirement is strongly encouraged to be half digital and half written, but the requirement can still be fulfilled by a traditional fully written thesis.\n\nDigital technology tools arrange ideas and promote the unique analysis of data, with many tools previously unavailable to historians opening new avenues for collaboration, text mining, and big data analysis. In addition, digital history offers tools for the presentation and access to historical knowledge online.\n\nDigital historians may use web development tools, such as WYSIWYG HTML-editor Adobe Dreamweaver. Other tools create more interactive digital history, such as Databases, which provide greater capacity for information storage and retrieval in a definable way. Databases with features like Structured Query Language (SQL) and Extensible Markup Language (XML) arrange materials in a formal manner and allow precise searching for keywords, dates, and other data characteristics. The online article \"The Differences Slavery Made: A Close Analysis of Two American Communities\" used XML for presenting and connecting evidence with detailed historiographical discussions. The \"Valley of the Shadow\" project also employed XML to convert all of the archive's letters, diaries, and newspapers for full text searching capabilities. Coding languages such as Python may be used in order to digitally sort and filter data, whilst Google Fusion Tables can be used for the geographical mapping of data.\n\nDigital historians may use content management systems (CSM) to store their digital collection that includes audio, visual, images and text for an online web display. Examples of these systems include: Drupal, WordPress, and Omeka.\n\n\"The Differences Slavery Made\" also used Geographic Information Systems (GIS) to analyze and understand the spatial arrangement of social structures. For the article, Ayers and Thomas created many new maps through GIS technology to produce detailed images of Augusta and Franklin counties never before possible. GIS and its many components remain helpful for studying history and visualizing change over time.\n\nThe Semantic Interoperability of Metadata and Information in unLike Environments (SIMILE) project at MIT develops robust, open source tools that enable access, management, and envisaging digital assets. Among the many tools built by SIMILE, the Timeline tool, which employs a DHTML-based AJAXy widget, allows digital historians to create dynamic, customizable timelines for visualizing time-based events. The Timeline page on the SIMILE website declares that their tool \"is like Google Maps for time-based information.\" Additionally, SIMILE's Exhibit tool boasts a customizable structure for sorting and presenting data . Exhibit, written in Javascript, creates interactive, data-rich web pages without the need for any programming or database creation knowledge.\n\nTextual analysis software allows historians to make new use of old sources by finding patterns in large collections of documents or even just analyzing a source for frequency of terms. Textual analysis software allows historians to \"text mine\", a term used to describe correlations and themes in the documents. There are several textual-analysis programs available online, from sophisticated ones that allow the researcher to tailor the program to handle large amounts of data, like MALLET, and straightforward programs like TokenX, which generates word-frequency lists and word clouds to illustrate language usage and significance, to basic ones like wordle, which offers simple visualizations of word frequency and relationships. Some websites provide textual analysis on their content automatically. Online bookmarking and research tool del.icio.us uses tag clouds to visually depict the frequency and importance of user-generated tags, and the recently instituted Google Ngram Viewer allows viewers to search the commonality of textual themes by year.\n\nHowever, with the development of digital history and the technology used to produce it, there has been questions raised over the validity of it. One such issue, is that raised by Jean Francois Baudrillard. He says that 'Western Culture introduced significant modifications to the way it produced the real, by intensifying it and heightening it into a domain of reality in hyperspace: hyper-reality'.\n\n\n\n\n"}
{"id": "363445", "url": "https://en.wikipedia.org/wiki?curid=363445", "title": "Dry cask storage", "text": "Dry cask storage\n\nDry cask storage is a method of storing high-level radioactive waste, such as spent nuclear fuel that has already been cooled in the spent fuel pool for at least one year and often as much as ten years. Casks are typically steel cylinders that are either welded or bolted closed. The fuel rods inside are surrounded by inert gas. Ideally, the steel cylinder provides leak-tight containment of the spent fuel. Each cylinder is surrounded by additional steel, concrete, or other material to provide radiation shielding to workers and members of the public.\n\nThere are various dry storage cask system designs. With some designs, the steel cylinders containing the fuel are placed vertically in a concrete vault; other designs orient the cylinders horizontally. The concrete vaults provide the radiation shielding. Other cask designs orient the steel cylinder vertically on a concrete pad at a dry cask storage site and use both metal and concrete outer cylinders for radiation shielding. Currently there is no long term permanent storage facility; dry cask storage is designed as an interim safer solution than spent fuel pool storage.\n\nSome of the cask designs can be used for both storage and transportation. Three companies – Holtec International, NAC International and Areva-Transnuclear NUHOMS – are marketing Independent Spent Fuel Storage Installations (ISFSI's) based upon an unshielded multi-purpose canister which is transported and stored in on-site vertical or horizontal shielded storage modules constructed of steel and concrete.\n\nDuring the 2000s, dry cask storage was used in the United States, Canada, Germany, Switzerland, Spain, Belgium, Sweden, the United Kingdom, Japan, Armenia, Argentina, Bulgaria, Czech Republic, Hungary, South Korea, Romania, Slovakia, Ukraine and Lithuania.\n\nA similar system is also being implemented in Russia. However, it is based on 'storage compartments' in a single structure, rather than individual casks.\n\nIn 2017 France's Areva launched the NUHOMS Matrix advanced used nuclear fuel storage overpack, a high-density system for storing multiple spent fuel rods in canisters.\n\nIn the late 1970s and early 1980s, the need for alternative storage in the United States began to grow when pools at many nuclear reactors began to fill up with stored spent fuel. As there was not a national nuclear storage facility in operation at the time, utilities began looking at options for storing spent fuel. Dry cask storage was determined to be a practical option for storage of spent fuel and preferable to leaving large concentrations of spent fuel in cooling tanks.\nThe first dry storage installation in the US was licensed by the Nuclear Regulatory Commission (NRC) in 1986 at the Surry Nuclear Power Plant in Virginia, at . Spent fuel is currently stored in dry cask systems at a growing number of power plant sites, and at an interim facility located at the Idaho National Laboratory near Idaho Falls, Idaho. The Nuclear Regulatory Commission estimates that many of the nuclear power plants in the United States will be out of room in their spent fuel pools by 2015, most likely requiring the use of temporary storage of some kind. Yucca Mountain was expected to open in 2017. However, on March 5, 2009, Energy Secretary Steven Chu reiterated in a Senate hearing that the Yucca Mountain site was no longer considered an option for storing reactor waste.\n\nThe 2008 NRC guideline calls for fuels to have spent at least five years in a storage pool before being moved to dry casks. The industry norm is about 10 years. The NRC describes the dry casks used in the US as \"designed to resist floods, tornadoes, projectiles, temperature extremes, and other unusual scenarios.\"\n\nAs of the end of 2009, 13,856 metric tons of commercial spent fuel – or about 22 percent – were stored in dry casks.\n\nIn the 1990s, the NRC had to “take repeated actions to address defective welds on dry casks that led to cracks and quality assurance problems; helium had leaked into some casks, increasing temperatures and causing accelerated fuel corrosion”.\n\nWith the zeroing of the budget for Yucca Mountain nuclear waste repository in Nevada, more nuclear waste is being loaded into sealed metal casks filled with inert gas. Many of these casks will be stored in coastal or lakeside regions where a salt air environment exists, and the Massachusetts Institute of Technology is studying how such dry casks perform in salt environments. Some hope that the casks can be used for 100 years, but cracking related to corrosion could occur in 30 years or less.\n\nCompany videos, covering the processes and remote handling, from the initial fuel loading to the removal and eventual dry-cask storage, are viewable on various video hosting domains.\n\nIn Canada, above-ground dry storage has been used. Ontario Power Generation is in the process of constructing a Dry Storage Cask storage facility on its Darlington site, which will be similar in many respects to existing facilities at Pickering Nuclear Generating Station and Bruce Nuclear Generating Station. NB Power's Point Lepreau Nuclear Generating Station and Hydro-Québec's Gentilly Nuclear Generating Station also both operate dry storage facilities.\n\nA centralized storage facility using dry casks is located at Ahaus. As of 2011, it housed 311 casks; 305 from the Thorium High Temperature Reactor, 3 from the Neckarwestheim Nuclear Power Plant, and 3 from the Gundremmingen Nuclear Power Plant. The transport from Gundremmingen to the Ahaus site met with considerable public protest and the power plant operators and the government later agreed to locate such casks at the powerplants.\n\n\"CASTOR\" (\"ca\"sk for \"s\"torage and \"t\"ransport \"o\"f \"r\"adioactive material) is a trademarked brand of dry casks used to store spent nuclear fuel (a type of nuclear waste). CASTORs are manufactured by Gesellschaft für Nuklear-Service (GNS), a German provider of nuclear services.\n\n\"CONSTOR\" is a cask used for transport and long-term storage of spent fuel and high-level waste also manufactured by GNS. Its inner and outer layers are steel, enclosing a layer of concrete.\nA 9-meter drop test of the V/TC model was conducted in 2004; the results conformed to expectations.\n\nIn 2008, officials at the Kozloduy Nuclear Power Plant announced their intention to use 34 CONSTOR casks at the Kozloduy NPP site before the end of 2010.\n\nSpent fuel from the now-closed Ignalina Nuclear Power Plant was placed in CASTOR and CONSTOR storage casks during the 2000s.\n\nThe Russian dry storage facility for spent nuclear fuel, the HOT-2 at Mining Chemical Combine in Zheleznogorsk, Krasnoyarsk Krai in Siberia, is not a 'cask' facility per se, as it is designed to accommodate the spent nuclear fuel (both VVER and RBMK) in a series of compartments. The structure of the facility is made up of monolithic reinforced concrete walls and top and bottom slabs, with the actual storage compartments formed by reinforced concrete partitions. The fuel is to be cooled by natural convection of air. The design capacity of the facility is 37,785 tonnes of uranium. It is now under construction and commissioning.\n\nIn Ukraine, a dry storage facility has been accepting spent fuel from the six-unit Zaporozhye Nuclear Power Plant (VVER-1000 reactors) since 2001, making it the longest-serving such facility in the former Soviet Union. The system was designed by the now-defunct Duke Engineering of the United States, with the storage casks being manufactured locally.\n\nAnother project is underway with Holtec International (again of the USA) to build a dry spent fuel storage facility at the 1986-accident-infamous Chernobyl Nuclear Power Plant (RBMK-1000 reactors). The project was initially started with Framatome (currently AREVA) of France, later suspended and terminated due to technical difficulties. Holtec was originally brought on board as a subcontractor to dehydrate the spent fuel, eventually taking over the entire project.\n\n\n"}
{"id": "26509895", "url": "https://en.wikipedia.org/wiki?curid=26509895", "title": "Dual revolution", "text": "Dual revolution\n\nThe Dual Revolution was a term first coined by Eric Hobsbawm. It refers specifically to the time period between 1789 and 1848 in which the political and ideological changes of the French Revolution fused with and reinforced the technological and economic changes of the Industrial Revolution. The French Revolution, inspired by the ideals of Enlightenment philosophy, spread ideas of democracy, nationalism, and liberalism. These political ideas were fused with the new technological advances of the industrial revolution such as the spinning jenny, steam engines, and the puddling process. With the defeat of the French Revolution and subsequently the Congress of Vienna, Metternich constructed a balance of power in Europe that would prevent one country from gaining too much power. This set the framework for a strong conservative, reactionary stance against the ideas of nationalism and liberalism spread by the dual revolution. The Holy Alliance, formed by Austria, Prussia and Russia in September 1815 became a symbol of oppression to the ideas spread by the Dual Revolution.\n\nGenerally, Industrial Revolution means transitioning from hand produced to machines. In the case of the dual revolution, the Industrial portion was more from social scientists like Count Henri de Saint-Simon, Charles Fourier, Louis Blanc, Pierre Joseph Proudhon, and Karl Marx who attempted to come up with ideas for “utopias”, conservative and socialist societies.\n\nThe French Revolution began in 1789 with the Tennis Court Oath, by which the members of the third estate, who had been locked out of the meeting the estates general, gathered together and swore not to disband until they had written a new constitution. Inspired by the ideas of Enlightenment Philosophers, most prominently Rousseau and his social contract, the National assembly, those who had sworn the tennis court oath, published the Declaration of the Rights of Man and Citizen. The Declaration of the Rights of Man was very much an expression of Enlightenment ideals. A major portion of the Declaration was dedicated to the natural rights of man. According to the Declaration every man is entitled to life, liberty and property. This is a reflection of the beliefs of John Locke who wrote the 2nd treatise of civil government. The Committee of Public Safety, headed by Robespierre, which rose to power after using the sans culotte to imprison the other members of the Jacobin club, was based on Rousseau's Social Contracts idea of the general will. Rousseau states that at times people have to sacrifice their natural rights for the greater good which is represented by the General Will. The General Will is not necessarily represented by the majority, a far-seeing minority can represent the general will. Robespierre believed that the Committee of Public Safety represented the general will. Robespierre used an aggressive revolutionary purge of those who opposed the Committee called the Terror, a planned economy, and French Nationalism inspired by the revolution to increase French military size and push armies into most of western Europe.\n\nNationalism was a driving force for European politics after the French revolution. The ideas of Nationalism emphasized both national pride and that each nation, which consists of a people of the same culture and language should coincide with the boundaries of the state. Metternich, the Austrian Chancellor, was conservative precisely because nationalism was very dangerous to the massive, multi-ethnic Austria. Increased national identity and pride garnered from the successes of ones country were another reason for increased European colonialism during the later parts of the 19th century.\n\nLiberalism was a major movement post French Revolution. It was an expression of enlightenment ideals. Liberals believed in freedom of the press, freedom of speech, civil rights, fair elections, capitalism, freedom of religion and private property. Ideas of liberalism greatly influenced the 1848 revolutions which were led by students and the middle class.\n\nConservatism is based on traditional strategies like absolute monarchies. It is a reactionary stance against the ideas spread by the dual revolution. Conservatism is closely linked to Classicism.\n\n\nhttp://www.britannica.com/EBchecked/topic/387301/modernization/12018/The-dual-revolution\n\nVan Zanden, Jan Luiten (2009). https://books.google.com/books?id=Djqxy6VnOLIC&pg=PA12&dq=dual+revolution&hl=en&sa=X&ei=4m6nUfGmJ6OKiAL144HIBw&ved=0CEEQ6AEwAw#v=onepage&q=dual%20revolution&f=false\n"}
{"id": "3783748", "url": "https://en.wikipedia.org/wiki?curid=3783748", "title": "Fire iron", "text": "Fire iron\n\nA fire iron is any metal instrument for tending a fire.\n\nThere are three types of tools commonly used to tend a small fire, such as an indoor fireplace fire or yule log, the spade, the tongs and the poker itself. These tools make it possible to handle a fire without risk of burns or blisters.\n\nA fireplace poker, (also known as a fire iron) is a short, rigid rod made of fireproof material used to adjust coal and wood fuel burning in a fireplace, and can be used to stir up a fire. A fireplace poker is usually metallic and has a point at one end for pushing burning materials (or a hook for pulling/raking, or a combination) and a handle at the opposite end, sometimes with an insulated grip. Iron is the most popular metal from which the pokers are wrought. Brass is a more expensive alternative for a home poker set.\n\nA slice bar has a flatter tip and can be used to stir up the fire or to clear the grates of ashes. \nOther fires irons include the fire rake (not to be confused with the firefighter's tool), fire tongs and fire shovel.\n\nMany fireplace sets also include a small broom for sweeping up ash.\n\nIn Japan, traditional fire-tending device for a Japanese brazier (\"hibachi\") is a pair of long metal chopsticks, called , used to pick up and manipulate the charcoal.\n\nLarge bonfires are not amenable to the use of tools of the size commonly used in an indoor fireplace. However some individuals have been known to weld rebar into clever shapes to move the wood in a moderately large bonfire.\n\nAs a steam locomotive runs, by-products are produced by the coal fire such as ash and clinker. If these waste products are allowed to build up in the fire, it would have an adverse effect on the performance of the locomotive. A fireman will employ various fire irons in order to clean the fire, whilst the locomotive stands. Below is a list of different types of fire iron that would typically be carried aboard a locomotive during operation. Note: not all the fire irons listed would be carried at once, only the ones needed:\n\n\nThe earliest and most primitive pokers were likely made from the same material as the fuel (that is, wood in the form of a hefty branch). This \"ersatz\" wooden-type fire-tool may be called a poker or a \"firestick\" in colloquial terminology. The first successful mass production of stokers as a part of an entire fireplace-regalia set was designed and manufactured in Cape Girardeau, Missouri by the RL Hendrickson Manufacturing Corporation in 1898 at a cost of US$1. Today, one of the sets in fair condition can garner more than US$3500 at auction.\n\nEdward II of England, popular yet largely-uncorroborated story of how a poker may have led to a monarch's death.\n"}
{"id": "36787028", "url": "https://en.wikipedia.org/wiki?curid=36787028", "title": "Fluxus (programming environment)", "text": "Fluxus (programming environment)\n\nFluxus is a live coding environment for 3D graphics, music and games. It uses the programming language Racket (a dialect of Scheme/Lisp) to work with a games engine with built-in 3D graphics, physics simulation and sound synthesis. All programming is done on-the-fly, where the code editor appears on top of the graphics that the code is generating. It is an important reference for research and practice in exploratory programming, pedagogy, live performance and games programming.\n\nFluxus is known for hosting some of the most cutting-edge live coding research systems by its author Dave Griffiths, such as the BetaBlocker language inspired by Core War, the Al-Jazari music environment based on interacting robots, the Daisy Chain music environment based on the Petri net model of computation, and the SchemeBricks visual interface for Scheme.\n"}
{"id": "39556262", "url": "https://en.wikipedia.org/wiki?curid=39556262", "title": "Focke's wind tunnel (Bremen, Germany)", "text": "Focke's wind tunnel (Bremen, Germany)\n\nFocke's wind tunnel is a fully operational wind tunnel in the former private laboratory of the aviation pioneer Henrich Focke (1890–1979), co-founder of Focke-Wulf and designer of the first fully controllable helicopter, the Focke-Wulf Fw 61. Henrich Focke built the laboratory in 1960 at the age of 70 in the city of Bremen.\nUntil shortly before his death, in 1979, Focke continued aerodynamic studies in slow flight characteristics and the stability problem of helicopters. The rediscovery of his aerodynamic laboratory, together with its wind tunnel was regarded as a sensation for science and technology. Since 2004, the technical monument Focke Flight Laboratory is protected by law.\n\nThe lab was discovered in 1997 by Kai Steffen, after having read the memoirs of Henrich Focke, the former graduate student searched for the missing wind tunnel, contacting the Focke family and finding the laboratory in a backyard shed near the main station of Bremen. The laboratory had not been entered by anyone for about 20 years.\n\nThe wind tunnel is a closed recirculating wind tunnel for subsonic speed range, produces wind speeds up to 16 m per second and is still fully functional. The simplicity of the instrumentation is demonstrated by the use of kitchen scales to measure the forces acting in the wind tunnel, stove pipes and curtains controlling the airflow and reducing swirl. Everything in this lab was still in place, as the 85-year-old Henrich Focke had left it, but the building was not in good condition.\n\nAfter renovation of the building and the restoration of the wind tunnel, the laboratory is now shown as Henrich Focke used it until 1975, with scientific experiments carried out by schools and universities.\n\nThe museum was opened in 2005, but the aerodynamic laboratory was not fully repaired until the Autumn of 2008. In addition, modern measurement technology, using a personal computer and electronic pressure cells, was installed.\n\n"}
{"id": "41170519", "url": "https://en.wikipedia.org/wiki?curid=41170519", "title": "Fuel cell forklift", "text": "Fuel cell forklift\n\nA fuel cell forklift (also called a fuel cell lift truck or a fuel cell forklift) is a fuel cell powered industrial forklift truck used to lift and transport materials.\n\n\nIn 2013 there were over 4,000 fuel cell forklifts used in material handling in the USA. Fuel cell fleets are currently being operated by a number of companies, including Sysco Foods, FedEx Freight, GENCO (at Wegmans, Coca-Cola, Kimberly Clark, and Whole Foods). Europe demonstrated 30 Fuel cell forklifts with Hylift and extended it with HyLIFT-EUROPE to 200 units. With other projects in France and Austria.\n\nPEM fuel-cell-powered forklifts provide benefits over petroleum powered forklifts as they produce no local emissions. While LP Gas (propane) forklifts are more popular and often used indoors, they cannot accommodate certain food industry applications. Fuel cell power efficiency (40-50%) is about half that of lithium-ion batteries (80-90%), but they have a higher energy density which may allow forklifts to run longer. Fuel-cell-powered forklifts are often used in refrigerated warehouses as their performance is not as affected by temperature as some types of lithium batteries. Most fuel cells used for material handling purposes are powered by PEM fuel cells, although some DMFC forklifts are coming onto the market. In design the FC units are often made as drop-in replacements.\n\n2013 -Toyota Industries (Toyota Shokki) has revealed a new fuel cell powered forklift truck, co-developed with Toyoda Gosei Co., Ltd.,\n2015 - HySA Systems (UWC) revealed fuel cell powered forklift and refueling station based on metal hydrides. Customer was mining company Implats from South Africa. This was first project of this type in African continent.\n\n"}
{"id": "36434851", "url": "https://en.wikipedia.org/wiki?curid=36434851", "title": "Ghodiyu", "text": "Ghodiyu\n\nA ghodiyu is an infant cradle formed of a wooden frame and a cloth hammock (\"jholi\").\n\nThe device originated in Gujarat, India to help babies sleep. People in India have used this type of device for hundreds of years so that their baby can fall asleep quickly and get the rest they need while developing proper sleeping habits. A similar functioning device from the South India is called a jhula or parnu.\n\nA ghodiyu is a swing framework with a pair of legs on two ends of middle rod. This was originally made of wood (called \"sankheda\"), however, in modern times strong and durable material such as stainless steel is used during the manufacturing. The frame is often hooked onto a hammock which harnesses the young baby.\n\nTypically, a ghodiyu is designed to have a low center of gravity. This enables a person to swing the child back and forth.\n\nIndian people believe that the rocking motion soothes and relaxes the child and puts them to sleep quickly by replicating the comfort and security of the womb. Indian mothers claim that using a ghodiyu for their child can relieve baby colic symptoms due to the rocking motion which they believe relaxes the baby.\n"}
{"id": "4001289", "url": "https://en.wikipedia.org/wiki?curid=4001289", "title": "Grain trade", "text": "Grain trade\n\nThe grain trade refers to the local and international trade in cereals and other food grains such as wheat, maize, and rice.\n\nThe grain trade is probably nearly as old as grain growing, going back the Neolithic Revolution ( around 11,500 BC ). Wherever there is a scarcity of land (e.g. cities) people must bring in food from outside to sustain themselves, either by force or by trade. However, many farmers throughout history (and today) have operated at the subsistence level, meaning they produce for household needs and have little leftover to trade. The goal for such farmers in not to specialize in one crop and grow a surplus of it, but rather to produce everything his family needs and become self-sufficient. Only in places and eras where production is geared towards producing a surplus for trade (commercial agriculture), does a major grain trade become possible.\n\nEarly trade was most likely by barter, and because hauling large amounts of grain around was so difficult with ancient technology, the trade was probably quite limited in terms of the volume traded and the area moved. The development of the money economy and the wheel would have facilitated a much more expansive trade.\n\nIn the ancient world, grain regularly flowed from the hinterlands to the cores of great empires: maize in ancient Mexico, rice in ancient China, and wheat and barley in the ancient Near East. With this came improving technologies for storing and transporting grains; the Hebrew Bible makes frequent mention to ancient Egypt's massive grain silos.\n\nDuring the classical age, the unification of China and the pacification of the Mediterranean basin by the Roman Empire created vast regional markets in commodities at either end of Eurasia. The grain supply to the city of Rome was considered by be of the utmost strategic importance to Roman generals and politicians.\n\nIn Europe with the collapse of the Roman system and the rise of feudalism many farmers were reduced to a subsistence level, producing only enough to fulfill their obligation to their lord and the Church, with little for themselves, and even less for trade. The little that was traded was moved around locally at regular fairs.\n\nA massive expansion in the grain trade occurred when Europeans were able to bring millions of square kilometers of new land under cultivation in the Americas, Russia, and Australia, in an expansion starting in the fifteenth and lasting into the twentieth century. In addition the consolidation of farmland in Britain and Eastern Europe, and the development of the railway and steam ship shifted trade from local to more international patterns.\n\nDuring this time debate over tariffs and free trade in grain was fierce. Poor industrial workers relied on cheap bread for sustenance, but farmers wanted their government to create a higher local price to protect them from cheap foreign imports, Britain's famous Corn Laws being an example.\n\nAs Britain and other European countries industrialized and urbanized they became net importers of grain from the various breadbaskets of the world. In many parts of Europe as serfdom was abolished, great estates were accompanied by many inefficient smallholdings, but in the newly colonized regions massive operations were available to the average farmer and not only great nobles. In the United States and Canada the Homestead Act and the Dominion Lands Act allowed pioneers on the western plains to gain tracts of or more for little or no fee. This moved grain growing, and hence trading, to a much more massive scale. Huge grain elevators were built to take in farmers' produce and move it out via the railways to port. Transportation costs were a major concern for farmers in remote regions, however, and any technology that allowed the easier movement of grain was of great assistance, meanwhile farmers in Europe struggled to remain competitive while operating on a much smaller scale and in 1911 Encyclopædia Britannica wrote:\n\nThe farmers of the United States have met a greatly increased output from Canada, the cost of transport from that country to England being much the same as from the United States in the 20th century. So much improved is the position of the farmer in North America compared with what it was about 1870, that the transport companies in 1901 carried 17¾ bushels of his grain to the seaboard in exchange for the value of one bushel, whereas in 1867 he had to give up one bushel in every six in return for the service.\n\nAs regards with the British farmer, it does not appear as if he had improved his position; for he has to send his wheat to greater distances, owing to the collapse of many country millers or their removal to the seaboard, while railway rates have fallen only to a very small extent; again the farmers wheat is worth only half of what it was formerly; it may be said that the British farmer has to give up one bushel in nine to the railway company for the purpose of transportation, whereas in the seventies he gave up one in eighteen only. Enough has been said to prove that the advantage of position claimed for the British farmer by Caird was somewhat illusory. Speaking broadly, the Kansas or Minnesota farmers wheat does not have to pay for carriage to Liverpool more than 2s. 6d. to 7s. 6d. per ton in excess of the rate paid by a Yorkshire farmer; this, it will be admitted, does not go very far towards enabling the latter to pay rent, tithes and rates and taxes.\n- Encyclopædia Britannica Eleventh Edition\n\nIn the 1920s and '30s farmers in Australia and Canada reacted against the pricing power of the large grain-handling and shipping companies. Their governments created the Australian Wheat Board and Canadian Wheat Board as monopsony marketing boards, buying all the wheat in those countries for export. Together those two boards controlled a large percentage of the world's grain trade in the mid-20th century. Additionally, farmers' cooperatives such the wheat pools became a popular alternative to the major grain companies.\n\nAt the same time in the Soviet Union and soon after in China, disastrous collectivization programs effectively turned the world's largest farming nations into net importers of grain.\n\nBy the second half of the 20th century, the grain trade was divided between a few state-owned and privately owned giants. The state giants were Exportkhleb of the Soviet Union, the Canadian Wheat Board, the Australian Wheat Board, the Australian Barley Board, and so on. The largest private companies, known as the \"big five\", were Cargill, Continental, Louis Dreyfus, Bunge, and Andre, an older European company not to be confused with the more recent André Maggi Group from Brazil.\n\nIn 1972, the Soviet Union's wheat crop failed. To prevent shortages in their own country, Soviet authorities were able to buy most of the surplus American harvest through private companies without the knowledge of the United States government. This drove up prices across the world, and was dubbed the \"great grain robbery\" by critics, leading to greater public attention being paid by Americans to the large trading companies.\n\nBy contrast in 1980 the US government attempted to use its food power to punish the Soviet Union for its invasion of Afghanistan with a embargo on grain exports. This was seen as a failure in terms of foreign policy (the Soviets made up the deficit on the international market) and negatively impacted American farmers.\n\nSince the Second World War, the trend in North America has been toward further consolidation of already vast farms. Transportation infrastructure has also promoted more economies of scale. Railways have switched from coal to diesel fuel, and introduced hopper car to carry more mass with less effort. The old wooden grain elevators have been replaced by massive concrete inland terminals, and rail transportation has retreated in the face of ever larger trucks.\n\nFarmers in the European Union, United States and Japan are protected by agricultural subsidies. The European Union's programs are organized under the Common Agricultural Policy. The agricultural policy of the United States is demonstrated through the \"farm bill\", while rice production in Japan is also protected and subsidized. Farmers in other countries has attempted to have these policies disallowed by the World Trade Organization, or attempted to negotiate them away though the Cairns Group, at the same time the wheat boards have been reformed and many tariffs have been greatly reduced, leading to a further globalization of the industry. For example, in 2008 Mexico was required by the North American Free Trade Agreement (NAFTA) to remove its tariffs on US and Canadian maize.\n\n\"Price volatility is a life-and-death issue for many people around the world\" warned ICTSD Senior Fellow Sergio Marchi. \"Trade policies need to incentivise investment in developing country agriculture, so that poor farmers can build resistance to future price shocks\".\n\n\n\n"}
{"id": "43964977", "url": "https://en.wikipedia.org/wiki?curid=43964977", "title": "Hackpad", "text": "Hackpad\n\nHackpad is a web-based collaborative real-time text editor forked from Etherpad.\n\nIn April 2014, Hackpad was acquired by Dropbox. In April 2015, it was announced that Hackpad would be released as open source and source code was published on Github in August 2015, under the Apache license 2.0. On April 25, 2017, Hackpad announced that it is to shut down on July 19, 2017, permanently migrating to Dropbox Paper.\n\n"}
{"id": "39410960", "url": "https://en.wikipedia.org/wiki?curid=39410960", "title": "Hans van der Zouwen", "text": "Hans van der Zouwen\n\nJohannes (Hans) van der Zouwen (born 1939) is a Dutch sociologist, and Emeritus Professor of Social Research Methodology at the Vrije Universiteit in Amsterdam, known for his pioneering work with Felix Geyer in the field of sociocybernetics.\n\nIn the 1960s Van der Zouwen received his MA in sociology and 1970 his PhD in sociology from the Vrije Universiteit with a thesis about the sociological aspects of volunteer work around the Vrije Universiteit Amsterdam.\n\nIn the late 1960s Van der Zouwen had started his academic career at the Vrije University in Amsterdam, where in 1969 he had become chair of the department of Social Research Methodology. From 1971 to early 2000 he was Professor of Social Research Methodology. Since then he is affiliated with the Interuniversity Graduate School of Psychometrics and Sociometrics (IOPS). In the 1990s Van der Zouwen is awarded Honorary Fellow of the World Organisation of Systems and Cybernetics.\n\n\nArticles, a selection:\n"}
{"id": "37196381", "url": "https://en.wikipedia.org/wiki?curid=37196381", "title": "Indian Institute of Food Processing Technology", "text": "Indian Institute of Food Processing Technology\n\nThe Indian Institute of Food Processing Technology (formerly Indian Institute of Crop Processing Technology) (IIFPT) is a pioneer Educational and R&D Institution under the Ministry of Food Processing Industries (MoFPI). The IIFPT is located on a 15-acre campus in Thanjavur to Pudukkottai Highway located at a distance of 2 km from Thanjavur New Bustand.\n\nThe Institute was organized in 1967 as R&D laboratory in the Modern Rice Mill Complex of Thanjavur Co-operative Marketing Federation (TCMF) at Thiruvarur and aimed at preserving high moisture paddy, since the harvest season in Thanjavur District coincided with the end of the monsoon season.\n\nIn 1972, the institute was upgraded and named the Paddy Processing Research Centre (PPRC). At the time of upgrading the mandates of the institute were also changed and the research focussed on identifying post harvest processing and preservation technology for paddy. PPRC then moved into its current location in Thanjavur\n\nConsidering the vital importance of strengthening R&D efforts in the post harvest processing of foods, MoFPI strengthened and upgraded the PPRC as a National Level Institute. In February 2008 the institute was upgraded as a National Institute and renamed as the Indian Institute of Crop Processing Technology.\n\nIIFPT has a total of 14 departments catering to the needs of students and researchers.\n\nResearch in the Institute is facilitated by laboratories and facilities, these include :\n\n\nAs part of creating manpower for the food processing sector, IIFPT has been offering B.Tech., M.Tech. and Ph.D. programs in food process engineering from 2009–10 academic year with an intake of 60 B.Tech., 20 M.Tech. and 10 Ph.D. students each year. The academic programs are affiliated to Tamil Nadu Agricultural University, Coimbatore, Tamil Nadu.\n\nIIFPT has collaborated and signed MOUs with many international institutions across India and the World to facilitate a collaborative programme of research, training, curriculum, institutional development, information dissemination, and exchange of faculty, students and staff. The Institute has signed MoUs with the following International and National Institutions so far.\n\n\n"}
{"id": "510278", "url": "https://en.wikipedia.org/wiki?curid=510278", "title": "Infineon Technologies", "text": "Infineon Technologies\n\nInfineon Technologies AG is a German semiconductor manufacturer founded on 1 April 1999, when the semiconductor operations of the parent company Siemens AG were spun off to form a separate legal entity. As of 30 September 2016, Infineon had 36,299 employees worldwide. In fiscal year 2016, the company achieved sales of €6.473 billion.\n\nOn 1 May 2006, Infineon's Memory Products division was carved out as a distinct company called Qimonda AG, which at its height employed about 13,500 people worldwide. Qimonda was listed on the New York Stock Exchange until 2009.\n\nInfineon Technologies AG, in Neubiberg near Munich, offers semiconductors and systems for automotive, industrial, and multimarket sectors, as well as chipcard and security products. With a global presence, Infineon operates through its subsidiaries in the USA, from Milpitas, California, and in the Asia-Pacific region, from Singapore and from Tokyo, Japan.\n\nInfineon has a number of facilities in Europe, one in Dresden, Germany, Europe's microelectronic, and emerging technologies center. Infineon's high power segment is in Warstein, Germany; Villach and Graz in Austria; Cegléd in Hungary; and Italy. It also runs R&D centers in France, Singapore, Romania, Taiwan, UK and India, as well as fabrication units in Singapore, Malaysia, Indonesia, and China. There's also a Shared Service Center in Maia, Portugal.\n\nInfineon is listed in the DAX index of the Frankfurt Stock Exchange.\n\nIn 2010, a proxy contest broke out in advance of the impending shareholders' meeting over whether board member Klaus Wucherer would be allowed to step into the chairman's office upon the retirement of the then-current chairman Max Dietrich Kley.\n\nAfter several restructurings, Infineon today comprises four business areas:\n\nInfineon provides semiconductor products for use in powertrains (engine and transmission control), comfort electronics (e.g., steering, shock absorbers, air conditioning) as well as in safety systems (ABS, airbags, ESP). The product portfolio includes microcontrollers, power semiconductors and sensors. In fiscal year 2013 (ending September), sales amounted to €1,714 million for the ATV segment.\n\nThe industrial division of the company includes power semiconductors and modules which are used for generation, transmission and consumption of electrical energy. Its application areas include control of electric drives for industrial applications and household appliances, modules for renewable energy production, conversion and transmission. This segment achieved sales of €651 million in fiscal year 2013.\n\nThe division Power Management & Control sums up the business with semiconductor components for efficient power management or high-frequency applications. Those find application in lighting management systems and LED lighting, power supplies for servers, PCs, notebooks and consumer electronics, custom devices for peripheral devices, game consoles, applications in medical technology, high-frequency components having a protective function for communication and tuner systems and silicon MEMS microphones. In fiscal year 2013 PMM generated €987 million.\n\nThe CCS business provides microcontrollers for mobile phone SIM cards, payment cards, security chips and chip-based solutions for passports, identity cards and other official documents. Infineon delivers a significant number of chips for the new German identity card. In addition, CCS provides solutions for applications with high security requirements such as pay television and Trusted Computing. CCS achieved €463 million in fiscal year 2013. \"Infineon is the number 1 in embedded security\" (IHS, 2016 – IHS Embedded Digital Security Report).\n\nThe former memory chip division was carved out in 2006 as Infineon’s subsidiary Qimonda, of which Infineon last held a little over three quarters. In January 2009, Qimonda filed for bankruptcy with the district court in Munich.\n\nOn 7 July 2009, Infineon Technologies AG agreed by contract with the U.S. investor Golden Gate Capital on the sale of its Wireline Communications for €250 million. The resulting new company is now known as Lantiq.\n\nOn 31 January 2011, the sale of the business segment of wireless solutions to Intel was completed. The resulting new company has approximately 3,500 employees and now operates as Intel Mobile Communications (IMC).\n\nIn July 2016, Infineon announced it agreed to buy the North Carolina-based company Wolfspeed from Cree Inc. for $850 million in cash. The deal was however stopped due to US security concerns.\n\nIn March 2018, Infineon Technologies AG sold its RF Power Business Unit to Cree Inc. for €345 Million.\n\nInfineon bought ADMtek in 2004.\n\nInfineon Technologies agreed on 20 August 2014 to buy the International Rectifier Corporation for about $3 billion, one third by cash and two third by credit line. The acquisition of International Rectifier was officially closed on 13 January 2015.\n\nIn October 2016, Infineon acquired the company Innoluce which has expertise in MEMS and LiDAR systems for use in autonomous cars. The MEMS lidar system can scan up to 5,000 data points a second with a range of 250 meters with an expected unit cost of $250 in mass production.\n\nOn 18 May 2016 Infineon decided to reinforce the management board and therefore added a new, 4th member to the board of directors. From July 1, 2016 the board of directors consists of:\n\nReinhard Ploss, CEO\n\nDominik Asam, CFO\n\nHelmut Gassel, Sales and Marketing\n\nJochen Hanebeck, Operations\n\nMajor institutional investors in Infineon are: Dodge and Cox International Stock Fund: 9.82%, BlackRock, Inc.: 5.10%, Capital Research and Management: 5.09%, UBS AG: 3.28%.\n\nIn 2004–2005, an investigation was carried out into a worldwide DRAM price fixing conspiracy during 1999–2002 that damaged competition and raised PC prices. As a result, Samsung is to pay $300 million fine, Hynix was to pay $185 million in 2005, Infineon: $160 million in 2004. Micron Technology cooperated with prosecutors and no fine is expected.\n\nIn October 2017, it was reported that a flaw, dubbed ROCA, in a code library developed by Infineon, which had been in widespread use in security products such as smartcards and TPMs, enabled private keys to be inferred from public keys. As a result, all systems depending upon the privacy of such keys were vulnerable to compromise, such as identity theft or spoofing. Affected systems include 750,000 Estonian national ID cards, 300,000 Slovak national ID cards, and computers that use Microsoft BitLocker drive encryption in conjunction with an affected TPM. Microsoft released an updated version of the firmware for Infineon TPM chips that fixes the flaw via Windows Update.\n\n"}
{"id": "3242265", "url": "https://en.wikipedia.org/wiki?curid=3242265", "title": "Ink jet material deposition", "text": "Ink jet material deposition\n\nInk jet material deposition is an emerging manufacturing technique in which ink jet technology is used to deposit materials on substrates. The technique aims to eliminate fixed costs of production and reduce the amount of materials used.\n\nAnything that is produced using traditional printing techniques is a candidate for ink jet material deposition. In addition, the precision available with ink jet technology may be required for some industrial applications. Examples include:\n\n\n\n\n"}
{"id": "7950779", "url": "https://en.wikipedia.org/wiki?curid=7950779", "title": "Jesse Beams", "text": "Jesse Beams\n\nJesse Wakefield Beams (December 25, 1898 in Belle Plaine, Kansas – July 23, 1977) was an American physicist at the University of Virginia.\n\nBeams completed his undergraduate B.A. in physics at Fairmount College in 1921 and his master's degree the next year at the University of Wisconsin. He spent most of his academic career at the University of Virginia, where he received his Ph.D. in physics in 1925. He spent the next three years in a physics fellowship at Yale University, where he performed research on the photoelectric effect with Ernest Lawrence. Beams was appointed a professor of physics at the University of Virginia in 1929 and was chair of the department from 1948 to 1962. During World War II, he worked on the Manhattan Project, where his ultracentrifuge was used to demonstrate the separation of the uranium isotope U-235 from other isotopes. Officials in charge of the atomic bomb project concluded, however, that Beams's centrifuges were not as likely as other methods to produce enough highly enriched uranium for a bomb in the time available, and the centrifuge program was abandoned. After World War II, centrifuge separation of uranium isotopes was perfected by German scientists and engineers working in the Soviet Union. In 1953 Beams was appointed the Francis H. Smith Professor of Physics at the University of Virginia. Beams was awarded the National Medal of Science in 1967 for his work on the ultracentrifuge. He retired from the University in 1969.\n\nBeams' contributions include the first linear electron accelerator, the magnetic ultracentrifuge, and the application of the ultracentrifuge to the separation of isotopes and to the separation of viruses from liquids. He held many patents in magnetic bearings and ultracentrifuges. In addition to the National Science Medal, he was awarded the American Physical Society's John Scott Medal, the Lewis Prize of the American Philosophical Society, and the University of\nVirginia's first annual Thomas Jefferson Award.\nHe is buried at the University of Virginia Cemetery.\n\n\n\n"}
{"id": "37437983", "url": "https://en.wikipedia.org/wiki?curid=37437983", "title": "List of adhesive tapes", "text": "List of adhesive tapes\n\nThe following is a list of adhesive tapes with pressure-sensitive adhesives:\n"}
{"id": "44357780", "url": "https://en.wikipedia.org/wiki?curid=44357780", "title": "List of industry trade groups in Ukraine", "text": "List of industry trade groups in Ukraine\n\nThis is a list of known industry trade groups located in Ukraine.\n\n"}
{"id": "1859768", "url": "https://en.wikipedia.org/wiki?curid=1859768", "title": "Lock-in amplifier", "text": "Lock-in amplifier\n\nA lock-in amplifier is a type of amplifier that can extract a signal with a known carrier wave from an extremely noisy environment. Depending on the dynamic reserve of the instrument, signals up to 1 million times smaller than noise components, potentially fairly close by in frequency, can still be reliably detected. It is essentially a homodyne detector followed by low-pass filter that is often adjustable in cut-off frequency and filter order. Whereas traditional lock-in amplifiers use analog frequency mixers and RC filters for the demodulation, state-of-the-art instruments have both steps implemented by fast digital signal processing, for example, on an FPGA. Usually sine and cosine demodulation is performed simultaneously, which is sometimes also referred to as dual-phase demodulation. This allows the extraction of the in-phase and the quadrature component that can then be transferred into polar coordinates, i.e. amplitude and phase, or further processed as real and imaginary part of a complex number (e.g. for complex FFT analysis).\n\nThe device is often used to measure phase shift, even when the signals are large and of high signal-to-noise ratio and do not need further improvement.\n\nRecovering signals at low signal-to-noise ratios requires a strong, clean reference signal with the same frequency as the received signal. This is not the case in many experiments, so the instrument can recover signals buried in the noise only in a limited set of circumstances.\n\nThe lock-in amplifier is commonly believed to be invented by Princeton University physicist Robert H. Dicke who founded the company Princeton Applied Research (PAR) to market the product. However, in an interview with Martin Harwit, Dicke claims that even though he is often credited with the invention of the device, he believes that he read about it in a review of scientific equipment written by Walter C. Michels, a professor at Bryn Mawr College. This could have been a 1941 article by Michels and Curtis, which in turn cites a 1934 article by C. R. Cosens, while another timeless article was written by C. A. Stutt in 1949.\n\nThe operation of a lock-in amplifier relies on the orthogonality of sinusoidal functions. Specifically, when a sinusoidal function of frequency \"f\" is multiplied by another sinusoidal function of frequency \"f\" not equal to \"f\" and integrated over a time much longer than the period of the two functions, the result is zero. Instead, when \"f\" is equal to \"f\" and the two functions are in phase, the average value is equal to half of the product of the amplitudes.\n\nIn essence, a lock-in amplifier takes the input signal, multiplies it by the reference signal (either provided from the internal oscillator or an external source), and integrates it over a specified time, usually on the order of milliseconds to a few seconds. The resulting signal is a DC signal, where the contribution from any signal that is not at the same frequency as the reference signal is attenuated close to zero. The out-of-phase component of the signal that has the same frequency as the reference signal is also attenuated (because sine functions are orthogonal to the cosine functions of the same frequency), making a lock-in a phase-sensitive detector.\n\nFor a sine reference signal and an input waveform formula_1, the DC output signal formula_2 can be calculated for an analog lock-in amplifier as\n\nwhere \"φ\" is a phase that can be set on the lock-in (set to zero by default).\n\nIf the averaging time \"T\" is large enough (e.g. much larger than the signal period) to suppress all unwanted parts like noise and the variations at twice the reference frequency, the output is \nwhere formula_5 is the signal amplitude at the reference frequency, and formula_6 is the phase difference between the signal and reference.\n\nMany applications of the lock-in amplifier only require recovering the signal amplitude rather than relative phase to the reference signal. For a simple so called single-phase lock-in-amplifier the phase difference is adjusted (usually manually) to zero to get the full signal.\n\nMore advanced, so called two-phase lock-in-amplifiers have a second detector, doing the same calculation as before, but with an additional 90° phase shift. Thus one has two outputs: formula_7 is called the \"in-phase\" component, and formula_8 the \"quadrature\" component. These two quantities represent the signal as a vector relative to the lock-in reference oscillator. By computing the magnitude (\"R\") of the signal vector, the phase dependency is removed:\n\nThe phase can be calculated from\n\nThe majority of today's lock-in amplifiers are based on high-performance digital signal processing (DSP). Over the last 20 years, digital lock-in amplifiers have been replacing analog models across the entire frequency range, allowing users to perform measurements up to a frequency of 600 MHz. Initial problems of the first digital lock-in amplifiers, e.g. the presence of digital clock noise on the input connectors, could be completely eliminated by use of improved electronic components and better instrument design. Today's digital lock-in amplifiers outperform analog models in all relevant performance parameters, such as frequency range, input noise, stability and dynamic reserve. In addition to better performance, digital lock-in amplifiers can include multiple demodulators, which allows analyzing a signal with different filter settings or at multiple different frequencies simultaneously. Moreover, experimental data can be analyzed with additional tools such as an oscilloscope, FFT spectrum analyzers, boxcar averager or used to provide feedback by using internal PID controllers. Some models of the digital lock-in amplifiers are computer-controlled and feature a graphical user interface (can be a platform-independent browser user interface) and a choice of programming interfaces.\n\nSignal recovery takes advantage of the fact that noise is often spread over a much wider range of frequencies than the signal. In the simplest case of white noise, even if the root mean square of noise is 10 times as large as the signal to be recovered, if the bandwidth of the measurement instrument can be reduced by a factor much greater than 10 around the signal frequency, then the equipment can be relatively insensitive to the noise. In a typical 100 MHz bandwidth (e.g. an oscilloscope), a bandpass filter with width much narrower than 100 Hz would accomplish this. The averaging time of the lock-in amplifier determines the bandwidth and allows very narrow filters, less than 1 Hz if needed. However, this comes at the price of a slow response to changes in the signal.\n\nIn summary, even when noise and signal are indistinguishable in the time domain, if the signal has a definite frequency band and there is no large noise peak within that band, noise and signal can be separated sufficiently in the frequency domain.\n\nIf the signal is either slowly varying or otherwise constant (essentially a DC signal), then 1/\"f\" noise typically overwhelms the signal. It may then be necessary to use external means to modulate the signal. For example, when detecting a small light signal against a bright background, the signal can be modulated either by a chopper wheel, acousto-optical modulator, photoelastic modulator at a large enough frequency so that 1/\"f\" noise drops off significantly, and the lock-in amplifier is referenced to the operating frequency of the modulator. In the case of an atomic-force microscope, to achieve nanometer and piconewton resolution, the cantilever position is modulated at a high frequency, to which the lock-in amplifier is again referenced.\n\nWhen the lock-in technique is applied, care must be taken to calibrate the signal, because lock-in amplifiers generally detect only the root-mean-square signal of the operating frequency. For a sinusoidal modulation, this would introduce a factor of formula_11 between the lock-in amplifier output and the peak amplitude of the signal, and a different factor for non-sinusoidal modulation.\n\nIn the case of nonlinear systems, higher harmonics of the modulation frequency appear. A simple example is the light of a conventional light bulb being modulated at twice the line frequency. Some lock-in amplifiers also allow separate measurements of these higher harmonics.\n\nFurthermore, the response width (effective bandwidth) of detected signal depends on the amplitude of the modulation. Generally, linewidth/modulation function has a monotonically increasing, non-linear behavior.\n\n\n"}
{"id": "11960395", "url": "https://en.wikipedia.org/wiki?curid=11960395", "title": "Manual fare collection", "text": "Manual fare collection\n\nManual fare collection is the practice of collecting fares manually (without the aid of an automated machine). \"Fare collection\" generally refers to the collection of fares in the transport industry in return for a ticket or passes to travel. Commonly used on buses and train transport systems (in the UK; in Poland, for example, buying and validating the tickets by machine is the passenger's task; the passengers enter the bus through any of the doors and buying a ticket from the driver is an option where there is no automatic selling machine or if somebody forgets to buy a ticket before), manual fare collection is increasingly becoming obsolete with the introduction of smart cards such as the Transport for London 'Oyster card'. However, in the face of this trend, some companies have opted to retain more traditional methods of manual fare collection to both save money (automatic equipment is expensive) and ensure reliability. In the United Kingdom, examples of this can be seen on the Transport for London Heritage lines and the FirstGroup FTR routes in York, Leeds, Luton, and Swansea where bus conductors (dubbed 'customer hosts') have returned to work. The other reason(with lowering prices of electronics, and in most cases need to buy it once) may be for Heritage routes - tradition \"look\", for other routes(because of quite high monthly labor cost in UK) agreements and strong unions with the tries from politics to lower the unemployment rate by making overworking in public service.\n\nA range of fare collection equipment has been developed over the last century in the United Kingdom.\n\nA conductor may carry a cash bag in which cash is kept, thus making it easier while issuing tickets. \n\nPerhaps the biggest development in manual fare collection is the coin dispenser, distributed by Jacques L. Galef. Mounted either in a driver's cab or on the belt of a conductor, the coin dispenser usually takes the form of a number of tubes fitted in a line together. Each tube holds a different denomination and tends to have some sort of trigger which will release the coin from the bottom. In Britain, the Quick-Change and Pendamatic units, for example, has labelled plastic funnels at the top, which filter the coins into the tube. A trigger on the front of the machine then releases the coin by pushing a kicker, which holds the coin, forward in a pivotal motion. Other models in Britain, the Cambist and Metro Coin Dispensers, works on a similar principle but also have the option to be attached to the fare collection table allowing the operator (usually the bus driver) to simply slide the coins into the respective tubes. Generally, coin dispensers in the UK are configured with the £1 coin to the left (as the machine faces you), then the 50p, 20p, 10p, 5p, 2p, 1p. However, some operators have customised their dispensers to better suit their individual needs.\n\nOther fare collecting equipment includes tender trays which can be fitted to bus driver doors to allow the customer to put the fare down. These are common on most buses in the United Kingdom now, since they facilitate quick payment and also allow for the driver to have a screen protecting his cab, yet still securely collect change (the tray is placed with a small gap above it to allow room for the drivers hand to pass through).\n\n"}
{"id": "49419499", "url": "https://en.wikipedia.org/wiki?curid=49419499", "title": "Muffin rings", "text": "Muffin rings\n\nMuffin rings are metal cookware used for oven-baking or griddle-cooking muffins or English muffins. Muffin rings are circle-shaped objects made of thin metal. The rings are about one inch high. Batter is poured into the rings, which are placed on a griddle. The griddle is then baked in an oven. The muffins and the ring are turned over midway through the baking to bake the other side of the muffin.\n\n\"Dollar Monthly Magazine\" in 1864 states that muffin rings were used with griddles. First the griddle was heated and greased with pork fat.An 1861 article states that once the yeast-based dough was prepared, it was molded, allowed to sit for a while, and then placed in buttered muffin rings which were placed on the griddle. \"Godey's Lady's Book and Magazine\" states that the muffins and rings were turned over with a thin tool resembling a palette knife. The American author Parloa states that muffin pans, cups or tins should be used rather than muffin rings, since at the time she was writing, muffin rings had faded in popularity.\n"}
{"id": "31687146", "url": "https://en.wikipedia.org/wiki?curid=31687146", "title": "Naval arms race", "text": "Naval arms race\n\nA naval arms race is when two or more countries continuously construct warships that are consistently more powerful than warships built by the other country built in the previous years. These races often lead to high tension and near-wars, if not outright conflict.\n\nExamples include:\n"}
{"id": "1705172", "url": "https://en.wikipedia.org/wiki?curid=1705172", "title": "Nyotaimori", "text": "Nyotaimori\n\n, often referred to as \"body sushi\", is the Japanese practice of serving sashimi or sushi from the naked body of a woman. refers to the same practice using a male model. \n\nThe Japanese practice of \"nyotaimori\" – serving sushi on a naked body – is said to have its origins in the samurai period in Japan. In the words of chef Mike Keenan, \"The naked sushi idea began during the samurai period in Japan. It was a subculture to the geishas. It would take place in a geisha house as a celebration after a victorious battle.\"\n\nNyotaimori originated in Ishikawa Prefecture and continues to be practiced there.\n\nIn traditional \"nyotaimori\", the model is generally expected to lie still at all times and not talk with guests. The sushi is placed on sanitized leaves on the model's body to prevent skin-to-fish contact and on sufficiently flat areas of the body off which the sushi will not roll. \"Nyotaimori\" is considered an art form.\n\nUsually champagne and sake are served in naked sushi restaurants. Guests must be respectful and observe the strictest decorum. Talking with the models is highly discouraged. Inappropriate gestures or comments are not tolerated and diners can only pick up sushi with chopsticks, although rules in some restaurants are less strict. For example, in some restaurants guests can nibble nori rolls off nipples if they choose.\n\nThe practice has been described as decadent, humiliating, cruel, and objectifying. \"Guardian\" columnist Julie Bindel notes that the woman being used to serve the food, on at least one occasion in London, looked \"as if in a morgue, awaiting a postmortem\". It has received popularity in Japanese organized crime.\n\nWorldwide reception varies as several countries have banned the practice. In 2005, China has outlawed nyotaimori on naked bodies, condemning it due to public health reasons and moral issues. In Hong Kong, organizers of a brunch event with nyotaimori have met with backlash from the public, as they were accused of sexism under the pretense of art. The nyotaimori was subsequently cancelled for the then-upcoming event.\n\nThe birthday party of South African entrepreneur Kenny Kunene on 21 October 2010, which hosted African National Congress Youth League president Julius Malema and featured \"nyotaimori\", was criticised by Congress of South African Trade Unions secretary general Zwelinzima Vavi, leading to a political row. The ANC Women's League condemned \"nyotaimori\" at Kunene's party as an attack on the bodily integrity and dignity of women in South Africa.\n\n\n"}
{"id": "8651817", "url": "https://en.wikipedia.org/wiki?curid=8651817", "title": "Pastry brush", "text": "Pastry brush\n\nA pastry brush, also known as a basting brush, is a cooking utensil used to spread butter, oil or glaze on food. Traditional pastry brushes are made with natural bristles or a plastic or nylon fiber similar to a paint brush, while modern kitchen brushes may have silicone bristles. In baking breads and pastries, a pastry brush is used to spread a glaze or egg wash on the crust or surface of the food.\nIn roasting meats, a pastry brush may be used to sop up juices or drippings from under pan and spread them on the surface of the meat to crisp the skin.\n\n"}
{"id": "15671531", "url": "https://en.wikipedia.org/wiki?curid=15671531", "title": "Pauline Gower", "text": "Pauline Gower\n\nPauline Mary de Peauly Gower Fahie (22 July 1910 – 2 March 1947) was a British pilot and writer who established the women's branch of the Air Transport Auxiliary during the Second World War.\n\nDaughter of MP Sir Robert Gower, and educated at Beechwood Sacred Heart School, she first flew with Alan Cobham and was fascinated by flying. Gower met Dorothy Spicer at the London Aeroplane Club at Stag Lane Aerodrome and they became friends. In August 1931 they established a joy-riding and air taxi service in Kent. Gower was licensed to carry passengers for 'hire or reward', and Spicer was qualified as a ground engineer and held an 'A' (private) pilot's licence. They hired a plane and later bought a Gypsy Moth for the business, but struggled to make a living so decided to join the Crimson Fleet air circus and later the British Hospitals' air pageant. \n\nIn 1932, to support British Hospitals, they toured the country with an Air Circus, giving air pageants in 200 towns. They joined the Aeronautical Section of the Women's Engineering Society in 1932. Gower also wrote for \"Girl's Own Paper\" and \"Chatterbox\" and published a collection of poetry, \"Piffling Poems for Pilots\", in 1934. As a writer she was acquainted with W. E. Johns whose character Worrals was based on herself as well as Amy Johnson.\n\nIn 1935 she was appointed as a council member for the Women's Engineering Society. She chaired a meeting on \"The History of British Airships\", where Mr. M. Langley championed the airboat and Hon. A. F. de Moleyns the airship. In 1936, Gower was the first woman to be awarded the Air Ministry's Second Class Navigator's Licence. Later that year, Gower and her colleague Dorothy Spicer ('daring aeronauts') presented a technical paper at the Women's Engineering Society Annual General Meeting on the treatment of metals for aircraft engineers. \nIn 1938, she was appointed a civil defence commissioner in London with the Civil Air Guard. That year her work on women in aviation—\"Women with Wings\"—was published. On the outbreak of the Second World War, Gower made use of her high-level connections to propose the establishment of a women's section in the new Air Transport Auxiliary —the ATA would be responsible for ferrying military aircraft from factory or repair facility to storage unit or operational unit—to the authorities.\n\nGower was appointed as the head of the women's branch, and commenced the selection and testing of women pilots, the first eight being appointed by the ATA on 1 January 1940. Early members included ice-hockey international Mona Friedlander, Margaret Fairweather (Lord Runciman's daughter) and former ballet dancer Rona Rees. Later members included Amy Johnson and former Olympic skier Lois Butler. Gower received the MBE for her services in 1942 and received a Harmon Trophy award posthumously in 1950.\nGower married Wing Commander Bill Fahie in 1945. She died in 1947 giving birth to twin sons, who survived.\n\n\n"}
{"id": "903791", "url": "https://en.wikipedia.org/wiki?curid=903791", "title": "Persian drill", "text": "Persian drill\n\nA Persian drill is a drill which is turned by pushing a nut back and forth along a spirally grooved drill holder. It was formerly used for delicate operations such as jewellery making and dentistry. A ratcheting screwdriver with a 'spiral ratchet' mechanism may be used as a Persian drill.\n"}
{"id": "15990281", "url": "https://en.wikipedia.org/wiki?curid=15990281", "title": "Post-and-plank", "text": "Post-and-plank\n\nThe method of building wooden buildings with a traditional timber frame with horizontal plank or log infill has many names, the most common of which are piece sur piece (French. Also used to describe log building), corner post construction, post-and-plank, standerbohlenbau (German) and skiftesverk (Swedish). This traditional building method is believed to be the predecessor to half-timber construction widely known by its German name \"fachwerkbau\" which has wall infill of wattle and daub, brick, or stone. This carpentry was used from parts of Scandinavia to Switzerland to western Russia. Though relatively rare now, two types are found in a number of regions in North America, more common are the walls with planks or timbers which slide in a groove in the posts and less common is a type where horizontal logs are tenoned into individual mortises in the posts. This method is not the same as the plank-frame buildings in North America with vertical plank walls.\n\n\n\"The support of horizontal timbers by corner posts is an old form of construction in Europe. It was apparently carried across much of the continent from Silesia by the Lausitz urnfield culture in the late Bronze Age.\" The Lausitz culture is also known as the Lusatian culture and within their territory is an archaeological site and archaeological open-air museum at Biskupin, Poland, where remnants of such structures were found and reconstructed. The structures found dated from 747–722 B.C and are similar in concept to piece sur piece construction. This historic carpentry is known in southern Sweden (skiftesverk), particularly Gotland where it is also known as \"bulhus\", Germany, Poland, Hungary, Lithuania, Selsia, Switzerland, Austria.\n\nSome researchers believe this building method was introduced to the United States by Alpine-Alemannic Germans or Swiss, and to by French fur trappers working for the Hudson's Bay Company. And, Others, who have studied the development house building in New France believe that the method was developed endemically in Canada as a local adaption of the half-timbered house, spreading from Québec to the Pacific through the Hudson's Bay Company. The Hudson's Bay Company adopted this style for most of its outposts all the way to the Pacific coast.\n\nSome examples of surviving houses of this structural type are the circa 1809 Cray House in Stevensville, Maryland, 1832 Jacob Highbarger House in Maryland, and the George Diehl Homestead.\n\nRed River Frame was a popular name for the post-and-plank construction technique used in the Red River Colony in the 19th Century. The building style was characterized by a dressed timber structure with a horizontal log infill. The spaces between the logs were filled or 'chinked' with clay and straw. The exterior would either be whitewashed with a limestone/water plaster mixture, or in later years, the exterior would be covered by board siding. This style was popular because it could use smaller trees for logs—the longest trees needed were for the vertical logs. The Farm Manager's House at Lower Fort Garry, the William Brown House at the Historical Museum of St James—Assiniboia, the historical Fur Warehouse at Fort St. James National Historic Site of Canada and Riel House in Winnipeg, Manitoba are excellent examples of Red River Frame construction.\n\nIn southeastern Pennsylvania, numerous log houses feature corner post construction. In many cases, these houses feature diagonal bracing that resembles half-timbered architecture of Europe. In Lancaster County, Pennsylvania, it is estimated that about a quarter of log houses are corner post construction.\n\n"}
{"id": "4671692", "url": "https://en.wikipedia.org/wiki?curid=4671692", "title": "Power electronic substrate", "text": "Power electronic substrate\n\nThe role of the substrate in power electronics is to provide the interconnections to form an electric circuit (like a printed circuit board), and to cool the components. Compared to materials and techniques used in lower power microelectronics, these substrates must carry higher currents and provide a higher voltage isolation (up to several thousand volts). They also must operate over a wide temperature range (up to 150 or 200 °C).\n\nDirect bonded copper (DBC) substrates are commonly used in power modules, because of their very good thermal conductivity. They are composed of a ceramic tile (commonly alumina) with a sheet of copper bonded to one or both sides by a high-temperature oxidation process (the copper and substrate are heated to a carefully controlled temperature in an atmosphere of nitrogen containing about 30 ppm of oxygen; under these conditions, a copper-oxygen eutectic forms which bonds successfully both to copper and the oxides used as substrates). The top copper layer can be preformed prior to firing or chemically etched using printed circuit board technology to form an electrical circuit, while the bottom copper layer is usually kept plain. The substrate is attached to a heat spreader by soldering the bottom copper layer to it.\n\nCeramic material used in DBC include:\n\nOne of the main advantages of the DBC substrates is their low coefficient of thermal expansion, which is close to that of silicon (compared to pure copper). This ensures good thermal cycling performances (up to 50,000 cycles). The DBC substrates also have excellent electrical insulation and good heat spreading characteristics.\n\nA related technique uses a seed layer, photoimaging, and then additional copper plating to allow for fine lines (as small as 50 micrometres) and through-vias to connect front and back sides. This can be combined with polymer-based circuits to create high density substrates that eliminate the need for direct connection of power devices to heat sinks.\n\nAnother technology to attach thick metal layers to ceramic plates is the AMB (active metal braze) technology. With this process a metal foil is soldered to the ceramic using als solder paste and high temperature (800 °C – 1000 °C). The process itself requires vacuum. Therefore, though AMB is electrically very similar to DBC, it is only suited for small production lots.\n\nInsulated metal substrate (IMS) consists of a metal baseplate (aluminium is commonly used because of its low cost and density) covered by a thin layer of dielectric (usually an epoxy-based layer) and a layer of copper (35 µm to more than 200 µm thick). The FR-4-based dielectric is usually thin (about 100 μm) because it has poor thermal conductivity compared to the ceramics used in DBC substrates.\n\nDue to its structure, the IMS is a single-sided substrate, i.e. it can only accommodate components on the copper side. In most applications, the baseplate is attached to a heatsink to provide cooling, usually using thermal grease and screws. Some IMS substrates are available with a copper baseplate for better thermal performances.\n\nCompared to a classical printed circuit board, the IMS provides a better heat dissipation. It is one of the simplest way to provide efficient cooling to surface mount components.\n\n\n\n"}
{"id": "23104597", "url": "https://en.wikipedia.org/wiki?curid=23104597", "title": "Sigma (couch)", "text": "Sigma (couch)\n\nThe Ancient Roman furniture, sigma was a semi-circular couch sometimes used at banquets instead of the \"triclinium\". Its name comes from the lunate sigma (upper case C, lower case ϲ) — which resembles, but which is not at all related to, the Latin letter C and was used in Eastern forms of Greek writing and in the Middle Ages.\n"}
{"id": "35670957", "url": "https://en.wikipedia.org/wiki?curid=35670957", "title": "Sovereign Wealth Fund Institute", "text": "Sovereign Wealth Fund Institute\n\nThe Sovereign Wealth Fund Institute or SWF Institute, or SWFI, is a global corporation analyzing public asset owners such as sovereign wealth funds and other long-term governmental investors. Initially, the Sovereign Wealth Fund Institute focused solely on sovereign wealth funds. It has branched out to cover all types of public institutional investors. The institute is a financial data vendor but provides information to the media as well. It was founded by Michael Maduell and Carl Linaburg in late 2007.\n\nSWFI sells data subscriptions to asset managers, banks, researchers, universities, governments, institutional investors, asset owners, corporations, law firms and other entities.\n\nThe SWFI came out with the Linaburg-Maduell Transparency Index in 2008. It is a 10-point scale based on ten principles of transparency, each adding one point to the index rating. The index is used by sovereign wealth funds in their annual reports.\n\nThe SWFI tracks direct deals in sovereign wealth funds. The value of global direct deals by sovereign-wealth funds hit $50.02 billion in the first half of 2014.\n\nSWFI tracks the assets of sovereign wealth funds, pensions, endowments and other asset owners. Norway's sovereign wealth fund crossed $1 trillion in assets in 2017.\n"}
{"id": "40431495", "url": "https://en.wikipedia.org/wiki?curid=40431495", "title": "Ulla Mitzdorf", "text": "Ulla Mitzdorf\n\nUlla Mitzdorf (March 15, 1944 – July 19, 2013) was a German scientist. She contributed to diverse areas including physics, chemistry, psychology, physiology, medicine and gender studies.\n\nMitzdorf gained her doctorate in 1974 at the Technical University Munich in theoretical chemistry. Subsequently she worked as scholar at the Max-Planck Institute of Psychiatry in Munich. In 1983 she habilitated in physiology, and in 1984 in medical psychology and neurobiology at the Ludwig Maximilian University of Munich.\n\nFrom 1988 to 2009 she was Fiebiger Professor for medical psychology at the Ludwig Maximilan University. Simultaneously, she was from 2000 to 2006 women's affairs officer and spokeswoman of the state conference of women and gender equality officers in Bavarian universities.\n\nMitzdorf died after a short illness on July 19, 2013, aged 69.\n\n\n"}
{"id": "4280837", "url": "https://en.wikipedia.org/wiki?curid=4280837", "title": "Video-signal generator", "text": "Video-signal generator\n\nA video signal generator is a type of signal generator which outputs predetermined video and/or television oscillation waveforms, and other signals used in the synchronization of television devices and to stimulate faults in, or aid in parametric measurements of, television and video systems. There are several different types of video signal generators in widespread use. Regardless of the specific type, the output of a video generator will generally contain synchronization signals appropriate for television, including horizontal and vertical sync pulses (in analog) or sync words (in digital). Generators of \"composite\" video signals (such as NTSC and PAL) will also include a colorburst signal as part of the output.\n\nVideo signal generators are primarily classed according to their function. In addition, they may be classified according to the video formats and interface standard they support—one generator may generate composite analog signals (typically NTSC, PAL, or both), another may generate CCIR 601, and a third may generate MPEG streams over an ASI.\n\nMany manufacturers sell signal generation platforms, which can be populated with multiple modules providing the above capabilities (and supporting different formats). Many such platforms also include audio generation capability (as television includes audio as well as video), supporting either embedded audio or standalone audio formats.\n\nA test signal generator generates test patterns, and other useful test signals, for troubleshooting and analyzing television systems. These devices are generally intended for off-line use (test patterns are seldom broadcast, unless a station is not operating properly or is off the air at the time), as they output complete television signals. \nExamples of signals output by such a device include:\n\nA few specialized signals are used in digital environments:\n\n\nIn addition, sophisticated signal generators may allow modification of the video timing, adjustment of the gains of the various components (including out of range), the introduction of jitter or bit errors (into digital signals), the introduction of motion, or other effects.\n\nA vertical interval test signal inserter, or VITS inserter inserts test patterns into the vertical interval of a television signal. Unlike test signal generators; a VITS inserter is used to insert the test signal into live programming, so that inline measurements of a transmission chain can be made while the chain is operational. (As the vertical interval is typically not visible on end-user televisions, this can be done without producing any artifacts noticeable to viewers). Since VITS signals can often be transmitted, it is also possible for a television station to receive its own on-air feed, and use the VITS to detect and troubleshoot problems in on-air transmission.\n\nA sync pulse generator is a special type of generator which produces synchronization signals, with a high level of stability and accuracy. These devices are used to provide a master timing source for a video facility. The output of an SPG will typically be in one of several forms, depending on the needs of the facility:\n\nLogo inserters are devices used to insert a television station's logo, or other fixed graphics, into a live television signal. Often called a \"Bug Generator.\"\n\n\n"}
{"id": "1111473", "url": "https://en.wikipedia.org/wiki?curid=1111473", "title": "Well logging", "text": "Well logging\n\nWell logging, also known as borehole logging is the practice of making a detailed record (a \"well log\") of the geologic formations penetrated by a borehole. The log may be based either on visual inspection of samples brought to the surface (\"geological\" logs) or on physical measurements made by instruments lowered into the hole(\"geophysical\" logs). Some types of geophysical well logs can be done during any phase of a well's history: drilling, completing, producing, or abandoning. Well logging is performed in boreholes drilled for the oil and gas, groundwater, mineral and geothermal exploration, as well as part of environmental and geotechnical studies.\n\nThe oil and gas industry uses wireline logging to obtain a continuous record of a formation's rock properties. Wireline logging can be defined as being \"The acquisition and analysis of geophysical data performed as a function of well bore depth, together with the provision of related services.\" Note that \"wireline logging\" and \"mud logging\" are not the same, yet are closely linked through the integration of the data sets. The measurements are made referenced to \"TAH\" - True Along Hole depth: these and the associated analysis can then be used to infer further properties, such as hydrocarbon saturation and formation pressure, and to make further drilling and production decisions.\n\nWireline logging is performed by lowering a 'logging tool' - or a string of one or more instruments - on the end of a wireline into an oil well (or borehole) and recording petrophysical properties using a variety of sensors. Logging tools developed over the years measure the natural gamma ray, electrical, acoustic, stimulated radioactive responses, electromagnetic, nuclear magnetic resonance, pressure and other properties of the rocks and their contained fluids. For this article, they are broadly broken down by the main property that they respond to.\n\nThe data itself is recorded either at surface (real-time mode), or in the hole (memory mode) to an electronic data format and then either a printed record or electronic presentation called a \"well log\" is provided to the client, along with an electronic copy of the raw data. Well logging operations can either be performed during the drilling process (see Logging While Drilling), to provide real-time information about the formations being penetrated by the borehole, or once the well has reached Total Depth and the whole depth of the borehole can be logged.\n\nReal-time data is recorded directly against measured cable depth. Memory data is recorded against time, and then depth data is simultaneously measured against time. The two data sets are then merged using the common time base to create an instrument response versus depth log. Memory recorded depth can also be corrected in exactly the same way as real-time corrections are made, so there should be no difference in the attainable TAH accuracy.\n\nThe measured cable depth can be derived from a number of different measurements, but is usually either recorded based on a calibrated wheel counter, or (more accurately) using magnetic marks which provide calibrated increments of cable length. The measurements made must then be corrected for elastic stretch and temperature.\n\nThere are many types of wireline logs and they can be categorized either by their function or by the technology that they use. \"Open hole logs\" are run before the oil or gas well is lined with pipe or cased. \"Cased hole logs\" are run after the well is lined with casing or production pipe.\n\nWireline logs can be divided into broad categories based on the physical properties measured.\n\nConrad and Marcel Schlumberger, who founded Schlumberger Limited in 1926, are considered the inventors of electric well logging. Conrad developed the Schlumberger array, which was a technique for prospecting for metal ore deposits, and the brothers adapted that surface technique to subsurface applications. On September 5, 1927, a crew working for Schlumberger lowered an electric sonde or tool down a well in Pechelbronn, Alsace, France creating the first well log. In modern terms, the first log was a resistivity log that could be described as 3.5-meter upside-down lateral log.\n\nIn 1931, Henri George Doll and G. Dechatre, working for Schlumberger, discovered that the galvanometer wiggled even when no current was being passed through the logging cables down in the well. This led to the discovery of the spontaneous potential (SP) which was as important as the ability to measure resistivity. The SP effect was produced naturally by the borehole mud at the boundaries of permeable beds. By simultaneously recording SP and resistivity, loggers could distinguish between permeable oil-bearing beds and impermeable nonproducing beds.\n\nIn 1940, Schlumberger invented the spontaneous potential dipmeter; this instrument allowed the calculation of the dip and direction of the dip of a layer. The basic dipmeter was later enhanced by the resistivity dipmeter (1947) and the continuous resistivity dipmeter (1952).\n\nOil-based mud (OBM) was first used in Rangely Field, Colorado in 1948. Normal electric logs require a conductive or water-based mud, but OBMs are nonconductive. The solution to this problem was the induction log, developed in the late 1940s.\n\nThe introduction of the transistor and integrated circuits in the 1960s made electric logs vastly more reliable. Computerization allowed much faster log processing, and dramatically expanded log data-gathering capacity. The 1970s brought more logs and computers. These included combo type logs where resistivity logs and porosity logs were recorded in one pass in the borehole.\n\nThe two types of porosity logs (acoustic logs and nuclear logs) date originally from the 1940s. Sonic logs grew out of technology developed during World War II. Nuclear logging has supplemented acoustic logging, but acoustic or sonic logs are still run on some combination logging tools.\n\nNuclear logging was initially developed to measure the natural gamma radiation emitted by underground formations. However, the industry quickly moved to logs that actively bombard rocks with nuclear particles. The gamma ray log, measuring the natural radioactivity, was introduced by Well Surveys Inc. in 1939, and the WSI neutron log came in 1941. The gamma ray log is particularly useful as shale beds which often provide a relatively low permeability cap over hydrocarbon reservoirs usually display a higher level of gamma radiation. These logs were important because they can be used in cased wells (wells with production casing). WSI quickly became part of Lane-Wells. During World War II, the US Government gave a near wartime monopoly on open-hole logging to Schlumberger, and a monopoly on cased-hole logging to Lane-Wells. Nuclear logs continued to evolve after the war.\n\nAfter the discovery of nuclear magnetic resonance by Bloch and Purcell in 1946, the nuclear magnetic resonance log using the Earth's field was developed in the early 1950s by Chevron and Schlumberger. The NMR log was a scientific success but an engineering failure. More recent engineering developments by NUMAR (a subsidiary of Halliburton) in the 1990s has resulted in continuous NMR logging technology which is now applied in the oil and gas, water and metal exploration industry.\n\nMany modern oil and gas wells are drilled directionally. At first, loggers had to run their tools somehow attached to the drill pipe if the well was not vertical. Modern techniques now permit continuous information at the surface. This is known as logging while drilling (LWD) or measurement-while-drilling (MWD). MWD logs use mud pulse technology to transmit data from the tools on the bottom of the drillstring to the processors at the surface.\n\nResistivity logging measures the subsurface electrical resistivity, which is the ability to impede the flow of electric current. This helps to differentiate between formations filled with salty waters (good conductors of electricity) and those filled with hydrocarbons (poor conductors of electricity). Resistivity and porosity measurements are used to calculate water saturation. Resistivity is expressed in ohms or ohms/meter, and is frequently charted on a logarithm scale versus depth because of the large range of resistivity. The distance from the borehole penetrated by the current varies with the tool, from a few centimeters to one meter.\n\nThe term \"borehole imaging\" refers to those logging and data-processing methods that are used to produce centimeter-scale images of the borehole wall and the rocks that make it up. The context is, therefore, that of open hole, but some of the tools are closely related to their cased-hole equivalents. Borehole imaging has been one of the most rapidly advancing technologies in wireline well logging. The applications range from detailed reservoir description through reservoir performance to enhanced hydrocarbon recovery. Specific applications are fracture identification, analysis of small-scale sedimentological features, evaluation of net pay in thinly bedded formations, and the identification of breakouts (irregularities in the borehole wall that are aligned with the minimum horizontal stress and appear where stresses around the wellbore exceed the compressive strength of the rock).\nThe subject area can be classified into four parts:\n\nPorosity logs measure the fraction or percentage of pore volume in a volume of rock. Most porosity logs use either acoustic or nuclear technology. Acoustic logs measure characteristics of sound waves propagated through the well-bore environment. Nuclear logs utilize nuclear reactions that take place in the downhole logging instrument or in the formation. Nuclear logs include density logs and neutron logs, as well as gamma ray logs which are used for correlation.\n\nThe density log measures the bulk density of a formation by bombarding it with a radioactive source and measuring the resulting gamma ray count after the effects of Compton Scattering and Photoelectric absorption. This bulk density can then be used to determine porosity.\n\nThe neutron porosity log works by bombarding a formation with high energy epithermal neutrons that lose energy through elastic scattering to near thermal levels before being absorbed by the nuclei of the formation atoms. Depending on the particular type of neutron logging tool, either the gamma ray of capture, scattered thermal neutrons or scattered, higher energy epithermal neutrons are detected. The neutron porosity log is predominantly sensitive to the quantity of hydrogen atoms in a particular formation, which generally corresponds to rock porosity.\n\nBoron is known to cause anomalously low neutron tool count rates due to it having a high capture cross section for thermal neutron absorption. An increase in hydrogen concentration in clay minerals has a similar effect on the count rate.\n\nA sonic log provides a formation interval transit time, which typically a function of lithology and rock texture but particularly porosity. The logging tool consists of a piezoelectric transmitter and receiver and the time taken to for the sound wave to travel the fixed distance between the two is recorded as an \"interval transit time\".\n\nA log of the natural radioactivity of the formation along the borehole, measured in API units, particularly useful for distinguishing between sands and shales in a siliclastic environment. This is because sandstones are usually nonradioactive quartz, whereas shales are naturally radioactive due to potassium isotopes in clays, and adsorbed uranium and thorium.\n\nIn some rocks, and in particular in carbonate rocks, the contribution from uranium can be large and erratic, and can cause the carbonate to be mistaken for a shale. In this case, The carbonate gamma ray is a better indicator of shaliness. the carbonate gamma ray log is a gamma ray log from which the uranium contribution has been subtracted.\n\nThe Spontaneous Potential (SP) log measures the natural or spontaneous potential difference between the borehole and the surface, without any applied current. It was one of the first wireline logs to be developed, found when a single potential electrode was lowered into a well and a potential was measured relative to a fixed reference electrode at the surface.\n\nThe most useful component of this potential difference is the electrochemical potential because it can cause a significant deflection in the SP response opposite permeable beds. The magnitude of this deflection depends mainly on the salinity contrast between the drilling mud and the formation water, and the clay content of the permeable bed. Therefore, the SP log is commonly used to detect permeable beds and to estimate clay content and formation water salinity.\nThe SP log can be used to distinguish between impermeable shale and permeable shale and porous sands.\n\nA tool that measures the diameter of the borehole, using either 2 or 4 arms. It can be used to detect regions where the borehole walls are compromised and the well logs may be less reliable.\n\nNuclear magnetic resonance (NMR) logging uses the NMR response of a formation to directly determine its porosity and permeability, providing a continuous record along the length of the borehole.\nThe chief application of the NMR tool is to determine moveable fluid volume (BVM) of a rock. This is the pore space excluding clay bound water (CBW) and irreducible water (BVI). Neither of these are moveable in the NMR sense, so these volumes are not easily observed on older logs. On modern tools, both CBW and BVI can often be seen in the signal response after transforming the relaxation curve to the porosity domain. Note that some of the moveable fluids (BVM) in the NMR sense are not actually moveable in the oilfield sense of the word. Residual oil and gas, heavy oil, and bitumen may appear moveable to the NMR precession measurement, but these will not necessarily flow into a well bore.\n\nSpectral noise logging (SNL) is an acoustic noise measuring technique used in oil and gas wells for well integrity analysis, identification of production and injection intervals and hydrodynamic characterisation of the reservoir. SNL records acoustic noise generated by fluid or gas flow through the reservoir or leaks in downhole well components.\n\nNoise logging tools have been used in the petroleum industry for several decades. As far back as 1955, an acoustic detector was proposed for use in well integrity analysis to identify casing holes.\nOver many years, downhole noise logging tools proved effective in inflow and injectivity profiling of operating wells, leak detection, location of cross-flows behind casing, and even in determining reservoir fluid compositions. Robinson (1974) described how noise logging can be used to determine effective reservoir thickness.\n\nIn the 1970s, a new approach to wireline logging was introduced in the form of logging while drilling (LWD). This technique provides similar well information to conventional wireline logging but instead of sensors being lowered into the well at the end of wireline cable, the sensors are integrated into the drill string and the measurements are made in real-time, whilst the well is being drilled. This allows drilling engineers and geologists to quickly obtain information such as porosity, resistivity, hole direction and weight-on-bit and they can use this information to make immediate decisions about the future of the well and the direction of drilling.\nIn LWD, measured data is transmitted to the surface in real time via pressure pulses in the well's mud fluid column. This mud telemetry method provides a bandwidth of less than 10 bits per second, although, as drilling through rock is a fairly slow process, data compression techniques mean that this is an ample bandwidth for real-time delivery of information. A higher sample rate of data is recorded into memory and retrieved when the drillstring is withdrawn at bit changes. High-definition downhole and subsurface information is available through networked or wired drillpipe that deliver memory quality data in real time.\n\nThroughout the life of the wells, integrity controles of the steel and cemented column (casing and tubing) are performed using calipers and thickness gauges. These advanced technical methods use non destructive technologies as ultrasonic, electromagnetic and magnetic transducers.\n\nThis method of data acquisition involves recording the sensor data into a down hole memory, rather than transmitting \"Real Time\" to surface. There are some advantages and disadvantages to this memory option.\n\nCoring is the process of obtaining an actual sample of a rock formation from the borehole. There are two main types of coring: 'full coring', in which a sample of rock is obtained using a specialised drill-bit as the borehole is first penetrating the formation and 'sidewall coring', in which multiple samples are obtained from the side of the borehole after it has penetrated through a formation. The main advantage of sidewall coring over full coring are that it is cheaper (drilling doesn't have to be stopped) and multiple samples can be easily acquired, with the main disadvantages being that there can be uncertainty in the depth at which the sample was acquired and the tool can fail to acquire the sample.\n\n\"Mud logs\" are well logs prepared by describing rock or soil cuttings brought to the surface by mud circulating in the borehole. In the oil industry they are usually prepared by a mud logging company contracted by the operating company. One parameter a typical mud log displays is the formation gas (gas units or ppm). \"The gas recorder usually is scaled in terms of arbitrary gas units, which are defined differently by the various gas-detector manufactures. In practice, significance is placed only on relative changes in the gas concentrations detected.\" The current oil industry standard mud log normally includes real-time drilling parameters such as rate of penetration (ROP), lithology, gas hydrocarbons, flow line temperature (temperature of the drilling fluid) and chlorides but may also include mud weight, estimated pore pressure and corrected d-exponent (corrected drilling exponent) for a pressure pack log. Other information that is normally notated on a mud log include directional data (deviation surveys), weight on bit, rotary speed, pump pressure, pump rate, viscosity, drill bit info, casing shoe depths, formation tops, mud pump info, to name just a few.\n\nIn the oil industry, the well and mud logs are usually transferred in 'real time' to the operating company, which uses these logs to make operational decisions about the well, to correlate formation depths with surrounding wells, and to make interpretations about the quantity and quality of hydrocarbons present. Specialists involved in well log interpretation are called log analysts.\n\n\n"}
{"id": "48968962", "url": "https://en.wikipedia.org/wiki?curid=48968962", "title": "Wireless Communications of the German Army in World War II", "text": "Wireless Communications of the German Army in World War II\n\nDuring World War II, the German Army relied on an diverse array of communications to maintain contact with its mobile forces and in particular with its armoured forces. Most of this equipment received the generic prefix FuG for Funkgerät, meaning \"radio device\". Occasionally the shorted Fu designation were used and there were exceptions to both these systems. Number ranges were not unique across the services so sometimes different equipment used by different services had the same FuG prefix. This article is a list and a description of the radio equipment.\n\nFuG 2: A high-band HF/low-band VHF receiver. It operated in the 27,000 to 33,3000  kHz (27-33.3 MHz) range. The FuG 2 was never used on its own but as an additional receiver in command tanks and relay devices. It was usually installed in Section leader and company commanders vehicles, to allow then to listen on one frequency while transmitting and receiving on another. As it operated on the same band as the FuG 5 it allowed them, for example,to listen to the regimental command net while talking to the subbornate units at the same time. Matching the transmitters that operated in this frequency range, this receiver provided for 50 kHz channel steps in the 27.0 to 33.3 MHz range for a total of 125 available channels. \n\nFuG 4: A medium wave receiver used in command tanks. It operated in the 1,130 to 3,000  kHz frequency range. It was used with the same antenna as the Fug 8. It was usually used for communication with a Fug 8 in the same installation. (Fug 8 + Fug 4)\n\nFuG 5: A high-band HF/low-band VHF transceiver. It operated in the 27,000 to 33,3000  kHz (27-33.3 MHz) frequency range with a transmit power of 10 Watts. This equipment provided for 125 radio channels at 50 kHz channel spacing. It was usually used with a 2-metre antenna. Was present in almost all German tanks and some other vehicles. Was the standard kit for tank-to-tank communication within platoons and companies. Range was approx 2–3 km when using AM voice and 3–4 km when using CW. \n\nFuG 6: A high-band HF/low-band VHF transceiver. It operated in the 27,000 to 33,3000  kHz (27-33.3 MHz) frequency range with a transmit power of 20 Watts. It was usually used with a 2-metre antenna. It was used by armoured observation posts. These were usually early model tanks with some of their armament removed and replaced by equipment for artillery observers. It was used by the observers to communicate with the armoured unit leaders via their Fug 5 radios. Main advantage over the FuG 5 was greater range. Range was approx 4–6 km when using AM voice and 6–8 km when using CW. Comparable with the American SCR-508 tank radio, which covered a similar frequency range (20-27.9 MHz) at 25 watts. The major difference between German Army tank sets and US Army tank sets was the American use of FM for the high-HF/low-VHF bands.\n\nFuG 7: A VHF transceiver (receiver/transmitter) used in command tanks. It operated in the 42,000 to 48,3000  kHz (42-48.3 MHz) frequency range with a transmit power of 20 Watts. It was usually used with a 1.4 metre antenna. It was matched with the Luftwaffe transceiver Fug 17 in ground support operations. It was used for CAS operations, though these became rare after late 1944 with the result that many command tanks with this equipment fitted were converted to other roles. Range 60 km AM voice to 80 km CW.\n\nFuG 8: A medium wave transceiver (receiver/transmitter) used in command tanks. It operated in the 1,130 to 3,000  kHz frequency range with a transmit power of 30 Watts. It was used with various antenna ranging from 1.8 to 9 metres antenna. It was used for communication back to the regimental command post. Range 25 km AM voice to 140 km CW using the 9 m antenna, the station had to be stopped to use this antenna.\n\nFuG 10: A medium wave transceiver (receiver/transmitter) used in command tanks. It operated in the 1,130 to 3,000  kHz frequency range with a transmit power of 30 Watts. It was used with a frame antenna on various reconnance units. Range 10 km AM voice to 40 km CW.\n\nFuG 11: A medium wave transceiver (receiver/transmitter) used in command tanks. It operated in the 1,130 to 3,000  kHz frequency range with a transmit power of 100 Watts. Used at the regimental command post. Range 70 km AM voice to 200 km CW.\n\nFuG 12: A medium wave transceiver (receiver/transmitter) used in command tanks. It operated in the 1,130 to 3,000  kHz frequency range with a transmit power of 80 Watts. It was used with a 2-metre antenna. reconnaissance units later in the war.\n\nFuG 13: A Fug 6 with two receivers rather than 1.\n\nFuG 15: A HF receiver. It operated in the 23,000 to 24,950,3000  kHz (23-24.95 MHz) frequency range. It was matched with transceiver Fug 16 for use with Sturmartillerie (self-propelled artillery).\n\nFuG 16: A HF transceiver (receiver/transmitter) used in the command vehicles of Sturmartillerie units. It operated in the 23,000 to 24,950  kHz (23-24.95 MHz) frequency range with a transmit power of 10 Watts. It was usually used with a 2.0 metre antenna. Range 2 km AM voice to 4 km CW.\n\nFuSpr.a: A mid-band HF transceiver. It operated in the 24.1 to 25  MHz (24100 to 25000  kHz) frequency range with a transmit power of 5 Watts. It was used with antenna of either 1.4 to 2.0 metres antenna. It was used for by reconnaissance elements for intercommunication . Range 5 km AM voice.\n\nFuSpr.f: A mid-band HF transceiver. It operated in the 19,990 to 21,470  kHz (19.99-21.47 MHz) frequency range with a transmit power of 5 Watts. It was used with antenna of either 1.4 to 2.0 metres antenna. It was used for by reconnaissance elements of the SPG force to communicate with artillery units. Range 5 km AM voice.\n\nThe German army issued infantry radios in two main series. The \"Backpack\" or Torn series and the field or Feld series.\n\nTorn.Fu.a A HF transceiver. It operated in the 3-6.67 MHz frequency range. 2 Watt output AM voice and CW\n\nTorn.Fu.b: A HF transceiver. It operated in the 3-5 MHz frequency range. 0.7 Watt output AM voice and CW. Formed the base design of other units. Used by the infantry at the regimental and battalion level. Not intended to be used on the move.\n\nTorn.Fu.c: A HF transceiver. It operated in the 1.5-2.6 MHz frequency range. Otherwise identical to Torn.Fu.b. Used by artillery observers.\n\nTorn.Fu.d2: A VHF transceiver. It operated in the 33.8-38 MHz frequency range. 1.5 Watt output AM voice and CW. Range approx 10 km with Cw and 3 with voice.Used by the infantry. A very common infantry set. Could be used on the move with one man carrying the transmitter/receiver and the other the battery/power supply and handset. A cable linked the two men together. Comparable to US military backpack \"walkie-talkie\" radio SCR-300 that operated in the 40-48 MHz frequency range (however, the US equipment operating in the high-HF and low-VHF region of roughly 20-50 MHz used FM modulation instead of AM.\n\nTorn.Fu.f: A HF transceiver. It operated in the 4.5-6.67 MHz frequency range. 0.7 Watt output AM voice and CW. Panzergrenadier units. Identical to Torn.Fu.b with exception of frequency range.\n\nTorn.Fu.g: A HF transceiver. It operated in the 2.5-3.5 MHz frequency range. .5 Watt output AM voice and CW. Used at the battalion to company level. Some command and reconnaissance units had this infantry transceiver to allow then to talk to Panzergrenadier units.\n\nTorn.Fu.h: A HF transceiver. It operated in the 23-28 MHz frequency range. 0.6 Watt output AM voice. SPG units.\n\nTorn.Fu.i A HF transceiver. It operated in the 4.5-6.67 MHz frequency range. Replacement for Torn.Fu.f\n\nTorn.Fu.k A HF transceiver. It operated in the 2.5-3.5 MHz frequency range. 1 Watt output AM voice and CW.Replacement for Torn.Fu.g\n\nFeldfu.a1 120-156 MHZ Infantry, A VHF transceiver. .15 Watt output AM voice and CW. Range 1.5 km.\n\nFeldfu b 90-110MHZ Infantry Used in two versions. .15 Watt output AM voice and CW. The b version was used by the Infantry until they changed to the 'c' model radio in another band. Thereafter version b1 was used by the Pioneers. Later a simplified version (b2), of lower power (.12w), was used by the Panzergrenadiers.\n\nFeldfu b1 Pioniere 90-110Mhz Range 1.5 km.\n\nFeldfu b2 Panzergrenadier only 2 tubes,no AF tube Rv2,4P700\n\nFeldfu c 130 - 160 MHZ Infantry, .15 Watt output AM voice and CW. Infantry. Range 1.5 km.\n\nFeldfu f 28. 0 - 33.0 MHz Panzergrenadier\n\nFeldfu h 23.1 - 28.0 MHz Assault Guns\n\nKL.Fuspr.d: A VHF transceiver. It operated in the 32-38 MHz frequency range. Single unit \"walkie talkie\" system.Used by Artillery forward observers.\n\nFrom 1936 the German forces started deploying a range of communication links using UHF frequencies to form a point-to-point mesh communication network.\nDuring the war this was extended to cover the majority of the occupied areas or Europe and North Africa. Both the Army and the Air force made use of the same equipment. The units of the T series were \"backpack\" style mobile equipment. The K series were truck mobile units using a range of masts up to approximately 11 m tall. The G series were still mobile but were intended to form more of a backbone for communications and were intended to remain in place for longer periods than the K. They again used a series of masts up to 50 m high. Depending on the set a range of communication facilities were available consisting of voice and/or teletype and Hellscriber. Range was line-or-sight so the ranges reported here were assuming the antenna could \"see\" one another.\n\nDMG 2 T : Pack unit, Operating on the 475 - 525 MHz band, single channel voice or 800 character per second Teletype. Range up to 50 km. Tripod mast or 11 m mast. Transmit power 0.15 watt. Power battery.\n\nMichael 2 R - DMG 5 K: Truck mounted. Operating on the 502 - 555 MHz band, two channels. Channel 1 Voice or 5500 CPS teletype or 3 channels teletype. Channel 2 8000 CPS teletype. Transmitter power 1 watt. Range up to 50 Km. Mast or tower. Power 22 V AC 50 Hz 300 VA.\n\n\n"}
{"id": "22930487", "url": "https://en.wikipedia.org/wiki?curid=22930487", "title": "Yaw damper (railroad)", "text": "Yaw damper (railroad)\n\nA yaw damper is a transverse mounted shock absorber used to prevent railcars and locomotives from swaying excessively from side to side. Yaw dampers prevent locomotives and passenger railcars from striking station platforms as they roll past them and reduce the gap that must be left between the railroad vehicle and the platform.\n\n"}
{"id": "2229424", "url": "https://en.wikipedia.org/wiki?curid=2229424", "title": "ZEEP", "text": "ZEEP\n\nThe ZEEP (Zero Energy Experimental Pile) reactor was a nuclear reactor built at the Chalk River Laboratories near Chalk River, Ontario, Canada (which superseded the Montreal Laboratory for nuclear research in Canada). ZEEP first went critical at 15:45 on September 5, 1945. ZEEP's was the first operational nuclear reactor outside the United States.\n\nThe reactor was designed by Canadian, British and French scientists as a part of an effort to produce plutonium for nuclear weapons during World War II. It was developed while the Montreal Laboratory and Chalk River Laboratories research facility were under the supervision of the National Research Council of Canada (NRC). ZEEP was instrumental in the development of the NRX and NRU reactors, which led to the development of the successful CANDU reactor. ZEEP was used to test reactivity effects and other physics parameters needed for reactor development at Chalk River Laboratories, including fuel lattices for the NRU reactor situated next door.\n\nZEEP was one of the world's first heavy water reactors, and it was also designed to use natural (unenriched) uranium; a feature carried through to the CANDU design. Uranium enrichment is a complex and expensive process; thus, the ability to use unenriched uranium gave ZEEP and its descendants a number of distinct advantages.\n\nZEEP continued to be used for basic research until 1970. It was decommissioned in 1973 and dismantled in 1997. In 1966 ZEEP was designated a historic site by Ontario, and commemorated with an historic plaque. Both this plaque and ZEEP itself are now on display at the Canada Science and Technology Museum in Ottawa, Canada.\n\n\n"}
