{"id": "5088635", "url": "https://en.wikipedia.org/wiki?curid=5088635", "title": "ACE (editor)", "text": "ACE (editor)\n\nACE (\"ACE - a collaborative editor\") is a platform-independent, collaborative real-time editor. It is a real-time cooperative editing system that allows multiple geographically dispersed users to view and edit a shared text document at the same time.\n\nACE is a simple text editor with standard features such as copy/paste and load/save. Multiple documents can be edited at the same time. Furthermore, ACE can share documents with other users on different computers, connected by communication networks (LAN, Internet). ACE also discovers users and their shared documents automatically in a local area network. Users can opt to join any discovered shared document. For all this, no configuration is necessary because it is based on zero-conf networking.\n\nOnce a user has joined a shared document, he can freely edit the document at the same time with all participants as a virtual team. So-called awareness information helps to avoid unnecessary conflicts (that is, two users editing at the same time and text location). Awareness information includes the cursor and the currently selected text of the other users marked with the color of the respective user.\n\nThe heart of the application is a concurrency control algorithm based on the innovative concept of operational transformation, which allows for lock-free editing of a document by multiple users. It imposes no editing constraints and resolves all conflicts automatically. The algorithm overcomes one of the most significant challenges in designing and implementing real-time collaborative editing systems, namely consistency preservation. That is to ensure that at quiescence (that is, when no messages are in transit) the shared document is identical for all participants.\n\nACE builds upon open technologies such as BEEP (RFC 3080) and zero-conf networking. All this leaves the possibility open to communicate even with different applications that understand the public protocol of ACE.\n\nACE runs on all major operating systems such as Windows, Mac OS X, or Linux and is free software.\n\nConsider a collaborative editing session with three participants, named Bill, Steve, and Scott, as depicted by the figure. There is also a fourth user, named Sam, who does not participate in the session. The document which the users collaboratively edit is \"Collaboration.txt\". User Scott is the owner and publisher of it. Note that users Steve and Bill work on Microsoft Windows OS, whereas Scott uses Mac OS X and Sam Linux OS. Note that the blue color always denotes the text written by the local user of the ACE instance. The color to matching a participant is depicted in the participants view of the GUI.\n\nHere is a list with a few examples on how ACE can be used:\n\nThe last point reveals some more potential for cooperative editing systems: Groups of two to three students can write essays together. On the one hand, they teach each other how to use the written language, and on the other hand, they can have fun together using a hands-on application on the computer, thus losing fear of contact with the computer, which is essential in today's education.\n\nA real-time cooperative editing system such as ACE allows multiple users to view and edit the same document at the same time from multiple geographically dispersed sites. The sites are connected by communication networks.\nThe following requirements have been identified for such systems:\n\nA real-time cooperative editing system consists of \"n\" instances, each instance run by a different user. All instances are connected by a network. One of the most significant challenges in designing and implementing real-time cooperative editing systems is consistency maintenance among the different document replicas (one for each site). A cooperative editing system is said to be consistent if it always maintains the following properties:\n\nOne solution to achieve consistency control is provided by \"Operational Transformation\" (OT) algorithms. The OT algorithm approach consists of two main components:\n\nThe theory on consistency maintenance stems from the research field called \"Computer Supported Cooperative Work\", for short \"CSCW\".\n\n\n"}
{"id": "22066430", "url": "https://en.wikipedia.org/wiki?curid=22066430", "title": "AgCam", "text": "AgCam\n\nThe AgCam is a ruggedized camera system developed for use on Agricultural equipment to assist in viewing on large machinery. The system is manufactured by Dakota Micro, Inc., a North Dakota Corporation, United States.\n\nThe University of North Dakota has developed a camera for use on the International Space Stations that was named the AgCam, but has since changed the name of their camera to ISSAC in the light of copyright issues.\n\nThe AgCam is sold to a network of equipment dealerships. AgCam is sold by OEM Companies, including: AGCO Corporation, Trimble, Pickett Equipment, Morris Industries Ltd., Double L Inc. and TOPCON,\n"}
{"id": "55677094", "url": "https://en.wikipedia.org/wiki?curid=55677094", "title": "Alb-Leisa", "text": "Alb-Leisa\n\nThe term Alb-Leisa firstly means the \"Öko-Erzeugergemeinschaft Alb-Leisa\" (engl. \"Eco-producer association Alb-Leisa\"), secondly a trade name and thirdly traditional varieties of lentils from the Swabian Jura. \"Leisa\" means lentils in Swabian. \n\nThe Alb-Leisa was included in the Ark of Taste at Slow Food Deutschland in 2012. There are currently 70 small farms in this group. The marketing of Alb-Leise is done by \"Lauteracher Alb-Feld-Früchte\", formerly known as \"Biohof Mammel\". Especially in the region Baden-Württemberg and Bavaria, but also via webshops.\n\nUntil the 1950s, the Swabian Jura and its surroundings were the centre of lentil cultivation in Germany. After that there was no lentil cultivation in this region anymore, it had become uneconomical. In 1985 the Biohof Mammel in Lauterach started again with the cultivation of lentils. Demand rose slowly. From 2001 onwards, other organic farms were also interested in lentil cultivation. That is why the \"Öko-Erzeugergemeinschaft Alb-Leisa\" (Eco-producer association Alb-Leisa) was founded.\n\nThe traditional Swabian lentil varieties have been disappeared completely in 1985. There was no seed available at all. For this reason, the organic farm Mammel and later the producer's association cultivated the French \"Puy lentil\". In 2006, in the gene bank of the Wawilow Institute in St. Petersburg the classic Swabian lentil varieties have been accidentally discovered and brought back to Germany. Since then they have been grown again on the Swabian Jura.\n\nAt present, three genotypes of these lentils are used in the Swabian Jura, which are protected by the producers' association under the name of Alb-Leisa. These are the classic Swabian varieties of lentils of the breeder Fritz Späth from Haigerloch: \"Späths Alb-Leisa small\", \"Späths Alb-Leisa large\" and \"Späths Heller-Leisa\".\n\n\n\n"}
{"id": "30336475", "url": "https://en.wikipedia.org/wiki?curid=30336475", "title": "Archos 101 Internet Tablet", "text": "Archos 101 Internet Tablet\n\nThe ARCHOS 101 Internet Tablet is part of the Archos Generation 8 tablet range, distributed between 2010-11. After a hardware upgrade, it was also part of Generation 9 range sold between 2011-12. It is a 10.1 inch (256.5 mm) Internet tablet with dual-boot capability running Android out of the box.\n\nThe Archos 101 IT is a 10.1 inch capacitive, multitouch screen tablet. It includes a WSVGA 1024x600 LCD screen with a pixel density of 118 ppi. The device also includes a light sensor and TFT technology.\n\nThe devices uses a Li-Poymer irreplaceable battery. The battery's life for music playback is 36 hours while 7 hours for video playback.\n\nThe tablet includes an ARM Cortex-A8 single core processor, and 256 MB of RAM with a Texas Instrument OMAP 3660 chipset. It is sold with 8 and 16GB version with expandable memory (microSD, microSDHC) of up to 32GB.\n\nThe device includes Bluetooth support for 2.1, EDR Advanced Audio Distribution (A2DP), Audio/Video Control Transport Protocol (AVCTP), Audio/Video Distribution Transport Protocol (AVDTP), Audio/Visual Remote Control Profile (AVRCP), Generic Access (GAP), Generic Audio/Video Distribution (GAVDP), Object Push (OPP), Phone Book Access (PBAP), Service Discovery Protocol (SDP). It also includes 802.11 b, g, n Wi-Fi connectivity.\n\nThe Archos tablet also includes a USB 2.0 for mass storage device and USB charging, miniHDMI (Type C), 3.5 mm headphone jack, and computer/OTA sync.\n\n\n"}
{"id": "33381057", "url": "https://en.wikipedia.org/wiki?curid=33381057", "title": "Bend stiffener", "text": "Bend stiffener\n\nA bend stiffener is a type of cable protection system. They are conically shaped polyurethane mouldings designed to add local stiffness to a riser, flowline, cable or umbilical. They limit the bending stresses and curvature to acceptable levels.\nThey are used in the oil and gas industry as part of offshore deep sea drilling operations.\n\n\n"}
{"id": "5010296", "url": "https://en.wikipedia.org/wiki?curid=5010296", "title": "Beulah Louise Henry", "text": "Beulah Louise Henry\n\nBeulah Louise Henry (February 11, 1887 – February 1973) was an American inventor. In the 1930s, she was given the nickname \"Lady Edison\" for her many inventions.\n\nHer inventions include a bobbin-free sewing machine and a vacuum ice cream freezer. Though she was awarded around 49 patents over her lifetime, she had around 110 inventions total.\n\nShe was born in North Carolina, the daughter of Walter R. and Beulah Henry. She was the granddaughter of former North Carolina Governor W. W. Holden and a direct descendant of Patrick Henry. From 1909 to 1912 she attended North Carolina Presbyterian College and Elizabeth College in Charlotte, North Carolina, where she submitted her first patents.\n\nShe moved to New York City by 1924, where she founded two companies. She worked as an inventor for the Nicholas Machine Works from 1939 to 1955. She also served as a consultant for many companies that manufactured her inventions, including the Mergenthaler Linotype Company and the International Doll Company. She lived in New York hotels, belonged to a variety of scientific societies, and never married.\n\nA partial list of Henry's inventions includes:\n\n\nHenry was inducted into the National Inventors Hall of Fame in 2006.\n\nhttp://cr4.globalspec.com/blogentry/4685/Beulah-Louise-Henry-Lady-Edison-1887-1973\n"}
{"id": "50805789", "url": "https://en.wikipedia.org/wiki?curid=50805789", "title": "Blue Ribbon Commission on America's Nuclear Future", "text": "Blue Ribbon Commission on America's Nuclear Future\n\nA Blue Ribbon Commission on America’s Nuclear Future was appointed by President Obama to look into future options for existing and future nuclear waste, following the ending of work on the incomplete Yucca Mountain Repository. At present there are 70 nuclear power plant sites where 65,000 tons of spent fuel is stored in the USA. Each year, more than 2,000 tons are added to this total. Nine states have \"explicit moratoria on new nuclear power until a storage solution emerges\". A deep geological repository seems to be the favored approach to storing nuclear waste.\n\nOn January 26, 2012, the Commission submitted its final report to Energy Secretary Steven Chu. The Commission put forth seven recommendations for developing a comprehensive strategy to pursue. A major recommendation was that \"the United States should undertake an integrated nuclear waste management program that leads to the timely development of one or more permanent deep geological facilities for the safe disposal of spent fuel and high-level nuclear waste\".\n\nIn the United States, a blue-ribbon panel (or blue ribbon commission) is a group of exceptional people appointed to investigate or study or analyze a given issue. Blue-ribbon panels generally have a degree of independence from political influence or other authority, and such panels usually have no direct authority of their own. Their value comes from their ability to use their expertise to issue findings or recommendations which can then be used by those with decision-making power to act.\n\nAt present there are 70 nuclear power plant sites where 65,000 tons of spent fuel is stored in the USA. Each year, more than 2,000 tons are added to this total. US nuclear waste management policy completely broke down with the ending of work on the incomplete Yucca Mountain Repository. Without a long-term solution to store nuclear waste, a nuclear renaissance in the U.S. remains unlikely. Nine states have \"explicit moratoria on new nuclear power until a storage solution emerges\".\n\nIn a Presidential Memorandum dated January 29, 2010, President Obama established the Blue Ribbon Commission on America’s Nuclear Future (the Commission). The Commission, composed of fifteen members, conducted an extensive two-year study of nuclear waste disposal, what is referred to as the \"back end\" of the nuclear energy process. The Commission established three subcommittees: Reactor and Fuel Cycle Technology, Transportation and Storage, and Disposal.\n\nDuring their research the Commission visited Finland, France, Japan, Russia, Sweden, and the UK, and in 2012, the Commission submitted its final report. The Commission did not issue recommendations for a specific site but rather presented a comprehensive recommendation for disposal strategies.\n\nSome nuclear power advocates argue that the United States should develop factories and reactors that will recycle some spent fuel. However, the Obama administration has disallowed reprocessing of nuclear waste, citing nuclear proliferation concerns. The Blue Ribbon Commission said that \"no existing technology was adequate for that purpose, given cost considerations and the risk of nuclear proliferation\". A deep geological repository seems to be favored.\n\nOn January 26, 2012, the Commission submitted its final report to Energy Secretary Steven Chu. In their final report the Commission put forth several recommendations for developing a comprehensive strategy to pursue. A major recommendation was that \"the United States should undertake an integrated nuclear waste management program that leads to the timely development of one or more permanent deep geological facilities for the safe disposal of spent fuel and high-level nuclear waste\". There is an \"international consensus on the advisability of storing nuclear waste in deep underground repositories\", but no country in the world has yet opened such a site.\n\nIn their final report the Commission put forth seven recommendations for developing a comprehensive strategy to pursue:\n\n\n\n\n\n\n\n\nCo-chairmen of the Commission were Lee H. Hamilton and Brent Scowcroft, and members of the Commission included, Vicky Bailey, Albert Carnesale, Pete Domenici, Susan Eisenhower, Chuck Hagel, Jonathan Lash, Allison M. Macfarlane, Richard Meserve, Ernest Moniz, John Rowe, and Phil Sharp.\n\n\n"}
{"id": "59701", "url": "https://en.wikipedia.org/wiki?curid=59701", "title": "Broom", "text": "Broom\n\nA broom is a cleaning tool consisting of usually stiff fibers (often made of materials such as plastic, hair, or corn husks) attached to, and roughly parallel to, a cylindrical handle, the broomstick. It is thus a variety of brush with a long handle. It is commonly used in combination with a dustpan.\n\nA distinction is made between a \"hard broom\" and a \"soft broom\". Soft brooms are for sweeping walls of cobwebs and spiders. Hard brooms are for sweeping dirt off sidewalks.\n\nThe word \"broom\" derives from the name of certain thorny shrubs (\"Genista\" and others) used for sweeping. The name of the shrubs began to be used for the household implement in Late Middle English and gradually replaced the earlier \"besom\" during the Early Modern English period. The song \"Buy Broom Buzzems\" (by William Purvis 1752–1832) still refers to the \"broom besom\" as one type of besom (i.e. \"a besom made from broom\").\n\nFlat brooms, made of broom corn, were invented by Shakers in the 19th century with the invention of the broom vice.\n\nA smaller whisk broom or brush is sometimes called a duster.\n\nIn 1797, the quality of brooms changed when Levi Dickenson, a farmer in Hadley, Massachusetts, made a broom for his wife, using the tassels of sorghum, a grain he was growing for the seeds. His wife spread good words around town, creating demand for Dickenson's sorghum brooms. The sorghum brooms held up well, but ultimately, like all brooms, fell apart. Dickenson subsequently invented a machine that would make better brooms, and faster than he could. In 1810, the foot treadle broom machine was invented. This machine played an integral part in the Industrial Revolution.\n\nOne source mentions that the United States had 303 broom factories by 1839 and that the number peaked at 1,039 in 1919. Most of these were in the Eastern United States; during the Great Depression in the 1930s, the number of factories declined to 320 in 1939. The state of Oklahoma became a major center for broom production because broom corn grew especially well there, with The Oklahoma Broom Corn Company opening a factory in El Reno in 1906. Faced with competition from imported brooms and synthetic bristles, most of the factories closed by the 1960s.\n\nIn the context of witchcraft, \"broomstick\" is likely to refer to the broom as a whole, known as a besom. The first known reference to witches flying on broomsticks dates to 1453, confessed by the male witch Guillaume Edelin. The concept of a flying ointment used by witches appears at about the same time, recorded in 1456.\n\nIn Metro-Goldwyn-Mayer's 1939 film, \"The Wizard of Oz\", the Wicked Witch of the West used a broomstick to fly over Oz. She also used it to skywrite \"Surrender Dorothy\" above the Emerald City. The Wizard commands Dorothy and her three traveling companions to bring the Wicked Witch's broomstick to him in order to grant their wishes. Dorothy carries it to the Wizard with the Scarecrow, Tin Man, and Lion after the Wicked Witch's death.\n\nIn Disney's 1940 film \"Fantasia\", Mickey Mouse, playing The Sorcerer's Apprentice, brings a broom to life to do his chore of filling a well full of water. The broom overdoes its job and when chopped into pieces, each splinter becomes a new broom that flood the room until Yen Sid stops them. This story comes from a poem by Goethe called \"Der Zauberlehrling\" (\"The Sorcerer's Apprentice\"). The Disney brooms have had recurring cameos in Disney media, mostly portrayed as janitors, albeit not out of control or causing chaos such as in the original appearance.\n\nThis flight was also in \"Bedknobs and Broomsticks\" as well as \"Hocus Pocus\".\n\nIn eSwatini, witches' broomsticks are short bundles of sticks tied together without a handle.\n\nFlying brooms play an important role in the fantasy world of Harry Potter, used for transportation as well as for playing the popular airborne game of Quidditch. Flying brooms, along with Flying carpets, are the main means of transportation in the world of Poul Anderson's \nOperation Chaos.\n\nThe Flying Broom () is a feminist organization in Turkey, deliberately evoking the associations of a Flying Broom with witches.\n\n\n\n\nIt is used as a symbol of the following political parties:\n\n\n\n"}
{"id": "53496177", "url": "https://en.wikipedia.org/wiki?curid=53496177", "title": "Cable protection system", "text": "Cable protection system\n\nA cable protection system (CPS) protects subsea power cables against various factors that negatively impact on the cable lifetime, normally used when entering an offshore structure. When a subsea power cable is laid, there is an area where the cable can be subjected to increased dynamic forces, which the cable is not necessarily designed to survive over the lifetime of the installation.\n\nCable protection systems are used to allow the specification, and thus cost, of a subsea power cable to be reduced, by removing the need to include additional armoring of the cable. The resulting cables can be produced more cheaply, whilst still prividing the 20 years + lifetime required.\n\nOffshore windfarm developers in particular have adopted the use of Cable protection systems due to the dynamic area where the cable comes from the seabed and enters the monopile/J-tube. This is in part due to the potential for localised scouring to occur near the structure.\n\nA CPS generally consists of three sections, a Centraliser or Monopile interface, a protection system for the dynamic area, and a protection system for the static area.\n\nThe installation of J-Tubes for offshore renewable monopiles was viewed as a costly approach, and a 'latching' type of cable protection system which penetrates the outer wall of the monopile, via a specifically designed angled aperture enables the simplification of monopile design, and removes the need for additional works post pile driving which usually involved the use of divers. This approach is becoming the industry standard in monopile design, assisting developers to reduce their costs for construction.\n\nArticulated half-pipe Cable protections systems have traditionally been used for the protection of cables at shore landings, and other areas where cable damage could be envisaged, and burial was not practical. Patents for variations of articulated pipe cable protections date back to 1929. The system was described as a cable armor shield \n\"adapted to protect the cable from damage and wear occasioned by rubbing on rocks, contacting with ships, anchors or other objects, and has for its object to provide a practical flexible armor shield of this class which can be readily applied to the cable at any point along its length.\"\n\nFrom their outset cable protection systems were designed to be simple, effective, and easy to assemble. The systems consisted of a series of half shells which had a convex flange at one end and a larger socket flange at the other allowing the sections to form a flexible universal joint connection between them. Due to the intended use of heavy cast or forged metals they also had the added advantage of increasing the weight of the cable being installed, thus reducing movement on the seabed.\n\nOver the years innovations have occurred improving the articulation of the joints with modern articulated pipes being more akin to ball-joints, and some manufacturers providing 'boltless' articulated pipes, thus saving assembly time.\n\nChanges in the metallurgy have also happened, leading to most half shell articulated pipe now being made from ductile iron, due to its improved strength and elasticity characteristics.\n\nToday these articulated pipes are also utilised for their bend restriction properties, allowing them to be utilised as bend restrictors for the protected cable.\n\nCable protection systems are predominantly designed to protect the system from damage throughout the lifetime of the cable caused by fatigue, overbending of the cable, and to provide protection of the cable until it reaches an area of burial.\n\nThe cable protection system will be designed to provide protection for a specific lifetime, the 'design life' of the system, which may vary dependent upon the conditions encountered.\n\nOverbending of the cable occurs when the cable is bent in a radius of less than the minimum bending radius defined by the manufacturer. Although the cable may initially survive the overbending, this can lead to subsequent fatigue within the cable ultimately leading to cable failure. The CPS selected should maintain a radius which is greater than the specified minimum bend radius.\n\nSubsea cable protection systems can encounter wear due to movement, and general changes in composition due to being submerged for a prolongued period of time, such as corrosion or changes in polymer based compounds. Consideration should be given to the induced effects on the CPS resulting from the dynamic elements in the environment. Simple changes such as changes in temperature, current or salinity can result in changes in the ability of the CPS to offer protection for the life of the cable. It is advisable to carefully assess the potential effects of movement of the CPS, relating to the dynamic abilities of the cable. The CPS may withstand the worst conditions seen over a 100yr period, but would the cable inside the CPS survive these movements. In some instances, such as shore ends for fibre optic cables where rocky outcrops are present, dynamic influences can be reduced by securing the articulated pipe to the seabed rock, thus reducing the degree of movement remaining.\n\nSome manufacturers have performed independent empirical testing to provide a simulated 25yr life cycle of the dynamic forces applicable to their product in order to provide customers with improved confidence in the survivability of the system.\n\nAnother cause for failure of subsea power cables is caused by overheating, which can occur where a cable is contained within a CPS without adequate ability to dissipate the heat produced by the cable. These lead to early fatigue of the cable insulation, necessitating the replacement of the cable.\n\nSubsea cable incidents account for around 77% of the total global cost of wind farm losses. Since 2007 this percentage, which has varied between 70% and 80%, is statistically reported year after year.\n\nSeabed stability is an important factor associated with cable protection systems. Should the cable protection system be too buoyant, it is less likely to remain in contact with the seabed, thus the CPS is more likely to require additional remedial stability measures, such as installation of concrete mattresses, rockbags, or rockdumping.\n\nWhen a CPS is being installed to interface with a monopile structure, there is likely to be seabed scouring to some degree. Should the scouring become excessive, the CPS may be suspended within a scour hole, and needs to be capable of supporting its own weight, and that of the cable within. Failure to sustain this loading scenario will lead to failure of the CPS, which will in turn allow the forces to act upon the cable within, ultimately leading to cable damage.\n\nWithin the renewables market in particular, installation of CPS's are preferred to be completely diverless, as this reduces the developers cost, and removes risk to human life through diving in a hazardous area.\n\nA final consideration for CPS is that of removal of the cable should a failure occur. Some designs require diver intervention to recover the cable with the CPS. Due consideration should also be given to the removal of a CPS should the CPS itself fail. The costs associated with CPS replacement during the operational lifetime of an offshore wind farm are not insignificant, as the cable will most likely require repair/replacement as part of the process.\n\nVarious innovative systems have been developed to provide restriction of bending, including ductile iron articulated pipe, and polymer or metal based vertebrae systems. Vertebrae bend restrictors are available in both metal and polymer based forms. Some cable protection systems include a polymer based vertebrae system which restricts the bend radius to a maximum of a few degrees per segment. These systems are lighter than their metal equivalents and often cheaper to produce but must be carefully assessed for longevity in the proposed application. Due to the use of polymers these systems tend to be of a larger diameter than their metal counterparts, which presents a larger surface area for drag induced forces caused by currents.\n\nBend stiffeners are conically shaped polymer mouldings designed to add local stiffness to the product contained within, limiting bending stresses and curvature to acceptable levels. Bend stiffeners are generally suitable for water depths of 35 metres or less, and their suitability is highly dependent on currents and seabed conditions at site. Extreme care must be taken when selecting a stiffener, especially relating to the lifespan of the system as these themselves can become fatigued/fragile. As the stiffness of these products are dependent upon the nature of the plastic used, careful testing and QA of plastics should be carefully considered as flaws introduced during material manufacture, processing, machining and molding.\n\nVarious other polymer based systems have been developed which provide a flexible 'tube' which can be attached to the structure in advance of the cable being installed, although these are relatively new to the industry, and considered by some as unproven.\n\nAlthough there are no specific standards for cable protections systems, DNVGL-RP-0360 Subsea power cables in shallow water includes a section on Cable Protection at the interface to a structure (Section 4.7).\n"}
{"id": "31579355", "url": "https://en.wikipedia.org/wiki?curid=31579355", "title": "Carnegie Commission on Educational Television", "text": "Carnegie Commission on Educational Television\n\nThe Carnegie Commission on Educational Television was established in 1965 by the Carnegie Corporation of New York in the United States. This commission was created to research the role noncommercial educational television played on society in America. The 15 member commission attempted to carry out this goal by; distinguishing between commercial television, entertainment for large mass audiences: instructional television, in-class educational material, and public television. Public television was considered anything else that largely helped public affairs and was not supported by advertising.\n\nThe commission first gained momentum at the First National Conference on Long-Range Financing of Educational Television Stations in December 1964. Although not created until 1965, the commission was first incepted at this conference.\n\nOn January 26, 1967, the commission published its most famous and influential report on broadcast history, \"Public Television: A Program for Action\". The reports main rhetoric was persuading America’s institutions (and proposing new institutions) to enhance educational television. Among the final recommendations in the report were; that a Corporation for Public Television be created to receive and disburse funds from the government and other sources, that it support at least two national and many local production agencies, that it seek ways to encourage interconnection of stations, and that sufficient funds, not subject to the annual appropriation process, be provided through a 2-5% tax on television receivers. The recommendations of the report were stated with 12 actions to be carried out by Congress and the Commission itself, although some of these actions were vague as to whom the primary target was.\n\nThe following actions were used by the Commission to persuade U.S. political institutions to enhance educational television, authorize the Corporation for Public Broadcasting and provide said organization with funds, and sponsor further research and studies to improve and advocate educational television. These actions also included steps that the proposed Corporation for Public Broadcasting should carry out to reach their goals.\n\n\n\"Public Television: A Program for Action\" sold 50,000 copies in just a few days and received wide attention. The report also led to rapid and drastic actions. President Johnson mentioned public television in his 1967 State of the Union address, and shortly afterwards proposed legislation that was similar to the proposals in the report. In November 1967, the Public Broadcasting Act of 1967 became law and created the Corporation for Public Broadcasting (CPB).\n"}
{"id": "21888016", "url": "https://en.wikipedia.org/wiki?curid=21888016", "title": "Co-processing", "text": "Co-processing\n\nCo-processing is the use of waste as raw material, or as a source of energy, or both to replace natural mineral resources (material recycling) and fossil fuels such as coal, petroleum and gas (energy recovery) in industrial processes, mainly in energy intensive industries (EII) such as cement, lime, steel, glass, and power generation. Waste materials used for Co-processing are referred to as alternative fuels and raw materials (AFR).\n\nCo-processing is a proven sustainable development concept that reduces demands on natural resources, reduces pollution and landfill space, thus contributing to reducing the environmental footprint. Co-processing is also based on the principles of industrial ecology, which considers the best features of the flow of information, materials, and energy of biological ecosystems, with the aim of improving the exchange of these essential resources in the industrial world.\n\nFigure 1: Types of Co-processing\n\nIn summary, the benefits of Co-processing are:\n\nCo-processing contributes to the industrial competitiveness, is a complementary technology to concepts such as cleaner production or recycling and should be considered as a treatment alternative within an integrated waste management concept. Some EII offer co-processing as a sustainable waste management service. It is usually more cost effective to adapt existing facilities of EII than building new waste treatment capacities thereby reducing waste management cost to society.\n\nThe waste management hierarchy (see figure below) shows that Co-processing is a recovery activity which should be considered after waste prevention and recycling; Co-processing ranks higher in this hierarchy in comparison to disposal activities such as landfilling or incineration.\n\nFigure 2: Waste Management Hierarchy\n\nThe global industrial demand for energy is roughly 45% of the total demand and the requirements of the energy intensive industries (EII) are more than half of the total industrial demand, at 27%.\n\nWorldwide, wastes suitable for Co-processing have an energy potential equivalent to nearly 20% of the fossil fuel energy used by the EII and coal-fired power plants. By 2030, the thermal substitution rate of waste could rise to nearly 30%. In the EU-25 countries of Europe, the available energy potential in waste currently represents nearly 40% of this demand, and this is expected to rise to almost 50% by 2030.\n\nThe EU cement industry already uses more than 40% fuels derived from waste and biomass in supplying the thermal energy to the grey clinker making process. Although the choice for this so-called alternative fuels (AF) is typically cost driven, other factors are becoming more important. Use of AF provides benefits for both society and the company: CO2-emissions are lower than with fossil fuels, waste can be co-processed in an efficient and sustainable manner and the demand for certain virgin materials can be reduced. Yet there are large differences in the share of AF used between the European Union (EU) member states. Clearly, the societal benefits can be enlarged if more member states increase their AF share. In this study Ecofys assess the barriers and opportunities for further uptake of AF in 14 EU member states. Ecofys found that local factors constrain the market potential to a much larger extent than the technical and economic feasibility of the cement industry itself. In this summary they present the overall findings. The detailed assessments are available in separate cases studies.Source\n\nRoughly 60% of the waste that could be used for Co-processing is biomass and therefore carbon neutral. In this way Co-processing offers a significant potential for the reduction of greenhouse gas emissions from fossil fuels. Furthermore, diverting industrial waste streams from landfills and incinerators without energy recovery contributes to reducing overall CO emissions when used to substitute fossil fuels through Co-processing (as illustrated in the figure below).\n\nFigure 3: Reduction of Emissions through Co-processing\n\nOther factors that must be considered when Co-processing waste include product quality standards, permitting aspects, and transparent communication in order to gain public acceptance.\n\nCoprocessing is the combination of two or more excipients to form materials (coprocessed excipients) of superior functionality and limited unwanted effects. Co-processing is a tool that is employed by pharmaceutical excipients manufacturers and formulation scientists to develop materials with superior performance. Coprocessing is fundamentally based on particle engineering that allows modification of Critical Material Attributes (CMA) of the primary excipients. These modifications are reflected in the resulting coprocessed material as enhanced functionality. The primary excipients are established pharmacopoeial materials. \n\n\nDownloads\n\nVideos\n\n"}
{"id": "3821864", "url": "https://en.wikipedia.org/wiki?curid=3821864", "title": "Colored fire", "text": "Colored fire\n\nColored fire is a common pyrotechnic effect used in stage productions, fireworks and by fire performers the world over. Generally, the color of a flame may be red, orange, blue, yellow, or white, and is dominated by blackbody radiation from soot and steam. When additional chemicals are added to the fuel burning, their atomic emission spectra can affect the frequencies of visible light radiation emitted - in other words, the flame appears in a different color dependent upon the chemical additives. Flame coloring is also a good way to demonstrate how fire changes when subjected to heat and how they also change the matter around them.\n\nTo color their flames, pyrotechnicians will generally use metal salts. Specific combinations of fuels and co-solvents are required in order to dissolve the necessary chemicals. Color enhancers (usually chlorine donors) are frequently added too, the most common of which is polyvinyl chloride. A practical use of colored fire is the flame test, where metal cations are tested by placing the sample in a flame and analyzing the color produced.\n\nEmitted colors depend on the electronic configuration of the elements involved. Heat energy from the flame excites electrons to a higher quantum level, and the atoms emit characteristic colors (photons with energies corresponding to the visible spectrum) as they return to lower energy levels.\n\nFlame colorants are becoming popular while camping. Scouts and other outdoor \"enthusiasts\" have placed sections of copper pipe with holes drilled throughout and stuffed with garden hose onto campfires to create a variety of flame colors. An easier method of coloring campfires has been fueled by commercial products. These packages of flame colorants are tossed onto a campfire or into a fireplace to produce effects.\n\nAlthough these chemicals are very effective at imparting their color into an already existing flame, these substances are not flammable alone. To produce a powder or solid that, when lit, produces a colored flame, the necessary steps are more complex. To get a powder to burn satisfactorily, both a fuel and oxidizer will mostly be needed. Common oxidizers include.\n\n\nMany of these oxidizers also produce a colored flame by themselves. Some of them - as well as the main colorants - are severely toxic and therefore environmentally damaging.\n"}
{"id": "164494", "url": "https://en.wikipedia.org/wiki?curid=164494", "title": "Computer magazine", "text": "Computer magazine\n\nComputer magazines are about computers and related subjects, such as networking and the Internet. Most computer magazines offer (or offered) advice, some offer programming tutorials, reviews of the latest technologies, and advertisements.\n\nMathematics of Computation established in 1943, articles about computers began to appear from 1946 (Volume 2, Number 15) to the end of 1954. Scientific journal.\n\n\"Digital Computer Newsletter\", 1949-1968, founded by Albert Eugene Smith.\n\n\"Computers and Automation\" was published from 1951 to 1978 and is arguably the first computer magazine. It was briefly called \"Roster of Organizations in the Computing Machinery Field\" (1951-1952), and \"The Computing Machinery Field\" (1952-1953) and was published by Edmund Berkeley. It held the first Computer Art Contest circa 1963 and maintained a bibliography on computer art starting in 1966. It also included a monthly estimated census of all installed computer systems starting in 1962.\n\nIEEE Transactions on Computers from 1952, scientific journal.\n\nJournal of the ACM from 1954, scientific journal.\n\nDatamation published in 1957 was another early Computer and Data Processing magazine. It is still being published as an ePublication on the Internet. Futurist Donald Prell was its founder. \n\nInformation and Computation from 1957, scientific journal.\n\nIBM Journal of Research and Development from 1957, scientific journal.\n\nCommunications of the ACM from 1958, mix of science magazine, trade magazine, and a scientific journal\n\nThe Computer Journal from 1958, scientific journal.\n\n\"ACS Newsletter\" 1966-1976, Amateur Computer Society newsletter.\n\nDr Dobbs Journal (1976) is one of the oldest computer magazines still being published, and it was the first to focus on software, rather than hardware.\n\n1980s computer magazines skewed their content towards the hobbyist end of the then-microcomputer market, and used to contain type-in programs, but these have gone out of fashion. The first magazine devoted to this class of computers was Creative Computing. Byte was an influential technical journal that published until the 1990s.\n\nIn 1983 an average of one new computer magazine appeared each week. By late that year more than 200 existed. Their numbers and size grew rapidly with the industry they covered, and \"BYTE\" and \"80 Micro\" were among the three thickest magazines of any kind per issue. \"Compute!\"s editor in chief reported in the December 1983 issue that \"all of our previous records are being broken: largest number of pages, largest-number of four-color advertising pages, largest number of printing pages, and the largest number of editorial pages\". Computers were the only industry with product-specific magazines, like \"80 Micro\", \"PC Magazine\", and \"Macworld\"; their editors vowed to impartially cover their computers whether or not doing so hurt their readers' and advertisers' market, while claiming that their rivals pandered to advertisers by only publishing positive news.\n\nMany magazines ended in 1984, however, as their number exceeded the amount of available advertising revenue despite revenue in the first half of the year five times that of the same period in 1982. Consumers typically bought computer magazines more for advertising than articles, which benefited already leading journals like \"BYTE\" and \"PC Magazine\" and hurt weaker ones. Also affecting magazines was the computer industry's economic difficulties, including the video game crash of 1983, which badly hurt the home-computer market. Dan Gutman, the founder of \"Computer Games\", recalled in 1987 that \"the computer games industry crashed and burned like a bad night of \"Flight Simulator\"—with my magazine on the runway\". \"Antic\"s advertising sales declined by 50% in 90 days, \"Compute!\" number of pages declined from 392 in December 1983 to 160 ten months later, and \"Compute!\" and \"Compute!'s Gazette\"s publisher assured readers in an editorial that his company \"is and continues to be quite successful ... even during these particularly difficult times in the industry\". \"Computer Gaming World\" stated in 1988 that it was the only one of the 18 color magazines that covered computer games in 1983 to survive the crash. \"Compute!\" similarly stated that year that it was the only general-interest survivor of about 150 consumer-computing magazines published in 1983.\n\nSome computer magazines in the 1980s and 1990s were issued only on disk (or cassette tape, or CD-ROM) with no printed counterpart; such publications are collectively (though somewhat inaccurately) known as \"disk magazines\" and are listed separately.\n\nIn some ways the heyday of printed computer magazines was a period during the 1990s, in which a large number of computer manufacturers took out advertisements in computer magazines, so they became quite thick and could afford to carry quite a number of articles in each issue, (Computer Shopper (UK magazine) was a good example of this trend). Some printed computer magazines used to include floppy disks, CD-ROMs, or other media as inserts; they typically contained software, demos, and electronic versions of the print issue.\n\nHowever, with the rise in popularity of the internet, many computer magazines went bankrupt or transitioned to an online-only existence. Exceptions include Wired magazine, which is more of a technology magazine than a computer magazine.\n"}
{"id": "22335708", "url": "https://en.wikipedia.org/wiki?curid=22335708", "title": "Conductor support system", "text": "Conductor support system\n\nOn offshore oil platforms, conductor support systems, also known as conductor supported systems or satellite platforms, are small unmanned installations consisting of little more than a well bay, and a small process plant. They are designed to operate in conjunction with a static production platform which is connected to the platform by flow lines and/or by Umbilical cable.\n\nTraditionally, these jacket-type structures have been installed and used in shallow to medium water depths of up to 40 – 60 meters. The conductor supported system use its inherited strength of the well conductors to support both the wells and the topside structure. \n\nThe conductor supported system is particularly suited to areas with more benign environmental conditions, however it is a common development option even in hurricane / cyclone prone areas such as the Gulf of Mexico or Australia's Carnarvon Basin. The well conductors act as both structural, weight-supporting piles and flowlines for the produced fluids from the well. These are drilled and installed with a drilling jackup rig using conventional drilling / lifting techniques. \n\nA leading proponent of this cost effective style of offshore development was Apache Energy which commissioned numerous Conductor Supported Wellhead Platforms in the Carnarvon Basin to feed its so called \"String of Pearls\" discoveries. \n\nNotably, in this Basin, the small field designs such as Conductor Supported Platforms and Monopods, often lay in proximity to the massive offshore Liquefied Natural Gas fields of the North West Shelf, Gorgon, Wheatstone and Pluto developments.\n"}
{"id": "357854", "url": "https://en.wikipedia.org/wiki?curid=357854", "title": "Crampons", "text": "Crampons\n\nA crampon is a traction device that is attached to footwear to improve mobility on snow and ice during ice climbing. Besides ice climbing, crampons are also used for secure travel on snow and ice, such as crossing glaciers, snowfields and icefields, ascending snow slopes, and scaling ice-covered rock. There are three main attachment systems for footwear: step-in, hybrid, and strap bindings. The first two require boots with welts, as a tension lever attaches the crampon to the heel. The last type (strap bindings) are more versatile and can adapt to virtually any boot or shoe, but often do not fit as precisely as the other two types. \n\nOscar Eckenstein designed the first 10-point crampon in 1908, dramatically reducing the need for step cutting. This design was then made commercially available by the Italian Henry Grivel.\n\nCrampons are made of steel alloy, light weight aluminium, or a combination of the two. Lighter weight crampons are popular for alpine ski touring where demands are generally lower and light weight a premium.\n\nEarly 10-point crampons lacked forward angled spikes and thus required step cutting on steep terrain. In the 1930s two additional forward-slanting points were added, making them exceptional for mountaineering and glacier travel and beginning a revolution in front pointing. There is currently a range of models, including specialized crampons with as many as 14 points and models with single points for ice climbing.\n\nCrampons are fastened to footwear by means of a binding system. Improved attachment systems - such as a cam action \"step-in\" system similar to a ski binding and particularly well adapted to plastic technical mountaineering boots - have widely increased crampon use. Crampons also use a full \"strap-in\" system and a \"hybrid\" binding that features a toe strap at the front and a heel lever at the back.\n\nTo prevent snow from balling up under crampons, especially in temperatures around freezing, most models can be fitted with plastic or rubber \"anti-balling\" systems to reduce build-up. Rubber models use flexion to repel snow while plastic anti-balling plates employ a hydrophobic surface to prevent adhesion.\n\nCrampons are graded C1, C2 and C3 relative to their flexibility and general compatibility with different styles of boots. No crampons are suitable for B0 boots (flexible walking boots).\n\nSpecialized \"ski crampons\" are employed in ski mountaineering on hard snow and ice. Far more common in the Alps than in the United States, these ski crampons are known by their European names: \"\" (German), \"couteaux\" (French) and \"coltelli\" (Italian), literally French and Italian for \"knives\" in those languages.\n\n\n"}
{"id": "18097527", "url": "https://en.wikipedia.org/wiki?curid=18097527", "title": "Delivery bar code sorter", "text": "Delivery bar code sorter\n\nDelivery Bar Code Sorter or DBCS refers to a set of sorting machines used in mail sorting industry primarily by USPS. They sort letters at a rate of approximately 38,000 pieces per hour, with a 99% accuracy rate. A computerized camera reads the addresses of the mail, and sorts it to one of up to 286 pockets, setting it up for delivery by the letter carrier.\n"}
{"id": "4701859", "url": "https://en.wikipedia.org/wiki?curid=4701859", "title": "Department of Science and Technology (Philippines)", "text": "Department of Science and Technology (Philippines)\n\nThe Philippines' Department of Science and Technology (abbreviated as DOST; ), is the executive department of the Philippine Government responsible for the coordination of science and technology-related projects in the Philippines and to formulate policies and projects in the fields of science and technology in support of national development.\n\nThe DOST was formed as the National Science Development Board in June 13, 1958, during the administration of President Carlos P. Garcia. The science body was formed as a result of a law passed in the Congress upon the recommendation of Dr. Frank Co Tui, who was tasked by Garcia to conduct a survey regarding the state of science and technology in the country. It was reorganized as the National Science and Technology Authority (NSTA) in March 17, 1981 and was given broader policy-making and program implementing functions.\n\nIn January 30, 1987, during the administration of President Corazon Aquino, the NSTA was elevated to cabinet-level status with the signing of Executive Order 128, and was renamed as the Department of Science and Technology.\n\n\nThe Department is headed by the Secretary of Science and Technology (Philippines), with the following four undersecretaries and three assistant secretaries\n\n\n\n\n\n"}
{"id": "934683", "url": "https://en.wikipedia.org/wiki?curid=934683", "title": "Digital obsolescence", "text": "Digital obsolescence\n\nDigital obsolescence is a situation where a digital resource is no longer readable because of its archaic format: the physical media, the reader (required to read the media), the hardware, or the software that runs on it is no longer available. \n\nA prime example of this is the BBC Domesday Project from the 1980s, although its data was eventually recovered after a significant amount of effort. Cornell University Library’s digital preservation tutorial (now hosted by ICPSR) has a timeline of obsolete media formats, called the \"Chamber of Horrors\", that shows how rapidly new technologies are created and cast aside.\n\nThe rapid evolution and proliferation of different kinds of computer hardware, modes of digital encoding, operating systems and general or specialized software ensures that digital obsolescence will become a problem in the future. Many versions of word-processing programs, data-storage media, standards for encoding images and films are considered \"standards\" for some time, but in the end are always replaced by new versions of the software or completely new hardware. Files meant to be read or edited with a certain program (for example Microsoft Word) will be unreadable in other programs, and as operating systems and hardware move on, even old versions of programs developed by the same company become impossible to use on the new platform (for instance, older versions of Microsoft Works, before Works 4.5, cannot be run under Windows 2000 or later). \n\nEarly attention was brought to the challenges of preserving machine-readable data by the work of Charles M Dollar in the 1970s, but it was only during the 1990s that libraries and archives came to appreciate the significance of the problem and has been discussed among professionals in those branches, though so far without any obvious solutions other than continual forward-migration of files and information to the latest data-storage standards. File formats should be widespread, backward compatible, often upgraded, and, ideally, open format. In 2002, the National Initiative for a Networked Cultural Heritage cited the following as \"de facto\" formats that are unlikely to be rendered obsolete in the near future: uncompressed TIFF and ASCII and RTF (for text).\n\nIn order to prevent this from happening, it is important that an institution regularly evaluate and explore its current technologies and evaluate its long term business model.\n\nDigital objects are vulnerable to three types of obsolescence:\n\nIn some cases, obsolete technologies are used in a deliberate attempt to avoid data intrusion in a strategy known as \"security through obsolescence\".\n\nUntangling copyright issues also presented a significant challenge for projects attempting to overcome the obsolescence issues related to the BBC Domesday Project. In addition to copyright surrounding the many contributions made by the estimated 1 million people who took part in the project, there are also copyright issues that relate to the technologies employed. It is likely that the Domesday Project will not be completely free of copyright restrictions until at least 2090, unless copyright laws are revised for earlier expiration of software into public domain.\n\nAny organization that has digital records should assess its records to identify any potential risks for file format obsolescence. The Library of Congress maintains Sustainability of Digital Formats, which includes technical details about many different format types. The UK National Archives maintains an online registry of file formats called PRONOM.\n\nIn its 2014 agenda, the National Digital Stewardship Alliance recommended developing File Format Action Plans: \"it is important to shift from more abstract considerations about file format obsolescence to develop actionable strategies for monitoring and mining information about the heterogeneous digital files the organizations are managing.\" \n\nFile Format Action Plans are documents internal to an organization which list the type of digital files in its holdings and assess what actions should be taken to ensure its ongoing accessibility. Examples include the Florida Digital Archive Action Plan and University of Michigan's Deep Blue Preservation and Format Support Policy.\n\nOpen source software is often cited as solution for preventing digital obsolescence. With the available source code the implementation and functionality is transparent and adaptions to modern not obsolete hardware platforms is always possible. Also, there is in general a strong cross-platform culture in the open source software ecosystem, which makes the systems and software more future proof.\n\nOpen standards were created to prevent digital obsolesce of file formats and hardware interfaces. For instance, PDF/A is an open standard based on Adobe Systems PDF format. It has been widely adopted by governments and archives around the world, such as the United Kingdom. The Open Document Format for Office Applications (OpenDocument) has been standardized by OASIS in 2005, and by ISO in 2006.\n\nOn 30 November 2017, the Digital Preservation Coalition released The 'Bit List' of Digitally Endangered Species , identifying file formats at risk, as part of an international campaign to raise awareness of the need to preserve digital materials on the first International Digital Preservation Day. \n\n\n"}
{"id": "48587054", "url": "https://en.wikipedia.org/wiki?curid=48587054", "title": "Dochub", "text": "Dochub\n\nDocHub is an online PDF editor and document signing platform. DocHub lets users add text, draw, add signatures and make document templates. DocHub can integrate with Dropbox, Google Drive, Gmail and Box accounts.\n\nAs of April 2017, DocHub has at least 18.1 million registered users. Over 68 million documents have been created on DocHub.\n\n"}
{"id": "4459250", "url": "https://en.wikipedia.org/wiki?curid=4459250", "title": "Electronics Technicians Association", "text": "Electronics Technicians Association\n\nThe Electronics Technicians Association, International (dba ETA International) is a US-based not-for-profit 501(c) 6 professional association founded in 1978. The association provides certifications in industries such as basic electronics, fiber optics and data cabling, renewable energy, information technology, photonics and precision optics, customer service, biomedical, avionics, wireless communications, radar, and smart home. ETA is also one of the 13 COLEMs (Commercial Operator License Examination Manager) for U.S. Federal Communications Commission (FCC) testing. ETA works with technicians, educators, and military personnel. ETA also partners with companies such as Motorola Solutions to provide certification to their employees.\n\nIn 1965 the U.S. Labor Department, Bureau of Apprenticeship & Training (BAT) instigated a jobs program in cooperation with NEA (National Electronics Association). Local school systems, local TV association members and USDL worked together on an 8,000 hour apprenticeship program aimed at solving the labor shortage problem while finding new vocations for those put out of work by modern technology. This new program would reward trainees, but would not cover experienced technicians. Because of this, the Certified Electronics Technician (CET) program was created.\n\nIn 1970 a group of technicians decided to form an organization to promote the CET program and the electronics industry as a whole. This organization would be called the International Society of Certified Electronics Technicians (ISCET). It became a subdivision of NEA.\n\nIn the mid-1970s NEA and NATESA merged to form the National Electronic Service Dealers Association (NESDA) with ISCET remaining as a subdivision. Due to a power struggle within the organization, ETA was formed in 1978 by a group of former NESDA members and officers. Among those were Richard \"Dick\" Glass and Ron Crow, two of the original founders of the CET program and only administrators at that time. This made it easy to continue the CET program with the new organization.\n\nETA offers certifications in various knowledge areas, but does not offer courses or training in these areas. ETA does, however, offer endorsements of courses offered through educational institutions through their Course Approval program. Maintenance or renewal<ref name=\"http://www.eta-i.org/renew_maintain.html\"></ref> of certifications is required to keep in line with the ISO-17024 standards. Most certifications are good for four years.\n\n\n\n\n\n\n\n\n\n\n\nAll technical certifications are accredited by the International Certification Accreditation Council (ICAC) and align with the ISO-17024 standards.\n\nMembership is open to anyone who is involved in one of the industries ETA serves. Membership allows voting rights for such things as yearly officer elections and service awards as well as by-law changes and other association business. ETA offers six types of membership for educators, professionals, technicians, and students. Each membership includes an subscription to the \"High Tech News\", ETA's bi-monthly membership magazine.\n\n\n"}
{"id": "21577832", "url": "https://en.wikipedia.org/wiki?curid=21577832", "title": "Embedded hypervisor", "text": "Embedded hypervisor\n\nAn embedded hypervisor is a hypervisor that supports the requirements of embedded systems.\n\nThe requirements for an embedded hypervisor are distinct from hypervisors targeting server and desktop applications.\nAn embedded hypervisor is designed into the embedded device from the outset, rather than loaded subsequent to device deployment.\nWhile desktop and enterprise environments use hypervisors to consolidate hardware and isolate computing environments from one another, in an embedded system, the various components typically function collectively to provide the device's functionality. Mobile virtualization overlaps with embedded system virtualization, and shares some use cases.\n\nTypical attributes of embedded virtualization include efficiency, security, communication, isolation and real-time capabilities.\n\nSoftware virtualization has been a major topic in the enterprise space since the late 1960s, but only since the early 2000s has its use appeared in embedded systems. The use of virtualization and its implementation in the form of a hypervisor in embedded systems are very different from enterprise applications. An effective implementation of an embedded hypervisor must deal with a number of issues specific to such applications. These issues include the highly integrated nature of embedded systems, the requirement for isolated functional blocks within the system to communicate rapidly, the need for real-time/deterministic performance, the resource-constrained target environment and the wide range of security and reliability requirements.\n\nA hypervisor provides one or more software virtualization environments in which other software, including operating systems, can run with the appearance of full access to the underlying system hardware, where in fact such access is under the complete control of the hypervisor. These virtual environments are called virtual machines (VM)s, and a hypervisor will typically support multiple VMs managed simultaneously.\n\nHypervisors are generally classed as either type 1 or type 2, depending on whether the hypervisor runs exclusively in supervisor mode or privileged mode (type 1) or is itself hosted by an operating system as a regular application (type 2).\n\nType 1 hypervisors manage key system resources required to maintain control over the virtual machines, and facilitate a minimal trusted computing base (TCB). Type 2 hypervisors typically run as an application within a more general purpose operating system, relying on services of the OS to manage system resources. Nowadays kernel extensions are often loaded to take advantage of hardware with virtualization support.\n\nAn embedded hypervisor is most often a type 1 hypervisor which supports the requirements of embedded systems development. See references and for a more detailed discussion.\n\nThese requirements are summarized below.\n\n\nAn embedded hypervisor typically provides multiple VMs, each of which emulates a hardware platform on which the virtualised software executes. The VM may emulate the underlying native hardware, in which case embedded code that runs on the real machine will run on the virtual machine and vice versa. An emulation of the native hardware is not always possible or desired, and a \"virtual platform\" may be defined instead.\n\nWhen a VM provides a virtual platform, guest software has to be ported to run in this environment, however since a virtual platform can be defined without reliance on the native hardware, guest software supporting a virtual platform can be run unmodified across various distinct hardware platforms supported by the hypervisor.\n\nEmbedded hypervisors employ either paravirtualization or use virtualization features of the underlying CPU. Paravirtualization\nis required in cases where the hardware does not assist, and involves often extensive modifications to core architecture support core of guest kernels. Emulation of hardware at the register level is rarely seen in embedded hypervisors as this is very complex and slow. The custom nature of embedded systems means that the need to support unmodified binary-only guest software which require these techniques is rare.\n\nThe size and efficiency of the implementation is also an issue for an embedded hypervisor, as embedded systems are often much more resource constrained than desktop and server platforms. It is also desirable for the hypervisor to maintain, as closely as possible, the native speed, real-time response and determinism and power efficiency of the underlying hardware platform.\n\nImplementations for embedded systems applications have most commonly been based on small microkernel and separation kernel designs, with virtualization built-in as an integral capability. This was introduced with PikeOS in 2005. Examples of these approaches have been produced by companies such as Open Kernel Labs (microkernel followed by a separation kernel) and LynuxWorks (separation kernel). VirtualLogix appears to take the position that an approach based on a dedicated Virtual Machine Monitor (VMM) would be even smaller and more efficient. This issue is the subject of some ongoing debate. However, the main point at issue is the same on all sides of the discussion – the speed and size of the implementation (for a given level of functionality) are of major importance. For example: \" ... hypervisors for embedded use must be real-time capable, as well as resource-miserly.\"\n\nEmbedded systems are typically highly resource constrained due to cost and technical limitations of the hardware. It is therefore important for an embedded hypervisor to be as efficient as possible. The microkernel and separation kernel based designs allow for small and efficient hypervisors. \nThus embedded hypervisors usually have a memory footprint from several tens to several hundred kilobytes, depending on the efficiency of the implementation and the level of functionality provided. An implementation requiring several megabytes of memory (or more) is generally not acceptable.\n\nWith the small TCB of a type 1 embedded hypervisor, the system can be made highly secure & reliable. Standard software-engineering techniques, such as code inspections and systematic testing, can be used to reduce the number of bugs in such a small code base to a tiny fraction of the defects that must be expected for a hypervisor and guest OS combination that may be 100,000–300,000 lines in total.\n\nOne of the most important functions required in an embedded hypervisor is a secure message-passing mechanism, which is needed to support real-time communication between processes. In the embedded environment, a system will typically have a number of closely coupled tasks, some of which may require secure isolation from each other. In a virtualized environment, the embedded hypervisor will support and enforce this isolation between multiple VMs. These VMs will therefore require access to a mechanism that provides low-latency communication between the tasks.\n\nAn inter-process communication (IPC) mechanism can be used to provide these functions, as well as invoking all system services, and implemented in a manner which ensures that the desired level of VM isolation is maintained. Also, due to its significant impact on system performance, such an IPC mechanism should be highly optimised for minimal latency.\n\nAn embedded hypervisor needs to be in complete control of system resources, including memory accesses, to ensure that software cannot break out of the VM. A hypervisor therefore requires the target CPU to provide memory management support (typically using an MMU). Many embedded processors including such as ARM, MIPS and PowerPC have followed desktop and server chip vendors in adding hardware support for virtualization. There are still a large proportion of embedded processors however which do not provide such support and a hypervisor supporting paravirtualization is required.\n\nARM processors are notable in that most of their application class processor designs support a technology called ARM TrustZone, which provides essentially hardware support for one privileged and one unprivileged VM. Normally a minimal Trusted Execution Environment (TEE) OS is run in the \"Secure World\" and a native kernel running in the \"Non-secure World\".\n\nSome of the most common use cases for an embedded hypervisor are:\n\n1. OS independence\nDesigners of embedded systems may have many hardware drivers and system services which are specific to a target platform. If support for more than one OS is required on the platform, either concurrently or consecutively using a common hardware design, an embedded hypervisor can greatly simplify the task. Such drivers and system services can be implemented just once for the virtualized environment; these services are then available to any hosted OS. This level of abstraction also allows the embedded developer to implement or change a driver or service in either hardware or software at any point, without this being apparent to the hosted OS.\n\n2. Support for multiple operating systems on a single processor\nTypically this is used to run a real-time operating system (RTOS) for low-level real-time functionality (such as the communication stack) while at the same time running a general purpose OS, (GPOS) like Linux or Windows, to support user applications, such as a web browser or calendar. The objective might be to upgrade an existing design without the added complexity of a second processor, or simply to minimize the bill of materials (BoM).\n\n3. System security\nAn embedded hypervisor is able to provide secure encapsulation for any subsystem defined by the developer, so that a compromised subsystem cannot interfere with other subsystems. For example, an encryption subsystem needs to be strongly shielded from attack to prevent leaking the information the encryption is supposed to protect. As the embedded hypervisor can encapsulate a subsystem in a VM, it can then enforce the required security policies for communication to and from that subsystem.\n\n4. System reliability\nThe encapsulation of a subsystem components into a VM ensures that failure of any subsystem cannot impact other subsystems. This encapsulation keeps faults from propagating from a subsystem in one VM to a subsystem in another VM, improving reliability. This may also allow a subsystem to be automatically shut down and restarted on fault detection. This can be particularly important for embedded device drivers, as this is where the highest density of fault conditions is seen to occur, and is thus the most common cause of OS failure and system instability. It also allows the encapsulation of operating systems that were not necessarily built to the reliability standards demanded of the new system design.\n\n5. Dynamic update of system software\nSubsystem software or applications can be securely updated and tested for integrity, by downloading to a secure VM before “going live” in an executing system. Even if this process then fails, the system can revert to its former state by restarting the original software subsystem/application, without halting system operation.\n\n6. Legacy code re-use\nVirtualization allows legacy embedded code to be used with the OS environment it has been developed and validated with, while freeing the developer to use a different OS environment in a separate VM for new services and applications. Legacy embedded code, written for a particular system configuration may assume exclusive control of all system resources of memory, I/O and processor. This code base can be re-used unchanged on alternative system configurations of I/O and memory through the use of a VM to present a resource map and functionality that is consistent with the original system configuration, effectively de-coupling the legacy code from the specifics of a new or modified hardware design.\nWhere access to the operating system source code is available, paravirtualization is commonly used to virtualize the OS’s on processors without hardware virtualization support, and thus the applications supported by the OS can also run unmodified and without re-compilation in new hardware platform designs.\nEven without source access, legacy binary code can be executed in systems running on processors with hardware virtualization support such as the AMD-V, Intel VT technologies and the latest ARM processors with virtualization support. The legacy binary code could run completely unmodified in a VM with all resource mapping handled by the embedded hypervisor, assuming the system hardware provides equivalent functionality.\n\n7. IP protection\nValuable proprietary IP may need protection from theft or misuse when an embedded platform is being shipped for further development work by (for example) an OEM customer. An embedded hypervisor makes it possible to restrict access by other system software components to a specific part of the system containing IP that needs to be protected.\n\n8. Software license segregation\nSoftware IP operating under one licensing scheme can be separated from other software IP operating under a different scheme. For example, the embedded hypervisor can provide an isolated execution environment for proprietary software sharing the processor with open source software subject to the GPL.\n\n9. Migration of applications from uni-core to multi-core systems\nAs new processors utilise multi-core architectures to increase performance, the embedded hypervisor can manage the underlying architecture and present a uni-processor environment to legacy applications and operating systems while efficiently using the new multiprocessor system design. In this way a change in hardware environment does not require a change to the existing software.\n\n\n"}
{"id": "29649962", "url": "https://en.wikipedia.org/wiki?curid=29649962", "title": "Enzmann starship", "text": "Enzmann starship\n\nThe Enzmann starship is a concept for a manned interstellar spacecraft proposed in 1964 by Dr. Robert Enzmann. A three million ton ball of frozen deuterium would fuel nuclear fusion rocket engines contained in a cylindrical section behind that ball with the crew quarters. It would be longer than the Empire State Buildingthe craft would be about long overall.\n\nThe ball of frozen deuterium would fuel thermonuclear-powered pulse propulsion units, similar to Project Orion engines. The spacecraft would be assembled in Earth orbit as part of a larger project preceded by interstellar probes and telescopic observation of target star systems. The rest of the spacecraft would be attached behind the ball as a seamless metallic fuel tank. The proposed method of tank construction would be to expand a plastic balloon in space and coat it with metal.\n\nThe spacecraft would be modular, and the main living area would be three identical wide and long cylindrical modules. The Enzmann could function as an interstellar ark, supporting a crew of 200 but with space for expansion.\n\nThe Enzmann starship was detailed in the October 1973 issue of \"Analog\", with a cover by space artist Rick Sternbach. The spacecraft described in that issue had some differences compared to the 1960s proposal, such as using a 12,000,000 ton (11,000,000 tonnes) ball of frozen deuterium. Enzmanns have been depicted by many space artists including Don Dixon, David A. Hardy, Syd Mead, Bob Eggleton, and Rick Sternbach.\n\nSources conflict about the projected speed, perhaps 30% of the speed of light, \"c\", but 9% may be more likely. At 30%, relativistic effects between people on Earth and on the spacecraft, such as time dilation would become more noticeable, such as the shipboard time being less than the Earth observed time.\n\nOverall specifications have varied somewhat, but the design has nuclear pulse engines at the rear, then cylinders for human habitation, then closer the front a large ball of fuel. Early versions were said to have 8 engines and later 24 nuclear rocket engines, which would be powered by the fusing of deuterium into Helium three. A common feature was that the crew area was replicated 3 times for redundancy, and the there was common core pillar that ran the length of the spacecraft and through the center of each habitation unit.\n\n\n\n\n\n"}
{"id": "2124647", "url": "https://en.wikipedia.org/wiki?curid=2124647", "title": "Forever Living Products", "text": "Forever Living Products\n\nForever Living Products International, Inc. (FLPI) is an American privately-held multi-level marketing (MLM) company based in Scottsdale, Arizona, which manufactures and markets aloe vera-based drinks and bee-derived cosmetics, dietary supplements, and personal care products. The company was founded in 1978 by CEO Rex Maughan. After acquiring the company Aloe Vera of America by the 1990s, In 2010, the company reported having over 4,000 employees, a network of 9.3 million distributors, and revenue of $1.7 billion.\n\nForever Living was founded in 1978 in Tempe, Arizona by Carl Jensen and Rex Maughan. By the 1990s, Maughan had purchased the Texas company Aloe Vera of America, with Aloe Vera of America selling its products to Forever Living for distribution. Some journalists have likened the multi-level marketing business model of Forever Living's distribution system to that of a pyramid scheme.\nIn 1983, the company was named No. 6 on \"Inc. Magazine\"'s annual Inc. 500 list of the fastest-growing private companies in the United States.\n\nAccording to Arthur Andersen's Top 100, as of 1993, Forever Living Products International was Arizona's second-largest private company. As of August 1995, \"Forbes\" reported the company's product line included \"deodorants, toothpaste, laundry detergent and three dozen other products, nearly all of which contain extract of aloe.\"\n\nAnnual revenue exceeded $1.15 billion in 2005 and Forever Living ended the year with around 150,000 distributors, and 55 employees. The following year, Forever Living was listed at No. 340 on the Forbes 400 list, which ranks the largest private companies in the United States. At the time, the company was described as having 4,100 employees and sold its product in 100 countries.\"\n\nIn 2010, revenue was $1.7 billion, and the company was selling its products through 9.3 million distributors. In 2013 the publication \"New Vision\" reported that Forever Living had over 20,000 distributors in Uganda, of which only 83 had reached a managerial level and begun to recoup expenses; their investigation concluded that Forever Living Products' \"distribution system does not guarantee profits and majority of members drop out along the way, after losing millions.\" The company was active in over 150 countries as of 2015. In February 2015, the company announced they had appointed a new management team to \"oversee the affairs of the company in Nigeria.\"\n\nIn 1996, upon suggestion of the American authorities, the Internal Revenue Service (IRS) and the National Tax Agency of Japan (NTA) initiated a joint audit of Rex and Ruth Maughan and related entities Aloe Vera of America (AVA), Selective Art Inc., FLP International, and FLP Japan for the period of 1991 to 1995. In 1997, the NTA imposed a penalty tax of 3.5 billion yen on Forever Living's Japan division for concealing income of 7.7 billion yen over the five-year period. Later that year, AVA, Rex and Ruth Maughan, Maughan Holdings, Gene Yamagata, and Yamagata Holdings sued the IRS for unauthorized disclosure of tax return information. In the midst of the lawsuit, The IRS asked the NTA to drop its decision against Forever Living, and in 2002, the agency “grudgingly complied with the IRS's request”, announcing that the penalty tax had been effectively withdrawn. In February 2015, a USA district court ruled that the IRS knowingly provided some false information about AVA to the NTA, in violation of the United States' tax treaty with Japan. and awarded three of the plaintiffs one thousand dollars each in statutory damages.\n\nIn 2004 claims made about Forever Living products were found to be in violation of several laws in Hungary related to advertising, registration of nutritional products, and the use of cosmetics as medicinal agents. As a result, the company was fined 60 million HUF (approximately $280,000 USD).\n\nIn 2007, author Richard Bach made claims against the company for copyright infringement and trademark infringement. The lawsuit stated that for over 20 years Forever Living had used the character, storyline, and copyrighted excerpts from the novel \"Jonathan Livingston Seagull\" to promote its marketing plan, and also used the motion picture and novel as its corporate logo. The claim was satisfied through arbitration, and shortly after, Forever Living changed its company logo from a seagull to an eagle.\n\nIn 2015, Forever Living was criticized by the UK Advertising Standards Authority for making false claims about the health benefits of its products, which were sold as a cure for various diseases ranging from diabetes to Crohn's disease. The company was also warned not to use health professionals in its promotional materials. Subsequently, the UK Medicines And Healthcare Products Regulatory Agency launched an investigation after it was revealed that NHS staff were moonlighting as sales people.\n\n\n"}
{"id": "40254820", "url": "https://en.wikipedia.org/wiki?curid=40254820", "title": "Fuels America", "text": "Fuels America\n\nFuels America is a coalition of organizations working in support of the Renewable Fuel Standard. The website of the group states their purpose as \"promoting the benefits of all types of renewable fuel already growing in America.\n\nFuels America was announced in 2012 during a conference call of the Biotechnology Industry Organization (BIO).\n\nExamples of the efforts of Fuels America include an ad campaign launched in July 2013, with a focus on choice in energy. The ad campaign was \"targeted to policy makers in Washington DC and focused on supporting the Renewable Fuel Standard.\" Concurrent activities also included testifying in front of the United States Congress, such as testimony given by Bob Dinneen in July 2013 to a subcommittee for the United States House Committee on Energy and Commerce.\n\nIn late 2015, Fuels America conducted an advertising campaign to support the Renewable Fuel Standard. The campaign included a full-page advertisement in the New York Times and online ads targeted at the Washington, DC area.\n"}
{"id": "17800413", "url": "https://en.wikipedia.org/wiki?curid=17800413", "title": "GPS navigation device", "text": "GPS navigation device\n\nA GPS navigation device, GPS receiver, or simply GPS is a device that is capable of receiving information from GPS satellites and then to calculate the device's geographical position. Using suitable software, the device may display the position on a map, and it may offer directions. The Global Positioning System (GPS) is a global navigation satellite system (GNSS) made up of a network of a minimum of 24, but currently 30, satellites placed into orbit by the U.S. Department of Defense.\n\nThe GPS was originally developed for use by the United States military, but in the 1980s, the United States government allowed the system to be used for civilian purposes. Though the GPS satellite data is free and works anywhere in the world, the GPS device and the associated software must be bought or rented.\n\nA GPS device can retrieve from the GPS system location and time information in all weather conditions, anywhere on or near the Earth. A GPS reception requires an unobstructed line of sight to four or more GPS satellites, and is subject to poor satellite signal conditions. In exceptionally poor signal conditions, for example in urban areas, satellite signals may exhibit multipath propagation where signals bounce off structures, or are weakened by meteorological conditions. Obstructed lines of sight may arise from a tree canopy or inside a structure, such as in a building, garage or tunnel. Today, most standalone GPS receivers are used in automobiles. The GPS capability of smartphones may use assisted GPS (A-GPS) technology, which can use the base station or cell towers to provide a faster Time to First Fix (TTFF), especially when GPS signals are poor or unavailable. However, the mobile network part of the A-GPS technology would not be available when the smartphone is outside the range of the mobile reception network, while the GPS aspect would otherwise continue to be available.\n\nThe Russian Global Navigation Satellite System (GLONASS) was developed contemporaneously with GPS, but suffered from incomplete coverage of the globe until the mid-2000s. GLONASS can be added to GPS devices to make more satellites available and enabling positions to be fixed more quickly and accurately, to within 2 meters.\n\nUsing the GPS information and subject to the sophistication of installed GPS software, a GPS device used as an automobile navigation system may be used in a number of contexts, including:\n\nGPS devices may be able to indicate:\n\nAs with many other technological breakthroughs of the latter 20th century, the modern GPS system can reasonably be argued to be a direct outcome of the Cold War of the latter 20th century. The multibillion-dollar expense of the program was initially justified by military interest.\n\nIn 1960, the US Navy put into service its Transit satellite based navigation system to aid in ship navigation. Between 1960 and 1982, as the benefits were been shown, the US military consistently improved and refined its satellite navigation technology and satellite system. In 1973, the US military began to plan for a comprehensive worldwide navigational system which eventually became known as the GPS (global positioning system). In 1983, in the wake of the tragedy of the downing of the Korean Airlines Flight 007, an aircraft which was shot down while in Soviet airspace due to a navigational error, President Reagan made the navigation capabilities of the existing military GPS system available for dual civilian use. However, civilian use was initially only a slightly degraded \"Selective Availability\" positioning signal. This new availability of the US military GPS system for civilian use required a certain technical collaboration with the private sector for some time, before it could become a commercial reality. In 1989, Magellan Navigation Inc. unveiled its Magellan NAV 1000, the world’s first commercial handheld GPS receiver. These units initially sold for approximately US$2,900 each. In 2000, the Clinton administration removed the military use signal restrictions, thus providing full commercial access to the US GPS satellite system.\n\nIn 1990, Mazda's Eunos Cosmo was the first production car in the world with a built-in GPS navigation system. In 1991, Mitsubishi introduced GPS car navigation on the Mitsubishi Debonair (MMCS: Mitsubishi Multi Communication System). In 1997, a navigation system using Differential GPS was developed as a factory-installed option on the Toyota Prius.\n\nAs GPS navigation systems became more and more widespread and popular, the pricing of such systems began to fall, and their widespread availability steadily increased. Also, several additional manufacturers of these systems, such as Garmin (1991), Benefon (1999), Mio (2002) and TomTom (2002) entered the market. Mitac Mio 168 was the first PocketPC to contain a built-in GPS receiver. Benefon's 1999 entry into the market also presented users with the world's first phone based GPS navigation system. Later, as smartphone technology developed, a GPS chip eventually became standard equipment for most smartphones. To date, ever more popular GPS navigation systems and devices continue to proliferate with newly developed software and hardware applications. It has been incorporated, for example, into cameras.\n\nWhile the American GPS was the first satellite navigation system to be deployed on a fully global scale, and to be made available for commercial use, this is not the only system of its type. Due to military and other concerns, similar global or regional systems have been, or will soon be deployed by Russia, the European Union, China, India, and Japan.\n\nGPS devices vary in sensitivity, speed, vulnerability to multipath propagation, and other performance parameters. High Sensitivity GPS receivers use large banks of correlators and digital signal processing to search for GPS signals very quickly. This results in very fast times to first fix when the signals are at their normal levels, for example outdoors. When GPS signals are weak, for example indoors, the extra processing power can be used to integrate weak signals to the point where they can be used to provide a position or timing solution.\n\nGPS signals are already very weak when they arrive at the Earth’s surface. The GPS satellites only transmit 27 W (14.3 dBW) from a distance of 20,200 km in orbit above the Earth. By the time the signals arrive at the user's receiver, they are typically as weak as −160 dBW, equivalent to one tenth of a million-billionth of a watt (100 attowatts). This is well below the thermal noise level in its bandwidth. Outdoors, GPS signals are typically around the −155 dBW level (−125 dBm).\n\nConventional GPS receivers integrate the received GPS signals for the same amount of time as the duration of a complete C/A code cycle which is 1 ms. This results in the ability to acquire and track signals down to around the −160 dBW level. High Sensitivity GPS receivers are able to integrate the incoming signals for up to 1,000 times longer than this and therefore acquire signals up to 1,000 times weaker, resulting in an integration gain of 30 dB. A good High Sensitivity GPS receiver can acquire signals down to −185 dBW, and tracking can be continued down to levels approaching −190 dBW.\n\nHigh Sensitivity GPS can provide positioning in many but not all indoor locations. Signals are either heavily attenuated by the building materials or reflected as in multipath. Given that High Sensitivity GPS receivers may be up to 30 dB more sensitive, this is sufficient to track through 3 layers of dry bricks, or up to 20 cm (8 inches) of steel reinforced concrete for example.\n\nExamples of high sensitivity receiver chips include SiRFstarIII and MediaTekʼs MTK II.\n\nConsumer GPS navigation devices include:\n\nDedicated devices have various degrees of mobility. \"Hand-held\", \"outdoor\", or \"sport\" receivers have replaceable batteries that can run them for several hours, making them suitable for hiking, bicycle touring and other activities far from an electric power source. Their screens are small, and some do not show color, in part to save power. Some use transflective liquid-crystal displays, allowing use in bright sunlight. Cases are rugged and some are water resistant.\n\nOther receivers, often called \"mobile\" are intended primarily for use in a car, but have a small rechargeable internal battery that can power them for an hour or two away from the car. Special purpose devices for use in a car may be permanently installed and depend entirely on the automotive electrical system.\n\nThe pre-installed embedded software of early receivers did not display maps; 21st century ones commonly show interactive street maps (of certain regions) that may also show points of interest, route information and step-by-step routing directions, often in spoken form with a feature called \"text to speech\".\n\nManufacturers include:\n\nDue in part to regulations encouraging mobile phone tracking, including E911, the majority of GPS receivers are built into mobile telephones, with varying degrees of coverage and user accessibility. Commercial navigation software is available for most 21st-century smartphones as well as some Java-enabled phones that allows them to use an internal or external GPS receiver (in the latter case, connecting via serial or Bluetooth). Some phones using assisted GPS (A-GPS) function poorly when out of range of their carrier's cell towers. Others can navigate worldwide with satellite GPS signals as well as a dedicated portable GPS receiver does, upgrading their operation to A-GPS mode when in range. Still others have a hybrid positioning system that can use other signals when GPS signals are inadequate.\nMore bespoke solutions also exist for smartphones with inbuilt GPS capabilities. Some such phones can use tethering to double as a wireless modem for a laptop, while allowing GPS-navigation/localisation as well. One such example is marketed by Verizon Wireless in the United States, and is called VZ Navigator. The system uses gpsOne technology to determine the location, and then uses the mobile phone's data connection to download maps and calculate navigational routes. Other products including iPhone are used to provide similar services. Nokia gives HERE Maps free on mobile operating systems: Windows Phone 8, Android, Sailfish OS and FirefoxOS, but excluding iOS. Maps can be preloaded onto the device.\n\nAccording to market research from the independent analyst firm Berg Insight, the sales of GPS-enabled GSM/WCDMA handsets was 150 million units in 2009, while only 40 million separate GPS receivers were sold.\n\nGPS navigation applications for mobile phones include on-line (e.g. Waze, Google Maps Navigation, Apple Maps) and off-line (e.g. iGo for Android, Maverick and HERE for Windows Phone) navigation applications. Google Maps Navigation, which is included with Android, means most smartphone users only need their phone to have a personal navigation assistant.\n\nMany Android smartphones have an additional GPS feature, called \"Extended Prediction Orbit\" (EPO). The phone downloads a file to help it locate GPS satellites more quickly and reduce the time to first fix.\n\nSoftware companies have made available GPS navigation software programs for in-vehicle use on laptop computers. Benefits of GPS on a laptop include larger map overview, ability to use the keyboard to control GPS functions, and some GPS software for laptops offers advanced trip-planning features not available on other platforms, such as midway stops, capability of finding alternative scenic routes as well as only highway option.\n\nPalms and Pocket PC's can also be equipped with GPS navigation. A pocket PC differs from a dedicated navigation device as it has an own operating system and can also run other applications.\n\nOther GPS devices need to be connected to a computer in order to work. This computer can be a home computer, laptop, PDA, digital camera, or smartphones. Depending on the type of computer and available connectors, connections can be made through a serial or USB cable, as well as Bluetooth, CompactFlash, SD, PCMCIA and the newer ExpressCard. Some PCMCIA/ExpressCard GPS units also include a wireless modem.\n\nDevices usually do not come with pre-installed GPS navigation software, thus, once purchased, the user must install or write their own software. As the user can choose which software to use, it can be better matched to their personal taste. It is very common for a PC-based GPS receiver to come bundled with a navigation software suite. Also, GPS modules are significantly cheaper than complete stand-alone systems (around €50 to €100). The software may include maps only for a particular region, or the entire world, if software such as Google Maps are used.\n\nSome hobbyists have also made some GPS devices and open-sourced the plans. Examples include the Elektor GPS units. These are based around a SiRFstarIII chip and are comparable to their commercial counterparts. Other chips and software implementations are also available.\n\nCommercial aviation applications include GPS devices that calculate location and feed that information to large multi-input navigational computers for autopilot, course information and correction displays to the pilots, and course tracking and recording devices.\n\nMilitary applications include devices similar to consumer sport products for foot soldiers (commanders and regular soldiers), small vehicles and ships, and devices similar to commercial aviation applications for aircraft and missiles. Examples are the United States military's Commander's Digital Assistant and the Soldier Digital Assistant. Prior to May 2000 only the military had access to the full accuracy of GPS. Consumer devices were restricted by selective availability (SA), which was scheduled to be phased out but was removed abruptly by President Clinton. Differential GPS is a method of cancelling out the error of SA and improving GPS accuracy, and has been routinely available in commercial applications such as for golf carts. GPS is limited to about 15 meter accuracy even without SA. DGPS can be within a few centimeters.\n\nA sequential GPS receiver tracks the necessary satellites by typically using one or two hardware channels. The set will track one satellite at a time, time tag the measurements and combine them when all four satellite pseudoranges have been measured. These receivers are among the least expensive available, but they cannot operate under high dynamics and have the slowest time-to-first-fix (TTFF) performance.\n\nGPS maps and directions are occasionally imprecise. Some people have gotten lost by asking for the shortest route, like a couple in the United States who were looking for the shortest route from South Oregon to Jackpot, Nevada.\n\nIn August 2009 a young mother and her six-year-old son became stranded in Death Valley after following GPS directions that led her up an unpaved dead end road. When they were found five days later, her son had died from the effects of heat and dehydration.\n\nIn May 2012, Japanese tourists in Australia were stranded when traveling to North Stradbroke Island and their GPS receiver instructed them to drive into Moreton Bay.\n\nOther hazards involve an alley being listed as a street, a lane being identified as a road, or rail tracks as a road.\n\nObsolete maps sometimes cause the unit to lead a user on an indirect, time-wasting route, because roads may change over time. Smartphone GPS information is usually updated automatically, and free of additional charge. Manufacturers of separate GPS devices also offer map update services for their merchandise, usually for a fee.\n\nDue to the popularity of GPS devices, privacy of the user becomes a subject of debate. This is because they can give geo-location information of the user. Some commentators think this is private information, and not to be violated without legal approval. However, there were several incidents where the privacy of GPS devices was questioned.\n\nSince GPS devices can give the user's exact location, this helps with location-based advertising. Agencies might promote shops which are near the user, rather than irrelevant, distant ones. The advertising agency also will store the user's location for future use. However, regulatory agents (especially in USA and Europe) have questioned whether geo-location data should be a sensitive data or not. If it is sensitive data, the can not store it since this amounts to a privacy violation. However, if the regulatory agents choose to consider geo-location as non-sensitive data, then private companies can have permission to store it.\n\nPrivacy concerns also arise when employers use GPS tracking units to track their employees' location, for example using vehicle tracking systems. This raises a major question about whether this violates personal privacy of employees. Concern heightens if the employers collect geo-location data of their employee when not at work. In 2010, New York Civil Liberties Union filed a case against the Labor Department for firing Michael Cunningham after tracking Michael Cunningham's daily activity and locations using a GPS device that was attached to his car. This raises a few questions regarding the limit of surveillance. In 2011 the FBI tracked down Antoine Jones's GPS devices, without a search warrant. Later the Federal Appeal Court rejected the FBI's surveillance data as evidence against Antoine Jones.\n\nGPS devices are also used by private investigators to give more information to their clients. They will plant their own GPS devices to learn about their target. Moreover, some rental car services use the same technique to prevent their customers from going out of their targeted area. They charge additional fees for those who violate their rules. They get this information from the car's GPS devices.\n\n"}
{"id": "12293814", "url": "https://en.wikipedia.org/wiki?curid=12293814", "title": "Girt", "text": "Girt\n\nIn architecture or structural engineering, a girt is a horizontal structural member in a framed wall. Girts provide lateral support to the wall panel, primarily, to resist wind loads.\n\nMay also be known as a sheeting rail.\n\nA comparable element in roof construction is a purlin.\n\nThe girt is commonly used as a stabilizing element to the primary structure (e.g. column, post). Wall cladding fastened to the girt, or a discrete bracing system which includes the girt, can provide shear resistance, in the plane of the wall, along the length of the primary member. Since the girts are normally fastened to, or near, the exterior flange of a column, stability braces may be installed at a girt to resist rotation of the unsupported, inner flange of the primary member. The girt system must be competent and adequately stiff to provide the required stabilizing resistance in addition to its role as a wall panel support.\n\nGirts are stabilized by (sag) rods/angles/straps and by the wall cladding. Stabilizing rods are discrete brace members to prevent rotation of an unsupported flange of the girt. Sheet metal wall panels are usually considered to provide lateral bracing to the connected, typically exterior flange along the length of the girt. Under restricted circumstances, sheet metal wall panels are also capable of providing rotational restraint to the girt section.\n\nIn general: Girt supports panel, panel stabilizes girt; Column supports girt, girt stabilizes column. The building designer should be knowledgeable in the complexities of this interactive design condition to ensure competent design of the complete structure.\n\nAn article of clothing: Reference to wearing a girt and having to bleach it often was made by British soldiers during World War II.\n\n"}
{"id": "1524152", "url": "https://en.wikipedia.org/wiki?curid=1524152", "title": "Glitching", "text": "Glitching\n\nGlitching is an activity in which a person finds and exploits flaws or glitches in video games to achieve something that was not intended by the game designers. Gamers who engage in this practice are known as glitchers. Glitches can help or disable the player.\n\n\"Glitching\" is also used to describe the state of a video game undergoing a glitch. The frequency in which a game undergoes glitching is often used by reviewers when examining the overall gameplay, or specific game aspects such as graphics. Some games such as Metroid have lower review scores today because in retrospect, the game may be very prone to glitches and be below what would be acceptable today.\n\nVideo game glitches that go \"out of bounds\" are mostly performed by either moving through walls or corners or jumping to places in the map that do not have invisible walls. For example, in \"Tony Hawk's Underground 2\", in the L.A. level there is a glitch that can allow players to leave the provided play area and pass through the background. Another example of this is a glitch on the Nürburgring track in Gran Turismo 5 where if the player squeezes through any gap between the walls, the car can drive through the scenery and under the paved course and can finish a lap or the race faster than usual by driving directly under the finish line in the direction of the race's path. Another example in the Need for Speed series is . In Rockport there is a spot where you can glitch in Point Camden near Bay Bridge.\n\nIn \"out of bounds\" areas, many maps have hollow objects that the player can move through freely. These objects usually are in the distance and are for decoration. The floor or terrain can also be hollow. The floor can appear the same as a normal floor, but moving over it will cause the player to fall as if it does not exist. Depending on the game, after falling a certain distance the player will freeze, die, respawn on the map again or just keep falling. Two good examples are the Nintendo 64 games \"\" and \"\". In the former, there is a section of \"wall\" at the entrance to the water temple that will allow players to fall through the ground. Eventually players respawn rather than the game crashing. In the latter, there is a 3-day cycle (after which the moon will fall) that can be surpassed by going to the observatory and looking out the telescope on the last minutes of the final day. If you look out the telescope while the timer is counting down and exit the telescope when the timer hits 0, (if you're lucky) the timer will be gone and you can continue the game without it. The glitch is removed when you play the Song of Time.\n\nMany other glitches may also include background music being played at the time it was not intended to play, such as the Western \"Super Mario Bros. 2\" (based on the Japan-only game \"Doki Doki Panic\") has a glitch where the Subspace music (the Overworld background music from the original \"Super Mario Bros.\") can be played outside of Subspace after the player becomes invincible and enters Subspace and leaves before invincibility drains away (this was fixed for \"Super Mario All-Stars\" and \"Super Mario Advance\" so Subspace music can only be played in Subspace). One of the best known examples for glitching in an online game is , where people can glitch into rooms they're not supposed to, or get under the map (for example by glitching a helicopter to spawn under the map and fly into beta rooms). is also known for glitching and hacking. Glitching Players are able to edit the classes far beyond regular players. The two almost make the game unplayable because of the cheating from the glitching and hacking.\n\nSome games you can take advantage of the programmers only assuming that you will have positive values for things like money and when a negative value is acquired you can then get near-unlimited money. An example of this is in FIFA 11 where if you have a player on loan and no money left in the transfer budget, you can cancel the contract and the game will not check for a negative value. Once the loan is cancelled, the money is subtracted from the player resulting in a negative money value that then overflows back to the top of the positive value possibilities, resulting in the player having near the highest amount of money possible.\n\nThere are also physics glitches, such as the glitch with the swingset in \"Grand Theft Auto IV\", where if the player climbs on top of the swingset, the shaking of its chains will cause the physics to oversell the impact and send the player flying a great distance away. Similarly in GTA III, if the player stands in a gate or is in a car (but still aligned within in a gate) and the gate is closed, the player character will be flung a great distance.\n\nSome glitches occur randomly, as with a glitch in NCAA Football 11 which will cause the football game to run longer than the time indicated on the clock and, most often, indefinitely.\n\n"}
{"id": "23218263", "url": "https://en.wikipedia.org/wiki?curid=23218263", "title": "Homeostat", "text": "Homeostat\n\nThe Homeostat is one of the first devices capable of adapting itself to the environment; it exhibited behaviours such as habituation, reinforcement and learning through its ability to maintain homeostasis in a changing environment. It was built by William Ross Ashby in 1948 at Barnwood House Hospital. After a few technical hiccups with short-circuits causing burn-outs, the homeostat was finally completed on the 16th March, 1948. It was an adaptive ultrastable system, consisting of four interconnected Royal Air Force bomb control units with inputs, feedback, and magnetically driven, water-filled potentiometers. It illustrated his law of requisite variety — automatically adapting its configuration to stabilize the effects of any disturbances introduced into the system.\n\nIn 1946, Ashby described the design of the units thus \"Its principle is that it uses multiple coils in a milliammeter & uses the needle movement to dip in a trough carrying a current, so getting a potential which goes to the grid of a valve, the anode of which provides an output current.\" It was the realization of what he had described in 1946 as an \"Isomorphism making machine\".\n\nWhen Alan Turing heard of Ashby's intention to build the Homeostat, he wrote to Ashby to suggest that he could run a simulation on Turing's Automatic Computing Engine (ACE) instead of building a special machine.\n\nThe first published account of the homeostat appeared under the title of \"Design for a Brain\" in the December 1948 issue of 'Electronic Engineering', where he speculates about a perfected homeostat that could eventually play chess \"with a subtlety and depth of strategy beyond that of the man who designed it.\"\n\nIn 1949 \"Time\" described it as \"the closest thing to a synthetic brain so far designed by man\".\n\nIn 1952, Ashby demonstrated it at the ninth Macy conference on cybernetics. In the same year he published a description of the Homeostat in his influential book \"Design for a brain\". In total, between 1946 and 1967, he wrote 38 entries about the Homeostat in his journal.\n\n\n"}
{"id": "554215", "url": "https://en.wikipedia.org/wiki?curid=554215", "title": "Inertial platform", "text": "Inertial platform\n\nAn inertial platform, also known as a gyroscopic platform or stabilized platform, is a system using gyroscopes to maintain a platform in a fixed orientation in space despite the movement of the vehicle that it is attached to. These can then be used to stabilize gunsights in tanks, anti-aircraft artillery on ships, and as the basis for older mechanically-based inertial navigation systems.\n"}
{"id": "83139", "url": "https://en.wikipedia.org/wiki?curid=83139", "title": "Intermediate frequency", "text": "Intermediate frequency\n\nIn communications and electronic engineering, an intermediate frequency (IF) is a frequency to which a carrier wave is shifted as an intermediate step in transmission or reception. The intermediate frequency is created by mixing the carrier signal with a local oscillator signal in a process called heterodyning, resulting in a signal at the difference or beat frequency. Intermediate frequencies are used in superheterodyne radio receivers, in which an incoming signal is shifted to an IF for amplification before final detection is done.\n\nConversion to an intermediate frequency is useful for several reasons. When several stages of filters are used, they can all be set to a fixed frequency, which makes them easier to build and to tune. Lower frequency transistors generally have higher gains so fewer stages are required. It's easier to make sharply selective filters at lower fixed frequencies.\n\nThere may be several such stages of intermediate frequency in a superheterodyne receiver; two or three stages are called \"double\" (alternatively, \"dual\") or \"triple\" \"conversion\", respectively.\n\nIntermediate frequencies are used for three general reasons. At very high (gigahertz) frequencies, signal processing circuitry performs poorly. Active devices such as transistors cannot deliver much amplification (gain). Ordinary circuits using capacitors and inductors must be replaced with cumbersome high frequency techniques such as striplines and waveguides. So a high frequency signal is converted to a lower IF for more convenient processing. For example, in satellite dishes, the microwave downlink signal received by the dish is converted to a much lower IF at the dish, to allow a relatively inexpensive coaxial cable to carry the signal to the receiver inside the building. Bringing the signal in at the original microwave frequency would require an expensive waveguide.\n\nA second reason, in receivers that can be tuned to different frequencies, is to convert the various different frequencies of the stations to a common frequency for processing. It is difficult to build multistage amplifiers, filters, and detectors that can have all stages track in tuning different frequencies, but it is comparatively easy to build tunable oscillators. Superheterodyne receivers tune in different frequencies by adjusting the frequency of the local oscillator on the input stage, and all processing after that is done at the same fixed frequency, the IF. Without using an IF, all the complicated filters and detectors in a radio or television would have to be tuned in unison each time the frequency was changed, as was necessary in the early tuned radio frequency receivers. A more important advantage is that it gives the receiver a constant bandwidth over its tuning range. The bandwidth of a filter is proportional to its center frequency. In receivers like the TRF in which the filtering is done at the incoming RF frequency, as the receiver is tuned to higher frequencies its bandwidth increases.\n\nThe main reason for using an intermediate frequency is to improve frequency selectivity. In communication circuits, a very common task is to separate out or extract signals or components of a signal that are close together in frequency. This is called filtering. Some examples are, picking up a radio station among several that are close in frequency, or extracting the chrominance subcarrier from a TV signal. With all known filtering techniques the filter's bandwidth increases proportionately with the frequency. So a narrower bandwidth and more selectivity can be achieved by converting the signal to a lower IF and performing the filtering at that frequency. FM and television broadcasting with their narrow channel widths, as well as more modern telecommunications services such as cell phones and cable television, would be impossible without using frequency conversion.\n\nPerhaps the most commonly used intermediate frequencies for broadcast receivers are around 455 kHz for AM receivers and 10.7 MHz for FM receivers. In special purpose receivers other frequencies can be used. A dual-conversion receiver may have two intermediate frequencies, a higher one to improve image rejection and a second, lower one, for desired selectivity. A first intermediate frequency may even be higher than the input signal, so that all undesired responses can be easily filtered out by a fixed-tuned RF stage.\n\nIn a digital receiver, the analog to digital converter (ADC) operates at low sampling rates, so input RF must be mixed down to IF to be processed. Intermediate frequency tends to be lower frequency range compared to the transmitted RF frequency. However, the choices for the IF are most dependent on the available components such as mixer, filters, amplifiers and others that can operate at lower frequency. There are other factors involved in deciding the IF frequency, because lower IF is susceptible to noise and higher IF can cause clock jitters.\n\nModern satellite television receivers use several intermediate frequencies. The 500 television channels of a typical system are transmitted from the satellite to subscribers in the Ku microwave band, in two subbands of 10.7 - 11.7 and 11.7 - 12.75 GHz. The downlink signal is received by a satellite dish. In the box at the focus of the dish, called a low-noise block downconverter (LNB), each block of frequencies is converted to the IF range of 950 - 2150 MHz by two fixed frequency local oscillators at 9.75 and 10.6 GHz. One of the two blocks is selected by a control signal from the set top box inside, which switches on one of the local oscillators. This IF is carried into the building to the television receiver on a coaxial cable. At the cable company's set top box, the signal is converted to a lower IF of 480 MHz for filtering, by a variable frequency oscillator. This is sent through a 30 MHz bandpass filter, which selects the signal from one of the transponders on the satellite, which carries several channels. Further processing selects the channel desired, demodulates it and sends the signal to the television.\n\nAn intermediate frequency was first used in the superheterodyne radio receiver, invented by American scientist Major Edwin Armstrong in 1918, during World War I. A member of the Signal Corps, Armstrong was building radio direction finding equipment to track German military signals at the then-very high frequencies of 500 to 3500 kHz. The triode vacuum tube amplifiers of the day would not amplify stably above 500 kHz, however, it was easy to get them to oscillate above that frequency. Armstrong's solution was to set up an oscillator tube that would create a frequency near the incoming signal, and mix it with the incoming signal in a 'mixer' tube, creating a 'heterodyne' or signal at the lower difference frequency, where it could be amplified easily. For example, to pick up a signal at 1500 kHz the local oscillator would be tuned to 1450 kHz. Mixing the two created an intermediate frequency of 50 kHz, which was well within the capability of the tubes.\n\nAfter the war, in 1920, Armstrong sold the patent for the superheterodyne to Westinghouse, who subsequently sold it to RCA. The increased complexity of the superheterodyne circuit compared to earlier regenerative or tuned radio frequency receiver designs slowed its use, but the advantages of the intermediate frequency for selectivity and static rejection eventually won out; by 1930, most radios sold were 'superhets'. During the development of radar in World War II, the superheterodyne principle was essential for downconversion of the very high radar frequencies to intermediate frequencies. Since then, the superheterodyne circuit, with its intermediate frequency, has been used in virtually all radio receivers.\n\n\n"}
{"id": "37753951", "url": "https://en.wikipedia.org/wiki?curid=37753951", "title": "Internally grooved copper tube", "text": "Internally grooved copper tube\n\nInternally grooved copper tubes also known as \"microfin tubes\" are a small diameter coil technology for modern air conditioning and refrigeration systems. Grooved coils facilitate more efficient heat transfer than smooth coils. Small diameter coils have better rates of heat transfer than conventional-sized condenser and evaporator coils with round copper tubes and aluminium or copper fin that have been the standard in the HVAC industry for many years. Small diameter coils can withstand higher pressures required by the new generation of environmentally friendlier refrigerants. They have lower material costs because they require less refrigerant, fin, and coil materials. And they enable the design of smaller and lighter high-efficiency air conditioners and refrigerators because the evaporators and condensers coils are smaller and lighter.\n\nThe performance benefits of copper as a heat transfer material include its high heat exchange properties, long-term durability, resistance to corrosion, low maintenance cost, and antimicrobial properties.\n\nWith MicroGroove technology, heat transfer is enhanced by grooving the inside surface of the tube. This increases the surface to volume ratio, mixes the refrigerant, and homogenizes refrigerant temperatures across the tube.\n\nTubes with MicroGroove technology can be made with copper or aluminium. Copper fins are an attractive alternative to aluminium due to the better corrosion resistance of copper and its antimicrobial benefits.\n\nTo use smaller tubes instead of conventional-sized tubes in air conditioners, heat exchangers must be redesigned. This includes redesigning the fin and tube circuits. Design optimization requires the use of computational fluid dynamics to analyze airflow around the tubes and fins, as well as computer simulations of refrigerant flow and temperatures inside the tubes. This is important because the overall heat transfer coefficient of a coil is a function of the convection of the refrigerant inside the tube to the tube wall, conduction through the tube wall, and dissipation through the fins.\n\nEngineering considerations for using Microgroove include:\n\nPublished experiments on MicroGroove coil performance and energy efficiency take into account the effects of fin spacing and fin design, tube diameter, and tube circuitry. Tube circuitry is substantially different than for conventional coils. Coils should be optimized with respect to the number of paths between the inlet and outlet manifolds. Typically, smaller diameter tubes require more paths of shorter lengths. Published research on tube circuitry and fin design for heat exchangers made with 4 mm tubes are available.\n\nResearch on a heat exchanger redesign with 5-mm diameter tubes demonstrated a 5% greater heat exchange capacity than that of the same size heat exchanger with 7-mm diameter tubes. Also, the refrigerant charge of the 5-mm diameter tubes was less than the 7-mm diameter tubes. In China, Chigo, Gree, and Kelon are producing air conditioners with coils that have 5-mm diameter tubes.\n\nA variety of fin designs have been developed for use with small-diameter copper tubes. The performance of slotted and louvered fin designs have been evaluated and compared as a function of various fin dimensions. Simulations have been used to optimize fin design performance.\n\nThe phasing out of CFC and HCFC refrigerants (e.g., HCFC-22, also known as R22) due to global warming concerns has helped to spur innovations in cooling technologies. Natural refrigerants such as carbon dioxide (R744) and propane (R290), as well as R-410A, have become attractive replacements for air conditioning and refrigeration applications.\n\nHigher pressures are typically required to condense these new environmentally friendly refrigerants compared to those that are being phased out. Small diameter copper tubes are more desirable in applications with higher pressures. For tubes of the same thickness, smaller diameter tubes can withstand higher pressures than larger diameter tubes. Hence, as tube diameters decrease, burst pressures increase. This is because working pressure is directly proportional to wall thickness and inversely proportional to diameter. By designing coils with shorter tube lengths, less work is required to circulate the refrigerant. Therefore, refrigerant pressure drop factors due to small diameter tubes can be offset.\n\nCarbon dioxide (R744) refrigerants are used in modern vending machines, refrigerated supermarket display cases, ice-skating rinks, and other emerging applications. Microgroove’s smaller diameter copper tubes have the strength to withstand the very high gas cooler and burst pressures of R744 while allowing for lower overall refrigerant volumes.\n\nPropane (R290) is an eco-friendly refrigerant with outstanding thermodynamic propertiies. The pressure requirements for R290 are much less than for carbon dioxide, but R290 is extremely flammable. Research has demonstrated that MicroGroove is suitable for R290-charged room air conditioners because the refrigerant charge requirement is dramatically reduced with smaller diameter copper tubes. The risk of tube explosions is dramatically reduced as well. Research conducted with propane in MicroGroove has implications for heat exchanger coils used in refrigerators, heat pumps and commercial air conditioning systems.\n\nIn a design study of functionally equivalent 5-kW HVAC heat exchangers, tube materials in the coils weighed 3.09 kg for 9.52-mm diameter tube, 2.12 kg for 7-mm diameter tube, and 1.67 kg for 5-mm diameter tube. Tube weight was reduced by 31% when copper tube diameters were downsized from 3/8 inch to 7 mm. Tube weight was reduced by 46% when copper tube diameters were downsized from 3/8 inch to 5 mm. The weights of the fin materials in the coils was 3.55 kg for the 9.52 mm coils, 2.61 kg for the 7 mm coils, and 1.55 kg for the 5 mm coils.\n\nCopper is an antimicrobial material. Bio buildup can be reduced with copper coils. This helps to maintain high levels of energy efficiency for longer periods of time and avoids energy efficiency drop off over time.\n\nThe use of copper coils to inhibit the growth of fungi and bacteria is a recent development in innovative air conditioning and refrigeration products. OEM companies, such as Chigo in China and Hydronic in France, are now manufacturing all-copper antimicrobial air conditioning systems to improve indoor air quality.\n\nSmaller diameter refrigerant paths can also be realized with extruded aluminium tubes. These have been designed with several microchannels in one flat, ribbon-like tube. Aluminium microchannel technology offers significant advantages over conventional copper-aluminium round tube plate fin coil, including improved heat transfer performance and reduced refrigerant charge.\nHowever, copper MicroGroove offers higher heat transfer efficiencies than aluminium microchannel tubes and it enables smaller refrigerant volumes because the tube ends of MicroGroove are connected by small U-joints rather than large headers.\n\nCopper tubes are often produced by a cast and roll process. Copper ingots are cast into mother tubes and these tubes are then drawn to a final shape, annealed, and enhanced with an inner surface texture to improve heat transfer performance. The production of small diameter copper tubes requires only the addition of one or two additional drawing passes to achieve 5-mm tube diameters.\n\nExisting air conditioner coils made of round copper tubes and aluminium fins (CTAF coils) typically are mechanically assembled using tube expansion.\n\nThe equipment used in manufacturing Microgroove products expands the tubes circumferentially (i.e., the circumference of the tube is increased without changing the length). This \"non-shrinkage\" expansion allows for better control of tube lengths in preparation for subsequent assembly operations. Tubes are inserted, or laced, into the holes in a stack of precisely spaced fins. Expanders are inserted into the tubes and the tube diameters are increased slightly until mechanical contact is achieved between the tubes and fins. The high ductility of copper allows for this process to be performed accurately and precisely. Heat exchanger coils made in this manner have excellent durability and heat transfer properties.\n\nThe small-diameter tube project in China involves manufacturers who together account for more than 80 percent of HVAC production of approximately 75 million units. Several OEMs in North America are marketing residential air-conditioner products with copper tubes. Air-conditioner OEMs, including Guangdong Chigo Air Conditioning, the Refrigeration Research Institute of Guangdong Midea Refrigeration Appliances Group, and Shanghai Golden Dragon Refrigeration Technology Co., Ltd. have described the benefits of small-diameter copper tubes versus the standard for various designs and diameters. ACR coils from original equipment manufacturers (OEMs) Gree, Haier, Midea, Chigo and HiSense Kelon are also available.\n\nCopper in heat exchangers\n\nSimulation-Based Comparison of Optimized AC Coils Using Small Diameter Copper and Aluminium Microchannel Tubes, by John Hipchen, Robert Weed, Ming Zhang, Dennis Nasuta (2012). The Fourteenth International Refrigeration and Air Conditioning Conference; July 2012; (Purdue)\n"}
{"id": "43632558", "url": "https://en.wikipedia.org/wiki?curid=43632558", "title": "Joe Wiseman Howland", "text": "Joe Wiseman Howland\n\nJoe Wiseman Howland, M.D., Ph.D. (21 December 1908 – 12 October 1978) a pioneer researcher in radiation toxicity, health and safety. Howland served as a Major in the U.S. Army as Chief, Research Branch, Medical Division on the Manhattan Project. He worked in the Medical Division of the Atomic Energy Project at the University of Rochester. Dr. Howland was a consultant on radiation exposure, occupational safety and civil defense with various state and federal agencies.\n\nJoe Wiseman Howland was born in Plain City, Ohio in 1908. In 1928 he received his B.S. degree from Denison University in Granville, Ohio. In 1929 he received his M.Sc. in zoology from Ohio State University, and in 1931 his Ph.D. in zoology.\nFrom 1931-1932 he was Instructor in Biology at St. Lawrence University in Canton, New York, and returned to Ohio State in 1932-1933 as Honorary Fellow.\n\nFrom 1933-1934 Howland attended the University of Rochester School of Medicine & Dentistry as a medical student. From 1935-1936 he was a Student Fellow in Pathology and received his medical degree in 1938 at the age of 30.\nHowland stayed at the University of Rochester until his retirement in 1965. From 1938-1939 he served his internship at Strong Memorial Hospital. Howland was an Assistant in Medicine and Assistant Resident Physician in 1939. In 1940 he received the Edith H. Gleason Fellow in Medicine. In 1941 he was appointed Instructor in Medicine at the Medical School and Resident Physician in Charge of the Medical Outpatient Department.\n\nUnder the direction of Howland, the University of Rochester was the first university to be licensed by the U.S. Atomic Energy Commission to use radioisotopes in clinical procedures at hospitals. The University of Rochester Isotope Distribution Center organized and conducted radioisotope use programs in teaching, diagnostic and therapy applications at hospitals across central New York and western New York.\n\n\nFrom 1944-1947 Howland served in the Armed Forces. In 1944 he joined the Manhattan Project of the Corps of Engineers, which had started to develop a fission bomb. He served as medical officer in charge of special problems. This opportunity allowed him to become one of the early authorities on the medical effects of radiation exposure. Howland designed toxicity experiments and established occupational standards and emergency procedures.\nIn 1945 he was assigned to Oak Ridge, Tennessee as Assistant Chief of Medical Research for the Manhattan Project. He organized the information from the Manhattan Project's laboratories and work sites. Howland organized the parties that went to Japan to analyze the effects of radiation poisoning, and served as chief internist-pathologist to the Nagasaki group. This was the first American group to investigate the effects of radiation on the casualties at Hiroshima and Nagasaki. The group was replaced by the U.S. Army team led by Stafford Warren which later became the Atomic Bomb Casualty Commission.\nIn 1946 Howland was appointed Chief of Medical Research for the Manhattan District during the transition into the U.S. Atomic Energy Commission. In 1947 he was discharged with the rank of major, and remained a consultant to the U.S. Atomic Energy Commission.\n\nHowland returned to Rochester. He was: \"convinced that there was a future in atomic medicine.\" He advanced to Professor of Radiation Biology and Chief of the Medical Division of the Atomic Energy Project at the Medical School.\nFrom 1947-1965 Howland was Chief, Atomic Energy Project Medical Division, and supervised the health & safety activities. He directed and conducted research into the pathologic physiology of exposure to ionizing radiation, dosimetry of radiation exposure, effects of microwave irradiation, standards for radiation protection and the medical aspects of civil defense.\n\nOn 8 March 1960 in Lockport, New York an accidental X-ray exposure was received by nine civilian radar technicians that worked at the Lockport Air Force Station, a U.S. Air Force radar station that was part of the SAGE network. The recently installed Klystron tube failed to operate properly when voltage was applied. The technicians were exposed to X-rays while troubleshooting the problem. (NOTE: The Klystron tube serves as a voltage amplifier for the radar transmitter.) The technicians made an error in judgment and thought that the Klystron tube would not produce X-rays when the machine was not producing a radiofrequency signal. The men were exposed for a two-hour period with the Klystron tube at 60 percent of full voltage with 150 KeV X-rays being produced at 90 mA current.\n\nHowland worked as a consultant to the Surgeon General's Office, Walter Reed Army Institute of Research, the Armed Forces Radiobiology Research Institute, NASA, the New York State Department of Health, and the New York State Office of Atomic and Space Development.\n\nHowland was a prolific writer for medical and scientific journals. He also maintained a private medical practice. He was married and the father of four sons.\nIn 1973 Howland moved to Chapel Hill, North Carolina and was Director of the North Carolina Alcoholic Rehabilitation Center at Butner, North Carolina. Joe Wiseman Howland died at Chapel Hill, North Carolina in 1978.\n\n\n\n\n\n"}
{"id": "41795239", "url": "https://en.wikipedia.org/wiki?curid=41795239", "title": "John R. Huizenga", "text": "John R. Huizenga\n\nJohn Robert Huizenga (April 21, 1921 – January 25, 2014) was an American physicist who helped build the first atomic bomb and who also received more recent fame for attempting to debunk Utah scientists' claim of achieving cold fusion.\n\nJohn Robert Huizenga was born on a farm near Fulton, Illinois, the son of Henry (Harry) and Josie (Brands) Huizenga. He attended Erie High School and Morrison High School, graduating from the latter in 1940. He continued his education at Calvin College in Michigan, from which he received a bachelor's degree in 1944. He would maintain his ties to Calvin later in life, for example collaborating on fundamental nuclear research with his Calvin friend Roger Griffioen, who had gone on to become a professor there. Calvin would name him one of the college's Distinguished Alumni in 1975.\n\nAlong with other Calvin students, he was recruited after graduation to work for the Manhattan Project, at the Project's site in Oak Ridge, Tennessee that was dedicated to the production of highly enriched uranium. Following his time in Oak Ridge, he continued his education at the University of Illinois, receiving a Doctor of Philosophy degree in physical chemistry in 1949. On completing his studies he held joint appointments at the University of Chicago and Argonne National Laboratory.\n\nDuring World War II, Huizenga supervised teams at the Manhattan Project in Oak Ridge, Tenn. involved in enriching uranium used in the atomic weapon dropped on Hiroshima in August 1945. During his Argonne years, as a result of examining debris from the \"Ivy Mike\" nuclear test in 1952, Huizenga was part of the team that added two new synthetic chemical elements, einsteinium and fermium, to the periodic table. Huizenga and his colleagues were at first unable to publish papers on their discoveries in the open literature, because of classification concerns relating to the nuclear test, but these concerns were eventually resolved and the team was able to publish in \"Physical Review\" and thus claim priority for their discovery. During his Argonne years he was one of the founders of the Gordon Research Conferences on nuclear chemistry, serving as chairman of the nuclear chemistry Gordon Conference in 1958. He received a Guggenheim Fellowship in 1964 and took a sabbatical from Argonne to further his studies as a visiting professor at the University of Paris for the 1964-1965 academic year.\n\nIn 1967, he became a professor of chemistry and physics at the University of Rochester where he worked for the remainder of his career, apart from a second Guggenheim Fellowship that allowed him to engage in research during the 1973-1974 school year at the University of California, Berkeley, the Technische Universität München, and the Niels Bohr Institute in Copenhagen. His research interests at Rochester covered topics in nuclear structure of actinides, nuclear fission, and nuclear reactions between heavy ions. He was chairman of the Department of Chemistry from 1983 to 1988, retiring as Tracy H. Harris Professor (later Professor Emeritus) of Chemistry.\n\nDuring Huizenga's time at Rochester, the university had its own particle accelerator, a tandem Van de Graaff accelerator that produced beams of nuclei accelerated to energies of several MeV per nucleon. This facility, which opened in 1966, afforded him the opportunity to continue his research program in experimental nuclear science. However, the limited beam energies available led him to more powerful accelerators, such as the SuperHILAC at Berkeley and the Los Alamos Meson Physics Facility, LAMPF, at Los Alamos National Laboratory, for his experimental work. His LAMPF proposal to study actinide muonic atoms was one of the earliest experiments to receive beam time at the LAMPF stopped-muon facility.\n\nIn 1989, Huizenga co-chaired, with Norman Ramsey, a panel convened by the United States Department of Energy which attempted to debunk claims by two University of Utah chemists that they had achieved nuclear fusion at room temperature. The findings of the Huizenga/Ramsey panel, although highly skeptical of the reality of cold fusion, were cautious:\n\nBased on the examination of published reports, reprints, numerous communications to the Panel and several site visits, the Panel concludes that the experimental results of excess heat from calorimetric cells reported to date do not present convincing evidence that useful sources of energy will result from the phenomena attributed to cold fusion. ... The Panel concludes that the experiments reported to date do not present convincing evidence to associate the reported anomalous heat with a nuclear process. ...\n\nCurrent understanding of the very extensive literature of experimental and theoretical results for hydrogen in solids gives no support for the occurrence of cold fusion in solids. Specifically, no theoretical or experimental evidence suggests the existence of D-D distances shorter than that in the molecule D2 or the achievement of \"confinement\" pressure above relatively modest levels. The known behavior of deuterium in solids does not give any support for the supposition that the fusion probability is enhanced by the presence of the palladium, titanium, or other elements.\n\nNuclear fusion at room temperature, of the type discussed in this report, would be contrary to all understanding gained of nuclear reactions in the last half century; it would require the invention of an entirely new nuclear process.\n\nHowever, Huizenga later published a book titled \"Cold Fusion: The Scientific Fiasco of the Century\".\n\nHuizenga was elected to the National Academy of Sciences in 1976 and the American Academy of Arts and Sciences (Fellow) in 1992. He was a 1966 recipient of the Ernest Orlando Lawrence Award bestowed by the United States Atomic Energy Commission.\n\nHuizenga married the former Dorothy (Dolly) Koeze in 1946. They had two sons and two daughters. Their son Robert Huizenga is a prominent physician whose career has included a stint as team physician for the Los Angeles Raiders American football team. Dolly Huizenga died in 1999.\n\nFollowing his retirement from Rochester, Huizenga and his wife moved to North Carolina, where he continued to serve on advisory committees at major accelerator laboratories, worked to debunk cold fusion, and wrote his memoirs. He died of heart failure in San Diego, California in January 2014.\n\n"}
{"id": "48839520", "url": "https://en.wikipedia.org/wiki?curid=48839520", "title": "Karlovitz number", "text": "Karlovitz number\n\nIn Combustion, the Karlovitz number is defined as the ratio of chemical time scale formula_1 to Kolmogorov time scale formula_2, named after Béla Karlovitz. The number reads as\n\nIn premixed turbulent combustion, the chemical time scale can be defined as formula_4, where formula_5 is the thermal diffusivity and formula_6 is the laminar flame speed and the flame thickness is given by formula_7, in which case,\n\nwhere formula_9 is the Kolmogorov scale. The Karlovitz number is related to Damköhler number as\n\nif the Damköhler number is defined with Kolmogorov scale. If formula_11, the premixed turbulent flame falls into the category of corrugated flamelets and wrinked flamelets, otherwise into the thin reaction zone or broken reaction zone flames.\n\nIn premixed turbulent combustion, Klimov-Williams criterion or Klimov-Williams limit is the condition where formula_12, named after A.M. Klimov and Forman A. Williams. When formula_11, the flame thickness is smaller than the Kolmogorov scale, thus the flame burning velocity is not affected by the turbulence field. Here, the burning velocity is given by the laminar flame speed and these laminar flamelets are called as wrinkled flamelets or corrugated flamelets, depending on the turbulence intensity. When formula_14, the turbulent transport penetrates into the preheat zone of the flame (thin reaction zone) or even into the reactive-diffusive zone (distributed flames).\n"}
{"id": "16546820", "url": "https://en.wikipedia.org/wiki?curid=16546820", "title": "Lise Watier", "text": "Lise Watier\n\nLise Watier is a Canadian prestige cosmetic company and brand, launched in 1972 by Madame Lise Watier, and distributed through department and select drugstores.\n\nLise Watier was born in Montreal in 1942. Her father worked his way up to become manager of a car dealership and her mother had a sense of fashion.\n\nShe told the \"Toronto Star\" that the heavy makeup she wore for her job as a researcher and host for a women's interest television program from 1963 to 1968 inspired her to create a self-improvement and makeup course company, Charme et Beauté Lise Watier Inc., in 1965, and to launch her own line of cosmetics in 1972.\n\nThe Montreal-based company first attempted to expand in the 1980s, but a fire in the company's factory and offices was a major setback. Watier later told the \"National Post\", \"It took until 1995 to get back to the point we were at in 1990.\" By the turn of the century, the company's cosmetics were distributed in France and across Canada.\n\nWatier was a family-owned company; Watier's husband, Serge Rocheleau was chairman and general manager, and daughters Nathalie and Marie-Lise worked in marketing.\n\nThe brand is distributed in Canada and the United States.\n\nIn 1986, the company's founder was honoured as Canadian business woman of the year.\n\nIn 2007, Imperial Capital Corporation of Toronto, an investment firm, purchased a majority stake in the company, with Watier and Rocheleau retaining seats on the board and a minority share in the company.\n\nIn 2009, the company signed Mitsou Gélinas, a Canadian singer, actor and television personality, as ambassador.\n\nIn 2013, Watier stepped down as CEO, appointing Pierre Plasard, previously of L'Oréal, to take her place. As of that time, the company had 175 employees and did approximately $90 million in sales annually.\n\nThe company was acquired by Groupe Marcelle Inc. in 2016.\n\n"}
{"id": "57185549", "url": "https://en.wikipedia.org/wiki?curid=57185549", "title": "List of Vietnamese inventions and discoveries", "text": "List of Vietnamese inventions and discoveries\n\nThis is a list of Vietnamese inventions and discoveries which includes cultural and historical inventions. This list is incomplete.\n\n"}
{"id": "408778", "url": "https://en.wikipedia.org/wiki?curid=408778", "title": "List of aircraft weapons", "text": "List of aircraft weapons\n\nThis is a list of weapons (aircraft ordnance) carried by aircraft.\n\nIn World War I, aircraft were initially intended for aerial reconnaissance, however some pilots began to carry rifles in case they spotted enemy planes. Soon, planes were fitted with machine guns with a variety of mountings; initially the only guns were carried in the rear cockpit supplying defensive fire (this was employed by two-seat aircraft all through the war). Seeing a need for offensive fire, forward-firing weapons were devised. The Airco DH.2 pusher plane had its gun in the front while the engine was in the back, some experimented with mountings on the (side) wing or on the biplane's upper wing (above the cockpit), until by 1916 most fighter aircraft mounted their guns in the forward fuselage using a synchronization gear so that the bullets did not strike the propeller.\n\nIn World War II, fighter aircraft carried machine guns and cannons mounted in the wings, engine cowlings, nose, or between the banks of the engine, firing through the propeller spinner. Night fighters sometimes utilized guns firing upwards as well. Bombers typically carried from one to 14 flexible machine guns and/or autocannon as defensive armament, while certain types added fixed offensive guns as well.\n\nWhile missiles have been the primary armament since the early 1960s, the Vietnam War showed that guns still had a role to play and most fighters built since then are fitted with cannons (typically between 20 and 30 mm in caliber) as an adjunct to missiles. Modern European fighter aircraft are usually equipped with the revolver cannon, whereas the United States and to some extent Russia generally favor the Gatling gun. The Gatling gun quickly became the weapon of choice for most air forces.\n\n\n\n\n\n\n"}
{"id": "47300", "url": "https://en.wikipedia.org/wiki?curid=47300", "title": "Movable type", "text": "Movable type\n\nMovable type (US English; moveable type in British English) is the system and technology of printing and typography that uses movable components to reproduce the elements of a document (usually individual alphanumeric characters or punctuation marks) usually on the medium of paper.\n\nThe world's first movable type printing press technology for printing paper books was made of porcelain materials and was invented around AD 1040 in China during the Northern Song Dynasty by the inventor Bi Sheng (990–1051). Subsequently in 1377, the world's oldest extant movable metal print book, Jikji, was printed in Korea during the Goryeo dynasty. \n\nBecause of this, the diffusion of both movable-type systems was, to some degree, limited to primarily East Asia, although various sporadic reports of movable type technology were brought back to Europe by Christian missionaries, traders and business people who were returning to Europe after having worked in China for several years; and influenced the development of printing technology in Europe. Some of these medieval European accounts are still preserved in the library archives of the Vatican and Oxford University among many others. \n\nAround 1450, Johannes Gutenberg introduced the metal movable-type printing press in Europe, along with innovations in casting the type based on a matrix and hand mould. The small number of alphabetic characters needed for European languages was an important factor. Gutenberg was the first to create his type pieces from an alloy of lead, tin, and antimony—and these materials remained standard for 550 years.\n\nFor alphabetic scripts, movable-type page setting was quicker than woodblock printing. The metal type pieces were more durable and the lettering was more uniform, leading to typography and fonts. The high quality and relatively low price of the Gutenberg Bible (1455) established the superiority of movable type in Europe and the use of printing presses spread rapidly. The printing press may be regarded as one of the key factors fostering the Renaissance and due to its effectiveness, its use spread around the globe.\n\nThe 19th-century invention of hot metal typesetting and its successors caused movable type to decline in the 20th century.\n\nThe technique of imprinting multiple copies of symbols or glyphs with a master \"type\" punch made of hard metal first developed around 3000 BC in ancient Sumer. These metal punch types can be seen as precursors of the letter punches adapted in later millennia to printing with movable metal type. Cylinder seals were used in Mesopotamia to create an impression on a surface by rolling the seal on wet clay. They were used to \"sign\" documents and mark objects as the owner's property. Cylinder seals were a related form of early typography capable of printing small page designs in relief (\"cameo\") on wax or clay—a miniature forerunner of rotogravure printing used by wealthy individuals to seal and certify documents. By 650 BC the ancient Greeks were using larger diameter punches to imprint small page images onto coins and tokens.\n\nThe designs of the artists who made the first coin punches were stylized with a degree of skill that could not be mistaken for common handiwork—salient and very specific types designed to be reproduced \"ad infinitum\". Unlike the first typefaces used to print books in the 13th century, coin types were neither combined nor printed with ink on paper, but \"published\" in metal—a more durable medium—and survived in substantial numbers. As the portable face of ruling authority, coins were a compact form of standardized knowledge issued in large editions, an early mass medium that stabilized trade and civilization throughout the Mediterranean world of antiquity.\n\nSeals and stamps may have been precursors to movable type. The uneven spacing of the impressions on brick stamps found in the Mesopotamian cities of Uruk and Larsa, dating from the 2nd millennium BC, has been conjectured by some archaeologists as evidence that the stamps were made using movable type. The enigmatic Minoan Phaistos Disc of 1800–1600 BC has been considered by one scholar as an early example of a body of text being reproduced with reusable characters: it may have been produced by pressing pre-formed hieroglyphic \"seals\" into the soft clay. A few authors even view the disc as technically meeting all definitional criteria to represent an early incidence of movable-type printing. Recently it has been alleged by Jerome Eisenberg that the disk is a forgery.\n\nThe Prüfening dedicatory inscription is medieval example of movable type stamps being used.\n\nFollowing the invention of paper in the 2nd century AD during the Chinese Han Dynasty, writing materials became more portable and economical than the bones, shells, bamboo slips, metal or stone tablets, silk, etc. previously used. Yet copying books by hand was still labour-consuming. Not until the Xiping Era (172–178 AD), towards the end of the Eastern Han Dynasty did sealing print and monotype appear. It was soon used for printing designs on fabrics, and later for printing texts.\n\nWoodblock printing, invented by about the 8th century during the Tang Dynasty, worked as follows. First, the neat hand-copied script was stuck on a relatively thick and smooth board, with the front of the paper, which was so thin that it was nearly transparent, sticking to the board, and characters showing in reverse, but distinctly, so that every stroke could be easily recognized. Then carvers cut away the parts of the board that were not part of the character, so that the characters were cut in relief, completely differently from those cut intaglio. When printing, the bulging characters would have some ink spread on them and be covered by paper. With workers' hands moving on the back of paper gently, characters would be printed on the paper. By the Song Dynasty, woodblock printing came to its heyday. Although woodblock printing played an influential role in spreading culture, there remained some apparent drawbacks. Firstly, carving the printing plate required considerable time, labour and materials; secondly, it was not convenient to store these plates; and finally, it was difficult to correct mistakes.\n\nWith woodblock printing, one printing plate could be used for tens of hundreds of books, playing a magnificent role in spreading culture. Yet carving the plate was time and labour consuming. Huge books cost years of effort. The plates needed a lot of storage space, and were often damaged by deformation, worms and corrosion. If books had a small print run, and were not reprinted, the printing plates would become nothing but waste; and worse, if a mistake was found, it was difficult to correct it without discarding the whole plate.\n\nBi Sheng (毕昇/畢昇) (990–1051) developed the first known movable-type system for printing in China around 1040 AD during the Northern Song dynasty, using ceramic materials. As described by the Chinese scholar Shen Kuo (沈括) (1031–1095):\n\nIn 1193, Zhou Bida, an officer of Southern Song Dynasty, made a set of clay movable-type method according to the method described by Shen Kuo in his \"Dream Pool Essays\", and printed his book \"Notes of The Jade Hall\" (《玉堂雜記》).\n\nThe claim that Bi Sheng's clay types were \"fragile\" and \"not practical for large-scale printing\" and \"short lived\" was refuted by facts and experiments. Bao Shicheng (1775–1885) wrote that baked clay moveable type was \"as hard and tough as horn\"; experiments show that clay type, after being baked in an oven, becomes hard and difficult to break, such that it remains intact after being dropped from a height of two metres onto a marble floor. The length of clay movable types in China was 1 to 2 centimetres, not 2mm, thus hard as horn.\n\nThere has been an ongoing debate regarding the success of ceramic printing technology as there have been no printed materials found with ceramic movable types. However, it is historically recorded to have been used as late as 1844 in China from the Song dynasty through the Qing dynasty.\n\nBi Sheng (990–1051) also pioneered the use of wooden movable type around 1040 AD, as described by the Chinese scholar Shen Kuo (1031–1095). However, this technology was abandoned in favour of clay movable types due to the presence of wood grains and the unevenness of the wooden type after being soaked in ink.\nIn 1298, Wang Zhen (王祯/王禎), a Yuan dynasty governmental official of Jingde County, Anhui Province, China, re-invented a method of making movable wooden types. He made more than 30,000 wooden movable types and printed 100 copies of \"Records of Jingde County\" (《旌德縣志》), a book of more than 60,000 Chinese characters. Soon afterwards, he summarized his invention in his book \"A method of making moveable wooden types for printing books\". Although the wooden type was more durable under the mechanical rigors of handling, repeated printing wore the character faces down, and the types could only be replaced by carving new pieces. This system was later enhanced by pressing wooden blocks into sand and casting metal types from the depression in copper, bronze, iron or tin. This new method overcame many of the shortcomings of woodblock printing. Rather than manually carving an individual block to print a single page, movable type printing allowed for the quick assembly of a page of text. Furthermore, these new, more compact type fonts could be reused and stored. The set of wafer-like metal stamp types could be assembled to form pages, inked, and page impressions taken from rubbings on cloth or paper. In 1322，a Fenghua county officer Ma Chengde (馬称德) in Zhejiang, made 100,000 wooded movable types and printed the 43-volume \"Daxue Yanyi\" (《大學衍義》). Wooden movable types were used continually in China. Even as late as 1733, a 2300-volume \"Wuying Palace Collected Gems Edition\" (《武英殿聚珍版叢書》) was printed with 253,500 wooden movable types on order of the Qianlong Emperor, and completed in one year.\n\nA number of books printed in Tangut script during the Western Xia (1038–1227) period are known, of which the Auspicious Tantra of All-Reaching Union that was discovered in the ruins of Baisigou Square Pagoda in 1991 is believed to have been printed sometime during the reign of Emperor Renzong of Western Xia (1139–1193). It is considered by many Chinese experts to be the earliest extant example of a book printed using wooden movable type.\n\nThe logistical problems of handling the several thousand logographs (required for full literacy in Chinese language) posed a particular difficulty. It was faster to carve one woodblock per page than to composit a page from so many different types. However, if one used movable type to produce multiple copies of the same document, the speed of printing would increase relatively.\n\nAt least 13 material finds in China indicate the invention of bronze movable type printing in China no later than the 12th century, with the country producing large-scale bronze-plate-printed paper money and formal official documents issued by the Jin (1115–1234) and Southern Song (1127–1279) dynasties with embedded bronze metal types for anti-counterfeit markers. Such paper-money printing might date back to the 11th-century \"jiaozi\" of Northern Song (960–1127).\nThe typical example of this kind of bronze movable type embedded copper-block printing is a printed \"check\" of the Jin Dynasty with two square holes for embedding two bronze movable-type characters, each selected from 1,000 different characters, such that each printed paper note has a different combination of markers. A copper-block printed note dated between 1215–1216 in the collection of Luo Zhenyu's \"Pictorial Paper Money of the Four Dynasties\", 1914, shows two special characters – one called \"Ziliao\", the other called \"Zihao\" – for the purpose of preventing counterfeiting; over the \"Ziliao\" there is a small character (輶) printed with movable copper type, while over the \"Zihao\" there is an empty square hole – apparently the associated copper metal type was lost. Another sample of Song dynasty money of the same period in the collection of the Shanghai Museum has two empty square holes above \"Ziliao\" as well as \"Zihou\", due to the loss of the two copper movable types. Song dynasty bronze block embedded with bronze metal movable type printed paper money was issued on a large scale and remained in circulation for a long time.\n\nThe 1298 book \"Zao Huozi Yinshufa\" (《造活字印书法》/《造活字印書法》) by the Yuan dynasty (1271–1368) official Wang Zhen mentions tin movable type, used probably since the Southern Song dynasty (1127–1279), but this was largely experimental. It was unsatisfactory due to its incompatibility with the inking process.\n\nDuring the Mongol Empire (1206–1405), printing using movable type spread from China to Central Asia. The Uyghurs of Central Asia used movable type, their script type adopted from the Mongol language, some with Chinese words printed between the pages – strong evidence that the books were printed in China.\n\nDuring the Ming Dynasty (1368–1644), Hua Sui in 1490 used bronze type in printing books. In 1574 the massive 1000-volume encyclopedia \"Imperial Readings of the Taiping Era\" (《太平御览》/《太平御覧》) was printed with bronze movable type.\n\nIn 1725 the Qing Dynasty government made 250,000 bronze movable-type characters and printed 64 sets of the encyclopedic \"Gujin Tushu Jicheng\" (《古今图书集成》/《古今圖書集成》, \"Complete Collection of Illustrations and Writings from the Earliest to Current Times\"). Each set consisted of 5,040 volumes, making a total of 322,560 volumes printed using movable type.\n\nIn 1234 the first books known to have been printed in metallic type set were published in Goryeo Dynasty Korea. They form a set of ritual books, \"Sangjeong Gogeum Yemun\", compiled by Choe Yun-ui. \nWhile these books have not survived, the oldest book in the world printed in metallic movable types is \"Jikji\", printed in Korea in 1377.\nThe Asian Reading Room of the Library of Congress in Washington, D.C. displays examples of this metal type. \nCommenting on the invention of metallic types by Koreans, French scholar Henri-Jean Martin described this as \"[extremely similar] to Gutenberg's\".\n\nThe techniques for bronze casting, used at the time for making coins (as well as bells and statues) were adapted to making metal type. The Joseon dynasty scholar Seong Hyeon (성현, 成俔, 1439–1504) records the following description of the Korean font-casting process:\n\nA potential solution to the linguistic and cultural bottleneck that held back movable type in Korea for 200 years appeared in the early 15th century—a generation before Gutenberg would begin working on his own movable-type invention in Europe—when Sejong the Great devised a simplified alphabet of 24 characters (hangul) for use by the common people, which could have made the typecasting and compositing process more feasible. But Korea's cultural elite, \"appalled at the idea of losing hanja, the badge of their elitism\", stifled the adoption of the new alphabet.\n\nA \"Confucian prohibition on the commercialization of printing\" also obstructed the proliferation of movable type, restricting the distribution of books produced using the new method to the government. The technique was restricted to use by the royal foundry for official state publications only, where the focus was on reprinting Chinese classics lost in 1126 when Korea's libraries and palaces had perished in a conflict between dynasties.\n\nScholarly debate and speculation has occurred as to whether Eastern movable type spread to Europe between the late 14th century and early 15th centuries.\n\nJohannes Gutenberg of Mainz, Germany is acknowledged as the first to invent a metal movable-type printing system in Europe, the printing press. Gutenberg was a goldsmith familiar with techniques of cutting punches for making coins from moulds. Between 1436 and 1450 he developed hardware and techniques for casting letters from matrices using a device called the hand mould. Gutenberg's key invention and contribution to movable-type printing in Europe, the hand mould, was the first practical means of making cheap copies of letterpunches in the vast quantities needed to print complete books, making the movable-type printing process a viable enterprise.\n\nBefore Gutenberg, books were copied out by hand on scrolls and paper, or printed from hand-carved wooden blocks. It was extremely time-consuming; even a small book could take months to complete, and because the carved letters or blocks were flimsy and the wood susceptible to ink the blocks had a limited lifespan.\n\nGutenberg and his associates developed oil-based inks ideally suited to printing with a press on paper, and the first Latin typefaces. His method of casting type may have been different from the hand mould used in subsequent decades. Detailed analysis of the type used in his 42-line Bible has revealed irregularities in some of the characters that cannot be attributed to ink spread or type wear under the pressure of the press. Scholars conjecture that the type pieces may have been cast from a series of matrices made with a series of individual stroke punches, producing many different versions of the same glyph.\nThis raises the possibility that the development of movable type in the West may have been progressive rather than a single innovation.\n\nGutenberg's movable-type printing system spread rapidly across Europe, from the single Mainz printing press in 1457 to 110 presses by 1480, of which 50 were in Italy. Venice quickly became the center of typographic and printing activity. Significant were the contributions of Nicolas Jenson, Francesco Griffo, Aldus Manutius, and other printers of late 15th-century Europe.\n\nType-founding as practiced in Europe and the west consists of three stages.\n\n\nThe type-height was quite different in different countries. The Monotype Corporation Limited in London UK produced moulds in various heights:\n\n\nA Dutch printers manual mentions a tiny difference between French and German Height:\n\nTiny differences in type-height will cause quite bold images of characters.\n\nAt the end of the 19th century there were only two typefoundries left in the Netherlands: Johan Enschedé & Zonen, at Haarlem, and Lettergieterij Amsterdam, voorheen Tetterode. They both had their own type-height: Enschedé: 65 23/24 points Didot, and Amsterdam: 66 1/24 points Didot. Enough difference to prevent a combined use of fonts of both typefoundries: Enschede would be to light, or otherwise the Amsterdam-font would print rather bold. A perfect way of binding clients.\n\nIn 1905 the Dutch governmental \"Algemeene Landsdrukkerij\", later: \"State-printery\" (Staatsdrukkerij) decided during a reorganisation to use a standard type-height of 63 points Didot. \"Staatsdrukkerij-hoogte\", actually Belgium-height, but this fact was not widely known.\n\nModern, factory-produced movable type was available in the late 19th century. It was held in the printing shop in a \"job case\", a drawer about 2 inches high, a yard wide, and about two feet deep, with many small compartments for the various letters and ligatures. The most popular and accepted of the job case designs in America was the California Job Case, which took its name from the Pacific coast location of the foundries that made the case popular.\n\nTraditionally, the capital letters were stored in a separate drawer or case that was located above the case that held the other letters; this is why capital letters are called \"upper case\" characters while the non-capitals are \"lower case\".\n\nCompartments also held spacers, which are blocks of blank type used to separate words and fill out a line of type, such as \"em\" and \"en\" quads (\"quadrats\", or spaces. A \"quadrat\" is a block of type whose face is lower than the printing letters so that it does not itself print.). An em space was the width of a capital letter \"M\" – as wide as it was high – while an en space referred to a space half the width of its height (usually the dimensions for a capital \"N\").\n\nIndividual letters are assembled into words and lines of text with the aid of a composing stick, and the whole assembly is tightly bound together to make up a page image called a \"forme\", where all letter faces are exactly the same height to form a flat surface of type. The forme is mounted on a printing press, a thin coating of viscous ink is applied and impressions made on paper under great pressure in the press. \"Sorts\" is the term given to special characters not freely available in the typical type case, such as the \"@\" mark.\n\nSometimes it is erroneously stated that printing with metal type replaced the earlier methods. In the industrial era printing methods would be chosen to suit the purpose. For example, when printing large scale letters in posters etc. the metal type would have proved too heavy and economically unviable. Thus, large scale type was made as carved wood blocks as well as ceramics plates. Also in many cases where large scale text was required, it was simpler to hand the job to a sign painter than a printer.\nImages could be printed together with movable type if they were made as woodcuts or wood engravings as long as the blocks were made to the same type height. If intaglio methods, such as copper plates, were used for the images, then images and the text would have required separate print runs on different machines.\n\n\n\n"}
{"id": "624270", "url": "https://en.wikipedia.org/wiki?curid=624270", "title": "Neihu District", "text": "Neihu District\n\nNeihu District is a district of Taipei City, Taiwan. Neihu means \"inner lake.\" The older name originates from the Ketagalan word \"Tayour\" (transliterated by the Dutch as \"Cattajo\"), meaning woman's head ornament.\n\nMany mountainous roads and paths, which are ideal for hiking, connect Neihu with the neighboring Shilin District and Yangmingshan National Park. The Tri-Service General Hospital, which is a teaching hospital of the National Defense Medical Center, is also in Neihu. The Wuchih Mountain Military Cemetery borders Neihu.\n\nIn 2018, Neihu became the seat of the American Institute in Taiwan (AIT), the de facto embassy of the United States in Taiwan.\n\nDuring Japanese rule, covered modern day Neihu in addition to Nangang. The village was under Shichisei District, Taihoku Prefecture.\n\nAlthough it is a flood-prone region, Neihu has experienced huge growth with the construction of the Neihu Technology Park and hypermarkets such as Costco, RT Mart, Carrefour, B&Q. The extension of the Taipei Metro to Neihu in the 1990s and early 2000s has also boosted residential and commercial growth. TransAsia Airways, Delta Electronics and RT-Mart have their headquarters in the district. Neihu is the location of the Garena e-Sports Stadium, which is host to the League of Legends Masters Series, the premier \"League of Legends\" esports professional video game league in Taiwan, Hong Kong, and Macau.\n\nStraightening of the Keelung River, which runs along Neihu's southern and eastern borders has changed the natural boundaries of the district at several points in the latter 20th century.\n\n\n\n\nNeihu's attractions include Dahu Park, famous for its picturesque footbridge, Bihu Park, where cherry blossoms can be viewed in the spring, and Bishan Temple (碧山巖), a large Taoist temple dedicated to Chen Yuanguang on the mountainside that affords a panoramic view of Taipei. In the vicinity of Bishan Temple are a number of other Taoist and Buddhist temples, as well as orchards and fruit farms where tourists can pick their own fruit. An extensive network of hiking trails criss-crosses the mountainous regions of the northern part of Neihu.\n\nThere are many popular restaurants in the Neihu area, including Journey Kaffe, a restaurant where patrons can make their own pizza, and Holly Brown Coffee, a cafe popular for its extravagant milkshakes.\n\nOther tourist attractions are Guo Ziyi Memorial Hall.\n\nThe district is served by the Neihu Line of Taipei Metro (Wenhu or Brown Line). Stations located within the district are Donghu Station, Huzhou Station, Dahu Park Station, Neihu Station, Wende Station, Gangqian Station and Xihu Station.\n\n\n"}
{"id": "2109666", "url": "https://en.wikipedia.org/wiki?curid=2109666", "title": "Remote keyless system", "text": "Remote keyless system\n\nA keyless entry system is an electronic lock that controls access to a building or vehicle without using a traditional mechanical key. The term \"keyless entry system\" originally meant a lock controlled by a keypad located at or near the driver's door, which required entering a predetermined (or self-programmed) numeric code. Such systems now have a hidden touch-activated keypad and are still available on certain Ford and Lincoln models.\n\nThe term remote keyless system (RKS), also called keyless entry or remote central locking, refers to a lock that uses an electronic remote control as a key which is activated by a handheld device or automatically by proximity.\n\nWidely used in automobiles, an RKS performs the functions of a standard car key without physical contact. When within a few yards of the car, pressing a button on the remote can lock or unlock the doors, and may perform other functions. A remote keyless system can include both a remote keyless entry system (RKE), which unlocks the doors, and a remote keyless ignition system (RKI), which starts the engine.\n\nOne of the first introductions was in 1980 on the Ford Thunderbird, Mercury Cougar, Continental Mark VI, and Lincoln Town Car, which Ford called Keyless Entry System (later renamed SecuriCode). It was a keypad on the driver-side exterior door above the door handle. It consisted of a keypad with five buttons that when the code was entered, would unlock the driver's door, with subsequent code entries to unlock all doors, and the trunk. Nissan offered the same technology on the Nissan Maxima and Nissan Fairlady beginning in 1984, essentially using the same approach as Ford, with the addition of being able to roll the windows down and open the optional moonroof from outside the vehicle on the door handle installed keypad on both the driver's and front passengers door.\n\nThe remote keyless systems using a handheld transmitter first began appearing on the French made Renault Fuego in 1982, and as an option on several American Motors vehicles in 1983, including the Renault Alliance. The feature gained its first widespread availability in the U.S. on several General Motors vehicles in 1989.\n\nKeyless remotes contain a short-range radio transmitter, and must be within a certain range, usually 5–20 meters, of the car to work. When a button is pushed, it sends a coded signal by radio waves to a receiver unit in the car, which locks or unlocks the door. Most RKEs operate at a frequency of 315 MHz for North America-made cars and at 433.92 MHz for European, Japanese and Asian cars. Modern systems since the mid-1990s implement encryption as well as rotating entry codes to prevent car thieves from intercepting and spoofing the signal. Earlier systems used infrared instead of radio signals to unlock the vehicle, such as systems found on Mercedes-Benz, BMW and other manufacturers.\n\nThe system signals that it has either locked or unlocked the car usually through some fairly discreet combination of flashing vehicle lamps, a distinctive sound other than the horn, or some usage of the horn itself. A typical setup on cars is to have the horn or other sound chirp twice to signify that the car has been unlocked, and chirp once to indicate the car has been locked. For example, Toyota, Scion, and Lexus use a chirp system to signify the car being locked/unlocked. While two beeps means that driver's door is unlocked, four beeps means all doors are unlocked. One long beep is for the trunk or power tailgate. One short beep signifies that the car is locked and alarm is set.\n\nThe functions of a remote keyless entry system are contained on a key fob or built into the ignition key handle itself. Buttons are dedicated to locking or unlocking the doors and opening the trunk or tailgate. On some minivans, the power sliding doors can be opened/closed remotely. Some cars will also close any open windows and roof when remotely locking the car. Some remote keyless fobs also feature a red panic button which activates the car alarm as a standard feature. Further adding to the convenience, some cars' engines with remote keyless ignition systems can be started by the push of a button on the key fob (useful in cold weather), and convertible tops can be raised and lowered from outside the vehicle while it's parked.\n\nOn cars where the trunk release is electronically operated, it can be triggered to open by a button on the remote. Conventionally, the trunk springs open with the help of hydraulic struts or torsion springs, and thereafter must be lowered manually. Premium models, such as SUVs and estates with tailgates, may have a motorized assist that can both open and close the tailgate for easy access and remote operation.\n\nFor offices, or residences, the system can also be coupled with the security system, garage door opener or remotely activated lighting devices.\n\nMost keyless systems use a technique called rolling code to avoid replay attacks, in which the open command is intercepted to be used by a thief at a later time. In the rolling code, a pseudorandom number generator is used to generate a different unlock sequence to be sent each time the car is unlocked.\n\nRemote keyless entry fobs emit a radio frequency with a designated, distinct digital identity code. Inasmuch as \"programming\" fobs is a proprietary technical process, it is typically performed by the automobile manufacturer. In general, the procedure is to put the car computer in 'programming mode'. This usually entails engaging the power in the car several times while holding a button or lever. It may also include opening doors, or removing fuses. The procedure varies amongst various makes, models, and years. Once in 'programming mode' one or more of the fob buttons is depressed to send the digital identity code to the car's onboard computer. The computer saves the code and the car is then taken out of programming mode.\n\nAs RKS fobs have become more prevalent in the automobile industry a secondary market of unprogrammed devices has sprung up. Some websites sell steps to program fobs for individual models of cars as well as accessory kits to remotely activate other car devices.\n\nOn early (1998-2012) keyless entry remotes, the remotes can be individually programmed by the user, by pressing a button on the remote, and starting the vehicle. However, newer (2013+) keyless entry remotes require dealership or locksmith programming via a computer with special software . The Infrared keyless entry systems offered user programming, though radio frequency keyless entry systems mostly require dealer programming.\n\nSome cars have a proximity system that is triggered if a transponder car key is within a certain distance of the car and is sometimes called hands-free or advanced key.\n\nOne of the earliest systems was found on the 1993 Chevrolet Corvette (called the Passive Keyless Entry System) and in Mercedes-Benz vehicles from 1998. \n\nToday, this system is commonly found on a variety of vehicles, and although the exact method of operation differs between makes and models, their operation is generally similar: a vehicle can be unlocked without the driver needing to physically push a button on the key fob to lock or unlock the car and is also able to start or stop the ignition without physically having to insert the key and turning the ignition. Instead, the vehicle senses that the key (which may be located in the user's pocket, purse, etc.) is approaching the vehicle.\n\nA simpler version of the smart key system is the engine immobiliser, involving a security key system embedded into most modern vehicle's keys. A small chip rests on the vehicle's key or under the plastic key cover. When any key is inserted into the ignition, the ignition is coded. The key sends its security code to the ignition, which also has its own security code, and if the security codes match, the vehicle will start when the key is turned. However, if the key codes do NOT match, the vehicle will NOT start when the key is turned. Some early examples of this technology include Chrysler Corporation's Sentry Key System, or General Motors's PASSKey System.\n\nSome security keys can be programmed by the user, though most of these keys have to be programmed by a dealership or locksmith via a computer. It was not possible to copy these keys at a hardware store or auto parts store, but nowadays it is.\n\nKeyless ignition does not by default provide better security. In October 2014, it was found that some insurers in the United Kingdom would not insure certain vehicles with keyless ignition unless there were additional mechanical locks in place due to weaknesses in the keyless system.\n\nNews media have reported cases where it is suspected that criminals managed to open cars by using signal boosters to trick vehicles into thinking that their keyless entry fobs were close by even when they were far away, though they have not reported that any such devices have been found. The articles speculate that keeping fobs in aluminum foil or a freezer when not in use can prevent criminals from exploiting this vulnerability.\n\nIn 2015, it was reported that Samy Kamkar had built an inexpensive electronic device about the size of a wallet that could be concealed on or near a locked vehicle to capture a single keyless entry code to be used at a later time to unlock the vehicle. The device transmits a jamming signal to block the vehicle's reception of rolling code signals from the owner's fob, while recording these signals from both of his two attempts needed to unlock the vehicle. The recorded first code is sent to the vehicle only when the owner makes the second attempt, while the recorded second code is retained for future use. Kamkar stated that this vulnerability had been widely known for years to be present in many vehicle types but was previously undemonstrated. A demonstration was announced for DEF CON 23.\n\n\n"}
{"id": "22661665", "url": "https://en.wikipedia.org/wiki?curid=22661665", "title": "Roadbook", "text": "Roadbook\n\nA roadbook is a diagrammatic book typically used by rally co-drivers and overland travelers to navigate across uncertain terrain. Usually, the roadbook consists of several pages of tulip-diagrams, GPS co-ordinates and written instructions to assist in navigation.\n\nIn rally sport, the term pacenotes is more usual.\n"}
{"id": "1078201", "url": "https://en.wikipedia.org/wiki?curid=1078201", "title": "Self-heating can", "text": "Self-heating can\n\nA self-heating can is an enhancement of the common food can. Self-heating cans have dual chambers, one surrounding the other, making a self-heating food package.\n\nIn one version, the inner chamber holds the food or drink, and the outer chamber houses chemicals which undergo an exothermic reaction when combined. When the user wants to heat the contents of the can, a ring on the can - when pulled - breaks the barrier which keeps the chemicals in the outer chamber apart from the water. In another type, the chemicals are in the inner chamber and the beverage surrounds it in the outer chamber. To heat the contents of the can, the user pushes on the bottom of the can to break the barrier separating the chemical from the water. This design has the advantages of being more efficient (less heat is lost to the surrounding air) as well as reducing excessive heating of the product's exterior, causing possible discomfort to the user. In either case, after the heat from the reaction has been absorbed by the food, the user can enjoy a hot meal or drink.\n\nSelf-heating cans offer benefits to campers and people without access to oven, stove or camp-fire, but their use is not widespread. This is because self-heating cans are considerably more expensive than the conventional type, take more space, and have problems with uneven heating of their contents.\n\nThe source of the heat for the self-heated can is an exothermic reaction that the user initiates by pressing on the bottom of the can. The can is manufactured as a triple-walled container. A container for the beverage surrounds a container of the heating agent separated from a container of water by a thin breakable membrane. When the user pushes on the bottom of the can, a rod pierces the membrane, allowing the water and heating agent to mix. The resulting reaction releases heat and thus warms the beverage surrounding it.\n\nThe heating agent and responsible reaction vary from product to product. Calcium oxide is used in the following reaction:\n\nCopper sulphate and powdered zinc can also be used, but this process is less efficient:\n\nAnhydrous calcium chloride is often used as well. In this case, no chemical reaction occurs, instead the heat of solution is generated.\n\n\n\n"}
{"id": "2444044", "url": "https://en.wikipedia.org/wiki?curid=2444044", "title": "Shirley Thomas (USC professor)", "text": "Shirley Thomas (USC professor)\n\nShirley Thomas (1920 – July 21, 2005), also known as Shirley Thomas Perkins, was a radio/television actress/writer/producer, author, and professor in the Master of Professional Writing Program at the University of Southern California. In 2010, The Aerospace Historical Society established the Shirley Thomas Academic Scholarship. It is \"presented annually at the International von Kármán Wings Award banquet to a student in aerospace engineering or a related science that shows promise for continued future contributions to the field.\"\n\nThomas was born in Glendale, California, the daughter of an electrical engineer and a homemaker. She earned her B.A. in 1960 and her Ph.D. in Communications in 1967 from the University of Sussex. She was also awarded a diploma by the Russian Federation of Cosmonautics in 1995. She died of cancer on July 21, 2005 in Los Angeles, California.\n\nActive in Hollywood for a number of decades, Thomas conducted red carpet interviews at motion picture premiers and special event broadcasts from 1952 to 1956. She was also involved in the coverage of the New Year’s Day Rose Parade for CBS and later did broadcasts for Voice of America.\n\nThomas authored 15 books, including her eight-volume series on astronauts, \"Men of Space\" (published between 1960 and 1968). She also organized and chaired the Woman's Space Symposia from 1962-1973. In 1961 Thomas was the recipient of the Air Force Association's Airpower Arts and Letters Award and in 1991 she received the Aerospace Excellence Award from the California Museum Foundation. She was a fellow in the British Interplanetary Society and acted as a consultant for the Stanford Research Institute (SRI) and Jet Propulsion Laboratory (JPL).\n\nBeginning in the 1970s, Thomas organized the Theodore von Karman Stamp committee, succeeding in 1992 in getting a U.S. stamp issued in his honor. She also founded and chaired the Aerospace Historical Society, an organization that for 22 years has presented the international Von Karman Wings award to outstanding and innovative contributors to the world of aerospace.\n\nThomas, an associate fellow and advocate for the national Society for Technical Communication (as well as the local Los Angeles chapter, LASTC) taught Technical and Fundamental Writing in the Master of Professional Writing Program, University of Southern California for over three decades.\n\n"}
{"id": "346599", "url": "https://en.wikipedia.org/wiki?curid=346599", "title": "Spade", "text": "Spade\n\nA spade is a tool primarily for digging, comprising a blade – typically narrower and less curved than that of a shovel – and a long handle. Early spades were made of riven wood or of animal bones (often shoulder blades). After the art of metalworking was developed, spades were made with sharper tips of metal. Before the introduction of metal spades manual labor was less efficient at moving earth, with picks being required to break up the soil in addition to a spade for moving the dirt. With a metal tip, a spade can both break and move the earth in most situations, increasing efficiency.\n\nEnglish \"spade\" is from Old English ' (f.) or ' (m.). The same word is found in Old Frisian ' and Old Saxon '. High German \"\" only appears in Early Modern German, probably loaned from Low German. In Denmark, Sweden and Norway the word is \"spade\" as well. Other Scandinavian forms are in turn loaned from German.\nThe term may thus not originate in Common Germanic and appears to be a North Sea Germanic innovation or loaned. Closely related is Greek , whence Latin \"\".\n\nSpades are made in many shapes and sizes, for a variety of different functions and jobs, and there are many different designs used in spade manufacturing. People often mistakenly use the term \"shovel\" interchangeably with spade—but, strictly speaking, shovels generally are broad-bottomed tools for moving loose materials, whereas spades tend to have a flat bottom edge for digging.\n\nThe most common spade is a \"garden spade,\" which typically has a long handle, is wide, and is treaded (has rests for the feet to drive the spade into the ground). An \"Irish spade\" is similar to a common garden spade, with the same general design, although it has a much thinner head. A \"sharpshooter\" is a narrow spade. A \"turfing iron\" has a short, round head, and is used for cutting and paring off turf. A \"digging fork,\" or \"grape,\" is forked much like a pitchfork, and is useful for loosening ground and gardening. There can also be toy spades for children.\n\nLoy ploughing was a form of manual ploughing carried out in Ireland using a form of spade called a loy. It took place on very small farms or on very hilly ground, where horses couldn't work or where farmers couldn't afford them. It was used up until the 1960s on poorer land. This suited the moist climate of Ireland as the trenches formed by turning in the sods providing drainage. It also allowed the growing of potatoes on mountain slopes where no other cultivation could take place.\n\nIn gardening, a spade is a hand tool used to dig or loosen ground, or to break up lumps in the soil. Together with the fork it forms one of the chief implements wielded by the hand in agriculture and horticulture. It is sometimes considered a type of shovel. Its typical shape is a broad flat blade with a sharp lower edge, straight or curved. The upper edge on either side of the handle affords space for the user's foot, which drives it into the ground. The wooden handle ends in a cross-piece, sometimes T-shaped and sometimes forming a kind of loop for the hand.\n\nA small, narrow one-hand shovel for gardening is called a transplanter.\n\nSmall and/or plastic toy versions of the same tool are used to dig sand castles on a beach or in a sand-box.\n\nThe blade of the spade was used as currency in ancient China. Later, they were miniaturized and then stylized into a flat piece. The Qin Dynasty replaced them with round coins.\n\n\n"}
{"id": "27130178", "url": "https://en.wikipedia.org/wiki?curid=27130178", "title": "Structural Soil", "text": "Structural Soil\n\nStructural Soil is a medium that can be compacted to pavement design and installation requirements while permitting root growth. It is a mixture of gap-graded gravels (made of crushed stone), clay loam, and a hydrogel stabilizing agent to keep the mixture from separating. It provides an integrated, root penetrable, high strength pavement system that shifts design away from individual tree pits. \n\nPreviously the main problem facing the establishment of trees in paved areas is the lack of enough volume of soil for tree root growth. Soils under pavements are typically so compacted that it stops roots from growing. Older established trees with their roots under pavement grow poorly and often die. They can also cause pavement failure and displacement. Overall pavement preparation and repairs can shorten the life expectancy of a tree to 7–10 years where we could see them grow for at least 50 more years.\n\nStructural soil was researched and developed in the 1990s by Cornell University’s Urban Horticulture Institute. In 1999, AMEREQ signed a licensing agreement with Cornell University and currently holds the patent rights to Cornell’s CU-Structural Soil Urban Tree Planting Mix. It is marketed as CU-Structural Soil for quality control and is produced by a network of qualified AMEREQ-licensed companies. CU-Structural Soil on average costs $35–$42 per ton. Other companies have formed their own brand of structural soil based on Cornell’s work. For example, STALITE has developed STALITE MATRIX Structural Soil that they claim holds more moisture.\n\nWallace Laboratories modified work in 1994 which had been done by others to help improve the technology. The concept had been used in Europe in the 1980s. They did not seek a patent on their work but left the technology in the public domain for others to freely use. Their work preceded the filing of the Cornell University patent.\n\nBriefly, the aggregate size was increased to about , the soil texture was changed to a clay or clay loam in order to increase the water holding capacity and nutrient capacity, the soil was conditioned with the linear polyacrylamide, and the soil chemical and physical properties were specified.\n\nTheir procedure has been extensively used worldwide. One municipal installation used about 50,000 cubic yards of the Wallace Labs formulation.\n\nStructural soil is composed of crushed stone (typically limestone or granite) narrowly graded from ¾-1 ½” highly angular with no fines, clay loam which should conform to the USDA soil classification system. The hydrogel is added in a small amount to prevent the separation of the stone and soil during mixing and installation. Usually a layer of stone is spread, then the dry hydrogel is spread evenly on top and screened moist loam is placed on top. The entire mixture is then turned until a uniform blend is produced. Structural soil is not typically stockpiled; it should be mixed and installed soon after delivery. If a stockpile is required, the soil needs to be protected from the elements so it does not become contaminated. Installation typically calls for two cubic feet of soil is needed for every square foot of crown projected. It is also recommended for irrigated trees to have a low-volume drip irrigation. Cornell also suggests a minimum of 24” to 36” for CU-Structural Soil depth and they have established no minimum for length and width of installation, however, because it is a structural soil it was designed to go under the entire pavement area. Testing has shown that structural soil is safe around utilities and that you can use trees from any production system such as balled-and-bur lapped, bare root, containerized and boxed trees.\n\nCornell is continuing its development of CU-Structural Soil, expanding its use as the need for trees and other greenery within highly urbanized areas grows. CU-Structural Soil has been used in over 1000 applications and has been proven a very viable option for construction in cities.\n"}
{"id": "51421253", "url": "https://en.wikipedia.org/wiki?curid=51421253", "title": "The Wright Brothers (book)", "text": "The Wright Brothers (book)\n\nThe Wright Brothers is a 2015 non-fiction book written by the popular historian David McCullough and published by Simon & Schuster. It is a history of the American inventors and aviation pioneers Orville and Wilbur Wright. The book was on \"The New York Times\" Non-Fiction Best Sellers list for seven weeks in 2015.\n\nMcCullough first became interested in writing a book on the Wright brothers while researching for his book \"The Greater Journey\", which explored the history of various notable Americans who lived in Paris during the 19th century. In an interview with \"The Seattle Times\", McCullough recalled, \"I didn't know when (chronologically) I was going to end that book, and who do I run into in France but the Wright brothers.\" He continued, \"I was delighted to find that Wilbur, at every chance, went to the Louvre to look at paintings, and the degree that he was moved by the great Gothic works of France was far beyond that of an ordinary tourist. [...] Much of what has been written about the Wright brothers (in French) has been ignored. That's what pulled me into doing the book.\"\n\nMcCullough has had a lifelong interest in aviation. In an interview with the \"Santa Barbara Independent\", he remarked, \"I loved to make model airplanes when I was a young boy and I took flying lessons later on; I would have continued if they weren’t so expensive. But I have to say in all candor that I knew very little about [the Wright brothers]. I knew they were from Ohio, I knew they were bicycles mechanics, and I knew they invented the airplane. But I really didn’t know anything beyond that of any substance...\" He added, \"Once I got into their lives and into all that they went through and the truly admirable human qualities that they personified, I would have wanted to write the book even if they hadn't succeeded in inventing the flying machine.\"\n\n\"The Wright Brothers\" has been praised by literary critics and historians. Janet Maslin of \"The New York Times\" described it as a \"concise, exciting and fact-packed book [that] sees the easy segue between bicycling and aerial locomotion, which at that point was mostly a topic for bird fanciers and dreamers.\" She added, \"Mr. McCullough presents all this with dignified panache, and with detail so granular you may wonder how it was all collected.\" In \"The New York Review of Books\", the novelist James Salter wrote, \"Having twice won both the National Book Award and the Pulitzer Prize for his best-selling histories and biographies, McCullough is a much-loved dean of Americana, and his new book, a dual biography, has a warm appeal.\" Writing in \"The Washington Post\", Reeve Lindbergh, daughter of the famed aviator Charles Lindbergh, called \"The Wright Brothers\" a \"superb new book\" and wrote, \"McCullough's magical account of their early adventures — enhanced by volumes of family correspondence, written records, and his own deep understanding of the country and the era — shows as never before how two Ohio boys from a remarkable family taught the world to fly.\" Bruce Watson of the \"San Francisco Chronicle\" observed, \"\"The Wright Brothers\" will do more than help Americans tell Orville from Wilbur. Fighting the persistent myth of invention's 'aha' moment, McCullough shows the importance of experiment, error and inspiration in nature. Although they studied early gliders, Orville and Wilbur also watched birds. 'Learning the secret of flight from a bird,' Orville said, 'is a good deal like learning the secret of magic from a magician.' Likewise, to learn history from a master storyteller is to relive the past.\"\n\nBob Hoover of the \"Star Tribune\" was slightly more critical of the work, remarking, \"Although this new biography, \"The Wright Brothers\", refreshes their often-told story in McCullough's upbeat, minutely researched manner, something's lacking — interesting characters. While the brothers accomplished their goal of powered flight, they missed out on the stuff that makes life interesting — relationships, children, hobbies, fun and, most of all, self-reflection.\" He concluded, \"Despite their old-fashioned manners and plain style, the Wright brothers were reticent and difficult people, traits that McCullough seemed unwilling to explore in his search for the virtues and strengths he values so much in American life.\" Buzzy Jackson of \"The Boston Globe\" similarly described it as \"a tidy and relatively short history\" of the Wright brothers and praised the author's attention to detail, despite noting, \"While there is much to like here, McCullough's gee-whiz attitude toward America's favorite flying Boy Scouts does feel a bit retro.\"\n\n"}
{"id": "5857763", "url": "https://en.wikipedia.org/wiki?curid=5857763", "title": "Udhcpc", "text": "Udhcpc\n\nUdhcpc is a very small DHCP client program geared towards embedded systems. The letters are an abbreviation for Micro - DHCP - Client (µDHCPc). The program tries to be fully functional and RFC 2131 compliant.\n\nIt was once packaged with a similarly small DHCP server program named udhcpd, with the package called udhcp. It is now maintained as part of Busybox.\n\nBuilt for uClibc, the client executable is around 18k.\n\nThe program accepts all options on the command line, and calls external scripts to handle the configuration of interfaces.\n\nUdhcp was originally developed in 1999 by Matthew Ramsay and distributed under the GNU GPL by Moreton Bay.\n\n"}
{"id": "20575029", "url": "https://en.wikipedia.org/wiki?curid=20575029", "title": "Unitized regenerative fuel cell", "text": "Unitized regenerative fuel cell\n\nA unitized regenerative fuel cell (URFC) is a fuel cell based on the proton exchange membrane which can do the electrolysis of water in regenerative mode and function in the other mode as a fuel cell recombining oxygen and hydrogen gas to produce electricity. Both modes are done with the same fuel cell stack\n\nBy definition, the process of any fuel cell could be reversed. However, a given device is usually optimized for operating in one mode and may not be built in such a way that it can be operated backwards. Fuel cells operated backwards generally do not make very efficient systems unless they are purpose-built to do so as in high pressure electrolyzers, unitized regenerative fuel cells and regenerative fuel cells.\n\nLivermore physicist Fred Mitlitsky studied the possibilities of reversible technology. In the mid-1990s Mitlitsky received some funding from NASA for development of Helios and from the Department of Energy for leveling peak and intermittent power usage with sources such as solar cells or wind turbines. By 1996 he produced a 50-watt prototype single proton-exchange membrane cell which operated for 1,700 ten-minute charge-discharge cycles, and degradation was less than a few percent at the highest current densities. A rated power of 18.5 kW URFC was installed in the Helios and was tested on-board during test flights in 2003. The aircraft unfortunately crashed on its second URFC test flight June 26, 2003.\n\n\n"}
{"id": "5294103", "url": "https://en.wikipedia.org/wiki?curid=5294103", "title": "Voice logging", "text": "Voice logging\n\nVoice logging is the practice of regularly recording telephone conversations. Business sectors which often do voice logging include public safety (e.g. 9-1-1 and emergency response systems), customer service call centers (conversations are recorded for quality assurance purposes), and finance (e.g. telephone-initiated stock trades are recorded for compliance purposes). Although voice logging is usually performed on conventional telephone lines, it is also frequently used for recording open microphones (e.g. on a stock trading floor) and for broadcast radio.\n\nEarly voice loggers recorded POTS lines onto analog magnetic tape. As telephony became more digital, so did voice loggers, and starting in the 1990s, voice loggers digitized the audio using a codec and recorded to digital tape. With modern VoIP systems, many voice loggers now simply store calls to a file on a hard drive.\n\nThe original voice logging system was a large analog tape recorder developed by Magnasync in 1950. In 1953, Magnasync Corporation sold 300 voice loggers to the U.S. Air Force. Later systems were designed and manufactured by Arcane logger, ASC, Comverse, Cybertech, Dictaphone, Eventide, Eyretel, Mercom, NICE, Racal, Verint, and Witness. Over the years, there has been substantial consolidation in the field and current major vendors include ASC, NICE, Red Box, Verint.\n\n\n\n\n"}
{"id": "23926449", "url": "https://en.wikipedia.org/wiki?curid=23926449", "title": "Wartime reserve mode", "text": "Wartime reserve mode\n\nWartime reserve modes (abbreviated as WARM) are military procedures held in reserve for wartime or emergency use. They concern the characteristics and operating procedures of sensor, communications, navigation aids, threat recognition, weapons, and countermeasures systems. Since the military effectiveness of these procedures links to them being unknown to or misunderstood by opposing commanders before they are used, stopping their use by making them reserved has the effect of helping to ensure they remain effective by making it difficult for them to be known about in advance by such opposing commanders. This prevents them being exploited or neutralized.\n\n"}
