{"id": "58714870", "url": "https://en.wikipedia.org/wiki?curid=58714870", "title": "Accessibility (transport)", "text": "Accessibility (transport)\n\nIn transport planning accessibility refers to a measure of the ease of reaching (and interacting with) destinations or activities distributed in space, e.g. around a city or country. Accessibility is generally associated with a place (or places) of origin. A place with \"high accessibility\" is one from which many destinations can be reached, or destinations can be reached with relative ease. \"Low accessibility\" implies that relatively few destinations can be reached for a given amount of time/effort/cost or that reaching destinations is more difficult or costly from that place. \n\nThe concept can also be defined in the other direction, and we can speak of a place having accessibility \"from\" some set of surrounding places. For example, one could measure the accessibility of a store to customers as well as the accessibility of a potential customer to some set of stores. \n\nIn time geography, accessibility has also been defined as \"person based\" rather than \"place based\", were one would consider a person's access to some type of amenity through the course of their day as they move through space. For example, a person might live in a food desert but have easy access to a grocery store from their place of work.\n\nAccessibility is often calculated separately for different modes of transport. \n\nIn general, accessibility formula_1 is defined as:\n\nformula_2\n\nwhere:\n\nTravel cost metrics (formula_6 in the equation above) can take a variety of forms such as:\nCost metrics may also be defined using any combination of these or other metrics. For a non-motorized mode of transport, such as walking or cycling, the generalized travel cost may include additional factors such as safety or gradient. The essential idea is to define a function that describes the ease of travelling from any origin formula_3 to any destination formula_4. \n\nThe function on the travel cost formula_9 determines how accessible a destination is based on the travel cost associated with reaching that destination. Two common impedance functions are \"cumulative opportunities\" and a negative exponential function. Cumulative opportunities is a binary function yielding 1 if an opportunity can be reached within some threshold and 0 otherwise. It is defined as: \n\nformula_14\n\nwhere formula_15 is the threshold parameter.\n\nA negative exponential impedance function can be defined as:\n\nformula_16\n\nwhere formula_17 is a parameter defining how quickly the function decays with distance.\n\nAccessibility has long been associated with landuse; as accessibility increases in a given place, the utility of developing the land increases. This association is often used in integrated transport and landuse forecasting models. \n\nTransport for London utilize a calculated approach known as Public Transport Accessibility Level (PTAL) that uses the distance from any point to the nearest public transport stops, and service frequency at those stops, to assess the accessibility of a site to public transport services.\n\nThere are three ways in spatial planning to improve accessibility and they are respectively mobility, connectivity and proximity. They are considered the means to the end which is accessibility.\n"}
{"id": "40598604", "url": "https://en.wikipedia.org/wiki?curid=40598604", "title": "Arms Control and Disarmament Act of 1961", "text": "Arms Control and Disarmament Act of 1961\n\nThe Arms Control and Disarmament Act of 1961, 22 U.S.C. § 2551, was created to establish a governing body for the control and reduction of apocalyptic armaments with regards to protect a world from the burdens of armaments and the scourge of war. The Act provided an important aspect for the Kennedy Administration's foreign policy which was coherent with the United States national security policy.\n\nThe H.R. 9118 legislation was passed by the United States 87th Congressional session and signed by the 35th President of the United States John F. Kennedy on September 26, 1961.\n\nThe Arms Control and Disarmament Act established the Arms Control and Disarmament Agency (ACDA). The U.S. federal organization developed the formulation and implementation of the United States arms control and disarmament policy. The agency provided information and recommendations with regards to U.S. economic, foreign, and national security policies to executive and legislative officials of the United States government.\n\nThe Act established several core functions for the Arms Control and Disarmament Agency;\n\nThe federal statute was penned as four titles created as Chapter 35 within Title 22 which defines the United States foreign policies for international relations and intercourse records.\n\nChronological timeline of authorizations for U.S. Congressional legislation related to United States arms control and disarmament provisions.\n\n"}
{"id": "1804100", "url": "https://en.wikipedia.org/wiki?curid=1804100", "title": "Arri", "text": "Arri\n\nThe Arri Group is a global supplier of motion picture film equipment. Based in Munich, the company was founded in 1917. It produces professional motion picture cameras, lenses, lighting and postproduction equipment. Hermann Simon mentioned this company in his book \"Hidden Champions of the 21st Century\" as an example of a Hidden Champion. The Arri Alexa camera system was used to film Academy Award winners for Best Cinematography including \"Hugo\", \"Life of Pi\", \"Gravity\", \"Birdman\" and \"The Revenant\".\n\nArri was founded in Munich, Germany in 1917 by August Arnold and Robert Richter as Arnold & Richter Cine Technik. The abbreviation \"Arri\" was derived from the initial two letters of the founders' surnames, \"Ar\"nold and \"Ri\"chter.\n\nIn 1924, Arnold and Richter developed their first film camera, the small and portable Kinarri 35. In 1937, Arri introduced the world's first reflex mirror shutter in the Arriflex 35 camera, an invention of longtime engineer Erich Kästner. This technology employs a rotating mirror that allows a continuous motor to operate the camera while providing parallax-free reflex viewing to the operator, and the ability to focus the image by eye through the viewfinder, much like an SLR camera for still photography. The reflex design was subsequently used in almost every professional motion picture film camera and is still used in the Arri Alexa Studio digital camera. The first Hollywood film to employ an Arriflex was the 1947 Humphrey Bogart and Lauren Bacall film \"Dark Passage\" in 1947. Over the years, more than 17,000 Arriflex 35s were built. The design was recognized with two Scientific and Technical Academy Awards in 1966 and 1982.\n\nIn 1952, Arri introduced the Arriflex 16ST, the first professional 16mm camera with a reflex viewing system. In 1965, a self-blimped 16mm camera was released: the Arriflex 16BL. The Arriflex 35BL followed in 1972 as a lightweight, quiet alternative to the rather heavy and cumbersome blimped cameras of the time. Also in 1972, Arri pioneered the development of daylight luminaires with the Arrisonne 2000 W. The Arriflex 16SR, launched in 1975, featured a redesigned viewfinder with a through-the-lens light meter. The Arriflex 765, a 65mm camera, was released in 1989, partly in response to the growing industry demand for 70mm release prints.\n\nThe Arriflex 535 camera was released in 1990, followed by the Arriflex 535B and the Arriflex 16SR 3 in 1992. The Arriflex 435 was released in 1994.\n\nArri partnered with Carl Zeiss AG in order to develop and manufacture advanced lenses for the motion picture industry. In 1998, Arri released the Ultra Prime lenses.\n\nDevelopment of the Arrilaser, a postproduction film recorder, began in 1997 and it was released for beta testing in 1998.\n\nIn 2000, Arri purchased the company Moviecam and developed Arricam, a 35mm camera platform. In 2003, Arri developed its first digital camera, the Arriflex D-20, which later evolved into the D-21. The camera used a 35mm CMOS sensor (instead of CCD) and allowed cinematographers to utilize standard 35mm lenses. This technology was further developed and improved for the Arri Alexa camera.\n\nArri revealed its Arriscan prototype during IBC 2003. The 16mm/35mm film scanner worked alongside the Arrilaser to support the increasingly popular digital intermediate route through postproduction. Later, the Arriscan became a widely used tool for film restoration work and was recognized with a Scientific and Engineering Academy Award in 2009.\n\nArri released the Master Prime lenses in 2005, designed for a super-fast aperture of T1.3 without breathing and distortion. In 2007, the Master Prime 14mm and 150mm lenses were released.\n\nThe Arrilaser 2 was released in 2009, with new client-server architecture and speeds twice as fast as the original model. In 2011, the Arrilaser was recognized with an Academy Award of Merit.\n\nIn 2010, the Arri Alexa camera was released. The camera had the ability to compress 1080p footage to ProRes QuickTime formats and allowed direct-to-edit workflows. Later, models were added to the range including the Alexa Plus, Alexa Studio and Alexa M, which was designed to get the camera closer to the action, The Alexa Plus 4:3, like the Alexa Studio, allowed the full area of the sensor to be used with anamorphic lenses. \nThe 16mm Arriflex 416 camera and Ultra Prime 16 lenses were used in the filming of the 2010 film, \"Black Swan\".\n\nArri announced a strategic partnership with Zeiss and Fujinon in 2010 to create new lenses that incorporated enhanced electronic lens data transfer in order to simplify visual effects workflows in postproduction. The Arri/Fujinon Alura Zooms were released that same year, while the Arri/Zeiss Master Anamorphic lens series was released in 2012.\n\nIn 2013, Arri created \"Arri Medical\", a business unit that utilizes its camera technology for medical purposes. Apart from a medical imaging documentation service, it has developed a fully digital 3D surgical microscope called the Arriscope.\nThe Arri Alexa 65, released in 2014, was used in the filming of \"The Revenant\" as well as \"\" and \"\". The Arri Amira camera was also released in 2014. In 2015, four of the five nominees for the cinematography category of the Academy Awards were filmed using the Arri Alexa.\n\nArri's subsidiary postproduction and creative services company, Arri Film & TV, was renamed Arri Media in 2015 as part of a company restructuring. At NAB 2015, the SkyPanel LED fixtures were introduced by Arri. The SC60 and the SC30 have a full color tunable LED option.\n\nIn April 2016, Arri acquired the Artemis camera stabilizer systems developed by Curt O. Schaller from Sachtler / Vitec Videocom. As a result, Arri became the exclusive seller of Artemis Trinity stabilizers. At NAB 2016, Arri unveiled its version of the Trinity system.\n\n\n\n\nArrilaser film recorder is used for film-out.\n\nArriscan\n\nIn 2011, it was alleged that Michael Bravin, an executive of the US-based subsidiary Arri Inc., had unlawfully accessed a rival company email account. A suit was brought before a US court and in September 2011, Bravin entered a guilty plea. Arri Inc. denied knowledge or gains from Bravin's actions, and a separate lawsuit against the company was dropped as a result of an out-of-court settlement.\n\n\n"}
{"id": "44403627", "url": "https://en.wikipedia.org/wiki?curid=44403627", "title": "Austin Cartridge Company", "text": "Austin Cartridge Company\n\nAustin Cartridge Company was an Austin Powder Company subsidiary manufacturing cartridge ammunition for small arms. The company made shotgun shells and rimfire cartridges from 1895 until purchased by Western Cartridge Company in 1907.\n\nAustin Powder Company was founded in 1833 by brothers Daniel, Alvin, Lorenzo, Henry, and Linus Austin of Wilmington, Vermont. After exploring the market for gunpowder on the western frontier, they built and operated a gunpowder mill at Old Forge on the Cuyahoga River near Akron, Ohio. Early production was largely used as blasting powder for coal mining but some was sold as sporting rifle powder. In 1884 the company became the exclusive supplier of gunpowder for shotgun shells manufactured by Chamberlain Cartridge Company.\n\nAustin Powder Company built the company town of Glenwillow, Ohio in 1892, and established the Austin Cartridge Company there in 1895 manufacturing \"Crack Shot\", \"Club Sporting\", and \"Champion Ducking\" shotgun shells. The Union Cap and Chemical Company was formed in 1900 as a joint venture of Austin Cartridge Company and Western Cartridge Company to manufacture blasting caps, primers, and .22 and .32 caliber rimfire cartridges. The Union Cap and Chemical Company trademark was a Maltese cross and a UCC headstamp appeared on the rimfire cartridges. These were the first .22 rimfire cartridges manufactured by Western Cartridge Company. Austin Cartridge Company was sold to Western Cartridge Company in 1907 as Austin Powder Company reorganized to focus on production of blasting explosives. Western Cartridge Company used the Maltese cross trade mark until replacing it with a diamond trade mark during World War I.\n"}
{"id": "41821489", "url": "https://en.wikipedia.org/wiki?curid=41821489", "title": "Azimuth compass", "text": "Azimuth compass\n\nAn azimuth compass (or azimuthal compass) is a nautical instrument used to measure the magnetic azimuth, the angle of the arc on the horizon between the direction of the sun or some other celestial object and the magnetic north. This can be compared to the true azimuth obtained by astronomical observation to determine the magnetic declination, the amount by which the reading of a ship's compass must be adjusted to obtain an accurate reading. Azimuth compasses were important in the period before development of the reliable chronometers needed to determine a vessel's exact position from astronomical observations.\n\nIn navigation, the true azimuth of a heavenly body is the arc of the horizon between the point where a vertical plane containing the observer and the heavenly body intersects the horizon and the direction of true north. The magnetic azimuth is the arc between the point on the horizon below the heavenly body and the direction of magnetic north.\n\nWhen the latitude and date are known, the bearing of the sun at sunrise or sunset relative to true north can be readily determined.\nIf the sun is observed at some time between sunrise and sunset, its altitude must also be recorded to calculate the true azimuth.\nThe true azimuth may be compared to the magnetic azimuth to find the magnetic declination, the angle between the direction that the compass indicates as north and the true north direction.\n\nAn azimuth compass is a magnetic compass where the circumference of the card is divided into 360 degrees.\nIt has two sights, diametrically opposite each other, through which the sun, planet or star is viewed. \nThe sun is typically observed when the sun's center is about one solar diameter above the horizon.\nTypically the compass will have a stop in the side of the box, which the observer pushes when the sun is lined up in the sights. This fixes the compass card, from which the magnetic azimuth or amplitude can be read.\n\nThe sights may consist of two vanes, one with a narrow slit and the other with a wider slit bisected by a thread. The observer looks towards the sun, seen through a dark glass, lining up the thread and the narrow slit on the center of the sun. Another arrangement has a vertical bar on one vane and a slit bisected by a thread on the other. \nThe instrument is aligned with the sun when the shadow of the thread falls on the vertical bar.\nYet another design has a magnifying glass on one vane that focuses the rays of the sun on the opposite vane.\n\nMeridional azimuth compasses included a universal equatorial sundial.\nBy setting the date and latitude, the true north and the magnetic variation could be read from the instrument without the need for calculation.\nWith the iron vessels introduced in the 19th century the azimuth compass would be mounted on a pedestal at a neutral point where the readings would not be affected by magnetism of the vessel.\n\nEarly navigators in the northern hemisphere could calculate latitude relatively easily when the night sky was clear by observing the elevation of Polaris, a star that is very close to the north celestial pole.\nHowever, calculating longitude was impossible until chronometers that could keep time accurately throughout a long voyage were developed.\nIn 1688 King Philip III of Spain (1578–1621) offered a large reward to anyone who could find a practical solution to determining longitude.\n\nA novel approach, apparently originating with the Jesuit missionary Christoforo Borri, was to create charts that mapped points of equal magnetic declination. With an accurate reading of the latitude and the magnetic declination the navigator could determine their longitude using the chart.\nA drawback to this approach is that magnetic variations change over time, so the charts would need constant revision.\n\nThe azimuth compass still had great value in letting the master of a ship determine how far the magnetic compass varied from true north, so he could set a more accurate course while following a line of constant latitude or using dead reckoning to navigate.\nIn 1795 a British First Rate ship would have up to eight compasses, of which one was an improved steering compass. This azimuth compass may have been specialized for the purpose of measuring magnetic variation through taking readings from stars, and used to determine the accuracy of the other compasses.\n\nAn azimuthal compass was described in 1736, but the inventor was not named.\nAzimuth compasses were sometimes large, with a brass case mounted in gimbals containing the rose, and sights on top of the case.\nWalter Hayes, Richard Glynne and Benjamin Ayres (died c. 1775) made accurate large azimuth compasses.\nHenry Gregory (1744–1782) had an establishment known as \"The Azimuth Compass\" in Leadenhall Street, London.\nHe supplied an azimuth compass designed by Gowin Knight to Joseph Banks for the voyage of HMS \"Resolution\" commanded by Captain James Cook in 1772.\nThe instrument cost ₤80.0.0.\n\nOn 6 February 1808 the American sealer \"Topaz\", commanded by Captain Mayhew Folger, arrived at Pitcairn Island to take on fresh water. There he found thirty five survivors of the Mutiny on the Bounty led by John Adams, who gave Folger the HMS \"Bounty\"'s azimuth compass and chronometer.\n\n"}
{"id": "35131662", "url": "https://en.wikipedia.org/wiki?curid=35131662", "title": "BTX (chemistry)", "text": "BTX (chemistry)\n\nIn the petroleum refining and petrochemical industries, the initialism BTX refers to mixtures of benzene, toluene, and the three xylene isomers, all of which are aromatic hydrocarbons. The xylene isomers are distinguished by the designations \"ortho\" – (or \"o\" –), \"meta\" – (or \"m\" –), and \"para\" – (or \"p\" –) as indicated in the adjacent diagram. If ethylbenzene is included, the mixture is sometimes referred to as BTEX.\n\nThe BTX aromatics are very important petrochemical materials. Global consumption of benzene, estimated at more than 40,000,000 tons in 2010, showed an unprecedented growth of more than 3,000,000 tons from the level seen in 2009. Likewise, the para-xylene consumption showed unprecedented growth in 2010, growing by 2,800,000 tons, a full ten percent growth from 2009.\n\nToluene is also a valuable petrochemical for use as a solvent and intermediate in chemical manufacturing processes and as a high octane gasoline component.\n\nThe table below lists some of the properties of the BTX aromatic hydrocarbons, all of which are liquids at typical room conditions: \nBenzene, toluene, and xylenes can be made by various processes. However, most BTX production is based on the recovery of aromatics derived from the catalytic reforming of naphtha in a petroleum refinery.\n\nCatalytic reforming usually utilizes a feedstock naphtha that contains non-aromatic hydrocarbons with 6 to 11 or 12 carbon atoms and typically produces a reformate product containing C to C aromatics (benzene, toluene, xylenes) as well as paraffins and heavier aromatics containing 9 to 11 or 12 carbon atoms.\n\nAnother process for producing BTX aromatics involves the steam cracking of hydrocarbons which typically produces a cracked naphtha product commonly referred to as pyrolysis gasoline, pyrolysis gas or pygas. The pyrolysis gasoline typically consists of C to C aromatics, heavier aromatics containing 9 to 11 or 12 carbon atoms, and non-aromatic cyclic hydrocarbons (naphthenes) containing 6 or more carbon atoms.\n\nThe adjacent table compares the BTX content of pyrolysis gasoline produced at standard cracking severity or at medium cracking severity with the BTX content of catalytic reformate produced by either a continuous catalytic regenerative (CCR) reformer or by a semi-regenerative catalytic reformer. About 70 percent of the global production of benzene is by extraction from either reformate or pyrolysis gasoline.\n\nThe BTX aromatics can be extracted from catalytic reformate or from pyrolysis gasoline by many different methods. Most of those methods, but not all, involve the use of a solvent either for liquid-liquid extraction or extractive distillation. Many different solvents are suitable, including sulfolane (CHOS), furfural (CHO), tetraethylene glycol (CHO), dimethylsulfoxide (CHOS), and N-methyl-2-pyrrolidone (CHNO).\n\nBelow is a schematic flow diagram of one method, involving extractive distillation, for extraction of the BTX aromatics from a catalytic reformate:\n\nThere are a very large number of petrochemicals produced from the BTX aromatics. The following diagram shows the chains leading from the BTX components to some of the petrochemicals that can be produced from those components:\n\n\n"}
{"id": "34201145", "url": "https://en.wikipedia.org/wiki?curid=34201145", "title": "Battlefield management system", "text": "Battlefield management system\n\nBattlefield management system (BMS) is a system meant to integrate information acquisition and processing to enhance command and control of a military unit.\n\nAs an example of modern combat force Pakistan Army has been using integrated battlefield management system called PAK-IBMS (Rehbar).\n\nIndian Army is on it way of developing its first BMS. Is estimated to be completed by 2025.\nHowever, recent development indicates foreclosure of Army's Battlefield Management System.\n"}
{"id": "9799339", "url": "https://en.wikipedia.org/wiki?curid=9799339", "title": "Bi-pin lamp base", "text": "Bi-pin lamp base\n\nA bipin or bi-pin, (sometimes referred to as two-pin, bipin cap or bipin socket), is a type of lamp fitting. They are included in the IEC standard \"IEC 60061 Lamp caps and holders together with gauges for the control of interchangeability and safety\". They are used on many small incandescent light bulbs (especially halogen lamps), and for starters on some types of fluorescent lights.\n\nSome sockets have pins placed closer together, preventing the low-power bulbs they use from being replaced by bulbs that are too high power, which may generate excessive heat and possibly cause a fire. These are sometimes called \"mini-bipin\". Where the terminals of the lamp are bent back onto the sides of the base of the bulb, this forms a wedge base, often used in small bulbs for automotive lighting.\n\nThe bi-pin base was invented by Reginald Fessenden for the 1893 World's Fair in Chicago. After Westinghouse won the contract to wire and illuminate the first electrified fair with AC instead of arch-rival Thomas Edison's DC, Edison and his General Electric company refused to allow his patented Edison screw-base bulbs to be used. Westinghouse overcame this by developing the bi-pin base for use at the fair.\n\nThe suffix after the G indicates the pin spread; the G dates to the use of Glass for the original bulbs. GU usually also indicates that the lamp provides a mechanism for physical support by the luminaire: in some cases, each pin has a short section of larger diameter at the end (sometimes described as a \"peg\" rather than a \"pin\"); the socket allows the bulb to lock into place by twisting it; in others, the base of the lamp has a groove which can be held by a spring or clip.\n\nA lowercase \"q\" at the end of the designation indicates that it is a quad-pin base, with two bi-pin pairs. These are used with compact fluorescent tubes that plug into a light fixture that has a permanent ballast.\n\nThere are also double-ended halogen and fluorescent tubes with one pin at each end, and high-output fluorescents with recessed or shrouded contacts, which are not covered here.\n\n\n"}
{"id": "22819973", "url": "https://en.wikipedia.org/wiki?curid=22819973", "title": "Bioelectrospray", "text": "Bioelectrospray\n\nElectrospray\nElectrospray ionization\n\nBio-electrospraying is a new technology that enables the deposition of living cells on various targets with a resolution that depends on cell size and not on the jetting phenomenon. It is envisioned that \"unhealthy cells would draw a different charge at the needle from healthy ones, and could be identified by the mass spectrometer\", with tremendous implications in the health care industry.\n\nThe early versions of bio-electrosprays were employed in several areas of research, most notably self-assembly of carbon nanotubes. Although the self-assembly mechanism is not clear yet, \"elucidating electrosprays as a competing nanofabrication route for forming self-assemblies with a wide range of nanomaterials in the nanoscale for top-down based bottom-up assembly of structures.\" Future research may reveal important interactions between migrating cells and self-assembled nanostructures. \"Such nano-assemblies formed by means of this top-down approach could be explored as a bottom-up methodology for encouraging cell migration to those architectures for forming cell patterns to nano-electronics, which are a few examples, respectively.\"\n\nAfter initial exploration with a single protein, increasingly complex systems were studied by bio-electrosprays. These include, but are not limited to, neuronal cells, stem cells, and even whole embryos. The potential of the method was demonstrated by investigating cytogenetic and physiological changes of human lymphocyte cells as well as conducting comprehensive genetic, genomic and physiological state studies of human cells and cells of the model yeast Saccharomyces cerevisiae.\n"}
{"id": "34855410", "url": "https://en.wikipedia.org/wiki?curid=34855410", "title": "Biopeople", "text": "Biopeople\n\nBiopeople - Denmark's Life Science Cluster is a publicly funded partnership and National Center established, authorised, and funded by the Ministry for Science and Higher Education to improve innovation, collaboration and education within the National Danish Innovation System. Biopeople is established as a Center at the Faculty of Health and Medical Sciences at University of Copenhagen.\n\nBiopeople was the first European cluster organisation within health and life sciences to receive the recognition of Gold Label of the European Cluster Excellence Initiative (ECEI).\n\nBiopeople helps academia and industry to co-create and develop ideas into new projects, products and services to benefit global health and welfare. Biopeople embraces and clusters universities, research organisations, and hospitals, the National Board of Health (Denmark) / Danish Health and Medical Authority, industry associations as well as pharma, medtech, medical device, food and biotech companies. The aim is to stimulate innovation through activities that bring researchers and stakeholders together across disciplines, sectors and public-private boundaries.\n\nBiopeople embeds all relevant Danish stakeholders. Biopeople is a Center at University of Copenhagen. Member Companies affiliate by in-kind means by participating in innovation activities and projects. Companies include major large companies - fx Novo Nordisk, Lundbeck, LEO Pharma, Danisco, Novozymes, and Chr. Hansen – and many small and medium enterprises.\n\nThe founding partners are:\n\n\n\n\n"}
{"id": "16488409", "url": "https://en.wikipedia.org/wiki?curid=16488409", "title": "CBERS-1", "text": "CBERS-1\n\nChina–Brazil Earth Resources Satellite 1 (CBERS-1), also known as Ziyuan I-01 or Ziyuan 1A, is a remote sensing satellite which was operated as part of the China–Brazil Earth Resources Satellite programme between the China National Space Administration and Brazil's National Institute for Space Research. The first CBERS satellite to fly, it was launched by China in 1999.\n\nCBERS-1 was a spacecraft built by the China Academy of Space Technology and based on the Phoenix-Eye 1 satellite bus. The spacecraft was powered by a single solar array, providing 1,100 watts of electricity for the satellite's systems. The instrument suite aboard the CBERS-1 spacecraft consisted of three systems: the Wide Field Imager (WFI) produced visible-light to near-infrared images with a resolution of and a swath width of ; a high-resolution CCD camera was used for multispectral imaging at a resolution of with a swath width of ; the third instrument, the Infrared Multispectral Scanner (IMS), had a resolution of and a swath width of .\n\nA Chang Zheng 4B carrier rocket, operated by the China Academy of Launch Vehicle Technology, was used to launch CBERS-1. The launch took place at 03:15 UTC on 14 October 1999, using Launch Complex 7 at the Taiyuan Satellite Launch Centre. The satellite was successfully placed into a sun-synchronous orbit.\n\nCBERS-1 was decommissioned in September 2003, almost four years after launch. The derelict satellite remains in orbit; as of 30 November 2013 it is in an orbit with a perigee of , an apogee of , 98.34 degrees inclination and a period of 100.35 minutes. The orbit has a semimajor axis of , and eccentricity of 0.0004025.\n"}
{"id": "44418198", "url": "https://en.wikipedia.org/wiki?curid=44418198", "title": "Cokodeal", "text": "Cokodeal\n\nCokodeal is a marketplace E-commerce service that connects traders in Africa to the world. headquartered in Nigeria with reg no: 1165256. The Cokodeal service helps connect African traders and customers. With Cokodeal, individual users, organizations and businesses can create online stores to market African manufactured goods and services. Its founders partnered with Neoteric a UK enterprise to handle its development and support. Presently, its major trading are in Ghana, Nigeria, Tanzania, South Africa, and Kenya. Cokodeal is an avenue for African traders to meet global customers and make deals.\n\nCokodeal was founded in 2012 by Mike Dola and launched in 2014. Using Nigeria as an Entry point to Africa, then expanding to other African countries. cokodeal.com took part in the US$1 million SpeedUPAfrica bootcamp held in Accra, hosted by DraperDarkFlow, 500startups, Singularity University. \n\nCokodeal was created as a need for a platform that alleviates the challenges faced by people in Europe and other parts of the world in finding Nigerian traders or African produced goods and services. To find businesses, products, quantity of goods and location. Also coming to Africa, noted with vast resources of Africa, She cannot continue to be a consuming continent as it will not aid its growth. Hence there is need to show case to the world African goods.\n\nIt is designed to promote solely African local content. i.e. African manufactured goods and services such as agricultural products, textile, crafts, art works, machines, minerals etc. Cokodeal facilitates African traders meeting customers, it connects African traders to Africa and global markets. It aims to solve some of the social challenges also enables SMEs and businesses to leverage on its platform to save cost and reach new markets. It serves as a market linkage for African businesses to trade internationally and develop its local content \n\nNigeria business owner can market their made in Nigeria goods on cokodeal to access new markets. However, the platform needs improvement in its layers and design layout, It has recently been improved, with more improvement to come to better serve its users.\n\nIn recent economic times with the diversification goal for Nigeria and some other African countries, cokodeal has supported government Ministries, Development and Agencies (MDAs) to get procurement by sourcing buyers for the local producers also for international buyers \n\n"}
{"id": "10429990", "url": "https://en.wikipedia.org/wiki?curid=10429990", "title": "Computer appliance", "text": "Computer appliance\n\nA computer appliance is a computer with software or firmware that is specifically designed to provide a specific computing resource. Such devices became known as \"appliances\" because of the similarity in role or management to a home appliance, which are generally \"closed and sealed\", and are not serviceable by the user or owner. The hardware and software are delivered as an integrated product and may even be pre-configured before delivery to a customer, to provide a turn-key solution for a particular application. Unlike general purpose computers, appliances are generally not designed to allow the customers to change the software and the underlying operating system, or to flexibly reconfigure the hardware.\n\nAnother form of appliance is the virtual appliance, which has similar functionality to a dedicated hardware appliance, but is distributed as a software virtual machine image for a hypervisor-equipped device.\n\nTraditionally, software applications run on top of a general-purpose operating system, which uses the hardware resources of the computer (primarily memory, disk storage, processing power, and networking bandwidth) to meet the computing needs of the user. The main issue with the traditional model is related to complexity. It is complex to integrate the operating system and applications with a hardware platform, and complex to support it afterwards.\n\nBy tightly constraining the variations of the hardware and software, the appliance becomes easily deployable, and can be used without nearly as wide (or deep) IT knowledge. Additionally, when problems and errors appear, the supporting staff very rarely needs to explore them deeply to understand the matter thoroughly. The staff needs merely training on the appliance management software to be able to resolve most of problems.\n\nIn all forms of the computer appliance model, customers benefit from easy operations. The appliance has exactly one combination of hardware and operating system and application software, which has been pre-installed at the factory. This prevents customers from needing to perform complex integration work, and dramatically simplifies troubleshooting. In fact, this \"turnkey operation\" characteristic is the driving benefit that customers seek when purchasing appliances.\n\nTo be considered an appliance, the (hardware) device needs to be integrated with software, and both are supplied as a package. This distinguishes appliances from \"home grown\" solutions, or solutions requiring complex implementations by integrators or Value-added resellers (VARs).\n\nThe appliance approach helps to decouple the various systems and applications, for example in the data center. Once a resource is decoupled, in theory it can be also centralized to become shared among many systems, centrally managed and optimized, all without requiring changes to any other system.\n\nThe major disadvantage of deploying a computer appliance is that since they are designed to supply a specific resource, they most often include a customized operating system running over specialized hardware, neither of which are likely to be compatible with the other systems previously deployed. Customers lose flexibility.\n\nOn the other hand, a proprietary embedded operating system, or operating system within an application, can make the appliance much more secure from common cyber attacks. \n\nThe variety of computer appliances reflects the wide range of computing resources they provide to applications. Some examples:\n\nAside from its deployment within data centers, many computer appliances are directly used by the general public. These include:\n\n\nThe world of industrial automation has been rich in appliances. These appliances have been hardened to withstand temperature and vibration extremes. These appliances are also highly configurable, enabling customization to meet a wide variety of applications. The key benefits of an appliance in automation are:\n\nTypes of automation appliances:\n\nThere are several design patterns adopted by computer appliance vendors, a few of which are shown below. Since the whole concept of an appliance rests on keeping such implementation details away from the end user, it is difficult to match these patterns to specific appliances, particularly since they can and do change without affecting external capabilities or performance.\n\nSometimes, these techniques are mixed. For example, a VPN appliance might contain a limited access software firewall running on Linux, with an encryption ASIC to speed up VPN access.\n\nSome computer appliances use solid state storage, while others use a hard drive to load an operating system. Again, the two methods might be mixed—an ASIC print server might allow an optional hard drive for job queueing, or a Linux-based device may encode Linux in firmware, so that a hard drive is not needed to load the operating system.\n\n\n"}
{"id": "34033476", "url": "https://en.wikipedia.org/wiki?curid=34033476", "title": "Cricket (warning sound)", "text": "Cricket (warning sound)\n\nA cricket is a type of cockpit audio alert onboard commercial aircraft such as those of Airbus. Its sound is intentionally designed to be extremely difficult for pilots to ignore. The \"chirp chirp\" sound is named after the insect that it imitates.\n"}
{"id": "32244195", "url": "https://en.wikipedia.org/wiki?curid=32244195", "title": "DataSplice", "text": "DataSplice\n\nDataSplice, LLC is a mobile software company headquartered in Fort Collins, Colorado and offers mobile applications which extend enterprise systems, including packaged software for Enterprise Asset Management (EAM) and computerized maintenance management systems (CMMS). The software provides an interface from these systems to handhelds, smartphones, tablet computers and mobile computers. It may also be used on a desktop system as a unified/simplified interface for multiple systems.\nThe software offered is a mobile middleware, with an emphasis on IBM's Maximo EAM system. Its primary client base is focused on utilities, gas/oil, defense, aerospace and other markets which utilizes field service management systems which require tracking, asset management and regulatory accountability.\n\nDataSplice does not use a proprietary software platform, but rather utilizes the Common Language Infrastructure (CLI) of Microsoft.NET framework’s ADO.NET, which allows for connectivity to different database systems such as MySQL and Oracle. The extensible system consists of three components, including a remote client (for handheld and/or desktop use), a server which communicates with the primary EAM, and an administration client for configuring the system.\n\nOriginally known as Optimization Resources, which was founded in 1991, DataSplice was spun off as both a company and product in 2001. The original management and development staff continue to be engaged in daily operations. DataSplice is a privately held company.\n\nThe product's main emphasis is providing a simplified mobile interface into IBM Maximo. \nThe product consists of three components: Remote Client, Administration Client and the Server. The primary modules are Inventory, Work Orders, Inspections, Condition Monitoring and Asset Management.\n\nThe product's remote client, is HTML5 compliant, and as such is platform agnostic. Supported systems include iOS (iPad), Android (Droid), and Windows Mobile and 8 (Surface, Phone and Desktop). The remote client is also able to utilize bar code scanners, mobile printers, and RFID readers.\n\nIn 2010, DataSplice introduced InspecTMI, a field inspections and operator rounds mobile collection system geared toward highly regulated inspection scenarios, such as substations, power generation, and safety inspections.\n"}
{"id": "547219", "url": "https://en.wikipedia.org/wiki?curid=547219", "title": "Dipole magnet", "text": "Dipole magnet\n\nA dipole magnet is a magnet in which opposite poles (\"i.e.\", North and South poles) are on opposite sides of the magnet. The simplest example of a dipole magnet is a \"bar magnet\".\n\nIn particle accelerators, a dipole magnet is the electromagnet used to create a homogeneous magnetic field over some distance. Particle motion in that field will be circular in a plane perpendicular to the field and collinear to the direction of particle motion and free in the direction orthogonal to it.\nThus, a particle injected into a dipole magnet will travel on a circular or helical trajectory. By adding several dipole sections on the same plane, the bending radial effect of the beam increases.\n\nIn accelerator physics, dipole magnets are used to realize \"bends\" in the design trajectory (or 'orbit') of the particles, as in circular accelerators. Other uses include:\n\nOther uses of dipole magnets include isotope mass measurement in mass spectrometry, and particle momentum measurement in particle physics.\n\nSuch magnets are also used in traditional televisions, which contain a cathode ray tube, which is essentially a small particle accelerator. Their magnets are called \"deflecting coils\". The magnets move a single spot on the screen of the TV tube in a controlled way all over the screen.\n\n"}
{"id": "42362242", "url": "https://en.wikipedia.org/wiki?curid=42362242", "title": "Dwight C. Olson", "text": "Dwight C. Olson\n\nDwight C. Olson, known as the \"father of technology escrow\" was the founder of Data Securities International.\n\nMr. Olson received a bachelor’s degree in mathematics and teaching credentials from Augsburg College in Minneapolis, Minnesota.\n\nFounded Data Securities International in 1982. Pioneered technology escrow.\n\nHe began his career in the research and development of supercomputers, parallel processing systems and related application architectures in the 60's and 70’s. Mr. Olson is a former Chairman of the Board of Governors of the Certified Licensing Professionals, Inc (CLP), a former President of the Licensing Executives Society (LES), USA and Canada, and he has served as chair of LESI IP Valuation committee. He was an associate member of the American Bar Association's Electronic Commerce Law and Information Security Committee working on the ABA’s Digital Signature Guidelines.\n\nhttp://trumanenamels.com/Bio%20DCO%202017.htm\n"}
{"id": "1513863", "url": "https://en.wikipedia.org/wiki?curid=1513863", "title": "E-procurement", "text": "E-procurement\n\nE-procurement (electronic procurement, sometimes also known as supplier exchange) is the business-to-business or business-to-consumer or business-to-government purchase and sale of supplies, work, and services through the Internet as well as other information and networking systems, such as electronic data interchange and enterprise resource planning.\n\nThe e-procurement value chain consists of indent management, e-Informing, e-Tendering, e-Auctioning, vendor management, catalogue management, Purchase Order Integration, Order Status, Ship Notice, e-invoicing, e-payment, and contract management. Indent management is the workflow involved in the preparation of tenders. This part of the value chain is optional, with individual procuring departments defining their indenting process. In works procurement, administrative approval and technical sanction are obtained in electronic format. In goods procurement, indent generation activity is done online. The end result of the stage is taken as inputs for issuing the NIT.\n\nElements of e-procurement include request for information, request for proposal, request for quotation, RFx (the previous three together), and eRFx (software for managing RFx projects).\n\nIt was first used by IBM in the year 2000, when the company launched its \"Replenishment Management System and Method\", created by Mexican engineer Daniel Delfín, who was then the procurement director at IBM's largest production plant, and Alberto Wario, an IT programmer. The system was designed to solve IBM's complex procurement process for the plant in Guadalajara, Mexico, the largest IBM laptop producing plant in the World, with a production value of 1.6 billion dollars a year. Three years after the system was implemented, the production of the plant grew to 3.6 billion dollars, after which, the company used the system in Germany, and later sold using licenses to other companies around the World.\n\nE-procurement in the public sector is emerging internationally. Hence, initiatives have been implemented in Ukraine, India, Singapore, Estonia, United Kingdom, United States, Malaysia, Indonesia, Australia, European Union.\n\nPublic sector organizations use e-procurement for contracts to achieve benefits such as increased efficiency and cost savings (faster and cheaper) in government procurement and improved transparency (to reduce corruption) in procurement services. E-procurement in the public sector has seen rapid growth in recent years. Act 590 of Louisiana's 2008 Regular Legislative Session requires political subdivisions to make provisions for the receipt of electronic bids.\n\nE-procurement projects are often part of the country's larger e-Government efforts to better serve its citizens and businesses in the digital economy. For example, Singapore's GeBIZ was implemented as one of the programmes under its e-Government masterplan.<br>\nThe Procurement G6 leads the use of e-procurement instruments in Public procurement.\n\nAn example of successful reform is shown by Ukraine Prozorro. The result of collaboration between Ukrainian government, business sector, and civil society. This system was developed by international anti-corruption organization, Transparency International Ukraine, with a help of volunteers, NGOs, business community and state bodies of Ukraine, the WNISEF fund, the EBRD and other partners.\n\nThis field is populated by two types of vendors: big enterprise resource planning (ERP) providers which offer e-procurement as one of their services, and the more affordable services focused specifically on e-procurement.\nImplementing an e-procurement system benefits all levels of an organisation. E-procurement systems offer improved spend visibility and control and help finance officers match purchases with purchase orders, receipts and job tickets.\nAn e-procurement system also manages tenders through a web site. This can be accessed anywhere globally and has greatly improved the accessibility of tenders. An example is the System for Acquisition Management (SAM), which on July 30, 2013 combined information from the former Central Contractor Registration and Online Representations and Certifications Application (ORCA), in the United States.\n\n\n]"}
{"id": "2248385", "url": "https://en.wikipedia.org/wiki?curid=2248385", "title": "Eames Lounge Chair", "text": "Eames Lounge Chair\n\nThe Eames Lounge Chair and ottoman are furnishings made of molded plywood and leather, designed by Charles and Ray Eames for the Herman Miller furniture company. They are officially titled Eames Lounge (670) and Ottoman (671) and were released in 1956 after years of development by designers. It was the first chair that the Eameses designed for a high-end market. Examples of these furnishings are part of the permanent collection of New York's Museum of Modern Art.\n\nCharles and Ray Eames aimed to develop furniture that could be mass-produced and affordable, with the exception of the Eames Lounge Chair. This luxury item was inspired by the traditional English Club Chair. The Eames Lounge Chair is an icon of Modern style design, although when it was first made, Ray Eames remarked in a letter to Charles that the chair looked \"comfortable and un-designy\". Charles's vision was for a chair with \"the warm, receptive look of a well-used first baseman's mitt.\" The chair is composed of three curved plywood shells: the headrest, the backrest and the seat. In early production, beginning in 1956 and running through the very early 1990s, the shells were made up of five thin layers of plywood which were covered by a veneer of Brazilian rosewood. The use of Brazilian rosewood was discontinued in the early 1990s, and current production since then consists of seven layers of plywood covered by finishing veneers of cherry, walnut, Palisander rosewood (a sustainably grown wood with similar grain patterns to the original Brazilian versions), and other finishes.\n\nThe layers are glued together and shaped under heat and pressure. Earlier models are differentiated from newer models by the sets of rubber spacers between the aluminum spines and the wood panels first used in the earliest production models (and then hard plastic washers used in later versions) early first series versions of the chair used three screws to secure the armrests, second series models used two, the domes of silence (glides/feet) on the chair base had thinner screws originally (1950's era) attaching them to the aluminium base, these are not compatible with later chairs. In the earlier models, the zipper around the cushions may have been brown or black as well, and in newer models the zippers are black. The shells and the seat cushions are essentially the same shape, and composed of two curved forms interlocking to form a solid mass. The chair back and headrest are identical in proportion, as are the seat and the ottoman. Early ottomans had removable rubber slide on feet with metal glides. Early labels are oblong foil type.\n\nThe Eameses constantly made use of new materials. The pair's first plywood chair—the Eames Lounge Chair Wood (LCW)—made use of a heavy rubber washer glued to the backrest of the chair and screwed to the lumbar support. These washers, which have come to be called 'shock mounts', allow the backrest to flex slightly. This technology was brought back in the 670 Lounge chair. The backrest and headrest are screwed together by a pair of aluminum supports. This unit is suspended on the seat via two connection points in the armrests. The armrests are screwed to shock mounts which are connected only via glue to the interior of the backrest shell, allowing the backrest and headrest to flex when the chair is in use. This is part of the chair's unusual design, as well as its weakest link. The shock mounts have been known to tear free causing catastrophic collapse and damage.\n\nOther creative uses of materials include the seat cushions - which eschew standard stapled or nailed upholstery. Instead, the cushions are sewn with a zipper around the outer edge that connects them to a stiff plastic backing. The backing affixes to the plywood shells with a series of hidden clips and rings. This design, along with the hidden shock mounts in the armrest allow the outside veneer of the chair to be unmarred by screws or bolts. The chair has a low seat which is permanently fixed at a recline. The seat of the chair swivels on a cast aluminum base, with glides that are threaded so that the chair may remain level.\n\nThe Eames Lounge Chair first appeared on the Arlene Francis \"Home\" show broadcast on the NBC television network in the USA in 1956. Immediately following the debut, Herman Miller launched an advertising campaign that highlighted the versatility of the chair. Print ads depicted the 670 in a Victorian parlor, occupied by a grandmother shelling peas on the front porch of an American Gothic style house, and in the middle of a sunny field of hay. It has been frequently featured in \"Frasier\" as a piece of furniture in the title character's apartment. In the final episode of the series, Martin Crane remarks that he finds it comfortable and hints that he may not have needed his recliner after all. A knockoff of the Eames Lounge Chair has been frequently featured in the show \"House,\" in the protagonist's office\".\" Malory Archer's office chair in \"Archer\" is also an animation of the Eames Lounge Chair.\n\nSince its introduction, the chair has been in continuous production by Herman Miller in America. Later, Vitra (in cooperation with the German furniture company Fritz Becker KG) began producing the chair for the European market. It was licensed in the UK for 10 years to Hille International LTD from 1957. Immediately following its release, other furniture companies began to copy the chair's design. Some made direct copies, others were merely influenced by the design. The former Plycraft Company issued dozens of chairs that were direct copies of or in-the-style-of the Eames 670. Later Chinese and European companies began making direct copies. However, Herman Miller and Vitra remain the only two companies to produce these chairs with the Eames name attached.\n\nIn 2006, to commemorate the 50th anniversary of the chair, Herman Miller released models using a sustainable Palisander rosewood veneer.\n\n\n\n"}
{"id": "31940972", "url": "https://en.wikipedia.org/wiki?curid=31940972", "title": "Ericsson DBH 1001 telephone", "text": "Ericsson DBH 1001 telephone\n\nL'Ericsson DBH 1001 de 1931 était un projet de collaboration entre le Elektrisk Bureau d'Oslo, Televerket (Suède) et Lars Magnus Ericsson (1846-1926). Il a été conçu par l'ingénieur électricien norvégien Johan Christian Bjerknes (1889-1983) et par l'artiste et designer norvégien Jean Heiberg (1884-1976). C'était le premier téléphone en bakélite avec berceau, cadran et sonnerie intégrés; il était très moderne à l'époque [3]. [4] [5]\n\nJusqu'au début des années 1930, les modèles de téléphones suédois étaient fabriqués en acier pressé. Le passage de l'acier à la bakélite a ouvert de nouvelles perspectives en matière de conception, tout en réduisant le temps de production du boîtier. Le téléphone en bakélite était non seulement compact, mais aussi léger, juste au-dessous de 3 kg, et pouvait être saisi d’une seule main. Avec son design angulaire simple et incurvé, l’appareil a immédiatement séduit l’industrie et les consommateurs et a exercé une grande influence. Dans la plupart des pays européens, il était connu sous le nom de téléphone suédois [6]\n\nThe Ericsson DBH 1001 of 1931 was a collaborative project between the Elektrisk Bureau in Oslo, Televerket (Sweden) and Lars Magnus Ericsson(1846–1926). It was designed by Norwegian electrical engineer Johan Christian Bjerknes (1889-1983) and Norwegian artist and designer Jean Heiberg (1884–1976). It was the first Bakelite phone with integral cradle, dial and ringer, and was very modern for its time.\nUntil the early 1930s, the housing of the Swedish phone models was made from pressed steel. Material change from steel to Bakelite brought new opportunities in design, while also reducing the production time for the housing. The Bakelite phone was not only compact but also light at just below 3kg, and could be grasped by one hand. The device with its simple, curved angular design became an instant hit with the industry as well as the consumers, and was highly influential. In most of Europe, it was known as the \"Swedish type of telephone\".\n\nAlready in the mid 1930s, Ericsson showed a white Bakelite phone in advertisements, often in the hands of a young woman. However, this model seems never to have been offered to the public. At the 1939 World Fair in New York City, Ericsson showed a transparent variant made of acrylic and diakon plastic. \n\nThe standard color of the phone was black, but there were also variants in drab brown, red, and green. The process of using Bakelite did not permit the production of bright colors. The device was manufactured from 1933 in a smaller format, and from 1947 redesigned with softer, more rounded edges as the model designated M50. It was also offered in white melamine. The metal dial rotor was replaced with a plastic version, and a spiral cord became standard.\n\nDuring 1950, Ericsson also experimented with a keypad version, instead of the rotary dial, but it would take another ten years before they became standard in Swedish phones. In 1962, the Bakelite Phone was replaced by the Ericsson Dialog model.\n\n\n"}
{"id": "44302704", "url": "https://en.wikipedia.org/wiki?curid=44302704", "title": "Food Processing Technology Building", "text": "Food Processing Technology Building\n\nThe Food Processing Technology Building is a Georgia Institute of Technology and Georgia Tech Research Institute facility. It houses the Food Processing Technology Division of GTRI, which includes the Agricultural Technology Research Program (ATRP) and Georgia’s Traditional Industries Program for Food Processing. It opened on March 1, 2005, and was dedicated on May 19, 2005.\n\nThe Food Processing Technology Building contains over 36,000 square feet of office and laboratory space, including a 4,370 square foot high-bay testing and fabrication space, a 16-by-24-foot climate-controlled experiment chamber, an indoor environmental pilot area, a full-service chemical wet laboratory, and a 48-seat auditorium. The building houses five research laboratories: an automation research laboratory, an electronics lab, a systems development and integration laboratory, an environmental laboratory, and an optics laboratory. The building's lower lobby area features an interactive exhibit about the role of technology in poultry and food processing.\n"}
{"id": "1901396", "url": "https://en.wikipedia.org/wiki?curid=1901396", "title": "Hot comb", "text": "Hot comb\n\nA hot comb (also known as a straightening comb) is a metal comb that is used to straighten moderate or coarse hair and create a smoother hair texture. A hot comb is heated and used to straighten the hair from the roots. It can be placed directly on the source of heat or it may be electrically heated.\n\nThe hot comb was an invention developed in France as a way for women with coarse curly hair to achieve a fine straight look traditionally modeled by historical Egyptian women. \n\nParisian Francois Marcel Grateau is said to have revolutionized hair styling when he invented and introduced heated irons to curl and wave his customers' hair in France in 1872. His Marcel Wave remained fashionable for many decades. Britain's Science and Society Library credits L. Pelleray of Paris with manufacturing the heated irons in the 1870s. An example of an 1890s version of Pelleray's curling iron is housed at the Chudnow Museum in Milwaukee.\n\nElroy J. Duncan is believed to have invented and manufactured the first hot comb or heated metal straightening comb in America. Sometimes the device is called a \"pressing comb.\" During the late 19th century, Dr. Scott's Electric Curler was advertised in several publications including the 1886 Bloomingdale's catalog and in the June 1889 issue of \"Lippincott's Magazine\" Marketed to men to groom beards and moustaches, the rosewood-handled device also promised women the ability to imitate the \"loose and fluffy\" hairstyles of actress Lillie Langtry and opera singer Adelina Patti, popular white entertainers of the era.\n\nMme. Baum's Hair Emporium, a store on Eighth Avenue in New York with a large black female clientele, advertised Mme. Baum's \"entirely new and improved\" straightening comb in 1912. In May and June 1914, other Mme. Baum advertisements claimed that she now had a \"shampoo dryer and hair straightening comb,\" said to have been patented on April 1, 1914. U.S. Patent 1,096,666 for a heated \"hair drying\" comb – but not a hair straightening comb – is credited to Emilia Baum and was granted on May 12, 1914.\n\nIn May 1915, the Humania Hair Company of New York marketed a \"straightening comb made of solid brass\" for 89 cents. That same month, Wolf Brothers of Indianapolis advertised its hair straightening comb and alcohol heater comb for $1.00. The La Creole Company of Louisville claimed to have invented a self-heating comb that required no external flame. In September 1915, J. E. Laing, owner of Laing's Hair Dressing Parlor in Kansas City, Kansas claimed to have invented the \"king of all straighteners\" with a 3/4 inch wide, 9 1/2 inches long comb that also had a reversible handle to accommodate use with either the left or right hand. Indol Laboratories, owned by Bernia Austin in Harlem, offered a steel magnetic comb for $5.00 in November 1916.\n\nWalter Sammons of Philadelphia filed an application for Patent No. 1,362,823 on April 9, 1920. The patent was granted on December 21, 1920. Poro Company founder Annie Malone has been credited by some sources with receiving the first patent for this tool in that same year but the \"Official Gazette of the U. S. Patent Office\" does not list her as a holder of a hot comb patent in 1920. \n\nThe \"Patent Office Gazette\" of May 16, 1922, however, includes Annie M. Malone of St. Louis in a list of patentees of designs as being granted Patent No. 60,962 for \"sealing tape,\" which Chajuana V. Trawick describes in a December 2011 doctoral dissertation as an ornamental tape used to \"secure the closure of the box lid of Poro products\" to prevent others from selling products in packages made to look like Poro products.\n\nHair care entrepreneur Madam C. J. Walker never claimed to have invented the hot comb, though often has been inaccurately credited with the invention and with modifying the spacing of the teeth, but there is no evidence or documentation to support that assertion. During the 1910s, Walker obtained her combs from different suppliers, including Louisa B. Cason of Cincinnati, Ohio, who eventually filed patent application 1,413,255 on February 17, 1921 for a comb Cason had developed some years earlier. The patent was granted on April 18, 1922 though Cason had been producing the combs for many years without a patent.\n\nIt is not uncommon, especially when using a traditional hot comb, to burn and damage hair. A hot comb is often heated to over 65 degrees Celsius (149 degrees Fahrenheit), therefore if not careful severe burns and scarring can occur. Hot comb alopecia and follicular degeneration syndrome are irreversible alopecia of the scalp that was believed to occur in people who straighten their hair with hot combs, but this idea was later debunked. \n\nThe hot petrolatum used with the iron was thought to cause a chronic inflammation around the upper segment of the hair follicle leading to degeneration of the external root sheath. \n\nIn 1992, a hot comb alopecia study was conducted, and it was discovered that there was a poor correlation between the usage of a hot comb and the onset and progression of disease. The study concludes that the term follicular degeneration syndrome (FDS) is proposed for this clinically and histologically distinct form of scarring alopecia.\n\n"}
{"id": "14276", "url": "https://en.wikipedia.org/wiki?curid=14276", "title": "Hotel", "text": "Hotel\n\nA hotel is an establishment that provides paid lodging on a short-term basis. Facilities provided may range from a modest-quality mattress in a small room to large suites with bigger, higher-quality beds, a dresser, a refrigerator and other kitchen facilities, upholstered chairs, a flat screen television, and en-suite bathrooms. Small, lower-priced hotels may offer only the most basic guest services and facilities. Larger, higher-priced hotels may provide additional guest facilities such as a swimming pool, business centre (with computers, printers, and other office equipment), childcare, conference and event facilities, tennis or basketball courts, gymnasium, restaurants, day spa, and social function services. Hotel rooms are usually numbered (or named in some smaller hotels and B&Bs) to allow guests to identify their room. Some boutique, high-end hotels have custom decorated rooms. Some hotels offer meals as part of a room and board arrangement. In the United Kingdom, a hotel is required by law to serve food and drinks to all guests within certain stated hours. In Japan, capsule hotels provide a tiny room suitable only for sleeping and shared bathroom facilities.\nThe precursor to the modern hotel was the inn of medieval Europe. For a period of about 200 years from the mid-17th century, coaching inns served as a place for lodging for coach travelers. Inns began to cater to richer clients in the mid-18th century. One of the first hotels in a modern sense was opened in Exeter in 1768. Hotels proliferated throughout Western Europe and North America in the early 19th century, and luxury hotels began to spring up in the later part of the 19th century.\n\nHotel operations vary in size, function, complexity, and cost. Most hotels and major hospitality companies have set industry standards to classify hotel types. An upscale full-service hotel facility offers luxury amenities, full service accommodations, an on-site restaurant, and the highest level of personalized service, such as a concierge, room service, and clothes pressing staff. Full service hotels often contain upscale full-service facilities with a large number of full service accommodations, an on-site full service restaurant, and a variety of on-site amenities. Boutique hotels are smaller independent, non-branded hotels that often contain upscale facilities. Small to medium-sized hotel establishments offer a limited amount of on-site amenities. Economy hotels are small to medium-sized hotel establishments that offer basic accommodations with little to no services. Extended stay hotels are small to medium-sized hotels that offer longer-term full service accommodations compared to a traditional hotel.\n\nTimeshare and destination clubs are a form of property ownership involving ownership of an individual unit of accommodation for seasonal usage. A motel is a small-sized low-rise lodging with direct access to individual rooms from the car park. Boutique hotels are typically hotels with a unique environment or intimate setting. A number of hotels have entered the public consciousness through popular culture, such as the Ritz Hotel in London. Some hotels are built specifically as a destination in itself, for example at casinos and holiday resorts.\n\nMost hotel establishments are run by a General Manager who serves as the head executive (often referred to as the \"Hotel Manager\"), department heads who oversee various departments within a hotel (e.g., food service), middle managers, administrative staff, and line-level supervisors. The organizational chart and volume of job positions and hierarchy varies by hotel size, function and class, and is often determined by hotel ownership and managing companies.\n\nThe word \"hotel\" is derived from the French \"hôtel\" (coming from the same origin as \"hospital\"), which referred to a French version of a building seeing frequent visitors, and providing care, rather than a place offering accommodation. In contemporary French usage, \"hôtel\" now has the same meaning as the English term, and \"hôtel particulier\" is used for the old meaning, as well as \"hôtel\" in some place names such as Hôtel-Dieu (in Paris), which has been a hospital since the Middle Ages. The French spelling, with the circumflex, was also used in English, but is now rare. The circumflex replaces the 's' found in the earlier \"hostel\" spelling, which over time took on a new, but closely related meaning. Grammatically, hotels usually take the definite article – hence \"The Astoria Hotel\" or simply \"The Astoria.\"\n\nFacilities offering hospitality to travellers have been a feature of the earliest civilizations. In Greco-Roman culture and ancient Persia, hospitals for recuperation and rest were built at thermal baths. Japan's Nishiyama Onsen Keiunkan, founded in 705, was officially recognised by the Guinness World Records as the oldest hotel in the world. During the Middle Ages, various religious orders at monasteries and abbeys would offer accommodation for travellers on the road.\n\nThe precursor to the modern hotel was the inn of medieval Europe, possibly dating back to the rule of Ancient Rome. These would provide for the needs of travellers, including food and lodging, stabling and fodder for the traveller's horse(s) and fresh horses for the mail coach. Famous London examples of inns include the George and the Tabard. A typical layout of an inn had an inner court with bedrooms on the two sides, with the kitchen and parlour at the front and the stables at the back.\n\nFor a period of about 200 years from the mid-17th century, coaching inns served as a place for lodging for coach travellers (in other words, a roadhouse). Coaching inns stabled teams of horses for stagecoaches and mail coaches and replaced tired teams with fresh teams. Traditionally they were seven miles apart, but this depended very much on the terrain.\nSome English towns had as many as ten such inns and rivalry between them was intense, not only for the income from the stagecoach operators but for the revenue for food and drink supplied to the wealthy passengers. By the end of the century, coaching inns were being run more professionally, with a regular timetable being followed and fixed menus for food.\n\nInns began to cater for richer clients in the mid-18th century, and consequently grew in grandeur and the level of service provided. One of the first hotels in a modern sense was opened in Exeter in 1768, although the idea only really caught on in the early 19th century. In 1812 Mivart's Hotel opened its doors in London, later changing its name to Claridge's.\n\nHotels proliferated throughout Western Europe and North America in the 19th century, and luxury hotels, including the Savoy Hotel in the United Kingdom and the Ritz chain of hotels in London and Paris and Tremont House and Astor House in the United States, began to spring up in the later part of the century, catering to an extremely wealthy clientele.\n\nHotels cater to travelers from many countries and languages, since no one country dominates the travel industry.\n\nHotel operations vary in size, function, and cost. Most hotels and major hospitality companies that operate hotels have set widely accepted industry standards to classify hotel types. General categories include the following:\n\nA luxury hotel offers high quality amenities, full service accommodations, on-site full-service restaurants, and the highest level of personalized and professional service. Luxury hotels are normally classified with at least a Five Diamond rating by American Automobile Association or Five Star hotel rating depending on the country and local classification standards. \"Examples include: InterContinental, Fairmont, Sofitel, Raffles, Jumeirah, Conrad, Mandarin Oriental, Four Seasons, The Peninsula, Grand Hyatt, JW Marriott and The Ritz-Carlton.\"\n\nBoutique hotels are smaller independent non-branded hotels that often contain upscale facilities of varying size in unique or intimate settings with full service accommodations. These hotels are generally 100 rooms or fewer. Lifestyle hotels are branded properties that appeal to a guest with specific lifestyle or personal image. They are typically full-service and sometimes classified as luxury. A key characteristic of boutique and lifestyle hotels is their focus on providing a unique guest experience as opposed to simply providing lodging. Examples include W Hotels, Shangri-La Hotels, Sheraton, Andaz, Hoshino Resorts and Banyan Tree.\n\nFull service hotels often provide a wide array of guest services and on-site facilities. Commonly found amenities may include: on-site food and beverage (room service and restaurants), meeting and conference services and facilities, fitness center, and business center. Full-service hotels range in quality from mid-scale to luxury. This classification is based upon the quality of facilities and amenities offered by the hotel. Examples include: Holiday Inn, Kimpton Hotels, Hilton, Marriott and Hyatt Regency brands.\n\nSmall to medium-sized hotel establishments that offer a limited number of on-site amenities that only cater and market to a specific demographic of travelers, such as the single business traveler. Most focused or select service hotels may still offer full service accommodations but may lack leisure amenities such as an on-site restaurant or a swimming pool. Examples include Hyatt Place, Courtyard by Marriott and Hilton Garden Inn.\n\nSmall to medium-sized hotel establishments that offer a very limited number of on-site amenities and often only offer basic accommodations with little to no services, these facilities normally only cater and market to a specific demographic of travelers, such as the budget-minded traveler seeking a \"no frills\" accommodation. Limited service hotels often lack an on-site restaurant but in return may offer a limited complimentary food and beverage amenity such as on-site continental breakfast service. Examples include Ibis Budget, Hampton Inn, Aloft, Holiday Inn Express, Fairfield Inn, Four Points by Sheraton.\n\nExtended stay hotels are small to medium-sized hotels that offer longer term full service accommodations compared to a traditional hotel. Extended stay hotels may offer non-traditional pricing methods such as a weekly rate that caters towards travelers in need of short-term accommodations for an extended period of time. Similar to limited and select service hotels, on-site amenities are normally limited and most extended stay hotels lack an on-site restaurant. Examples include Staybridge Suites, Candlewood Suites, Homewood Suites by Hilton, Home2 Suites by Hilton, Residence Inn by Marriott, Element, and Extended Stay America.\n\nTimeshare and Destination clubs are a form of property ownership also referred to as a vacation ownership involving the purchase and ownership of an individual unit of accommodation for seasonal usage during a specified period of time. Timeshare resorts often offer amenities similar that of a Full service hotel with on-site restaurant(s), swimming pools, recreation grounds, and other leisure-oriented amenities. Destination clubs on the other hand may offer more exclusive private accommodations such as private houses in a neighborhood-style setting. Examples of timeshare brands include Hilton Grand Vacations, Marriott Vacation Club International, Westgate Resorts, Disney Vacation Club, and Holiday Inn Club Vacations.\n\nA motel, an abbreviation for \"motor hotel\", is a small-sized low-rise lodging establishment similar to a limited service, lower-cost hotel, but typically with direct access to individual rooms from the car park. Motels were built to serve road travellers, including travellers on road trip vacations and workers who drive for their job (travelling salespeople, truck drivers, etc.). Common during the 1950s and 1960s, motels were often located adjacent to a major highway, where they were built on inexpensive land at the edge of towns or along stretches of freeway.\n\nNew motel construction is rare in the 2000s as hotel chains have been building economy-priced, limited service franchised properties at freeway exits which compete for largely the same clientele, largely saturating the market by the 1990s. Motels are still useful in less populated areas for driving travelers, but the more populated an area becomes, the more hotels move in to meet the demand for accommodation. While many motels are unbranded and independent, many of the other motels which remain in operation joined national franchise chains, often rebranding themselves as hotels, inns or lodges. Some examples of chains with motels include EconoLodge, Motel 6, Super 8, and Travelodge.\n\nHotels may offer rooms for microstays, a type of booking for less than 24 hours where the customer chooses the check in time and the length of the stay. This allows the hotel increased revenue by reselling the same room several times a day.\n\nHotel management is a globally accepted professional career field and academic field of study. Degree programs such as hospitality management studies, a business degree, and/or certification programs formally prepare hotel managers for industry practice.\n\nMost hotel establishments consist of a General Manager who serves as the head executive (often referred to as the \"Hotel Manager\"), department heads who oversee various departments within a hotel, middle managers, administrative staff, and line-level supervisors. The organizational chart and volume of job positions and hierarchy varies by hotel size, function, and is often determined by hotel ownership and managing companies.\n\nBoutique hotels are typically hotels with a unique environment or intimate setting.\nSome hotels have gained their renown through tradition, by hosting significant events or persons, such as Schloss Cecilienhof in Potsdam, Germany, which derives its fame from the Potsdam Conference of the World War II allies Winston Churchill, Harry Truman and Joseph Stalin in 1945. The Taj Mahal Palace & Tower in Mumbai is one of India's most famous and historic hotels because of its association with the Indian independence movement. Some establishments have given name to a particular meal or beverage, as is the case with the Waldorf Astoria in New York City, United States where the Waldorf Salad was first created or the Hotel Sacher in Vienna, Austria, home of the Sachertorte. Others have achieved fame by association with dishes or cocktails created on their premises, such as the Hotel de Paris where the crêpe Suzette was invented or the Raffles Hotel in Singapore, where the Singapore Sling cocktail was devised.\n\nA number of hotels have entered the public consciousness through popular culture, such as the Ritz Hotel in London, through its association with Irving Berlin's song, 'Puttin' on the Ritz'. The Algonquin Hotel in New York City is famed as the meeting place of the literary group, the Algonquin Round Table, and Hotel Chelsea, also in New York City, has been the subject of a number of songs and the scene of the stabbing of Nancy Spungen (allegedly by her boyfriend Sid Vicious).\n\nSome hotels are built specifically as a destination in itself to create a captive trade, example at casinos, amusement parks and holiday resorts. Though hotels have always been built in popular destinations, the defining characteristic of a resort hotel is that it exists purely to serve another attraction, the two having the same owners.\n\nOn the Las Vegas Strip there is a tradition of one-upmanship with luxurious and extravagant hotels in a concentrated area. This trend now has extended to other resorts worldwide, but the concentration in Las Vegas is still the world's highest: nineteen of the world's twenty-five largest hotels by room count are on the Strip, with a total of over 67,000 rooms.\n\nIn Europe Center Parcs might be considered a chain of resort hotels, since the sites are largely man-made (though set in natural surroundings such as country parks) with captive trade, whereas holiday camps such as Butlins and Pontin's are probably not considered as resort hotels, since they are set at traditional holiday destinations which existed before the camps.\n\n\nThe Null Stern Hotel in Teufen, Appenzellerland, Switzerland and the Concrete Mushrooms in Albania are former nuclear bunkers transformed into hotels.\n\nThe Cuevas Pedro Antonio de Alarcón (named after the author) in Guadix, Spain, as well as several hotels in Cappadocia, Turkey, are notable for being built into natural cave formations, some with rooms underground. The Desert Cave Hotel in Coober Pedy, South Australia is built into the remains of an opal mine.\n\nLocated on the coast but high above sea level, these hotels offer unobstructed panoramic views and a great sense of privacy without the feeling of total isolation. Some examples from around the globe are the Riosol Hotel in Gran Canaria, Caruso Belvedere Hotel in Amalfi Coast (Italy), Aman Resorts Amankila in Bali, Birkenhead House in Hermanus (South Africa), The Caves in Jamaica and Caesar Augustus in Capri.\n\nCapsule hotels are a type of economical hotel first introduced in Japan, where people sleep in stacks of rectangular containers.\n\nSome hotels fill daytime occupancy with day rooms, for example, Rodeway Inn and Suites near Port Everglades in Fort Lauderdale, Florida. Day rooms are booked in a block of hours typically between 8 am and 5 pm, before the typical night shift. These are similar to transit hotels in that they appeal to travelers, however, unlike transit hotels, they do not eliminate the need to go through Customs.\n\nGarden hotels, famous for their gardens before they became hotels, include Gravetye Manor, the home of garden designer William Robinson, and Cliveden, designed by Charles Barry with a rose garden by Geoffrey Jellicoe.\n\nThe Ice Hotel in Jukkasjärvi, Sweden, was the first ice hotel in the world; first built in 1990, it is built each winter and melts every spring. Other ice hotels include the Igloo Village in Kakslauttanen, Finland, and the Hotel de Glace in Duschenay, Canada. They can also be included within larger ice complexes; for example, the Mammut Snow Hotel in Finland is located within the walls of the Kemi snow castle; and the Lainio Snow Hotel is part of a snow village near Ylläs, Finland.\n\nA love hotel (also 'love motel', especially in Taiwan) is a type of short-stay hotel found around the world, operated primarily for the purpose of allowing guests privacy for sexual activities, typically for one to three hours, but with overnight as an option. Styles of premises vary from extremely low-end to extravagantly appointed. In Japan, love hotels have a history of over 400 years.\n\nA referral hotel is a hotel chain that offers branding to independently-operated hotels; the chain itself is founded by or owned by the member hotels as a group. Many former referral chains have been converted to franchises; the largest surviving member-owned chain is Best Western.\n\nThe first recorded purpose-built railway hotel was the Great Western Hotel, which opened adjacent to Reading railway station in 1844, shortly after the Great Western Railway opened its line from London. The building still exists, and although it has been used for other purposes over the years, it is now again a hotel and a member of the Malmaison hotel chain.\n\nFrequently, expanding railway companies built grand hotels at their termini, such as the Midland Hotel, Manchester next to the former Manchester Central Station, and in London the ones above St Pancras railway station and Charing Cross railway station. London also has the Chiltern Court Hotel above Baker Street tube station, there are also Canada's grand railway hotels. They are or were mostly, but not exclusively, used by those traveling by rail.\n\nThe Maya Guesthouse in Nax Mont-Noble in the Swiss Alps, is the first hotel in Europe built entirely with straw bales. Due to the insulation values of the walls it needs no conventional heating or air conditioning system, although the Maya Guesthouse is built at an altitude of in the Alps.\n\nTransit hotels are short stay hotels typically used at international airports where passengers can stay while waiting to change airplanes. The hotels are typically on the airside and do not require a visa for a stay or re-admission through security checkpoints.\n\nSome hotels are built with living trees as structural elements, for example the Treehotel near Piteå, Sweden, the Costa Rica Tree House in the Gandoca-Manzanillo Wildlife Refuge, Costa Rica; the Treetops Hotel in Aberdare National Park, Kenya; the Ariau Towers near Manaus, Brazil, on the Rio Negro in the Amazon; and Bayram's Tree Houses in Olympos, Turkey.\n\nSome hotels have accommodation underwater, such as Utter Inn in Lake Mälaren, Sweden. Hydropolis, project in Dubai, would have had suites on the bottom of the Persian Gulf, and Jules' Undersea Lodge in Key Largo, Florida requires scuba diving to access its rooms.\n\nA resort island is an island or an archipelago that contains resorts, hotels, overwater bungalows, restaurants, tourist attractions and its amenities. Maldives has the most overwater bungalows resorts.\n\nIn 2006, \"Guinness World Records\" listed the First World Hotel in Genting Highlands, Malaysia, as the world's largest hotel with a total of 6,118 rooms (and which has now expanded to 7,351 rooms). The Izmailovo Hotel in Moscow has the most beds, with 7,500, followed by The Venetian and The Palazzo complex in Las Vegas (7,117 rooms) and MGM Grand Las Vegas complex (6,852 rooms).\n\nAccording to the Guinness Book of World Records, the oldest hotel in operation is the Nisiyama Onsen Keiunkan in Yamanashi, Japan. The hotel, first opened in AD 707 has been operated by the same family for forty-six generations. The title was held until 2011 by the Hoshi Ryokan, in the Awazu Onsen area of Komatsu, Japan, which opened in the year 718, as the history of the Nisiyama Onsen Keiunkan was virtually unknown.\n\nThe Ritz-Carlton, Hong Kong claims to be the world's highest hotel. It is located on the top floors of the International Commerce Centre in Hong Kong, at above ground level.\n\nIn October 2014, the Anbang Insurance Group, based in China, purchased the Waldorf Astoria New York in Manhattan for US$1.95 billion, making it the world's most expensive hotel ever sold.\n\nA number of public figures have notably chosen to take up semi-permanent or permanent residence in hotels.\n\n"}
{"id": "34890114", "url": "https://en.wikipedia.org/wiki?curid=34890114", "title": "Laflin &amp; Rand Powder Company", "text": "Laflin &amp; Rand Powder Company\n\nLaflin & Rand Powder Company was a gunpowder and early smokeless powder manufacturer notable for producing the smokeless powder used by United States Army infantry rifles from 1896 to 1908, which included the period of development of the M1903 Springfield rifle and .30-06 Springfield cartridge.\n\nMatthew Laflin manufactured potassium nitrate for the Massachusetts militia during the American Revolutionary War, and built a gunpowder mill in Southwick, Massachusetts after the war. After Laflin's death in 1810, his grandchildren expanded the family business with two mills in New York and one in Wisconsin. These mills produced gunpowder for the Union forces through the American Civil War. Laflin Powder Company was incorporated in 1866 to consolidate operations to compete successfully for the reduced gunpowder demand after the war. Laflin Powder Company further consolidated gunpowder manufacturing around the Orange Mill Historic District near Newburgh, New York by merger with the competing Smith & Rand Powder Company on 24 August 1869 to form the Laflin & Rand Powder Company of New York City, with Albert Rand as president.\n\nIn 1872 Laflin & Rand formed the United States Gunpowder Trade Association (popularly known as the powder trust) with DuPont, Hazard Powder Company, and three smaller gunpowder manufacturers. DuPont and Laflin & Rand jointly established the Repauno Chemical Company in 1880, the Hercules Powder Company in 1882, and the Eastern Dynamite Company in 1895.\n\nSmokeless powder became a threat to traditional United States gunpowder markets in 1893. Winchester Repeating Arms Company began loading sporting ammunition with smokeless powder manufactured by the Anglo-American Explosives Company; and the United States Army published smokeless powder specifications for the new Krag-Jørgensen service rifle. Laflin & Rand negotiated a license to produce Alfred Nobel's patented \"Ballistite\" smokeless powder adopted by many European military forces; but the U.S. Army selected \"Ruby\" smokeless powder produced by the newly formed Leonard Smokeless Powder Company. Rather than pay the required royalties to manufacture \"Ballistite\", Laflin & Rand loaned the financially troubled Leonard company $30,000 to reorganize as the American Smokeless Powder Company. U.S. Army Lieutenant Garland Whistler assisted American Smokeless Powder Company factory superintendent Henry Aspinwall in formulating an improved powder. They reduced the nitroglycerine content from 58% to 30%; and the extruded formulation, named W.A. in recognition of their work, was cut into black perforated discs in diameter and thick. W.A. .30 caliber smokeless powder was the standard for United States military service rifles from 1896 until 1908. When the American Smokeless Powder Company plant was destroyed by an explosion in 1898, Laflin & Rand took over the company and rebuilt the plant in Haskell, New Jersey (named for Laflin & Rand president Jonathan Haskell.) The Haskell plant became Laflin & Rand's primary smokeless powder factory producing not only military W.A. powder, but sporting powders including \"Bullseye\" pistol powder (introduced in 1898 using small, irregular particles removed by screening runs of larger grained powders), \"Lightning\" powder for lever-action sporting rifles (introduced in 1899), and shotgun powders \"Infallible\" and \"Unique\" (introduced in 1900).\n\nLaflin & Rand and DuPont together commanded over two-thirds of the United States explosives and gunpowder industry by 1900. In October 1902 DuPont purchased Laflin & Rand and operated the company as a subsidiary until Federal antitrust action required divestiture in 1912. When Laflin & Rand was dissolved, the reorganized Hercules Powder Company received Laflin & Rand patents for smokeless powder and continued manufacture at Kenvil, New Jersey under the direction of Haskell ballistics laboratory director Bernhart Troxler. The former Laflin & Rand plant at Haskell operated until dismantled in 1926. Hercules Powder Company continued to produce \"Bullseye\" and \"Unique\" powders at other plants until selling the product line to the Alliant Powder Company. Alliant Powder Company continued manufacturing these smokeless powders into the 21st century.\n"}
{"id": "19275267", "url": "https://en.wikipedia.org/wiki?curid=19275267", "title": "List of crude oil products", "text": "List of crude oil products\n\nIn the international petroleum industry, crude oil products are traded on various oil bourses based on established chemical profiles, delivery locations, and financial terms. The chemical profiles, or crude oil assays, specify important properties such as the oil's API gravity. The delivery locations are usually sea ports close to the oil fields from which the crude was obtained (and new fields are constantly being explored), and the pricing is usually quoted based on F.O.B. (\"free on board\", without consideration of final delivery costs).\n\nThe three most quoted oil products are North America's West Texas Intermediate crude (WTI), North Sea Brent Crude, and the UAE Dubai Crude, and their pricing is used as a barometer for the entire petroleum industry, although, in total, there are 46 key oil exporting countries. Brent Crude is typically priced at about $2 over the WTI Spot price, which is typically priced $5 to $6 above the EIA's Imported Refiner Acquisition Cost (IRAC) and OPEC Basket prices. WTI and Brent are quoted F.O.B specific locations, not F.O.B. the oilfields. For WTI, the delivery point is Cushing, OK; for Brent, it is Sullom Voe, located on Shetland, an island archipelago north of mainland Scotland.\n\nAlthough crude oil assays evaluate various chemical properties of the oil, the two most important properties determining a crude's value are its density (measured as API specific gravity) and its sulphur content (measured per mass). Crude oil is considered \"heavy\" if it has long hydrocarbon chains, or \"light\" if it has short hydrocarbon chains: an API gravity of 34 or higher is \"light\", between 31-33 is \"medium\", and 30 or below is \"heavy\".\nCrude is considered \"sweet\" if it is low in sulphur content (< 0.5%/weight), or \"sour\" if high (> 1.0%/weight). Generally, the higher the API gravity (the \"lighter\" it is), the more valuable the crude.\n\n"}
{"id": "20932823", "url": "https://en.wikipedia.org/wiki?curid=20932823", "title": "List of filling station chains in North America", "text": "List of filling station chains in North America\n\nThis is a list of major gas station chains in the United States, Canada, and Mexico. For notable single filling stations, see List of historic filling stations.\n\nA list of gas station chains in Canada:\n\nA list of gas station chains in Mexico:\n\n"}
{"id": "11346425", "url": "https://en.wikipedia.org/wiki?curid=11346425", "title": "List of valve amplifier designs", "text": "List of valve amplifier designs\n\nSome of the best known valve amplifiers are listed here.\n\nThe first commercially produced amplifier with distortion of 0.1% was the LEAK Type 15 \"Point One\" of 1945, using KT66 vacuum tubes (valves) connected as triodes, with 26dB feedback over 4 stages including the output transformer. In 1948 LEAK produced the TL/12 which was also rated at 0.1% but featured improved performance with 26dB over 3 stages and the output transformer (giving better gain-margin and phase-margin). The TL/12 sold in large quantities to professional users, radio stations, laboratories, as well as to the emerging market for hi-fi equipment. It is highly prized even today by audio enthusiasts.\n\nThe Williamson amplifier was published in 1947 as an article in \"Wireless World\", and was a milestone which defined the mainstream topology for the majority of amplifiers thereafter. The design gave particular attention to a very high specification for the output transformer, in addition to being generally a consistently well worked-through design. It was not itself originally a commercial design, but many commercial versions and derivatives were subsequently made (with and without due credit).\n\nTo promote their new 9-pin tubes EZ80, EF86 and EL84, the Anglo-Dutch Mullard Company developed, in 1954, a famous and very popular mono amplifier circuit, the Mullard 5-10. The design featured 5 tubes with 10 watts output: EF86 as preamplifier, ECC83 phase-splitting, two EL84 in push-pull configuration, EZ80 rectifier. The amplifier used the excellent Partridge output transformer and was well known for its great sound reproduction.\n\nDynaco was a major US manufacturer of completed and kit amplifiers of high quality, notably of the Mark III and the Stereo 70, claimed to be the most popular tube amp ever made with over 300,000 produced.\n\nMcIntosh Laboratory long held a reputation for producing very high quality high-end equipment in the US. The model MC275 was produced from May 1961 through July 1973.\n\nThe model MC275 perhaps demonstrates most clearly the revival in tube amplifiers, as used MC275s in mint condition can sell for up to eight times their original purchase price, and a \"Commemorative Edition\" made during the 1990s was priced at $4000.\n\nAnother historic manufacturer of valve amplifiers was Marantz. The Marantz 8B is another design that has been considered a classic and for which a \"Reissue\" edition has been produced.\n"}
{"id": "31488017", "url": "https://en.wikipedia.org/wiki?curid=31488017", "title": "MT-Energie", "text": "MT-Energie\n\nMT-Energie GmbH is a biogas technology company operating in the field of renewable energies. The company's headquarters are in Zeven (Germany) The UK branch office is located in Bridgnorth, Shropshire. MT-Energie develops and distributes both turnkey biogas powerplants and special bioprocess engineering components. Furthermore, it offers technical and biological services for biogas plants. The company’s subsidiary MT-Biomethan GmbH, which was founded as part of the MT Group in 2008, offers technologies for biogas upgrading based on the process of pressureless amine scrubbing and a membrane-based gas permeation.\n\nThe company was founded by Christoph Martens in 1995 and initially operated as engineering consultants. The early years of the company’s activities focused on the development of special biogas components. In 1997, Martens invented the so-called air-supported membrane cover for biogas plants. This technology is now used by various manufacturers of biogas plants. With the foundation of MT-Energie GmbH & Co. KG in 2001, the company increasingly focused on the construction of turnkey biogas plants. By the end of 2013, about 600 biogas projects with a total installed power of 350 MW had been completed. In 2007 the company opened its first site abroad in Italy. Today MT-Energie operates throughout Europe and distributes its products and services in many countries worldwide.\nMT-Energie was founded in the German town of Rockstedt in the district of Rotenburg (Wümme). In November 2008 the company moved its premises to the small town of Zeven in Lower Saxony. There, on the premises of , the new headquarters including offices, logistics area and manufacturing buildings for gas processing plants were constructed.\n\nWhen MT-Energie released preliminary results for the 2012 financial year on 25 April 2013, they talked of a collapse of demand in the German market and the high costs of internationalisation, especially in Italy and the United States. MT-Energie has reduced the number of full-time employees in Italy from 40 to 15 and has pulled out of the US market. A company press release from earlier in the month (April 2, 2013) denied rumours of liquidity problems and imminent insolvency.\nThe 2013 preliminary financial results were released on 30 June 2014 and they stated an expected loss of 23.3 million euros for the entire company. This loss is more than 25% of the expected company turnover of 87.7 million euros in the same period. \nMT-Energie has closed down the Australian, Canadian and US offices and pulled out of the North American and Australian markets completely. The company's only future involvement in North America will be completion of the already-begun projects in the USA. Due to the financial situation, all of the experienced country sales and project management directors were sacked or left the company, which leaves MT-Energie's foreign sales departments with inexperienced managers.\nMT-Energie also had a covenant breach on their 2012-issued company bond (Anleihe) due to owner's equity also falling to 4.1%. In order to avoid defaulting on their bonds, a deal was made with the bond holders.\n\nThis low level of owner's equity also means that the banks offering credit can cancel their credit agreements with MT-Energie. The banks and MT-Energie are now negotiating to find a solution.\n\nIn winter 2010/11, MT-Energie offered the national archery team of the German Disabled Sports Association its Zeven storage and logistics centre for their preparations for the summer season. The team members included double Olympic medallist Mario Oehme as well as the incumbent runner-up world champions Michael Arenz and Michael Müller of the “compound team”.\n\n"}
{"id": "22682042", "url": "https://en.wikipedia.org/wiki?curid=22682042", "title": "Magnetolithography", "text": "Magnetolithography\n\nMagnetolithography (ML) is a method for pattern surfaces. ML based on applying a magnetic field on the substrate using paramagnetic metal masks named \"magnetic mask\". Magnetic masks are analogous to a photomask in photolithography, in that they define the spatial distribution and shape of the applied magnetic field. The second component of the process is ferromagnetic nanoparticles (analogous to the photoresist in photolithography) that are assembled onto the substrate according to the field induced by the magnetic mask.\n\nML can be used for applying either a positive or negative approach. In the positive approach, the magnetic nanoparticles react chemically or interact via chemical recognition with the substrate. Hence, the magnetic nanoparticles are immobilized at selected locations, where the mask induces a magnetic field, resulting in a patterned substrate. In the negative approach, the magnetic nanoparticles are inert to the substrate. Hence, once they pattern the substrate, they block their binding site on the substrate from reacting with another reacting agent. After the adsorption of the reacting agent, the nanoparticles are removed, resulting in a negatively patterned substrate.\n\nML is a backside lithography, which has the advantage of ease in producing multilayer with high accuracy of alignment and with the same efficiency for all layers.\n\n"}
{"id": "16100068", "url": "https://en.wikipedia.org/wiki?curid=16100068", "title": "Microwave Power Module", "text": "Microwave Power Module\n\nA Microwave Power Module (MPM) is a microwave device used to amplify radio frequency signals to high power levels. It is a hybrid combination of solid-state and vacuum tube electronics, which encloses a solid-state driver amplifier (SSPA), traveling wave tube amplifier (TWTA) and electronic power conditioning (EPC) modules into a single unit. Their average output power capability falls between that of solid-state power amplifiers (SSPAs) and dedicated Traveling Wave Tube (TWT) amplifiers. They may be applied wherever high power microwave amplification is required, and space is at a premium. They are available in various frequency ranges, from S band up to W band. Typical output power at ranges from 20W to 1 kW.\n\nThe microwave power module concept was designed for use in active phased array antennas, where their compact size permits packing a large number of modules into the radiating face of the antenna. The concept was explored in detail by the 1989 Microwave Power Module Panel, supported by the US Naval Research Laboratory. While the eventual goal was to design a power module with a cross section as small as a half square inch, most MPMs today are larger, and suitable only for line arrays, partially distributed arrays and single-module applications.\n\nMicrowave power modules are available at various frequencies, from S band up to W band. Both CW and pulsed MPMs are available, the pulsed MPMs having a wide duty cycle range. Power levels range from less than 20W to over 1 kW. MPMs are light-weight compared to traditional TWTAs, and the power supply requirements are typically low-voltage DC (28 - 270V DC).\n\nA microwave power module consists of a solid state power amplifier, which drives a vacuum power booster, typically a traveling wave tube. The high voltage power supply required by the TWT is provided by an electronic power conditioner. In pulsed-mode MPMs, the power conditioner provides a pulsed high voltage that is triggered by a trigger input. MPMs also include a microcontroller, which is responsible for controlling the operation of the module, such as making sure the various power supply voltages come up in the appropriate sequence to prevent damage to the TWT. It also reports the module status, including the various voltages, currents and temperatures.\n\nMicrowave power modules are used in\n\n\n"}
{"id": "53191644", "url": "https://en.wikipedia.org/wiki?curid=53191644", "title": "Mike Butcher (journalist)", "text": "Mike Butcher (journalist)\n\nMike Butcher is a UK-based journalist and editor at large for TechCrunch.\n\n\n"}
{"id": "35085928", "url": "https://en.wikipedia.org/wiki?curid=35085928", "title": "Mineral cosmetics", "text": "Mineral cosmetics\n\nMineral cosmetics are forms of make-up that are composed of compressed minerals. Most of these products are pure mineral, and do not contain any oil or wax additives. Some kinds of minerals can have beneficial health results for the skin, combining health effects with cosmetic results. The claim that applying minerals directly to the skin in the form of cosmetics has health benefits, however, has inspired some scientific controversy. Historically some mineral additives have proven to be poisonous to the skin, such as lead. However the kinds of minerals used in modern products are not harmful to the skin.\n"}
{"id": "10929036", "url": "https://en.wikipedia.org/wiki?curid=10929036", "title": "Ministry of New and Renewable Energy", "text": "Ministry of New and Renewable Energy\n\nMinistry of New and Renewable Energy or MNRE is a ministry of the Government of India. The ministry is currently headed by R. K. Singh, a Minister of State-In Charge. The ministry was established as the Ministry of Non-Conventional Energy Sources in 1992. It adopted its current name in October 2006. \n\nThe Ministry is mainly responsible for research and development, intellectual property protection, and international cooperation, promotion, and coordination in renewable energy sources such as wind power, small hydro, biogas, and solar power. The broad aim of the ministry is to develop and deploy new and renewable energy for supplementing the energy requirements of India.\n\nThe ministry is headquartered in Lodhi Road, New Delhi. According to the Ministry's 2016-17 annual report, India has made significant advances in several renewable energy sectors which include, solar energy, wind power, and hydroelectricity.\n\n1970s energy crisis led to the establishment of the Commission for Additional Sources of Energy (CASE) in the Department of Science & Technology (India) in March 1981. The CASE was responsible for the formulation of policies and their implementation, creation of programmes for development of new and renewable energy and coordinating and intensifying R&D in the sector.\n\nIn 1982, a new department was created in the then Ministry of Energy, i.e., Department of Non-conventional Energy Sources (DNES). DNES incorporated CASE under its umbrella.\n\nA decade later, in 1992, DNES became the Ministry of Non-conventional Energy Sources (MNES). In October 2006, the Ministry was re-christened as the Ministry of New and Renewable Energy.\n\nThe Mission of the Ministry is to ensure\n\nTo develop new and renewable energy technologies, processes, materials, components, sub-systems, products & services at par with international specifications, standards and performance parameters in order to make the country a net foreign exchange earner in the sector and deploy such indigenously developed and/or manufactured products and services in furtherance of the national goal of energy security. \n\nThe major functional area or Allocation of Business of MNRE are:\n\n\n\n\n\n\n\n\n\n\nAs per Annual Report 2016-17 of Ministry, As of December 2016, the Ministry was successful in deploying a total of 50068.37 Megawatt (MW) capacity of grid-based renewable energy. 28700.44 MW of which was from Wind power, 4333.85 MW from Small hydro Power, 7907.34 MW from Bio power 9012.66 MW from Solar power (SPV), and the rest 114.08 MW from Waste to Power.\n\nDuring the same time period, the total deployment of an Off-grid based renewable energy capacity was about 1403.70 MW. Of these, Biomass [(non-bagasse) Cogeneration consisted of 651.91 MW, Bio mass Gasifier was 186.88 MW Waste to energy was 163.35 MW, SPV Systems (of less than 1 Kilowatt (kW)) capacity was 405.54 1 MW, and the rest from micro-Hydro and Wind power.\n\nThe total number of deployment of Family Biogas plant was 49.40 lakhs. And the total area that is covered with Solar water heating (SWH) systems was 4.47 Million m.\n\nThe Ministry has 5 specialized technical institution. They are:-\n\n\n\n\n\nThe Ministry has established state nodal agencies in different states and union territories of India to promote and expand the growth of efficient energy use of renewable energy in their respective states. The primary objective of a state nodal agency under this ministry is to develop, coordinate, finance and promote research projects in the new and renewable energy field. It is also expected to devise programmes for research and development as well as applicative extensions of new and renewable energy sources.\n\n"}
{"id": "10940142", "url": "https://en.wikipedia.org/wiki?curid=10940142", "title": "Multi-gigabit transceiver", "text": "Multi-gigabit transceiver\n\nA Multi-Gigabit Transceiver (MGT) is a SerDes capable of operating at serial bit rates above 1 Gigabit/second. MGTs are used increasingly for data communications because they can run over longer distances, use fewer wires, and thus have lower costs than parallel interfaces with equivalent data throughput.\n\nLike other SerDes, the primary function of the MGT is to transmit parallel data as stream of serial bits, and convert the serial bits it receives to parallel data. The most basic performance metric of an MGT is its serial bit rate, or line rate, which is the number of serial bits it can transmit or receive per second. Although there is no strict rule, MGTs can typically run at line rates of 1 Gigabit/second or more.\nMGTs have become the 'data highways' for data processing systems that demand a high in/out raw data input and output (e.g. video processing applications). They are becoming very common on FPGA - such programmable logic devices being especially well fitted for parallel data processing algorithms.\n\nBeyond serialization and deserialization, MGTs must incorporate a number of additional technologies to allow them to operate at high line rates. Some of these are listed below:\nSignal Integrity is critical for MGTs due to their high line rates. The quality of a given high-speed link is characterized by the Bit Error Ratio (BER) of the connection (the ratio of bits received in error to total bits received), and jitter.\n\nBER and jitter are functions of the entire MGT connection, including the MGTs themselves, their serial lines, their reference clocks, their power supplies, and the digital systems that create and consume their parallel data. As a result, MGTs are often measured by how little jitter they transmit (Jitter Transfer/Jitter Generation), and how much jitter they can tolerate before their BER is too high (Jitter Tolerance). These measurements are commonly taken using a BERT, and analyzed using an Eye diagram.\n\nSome other metrics for MGTs include:\n\nMGTs are used in the implementation of the following serial protocols:\n\n\n"}
{"id": "8133737", "url": "https://en.wikipedia.org/wiki?curid=8133737", "title": "Multi-project wafer service", "text": "Multi-project wafer service\n\nMulti-project chip (MPC), also known as multi-project wafer (MPW), services integrate onto microelectronics wafers a number of different integrated circuit designs from various teams including designs from private firms, students and researchers from universities. Because IC fabrication costs are extremely high, it makes sense to share mask and wafer resources to produce designs in low quantities. Worldwide, several MPW services are available from government-supported institutions or from private firms including Canadian Microelectronics Corporation (CMC), MOSIS, CMP, Europractice, eSilicon, WaferCatalyst, and Muse Semiconductor . \n\nThe first well known MPW service was MOSIS (Metal Oxide Silicon Implementation Service), established by DARPA as a technical and human infrastructure for VLSI. MOSIS began in 1981 after Lynn Conway organized the first VLSI System Design Course at MIT in 1978. \nMOSIS primarily services commercial users now but continues to serve university students and researchers.\n\nWith MOSIS, designs are submitted for fabrication using either open (i.e., non-proprietary) VLSI layout design rules or vendor proprietary rules.\nDesigns are pooled into common lots and run through the fabrication process at foundries. The completed chips (packaged or unpackaged) are returned to customers.\n\nBaySand announced their ASIC MPW Shuttle Program, named ASIC UltraShuttle. BaySand stated that their shuttle program enables customer to tapeout from RTL and BaySand will deliver 100 units of tested, packaged chips within 8 weeks.\n\nMany silicon fabrication facilities offer MPW runs or a company can produce its own MPW, e.g. combine several of its own designs to form one wafer completely owned by the company. In the latter case, it may be profitable to use most of the wafer for production chips and a small portion for producing prototypes of next generation chips.\n\n"}
{"id": "597340", "url": "https://en.wikipedia.org/wiki?curid=597340", "title": "Nancy J. Currie-Gregg", "text": "Nancy J. Currie-Gregg\n\nNancy Jane Currie-Gregg (former married name Sherlock, born December 29, 1958) is an engineer, United States Army officer and a NASA astronaut. Currie-Gregg has served in the United States Army for over 22 years and holds the rank of colonel. With NASA, she has participated in four space shuttle missions: STS-57, STS-70, STS-88, and STS109, accruing 1,000 hours in space. She currently holds an appointment as an adjunct associate professor in the Department of Industrial & Systems Engineering at Texas A&M University.\n\nCurrie-Gregg, was born Nancy Jane Decker in Wilmington, Delaware, but considers Troy, Ohio to be her hometown. She graduated from Troy High School in Troy, Ohio, in 1977, then received a Bachelor of Arts degree, with honors, in biological science from Ohio State University in 1980, a Master of Science degree in safety engineering from the University of Southern California in 1985, and a Doctorate in industrial engineering from the University of Houston in 1997.\n\nCurrie-Gregg is a member of Army Aviation Association of America, Phi Kappa Phi, Ohio State University and ROTC Alumni Associations, Institute of Industrial Engineers, and Human Factors and Ergonomics Society.\n\nCurrie-Gregg has served in the United States Army for over 22 years. Prior to her assignment at NASA in 1987, she attended initial rotary-wing pilot training and was subsequently assigned as an instructor pilot at the U.S. Army Aviation School. She has served in a variety of leadership positions including section leader, platoon leader, and brigade flight-standardization officer. As a Master Army Aviator she has logged over 3,900 flying hours in a variety of rotary-wing and fixed-wing aircraft.\n\nCurrie-Gregg was assigned to NASA Johnson Space Center in September 1987 as a flight simulation engineer on the Shuttle Training Aircraft, a complex airborne simulator which models flight characteristics of the Shuttle orbiter. An astronaut since 1990, she has been involved in robotic hardware and procedure development for the shuttle and space station and has worked as a spacecraft communicator. Dr. Currie-Gregg has also served as the chief of the Astronaut Office Robotics and Payloads-Habitability branches and the Habitability and Human Factors Office in JSC’s Space and Life Sciences Directorate. She has assisted the Johnson Space Center’s Automation, Robotics, and Simulation Division in the development of advanced robotics systems and is a consultant to NASA’s Space Human Factors Engineering Project. A veteran of four Space Shuttle missions, she has accrued 1,000 hours in space. She flew as mission specialist – flight engineer, on STS-57 (1993), STS-70 (1995), STS-88 (1998; the first International Space Station assembly mission), and STS-109 (2002).\n\nIn September 2003, Currie-Gregg was selected to lead the Space Shuttle Program’s Safety and Mission Assurance Office. As of 2006, she serves as the Senior Technical Advisor to the Automation, Robotics, and Simulation Division in the JSC Engineering Directorate.\n\nSTS-57 \"Endeavour\" (June 21 to July 1, 1993). The primary objective of this mission was the retrieval of the European Retrievable Carrier satellite (EURECA). Additionally, this mission featured the first flight of Spacehab, a commercially provided middeck augmentation module for the conduct of microgravity experiments, as well as a spacewalk by two crewmembers, during which Dr. Currie-Gregg operated the Shuttle’s robotic arm. Spacehab carried 22 individual flight experiments in materials and life sciences research. STS-57 orbited the Earth 155 times and covered over 4.1 million miles in over 239 hours and 45 minutes.\n\nSTS-70 \"Discovery\" (July 13–22, 1995). The five-member crew deployed the final NASA Tracking and Data Relay Satellite to complete the constellation of NASA’s orbiting communication satellite system. Dr. Currie-Gregg also conducted a myriad of biomedical and remote sensing experiments. STS-70 orbited the Earth 143 times and covered over 3.7 million miles in over 214 hours and 20 minutes.\n\nSTS-88 \"Endeavour\" (December 4–15, 1998). STS-88, ISS Flight 2A was the first International Space Station assembly mission. The primary objective of this 12-day mission was to mate the first American-made module, Unity, to the first Russian-made module, Zarya. Dr. Currie-Gregg's primary role was to operate the Shuttle's 50-foot robotic arm to retrieve Zarya and connect the first two station segments. Two crewmembers performed a series of three space walks to connect electrical umbilicals and to attach hardware to the exterior structure for use during future EVAs. Dr. Currie-Gregg also operated the robot arm during the space walks. During the mission the STS-88 crew ingressed the International Space Station to complete systems activation and installation of communication's equipment. The crew also deployed two small satellites. STS-88 completed 185 orbits of the Earth and covered over 4.7 million miles in 283 hours and 18 minutes.\n\nSTS-109 \"Columbia\" (March 1–12, 2002). STS-109 was the fourth mission to service the Hubble Space Telescope. During the flight, Dr. Currie-Gregg’s primary role was to operate the Shuttle’s 50-foot robot arm to retrieve and redeploy the telescope following the completion of numerous upgrades and repairs. She also operated the robot arm during a series of five consecutive spacewalks performed by four crewmembers. Hubble’s scientific capabilities and power system were significantly upgraded with the replacement of both solar arrays and the primary power control unit, the installation of the Advanced Camera for Surveys, and a scientific instrument cooling system. The Hubble Space Telescope was then boosted to a higher orbit and redeployed to continue its mission of providing views of the universe which are unmatched by ground-based telescopes or other satellites. STS-109 completed 165 earth orbits and covered over 3.9 million miles in over 262 hours.\n\nCurrie-Gregg served as principal engineer in NASA's engineering safety center. She currently holds an appointment as an Adjunct Associate Professor in the Department of Industrial Engineering at North Carolina State University.\n\n\n"}
{"id": "6117906", "url": "https://en.wikipedia.org/wiki?curid=6117906", "title": "Non-timber forest product", "text": "Non-timber forest product\n\nNon-timber forest products (NTFPs), also known as non-wood forest products (NWFPs), minor forest produce, special, minor, alternative and secondary forest products, are useful substances, materials and/or commodities obtained from forests which do not require harvesting (logging) trees. They include game animals, fur-bearers, nuts, seeds, berries, mushrooms, oils, foliage, pollarding, medicinal plants, peat, mast, fuelwood, fish, spices, and forage.\n\nResearch on NTFPs has focused on their ability to be produced as commodities for rural incomes and markets, as an expression of traditional knowledge or as a livelihood option for rural household needs, and as a key component of sustainable forest management and conservation strategies. All research promotes forest products as valuable commodities and tools that can promote the conservation of forests.\n\nThe wide variety of NTFPs includes mushrooms, huckleberries, ferns, transplants, seed cones, piñon seeds, tree nuts, moss, maple syrup, cork, cinnamon, rubber, wild pigs, tree oils and resins, and ginseng. The United Kingdom's Forestry Commission defines NTFPs as \"any biological resources found in woodlands except timber\", and Forest Harvest, part of the Reforesting Scotland project, defines them as \"materials supplied by woodlands - except the conventional harvest of timber\". These definitions include wild and managed game, fish, and insects. NTFPs are commonly grouped into categories such as floral greens, decoratives, medicinal plants, foods, flavors and fragrances, fibers, and saps and resins.\n\nOther terms similar to NTFPs include special, non-wood, minor, alternative, and secondary forest products. NTFPs in particular highlight forest products which are of value to local people and communities, but have been overlooked in the wake of forest management priorities (for example, timber production and animal forage). In recent decades, interest has grown in using NTFPs as alternatives or supplements to forest management practices. In some forest types, under the right political and social conditions, forests can be managed to increase NTFP diversity, and consequently, to increase biodiversity and potentially economic diversity. Black truffle cultivation in the Mediterranean area is a good example of a high profitability when well managed.\n\nThe harvest of NTFPs remains widespread throughout the world. People from a wide range of socioeconomic, geographical, and cultural contexts harvest NTFPs for a number of purposes, including household subsistence, maintenance of cultural and familial traditions, spiritual fulfillment, physical and emotional well-being, house heating and cooking, animal feeding, indigenous medicine and healing, scientific learning, and income. Other terms synonymous with harvesting include wild-crafting, gathering, collecting, and foraging. NTFPs also serve as raw materials for industries ranging from large-scale floral greens suppliers and pharmaceutical companies to microenterprises centered upon a wide variety of activities (such as basket-making, woodcarving, and the harvest and processing of various medicinal plants).\n\nEstimate the contribution of NTFPs to national or regional economies is difficult, broad-based systems for tracking the combined value of the hundreds of products that make up various NTFP industries are lacking. One exception to this is the maple syrup industry, which in 2002 in the US alone yielded worth US$D38.3 million. In temperate forests such as in the US, wild edible mushrooms such as matsutake, medicinal plants such as ginseng, and floral greens such as salal and sword fern are multimillion-dollar industries. While these high-value species may attract the most attention, a diversity of NTFPs can be found in most forests of the world.\n\nIn tropical forests, for example, NTFPs can be an important source of income that can supplement farming and/or other activities. A value analysis of the Amazon rainforest in Peru found that exploitation of NTFPs could yield higher net revenue per hectare than would timber harvest of the same area, while still conserving vital ecological services. Their economic, cultural, and ecological values, when considered in aggregate, make managing NTFPs an important component of sustainable forest management and the conservation of biological and cultural diversity.\n\n\n\nMinority people in Vietnam, Myanmar, and Laos are living away from the mainstream settlements. The hill tribes and many other minority groups are closely associated with forests for centuries. Much of their household subsistence and part of the income is generated from the sale of a variety of NTFP products. In the highlands of Vietnam, NTFPs production is spread almost throughout the year, so provides a sustained income for the ethnic minority people. From June to August is the wild berry called \"uoi\" (\"Scaphium macropodium\") collection that provides the bulk of household income. Every family sends several people into the forest on a regular basis during this period where they stay for 2–3 days during which 5–6 kg of berries are collected. A kilogram of dried berries (2–3 days of sun-dry) is sold for $1.50. The next comes bamboo shoots, mushrooms, and vegetable collection that goes through to February. The minority people in Sa Pa area depends mainly on a variety of NTFPs for their livelihoods. Among the products collected are fruits, berries, leaves, mushrooms, fish, bees honey, bamboo shoots, wild orchids and the list goes on. The Friday market is full of orchids and other wild plants put forward by these people for the tourists, both domestic and international, that flock there. Between 10-15% of the total household income is derived from the sale of NTFPs. The harvesting of leaves in the diet of family goes round the year where different species are readily available in specific months. Water from forest areas is yet another service that is useful in the livelihoods of these people. They have micro-hydro plants installed in streams that generate the much needed power for pounding (grain and seeds) and lighting too.\n\nIn the drier areas of Sri Lanka, the harvesting of curry leaves to be sold to traders is an important income. The harvesting of velvet tamarind (\"Dialium ovoideum\") is an important income source to the rural people. This tree which is endemic to the country provides a fruit that has a high-popularity during certain months of the year. The returns from the sale of these two products is an important addition to the household incomes of rural people.\n\nResearch on NTFPs has focused on three perspectives: NTFPs as a commodity with a focus on rural incomes and markets, as an expression of traditional knowledge or as a livelihood option for rural household needs, and finally, as a key component of sustainable forest management and conservation strategies. These perspectives promote forest products as valuable commodities and important tools that can promote the conservation of forests. In some contexts, the gathering and use of NTFPs can be a mechanism for poverty alleviation and local development.\n\n\n\n"}
{"id": "52507708", "url": "https://en.wikipedia.org/wiki?curid=52507708", "title": "Philips Innovation Award", "text": "Philips Innovation Award\n\nThe Philips Innovation Award is an international student entrepreneur award that is organised annually. The award is presented for students (or recently graduated students) who have transformed an innovative idea into a startup. This award may be presented to an individual or team. Participants of the program receive coaching from diverse partners.\n\nRecipients of the Philips Innovation award receive prizes worth of €50,000. Since 2016 a second catagory award is initiated called the Rough Diamond Award\n\n"}
{"id": "44839049", "url": "https://en.wikipedia.org/wiki?curid=44839049", "title": "Rujno Monastery printing house", "text": "Rujno Monastery printing house\n\nThe Rujno Monastery printing house () was a printing house established in 1537 in the Monastery of Saint George (Rujno Monastery) in village Vrutci of Rujno Župa near Užice, Ottoman Empire (modern day Serbia). The monastery had substantial income from its nearby spa so it established the printing house. Still, this income was insufficient for metal types. That is why hieromonk Teodosije, hegumen of the monastery, and his fellow monks, made types of 250 engraved wooden plates. They printed one book in this printing house, \"The Rujan Four Gospels\" (). The Ottomans burned monastery together with its printing house to prevent further printing of books. Since 1984 the remnants of this monastery are below Lake Vrutci.\n\nThe Rujno Monastery printing house was one of the oldest printing houses in the Balkans and the oldest printing house on the territory of modern day Serbia (then Ottoman Empire). It was established and operational in a small monastery of Saint George in village Vrutci of Rujno Župa near Užice in 1537. The monastery is also known as Rujno Monastery. Only one book was printed in Rujno Monastery printing house, \"The Rujan Four Gospels\". It was printed by hieromonk Teodosije who used wooden types to print it. Dejan Medaković concluded that Rujno Monastery was poor so its hieromonk and printer Teodosije had to use wooden types. Some other authors believe that substantial income of monastery received from its spa actually contributed to establishment of the printing house. Medaković emphasize that this was not an advancement, but step back to older woodcut technique. For many months Teodosije carefully engraved 250 wooden plates to be used for printing. Because of the different shape of some letters it was concluded that he did not engrave all letters by himself. One book was in possession of Vuk Karadžić.\n\nWhen Ottomans received information about the existence of this printing house they burned monastery while its monks fled to Rača monastery. The remnants of the monastery were used to build madrasa in Užice. In one letter written by Vuk Karadžić in 1857 he described wide red columns of the monastery he saw in madrasa.\n\nThe books printed in the printing house of Rujno Monastery had great influence to other nearby monasteries where they were manually transcribed. Today the exact location of the original monastery is unknown. The region where original Rujno Monastery was built is today below the surface of lake Vrutci built in 1984 for supply of Užice with potable water. New Rujno Monastery on another location was built in period 2004—2009.\n\n\n"}
{"id": "7442513", "url": "https://en.wikipedia.org/wiki?curid=7442513", "title": "Samuel Cate Prescott Award", "text": "Samuel Cate Prescott Award\n\nThe Samuel Cate Prescott Award has been awarded since 1964 by the Institute of Food Technologists (IFT) in Chicago, Illinois. It is awarded to food science or technology researchers who are under 36 years of age or who earned their highest degree within ten years before July 1 of the year the award is presented. This award is named for Samuel Cate Prescott (1872-1962), a food science professor from the Massachusetts Institute of Technology who was also the first president of IFT.\n\nAward winners receive a plaque from IFT and a USD 3,000 honorarium.\n\n"}
{"id": "36157098", "url": "https://en.wikipedia.org/wiki?curid=36157098", "title": "Secretary of Science and Technology (Philippines)", "text": "Secretary of Science and Technology (Philippines)\n\nThe Secretary of Science and Technology (Filipino: \"Kalihim ng Agham at Teknolohiya\") is the head of the Department of Science and Technology and is a member of the President’s Cabinet.\n\nThe position was created in 1987 by the then president Corazon Aquino, and was first assumed by Antonio Arizabal.\n\nThe current Secretary is Fortunato de la Peña, who assumed office on June 30, 2016.\n\n\"(*) Acting Capacity\"\n"}
{"id": "7892070", "url": "https://en.wikipedia.org/wiki?curid=7892070", "title": "Simplus", "text": "Simplus\n\nSimplus is a Platinum Salesforce Consulting Partner and Oracle Gold Partner with headquarters located in Salt Lake City, Utah. Simplus provides Advisory, Change Management, Implementation, Custom Configuration, and Managed Services. As the largest Quote-to-Cash implementation partner, Simplus serves more than 4000 customers across the globe. \n\nSimplus was founded by current CEO Ryan Westwood. After launching his first startup, PcCareSupport, in 2010, Westwood then launched Outbox Systems in 2014. It changed names and became Simplus in 2015. It has since had several high-level acquisitions, awards, and appointments, including Gilles Muys as Vice President of Customer Solutions, Amy Cook as Vice President of Marketing, and Suman Konidana as Managing Director. Simplus raised $7.3 million in a Series A funding round and $9.3 million in a Series B funding round. EPIC Ventures and Salesforce Ventures participated in both rounds. The company has been recognized by Utah Business for its culture and welcoming of new employees gained through acquisitions.\n\n\n\n"}
{"id": "39103801", "url": "https://en.wikipedia.org/wiki?curid=39103801", "title": "Slotted line", "text": "Slotted line\n\nSlotted lines are used for microwave measurements and consist of a movable probe inserted into a slot in a transmission line. They are used in conjunction with a microwave power source and usually, in keeping with their low-cost application, a low cost Schottky diode detector and VSWR meter rather than an expensive microwave power meter.\n\nSlotted lines can measure standing waves, wavelength, and, with some calculation or plotting on Smith charts, a number of other parameters including reflection coefficient and electrical impedance. A precision variable attenuator is often incorporated in the test setup to improve accuracy. This is used to make level measurements, while the detector and VSWR meter are retained only to mark a reference point for the attenuator to be set to, thus eliminating entirely the detector and meter measurement errors. The parameter most commonly measured by a slotted line is SWR. This serves as a measure of the accuracy of the impedance match to the item under test. This is especially important for transmitting antennas and their feed lines; high standing wave ratio on a radio or TV antenna can distort the signal, increase transmission line loss and potentially damage components in the transmission path, possibly even the transmitter. \n\nSlotted lines are no longer widely used, but can still be found in budget applications. Their main drawback is that they are labour-intensive to use and require calculation, tables, or plotting to make use of the results. They need to be made with mechanical precision and the probe and its detector need to be adjusted with care, but they can give very accurate results.\n\nThe slotted line is one of the basic instruments used in radio frequency test and measurement at microwave frequencies. It consists of a precision transmission line, usually co-axial but waveguide implementations are also used, with a movable insulated probe inserted into a longitudinal slot cut into the line. In a co-axial slotted line, the slot is cut into the outer conductor of the line. The probe is inserted past the outer conductor, but not so far that it touches the inner conductor. In a rectangular waveguide, the slot is usually cut along the centre of the broad wall of the waveguide. Circular waveguide slotted lines are also possible.\n\nSlotted lines are relatively cheap and can perform many of the measurements done by more expensive equipment such as network analysers. However, slotted line measurement techniques are more labour-intensive and often do not directly output the desired parameter; some calculation or plotting is frequently required. In particular, they can only carry out a measurement at one spot frequency at a time so producing a plot of a parameter versus frequency is very time consuming. This is to be compared to modern instruments like network and spectrum analysers which are intrinsically frequency swept and produce a plot instantly. Slotted lines have now largely been superseded, but are still found where capital costs are an issue. Their remaining uses are mostly in the millimetre band, where modern test apparatus is either prohibitively expensive or not available at all, and with academic laboratories and hobbyists. They are also useful as a teaching aid as the user is more directly exposed to basic line phenomena than with more sophisticated instruments.\n\nThe slotted line works by sampling the electric field inside the transmission line with the probe. For accuracy, it is important that the probe disturbs the field as little as possible. For this reason the probe diameter and slot width are kept small (usually around ) and the probe is inserted in no further than necessary. It is also necessary in waveguide slotted lines to place the slot at a position where the current in the waveguide walls is parallel to the slot. The current will then not be disturbed by the presence of the slot as long as it is not too wide. For the dominant mode this is on the centre-line of the broad face of the waveguide, but for some other modes it may need to be off-centre. This is not an issue for the co-axial line because this operates in the TEM (transverse electromagnetic) mode and hence the current is everywhere parallel to the slot. The slot may be tapered at its ends to avoid discontinuities causing reflections.\n\nThe disturbance to the field inside the line caused by the insertion of the probe is minimised as far as possible. There are two parts to this disturbance. The first part is due to the power the probe has extracted from the line and manifests as a lumped equivalent circuit of a resistor. This is minimised by limiting the distance the probe is inserted into the line so that only enough power is extracted for the detector to operate effectively. The second part of the disturbance is due to energy stored in the field around the probe and manifests as a lumped equivalent of a capacitor. This capacitance can be cancelled out with an inductance of equal and opposite impedance. Lumped inductors are not practical at microwave frequencies; instead, an adjustable stub with an inductive equivalent circuit is used to \"tune out\" the probe capacitance. The result is an equivalent circuit of a high impedance in shunt across the line which has little effect on the transmitted power in the line. The probe is more sensitive as a result of this tuning and the distance it is inserted can be further limited as a result.\n\nA typical test setup with a waveguide slotted line is shown in figure 2. Referring to this figure, power from a test equipment source (not shown) enters the apparatus through the co-axial cable on the left and is converted to waveguide format by means of a launcher (1). This is followed by a section of waveguide (2) providing a transition to a smaller size of guide. An important component in the setup is the isolator (3) which prevents power being reflected back into the source. Depending on the test conditions, such reflections can be large and a high-power source may be damaged by the returning wave. The power entering the slotted line is controlled by a rotary variable attenuator (4). This is followed by the slotted line itself (5) above which is the probe mounted on a movable carriage. The carriage also carries the probe adjustements: (6) is the probe depth adjustment, (7) is a length of co-axial section with tuning adjustments, and (8) is a \"detector\", which uses either a point contact \"crystal rectifier\" or a Schottky barrier diode. The right-hand end of the slotted line is terminated in a matched load (9) which absorbs all the power exiting the end of the waveguide. The load can be replaced by the component or system that it is desired to test. It can also be replaced with a reference short-circuit (10) which is used to calibrate the slotted line. The carriage can be moved along the slotted line by means of a rotary knob (11) which simultaneously moves a vernier gauge (12) for accurate measurement of the probes position along the line.\n\nThe probe is connected to a detector and a display meter (not shown in figure 2). These can be, respectively, a thermistor and power meter, or an envelope detector and VSWR meter. The detector can be a crystal detector or a Schottky barrier diode. The detector is mounted on the probe assembly, usually a distance λ/4 from the probe tip as shown in figure 3. This is because the detector looks almost like a short circuit to the transmission line, and this distance will convert it to an open circuit through the quarter-wave impedance transformer effect. Thus, the detector has minimal effect on loading the line. The probe tuning stub can be seen on figure 3 branching from the line linking the probe to the detector. Figure 2 has a slightly different arrangement; the main probe into the waveguide leads to a vertical co-axial tuning and adjustment section but the detector is on a horizontal side-section with a secondary probe into the upright co-axial section.\n\nMeasurements of microwave power can be made directly, usually with a thermistor based detector and meter. However, these instruments are expensive and a common meter used in measurements with a slotted line is instead a cheaper low-frequency VSWR meter. The microwave power source is amplitude modulated with, typically, a signal which is recovered by the envelope detector in the probe and sent to the VSWR meter. This scheme is preferred to simply detecting the unmodulated carrier directly, which would result in a DC output, because a stable, narrowband, tuned amplifier can be used to amplify the signal. A large amplification is required in the VSWR meter because the limit of the square law range of the detector diode is no more than .\n\nWhen the slotted line is terminated with a precision matching load there is no variation in the detected power along the line, other than a very small decrease due to losses in the line. However, when this is replaced by a device under test (DUT) which is not perfectly matched to the line there will be a reflection back towards the source. This causes a standing wave to be set up on the line with periodic maxima and minima (collectively, \"extrema\") due to alternating constructive and destructive interference. These extrema are found by moving the probe back and forth along the line and the level at that point can then be measured on the meter.\n\nThe extrema are not of any great interest in themselves, but are used in the calculation of several more useful parameters. Some of these parameters require the measurement of the exact position of the extremum. Either maxima or minima can equally be used, from a mathematical point of view, but minima are preferred because they are always much sharper than maxima, especially for large reflections, as shown in figure 4. Additionally, the probe causes less disturbance to the field near a minimum than it does near a maximum.\n\nWavelength is determined by measuring the distance between two adjacent minima. This distance will be λ/2. There is no need for a DUT, better results are obtained with the reference short in position.\n\nStanding wave ratio (SWR or VSWR) is a basic parameter and the one most commonly measured on a slotted line. This quantity is of particular importance for transmitter antennae. A high SWR indicates a poor match between the feed line and the antenna, which increases wasted power, can cause damage to components in the transmission path, possibly including the transmitter, and cause distortion to TV, FM stereo and digital signals. With the input power set so that the maxima are at 0 dBm, a measurement of a minimum in decibels will directly give SWR (after discarding the minus sign).\n\nThe reflection coefficient, ρ, is the ratio of the reflected wave to the incident wave. In general it is a complex number. The magnitude of the reflection coeffient can be calculated from the VSWR measurement by,\n\nwhere VSWR is the standing wave ratio expressed as a voltage ratio (not in decibels). However, to completely characterise the reflection coefficient, the phase of ρ must also be found. This is done on a slotted line by measuring the distance of the first minimum from the DUT. Moving the probe right up to the DUT is not practicable so a different approach is usually adopted. The position of the first minimum when the reference short is in place is noted. The distance back along the line from this reference point to the next minimum when the DUT is in place will be the same as the distance from the DUT to the first minimum. This is so because the reference short guarantees a minimum at the DUT position.\n\nThe phase part of ρ is given by,\n\nwhere λ is the wavelength and \"x\" is the distance to the first minimum as described earlier. The magnitude and phase representation of ρ can, if required, be expressed as real and imaginary parts instead by the usual manipulation of complex numbers.\n\nThe impedance, \"Z\", of the DUT can be calculated from the reflection coefficient by,\n\nwhere \"Z\" is the characteristic impedance of the line. An alternative method is to plot the VSWR and distance to the node (in wavelengths) on a Smith chart. These quantities are directly measured by the slotted line. From this plot the DUT impedance (normalised to \"Z\") can be read directly off the Smith chart.\n\nGood slotted lines are precision made instruments. They need to be because mechanical defects can affect accuracy. Some of the mechanical issues that are relevant to this include backlash of the vernier, concentricity of the inner and outer conductor, circularity of the outer conductor, centrality and straightness of the inner conductor, variations in cross-section, and the ability of the carriage to maintain a constant probe depth. Issues with probe tuning and disturbances to the field have already been discussed, but the insulated spacers holding the centre conductor in place can also disturb the field. Consequently, these are made as discrete as is compatible with mechanical strength. However, the greatest source of inaccuracy is usually not the slotted line itself, but the characteristics of the detector diode.\n\nThe detected voltage signal output of the Schottky barrier diodes typically used in microwave detectors have a square law relationship to the power being measured and meters are calibrated accordingly. However, as the power increases, the diode deviates significantly from a square law and remains accurate up to an output voltage of only around . This can be improved a little by adding a load resistor to the detector output, but this also has the undesirable effect of decreasing sensitivity. Another technique is to reduce the range of power being measured (so that it is brought within the square law range of the detector) by measuring at a point other than a maximum. The maximum is then calculated from the known mathematical shape of the standing wave pattern. This has the objection that it adds significantly to the labour required to make the measurements, as does the technique of precisely calibrating the detector and adjusting the readings on the meter according to a calibration chart.\n\nIt is possible to completely eliminate errors in the detector and meter if a precision variable attenuator is used in the test setup. In this technique a minimum is first found and the attenuator adjusted so that the meter is indicating precisely some convenient mark. A maximum is then found and the attenuation increased until the meter is indicating the same mark. The amount the attenuation had to be increased is the VSWR of the standing wave. Accuracy here depends on the accuracy of the attenuator and not at all on the detector.\n\n"}
{"id": "11982591", "url": "https://en.wikipedia.org/wiki?curid=11982591", "title": "Smart cut", "text": "Smart cut\n\nSmart cut is a technological process that enables the transfer of very fine layers of crystalline silicon material onto a mechanical support. It was invented by Michel Bruel of , and is protected by US patent 5374564. The application of this technological procedure is mainly in the production of silicon-on-insulator (SOI) wafer substrates.\n\nThe role of SOI is to electronically insulate a fine layer of monocrystalline silicon from the rest of the silicon wafer; an ultra-thin silicon film is transferred to a mechanical support, thereby introducing an intermediate, insulating layer. Semiconductor manufacturers can then fabricate integrated circuits on the top layer of the SOI wafers using the same processes they would use on plain silicon wafers. \n\nThe sequence of illustrations pictorially describes the process involved in fabricating SOI wafers using the smart cut technology.\n\n"}
{"id": "1049636", "url": "https://en.wikipedia.org/wiki?curid=1049636", "title": "Solid oxide fuel cell", "text": "Solid oxide fuel cell\n\nA solid oxide fuel cell (or SOFC) is an electrochemical conversion device that produces electricity directly from oxidizing a fuel. Fuel cells are characterized by their electrolyte material; the SOFC has a solid oxide or ceramic electrolyte. Advantages of this class of fuel cells include high efficiency, long-term stability, fuel flexibility, low emissions, and relatively low cost. The largest disadvantage is the high operating temperature which results in longer start-up times and mechanical and chemical compatibility issues.\n\nSolid oxide fuel cells are a class of fuel cells characterized by the use of a solid oxide material as the electrolyte. SOFCs use a solid oxide electrolyte to conduct negative oxygen ions from the cathode to the anode. The electrochemical oxidation of the oxygen ions with hydrogen or carbon monoxide thus occurs on the anode side. More recently, proton-conducting SOFCs (PC-SOFC) are being developed which transport protons instead of oxygen ions through the electrolyte with the advantage of being able to be run at lower temperatures than traditional SOFCs.\n\nThey operate at very high temperatures, typically between 500 and 1,000 °C. At these temperatures, SOFCs do not require expensive platinum catalyst material, as is currently necessary for lower temperature fuel cells such as PEMFCs, and are not vulnerable to carbon monoxide catalyst poisoning. However, vulnerability to sulfur poisoning has been widely observed and the sulfur must be removed before entering the cell through the use of adsorbent beds or other means.\n\nSolid oxide fuel cells have a wide variety of applications, from use as auxiliary power units in vehicles to stationary power generation with outputs from 100 W to 2 MW. In 2009, Australian company, Ceramic Fuel Cells successfully achieved an efficiency of an SOFC device up to the previously theoretical mark of 60%. The higher operating temperature make SOFCs suitable candidates for application with heat engine energy recovery devices or combined heat and power, which further increases overall fuel efficiency.\n\nBecause of these high temperatures, light hydrocarbon fuels, such as methane, propane, and butane can be internally reformed within the anode. SOFCs can also be fueled by externally reforming heavier hydrocarbons, such as gasoline, diesel, jet fuel (JP-8) or biofuels. Such reformates are mixtures of hydrogen, carbon monoxide, carbon dioxide, steam and methane, formed by reacting the hydrocarbon fuels with air or steam in a device upstream of the SOFC anode. SOFC power systems can increase efficiency by using the heat given off by the exothermic electrochemical oxidation within the fuel cell for endothermic steam reforming process. Additionally, solid fuels such as coal and biomass may be gasified to form syngas which is suitable for fueling SOFCs in integrated gasification fuel cell power cycles.\n\nThermal expansion demands a uniform and well-regulated heating process at startup. SOFC stacks with planar geometry require on the order of an hour to be heated to light-off temperature. Micro-tubular fuel cell design geometries promise much faster start up times, typically in the order of minutes.\n\nUnlike most other types of fuel cells, SOFCs can have multiple geometries. The planar fuel cell design geometry is the typical sandwich type geometry employed by most types of fuel cells, where the electrolyte is sandwiched in between the electrodes. SOFCs can also be made in tubular geometries where either air or fuel is passed through the inside of the tube and the other gas is passed along the outside of the tube. The tubular design is advantageous because it is much easier to seal air from the fuel. The performance of the planar design is currently better than the performance of the tubular design, however, because the planar design has a lower resistance comparatively. Other geometries of SOFCs include modified planar fuel cell designs (MPC or MPSOFC), where a wave-like structure replaces the traditional flat configuration of the planar cell. Such designs are highly promising because they share the advantages of both planar cells (low resistance) and tubular cells.\n\nA solid oxide fuel cell is made up of four layers, three of which are ceramics (hence the name). A single cell consisting of these four layers stacked together is typically only a few millimeters thick. Hundreds of these cells are then connected in series to form what most people refer to as an \"SOFC stack\". The ceramics used in SOFCs do not become electrically and ionically active until they reach very high temperature and as a consequence, the stacks have to run at temperatures ranging from 500 to 1,000 °C. Reduction of oxygen into oxygen ions occurs at the cathode. These ions can then diffuse through the solid oxide electrolyte to the anode where they can electrochemically oxidize the fuel. In this reaction, a water byproduct is given off as well as two electrons. These electrons then flow through an external circuit where they can do work. The cycle then repeats as those electrons enter the cathode material again.\n\nMost of the downtime of a SOFC stems from the mechanical balance of plant, the air preheater, prereformer, afterburner, water heat exchanger, anode tail gas oxidizer, and electrical balance of plant, power electronics, hydrogen sulfide sensor and fans. Internal reforming leads to a large decrease in the balance of plant costs in designing a full system.\n\nThe ceramic anode layer must be very porous to allow the fuel to flow towards the electrolyte. Consequently, granular matter is often selected for anode fabrication procedures. Like the cathode, it must conduct electrons, with ionic conductivity a definite asset. The most common material used is a cermet made up of nickel mixed with the ceramic material that is used for the electrolyte in that particular cell, typically YSZ (yttria stabilized zirconia) nanomaterial-based catalysts, this YSZ part helps stop the grain growth of nickel. Larger grains of nickel would reduce the contact area that ions can be conducted through, which would lower the cells efficiency. The anode is commonly the thickest and strongest layer in each individual cell, because it has the smallest polarization losses, and is often the layer that provides the mechanical support. Electrochemically speaking, the anode’s job is to use the oxygen ions that diffuse through the electrolyte to oxidize the hydrogen fuel.\nThe oxidation reaction between the oxygen ions and the hydrogen produces heat as well as water and electricity.\nIf the fuel is a light hydrocarbon, for example, methane, another function of the anode is to act as a catalyst for steam reforming the fuel into hydrogen. This provides another operational benefit to the fuel cell stack because the reforming reaction is endothermic, which cools the stack internally. Perovskite materials (mixed ionic/electronic conducting ceramics) have been shown to produce a power density of 0.6 W/cm2 at 0.7 V at 800 °C which is possible because they have the ability to overcome a larger activation energy.\n\nThe electrolyte is a dense layer of ceramic that conducts oxygen ions. Its electronic conductivity must be kept as low as possible to prevent losses from leakage currents. The high operating temperatures of SOFCs allow the kinetics of oxygen ion transport to be sufficient for good performance. However, as the operating temperature approaches the lower limit for SOFCs at around the electrolyte begins to have large ionic transport resistances and affect the performance. Popular electrolyte materials include yttria-stabilized zirconia (YSZ) (often the 8% form 8YSZ), scandia stabilized zirconia (ScSZ) (usually 9 mol%Sc2O3 – 9ScSZ) and gadolinium doped ceria (GDC). The electrolyte material has crucial influence on the cell performances. Detrimental reactions between YSZ electrolytes and modern cathodes such as lanthanum strontium cobalt ferrite (LSCF) have been found, and can be prevented by thin (<100 nm) ceria diffusion barriers.\n\nIf the conductivity for oxygen ions in SOFC can remain high even at lower temperatures (current target in research ~500 °C), material choices for SOFC will broaden and many existing problems can potentially be solved. Certain processing techniques such as thin film deposition can help solve this problem with existing materials by:\n\n\nThe cathode, or air electrode, is a thin porous layer on the electrolyte where oxygen reduction takes place. The overall reaction is written in Kröger-Vink Notation as follows:\n\nCathode materials must be, at a minimum, electronically conductive. Currently, lanthanum strontium manganite (LSM) is the cathode material of choice for commercial use because of its compatibility with doped zirconia electrolytes. Mechanically, it has a similar coefficient of thermal expansion to YSZ and thus limits stress buildup because of CTE mismatch. Also, LSM has low levels of chemical reactivity with YSZ which extends the lifetime of the materials. Unfortunately, LSM is a poor ionic conductor, and so the electrochemically active reaction is limited to the triple phase boundary (TPB) where the electrolyte, air and electrode meet. LSM works well as a cathode at high temperatures, but its performance quickly falls as the operating temperature is lowered below 800 °C. In order to increase the reaction zone beyond the TPB, a potential cathode material must be able to conduct both electrons and oxygen ions. Composite cathodes consisting of LSM YSZ have been used to increase this triple phase boundary length. Mixed ionic/electronic conducting (MIEC) ceramics, such as perovskite LSCF, are also being researched for use in intermediate temperature SOFCs as they are more active and can make up for the increase in the activation energy of the reaction.\n\nThe interconnect can be either a metallic or ceramic layer that sits between each individual cell. Its purpose is to connect each cell in series, so that the electricity each cell generates can be combined. Because the interconnect is exposed to both the oxidizing and reducing side of the cell at high temperatures, it must be extremely stable. For this reason, ceramics have been more successful in the long term than metals as interconnect materials. However, these ceramic interconnect materials are very expensive as compared to metals. Nickel- and steel-based alloys are becoming more promising as lower temperature (600–800 °C) SOFCs are developed. The material of choice for an interconnect in contact with Y8SZ is a metallic 95Cr-5Fe alloy. Ceramic-metal composites called 'cermet' are also under consideration, as they have demonstrated thermal stability at high temperatures and excellent electrical conductivity.\n\nPolarizations, or overpotentials, are losses in voltage due to imperfections in materials, microstructure, and design of the fuel cell. Polarizations result from ohmic resistance of oxygen ions conducting through the electrolyte (iRΩ), electrochemical activation barriers at the anode and cathode, and finally concentration polarizations due to inability of gases to diffuse at high rates through the porous anode and cathode (shown as ηA for the anode and ηC for cathode). The cell voltage can be calculated using the following equation:\n\nwhere:\n\nIn SOFCs, it is often important to focus on the ohmic and concentration polarizations since high operating temperatures experience little activation polarization. However, as the lower limit of SOFC operating temperature is approached (~600 °C), these polarizations do become important.\n\nAbove mentioned equation is used for determining the SOFC voltage (in fact for fuel cell voltage in general). This approach results in good agreement with particular experimental data (for which\nadequate factors were obtained) and poor agreement for other than original experimental working parameters. Moreover, most of the equations used require the addition of numerous factors which are difficult or impossible to determine. It makes very difficult any optimizing process of the SOFC working parameters as well as design architecture configuration selection. Because of those circumstances a few other equations were proposed:\n\nwhere: \n\nThis method was validated and found to be suitable for optimization and sensitivity studies in plant-level modelling of various systems with solid oxide fuel cells. With this mathematical description it is possible to account for different properties of the SOFC. There are many parameters which impact cell working conditions, e.g. electrolyte material, electrolyte thickness, cell temperature, inlet and outlet gas compositions at anode and cathode, and electrode porosity, just to name some. The flow in these systems is often calculated using the Navier-stokes equation.\n\nOhmic losses in an SOFC result from ionic conductivity through the electrolyte. This is inherently a materials property of the crystal structure and atoms involved. However, to maximize the ionic conductivity, several methods can be done. Firstly, operating at higher temperatures can significantly decrease these ohmic losses. Substitutional doping methods to further refine the crystal structure and control defect concentrations can also play a significant role in increasing the conductivity. Another way to decrease ohmic resistance is to decrease the thickness of the electrolyte layer.\n\nAn ionic specific resistance of the electrolyte as a function of temperature can be described by the following relationship:\n\nwhere: formula_15 – electrolyte thickness, and formula_16 – ionic conductivity.\n\nThe ionic conductivity of the solid oxide is defined as follows:\n\nwhere: formula_18 and formula_19 – factors depended on electrolyte materials, formula_20 – electrolyte temperature, and formula_4 – ideal gas constant.\n\nThe concentration polarization is the result of practical limitations on mass transport within the cell and represents the voltage loss due to spatial variations in reactant concentration at the chemically active sites. This situation can be caused when the reactants are consumed by the electrochemical reaction faster than they can diffuse into the porous electrode, and can also be caused by variation in bulk flow composition. The latter is due to the fact that the consumption of reacting species in the reactant flows causes a drop in reactant concentration as it travels along the cell, which causes a drop in the local potential near the tail end of the cell.\n\nThe concentration polarization occurs in both the anode and cathode. The anode can be particularly problematic, as the oxidation of the hydrogen produces steam, which further dilutes the fuel stream as it travels along the length of the cell. This polarization can be mitigated by reducing the reactant utilization fraction or increasing the electrode porosity, but these approaches each have significant design trade-offs.\n\nThe activation polarization is the result of the kinetics involved with the electrochemical reactions. Each reaction has a certain activation barrier that must be overcome in order to proceed and this barrier leads to the polarization. The activation barrier is the result of many complex electrochemical reaction steps where typically the rate limiting step is responsible for the polarization. The polarization equation shown below is found by solving the Butler–Volmer equation in the high current density regime (where the cell typically operates), and can be used to estimate the activation polarization:\n"}
{"id": "9490876", "url": "https://en.wikipedia.org/wiki?curid=9490876", "title": "Technics and Time, 1", "text": "Technics and Time, 1\n\nTechnics and Time, 1: The Fault of Epimetheus () is a book by the French philosopher Bernard Stiegler, first published by Galilée in 1994.\n\nThe English translation, by George Collins and Richard Beardsworth, was published by Stanford University Press in 1998. The \"Technics and Time\" series is the fullest systematic statement by Stiegler of his philosophy, and the first volume draws on the work of Martin Heidegger, André Leroi-Gourhan, Gilbert Simondon, Bertrand Gille, Jean-Jacques Rousseau, and Jean-Pierre Vernant in order to outline and develop Stiegler's major philosophical theses. The series currently consists of three books.\n\nStiegler argues that \"technics\" forms the horizon of human existence. This fact has been suppressed throughout the history of philosophy, which has never ceased to operate on the basis of a distinction between \"episteme\" and \"tekhne\". The thesis of the book is that the genesis of technics corresponds not only to the genesis of what is called \"human\" but of temporality as such, and that this is the clue toward understanding the future of the dynamic process in which the human and the technical consists.\n\n\nStiegler has thus far published three volumes in the \"Technics and Time\" series. \"The Fault of Epimetheus\" was followed by \"Tome 2: La désorientation\" (1996) and \"Tome 3: Le temps du cinéma et la question du mal-être\" (2001). Volume Two was published in translation by Stanford University Press in 2008 with the subtitle, \"Disorientation\", with Volume Three appearing in 2010 with the subtitle, \"Cinematic Time and the Question of Malaise\" (both volumes translated by Stephen Barker). Stiegler has at times mentioned his intention to publish further volumes in this series, but these are yet to appear.\n\n"}
{"id": "12192262", "url": "https://en.wikipedia.org/wiki?curid=12192262", "title": "Turing switch", "text": "Turing switch\n\nIn theoretical network science, the Turing switch is a logical construction modeling the operation of the network switch, just as in theoretical computer science a Turing machine models the operation of a computer. Both are named in honor of the English logician Alan Turing, although the research in Turing switches is not based on Turing's research. Some introductory research on the Turing switch was started at the University of Cambridge by Jon Crowcroft (Homepage).\n\nIn essence, Crowcroft suggests that instead of using general-purpose computers to do packet switching, the required operations should be reduced to application specific logic and then that application specific logic should be implemented using optical components. The work is not actually based on Turing's research.\n\nA Turing switch consists of a switched fabric, one or more ingress interfaces (also referred to as sources), one or more egress interfaces (sinks), and a decision procedure to determine an egress interface given an ingress interface. Interfaces are sometimes referred to as ports. A packet (cell or switched unit) arrives at an ingress interface, the appropriate egress interface is determined by the decision procedure, and the packet is then transported across the switching fabric to the egress interface. A packet is a symbol or sequence of 1's and 0's. An ingress interface is connected to an ingress line and an egress interface to an egress line. The ingress line is said to feed the ingress interface; the egress interface feeds the egress line.\n\n"}
{"id": "22719240", "url": "https://en.wikipedia.org/wiki?curid=22719240", "title": "Wave motor", "text": "Wave motor\n\nWave motors were machines designed and built in the late 19th and early 20th century to harness the power of wave or tidal energy. Many experiments were planned or built in California employing various methods. The earliest wave motors were not intended for the creation of electricity. Prior to 1880, wave motors were designed to operate non-electrically to power vehicles or mills.\n\nSources:\n"}
