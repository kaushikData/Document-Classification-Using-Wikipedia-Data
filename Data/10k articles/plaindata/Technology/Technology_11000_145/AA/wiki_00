{"id": "882893", "url": "https://en.wikipedia.org/wiki?curid=882893", "title": "4D Man", "text": "4D Man\n\n4D Man (also known as The Evil Force in the UK; reissued as Master of Terror in the US) is a 1959 independent American science fiction film in color by De Luxe, produced by Jack H. Harris (from his original screenplay), directed by Irvin S. Yeaworth Jr., and starring Robert Lansing, Lee Meriwether, and James Congdon. The film was released by Universal-International.\n\nBrilliant but irresponsible scientist, Dr. Tony Nelson, develops an electronic amplifier that he hopes will allow any object to achieve a 4th dimensional (4D) state. While in this state, any object can pass freely through any other object. Tony, however, fails to pay attention to the overload, which sparks an electrical fire that burns down his lab. This results in the university terminating his contract. Now unemployed, Tony seeks out his brother, Scott, also a Doctor, to help him with his experiment. Scott is a researcher working on a material called \"Cargonite\" that is so dense that it is impenetrable.\n\nScott is underpaid and unappreciated at his new job. He does not have the necessary drive to ask his employer, Mr. Carson, for greater recognition. Scott has become the driving force behind the development of Cargonite, named after Carson, who is now taking much of the credit for Scott's work. When his girlfriend, Linda Davis, falls for Tony, an enraged Scott steals Tony's experiment and starts playing around with it, eventually transforming himself into a 4D state. When demonstrating this to Tony, Scott leaves the amplifier power turned off, yet he successfully passes his hand through a block of steel. Scott can now enter a 4D state via his own will. Tony is amazed, but warns Scott not to reveal this ability until he can further test for possible side effects.\n\nWhile in the 4D state, Scott can pass through any solid object, but he ages at a greatly accelerated rate. The aged Scott soon learns how to survive, when he visits the company doctor, who, while examining him, suddenly drops dead. Simply by touching others, Scott can drain anyone's lifeforce, thereby rejuvenating himself. He experiments with his new abilities by shoplifting a piece of fruit \"through\" a grocery store's solid window. Scott also notices a diamond necklace on display in a nearby jewelry store window, but decides against stealing it. When he sees a bank, however, his face breaks into a sly grin.\n\nThe police wonder about a bizarre crime. More than $50,000 was stolen from the bank with no sign of forced entry, nor any video footage of the crime. Strangely, a $1000 bill was found \"protruding\" from a solid piece of tempered steel. Tony realizes that Scott is abusing his power and tries to convince the police.\n\nScott starts using his newly-found power to acquire all the things he felt he was denied: money, recognition, power, and women. Scott confronts Carson, revealing the experiment, then taking his revenge \"for the life drained from me\" by literally draining Carson's life force. Scott then proceeds to a sleazy bar, where he gets some street toughs to back down. With his new found bravado, combined with his ill-gotten money, he tries to impress a bar girl. When they later kiss, Scott does not kill her, but drains her life force to the point where she has aged badly.\n\nThe police have to find a way to stop a man who is unstoppable. Looking very old now, Scott returns to the lab, but they are unable to stop him. Scott's former girlfriend catches him in solid form and shoots and wounds him. Bleeding and feeling betrayed, Scott maniacally proclaims his invincibility and defiantly phase-shifts his body with difficulty through a wall embedded with supposedly impenetrable Cargonite. \"The End\" appears on screen, followed a moment later by a question mark. This interrogative statement leaves in question whether the aged Scott died or survived.\n\nJack H. Harris was able to begin production of the film with the money he had received from distributing \"The Blob\" (1958). It was the film debut of Lee Meriwether and Robert Lansing. Young Patty Duke also makes a small cameo appearance in the film.\n\nThe film was released in the United-States on October 7, 1959.\n\n"}
{"id": "38401073", "url": "https://en.wikipedia.org/wiki?curid=38401073", "title": "6066 aluminium alloy", "text": "6066 aluminium alloy\n\n6066 aluminium alloy is an aluminium alloy used in forgings and extrusion for welded structures.\n"}
{"id": "5175822", "url": "https://en.wikipedia.org/wiki?curid=5175822", "title": "American Frozen Food Institute", "text": "American Frozen Food Institute\n\nThe American Frozen Food Institute (AFFI) is the national trade association that promotes the interests of all segments of the frozen food industry. AFFI unifies a diverse and essential industry in the following ways:\n\nAFFI advocates the public policy interests of the industry before legislative and regulatory entities;\n\nAFFI serves as the voice of the industry before consumers, policymakers and the media and as a resource to its members;\n\nAFFI fosters industry development and growth through networking, educational, research and statistical programs;\n\nAFFI promotes increased consumption of frozen foods; and\n\nAFFI is a member-driven organization that operates in an efficient, effective and ethical manner.\n\nAFFI was founded in 1942 and is headquartered in McLean, Virginia.\n\nIts subsidiaries are:\nAlliance for Listeriosis Prevention\nFrozen Food Foundation\nNational Yogurt Association \n\n\n"}
{"id": "9740633", "url": "https://en.wikipedia.org/wiki?curid=9740633", "title": "Automated trading system", "text": "Automated trading system\n\nAn automated trading system (ATS), also referred to as algorithmic trading, is a computer program that creates orders and automatically submits them to a market center or exchange. Advances in computer and communications technology have led to electronic trading exchange system networks. Electronic trading exchange system networks use communications networks and computers to replicate traditional face-to-face exchange functions. The program will automatically generate orders based on predefined set of rules using a trading strategy which is often based on technical analysis but can also be based on input from other electronic sources.\n\nAutomated trading systems are often used with electronic trading in automated market centers, including electronic communication networks, \"dark pools\", and automated exchanges. Automated trading systems and electronic trading platforms can execute repetitive tasks at speeds with orders of magnitude greater than any human equivalent. Traditional risk controls and safeguards that relied on human judgment are not appropriate for automated trading and this has caused issues such as the 2010 Flash Crash. New controls such as trading curbs or 'circuit breakers' have been put in place in some electronic markets to deal with automated trading systems.\n\nThe automated trading system determines whether an order should be submitted based on, for example, the current market price of an option and theoretical buy and sell prices. The theoretical buy and sell prices are derived from, among other things, the current market price of the security underlying the option. A look-up table stores a range of theoretical buy and sell prices for a given range of current market price of the underlying security. Accordingly, as the price of the underlying security changes, a new theoretical price may be indexed in the look-up table, thereby avoiding calculations that would otherwise slow automated trading decisions. \nA distributed processing on-line automated trading system uses structured messages to represent each stage in the negotiation between a market maker (quoter) and a potential buyer or seller (requestor). \n\nAutomated trading systems minimize emotions throughout the trading process. By keeping emotions in check, traders typically have an easier time sticking to the plan. Since trade orders are executed automatically once the trade rules have been met, traders will not be able to hesitate or question the trade. In addition to helping traders who are afraid to \"pull the trigger,\" automated trading can curb those who are apt to overtrade – buying and selling at every perceived opportunity. \n\nBacktesting applies trading rules to historical market data to determine the viability of the idea. When designing a system for automated trading, all rules need to be absolute, with no room for interpretation (the computer cannot make guesses). Traders can take these precise sets of rules and test them on historical data before risking money in live trading. Careful backtesting allows traders to evaluate and fine-tune a trading idea, and to determine the system's expectancy – i.e., the average amount that a trader can expect to win per unit of risk. \n\nBecause the trade rules are established and trade execution is performed automatically, discipline is preserved even in volatile markets. Discipline is often lost due to emotional factors such as fear of taking a loss, or the desire to eke out a little more profit from a trade. Automated trading helps ensure that discipline is maintained because the trading plan will be followed exactly. In addition, \"Pilot error\" is minimized; for instance, an order to buy 100 shares will not be incorrectly entered as an order to sell 1,000 shares. \n\nOne of the biggest challenges in trading is to plan the trade and trade the plan. Even if a trading plan has the potential to be profitable, traders who ignore the rules are altering any expectancy the system would have had. There is no such thing as a trading plan that wins 100% of the time – losses are a part of the game. But losses can be psychologically traumatizing, so a trader who has two or three losing trades in a row might decide to skip the next trade. If this next trade would have been a winner, the trader has already destroyed any expectancy the system had. Automated trading systems allow traders to achieve consistency by trading the plan. \n\nSince computers respond immediately to changing market conditions, automated systems are able to generate orders as soon as trade criteria are met. Getting in or out of a trade a few seconds earlier can make a big difference in the trade's outcome. As soon as a position is entered, all other orders are automatically generated, including protective stop losses and profit targets. Markets can move quickly, and it is demoralizing to have a trade reach the profit target or blow past a stop-loss level – before the orders can even be entered. An automated trading system prevents this from happening. \n\nAutomated trading systems permit the user to trade multiple accounts or various strategies at one time. This has the potential to spread risk over various instruments while creating a hedge against losing positions. What would be incredibly challenging for a human to accomplish is efficiently executed by a computer in milliseconds. The computer is able to scan for trading opportunities across a range of markets, generate orders and monitor trades. \n\nSuch a system is subject to uncertainties caused by the fact that a variable time is required for an order (buy or sell) message to be transmitted from the requestor to the quoter, or for a cancel (quote interrupt) message to be transmitted from the quoter to the requestor. Furthermore, it is possible that an equipment failure in the network, either in a communication link or even at the workstation of one of the traders, will prevent a small fraction of such order messages and cancel messages from reaching their intended destination within the relatively short time-frame typically associated with an on-line transaction system. \n\nThe theory behind automated trading makes it seem simple: Set up the software, program the rules and watch it trade. In reality, however, automated trading is a sophisticated method of trading, yet not infallible. Depending on the trading platform, a trade order could reside on a computer and not a server. What that means is that if an internet connection is lost, an order might not be sent to the market. There could also be a discrepancy between the \"theoretical trades\" generated by the strategy and the order entry platform component that turns them into real trades. Most traders should expect a learning curve when using automated trading systems, and it is generally a good idea to start with small trade sizes while the process is refined. \n\nAlthough it would be great to turn on the computer and leave for the day, automated trading systems do require monitoring. This is due do the potential for mechanical failures, such as connectivity issues, power losses or computer crashes, and to system quirks. It is possible for an automated trading system to experience anomalies that could result in errant orders, missing orders, or duplicate orders. If the system is monitored, these events can be identified and resolved quickly. \n\nTraders who employ backtesting techniques can create systems that look great on paper and perform terribly in a live market. Over-optimization refers to excessive curve-fitting that produces a trading plan that is unreliable in live trading. It is possible, for example, to tweak a strategy to achieve exceptional results on the historical data on which it was tested. Traders sometimes incorrectly assume that a trading plan should have close to 100% profitable trades or should never experience a drawdown to be a viable plan. As such, parameters can be adjusted to create a \"near perfect\" plan that completely fails as soon as it is applied to a live market. \n\nThe most common algorithmic trading strategies follow trends in moving averages, channel breakouts, price level movements and related technical indicators. These are the easiest and simplest strategies to implement through algorithmic trading because these strategies do not involve making any predictions or price forecasts. Trades are initiated based on the occurrence of desirable trends, which are easy and straightforward to implement through algorithms without getting into the complexity of predictive analysis. \n\nVolume weighted average price strategy breaks up a large order and releases dynamically determined smaller chunks of the order to the market using stock-specific historical volume profiles. The aim is to execute the order close to the Volume Weighted Average Price (VWAP). It is common to evaluate the performance\nof traders by their ability to execute orders at prices better than the volume weighted average price (VWAP) over the trading horizon. \n\nwhere:\nMean reversion strategy is based on the idea that the high and low prices of an asset are a temporary phenomenon that revert to their mean value (average value) periodically. Identifying and defining a price range and implementing an algorithm based on that allows trades to be placed automatically when the price of asset breaks in and out of its defined range.\nEarly form of Automated Trading System has been used by financial managers and brokers, software based on algorithm. These kind of software were used to automatically manage clients' portfolios. But first service to free market without any supervision from financial advisers and managers to serve clients directly was given in 2008 with the launch of Betterment by Jon Stein. Since then this system is getting improved with development in IT industry, now Automated Trading System is managing huge assets all around the globe. , more than 75 percent of the stock shares traded on United States exchanges (including the New York Stock Exchange and NASDAQ) originate from automated trading system orders. ATSs can be based on a predefined set of rules which determine when to enter an order, when to exit a position and how much money to invest in each trading product. Trading strategies differ; some are designed to pick market tops and bottoms, others to follow a trend, and others involve complex strategies including randomizing orders to make them less visible in the marketplace. ATSs allow a trader to execute orders much quicker and manage their portfolio easily by automatically generating protective precautions.\n\nBacktesting of a trading system involves programmers running the program using historical market data in order to determine whether the underlying algorithm guiding the system may produce the expected results. Developers can create backtesting software to enable a trading system designer to develop and test their trading systems using historical market data to optimize the results obtained with the historical data. Although backtesting of automated trading systems cannot accurately determine future results, an automated trading system can be backtested using historical prices to see how the system theoretically would have performed if it had been active in a past market environment.\n\nForward testing of an algorithm can also be achieved using simulated trading with real-time market data to help confirm the effectiveness of the trading strategy in the current market and may be used to reveal issues inherent in the computer code. \n\nLive testing is the final stage of the development cycle. In this stage, live performance is compared against the backtested and walk forward results. Metrics compared include Percent Profitable, Profit Factor, Maximum Drawdown and Average Gain per Trade. The goal of an automated trading system is to meet or exceed the backtested performance with a high efficiency rating. \n\nImproved order entry speed allows a trader to enter or exit a position as soon as the trade criteria are satisfied. Furthermore, stop losses and profit targets can be automatically generated using an automated trading system.\n\nAutomated trading, or high-frequency trading, causes regulatory concerns as a contributor to market fragility.\n\nUnited States regulators have published releases discussing several types of risk controls that could be used to limit the extent of such disruptions, including financial and regulatory controls to prevent the entry of erroneous orders as a result of computer malfunction or human error, the breaching of various regulatory requirements, and exceeding a credit or capital limit.\n\nThe use of high-frequency trading (HFT) strategies has grown substantially over the past several years and drives a significant portion of activity on U.S. markets. Although many HFT strategies are legitimate, some are not and may be used for manipulative trading. Given the scale of the potential impact that these practices may have, the surveillance of abusive algorithms remains a high priority for regulators. The Financial Industry Regulatory Authority (FINRA) has reminded firms using HFT strategies and other trading algorithms of their obligation to be vigilant when testing these strategies pre- and post-launch to ensure that the strategies do not result in abusive trading.\n\nFINRA continues to be concerned about the use of so-called \"momentum ignition strategies\" where a market participant attempts to induce others to trade at artificially high or low prices. Examples of this activity include layering and spoofing strategies where a market participant places a nonbona fide order on one side of the market (typically, but not always, above the offer or below the bid) in an attempt to bait other market participants to react to the non-bona fide order and trade with another order on the other side of the market.\n\nOther examples of problematic HFT or algorithmic activity include order entry strategies related to placing orders near the open or close of regular trading hours that involve distorting disseminated market imbalance indicators through the entry of non-bona fide orders and/or aggressive trading activity near the open or close.\n\nFINRA also continues to focus concern on the entry of problematic HFT and algorithmic activity through sponsored participants who initiate their activity from outside of the United States. In this regard, FINRA reminds firms of their surveillance and control obligations under the SEC's Market Access Rule and Notice to Members 04-66, as well as potential issues related to treating such accounts as customer accounts, anti-money laundering and margin levels, as highlighted in Regulatory Notice 10-18 and the SEC's Office of Compliance Inspections and Examination's National Exam Risk Alert dated September 29, 2011.\n\nFINRA conducts surveillance to identify cross-market, cross-product manipulation of the price of underlying equity securities, typically through abusive trading algorithms, and strategies used to close out pre-existing option positions at favorable prices or establish new option positions at advantageous prices.\n\nIn recent years, there have been a number of algorithmic trading malfunctions that caused substantial market disruptions. These raise concern about firms' ability to develop, implement and effectively supervise their automated systems. FINRA has stated that it will assess whether firms' testing and controls related to algorithmic trading and other automated trading strategies and trading systems are adequate in light of the U.S. Securities and Exchange Commission and firms' supervisory obligations. This assessment may take the form of examinations and targeted investigations. Firms will be required to address whether they conduct separate, independent and robust pre-implementation testing of algorithms and trading systems and whether the firm's legal, compliance and operations staff are reviewing the design and development of the algorithms and trading systems for compliance with legal requirements. FINRA will review whether a firm actively monitors and reviews algorithms and trading systems once they are placed into production systems and after they have been modified, including procedures and controls used to detect potential trading abuses such as wash sales, marking, layering and momentum ignition strategies. Finally, firms will need to describe their approach to firm-wide disconnect or \"kill\" switches, as well as procedures for responding to catastrophic system malfunctions.\n\nExamples of recent substantial market disruptions include the following:\n\n"}
{"id": "45231593", "url": "https://en.wikipedia.org/wiki?curid=45231593", "title": "Boiler (water heating)", "text": "Boiler (water heating)\n\nThe term Boiler may refer to an appliance for heating water. Applications include water heating and central heating. \n\nThe boiler heats water to a temperature controlled by a thermostat. The water then flows (either by natural circulation or by a pump) to radiators in the rooms which are to be heated. Water also flows through a coil in the hot water tank to heat a separate mass of water for bathing, etc.\n\nA back boiler is a device which is fitted to a residential heating stove or open fireplace to enable it to provide both room heat and domestic hot water or central heating.\n\n"}
{"id": "4932111", "url": "https://en.wikipedia.org/wiki?curid=4932111", "title": "Capacitor", "text": "Capacitor\n\nA capacitor is a passive two-terminal electronic component that stores electrical energy in an electric field. The effect of a capacitor is known as capacitance. While some capacitance exists between any two electrical conductors in proximity in a circuit, a capacitor is a component designed to add capacitance to a circuit. The capacitor was originally known as a condenser or condensator. The original name is still widely used in many languages, but not in English.\n\nThe physical form and construction of practical capacitors vary widely and many capacitor types are in common use. Most capacitors contain at least two electrical conductors often in the form of metallic plates or surfaces separated by a dielectric medium. A conductor may be a foil, thin film, sintered bead of metal, or an electrolyte. The nonconducting dielectric acts to increase the capacitor's charge capacity. Materials commonly used as dielectrics include glass, ceramic, plastic film, paper, mica, and oxide layers. Capacitors are widely used as parts of electrical circuits in many common electrical devices. Unlike a resistor, an ideal capacitor does not dissipate energy.\n\nWhen two conductors experience a potential difference, for example, when a capacitor is attached across a battery, an electric field develops across the dielectric, causing a net positive charge to collect on one plate and net negative charge to collect on the other plate. No current actually flows through the dielectric, however, there is a flow of charge through the source circuit. If the condition is maintained sufficiently long, the current through the source circuit ceases. However, if a time-varying voltage is applied across the leads of the capacitor, the source experiences an ongoing current due to the charging and discharging cycles of the capacitor.\n\nCapacitance is defined as the ratio of the electric charge on each conductor to the potential difference between them. The unit of capacitance in the International System of Units (SI) is the farad (F), defined as one coulomb per volt (1 C/V). Capacitance values of typical capacitors for use in general electronics range from about 1 picofarad (pF) (10 F) to about 1 millifarad (mF) (10 F).\n\nThe capacitance of a capacitor is proportional to the surface area of the plates (conductors) and inversely related to the gap between them. In practice, the dielectric between the plates passes a small amount of leakage current. It has an electric field strength limit, known as the breakdown voltage. The conductors and leads introduce an undesired inductance and resistance.\n\nCapacitors are widely used in electronic circuits for blocking direct current while allowing alternating current to pass. In analog filter networks, they smooth the output of power supplies. In resonant circuits they tune radios to particular frequencies. In electric power transmission systems, they stabilize voltage and power flow. The property of energy storage in capacitors was exploited as dynamic memory in early digital computers.\n\nIn October 1745, Ewald Georg von Kleist of Pomerania, Germany, found that charge could be stored by connecting a high-voltage electrostatic generator by a wire to a volume of water in a hand-held glass jar. Von Kleist's hand and the water acted as conductors, and the jar as a dielectric (although details of the mechanism were incorrectly identified at the time). Von Kleist found that touching the wire resulted in a powerful spark, much more painful than that obtained from an electrostatic machine. The following year, the Dutch physicist Pieter van Musschenbroek invented a similar capacitor, which was named the Leyden jar, after the University of Leiden where he worked. He also was impressed by the power of the shock he received, writing, \"I would not take a second shock for the kingdom of France.\"\n\nDaniel Gralath was the first to combine several jars in parallel to increase the charge storage capacity. Benjamin Franklin investigated the Leyden jar and came to the conclusion that the charge was stored on the glass, not in the water as others had assumed. He also adopted the term \"battery\", (denoting the increasing of power with a row of similar units as in a battery of cannon), subsequently applied to clusters of electrochemical cells. Leyden jars were later made by coating the inside and outside of jars with metal foil, leaving a space at the mouth to prevent arcing between the foils. The earliest unit of capacitance was the jar, equivalent to about 1.11 nanofarads.\n\nLeyden jars or more powerful devices employing flat glass plates alternating with foil conductors were used exclusively up until about 1900, when the invention of wireless (radio) created a demand for standard capacitors, and the steady move to higher frequencies required capacitors with lower inductance. More compact construction methods began to be used, such as a flexible dielectric sheet (like oiled paper) sandwiched between sheets of metal foil, rolled or folded into a small package.\nEarly capacitors were known as \"condensers\", a term that is still occasionally used today, particularly in high power applications, such as automotive systems. The term was first used for this purpose by Alessandro Volta in 1782, with reference to the device's ability to store a higher density of electric charge than was possible with an isolated conductor. The term became deprecated because of the ambiguous meaning of steam condenser, with \"capacitor\" becoming the recommended term from 1926.\n\nSince the beginning of the study of electricity non conductive materials like glass, porcelain, paper and mica have been used as insulators. These materials some decades later were also well-suited for further use as the dielectric for the first capacitors. \nPaper capacitors made by sandwiching a strip of impregnated paper between strips of metal, and rolling the result into a cylinder were commonly used in the late 19th century; their manufacture started in 1876, and they were used from the early 20th century as decoupling capacitors in telecommunications (telephony).\n\nPorcelain was used in the first ceramic capacitors. In the early years of Marconi's wireless transmitting apparatus porcelain capacitors were used for high voltage and high frequency application in the transmitters. On the receiver side smaller mica capacitors were used for resonant circuits. Mica dielectric capacitors were invented in 1909 by William Dubilier. Prior to World War II, mica was the most common dielectric for capacitors in the United States.\n\nCharles Pollak (born Karol Pollak), the inventor of the first electrolytic capacitors, found out that the oxide layer on an aluminum anode remained stable in a neutral or alkaline electrolyte, even when the power was switched off. In 1896 he was granted U.S. Patent No. 672,913 for an \"Electric liquid capacitor with aluminum electrodes\". Solid electrolyte tantalum capacitors were invented by Bell Laboratories in the early 1950s as a miniaturized and more reliable low-voltage support capacitor to complement their newly invented transistor.\n\nWith the development of plastic materials by organic chemists during the Second World War, the capacitor industry began to replace paper with thinner polymer films. One very early development in film capacitors was described in British Patent 587,953 in 1944.\n\nLast but not least the electric double-layer capacitor (now Supercapacitors) were invented. In 1957 H. Becker developed a \"Low voltage electrolytic capacitor with porous carbon electrodes\". He believed that the energy was stored as a charge in the carbon pores used in his capacitor as in the pores of the etched foils of electrolytic capacitors. Because the double layer mechanism was not known by him at the time, he wrote in the patent: \"It is not known exactly what is taking place in the component if it is used for energy storage, but it leads to an extremely high capacity.\"\n\nA capacitor consists of two conductors separated by a non-conductive region. The non-conductive region can either be a vacuum or an electrical insulator material known as a dielectric. Examples of dielectric media are glass, air, paper, plastic, ceramic, and even a semiconductor depletion region chemically identical to the conductors. From Coulomb's law a charge on one conductor will exert a force on the charge carriers within the other conductor, attracting opposite polarity charge and repelling like polarity charges, thus an opposite polarity charge will be induced on the surface of the other conductor. The conductors thus hold equal and opposite charges on their facing surfaces, and the dielectric develops an electric field.\n\nAn ideal capacitor is characterized by a constant capacitance \"C\", in farads in the SI system of units, defined as the ratio of the positive or negative charge \"Q\" on each conductor to the voltage \"V\" between them:\nA capacitance of one farad (F) means that one coulomb of charge on each conductor causes a voltage of one volt across the device. Because the conductors (or plates) are close together, the opposite charges on the conductors attract one another due to their electric fields, allowing the capacitor to store more charge for a given voltage than when the conductors are separated, yielding a larger capacitance.\n\nIn practical devices, charge build-up sometimes affects the capacitor mechanically, causing its capacitance to vary. In this case, capacitance is defined in terms of incremental changes:\n\nIn the hydraulic analogy, charge carriers flowing through a wire are analogous to water flowing through a pipe. A capacitor is like a rubber membrane sealed inside a pipe. Water molecules cannot pass through the membrane, but some water can move by stretching the membrane. The analogy clarifies a few aspects of capacitors:\n\nThe simplest model capacitor consists of two thin parallel conductive plates each with an area of formula_3 separated by a uniform gap of thickness formula_4 filled with a dielectric with permittivity formula_5. It is assumed the gap formula_4 is much smaller than the dimensions of the plates. This model applies well to many practical capacitors which are constructed of metal sheets separated by a thin layer of insulating dielectric, since manufacturers try to keep the dielectric very uniform in thickness to avoid thin spots which can cause failure of the capacitor.\n\nSince the separation between the plates is uniform over the plate area, the electric field between the plates formula_7 is constant, and directed perpendicularly to the plate surface, except for an area near the edges of the plates where the field decreases because the electric field lines \"bulge\" out of the sides of the capacitor. This \"fringing field\" area is approximately the same width as the plate separation, formula_4, and assuming formula_4 is small compared to the plate dimensions, it is small enough to be ignored. Therefore, if a charge of formula_10 is placed on one plate and formula_11 on the other plate, the charge on each plate will be spread evenly in a surface charge layer of constant charge density formula_12 coulombs per square meter, on the inside surface of each plate. From Gauss's law the magnitude of the electric field between the plates is formula_13. The voltage formula_14 between the plates is defined as the line integral of the electric field over a line from one plate to another\nThe capacitance is defined as formula_16. Substituting formula_14 above into this equation\n\nTherefore, in a capacitor the highest capacitance is achieved with a high permittivity dielectric material, large plate area, and small separation between the plates.\n\nSince the area formula_3 of the plates increases with the square of the linear dimensions and the separation formula_4 increases linearly, the capacitance scales with the linear dimension of a capacitor (formula_20), or as the cube root of the volume.\n\nA parallel plate capacitor can only store a finite amount of energy before dielectric breakdown occurs. The capacitor's dielectric material has a dielectric strength \"U\" which sets the capacitor's breakdown voltage at \"V\" = \"V\" = \"U\"\"d\". The maximum energy that the capacitor can store is therefore\n\nThe maximum energy is a function of dielectric volume, permittivity, and dielectric strength. Changing the plate area and the separation between the plates while maintaining the same volume causes no change of the maximum amount of energy that the capacitor can store, so long as the distance between plates remains much smaller than both the length and width of the plates. In addition, these equations assume that the electric field is entirely concentrated in the dielectric between the plates. In reality there are fringing fields outside the dielectric, for example between the sides of the capacitor plates, which increase the effective capacitance of the capacitor. This is sometimes called parasitic capacitance. For some simple capacitor geometries this additional capacitance term can be calculated analytically. It becomes negligibly small when the ratios of plate width to separation and length to separation are large.\n\nFor formula_22 number of plates in a capacitor, the total capacitance would be \n\nformula_23\n\nwhere formula_24 is the capacitance for a single plate and formula_22 is the number of interleaved plates.\n\nAs shown to the figure on the right, the interleaved plates can be seen as parallel plates connected to each other. With the number of capacitor equal to the number of the spaces in between the plates. Thus the formula_26 multiplier.\n\nTo increase the charge and voltage on a capacitor, work must be done by an external power source to move charge from the negative to the positive plate against the opposing force of the electric field. If the voltage on the capacitor is formula_14, the work formula_28 required to move a small increment of charge formula_29 from the negative to the positive plate is formula_30. The energy is stored in the increased electric field between the plates. The total energy formula_31 stored in a capacitor (expressed in Joule) is equal to the total work done in establishing the electric field from an uncharged state.\nwhere formula_33 is the charge stored in the capacitor, formula_14 is the voltage across the capacitor, and formula_35 is the capacitance. This potential energy will remain in the capacitor until the charge is removed. If charge is allowed to move back from the positive to the negative plate, for example by connecting a circuit with resistance between the plates, the charge moving under the influence of the electric field will do work on the external circuit.\n\nIf the gap between the capacitor plates formula_4 is constant, as in the parallel plate model above, the electric field between the plates will be uniform (neglecting fringing fields) and will have a constant value formula_37. In this case the stored energy can be calculated from the electric field strength\nThe last formula above is equal to the energy density per unit volume in the electric field multiplied by the volume of field between the plates, confirming that the energy in the capacitor is stored in its electric field.\n\nThe current \"I\"(\"t\") through any component in an electric circuit is defined as the rate of flow of a charge \"Q\"(\"t\") passing through it, but actual charges—electrons—cannot pass through the dielectric layer of a capacitor. Rather, one electron accumulates on the negative plate for each one that leaves the positive plate, resulting in an electron depletion and consequent positive charge on one electrode that is equal and opposite to the accumulated negative charge on the other. Thus the charge on the electrodes is equal to the integral of the current as well as proportional to the voltage, as discussed above. As with any antiderivative, a constant of integration is added to represent the initial voltage \"V\"(\"t\"). This is the integral form of the capacitor equation:\n\nTaking the derivative of this and multiplying by \"C\" yields the derivative form:\n\nThe dual of the capacitor is the inductor, which stores energy in a magnetic field rather than an electric field. Its current-voltage relation is obtained by exchanging current and voltage in the capacitor equations and replacing \"C\" with the inductance \"L\".\n\nA series circuit containing only a resistor, a capacitor, a switch and a constant DC source of voltage \"V\" is known as a \"charging circuit\". If the capacitor is initially uncharged while the switch is open, and the switch is closed at \"t\", it follows from Kirchhoff's voltage law that\n\nTaking the derivative and multiplying by \"C\", gives a first-order differential equation:\n\nAt \"t\" = 0, the voltage across the capacitor is zero and the voltage across the resistor is \"V\". The initial current is then \"I\"(0) =\"V\"/\"R\". With this assumption, solving the differential equation yields\n\nwhere τ = \"RC,\" the \"time constant\" of the system. As the capacitor reaches equilibrium with the source voltage, the voltages across the resistor and the current through the entire circuit decay exponentially. In the case of a \"discharging\" capacitor, the capacitor's initial voltage (V) replaces \"V\". The equations become\n\nImpedance, the vector sum of reactance and resistance, describes the phase difference and the ratio of amplitudes between sinusoidally varying voltage and sinusoidally varying current at a given frequency. Fourier analysis allows any signal to be constructed from a spectrum of frequencies, whence the circuit's reaction to the various frequencies may be found. The reactance and impedance of a capacitor are respectively\n\nwhere \"j\" is the imaginary unit and ω is the angular frequency of the sinusoidal signal. The −\"j\" phase indicates that the AC voltage \"V\" = \"ZI\" lags the AC current by 90°: the positive current phase corresponds to increasing voltage as the capacitor charges; zero current corresponds to instantaneous constant voltage, etc.\n\nImpedance decreases with increasing capacitance and increasing frequency. This implies that a higher-frequency signal or a larger capacitor results in a lower voltage amplitude per current amplitude—an AC \"short circuit\" or AC coupling. Conversely, for very low frequencies, the reactance is high, so that a capacitor is nearly an open circuit in AC analysis—those frequencies have been \"filtered out\".\n\nCapacitors are different from resistors and inductors in that the impedance is inversely proportional to the defining characteristic; i.e., capacitance.\n\nA capacitor connected to a sinusoidal voltage source causes a displacement current to flow through it. In the case that the voltage source is Vcos(ωt), the displacement current can be expressed as:\n\nAt sin(ωt) = -1, the capacitor has a maximum (or peak) current whereby I = ωCV. The ratio of peak voltage to peak current is due to capacitive reactance (denoted X).\n\nformula_47\n\nX approaches zero as ω approaches infinity. If X approaches 0, the capacitor resembles a short wire that strongly passes current at high frequencies. X approaches infinity as ω approaches zero. If X approaches infinity, the capacitor resembles an open circuit that poorly passes low frequencies.\n\nThe current of the capacitor may be expressed in the form of cosines to better compare with the voltage of the source:\n\nIn this situation, the current is out of phase with the voltage by +π/2 radians or +90 degrees, i.e. the current leads the voltage by 90°.\n\nWhen using the Laplace transform in circuit analysis, the impedance of an ideal capacitor with no initial charge is represented in the \"s\" domain by:\n\nwhere\n\n\n\n\nCapacitors deviate from the ideal capacitor equation in a number of ways. Some of these, such as leakage current and parasitic effects are linear, or can be analyzed as nearly linear, and can be dealt with by adding virtual components to the equivalent circuit of an ideal capacitor. The usual methods of network analysis can then be applied. In other cases, such as with breakdown voltage, the effect is non-linear and ordinary (normal, e.g., linear) network analysis cannot be used, the effect must be dealt with separately. There is yet another group, which may be linear but invalidate the assumption in the analysis that capacitance is a constant. Such an example is temperature dependence. Finally, combined parasitic effects such as inherent inductance, resistance, or dielectric losses can exhibit non-uniform behavior at variable frequencies of operation.\n\nAbove a particular electric field, known as the dielectric strength \"E\", the dielectric in a capacitor becomes conductive. The voltage at which this occurs is called the breakdown voltage of the device, and is given by the product of the dielectric strength and the separation between the conductors,\n\nThe maximum energy that can be stored safely in a capacitor is limited by the breakdown voltage. Due to the scaling of capacitance and breakdown voltage with dielectric thickness, all capacitors made with a particular dielectric have approximately equal maximum energy density, to the extent that the dielectric dominates their volume.\n\nFor air dielectric capacitors the breakdown field strength is of the order 2–5 MV/m (or kV/mm); for mica the breakdown is 100–300 MV/m; for oil, 15–25 MV/m; it can be much less when other materials are used for the dielectric. The dielectric is used in very thin layers and so absolute breakdown voltage of capacitors is limited. Typical ratings for capacitors used for general electronics applications range from a few volts to 1 kV. As the voltage increases, the dielectric must be thicker, making high-voltage capacitors larger per capacitance than those rated for lower voltages.\n\nThe breakdown voltage is critically affected by factors such as the geometry of the capacitor conductive parts; sharp edges or points increase the electric field strength at that point and can lead to a local breakdown. Once this starts to happen, the breakdown quickly tracks through the dielectric until it reaches the opposite plate, leaving carbon behind and causing a short (or relatively low resistance) circuit. The results can be explosive as the short in the capacitor draws current from the surrounding circuitry and dissipates the energy. However, in capacitors with particular dielectrics and thin metal electrodes shorts are not formed after breakdown. It happens because a metal melts or evaporates in a breakdown vicinity, isolating it from the rest of the capacitor.\n\nThe usual breakdown route is that the field strength becomes large enough to pull electrons in the dielectric from their atoms thus causing conduction. Other scenarios are possible, such as impurities in the dielectric, and, if the dielectric is of a crystalline nature, imperfections in the crystal structure can result in an avalanche breakdown as seen in semi-conductor devices. Breakdown voltage is also affected by pressure, humidity and temperature.\n\nAn ideal capacitor only stores and releases electrical energy, without dissipating any. In reality, all capacitors have imperfections within the capacitor's material that create resistance. This is specified as the \"equivalent series resistance\" or ESR of a component. This adds a real component to the impedance:\nAs frequency approaches infinity, the capacitive impedance (or reactance) approaches zero and the ESR becomes significant. As the reactance becomes negligible, power dissipation approaches \"P\" = \"V\"² /\"R\".\n\nSimilarly to ESR, the capacitor's leads add \"equivalent series inductance\" or ESL to the component. This is usually significant only at relatively high frequencies. As inductive reactance is positive and increases with frequency, above a certain frequency capacitance is canceled by inductance. High-frequency engineering involves accounting for the inductance of all connections and components.\n\nIf the conductors are separated by a material with a small conductivity rather than a perfect dielectric, then a small leakage current flows directly between them. The capacitor therefore has a finite parallel resistance, and slowly discharges over time (time may vary greatly depending on the capacitor material and quality).\n\nThe quality factor (or \"Q\") of a capacitor is the ratio of its reactance to its resistance at a given frequency, and is a measure of its efficiency. The higher the Q factor of the capacitor, the closer it approaches the behavior of an ideal capacitor.\n\nThe Q factor of a capacitor can be found through the following formula:\n\nwhere formula_59 is angular frequency, formula_35 is the capacitance, formula_61 is the capacitive reactance, and formula_62 is the equivalent series resistance (ESR) of the capacitor.\n\nRipple current is the AC component of an applied source (often a switched-mode power supply) whose frequency may be constant or varying. Ripple current causes heat to be generated within the capacitor due to the dielectric losses caused by the changing field strength together with the current flow across the slightly resistive supply lines or the electrolyte in the capacitor. The equivalent series resistance (ESR) is the amount of internal series resistance one would add to a perfect capacitor to model this.\n\nSome types of capacitors, primarily tantalum and aluminum electrolytic capacitors, as well as some film capacitors have a specified rating value for maximum ripple current.\n\nThe capacitance of certain capacitors decreases as the component ages. In ceramic capacitors, this is caused by degradation of the dielectric. The type of dielectric, ambient operating and storage temperatures are the most significant aging factors, while the operating voltage usually has a smaller effect, i.e., usual capacitor design is to minimize voltage coefficient. The aging process may be reversed by heating the component above the Curie point. Aging is fastest near the beginning of life of the component, and the device stabilizes over time. Electrolytic capacitors age as the electrolyte evaporates. In contrast with ceramic capacitors, this occurs towards the end of life of the component.\n\nTemperature dependence of capacitance is usually expressed in parts per million (ppm) per °C. It can usually be taken as a broadly linear function but can be noticeably non-linear at the temperature extremes. The temperature coefficient can be either positive or negative, sometimes even amongst different samples of the same type. In other words, the spread in the range of temperature coefficients can encompass zero.\n\nCapacitors, especially ceramic capacitors, and older designs such as paper capacitors, can absorb sound waves resulting in a microphonic effect. Vibration moves the plates, causing the capacitance to vary, in turn inducing AC current. Some dielectrics also generate piezoelectricity. The resulting interference is especially problematic in audio applications, potentially causing feedback or unintended recording. In the reverse microphonic effect, the varying electric field between the capacitor plates exerts a physical force, moving them as a speaker. This can generate audible sound, but drains energy and stresses the dielectric and the electrolyte, if any.\n\nCurrent reversal occurs when the current changes direction. Voltage reversal is the change of polarity in a circuit. Reversal is generally described as the percentage of the maximum rated voltage that reverses polarity. In DC circuits, this is usually less than 100%, often in the range of 0 to 90%, whereas AC circuits experience 100% reversal.\n\nIn DC circuits and pulsed circuits, current and voltage reversal are affected by the damping of the system. Voltage reversal is encountered in RLC circuits that are underdamped. The current and voltage reverse direction, forming a harmonic oscillator between the inductance and capacitance. The current and voltage tends to oscillate and may reverse direction several times, with each peak being lower than the previous, until the system reaches an equilibrium. This is often referred to as ringing. In comparison, critically damped or overdamped systems usually do not experience a voltage reversal. Reversal is also encountered in AC circuits, where the peak current is equal in each direction.\n\nFor maximum life, capacitors usually need to be able to handle the maximum amount of reversal that a system may experience. An AC circuit experiences 100% voltage reversal, while underdamped DC circuits experience less than 100%. Reversal creates excess electric fields in the dielectric, causes excess heating of both the dielectric and the conductors, and can dramatically shorten the life expectancy of the capacitor. Reversal ratings often affect the design considerations for the capacitor, from the choice of dielectric materials and voltage ratings to the types of internal connections used.\n\nCapacitors made with any type of dielectric material show some level of \"dielectric absorption\" or \"soakage\". On discharging a capacitor and disconnecting it, after a short time it may develop a voltage due to hysteresis in the dielectric. This effect is objectionable in applications such as precision sample and hold circuits or timing circuits. The level of absorption depends on many factors, from design considerations to charging time, since the absorption is a time-dependent process. However, the primary factor is the type of dielectric material. Capacitors such as tantalum electrolytic or polysulfone film exhibit relatively high absorption, while polystyrene or Teflon allow very small levels of absorption. In some capacitors where dangerous voltages and energies exist, such as in flashtubes, television sets, and defibrillators, the dielectric absorption can recharge the capacitor to hazardous voltages after it has been shorted or discharged. Any capacitor containing over 10 joules of energy is generally considered hazardous, while 50 joules or higher is potentially lethal. A capacitor may regain anywhere from 0.01 to 20% of its original charge over a period of several minutes, allowing a seemingly safe capacitor to become surprisingly dangerous.\n\nLeakage is equivalent to a resistor in parallel with the capacitor. Constant exposure to heat can cause dielectric breakdown and excessive leakage, a problem often seen in older vacuum tube circuits, particularly where oiled paper and foil capacitors were used. In many vacuum tube circuits, interstage coupling capacitors are used to conduct a varying signal from the plate of one tube to the grid circuit of the next stage. A leaky capacitor can cause the grid circuit voltage to be raised from its normal bias setting, causing excessive current or signal distortion in the downstream tube. In power amplifiers this can cause the plates to glow red, or current limiting resistors to overheat, even fail. Similar considerations apply to component fabricated solid-state (transistor) amplifiers, but owing to lower heat production and the use of modern polyester dielectric barriers this once-common problem has become relatively rare.\n\nAluminum electrolytic capacitors are \"conditioned\" when manufactured by applying a voltage sufficient to initiate the proper internal chemical state. This state is maintained by regular use of the equipment. If a system using electrolytic capacitors is unused for a long period of time it can lose its conditioning. Sometimes they fail with a short circuit when next operated.\n\nPractical capacitors are available commercially in many different forms. The type of internal dielectric, the structure of the plates and the device packaging all strongly affect the characteristics of the capacitor, and its applications.\n\nValues available range from very low (picofarad range; while arbitrarily low values are in principle possible, stray (parasitic) capacitance in any circuit is the limiting factor) to about 5 kF supercapacitors.\n\nAbove approximately 1 microfarad electrolytic capacitors are usually used because of their small size and low cost compared with other types, unless their relatively poor stability, life and polarised nature make them unsuitable. Very high capacity supercapacitors use a porous carbon-based electrode material.\n\nMost capacitors have a dielectric spacer, which increases their capacitance compared to air or a vacuum. In order to maximise the charge that a capacitor can hold, the dielectric material needs to have as high a permittivity as possible, while also having as high a breakdown voltage as possible. The dielectric also needs to have as low a loss with frequency as possible.\n\nHowever, low value capacitors are available with a vacuum between their plates to allow extremely high voltage operation and low losses. Variable capacitors with their plates open to the atmosphere were commonly used in radio tuning circuits. Later designs use polymer foil dielectric between the moving and stationary plates, with no significant air space between the plates.\n\nSeveral solid dielectrics are available, including paper, plastic, glass, mica and ceramic.\n\nPaper was used extensively in older capacitors and offers relatively high voltage performance. However, paper absorbs moisture, and has been largely replaced by plastic film capacitors.\n\nMost of the plastic films now used offer better stability and ageing performance than such older dielectrics such as oiled paper, which makes them useful in timer circuits, although they may be limited to relatively low operating temperatures and frequencies, because of the limitations of the plastic film being used. Large plastic film capacitors are used extensively in suppression circuits, motor start circuits, and power factor correction circuits.\n\nCeramic capacitors are generally small, cheap and useful for high frequency applications, although their capacitance varies strongly with voltage and temperature and they age poorly. They can also suffer from the piezoelectric effect. Ceramic capacitors are broadly categorized as class 1 dielectrics, which have predictable variation of capacitance with temperature or class 2 dielectrics, which can operate at higher voltage. Modern multilayer ceramics are usually quite small, but some types have inherently wide value tolerances, microphonic issues, and are usually physically brittle.\n\nGlass and mica capacitors are extremely reliable, stable and tolerant to high temperatures and voltages, but are too expensive for most mainstream applications.\n\nElectrolytic capacitors and supercapacitors are used to store small and larger amounts of energy, respectively, ceramic capacitors are often used in resonators, and parasitic capacitance occurs in circuits wherever the simple conductor-insulator-conductor structure is formed unintentionally by the configuration of the circuit layout.\n\nElectrolytic capacitors use an aluminum or tantalum plate with an oxide dielectric layer. The second electrode is a liquid electrolyte, connected to the circuit by another foil plate. Electrolytic capacitors offer very high capacitance but suffer from poor tolerances, high instability, gradual loss of capacitance especially when subjected to heat, and high leakage current. Poor quality capacitors may leak electrolyte, which is harmful to printed circuit boards. The conductivity of the electrolyte drops at low temperatures, which increases equivalent series resistance. While widely used for power-supply conditioning, poor high-frequency characteristics make them unsuitable for many applications. Electrolytic capacitors suffer from self-degradation if unused for a period (around a year), and when full power is applied may short circuit, permanently damaging the capacitor and usually blowing a fuse or causing failure of rectifier diodes. For example, in older equipment, this may cause arcing in rectifier tubes. They can be restored before use by gradually applying the operating voltage, often performed on antique vacuum tube equipment over a period of thirty minutes by using a variable transformer to supply AC power. The use of this technique may be less satisfactory for some solid state equipment, which may be damaged by operation below its normal power range, requiring that the power supply first be isolated from the consuming circuits. Such remedies may not be applicable to modern high-frequency power supplies as these produce full output voltage even with reduced input.\n\nTantalum capacitors offer better frequency and temperature characteristics than aluminum, but higher dielectric absorption and leakage.\n\nPolymer capacitors (OS-CON, OC-CON, KO, AO) use solid conductive polymer (or polymerized organic semiconductor) as electrolyte and offer longer life and lower ESR at higher cost than standard electrolytic capacitors.\n\nA feedthrough capacitor is a component that, while not serving as its main use, has capacitance and is used to conduct signals through a conductive sheet.\n\nSeveral other types of capacitor are available for specialist applications. Supercapacitors store large amounts of energy. Supercapacitors made from carbon aerogel, carbon nanotubes, or highly porous electrode materials, offer extremely high capacitance (up to 5 kF ) and can be used in some applications instead of rechargeable batteries. Alternating current capacitors are specifically designed to work on line (mains) voltage AC power circuits. They are commonly used in electric motor circuits and are often designed to handle large currents, so they tend to be physically large. They are usually ruggedly packaged, often in metal cases that can be easily grounded/earthed. They also are designed with direct current breakdown voltages of at least five times the maximum AC voltage.\n\nThe dielectric constant for a number of very useful dielectrics changes as a function of the applied electrical field, for example ferroelectric materials, so the capacitance for these devices is more complex. For example, in charging such a capacitor the differential increase in voltage with charge is governed by:\n\nwhere the voltage dependence of capacitance, \"C\"(\"V\"), suggests that the capacitance is a function of the electric field strength, which in a large area parallel plate device is given by \"ε = V/d\". This field polarizes the dielectric, which polarization, in the case of a ferroelectric, is a nonlinear \"S\"-shaped function of the electric field, which, in the case of a large area parallel plate device, translates into a capacitance that is a nonlinear function of the voltage.\n\nCorresponding to the voltage-dependent capacitance, to charge the capacitor to voltage \"V\" an integral relation is found:\n\nwhich agrees with \"Q\" = \"CV\" only when \"C\" does not depend on voltage \"V\".\n\nBy the same token, the energy stored in the capacitor now is given by\n\nIntegrating:\n\nwhere interchange of the order of integration is used.\n\nThe nonlinear capacitance of a microscope probe scanned along a ferroelectric surface is used to study the domain structure of ferroelectric materials.\n\nAnother example of voltage dependent capacitance occurs in semiconductor devices such as semiconductor diodes, where the voltage dependence stems not from a change in dielectric constant but in a voltage dependence of the spacing between the charges on the two sides of the capacitor. This effect is intentionally exploited in diode-like devices known as varicaps.\n\nIf a capacitor is driven with a time-varying voltage that changes rapidly enough, at some frequency the polarization of the dielectric cannot follow the voltage. As an example of the origin of this mechanism, the internal microscopic dipoles contributing to the dielectric constant cannot move instantly, and so as frequency of an applied alternating voltage increases, the dipole response is limited and the dielectric constant diminishes. A changing dielectric constant with frequency is referred to as dielectric dispersion, and is governed by dielectric relaxation processes, such as Debye relaxation. Under transient conditions, the displacement field can be expressed as (see electric susceptibility):\n\nindicating the lag in response by the time dependence of \"ε\", calculated in principle from an underlying microscopic analysis, for example, of the dipole behavior in the dielectric. See, for example, linear response function. The integral extends over the entire past history up to the present time. A Fourier transform in time then results in:\n\nwhere \"ε\"(\"ω\") is now a complex function, with an imaginary part related to absorption of energy from the field by the medium. See permittivity. The capacitance, being proportional to the dielectric constant, also exhibits this frequency behavior. Fourier transforming Gauss's law with this form for displacement field:\n\nwhere \"j\" is the imaginary unit, \"V\"(\"ω\") is the voltage component at angular frequency \"ω\", \"G\"(\"ω\") is the \"real\" part of the current, called the \"conductance\", and \"C\"(\"ω\") determines the \"imaginary\" part of the current and is the \"capacitance\". \"Z\"(\"ω\") is the complex impedance.\n\nWhen a parallel-plate capacitor is filled with a dielectric, the measurement of dielectric properties of the medium is based upon the relation:\n\nwhere a single \"prime\" denotes the real part and a double \"prime\" the imaginary part, \"Z\"(\"ω\") is the complex impedance with the dielectric present, \"C\"(\"ω\") is the so-called \"complex\" capacitance with the dielectric present, and \"C\" is the capacitance without the dielectric. (Measurement \"without the dielectric\" in principle means measurement in free space, an unattainable goal inasmuch as even the quantum vacuum is predicted to exhibit nonideal behavior, such as dichroism. For practical purposes, when measurement errors are taken into account, often a measurement in terrestrial vacuum, or simply a calculation of \"C\", is sufficiently accurate.)\n\nUsing this measurement method, the dielectric constant may exhibit a resonance at certain frequencies corresponding to characteristic response frequencies (excitation energies) of contributors to the dielectric constant. These resonances are the basis for a number of experimental techniques for detecting defects. The \"conductance method\" measures absorption as a function of frequency. Alternatively, the time response of the capacitance can be used directly, as in \"deep-level transient spectroscopy\".\n\nAnother example of frequency dependent capacitance occurs with MOS capacitors, where the slow generation of minority carriers means that at high frequencies the capacitance measures only the majority carrier response, while at low frequencies both types of carrier respond.\n\nAt optical frequencies, in semiconductors the dielectric constant exhibits structure related to the band structure of the solid. Sophisticated modulation spectroscopy measurement methods based upon modulating the crystal structure by pressure or by other stresses and observing the related changes in absorption or reflection of light have advanced our knowledge of these materials.\n\nThe arrangement of plates and dielectric has many variations in different styles depending on the desired ratings of the capacitor. For small values of capacitance (microfarads and less), ceramic disks use metallic coatings, with wire leads bonded to the coating. Larger values can be made by multiple stacks of plates and disks. Larger value capacitors usually use a metal foil or metal film layer deposited on the surface of a dielectric film to make the plates, and a dielectric film of impregnated paper or plasticthese are rolled up to save space. To reduce the series resistance and inductance for long plates, the plates and dielectric are staggered so that connection is made at the common edge of the rolled-up plates, not at the ends of the foil or metalized film strips that comprise the plates.\n\nThe assembly is encased to prevent moisture entering the dielectricearly radio equipment used a cardboard tube sealed with wax. Modern paper or film dielectric capacitors are dipped in a hard thermoplastic. Large capacitors for high-voltage use may have the roll form compressed to fit into a rectangular metal case, with bolted terminals and bushings for connections. The dielectric in larger capacitors is often impregnated with a liquid to improve its properties.\n\nCapacitors may have their connecting leads arranged in many configurations, for example axially or radially. \"Axial\" means that the leads are on a common axis, typically the axis of the capacitor's cylindrical bodythe leads extend from opposite ends. Radial leads are rarely aligned along radii of the body's circle, so the term is conventional. The leads (until bent) are usually in planes parallel to that of the flat body of the capacitor, and extend in the same direction; they are often parallel as manufactured.\n\nSmall, cheap discoidal ceramic capacitors have existed from the 1930s onward, and remain in widespread use. After the 1980s, surface mount packages for capacitors have been widely used. These packages are extremely small and lack connecting leads, allowing them to be soldered directly onto the surface of printed circuit boards. Surface mount components avoid undesirable high-frequency effects due to the leads and simplify automated assembly, although manual handling is made difficult due to their small size.\n\nMechanically controlled variable capacitors allow the plate spacing to be adjusted, for example by rotating or sliding a set of movable plates into alignment with a set of stationary plates. Low cost variable capacitors squeeze together alternating layers of aluminum and plastic with a screw. Electrical control of capacitance is achievable with varactors (or varicaps), which are reverse-biased semiconductor diodes whose depletion region width varies with applied voltage. They are used in phase-locked loops, amongst other applications.\n\nMost capacitors have numbers printed on their bodies to indicate their electrical characteristics. Larger capacitors like electrolytics usually display the actual capacitance together with the unit, for example, \"220 μF\". Smaller capacitors like ceramics, however, use a shorthand-notation consisting of three digits and a letter, where the digits indicate the capacitance in pF, calculated as XY × 10 for digits XYZ, and the letter indicates the tolerance. Common tolerance indications are J, K, and M for ±5%, ±10%, and ±20%, respectively.\n\nAdditionally, the capacitor may be labeled with its working voltage, temperature and other relevant characteristics.\n\nFor typographical reasons, some manufacturers print \"MF\" on capacitors to indicate microfarads (μF).\n\nA capacitor labeled or designated as \"473K 330V\" has a capacitance of 47 × 10 pF = 47 nF (±10%) with a maximum working voltage of 330 V. The working voltage of a capacitor is nominally the highest voltage that may be applied across it without undue risk of breaking down the dielectric layer.\n\nThe notation to state a capacitor's value in a circuit diagram varies. The RKM code following IEC 60062 and BS 1852 avoids using a decimal separator and replaces the decimal separator with the SI prefix symbol for the particular value (and the letter F for weight 1). Example: 4n7 for 4.7 nF or 2F2 for 2.2 F.\n\nIn the past, alternate capacitance subunits were used in historical electronic books and old electronics catalogs; \"mfd\" and \"mf\" for microfarad (µF); \"mmfd\", \"mmf\", \"µµF\" for picofarad (pF); but are rarely used any more.\n\nA capacitor can store electric energy when disconnected from its charging circuit, so it can be used like a temporary battery, or like other types of rechargeable energy storage system. Capacitors are commonly used in electronic devices to maintain power supply while batteries are being changed. (This prevents loss of information in volatile memory.)\n\nA capacitor can facilitate conversion of kinetic energy of charged particles into electric energy and store it.\n\nConventional capacitors provide less than 360 joules per kilogram of specific energy, whereas a conventional alkaline battery has a density of 590 kJ/kg. There is \nan intermediate solution: Supercapacitors, which can accept and deliver charge much faster than batteries, and tolerate many more charge and discharge cycles than rechargeable batteries. They are, however, 10 times larger than conventional batteries for a given charge. On the other hand, it has been shown that the amount of charge stored in the dielectric \nlayer of the thin film capacitor can be equal to, or can even exceed, the amount of charge stored on its plates.\n\nIn car audio systems, large capacitors store energy for the amplifier to use on demand. Also, for a flash tube, a capacitor is used to hold the high voltage.\n\nIn the 1930s, John Atanasoff applied the principle of energy storage in capacitors to construct dynamic digital memories for the first binary computers that used electron tubes for logic.\n\nGroups of large, specially constructed, low-inductance high-voltage capacitors (\"capacitor banks\") are used to supply huge pulses of current for many pulsed power applications. These include electromagnetic forming, Marx generators, pulsed lasers (especially TEA lasers), pulse forming networks, radar, fusion research, and particle accelerators.\n\nLarge capacitor banks (reservoir) are used as energy sources for the exploding-bridgewire detonators or slapper detonators in nuclear weapons and other specialty weapons. Experimental work is under way using banks of capacitors as power sources for electromagnetic armour and electromagnetic railguns and coilguns.\n\nReservoir capacitors are used in power supplies where they smooth the output of a full or half wave rectifier. They can also be used in charge pump circuits as the energy storage element in the generation of higher voltages than the input voltage.\n\nCapacitors are connected in parallel with the power circuits of most electronic devices and larger systems (such as factories) to shunt away and conceal current fluctuations from the primary power source to provide a \"clean\" power supply for signal or control circuits. Audio equipment, for example, uses several capacitors in this way, to shunt away power line hum before it gets into the signal circuitry. The capacitors act as a local reserve for the DC power source, and bypass AC currents from the power supply. This is used in car audio applications, when a stiffening capacitor compensates for the inductance and resistance of the leads to the lead-acid car battery.\n\nIn electric power distribution, capacitors are used for power factor correction. Such capacitors often come as three capacitors connected as a three phase load. Usually, the values of these capacitors are not given in farads but rather as a reactive power in volt-amperes reactive (var). The purpose is to counteract inductive loading from devices like electric motors and transmission lines to make the load appear to be mostly resistive. Individual motor or lamp loads may have capacitors for power factor correction, or larger sets of capacitors (usually with automatic switching devices) may be installed at a load center within a building or in a large utility substation.\n\nBecause capacitors pass AC but block DC signals (when charged up to the applied dc voltage), they are often used to separate the AC and DC components of a signal. This method is known as \"AC coupling\" or \"capacitive coupling\". Here, a large value of capacitance, whose value need not be accurately controlled, but whose reactance is small at the signal frequency, is employed.\n\nA decoupling capacitor is a capacitor used to protect one part of a circuit from the effect of another, for instance to suppress noise or transients. Noise caused by other circuit elements is shunted through the capacitor, reducing the effect they have on the rest of the circuit. It is most commonly used between the power supply and ground.\nAn alternative name is \"bypass capacitor\" as it is used to bypass the power supply or other high impedance component of a circuit.\n\nDecoupling capacitors need not always be discrete components. Capacitors used in these applications may be built into a printed circuit board, between the various layers. These are often referred to as embedded capacitors. The layers in the board contributing to the capacitive properties also function as power and ground planes, and have a dielectric in between them, enabling them to operate as a parallel plate capacitor.\n\nWhen an inductive circuit is opened, the current through the inductance collapses quickly, creating a large voltage across the open circuit of the switch or relay. If the inductance is large enough, the energy may generate a spark, causing the contact points to oxidize, deteriorate, or sometimes weld together, or destroying a solid-state switch. A snubber capacitor across the newly opened circuit creates a path for this impulse to bypass the contact points, thereby preserving their life; these were commonly found in contact breaker ignition systems, for instance. Similarly, in smaller scale circuits, the spark may not be enough to damage the switch but may still radiate undesirable radio frequency interference (RFI), which a filter capacitor absorbs. Snubber capacitors are usually employed with a low-value resistor in series, to dissipate energy and minimize RFI. Such resistor-capacitor combinations are available in a single package.\n\nCapacitors are also used in parallel with interrupting units of a high-voltage circuit breaker to equally distribute the voltage between these units. These are called \"grading capacitors\".\n\nIn schematic diagrams, a capacitor used primarily for DC charge storage is often drawn vertically in circuit diagrams with the lower, more negative, plate drawn as an arc. The straight plate indicates the positive terminal of the device, if it is polarized (see electrolytic capacitor).\n\nIn single phase squirrel cage motors, the primary winding within the motor housing is not capable of starting a rotational motion on the rotor, but is capable of sustaining one. To start the motor, a secondary \"start\" winding has a series non-polarized \"starting capacitor\" to introduce a lead in the sinusoidal current. When the secondary (start) winding is placed at an angle with respect to the primary (run) winding, a rotating electric field is created. The force of the rotational field is not constant, but is sufficient to start the rotor spinning. When the rotor comes close to operating speed, a centrifugal switch (or current-sensitive relay in series with the main winding) disconnects the capacitor. The start capacitor is typically mounted to the side of the motor housing. These are called capacitor-start motors, that have relatively high starting torque. Typically they can have up-to four times as much starting torque than a split-phase motor and are used on applications such as compressors, pressure washers and any small device requiring high starting torques.\n\nCapacitor-run induction motors have a permanently connected phase-shifting capacitor in series with a second winding. The motor is much like a two-phase induction motor.\n\nMotor-starting capacitors are typically non-polarized electrolytic types, while running capacitors are conventional paper or plastic film dielectric types.\n\nThe energy stored in a capacitor can be used to represent information, either in binary form, as in DRAMs, or in analogue form, as in analog sampled filters and CCDs. Capacitors can be used in analog circuits as components of integrators or more complex filters and in negative feedback loop stabilization. Signal processing circuits also use capacitors to integrate a current signal.\n\nCapacitors and inductors are applied together in tuned circuits to select information in particular frequency bands. For example, radio receivers rely on variable capacitors to tune the station frequency. Speakers use passive analog crossovers, and analog equalizers use capacitors to select different audio bands.\n\nThe resonant frequency \"f\" of a tuned circuit is a function of the inductance (\"L\") and capacitance (\"C\") in series, and is given by:\nwhere \"L\" is in henries and \"C\" is in farads.\n\nMost capacitors are designed to maintain a fixed physical structure. However, various factors can change the structure of the capacitor, and the resulting change in capacitance can be used to sense those factors.\n\nChanging the dielectric:\n\nChanging the distance between the plates:\n\nChanging the effective area of the plates:\n\nA capacitor can possess spring-like qualities in an oscillator circuit. In the image example, a capacitor acts to influence the biasing voltage at the npn transistor's base. The resistance values of the voltage-divider resistors and the capacitance value of the capacitor together control the oscillatory frequency.\n\nA light-emitting capacitor is made from a dielectric that uses phosphorescence to produce light. If one of the conductive plates is made with a transparent material, the light is visible. Light-emitting capacitors are used in the construction of electroluminescent panels, for applications such as backlighting for laptop computers. In this case, the entire panel is a capacitor used for the purpose of generating light.\n\nThe hazards posed by a capacitor are usually determined, foremost, by the amount of energy stored, which is the cause of things like electrical burns or heart fibrillation. Factors such as voltage and chassis material are of secondary consideration, which are more related to how easily a shock can be initiated rather than how much damage can occur. Under certain conditions, including conductivity of the surfaces, preexisting medical conditions, the humidity of the air, or the pathways it takes through the body (i.e.: shocks that travel across the core of the body and, especially, the heart are more dangerous than those limited to the extremities), shocks as low as one joule have been reported to cause death, although in most instances they may not even leave a burn. Shocks over ten joules will generally damage skin, and are usually considered hazardous. Any capacitor that can store 50 joules or more should be considered potentially lethal.\n\nCapacitors may retain a charge long after power is removed from a circuit; this charge can cause dangerous or even potentially fatal shocks or damage connected equipment. For example, even a seemingly innocuous device such as a disposable-camera flash unit, powered by a 1.5 volt AA battery, has a capacitor which may contain over 15 joules of energy and be charged to over 300 volts. This is easily capable of delivering a shock. Service procedures for electronic devices usually include instructions to discharge large or high-voltage capacitors, for instance using a Brinkley stick. Capacitors may also have built-in discharge resistors to dissipate stored energy to a safe level within a few seconds after power is removed. High-voltage capacitors are stored with the terminals shorted, as protection from potentially dangerous voltages due to dielectric absorption or from transient voltages the capacitor may pick up from static charges or passing weather events.\n\nSome old, large oil-filled paper or plastic film capacitors contain polychlorinated biphenyls (PCBs). It is known that waste PCBs can leak into groundwater under landfills. Capacitors containing PCB were labelled as containing \"Askarel\" and several other trade names. PCB-filled paper capacitors are found in very old (pre-1975) fluorescent lamp ballasts, and other applications.\n\nCapacitors may catastrophically fail when subjected to voltages or currents beyond their rating, or as they reach their normal end of life. Dielectric or metal interconnection failures may create arcing that vaporizes the dielectric fluid, resulting in case bulging, rupture, or even an explosion. Capacitors used in RF or sustained high-current applications can overheat, especially in the center of the capacitor rolls. Capacitors used within high-energy capacitor banks can violently explode when a short in one capacitor causes sudden dumping of energy stored in the rest of the bank into the failing unit. High voltage vacuum capacitors can generate soft X-rays even during normal operation. Proper containment, fusing, and preventive maintenance can help to minimize these hazards.\n\nHigh-voltage capacitors may benefit from a pre-charge to limit in-rush currents at power-up of high voltage direct current (HVDC) circuits. This extends the life of the component and may mitigate high-voltage hazards.\n\n\n"}
{"id": "7459059", "url": "https://en.wikipedia.org/wiki?curid=7459059", "title": "Center for Advanced Biotechnology and Medicine", "text": "Center for Advanced Biotechnology and Medicine\n\nThe Center for Advanced Biotechnology and Medicine (CABM) is located on Busch Campus in Piscataway, New Jersey. It was established in 1985 to advance knowledge in the life sciences for the improvement of human health. It is administered by Rutgers, The State University of New Jersey. The building was completed in 1990, and has of lab and office space.\n\n"}
{"id": "1550386", "url": "https://en.wikipedia.org/wiki?curid=1550386", "title": "Chaff (countermeasure)", "text": "Chaff (countermeasure)\n\nChaff, originally called Window by the British and Düppel by the Second World War era German Luftwaffe (from the Berlin suburb where it was first developed), is a radar countermeasure in which aircraft or other targets spread a cloud of small, thin pieces of aluminium, metallized glass fibre(fiber) or plastic, which either appears as a cluster of primary targets on radar screens or swamps the screen with multiple returns.\n\nModern armed forces use chaff (in naval applications, for instance, using short-range SRBOC rockets) to distract radar-guided missiles from their targets. Most military aircraft and warships have chaff dispensing systems for self-defense. An intercontinental ballistic missile may release in its midcourse phase several independent warheads as well as penetration aids such as decoy balloons and chaff.\n\nThe idea of using chaff developed independently in the United Kingdom, Germany, the United States and Japan. In 1937, British researcher Gerald Touch, while working with Robert Watson-Watt on radar, suggested that lengths of wire suspended from balloons or parachutes might overwhelm a radar system with false echoes and R. V. Jones had suggested that pieces of metal foil falling through the air might do the same. In early 1942, a Telecommunications Research Establishment (TRE) researcher named Joan Curran investigated the idea and came up with a scheme for dumping packets of aluminium strips from aircraft to generate a cloud of false echoes. An early idea was to use sheets the size of a notebook page; these would be printed so they would also serve as propaganda leaflets. It was found that the most effective version was strips of black paper backed with aluminium foil, exactly and packed into bundles each weighing . The head of the TRE, A. P. Rowe, code-named the device \"Window\". In Germany, similar research had led to the development of \"Düppel\". The German code name came from the estate where the first German tests with chaff took place, circa 1942. Once the British had passed the idea to the US via the Tizard Mission, Fred Whipple developed a system (according to Harvard Gazette Archives) for dispensing strips for the USAAF, but it is not known if this was ever used.\n\nThe systems used the same concept of small aluminium strips (or wires) cut to a half of the target radar's wavelength. When hit by the radar, such lengths of metal resonate and re-radiate the signal. Opposing defences would find it almost impossible to distinguish the aircraft from the echoes caused by the chaff. Other radar-confusing techniques included Mandrel, Piperack and Jostle. Ignorance about the extent of knowledge of the principle in the opposing air force led planners to judge that it was too dangerous to use, since the opponent could duplicate it. The British government's leading scientific adviser, Professor Lindemann, pointed out that if the Royal Air Force (RAF) used it against the Germans, the \"Luftwaffe\" would quickly copy it and could launch a new Blitz. This caused concern in RAF Fighter Command and Anti-Aircraft Command, who managed to suppress the use of Window until July 1943. It was felt that the new generation of centimetric radars available to Fighter Command would cope with \"Luftwaffe\" retaliation.\nExamination of the Würzburg radar equipment brought back to the UK during Operation Biting (February 1942) and subsequent reconnaissance revealed to the British that all German radars were operating in no more than three frequency ranges, making them prone to jamming. \"Bomber\" Harris, Commander-in-Chief (C-in-C) of RAF Bomber Command, finally got approval to use Window as part of Operation Gomorrah, the fire raids against Hamburg. The first aircrew trained to use Window were in 76 Squadron. Twenty-four crews were briefed on how to drop the bundles of aluminised-paper strips (treated-paper was used to minimise the weight and to maximise the time that the strips would remain in the air, prolonging the effect), one every minute through the flare chute, using a stopwatch to time them. The results proved spectacular. The radar-guided master searchlights wandered aimlessly across the sky. The anti-aircraft guns fired randomly or not at all and the night fighters, their radar displays swamped with false echoes, utterly failed to find the bomber stream. Over a week of attacks, Allied attacks devastated a vast area of Hamburg, resulting in more than 40,000 civilian deaths, with the loss of only 12 out of the 791 bombers on the first night. Squadrons quickly had special chutes fitted to their bombers to make chaff deployment even easier. Seeing this as a development that made it safer to go on operations, many crews got in as many trips as they could before the Germans found a counter-countermeasure.\n\nSix weeks after the Hamburg raid, the \"Luftwaffe\" used \"Düppel\" in lengths during a raid on the night of 7/8 October 1943. In raids in 1943 and the 'mini-blitz' of Operation Steinbock between February and May 1944, \"Düppel\" allowed German bombers again to attempt operations over London. Although theoretically effective, the small number of bombers, notably in relation to the large RAF night-fighter force, doomed the effort from the start. The British fighters were able to go aloft in large numbers and often found the German bombers in spite of \"Düppel\". The Germans obtained better results during the air raid on Bari in Italy, on 2 December 1943, when Allied radars were deceived by the use of \"Düppel\".\nChaff in the United States was co-invented by astronomer Fred Whipple and Navy engineer Merwyn Bly. Whipple proposed the idea to the Air Force he was working with at the time. Early tests failed as the foil strips stuck together and fell as clumps to little or no effect. Bly solved this by designing a cartridge that forced the strips to rub against it as they were expelled, gaining an electrostatic charge. Since the strips all had a similar charge they repelled each other, enabling the full countermeasure effect. After the war, Bly received the Navy Distinguished Civilian Service Award for his work.\n\nIn the Pacific Theater, Navy Lieutenant Commander Sudo Hajime invented a Japanese version called \"Giman-shi\", or \"deceiving paper.\" It was first used with some success in mid 1943, during night battles over the Solomon Islands. Competing demands for the scarce aluminum necessary for its manufacture limited its use. On February 21, 1945, during the Battle of Iwo Jima, \"Giman-shi\" was successfully used prior to a Kamikaze attack on the .\n\nBritish warships in the Falklands War (1982) made heavy use of chaff.\n\nDuring this war, British Sea Harrier aircraft lacked their conventional chaff-dispensing mechanism.\nTherefore, Royal Navy engineers designed an impromptu delivery system of welding rods, split pins and string, which allowed six packets of chaff to be stored in the airbrake well and be deployed in flight. It was often referred to as the \"Heath Robinson chaff modification\", due to its complexity.\n\n\n\n"}
{"id": "56244619", "url": "https://en.wikipedia.org/wiki?curid=56244619", "title": "DataGravity", "text": "DataGravity\n\nDataGravity Inc. was an industry data management company, which produced security software. The company was founded in April 2012 by Paula Long and John Joseph.\n\nThe company raised $92M from Charles River Ventures, General Catalyst, Andreessen Horowitz, and Accel Partners.\n\nDataGravity announced its first products at VMworld in 2014. It won Best of Show, and New Technology awards for the event. It began shipping their first products in October 2014.\n\nThe company focused on protection and security of the data stored on the array, and named this new type of storage as \"data-aware storage\". It publicly changed its product strategy in February 2016 from data storage appliances to a software solution focused on behavioral data security. This product strategy change resulted in multiple rounds of layoffs.\n\nMultiple reports use conflicting terminology about the final fate of the company.\n\nSome reports say HyTrust acquired DataGravity.\n\nOther reports, including a press release issued by HyTrust itself, say HyTrust acquired the assets of DataGravity after it was signed over to a liquidator.\n\nHyTrust told Fortune that founder and CEO Paula Long left DataGravity a few weeks before the transaction was announced, and that co-founder John Joseph left some time before that.\n\nAccording to some reports, DataGravity ceased day-to-day operations in June 2017, when it cancelled employee benefit plans and signed the company over to liquidator Barry Kallander of the Kallander Group. In one such report, correspondence from DataGravity President Barry Kallander states \"The corporation was not sold - the assets of the company were...Unfortunately the common shares are worthless.\"\n\nConversely, DataGravity CTO David Siles was quoted as saying the company \"did not shut down\", and that the transaction \"wasn't a fire sale. We were acquired because we complete a vision, add value, have customers who love what we do. Together we will offer a very compelling offering to the marketplace solving very pressing needs for many enterprises.\"\n\nApproximately 20 former DataGravity employees joined HyTrust to support DataGravity's product integration, led by former DataGravity CTO David Siles.\n\nDataGravity's products remain a part of HyTrust's portfolio under its CloudAdvisor suite.\n"}
{"id": "5677189", "url": "https://en.wikipedia.org/wiki?curid=5677189", "title": "Drill stem test", "text": "Drill stem test\n\nA drill stem test (DST) is a procedure for isolating and testing the pressure, permeability and productive capacity of a geological formation during the drilling of a well. The test is an important measurement of pressure behaviour at the drill stem and is a valuable way of obtaining information on the formation fluid and establishing whether a well has found a commercial hydrocarbon reservoir.\n\nWorking in El Dorado, Arkansas, in the 1920s, E.C. Johnston and his brother M.O. Johnston developed the first drill stem tester and ran the first commercial drill stem test in 1926. In April 1929, the Johnston Formation Testing Corporation was granted a patent (U.S. Patent 1,709,940) and they subsequently refined the testing system in the early 1930s.\n\nIn the 1950s, Schlumberger introduced a method for testing formations using wireline. The Schlumberger formation-testing tool, placed in operation in 1953, fired a shaped charge through a rubber pad that had been expanded in the hole until it was securely fixed in the hole at the depth required. Formation fluids flowed through the perforation and connecting tubing into a container housed inside the tool. When filled, the container was closed, sealing the fluid sample at the formation pressure. The tool was then brought to the surface, where the sample could be examined. In 1956, Schlumberger acquired Johnston Testers and continues to perform drill stem tests and wireline formation tests in both open and cased holes.\n\nDrill stem testing is an oil and gas exploration procedure to isolate, stimulate and flow a downhole formation to determine the fluids present and the rate at which they can be produced. The main objective of a DST is to evaluate the commercial viability of a zones economic potential by identifying productive capacity, pressure, permeability or extent of an oil or gas reservoir. These tests can be performed in both open and cased hole environments and provide exploration teams with valuable information about the nature of the reservoir. Drill stem testing involves deploying a series of tools known as a test bottomhole assembly (BHA). A basic drill stem test BHA consist of a packer or packers, which act as an expanding plug to be used to isolate sections of the well for the testing process, valves that may be opened or closed from the surface during the test, and recorders used to document pressure during the test. In addition to packers a downhole valve is used to open and close the formation to measure reservoir characteristics such as pressure and temperature which are charted on downhole recorders within the BHA. Below are two types of BHA DST, Cased Hole which can be applied after the well has been cased, and Open Hole which may be performed before casing.\n\nPerformed after the well is cased, cased hole drill stem testing uses a retrievable production packer that is set above the zone of interest. The well is then flow tested through perforations in the casing. The two types of cased hole testing are pressure operated and mechanically operated.\n\nBecause it's performed before casing is run, open hole drill stem testing can be the most economical way to determine productive capacity, pressure, permeability or the extent of an oil or gas reservoir. The testing equipment is run into the well and the zone of interest is isolated using inflate or compression-set packers, depending on your requirements and drilling conditions.\n\nDepending on testing objectives and scope of work, drill stem testing may also be performed in combination with various other exploration and completion process such as fluid loss control and well control, closed chamber tests, well stimulation, and a combination of DST and TCP.\n\nDuring normal well drilling, drilling mud is pumped through the drill stem and out of the drill bit. In a drill stem test, the drill bit is removed and replaced with the DST tool and devices are inflated above and below the section to be tested. These devices are known as packers and are used to make a seal between the borehole wall and the drill pipe, isolating the region of interest. A valve is opened, reducing the pressure in the drill stem to surface pressure, causing fluid to flow out of the packed-off formation and up to the surface.\n\nIn a low permeability or low pressure formation, surface production may not be achieved but the volume and flow rate of fluid can still be analysed within the drill stem.\n\n\n"}
{"id": "3502601", "url": "https://en.wikipedia.org/wiki?curid=3502601", "title": "EDA database", "text": "EDA database\n\nAn EDA database is a database specialized for the purpose of electronic design automation. These application specific databases are required because general purpose databases have historically not provided enough performance for EDA applications.\n\nIn examining EDA design databases, it is useful to look at EDA tool architecture, to determine\nwhich parts are to be considered part of the design database, and which parts are the application levels.\nIn addition to the database itself, many other components are needed for a useful EDA application. Associated with a database are one or more language systems (which, although not directly part of the database, are used by EDA applications such as \"parameterized cells\" and user scripts). On top of the database are built the algorithmic engines within the tool (such as timing, placement, routing, or simulation engines ), and the highest level represents the applications built from these component blocks, such as floorplanning. The scope of the design database includes the actual design, library information, technology information, and the set of translators to and from external formats such as Verilog and GDSII.\n\nMany instances of mature design databases exist in the EDA industry, both as a basis for commercial EDA tools as well as proprietary EDA tools developed by the CAD groups of major electronics companies.\nIBM, Hewlett-Packard, SDA Systems and ECAD (now Cadence Design Systems), High Level Design Systems, and many other companies developed EDA specific databases over the last 20 years, and these continue to be the basis of IC-design systems today. Many of these systems took ideas from university research and successfully productized them. Most of the mature design databases have evolved to the point where they can represent netlist data, layout data, and the ties between the two. They are hierarchical to allow for reuse and smaller designs. They can support styles of layout from digital through pure analog and many styles of mixed-signal design.\n\nGiven the importance of a common design database in the EDA industry, the OpenAccess Coalition has been formed to develop, deploy, and support an open-sourced EDA design database with shared control. The data model presented in the OA DB provides a unified model that currently extends from structural RTL through GDSII-level mask data, and now into the reticle and wafer space. It provides a\nrich enough capability to support digital, analog, and mixed-signal design data. It provides technology data that can express foundry process design rules through at least 20 nm, contains the definitions of the layers and purposes used in the design, definitions of VIAs and routing rules, definitions of operating points used for analysis, and so on. OA makes extensive use of IC-specific data compression techniques to reduce the memory footprint, to address the size, capacity, and performance problems of previous DBs. As of 2007, OA is the only modern IC database where the implementation is publicly available.\n\nThe Milkyway database was originally developed by Avanti Corporation, which has since been acquired by Synopsys. It was first released in 1997. Milkyway is the database underlying most of Synopsys' physical design tools:\n\nMilkyway stores topological, parasitic and timing data. Having been used to design thousands of chips, Milkyway is very stable and production worthy. Milkyway is known to be written in C. Its internal implementation is not available outside Synopsys, so no comments may be made about the implementation.\n\nAt the request of large customers such as Texas Instruments, Avanti released the MDX C-API in 1998. This enables the customers' CAD developers to create plugins that add custom functionality to Milkyway tools (chiefly Astro).\n\nMDX allows fairly complete access to topological data in Milkyway, but does not support timing or RC parasitic data.\n\nIn early 2003, Synopsys (which acquired Avanti) opened Milkyway through the Milkyway Access Program (MAP-In). Any EDA company may become a MAP-in member for free (Synopsys customers must use MDX). Members are provided the means to interface their software to Milkyway using C, Tcl, or Scheme. The Scheme interface is deprecated in favor of TCL. IC Compiler supports only TCL.\n\nThe MAP-in C-API enables a non-Synopsys application to read and write Milkyway databases. Unlike MDX, MAP-in does not permit the creation of a plugin that can be used from within Synopsys Milkyway tools.\nMAP-in does not support access to timing or RC parasitic data. MAP-in also lacks direct support of certain geometric objects.\n\nMAP-in includes Milkyway Development Environment (MDE). MDE is a GUI application used to develop TCL and Scheme interfaces and diagnose problems. Its major features include:\n\nAnother significant design database is \"Falcon\", from Mentor Graphics. This database was one of the first in the industry written in C++. Like Milkyway is for Synopsys, Falcon seems to be a stable and mature platform for Mentor’s IC products. Again, the implementation is not publicly available, so little can be said about its features or performance relative to other industry standards.\n\nMagma Design Automation’s database is not just a disk format with an API, but is an entire system built around their DB as a central data structure. Again, since the details of the system are not publicly available, a direct comparison of features or performance is not possible. Looking at the capabilities of the Magma tools would indicate that this DB has a similar functionality to OpenAccess, and may be capable of representing behavioral (synthesis input) information.\n\nAn EDA specific database is expected to provide many basic constructs and services. Here is a brief and incomplete list of what is needed:\n\n"}
{"id": "359729", "url": "https://en.wikipedia.org/wiki?curid=359729", "title": "Electronic Industries Alliance", "text": "Electronic Industries Alliance\n\nThe Electronic Industries Alliance (EIA; until 1997 Electronic Industries Association) was a standards and trade organization composed as an alliance of trade associations for electronics manufacturers in the United States. They developed standards to ensure the equipment of different manufacturers was compatible and interchangeable. The EIA ceased operations on February 11, 2011, but the former sectors continue to serve the constituencies of EIA.\n\nIn 1924, United States-based radio manufacturers formed a trade group called the \"Associated Radio Manufacturers\". Later that same year, the group renamed itself the \"Radio Manufacturers Association\" (\"RMA\"). Over time, new electronic technologies brought new members and name changes, including \"Radio Television Manufacturers Association\" (\"RTMA\") (1950), \"Radio Electronics Television Manufacturers Association\" (\"RETMA\") (1953) and \"Electronic Industries Association\" (\"EIA\") (1957). The group renamed itself for the last time in 1997, when the Electronic Industries Association became the \"Electronic Industries Alliance\" (\"EIA\"), reflecting the changing membership of the group, including non-manufacturer members.\n\nThe organization's headquarters were in Arlington, Virginia. The EIA divided its activities into the following sectors:\n\nThe EIA announced in 2007 that it would be dissolved into its constituent divisions, and transferred operations soon after. The Alliance formally ceased to exist on February 11, 2011. EIA designated ECA to continue to develop standards for interconnect, passive and electro-mechanical (IP&E) electronic components under the ANSI-designation of EIA standards. All other electronic components standards will be managed by their respective sectors.\n\nThe ECA merged with the National Electronic Distributors Association (NEDA) in 2011 to form the Electronic Components Industry Association (ECIA). However, the EIA standards brand will continue for IP&E standards within ECIA.\n\n \nWith the changing names of the EIA, the naming convention of the standards was also adapted. For example, a standard defining serial communication between computers and modems e.g. was originally drafted as a Recommended Standard, thus the \"RS\" RS-232. Later it was taken over by the EIA as \"EIA-232\". Later this standard was managed by the TIA and the name was changed to the current \"TIA-232\". Because the EIA was accredited by ANSI to help develop standards in its areas, these standards are often described as e.g. \"ANSI TIA-232\" (or formerly as \"ANSI EIA/TIA-232\"). As currently authorized, any ANSI standard designated at ANSI EIA-xxx is developed or managed by ECA (and, in the future, ECIA).\n\n"}
{"id": "9790", "url": "https://en.wikipedia.org/wiki?curid=9790", "title": "Electronic data interchange", "text": "Electronic data interchange\n\nElectronic data interchange (EDI) is the concept of businesses electronically communicating information that was traditionally communicated on paper, such as purchase orders and invoices. Technical standards for EDI exist to facilitate parties transacting such instruments without having to make special arrangements.\n\nEDI has existed at least since the early 70s, and there are many EDI standards (including X12, EDIFACT, ODETTE, etc.), some of which address the needs of specific industries or regions. It also refers specifically to a family of standards. In 1996, the National Institute of Standards and Technology defined electronic data interchange as \"the computer-to-computer interchange of strictly formatted messages that represent documents other than monetary instruments. EDI implies a sequence of messages between two parties, either of whom may serve as originator or recipient. The formatted data representing the documents may be transmitted from originator to recipient via telecommunications or physically transported on electronic storage media.\" It distinguished mere electronic communication or data exchange, specifying that \"in EDI, the usual processing of received messages is by computer only. Human intervention in the processing of a received message is typically intended only for error conditions, for quality review, and for special situations. For example, the transmission of binary or textual data is not EDI as defined here unless the data are treated as one or more data elements of an EDI message and are not normally intended for human interpretation as part of on-line data processing.\" In short, EDI can be defined as the transfer of structured data, by agreed message standards, from one computer system to another without human intervention.\n\nLike many other early information technologies, EDI was inspired by developments in military logistics. The complexity of the 1948 Berlin airlift required the development of concepts and methods to exchange, sometimes over a 300 baud teletype modem, vast quantities of data and information about transported goods. These initial concepts later shaped the first TDCC (Transportation Data Coordinating Committee) standards in the US. Among the first integrated systems using EDI were Freight Control Systems. One such real-time system was the London Airport Cargo EDP Scheme (LACES) at Heathrow Airport, London, UK, in 1971. Implementing the direct trader input (DTI) method, it allowed forwarding agents to enter information directly into the Customs processing system reducing the time for clearance. The increase of maritime traffic and problems at Customs similar to those experienced at Heathrow Airport led to the implementation of DTI systems in individual ports or group of ports in the 1980s.\n\nEDI provides a technical basis for automated commercial \"conversations\" between two entities, either internal or external. The term EDI encompasses the entire electronic data interchange process, including the transmission, message flow, document format, and software used to interpret the documents. However, EDI standards describe the rigorous format of electronic documents, and the EDI standards were designed by the implementers, initially in the Automotive industry, to be independent of communication and software technologies. EDI can be transmitted using any methodology agreed to by the sender and recipient. This includes a variety of technologies, including modem (asynchronous and synchronous), FTP, e-mail, HTTP, AS1, AS2, AS4 etc. It is important to differentiate between the EDI documents and the methods for transmitting them. When they compared the synchronous protocol 2400 bit/s modems, CLEO devices, and value-added networks used to transmit EDI documents to transmitting via the Internet, some people equated the non-Internet technologies with EDI and predicted erroneously that EDI itself would be replaced along with the non-Internet technologies. These non-internet transmission methods are being replaced by Internet protocols such as FTP, HTTP, telnet, and e-mail, but the EDI documents themselves still remain.\n\nAs more trading partners use the Internet for transmission, standards have emerged. In 2002, the IETF published RFC 3335, offering a standardized, secure method of transferring EDI data via e-mail. On July 12, 2005, an IETF working group ratified RFC4130 for MIME-based HTTP EDIINT (a.k.a. AS2) transfers, and the IETF has prepared a similar RFC for FTP transfers (a.k.a. AS3). EDI via web services (a.k.a. AS4) has also been standardised by the OASIS standards body. While some EDI transmission has moved to these newer protocols, the providers of value-added networks remain active.\n\nEDI documents generally contain the same information that would normally be found in a paper document used for the same organizational function. For example, an EDI 940 ship-from-warehouse order is used by a manufacturer to tell a warehouse to ship product to a retailer. It typically has a 'ship-to' address, a 'bill-to' address, and a list of product numbers (usually a UPC) and quantities. Another example is the set of messages between sellers and buyers, such as request for quotation (RFQ), bid in response to RFQ, purchase order, purchase order acknowledgement, shipping notice, receiving advice, invoice, and payment advice. However, EDI is not confined to just business data related to trade but encompasses all fields such as medicine (e.g., patient records and laboratory results), transport (e.g., container and modal information), engineering and construction, etc. In some cases, EDI will be used to create a new business information flow (that was not a paper flow before). This is the case in the Advanced Shipment Notification (ASN) which was designed to inform the receiver of a shipment, the goods to be received and how the goods are packaged.\n\nSome major sets of EDI standards:\nMany of these standards first appeared in the early to mid 1980s. The standards prescribe the formats, character sets, and data elements used in the exchange of business documents and forms. The complete X12 Document List includes all major business documents, including purchase orders and invoices.\n\nThe EDI standard prescribes mandatory and optional information for a particular document and gives the rules for the structure of the document. The standards are like building codes. Just as two kitchens can be built \"to code\" but look completely different, two EDI documents can follow the same standard and contain different sets of information. For example, a food company may indicate a product's expiration date while a clothing manufacturer would choose to send color and size information.\nOrganizations that send or receive documents between each other are referred to as \"trading partners\" in EDI terminology. The trading partners agree on the specific information to be transmitted and how it should be used. This is done in human readable specifications (also called Message Implementation Guidelines). While the standards are analogous to building codes, the specifications are analogous to blue prints. (The specification may also be called a \"mapping,\" but the term mapping is typically reserved for specific machine-readable instructions given to the translation software.) Larger trading \"hubs\" have existing Message Implementation Guidelines which mirror their business processes for processing EDI and they are usually unwilling to modify their EDI business practices to meet the needs of their trading partners. Often in a large company these EDI guidelines will be written to be generic enough to be used by different branches or divisions and therefore will contain information not needed for a particular business document exchange. For other large companies, they may create separate EDI guidelines for each branch/division.\n\nTrading partners are free to use any method for the transmission of documents. Furthermore, they can either interact directly, or through an intermediary.\n\nTrading partners can connect directly to each other. For example, an automotive manufacturer might maintain a modem-pool that all of its hundreds of suppliers are required to dial into to perform EDI. However, if a supplier does business with several manufacturers, it may need to acquire a different modem (or VPN device, etc.) and different software for each one.\n\nTo address the limitations in peer-to-peer adoption of EDI, VANs (value-added networks) were established. A VAN acts as a regional post office. It receives transactions, examines the 'from' and the 'to' information, and routes the transaction to the final recipient. VANs may provide a number of additional services, e.g. retransmitting documents, providing third party audit information, acting as a gateway for different transmission methods, and handling telecommunications support. Because of these and other services VANs provide, businesses frequently use a VAN even when both trading partners are using Internet-based protocols. Healthcare clearinghouses perform many of the same functions as a VAN, but have additional legal restrictions.\n\nVANs may be operated by various entities:\n\nAs more organizations connected to the Internet, eventually most or all EDI was pushed onto it. Initially, this was through ad hoc conventions, such as unencrypted FTP of ASCII text files to a certain folder on a certain host, permitted only from certain IP addresses. However, the IETF has published several informational documents (the \"Applicability Statements\"; see below under Protocols) describing ways to use standard Internet protocols for EDI.\n\nAs of 2002, Walmart have pushed the AS2 for EDI. Because of its significant presence in the global supply chain, AS2 have become a commonly adopted approach for EDI.\n\n\"EDI translation software\" provides the interface between internal systems and the EDI format sent/received. For an \"inbound\" document the EDI solution will receive the file (either via a Value Added Network or directly using protocols such as FTP or AS2), take the received EDI file (commonly referred to as an \"envelope\"), validate that the trading partner who is sending the file is a valid trading partner, that the structure of the file meets the EDI standards, and that the individual fields of information conform to the agreed upon standards. Typically the translator will either create a file of either fixed length, variable length or XML tagged format or \"print\" the received EDI document (for non-integrated EDI environments). The next step is to convert/transform the file that the translator creates into a format that can be imported into a company's back-end business system or ERP. This can be accomplished by using a custom program, an integrated proprietary \"mapper\" or an integrated standards based graphical \"mapper,\" using a standard data transformation language such as XSLT. The final step is to import the transformed file (or database) into the company's back-end system.\n\nFor an \"outbound\" document the process for integrated EDI is to export a file (or read a database) from a company's information systems and transform the file to the appropriate format for the translator. The translation software will then \"validate\" the EDI file sent to ensure that it meets the standard agreed upon by the trading partners, convert the file into \"EDI\" format (adding the appropriate identifiers and control structures) and send the file to the trading partner (using the appropriate communications protocol).\n\nAnother critical component of any EDI translation software is a complete \"audit\" of all the steps to move business documents between trading partners. The audit ensures that any transaction (which in reality is a business document) can be tracked to ensure that they are not lost. In case of a retailer sending a Purchase Order to a supplier, if the Purchase Order is \"lost\" anywhere in the business process, the effect is devastating to both businesses. To the supplier, they do not fulfill the order as they have not received it thereby losing business and damaging the business relationship with their retail client. For the retailer, they have a stock outage and the effect is lost sales, reduced customer service and ultimately lower profits.\n\nIn EDI terminology \"inbound\" and \"outbound\" refer to the direction of transmission of an EDI document in relation to a particular system, not the direction of merchandise, money or other things represented by the document. For example, an EDI document that tells a warehouse to perform an outbound shipment is an inbound document in relation to the warehouse computer system. It is an outbound document in relation to the manufacturer or dealer that transmitted the document.\n\nEDI and other similar technologies save a company money by providing an alternative to, or replacing, information flows that require a great deal of human interaction and paper documents. Even when paper documents are maintained in parallel with EDI exchange, e.g. printed shipping manifests, electronic exchange and the use of data from that exchange reduces the handling costs of sorting, distributing, organizing, and searching paper documents. EDI and similar technologies allow a company to take advantage of the benefits of storing and manipulating data electronically without the cost of manual entry. Another advantage of EDI is the opportunity to reduce or eliminate manual data entry errors, such as shipping and billing errors, because EDI eliminates the need to rekey documents on the destination side. One very important advantage of EDI over paper documents is the speed in which the trading partner receives and incorporates the information into their system thus greatly reducing cycle times. For this reason, EDI can be an important component of just-in-time production systems.\n\nAccording to the 2008 Aberdeen report \"A Comparison of Supplier Enablement around the World\", only 34% of purchase orders are transmitted electronically in North America. In EMEA, 36% of orders are transmitted electronically and in APAC, 41% of orders are transmitted electronically. They also report that the average paper requisition to order costs a company $37.45 in North America, $42.90 in EMEA and $23.90 in APAC. With an EDI requisition to order costs are reduced to $23.83 in North America, $34.05 in EMEA and $14.78 in APAC.\n\nThere are a few barriers to adopting electronic data interchange. One of the most significant barriers is the accompanying business process change. Existing business processes built around paper handling may not be suited for EDI and would require changes to accommodate automated processing of business documents. For example, a business may receive the bulk of their goods by 1 or 2 day shipping and all of their invoices by mail. The existing process may therefore assume that goods are typically received before the invoice. With EDI, the invoice will typically be sent when the goods ship and will therefore require a process that handles large numbers of invoices whose corresponding goods have not yet been received.\n\nAnother significant barrier is the cost in time and money in the initial set-up. The preliminary expenses and time that arise from the implementation, customization and training can be costly. It is important to select the correct level of integration to match the business requirement. For a business with relatively few transactions with EDI-based partners, it may make sense for businesses to implement inexpensive \"rip and read\" solutions, where the EDI format is printed out in human-readable form and people, rather than computers, respond to the transaction. Another alternative is outsourced EDI solutions provided by EDI \"Service Bureaus\". For other businesses, the implementation of an integrated EDI solution may be necessary as increases in trading volumes brought on by EDI force them to re-implement their order processing business processes.\n\nThe key hindrance to a successful implementation of EDI is the perception many businesses have of the nature of EDI. Many view EDI from the technical perspective that EDI is a data format; it would be more accurate to take the business view that EDI is a system for exchanging business documents with external entities, and integrating the data from those documents into the company's internal systems. Successful implementations of EDI take into account the effect externally generated information will have on their internal systems and validate the business information received. For example, allowing a supplier to update a retailer's Accounts Payable system without appropriate checks and balances would put the company at significant risk. Businesses new to the implementation of EDI must understand the underlying business process and apply proper judgment.\n\nBelow are common EDI acknowledgement\n\n\nFormats\n\n\n\n"}
{"id": "40623157", "url": "https://en.wikipedia.org/wiki?curid=40623157", "title": "Electrostatic discharge materials", "text": "Electrostatic discharge materials\n\nElectrostatic discharge materials (ESD materials) are plastics that reduce static electricity to protect electrostatic-sensitive devices (ESD) or contain flammable liquids or gases.\n\nESD materials are generally subdivided into categories with related properties: Anti-Static, Conductive, and Dissipative.\n\nConductive materials have a low electrical resistance, thus electrons flow easily across the surface or through these materials. Charges go to ground or to another conductive object that the material contacts.\n\nDissipative materials allow the charges to flow to ground more slowly in a more controlled manner than with conductive materials.\n\nAnti-static materials are generally referred to as any material which inhibits triboelectric charging. This kind of charging is the buildup of an electric charge by the rubbing or contact with another material.\n\nInsulative materials prevent or limit the flow of electrons across their surface or through their volume. Insulative materials have a high electrical resistance and are difficult to ground, thus are not ESD materials. Static charges remain in place on these materials for a very long time.\n\n\n\n"}
{"id": "45679510", "url": "https://en.wikipedia.org/wiki?curid=45679510", "title": "Emily White", "text": "Emily White\n\nEmily White is an American tech executive, most notable as a former chief operating officer at Snapchat Inc, and current board member of Hyperloop One.\n\nIn the past White was a Director of Business Operations at Instagram, as well as Director, Local at Facebook Inc. and Director, Asia Pacific - Latin America Online Sales & Operations at Google Inc. White obtained her bachelor's degree at Vanderbilt University. Her boardroom appointments include Lululemon Athletica Inc and the National Center for Women in I.T. In September 2015, Emily joined Hyperloop One as a board observer.\n\nWhite was hired by Snapchat Inc as a business strategist and advisor. She served as the COO from 2013 till March 2015. The public announcement of her leaving the company occurred days after Snapchat received funding from Alibaba Group.\n"}
{"id": "41281070", "url": "https://en.wikipedia.org/wiki?curid=41281070", "title": "Entropic Communications", "text": "Entropic Communications\n\nEntropic Communications is a provider of semiconductor solutions for the connected home. Founded in 2001, the company is headquartered in San Diego, California, USA, and maintains offices worldwide. The fabless semiconductor company is recognized for inventing the MoCA® (Multimedia over Coax Alliance) home networking technology, creating Direct Broadcast Satellite (DBS) Outdoor Unit (ODU) single-wire technology, and developing the industry’s first ARM® processor and OpenGL graphics Set-top box (STB), System-on-a-Chip (SoC). Entropic completed its initial public offering on December 7, 2007, listing on the NASDAQ exchange under the ticker symbol ENTR.\n\nEntropic was acquired by MaxLinear for a total consideration of $287 million on February 3, 2015.\n\nEntropic is the only pure-play platform semiconductor company in the connected home market and offers a diverse portfolio of integrated circuit (IC) technology and advanced platform software solutions:\n\n\nIn 2005, the company began its global expansion with the opening of its Asia regional headquarters in Hong Kong. The company expanded its Asia-Pacific presence in Korea in 2010 and in Japan in 2011.\n\nIn March of 2014, Entropic's former CEO Patrick Henry was charged with three misdemeanors, two of which were dropped, and one reduced to an infraction. The charges stemmed from an altercation at Sundance Film Festival involving reality TV star Ariane Bellamar. On June 19th, 2014 he was found guilty of the assault charge; his sentence consisted of a $400 fine, 40 hours of community service, and counseling for alcohol and anger management. Patrick Henry was not terminated from his position. In September of 2014 Entropic hired Barclays to explore the possibilities of selling the business, which concluded with a sale to MaxLinear on February 3, 2015. Patrick Henry resigned as CEO prior to the sale, in November of 2014.\n\nUnder Patrick Henry's leadership, Entropic Communications completed its first initial public offering and introduced several market leading technologies. On November 10th, 2014, a decision by the Board of Directors stated that it was the right time for a leadership transition, leading to Dr. Ted Tewksbury replacing Patrick Henry as interim President and Chief Executive Officer.\n\n"}
{"id": "9687666", "url": "https://en.wikipedia.org/wiki?curid=9687666", "title": "Evaporation pond", "text": "Evaporation pond\n\nEvaporation ponds are artificial ponds with very large surface areas that are designed to efficiently evaporate water by sunlight and exposure to the ambient temperatures.\n\nEvaporation ponds have several uses. Salt evaporation ponds produce salt from seawater. They are also used to dispose of brine from desalination plants. Mines use ponds to separate ore from water. Evaporation ponds at contaminated sites remove the water from hazardous waste, which greatly reduces its weight and volume and allows the waste to be more easily transported, treated and stored.\n\nEvaporation ponds can also be used to evaporate the precipitation that falls on a contaminated site. The contaminants that the water picks up on the ground are left behind after it evaporates. This prevents the contamination from spreading further down the watershed.\n\nEvaporation ponds are used to prevent pesticides, fertilizers and salts from agricultural wastewater from contaminating the water bodies they would flow into. In California, selenium in agricultural wastewater has been especially problematic, causing birth defects in waterfowl.\n\n\n"}
{"id": "36010480", "url": "https://en.wikipedia.org/wiki?curid=36010480", "title": "Gary Gadget", "text": "Gary Gadget\n\nGary Gadget (called \"Mulle Meck\" in Sweden and \"Masa Mainio\" in Finland) is a series of computer games originally published in Sweden by Levande Böcker. The series debuted in 1997. Since then, five games have been published, one of which has been translated into English. There are also translations into other languages. In Germany, four games were published by the company \"Terzio Verlag\". Gary Gadget is called \"Willy Werkel\" in German. All five games have been translated into Dutch and published by the company\" Transposia\". Gary Gadget is called \"Miel Monteur\" in Dutch.\n\nThe computer games are based on the Swedish children's book series of the same name, by George Johansson and Jens Ahlbom.\n\nAll five parts are available for Windows, four of them also for Apple Mac, except for the computer game \"Bygg flygplan med Mulle Meck\".\n\nIn all of the five published games about \"Gary Gadget\" there are some elements that always persist. Depending on the game you are playing the objective of the game is to build a certain thing, ranging from cars and ships to spacecraft. You always start from Gary's base of operations, which differ from game to game, where the actual building takes place. You build through a drag and drop-system by dragging fitting parts from Gary's storage house onto a template of the thing that you are building.\n\nThe games don´t have a final objective. Instead, you gather parts, keep building new things and explore the world.\n\nThe computer games are about a man named \"Gary Gadget\", a general handyman who throughout the five games builds cars, ships, planes, houses and space ships.\n\nIn the first video game, \"Bygg bilar med Mulle Meck\", published in 1997, Mulle builds cars from various parts and junk that he finds and drives through the town. You can find new parts at random on the ground while driving through the in-game map. You can also receive parts from Gary's friend \"Figge Ferrum\", the scrap dealer, who gives Gary car parts at random instances in the game. Figge also often loses his dog which you can bring back to him. Gary gets orders from different people during the video game, which the player should fulfill.\n\nThe first game is the only one which has been published in English so far.\n\nIn the second game \"Bygg båtar med Mulle Meck\", published in 1998, Gary builds ships. In the beginning, the player learns that Gary has driven down a road in a car and wanted to know how to go on. So Gary made a rowing boat and paddled along. Gary reaches a shipyard. There, the owner asks him to take care of the yard, while she goes sailing around the world. In this game, Gary receives again tasks which the player must fulfill. The parts for building his ships are bought from \"Doris Digital\".\n\nIn the third game \"Bygg flygplan med Mulle Meck\", published in 2000, the player gets to know that Gary has found the abandoned hangar of two airplanes constructors and wants to try that trade. Again he receives his parts from \"Doris Digital\". Gary flies to visit his friends and like in the previous episodes he gets orders which the player must complete.\n\nIn the fourth part \"Bygg hus med Mulle Meck\", published in 2002, Gary Gadget finds his house in debris after a storm caused a tree to fall on it. He promptly starts to builds houses. In this instance you can travel around the game map via both car and boat.\n\nThe fifth episode \"Upptäck rymden med Mulle Meck\", which was published in 2004, starts with Gary contemplating that while he had built houses in \"Bygg hus med Mulle Meck\" he had also been sitting on the porch of his house, watching the sky. Thereupon he became eager to fly into space. In this game, the player not only flies into outer space but also completes tasks down on Earth. Gary visits his friends by car. Of course he also builds his own spacecraft.\n\nThe German \"c’t\" magazine rated the game \"Bygg bilar med Mulle Meck\" thus: \"\"Autos bauen mit Willi Werkel\" ist nett gemacht, kann aber eigene Bastelerfahrungen nicht ersetzen. [\"Bygg bilar med Mulle Meck\" is made nicely but cannot make up for one's own construction experience.]\"\n\"Netzwelt.de\" tested \"Bygg flygplan med Mulle Meck\" on 7 February 2010, concluding that it was well suitable for children at primary school age.\n\n\"c’t\" tested the episodes \"Bygg flygplan med Mulle Meck\" in issue 17/2001, \"Bygg hus med Mulle Meck\" in 23/2003 and \"Upptäck rymden med Mulle Meck\" in 24/2005.\n\n"}
{"id": "7442757", "url": "https://en.wikipedia.org/wiki?curid=7442757", "title": "Gazaz", "text": "Gazaz\n\nGazaz (Pty) Ltd is one of the largest South African computer hardware manufacturers and distributors. It distributes computer hardware throughout Africa as well. The company's signature lines are ProView, Chronos, JetWay, and DFi.\n\n"}
{"id": "14440263", "url": "https://en.wikipedia.org/wiki?curid=14440263", "title": "Horst H. Berger", "text": "Horst H. Berger\n\nHorst H. Berger (born March 30, 1933) is a German electrical engineer noted for his contributions to semiconductor technologies for integrated circuits.\n\nBerger was born in Liegnitz (Legnica), Lower Silesia, and received the Vordiplom. from the Technische Hochschule of Dresden, then worked at the IBM Laboratories in Böblingen. Afterwards he became a researcher and teacher at the Technical University of Berlin.\n\nTogether with Siegfried K. Wiedmann, Berger received the 1977 IEEE Morris N. Liebmann Memorial Award \"for the invention and exploration of the Merged Transistor Logic, MTL\".\n\n\n"}
{"id": "5333892", "url": "https://en.wikipedia.org/wiki?curid=5333892", "title": "In situ resource utilization", "text": "In situ resource utilization\n\nIn space exploration, in situ resource utilization (ISRU) is defined as \"the collection, processing, storing and use of materials encountered in the course of human or robotic space exploration that replace materials that would otherwise be brought from Earth.\"\nISRU is the practice of leveraging resources found or manufactured on other astronomical objects (the Moon, Mars, asteroids, etc.) to fulfill or enhance the requirements and capabilities of a space mission.\n\nISRU can provide materials for life support, propellants, construction materials, and energy to a spacecraft payloads or space exploration crews.\nIt is now very common for spacecraft and robotic planetary surface mission to harness the solar radiation found \"in situ\" in the form of solar panels. The use of ISRU for material production has not yet been implemented in a space mission, though several field tests in the late 2000s demonstrated various lunar ISRU techniques in a relevant environment.\n\nISRU has long been considered as a possible avenue for reducing the mass and cost of space exploration architectures, in that it may be a way to drastically reduce the amount of payload that must be launched from Earth in order to explore a given planetary body.\nAccording to NASA, \"in-situ resource utilisation will enable the affordable establishment of extraterrestrial exploration and operations by minimizing the materials carried from Earth.\"\n\nIn the context of ISRU water is most often sought directly as fuel or as feedstock for fuel production. Applications include its use in life support either directly by drinking, for growing food, producing oxygen, or numerous other industrial processes. All of which require a ready supply of water in the environment and the equipment to extract it. Such extraterrestrial water has been discovered in a variety of forms throughout the solar system, and a number of potential water extraction technologies have been investigated. For water that is chemically bound to regolith, solid ice, or some manner of permafrost, sufficient heating can recover the water. However this is not as easy as it appears because ice and permafrost can often be harder than plain rock, necessitating laborious mining operations. Where there is some level of atmosphere, such as on Mars, water can be extracted directly from the air using a simple process such as WAVAR. Another possible source of water is deep aquifers kept warm by Mars's latent geological heat, which can be tapped to provide both water and geothermal power.\n\nRocket propellant from water ice has also been proposed for the Moon, mainly from ice that has been found at the poles. The likely difficulties include working at extremely low temperatures and extraction from the regolith. Most schemes electrolyse the water and form hydrogen and oxygen and liquify and cryogenically store them. This requires large amounts of equipment and power to achieve. Alternatively it is possible to simply heat the water in a nuclear or solar thermal rocket, which seems to give very much more mass delivered to low Earth orbit (LEO) in spite of the much lower specific impulse, for a given amount of equipment.\n\nThe monopropellant hydrogen peroxide (HO) can be made from water on Mars and the Moon.\n\nAluminum as well as other metals have been proposed for use as rocket propellant made using lunar resources, and proposals include reacting the aluminum with water. For Mars, methane propellant can be manufactured via the Sabatier process.\n\nIt has long been suggested that solar cells could be produced from the materials present in lunar soil. Silicon, aluminium, and glass, three of the primary materials required for solar cell production, are found in high concentrations in lunar soil and can be utilised to produce solar cells. In fact, the native vacuum on the lunar surface provides an excellent environment for direct vacuum deposition of thin-film materials for solar cells.\n\nSolar arrays produced on the lunar surface can be used to support lunar surface operations as well as satellites off the lunar surface. Solar arrays produced on the lunar surface may prove more cost effective than solar arrays produced and shipped from Earth, but this trade depends heavily on the location of the particular application in question.\n\nAnother potential application of lunar-derived solar arrays is providing power to Earth. In its original form, known as the solar power satellite, the proposal was intended as an alternate power source for Earth. Solar cells would be shipped to Earth orbit and assembled, the power being transmitted to Earth via microwave beams. Despite much work on the cost of such a venture, the uncertainty lay in the cost and complexity of fabrication procedures on the lunar surface.\n\nAsteroid mining could also involve extraction of metals for construction material in space, which may be more cost-effective than bringing such material up out of Earth's deep gravity well, or that of any other large body like the Moon or Mars. Metallic asteroids contain huge amounts of siderophilic metals, including precious metals.\n\nThe colonisation of planets or moons will require obtaining local building materials, such as regolith. For example, studies employing artificial Mars soil mixed with epoxy resin and tetraethoxysilane, produce high enough values of strength, resistance, and flexibility parameters.\n\nISRU research for Mars is focused primarily on providing rocket propellant for a return trip to Earth — either for a manned or a sample return mission — or for use as fuel on Mars. Many of the proposed techniques utilise the well-characterised atmosphere of Mars as feedstock. Since this can be easily simulated on Earth, these proposals are relatively simple to implement, though it is by no means certain that NASA or the ESA will favour this approach over a more conventional direct mission.\n\nA typical proposal for ISRU is the use of a Sabatier reaction, , in order to produce methane on the Martian surface, to be used as a propellant. Oxygen is liberated from the water by electrolysis, and the hydrogen recycled back into the Sabatier reaction. The usefulness of this reaction is that—, when the availability of water on Mars was less scientifically demonstrated—only the hydrogen (which is light) was thought to need to be brought from Earth.\n\n, SpaceX is currently developing the technology for a Mars propellant plant that will use a variation on what is described in the previous paragraph. Rather than transporting hydrogen from Earth to use in making the methane and oxygen, they intend to mine the requisite water from subsurface water ice that is now known to be abundant across much of the Martian surface, produce and then store the post-Sabatier reactants, and then use it as propellant for return flights of their \"Interplanetary Spaceship\" no earlier than 2023.\n\nA similar reaction proposed for Mars is the reverse water gas shift reaction, . This reaction takes place rapidly in the presence of an iron-chrome catalyst at 400 Celsius, and has been implemented in an Earth-based testbed by NASA. Again, oxygen is recycled from the water by electrolysis, and the reaction only needs a small amount of hydrogen from Earth. The net result of this reaction is the production of oxygen, to be used as the oxidizer component of rocket fuel.\n\nAnother reaction proposed for the production of oxygen and fuel is the electrolysis of the atmospheric carbon dioxide, \n\nMore recently, it has been proposed the in situ production of oxygen, hydrogen and CO from the martian hematite deposits via a two-step thermochemical /HO splitting process, and specifically in the magnetite/wustite redox cycle. Although thermolysis is the most direct, one-step process for splitting molecules, it is neither practical nor efficient in the case of either HO or CO. This is because the process requires a very high temperature (> 2500 C) to achieve a meaningful dissociation fraction. This poses problems in finding suitable reactor materials, losses due to vigorous product recombination, and excessive aperture radiation losses when concentrated solar heat is used. The magnetite/wustite redox cycle was first proposed for solar application on earth by Nakamura, and was one of the first used for solar-driven two-step water splitting. In this cycle, water reacts with wustite (FeO) to form magnetite (FeO) and hydrogen. The summarised reactions in this two-step splitting process are as follows:\n\nand the obtained FeO is used for the thermal splitting of water or CO :\n\nThis process is repeated cyclically. The above process results in a substantial reduction in the thermal input of energy if compared with the most direct, one-step process for splitting molecules.\n\nHowever, the process needs wustite (FeO) to start the cycle, but on Mars there is no wustite or at least not in significant amounts. Nevertheless, wustite can be easily obtained by reduction of hematite (FeO) which is an abundant material on Mars, being specially conspicuous the strong hematite deposits located at Terra Meridiani.\nThe intention of wustite from the hematite -abundantly available on Mars, is an industrial process well-known on earth, and us performed by the following two main reduction reactions, namely: \n\nMars Surveyor 2001 Lander MIP (Mars ISPP Precursor) was to demonstrate manufacture of oxygen from the atmosphere of Mars, and test solar cell technologies and methods of mitigating the effect of Martian dust on the power systems. The proposed Mars 2020 rover mission might include ISRU technology demonstrator that would extract CO from the atmosphere and produce O for rocket fuel.\n\nIt has been suggested that buildings on Mars could be made from basalt as it has good insulating properties. An underground structure of this type would be able to protect life forms against radiation exposure.\n\nAll of the resources required to make plastics exist on Mars. Many of these complex reactions are able to be completed from the gases harvested from the martian atmosphere. Traces of free oxygen, carbon monoxide, water and methane are all known to exist. Hydrogen and oxygen can be made by the electrolysis of water, carbon monoxide and oxygen by the electrolysis of carbon dioxide and methane by the Sabatier reaction of carbon dioxide and hydrogen. These basic reactions provide the building blocks for more complex reaction series which are able to make plastics. Ethylene is used to make plastics such as polyethylene and polypropylene and can be made from carbon monoxide and hydrogen, \n\nThe Moon possesses abundant raw materials that are potentially relevant to a hierarchy of future applications, beginning with the use of lunar materials to facilitate human activities on the Moon itself and progressing to the use of lunar resources to underpin a future industrial capability within the Earth-Moon system.\n\nThe lunar highland material anorthite can be used as aluminium ore. Smelters can produce pure aluminium, calcium metal, oxygen and silica glass from anorthite. Raw anorthite is also good for making fiberglass and other glass and ceramic products. One particular processing technique is to use fluorine brought from Earth as potassium fluoride to separate the raw materials from the lunar rocks.\n\nOver twenty different methods have been proposed for oxygen extraction on the Moon. Oxygen is often found in iron rich lunar minerals and glasses as iron oxide. The oxygen can be extracted by heating the material to temperatures above 900 °C and exposing it to hydrogen gas. The basic equation is: FeO + H → Fe + HO. This process has recently been made much more practical by the discovery of significant amounts of hydrogen-containing regolith near the Moon's poles by the Clementine spacecraft.\n\nLunar materials may also be valuable for other uses. It has also been proposed to use lunar regolith as a general construction material, through processing techniques such as sintering, hot-pressing, liquification, and the cast basalt method. Cast basalt is used on Earth for construction of, for example, pipes where a high resistance to abrasion is required. Cast basalt has a very high hardness of 8 Mohs (diamond is 10 Mohs) but is also susceptible to mechanical impact and thermal shock which could be a problem on the Moon.\n\nGlass and glass fiber are straightforward to process on the Moon and Mars, and it has been argued that the glass is optically superior to that made on the Earth because it can be made anhydrous. Successful tests have been performed on Earth using two lunar regolith simulants MLS-1 and MLS-2. Basalt fibre has also been made from lunar regolith simulators.\n\nIn August 2005, NASA contracted for the production of 16 tonnes of simulated lunar soil,\nor \"Lunar Regolith Simulant Material.\"\nThis material is now commercially available for research on how lunar soil could be utilized \"in situ\".\n\nOther proposals are based on Phobos and Deimos. These moons are in reasonably high orbits above Mars, have very low escape velocities, and unlike Mars have return delta-v's from their surfaces to LEO which are less than the return from the Moon.\n\nCeres is further out than Mars, with a higher delta-v, but launch windows and travel times are better, and the surface gravity is just 0.028 g, with a very low escape velocity of 510 m/s. Researchers have speculated that the interior configuration of Ceres includes a water-ice-rich mantle over a rocky core.\n\nNear Earth Asteroids and bodies in the asteroid belt could also be sources of raw materials for ISRU.\n\nProposals have been made for \"mining\" for rocket propulsion, using what is called a Propulsive Fluid Accumulator. Atmospheric gases like oxygen and argon could be extracted from the atmosphere of planets like the Earth, Mars, and the outer Gas Giants by Propulsive Fluid Accumulator satellites in low orbit.\n\nIn October 2004, NASA’s Advanced Planning and Integration Office commissioned an ISRU capability roadmap team.\nThe team's report, along with those of 14 other capability roadmap teams, were published May 22, 2005.\nThe report identifies seven ISRU capabilities:\n(i) resource extraction, (ii) material handling and transport, (iii) resource processing, (iv) surface manufacturing with \"in situ\" resources, (v) surface construction, (vi) surface ISRU product and consumable storage and distribution, and (vii) ISRU unique development and certification capabilities. \n\nThe report focuses on lunar and martian environments. It offers a detailed timeline and capability roadmap to 2040 but it assumes lunar landers in 2010 and 2012.\n\nThe Mars Surveyor 2001 Lander was intended to carry to Mars a test payload, MIP (Mars ISPP Precursor), that was to demonstrate manufacture of oxygen from the atmosphere of Mars, but the mission was cancelled.\n\nThe Mars Oxygen ISRU Experiment (MOXIE) is a 1% scale prototype model aboard the planned Mars 2020 rover that will produce oxygen from Martian atmospheric carbon dioxide (CO) in a process called solid oxide electrolysis.\n\n\n"}
{"id": "40326143", "url": "https://en.wikipedia.org/wiki?curid=40326143", "title": "Inter Glass", "text": "Inter Glass\n\nInter Glass ASC is an Azerbaijani glass manufacturing company created in December 2008. According to the company, it is one of the biggest glass producers in the Caucasus region.\n\nThe company makes over 70 different kinds of glass bottles.\nThe company is located at 25, Salyan Highway, Baku, Azerbaijan.\nThe company’s production is estimated at around 85 million units per year. In 2008, the International Bank of Azerbaijan funded construction of an Inter Glass factory in the amount of $5 million Euro.\n\nThe Director of the company is Fariz Muradov.\n\nAs of April 2012, the CEO of the company was Alis Muradov.\n\n"}
{"id": "1371949", "url": "https://en.wikipedia.org/wiki?curid=1371949", "title": "Kernmantle rope", "text": "Kernmantle rope\n\nKernmantle rope () is rope constructed with its interior core protected by a woven exterior sheath designed to optimize strength, durability, and flexibility. The core fibers provide the tensile strength of the rope, while the sheath protects the core from abrasion during use.\n\nParachute cord (also paracord or 550 cord when referring to type-III paracord) is a lightweight nylon kernmantle rope originally used in the suspension lines of parachutes. This cord is useful for many other tasks and is now used as a general purpose utility cord by both military personnel and civilians.\n\nNylon ropes that were used in yachts for hauling were tested and found useful in climbing and caving and are now the modern standard. The German company Edelrid introduced the first kernmantel rope in 1953, which revolutionized fall prevention. Hemp climbing rope became a thing of the past and rope breakage was greatly reduced. In 1964, Edelrid and Mammut both developed dynamic ropes capable of withstanding multiple falls. These became the forerunner of the modern dynamic climbing rope. Although there were occasional innovations, the rope used today is similar in construction, strength, and durability across manufacturers. There are several major manufacturers, including Pigeon Mountain Industries (PMI), which is popular with cavers, Mammut, Sterling, Beal, Edelweiss, Blue Water, Roca, Tendon and Maxim. Overall there is a huge variety of climbing ropes available for different purposes; for instance, there are well over one hundred different dynamic single ropes (the most popular rope system in climbing). Kernmantle ropes are still used in sailing and other sports, but the technical requirements are usually not as rigorous for such purposes as for climbing, since those applications are not as critical to safety. Small kernmantle ropes are commonly called accessory cords; they are often used to make prusik knots and loops or to attach accessories such as chalk bags.\n\nOne or more of the rope characteristics (strength, durability, and flexibility) is often altered, depending upon the ultimate use of the rope, at the expense of the other properties. For example, rope used in caving is generally exposed to increased abrasion, so the mantle is woven more tightly than rope used in climbing or rappelling. However, the resulting rope is cumbersome and difficult to tie knots in.\n\nKernmantle construction may be used for both static and dynamic ropes. Static ropes are designed to allow relatively little stretch, which is most useful for applications such as hauling and rappelling. Dynamic rope is used to belay climbers, and is designed to stretch under a heavy load to absorb the shock of a fallen climber. Dynamic ropes manufactured for climbing are tested by the UIAA. A test of \"single\" standard rope involves tying an 80 kg (176 pound) weight to the end of a length of rope. This weight is then dropped 5 meters (16½ feet) on 2.7 meters (9 feet) of rope, with the rope running over a rounded surface simulating that of a standard carabiner. This process is repeated until the rope breaks. For \"double\" ropes the weight is 55 kg, and for twin ropes two strands are used. In addition to the number of drops, the impact force is also measured. It is a common misunderstanding to think that the number of drop test falls (as conducted by the UIAA) is the number of real-life climbing falls a rope can sustain before it becomes unsafe. The drop test falls are of extreme severity and a real-life climbing fall will not often generate a comparable force. This adds a margin of safety for climbers who use such ropes.\n\nKernmantle rope should be inspected before and after every use for any form of damage. \"Boogers,\" which indicate internal damage to the kern, appear as tufts of white threads poking out from the mantle. Ropes that have been severely stressed have tapered sections which are visibly or palpably thinner. Rope that has been abraded or cut on sharp edges should be examined closely by an experienced user, who may choose to cut the rope at that point, rather than risk it parting at that location.\n\nA rope can be cleaned by forming it into a chain sinnet to prevent excessive tangling and washing it in a front-loading clothes washing machine with soap flakes. Strong cleansers, including bleach and detergent should not be used on life-critical nylon components. Commercial rope cleaning devices are also available.\n\nDynamic ropes are rated for a certain number of falls (usually 5-10) at a given impact force.\n\n\n"}
{"id": "52835990", "url": "https://en.wikipedia.org/wiki?curid=52835990", "title": "LTE Advanced Pro", "text": "LTE Advanced Pro\n\nLTE Advanced Pro (LTE-A Pro, also known as 4.5G, 4.5G Pro, 4.9G, Pre-5G, 5G Project) is a name for 3GPP release 13 and 14. It is the next-generation cellular standard following LTE Advanced (LTE-A) and supports data rates in excess of 3 Gbps using 32-carrier aggregation. It also introduces the concept of License Assisted Access, which allows sharing of licensed and unlicensed spectrum.\n\nAdditionally, it incorporates several new technologies associated with 5G, such as 256-QAM, Massive MIMO, LTE-Unlicensed and LTE IoT, that allow evolution of existing networks into supporting the 5G standard.\n\n"}
{"id": "26132309", "url": "https://en.wikipedia.org/wiki?curid=26132309", "title": "List of biofuel companies and researchers", "text": "List of biofuel companies and researchers\n\nFirst-generation biofuels use the edible parts of food plants as their carbon source feedstock. Due to this, the production of fuel from these crops effectively creates problems in regard to the global food production.\n\n\nSecond-generation biofuels use non-food substances as a feedstock carbon source. Examples include non-food plants, the inedible parts of food plants, and waste cooking fat. Unlike first-generation biofuels, they do not create problems in regard to the global food production.\n\n\nThe so-called \"third-generation biofuels\" (which are basically second-generation biofuels) have an additional advantage as they do not take up any space, and may also help to reduce seawater eutrophication. They use algae to convert carbon dioxide into biomass.\n\n\nSome fourth-generation technology pathways include pyrolysis, gasification, upgrading, solar-to-fuel, and genetic manipulation of organisms to secrete hydrocarbons.\n\n\nHydrocarbon plants or petroleum plants are plants which produce terpenoids as secondary metabolites that can be converted to gasoline-like fuels. Latex-producing members of the Euphorbiaceae such as \"Euphorbia lathyris\" and \"E. tirucalli\" and members of Apocynaceae have been studied for their potential energy uses.\n\nSome other companies making 4th generation biofuels are:\n\n"}
{"id": "35240886", "url": "https://en.wikipedia.org/wiki?curid=35240886", "title": "Marel", "text": "Marel\n\nMarel hf. () is an Icelandic based multi-national food processing company that manufactures and provides equipment, systems, software and services to the poultry, meat and fish processing industries. Marel is one of the world's largest manufacturers of food processing machinery.The company employs approximately 5,500 people in offices and subsidiaries in over 30 countries, across six continents. The company is headquartered in Gardabaer, Iceland.\n\nMarel was founded in 1977, when some young engineers at the University of Iceland began to experiment with electronics and early computers. Jon Thor Olafsson, Hreinn Vilhjalmsson, Petur Jonsson and Tomas Rikardsson identified the need of many fish processors in Iceland to quickly and accurately weigh fish as they were being processed to minimize the giveaway of product, and withstand the harsh and wet conditions of fish processing plants.\n\nThe Marel engineers focused on implementing early computer technology into food processing equipment, an area where it had not been used extensively. The design plan was to have an array of interconnected systems to increase automation in food processing.\n\nMarel develops, manufactures, sells, and distributes equipment, further processing solutions, software and services for the poultry, fish and meat industries. Marel’s poultry processing sector offers integrated systems for processing broilers, turkeys, and ducks. The Marel Fish Processing segment provides equipment and systems for farmed and wild salmon and whitefish processing. Marel’s Meat Processing division offers systems and equipment for the processes of slaughtering, deboning and trimming, case ready food services, and bacon processing. Marel also offers wastewater treatment solutions designed specifically for the food industry, and packing and logistics solutions.\n\nThe Marel brand was established in 1983 and went public on the Icelandic Stock Exchange in 1992. The listing in 1992 marked a significant milestone in Marel’s progress from a startup that exclusively focused on the fishing industry, to a global company with operations in diverse industries.\n\nIn 2007, Marel changed it corporate identity to Marel Food Systems after acquiring four new brands in 2006: AEW Delford, Carnitech, Marel and Scanvaegt. The acquisition of these companies significantly increased the size of Marel’s operations around the world. On January 1, 2010, Marel Food Systems hf. changed their name back to Marel hf. after their integration with Stork Food Systems of the Netherlands was completed. This was part of Marel’s strategy to integrate all of the companies they have acquired throughout the years under a common identity and company name.\n\nMarel bought MPS meat processing systems in 2015 for EUR 382 million (USD 458 million) to increase their product offerings in the meat segment (pig, cattle and sheep) to balance their knowhow in the fish and poultry industries. In conjunction with the sale, Marel also acquired MPS’s intra-logistics systems for food industries and industrial wastewater treatment systems. MPS has headquarters in the Netherlands, with production sites in the Netherlands and China.\n\nIn July of 2017, Marel acquired Sulmaq in Brazil with to expand their operations in South and Central America. Brazil is the second largest producer of beef and the third largest producer of poultry products in the world. Sulmaq is based in the state of Rio Grande do Sul in southern Brazil and employs approximately 400 people. Sulmaq’s processing operations include hog slaughtering, cattle slaughtering, cutting and deboning, viscera processing and logistics.\n\nArni Oddur Thordarson has been the Chief Executive Officer of Marel hf since 2013. Prior to becoming CEO, Mr. Thordarson was the Chairman of Marel’s Board of Directors from 2005 to October 31, 2013. Under his direction Marel has seen significant growth and expansion of their business from mainly fish processing to other meat processors, and the consolidation of other industries. Marel has expanded both geographically and in product offerings. 2017 was the first year that Marel’s revenues exceeded EUR 1 billion (USD 1.2 billion) in revenues with an EBIT margin of 15%.\n\nAs the demand for protein is increasing worldwide, food industries and processors are transforming their practices to meet these larger expectations while satisfying the demands of efficiency, safety and sustainability. Marel is focused on using innovative technologies to maximize the yields of the food. By implementing advanced technologies to maximize the yields of fish, meat and poultry, the amount of an animal being processed can add up significantly. According to Mr. Thordarson, “Traditionally, fish production in Iceland had a yield of 60%, which means that 60% of every fish caught was consumable. Today, that figure is more like 85%, so we actually produce more from less.”\n\nDuring the 2007 transition period, Marel Food Systems rolled out their new corporate logo with “marel” all in lowercase letters and a symbolic electron that represents the company’s ideals of technology, innovation, and interconnectivity. All the companies of Marel Food Systems (now Marel hf.) will have the same logotype to match and represent a clear message: these are individual companies that represent a collective force and a unified corporate identity, sharing the same image and the same innovative spirit.\n\nIn 2012 Marel was awarded the EuroTier Gold Award for their contribution in environmental conservation and product safety for poultry processing. The American Meat Institute named Marel their supplier of the year in 2013. In 2014, Marel Stork Poultry Processing won an award in the category of Processing with their \"New reference in whole product distribution\". Marel also received the most public votes at the event, making them the overall winner of the VIV Europe 2014 innovation award.\n\nIn Georgia, USA, Marel Stork Poultry Processing was recognized in 2014 by Gainesville-Hall County in their Industry of the Year Awards for the range of benefits they offer to their employees. In October 2017, Marel’s \"Robot with a Knife\" won the prestigious Food Processing Award in the category of \"Robotics and Automation\".\n"}
{"id": "4010566", "url": "https://en.wikipedia.org/wiki?curid=4010566", "title": "Melamine foam", "text": "Melamine foam\n\nMelamine foam is a foam-like material consisting of a formaldehyde-melamine-sodium bisulfite copolymer. The foam is manufactured by several manufacturers worldwide, most notably by Germany-based BASF under the name Basotect. It has been used for over twenty years as insulation for pipes and ductwork, and has a long history as a soundproofing material for studios, sound stages, auditoriums, and the like. The low smoke and flame properties of melamine foam prevent it from being a fire hazard.\n\nMelamine foam is the active component of a number of abrasive cleaner sponges. It is also used as the main sound and thermal insulation material for bullet trains, due to its high sound absorption, excellent thermal insulation performance and light weight.\n\nIn the early 21st century it was discovered that melamine foam is an effective abrasive cleaner. The open-cell foam is microporous and its polymeric substance is very hard, so that when used for cleaning it works like extremely fine sandpaper, getting into tiny grooves and pits in the object being cleaned. On a larger scale the material feels soft because the reticulated foam bubbles interconnect. Its structure is a 3D network of very hard strands, when compared to the array of separate bubbles in a material such as styrofoam.\n\nRubbing with a slightly moistened foam may remove otherwise \"uncleanable\" external markings from surfaces. For example, melamine foam can remove crayon, marker pen, and grease from painted walls and wood finishings, plastic-adhering paints from treated wooden tables, and adhesive residue and grime from hubcaps. If the surface being cleaned is not sufficiently hard, it may be finely scratched by the melamine material. The foam wears away, rather like a pencil eraser, leaving behind a slight residue which can be rinsed off. The material is also sold as an effective dental stain remover for personal use. It is however warned to not use it on dental fillings as their surface may lose its shine.\n\n\n"}
{"id": "4429175", "url": "https://en.wikipedia.org/wiki?curid=4429175", "title": "Mixed-use development", "text": "Mixed-use development\n\nMixed-use development is a type of urban development that blends residential, commercial, cultural, institutional, or entertainment uses, where those functions are physically and functionally integrated, and that provides pedestrian connections. Mixed-use development can take the form of a single building, a city block, or entire neighbourhoods. The term may also be used more specifically to refer to a mixed-use real estate development project—a building, complex of buildings, or district of a town or city that is developed for mixed-use by a private developer, (quasi-) governmental agency, or a combination thereof.\n\nTraditionally, human settlements have developed in mixed-use patterns. However, with industrialisation as well as the invention of the skyscraper, governmental zoning regulations were introduced to separate different functions, such as manufacturing, from residential areas. In the United States, the heyday of separate-use zoning was after World War II, but since the 1990s, mixed-use zoning has once again become desirable as the benefits are recognized.\n\nIn the United States, the Environmental Protection Agency (EPA) collaborates with local governments by providing researchers developing new data that estimates how a city can be impacted by Mixed-use development. With the EPA putting models in the spreadsheet, it makes it much easier for municipalities, and developers to estimate the traffic, with Mixed-use spaces .The linking models also used as a resource tool measures the geography, demographics, and land use characteristics in a city. The Environmental Protection Agency has conducted an analysis on six major metropolitan areas using land usage, household surveys, and GIS databases. States such as California, Washington, New Mexico, and Virginia has adopted this standard as statewide policy when assessing how urban developments can impact traffic. Preconditions for the success of Mixed-use developments is employment, population, and consumer spending. The three preconditions ensures that a development can attract quality tenants and financial success. Other factors determining the success of the Mixed-use development is the proximity of production time, and the costs from the surrounding market.\n\nOne of the earliest cities to adopt a policy on Mixed-use development is Toronto, Ontario .The local government first played a role in 1986 with a zoning bylaw that allowed for commercial and residential units to be mixed. At the time, Toronto was in the beginning stages planning a focus on developing mixed-use development due to a growing popularity of more social housing . The law has since been updated as recently as 2013, refining much of its focus outside the downtown area which has been amalgamated into the main city since 1998. With the regulations in place, the city has oversaw the development of high-rise condominiums throughout the city with the supply of amenities and transit stops nearby. Toronto case of developing Mixed-uses has expand to encompass other North American cities in Canada and The United States to bring in similar changes.\n\nMixed-use zones has been implemented in Portland, Oregon since the early 1990's as the local government was trying to figure out how to lower auto oriented development which was prominent in the city at the time. In the state of Oregon alone, that housing must provide a clear objective towards design review. The city of Portland bureau of Planning and Sustainability has released a report in 2014 discussing the development trends in the city. The report eventuates the development of mixed-use spaces mainly by focusing on the city center and its corridors. Portland's light rail system, MAX provides the encouragement of mixing up residential, commercial, and work spaces into one zone. With this one zoning planning system, the use of land at increased densities provides a return in public investments throughout the city. Main street corridors provide flexible building heights and high density uses to provide opportunities for gathering places.\n\nMixed-use development allows the creation of plazas and outdoor corridors between buildings and sidewalks. Street facing facades have a maximum setback to how much space is allocated for pedestrians to gather in. Landscaping another feature in outdoor spaces allow trees and plants to grow on buildings vertically rather than being faced out in a front row.\n\nPublic Infrastructure\n\nMixed-use in centers that have increased in population density has allowed people to access places through public transit and has helped encourage walking, biking, and cycling to places of work and errands. Transportation has played a role in mitigating climate change by reducing congestion on roads and building up freight movement for goods and services. With street-level design in place in cities like Boston, Seattle, and Denver Mixed-uses allowed the designs of pedestrian walkways, plazas, and eye distances to shops and workplaces. This in turn has reduced parking lots in alleyways and garages.\n\nHistoric Preservation\n\nOlder cities such as Chicago and San Francisco landmark preservation policies to allow more flexibility on older buildings being reused as third spaces.\n\n\n\nWhile traditional zoning development focuses on separating commercial, residential, and recreational areas, Mixed-use development encourages the fill up of land use. With sparsely populated land, there is lack of pressure to density. The lack of urban renewal has led to urban decay, more fuel consumption, and racial ghettos. Mixed-use development on Brown sites has transformed sites into more sustainable populated centers as a result of economic factors being draw in to redevelop.\nSome of the more frequent mixed-use scenarios in the United States are:\n\n"}
{"id": "1830803", "url": "https://en.wikipedia.org/wiki?curid=1830803", "title": "Motorola 68881", "text": "Motorola 68881\n\nThe Motorola 68881 and Motorola 68882 are floating-point coprocessor (FPU) devices that were used in some computer systems in conjunction with the 68020 or 68030 microprocessors. The Motorola 68881 was introduced in 1984. The addition of one of these devices added substantial cost to a computer, but added a floating point unit that could rapidly perform floating point mathematical calculations. In the mid 1980s, this feature was useful mostly for scientific and mathematical software.\n\nThe 68020 and 68030 CPUs were designed with the separate 68881 chip in mind. Their instruction sets reserved the \"F-line\" instructions – that is, all opcodes beginning with the hexadecimal digit \"F\" could either be forwarded to an external coprocessor or be used as \"traps\" which would throw an exception, handing control to the computer's operating system. If an FPU is not present in the system, the OS would then either call an FPU emulator to execute the instruction's equivalent using 68020 integer-based software code, return an error to the program, terminate the program, or crash and require a reboot.\n\nThe 68881 had eight 80-bit data registers (a 64-bit mantissa plus a sign bit, and a 15-bit signed exponent). It allowed seven different modes of numeric representation, including single-precision, double-precision, and extended-precision, as defined by the IEEE floating-point standard, IEEE 754. It was designed specifically for floating-point math and was not a general-purpose CPU. For example, when an instruction required any address calculations, the main CPU would handle them before the 68881 took control.\n\nThe CPU/FPU pair were designed such that both could run at the same time. When the CPU encountered a 68881 instruction, it would hand the FPU all operands needed for that instruction, and then the FPU would release the CPU to go on and execute the next instruction.\n\nThe 68882 was an improved version of the 68881, with better pipelining, and eventually available at higher clock speeds. Its instruction set was exactly the same as that of the 68881. Motorola claimed in some marketing literature that it executed some instructions 40% faster than a 68881 at the same clock speed, though this did not reflect typical performance, as seen by its more modest improvement in the table below. The 68882 is pin compatible with the 68881 and can be used as a direct replacement in most systems. The most important software incompatibility was that the 68882 used a larger FSAVE state frame, which affected UNIX and other preemptive multitasking OSes that had to be modified to allocate more space for it.\n\nThe 68881 or 68882 were used in the Sun Microsystems Sun-3 workstations, IBM RT PC workstations, Apple Computer Macintosh II family, NeXT Computer, Amiga 3000, and Atari Mega STE, TT, and Falcon030. Some third-party Amiga and Atari products used the 68881 or 68882 as a memory-mapped peripheral to the 68000.\n\n\n\nThese statistics came from the comp.sys.m68k FAQ. No statistics are listed for the 16 MHz and 20 MHz 68882, though these chips were indeed produced.\n\nStarting with the Motorola 68040, floating point support was included in the CPU itself.\n\n"}
{"id": "290793", "url": "https://en.wikipedia.org/wiki?curid=290793", "title": "Nephelometer", "text": "Nephelometer\n\nA nephelometer is an instrument for measuring concentration of suspended particulates in a liquid or gas colloid. A nephelometer measures suspended particulates by employing a light beam (source beam) and a light detector set to one side (often 90°) of the source beam. Particle density is then a function of the light reflected into the detector from the particles. To some extent, how much light reflects for a given density of particles is dependent upon properties of the particles such as their shape, color, and reflectivity. Nephelometers are calibrated to a known particulate, then use environmental factors (k-factors) to compensate lighter or darker colored dusts accordingly. K-factor is determined by the user by running the nephelometer next to an air sampling pump and comparing results. There are a wide variety of research-grade nephelometers on the market as well as open source varieties.\n\nThe main uses of nephelometers relate to air quality measurement for pollution monitoring, climate monitoring, and visibility. Airborne particles are commonly either biological contaminants, particulate contaminants, gaseous contaminants, or dust.\n\nThe chart to the left shows the types and sizes of various particulate contaminants. This information is helpful toward understanding the character of particulate pollution inside a building or in the ambient air. It is also useful for understanding the cleanliness level in a controlled environment.\n\nBiological contaminants include mold, fungus, bacteria, viruses, animal dander, dust mites, pollen, human skin cells, cockroach parts, or anything alive or living at one time. They are the biggest enemy of indoor air quality specialists because they are contaminants that cause health problems. Levels of biological contamination depend on humidity and temperature that supports the livelihood of micro-organisms. The presence of pets, plants, rodents, and insects will raise the level of biological contamination.\n\nSheath air is clean filtered air that surrounds the aerosol stream to prevent particulates from circulating or depositing within the optic chamber. Sheath air prevents contamination caused by build-up and deposits, improves response time by containing the sample, and improves maintenance by keeping the optic chamber clean. The nephelometer creates the sheath air by passing air through a zero filter before beginning the sample.\n\nNephelometers are also used in global warming studies, specifically measuring the global radiation balance. Three wavelength nephelometers fitted with a backscatter shutter can determine the amount of solar radiation that is reflected back into space through dust and particulate matter. This reflected light influences the amount of radiation reaching the earth's lower atmosphere and warming the planet.\n\nNephelometers are also used for measurement of visibility with simple one-wavelength nephelometers used throughout the world by many EPAs. Nephelometers, through the measurement of light scattering, can determine visibility in distance through the application of a conversion factor called Koschmieder's formula.\n\nIn medicine, nephelometry is used to measure immune function.\n\nGas-phase nephelometers are also used in the detection of smoke and other particles of combustion. In such use, the apparatus is referred to as an aspirated smoke detector. These have the capability to detect extremely low particle concentrations (to 0.005%) and are therefore highly suitable to protecting sensitive or valuable electronic equipment, such as mainframe computers and telephone switches.\n\n\nA more popular term for this instrument in water quality testing is a turbidimeter. However, there can be differences between models of turbidimeters, depending upon the arrangement (geometry) of the source beam and the detector. A nephelometric turbidimeter always monitors light reflected off the particles and not attenuation due to cloudiness. In the United States environmental monitoring the turbidity standard unit is called Nephelometric Turbidity Units (NTU), while the international standard unit is called Formazin Nephelometric Unit (FNU). The most generally applicable unit is Formazin Turbidity Unit (FTU), although different measurement methods can give quite different values as reported in FTU (see below).\n\nGas-phase nephelometers are also used to study the atmosphere. These can provide information on visibility and atmospheric albedo.\n\n"}
{"id": "40493225", "url": "https://en.wikipedia.org/wiki?curid=40493225", "title": "Nicolas Appert", "text": "Nicolas Appert\n\nNicolas Appert (17 November 1749 Châlons-sur-Marne (present Châlons-en-Champagne), present Marne – 1 June 1841 Massy) was the French inventor of airtight food preservation. Appert, known as the \"father of canning\", was a confectioner.\nAppert described his invention as a way \"of conserving all kinds of food substances in containers\".\n\nAppert was a confectioner and chef in Paris from 1784 to 1795. In 1795, he began experimenting with ways to preserve foodstuffs, succeeding with soups, vegetables, juices, dairy products, jellies, jams, and syrups. He placed the food in glass jars, sealed them with cork and sealing wax and placed them in boiling water.\n\nIn 1800 Napoleon offered a prize of 12,000 francs for a new method to preserve food.\nIn 1806 Appert presented a selection of bottled fruits and vegetables from his manufacture at the Exposition des produits de l'industrie française, but did not win any reward.\nIn 1810 the Bureau of Arts and Manufactures of the Ministry of the Interior gave Appert an \"ex gratia\" payment of 12,000 francs on condition that he make his process public.\nAppert accepted and published a book describing his process that year.\nAppert's treatise was entitled \"L'Art de conserver les substances animales et végétales\" (\"The Art of Preserving Animal and Vegetable Substances\").\n200 copies were printed in 1810.\nThis was the first cookbook of its kind on modern food preservation methods.\nLa Maison Appert (), in the town of Massy, near Paris, became the first food bottling factory in the world, years before Louis Pasteur proved that heat killed bacteria. Appert patented his invention and established a business to preserve a variety of food in sealed bottles. Appert's method was to fill thick, large-mouthed glass bottles with produce of every description, ranging from beef, fowl, eggs, milk, and prepared dishes (according to sources). \nAppert deliberately avoided using tinplate in his early manufacture because the quality of French tinplate was poor.\nHis greatest success for publicity was an entire sheep. He left air space at the top of the bottle, and the cork would then be sealed firmly in the jar by using a vise. The bottle was then wrapped in canvas to protect it, while it was dunked into boiling water and then boiled for as much time as Appert deemed appropriate for cooking the contents thoroughly.\n\nIn honor of Appert, canning is sometimes called \"appertisation\", but should be distinguished from pasteurization. Appert's early attempts at food preservation by boiling involved cooking the food to a temperature far in excess of what is used in pasteurization (), and can destroy some of the flavour of the preserved food.\n\nAppert's method was so simple and workable that it quickly became widespread. In 1810, British inventor and merchant Peter Durand patented his own method, but this time in a tin can, so creating the modern-day process of canning foods. In 1812 Englishmen Bryan Donkin and John Hall purchased both patents and began producing preserves. Just a decade later, the Appert method of canning had made its way to America. Tin can mass production was, however, not common until the beginning of the 20th century, partly because a hammer and chisel were needed to open cans until the invention of a can opener by an Englishman named Robert Yeates in 1855.\n\nIn 1991, a monumental statue of Appert, a work in bronze by the artist Jean-Robert Ipousteguy, was erected in Châlons-en-Champagne. A plaque was affixed to his birthplace in 1986.\n\nIn 1999, busts of Appert by Richard Bruyère were erected in Institute of Food Technologists I.F.T. Chicago (USA), Massy, and Museum of Fine Arts in Châlons-en-Champagne.\n\nIn 2010, a statue of Appert by Roger Marion was erected in Malataverne (France).\n\nA room in the Museum of Fine Arts and Archeology of Châlons-en-Champagne was dedicated to him, (collection Jean Paul Barbier and AINA detail objects on the site of the international association Nicolas Appert.\n\nThere are 72 streets named after Nicolas Appert in France, and one in Canada.\n\nThere is a high school named after Nicolas Appert in Orvault, France.\n\nIn 1955 a French postal stamp commemorated him.\n\n2010 was declared Nicolas Appert Year, a national celebration, by the French ministry of culture. The Principality of Monaco issued a postage stamp featuring Appert. An exhibition entitled \"Mise en boîte\" was held at the Musée des Beaux-Arts et d'Archéologie de Châlons-en-Champagne.\n\nSince 1942, each year the Chicago Section of the Institute of Food Technologists awards the Nicolas Appert Award, recognizing lifetime achievement in food technology.\n\nThe student association of the Food Technology education at Wageningen University is called Nicolas Appert. Since 1972 this association has focused on improving the courses related to food technology education and organises several events each year for students and alumni. Currently almost 800 bachelor and master students are members. In 2017 the association celebrated its 11th lustrum.\n\n\n\n"}
{"id": "27352320", "url": "https://en.wikipedia.org/wiki?curid=27352320", "title": "Open-source appropriate technology", "text": "Open-source appropriate technology\n\nOpen-source appropriate technology (OSAT) is appropriate technology developed through the principles of the open-design movement. OSAT refers to, on the one hand, technology designed with special consideration to the environmental, ethical, cultural, social, political, and economic aspects of the community it is intended for. On the other hand, OSAT is developed in the open and licensed in such a way as to allow their designs to be used, modified and distributed freely.\n\nOpen source is a development method for appropriate technology that harnesses the power of distributed peer review and transparency of process. is an example of open-source appropriate technology. There anyone can both learn how to make and use AT free of concerns about patents. At the same time anyone can also add to the collective open-source knowledge base by contributing ideas, observations, experimental data, deployment logs, etc. It has been claimed that the potential for open-source-appropriate technology to drive applied sustainability is enormous. The built in continuous peer-review can result in better quality, higher reliability, and more flexibility than conventional design/patenting of technologies. The free nature of the knowledge also obviously provides lower costs, particularly for those technologies that do not benefit to a large degree from scale of manufacture. Finally, OSAT also enables the end to predatory intellectual property lock-in. This is particularly important in the context of technology focused on relieving suffering and saving lives in the developing world.\n\nThe \"open-source\" model can act as a driver of sustainable development. Reasons include:\n\n\nFor solutions, many researchers, companies, and academics do work on products meant to assist sustainable development. Vinay Gupta has suggested that those developers agree to three principles:\n\n\nThe ethics of information sharing in this context has been explored in depth.\n\n\n\n\nAppropriate technology is designed to promote decentralized, labor-intensive, energy-efficient and environmentally sound businesses. Carroll Pursell says that the movement declined from 1965 to 1985, due to an inability to counter advocates of agribusiness, large private utilities, and multinational construction companies. Recently (2011), several barriers to OSAT deployment have been identified:\n\n"}
{"id": "28752359", "url": "https://en.wikipedia.org/wiki?curid=28752359", "title": "Paul-François Huart-Chapel", "text": "Paul-François Huart-Chapel\n\nPaul-François Huart-Chapel was a Belgian industrialist, and politician.\n\nPaul-François Huart was born in Charleroi in 1770. He married Mary Chapel, the daughter of an industrialist.\n\nIn 1806 he inherited the factories of the Chapel family. He introduced a reverbatory furnace for melting metal in 1807, in 1821 the first Puddling furnace in Belgium (with J.M. Orban).\n\nShortly after John Cockerill had built the first blast furnace in Belgium in Liege, he built a coke fired blast furnace in 1827 in Charleroi, 12m high and producing 6 to 10tonnes of pig iron a day.\n\nBetween 1831 and 1834 he was Mayor of Charleroi. He died in 1850 aged 80.\n"}
{"id": "16728321", "url": "https://en.wikipedia.org/wiki?curid=16728321", "title": "Philippine Society of Information Technology Educators", "text": "Philippine Society of Information Technology Educators\n\nThe Philippine Society of Information Technology Educators (PSITE) is a professional body of information technology education practitioners in the Philippines. Its members are primarily academics; teachers of computer science, information technology, information and communication technology, engineering, mathematics and other allied fields. Industrial practitioners, however, such as programmers, systems analysts, web developers and others are welcomed as well.\n\n\n"}
{"id": "2611670", "url": "https://en.wikipedia.org/wiki?curid=2611670", "title": "Post Office Research Station", "text": "Post Office Research Station\n\nThe Post Office Research Station was first established as a separate section of the General Post Office in 1909.\n\nIn 1921, the Research Station moved to Dollis Hill, north west London, initially in ex-army huts.\n\nThe main permanent buildings at Dollis Hill were opened in 1933 by Prime Minister Ramsay MacDonald.\n\nIn 1968 it was announced that the station would be relocated to a new centre to be built at Martlesham Heath in Suffolk. This was formally opened on 21 November 1975 by Queen Elizabeth and is today known as Adastral Park. \n\nThe old Dollis Hill site was released for housing, with the main building converted into a block of luxury flats and an access road named Flowers Close, in honour of Tommy Flowers. Much of the rest of the site contains affordable housing administered by Network Housing.\n\nPaddock, a World War II concrete two-level underground bunker, was built in secret in 1939 as an alternative Cabinet War Room underneath a corner of the Dollis Hill site. Its surface building was demolished after the war.\n\nThe first transatlantic radio telephone service.\n\nIn 1943 the world's first programmable electronic computer, Colossus Mark 1 was built by Tommy Flowers and his team, followed in 1944 and 1945 by nine Colossus Mark 2s. These were used at Bletchley Park in Cryptanalysis of the Lorenz cipher.\n\nIn 1957 ERNIE (Electronic Random Number Indicator Equipment) was built for the government's Premium Bond lottery, by Sidney Broadhurst's team.\n\nIn 1971 Samuel Fedida conceived Viewdata and the Prestel service was launched in 1979.\n\n"}
{"id": "29016014", "url": "https://en.wikipedia.org/wiki?curid=29016014", "title": "Prodigy house", "text": "Prodigy house\n\nProdigy house is a term for large and showy English country houses built by courtiers and other wealthy families, either \"noble palaces of an awesome scale\" or \"proud, ambitious heaps\" according to taste. The prodigy houses stretch over the periods of Tudor, Elizabethan, and Jacobean architecture, though the term may be restricted to a core period of roughly 1570 to 1620. Many of the grandest were built with a view to housing Elizabeth I and her large retinue as they made their annual royal progress around her realm. Many are therefore close to major roads, often in the English Midlands. \n\nThe term originates with the architectural historian Sir John Summerson, and has been generally adopted. He called them \"...the most daring of all English buildings.\" The houses fall within the broad style of Renaissance architecture, but represent a distinctive English take on the style, mainly reliant on books for their knowledge of developments on the Continent. Andrea Palladio (1508–1580) was already dead before the prodigy houses reached their peak, but his much more restrained classical style did not reach England until the work of Inigo Jones in the 1620s. For ornament, French and Flemish Northern Mannerist decoration was more influential than Italian.\n\nElizabeth I travelled southern England in annual summer \"progresses\", staying at the houses of wealthy courtiers; however she never went north of Worcester or west of Bristol, though by the end of her reign there were many large houses beyond these self-imposed boundaries. The hosts were expected to house the monarch in style, and provide sufficient accommodation for about 150 travelling members of the court, for whom temporary buildings might need to be erected. Elizabeth was not slow to complain if she felt her accommodation had not been appropriate, and did so even about two of the largest prodigy houses, Theobalds House and Old Gorhambury House (both now destroyed). \n\nPartly as a result of this imperative, but also general increasing wealth, there was an Elizabethan building boom, with large houses built in the most modern styles by courtiers, wealthy from acquired monastic estates, who wished to display their wealth and status. A characteristic was the large area of glass – a new feature that superseded the need for easily defended external walls and announced the owners' wealth. Hardwick Hall, for example was proverbially described as \"Hardwick Hall, more glass than wall.\" Many other smaller prodigy houses were built by businessmen and administrators, as well as long-established families of the nobility and gentry. The large Doddington Hall, Lincolnshire was built between 1593 and 1600 by Robert Smythson for Thomas Tailor, who was the recorder to the Bishop of Lincoln; \"Tailor was a lawyer and therefore rich\" says Simon Jenkins.\n\nSome recent uses of the term extend the meaning to describe large ostentatious houses in America of later periods, such as colonial mansions in Virginia, first so described by Cary Carson.\n\nIn many respects the style of the houses varies greatly, but consistent features are a love of glass, a high elevation, symmetrical exteriors, consistency between all sides of the building, a rather square plan, often with tower pavilions at the corners that rise above the main roofline, and a decorated skyline. Altogether \"...a strange amalgam of exuberant pinnacles and turrets, native Gothic mullioned windows, and Renaissance decoration.\" Many houses stand alone, with stables and other outbuildings at a discreet distance. Glass was then an expensive material, and its use on a large scale a demonstration of wealth. The large windows required mullions, normally in stone even in houses mainly in brick. For the main structure, stone is preferred, often as a facing over brick, but some buildings use mostly brick, for example Hatfield House, following the precedent of Hampton Court and other earlier houses. Though there were often reminiscences of the medieval castle, the houses were exceptionally without defences, compared to contemporary Italian and French equivalents. \n\nTo have two internal courtyards, requiring a very large building, was a status symbol, found at Audley End, Blickling Hall, and others. By the end of the Elizabethan period this sprawling style, essentially developing the form of late medieval buildings like Knole in Kent (which has a total of 7 courtyards), and many Oxbridge colleges, was giving way to more compact high-rising structures with a coherent and dramatic structural plan, making the whole form of the building visible from outside the house. Hardwick Hall, Burghley House, and on a smaller scale Wollaton Hall, exemplify this trend. The outer exteriors of the house are more decorated than internal exteriors such as courtyards, the reverse of the usual priority in medieval houses. The common E and H-shaped plans, and in effect incorporating an imposing gatehouse into the main facade, rather than placing it across an initial courtyard, increased the visibility of the most grandly decorated parts of the exterior.\n\nThe classical orders were often used as decoration, piled up one above the other on the storeys over the main entrance. But, with a few exceptions such as Kirby Hall, columns were restricted to such individual features; in other buildings such as the Bodleian Library similar \"Towers of the Five Orders\" sit at the centre of frankly Gothic facades. At Longleat and Wollaton shallow pilasters are used across the facades. A crib-book, \"The First and Chief Grounds of Architecture\" by John Shute (1563) had been commissioned or sponsored by \"Protector Somerset\", John Dudley, 1st Duke of Northumberland, and is recorded in the libraries of many important clients of buildings, along with Sebastiano Serlio's \" Architettura\", initially in Italian or another language until 1611, when Robert Peake published four of the volumes in English. The heavily-illustrated books on ornament by the Netherlander Hans Vredeman de Vries (1560s onwards) and German Wendel Dietterlin (1598) supplied much of the Northern Mannerist decorative detail such as strapwork. It is evident from surviving letters that courtiers took a keen and competitive interest in architectural matters.\n\nInside, most houses still had a large hall in the medieval style, often with a stone or wood screen at one end. But this was only used for eating in by the servants, except on special occasions. The main room for the family to eat and live in was the great chamber, usually on the first floor (above the ground floor), a continuation of late medieval developments. In the 16th century a withdrawing room was usually added between the great chamber and the principal bedroom, as well as the long gallery. The parlour was another name for a more private room, and increasingly there were a number of these in larger houses, where the immediate family would now usually eat, and where they might retreat entirely in cold weather. Although the first modern corridor in England was probably built in this period, in 1579, they remained rare, and houses continued to have most rooms only accessible through other rooms, with the most intimate spaces of the family at the end of a suite.\n\nStaircases became wide and elaborate, and normally made of oak; Burghley and Hardwick are exceptions using stone. The new concept of a large long gallery was an important space, and many houses had spaces for entertaining on the top floor, whether small rooms in towers on the roof, or the very large top-floor rooms at Hardwick and Wollaton. Meanwhile, the servants lived on the ground floor. This might be seen as a lingering memory of the medieval castle, where domestic spaces were often placed high above the soldiery, and viewpoints were highly functional, and is a feature rarely found in subsequent large houses for two centuries or more. At Hardwick the windows increase in size as the storeys rise up, reflecting the increasing status of the rooms. In several houses the mostly flat roof itself was part of the reception spaces, with banqueting houses in the towers that were only accessible from \"the leads\", and a layout that allowed walking around to admire the views.\n\nThe designers are often unclear, and the leading figures had a background in one of the specialisms of building. Sometimes owners played a part in the detailed design, though the age of the gentleman amateur architect mostly came later. Few original drawings survive, though there are some by the architect-mason Robert Smythson (1535–1614) who was an important figure; many houses at least show his influence. Robert Lyminge was in charge of Hatfield and Blickling. John Thorpe laid the foundation stone of Kirby Hall as a five-year old (his father was chief mason, and children were often asked to perform this ritual) and is associated with Charlton House, Longford Castle, Condover Hall and the original Holland House, and perhaps Rushton Hall and Audley End. The demand for skilled senior builders, able to design and manage projects or parts of them, exceeded supply, and, at least in the largest houses, they appear to have been usually given a great deal of freedom in deciding the actual design by their mainly absentee clients.\n\nThe first \"prodigy house\" might be said to be Henry VII's Richmond Palace, completed in 1501 but now destroyed. But as a royal palace it does not strictly fit the definition. Hampton Court Palace, built by Cardinal Wolsey but taken over by the king on his fall, is certainly an example. The trend continued through the reigns of Henry VIII, Elizabeth, and into the reign of James I, when it reached its height. Henry was a prolific builder himself, though little of his work survives, but the prudent Elizabeth (like her siblings) built nothing herself, instead encouraging her courtiers to \"...build on a scale which in the past would have been seen as a dynastic threat.\"\n\nOthers see the original Somerset House in the Strand, London as the first prodigy house, or at least the first English attempt at a thoroughly and consistently classical style. With some other Châteaux of the Loire Valley, the Château de Chambord of François I of France (built 1519–1547) had many features of the English houses, and certainly influenced Henry VIII's Nonsuch Palace.\n\nImportant political families such as the Cecils and Bacons were serial builders of houses. These newly-risen families were typically the most frenetic builders. Sites were chosen for their potential convenience for royal progresses, rather than being the centre of landholdings, which were looked after by agents, or any local political powerbase.\n\nThe term prodigy house ceases to be used for houses built after about 1620. Despite some features of more strictly classical houses like Wilton House (rebuilding begun 1630) continuing those of the prodigy house, the term is not used of them. Much later houses like Houghton Hall and Blenheim Palace show a lingering fondness for elements of the 16th-century prodigy style. \n\nIn the 19th century Jacobethan revivals began, most spectacularly at Harlaxton Manor, which Anthony Salvin began in 1837. This manages to impart a Baroque swagger to the Northern Mannerist vocabulary. Mentmore Towers, by Joseph Paxton, is an enormous revival of a Smythson-type style, and like Westonbirt House (Lewis Vulliamy, 1860s) and Highclere Castle (Sir Charles Barry 1839–42, the setting for \"Downton Abbey\"), is something of an inflated Wollaton. The royal Sandringham House in Norfolk includes prodigy elements in its mixed styles.\n\nMany of the houses were later demolished, in the English Civil War or other times, and many smothered by later rebuilding. But the period retained a prestige, especially for families who rose to prominence during it, and in many the exteriors at least were largely retained. The north fronts of The Vyne and Lyme Park are examples of a slightly inconguous mixture of the Elizabethan and Palladian in a single facade.\n\nThe houses attracted criticism from the first, surprisingly often from their owners. The flattering poem \"To Penshurst\" by Ben Jonson (1616), contrasts Penshurst Place, a large and important late medieval house that was extended in a similar style under Elizabeth, with prodigy houses:\n\n<poem>\nThou art not, Penshurst, built to envious show,\nOf touch or marble; nor canst boast a row\nOf polished pillars, or a roof of gold;\nThou hast no lantern, whereof tales are told,\nOr stair, or courts; but stand’st an ancient pile, ...\n\nAnd though thy walls be of the country stone,\nThey’re reared with no man’s ruin, no man’s groan;\nThere’s none that dwell about them wish them down;\nBut all come in, the farmer and the clown, ...\n\nNow, Penshurst, they that will proportion thee\nWith other edifices, when they see\nThose proud, ambitious heaps, and nothing else,\nMay say their lords have built, but thy lord dwells.\n</poem>\n\nThough the style became dominant for very large houses from around 1570, there were alternatives. At Kenilworth Castle, Robert Dudley, 1st Earl of Leicester did not want to lose the historic royal associations of his building, and from 1563 modernised and extended it to harmonize the old and new, though the expanses of glass still impressed Midlanders. Bolsover Castle, Broughton Castle, Haddon Hall and Carew Castle in Wales were other sympathetic expansions of a medieval castle. The vernacular half-timbered style retained some popularity for gentry houses like Speke Hall and Little Moreton Hall, mostly in areas short of good building stone. \n\nEarlier, Compton Wynyates (begun c. 1481, greatly extended 1515–1525) was a resolutely unsymmetrical jumble of essentially medieval styles, including prominent half-timbering on the gables of the facade. It also nestles in a hollow, as medieval houses often did, avoiding the worst of the wind. In contrast, prodigy houses, like castles before them, often deliberately chose exposed sites where they could command the landscape (Wollaton, Hardwick); their owners mostly did not anticipate being there in winter.\n\nFor individual houses, see Airs, Jenkins, Norwich, and of course the Pevsner Architectural Guides\n\n"}
{"id": "31261943", "url": "https://en.wikipedia.org/wiki?curid=31261943", "title": "Ready Financial", "text": "Ready Financial\n\nReady Financial, headquartered in Boise, Idaho, is the creator of the ReadyDebit Visa Inc. prepaid debit card and online bill payment and check writing service. Ready Financial was founded by Will Tumulty and has received substantial investments by private equity group Rockbridge Growth Equity LLC, founded by Dan Gilbert. The company provides consumers lacking established credit with the security and convenience associated with debit and credit cards. The card offers the Path2credit score Tracker that helps consumers watch their credit score rise over time.\n\n"}
{"id": "27048957", "url": "https://en.wikipedia.org/wiki?curid=27048957", "title": "Rehydroxylation dating", "text": "Rehydroxylation dating\n\nRehydroxylation [RHX] dating is a developing method for dating fired-clay ceramics. It is based on the fact that after a ceramic specimen is removed from the kiln at the time of production, it immediately begins to recombine chemically with moisture from the environment. This reaction reincorporates hydroxyl (OH) groups into the ceramic material, and is described as rehydroxylation (RHX). The RHX process produces an increase in specimen weight. This weight increase provides an accurate measure of the extent of rehydroxylation. The dating clock is provided by the experimental finding that the RHX reaction follows a precise kinetic law: the weight gain increases as the fourth root of the time which has elapsed since firing. This so-called power law and the RHX method which follows from it were discovered by scientists from the University of Manchester and the University of Edinburgh.\n\nThe concept of RHX dating was first stated in 2003 by Wilson and collaborators who noted that \"results ... suggest a new method for archaeological dating of ceramics\". The RHX method was then described in detail in 2009 for brick and tile materials, and in relation to pottery in 2011.\n\nRHX dating is not yet routinely or commercially available. It is the subject of a number of research and validation studies in several countries.\n\nAccording to the RHX power-law, if the weight of a fired-clay ceramic increases as a result of RHX by 0.1% in 1 yr from firing, then the weight increase is 0.2% in 16 yr, 0.3% in 81 yr and 0.4% in 256 yr (and so on). The RHX method depends on the validity of this law for describing long-term RHX weight gain on archaeological timescales. There is now strong support for power-law behaviour from analyses of long-term moisture expansion data in brick ceramic, some of which now extends over more than 60 y. Moisture expansion and weight gain are known to be proportional to each other for a specified material at any specified firing temperature.\n\nA small piece of the ceramic is first removed, weighed, and heated to 500 °C, effectively dehydrating it completely. The amount of water lost in the dehydration process (and thus the amount of water gained since the ceramic was created) is measured with a microbalance. Once removed from the furnace, the sample is monitored to determine the precise rate at which it combines with atmospheric moisture. Once that RHX rate is determined, it is possible to calculate exactly how long ago it was removed from the kiln. If the date of firing of a certain ceramic were known from another source, the method could be used inversely to determine the average temperature of the object's environment since firing.\n\nThe RHX rate is largely insensitive to the ambient humidity because the RHX reaction occurs extremely slowly, and only minute amounts of water are required to feed it. Sufficient water is available in virtually all terrestrial environments. Neither systematic nor transient changes in humidity have an effect on long-term rehydroxylation kinetics, though they do affect instantaneous gravimetric measurements or introduce systematic error (i.e. through capillary condensation).\n\nThe rate of rehydroxylation is affected by the ambient temperature. Thus, when calculating dates, scientists must be able to estimate the temperature history of the sample. The method of calculation is based on temperature data for the location, with adjustments for burial depth and long-term temperature variation from historical records. This information is used to estimate an effective lifetime temperature or ELT which is then used in the dating calculation. The ELT is generally close to (but not exactly the same as) the long-term annual mean surface air temperature. For southern England this is about 11 °C.\n\nAny event involving exposure to extreme heat may reset the \"clock\" by dehydroxylating the specimen, as though it were just out of the kiln. For example, a medieval brick examined by Wilson and collaborators produced a dating result of 66 years. In fact this brick had been dehydroxylated by the intense heat of incendiary bombing and fires during World War II.\n\nThe main application of the RHX technique is to date archaeological ceramics. Yet most archaeological material contains components which causes either addition mass gain or additional mass loss during the RHX measurement process. These components can be an intrinsic part of the object, for example materials added as temper, or compounds which have become incorporated into the object during use, for example organic residues, or compounds which have entered into the object during burial or conservation.\n\nThe RHX technique was the product of a three-year study by a collaboration of University of Manchester and University of Edinburgh researchers, led by Moira Wilson. Though it has only been established on bricks and tiles of up to 2,000 years of age, research is continuing to determine whether RHX can be accurately used on any fired-clay material, for example earthenware of up to 10,000 years of age.\n\nThe original work of Wilson and co-workers was undertaken on construction materials, bricks and tiles. Transferring the method to ceramics has brought additional challenges but initial results have demonstrated that ceramics have the same “internal clock” as bricks. \nSeveral other studies have attempted to replicate the RHX technique,\nbut using archaeological ceramics. These studies have encountered issues with components within the ceramics causing either addition mass gain or additional mass loss during the RHX measurement process. The quality of data generated by the Manchester and Edinburgh groups has been due to analysing fired-clay materials which do not contain these components. Efforts to successfully replicate the original work and overcome the challenges presented by archaeological ceramics are underway in several academic institutions worldwide.\n"}
{"id": "43301706", "url": "https://en.wikipedia.org/wiki?curid=43301706", "title": "Robert Yarnall Richie", "text": "Robert Yarnall Richie\n\nRobert Yarnall Richie (1908–1984) was an American photographer who worked as a freelance commercial and industrial photographer, in Texas and worldwide. Richie's work is significant for its artistic qualities as well as documentary information. Richie may be best known for his oil production and aviation images in such areas as Texas, Louisiana, the Gulf of Mexico, and Saudi Arabia.\n\nRichie had work published in the magazines \"Fortune\", \"Time\", \"Life\", \"Scientific American\", and \"National Geographic\", and in other publications. He also contributed photos to annual reports for Fortune 500 companies such as General Motors, U.S. Steel, Gulf Oil, and Phelps Dodge.\n\nRichie was an avid pilot, and his life work includes thousands of aerial photographs taken worldwide, as well as many photos of aircraft and other aviation-related subjects.\n\nMany of his photos are collected in the Robert Yarnall Richie Photograph Collection, held by the DeGolyer Library at Southern Methodist University. The SMU archive contains corporate and industrial photographs made by Richie from 1932–1975. Many are online, and available at the SMU Central University Libraries Flickr site, at Flickr's The Commons area with no known copyright restrictions.\n\n"}
{"id": "7812887", "url": "https://en.wikipedia.org/wiki?curid=7812887", "title": "Sheet pan", "text": "Sheet pan\n\nA sheet pan, baking tray or baking sheet is a flat, rectangular metal pan used in an oven. It is often used for baking bread rolls, pastries and flat products such as cookies, sheet cakes, Swiss rolls and pizzas.\n\nThese pans, like all bakeware, can be made of a variety of materials, but are primarily aluminum or stainless steel. The most basic sheet pan is literally a sheet of metal. Commercial pans are sometimes made from aluminized steel which combines the conductive, reflective, and food adherence properties of aluminum, with the rigidity, mass, and strength of the inner steel core (in this process, the aluminum surface is typically 90% aluminum and 10% silicon, which is not quite the same as pure aluminum). \n\nCommon features that may be found in sheet pans include: one or more flat edges to assist food removal, one or more raised edges (lips) to retain food, a contiguous rim to retain either food or shallow liquid, handles to assist in moving the pan into and out of the oven, a layer of insulation (typically air) designed to protect delicate food from burning (air bake pan), or perforations to aid in speeding cooking (pizza tray).\n\nRigidity of the pan is especially important if the pan is to be placed directly on a flat heat source (hearth stone, induction element, etc.) Rims and ridges contribute to rigidity. \n\nMass, thermal conductivity, and colour of the pan play key roles in achieving a uniform cooking temperature. The friction of the pan's under surface may be a safety consideration in some applications. \n\nUnlike other bakeware, smaller sheet pans function as convenient task trays. \n\nA sheet pan that has a continuous lip around all four sides may be called a jelly roll pan. A pan that has at least one side flat, so that it is easy to slide the baked product off the end, may be called a cookie sheet.\n\nProfessional sheet pans used in commercial kitchens typically are made of aluminum, with a raised lip around the edge, and in the United States come in standard sizes. \n\nEuropean pan dimensions are governed by GN numbers under European Committee for Standardisation EN 631. \n\nIn American sizing, the full-size sheet pan is , which is too large for most home ovens. A two thirds sheet pan (also referred to as a three quarter size sheet pan) is . A half sheet pan is ; quarter sheets are . The half sheet is approximately the same size as mass-market baking sheets found in supermarkets, and the quarter sheet is a common size for rectangular, single-layer cakes. \n\nWithin each standard, other commercial kitchen equipment, such as cooling racks, ovens, and shelving, is made to fit these standard pans. In many cases, American and European are matched closely enough in size to be used interchangeably. \n\nNote that values are approximate and vary based on rim size and style. \n\nTypically, for rimmed trays, each rim will sacrifice 0.5–0.75 inches (1–2 cm) of baking surface along that edge. Pans of a single design from a single vendor will usually share the same rim height and rake across all tray sizes in a series, thus the sacrifice of flat baking surface is proportionally greater (relative to outer dimension) for small pans than for large pans. \n\nWhile nominally half the size—and typically sharing one dimension—quarter sheets will not necessarily nest inside a half sheet side by side (whether for storage or other purposes). Some vendors supply quarter sheets that are half the size of a half sheet in outer dimension, while other vendors supply quarter sheets that are closer to half the size of a half sheet in flat baking area.\n\nCommercial sheet pans are used for many purposes besides baking. Kitchen or cooking processes are often designed around kitchen equipment such as sheet pans, presupposing their ubiquity in most commercial food preparation areas. \n\nIn bread baking, especially, the bread dough will often go through several long rest intervals on sheet pans stacked in open or enclosed sheet pan racks (sometimes mounted on wheels). Enclosed racks may also be ventilated or temperature controlled to some degree.\n\n\n"}
{"id": "43725237", "url": "https://en.wikipedia.org/wiki?curid=43725237", "title": "Streamup", "text": "Streamup\n\nStreamup is a website that allows anyone to watch and stream live video. Users are able to browse the site and search for channels to watch and chat with other viewers, or create their own channel. The company is based in Los Angeles, CA.\n\n"}
{"id": "567931", "url": "https://en.wikipedia.org/wiki?curid=567931", "title": "String trimmer", "text": "String trimmer\n\nA whipper-snipper, also called a \"weed-whip\", \"string trimmer\", \"weed-whacker\", a \"weed eater\", a \"line trimmer\" or a \"strimmer\" (in the UK and Ireland), is a tool which uses a flexible monofilament line instead of a blade for cutting grass and other plants near objects, or on steep or irregular terrain. It consists of a cutting tip at the end of a long shaft with a handle.\n\nThe whipper-snipper was invented in the early 1970s by George Ballas of Houston, Texas, who conceived the idea while watching the revolving action of the cleaning brushes in an automatic car wash. His first trimmer was made by attaching pieces of heavy-duty fishing line to a popcorn can bolted to an edger. Ballas developed this into what he called the \"Weed Eater\", since it chewed up the grass and weeds around trees.\n\nA whipper-snipper works on the principle that a line that is turned fast enough is held out from its housing (the rotating reel) very stiffly by centrifugal force. The faster it turns the stiffer the line. Even round-section nylon line is able to cut grass and slight, woody plants quite well. Some monofilament lines designed for more powerful cutters have an extruded shape, like a star, that helps the line slash the material being cut; the line is thus able to cut quite large woody plants (small shrubs) or at least ring-bark them very effectively. These lines make disks less necessary for tough jobs.\n\nThe line is hand-wound onto a reel before the job is started, leaving both ends extending from the reel housing. The motor turns the reel and the line extends horizontally while the operator swings the trimmer about where the plants are to be trimmed. The operator controls the height at which cutting takes place and can trim down to ground level quite easily. \nAs the line is worn, or breaks off, the operator knocks the reel on the ground so that a release mechanism allows some of the line in the reel to extend and replace the spent portion. The newer models have an 'auto-feed' operation where a small cutter on the line-guard ensures that the line length exposed for cutting does not exceed the length that can be swung efficiently by the motor. Newly extended line operates more efficiently because of its heavier weight and surface effects. The speed of the spinning hub is usually controlled by a trigger on the handle.\n\nFor vertical cutting the whole machine can be tilted or some trimmers allow the head to be adjusted at different angles. Vertical cutting is not recommended near sidewalks or other concrete and pavement edges, because it leaves open grooves that allow water to collect and cause damage.\n\nwhipper-snippers powered by an internal combustion engine have the engine on the opposite end of the shaft from the cutting head, while electric whipper-snippers typically have an electric motor in the cutting head, but some other arrangements exist too. One of such is an arrangement where the trimmer is connected to heavy machinery and is powered using a hydraulic motor.\n\nThe head contains a safety shield on the user side and a rotating hub which may also be called a head or spool. Disadvantages of a gasoline-powered whipper-snipper include its greater weight and the significant vibration that carries throughout the device, both of which interfere with its maneuverability and contribute to muscle fatigue, as well as the requirement that motor oil be added to its fuel (if it is equipped with a two-stroke engine). Advantages of gasoline-powered trimmers include mobility (because they are not attached to a power outlet) and the higher maximum power.\n\nLarge trimmers, used for cutting roadside grass in large areas, are often heavy enough to require two hands to operate, and some are even fitted with a harness enabling the user's torso to bear some of their weight. These very large trimmers are often referred to as brush cutters. Brush-cutter types are usually made so that a metal blade can be attached instead of the \"string\" (or monofilament). A metal blade enables cutting heavier woody brush.\n\nTrimmers that have nylon or metal blades usually require straight driveshafts to handle the higher torque required to turn the heavier disk, and because of the shock loads that are passed back from the blade to the drive shaft and its gearbox(es). Smaller line trimmers have curved driveshafts to make holding the cutting-head at ground level much easier and with less strain on the operator.\n\nMany whipper-snippers allow the hub, the head or the lower part of the shaft to be replaced with accessories. Common accessories include:\n\nQuick-release shafts are offered on many newer models which do not require any tools to switch in accessories.\n\nGasoline-engine powered trimmers usually have a minimum of 21 cc displacement motors. At this size they can easily turn line and some have nylon blades as accessories to the line-reel. A 32 cc engine can swing a line and often has metal-blade accessories. Most trimmers use two stroke engines and require gasoline mixed with oil. Due to pollution laws four stroke engines are becoming more popular with a number of commercial weed eater models now being powered by four stroke engines. For instance, Honda, MTD and Craftsman manufacture a four stroke engine trimmer. Other companies, such as John Deere, now carry low-emission two-stroke engine trimmers. Stihl manufactures a hybrid four stroke engine trimmer using a technology called 4MIX. 4MIX trimmers have no oil reservoir. This engine is lubricated using pre-mixed gasoline, like a two-stroke engine.\n\nElectric edge trimmers have the advantage of being very light, easy to maneuver and easy-to-operate devices. However, the length of power cord that can be deployed across the ground limits them, and they are usually less powerful and robust than the gasoline-engine ones. Electric machines normally are limited to maximum diameter nylon because of their lower power output (400 to about 1200 watts). Recently there are electric whipper-snippers that offer the same performance as gasoline-powered trimmers. Battery-powered units are to be recharged after each use. The recharge time is typically several hours long; some models offer a quick-charge option that cuts charging time to between 30 minutes and 2 hours, or a removable battery pack so the user can have more than one battery ready to swap out when the first one runs down.\n\nThe typical two-cycle engine used on whipper-snippers pollutes heavily due to incomplete combustion in the cylinder. This results in unburned fuel escaping through the exhaust system and into the environment. US emission standards specifically limit emissions from small engines. Electric models produce no emissions at the point of use. Battery-powered units typically use small or large sealed lead acid, nickel metal hydride, or lithium ion batteries.\n\nwhipper-snippers have long been a source of serious environmental pollution, due to the string continuously being cut to microplastic. A viable alternative to the whipper-snipper is the grass whip.\n\nwhipper-snippers can be dangerous tools due to the fact that they can cause debris, including rocks and stones, to go flying in several directions. For this reason, it is typical for the person who is using a whipper-snipper to wear either safety glasses or a visor to protect the face and particularly the eyes as a stone in the eye could cause a person to be blinded for life. This, however, does not offer any protection for passersby, or nearby people who may also be vulnerable to being hit by debris from a whipper-snipper. In addition to the safety risk posed to individual people, the debris from whipper-snipper also has the potential to cause damage to cars and buildings, with a particularly high risk of breaking glass on windows.\n\nChain-link flail rotors, and any other trimmer head with linked metal parts are prohibited from sale in Britain and EU member states after being identified as a serious safety risk. This follows a fatal accident in 2010 where a chain section became detached from such a rotor and struck a nearby worker at high speed, thereby injuring him lethally.\n\n"}
{"id": "3906986", "url": "https://en.wikipedia.org/wiki?curid=3906986", "title": "Taint checking", "text": "Taint checking\n\nTaint checking is a feature in some computer programming languages, such as Perl and Ruby, designed to increase security by preventing malicious users from executing commands on a host computer. Taint checks highlight specific security risks primarily associated with web sites which are attacked using techniques such as SQL injection or buffer overflow attack approaches.\n\nThe concept behind taint checking is that any variable that can be modified by an outside user (for example a variable set by a field in a web form) poses a potential security risk. If that variable is used in an expression that sets a second variable, that second variable is now also suspicious. The taint checking tool proceeds variable by variable until it has a complete list of all variables which are potentially influenced by outside input. If any of these variables is used to execute dangerous commands (such as direct commands to a SQL database or the host computer operating system), the taint checker warns that the program is using a potentially dangerous tainted variable. The computer programmer can then redesign the program to erect a safe wall around the dangerous input.\n\nTaint checking may be viewed as a conservative approximation of the full verification of non-interference or the more general concept of secure information flow. Because information flow in a system cannot be verified by examining a single execution trace of that system, the results of taint analysis will necessarily reflect approximate information regarding the information flow characteristics of the system to which it is applied.\n\nThe following dangerous Perl code opens a large SQL injection vulnerability by not checking the value of the codice_1 variable:\n\nIf taint checking is turned on, Perl would refuse to run the command and exit with an error message, because a tainted variable is being used in a SQL query. Without taint checking, a user could enter codice_2, thereby running a command that deletes the entire database table. Much safer would be to encode the tainted value of $name to a SQL string literal and use the result in the SQL query, guaranteeing that no dangerous command embedded in codice_1 will be evaluated. Another way to achieve that is to use a prepared statement to sanitize all variable input for a query.\n\nOne thing to note is that Perl DBI requires one to set the TaintIn attribute of a database handle \"as well as\" enabling taint mode to check one's SQL strings.\n\nPerl supported tainting from at least 1989 as the -T switch was included in Perl 3.\n\nIn 1996 Netscape implemented data tainting in server-side JavaScript in Netscape Communications Server, as well as client-side for Netscape Navigator 3. \nHowever, since the client-side support was considered experimental it shipped disabled (requiring user intervention to activate), and required page authors to modify scripts to benefit from it. Other browser vendors never implemented the functionality; nor did Communications Server's primary competition, Microsoft's (then) new Internet Information Server.\n\n"}
{"id": "3657465", "url": "https://en.wikipedia.org/wiki?curid=3657465", "title": "TheFeature", "text": "TheFeature\n\nTheFeature.com was an online magazine and community dedicated to covering the technological, cultural and business evolution of the mobile Internet and the wider mobile telecommunications industry. Sponsored by Nokia, it was launched in August 2000 and continued through June 2005. Over the years, TheFeature became known for seeding innovative ideas in the nascent mobile Internet industry. Its impressive cadre of authors included Howard Rheingold, Douglas Rushkoff, Mark Frauenfelder, David Pescovitz, Justin Hall, Kevin Werbach, Carol Posthumus and Steve Wallage among others. TheFeature's editor-in-chief was Justin Ried, and its executive editor was Carlo Longino.\n\n\"TheFeature\" was designed by Razorfish from 2000 until 2003. Sascha Höhne redesigned the site in 2004, and all subsequent iterations through 2005.\n\n\"TheFeature\" had the distinction of being nominated for two Webby Awards in 2005, one in each of the Magazine and Telecommunications categories.\n\n"}
{"id": "39146066", "url": "https://en.wikipedia.org/wiki?curid=39146066", "title": "Thor O. Hannevig", "text": "Thor O. Hannevig\n\nThor Olaf Hannevig (20 April 1891 – 17 February 1975) was a Norwegian shipmaster. During the Norwegian Campaign in 1940 he was in command of an army unit called the \"Telemark Infantry Regiment\", and this regiment was able to withstand the German forces until 5 May. Hannevig later acquired a legendary heroic status, and his story was the basis of the 1993 Norwegian film \"The Last Lieutenant\".\n\nHannevig was born in Åsgårdstrand. He had taken an 8-months' officer's course in 1915. In addition to being a shipmaster, during his life he also owned a distillery, was an adventurer and a Freemason, and worked in a hotel, as a banker, a merchant, and a farmer. He died in Skreia in 1975. A monument was erected in Vinje to mark the centenary of his birth in 1991.\n"}
{"id": "15419237", "url": "https://en.wikipedia.org/wiki?curid=15419237", "title": "Transformation design", "text": "Transformation design\n\nIn broad terms, transformation design is a human-centered, interdisciplinary process that seeks to create desirable and sustainable changes in behavior and form – of individuals, systems and organizations – often for socially progressive ends.\n\nIt is a multi-stage, iterative process applied to big, complex issues – often social issues.\n\nIts practitioners examine problems holistically rather than reductively to understand relationships as well as components to better frame the challenge. They then prototype small-scale systems – composed of objects, services, interactions and experiences – that support people and organizations in achievement of a desired change. Successful prototypes are then scaled.\n\nBecause transformation design is about applying design skills in non-traditional territories, it often results in non-traditional design outputs. Projects have resulted in the creation of new roles, new organizations, new systems and new policies. These designers are just as likely to shape a job description, as they are a new product.\n\nThis emerging field draws from a variety of design disciplines - service design, user-centered design, participatory design, concept design, information design, industrial design, graphic design, systems design, interactive design, experience design - as well as non-design disciplines including cognitive psychology and perceptual psychology, linguistics, cognitive science, architecture, haptics, information architecture, ethnography, storytelling and heuristics.\n\nThough academics have written about the economic value of and need for transformations over the years, its practice first emerged in 2004 when The Design Council, the UK’s national strategic body for design, formed RED: a self-proclaimed “do-tank” challenged to bring design thinking to the transformation of public services.\n\nThis move was in response to Prime Minister Tony Blair’s desire to have public services “redesigned around the needs of the user, the patients, the passenger, the victim of crime.”\n\nThe RED team, led by Hilary Cottam, studied these big, complex problems to determine how design thinking and design techniques could help government rethink the systems and structures within public services and possibly redesign them from beginning to end.\n\nBetween 2004 and 2006, the RED team, in collaboration with many other people and groups, developed techniques, processes and outputs that were able to “transform” social issues such as preventing illness, managing chronic illnesses, senior citizen care, rural transportation, energy conservation, re-offending prisoners and public education.\n\nIn 2015 Braunschweig University of Art / Germany has launched a new MA in Transformation Design. In 2016 The Glasgow School of Art Launched another masters program \"M.Des in Design Innovation and Transformation Design\".\n\nTransformation design, like user-centered design, starts from the perspective of the end user. Designers spend a great deal of time not only learning how users currently experience the system and how they want to experience the system, but also co-creating with them the designed solutions.\n\nBecause transformation design tackles complex issues involving many stakeholders and components, more expertise beyond the user and the designer is always required. People such as, but not limited to, policy makers, sector analysts, psychologists, economists, private businesses, government departments and agencies, front-line workers and academics are invited to participate in the entire design process - from problem definition to solution development.\n\nWith so many points-of-view brought into the process, transformation designers are not always ‘designers.’ Instead, they often play the role of moderator. Though varying methods of participation and co-creation, these moderating designers create hands-on, collaborative workshops (a.k.a. charrette) that make the design process accessible to the non-designers.\n\nIdeas from workshops are rapidly prototyped and beta-tested in the real world with a group of real end users. Their experience with and opinions of the prototypes are recorded and fed back into the workshops and development of the next prototype.\n\n1. http://www.designcouncil.info/RED/ RED's homepage \n2. http://www.designcouncil.org.uk/ Design Council's homepage \n3. http://www.designcouncil.info/mt/RED/transformationdesign/TransformationDesignFinalDraft.pdf White Paper published by RED which discusses transformation design \n4. http://www.designcouncil.info/mt/RED/transformationdesign/ RED's website page which talks about transformation design \n5. http://www.torinoworlddesigncapital.it/portale/en/content.php?sezioneID=10 Interview with Hilary Cottam at World Design Capital \n6. https://web.archive.org/web/20070818190054/http://www.hilarycottam.com/html/RED_Paper%2001%20Health_Co-creating_services.pdf Whitepaper on co-creation\n7. \"The Experience Economy\", B.J. Pine and J. Gilmore, Harvard Business School Press 1999. Book discussing the economic value and importance of companies offering transformations\n8. \"The Support Economy\", S. Zuboff and J. Maxmin, Viking Press 2002. Book discussing the need for companies and governments to realign themselves with how people live \n9. \"Transformationsdesign - Wege in eine zukunftsfähige Moderne\", H. Welzer and B. Sommer, oekom 2014 \n10. \"Transformation Design - Perspectives on a new Design Attitude\", W. Jonas, S. Zerwas and K. von Anshelm, Birkhäuser 2015 \n"}
{"id": "436728", "url": "https://en.wikipedia.org/wiki?curid=436728", "title": "Tunable diode laser absorption spectroscopy", "text": "Tunable diode laser absorption spectroscopy\n\nTunable diode laser absorption spectroscopy (TDLAS) is a technique for measuring the concentration of certain species such as methane, water vapor and many more, in a gaseous mixture using tunable diode lasers and laser absorption spectrometry. The advantage of TDLAS over other techniques for concentration measurement is its ability to achieve very low detection limits (of the order of ppb). Apart from concentration, it is also possible to determine the temperature, pressure, velocity and mass flux of the gas under observation. TDLAS is by far the most common laser based absorption technique for quantitative assessments of species in gas phase.\n\nA basic TDLAS setup consists of tunable diode laser light source, transmitting (i.e. beam shaping) optics, optically accessible absorbing medium, receiving optics and detector/s. The emission wavelength of the tunable diode laser, viz. VCSEL, DFB, etc., is tuned over the characteristic absorption lines of a species in the gas in the path of the laser beam. This causes a reduction of the measured signal intensity, which can be detected by a photodiode, and then used to determine the gas concentration and other properties as described later.\n\nDifferent diode lasers are used based on the application and the range over which tuning is to be performed. Typical examples are InGaAsP/InP (tunable over 900 nm to 1.6 µm), InGaAsP/InAsP (tunable over 1.6 µm to 2.2 µm), etc. These lasers can be tuned by either adjusting their temperature or by changing injection current density into the gain medium. While temperature changes allow tuning over 100 cm, it is limited by slow tuning rates (a few hertz), due to the thermal inertia of the system. On the other hand, adjusting the injection current can provide tuning at rates as high as ~10 GHz, but it is restricted to a smaller range (about 1 to 2 cm) over which the tuning can be performed. The typical laser linewidth is of the order of 10 cm or smaller. Additional tuning, and linewidth narrowing, methods include the use of extracavity dispersive optics.\n\nThe basic principle behind the TDLAS technique is simple. The focus here is on a single absorption line in the absorption spectrum of a particular species of interest. To start with the wavelength of a diode laser is tuned over a particular absorption line of interest and the intensity of the transmitted radiation is measured. The transmitted intensity can be related to the concentration of the species present by the Beer-Lambert law, which states that when a radiation of wavenumber formula_1 passes through an absorbing medium, the intensity variation along the path of the beam is given by,\n\nwhere,\n\nThe above relation requires that the temperature formula_14 of the absorbing species is known. However, it is possible to overcome this difficulty and measure the temperature simultaneously. There are number of ways to measure the temperature, a widely applied method, which can measure the temperature simultaneously, uses the fact that the line strength formula_15 is a function of temperature alone. Here two different absorption lines for the same species are probed while sweeping the laser across the absorption spectrum, the ratio of the integrated absorbance, is then a function of temperature alone.\n\nwhere,\n\nAnother way to measure the temperature is by relating the FWHM of the probed absorption line to the Doppler line width of the species at that temperature. This is given by,\n\nwhere,\nNote: In the last expression, formula_22 is in kelvins and formula_21 is in g/mol.\nHowever, this method can be used, only when the gas pressure is low (of the order of few mbar). At higher pressures (tens of millibars or more), pressure or collisional broadening becomes important and the lineshape is no longer a function of temperature alone.\n\nThe effect of a mean flow of the gas in the path of the laser beam can be seen as a shift in the absorption spectrum, also known as Doppler shift. The shift in the frequency spectrum is related to the mean flow velocity by,\n\nwhere,\n\nNote : formula_26 is not same as the one mentioned before where it refers to the width of the spectrum. The shift is usually very small (3×10 cm ms for near-IR diode laser) and the shift-to-width ratio is of the order of 10.\n\nThe main disadvantage of absorption spectrometry (AS) as well as laser absorption spectrometry (LAS) in general is that it relies on a measurement of a small change of a signal on top of a large background. Any noise introduced by the light source or the optical system will deteriorate the detectability of the technique. The sensitivity of direct absorption techniques is therefore often limited to an absorbance of ~10, far away from the shot noise level, which for single pass direct AS (DAS) is in the 10 – 10 range. Since this is insufficient for many types of applications, AS is seldom used in its simplest mode of operation.\n\nThere are basically two ways to improve on the situation; one is to reduce the noise in the signal, the other is to increase the absorption. The former can be achieved by the use of a modulation technique, whereas the latter can be obtained by placing the gas inside a cavity in which the light passes through the sample several times, thus increasing the interaction length. If the technique is applied to trace species detection, it is also possible to enhance the signal by performing detection at wavelengths where the transitions have larger line strengths, e.g. using fundamental vibrational bands or electronic transitions.\n\nModulation techniques make use of the fact that technical noise usually decreases with increasing frequency (which is why it is often referred to as 1/f noise) and improve the signal to noise ratio by encoding and detecting the absorption signal at a high frequency, where the noise level is low. The most common modulation techniques are wavelength modulation spectroscopy (WMS) and frequency modulation spectroscopy (FMS).\n\nIn WMS the wavelength of the light is continuously scanned across the absorption profile, and the signal is detected at a harmonic of the modulation frequency.\n\nIn FMS, the light is modulated at a much higher frequency but with a lower modulation index. As a result, a pair of sidebands separated from the carrier by the modulation frequency appears, giving rise to a so-called FM-triplet. The signal at the modulation frequency is a sum of the beat signals of the carrier with each of the two sidebands. Since these two sidebands are fully out of phase with each other, the two beat signals cancel in the absence of absorbers. However, an alteration of any of the sidebands, either by absorption or dispersion, or a phase shift of the carrier, will give rise to an unbalance between the two beat signals, and therefore a net-signal.\n\nAlthough in theory baseline-free, both modulation techniques are usually limited by residual amplitude modulation (RAM), either from the laser or from multiple reflections in the optical system (etalon effects). If these noise contributions are held low, the sensitivity can be brought into the 10 – 10 range or even better.\n\nIn general the absorption imprints are generated by a straight line light propagation through a volume with the specific gas. To further enhance the signal, the pathway of the light travel can be increased with multi-pass cells. There is however a variety of the WMS-technique that utilizes the narrow line absorption from gases for sensing even when the gases are situated in closed compartments (e.g. pores) inside solid materia. The technique is referred to as gas in scattering media absorption spectroscopy (GASMAS).\n\nThe second way of improving the detectability of TDLAS technique is to extend the interaction length. This can be obtained by placing the species inside a cavity in which the light bounces back and forth many times, whereby the interaction length can be increased considerably. This has led to a group of techniques denoted as cavity enhanced AS (CEAS). The cavity can either be placed inside the laser, giving rise to intracavity AS, or outside, when it is referred to as an external cavity. Although the former technique can provide a high sensitivity, its practical applicability is limited because of all the non-linear processes involved.\n\nExternal cavities can either be of multi-pass type, i.e. Herriott or White cells, of non- resonant type (off-axis alignment), or of resonant type, most often working as a Fabry–Pérot (FP) etalon. Multi-pass cells, which typically can provide an enhanced interaction length of up to ~2 orders of magnitude, are nowaday common together with TDLAS.\n\nResonant cavities can provide a much larger path length enhancement, in the order of the finesse of the cavity, \"F\", which for a balanced cavity with high reflecting mirrors with reflectivities of ~99.99–99.999% can be ~ 10 to 10. It should be clear that if all this increase in interaction length can be utilized efficiently, this vouches for a significant increase in detectability. A problem with resonant cavities is that a high finesse cavity has very narrow cavity modes, often in the low kHz range (the width of the cavity modes is given by FSR/F, where FSR is the free-spectral range of the cavity, which is given by \"c\"/2\"L\", where \"c\" is the speed of light and \"L\" is the cavity length). Since cw lasers often have free-running linewidths in the MHz range, and pulsed even larger, it is non-trivial to couple laser light effectively into a high finesse cavity.\n\nThe most important resonant CEAS techniques are cavity ring-down spectrometry (CRDS), integrated cavity output spectroscopy (ICOS) or cavity enhanced absorption spectroscopy (CEAS), phase-shift cavity ring-down spectroscopy (PS-CRDS) and Continuous wave Cavity Enhanced Absorption Spectrometry (cw-CEAS), either with optical locking, referred to as (OF-CEAS), as has been demonstrated Romanini et al. or by electronic locking., as for example is done in the Noise-Immune Cavity-Enhanced Optical-Heterodyne Molecular Spectroscopy (NICE-OHMS) technique. or combination of frequency modulation and optical feedback locking CEAS, referred to as (FM-OF-CEAS).\n\nThe most important non-resonant CEAS techniques are off-axis ICOS (OA-ICOS) or off-axis CEAS (OA-CEAS), wavelength modulation off-axis CEAS (WM-OA-CEAS), off-axis phase-shift cavity enhanced absorption spectroscopy (off-axis PS-CEAS).\n\nThese resonant and non-resonant cavity enhanced absorption techniques have so far not been used that frequently with TDLAS. However, since the field is developing fast, they will presumably be more used with TDLAS in the future.\n\nFreeze-drying (lyophilization) cycle development and optimization for pharmaceuticals.\n\nFlow diagnostics in hypersonic/re-entry speed research facilities and scramjet combustors.\n\n"}
{"id": "1256791", "url": "https://en.wikipedia.org/wiki?curid=1256791", "title": "User guide", "text": "User guide\n\nA user guide or user's guide, also commonly known as a manual, is a technical communication document intended to give assistance to people using a particular system. It is usually written by a technical writer, although user guides are written by programmers, product or project managers, or other technical staff, particularly in smaller companies.\n\nUser guides are most commonly associated with electronic goods, computer hardware and software, although they can be written for any product.\n\nMost user guides contain both a written guide and the associated images. In the case of computer applications, it is usual to include screenshots of the human-machine interface(s), and hardware manuals often include clear, simplified diagrams. The language used is matched to the intended audience, with jargon kept to a minimum or explained thoroughly.\n\nThe sections of a user manual often include:\n\nUser guides have been found with ancient devices. One example is the Antikythera Mechanism, a 2,000 year old Greek analogue computer that was found off the coast of the Greek island Antikythera in the year 1900. On the cover of this device are passages of text which describe the features and operation of the mechanism.\n\nAs the software industry was developing, the question of how to best document software programs was undecided. This was a unique problem for software developers, since users often became frustrated with current help documents. Some considerations for writing a user guide that developed at this time include:\n\n\n\n\n\nUser manuals and user guides for most non-trivial software applications are book-like documents with contents similar to the above list. They may be distributed either in print or electronically. Some documents have a more fluid structure with many internal links. The \"Google Earth User Guide\" is an example of this format. The term \"guide\" is often applied to a document that addresses a specific aspect of a software product. Some usages are \"Installation Guide\", \"Getting Started Guide\", and various \"How to\" guides. An example is the \"Picasa Getting Started Guide\".\n\nIn some business software applications, where groups of users have access to only a sub-set of the application's full functionality, a user guide may be prepared for each group. An example of this approach is the \"Autodesk Topobase 2010 Help\" document, which contains separate \"Administrator Guides\", \"User Guides\", and a \"Developer's Guide\".\n\n"}
{"id": "11087075", "url": "https://en.wikipedia.org/wiki?curid=11087075", "title": "Washington Technology Industry Association", "text": "Washington Technology Industry Association\n\nWashington Technology Industry Association (WTIA) (formerly Washington Software Alliance) is a prominent technology business association, with approximately 1,000 member companies in Washington state, United States.\n\nWSA hosts educational and training events, CEO roundtables, executive seminars, and special interest groups. It also engages in advocacy for technology interests in Olympia, WA and Washington DC.\n\nThe Washington Software Alliance (WSA) was renamed the Washington Technology Industry Association in 2008.\n"}
