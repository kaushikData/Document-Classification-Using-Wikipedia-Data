{"id": "17504593", "url": "https://en.wikipedia.org/wiki?curid=17504593", "title": "1:1 pixel mapping", "text": "1:1 pixel mapping\n\n1:1 pixel mapping is a video display technique applicable to devices with native fixed pixels, such as LCD monitors and plasma displays. A monitor that has been set to 1:1 pixel mapping will display an input source without scaling it, such that each pixel received is mapped to a single native pixel on the monitor. This technique avoids loss of sharpness due to scaling artifacts and normally avoids incorrect aspect ratio due to stretching. If the input resolution is less than the monitor's native resolution, this will result in black borders around the image (e.g. letterboxing or windowboxing).\n\n"}
{"id": "54241631", "url": "https://en.wikipedia.org/wiki?curid=54241631", "title": "ACM SIGOPS", "text": "ACM SIGOPS\n\nACM SIGOPS is the Association for Computing Machinery's Special Interest Group on Operating Systems, an international community of students, faculty, researchers, and practitioners associated with research and development related to operating systems. The organization sponsors prestigious international conferences related to computer systems, operating systems, computer architectures, distributed computing, and virtual environments. In addition, the organization offers multiple awards recognizing outstanding participants in the field, including the Dennis M. Ritchie Doctoral Dissertation Award, in honor of Dennis Ritchie, co-creator of the renowned C programming language and Unix operating system.\n\nIn 1965, Henriette Avram started the ACM Special Interest Committee on Time-Sharing (SICTIME), and Arthur M. Rosenberg became the first chair. In 1968, the name was changed to ACM SIGOPS. By 1969, the organization included nearly 1000 members.\n\nACM SIGOPS sponsors the following industry conferences, some independently and some in partnership with industry participants such as ACM SIGPLAN, USENIX, Oracle, Microsoft, and VMWare.\n\nACM SIGOPS includes a Hall of Fame Award, started in 2005, recognizing influential papers from ten or more years in the past. Notable recipients include:\n\nACM SIGOPS publishes the Operating Systems Review (OSR), a forum for topics including operating systems and architecture for multiprogramming, multiprocessing, and time-sharing, and computer system modeling and analysis.\n\n"}
{"id": "45457285", "url": "https://en.wikipedia.org/wiki?curid=45457285", "title": "All the World in a Design School", "text": "All the World in a Design School\n\nAll the World in a Design School () is a 2015 Swedish documentary film made for television about a group of international students at one of the world's top ranked industrial design schools, Umeå Institute of Design in Umeå, Västerbotten, Sweden.\n\nUmeå Institute of Design is well respected by the industrial design world for its unique study and design methods in addition to a multicultural environment that has shaped generations of industrial designers.\n\nThis documentary, from inside Umeå Institute of Design, was shot during the school's 25th anniversary and follow a group of students from all corners of the world in close-up portraits during a time when the school struggles to maintain its attractiveness. New regulations by the Riksdag of Sweden mean non-European students are charged steep tuition fees.\n\nThe film is produced by Freedom From Choice, Filmpool Nord and Sveriges Television, and directed by award-winning Swedish filmmaker Mattias Löw.\n\nThe documentary won the Best Television Program Award at the Northern Character Film & TV Festival in Murmansk, Russia, the Best Documentary Feature Film Award at the Atlas & Aeris Awards in Boston, United States, a Remi Gold Award at the WorldFest - Houston International Film Festival in Texas, United States and has received international attention with nominations and official selections at several recognized film festivals including Camerimage in Bydgoszcz, Poland.\n\n\n"}
{"id": "32294777", "url": "https://en.wikipedia.org/wiki?curid=32294777", "title": "Alphagov", "text": "Alphagov\n\nAlphagov was the project name of the experimental prototype website built by the Government Digital Service and launched on 11 May 2011 by the UK Cabinet Office that was open for public comment for two months in order to judge the feasibility of a single domain for British Government web services.\n\nLaunched in response to the report by Martha Lane Fox \"Directgov 2010 and Beyond: Revolution Not Evolution\" that was published in November 2010. Alphagov sought to act as a proof of concept for the way citizens could interact with Government through a series of useful online tools, where they were more useful than published content alone.\n\nAs well as improving the 'citizen experience' of using government web services online the project also identified the potential for £64 million in yearly savings on central government's annual £128 million current web publishing bill.\n\nThis initial consultation period was completed in June 2011. A beta version was then created, which led to the launch of GOV.UK.\n\n\n"}
{"id": "659", "url": "https://en.wikipedia.org/wiki?curid=659", "title": "American National Standards Institute", "text": "American National Standards Institute\n\nThe American National Standards Institute (ANSI ) is a private non-profit organization that oversees the development of voluntary consensus standards for products, services, processes, systems, and personnel in the United States. The organization also coordinates U.S. standards with international standards so that American products can be used worldwide.\n\nANSI accredits standards that are developed by representatives of other standards organizations, government agencies, consumer groups, companies, and others. These standards ensure that the characteristics and performance of products are consistent, that people use the same definitions and terms, and that products are tested the same way. ANSI also accredits organizations that carry out product or personnel certification in accordance with requirements defined in international standards.\n\nThe organization's headquarters are in Washington, D.C. ANSI's operations office is located in New York City. The ANSI annual operating budget is funded by the sale of publications, membership dues and fees, accreditation services, fee-based programs, and international standards programs.\n\nANSI was originally formed in 1918, when five engineering societies and three government agencies founded the American Engineering Standards Committee (AESC). In 1928, the AESC became the American Standards Association (ASA). In 1966, the ASA was reorganized and became United States of America Standards Institute (USASI). The present name was adopted in 1969.\n\nPrior to 1918, these five founding engineering societies:\nhad been members of the United Engineering Society (UES). At the behest of the AIEE, they invited the U.S. government Departments of War, Navy (combined in 1947 to become the Department of Defense or DOD) and Commerce to join in founding a national standards organization.\n\nAccording to Adam Stanton, the first permanent secretary and head of staff in 1919, AESC started as an ambitious program and little else. Staff for the first year consisted of one executive, Clifford B. LePage, who was on loan from a founding member, ASME. An annual budget of $7,500 was provided by the founding bodies.\n\nIn 1931, the organization (renamed ASA in 1928) became affiliated with the U.S. National Committee of the International Electrotechnical Commission (IEC), which had been formed in 1904 to develop electrical and electronics standards.\n\nANSI's members are government agencies, organizations, academic and international bodies, and individuals. In total, the Institute represents the interests of more than 270,000 companies and organizations and 30 million professionals worldwide.\n\nAlthough ANSI itself does not develop standards, the Institute oversees the development and use of standards by accrediting the procedures of standards developing organizations. ANSI accreditation signifies that the procedures used by standards developing organizations meet the Institute's requirements for openness, balance, consensus, and due process.\n\nANSI also designates specific standards as American National Standards, or ANS, when the Institute determines that the standards were developed in an environment that is equitable, accessible and responsive to the requirements of various stakeholders.\n\nVoluntary consensus standards quicken the market acceptance of products while making clear how to improve the safety of those products for the protection of consumers. There are approximately 9,500 American National Standards that carry the ANSI designation.\n\nThe American National Standards process involves:\n\nIn addition to facilitating the formation of standards in the United States, ANSI promotes the use of U.S. standards internationally, advocates U.S. policy and technical positions in international and regional standards organizations, and encourages the adoption of international standards as national standards where appropriate.\n\nThe Institute is the official U.S. representative to the two major international standards organizations, the International Organization for Standardization (ISO), as a founding member, and the International Electrotechnical Commission (IEC), via the U.S. National Committee (USNC). ANSI participates in almost the entire technical program of both the ISO and the IEC, and administers many key committees and subgroups. In many instances, U.S. standards are taken forward to ISO and IEC, through ANSI or the USNC, where they are adopted in whole or in part as international standards.\n\nAdoption of ISO and IEC standards as American standards increased from 0.2% in 1986 to 15.5% in May 2012.\n\nThe Institute administers nine standards panels:\n\n\nEach of the panels works to identify, coordinate, and harmonize voluntary standards relevant to these areas.\n\nIn 2009, ANSI and the National Institute of Standards and Technology (NIST) formed the Nuclear Energy Standards Coordination Collaborative (NESCC). NESCC is a joint initiative to identify and respond to the current need for standards in the nuclear industry.\n\n\n\n"}
{"id": "30879434", "url": "https://en.wikipedia.org/wiki?curid=30879434", "title": "Antenna tracking system", "text": "Antenna tracking system\n\nAn antenna tracking system tracks a primary antenna to follow a moving signal source, such as a communication satellite. A secondary antenna has a greater beam width than the primary antenna and receives the same tracking signal from the satellite. The primary antenna is tracked according to a predetermined search pattern which causes a variation in the signal amplitude depending upon the relative location of the satellite and the antenna position. The signal strength signals from the two antennas are input to a summation function which takes the difference of the two signals. The noise and signal variation component of the two signals is substantially the same and is therefore eliminated from the resulting difference signal. An antenna control unit utilizes the resulting difference signal to select the optimum signal strength for the particular step of the search pattern. This system is particularly applicable to extremely high frequency communication channels (86 GHz and above) which are subject to atmospheric distortion and noise.\n\nAntenna Tracking Systems can apply to Broadband and Microwave Radio Systems as well for Point-to-Point and Point-to-Multipoint communications.\n\n\n"}
{"id": "43998818", "url": "https://en.wikipedia.org/wiki?curid=43998818", "title": "Assistive eating devices", "text": "Assistive eating devices\n\nAssistive eating devices include devices ranging from low-tech utensils to high-tech powered robotic eating equipment. Low tech eating devices include utensils, plates and bowls with lips that make scooping food easier. Cups and mugs, and even a standard disposable straw can be considered assistive drinking devices. They are used by people when they have difficulty eating or drinking independently. These devices are typically used for people with disabilities, but can also be used for children or people that have poor dexterity. They can promote independence during meal times, but in many cases also can reduce the caregiver workload during meals. \"Assistive eating devices can increase self-care, increase self-esteem associated with increased independence, increase safety during meals, and make meal-time better for caregiver staff…\" (This topic is discussed in more detail in an article entitled the Benefits of Independent Eating ).\n\nMore sophisticated technology, typically described as high-tech equipment, is also available to accommodate the needs of individuals who have significant restrictions in their ability to eat and drink without assistance from another person. For people who have quadriplegia, Amyotrophic Lateral Sclerosis (ALS, also known as Lou Gehrig’s disease), cerebral palsy, spinal muscular atrophy (SMA), and many other conditions, a powered device like the assistive dining device can facilitate hands free eating. For those who have tremors, or weak muscles, that make eating in the traditional manner difficult, or impossible, a manual feeding device may be of value.\n\nThis category includes plates and bowls that will not allow food to fall off or out of the container and will not slide around. Some of these devices are as simple as a clip on guard that attaches to any normal plate or bowl There are many plates that also have a lip on the edge that prevents food from being pushed off the plate when it is being scooped. Other dinnerware are modified or specially made so they do not slide around on the table. Two common ways to keep the plates and bowls on the table are mats and grip bottoms. However, if the individual in question has tremors, then suction bases may be utilized. There are special bowls and plates that have a suction base that will attach to the table. This base will prevent the dinnerware from getting knocked around when the container or table is bumped.\n\nForks, knives and spoons may need to be adapted in order for people to use them Individuals may struggle with tremors or the movement of opening and closing the hand or lifting the arm for independent feeding. Adapted utensils may be the answer for these individuals. Much like the weighted mugs to help with tremors, weighted utensils can minimize the tremors. These types of utensils may be specially bought, but attachments can also be bought to adapt the current utensils into an assistive device. These attachments can include something to slip over the handle that is weighted or that may be large or made out of foam to enhance grip and prevent the utensil from falling out of the hand. Clips and straps may also be utilized if the individual is unable to hold the handle at all.\n\nManual feeding devices are designed to allow an individual who has a tremor, or who lacks the hand and arm coordination to control a utensil to scoop food and lift it to their mouth, to feed themselves. Some devices simply stabilize the user’s arm, while others provide a weighted arm that dampens extraneous movement. Manual feeding devices like the Neater Eater require the user to manipulate the device to have the spoon pick up food and then move the spoon to their mouth, so they require some degree of arm motion capability by the user (i.e., they are powered by the user’s own muscles).\n\nPowered feeding devices allow individuals who are unable to self-feed using any other type of assistive eating technology, to eat independently. Typically, these devices operate using power from a rechargeable battery (for portability). The user controls the device by the use of adaptive switches to activate the various functions that the specific device offers. Worldwide, there are only a few powered feeding devices commercially available. The designs and manner in which they operate vary considerably. Some offer a simple plate or bowl on which the food is served, while others provide divided dishes or multiple bowls that keep the different types of foods separated, to avoid mixing of food. Many of the early powered feeding device designs were simply mechanical. All of those that are still available in 2017 are more robotic in nature and are controlled by computer processors.\n\nCups and mugs may be adapted or bought to help with daily living. The most common assistive drinking utensil is the straw. This is inexpensive and allows for the user to not need to pick up the cup at all. The same principles for assistive dinnerware may also be utilized for mugs or cups. Non-slip bases are common so they do not slip on the table, while cup holders are used to prevent the cup from being knocked over. In case the cup gets knocked over, no spill lids may be used. Some cups are sold with lids, but there are also lids that can be used for several different types of cups and mugs. Weighted mugs are also common for the individual that suffers with tremors. For individuals who have disabilities that limit the use of arms and/or hands, hands free drinking products are also available.\n"}
{"id": "6161508", "url": "https://en.wikipedia.org/wiki?curid=6161508", "title": "Auspex Systems", "text": "Auspex Systems\n\nAuspex Systems was a computer data storage company founded in 1987 by Larry Boucher, who was previously CEO of Adaptec. It was headquartered in Santa Clara, California.\n\nAuspex introduced the first network-attached storage (NAS) devices. \nAfter an initial public offering in 1993, shares were traded on the NASDAQ exchange under symbol ASPX.\nOne of the unique features of their systems was the ease with which volumes could be transparently mirrored and migrated between physical disks. Auspex systems used Functional Multiprocessing, essentially Asymmetric multiprocessing, that allowed the systems to scale functions independently -- such as networking, file processing, or storage processing. There was a Host Processor running Unix that controlled the whole system. This 'system within a system' could even be rebooted without interrupting file servicing! They became a leading provider of data center storage in the mid-1990s but fell behind NetApp in the field.\nEarly cabinet rack models held sets of 3-4gb disks the size of small shoeboxes.\nBruce N. Moore joined in 1995 as president and chief executive officer.\nBoucher left the company in 1997 to found Alacritech.\n\nTheir 4Front or NS2000 model, initially offered in 1999 as a stackable system, held drawers of disks and was plagued by Mylex RAID controller issues which contributed to their bankruptcy in June 2003. Their last product, the NSc3000, was the first multi-vendor SAN-NAS gateway and essentially kept the same NAS front-end but could connect via Fibre Channel to any SAN disk array.\nAfter the burst of the dot-com bubble, in February, 2000, the company worked with Regent Pacific to select Gary J. Sbona as interim chief executive.\nWhen the company was liquidated in 2003, its patent portfolio was acquired by NetApp and its services business went to GlassHouse Technologies for about $280,000.\n"}
{"id": "1068485", "url": "https://en.wikipedia.org/wiki?curid=1068485", "title": "Ballistite", "text": "Ballistite\n\nBallistite is a smokeless propellant made from two high explosives, nitrocellulose and nitroglycerine. It was developed and patented by Alfred Nobel in the late 19th century.\n\nAlfred Nobel patented Ballistite in 1887 while he was living in Paris. His formulation was composed of 10% camphor and equal parts nitroglycerine and collodion. The camphor reacted with any acidic products of the chemical breakdown of the two explosives. This both stabilized the explosive against further decomposition and prevented spontaneous explosions. However, camphor tends to evaporate over time, leaving a potentially unstable mixture.\n\nNobel's patent specified that the nitrocellulose should be \"of the well-known soluble kind\". He offered to sell the rights of the new explosive to the French government, but they declined, largely because they had just adopted Poudre B for military use. He subsequently licensed the rights to the Italian government, who entered into a contract, on 1 August, 1889, to obtain 300,000 kilograms of Ballistite; and Nobel opened a factory at Avigliana, Turin.\n\nThe Italian Army swiftly replaced their M1870 and M1870/87 rifles, which used black powder cartridges, to a new model, the M1890 Vetterli, which used a cartridge loaded with ballistite.\n\nAs Italy was a competing great power to France, this was not received well by the French press and the public. The newspapers accused Nobel of industrial espionage, by spying on Vieille, and \"high treason against France\". Following a police investigation he was refused permission to conduct any more research, or to manufacture explosives in France. He therefore moved to San Remo in Italy, in 1891, where he spent the last five years of his life.\n\nBallistite is still manufactured as a solid fuel rocket propellant, although the less volatile but chemically similar diphenylamine is used instead of camphor.\n\nMeanwhile, a government committee in Great Britain, called the \"Explosives Committee\" and chaired by Sir Frederick Abel, monitored foreign developments in explosives. Abel and Sir James Dewar, who was also on the committee, jointly patented a modified form of ballistite in 1889. This consisted of 58% nitroglycerin by weight, 37% guncotton and 5% petroleum jelly. Using acetone as a solvent, it was extruded as spaghetti-like rods initially called \"cord powder\" or \"the committee's modification of ballistite\", but this was soon abbreviated to cordite.\n\nAfter unsuccessful negotiations, in 1893, Nobel sued Abel and Dewar over patent infringement and lost the case. It then went to the Court of Appeal and the House of Lords in 1895 where he also lost the two appeals and the Nobel's Explosives Company had to pay the costs. The claim was lost because the words \"of the well-known soluble kind\" in his patent were taken to mean soluble collodion, and to specifically exclude the water-insoluble guncotton.\n\n"}
{"id": "7690332", "url": "https://en.wikipedia.org/wiki?curid=7690332", "title": "British and Irish stained glass (1811–1918)", "text": "British and Irish stained glass (1811–1918)\n\nA revival of the art and craft of stained-glass window manufacture took place in early 19th-century Britain, beginning with an armorial window created by Thomas Willement in 1811–12. The revival led to stained glass windows becoming such a common and popular form of coloured pictorial representation that many thousands of people, most of whom would never commission or purchase a painting, contributed to the commission and purchase of stained-glass windows for their parish church.\n\nWithin 50 years of the beginnings of commercial manufacture in the 1830s, British stained glass grew into an enormous and specialised industry, with important centres in Newcastle upon Tyne, Birmingham, Whitechapel in London, Edinburgh, Glasgow, Liverpool, Norwich and Dublin. The industry also flourished in the United States, Canada, Australia and New Zealand. By 1900 British windows had been installed in Copenhagen, Venice, Athens, Bangalore, Nagasaki, Manila and Wellington. After the Great War from 1914 to 1918, stained glass design was to change radically.\n\nFollowing the Norman conquest of England in 1066, many churches, abbeys and cathedrals were built, initially in the Norman or Romanesque style, then in the increasingly elaborate and decorative Gothic style. In these churches the windows were generally either large or in multiples so that the light within the building was maximised. The windows were glazed, frequently with coloured glass held in place by strips of lead. Because flat glass could only be manufactured in small pieces, the method of glazing lent itself to patterning. The pictorial representation of biblical characters and narratives was a feature of Christian churches, often taking the form of murals. By the 12th century stained glass was well adapted to serve this purpose. For 500 years the art flourished and adapted to changing architectural styles.\n\nThe vast majority of English glass was smashed by Puritans under Oliver Cromwell. Churches which retain a substantial amount of early glass are rare. Very few of England's large windows are intact. Those that contain a large amount of Medieval glass are usually reconstructed from salvaged fragments. The east, west and south transept windows of York Minster and the west and north Transept windows of Canterbury give an idea of the splendours that have been mostly lost.\nIn Scotland, which never manufactured its own stained glass but bought from the south, they lost much of their glass not because it was smashed but because the monasteries were disbanded. These monasteries had monks with skills in repairing. When they went, windows gradually fell apart.\n\nMedieval windows and drawings of them provided the source and inspiration for nearly all the earlier 19th-century designers.\n\nCanterbury Cathedral retains more ancient glass than any other English cathedral except York. Much of it is appears to be imported from France and some is very early, dating from the 11th century. Much of the glass is in the style of Chartres Cathedral with deep blue featuring as the background colour in most windows. There are wide borders of stylised floral motifs and small pictorial panels of round, square or diaper shape. Other windows contain rows of apostles, saints and prophets. These windows, including the large west window have a predominance of red, pink, brown and green in the colours, with smaller areas of blue. Most of the glass in this remarkable window is older than the 15th-century stone tracery that contains it, the figure of Adam being part of a series of Ancestors of Christ that are among the oldest surviving panels in England.\n\nYork Minster also contains much of its original glass including important narrative windows of the Norman period, the famous \"Five Sister\" windows, the 14th-century west window and 15th-century east window. The \"Five Sisters\", although repaired countless times so that they now contain a spider's web of lead, still reveal their delicate pattern of simple geometric shapes enhanced by grisaille painting. They were the style of window which was most easily imitated by early 19th-century plumber-glaziers. The east and west windows of York are outstanding examples because in each case they are huge, intact, at their original location and by a known craftsman. The west window, designed in about 1340 by Master Robert, contains tiers of saints and stories of Christ and the Virgin Mary, each surmounted by a delicate Gothic canopy in white and yellow-stain, against a red background. The highly ornate tracery lights are filled with floral motifs. White glass \"stone borders\" surround each panel, making it appear to float in its frame. The east window of 1405 was glazed by John Thornton and is the largest intact medieval window in the world. It presents a narrative in sequential panels of the \"Creation\", the \"Fall of Man\", \"the Redemption\", \"the Apocalypse\", \"the Last Judgment\" and \"the Glory of God\".\nYork also has windows with small diaper-shaped \"quarries\" painted with little birds and other motifs which were much reproduced in the 19th century.\n\nBetween them, the windows of York and Canterbury cathedrals provided the examples for different styles of windows- geometric patterns, floral motifs and borders, narratives set in small panels, rows of figures, major thematic schemes.\n\nScattered all over England, sometimes in remote churches, is similar evidence of the designs, motifs and techniques used in the past. Two churches, St Neot's in Cornwall and Fairford in Gloucestershire, are of particular interest. Fairford escaped the depredations of the Puritan era and, uniquely in England, retained its complete medieval cycle of glass. The theme is that of the east window of York, \"the Salvation of Mankind\", but in this case the theme is spread across all the windows of the church, large and small. The west window, of seven lights, shows a single narrative incident, the \"Last Judgment\". This scheme and these particular windows provided a rare source for the designers of narrative windows for parish churches.\n\nIn the 18th century there was a growing trend for philosophers, writers and painters to commune with nature. Nature was seen as all the more attractive if it contained signs of the grand aspirations, ideals and follies of humankind. Few things were considered more romantic than a medieval ruin which conjured up images of the traditional \"romances\" or idealistic sagas of the Middle Ages.\nEngland contained a great number of large Medieval ruins. These were chiefly castles destroyed by the Civil War in the 17th century and, even more significantly, vast abbey churches ruined at the Dissolution of the Monasteries in the 16th century. These ruined abbey churches suffered three long-term fates. Some were used as quarries for their building materials. The more remote abbeys were simply left to slowly decay. Those that were conveniently placed were awarded, with their associated lands, to favourites of King Henry VIII and his heirs. Thus it was that many of England's nobility grew up in homes that included within their structure part the Gothic remains of an ancient church or its associated monastic buildings. Some of these houses, such as the poet Byron's home, Newstead Abbey, contain reference to their origin within their name.\n\nIn the 18th century the owning of such a pile became fashionable. Those of noble lineage who did not have a ruin or a battlemented tower or an interior with pointed arcades promptly built one. Among the earliest of these creations was the novelist Horace Walpole's refurbishing of his London villa, which gave its name to the pretty and somewhat superficial style of architectural decoration known as Strawberry Hill Gothick. The movement gained impetus- Sir Walter Scott built himself a Scottish Baronial mansion, Abbotsford; the castles of Warwick, Arundel and Windsor were refurbished by their owners. The movement was just as strong in Germany where \"Mad\" King Ludwig II of Bavaria indulged his medievalism by building the Disneyland icon of Neuschwanstein. All over Europe, those who could afford to do so began the restoration and refurbishment of Medieval buildings. The last great Romantic flourish was the building of Castle Drogo by Sir Edwin Lutyens between 1910 and 1930.\nBegun in Oxford in 1833 by the theologian John Keble and supported by John Henry Newman, the Oxford Movement stressed the universality or \"catholic\" nature of the Christian Church and urged priests of the Church of England to reconsider their pre-Reformation traditions in both Doctrine and Liturgy. While reinforcing the concept of direct descent from the Apostles through the Church of Rome, the movement did not advocate a return to Roman Catholicism. In practice, however, several hundred Anglican priests, including Newman, became Roman Catholic. The long-term effects of the Oxford Movement were the rapid expansion of the Roman Catholic Church in Britain and the establishment of Anglo-Catholic liturgical styles in many Anglican churches. The emphasis on liturgical rites brought about an artistic revolution in church building and decoration.\n\nFormed in 1839 as a society of Cambridge undergraduates with an interest in Medieval architecture, this group developed into a powerful movement for the recording, study preservation of England's ancient churches, the analysis and definition of architectural style and the dissemination of such information through its publications, chiefly a monthly journal \"The Ecclesiologist\" (1841–1869).\nThe Cambridge Camden Society did much to bring about a revival of medieval styles in the design and appointments of 19th-century churches, as well as in the restoration of older ones. Their notions were often highly prescriptive, inflexible and intolerant of diversity within the church. They were insistent upon revival rather than originality.\n\nJohn Ruskin (1819–1900), an art critic, wrote two books that were highly influential in Art Philosophy. In \"The Seven Lamps of Architecture\" (1849) and \"The Stones of Venice\" (1851–1853), he discussed the moral, social and religious implications of buildings, emphasising the desirability of an ethical approach to the practice of the arts. His thinking influenced the Pre-Raphaelites, whose artistic style Ruskin defended against criticism.\n\nThis group of artists, of whom Dante Gabriel Rossetti, Edward Burne-Jones, John Everett Millais and William Holman Hunt were the central figures, rejected the indulgent classicism, the materialism and the lack of social responsibility that they perceived in the artistic trends of mid-19th-century painting. They sought to recreate in their works the simple forms, bright colours, religious devotion and artistic anonymity of the period of art that preceded the rise of the great and famous individuals of the Renaissance period. They initially exhibited their works signed only with the initials PRB, Pre-Raphaelite Brotherhood.\n\nWilliam Morris (1834–1896) was for a time a member of the Pre-Raphaelite Brotherhood and was influenced by their ideals and those of John Ruskin. As a precociously diverse designer, he saw the creation of arts in terms of social responsibility. He rejected, as Ruskin did, the mass-production of ornate and decorative wares of all sorts, such as those products of industry that were displayed in the 1851 Crystal Palace Exhibition. Morris advocated a return to cottage crafts and the revival and promulgation of old skills. To this end he formed a business partnership with Marshall and Faulkner, employing Ford Madox Brown, another highly creative and dynamic artist and thus began what is termed \"the Arts and Crafts Movement\".\nStained glass was just one of many products of their studio. Morris and Brown saw themselves as original artists working in the spirit of their antecedents. They did not reproduce earlier forms exactly, and because of disagreements with the philosophy of the Cambridge Camden Society, rarely created stained glass windows for ancient churches.\nWilliam Morris's success as an entrepreneur was such that he was able to keep Rossetti, Burne-Jones and others in regular employment as designers. Through their teaching at the Working Men's College in London the group had enormous influence on many designers of all sorts.\n\nThe Aesthetic Movement was a reaction against both the works of industry and the influential Socialist and Christian idealism of Morris and Ruskin who both saw art as directly linked to morality. Followers of the Aesthetic Movement, who included Burne-Jones and other stained glass designers such as Henry Holiday, propounded a philosophy of \"Art for Art's sake\". The style that evolved was sensuous and luxurious, linked with the rise of Art Nouveau.\n\nA. W. N. Pugin (1812–1852) was the son of the Neo-Gothic architect Augustus Charles Pugin and was a convert to Roman Catholicism in 1835. He built his first church in 1837. He was an enormously productive and meticulous church architect and designer of interiors. With the growth of Roman Catholicism in England, and the development of large industrial centres there was much scope for his talents. He worked with and employed other designers and was instrumental in encouraging the firm of John Hardman and Co. of Birmingham to turn their attention to the production of glass.\nPugin's most renowned designs are the interiors, particularly the House of Lords, that he designed for the architect Sir Charles Barry, (1795–1860) at the Houses of Parliament in London. After the destruction by fire of the Houses of Parliament in 1834, Barry had won the commission for their rebuilding, the stipulation being that they should be in the Gothic style, as the most significant part of the Medieval complex, the Great Hall of Westminster, remained standing. The rebuilding, which took up the rest of Barry's life, included a vast array of arts and crafts of all kinds, not the least of which was stained glass windows, both pictorial and armorial. The knowledge, elegance and sophistication of Pugin's designs imbue and unite the interiors. As an ecclesiastical designer, his influence upon every medium is hard to overstate.\n\nAs an architect Scott, (1811–1878), was persuaded by Pugin to turn his creativity towards Gothic Revival. Scott was an Anglican, his first significant commission being the design of the Martyrs' Memorial in Oxford, a powerful and highly visible architectural statement against the Oxford Movement. To Sir Gilbert fell the task of restoration of many of England's finest Medieval structures including Salisbury, Worcester, Chester, Ely and Durham Cathedrals. In the restorations, he was to employ and influence a great number of designers, including major stained glass firms.\n\nPearson (1817–1897) was, like Scott, chiefly a restorer of churches. His major work was the creation of the new cathedral at Truro in Cornwall.\n\nAlexander Thomson, nicknamed \"Greek\", (1817–1875), was one of the best known Scottish architects of his day and had a profound effect on later architects, particularly Charles Rennie MacIntosh. His building designs are curious and eclectic combinations of elements from the Classical, the Italian Renaissance, the Egyptian and the Exotic. He designed a number of churches with richly decorated interiors and employed several stained glass firms to furnish them with glass in the appropriate style.\n\nWith the growth of industry in the late 18th and early 19th centuries, and in particular the growth of those industries associated with commercial glass production, metal trades, metallurgy and the associated technical advancements, the scene was set for the revival of stained glass manufacture and the development of that industry on an unprecedented scale. Thomas Willement, a plumber and glazier, produced his first armourial window in 1811, and is known as the father of the 19th-century stained glass industry. Because of the prevalence of leadlight windows, many glaziers had the required skills for making windows in geometric patterns using coloured glass for chapels and churches. Between 1820 and 1840 some 40 different glass painters appear in the London trades directories.\nAt the time of the showcase of Victorian enterprise, the Crystal Palace Exhibition of 1851, stained glass manufacture had reached a point where 25 firms were able to display their works, including John Hardman of Birmingham, William Wailes of Newcastle, Ballantine and Allen of Edinburgh, Betton and Evans of Shrewsbury and William Holland of Warwick.\n\nCharles Winston was a 19th-century lawyer whose hobby was the study of Medieval glass. In 1847 he published an influential book on its styles and production, including a translation from Theophilus' \"On Diverse Arts\", the foremost Medieval treatise on painting, stained glass and metalwork, written in the early 12th century.\n\nWinston's interest in the technicalities of coloured glass production led him to take shards of medieval glass to James Powell and Sons of Whitefriars for analyses and reproduction. Winston observed that windows of medieval glass appeared more luminous than those of early 19th-century production, and set his mind to discovering why this was the case. Winston observed that light streaming through a 19th-century window generally made a coloured pattern on the floor. This was rarely the case with medieval glass. He concluded that the reason that 19th-century glass lacked brilliance was because it was too flat and regular, allowing the light to pass through directly. He recommended a return to the manufacture of hand-made crown and cylinder glass with all their inherent refractive irregularities for the specific purpose of creating stained glass windows.\n\nThe stylistic trends below did not necessarily follow each other consecutively. Rather, they overlapped and co-existed. Some stained glass studios were essentially a one-man show in which a single craftsmen designed and made windows of a particular style. Other firms were managed with considerable entrepreneurial skills, employing a number of designers. Some designers freelanced- their work can be seen in windows by a number of firms. Some firms changed with changing tastes and survived well into the 20th century. John Hardman & Co. is still in business.\n\nThese contain precisely painted shields and heraldic decoration utilising painterly skills that had remained in use during the 17th and 18th centuries. Thomas Willement was an armorial painter of windows.\n\nThese windows are simple and decorative, frequently utilising the skills of the provincial plumber/glazier. Many of these windows are among the earliest use of coloured glass but comparatively few have survived because later Victorians have replaced them with more elaborate pictorial windows. Some of these windows date from the 1820s.\n\nThese windows are usually patterned with fleur de lys and other floral motifs that were suited to the shape of the \"diamond panes\". They added a pleasant glowing ambience to an interior and in many churches in the early 19th century made up the entire glazing. They could be painted or stencilled with designs and are sometimes mould-cast or have impressed motifs. They were systematically replaced one by one when more elaborate windows were donated. These windows generally date from 1830 to 1860. Powell was a major supplier of impressed and stencilled quarries.\n\nFrom studying Medieval windows, particularly those at Canterbury Cathedral, many stained glass artists became adept at designing foliage and decorative borders that reproduce archaeological originals. There are windows of this type in which the foliate design is overlaid with banners bearing scriptural texts.\n\nIn those windows that are set with figurative \"rondels\", the style within them is often Classical (see below) but sometimes Medievalising and sometimes seeks to reproduce the original style so accurately that to they casual eye they have the appearance of ancient windows. Ancient windows in Canterbury Cathedral were removed in the 19th century and replaced with copies. There is a very fine Jesse Tree (the Ancestors of Christ) window of this nature at the eastern end. Two panels of the original have since been returned.\n\nAlthough often striving for an exotic appearance, the figures in many early 19th-century windows are classicising in style. Set against a background of \"geometry\", \"quarries\" or \"foliage\", the small painted incidents within \"rondels\" and \"quatrefoils\" are nearly always conservatively academic in their appearance, with figures based on those in engravings of works by admired painters, Raphael, Titian, Andrea del Sarto and Perugino. Often in the case of windows with ornate foliage, the archaeologically correct surrounds are at variance with the style of the rondels which make no attempt to reproduce the medieval. William Wailes and Charles Edmund Clutterbuck were among the important firms.\n\nInspired by Ruskin, Pugin and the Gothic Revival, some artists sought to reproduce the style of figures that they saw in ancient glass, illuminated manuscripts and the few remaining English wall paintings of the Gothic period. A major source was provided by the Biblia Pauperum or so-called \"Poor Man's Bible\". The resulting figures are elongated, curvilinear and stylised rather than naturalistic. The drapery folds and scrolls are exaggerated and the gestures are expansive. The painted details are highly linear, crisply defined and elegant. The style lent itself to narrative, to pure colour and to highly decorative effects. While the figures may seem quaint or even naïve, the quality of design of many of these windows is often highly sophisticated and the detailing exquisite. The masters of Gothic revival were John Hardman and Co. of Birmingham and Clayton and Bell.\n\nThe group that surrounded and were influenced by William Morris passed through a series of trends, initially Pre-Raphaelite, which, although espousing the Medieval, was not Gothic Revival in the archaeological sense, being an esoteric mix of Medieval, Early Renaissance and deeply personal influences. The Arts and Crafts style lent itself to the depiction of solidly working-class apostles and virtues set against backgrounds of quarries that resemble glazed earthenware tiles. The botanically accurate and semi-realistic grapevines, sunflowers and other growing things were more prominent than Gothic canopies. Narratives that emphasised hard labour, human decency and charitable love were the themes that lent themselves to enthusiastic treatment by Morris and Ford Madox Brown. Burne-Jones and Dante Gabriel Rossetti were to become philosophic traitors to the Arts and Crafts Movement by their association with the Aesthetic Movement.\n\nThe 1870s was a time of rich ornamentation and eclecticism in the arts. The treatment of Gothic canopies which were a feature of so many windows began to change from the brightly coloured, two-dimensional, playful appearance of the 1850s and 60s to an appearance of having been carved from fine white limestone. Tudor and Renaissance architectural details made an appearance and were often used without reference to the nature of the real architecture that enclosed the window. The art of painting canopies in this manner was diligently maintained until after World War I.\n\nThe Gothic style of figure painting began to give way to a more naturalistic style in which the figures seem more three-dimensional and portrait-like. An important source at this time was German wood engravings and etchings. These were available in a number of forms. Bibles and Bible picture books were available with several different series of such engravings. The works of Albrecht Dürer were much admired. One of the advantages of using engravings as a source was that the essentially linear techniques that were employed by the engraver to define forms could be easily interpreted in lead and the fine linear treatment of shadows was likewise easy for the stained glass artist to achieve using the monochrome paint technique. There were also windows imported into England at this time from the studios of Mayer of Munich which influenced English designers towards this style.\n\nIn the late 19th century there is often a great richness in the colouration of the windows, marked by a use of tertiary colours including rich purple, salmon pink, olive green, claret red, saffron and brown. \"Flashed glass\" was skillfully employed to enhance deep folds in robes. With this interest in colour, many windows depict atmospheric effects. Sunsets, glowering storm clouds and blazing glories appear behind the figures.\n\nIn line with the naturalistic drafting of the figures, there is a pictorial emphasis on depicting human interaction and response, often with detailed facial expressions and rather flamboyant gestures. Large scenes with large figures were popular. Among the major exponents were Lavers, Barraud and Westlake; and Heaton, Butler and Bayne. These trends continued, taking two basic directions until World War I.\n\nThe notable exceptions to the general trend were a large number of the windows made by Clayton and Bell who produced a diverse range of styles and continued to supply cheerfully coloured Gothic-style windows with a proliferation of bright red and yellow to catch the morning sun in church chancels.\n\nA somewhat more reserved style emerged in which the vibrant colouration is toned down in favour of backgrounds that are basically white, discreetly enhanced with \"yellow-stain\". The clothing of the figures is often of the darker shades, royal blue, wine red and dark green and is lined or bordered with intricately decorated yellow-stained glass. The painting of canopies and draperies was taken to a new height. The monochrome painting of faces is intensely detailed. The style lent itself to the depiction of saints and prophets, bishops and admirals, and Christ (or Queen Victoria) enthroned. The most influential firm in this style was Burlison and Grylls. There are many windows by Charles Eamer Kempe of this type.\nAestheticism\n\nThe Aesthetic Movement included Dante Gabriel Rossetti, Burne-Jones, the illustrator Aubrey Beardsley and writer Oscar Wilde. They propounded \"Art for Art's sake\", claiming that Beauty was an end in itself and that the creation of art should not be bound to any social or moral ideals. The Aesthetic artists were primarily concerned with the creation of that which was beautiful. Because of this, windows created by these artists are often stylistically diverse from each other and from other styles, yet are highly recognisable as the work of a particular designer, rather than of a particular workshop.\n\nThese windows rarely pay homage to Medieval origins. They are closely associated with Art Nouveau. The designs are often sinuous, luscious and richly textured, making highly creative use of \"flashed glass\" and repetitive forms. Drifting clouds, sweeping draperies and angel's wings lent themselves to the art of the Aesthetes. The idiosyncrasies of Burne-Jones’ style make his windows particularly easy to recognise. While Charles Eamer Kempe made many windows that were traditional and \"safe\", he designed others that are clearly aesthetic. Christopher Whall and, in America, Tiffany were Aesthetic designers.\n\nIn the last days of the Empire, the technical proficiency and artistic excesses of the traditional stained glass artists reached their height. The great windows of this period demonstrate a mastery over figure drawing and stained glass painting. The artists had developed ways of achieving every possible textural effect through the expert application of ground-glass paint and yellow-stain:- babies’ ringlets, old men's beards, silk brocade, dove's feather, ripe grapes, gold braid, glowing pearls and greasy sheep's wool could all be painted to realistic perfection by any number of studios. Many windows of the Edwardian period are the most opulent creations of the stained glass industry. The youthful Virgin of the Annunciation, Peter the fisherman, John the itinerant preacher and Joseph the carpenter are all depicted in robes of the most sumptuous nature, lined with cloth of gold and lavishly decorated at the edges with rubies and pearls.\n\nIn the years immediately following World War I, many of these windows were created by the more conservative studios as memorials to fallen soldiers. Hence there are countless two-light windows of St George and St Michael and even more lancets of the Good Shepherd gathering his lost sheep to the fold. These are the last product of the second Golden Age of stained glass window production.\n\n\n\n\n\n\n"}
{"id": "1542741", "url": "https://en.wikipedia.org/wiki?curid=1542741", "title": "Bulk material handling", "text": "Bulk material handling\n\nBulk material handling is an engineering field that is centered on the design of equipment used for the handling of dry materials. Bulk materials are those dry materials which are powdery, granular or lumpy in nature, and are stored in heaps. Examples of bulk materials are minerals, ores, coal, cereals, woodchips, sand, gravel, clay, cement, ash, salt, chemicals, grain, sugar, flour and stone in loose bulk form. It can also relate to the handling of mixed wastes. Bulk material handling is an essential part of all industries that process bulk ingredients, including: food, beverage, confectionery, pet food, animal feed, tobacco, chemical, agricultural, polymer, plastic, rubber, ceramic, electronics, metals, minerals, paint, paper, textiles and more.\n\nMajor characteristics of bulk materials, so far as their handling is concerned, are: lump size, bulk weight (density), moisture content, flowability (particle mobility), angle of repose, abrasiveness and corrosivity, among others.\n\nBulk material handling systems are typically composed of stationary machinery such as conveyor belts, screw conveyors, tubular drag conveyors, moving floors, toploaders, stackers, reclaimers, bucket elevators, truck dumpers, railcar dumpers or wagon tipplers, shiploaders, hoppers and diverters and various mobile equipment such as loaders, mobile hopper loaders / unloaders, various shuttles, combined with storage facilities such as stockyards, storage silos or stockpiles. Advanced bulk material handling systems feature integrated bulk storage (silos), conveying (mechanical or pneumatic), and discharge.\n\nThe purpose of a bulk material handling facility may be to transport material from one of several locations (i.e. a source) to an ultimate destination or to process material such as ore in concentrating and smelting or handling materials for manufacturing such as logs, wood chips and sawdust at sawmills and paper mills. Other industries using bulk materials handling include flour mills and coal-fired utility boilers.\n\nProviding storage and inventory control and possibly material blending is usually part of a bulk material handling system.\n\nIn ports handling large quantities of bulk materials continuous ship unloaders are replacing gantry cranes.\n\nNon-bulk materials handling classifications include palletization and containerization.\n\n"}
{"id": "512842", "url": "https://en.wikipedia.org/wiki?curid=512842", "title": "Cheesecloth", "text": "Cheesecloth\n\nCheesecloth is a loose-woven gauze-like carded cotton cloth used primarily in cheese making and cooking.\n\nCheesecloth is available in at least seven different grades, from open to extra-fine weave. Grades are distinguished by the number of threads per inch in each direction.\n\nThe primary use of cheesecloth is in some styles of cheesemaking, where it is used to remove whey from cheese curds, and to help hold the curds together as the cheese is formed. Cheesecloth is also used in straining stocks and custards, bundling herbs, making tofu and ghee, and thickening yogurt. Queso blanco and queso fresco are Spanish and Mexican cheeses that are made from whole milk using cheesecloth. Quark is a type of German unsalted cheese that is sometimes formed with cheesecloth. Paneer is a kind of Indian fresh cheese that is commonly made with cheesecloth. Fruitcake is wrapped in rum-infused cheesecloth during the process of \"feeding\" the fruitcake as it ripens.\n\nCheesecloth can also be used for several printmaking processes including lithography for wiping up gum arabic. In intaglio a heavily starched cheesecloth called tarlatan is used for wiping away excess ink from the printing surface.\n\nCheesecloth #60 is used in product safety and regulatory testing for potential fire hazards. Cheesecloth is wrapped tightly over the device under test, which is then subjected to simulated conditions such as lightning surges conducted through power or telecom cables, power faults, etc. The device may be destroyed but must not ignite the cheesecloth. This is to ensure that the device can fail safely, and not start electrical fires in the vicinity.\n\nCheesecloth made to United States Federal Standard CCC-C-440 is used to test the durability of optical coatings per United States Military Standard MIL-C-48497. The optics are exposed to a 95%-100% humidity environment at for 24 hours, and then a thick by wide pad of cheesecloth is rubbed over the optical surface for at least 50 strokes under at least . The optical surface is examined for streaks or scratches, and then its optical performance is measured to ensure that no deterioration occurred.\n\nCheesecloth is used in India and Pakistan for making summer shirts. Cheesecloth material shirts were popular for beachwear during the 1960s and 1970s in the United States. Cheesecloth has been used to create the illusion of \"ectoplasm\" during spirit channelling or other ghost-related phenomena.\n\nCheesecloth is used in anatomical dissection laboratories to prevent desiccation. The cloth is soaked with a preservative solution such as formalin then wrapped around the specimen.\n\n"}
{"id": "26914142", "url": "https://en.wikipedia.org/wiki?curid=26914142", "title": "Commensurate line circuit", "text": "Commensurate line circuit\n\nCommensurate line circuits are electrical circuits composed of transmission lines that are all the same length; commonly one-eighth of a wavelength. Lumped element circuits can be directly converted to distributed element circuits of this form by the use of Richards' transformation. This transformation has a particularly simple result; inductors are replaced with transmission lines terminated in short-circuits and capacitors are replaced with lines terminated in open-circuits. Commensurate line theory is particularly useful for designing distributed element filters for use at microwave frequencies.\n\nIt is usually necessary to carry out a further transformation of the circuit using Kuroda's identities. There are several reasons for applying one of the Kuroda transformations; the principal reason is usually to eliminate series connected components. In some technologies, including the widely used microstrip, series connections are difficult or impossible to implement.\n\nThe frequency response of commensurate line circuits, like all distributed element circuits, will periodically repeat, limiting the frequency range over which they are effective. Circuits designed by the methods of Richards and Kuroda are not the most compact. Refinements to the methods of coupling elements together can produce more compact designs. Nevertheless, the commensurate line theory remains the basis for many of these more advanced filter designs.\n\nCommensurate lines are transmission lines that are all the same electrical length, but not necessarily the same characteristic impedance (\"Z\"). A commensurate line circuit is an electrical circuit composed only of commensurate lines terminated with resistors or short- and open-circuits. In 1948, Paul I. Richards published a theory of commensurate line circuits by which a passive lumped element circuit could be transformed into a distributed element circuit with precisely the same characteristics over a certain frequency range.\n\nLengths of lines in distributed element circuits, for generality, are usually expressed in terms of the circuit's nominal operational wavelength, λ. Lines of the prescribed length in a commensurate line circuit are called \"unit elements\" (UEs). A particularly simple relationship pertains if the UEs are λ/8. Each element in the lumped circuit is transformed into a corresponding UE. However, \"Z\" of the lines must be set according to the component value in the analogous lumped circuit and this may result in values of \"Z\" that are not practical to implement. This is particularly a problem with printed technologies, such as microstrip, when implementing high characteristic impedances. High impedance requires narrow lines and there is a minimum size that can be printed. Very wide lines, on the other hand, allow the possibility of undesirable transverse resonant modes to form. A different length of UE, with a different \"Z\", may be chosen to overcome these problems.\n\nElectrical length can also be expressed as the phase change between the start and the end of the line. Phase is measured in angle units. formula_1, the mathematical symbol for an angle variable, is used as the symbol for electrical length when expressed as an angle. In this convention λ represents 360°, or 2π radians.\n\nThe advantage of using commensurate lines is that the commensurate line theory allows circuits to be synthesised from a prescribed frequency function. While any circuit using arbitrary transmission line lengths can be analysed to determine its frequency function, that circuit cannot necessarily be easily synthesised starting from the frequency function. The fundamental problem is that using more than one length generally requires more than one frequency variable. Using commensurate lines requires only one frequency variable. A well developed theory exists for synthesising lumped element circuits from a given frequency function. Any circuit so synthesised can be converted to a commensurate line circuit using Richards' transformation and a new frequency variable.\n\nRichards' transformation transforms the angular frequency variable, ω, according to,\n\nor, more usefully for further analysis, in terms of the complex frequency variable, \"s\",\n\nComparing this transform with expressions for the driving point impedance of stubs terminated, respectively, with a short circuit and an open circuit,\n\nit can be seen that (for θ < π/2) a short circuit stub has the impedance of a lumped inductance and an open circuit stub has the impedance of a lumped capacitance. Richards' transformation substitutes inductors with short circuited UEs and capacitors with open circuited UEs.\n\nWhen the length is λ/8 (or θ=π/4), this simplifies to,\n\nThis is frequently written as,\n\n\"L\" and \"C\" are conventionally the symbols for inductance and capacitance, but here they represent respectively the characteristic impedance of an inductive stub and the characteristic admittance of a capacitive stub. This convention is used by numerous authors, and later in this article.\n\nRichards' transformation can be viewed as transforming from a s-domain representation to a new domain called the Ω-domain where,\n\nIf Ω is normalised so that Ω=1 when ω=ω, then it is required that,\n\nand the length in distance units becomes,\n\nAny circuit composed of discrete, linear, lumped components will have a transfer function \"H\"(\"s\") that is a rational function in \"s\". A circuit composed of transmission line UEs derived from the lumped circuit by Richards' transformation will have a transfer function \"H\"(\"j\"Ω) that is a rational function of precisely the same form as \"H\"(\"s\"). That is, the shape of the frequency response of the lumped circuit against the \"s\" frequency variable will be precisely the same as the shape of the frequency response of the transmission line circuit against the \"j\"Ω frequency variable and the circuit will be functionally the same.\n\nHowever, infinity in the Ω domain is transformed to ω=π/4\"k\" in the \"s\" domain. The entire frequency response is squeezed down to this finite interval. Above this frequency, the same response is repeated in the same intervals, alternately in reverse. This is a consequence of the periodic nature of the tangent function. This multiple passband result is a general feature of all distributed element circuits, not just those arrived at through Richards' transformation.\n\nA UE connected in cascade is a two-port network that has no exactly corresponding circuit in lumped elements. It is functionally a fixed delay. There are lumped-element circuits that can approximate a fixed delay such as the Bessel filter, but they only work within a prescribed passband, even with ideal components. Alternatively, lumped-element all-pass filters can be constructed that pass all frequencies (with ideal components), but they have constant delay only within a narrow band of frequencies. Examples are the lattice phase equaliser and bridged T delay equaliser.\n\nThere is consequently no lumped circuit that Richard's transformation can transform into a cascade-connected line, and there is no reverse transformation for this element. Commensurate line theory thus introduces a new element of \"delay\", or \"length\".\nTwo or more UEs connected in cascade with the same \"Z\" are equivalent to a single, longer, transmission line. Thus, lines of length \"n\"θ for integer \"n\" are allowable in commensurate circuits. Some circuits can be implemented \"entirely\" as a cascade of UEs: impedance matching networks, for instance, can be done this way, as can most filters.\n\nKuroda's identities are a set of four equivalent circuits that overcome certain difficulties with applying Richards' transformations directly. The four basic transformations are shown in the figure. Here the symbols for capacitors and inductors are used to represent open-circuit and short-circuit stubs. Likewise, the symbols \"C\" and \"L\" here represent respectively the susceptance of an open circuit stub and the reactance of a short circuit stub, which, for θ=λ/8, are respectively equal to the characteristic admittance and characteristic impedance of the stub line. The boxes with thick lines represent cascade connected commensurate lengths of line with the marked characteristic impedance.\n\nThe first difficulty solved is that all the UEs are required to be connected together at the same point. This arises because the lumped element model assumes that all the elements take up zero space (or no significant space) and that there is no delay in signals between the elements. Applying Richards' transformation to convert the lumped circuit into a distributed circuit allows the element to now occupy a finite space (its length) but does not remove the requirement for zero distance between the interconnections. By repeatedly applying the first two Kuroda identities, UE lengths of the lines feeding into the ports of the circuit can be moved between the circuit components to physically separate them.\n\nA second difficulty that Kuroda's identities can overcome is that series connected lines are not always practical. While series connection of lines can easily be done in, for instance, coaxial technology, it is not possible in the widely used microstrip technology and other planar technologies. Filter circuits frequently use a ladder topology with alternating series and shunt elements. Such circuits can be converted to all shunt components in the same step used to space the components with the first two identities.\n\nThe third and fourth identities allow characteristic impedances to be scaled down or up respectively. These can be useful for transforming impedances that are impractical to implement. However, they have the disadvantage of requiring the addition of an ideal transformer with a turns ratio equal to the scaling factor.\n\nIn the decade after Richards' publication, advances in the theory of distributed circuits took place mostly in Japan. K. Kuroda published these identities in 1955 in his Ph.D thesis. However, they did not appear in English until 1958 in a paper by Ozaki and Ishii on stripline filters.\n\nOne of the major applications of commensurate line theory is to design distributed element filters. Such filters constructed directly by Richards' and Kuroda's method are not very compact. This can be an important design consideration, especially in mobile devices. The stubs stick out to the side of the main line and the space between them is not doing anything useful. Ideally, the stubs should project on alternate sides to prevent them coupling with each other, taking up further space, although this is not always done for space considerations. More than that, the cascade connected elements that couple together the stubs contribute nothing to the frequency function, they are only there to transform the stubs into the required impedance. Putting it another way, the order of the frequency function is determined solely by the number of stubs, not by the total number of UEs (generally speaking, the higher the order, the better the filter). More complex synthesis techniques can produce filters in which all elements are contributing.\n\nThe cascade connected λ/8 sections of the Kuroda circuits are an example of impedance transformers, the archetypical example of such circuits is the λ/4 impedance transformer. Although this is double the length of the λ/8 line it has the useful property that it can be transformed from a low-pass filter to a high-pass filter by replacing the open circuit stubs with short circuit stubs. The two filters are exactly matched with the same cut-off frequency and mirror-symmetrical responses. It is therefore ideal for use in diplexers. The λ/4 transformer has this property of being invariant under a low-pass to high-pass transformation because it is not just an impedance transformer, but a special case of transformer, an impedance inverter. That is, it transforms any impedance network at one port, to the inverse impedance, or dual impedance, at the other port. However, a single length of transmission line can only be precisely λ/4 long at its resonant frequency and there is consequently a limit to the bandwidth over which it will work. There are more complex kinds of inverter circuit that more accurately invert impedances. There are two classes of inverter, the \"J\"-inverter, which transforms a shunt admittance into a series impedance, and the \"K\"-inverter which does the reverse transformation. The coefficients \"J\" and \"K\" are respectively the scaling admittance and impedance of the converter.\n\nStubs may be lengthened in order to change from an open circuit to a short circuit stub and vice versa. Low-pass filters usually consist of series inductors and shunt capacitors. Applying Kuroda's identities will convert these to all shunt capacitors, which are open circuit stubs. Open circuit stubs are preferred in printed technologies because they are easier to implement, and this is the technology likely to be found in consumer products. However, this is not the case in other technologies such as coaxial line, or twin-lead where the short circuit may actually be helpful for mechanical support of the structure. Short circuits also have a small advantage in that they are generally have a more precise position than open circuits. If the circuit is to be further transformed into the waveguide medium then open circuits are out of the question because there would be radiation out of the aperture so formed. For a high-pass filter the inverse applies, applying Kuroda will naturally result in short circuit stubs and it may be desirable for a printed design to convert to open circuits. As an example, a λ/8 open circuit stub can be replaced with a 3λ/8 short circuit stub of the same characteristic impedance without changing the circuit functionally.\n\nCoupling elements together with impedance transformer lines is not the most compact design. Other methods of coupling have been developed, especially for band-pass filters that are far more compact. These include parallel lines filters, interdigital filters, hairpin filters, and the semi-lumped design combline filters.\n\n"}
{"id": "1248057", "url": "https://en.wikipedia.org/wiki?curid=1248057", "title": "Curt Herzstark", "text": "Curt Herzstark\n\nCurt Herzstark (July 26, 1902 – October 27, 1988) was an Austrian engineer. During World War II, he designed plans for a mechanical pocket calculator (the \"Curta\").\n\nHerzstark was born in Vienna, the son of Marie and Samuel Jakob Herzstark. His father was Jewish and his mother, born a Catholic, converted to Lutheranism and raised Herzstark Lutheran. \n\nIn 1938, while he was technical manager of his father's company Rechenmaschinenwerk AUSTRIA Herzstark & Co., Herzstark had already completed the design of the Curta, but could not manufacture it due to the Nazi German annexation of Austria. Instead, the company was ordered to make measuring devices for the German Army. In 1943, perhaps influenced by the fact that his father was a liberal Jew, the Nazis arrested him for \"helping Jews and subversive elements\" and \"indecent contacts with Aryan women\" and sent him to the Buchenwald concentration camp. However, the reports of the army about the precision-production of the firm AUSTRIA and especially about the technical expertise of Herzstark led the Nazis to treat him as an \"intelligence-slave\".\n\nHis imprisonment at Buchenwald seriously threatened his health, but his condition improved when he was called to work in the factory linked to the camp, which was named after Wilhelm Gustloff. There he was ordered to make a drawing of the construction of his calculator, so that the Nazis could ultimately give the machine to the Führer as a gift after the successful end of the war. The preferential treatment this allowed ensured that he survived his stay at Buchenwald until the camp's liberation in 1945, by which time he had redrawn the complete construction from memory.\n\nHerzstark died in Nendeln, Liechtenstein.\n\nThe Curta is referenced in chapter four of William Gibson's \"Pattern Recognition\". The chapter is entitled 'Math Grenades', referring to protagonist Cayce Pollard mistaking them for hand grenades at first glance.\n\n\n"}
{"id": "7990", "url": "https://en.wikipedia.org/wiki?curid=7990", "title": "Data warehouse", "text": "Data warehouse\n\nIn computing, a data warehouse (DW or DWH), also known as an enterprise data warehouse (EDW), is a system used for reporting and data analysis, and is considered a core component of business intelligence. DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data in one single place that are used for creating analytical reports for workers throughout the enterprise.\n\nThe data stored in the warehouse is uploaded from the operational systems (such as marketing or sales). The data may pass through an operational data store and may require data cleansing for additional operations to ensure data quality before it is used in the DW for reporting.\n\nThe typical extract, transform, load (ETL)-based data warehouse uses staging, data integration, and access layers to house its key functions. The staging layer or staging database stores raw data extracted from each of the disparate source data systems. The integration layer integrates the disparate data sets by transforming the data from the staging layer often storing this transformed data in an operational data store (ODS) database. The integrated data are then moved to yet another database, often called the data warehouse database, where the data is arranged into hierarchical groups, often called dimensions, and into facts and aggregate facts. The combination of facts and dimensions is sometimes called a star schema. The access layer helps users retrieve data.\n\nThe main source of the data is cleansed, transformed, catalogued, and made available for use by managers and other business professionals for data mining, online analytical processing, market research and decision support. However, the means to retrieve and analyze data, to extract, transform, and load data, and to manage the data dictionary are also considered essential components of a data warehousing system. Many references to data warehousing use this broader context. Thus, an expanded definition for data warehousing includes business intelligence tools, tools to extract, transform, and load data into the repository, and tools to manage and retrieve metadata.\n\nA data warehouse maintains a copy of information from the source transaction systems. This architectural complexity provides the opportunity to:\n\nThe environment for data warehouses and marts includes the following:\n\n\nIn regards to source systems listed above, R. Kelly Rainer states, \"A common source for the data in data warehouses is the company's operational databases, which can be relational databases\".\n\nRegarding data integration, Rainer states, \"It is necessary to extract data from source systems, transform them, and load them into a data mart or warehouse\".\n\nRainer discusses storing data in an organization's data warehouse or data marts.\n\nMetadata are data about data. \"IT personnel need information about data sources; database, table, and column names; refresh schedules; and data usage measures\".\n\nToday, the most successful companies are those that can respond quickly and flexibly to market changes and opportunities. A key to this response is the effective and efficient use of data and information by analysts and managers. A \"data warehouse\" is a repository of historical data that are organized by subject to support decision makers in the organization. Once data are stored in a data mart or warehouse, they can be accessed.\n\nA data mart is a simple form of a data warehouse that is focused on a single subject (or functional area), hence they draw data from a limited number of sources such as sales, finance or marketing. Data marts are often built and controlled by a single department within an organization. The sources could be internal operational systems, a central data warehouse, or external data. Denormalization is the norm for data modeling techniques in this system. Given that data marts generally cover only a subset of the data contained in a data warehouse, they are often easier and faster to implement.\n\nTypes of data marts include dependent, independent, and hybrid data marts.\n\nOnline analytical processing (OLAP) is characterized by a relatively low volume of transactions. Queries are often very complex and involve aggregations. For OLAP systems, response time is an effectiveness measure. OLAP applications are widely used by Data Mining techniques. OLAP databases store aggregated, historical data in multi-dimensional schemas (usually star schemas). OLAP systems typically have data latency of a few hours, as opposed to data marts, where latency is expected to be closer to one day. The OLAP approach is used to analyze multidimensional data from multiple sources and perspectives. The three basic operations in OLAP are : Roll-up (Consolidation), Drill-down and Slicing & Dicing.\n\nOnline transaction processing (OLTP) is characterized by a large number of short on-line transactions (INSERT, UPDATE, DELETE). OLTP systems emphasize very fast query processing and maintaining data integrity in multi-access environments. For OLTP systems, effectiveness is measured by the number of transactions per second. OLTP databases contain detailed and current data. The schema used to store transactional databases is the entity model (usually 3NF). Normalization is the norm for data modeling techniques in this system.\n\nPredictive analytics is about finding and quantifying hidden patterns in the data using complex mathematical models that can be used to predict future outcomes. Predictive analysis is different from OLAP in that OLAP focuses on historical data analysis and is reactive in nature, while predictive analysis focuses on the future. These systems are also used for customer relationship management (CRM).\n\nThe concept of data warehousing dates back to the late 1980s when IBM researchers Barry Devlin and Paul Murphy developed the \"business data warehouse\". In essence, the data warehousing concept was intended to provide an architectural model for the flow of data from operational systems to decision support environments. The concept attempted to address the various problems associated with this flow, mainly the high costs associated with it. In the absence of a data warehousing architecture, an enormous amount of redundancy was required to support multiple decision support environments. In larger corporations, it was typical for multiple decision support environments to operate independently. Though each environment served different users, they often required much of the same stored data. The process of gathering, cleaning and integrating data from various sources, usually from long-term existing operational systems (usually referred to as legacy systems), was typically in part replicated for each environment. Moreover, the operational systems were frequently reexamined as new decision support requirements emerged. Often new requirements necessitated gathering, cleaning and integrating new data from \"data marts\" that was tailored for ready access by users.\n\nKey developments in early years of data warehousing were:\n\n\nA fact is a value or measurement, which represents a fact about the managed entity or system.\n\nFacts, as reported by the reporting entity, are said to be at raw level. E.g. in a mobile telephone system, if a BTS (base transceiver station) received 1,000 requests for traffic channel allocation, it allocates for 820, and rejects the remaining, it would report three facts or measurements to a management system:\n\nFacts at the raw level are further aggregated to higher levels in various dimensions to extract more service or business-relevant information from it. These are called aggregates or summaries or aggregated facts.\n\nFor instance, if there are three BTS in a city, then the facts above can be aggregated from the BTS to the city level in the network dimension. For example:\n\n\nThere are three or more leading approaches to storing data in a data warehouse — the most important approaches are the dimensional approach and the normalized approach.\n\nThe dimensional approach refers to Ralph Kimball's approach in which it is stated that the data warehouse should be modeled using a Dimensional Model/star schema. The normalized approach, also called the 3NF model (Third Normal Form) refers to Bill Inmon's approach in which it is stated that the data warehouse should be modeled using an E-R model/normalized model.\n\nIn a dimensional approach, transaction data are partitioned into \"facts\", which are generally numeric transaction data, and \"dimensions\", which are the reference information that gives context to the facts. For example, a sales transaction can be broken up into facts such as the number of products ordered and the total price paid for the products, and into dimensions such as order date, customer name, product number, order ship-to and bill-to locations, and salesperson responsible for receiving the order.\n\nA key advantage of a dimensional approach is that the data warehouse is easier for the user to understand and to use. Also, the retrieval of data from the data warehouse tends to operate very quickly. Dimensional structures are easy to understand for business users, because the structure is divided into measurements/facts and context/dimensions. Facts are related to the organization's business processes and operational system whereas the dimensions surrounding them contain context about the measurement (Kimball, Ralph 2008). Another advantage offered by dimensional model is that it does not involve a relational database every time. Thus, this type of modeling technique is very useful for end-user queries in data warehouse.\n\nThe model of facts and dimensions can also be understood as data cube. Where the dimensions are the categorical coordinates in a multi-dimensional cube, while the fact is a value corresponding to the coordinates.\n\nThe main disadvantages of the dimensional approach are the following:\n\nIn the normalized approach, the data in the data warehouse are stored following, to a degree, database normalization rules. Tables are grouped together by \"subject areas\" that reflect general data categories (e.g., data on customers, products, finance, etc.). The normalized structure divides data into entities, which creates several tables in a relational database. When applied in large enterprises the result is dozens of tables that are linked together by a web of joins. Furthermore, each of the created entities is converted into separate physical tables when the database is implemented (Kimball, Ralph 2008).\nThe main advantage of this approach is that it is straightforward to add information into the database. Some disadvantages of this approach are that, because of the number of tables involved, it can be difficult for users to join data from different sources into meaningful information and to access the information without a precise understanding of the sources of data and of the data structure of the data warehouse.\n\nBoth normalized and dimensional models can be represented in entity-relationship diagrams as both contain joined relational tables. The difference between the two models is the degree of normalization (also known as Normal Forms). These approaches are not mutually exclusive, and there are other approaches. Dimensional approaches can involve normalizing data to a degree (Kimball, Ralph 2008).\n\nIn \"Information-Driven Business\", Robert Hillard proposes an approach to comparing the two approaches based on the information needs of the business problem. The technique shows that normalized models hold far more information than their dimensional equivalents (even when the same fields are used in both models) but this extra information comes at the cost of usability. The technique measures information quantity in terms of information entropy and usability in terms of the Small Worlds data transformation measure.\n\nIn the \"bottom-up\" approach, data marts are first created to provide reporting and analytical capabilities for specific business processes. These data marts can then be integrated to create a comprehensive data warehouse. The data warehouse bus architecture is primarily an implementation of \"the bus\", a collection of conformed dimensions and conformed facts, which are dimensions that are shared (in a specific way) between facts in two or more data marts.\n\nThe \"top-down\" approach is designed using a normalized enterprise data model. \"Atomic\" data, that is, data at the greatest level of detail, are stored in the data warehouse. Dimensional data marts containing data needed for specific business processes or specific departments are created from the data warehouse.\n\nData warehouses (DW) often resemble the hub and spokes architecture. Legacy systems feeding the warehouse often include customer relationship management and enterprise resource planning, generating large amounts of data. To consolidate these various data models, and facilitate the extract transform load process, data warehouses often make use of an operational data store, the information from which is parsed into the actual DW. To reduce data redundancy, larger systems often store the data in a normalized way. Data marts for specific reports can then be built on top of the data warehouse.\n\nA hybrid DW database is kept on third normal form to eliminate data redundancy. A normal relational database, however, is not efficient for business intelligence reports where dimensional modelling is prevalent. Small data marts can shop for data from the consolidated warehouse and use the filtered, specific data for the fact tables and dimensions required. The DW provides a single source of information from which the data marts can read, providing a wide range of business information. The hybrid architecture allows a DW to be replaced with a master data management repository where operational, not static information could reside.\n\nThe data vault modeling components follow hub and spokes architecture. This modeling style is a hybrid design, consisting of the best practices from both third normal form and star schema. The data vault model is not a true third normal form, and breaks some of its rules, but it is a top-down architecture with a bottom up design. The data vault model is geared to be strictly a data warehouse. It is not geared to be end-user accessible, which when built, still requires the use of a data mart or star schema based release area for business purposes.\n\nThere are basic features that define the data in the data warehouse that include subject orientation, data integration, time-variant, nonvolatile data, and data granularity.\n\nUnlike the operational systems, the data in the data warehouse revolves around subjects of the enterprise (database normalization). Subject orientation can be really useful for decision making.\nGathering the required objects is called subject oriented.\n\nThe data found within the data warehouse is integrated. Since it comes from several operational systems, all inconsistencies must be removed. Consistencies include naming conventions, measurement of variables, encoding structures, physical attributes of data, and so forth.\n\nWhile operational systems reflect current values as they support day-to-day operations, data warehouse data represents data over a long time horizon (up to 10 years) which means it stores historical data. It is mainly meant for data mining and forecasting, If a user is searching for a buying pattern of a specific customer, the user needs to look at data on the current and past purchases.\n\nThe data in the data warehouse is read-only which means it cannot be updated, created, or deleted.\n\nIn the data warehouse, data is summarized at different levels.The user may start looking at the total sale units of a product in an entire region. Then the user looks at the states in that region. Finally, they may examine the individual stores in a certain state. Therefore, typically, the analysis starts at a higher level and moves down to lower levels of details.\n\nThe different methods used to construct/organize a data warehouse specified by an organization are numerous. The hardware utilized, software created and data resources specifically required for the correct functionality of a data warehouse are the main components of the data warehouse architecture. All data warehouses have multiple phases in which the requirements of the organization are modified and fine tuned.\n\nOperational systems are optimized for preservation of data integrity and speed of recording of business transactions through use of database normalization and an entity-relationship model. Operational system designers generally follow Codd's 12 rules of database normalization to ensure data integrity. Fully normalized database designs (that is, those satisfying all Codd rules) often result in information from a business transaction being stored in dozens to hundreds of tables. Relational databases are efficient at managing the relationships between these tables. The databases have very fast insert/update performance because only a small amount of data in those tables is affected each time a transaction is processed. To improve performance, older data are usually periodically purged from operational systems.\n\nData warehouses are optimized for analytic access patterns. Analytic access patterns generally involve selecting specific fields and rarely if ever 'select *' as is more common in operational databases. Because of these differences in access patterns, operational databases (loosely, OLTP) benefit from the use of a row-oriented DBMS whereas analytics databases (loosely, OLAP) benefit from the use of a column-oriented DBMS. Unlike operational systems which maintain a snapshot of the business, data warehouses generally maintain an infinite history which is implemented through ETL processes that periodically migrate data from the operational systems over to the data warehouse.\n\nThese terms refer to the level of sophistication of a data warehouse:\n\n\n"}
{"id": "928121", "url": "https://en.wikipedia.org/wiki?curid=928121", "title": "Depository Trust &amp; Clearing Corporation", "text": "Depository Trust &amp; Clearing Corporation\n\nThe Depository Trust & Clearing Corporation (DTCC) is an American post-trade financial services company providing clearing and settlement services to the financial markets. It performs the exchange of securities on behalf of buyers and sellers and functions as a central securities depository by providing central custody of securities.\n\nDTCC was established in 1999 as a holding company to combine The Depository Trust Company (DTC) and National Securities Clearing Corporation (NSCC). User-owned and directed, it automates, centralizes, standardizes, and streamlines processes in the capital markets. Through its subsidiaries, DTCC provides clearance, settlement, and information services for equities, corporate and municipal bonds, unit investment trusts, government and mortgage-backed securities, money market instruments, and over-the-counter derivatives. It also manages transactions between mutual funds and insurance carriers and their respective investors.\n\nIn 2011, DTCC settled the vast majority of securities transactions in the United States and close to $1.7 quadrillion in value worldwide, making it by far the highest financial value processor in the world. DTCC operates facilities in the New York metropolitan area, and at multiple locations in and outside America.\n\nEstablished in 1973, The Depository Trust Company (DTC) was created to alleviate the rising volumes of paperwork and the lack of security that developed after rapid growth in the volume of transactions in the U.S. securities industry in the late 1960s.\n\nBefore DTC and NSCC were formed, brokers physically exchanged certificates, employing hundreds of messengers to carry certificates and checks. The mechanisms brokers used to transfer securities and keep records relied heavily on pen and paper. The exchange of physical stock certificates was difficult, inefficient, and increasingly expensive.\n\nIn the late 1960s, with an unprecedented surge in trading leading to volumes of nearly 15 million shares a day on the NYSE in April 1968 (as opposed to 5 million a day just three years earlier, which at the time had been considered overwhelming), the paperwork burden became enormous. Stock certificates were left for weeks piled haphazardly on any level surface, including filing cabinets and tables. Stocks were mailed to wrong addresses, or not mailed at all. Overtime and night work became mandatory. Turnover was 60% a year.\n\nTo deal with this large volume, which was overwhelming brokerage firms, the stock exchanges were forced to close every week (they chose every Wednesday), and trading hours were shortened on other days of the week.\n\nTwo things solved the crisis:\n\nThe first was to hold all paper stock certificates in one centralized location, and automate the process by keeping electronic records of all certificates and securities clearing and settlement (changes of ownership and other securities transactions). The method was first used in Austria by the Vienna Giro and Depository Association in 1872.\n\nOne problem was state laws requiring brokers to deliver certificates to investors. Eventually all the states were convinced that this notion was obsolete and changed their laws. For the most part, investors can still request their certificates, but this has several inconveniences, and most people do not, except for novelty value.\n\nThis led the New York Stock Exchange to establish the Central Certificate Service (CCS) in 1968 at 44 Broad Street in New York City. Anthony P. Reres was appointed the head of CCS. NYSE President Robert W. Haack promised: \"We are going to automate the stock certificate out of business by substituting a punch card. We just can't keep up with the flood of business unless we do\". The CCS transferred securities electronically, eliminating their physical handling for settlement purposes, and kept track of the total number of shares held by NYSE members. This relieved brokerage firms of the work of inspecting, counting, and storing certificates. Haack labeled it \"top priority\", $5 million was spent on it, and its goal was to eliminate up to 75% of the physical handling of stock certificates traded between brokers. One problem, however, was that it was voluntary, and brokers responsible for two-thirds of all trades refused to use it.\n\nBy January 1969, it was transferring 10,000 shares per day, and plans were for it to be handling broker-to-broker transactions in 1,300 issues by March 1969. In 1970 the CCS service was extended to the American Stock Exchange. This led to the development of the Banking and Securities Industry Committee (BASIC), which represented leading U.S. banks and securities exchanges, and was headed by a banker named Herman Beavis, and finally the development of DTC in 1973, which was headed by Bill Dentzer, the former New York State Banking Superintendent. All the top New York banks were represented on the board, usually by their chairman. BASIC and the SEC saw this indirect holding system as a \"temporary measure\", on the way to a \"certificateless society\".\n\nThe second method involves \"multilateral netting\"; and led to the formation of the National Securities Clearing Corporation (NSCC) in 1976.\n\nIn 2010, Robert Druskin was named Executive Chairman of the company, and in July 2012 Michael Bodson was named President and Chief Executive Officer.\n\nIn 2008, The Clearing Corporation and The Depository Trust & Clearing Corporation announced CCorp members will benefit from CCorp's netting and risk management processes, and will leverage the asset servicing capabilities of DTCC's Trade Information Warehouse for credit default swaps (CDS).\n\nOn 1 July 2010, it was announced that DTCC had acquired all of the shares of Avox Limited, based in Wrexham, North Wales. Deutsche Börse had previously held over 76% of the shares. On 20 March 2017, it was announced that Thomson Reuters acquired Avox.\n\nDTCC entered into a joint venture with the New York Stock Exchange (NYSE) known as New York Portfolio Clearing, that would allow \"investors to combine cash and derivative positions in one clearinghouse to lower margin costs\".\n\nThe DTCC supported the Customer Protection and End User Relief Act (H.R. 4413; 113th Congress), arguing that it would \"help ensure that regulators and the public continue to have access to a consolidated and accurate view of the global marketplace, including concentrations of risk and market exposure\".\n\nSeveral companies have sued the DTCC, without success, over delivery failures in their stocks, alleging culpability for naked short selling. Furthermore, the question of whether DTCC is culpable for naked short selling has been raised by Senator Robert Bennett and the NASAA, and discussed in articles in the \"Wall Street Journal\" and \"Euromoney\". DTCC contends that the suits are orchestrated by a small group of lawyers and executives to make money and draw attention from the companies' problems.\n\nCritics blame DTCC, noting that it is the organization in charge of the system where it happens, alleging that DTCC turns a blind eye to the problem, and complaining that the Securities and Exchange Commission (SEC) has not taken sufficient action against naked shorting. DTCC has responded that it has no authority over trading activities, cannot force buy-ins of shares not delivered, and suggests that naked shorting is simply not widespread enough to be a major concern. \"We're not saying there is no problem, but to suggest the sky is falling might be a bit overdone\", said DTCC's chief spokesman. The SEC, however, viewed naked shorting as a sufficiently serious matter to have made two separate efforts to restrict the practice. DTCC has said that the SEC has supported its position in legal proceedings. DTCC General Counsel Larry Thompson calls the claims that DTCC is responsible for naked short selling \"pure invention\".\n\nIn July 2007, Senator Bob Bennett, Republican of Utah, suggested on the U.S. Senate floor that the allegations involving DTCC and naked short selling are \"serious enough\" to warrant a hearing. The committee's Chairman, Senator Christopher Dodd, indicated he was willing to hold such a hearing. To date, no such hearing was ever held, and no further action on naked short selling is anticipated. Representing state stock regulators, the North American Securities Administrators Association (NASAA) filed a brief in a 2009 suit against DTCC, arguing against federal preemption as a defense to the suit. NASAA said that \"if the Investors’ claims are taken as true, as they must be on a motion to dismiss, then the entrepreneurs and investors before the Court have been the victims of fraud and manipulation at the hands of the very entities that should be serving their interests by maintaining a fair and efficient national market\". This suit was later dismissed by the courts.\n\nCritics also contend that DTCC and the SEC have been too secretive with information about where naked shorting is taking place. DTCC says it has supported releasing more information to the public.\n\nIn recent years this controversy has died down, as the impact of changes to SEC rule 203 under Regulation SHO adopted in 2008 dramatically curtailed long-term short positions and complaints about \"naked short\" positions declined.\n\nDTCC has several subsidiaries:\n\nEstablished in 1973, it was created to reduce costs and provide efficiencies by immobilizing securities and making \"book-entry\" changes to show ownership of the securities. DTC moves securities for NSCC's net settlements, and settlement for institutional trades (which typically involve money and securities transfers between custodian banks and broker-dealers), as well as money market instruments. In 2007, DTC settled transactions worth $513 trillion, and processed 325 million book-entry deliveries. In addition to settlement services, DTC retains custody of 3.5 million securities issues, worth about $40 trillion, including securities issued in the United States and more than 110 other countries. DTC is a member of the U.S. Federal Reserve System, and a registered clearing agency with the Securities and Exchange Commission.\n\nMost large U.S. broker-dealers and banks are full DTC participants, meaning that they deposit and hold securities at DTC. DTC appears in an issuer’s stock records as the sole registered owner of securities deposited at DTC. DTC holds the deposited securities in “fungible bulk”, meaning that there are no specifically identifiable shares directly owned by DTC participants. Rather, each participant owns a pro rata interest in the aggregate number of shares of a particular issuer held at DTC. Correspondingly, each customer of a DTC participant, such as an individual investor, owns a pro rata interest in the shares in which the DTC participant has an interest.\n\nBecause the securities held by DTC are for the benefit of its participants and their customers (i.e., investors holding their securities at a broker-dealer), frequently the issuer and its transfer agent must interact with DTC in order to facilitate the distribution of dividend payments to investors, to facilitate corporate actions (i.e., mergers, splits, etc.), to effect the transfer of securities, and to accurately record the number of shares actually owned by DTC at all times.\n\nStocks held by DTC are kept in the name of its partnership nominee, Cede and Company. Not all securities are eligible to be settled through DTC (\"DTC eligible\").\n\n\"What is DTC eligibility?\" This means that a company's stock is eligible for deposit with DTC aka \"Cede and Company.\" A company's security holders will be able to deposit their particular shares with a brokerage firm. Clearing firms, as full participants with DTC, handle the DTC eligibility submissions to DTC. Transfer agents were responsible for eligibility coordination years ago. Now, in order to make a new issue of securities eligible for DTC’s delivery services, a completed and signed eligibility questionnaire must be submitted to DTC’s Underwriting Department, Eligibility. Parties that may submit the questionnaire include one of the following: Lead Manager/Underwriter, Issuer’s financial advisor or the DTC Participant clearing the transaction for its correspondent. The Lead Manager/ Underwriter must ensure that DTC’s Underwriting Department receives the issue’s offering document (e.g., prospectus, offering memorandum, official statement) and the CUSIP numbers assigned to the issue within the time frames outlined in DTC’s Operational Arrangements.\n\n\"What is FAST processing?\" FAST processing is functionality that can be turned on for issuers whom are fully DTC eligible. Participation in FAST (Fast Automated Securities Transfer) allows issuers, security holders and brokerage / clearing firms to move stock electronically between one another. Transfer agents, as limited participants, file for FAST participation. DTC approves each issuer on a merit review basis into this system.\n\n\"What are “chills” and “freezes” and why does DTC impose them?\" Occasionally a problem may arise with a company or its securities on deposit at DTC. In some of those cases DTC may impose a “chill” or a “freeze” on all the company’s securities. A “chill” is a restriction placed by DTC on one or more of DTC’s services, such as limiting a DTC participant’s ability to make a deposit or withdrawal of the security at DTC. A chill may remain imposed on a security for just a few days or for an extended period of time depending upon the reasons for the chill and whether the issuer or transfer agent corrects the problem. A “freeze” is a discontinuation of all services at DTC. Freezes may last a few days or an extended period of time, depending on the reason for the freeze. If the reasons for the freeze cannot be rectified, then the security will generally be removed from DTC, and securities transactions in that security will no longer be eligible to be cleared at any registered clearing agency. Chills and freezes are monitored by DTC's Office of Regulatory Compliance.\n\nDTC imposes chills and freezes on securities for various reasons. For example, DTC may impose a chill on a security because the issuer no longer has a transfer agent to facilitate the transfer of the security or the transfer agent is not complying with DTC rules in its interactions with DTC in transferring the security. Often this type of situation is resolved within a short period of time.\n\nChills and freezes can be imposed on securities for more complicated reasons, such as when DTC determines that there may be a legal, regulatory, or operational problem with the issuance of the security, or the trading or clearing of transactions involving the security. For example, DTC may chill or freeze a security when DTC becomes aware or is informed by the issuer, transfer agent, federal or state regulators, or federal or state law enforcement officials that an issuance of some or all of the issuer’s securities or transfer in those securities is in violation of state or federal law. If DTC suspects that all or a portion of its holdings of a security may not be freely transferable as is required for DTC services, it may decide to chill one or more of its services or place a freeze on all services for the security. When there is a corporate action, DTC will temporarily chill the security for book-entry activities. In other instances, a corporate action can cause a more permanent chill. This may force the issuer to reapply for eligibility altogether.\n\nWhen DTC chills or freezes a security, it will issue a “Participant Notice” to its participants. These notices are publicly available on DTC’s website. When securities are frozen, DTC also provides optional automated notifications to its participants. These processes provide participants the ability to update their systems to automatically block future trading of affected securities, in addition to alerting participant compliance departments. DTC has information regarding these processes on its website.\n\nEstablished in 1976, it provides clearing, settlement, risk management, central counterparty services, and a guarantee of completion for certain transactions for virtually all broker-to-broker trades involving equities, corporate and municipal debt, American depositary receipts, exchange-traded funds, and unit investment trusts. NSCC also nets trades and payments among its participants, reducing the value of securities and payments that need to be exchanged by an average of 98% each day. NSCC generally clears and settles trades on a \"T+3\" basis. NSCC has roughly 4,000 participants, and is regulated by the U.S. Securities and Exchange Commission (SEC).\n\n\nFICC was created in 2003 to handle fixed income transaction processing, integrating the Government Securities Clearing Corporation and the Mortgage-Backed Securities Clearing Corporation. The Government Securities Division (GSD) provides real-time trade matching (RTTM), clearing, risk management, and netting for trades in U.S. government debt issues, including repurchase agreements or repos. Securities transactions processed by FICC's Government Securities Division include Treasury bills, bonds, notes, zero-coupon securities, government agency securities, and inflation-indexed securities. The Mortgage-Backed Securities Division provides real-time automated and trade matching, trade confirmation, risk management, netting, and electronic pool notification to the mortgage-backed securities market. Participants in this market include mortgage originators, government-sponsored enterprises, registered broker-dealers, institutional investors, investment managers, mutual funds, commercial banks, insurance companies, and other financial institutions.\n\nGCA VS simplifies announcement processing by providing a centralized source of \"scrubbed\" information about corporate actions, including tender offers, conversions, stock splits, and nearly 100 other types of events for equities and fixed-income instruments traded in Europe, Asia Pacific, and the Americas. In 2006, GCA VS processed 899,000 corporate actions from 160 countries. Managed Accounts Service, introduced in 2006, standardizes the exchange of account and investment information through a central gateway.\n\nIt provides automated matching and confirmation services for over the counter (OTC) derivatives trades, including credit, equity, and interest rate derivatives. It also provides related matching of payment flows and bilateral netting services. Deriv/SERV's customers include dealers and buy-side firms from 30 countries. In 2006, Deriv/SERV processed 2.6 million transactions.\n\nEuroCCP began operations in August 2008, initially clearing for the pan-European trading platform Turquoise. EuroCCP has subsequently secured appointments from additional trading platforms and now provides central counterparty services for equity trades to Turquoise, SmartPool, NYSE Arca Europe and Pipeline Financial Group Limited. EuroCCP clears trades in more than 6,000 equities issues for these trading venues. In October 2009 EuroCCP began clearing and settling trades made on the Turquoise platform in 120 of the most heavily traded listed Depositary Receipts.\n\nCiti Global Transaction Services acts as settlement agent for trades cleared by EuroCCP, which now provides clearing services in 15 major national markets in Europe: Austria, Belgium, France, Denmark, Germany, Ireland, Italy, Finland, Netherlands, Norway, Portugal, United Kingdom, Switzerland, Sweden and Spain. Trades are handled in seven different currencies: the Euro, British Pound, U.S. Dollar, Swiss Franc, Danish Krone, Swedish Krona, and Norwegian Krone.\n\nEuroclear (in Brussels, Belgium) and Clearstream (in Luxembourg) are the second and third largest central securities depositories in the world.\n\n\n"}
{"id": "8301", "url": "https://en.wikipedia.org/wiki?curid=8301", "title": "Distillation", "text": "Distillation\n\nDistillation is the process of separating the components or substances from a liquid mixture by using selective boiling and condensation. Distillation may result in essentially complete separation (nearly pure components), or it may be a partial separation that increases the concentration of selected components in the mixture. In either case, the process exploits differences in the volatility of the mixture's components. In industrial chemistry, distillation is a unit operation of practically universal importance, but it is a physical separation process, not a chemical reaction.\n\nDistillation has many applications. For example:\n\nAn installation used for distillation, especially of distilled beverages, is called a distillery. The distillation equipment at a distillery is a still.\n\nEarly evidence of distillation was found on Akkadian tablets dated \"circa\" 1200 BC describing perfumery operations. The tablets provided textual evidence that an early primitive form of distillation was known to the Babylonians of ancient Mesopotamia. Early evidence of distillation was also found related to alchemists working in Alexandria in Roman Egypt in the 1st century. Distilled water has been in use since at least c. 200, when Alexander of Aphrodisias described the process. Work on distilling other liquids continued in early Byzantine Egypt under Zosimus of Panopolis in the 3rd century. Distillation was practiced in the ancient Indian subcontinent, which is evident from baked clay retorts and receivers found at Taxila and Charsadda in modern Pakistan, dating back to the early centuries of the Common Era. These \"Gandhara stills\" were only capable of producing very weak liquor, as there was no efficient means of collecting the vapors at low heat. Distillation in China may have begun during the Eastern Han dynasty (1st–2nd centuries), but the distillation of beverages began in the Jin (12th–13th centuries) and Southern Song (10th–13th centuries) dynasties, according to archaeological evidence.\n\nClear evidence of the distillation of alcohol comes from the Arab chemist Al-Kindi in 9th-century Iraq. The process later spread to Italy, where it was described by the School of Salerno in the 12th century. Fractional distillation was developed by Tadeo Alderotti in the 13th century. A still was found in an archaeological site in Qinglong, Hebei province, in China, dating back to the 12th century. Distilled beverages were common during the Yuan dynasty (13th–14th centuries).\n\nIn 1500, German alchemist Hieronymus Braunschweig published \"Liber de arte destillandi\" (\"The Book of the Art of Distillation\"), the first book solely dedicated to the subject of distillation, followed in 1512 by a much expanded version. In 1651, John French published \"The Art of Distillation\", the first major English compendium on the practice, but it has been claimed that much of it derives from Braunschweig's work. This includes diagrams with people in them showing the industrial rather than bench scale of the operation.\n\nAs alchemy evolved into the science of chemistry, vessels called retorts became used for distillations. Both alembics and retorts are forms of glassware with long necks pointing to the side at a downward angle to act as air-cooled condensers to condense the distillate and let it drip downward for collection. Later, copper alembics were invented. Riveted joints were often kept tight by using various mixtures, for instance a dough made of rye flour. These alembics often featured a cooling system around the beak, using cold water, for instance, which made the condensation of alcohol more efficient. These were called pot stills. Today, the retorts and pot stills have been largely supplanted by more efficient distillation methods in most industrial processes. However, the pot still is still widely used for the elaboration of some fine alcohols, such as cognac, Scotch whisky, Irish whiskey, tequila, and some vodkas. Pot stills made of various materials (wood, clay, stainless steel) are also used by bootleggers in various countries. Small pot stills are also sold for use in the domestic production of flower water or essential oils.\n\nEarly forms of distillation involved batch processes using one vaporization and one condensation. Purity was improved by further distillation of the condensate. Greater volumes were processed by simply repeating the distillation. Chemists reportedly carried out as many as 500 to 600 distillations in order to obtain a pure compound.\n\nIn the early 19th century, the basics of modern techniques, including pre-heating and reflux, were developed. In 1822, Anthony Perrier developed one of the first continuous stills, and then, in 1826, Robert Stein improved that design to make his patent still. In 1830, Aeneas Coffey got a patent for improving the design even further. Coffey's continuous still may be regarded as the archetype of modern petrochemical units. The French engineer Armand Savalle developed his steam regulator around 1846. In 1877, Ernest Solvay was granted a U.S. Patent for a tray column for ammonia distillation, and the same and subsequent years saw developments in this theme for oils and spirits.\n\nWith the emergence of chemical engineering as a discipline at the end of the 19th century, scientific rather than empirical methods could be applied. The developing petroleum industry in the early 20th century provided the impetus for the development of accurate design methods, such as the McCabe–Thiele method by Ernest Thiele and the Fenske equation. The availability of powerful computers also allowed direct computer simulations of distillation columns.\n\nThe application of distillation can roughly be divided into four groups: laboratory scale, industrial distillation, distillation of herbs for perfumery and medicinals (herbal distillate), and food processing. The latter two are distinctively different from the former two in that distillation is not used as a true purification method but more to transfer all volatiles from the source materials to the distillate in the processing of beverages and herbs.\n\nThe main difference between laboratory scale distillation and industrial distillation is that laboratory scale distillation is often performed on a batch basis, whereas industrial distillation often occurs continuously. In batch distillation, the composition of the source material, the vapors of the distilling compounds, and the distillate change during the distillation. In batch distillation, a still is charged (supplied) with a batch of feed mixture, which is then separated into its component fractions, which are collected sequentially from most volatile to less volatile, with the bottoms – remaining least or non-volatile fraction – removed at the end. The still can then be recharged and the process repeated.\n\nIn continuous distillation, the source materials, vapors, and distillate are kept at a constant composition by carefully replenishing the source material and removing fractions from both vapor and liquid in the system. This results in a more detailed control of the separation process.\n\nThe boiling point of a liquid is the temperature at which the vapor pressure of the liquid equals the pressure around the liquid, enabling bubbles to form without being crushed. A special case is the normal boiling point, where the vapor pressure of the liquid equals the ambient atmospheric pressure.\n\nIt is a common misconception that in a liquid mixture at a given pressure, each component boils at the boiling point corresponding to the given pressure, allowing the vapors of each component to collect separately and purely. However, this does not occur, even in an idealized system. Idealized models of distillation are essentially governed by Raoult's law and Dalton's law and assume that vapor–liquid equilibria are attained.\n\nRaoult's law states that the vapor pressure of a solution is dependent on 1) the vapor pressure of each chemical component in the solution and 2) the fraction of solution each component makes up, a.k.a. the mole fraction. This law applies to ideal solutions, or solutions that have different components but whose molecular interactions are the same as or very similar to pure solutions.\n\nDalton's law states that the total pressure is the sum of the partial pressures of each individual component in the mixture. When a multi-component liquid is heated, the vapor pressure of each component will rise, thus causing the total vapor pressure to rise. When the total vapor pressure reaches the pressure surrounding the liquid, boiling occurs and liquid turns to gas throughout the bulk of the liquid. Note that a mixture with a given composition has one boiling point at a given pressure when the components are mutually soluble. A mixture of constant composition does not have multiple boiling points.\n\nAn implication of one boiling point is that lighter components never cleanly \"boil first\". At boiling point, all volatile components boil, but for a component, its percentage in the vapor is the same as its percentage of the total vapor pressure. Lighter components have a higher partial pressure and, thus, are concentrated in the vapor, but heavier volatile components also have a (smaller) partial pressure and necessarily vaporize also, albeit at a lower concentration in the vapor. Indeed, batch distillation and fractionation succeed by varying the composition of the mixture. In batch distillation, the batch vaporizes, which changes its composition; in fractionation, liquid higher in the fractionation column contains more lights and boils at lower temperatures. Therefore, starting from a given mixture, it appears to have a boiling range instead of a boiling point, although this is because its composition changes: each intermediate mixture has its own, singular boiling point.\n\nThe idealized model is accurate in the case of chemically similar liquids, such as benzene and toluene. In other cases, severe deviations from Raoult's law and Dalton's law are observed, most famously in the mixture of ethanol and water. These compounds, when heated together, form an azeotrope, which is when the vapor phase and liquid phase contain the same composition. Although there are computational methods that can be used to estimate the behavior of a mixture of arbitrary components, the only way to obtain accurate vapor–liquid equilibrium data is by measurement.\n\nIt is not possible to completely purify a mixture of components by distillation, as this would require each component in the mixture to have a zero partial pressure. If ultra-pure products are the goal, then further chemical separation must be applied. When a binary mixture is vaporized and the other component, e.g., a salt, has zero partial pressure for practical purposes, the process is simpler.\n\nHeating an ideal mixture of two volatile substances, A and B, with A having the higher volatility, or lower boiling point, in a batch distillation setup (such as in an apparatus depicted in the opening figure) until the mixture is boiling results in a vapor above the liquid that contains a mixture of A and B. The ratio between A and B in the vapor will be different from the ratio in the liquid. The ratio in the liquid will be determined by how the original mixture was prepared, while the ratio in the vapor will be enriched in the more volatile compound, A (due to Raoult's Law, see above). The vapor goes through the condenser and is removed from the system. This, in turn, means that the ratio of compounds in the remaining liquid is now different from the initial ratio (i.e., more enriched in B than in the starting liquid).\n\nThe result is that the ratio in the liquid mixture is changing, becoming richer in component B. This causes the boiling point of the mixture to rise, which results in a rise in the temperature in the vapor, which results in a changing ratio of A : B in the gas phase (as distillation continues, there is an increasing proportion of B in the gas phase). This results in a slowly changing ratio of A : B in the distillate.\n\nIf the difference in vapor pressure between the two components A and B is large – generally expressed as the difference in boiling points – the mixture in the beginning of the distillation is highly enriched in component A, and when component A has distilled off, the boiling liquid is enriched in component B.\n\nContinuous distillation is an ongoing distillation in which a liquid mixture is continuously (without interruption) fed into the process and separated fractions are removed continuously as output streams occur over time during the operation. Continuous distillation produces a minimum of two output fractions, including at least one volatile distillate fraction, which has boiled and been separately captured as a vapor and then condensed to a liquid. There is always a bottoms (or residue) fraction, which is the least volatile residue that has not been separately captured as a condensed vapor.\n\nContinuous distillation differs from batch distillation in the respect that concentrations should not change over time. Continuous distillation can be run at a steady state for an arbitrary amount of time. For any source material of specific composition, the main variables that affect the purity of products in continuous distillation are the reflux ratio and the number of theoretical equilibrium stages, in practice determined by the number of trays or the height of packing. Reflux is a flow from the condenser back to the column, which generates a recycle that allows a better separation with a given number of trays. Equilibrium stages are ideal steps where compositions achieve vapor–liquid equilibrium, repeating the separation process and allowing better separation given a reflux ratio. A column with a high reflux ratio may have fewer stages, but it refluxes a large amount of liquid, giving a wide column with a large holdup. Conversely, a column with a low reflux ratio must have a large number of stages, thus requiring a taller column.\n\nBoth batch and continuous distillations can be improved by making use of a fractionating column on top of the distillation flask. The column improves separation by providing a larger surface area for the vapor and condensate to come into contact. This helps it remain at equilibrium for as long as possible. The column can even consist of small subsystems ('trays' or 'dishes') which all contain an enriched, boiling liquid mixture, all with their own vapor–liquid equilibrium.\n\nThere are differences between laboratory-scale and industrial-scale fractionating columns, but the principles are the same. Examples of laboratory-scale fractionating columns (in increasing efficiency) include\n\nLaboratory scale distillations are almost exclusively run as batch distillations. The device used in distillation, sometimes referred to as a \"still\", consists at a minimum of a reboiler or \"pot\" in which the source material is heated, a condenser in which the heated vapour is cooled back to the liquid state, and a receiver in which the concentrated or purified liquid, called the distillate, is collected. Several laboratory scale techniques for distillation exist (see also ).\n\nIn simple distillation, the vapor is immediately channeled into a condenser. Consequently, the distillate is not pure but rather its composition is identical to the composition of the vapors at the given temperature and pressure. That concentration follows Raoult's law.\n\nAs a result, simple distillation is effective only when the liquid boiling points differ greatly (rule of thumb is 25 °C) or when separating liquids from non-volatile solids or oils. For these cases, the vapor pressures of the components are usually different enough that the distillate may be sufficiently pure for its intended purpose.\n\nFor many cases, the boiling points of the components in the mixture will be sufficiently close that Raoult's law must be taken into consideration. Therefore, fractional distillation must be used in order to separate the components by repeated vaporization-condensation cycles within a packed fractionating column. This separation, by successive distillations, is also referred to as rectification.\n\nAs the solution to be purified is heated, its vapors rise to the fractionating column. As it rises, it cools, condensing on the condenser walls and the surfaces of the packing material. Here, the condensate continues to be heated by the rising hot vapors; it vaporizes once more. However, the composition of the fresh vapors are determined once again by Raoult's law. Each vaporization-condensation cycle (called a \"theoretical plate\") will yield a purer solution of the more volatile component. In reality, each cycle at a given temperature does not occur at exactly the same position in the fractionating column; \"theoretical plate\" is thus a concept rather than an accurate description.\n\nMore theoretical plates lead to better separations. A spinning band distillation system uses a spinning band of Teflon or metal to force the rising vapors into close contact with the descending condensate, increasing the number of theoretical plates.\n\nLike vacuum distillation, steam distillation is a method for distilling compounds which are heat-sensitive. The temperature of the steam is easier to control than the surface of a heating element, and allows a high rate of heat transfer without heating at a very high temperature. This process involves bubbling steam through a heated mixture of the raw material. By Raoult's law, some of the target compound will vaporize (in accordance with its partial pressure). The vapor mixture is cooled and condensed, usually yielding a layer of oil and a layer of water.\n\nSteam distillation of various aromatic herbs and flowers can result in two products; an essential oil as well as a watery herbal distillate. The essential oils are often used in perfumery and aromatherapy while the watery distillates have many applications in aromatherapy, food processing and skin care.\n\nSome compounds have very high boiling points. To boil such compounds, it is often better to lower the pressure at which such compounds are boiled instead of increasing the temperature. Once the pressure is lowered to the vapor pressure of the compound (at the given temperature), boiling and the rest of the distillation process can commence. This technique is referred to as vacuum distillation and it is commonly found in the laboratory in the form of the rotary evaporator.\n\nThis technique is also very useful for compounds which boil beyond their decomposition temperature at atmospheric pressure and which would therefore be decomposed by any attempt to boil them under atmospheric pressure.\n\nMolecular distillation is vacuum distillation below the pressure of 0.01 torr. 0.01 torr is one order of magnitude above high vacuum, where fluids are in the free molecular flow regime, i.e. the mean free path of molecules is comparable to the size of the equipment. The gaseous phase no longer exerts significant pressure on the substance to be evaporated, and consequently, rate of evaporation no longer depends on pressure. That is, because the continuum assumptions of fluid dynamics no longer apply, mass transport is governed by molecular dynamics rather than fluid dynamics. Thus, a short path between the hot surface and the cold surface is necessary, typically by suspending a hot plate covered with a film of feed next to a cold plate with a line of sight in between. Molecular distillation is used industrially for purification of oils.\n\nSome compounds have high boiling points as well as being air sensitive. A simple vacuum distillation system as exemplified above can be used, whereby the vacuum is replaced with an inert gas after the distillation is complete. However, this is a less satisfactory system if one desires to collect fractions under a reduced pressure. To do this a \"cow\" or \"pig\" adaptor can be added to the end of the condenser, or for better results or for very air sensitive compounds a Perkin triangle apparatus can be used.\n\nThe Perkin triangle, has means via a series of glass or Teflon taps to allows fractions to be isolated from the rest of the still, without the main body of the distillation being removed from either the vacuum or heat source, and thus can remain in a state of reflux. To do this, the sample is first isolated from the vacuum by means of the taps, the vacuum over the sample is then replaced with an inert gas (such as nitrogen or argon) and can then be stoppered and removed. A fresh collection vessel can then be added to the system, evacuated and linked back into the distillation system via the taps to collect a second fraction, and so on, until all fractions have been collected.\n\nShort path distillation is a distillation technique that involves the distillate travelling a short distance, often only a few centimeters, and is normally done at reduced pressure. A classic example would be a distillation involving the distillate travelling from one glass bulb to another, without the need for a condenser separating the two chambers. This technique is often used for compounds which are unstable at high temperatures or to purify small amounts of compound. The advantage is that the heating temperature can be considerably lower (at reduced pressure) than the boiling point of the liquid at standard pressure, and the distillate only has to travel a short distance before condensing. A short path ensures that little compound is lost on the sides of the apparatus. The Kugelrohr is a kind of a short path distillation apparatus which often contain multiple chambers to collect distillate fractions.\n\nZone distillation is a distillation process in long container with partial melting of refined matter in moving liquid zone and condensation of vapor in the solid phase at condensate pulling in cold area. The process is worked in theory. When zone heater is moving from the top to the bottom of the container then solid condensate with irregular impurity distribution is forming. Then most pure part of the condensate may be extracted as product. The process may be iterated many times by moving (without turnover) the received condensate to the bottom part of the container on the place of refined matter. The irregular impurity distribution in the condensate (that is efficiency of purification) increases with number of repetitions of the process.\nZone distillation is a distillation analog of zone recrystallization. Impurity distribution in the condensate is described by known equations of zone recrystallization with various numbers of iteration of process – with replacement distribution efficient k of crystallization on separation factor α of distillation.\n\n\nThe unit process of evaporation may also be called \"distillation\":\n\nOther uses:\n\nInteractions between the components of the solution create properties unique to the solution, as most processes entail nonideal mixtures, where Raoult's law does not hold. Such interactions can result in a constant-boiling azeotrope which behaves as if it were a pure compound (i.e., boils at a single temperature instead of a range). At an azeotrope, the solution contains the given component in the same proportion as the vapor, so that evaporation does not change the purity, and distillation does not effect separation. For example, ethyl alcohol and water form an azeotrope of 95.6% at 78.1 °C.\n\nIf the azeotrope is not considered sufficiently pure for use, there exist some techniques to break the azeotrope to give a pure distillate. This set of techniques are known as azeotropic distillation. Some techniques achieve this by \"jumping\" over the azeotropic composition (by adding another component to create a new azeotrope, or by varying the pressure). Others work by chemically or physically removing or sequestering the impurity. For example, to purify ethanol beyond 95%, a drying agent (or desiccant, such as potassium carbonate) can be added to convert the soluble water into insoluble water of crystallization. Molecular sieves are often used for this purpose as well.\n\nImmiscible liquids, such as water and toluene, easily form azeotropes. Commonly, these azeotropes are referred to as a low boiling azeotrope because the boiling point of the azeotrope is lower than the boiling point of either pure component. The temperature and composition of the azeotrope is easily predicted from the vapor pressure of the pure components, without use of Raoult's law. The azeotrope is easily broken in a distillation set-up by using a liquid–liquid separator (a decanter) to separate the two liquid layers that are condensed overhead. Only one of the two liquid layers is refluxed to the distillation set-up.\n\nHigh boiling azeotropes, such as a 20 weight percent mixture of hydrochloric acid in water, also exist. As implied by the name, the boiling point of the azeotrope is greater than the boiling point of either pure component.\n\nTo break azeotropic distillations and cross distillation boundaries, such as in the DeRosier Problem, it is necessary to increase the composition of the light key in the distillate.\n\nThe boiling points of components in an azeotrope overlap to form a band. By exposing an azeotrope to a vacuum or positive pressure, it's possible to bias the boiling point of one component away from the other by exploiting the differing vapour pressure curves of each; the curves may overlap at the azeotropic point, but are unlikely to be remain identical further along the pressure axis either side of the azeotropic point. When the bias is great enough, the two boiling points no longer overlap and so the azeotropic band disappears.\n\nThis method can remove the need to add other chemicals to a distillation, but it has two potential drawbacks.\n\nUnder negative pressure, power for a vacuum source is needed and the reduced boiling points of the distillates requires that the condenser be run cooler to prevent distillate vapours being lost to the vacuum source. Increased cooling demands will often require additional energy and possibly new equipment or a change of coolant.\n\nAlternatively, if positive pressures are required, standard glassware can not be used, energy must be used for pressurization and there is a higher chance of side reactions occurring in the distillation, such as decomposition, due to the higher temperatures required to effect boiling.\n\nA unidirectional distillation will rely on a pressure change in one direction, either positive or negative.\n\nPressure-swing distillation is essentially the same as the unidirectional distillation used to break azeotropic mixtures, but here both positive and negative pressures may be employed.\n\nThis improves the selectivity of the distillation and allows a chemist to optimize distillation by avoiding extremes of pressure and temperature that waste energy. This is particularly important in commercial applications.\n\nOne example of the application of pressure-swing distillation is during the industrial purification of ethyl acetate after its catalytic synthesis from ethanol.\n\nLarge scale industrial distillation applications include both batch and continuous fractional, vacuum, azeotropic, extractive, and steam distillation. The most widely used industrial applications of continuous, steady-state fractional distillation are in petroleum refineries, petrochemical and chemical plants and natural gas processing plants.\n\nTo control and optimize such industrial distillation, a standardized laboratory method, ASTM D86, is established. This test method extends to the atmospheric distillation of petroleum products using a laboratory batch distillation unit to quantitatively determine the boiling range characteristics of petroleum products.\n\nIndustrial distillation is typically performed in large, vertical cylindrical columns known as distillation towers or distillation columns with diameters ranging from about 65 centimeters to 16 meters and heights ranging from about 6 meters to 90 meters or more. When the process feed has a diverse composition, as in distilling crude oil, liquid outlets at intervals up the column allow for the withdrawal of different \"fractions\" or products having different boiling points or boiling ranges. The \"lightest\" products (those with the lowest boiling point) exit from the top of the columns and the \"heaviest\" products (those with the highest boiling point) exit from the bottom of the column and are often called the bottoms.\nIndustrial towers use reflux to achieve a more complete separation of products. Reflux refers to the portion of the condensed overhead liquid product from a distillation or fractionation tower that is returned to the upper part of the tower as shown in the schematic diagram of a typical, large-scale industrial distillation tower. Inside the tower, the downflowing reflux liquid provides cooling and condensation of the upflowing vapors thereby increasing the efficiency of the distillation tower. The more reflux that is provided for a given number of theoretical plates, the better the tower's separation of lower boiling materials from higher boiling materials. Alternatively, the more reflux that is provided for a given desired separation, the fewer the number of theoretical plates required. Chemical engineers must choose what combination of reflux rate and number of plates is both economically and physically feasible for the products purified in the distillation column.\n\nSuch industrial fractionating towers are also used in cryogenic air separation, producing liquid oxygen, liquid nitrogen, and high purity argon. Distillation of chlorosilanes also enables the production of high-purity silicon for use as a semiconductor.\nDesign and operation of a distillation tower depends on the feed and desired products. Given a simple, binary component feed, analytical methods such as the McCabe–Thiele method or the Fenske equation can be used. For a multi-component feed, simulation models are used both for design and operation. Moreover, the efficiencies of the vapor–liquid contact devices (referred to as \"plates\" or \"trays\") used in distillation towers are typically lower than that of a theoretical 100% efficient equilibrium stage. Hence, a distillation tower needs more trays than the number of theoretical vapor–liquid equilibrium stages. A variety of models have been postulated to estimate tray efficiencies.\n\nIn modern industrial uses, a packing material is used in the column instead of trays when low pressure drops across the column are required. Other factors that favor packing are: vacuum systems, smaller diameter columns, corrosive systems, systems prone to foaming, systems requiring low liquid holdup, and batch distillation. Conversely, factors that favor plate columns are: presence of solids in feed, high liquid rates, large column diameters, complex columns, columns with wide feed composition variation, columns with a chemical reaction, absorption columns, columns limited by foundation weight tolerance, low liquid rate, large turn-down ratio and those processes subject to process surges.\n\nThis packing material can either be random dumped packing (1–3\" wide) such as Raschig rings or structured sheet metal. Liquids tend to wet the surface of the packing and the vapors pass across this wetted surface, where mass transfer takes place. Unlike conventional tray distillation in which every tray represents a separate point of vapor–liquid equilibrium, the vapor–liquid equilibrium curve in a packed column is continuous. However, when modeling packed columns, it is useful to compute a number of \"theoretical stages\" to denote the separation efficiency of the packed column with respect to more traditional trays. Differently shaped packings have different surface areas and void space between packings. Both of these factors affect packing performance.\n\nAnother factor in addition to the packing shape and surface area that affects the performance of random or structured packing is the liquid and vapor distribution entering the packed bed. The number of theoretical stages required to make a given separation is calculated using a specific vapor to liquid ratio. If the liquid and vapor are not evenly distributed across the superficial tower area as it enters the packed bed, the liquid to vapor ratio will not be correct in the packed bed and the required separation will not be achieved. The packing will appear to not be working properly. The height equivalent to a theoretical plate (HETP) will be greater than expected. The problem is not the packing itself but the mal-distribution of the fluids entering the packed bed. Liquid mal-distribution is more frequently the problem than vapor. The design of the liquid distributors used to introduce the feed and reflux to a packed bed is critical to making the packing perform to it maximum efficiency. Methods of evaluating the effectiveness of a liquid distributor to evenly distribute the liquid entering a packed bed can be found in references. Considerable work has been done on this topic by Fractionation Research, Inc. (commonly known as FRI).\n\nThe goal of multi-effect distillation is to increase the energy efficiency of the process, for use in desalination, or in some cases one stage in the production of ultrapure water. The number of effects is inversely proportional to the kW·h/m of water recovered figure, and refers to the volume of water recovered per unit of energy compared with single-effect distillation. One effect is roughly 636 kW·h/m.\n\nThere are many other types of multi-effect distillation processes, including one referred to as simply multi-effect distillation (MED), in which multiple chambers, with intervening heat exchangers, are employed.\n\nCarbohydrate-containing plant materials are allowed to ferment, producing a dilute solution of ethanol in the process. Spirits such as whiskey and rum are prepared by distilling these dilute solutions of ethanol. Components other than ethanol, including water, esters, and other alcohols, are collected in the condensate, which account for the flavor of the beverage. Some of these beverages are then stored in barrels or other containers to acquire more flavor compounds and characteristic flavors.\n\n\n\n\n"}
{"id": "4236604", "url": "https://en.wikipedia.org/wiki?curid=4236604", "title": "ECIB", "text": "ECIB\n\nEcib is the short form of Electronic Credit Information Bureau, which is located in Pakistan.\n\neCIB is a software for monitoring credit reports for Pakistan. The State Bank of Pakistan monitors the Software and all the Financial Institutions in Pakistan are required to have this software installed. A monthly process updates the reports at the State Bank end.\n\nThe purpose of Electronic Credit Information Bureau (eCIB) is to capture credit data and to provide online information of consumer and corporate borrowers to the financial industry. For financial institutions, eCIB is divided into two areas:\n\n\nThe application can be accessed both through the Internet based VPN network and dialup connection.\n\n"}
{"id": "3818247", "url": "https://en.wikipedia.org/wiki?curid=3818247", "title": "Electronic kit", "text": "Electronic kit\n\nAn electronic kit is a package of electrical components used to build an electronic device. Generally, kits are composed of electronic components, a circuit diagram (schematic), assembly instructions and often a printed circuit board (PCB) or another type of prototyping board.\n\nThere are two types of kit. Some build a single device or system. Other types used for education demonstrate a range of circuits. Theses will include a solderless construction board of some type, such as:\n\nThe first type of kits, those for the construction of a single device, normally use a PCB on which components are soldered. They normally come with extended documentation describing which component goes where into the PCB. \n\nFor advanced hobby projects, sometimes the kit may only consist of a printed circuit board and assembly instructions, and the purchaser may have to source all the parts independently; or, the vendor may provide hard-to-get or pre-programmed parts while expecting the purchaser to obtain the rest of the components. \n\nPeople primarily purchase electronic kits to have fun and learn how things work. They were once popular as a means to reduce the cost of buying goods, but there is usually no cost saving in buying a kit today.\n\nSome electronic kits were assembled to make complete complex devices such as color television sets, oscilloscopes, high-end audio amplifiers, amateur radio equipment, electric organs, and even computers such as the Heathkit H-8, and the LNW-80. Many of the early microprocessor computers were sold as either electronic kits or assembled and tested. Heathkit sold millions of electronic kits during its 45-year history.\n\nHome assembly of common consumer electronics items no longer provides a cost advantage over commercially manufactured and distributed devices. People still build kits for custom devices and special-purpose electronics for professional and educational use, and as a hobby.\n\nAlso emerging is a trend to simplify the complexity by providing preprogrammed or modular kits and this is provided by many suppliers online often. The fun and thrill of making your own electronics has shifted in many cases from easy to comprehend application and analog devices to more sophisticated and digital devices.\n\n\n\n"}
{"id": "2935251", "url": "https://en.wikipedia.org/wiki?curid=2935251", "title": "Energy industry", "text": "Energy industry\n\nThe energy industry is the totality of all of the industries involved in the production and sale of energy, including fuel extraction, manufacturing, refining and distribution. Modern society consumes large amounts of fuel, and the energy industry is a crucial part of the infrastructure and maintenance of society in almost all countries.\n\nIn particular, the energy industry comprises:\n\nThe use of energy has been a key in the development of the human society by helping it to control and adapt to the environment. Managing the use of energy is inevitable in any functional society. In the industrialized world the development of energy resources has become essential for agriculture, transportation, waste collection, information technology, communications that have become prerequisites of a developed society. The increasing use of energy since the Industrial Revolution has also brought with it a number of serious problems, some of which, such as global warming, present potentially grave risks to the world.\n\nIn some industries, the word \"energy\" is used as a synonym of energy resources, which refer to substances like fuels, petroleum products and electricity in general, because a significant portion of the energy contained in these resources can easily be extracted to serve a useful purpose. After a useful process has taken place, the total energy is conserved, but the ressource itself is not conserved, since a process usually transforms the energy into unusable forms (such as unnecessary or excess heat).\n\nEver since humanity discovered various energy resources available in nature, it has been inventing devices, known as machines, that make life more comfortable by using energy resources. Thus, although the primitive man knew the utility of fire to cook food, the invention of devices like gas burners and microwave ovens has increased the usage of energy for this purpose alone manyfold. The trend is the same in any other field of social activity, be it construction of social infrastructure, manufacturing of fabrics for covering; porting; printing; decorating, for example textiles, air conditioning; communication of information or for moving people and goods (automobiles).\n\nProduction and consumption of energy resources is very important to the global economy. All economic activity requires energy resources, whether to manufacture goods, provide transportation, run computers and other machines.\n\nWidespread demand for energy may encourage competing energy utilities and the formation of retail energy markets. Note the presence of the \"Energy Marketing and Customer Service\" (EMACS) sub-sector.\n\nThe energy sector accounts for 4.6% of outstanding leveraged loans, compared with 3.1% a decade ago, while energy bonds make up 15.7% of the $1.3trillion junk bond market, up from 4.3% over the same period.\n\nSince the cost of energy has become a significant factor in the performance of economy of societies, management of energy resources has become very crucial. Energy management involves utilizing the available energy resources more effectively that is with minimum incremental costs. Many times it is possible to save expenditure on energy without incorporating fresh technology by simple management techniques. Most often energy management is the practice of using energy more efficiently by eliminating energy wastage or to balance justifiable energy demand with appropriate energy supply. The process couples energy awareness with energy conservation.\n\nThe United Nations developed the International Standard Industrial Classification, which is a list of economic and social classifications. There is no distinct classification for an energy industry, because the classification system is based on \"activities\", \"products\", and \"expenditures according to purpose\".\n\nCountries in North America use the North American Industry Classification System (NAICS). The NAICS sectors #21 and #22 (mining and utilities) might roughly define the energy industry in North America. This classification is used by the U.S. Securities and Exchange Commission.\n\nThe Global Industry Classification Standard used by Morgan Stanley define the energy industry as comprising companies primarily working with oil, gas, coal and consumable fuels, excluding companies working with certain industrial gases.\nAdd also to expand this section: Dow Jones Industrial Average\n\nGovernment encouragement in the form of subsidies and tax incentives for energy-conservation efforts has increasingly fostered the view of conservation as a major function of the energy industry: saving an amount of energy provides economic benefits almost identical to generating that same amount of energy. This is compounded by the fact that the economics of delivering energy tend to be priced for capacity as opposed to average usage. One of the purposes of a smart grid infrastructure is to smooth out demand so that capacity and demand curves align more closely.\nSome parts of the energy industry generate considerable pollution, including toxic and greenhouse gases from fuel combustion, nuclear waste from the generation of nuclear power, and oil spillages as a result of petroleum extraction. Government regulations to internalize these externalities form an increasing part of doing business, and the trading of carbon credits and pollution credits on the free market may also result in energy-saving and pollution-control measures becoming even more important to energy providers.\n\nConsumption of energy resources, (e.g. turning on a light) requires resources and has an effect on the environment. Many electric power plants burn coal, oil or natural gas in order to generate electricity for energy needs. While burning these fossil fuels produces a readily available and instantaneous supply of electricity, it also generates air pollutants including carbon dioxide (CO), sulfur dioxide and trioxide (SOx) and nitrogen oxides (NOx). Carbon dioxide is an important greenhouse gas which is thought to be responsible for some fraction of the rapid increase in climate change seen especially in the temperature records in the 20th century, as compared with tens of thousands of years worth of temperature records which can be read from ice cores taken in Arctic regions. Burning fossil fuels for electricity generation also releases trace metals such as beryllium, cadmium, chromium, copper, manganese, mercury, nickel, and silver into the environment, which also act as pollutants.\n\nThe large-scale use of renewable energy technologies would \"greatly mitigate or eliminate a wide range of environmental and human health impacts of energy use\". Renewable energy technologies include biofuels, solar heating and cooling, hydroelectric power, solar power, and wind power. Energy conservation and the efficient use of energy would also help.\n\nIn addition, it is argued that there is also the potential to develop a more efficient energy sector. This can be done by:\nBest available technology (BAT) offers supply-side efficiency levels far higher than global averages. The relative benefits of gas compared to coal are influenced by the development of increasingly efficient energy production methods. According to an impact assessment carried out for the European Commission, the levels of energy efficiency of coal-fired plants built have now increased to 46-49% efficiency rates, as compared to coals plants built before the 1990s (32-40%). However, at the same time gas can reach 58-59% efficiency levels with the best available technology. Meanwhile, combined heat and power can offer efficiency rates of 80-90%.\n\nSince now energy plays an essential role in industrial societies, the ownership and control of energy resources plays an increasing role in politics. At the national level, governments seek to influence the sharing (distribution) of energy resources among various sections of the society through pricing mechanisms; or even who owns resources within their borders. They may also seek to influence the use of energy by individuals and business in an attempt to tackle environmental issues.\n\nThe most recent international political controversy regarding energy resources is in the context of the Iraq Wars. Some political analysts maintain that the hidden reason for both 1991 and 2003 wars can be traced to strategic control of international energy resources. Others counter this analysis with the numbers related to its economics. According to the latter group of analysts, U.S. has spent about $336 billion in Iraq as compared with a background current value of $25 billion per year budget for the entire U.S. oil import dependence\n\nEnergy policy is the manner in which a given entity (often governmental) has decided to address issues of energy development including energy production, distribution and consumption. The attributes of energy policy may include legislation, international treaties, incentives to investment, guidelines for energy conservation, taxation and other public policy techniques.\n\nEnergy security is the intersection of national security and the availability of natural resources for energy consumption. Access to cheap energy has become essential to the functioning of modern economies. However, the uneven distribution of energy supplies among countries has led to significant vulnerabilities. Threats to energy security include the political instability of several energy producing countries, the manipulation of energy supplies, the competition over energy sources, attacks on supply infrastructure, as well as accidents, natural disasters, the funding to foreign dictators, rising terrorism, and dominant countries reliance to the foreign oil supply. The limited supplies, uneven distribution, and rising costs of fossil fuels, such as oil and gas, create a need to change to more sustainable energy sources in the foreseeable future. With as much dependence that the U.S. currently has for oil and with the peaking limits of oil production; economies and societies will begin to feel the decline in the resource that we have become dependent upon. Energy security has become one of the leading issues in the world today as oil and other resources have become as vital to the world's people. However, with oil production rates decreasing and oil production peak nearing the world has come to protect what resources we have left in the world. With new advancements in renewable resources less pressure has been put on companies that produce the world's oil, these resources are, geothermal, solar power, wind power and hydro-electric. Although these are not all the current and possible future options for the world to turn to as the oil depletes the most important issue is protecting these vital resources from future threats. These new resources will become more useful as the price of exporting and importing oil will increase due to increase of demand.\n\nProducing energy to sustain human needs is an essential social activity, and a great deal of effort goes into the activity. While most of such effort is limited towards increasing the production of electricity and oil, newer ways of producing usable energy resources from the available energy resources are being explored. One such effort is to explore means of producing hydrogen fuel from water. Though hydrogen use is environmentally friendly, its production requires energy and existing technologies to make it, are not very efficient. Research is underway to explore enzymatic decomposition of biomass.\n\nOther forms of conventional energy resources are also being used in new ways. Coal gasification and liquefaction are recent technologies that are becoming attractive after the realization that oil reserves, at present consumption rates, may be rather short lived. See alternative fuels.\n\nEnergy is the subject of significant research activities globally. For example, the UK Energy Research Centre is the focal point for UK energy research while the European Union has many technology programmes as well as a platform for engaging social science and humanities within energy research.\n\nAll societies require materials and food to be transported over \"distances\", generally against some \"force\" of friction. Since application of force over distance requires the presence of a source of usable energy, such sources are of great worth in society.\n\nWhile energy resources are an essential ingredient for all modes of transportation in society, the transportation of energy resources is becoming equally important. Energy resources are frequently located far from the place where they are consumed. Therefore, their transportation is always in question. Some energy resources like liquid or gaseous fuels are transported using tankers or pipelines, while electricity transportation invariably requires a network of grid cables. The transportation of energy, whether by tanker, pipeline, or transmission line, poses challenges for scientists and engineers, policy makers, and economists to make it more risk-free and efficient.\n\nEconomic and political instability can lead to an energy crisis. Notable oil crises are the 1973 oil crisis and the 1979 oil crisis. The advent of peak oil, the point in time when the maximum rate of global petroleum extraction is reached, will likely precipitate another energy crisis.\n\nBetween 1985 and 2018 there have been around 69,932 deals in the energy sector. This cumulates to an overall value of 9,578 bil USD. The most active year was 2010 with about 3.761 deals. In terms of value 2007 was the strongest year (684 bil. USD), which was followed by a steep decline until 2009 (-55,8%).\n\nHere is a list of the top 10 deals in history in the energy sector:\n\n\n"}
{"id": "57981357", "url": "https://en.wikipedia.org/wiki?curid=57981357", "title": "Eve Systems", "text": "Eve Systems\n\nEve Systems (branded as simply Eve) is a German smart home and home automation producer founded in June 27, 2018. The brand was originally a line of smart home products of manufacturer Elgato Systems, a company best known for a line of video-recording and gaming products. The Elgato brand and gaming division of the company was sold to Corsair Components in June 2018, while the Eve brand was spun off into a separate company.\n\nThe brand can be traced back in 2014 when Elgato introduced a home monitoring system called Eve, which provides alerts to users regarding things like air pressure, temperature and water use. Elgato also developed light bulbs that can respond to programming on a mobile device and respond to commands over Bluetooth. In May 2018, Elgato introduced Eve Aqua, a smart water controller.\n\nIn June 2018, Corsair acquired the gaming departments of Elgato, while Elgato's smart home team remains their own company and was spun off as Eve Systems.\n\nElgato manufactures and markets a smart-key system. The system comes with a small 10-gram device that is placed on a key ring, in a purse, inside a car, or somewhere else. Then it communicates with an Elgato app on an iOS device. If it is set up for keys, the app will alert the user when they are 10 meters away from their keys, indicating that they may have forgotten them. It takes advantage of the \"Smart Bluetooth\" Apple implemented in iOS 7. A review in TheNextWeb said it was \"money well spent\" and worked \"exactly as described\", but that the beeping of the device could be louder and users will still need to supplement it with the Find my Phone app. A review in Macworld gave it 4 out of 5 stars.\n\nIn September 2014, Elgato announced a home monitoring system called Eve, which monitors a home's air pressure, water usage, temperature, air quality and other factors. Elgato said the product won't be available until the HomeKit software, which is expected to come with Apple iOS 8, is released. It also introduced smart light bulbs, which communicate with iOS devices through Bluetooth and allow users to adjust home lighting from their mobile device.\n\nAs of July 2018, the current series of smart home products includes:\n\n\nIn late 2014, Elgato introduced the Smart Power battery backup for mobile devices. It communicates with the user's bluetooth-enabled device to provide notifications and calendar reminders when it needs to be charged.\n\nIn 2016, Elgato released Eve Energy, a smart plug which provides power meter features through its mobile app.\n"}
{"id": "344123", "url": "https://en.wikipedia.org/wiki?curid=344123", "title": "Heat sink", "text": "Heat sink\n\nA heat sink (also commonly spelled heatsink) is a passive heat exchanger that transfers the heat generated by an electronic or a mechanical device to a fluid medium, often air or a liquid coolant, where it is dissipated away from the device, thereby allowing regulation of the device's temperature at optimal levels. In computers, heat sinks are used to cool central processing units or graphics processors. Heat sinks are used with high-power semiconductor devices such as power transistors and optoelectronics such as lasers and light emitting diodes (LEDs), where the heat dissipation ability of the component itself is insufficient to moderate its temperature.\n\nA heat sink is designed to maximize its surface area in contact with the cooling medium surrounding it, such as the air. Air velocity, choice of material, protrusion design and surface treatment are factors that affect the performance of a heat sink. Heat sink attachment methods and thermal interface materials also affect the die temperature of the integrated circuit. Thermal adhesive or thermal grease improve the heat sink's performance by filling air gaps between the heat sink and the heat spreader on the device. A heat sink is usually made out of copper or aluminium. Copper is used because it has many desirable properties for thermally efficient and durable heat exchangers. First and foremost, copper is an excellent conductor of heat. This means that copper's high thermal conductivity allows heat to pass through it quickly. Aluminium heat sinks are used as a low-cost, lightweight alternative to copper heat sinks, and have a lower thermal conductivity than copper.\n\nA heat sink transfers thermal energy from a higher temperature device to a lower temperature fluid medium. The fluid medium is frequently air, but can also be water, refrigerants or oil. If the fluid medium is water, the heat sink is frequently called a cold plate. In thermodynamics a heat sink is a heat reservoir that can absorb an arbitrary amount of heat without significantly changing temperature. Practical heat sinks for electronic devices must have a temperature higher than the surroundings to transfer heat by convection, radiation, and conduction. The power supplies of electronics are not 100% efficient, so extra heat is produced that may be detrimental to the function of the device. As such, a heat sink is included in the design to disperse heat. \n\nTo understand the principle of a heat sink, consider Fourier's law of heat conduction. Fourier's law of heat conduction, simplified to a one-dimensional form in the \"x\"-direction, shows that when there is a temperature gradient in a body, heat will be transferred from the higher temperature region to the lower temperature region. The rate at which heat is transferred by conduction, formula_1, is proportional to the product of the temperature gradient and the cross-sectional area through which heat is transferred.\n\nConsider a heat sink in a duct, where air flows through the duct. It is assumed that the heat sink base is higher in temperature than the air. Applying the conservation of energy, for steady-state conditions, and Newton’s law of cooling to the temperature nodes shown in the diagram gives the following set of equations:\n\nwhere\n\nUsing the mean air temperature is an assumption that is valid for relatively short heat sinks. When compact heat exchangers are calculated, the logarithmic mean air temperature is used. formula_6 is the air mass flow rate in kg/s.\n\nThe above equations show that\n\nNatural convection requires free flow of air over the heat sink. If fins are not aligned vertically, or if fins are too close together to allow sufficient air flow between them, the efficiency of the heat sink will decline.\n\nFor semiconductor devices used in a variety of consumer and industrial electronics, the idea of \"thermal resistance\" simplifies the selection of heat sinks. The heat flow between the semiconductor die and ambient air is modeled as a series of resistances to heat flow; there is a resistance from the die to the device case, from the case to the heat sink, and from the heat sink to the ambient air. The sum of these resistances is the total thermal resistance from the die to the ambient air. Thermal resistance is defined as temperature rise per unit of power, analogous to electrical resistance, and is expressed in units of degrees Celsius per watt (°C/W). If the device dissipation in watts is known, and the total thermal resistance is calculated, the temperature rise of the die over the ambient air can be calculated.\n\nThe idea of thermal resistance of a semiconductor heat sink is an approximation. It does not take into account non-uniform distribution of heat over a device or heat sink. It only models a system in thermal equilibrium, and does not take into account the change in temperatures with time. Nor does it reflect the non-linearity of radiation and convection with respect to temperature rise. However, manufacturers tabulate typical values of thermal resistance for heat sinks and semiconductor devices, which allows selection of commercially manufactured heat sinks to be simplified.\n\nCommercial extruded aluminium heat sinks have a thermal resistance (heat sink to ambient air) ranging from for a large sink meant for TO3 devices, up to as high as for a clip-on heat sink for a TO92 small plastic case. The popular 2N3055 power transistor in a TO3 case has an internal thermal resistance from junction to case of . The contact between the device case and heat sink may have a thermal resistance of between , depending on the case size, and use of grease or insulating mica washer.\n\nThe most common heat sink materials are aluminium alloys. Aluminium alloy 1050 has one of the higher thermal conductivity values at 229 W/m•K but is mechanically soft. Aluminium alloys 6060 (low stress), 6061, and 6063 are commonly used, with thermal conductivity values of 166 and 201 W/m•K, respectively. The values depend on the temper of the alloy. Aluminium heat sinks can be cast or extruded.\n\nCopper has excellent heat sink properties in terms of its thermal conductivity, corrosion resistance, biofouling resistance, and antimicrobial resistance \"(See also Copper in heat exchangers)\". Copper has around twice the thermal conductivity of aluminium, around 400 W/m•K for pure copper. Its main applications are in industrial facilities, power plants, solar thermal water systems, HVAC systems, gas water heaters, forced air heating and cooling systems, geothermal heating and cooling, and electronic systems.\n\nCopper is three times as dense and more expensive than aluminium. Copper heat sinks are machined and skived. Another method of manufacture is to solder the fins into the heat sink base. Copper is less ductile than Aluminium, so it cannot be extruded into heat sinks.\n\nFin efficiency is one of the parameters which makes a higher thermal conductivity material important. A fin of a heat sink may be considered to be a flat plate with heat flowing in one end and being dissipated into the surrounding fluid as it travels to the other. As heat flows through the fin, the combination of the thermal resistance of the heat sink impeding the flow and the heat lost due to convection, the temperature of the fin and, therefore, the heat transfer to the fluid, will decrease from the base to the end of the fin. Fin efficiency is defined as the actual heat transferred by the fin, divided by the heat transfer were the fin to be isothermal (hypothetically the fin having infinite thermal conductivity). Equations 6 and 7 are applicable for straight fins:\n\nWhere:\nFin efficiency is increased by decreasing the fin aspect ratio (making them thicker or shorter), or by using more conductive material (copper instead of aluminium, for example).\n\nAnother parameter that concerns the thermal conductivity of the heat sink material is spreading resistance. Spreading resistance occurs when thermal energy is transferred from a small area to a larger area in a substance with finite thermal conductivity. In a heat sink, this means that heat does not distribute uniformly through the heat sink base. The spreading resistance phenomenon is shown by how the heat travels from the heat source location and causes a large temperature gradient between the heat source and the edges of the heat sink. This means that some fins are at a lower temperature than if the heat source were uniform across the base of the heat sink. This nonuniformity increases the heat sink's effective thermal resistance.\n\nTo decrease the spreading resistance in the base of a heat sink:\n\nA pin fin heat sink is a heat sink that has pins that extend from its base. The pins can be cylindrical, elliptical or square. A pin is one of the more common heat sink types available on the market. A second type of heat sink fin arrangement is the straight fin. These run the entire length of the heat sink. A variation on the straight fin heat sink is a cross cut heat sink. A straight fin heat sink is cut at regular intervals.\n\nIn general, the more surface area a heat sink has, the better it works. However, this is not always true. The concept of a pin fin heat sink is to try to pack as much surface area into a given volume as possible. As well, it works well in any orientation. Kordyban has compared the performance of a pin fin and a straight fin heat sink of similar dimensions. Although the pin fin has 194 cm surface area while the straight fin has 58 cm, the temperature difference between the heat sink base and the ambient air for the pin fin is . For the straight fin it was 44 °C or 6 °C better than the pin fin. Pin fin heat sink performance is significantly better than straight fins when used in their intended application where the fluid flows axially along the pins (see ) rather than only tangentially across the pins.\n\nAnother configuration is the flared fin heat sink; its fins are not parallel to each other, as shown in figure 5. Flaring the fins decreases flow resistance and makes more air go through the heat sink fin channel; otherwise, more air would bypass the fins. Slanting them keeps the overall dimensions the same, but offers longer fins. Forghan, et al. have published data on tests conducted on pin fin, straight fin and flared fin heat sinks. They found that for low approach air velocity, typically around 1 m/s, the thermal performance is at least 20% better than straight fin heat sinks. Lasance and Eggink also found that for the bypass configurations that they tested, the flared heat sink performed better than the other heat sinks tested. \n\nCavities (inverted fins) embedded in a heat source are the regions formed between adjacent fins that stand for the essential promoters of nucleate boiling or condensation. These cavities are usually utilized to extract heat from a variety of heat generating bodies to a heat sink.\n\nPlacing a conductive thick plate as a heat transfer interface between a heat source and a cold flowing fluid (or any other heat sink) may improve the cooling performance. In such arrangement, the heat source is cooled under the thick plate instead of being cooled in direct contact with the cooling fluid. It is shown that the thick plate can significantly improve the heat transfer between the heat source and the cooling fluid by way of conducting the heat current in an optimal manner. The two most attractive advantages of this method are that no additional pumping power and no extra heat transfer surface area, that is quite different from fins (extended surfaces).\n\nThe heat transfer from the heat sink occurs by convection of the surrounding air, conduction through the air, and radiation.\n\nHeat transfer by radiation is a function of both the heat sink temperature, and the temperature of the surroundings that the heat sink is optically coupled with. When both of these temperatures are on the order of 0 °C to 100 °C, the contribution of radiation compared to convection is generally small, and this factor is often neglected. In this case, finned heat sinks operating in either natural-convection or forced-flow will not be affected significantly by surface emissivity.\n\nIn situations where convection is low, such as a flat non-finned panel with low airflow, radiative cooling can be a significant factor. Here the surface properties may be an important design factor. Matte-black surfaces will radiate much more efficiently than shiny bare metal. A shiny metal surface has low emissivity. The emissivity of a material is tremendously frequency dependent, and is related to absorptivity (of which shiny metal surfaces have very little). For most materials, the emissivity in the visible spectrum is similar to the emissivity in the infrared spectrum; however there are exceptions, notably certain metal oxides that are used as \"selective surfaces\".\n\nIn a vacuum or in outer space, there is no convective heat transfer, thus in these environments, radiation is the only factor governing heat flow between the heat sink and the environment. For a satellite in space, a 100 °C (373 Kelvin) surface facing the Sun will absorb a lot of radiant heat, because the Sun's surface temperature is nearly 6000 Kelvin, whereas the same surface facing deep-space will radiate a lot of heat, since deep-space has an effective temperature of only a few Kelvin.\n\nHeat dissipation is an unavoidable by-product of electronic devices and circuits. In general, the temperature of the device or component will depend on the thermal resistance from the component to the environment, and the heat dissipated by the component. To ensure that the component does not overheat, a thermal engineer seeks to find an efficient heat transfer path from the device to the environment. The heat transfer path may be from the component to a printed circuit board (PCB), to a heat sink, to air flow provided by a fan, but in all instances, eventually to the environment.\n\nTwo additional design factors also influence the thermal/mechanical performance of the thermal design:\n\nAs power dissipation of components increases and component package size decreases, thermal engineers must innovate to ensure components won't overheat. Devices that run cooler last longer. A heat sink design must fulfill both its thermal as well as its mechanical requirements. Concerning the latter, the component must remain in thermal contact with its heat sink with reasonable shock and vibration. The heat sink could be the copper foil of a circuit board, or a separate heat sink mounted onto the component or circuit board. Attachment methods include thermally conductive tape or epoxy, wire-form z clips, flat spring clips, standoff spacers, and push pins with ends that expand after installing.\n\nThermally conductive tape is one of the most cost-effective heat sink attachment materials. It is suitable for low-mass heat sinks and for components with low power dissipation. It consists of a thermally conductive carrier material with a pressure-sensitive adhesive on each side.\n\nThis tape is applied to the base of the heat sink, which is then attached to the component. Following are factors that influence the performance of thermal tape:\n\nEpoxy is more expensive than tape, but provides a greater mechanical bond between the heat sink and component, as well as improved thermal conductivity. The epoxy chosen must be formulated for this purpose. Most epoxies are two-part liquid formulations that must be thoroughly mixed before being applied to the heat sink, and before the heat sink is placed on the component. The epoxy is then cured for a specified time, which can vary from 2 hours to 48 hours. Faster cure time can be achieved at higher temperatures. The surfaces to which the epoxy is applied must be clean and free of any residue.\n\nThe epoxy bond between the heat sink and component is semi-permanent/permanent. This makes re-work very difficult and at times impossible. The most typical damage caused by rework is the separation of the component die heat spreader from its package.\nMore expensive than tape and epoxy, wire form z-clips attach heat sinks mechanically. To use the z-clips, the printed circuit board must have anchors. Anchors can be either soldered onto the board, or pushed through. Either type requires holes to be designed into the board. The use of RoHS solder must be allowed for because such solder is mechanically weaker than traditional Pb/Sn solder.\n\nTo assemble with a z-clip, attach one side of it to one of the anchors. Deflect the spring until the other side of the clip can be placed in the other anchor. The deflection develops a spring load on the component, which maintains very good contact. In addition to the mechanical attachment that the z-clip provides, it also permits using higher-performance thermal interface materials, such as phase change types.\nAvailable for processors and ball grid array (BGA) components, clips allow the attachment of a BGA heat sink directly to the component. The clips make use of the gap created by the ball grid array (BGA) between the component underside and PCB top surface. The clips therefore require no holes in the PCB. They also allow for easy rework of components.\nFor larger heat sinks and higher preloads, push pins with compression springs are very effective. The push pins, typically made of brass or plastic, have a flexible barb at the end that engages with a hole in the PCB; once installed, the barb retains the pin. The compression spring holds the assembly together and maintains contact between the heat sink and component. Care is needed in selection of push pin size. Too great an insertion force can result in the die cracking and consequent component failure.\n\nFor very large heat sinks, there is no substitute for the threaded standoff and compression spring attachment method. A threaded standoff is essentially a hollow metal tube with internal threads. One end is secured with a screw through a hole in the PCB. The other end accepts a screw which compresses the spring, completing the assembly. A typical heat sink assembly uses two to four standoffs, which tends to make this the most costly heat sink attachment design. Another disadvantage is the need for holes in the PCB.\n\nThermal contact resistance occurs due to the voids created by surface roughness effects, defects and misalignment of the interface. The voids present in the interface are filled with air. Heat transfer is therefore due to conduction across the actual contact area and to conduction (or natural convection) and radiation across the gaps. If the contact area is small, as it is for rough surfaces, the major contribution to the resistance is made by the gaps. To decrease the thermal contact resistance, the surface roughness can be decreased while the interface pressure is increased. However, these improving methods are not always practical or possible for electronic equipment. Thermal interface materials (TIM) are a common way to overcome these limitations,\n\nProperly applied thermal interface materials displace the air that is present in the gaps between the two objects with a material that has a much-higher thermal conductivity. Air has a thermal conductivity of 0.022 W/m•K while TIMs have conductivities of 0.3 W/m•K and higher.\n\nWhen selecting a TIM, care must be taken with the values supplied by the manufacturer. Most manufacturers give a value for the thermal conductivity of a material. However, the thermal conductivity does not take into account the interface resistances. Therefore, if a TIM has a high thermal conductivity, it does not necessarily mean that the interface resistance will be low.\n\nSelection of a TIM is based on three parameters: the interface gap which the TIM must fill, the contact pressure, and the electrical resistivity of the TIM. The contact pressure is the pressure applied to the interface between the two materials. The selection does not include the cost of the material. Electrical resistivity may be important depending upon electrical design details.\n\nLight-emitting diode (LED) performance and lifetime are strong functions of their temperature. Effective cooling is therefore essential. A case study of a LED based downlighter shows an example of the calculations done in order to calculate the required heat sink necessary for the effective cooling of lighting system. The article also shows that in order to get confidence in the results, multiple independent solutions are required that give similar results. Specifically, results of the experimental, numerical and theoretical methods should all be within 10% of each other to give high confidence in the results.\n\nTemporary heat sinks are sometimes used while soldering circuit boards, preventing excessive heat from damaging sensitive nearby electronics. In the simplest case, this means partially gripping a component using a heavy metal crocodile clip, hemostat, or similar clamp. Modern semiconductor devices, which are designed to be assembled by reflow soldering, can usually tolerate soldering temperatures without damage. On the other hand, electrical components such as magnetic reed switches can malfunction if exposed to hotter soldering irons, so this practice is still very much in use.\n\nIn general, a heat sink performance is a function of material thermal conductivity, dimensions, fin type, heat transfer coefficient, air flow rate, and duct size. To determine the thermal performance of a heat sink, a theoretical model can be made. Alternatively, the thermal performance can be measured experimentally. Due to the complex nature of the highly 3D flow in present applications, numerical methods or computational fluid dynamics (CFD) can also be used. This section will discuss the aforementioned methods for the determination of the heat sink thermal performance.\n\nOne of the methods to determine the performance of a heat sink is to use heat transfer and fluid dynamics theory. One such method has been published by Jeggels, et al., though this work is limited to ducted flow. Ducted flow is where the air is forced to flow through a channel which fits tightly over the heat sink. This makes sure that all the air goes through the channels formed by the fins of the heat sink. When the air flow is not ducted, a certain percentage of air flow will bypass the heat sink. Flow bypass was found to increase with increasing fin density and clearance, while remaining relatively insensitive to inlet duct velocity.\n\nThe heat sink thermal resistance model consists of two resistances, namely the resistance in the heat sink base, formula_9, and the resistance in the fins, formula_10. The heat sink base thermal resistance, formula_9, can be written as follows if the source is a uniformly applied the heat sink base. If it is not, then the base resistance is primarily spreading resistance:\n\nwhere formula_13 is the heat sink base thickness, formula_14 is the heat sink material thermal conductivity and formula_15 is the area of the heat sink base.\n\nThe thermal resistance from the base of the fins to the air, formula_10, can be calculated by the following formulas:\n\nThe flow rate can be determined by the intersection of the heat sink system curve and the fan curve. The heat sink system curve can be calculated by the flow resistance of the channels and inlet and outlet losses as done in standard fluid mechanics text books, such as Potter, et al. and White.\n\nOnce the heat sink base and fin resistances are known, then the heat sink thermal resistance, formula_26 can be calculated as:\n\nUsing the equations 5 to 13 and the dimensional data in, the thermal resistance for the fins was calculated for various air flow rates. The data for the thermal resistance and heat transfer coefficient are shown in the diagram, which shows that for an increasing air flow rate, the thermal resistance of the heat sink decreases.\n\nExperimental tests are one of the more popular ways to determine the heat sink thermal performance. In order to determine the heat sink thermal resistance, the flow rate, input power, inlet air temperature and heat sink base temperature need to be known. Vendor-supplied data is commonly provided for ducted test results. However, the results are optimistic and can give misleading data when heat sinks are used in an unducted application. More details on heat sink testing methods and common oversights can be found in Azar, et al.\n\nIn industry, thermal analyses are often ignored in the design process or performed too late — when design changes are limited and become too costly. Of the three methods mentioned in this article, theoretical and numerical methods can be used to determine an estimate of the heat sink or component temperatures of products before a physical model has been made. A theoretical model is normally used as a first order estimate. Online heat sink calculators can provide a reasonable estimate of forced and natural convection heat sink performance based on a combination of theoretical and empirically derived correlations. Numerical methods or computational fluid dynamics (CFD) provide a qualitative (and sometimes even quantitative) prediction of fluid flows. What this means is that it will give a visual or post-processed result of a simulation, like the images in figures 16 and 17, and the CFD animations in figure 18 and 19, but the quantitative or absolute accuracy of the result is sensitive to the inclusion and accuracy of the appropriate parameters.\n\nCFD can give an insight into flow patterns that are difficult, expensive or impossible to study using experimental methods. Experiments can give a quantitative description of flow phenomena using measurements for one quantity at a time, at a limited number of points and time instances. If a full-scale model is not available or not practical, scale models or dummy models can be used. The experiments can have a limited range of problems and operating conditions. Simulations can give a prediction of flow phenomena using CFD software for all desired quantities, with high resolution in space and time and virtually any problem and realistic operating conditions. However, if critical, the results may need to be validated.\n\n\n"}
{"id": "3080510", "url": "https://en.wikipedia.org/wiki?curid=3080510", "title": "High-level design", "text": "High-level design\n\nHigh-level design (HLD) explains the architecture that would be used for developing a software product. The architecture diagram provides an overview of an entire system, identifying the main components that would be developed for the product and their interfaces.\nThe HLD uses possibly nontechnical to mildly technical terms that should be understandable to the administrators of the system. In contrast, low-level design further exposes the logical detailed design of each of these elements for programmers.\n\n\nIn both cases the high-level design should be a complete view of the entire system, breaking it down into smaller parts that are more easily understood. To minimize the maintenance overhead as construction proceeds and the lower-level design is done, it is best that the high-level design is elaborated only to the degree needed to satisfy these needs.\n\nA high-level design document or HLDD adds the necessary details to the current project description to represent a suitable model for coding. This document includes a high-level architecture diagram depicting the structure of the system, such as the database\narchitecture, application architecture (layers), application flow (navigation), security architecture and technology architecture.\n\nA high-level design provides an overview of a system, product, service or process.\n\nSuch an overview helps supporting components be compatible to others.\n\nThe highest-level design should briefly describe all platforms, systems, products, services and processes that it depends on and include any important changes that need to be made to them.\n\nIn addition, there should be brief consideration of all significant commercial, legal, environmental, security, safety and technical risks, issues and assumptions.\n\nThe idea is to mention every work area briefly, clearly delegating the ownership of more detailed design activity whilst also encouraging effective collaboration between the various project teams.\n\nToday, most high-level designs require contributions from a number of experts, representing many distinct professional disciplines.\n\nFinally, every type of end-user should be identified in the high-level design and each contributing design should give due consideration to customer experience.\n\nhttp://users.csc.calpoly.edu/~jdalbey/205/Deliver/designDocFormat.html\n\n"}
{"id": "38992674", "url": "https://en.wikipedia.org/wiki?curid=38992674", "title": "ISEE (company)", "text": "ISEE (company)\n\nISEE is a European multinational company that designs and manufactures small computer-on-modules (COMs), single-board computers, expansion boards, radars and other embedded systems.\nThe abbreviation of ISEE refers to Integration, Software & Electronics Engineering. Their products are based on the IGEP Technology, the ISEE Generic Enhanced Platform using Texas Instruments OMAP processors.\n\nSome of their products, including IGEPv2 and IGEP COM MODULE, are open hardware, licensed under a Creative Commons Attribution-Non Commercial-Share-alike 3.0 unported license.\n\nISEE products have been used in various industrial and commercial projects such automotive and transportation applications, medical devices, vending machines, security and protection, robotics and radar applications under the commercial brand name of IGEP Technology.\n\nAll IGEP products include pre-installed Linux-based distributions with functional software and other resources such developing tools, IDEs, schematics, mechanical drawings, hardware manuals and software manuals. Other tutorials, articles, FAQs and a public GIT repository are also available by the IGEP Community, a collaborative user support community.\n\nIGEPv2 was released in 2009.\nIt consists of a low-power, fanless, industrial single-board computer (SBC) based on the Texas Instruments DM3730 ARM Cortex-A8 processor in a 65mm x 95mm board. IGEPv2 was the first open hardware IGEP Processor Board from ISEE and may be used to evaluate IGEP Technology, develop full-fledged product prototypes or can be completely customized by the user thanks to the freely available schematics.\n\nIGEPv5 was presented in September 2013.\nIt is based on the Texas Instruments OMAP5 SoC, which uses a dual-core ARM Cortex-A15 CPU. IGEPv5 allows additional connectivity via its on-board connectors and can be used to develop applications with advanced multimedia requirements.\n\nIGEP COM PROTON was released in 2010.\nIt provides the same processor and performance as IGEPv2 but without most of its on-board connectors, so it results in a smaller industrial form factor. There are four connectors of 70 pins for extended connectivity and measures 35mm x 51,2mm.\n\nIGEP COM MODULE was released in 2010.\nIt measures 18mm x 68,5mm and is the smallest computer-on-module (COM) released by ISEE and features Texas Instruments DM3730. It provides USB OTG, Wifi and Bluetooth on-board and two connectors of 70 pins for extended connectivity.\n\nIGEP COM AQUILA was released in 2013.\nIt is based on Texas Instruments AM3354 Cortex-A8 CPU and is the first IGEP Processor Board with standard SO-DIMM size format.\n\nIGEPv2 EXPANSION was released in 2009.\nIt adds connectivity to IGEPv2 Processor Board (RS232, VGA Output, CAN interface and GSM/GPRS Modem).\n\nIGEP PARIS was released in 2010.\nIt consists of an Expansion Board for IGEP COM MODULE and IGEP COM PROTON with basic functional connectivity (Ethernet, UARTs, TFT Video interface and USB).\n\nIGEP BERLIN was released in 2010.\nIt is based on IGEP PARIS connectivity with extended connectivity (DVI video, stereo audio in/out, CAN interface, RS485 and other Digital and Analog I/O).\n\nIGEP NEW YORK is the simplest expansion board for IGEP COM MODULE and IGEP COM PROTON with two 2.54-inch DIP connectors.\n\nISEE presented their Radar Technology in 2009. It consists of a 24 GHz band FMCW Radar Technology for IGEPv2 and IGEP COM MODULE, that carry the digital processing and implement the communication with the user system.\n\nThe preinstalled demo software on all ISEE products consists of:\n\n\nAdditional software and firmware releases can be downloaded prebuilt directly from the IGEP Community GIT repositories or compiled using OpenEmbedded software framework.\n\nISEE offers free development tools and resources for developing under IGEP Technology:\n\n\n\n"}
{"id": "40804819", "url": "https://en.wikipedia.org/wiki?curid=40804819", "title": "Information Processing Society of Japan", "text": "Information Processing Society of Japan\n\nThe Information Processing Society of Japan (\"IPSJ\") is a Japanese learned society for computing. Founded in 1960, it is headquartered in Tokyo, Japan. IPSJ publishes a magazine and several professional journals mainly in Japanese, and sponsors conferences and workshops, also mainly conducted in Japanese. It has nearly 20,000 members. IPSJ is a full member of the International Federation for Information Processing. The current president is Masaru Kitsuregawa, appointed in 2013.\n\nIPSJ publishes one magazine, several journals, and several peer-reviewed transactions. Most of these publications primarily carry articles and peer-reviewed papers in Japanese, but accept some articles in English, especially for transactions special issues.\n\n\nEvery year since 1999, IPSJ has inducted a new group of Japanese Fellows. It has no foreign or international fellows and most, if not all, fellows are Japanese.\n\nIPSJ maintains an excellent online Computer Museum of computers developed in Japan, featuring equipment ranging from old mechanical calculators to modern supercomputers, in both English and Japanese.\n\n"}
{"id": "40817116", "url": "https://en.wikipedia.org/wiki?curid=40817116", "title": "Ingeborg Hochmair", "text": "Ingeborg Hochmair\n\nIngeborg J. Hochmair-Desoyer (born 1953) is an Austrian electrical engineer from Technical University of Vienna. She helped create the first micro-electronic multi-channel cochlear implant in the world with her husband Prof. Erwin Hochmair. In 1980 she co-founded together with Prof. Erwin Hochmair the medical device company MED-EL and serves as its CEO and CTO. In 2013, she was honored together with two more scientists with the Lasker-DeBakey Clinical Medical Research Award for developing the modern cochlear implant.\n\nIngeborg Hochmair was born in 1953 in Vienna, Austria. Her mother was a physicist and her father was Dean of the Faculty of Mechanical Engineering at Vienna University of Technology. Her grandmother was one of the first female chemical engineers in Austria. She commenced her studies at Technical University of Vienna in electrical engineering in 1971 [and was the first woman to do her PhD (with distinction)in 1979]. Her dissertation was on the \"Technical realization and psychoacoustic evaluation of a system for multichannel chronic stimulation of the auditory nerve.\" From 1976 to 1986 she worked as Assistant Professor at the Institute of General Electrical Engineering and Electronics at Technical University of Vienna. She also worked at Stanford University’s Institute for Electronics in Medicine as a Visiting Associate Professor in 1979. Together with her husband she decided to move from Vienna to Innsbruck in 1986 where she taught (first as Assistant Professor and later as Associate Professor) at the Institute of Applied Physics Electronics of University of Innsbruck till 1999. In 1998 she achieved Venia Legendi (Univ. Doz.) in Biomedical Engineering at the Faculty of Electrical Engineering of Technical University of Vienna.\n\nThe idea of creating a company to develop and manufacture hearing implants was put to effect in the early eighties when she co-founded together with her husband Erwin Hochmair the medical device company MED-EL. The first employees were hired in 1990 and mark the beginning of the company. Since 2000 she has founded and co-founded multiple companies linked to the area of hearing loss and hearing implants.\n\nIn 1975 Ingeborg and Erwin Hochmair started the cochlear implant development at Technical University of Vienna with the overall goal of enabling the user not only to hear sounds but also to provide some speech understanding. Together they developed the world’s first microelectronic multi-channel cochlear implant. This implant included a long, flexible electrode, which could, for the first time, deliver electric signals to the auditory nerve along a large part of the cochlea, the snail-shaped inner ear. It had \"8 channels, a stimulation rate of 10.000 pulses per second per channel, 8 independent current sources, and a flexible electrode for 22-25mm insertion into the cochlea.\" The first device was implanted on 16 December 1977 in Vienna by Prof. Kurt Burian followed by a second implantation in March 1978. Despite an early shunt in the first patient and some existing tinnitus in the second patient, place pitch could be demonstrated and the second patient could reliably discriminate and identify stimulation channels.\n\nWith a next version of this device modified for better signal transparency, the next milestone in cochlear implant development was reached in March 1980: the understanding of words and sentences without lip-reading in a quiet environment via a small, body-worn sound processor. Over the years, about 500 devices were implanted in adults and children. Owing to the implant’s very low power consumption and external processing, the world’s first behind-the-ear (BTE) processor for a cochlear implant was designed in 1991. It was called COMFORT.\n\nAs CEO and CTO of MED-EL Ingeborg Hochmair has developed many world’s first products and solutions to cater to the demand of patients and surgeons worldwide for a broad spectrum of indications of hearing loss.\n\nIngeborg Hochmair has over 100 scientific publications in the field of Cochlear Implants, Medical Devices, Neuroprotheses, Audio & Speech Processing Technology. Among the most important ones are the following:\n\n\n\n"}
{"id": "12239581", "url": "https://en.wikipedia.org/wiki?curid=12239581", "title": "Institute of Measurement and Control", "text": "Institute of Measurement and Control\n\nFounded in 1944 and incorporated by Royal Charter in 1975, The Institute of Measurement and Control (InstMC) is a professional members institution for individuals and companies that operate within the measurement and control industries. Its aims are to advance the science and practice of measurement and control technologies and their various applications, to foster the exchange of views and the communication of knowledge and ideas in these activities, and to promote the professional qualification and standing of its members. The Institute is both a learned society and a professional qualifying body. InstMC is registered with the Engineering Council and is one of the licensed member institutions allowed to register Chartered Engineers (CEng)\n\nThe Institute is the UK member body of the International Measurement Confederation (IMEKO) and is the secretariat to the United Kingdom Automatic Control Council (UKACC).\n\nThis Institute currently has range of Special Interest Groups that organise conferences, seminars and training courses on subjects in the measurement and control fields. Special Interest Groups contribute to national programmes in areas such as standards and training, and join with other organisations to provide expert input to inform government policy. Each Special Interest Group has a panel of volunteer members and external experts that decides its programme of events and activities, and establishes working groups for particular projects.\n\n\nMembers are able to join their nearest Local section, who are responsible for organising regional events.Each Local Section is represented on the Institute’s Council, providing a direct link between the members and the Council.\n\n\n\n87 Gower Street\n\nLondon\n\nWC1E 6AF\n\nT: 020 7387 4949 \n\nhttp://www.instmc.org.uk\n\nPresident - \"Dr Graham Machin BSc (Hons), DPhil, DSc\"\n\nVice President - Mr M Belshaw\n\nChief Executive Officer - \"Dr Patrick Finlay\" \n\nHon Secretary – \"Dr G S Philp\"\n\nHon Treasurer – \"Mr C Howard\"\n\n\"Notable former presidents of the Institute of Measurement and Control include:\"\n\nSir George Thomson MA FRS (1944 - 48)\n\nSir Harold Hartley GC VO FRS (1957 -58)\n\nL. Finkelstein MSc (1980 - 81)\n\nMJH Sterling BEng Phd DEng (1988 - 89)\n\nProf W S Bardo FREng HonFInstMC (2010 - 12)\n\nLord Oxburgh KBE FRS Hon FREng (2012 - 2014)\n\nProf Sarah Spurgeon OBE\n\nInternational Measurement Confederation (IMEKO) \n\nUnited Kingdom Automatic Control Council (UKACC)\n\nNational Physical Laboratory (NPL)\n\nTrade Association for Instrumentation, Control, Automation and Laboratory Technology (GAMBICA)\n\nWorshipful Company of Scientific Instrument Makers (SIM)\n\nBritish Standards Institute (BSI)\n\nParliamentary and Scientific Committee\n\n\n\n"}
{"id": "16912", "url": "https://en.wikipedia.org/wiki?curid=16912", "title": "Karl Benz", "text": "Karl Benz\n\nKarl Friedrich Benz (; 25 November 1844 – 4 April 1929) was a German engine designer and automobile engineer. His Benz Patent Motorcar from 1885 is considered the first practical automobile. He received a patent for the motorcar on 29 January 1886.\n\nKarl Benz was born Karl Friedrich Michael Vaillant, on 25 November 1844 in Mühlburg, now a borough of Karlsruhe, Baden-Württemberg, which is part of modern Germany, to Josephine Vaillant and a locomotive driver, Johann Georg Benz, whom she married a few months later. According to German law, the child acquired the name \"Benz\" by legal marriage of his parents Benz and Vaillant. When he was two years old, his father died of pneumonia, and his name was changed to Karl Friedrich Benz in remembrance of his father.\n\nDespite living in near poverty, his mother strove to give him a good education. Benz attended the local Grammar School in Karlsruhe and was a prodigious student. In 1853, at the age of nine he started at the scientifically oriented Lyceum. Next he studied at the Poly-Technical University under the instruction of Ferdinand Redtenbacher.\n\nBenz had originally focused his studies on locksmithing, but he eventually followed his father's steps toward locomotive engineering. On 30 September 1860, at age 15, he passed the entrance exam for mechanical engineering at the University of Karlsruhe, which he subsequently attended. Benz graduated 9 July 1864 aged 19.\n\nFollowing his formal education, Benz had seven years of professional training in several companies, but did not fit well in any of them. The training started in Karlsruhe with two years of varied jobs in a mechanical engineering company.\n\nHe then moved to Mannheim to work as a draftsman and designer in a scales factory. In 1868 he went to Pforzheim to work for a bridge building company \"Gebrüder Benckiser Eisenwerke und Maschinenfabrik\". Finally, he went to Vienna for a short period to work at an iron construction company.\n\nIn 1871, at the age of twenty-seven, Karl Benz joined August Ritter in launching the Iron Foundry and Mechanical Workshop in Mannheim, later renamed Factory for Machines for Sheet-metal Working.\n\nThe enterprise's first year went very badly. Ritter turned out to be unreliable, and the business's tools were impounded. The difficulty was overcome when Benz's fiancée, Bertha Ringer, bought out Ritter's share in the company using her dowry.\n\nOn 20 July 1872, Karl Benz and Bertha Ringer married. They had five children: Eugen (1873), Richard (1874), Clara (1877), Thilde (1882), and Ellen (1890).\n\nDespite the business misfortunes, Karl Benz led in the development of new engines in the early factory he and his wife owned. To get more revenues, in 1878 he began to work on new patents. First, he concentrated all his efforts on creating a reliable petrol two-stroke engine. Benz finished his two-stroke engine on 31 December 1878, New Year's Eve, and was granted a patent for it in 1879.\n\nKarl Benz showed his real genius, however, through his successive inventions registered while designing what would become the production standard for his two-stroke engine. Benz soon patented the speed regulation system, the ignition using sparks with battery, the spark plug, the carburetor, the clutch, the gear shift, and the water radiator.\n\nProblems arose again when the banks at Mannheim demanded that Bertha and Karl Benz's enterprise be incorporated due to the high production costs it maintained. The Benzes were forced to improvise an association with photographer Emil Bühler and his brother (a cheese merchant), in order to get additional bank support. The company became the joint-stock company \"Gasmotoren Fabrik Mannheim\" in 1882.\n\nAfter all the necessary incorporation agreements, Benz was unhappy because he was left with merely five percent of the shares and a modest position as director. Worst of all, his ideas weren't considered when designing new products, so he withdrew from that corporation just one year later, in 1883.\n\nBenz's lifelong hobby brought him to a bicycle repair shop in Mannheim owned by Max Rose and Friedrich Wilhelm Eßlinger. In 1883, the three founded a new company producing industrial machines: \"Benz & Companie Rheinische Gasmotoren-Fabrik\", usually referred to as \"Benz & Cie.\" Quickly growing to twenty-five employees, it soon began to produce static gas engines as well.\n\nThe success of the company gave Benz the opportunity to indulge in his old passion of designing a \"horseless carriage\". Based on his experience with, and fondness for, bicycles, he used similar technology when he created an automobile. It featured wire wheels (unlike carriages' wooden ones)\n\nIt was the first automobile entirely designed as such to generate its own power, not simply a motorized stage coach or horse carriage, which is why Karl Benz was granted his patent and is regarded as its inventor.\n\nThe Motorwagen was patented on 29 January 1886 as DRP-37435: \"automobile fueled by gas\". The 1885 version was difficult to control, leading to a collision with a wall during a public demonstration. The first successful tests on public roads were carried out in the early summer of 1886. The next year Benz created the Motorwagen Model 2, which had several modifications, and in 1889, the definitive Model 3 with wooden wheels was introduced, showing at the Paris Expo the same year.\n\nBenz began to sell the vehicle (advertising it as \"Benz Patent Motorwagen\") in the late summer of 1888, making it the first commercially available automobile in history. The second customer of the Motorwagen was a Parisian bicycle manufacturer Emile Roger, who had already been building Benz engines under license from Karl Benz for several years. Roger added the Benz automobiles (many built in France) to the line he carried in Paris and initially most were sold there.\n\nThe early 1888 version of the Motorwagen had no gears and could not climb hills unaided. This limitation was rectified after Bertha Benz made her famous trip driving one of the vehicles a great distance and suggested to her husband the addition of brake linings to act as brake pads.\n\nIn the first long distance automobile trip, Bertha Benz, supposedly without the knowledge of her husband, on the morning of 5 August 1888, took this vehicle on a trip from Mannheim to Pforzheim to visit her mother, taking her sons Eugen and Richard with her. In addition to having to locate pharmacies on the way to fuel up, she repaired various technical and mechanical problems and invented brake lining. After some longer downhill slopes, she ordered a shoemaker to nail leather on the brake blocks. Bertha Benz and sons finally arrived at nightfall, announcing the achievement to Karl by telegram. It had been her intention to demonstrate the feasibility of using the Benz Motorwagen for travel and to generate publicity in the manner now referred to as live marketing. Today, the event is celebrated every two years in Germany with an antique automobile rally. In 2008, the Bertha Benz Memorial Route was officially approved as a route of industrial heritage of mankind, because it follows Bertha Benz's tracks of the world's first long-distance journey by automobile in 1888. Now everybody can follow the 194 km of signposted route from Mannheim via Heidelberg to Pforzheim (Black Forest) and back. The return trip was along a different, slightly shorter, itinerary, as shown on the maps of the Bertha Benz Memorial Route.\n\nBenz's Model 3 made its wide-scale debut to the world in the 1889 World's Fair in Paris; about twenty-five Motorwagens were built between 1886 and 1893.\n\nThe great demand for stationary, static internal combustion engines forced Karl Benz to enlarge the factory in Mannheim, and in 1886 a new building located on Waldhofstrasse (operating until 1908) was added. \"Benz & Cie.\" had grown in the interim from 50 employees in 1889 to 430 in 1899.\n\nDuring the last years of the nineteenth century, \"Benz\" was the largest automobile company in the world with 572 units produced in 1899.\n\nBecause of its size, in 1899, \"Benz & Cie.\" became a joint-stock company with the arrival of Friedrich von Fischer and Julius Ganß, who came aboard as members of the Board of Management. Ganß worked in the commercialization department, which is somewhat similar to marketing in contemporary corporations.\n\nThe new directors recommended that Benz should create a less expensive automobile suitable for mass production. In 1893, Karl Benz created the \"Victoria\", a two-passenger automobile with a engine, which could reach the top speed of and had a pivotal front axle operated by a roller-chained tiller for steering. The model was successful with 85 units sold in 1893.\n\nThe Benz \"Velo\" also participated in the world's first automobile race, the 1894 Paris to Rouen, where Émile Roger finished 14th, after covering the in 10 hours 01 minute at an average speed of .\n\nIn 1895, Benz designed the first truck with an internal combustion engine in history. Benz also built the first motor buses in history in 1895, for the \"Netphener\" bus company.\nIn 1896, Karl Benz was granted a patent for his design of the first flat engine. It had horizontally opposed pistons, a design in which the corresponding pistons reach top dead centre simultaneously, thus balancing each other with respect to momentum. Flat engines with four or fewer cylinders are most commonly called boxer engines, \"boxermotor\" in German, and also are known as \"horizontally opposed engines\". This design is still used by Porsche, Subaru, and some high performance engines used in racing cars. In motorcycles, the most famous boxer engine is found in BMW Motorrad, though the boxer engine design was used in many other models, including Victoria, Harley-Davidson XA, Zündapp, Wooler, Douglas Dragonfly, Ratier, Universal, IMZ-Ural, Dnepr, Gnome et Rhône, Chang Jiang, Marusho, and the Honda Gold Wing.\n\nAlthough Gottlieb Daimler died in March 1900—and there is no evidence that Benz and Daimler knew each other nor that they knew about each other's early achievements—eventually, competition with Daimler Motoren Gesellschaft (DMG) in Stuttgart began to challenge the leadership of Benz & Cie. In October 1900, the main designer of DMG, Wilhelm Maybach, built the engine that would later be used in the \"Mercedes-35hp\" of 1902. The engine was built to the specifications of Emil Jellinek under a contract for him to purchase thirty-six vehicles with the engine, and for him to become a dealer of the special series. Jellinek stipulated the new engine be named Daimler-\"Mercedes\" (for his daughter). Maybach would quit DMG in 1907, but he designed the model and all of the important changes. After testing, the first was delivered to Jellinek on 22 December 1900. Jellinek continued to make suggestions for changes to the model and obtained good results racing the automobile in the next few years, encouraging DMG to engage in commercial production of automobiles, which they did in 1902.\n\nBenz countered with \"Parsifil\", introduced in 1903 with a vertical twin engine that achieved a top speed of . Then, without consulting Benz, the other directors hired some French designers.\n\nFrance was a country with an extensive automobile industry based on Maybach's creations. Because of this action, after difficult discussions, Karl Benz announced his retirement from design management on 24 January 1903, although he remained as director on the Board of Management through its merger with DMG in 1926 and, remained on the board of the new Daimler-Benz corporation until his death in 1929.\n\nHe was inducted into the Automotive Hall of Fame in 1984 and the European Automotive Hall of Fame.\n\nBenz's sons Eugen and Richard left Benz & Cie. in 1903, but Richard returned to the company in 1904 as the designer of passenger vehicles.\n\nThat year, sales of Benz & Cie. reached 3,480 automobiles, and the company remained the leading manufacturer of automobiles.\n\nAlong with continuing as a director of Benz & Cie., Karl Benz would soon found another company, \"C. Benz Söhne\", (with his son Eugen and closely held within the family), a privately held company for manufacturing automobiles. The brand name used the first initial of the French variant of Benz's first name, \"Carl\".\n\nIn 1909, the \"Blitzen Benz\" was built in Mannheim by Benz & Cie. The bird-beaked vehicle had a 21.5-liter (1312ci), engine, and on 9 November 1909 in the hands of Victor Hémery of France, the land speed racer at Brooklands, set a record of 226.91 km/h (141.94 mph), said to be \"faster than any plane, train, or automobile\" at the time, a record that was not exceeded for ten years by any other vehicle. It was transported to several countries, including the United States, to establish multiple records of this achievement.\n\nKarl Benz, Bertha Benz, and their son, Eugen, moved east of Mannheim to live in nearby Ladenburg, and solely with their own capital, founded the private company, C. Benz Sons (German: \"Benz Söhne\") in 1906, producing automobiles and gas engines. The latter type was replaced by petrol engines because of lack of demand.\nThis company never issued stocks publicly, building its own line of automobiles independently from Benz & Cie., which was located in Mannheim. The \"Benz Sons\" automobiles were of good quality and became popular in London as taxis.\n\nIn 1912, Karl Benz liquidated all of his shares in Benz Sons and left the family-held company in Ladenburg to Eugen and Richard, but he remained as a director of Benz & Cie.\n\nDuring a birthday celebration for him in his home town of Karlsruhe on 25 November 1914, the seventy-year-old Karl Benz was awarded an honorary doctorate by his alma mater, the \"Karlsruhe University\", thereby becoming—Dr. Ing. h. c. Karl Benz.\nAlmost from the very beginning of the production of automobiles, participation in sports car racing became a major method to gain publicity for manufacturers. At first, the production models were raced and the Benz \"Velo\" participated in the first automobile race: . Later, investment in developing racecars for motorsports produced returns through sales generated by the association of the name of the automobile with the winners. Unique race vehicles were built at the time, as seen in the photograph here of the Benz, the first mid-engine and aerodynamically designed, \"Tropfenwagen\", a \"teardrop\" body introduced at the 1923 European Grand Prix at Monza.\n\nIn the last production year of the \"Benz Sons\" company, 1923, three hundred and fifty units were built. During the following year, 1924, Karl Benz built two additional 8/25 hp units of the automobile manufactured by this company, tailored for his personal use, which he never sold; they are still preserved.\n\nThe German economic crisis worsened. In 1923 \"Benz & Cie.\" produced only 1,382 units in Mannheim, and \"DMG\" made only 1,020 in Stuttgart. The average cost of an automobile was 25 million marks because of rapid inflation. Negotiations between the two companies resumed and in 1924 they signed an \"Agreement of Mutual Interest\" valid until the year 2000. Both enterprises standardized design, production, purchasing, sales, and advertising—marketing their automobile models jointly—although keeping their respective brands.\n\nOn 28 June 1926, Benz & Cie. and DMG finally merged as the \"Daimler-Benz\" company, baptizing all of its automobiles, \"Mercedes-Benz\", honoring the most important model of the DMG automobiles, the 1902 \"Mercedes 35 hp\", along with the Benz name. The name of that DMG model had been selected after ten-year-old Mercédès Jellinek, the daughter of Emil Jellinek who had set the specifications for the new model. Between 1900 and 1909 he was a member of DMG's board of management and long before the merger Jellinek had resigned.\n\nKarl Benz was a member of the new \"Daimler-Benz\" board of management for the remainder of his life. A new logo was created in 1926, consisting of a three pointed star (representing Daimler's motto: \"engines for land, air, and water\") surrounded by traditional laurels from the Benz logo, and the brand of all of its automobiles was labeled \"Mercedes-Benz\". Model names would follow the brand name in the same convention as today.\n\nThe next year, 1927, the number of units sold tripled to 7,918 and the diesel line was launched for truck production. In 1928, the \"Mercedes-Benz SSK\" was presented.\n\nOn 4 April 1929, Karl Benz died at home in Ladenburg at the age of eighty-four from a bronchial inflammation. Until her death on 5 May 1944, Bertha Benz continued to reside in their last home. Members of the family resided in the home for thirty more years. The Benz home now has been designated as historic and is used as a scientific meeting facility for a nonprofit foundation, the \"Gottlieb Daimler and Karl Benz Foundation\", that honors both Bertha and Karl Benz for their roles in the history of automobiles.\nIn 2011, a dramatized television movie about the life of Karl and Bertha Benz was made named \"Carl & Bertha\" which premiered on 11 May and was aired by Das Erste on 23 May. A trailer of the movie and a \"making of\" special were released on YouTube.\n\n\n\n"}
{"id": "13885407", "url": "https://en.wikipedia.org/wiki?curid=13885407", "title": "Lit a la turque", "text": "Lit a la turque\n\nThe Lit à la Turque, a bed with two scrolling ends and a baldachin, was one of many products of the 18th century. Due to the obsession with anything exotic or unusual from foreign countries; beds, furniture and other objects reflected this style. The design of this bed is not directly linked with any Turkish design, but reflects the luxurious and romantic designs that inspired the craftsmen of this time.\n\nBeds of this type were usually placed sideways against the wall with a baldachin. Many times these beds were placed on wheels allowing the body of the bed to be pulled out and made up easily, while the back would stay attached to the wall.\n\nFashion for Turkish things peaked in the 18th century when Madame de Pompadour had a bedroom called the \"chambre à la turque\" (Turkish bedroom).\n"}
{"id": "12285020", "url": "https://en.wikipedia.org/wiki?curid=12285020", "title": "Mark 22 nuclear bomb", "text": "Mark 22 nuclear bomb\n\nThe Mark 22 nuclear bomb (Mk-22) was the first thermonuclear device test by the University of California Radiation Lab (UCRL). The test was part of the \"Koon\" shot of Operation Castle. The Mk-22 failed to achieve anything like its intended yield due to premature heating of the secondary from exposure to neutrons. As the other UCRL test planned for the Castle series, the liquid fueled \"Ramrod\" device had the same basic design flaw, that test was canceled. Within a month of the bomb’s failure, the Mk-22 was terminated because the United States Atomic Energy Commission realized there was nothing that could be done to salvage the design.\n\n"}
{"id": "840391", "url": "https://en.wikipedia.org/wiki?curid=840391", "title": "Mechanic", "text": "Mechanic\n\nA mechanic is a tradesman, craftsman or technician who uses tools to build or repair machinery.\n\nMost mechanics specialize in a particular field, such as auto mechanics, truck mechanic, bicycle mechanics, motorcycle mechanics, boiler mechanics, general mechanics, industrial maintenance mechanics (millwrights), air conditioning and refrigeration mechanics, bus mechanics, aircraft mechanics, diesel mechanics and tank mechanics in the armed services. Auto mechanics, for example, have many trades within. Some may specialize in the electrical aspects, while others may specialize in the mechanical aspects. Other areas include: brakes and steering, suspension, automatic or manual transmission, engine repairs or diagnosing customer complaints. An automotive technician, on the other hand, has a wide variety of topics to learn. A mechanic is typically certified by a trade association or regional government power. Mechanics may be separated into two classes based on the type of machines they work on, heavyweight and lightweight. Heavyweight work is on larger machines or heavy equipment, such as tractors and trailers, while lightweight work is on smaller items, such as automotive engines.\n"}
{"id": "150245", "url": "https://en.wikipedia.org/wiki?curid=150245", "title": "Mercury-in-glass thermometer", "text": "Mercury-in-glass thermometer\n\nThe mercury-in-glass or mercury thermometer was invented by physicist Daniel Gabriel Fahrenheit in Amsterdam (1714). It consists of a bulb containing mercury attached to a glass tube of narrow diameter; the volume of mercury in the tube is much less than the volume in the bulb. The volume of mercury changes slightly with temperature; the small change in volume drives the narrow mercury column a relatively long way up the tube. The space above the mercury may be filled with nitrogen gas or it may be at less than atmospheric pressure, a partial vacuum.\n\nIn order to calibrate the thermometer, the bulb is made to reach thermal equilibrium with a temperature standard such as an ice/water mixture, and then with another standard such as water/vapour, and the tube is divided into regular intervals between the fixed points. In principle, thermometers made of different material (e.g., coloured alcohol thermometers) might be expected to give different intermediate readings due to different expansion properties; in practice the substances used are chosen to have reasonably linear expansion characteristics as a function of true thermodynamic temperature, and so give similar results.\n\nThe application of mercury (1714) and Fahrenheit scale (1724) for liquid-in-glass thermometers ushered in a new era of accuracy and precision in thermometry, and is still to this day regarded as one of the most accurate thermometers available.\n\nThe thermometer was used by the originators of the Fahrenheit and Celsius scales.\n\nAnders Celsius, a Swedish scientist, devised the Celsius scale, which was described in his publication \"The origin of the Celsius temperature scale\" in 1742.\n\nCelsius used two fixed points in his scale: the temperature of melting ice and the temperature of boiling water. This wasn't a new idea, since Isaac Newton was already working on something similar. The distinction of Celsius was to use the condition of melting and not that of freezing. The experiments for reaching a good calibration of his thermometer lasted for 2 winters. By performing the same experiment over and over again, he discovered that ice always melted at the same calibration mark on the thermometer. He found a similar fixed point in the calibration of boiling water to water vapour (when this is done to high precision, a variation will be seen with atmospheric pressure; Celsius noted this). At the moment that he removed the thermometer from the vapour, the mercury level climbed slightly. This was related to the rapid cooling (and contraction) of the glass.\n\nWhen Celsius decided to use his own temperature scale, he originally defined his scale \"upside-down\", i.e. he chose to set the boiling point of pure water at 0 °C (212 °F) and the freezing point at -100 °C (-32 °F). One year later, Frenchman Jean-Pierre Christin proposed to invert the scale with the freezing point at and the boiling point at . He named it Centigrade (100 grades).\n\nFinally, Celsius proposed a method of calibrating a thermometer:\n\n\nThese points are adequate for approximate calibration but both vary with atmospheric pressure. Nowadays, the triple point of water is used instead of the freezing point (the triple point occurs at 273.16 kelvins (K), 0.01 °C).\n\nBefore the discovery of the true thermodynamic temperature, the thermometer \"defined\" the temperature; thermometers made with different materials would define different temperature scales (a coloured alcohol thermometer would give a slightly different reading than a mercury thermometer at, say half-scale). In practice, several materials gave very similar temperatures to each other and, when discovered, to the thermodynamic temperature.\n\nOne special kind of mercury-in-glass thermometer, called a maximum thermometer, works by having a constriction in the neck close to the bulb. As the temperature rises, the mercury is pushed up through the constriction by the force of expansion. When the temperature falls, the column of mercury breaks at the constriction and cannot return to the bulb, thus remaining stationary in the tube. The observer can then read the maximum temperature over the set period of time. To reset the thermometer it must be swung sharply. This design is used in the traditional type of medical thermometer.\n\nA maximum minimum thermometer, also known as Six's thermometer, is a thermometer which registers the maximum and minimum temperatures reached over a period of time, typically 24 hours. The original design contains mercury, but solely as a way to indicate the position of a column of alcohol whose expansion indicates the temperature; it is not a thermometer operated by the expansion of mercury; mercury-free versions are available.\n\nMercury thermometers cover a wide temperature range from ; the instrument's upper temperature range may be extended though the introduction of an inert gas such as nitrogen. This introduction of an inert gas increases the pressure on the liquid mercury and therefore its boiling point is increased, this in combination with replacing the Pyrex glass with fused quartz allows the upper temperature range to be extended to .\n\nMercury cannot be used below the temperature at which it becomes solid, . If the thermometer contains nitrogen, the gas may flow down into the column when the mercury solidifies and be trapped there when the temperature rises, making the thermometer unusable until returned to the factory for reconditioning. To avoid this, some weather services require that all mercury-in-glass thermometers be brought indoors when the temperature falls to .\n\nTo measure lower meteorological temperatures, a thermometer containing a mercury-thallium alloy which does not solidify until the temperature drops to may be used.\n\n, many mercury-in-glass thermometers are used in meteorology; however, they are becoming increasingly rare for other uses, as many countries banned them for medical use due to the toxicity of mercury. Some manufacturers use galinstan, a liquid alloy of gallium, indium, and tin, as a replacement for mercury.\n\nThe typical \"fever thermometer\" contains between of elemental mercury. Swallowing this amount of mercury would, it is said, pose little danger but the inhaling of the vapour could lead to health problems.\n\nIn February 2009, the Argentine Health Ministry instructed by resolution 139/09 that all health centres and hospitals should buy mercury-free thermometers and blood pressure meters and called on dentists, medical technicians, and environmental health specialists to start eliminating this toxin. mercury thermometers were still on sale to the public at pharmacies.\n\nThere was a voluntary take-back action for thermometers containing mercury based on the Federal Waste Management Plan 2006, and carried out in close cooperation between the Austrian Chamber of Pharmacists (Österreichische Apothekerkammer), the Federal Ministry of Environment, a private waste disposer, a producer of electronic thermometers and a pharmaceutical distributor. The disposal company supplied each pharmacy (approximately 1,200) with a collection bin and covered the cost of disposal. The pharmaceutical distributor covered the logistical costs for the distribution of the thermometers. The pharmacies accepted a refund of only 0.50 Euro per thermometer for handling (which is far below their normal margin). The supplier provided the thermometers at a reduced price. The Federal Ministry supported each sold thermometer (covering about 30% of the direct costs) and advertised the project. During the collection period, consumers could bring in a mercury thermometer and buy an electronic thermometer for a subsidised price of 1 Euro. Between October 2007 and January 2008, about 465,000 electronic thermometers were sold and about one million mercury thermometers (together containing about 1 tonne of mercury) were collected.\n\nBy the Philippines Department of Health’s Administrative Order 2008-0221, all mercury equipment from hospitals, including mercury-in-glass thermometers, will be phased out in the Philippines by September 28, 2010. Even before the order was released, 50 hospitals have already banned mercury from their establishments. Among these fifty hospitals, the Philippine Heart Center was the first one to do so. San Juan de Dios Hospital, Philippine Children’s Medical Center, San Lazaro Hospital, Ospital ng Muntinlupa, Lung Center of the Philippines, the National Kidney and Transplant Institute, Manila Adventist Medical Center and Las Piñas Hospital also made steps to ban the toxic chemical. The country was the first one to make a step to ban mercury from its health care system in Southeast Asia.\n\nSince European Union directive 2007/51/EC came into force on 3 April 2009, the UK Health Protection Agency (HPA) reported that mercury thermometers could no longer be sold to the general public. Shops holding stocks of unsold thermometers had to withdraw them from sale; mercury thermometers purchased before this date could be used without legal implications. The purpose of these restrictions is to protect the environment and public health by decreasing the amount of mercury waste released. The HPA had, in 2007, released a guide to dealing with small spills of mercury.\n\nIn the United States, both the American Academy of Pediatrics and the United States Environmental Protection Agency recommend that alternative thermometers be used in the home.\n\n"}
{"id": "7449438", "url": "https://en.wikipedia.org/wiki?curid=7449438", "title": "Micro-loop heat pipe", "text": "Micro-loop heat pipe\n\nA micro-loop heat pipe or MLHP is a miniature loop heat pipe in which the radius of curvature of the liquid meniscus in the evaporator is in the same order of magnitude of the micro grooves' dimensions; or a miniature loop heat pipe which has been fabricated using microfabrication techniques.\n"}
{"id": "7813116", "url": "https://en.wikipedia.org/wiki?curid=7813116", "title": "Microsoft Award", "text": "Microsoft Award\n\nThe Royal Society and Académies des sciences Microsoft Award was an annual award given by the Royal Society and the Académie des sciences to scientists working in Europe who had made a major contribution to the advancement of science through the use of computational methods. It was sponsored by Microsoft Research.\n\nThe award was open to any research scientist who had made a significant contribution at the intersection of computing and the sciences covering Biological Sciences, Physical Sciences, Mathematics and Engineering. The prize recognized the importance of interdisciplinary research at the interface of science and computing for advancing scientific boundaries, as well as the importance of investing in European scientists to give Europe a competitive science base. The recipient was selected by a Committee comprising members of the Académie des sciences and Fellows of the Royal Society. The prize consisted of a trophy and monetary amount of €250,000, of which €7,500 is prize money and the rest earmarked for further research. \n\nThe first award was made in 2006 and the last in 2009. It has now been replaced by the Royal Society Milner Award.\n\nToday Microsoft is giving awards for top-performing partners in various countries.\n\n"}
{"id": "1961844", "url": "https://en.wikipedia.org/wiki?curid=1961844", "title": "Microwave antenna", "text": "Microwave antenna\n\nA microwave antenna is a physical transmission device used to broadcast microwave transmissions between two or more locations . In addition to broadcasting, antennas are also used in radar, radio astronomy and electronic warfare. \n\n\nA parabolic antenna is an antenna that uses a parabolic reflector, a curved surface with the cross-sectional shape of a parabola, to direct the radio waves. These devices range anywhere from 6\" to more than 12' diameter depending on application and use.\n\nA horn antenna or microwave horn is an antenna that consists of a flaring metal waveguide shaped like a horn to direct radio waves in a beam. Horns are widely used as antennas at UHF and microwave frequencies, above 300 MHz.\n\nA lens antenna uses a lens to direct or collect microwave radiation.\n\nAn array antenna is a high gain antenna consisting of an array of smaller antenna elements.\n\nA leaky wave antenna uses a leaking transmission line to obtain radation.\n\n"}
{"id": "28942212", "url": "https://en.wikipedia.org/wiki?curid=28942212", "title": "Modified Transverse Mercator coordinate system", "text": "Modified Transverse Mercator coordinate system\n\nThe Modified Transverse Mercator (MTM) coordinate system is a metric grid-based method of specifying locations, similar to the Universal Transverse Mercator coordinate system. MTM uses a Transverse Mercator projection with zones spaced 3° of longitude apart.\n\nThe coordinate system is used in Eastern Canada.\n"}
{"id": "220806", "url": "https://en.wikipedia.org/wiki?curid=220806", "title": "Musk", "text": "Musk\n\nMusk is a class of aromatic substances commonly used as base notes in perfumery. They include glandular secretions from animals such as the musk deer, numerous plants emitting similar fragrances, and artificial substances with similar odors. \"Musk\" was a name originally given to a substance with a strong odor obtained from a gland of the musk deer. The substance has been used as a popular perfume fixative since ancient times and is one of the most expensive animal products in the world. The name originates from the Late Greek μόσχος 'moskhos', from Persian 'mushk', ultimately from Sanskrit मुष्क muṣka meaning \"a testicle\", from a diminutive of मूष् mūṣ (\"mouse\"). The deer gland was thought to resemble a scrotum. It is applied to various plants and animals of similar smell (e.g. musk-ox, 1744) and has come to encompass a wide variety of aromatic substances with similar odors, despite their often differing chemical structures and molecular shapes.\n\nNatural musk was used extensively in perfumery until the late 19th century when economic and ethical motives led to the adoption of synthetic musk, which is now used almost exclusively. The organic compound primarily responsible for the characteristic odor of musk is muscone.\n\nModern use of natural musk pods occurs in traditional Chinese medicine.\n\nThe musk deer belongs to the family Moschidae and lives in Nepal, Tibet,<ref name=\"http://hdl.handle.net/2027/uc1.32106000253184?urlappend=%3Bseq=72\"></ref> India, Pakistan, China, Siberia and Mongolia. The musk pod is normally obtained by killing the male deer through traps laid in the wild. Upon drying, the reddish-brown paste inside the musk pod turns into a black granular material called \"musk grain\", which is then tinctured with alcohol. The aroma of the tincture gives a pleasant odor only after it is considerably diluted. No other natural substance has such a complex aroma associated with so many contradictory descriptions; however, it is usually described abstractly as animalistic, earthy and woody or something akin to the odor of baby's skin.\n\nMusk has been a key constituent in many perfumes since its discovery, being held to give a perfume long-lasting power as a fixative. Today, the trade quantity of the natural musk is controlled by CITES, but illegal poaching and trading continues.\n\nMuskrat (\"Ondatra zibethicus\"), a rodent native to North America, has been known since the 17th century to secrete a glandular substance with a musky odor. A chemical means of extracting it was discovered in the 1940s, but it did not prove commercially worthwhile.\n\nGlandular substances with musk-like odors are also obtained from the musk duck (\"Biziura lobata\") of southern Australia, the muskox, the musk shrew, the musk beetle (\"Aromia moschata\"), the African civet (\"Civettictis civetta\"), the musk turtle (\"Sternotherus odoratus\"), the American alligator of North America, lynx musk, \"lungurion\" which, in antiquity, was highly valued, and from several other animals.\n\nIn crocodiles, there are two pairs of musk glands, one pair situated at the corner of the jaw and the other pair in the cloaca. Musk glands are also found in snakes.\n\nSome plants such as \"Angelica archangelica\" or \"Abelmoschus moschatus\" produce musky-smelling macrocyclic lactone compounds. These compounds are widely used in perfumery as substitutes for animal musk or to alter the smell of a mixture of other musks.\n\nThe plant sources include the musk flower (\"Mimulus moschatus\") of western North America, the muskwood (\"Olearia argophylla\") of Australia, and the musk seeds (\"Abelmoschus moschatus\") from India.\n\nSince obtaining the deer musk requires killing the endangered animal, nearly all musk fragrance used in perfumery today is synthetic, sometimes called \"white musk\". They can be divided into three major classes: aromatic nitro musks, polycyclic musk compounds, and macrocyclic musk compounds. The first two groups have broad uses in industry ranging from cosmetics to detergents. However, the detection of the first two chemical groups in human and environmental samples as well as their carcinogenic properties initiated a public debate on the use of these compounds and a ban or reduction of their use in many regions of the world. Macrocyclic musk compounds are expected to replace them since these compounds appear to be safer.\n\nMusk has been used to attract wild animals, including in man-made perfume mixtures. For example, in 2018 Indian authorities used the perfume \"Obsession\" by Calvin Klein to attract and thus trap a wild tiger that had attacked and killed more than a dozen humans.\n\n\n"}
{"id": "47888132", "url": "https://en.wikipedia.org/wiki?curid=47888132", "title": "Network Unaffiliated Virtual Operator", "text": "Network Unaffiliated Virtual Operator\n\nA Network Unaffiliated Virtual Operator (NUVO) is similar to an Mobile virtual network operator (MVNO) however it has one key difference — a NUVO is not affiliated with a specific carrier. \nNUVO’s use real telephone numbers, and through these they combine with all other commercial operators, allowing NUVO users to communicate with anyone — not just other people who use an mobile App, for example. \n\nBy assigning telephone numbers to their users, NUVO’s can transmit text messages and voice call to anyone who has a telephone number. \n\nThis allows an app user to communicate with anyone simply by dialing their phone number, rather than limiting their communications to just other app users, as most over the top content (OTT) apps do.\n\n"}
{"id": "47512065", "url": "https://en.wikipedia.org/wiki?curid=47512065", "title": "Open Location Code", "text": "Open Location Code\n\nThe Open Location Code (OLC) is a geocode system for identifying an area anywhere on the Earth.\nIt was developed at Google's Zürich engineering office, and released late October 2014. Open Location Codes are also referred to as \"plus codes\".\n\nOpen Location Codes are a way of encoding location into a form that is easier to use than showing coordinates in the usual form of latitude and longitude. They are designed to be used like street addresses, and may be especially useful in places where there is no formal system to identify buildings, such as street names, house numbers, and post codes.\n\nOpen Location Codes are derived from latitude and longitude coordinates, so they already exist everywhere. They are similar in length to a telephone number – 849VCWC8+R9, for example – but can often be shortened to only four or six digits when combined with a locality (CWC8+R9, Mountain View). Locations close to each other have similar codes. They can be encoded or decoded offline. The character set avoids similar looking characters, to reduce confusion and errors, and avoids vowels to make it unlikely that a code spells existing words. The Open Location Code is not case-sensitive, and can therefore be easily exchanged over the phone.\n\nSince August 2015, Google Maps supports plus codes in their search engine. The algorithm is licensed under the Apache License 2.0. and available on GitHub.\n\nThe Open Location Code system is based on latitudes and longitudes in WGS84 coordinates. Each code describes an area bounded by two parallels and two meridians out of a fixed grid, identified by the South-West corner and its size. The largest grid has blocks of 20 by 20 degrees (9 rows and 18 columns), and is divided in 20 by 20 subblocks up to four times. From that level onwards division is in 5 by 4 subblocks. The table shows the various block sizes at their maximum near the equator. The block width decreases with distance from the equator.\n\nThe full grid uses offsets from the South Pole (–90°) and the antimeridian (–180°) expressed in base 20 representation. To avoid misreading or spelling objectionable words, the encoding excludes vowels and symbols that may be easily confused with each other. The following table shows the mapping.\n\nThe code begins with up to five pairs of digits, each consisting of one digit representing latitude and one representing longitude. The biggest blocks have just two digits. After 8 digits, a plus sign \"+\" is inserted in the code as a delimeter to aid with visual parsing. After 10 digits at each subdivision, subblocks are coded in a single code digit as follows:\n\nConsider for example zooming in on the Merlion in Singapore (N 1.286785, E 103.854503). It lies in the block around the equator bounded by –10° and +10° and between 100° and 120° East. It has offsets 80° from the South Pole and 280° from the anti-meridian, or 4 and 14 as first base-20 digit, coded as 6 and P. Thus the code is 6P. This may be padded as 6P000000+\n\nNow refine this block to a subblock between 1° and 2° N and 103° and 104° E. This adds 11° and 3° to the SW corner. So the base-20 coordinate codes added are H and 5. The result is padded to 6PH50000+.\n\nAfter 4 refinements one lands on Merlion park as 6PH57VP3+PR.\n\nThe next step requires us to divide the square so far used to refine the position into a 4 by 5 grid and finding the cell the coordinates are pointing to. This is the cell named 6. The resulting Open Location Code is 6PH57VP3+PR6.\n\nWhen using the code in conjunction with Google Maps, it is common to omit the first 4 digits from the code and add a rough location, like a city, state or country. The above example then becomes 7VP3+PRJ Singapore.\n\n\n"}
{"id": "11390841", "url": "https://en.wikipedia.org/wiki?curid=11390841", "title": "Phryctoria", "text": "Phryctoria\n\nPhryctoria () was a means of communication used in Ancient Greece.\n\n\"Phryctoriae\" were towers build on selected mountaintops so that one tower (phryctoriae) would be visible to the next tower (usually 20 miles away). The towers were used for the transmission of a specific prearranged message. Flames were lit on one tower and then the next tower in succession also lit flames.\n\nAeschylus in the tragedy \"Agamemnon\" describe how the message for the fall of Troy arrived at Mycenae with phryctoriae.\n\nΙn the 2nd century BC the Greek engineers from Alexandria, Cleoxenes () and Democletus () invented the pyrseia (). \nThe letters of the Greek alphabet were listed on a table. Each letter corresponded to a row and a column on the table. By using two groups of torches (five torches in every group), the left indicating the row and the right the column of the table, they could send a message by defining a specific letter through combination of light torches.\n\nThe coding system was as follows:\n\nα β γ δ ε<br>\nζ η θ ι κ<br>\nλ μ ν ξ ο<br>\nπ ρ σ τ υ<br>\nφ χ ψ ω \n\nWhen they wanted to send the letter \"o\" (omicron), they opened five torches on the right set and three torches on the left set.\n\nLater Polybius used the same system in his Polybius square.\n\n\n"}
{"id": "8506247", "url": "https://en.wikipedia.org/wiki?curid=8506247", "title": "Plug (fishing)", "text": "Plug (fishing)\n\nPlugs are a popular type of hard-bodied fishing lure. They are widely known by a number of other names depending on the country and region. Such names include crankbait, wobbler, minnow, shallow-diver and deep-diver. The term minnow is usually used for long, slender, lures that imitate baitfish, while the term plug is usually used for shorter, deeper-bodied lures which imitate deeper-bodied fish, frogs and other prey. Shallow-diver and deep-diver refer to the diving capabilities of the lure, which depends on the size and angle of the lip, and lure buoyancy.\n\nThe concept of an attractant tied to the end of a line to entice fish goes back to prehistoric peoples, but the modern concept of the plug lure is attributed to James Heddon, a beekeeper from Dowagiac, Michigan, who was whittling a piece of wood one day in the late 19th century while relaxing alongside a millpond. When he rose to leave, he tossed the carved scrap of wood into the pond, and a large bass struck at it as it wobbled down through the water. Intrigued by this, Heddon began experimenting and perfected a design he dubbed the \"Lucky 13\" — a plug that is still sold today. By the early 20th century, many companies were in the business of designing and selling plug lures.\n\nThe construction of typical plug is:\n\nClassic plugs float on the surface or suspend in the water, but will dive under the surface of the water and swim with a side-to-side wobbling movement (hence the alternative name \"wobbler\") upon retrieval, which usually consists of slow to medium fast reel in. This makes the bait seen like a real fish which attracts predatory fish such as bass and pike. Plastic plugs can dive to either a very shallow depth, due to a small lip, or to a moderately deep depth (i.e. several meters), due to a large lip. The angle of the bib also affects diving depth. Sometimes, plugs are named after their diving ability, e.g. \"\"deep-diver\" or \"shallow-diver\"\". Plugs can also be designed to hover (neutral buoyancy), sink slowly, or sink rapidly. Some have a small metal ball inside to \"rattle\" when retrieved. \n\nOther lures are sometimes generically called plugs or wobblers. They come in all different shapes and sizes. These plugs are usually made by small companies and cost around twenty dollars. Each plug has its own action or has none unless you give it one. Wood plugs usually range from between 3/4 oz. and 4 oz. The different plugs come in a few basic categories. There are surface swimmers, subsurface swimmers, needlefish, poppers, pencil poppers, and darters. they all have their own specific place and time to be fished.\n\nPlugs range in size from around an inch (~3 cm) to around 8 inches (~20 cm). Plugs in the 2–3 inch (~5–7 cm) size range are most commonly used however. As a general rule large plugs are used for large fish, and small plugs for small fish. Fishermen casting for very small fish such as crappie will use very small plugs, and anglers fishing for large fish such muskellunge or Murray cod will use extremely large plugs. But trophy-sized fish are occasionally caught on very small plugs, and fingerling perch will sometimes strike — and hook themselves — on a plug as big as they are themselves. Plug fishing is more common in freshwater fishing than saltwater fishing, but it is growing greatly upon East-Coast striper fishermen.\n\nMost plugs have their \"action\" built into them by design, but good fishermen give nuance to the plug's action in many ways, such as varying the speed of the retrieve, occasionally \"twitching\" the rod tip during retrieve, or even letting the plug stop completely in the water, then resuming retrieve at a very high speed. Plugs are often cast so they land next to places where fish may be hiding, such as a snag pile or an overhanging tree and worked back enticingly. A skilled fisherman can methodically explore many possible hiding places for fish by continually casting and retrieving a plug.\n\nTheoretically, any plug design will catch fish (most anglers use them to catch bass) — fish will, out of anger, hunger, territorial protection or simple curiosity, occasionally strike at any small object moving or falling through the water. But some plugs have become famous for their high degree of effectiveness in the hands of a good fisherman, while others come and go from the market quickly when found to have limited success.\n"}
{"id": "3115478", "url": "https://en.wikipedia.org/wiki?curid=3115478", "title": "Screenless hammer mill", "text": "Screenless hammer mill\n\nThe screenless hammer mill, like regular hammer mills, is used to pound grain. However, rather than a screen, it uses air flow to separate small particles from larger ones. \n\nConventional hammer mills in poor and remote areas, such as many parts of Africa, suffer from the problem that screens break easily, and cannot be easily bought, made or repaired. Thus regular hammer mills break down and fall into disuse. The screenless hammer mill uses air flow to separate small particles from larger ones, rather than a screen, and is thus more reliable. \n\nThe screenless hammer mill is claimed to be 25% cheaper and much more energy efficient than regular hammer mills, as well as more reliable. \n\nIt was designed by Amy Smith of MIT.\n\n\n"}
{"id": "38097000", "url": "https://en.wikipedia.org/wiki?curid=38097000", "title": "Spin canting", "text": "Spin canting\n\nSome antiferromagnetic materials exhibit a non-zero magnetic moment at a temperature near absolute zero. This effect is ascribed to spin canting, a phenomenon through which spins are tilted by a small angle about their axis rather than being exactly co-parallel. \n\nSpin canting is due to two factors contrasting each other: isotropic exchange would align the spins exactly antiparallel, while antisymmetric exchange arising from relativistic effects (spin-orbit coupling) would align the spins at 90° to each other. The net result is a small perturbation, the extent of which depends on the relative strength of these effects.\n\nThis effect is observable in many clusters such as hematite.\n"}
{"id": "1990937", "url": "https://en.wikipedia.org/wiki?curid=1990937", "title": "Symphony of Fire", "text": "Symphony of Fire\n\nThe Symphony of Fire is an annual multi-day fireworks exhibition and friendly international competition held around the world. The fireworks are choreographed to music. They continue to be presented at Victoria & Alfred waterfront in Cape Town.\n\nIt was presented at English Bay in Vancouver, British Columbia, and at the Lake Ontario waterfront of the Ontario Place theme park in Toronto, Ontario, Canada until 2000, when these presentations were forced to seek new sponsors and, thus, the exhibition changed names.\n\nThe event had been previously sponsored by British American Tobacco (with the branding \"Benson & Hedges Symphony of Fire\"), but once tobacco advertising restrictions were legislated by the Canadian federal government in 2000, there were fears that the event would fail to receive sufficient funding to continue operations.\n\nThe Vancouver event, with various sponsors, has been re-branded as the \"Celebration of Light\".\n\nBritish American Tobacco informed Ontario Place Corporation, the hosts of the Toronto exhibition, that they could continue holding the annual event, but under a different name, as the \"Symphony of Fire\" trademark vested with Benson and Hedges. The Toronto event has since been renamed \"the Canada Dry Festival of Fire\".\n\nIn South Africa, the event is still marketed under the name \"Symphony of Fire\" and is sponsored by a local radio station.\n\nThe Cape Town events took place in 2000 and March 2006. Spain, Canada and South Africa took part in the 2006 event with South Africa winning on the 8 April.\n\n\n"}
{"id": "52847079", "url": "https://en.wikipedia.org/wiki?curid=52847079", "title": "Synchronverter", "text": "Synchronverter\n\nSynchronverters or virtual synchronous generators are inverters which mimic synchronous generators to provide \"synthetic inertia\" for ancillary services in electric power systems.\n\nStandard inverters are very low inertia elements. During transient periods, which are mostly because of faults or sudden changes in load, they follow changes rapidly and may cause a worse condition, but synchronous generators have a notable inertia that can maintain their stability.\n\nRecently by using more and more renewable energies, especially solar cells, more inverters have been used in grids and because of mentioned reason, this could endanger power system reliability.\n\nHydro-Québec began requiring synthetic inertia in 2005 as the first grid operator. To counter frequency drop, the grid operator demands a temporary 6% power boost by combining the power electronics with the rotational inertia of a wind turbine rotor. Similar requirements came into effect in Europe in 2016.\n\nSynchronverter structure can be divided into two parts: power part (see figure 2) and electronic part. The power part is energy transform and transfer path, including the bridge, filter circuit, power line, etc. The electronic part refers to measuring and control units, including sensors and DSP.\n\nThe important point in modeling synchronverter is to be sure that it has similar dynamic behavior to Synchronous generator (see figure 3). This model is classified into 2-order up to 7-order model, due to its complexity. However, 3-order model is widely used because of proper compromise between accuracy and complexity.\nwhere formula_3 and formula_4 are dq-axes components of terminal voltage.\n\nWhile synchronverter terminal voltage and current satisfy these equations, synchronverter can be looked as Synchronous generator. This make it possible to replace it by a synchronous generator model and solve the problems easily.\n\nAs shown in the figure 3, when the inverter is controlled as a voltage source, it consists of a synchronization unit to synchronize with the grid and a power loop to regulate the real power and reactive power exchanged with the grid. The synchronization unit often needs to provide frequency and amplitude. But when inverter is controlled as a current source, the synchronization unit is often required to provide the phase of the grid only, so it is much more easier to control it as a current source.\n\nSince a synchronous is inherently able to synchronize with the grid, it is possible to integrate the synchronization function into the power controller without synchronization unit. This results in a compact control unit, as shown in the figure 4.\n\nAs mentioned before, synchronverters can be treated like synchronous generator, which make it easier to control the source, so it should be widely used in PV primary energy sources (PES).\n\nSynchronverter also is suggested to be used in microgrids because DC sources can be coordinated together with the frequency of the ac voltage, without any communication network.\n\n"}
{"id": "5055146", "url": "https://en.wikipedia.org/wiki?curid=5055146", "title": "Thermal Battery", "text": "Thermal Battery\n\nA thermal energy battery is a physical structure used for the purpose of storing and releasing thermal energy—see also thermal energy storage. Such a thermal battery (a.k.a. TBat) allows energy available at one time to be temporarily stored and then released at another time. The basic principles involved in a thermal battery occur at the atomic level of matter, with energy being added to or taken from either a solid mass or a liquid volume which causes the substance's temperature to change. Some thermal batteries also involve causing a substance to transition thermally through a phase transition which causes even more energy to be stored and released due to the delta enthalpy of fusion or delta enthalpy of vaporization.\n\nThermal batteries are very common, and include such familiar items as a hot water bottle. Early examples of thermal batteries include stone and mud cook stoves, rocks placed in fires, and kilns. While stoves and kilns are ovens, they are also thermal storage systems that depend on heat being retained for an extended period of time. Earthships rely on heat from the Sun to charge insulated thermal batteries, securing stable temperatures and comfort in any climate without utility bills. This reduces nuclear and coal power dependence and infrastructure and provides a sustainable \"HVAC\" technology for future generations.\n\nThermal batteries generally fall into 3 categories:\n\n\nThese 3 types of batteries are each unique in their form and application, although fundamentally all are for the storage and retrieval of thermal energy. They also differ in method and density of heat storage. A description of each type of thermal battery follows.\n\nPhase change materials used for thermal storage are capable of storing and releasing significant thermal capacity at the temperature that they change phase. These materials are chosen based on specific applications because there is a wide range of temperatures that may be useful in different applications and a wide range of materials that change phase at different temperatures. These materials include salts and waxes that are specifically engineered for the applications they serve. In addition to manufactured materials, water is a phase change material. The latent heat of water is 334 joules/gram. The phase change of water occurs at 0°C (32°F).\n\nSome applications use the thermal capacity of water or ice as cold storage; others use it as heat storage. It can serve either application; ice can be melted to store heat then refrozen to warm an environment which is below freezing (putting liquid water at 0°C in such an environment warms it much more than the same mass of ice at the same temperature, because the latent heat of freezing is extracted from it, which is why the phase change is relevant), or water can be frozen to \"store cold\" then melted to make an environment above freezing colder (and again, a given mass of ice at 0°C will provide more cooling than the same mass of water at the same temperature).\n\nThe advantage of using a phase change in this way is that a given mass of material can absorb a large quantity of energy without its temperature changing. Hence a thermal battery that uses a phase change can be made lighter, or more energy can be put into it without raising the internal temperature unacceptably.\n\nAn encapsulated thermal battery is physically similar to a phase change thermal battery in that it is a confined amount of physical material which is thermally heated or cooled to store or extract energy. However, in a non-phase change encapsulated thermal battery the temperature of the substance is changed without inducing a phase change. Since a phase change is not needed many more materials are available for use in an encapsulated thermal battery.\n\nOne of the key properties of an encapsulated thermal battery is its volumetric heat capacity (VHC), also termed volume-specific heat capacity. Typical substances used for these thermal batteries include water, concrete, and wet sand.\n\nAn example of an encapsulated thermal battery is a residential water heater with a storage tank. This thermal battery is usually slowly charged over a period of about 30–60 minutes for rapid use when needed (e.g., 10–15 minutes). Many utilities, understanding the \"thermal battery\" nature of water heaters, have begun using them to absorb excess renewable energy power when available for later use by the homeowner. According to the above cited article, \"net savings to the electricity system as a whole could be $200 per year per heater – some of which may be passed on to its owner\".\n\nThere are some other items that have historically been termed \"thermal batteries\". In this group is the molten salt battery which is a device for generating electricity. Other examples include the heat packs that skiers use for keeping hands and feet warm (see hand warmer). These are a chemical battery which when activated (with air in this case) will produce heat. Other related chemical thermal batteries exist for producing cold (see instant cold pack) generally used for sport injuries.\n\nThe one common principle of these other thermal batteries is that the reaction involved is generally not reversible. Thus, these batteries are not used for storing and retrieving heat energy.\n"}
{"id": "19345639", "url": "https://en.wikipedia.org/wiki?curid=19345639", "title": "Underground hydrogen storage", "text": "Underground hydrogen storage\n\nUnderground hydrogen storage is the practice of hydrogen storage in underground caverns, salt domes and depleted oil/gas fields. Large quantities of gaseous hydrogen have been stored in underground caverns by ICI for many years without any difficulties. The storage of large quantities of hydrogen underground in solution-mined salt domes, aquifers or excavated rock caverns or mines can function as grid energy storage which is essential for the hydrogen economy. By using a turboexpander the electricity needs for compressed storage on 200 bar amounts to 2.1% of the energy content.\n\nThe Chevron Phillips Clemens Terminal in Texas has stored hydrogen since the 1980s in a solution-mined salt cavern. The cavern roof is about underground. The cavern is a cylinder with a diameter of , a height of and a usable hydrogen capacity of , or .\n\n\n\n"}
{"id": "10037111", "url": "https://en.wikipedia.org/wiki?curid=10037111", "title": "Unitary Plan Wind Tunnel (Mountain View, California)", "text": "Unitary Plan Wind Tunnel (Mountain View, California)\n\nThe Unitary Plan Wind Tunnel, located at the NASA Ames Research Center in Moffett Federal Airfield, Mountain View, California, United States, is a research facility used extensively to design and test new generations of aircraft, both commercial and military, as well as NASA space vehicles, including the Space Shuttle. The facility was completed in 1955 and is one of five facilities created after the 1949 Unitary Wind Tunnel Plan Act supporting aeronautics research.\n\nAfter the construction of the Variable Density Tunnel at Langley in 1921, the National Advisory Committee for Aeronautics built a variety of technical research facilities upon which the American aircraft industry was based. These facilities enabled the American aircraft industry to dominate the skies in both commercial and military aviation. By 1945, America's lead in the field of aviation seemed to be evaporating. The technological achievements of the German missiles and jet aircraft indicated a lag in American aeronautical research. \nIn 1949, Congress passed the Unitary Wind Tunnel Plan Act, under which the Federal government coordinated a national plan of facility construction encompassing NACA, as well as the Air Force, private industry, and universities. The Unitary Plan resulted in the construction of a new series of wind tunnel complexes to support the American aircraft industry, including the Ames Unitary Plan Wind Tunnel Complex.\n\nConstruction of this facility began in 1950-1951 and continued until 1955. Because no one wind tunnel could meet all the demands for additional research facilities simulating the entire range of aircraft and missile flight, NACA chose to build the Ames tunnel with three separate test sections drawing power from a common centralized power plant. The transonic test section spanned 11 by 11 feet (3.3 x 3.3 m), while the two supersonic sections were smaller: nine by seven feet (2.7 x 2.1 m) and eight by seven feet (2.4 x 2.1 m). Giant valves 20 feet (6 m) in diameter supplied air from one supersonic leg to another. \nThe American West Coast aircraft industry quickly capitalized on the Ames Unitary Plan Wind Tunnel Complex. The famed Boeing fleet of commercial transports and the Douglas DC-8, DC-9, and DC-10 were all tested here; as well as military aircraft such as the F-111 fighter, the C-5A Galaxy transport and the B-1 Lancer bomber. In addition to aircraft, in the 1960s and 1970s almost all NASA manned space vehicles including the Space Shuttle were tested in the Ames Unitary Plan Wind tunnel complex.\n\nThe Unitary Plan Wind Tunnel has three closed-loop wind tunnels, each with its own model test section, but all sharing the same powerful electric motors (100,000 horsepower, 75,000 kilowatts) driving their compressors to propel the air within them. Because of this shared layout, only one UPWT test section can be used at a time. The three wind tunnels that are part of this system are:\n\nThe Unitary Plan Wind Tunnel was declared a National Historic Landmark in 1985.\n\nThe major element of the tunnel complex is its drive system, consisting of four intercoupled electric motors. The transonic wind tunnel is a closed-return, variable density tunnel with a fixed geometry, ventilated throat, and a single-jack flexible nozzle. Airflow is produced by a three-stage, axial-flow compressor powered by four-wound-rotor, variable-speed induction motors. For conventional steady-state tests, models are generally supported on a string. A Schlieren system, one that allows regions of varying refraction in a transparent medium caused by pressure or temperature differences and detectable by photographing the passage of a beam of light, is available for studying flow patterns, either by direct viewing or by photographs.\n\nThe details of the larger supersonic tunnel are much the same, except that it is equipped with an asymmetric, sliding-block nozzle and the airflow is produced by an 11-stage, axial-flow compressor powered by four variable-speed, wound-rotor, induction motors. The smaller supersonic tunnel is a closed-return, variable-density tunnel equipped with a symmetrical, flexible-wall throat and the sidewalls are positioned by a series of jacks operated by hydraulic motors.\n\n\n"}
