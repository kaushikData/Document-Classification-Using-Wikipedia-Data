{"id": "34082915", "url": "https://en.wikipedia.org/wiki?curid=34082915", "title": "Avtobaza", "text": "Avtobaza\n\nKvant 1L222 Avtobaza is an ELINT system designed to detect Side looking airborne radars, air-to-ground fire-control radars and low-altitude flight control radars, as well as to provide intelligence data for the 1L125M APUR.\n\n\nThe ELINT system displays on the TV screen acquired targets with data on their direction finding, angular coordinates (azimuth and elevation), radiation signal parameters (carrier frequency, duration, pulse repetition frequency) and radar type classification (sidelooking, fire control, low-altitude flight control radar). The APUR automated jamming control system is fed with target data (frequency band number according to frequency assignment of jamming systems, type of emitting radars and their angular coordinates) via cable at a range of up to 100 meters.\n\nThere are unconfirmed reports of the system being used in the capture of an RQ-170 UAV by the Iranian forces on 4 December 2011.\n\nThere are also reports of it being used by the Russian forces during the Crimean Crisis in March 2014 to overtake controls of an American drone.\n\n\n"}
{"id": "30540221", "url": "https://en.wikipedia.org/wiki?curid=30540221", "title": "Birdsmouth joint", "text": "Birdsmouth joint\n\nIn light frame construction, a birdsmouth joint or bird's beak cut is a woodworking joint that is generally used to connect a roof rafter to the top plate of a supporting wall. It is an indentation cut into the rafter which consists of a \"seat cut\" (the face of which rests on the top plate) and a \"heel cut\" or \"plumb cut\" (the face of which lies parallel to the supporting wall), forming a shape resembling a bird's mouth. The indentation should not extend unsupported on the interior in order to maintain the structural integrity of the rafter because the unsupported section can split along the grain of the wood. The joint is generally fastened with nails by toenailing the rafter from the side into the top plate below.\n\nThe depth of a rafter cut varies according to local building code differences in different locations. As a general rule, no more than one third of the depth of the rafter should be removed, in order to maintain structural integrity.\n"}
{"id": "2824933", "url": "https://en.wikipedia.org/wiki?curid=2824933", "title": "Bit banging", "text": "Bit banging\n\nIn computer engineering and electrical engineering, bit banging is slang for various techniques in which serial communications use software instead of dedicated hardware to process and make use of signals. Software directly sets and samples the state of pins on a microcontroller, and is responsible for all parameters of the signal: timing, levels, synchronization, etc. In contrast to bit banging, dedicated hardware (such as a modem, UART, or shift register) handles these parameters and provides a (buffered) data interface in other systems, so software is not required to perform signal demodulation. Bit banging can be implemented at very low cost, and is used in embedded systems.\n\nBit banging allows the same device to use different protocols with minimal or no hardware changes required. In many cases, bit banging is made possible because more recent hardware operates much more quickly than hardware did when standard communications protocols were created.\n\nSending a byte on an SPI bus.\n\nThe question whether to deploy bit banging or not is a trade-off between load, performance and reliability on the one hand, and the availability of a hardware alternative on the other. The software emulation process consumes more processing power than does supporting dedicated hardware. The microcontroller spends much of its time sending or receiving samples to and from the pins, at the expense of other tasks. The signal produced usually has more jitter or glitches, especially if the processor is also executing other tasks while communicating. However, if the bit-banging software is interrupt-driven by the signal, this may be of minor importance, especially if control signals such as RTS, CTS, or DCD are available. The implementation in software can be a solution when specific hardware support is not available or requires a more expensive microcontroller.\n\n\n\n\n"}
{"id": "2910720", "url": "https://en.wikipedia.org/wiki?curid=2910720", "title": "Casing (borehole)", "text": "Casing (borehole)\n\nCasing is large diameter pipe that is assembled and inserted into a recently drilled section of a borehole. Similar to the bones of a spine protecting the spinal cord, casing is set inside the drilled borehole to protect and support the wellstream. The lower portion (and sometimes the entirety) is typically held in place with cement. Deeper strings usually are not cemented all the way to the surface, so the weight of the pipe must be partially supported by a casing hanger in the wellhead. \n\nCasing that is cemented in place aids the drilling process in several ways:\n\nOptimum design of the casing program decreases the well construction costs, enhances the efficiency of operations and also diminishes the environmental impacts. \nA slightly different metal string, called production tubing, is often used without cement inside the final casing string of a well to contain production fluids and convey them to the surface from an underground reservoir.\n\nIn the planning stages of a well, a drilling engineer, usually with input from geologists and others, will pick strategic depths at which the hole will need to be cased in order for drilling to reach the desired total depth. This decision is often based on subsurface data such as formation pressures and strengths, and is balanced against the cost objectives and desired drilling strategy.\n\nWith the casing set depths determined, hole sizes and casing sizes must follow. The hole drilled for each casing string must be large enough to accommodate the casing to be placed inside it, allowing room for cement between the outside of that casing and the hole. Also, subsequent bits that will continue drilling obviously must pass through existing casing strings. Thus, each casing string will have a subsequently smaller diameter. The inside diameter of the final casing string (or penultimate one in some instances of a liner completion) must accommodate the production tubing and associated hardware such as packers, gas lift mandrels and subsurface safety valves.\n\nCasing design for each size of designed pipes is done by calculating the worst conditions that may be faced during drilling and over the producing life of the well. Mechanical properties such as longitudinal tensile strength, and burst and collapse resistance (calculated considering biaxial effects of axial and hoop stresses), must be sufficient at various depths. Pipe of differing strengths often comprises a long casing string, which typically will have the greatest axial tension and perhaps highest internal burst pressure differentials in the upper parts, and the greatest collapsing loads deeper in the well from external pressure vs lowered internal pressure.\n\nCasing strings are supported by casing hangers that are set in the wellhead, which later will be topped with the Christmas tree. The lower members of the wellhead usually are installed on top of the first casing string after it has been cemented in place.\n\nTypically, a well contains multiple intervals of casing successively placed within the previous casing run. The following casing intervals are typically used in an oil or gas well:\n\n\nThe conductor casing serves as a support during drilling operations, to flowback returns during drilling and cementing of the surface casing, and to prevent collapse of the loose soil near the surface. It can normally vary from sizes such as 18\" to 30\".\n\nThe purpose of surface casing is to isolate freshwater zones so that they are not contaminated during drilling and completion. Surface casing is the most strictly regulated due to these environmental concerns, which can include regulation of casing depth and cement quality. A typical size of surface casing is 13⅜ inches.\n\nIntermediate casing may be necessary on longer drilling intervals where necessary drilling mud weight to prevent blowouts may cause a hydrostatic pressure that can fracture shallower or deeper formations. Casing placement is selected so that the hydrostatic pressure of the drilling fluid remains at a pressure level that is between formation pore pressures and fracture pressures.\n\nIn order to reduce cost, a liner may be used which extends just above the shoe (bottom) of the previous casing interval and hung off downhole rather than at the surface. It may typically be 7\", although many liners match the diameter of the production tubing.\n\nFew wells actually produce through casing, since producing fluids can corrode steel or form deposits such as asphaltenes or paraffin waxes and the larger diameter can make flow unstable. Production tubing is therefore installed inside the last casing string and the tubing annulus is usually sealed at the bottom of the tubing by a packer. Tubing is easier to remove for maintenance, replacement, or for various types of workover operations. It is significantly lighter than casing and does not require a drilling rig to run in and out of hole; smaller \"service rigs\" are used for this purpose.\n\nCementing is performed by circulating a cement slurry through the inside of the casing and out into the annulus through the casing shoe at the bottom of the casing string. In order to precisely place the cement slurry at a required interval on the outside of the casing, a plug is pumped with a displacement fluid behind the cement slurry column, which \"bumps\" in the casing shoe and prevents further flow of fluid through the shoe. This bump can be seen at surface as a pressure spike at the cement pump. To prevent the cement from flowing back into the inside of the casing, a float collar above the casing shoe acts as a check valve and prevents fluid from flowing up through the shoe from the annulus.\n\n"}
{"id": "45353668", "url": "https://en.wikipedia.org/wiki?curid=45353668", "title": "Chineasy", "text": "Chineasy\n\nChineasy is an Internet startup created with the purpose of teaching characters, created by the entrepreneur Shaolan Hsueh. For visual learners the human brain is able to memorize information better if it is put into a visual context. The approach is to learn Chinese characters with the help of illustrations to help memorizing Chinese characters better. The 2014 book \"Chineasy: The New Way to Read Chinese\" contains about 400 characters. It was based on her 2013 TED talk and funded via a crowdfunding campaign through Kickstarter.\n\nWhile the book introduces common Chinese characters, it does not teach pronunciation or grammar, and thus does not teach how to read or use the language.Although it does use voice recordings for the users to mimic.\n\nChineasy teaches sometimes traditional and sometimes simplified forms. Hsueh argued that traditional and simplified forms of Chinese still share a great number of characters, and in real life – just as in the case of British English and American English – you will come across both forms. Where they differ, she shows the other version as well.\n\nChineasy has been widely featured in the press, including the Financial Times, the Wall Street Journal, Time magazine, and National Public Radio. It won \"Wallpaper’s 2014 Design Award\". Hsueh's book uses illustrations and storytelling. Characters are illustrated by various illustrators including Noma Bar.\n"}
{"id": "3391305", "url": "https://en.wikipedia.org/wiki?curid=3391305", "title": "Copyholder", "text": "Copyholder\n\nA copyholder is a device that holds the hand written or printed material being typed by a copy typist. They were used in the past with typewriters and are now used with computers and word processors like Writer or Word.\n\nSome copyholders stand independently whilst others are attached to CRT based computer monitors. They can support entire booklets or a single page. Elaborate varieties can even be adjusted by height or angle and some come with rulers and guides that appear over the front of the page. \n"}
{"id": "32888896", "url": "https://en.wikipedia.org/wiki?curid=32888896", "title": "DTU Roadrunners", "text": "DTU Roadrunners\n\nDTU Roadrunners is a student driven project at the Technical University of Denmark competing in the Shell Eco-marathon races. The team participates in one or both of the two classes at Shell Eco-Marathon Europe: the UrbanConcept class and the Prototype class with the cars Dynamo and Innovator respectively. The team consists of 20 to 30 students. Project work in the course is based on the CDIO-working form and students are thus responsible for the development, construction and operation of the vehicles.\n\nThe UrbanConcept category in the Shell Eco-Marathon has the aim to create a very fuel-efficient car that looks similar to a small city car.\nThe Autonomous UrbanConcept challenge was launched in 2018 with multiple challenges for self-driving vehicles. DTU Dynamo became the first student-built car to complete a lap on the Shell Eco-Marathon track fully autonomously.\n\n"}
{"id": "414806", "url": "https://en.wikipedia.org/wiki?curid=414806", "title": "Darkroom", "text": "Darkroom\n\nA darkroom is a workshop used by photographers working with photographic film to make prints and carry out other associated tasks. It is a room that can be made completely dark to allow the processing of the light-sensitive photographic materials, including film and photographic paper. Various equipment is used in the darkroom, including an enlarger, baths containing chemicals, and running water.\n\nDarkrooms have been used since the inception of photography in the early 19th century. Darkrooms have many various manifestations, from the elaborate space used by Ansel Adams to a retooled ambulance wagon used by Timothy H. O'Sullivan. From the initial development of the film to the creation of prints, the darkroom process allows complete control over the medium.\n\nDue to the popularity of color photography and complexity of processing color film (\"see C-41 process\") and printing color photographs and also to the rise, first of Polaroid technology and later digital photography, darkrooms are decreasing in popularity, though are still commonplace on college campuses, schools and in the studios of many professional photographers.\n\nOther applications of darkrooms include the use in nondestructive testing, such as magnetic particle inspection.\n\nIn most darkrooms, an enlarger, an optical apparatus similar to a slide projector, that projects light through the image of a negative onto a base, finely controls the focus, intensity and duration of light, is used for printmaking. A sheet of photographic paper is exposed to the light coming through the negative (photography), resulting in a positive version of the image on the paper.\nWhen making black-and-white prints, a safelight is commonly used to illuminate the work area. Since the majority of black-and-white papers are sensitive to only blue, or to blue and green light, a red- or amber-colored light can be safely used without exposing the paper.\n\nColor print paper, being sensitive to all parts of the visible spectrum, must be kept in complete darkness until the prints are properly fixed.\n\nAnother use for a darkroom is to load film in and out of cameras, development spools, or film holders, which requires complete darkness. Lacking a darkroom, a photographer can make use of a changing bag, which is a small bag with sleeved arm holes specially designed to be completely light proof and used to prepare film prior to exposure or developing.\n\nDuring exposure, values in the image can be adjusted, most often by \"dodging\" (reducing the amount of light to a specific area of an image by selectively blocking light to it for part or all of the exposure time) and/or \"burning\" (giving additional exposure to specific area of an image by exposing only it while blocking light to the rest). Filters, usually thin pieces of colored plastic, can be used to increase or decrease an image's contrast (the difference between dark tones and light tones). One method of photographic printing, called \"split filter printing,\" is where the photographer determines two separate exposure times using two separate filters (typically a 0 or 00, and a 5) to create a single print. This method allows the photographer to achieve a broad tonal range, with detailed highlights and rich blacks. After exposure, the photographic printing paper (which still appears blank) is ready to be processed.\n\nPhotographers generally begin printing a roll of film by making a contact print of their negatives to use as a quick reference to decide which images to enlarge. Some large format photographers, such as Edward Weston, make only contact prints of their large (4x5\", 5x7\", 8x10\" or larger) negatives. \n\nThe paper that has been exposed is processed, first by immersion in a photographic developer, halting development with a stop bath, and fixing in a photographic fixer. The print is then washed to remove the processing chemicals and dried. There are a variety of other, additional steps a photographer may take, such as toning.\n\n"}
{"id": "12122274", "url": "https://en.wikipedia.org/wiki?curid=12122274", "title": "Data Processing and Analysis Consortium", "text": "Data Processing and Analysis Consortium\n\nThe Gaia Data Processing and Analysis Consortium (DPAC) is a group of over 400 European scientists and software engineers formed with the objective to design, develop and execute the data processing system for ESA's ambitious Gaia space astrometry mission. It was formally formed in June 2006 by European scientists, with the initial goal of answering an Announcement of Opportunity to be issued by ESA before the end of that year. At a meeting in Paris on 24–25 May 2007, ESA's Science Programme Committee (SPC) approved the DPAC proposal submitted in response to the Announcement of Opportunity for the Gaia data processing. The proposal describes a complete data processing system capable of handling the full size and complexity of the Gaia data within the mission schedule. Following the SPC approval, the DPAC is officially responsible for all Gaia data processing activities.\n\nOn 1 January 2010, DPAC comprises 430 members coming from 24 European countries, with the largest contributions coming from France, Italy, UK, Germany, Belgium, Spain and Switzerland. The consortium is organized around a set of nine Coordination Units (CUs), eight being each in charge of a particular aspect of the processing, and the last one being in charge of the publication of the Catalogue.\n\n"}
{"id": "2921458", "url": "https://en.wikipedia.org/wiki?curid=2921458", "title": "Declared net capacity", "text": "Declared net capacity\n\nDeclared net capacity (DNC) is a measure of the contribution that a power station makes to the overall capacity of a distribution grid. It is measured in megawatts (MW), or in \"megawatts electrical\" (MWe) for a thermal power station.\n\nDNC is sometimes expanded as developed net capacity in British English; The two expansions have exactly the same meaning.\n\nIn a conventional power station, the DNC rating is simply the maximum rated output minus the power consumed onsite. It is sometimes termed the \"switchyard\" output, and takes no account of transmission losses in the grid, which may be considerable in the case of a remote hydro station for example. Most but not all quoted power station ratings are DNC ratings rather than the simple capacity of the alternators.\n\nIn the case of a wind power station, the situation is more complex. The alternator of a wind turbine is normally specified to match the strongest wind in which the turbine is designed to operate. This is because most of the cost of a wind turbine is in the rotor and the tower and bearings that support it, rather than in the alternator. It makes no economic sense to restrict the size of the alternator to anything less than the maximum that the rotor will deliver. However, this means that, unlike a conventional power station, a wind turbine rarely achieves its maximum rated output while operating.\n\nWhile for conventional power stations, the station is only regarded as \"available\" if the full power output is achievable, for wind power stations no power at all may be available depending on the wind strength, and even if a turbine is operating it may be producing as little as a tenth of its maximum rated capacity. A typical average figure is between one-third and one-half of the maximum rated capacity. \n\nThere are several suggested methods of allowing for this when quoting a DNC figure for a wind farm, but none has gained general acceptance, and the capacity quoted for a wind farm is normally a simple total of the maximum rated capacities of the turbines, sometimes termed the \"peak capacity\". Many wind schemes now also quote their expected or actual annual output in GWh, to allow more meaningful comparisons with other forms of generation than is possible just by considering this total rated output.\n\n\n"}
{"id": "31321957", "url": "https://en.wikipedia.org/wiki?curid=31321957", "title": "Direct bonding", "text": "Direct bonding\n\nDirect bonding, or fusion bonding, describes a wafer bonding process without any additional intermediate layers. The bonding process is based on chemical bonds between two surfaces of any material possible meeting numerous requirements.\nThese requirements are specified for the wafer surface as sufficiently clean, flat and smooth. Otherwise unbonded areas so called voids, i.e. interface bubbles, can occur.\n\nThe procedural steps of the direct bonding process of wafers any surface is divided into\nEven though direct bonding as a wafer bonding technique is able to process nearly all materials, silicon is the most established material up to now. Therefore, the bonding process is also referred to as silicon direct bonding or silicon fusion bonding. The fields of application for silicon direct bonding are, e.g. manufacturing of Silicon on insulator (SOI) wafers, sensors and actuators.\n\nThe silicon direct bonding is based on intermolecular interactions including van der Waals forces, hydrogen bonds and strong covalent bonds.\nThe initial procedure of direct bonding was characterized by a high process temperature. There is demand to lower the process temperature due to several factors, one is for instance the increasing number of utilized materials with different coefficients of thermal expansion. Hence, the aim is to achieve a stable and hermetic direct bond at a temperature below 450 °C. Therefore, processes for wafer surface activation i.e. plasma treatment or chemical-mechanical polishing (CMP), are being considered and are actively being researched. The upper limit of 450 °C bases on the limitations of back-end CMOS processing and the beginning of interactions between the applied materials.\n\nThe adhering effect of smooth and polished solid surfaces is first mentioned by Desaguliers (1734). His discovery was based on the friction between two surfaces of solids. The better the surfaces are polished the lower the friction is between those solids. This statement he described is only valid until a specific point. From this point on the friction starts to rise and the surfaces of the solids start to adhere together.\nFirst reports of successful silicon direct bonding were published 1986 among others by J. B. Lasky.\n\nDirect bonding is mostly referred to as bonding with silicon. Therefore process techniques are divided in accordance with the chemical structure of the surface in hydrophilic (compare to scheme of a hydrophilic silicon surface) or hydrophobic (compare to scheme of a hydrophobic silicon surface).\n\nThe surface state of a silicon wafer can be measured by the contact angle a drop of water forms. In the case of a hydrophilic surface the angle is small (< 5 °) based on the excellent wettability whereas a hydrophobic surface shows a contact angle larger than 90 °.\n\nBefore bonding two wafers, those two solids need to be free of impurities that can base on particle, organic and/or ionic contamination. To achieve the cleanliness without degrading the surface quality, the wafer passes a dry cleaning, e.g. plasma treatments or UV/ozone cleaning, or a wet chemical cleaning procedure.\nThe utilization of chemical solutions combines sequential steps. An established industrial standard procedure is SC (Standard Clean) purification by RCA. It consists of two solutions\nSC1 is used for removing organic contaminations and particles at a temperature of 70 °C to 80 °C for 5 to 10 min and SC2 is used for removing metal ions at 80 °C for 10 min.\nSubsequently, the wafers are rinsed with or stored in deionized water. The actual procedure needs to be adapted to every application and device because of usually existing interconnects and metallization systems on the wafer.\n\nBefore contacting the wafers, those have to be aligned. If the surfaces are sufficiently smooth, the wafers start to bond as soon as they get in atomic contact as shown in infrared photograph of a bond wave.\n\nThe wafers are covered with water molecules so the bonding happens between chemisorbed water molecules on the opposing wafer surfaces. In consequence a significant fraction of Si-OH (silanol) groups start to polymerize at room temperature forming Si-O-Si and water and a sufficient bonding strength for handling the wafer stack is assured. The formed water molecules will migrate or diffuse along the interface during annealing.\n\nAfter the pre-bonding in air, in a special gaseous atmosphere or vacuum, the wafers have to pass an annealing process for increasing the bonding strength. The annealing therefore provides a certain amount of thermal energy which forces more silanol groups to react among each other and new, highly stable chemical bindings are formed. The kind of binding which forms directly depends on the amount of energy which has been delivered or the applied temperature respectively. In consequence the bonding strength rises with increasing annealing temperatures.\n\nBetween room temperature and 110 °C the interface energy remains low, water molecules diffuse at the bond interface, leading to a rearrangement, causing more hydrogen-bonds. At temperatures from 110 °C to 150 °C silanol groups polymerize to siloxane and water, but also a slow fracture takes place. This reaction equates a thermo dynamical equilibrium and a higher density of silanol groups results in a higher number of siloxane and an increasing bond strength.\n\nNo further processes are observed at the interface between 150 °C and 800 °C until all OH-groups are polymerized and the composite strength remains constant.\n\nAbove 800 °C native oxide gets viscous and starts to flow at the interface, which increases the area of contacted surfaces. So, the diffusion of trapped hydrogen molecules along the interface is enhanced and interface voids may reduce in size or disappear at all. The annealing process is finished by the cooling of the wafer stack. \nThe interface energy increases to more than 2  at 800 °C with a native oxide layer or at 1000 °C if the wafers are covered by thermal oxide (compare diagram of surface energy). In case one wafer contains a layer of thermal oxide and the other wafer is covered by a native oxide, the surface energy development is similar to a wafer pair both covered with a native oxide layer.\n\nA hydrophobic surface is generated if the native oxide layer is removed by either plasma treatment or by fluoride containing etching solutions, e.g. hydrogen fluoride (HF) or ammonium fluoride (NHF). This process enhances the formation of Si-F bonds of the exposed silicon atoms. For hydrophobic bonding it is important to avoid re-hydrophilization, e.g. by rinsing and spin-drying, since Si-F bonds contacted with water result in Si-OH.\n\nPrior to bonding the surface is covered with hydrogen and fluorine atoms. The bonding at room temperature is mostly based on van-der-Waals forces between those hydrogen and fluorine atoms. Compared to bonding with hydrophilic surfaces, the interface energy is lower directly after contacting. This fact builds up the need for a higher surface quality and cleanliness to prevent unbonded areas and thereby to achieve a full-surface contact between the wafers (compare infrared photograph of a bond wave). Similar to bonding of hydrophilic surfaces, the pre-bond is followed by an annealing process.\n\nFrom room temperature to 150 °C no important interface reactions occur and the surface energy is stable. Between 150 °C and 300 °C more Si-F-H-Si bonds are formed. Above 300 °C the desorption of hydrogen and fluoride from the wafer surface leads to redundant hydrogen atoms that diffuse in the silicon crystal lattice or along interface. As a result, covalent Si-Si bonds start to establish between opposing surfaces. At 700 °C the transition to Si-Si bonds is completed.\nThe bonding energy reaches cohesive strengths of bulk silicon (compare diagram of surface energy).\n\nEven though direct bonding is highly flexible in processing numerous materials, the mismatch of CTE (coefficient of thermal expansion) using different materials is a substantial restriction for wafer level bonding, especially the high annealing temperatures of direct bonding.\n\nThe focus in research is put on hydrophilic silicon surfaces. The increase of the bonding energy is based on the conversion of silanol- (Si-OH) into siloxane-groups (Si-O-Si). The diffusion of water is mentioned as limiting factor because water has to be removed from the interface before close contact of surfaces is established. The difficulty is that water molecules may react with already formed siloxane-groups (Si-O-Si), so the overall energy of adhesion gets weaker.\n\nLower temperatures are important for bonding pre-processed wafers or compound materials to avoid undesirable changes or decomposition. The reduction of the required annealing temperature can be achieved by different pretreatments such as:\n\n\nFurthermore, research has shown that a lower annealing temperature for hydrophobic surfaces is possible with wafer pre-treatment based on:\n\nThis technique is usable for the fabrication of multi wafer micro structures, i.e. accelerometers, micro valves and micro pumps.\n"}
{"id": "10333", "url": "https://en.wikipedia.org/wiki?curid=10333", "title": "EFTPOS", "text": "EFTPOS\n\nElectronic funds transfer at point of sale (EFTPOS ) is an electronic payment system involving electronic funds transfers based on the use of payment cards, such as debit or credit cards, at payment terminals located at points of sale. EFTPOS technology originated in the United States in 1981 and was adopted by other countries. In Australia and New Zealand, it is also the brand name of a specific system used for such payments; these systems are mainly country specific and do not interconnect.\n\nDebit and credit cards are embossed plastic cards complying with ISO/IEC 7810 ID-1 standard. The cards have an embossed bank card number conforming with the ISO/IEC 7812 numbering standard.\n\nEFTPOS technology originated in the United States in 1981 and was rolled out in 1982. Initially, a number of nationwide systems were set up, such as \"Interlink\", which were limited to participating correspondent banking relationships, not being linked to each other. Consumers and merchants were slow to accept it, and there was minimal marketing. As a result, growth and market penetration of EFTPOS was minimal in the US up to the turn of the century.\n\nIn a short time, other countries adopted the EFTPOS technology, but these systems too were limited to the national borders. Each country adopted various interbank co-operative models. In Australia, in 1984 Westpac was the first major Australian bank to implement an EFTPOS system, at BP petrol stations. The other major banks implemented EFTPOS systems during 1984, initially with petrol stations. The banks' existing debit and credit cards (but only allowed to access debit accounts) were used in the EFTPOS systems. In 1985, the State Bank of Victoria developed the capacity to host connect individual ATMS and helped create the ATM (Financial) Network. Banks started to link their EFTPOS systems to provide access for all customers across all EFTPOS devices. Cards issued by all banks could then be used at all EFTPOS terminals nationally, but debit cards issued in other countries could not. Prior to 1986, the Australian banks organized a widespread uniform credit card, called Bankcard, which had been in existence since 1974. There was a dispute between the banks whether Bankcard (or credit cards in general) should be permitted into the proposed EFTPOS system. At that time several banks were actively promoting MasterCard and Visa credit cards. Store cards and proprietary cards were shut out of the new system.\n\nIn New Zealand, Bank of New Zealand started issuing EFTPOS debit cards in 1985 with the first merchant terminals being installed in petrol stations.\n\nIn 1996, mobile EFTPOS arrived, with hotels in Kuala Lumpur installing systems in 1997 and the first example of a pizza delivery in Singapore accepting Visa card via cellular payment in 1998 (a collaboration between Signet, Visa, Citi Bank, and Dynamic Data Systems) beginning the rollout of mobile systems in Asia. By 2004 Cellular based Eftpos infrastructure had really taken off, and by 2010 Cellular Eftpos had become the standard for the Global market.\n\nSince 2002 the use of EFTPOS has grown significantly, and it has become the standard payment method, displacing the use of cash. Subsequently, networks facilitating the process of money transfer and payment settlement between the consumer and the merchant grew from a small number of nationwide systems to the majority of payment processing transactions. For EFTPOS, USA based systems allow the use of debit cards or credit cards.\n\nIn Australia, debit and credit cards are the most common non-cash payment methods at “points of sale” (POS) or via ATMs. Not all merchants provide EFTPOS facilities, but those who wish to accept EFTPOS payments must enter an agreement with one of the seven merchant service providers, which rent an EFTPOS terminal to the merchant. The EFTPOS system in Australia is managed by Eftpos Payments Australia Ltd, which also sets the EFTPOS interchange fee. For credit cards to be accepted by a merchant a separate agreement must be entered into with each credit card company, each of which has its own flexible merchant fee rate.\n\nThe clearing arrangements for EFTPOS are managed by Australian Payments Clearing Association (APCA). The system for ATM and EFTPOS interchanges is called Issuers and Acquirers Community (formerly Consumer Electronic Clearing System; CECS) also called CS3. CECS required authorisations from the Australian Competition and Consumer Commission (ACCC), which was obtained in 2001 and reaffirmed in 2009. ATM and EFTPOS clearances are the made under individual bilateral arrangements between the institutions involved.\n\nAustralian financial institutions provide their customers with a plastic card, which can be used as a debit card or as an ATM card, and sometimes as a credit card. The card merely provides the means by which a customer's linked bank or other accounts can be accessed using an EFTPOS terminal or ATM. These cards can also be used on some vending machines and other automatic payment mechanisms, such as ticket vending machines. Australian debit cards cannot be used for online and telephone banking transactions, unless they are also a credit card.\n\nEach Australian bank has given a different name to its debit cards, such as:\n\nSome banks offer alternative debit card facilities to their customers using the Visa or MasterCard clearance system. For example, St George Bank offers a Visa Debit Card, as does the National Australia Bank. The main difference with regular debit cards is that these cards can be used outside Australia where the respective credit card is accepted.\n\nThose merchants that enter the EFTPOS payment system must accept debit cards issued by any Australian bank, and some also accept various credit cards and other cards. Some merchants set minimum transaction amounts for EFTPOS transactions, which can be different for debit and credit card transactions. Some merchants impose a surcharge on the use of EFTPOS. These can vary between merchants and on the type of card being used, and generally are not imposed on debit card transactions, and widely not on MasterCard and a Visa credit card transactions.\n\nA feature of a debit card is that an EFTPOS transaction will only be accepted if there is an available credit balance in the bank cheque or savings account linked to the card.\n\nAustralian debit cards normally cannot be used outside Australia. They can only be used outside Australia if they carry the MasterCard/Maestro/Cirrus or Visa/Plus or other similar logos, in which case the non-Australian transaction will be processed through those transaction systems. Similarly, non-Australian debit and credit cards can only be used at Australian EFTPOS terminals or ATMs if they have these logos or the MasterCard or Visa logos. Diners Club and/or American Express cards will be accepted only if the merchant has an agreement with those card companies, or increasingly, if the merchant has modern alternative payment options available for those cards, such as through Paypal. The Discover Card is accepted in Australia as a Diners Club card .\n\nIn addition, credit card companies issue prepaid cards which act like generic gift cards, which are anonymous and not linked to any bank accounts. These cards are accepted by merchants who accept credit cards and are processed through the EFTPOS terminal in the same way as credit cards.\n\nA number of merchants permit customers using a debit card to withdraw cash as part of the EFTPOS transaction. In Australia, this facility (known as debit card cashback in many other countries) is known as \"cash out\". For the merchant, cash out is a way of reducing their net cash takings, saving on banking of cash. There is no additional cost to the merchant in providing cash out because banks charge a merchant a debit card transaction fee per EFTPOS transaction, and not on the transaction value. Cash out is a facility provided by the merchant, and not the bank, so the merchant can limit or vary how much cash can be withdrawn at a time, or suspend the facility at any time. When available, cash out is convenient for the customer, who can bypass having to visit a bank branch or ATM. Cash out is also cheaper for the customer, since only one bank transaction is involved. For people in some remote areas, cash out may be the only way they can withdraw cash from their personal accounts. However, most merchants who provide the facility set a relatively low limit on cash out, generally $50, and some also charge for the service. Some merchants in Australia only allow cash out with the purchase of goods; other merchants allow cash out whether or not customers buy any goods. Cash out is not available in association with credit card sales because on credit card transactions the merchant is charged a percentage commission based on the transaction value, and also because cash withdrawals are treated differently from purchase transactions by the credit card company. (However, though inconsistent with a merchant's agreement with each credit card company, the merchant may treat a cash withdrawal as part of an ordinary credit card sale.)\n\nEFTPOS transactions involving a debit, credit or prepaid card are primarily authenticated via the entry of a personal identification number (PIN) at the point of sale. Historically, these transactions were authenticated by the merchant using the cardholder's signature, as signed on their receipt. However, merchants had become increasingly lax in enforcing this verification, resulting in an increase in fraud. Australian banks have since deployed chip and PIN technology using the global EMV card standard; as of 1 August 2014, Australian merchants no longer accept signatures on transactions by domestic customers at point of sale terminals.\n\nAs a further security measure, if a user enters an incorrect PIN three times, the card may be locked out of EFTPOS and require reactivation over the phone or at a bank branch. In the case of an ATM, the card will not be returned, and the cardholder will need to visit the branch to retrieve the card, or request a new card to be issued.\n\nAll debit cards now have a magnetic stripe on which is encoded the card's service codes, consisting of three-digit values. These codes are used to convey instructions to merchant terminals on how a card should be processed. The first digit indicates if a card can be used internationally or is valid for domestic use only. It is also used to signal if the card is chip-enabled. The second digit indicates if the transaction must be sent online for authorization always or if transactions that are below floor limit can take place without authorization. The third digit is used to indicate the preferred card verification method (e.g. PIN) and the environment where the card can be used (e.g. at point of sale only). Merchant terminals are required to recognize and act on service codes or send all transactions for online authorization.\n\nIn the late 2000s, MasterCard and Visa introduced contactless smart debit cards under the brand names MasterCard PayPass and Visa payWave. These payments are made using either electronic payment networks separate from the regular EFTPOS payment networks, or newer EFTPOS with tap sensors, and is an alternative to the previous swipe or chip systems. These networks are operated by MasterCard and Visa, and not by the banks as is the EFTPOS network, through EFTPOS Payments Australia Limited (ePAL).\n\nThese cards are based on EMV technology and contain a RFID chip and antenna loop embedded in the plastic of the card. To pay using this system, a customer passes the card within 4 cm of a reader at a merchant checkout. Using this method, for transactions under $100, the customer does not need to authenticate his or her identity by PIN entry or signature, as on a regular EFTPOS machine. For transactions over $100, PIN verification is required.\n\nThe facility is only available for cards branded with the MasterCard PayPass or Visa payWave logos, indicating that they have the system-permitted embedded chip. ANZ has launched ATM solution based on Visa payWave in 2015, where the consumer tap the card on a reader at the ATM and insert a PIN to finalize the cash withdrawals, and not all merchants offer the facility. Bank debit cards and other credit cards do not currently offer a contactless payment facility. ePAL is developing a contactless payment system for debit cards based on EMV technology as well as an extension of debit cards for use for on-line transactions, and a mobile payment system. Using contactless debit cards on tap-and-go terminals routes the transaction through the more expensive credit card system instead of the EFTPOS route, adding to the cost to the merchant, and ultimately the consumer.\n\nThe name and logo for EFTPOS in Australia were originally owned by the National Australia Bank and were trade marks from 1986 until 1991. The ownership was for convenience and all the banks used the name and logo (commonly called \"fat-E\") on their cards and advertising.\n\nIn 1991 dialup EFTPOS was conceived by Key Corp (John Wood) and deployment of dialup commenced in 1993. Until 1993, communications, connections and transactions between banks, ATM banks and EFTPOS devices where conducted via leased lines (a specific power assisted communication line that detects any attempt to tamper with it) but in 1993 mobile wireless EFTPOS was conceived by Dynamic Data Systems (H. Daniel Elbaum). In 1995 Dynamic Data Systems and the banking industry worked together to implement, certify and introduce protocols and standards for cellular networks, and by 1998 the use of mobile EFTPOS began to appear in Australia.\n\nIn 2006 Commonwealth Bank and MasterCard ran a six-month trial of the contactless smart card system PayPass in Sydney and Wollongong, \nsupplementing the traditional EFTPOS swipe or chip system. The system was rolled out across Australia in 2009; other systems being rolled out are Westpac Bank's MasterCard PayPass and Visa payWave branded cards.\n\nIn April 2009, a company, “EFTPOS Payments Australia Ltd” (ePal) was formed to manage and promote the EFTPOS system in Australia. ePal regulation commenced in January 2011. The initial members of EFTPOS Payments Australia Ltd were:\n\n\nIn Australia, store cards have been excluded from participation in the EFTPOS and ATM systems. Consequently, several larger store accounts have entered into co-branding arrangements with credit card networks for the store-based accounts to be widely accepted. This was the case with Coles (previously, Coles-Myer) which co-branded with MasterCard, and David Jones which co-branded with American Express. Woolworths organized its credit card called Everyday Rewards (now Woolworths Money) which initially was partnered with credit provider HSBC Bank, but changed on 26 October 2014 to Macquarie Bank.\n\nAs of June 2018, there were 961,247 EFTPOS terminals in Australia and 30,940 ATMs. Of the terminals, over 60,000 offered cash withdrawals. In 2010, 183 million transactions, worth A$12 billion, were made using Australian EFTPOS terminals per month.\n\nIn 2011, these figures increased to 750,000 terminals, with 325,000 individual businesses, processing over 2 billion transactions with combined value of approximately $131 billion for the year.\n\nThe EFT network in Australia is made up of seven proprietary networks in which peers have interchange agreements, making an effective single network. A merchant who wishes to accept EFTPOS payments must enter an agreement with one of the seven merchant service providers, which rent the terminal to the merchant. All the merchant's EFTPOS transactions are processed through one of these gateways. Some of these peers are:\n\nOther organisations may have peering agreements with the one or more of the central peers.\n\nThe network uses the AS 2805 protocol, which is closely related to ISO 8583.\n\nEFTPOS is highly popular in New Zealand. The system is operated by two providers, Paymark Limited (formerly Electronic Transaction Services Limited) which processes 75% of all electronic transactions in New Zealand, and EFTPOS New Zealand. Although the term eftpos is popularly used to describe the system, EFTPOS is a trademark of EFTPOS New Zealand the smaller of the two providers. Both providers run an interconnected financial network that allows the processing of not only of debit cards at point of sale terminals but also credit cards and charge cards.\n\nThe Bank of New Zealand introduced EFTPOS to New Zealand in 1985 through a pilot scheme with petrol stations.\n\nIn 1989 the system was officially launched and two providers owned by the major banks now run the system. The larger of the two providers, Paymark Limited (formerly Electronic Transaction Services Limited) is owned equally by ASB Bank, Westpac, Bank of New Zealand and ANZ Bank New Zealand (formerly ANZ National Bank). The second is operated by EFTPOS New Zealand which is fully owned by VeriFone Systems, following its sale by ANZ New Zealand in December 2012.\n\n1995 was the first deployment of cellular Eftpos in NZ, by Dynamic Data Systems.\n\nDuring July 2006 the five billionth EFTPOS payment was processed, and at the start of 2012 the 10 billionth transaction was processed.\n\nEFTPOS is highly popular in New Zealand, and being used for about 60% of all retail transactions. In 2009, there were 200 EFTPOS transactions per person.\n\nPaymark process over 900 million transactions (worth over NZ$48 billion) yearly. More than 75,000 merchants and over 110,000 EFTPOS terminals are connected to Paymark.\n\n\n"}
{"id": "3095332", "url": "https://en.wikipedia.org/wiki?curid=3095332", "title": "Explicit parallelism", "text": "Explicit parallelism\n\nIn computer programming, explicit parallelism is the representation\nof concurrent computations by means of primitives\nin the form of special-purpose directives or function calls. Most parallel primitives are related to process synchronization, communication or task partitioning. As they seldom contribute to actually carry out the \nintended computation of the program, their computational cost is often considered\nas parallelization overhead.\n\nThe advantage of explicit parallel programming is the absolute programmer\ncontrol over the parallel execution. A skilled\nparallel programmer takes advantage of explicit parallelism to produce\nvery efficient code. However, programming with explicit parallelism is often difficult, especially for \nnon computing specialists, because of the extra work involved in planning\nthe task division and synchronization of concurrent processes. \n\nIn some instances, explicit parallelism may be avoided with the use of an optimizing compiler that automatically extracts the parallelism inherent to computations (see implicit parallelism).\n\n"}
{"id": "864950", "url": "https://en.wikipedia.org/wiki?curid=864950", "title": "Figure of Insensitivity", "text": "Figure of Insensitivity\n\nFigure of Insensitiveness (or \"F of I\") is an inverse scale of measure of the impact sensitivity of an explosive substance. In this particular context the term 'Insensitiveness' refers to the likelihood of initiation/detonation by accidental means e.g. impact, friction, electrostatic discharge, application of flame, etc. It is a quantitative measure of the level of stimulus required to cause initiation, typically by shock/impact. \n\nThe Figure of Insensitiveness is determined from impact testing, typically using a drop-weight tower. In this test, a small sample of the explosive is placed on a small steel anvil which is slotted into a recess in the base of the drop tower. A cylindrical, 1 kilogram steel weight (mounted inside a tube to accurately guide its descent to the impact point in the centre of the anvil) is then dropped onto the test specimen from a measured height. The specimen is monitored both during and after this process to determine whether initiation occurs. This test is repeated many times, varying the drop height according to a prescribed method. Various heights are used, starting with a small distance (e.g. 10 cm) and then progressively increasing it to as high as 3 metres. The series of drop heights and whether initiation occurred are analysed statistically to determine the drop height which has a 50% likelihood of initiating the explosives. The intention of these tests is to develop safety policies/rules which will govern the design, manufacturing, handling and storage of the explosive and any munitions containing it.\n\nA reference standard sample of RDX is currently used to calibrate the drop tower, so that the drop height to produce 50% likelihood of initiation in this material is measured and recorded. The drop height required to initiate other explosives can then be related to the RDX standard, so that a ready comparison of impact sensitiveness between different explosives can be made. By convention, explosives having a 50% initiation drop height equal to that of RDX are given a F of I of 80.\n\nThe scale was originally defined using TNT as the reference standard, with TNT having, by definition, an F of I of exactly 100. On this original scale, RDX yielded an F of I of around 80. Following World War II, when more complex explosive compositions replaced pure TNT as the most common energetic component of weapon systems, RDX was adopted as the reference standard.\n\nSensitiveness should not be confused with \"Sensitivity\" which is a measure of how easy it is to detonate an explosive, i.e. how sensitive it is to the mode of initiation to achieve a detonation. In this context an explosive which has greater sensitivity, (like specificity) requires a more specific stimulus in order to detonate it.\n\n\n"}
{"id": "11966", "url": "https://en.wikipedia.org/wiki?curid=11966", "title": "Firearm", "text": "Firearm\n\nA firearm is a portable gun (a barreled ranged weapon) that inflicts damage on targets by launching one or more projectiles driven by rapidly expanding high-pressure gas produced \"chemically\" by exothermic combustion (deflagration) of propellant within an ammunition cartridge. If gas pressurization is achieved through \"mechanical\" gas compression rather than through chemical propellant combustion, then the gun is technically an air gun, not a firearm.\n\nThe first primitive firearms originated in 10th-century China when bamboo tubes containing gunpowder and pellet projectiles were mounted on spears into the one-person-portable fire lance, which was later used as a shock weapon to good effect in the Siege of De'an in 1132. In the 13th century the Chinese invented the metal-barrelled hand cannon, widely considered the true ancestor of all firearms. The technology gradually spread through the rest of East Asia, South Asia, the Middle East, and Europe. Older firearms typically used black powder as a propellant, but modern firearms use smokeless powder or other propellants. Most modern firearms (with the notable exception of smoothbore shotguns) have rifled barrels to impart spin to the projectile for improved flight stability.\n\nModern firearms can be described by their caliber (i.e. their bore diameter; this is given in millimeters or inches e.g. 7.5 mm, .357 in.) or in the case of shotguns their gauge (e.g. 12 ga.); by the type of action employed (muzzle, breech, lever, bolt, pump, revolver, semi-automatic, automatic etc.) together with the usual means of deportment (hand-held or mechanical mounting). Further classification may make reference to the type of barrel used (rifled) and to the barrel length (24 inch), to the firing mechanism (e.g. matchlock, wheellock, flintlock, percussion lock), to the design's primary intended use (e.g. hunting rifle), or to the commonly accepted name for a particular variation (e.g. Gatling gun). \n\nShooters aim firearms at their targets with hand-eye coordination, using either iron sights or optical sights. The accurate range of pistols generally does not exceed , while most rifles are accurate to using iron sights, or to longer ranges using optical sights (firearm rounds may be dangerous or lethal well beyond their accurate range; the minimum distance for safety is much greater than the specified range). Purpose-built sniper rifles and anti-materiel rifles are accurate to ranges of more than .\n\nFirearms include a variety of ranged weapons and there is no agreed upon definition. Many soldiers consider a firearm to be any ranged weapon that uses gunpowder or a derivative as a propellant. \n\nSmall arms include handguns (revolvers and pistols) and long guns, such as rifles, carbines, shotguns, submachine guns, assault rifles, personal defense weapons, and light machine guns. \n\nThe world's top small arms manufacturing companies are Browning, Remington, Colt, Ruger, Smith & Wesson, Savage, Mossberg (USA), Heckler & Koch, SIG Sauer, Walther (Germany), Glock, Steyr-Mannlicher (Austria), FN Herstal (Belgium), Beretta (Italy), Norinco (China), Tula Arms and Kalashnikov (Russia), while former top producers were Mauser & Springfield Armory.\n\nIn 2018, Small Arms Survey reported that there are over one billion small arms distributed globally, of which 857 million (about 85 percent) are in civilian hands. U.S. civilians alone account for 393 million (about 46 percent) of the worldwide total of civilian held firearms. This amounts to \"120.5 firearms for every 100 residents.\" The world's armed forces control about 133 million (about 13 percent) of the global total of small arms, of which over 43 percent belong to two countries: the Russian Federation (30.3 million) and China (27.5 million). Law enforcement agencies control about 23 million (about 2 percent) of the global total of small arms.\n\nThe smallest of all firearms is the handgun. There are two common types of handguns: revolvers and semi-automatic pistols. Revolvers have a number of firing chambers or \"charge holes\" in a revolving cylinder; each chamber in the cylinder is loaded with a single cartridge or charge. Semi-automatic pistols have a single fixed firing chamber machined into the rear of the barrel, and a magazine so they can be used to fire more than one round. Each press of the trigger fires a cartridge, using the energy of the cartridge to activate the mechanism so that the next cartridge may be fired immediately. This is opposed to \"double-action\" revolvers which accomplish the same end using a mechanical action linked to the trigger pull.\n\nPrior to the 19th century, virtually all handguns were single-shot muzzleloaders. With the invention of the revolver in 1818, handguns capable of holding multiple rounds became popular. Certain designs of auto-loading pistol appeared beginning in the 1870s and had largely supplanted revolvers in military applications by the end of World War I. By the end of the 20th century, most handguns carried regularly by military, police and civilians were semi-automatic, although revolvers were still widely used. Generally speaking, military and police forces use semi-automatic pistols due to their high magazine capacities and ability to rapidly reload by simply removing the empty magazine and inserting a loaded one. Revolvers are very common among handgun hunters because revolver cartridges are usually more powerful than similar caliber semi-automatic pistol cartridges (which are designed for self-defense) and the strength, simplicity and durability of the revolver design is well-suited to outdoor use. Revolvers, especially in .22 LR and 38 Special/357 Magnum, are also common concealed weapons in jurisdictions allowing this practice because their simple mechanics make them smaller than many autoloaders while remaining reliable. Both designs are common among civilian gun owners, depending on the owner's intention (self-defense, hunting, target shooting, competitions, collecting, etc.).\n\nA long gun is generally any firearm that is larger than a handgun and is designed to be held and fired with both hands, either from the hip or the shoulder. Long guns typically have a barrel between 10 and 30 inches (there are restrictions on minimum barrel length in many jurisdictions; maximum barrel length is usually a matter of practicality), that along with the receiver and trigger group is mounted into a wood, plastic, metal or composite \"stock\", composed of one or more pieces that form a foregrip, rear grip, and optionally (but typically) a shoulder mount called the \"butt\". Early long arms, from the Renaissance up to the mid-19th century, were generally smoothbore firearms that fired one or more ball shot, called muskets or arquebus depending on caliber and firing mechanism.\n\nMost modern long guns are either rifles or shotguns. Both are the successors of the musket, diverging from their parent weapon in distinct ways. A rifle is so named for the spiral fluting (rifling) machined into the inner surface of its barrel, which imparts a self-stabilizing spin to the single bullets it fires. Shotguns are predominantly smoothbore firearms designed to fire a number of \"shot\"; pellet sizes commonly ranging between 2 mm #9 birdshot and 8.4 mm #00 (double-aught) buckshot. Shotguns are also capable of firing single slugs, or specialty (often \"less lethal\") rounds such as bean bags, tear gas or breaching rounds. Rifles have a very small impact area but a long range and high accuracy. Shotguns have a large impact area with considerably less range and accuracy. However, the larger impact area can compensate for reduced accuracy, since shot spreads during flight; consequently, in hunting, shotguns are generally used for flying game.\n\nRifles and shotguns are commonly used for hunting and often to defend a home or place of business. Usually, large game are hunted with rifles (although shotguns can be used, particularly with slugs), while birds are hunted with shotguns. Shotguns are sometimes preferred for defending a home or business due to their wide impact area, multiple wound tracks (when using buckshot), shorter range, and reduced penetration of walls (when using lighter shot), which significantly reduces the likelihood of unintended harm, although the handgun is also common.\n\nThere are a variety of types of rifles and shotguns based on the method they are reloaded. Bolt-action and lever-action rifles are manually operated. Manipulation of the bolt or the lever causes the spent cartridge to be removed, the firing mechanism recocked, and a fresh cartridge inserted. These two types of action are almost exclusively used by rifles. Slide-action (commonly called 'pump-action') rifles and shotguns are manually cycled by shuttling the foregrip of the firearm back and forth. This type of action is typically used by shotguns, but several major manufacturers make rifles that use this action.\n\nBoth rifles and shotguns also come in break-action varieties that do not have any kind of reloading mechanism at all but must be hand-loaded after each shot. Both rifles and shotguns come in single- and double-barreled varieties; however due to the expense and difficulty of manufacturing, double-barreled rifles are rare. Double-barreled rifles are typically intended for African big-game hunts where the animals are dangerous, ranges are short, and speed is of the essence. Very large and powerful calibers are normal for these firearms.\n\nRifles have been in nationally featured marksmanship events in Europe and the United States since at least the 18th century, when rifles were first becoming widely available. One of the earliest purely \"American\" rifle-shooting competitions took place in 1775, when Daniel Morgan was recruiting sharpshooters in Virginia for the impending American Revolutionary War. In some countries, rifle marksmanship is still a matter of national pride. Some specialized rifles in the larger calibers are claimed to have an accurate range of up to about , although most have considerably less. In the second half of the 20th century, competitive shotgun sports became perhaps even more popular than riflery, largely due to the motion and immediate feedback in activities such as skeet, trap and sporting clays.\n\nIn military use, bolt-action rifles with high-power scopes are common as sniper rifles, however by the Korean War the traditional bolt-action and semi-automatic rifles used by infantrymen had been supplemented by select-fire designs known as \"automatic rifles\".\n\nA carbine is a firearm similar to a rifle in form and intended usage, but generally shorter or smaller than the typical \"full-size\" hunting or battle rifle of a similar time period, and sometimes using a smaller or less-powerful cartridge. Carbines were and are typically used by members of the military in roles that are expected to engage in combat, but where a full-size rifle would be an impediment to the primary duties of that soldier (vehicle drivers, field commanders and support staff, airborne troops, engineers, etc.). Carbines are also common in law enforcement and among civilian owners where similar size, space and/or power concerns may exist. Carbines, like rifles, can be single-shot, repeating-action, semi-automatic or select-fire/fully automatic, generally depending on the time period and intended market. Common historical examples include the Winchester Model 1892, Lee–Enfield \"Jungle Carbine\", SKS, M1 carbine (no relation to the larger M1 Garand) and M4 carbine (a more compact variant of the current M16 rifle). Modern U.S. civilian carbines include compact customizations of the AR-15, Ruger Mini-14, Beretta Cx4 Storm, Kel-Tec SUB-2000, bolt-action rifles generally falling under the specifications of a scout rifle, and aftermarket conversion kits for popular pistols including the M1911 and Glock models.\n\nA machine gun is a fully automatic emplaceable weapon, most often separated from other classes of automatic weapon by the use of belt-fed ammunition (though some designs employ drum, pan or hopper magazines), generally in a rifle-inspired caliber ranging between 5.56×45mm NATO (.223 Remington) for a light machine gun to as large as .50 BMG or even larger for crewed or aircraft weapons. Although not widely fielded until World War I, early machine guns were being used by militaries in the second half of the 19th century. Notables in the U.S. arsenal during the 20th century included the M2 Browning .50 caliber heavy machine gun and M1919 Browning .30 caliber medium machine gun, and the M60 7.62×51mm NATO general-purpose machine gun which came into use around the Vietnam War. Machine guns of this type were originally defensive firearms crewed by at least two men, mainly because of the difficulties involved in moving and placing them, their ammunition, and their tripod. In contrast, modern light machine guns such as the FN Minimi are often wielded by a single infantryman. They provide a large ammunition capacity and a high rate of fire, and are typically used to give suppressing fire during infantry movement. Accuracy on machine guns varies based on a wide number of factors from design to manufacturing tolerances, most of which have been improved over time. Machine guns are often mounted on vehicles or helicopters, and have been used since World War I as offensive firearms in fighter aircraft and tanks (e.g. for air combat or suppressing fire for ground troop support).\n\nThe definition of machine gun is different in U.S. law. The National Firearms Act and Firearm Owners Protection Act define a \"machine gun\" in the United States code \"Title 26, Subtitle E, Chapter 53, Subchapter B, Part 1, § 5845\" as:\n\"... any firearm which shoots ... automatically more than one shot, without manual reloading, by a single function of the trigger\". \"Machine gun\" is therefore largely synonymous with \"automatic weapon\" in the U.S. civilian parlance, covering all automatic firearms.\n\nA submachine gun is a magazine-fed firearm, usually smaller than other automatic firearms, that fires pistol-caliber ammunition; for this reason certain submachine guns can also be referred to as \"machine pistols\", especially when referring to handgun-sized designs such as the Škorpion vz. 61 and Glock 18. Well-known examples are the Israeli Uzi and Heckler & Koch MP5 which use the 9×19mm Parabellum cartridge, and the American Thompson submachine gun which fires .45 ACP. Because of their small size and limited projectile penetration compared to high-power rifle rounds, submachine guns are commonly favored by military, paramilitary and police forces for close-quarters engagements such as inside buildings, in urban areas or in trench complexes.\n\nSubmachine guns were originally about the size of carbines. Because they fire pistol ammunition, they have limited long-range use, but in close combat can be used in fully automatic in a controllable manner due to the lighter recoil of the pistol ammunition. They are also extremely inexpensive and simple to build in time of war, enabling a nation to quickly arm its military. In the latter half of the 20th century, submachine guns were being miniaturized to the point of being only slightly larger than some large handguns. The most widely used submachine gun at the end of the 20th century was the Heckler & Koch MP5. The MP5 is actually designated as a \"machine pistol\" by Heckler & Koch (MP5 stands for \"Maschinenpistole 5\", or Machine Pistol 5), although some reserve this designation for even smaller submachine guns such as the MAC-10 and Glock 18, which are about the size and shape of pistols.\n\nAn automatic rifle is a magazine-fed firearm, wielded by a single infantryman, that is chambered for rifle cartridges and capable of automatic fire. The M1918 Browning Automatic Rifle was the first U.S. infantry weapon of this type, and was generally used for suppressive or support fire in the role now usually filled by the light machine gun. Other early automatic rifles include the Fedorov Avtomat and the Huot Automatic Rifle. Later, German forces fielded the Sturmgewehr 44 during World War II, a light automatic rifle firing a reduced power \"intermediate cartridge\". This design was to become the basis for the \"assault rifle\" subclass of automatic weapons, as contrasted with \"battle rifles\", which generally fire a traditional \"full-power\" rifle cartridge.\n\nIn World War II, Germany introduced the StG 44, and brought to the forefront of firearm technology what eventually became the class of firearm most widely adopted by the military, the assault rifle. An assault rifle is usually slightly smaller than a battle rifle such as the Karabiner 98k, but the chief differences defining an assault rifle are select-fire capability and the use of a rifle round of lesser power, known as an intermediate cartridge. \n\nSoviet engineer Mikhail Kalashnikov quickly adapted the German concept, using a less-powerful 7.62×39mm cartridge derived from the standard 7.62×54mmR Russian battle rifle round, to produce the AK-47, which has become the world's most widely used assault rifle. Soon after World War II, the Automatic Kalashnikov AK-47 assault rifle began to be fielded by the Soviet Union and its allies in the Eastern Bloc, as well as by nations such as China, North Korea, and North Vietnam.\n\nIn the United States, the assault rifle design was later in coming; the replacement for the M1 Garand of WWII was another John Garand design chambered for the new 7.62×51mm NATO cartridge; the select-fire M14, which was used by the U.S. military until the 1960s. The significant recoil of the M14 when fired in full-automatic mode was seen as a problem as it reduced accuracy, and in the 1960s it was replaced by Eugene Stoner's AR-15, which also marked a switch from the powerful .30 caliber cartridges used by the U.S. military up until early in the Vietnam War to the much less powerful but far lighter and light recoiling .223 caliber (5.56mm) intermediate cartridge. The military later designated the AR-15 as the \"M16\". The civilian version of the M16 continues to be known as the AR-15 and looks exactly like the military version, although to conform to B.A.T.F.E. regulations in the U.S., it lacks the mechanism that permits fully automatic fire.\n\nVariants of both of the M16 and AK-47 are still in wide international use today, though other automatic rifle designs have since been introduced. A smaller version of the M16A2, the M4 carbine, is widely used by U.S. and NATO tank and vehicle crews, airbornes, support staff, and in other scenarios where space is limited. The IMI Galil, an Israeli-designed weapon based on the action of the AK-47, is in use by Israel, Italy, Burma, the Philippines, Peru, and Colombia. Swiss Arms of Switzerland produces the SIG SG 550 assault rifle used by France, Chile, and Spain among others, and Steyr Mannlicher produces the AUG, a bullpup rifle in use in Austria, Australia, New Zealand, Ireland, and Saudi Arabia among other nations.\n\nModern designs call for compact weapons retaining firepower. The bullpup design, by mounting the magazine behind the trigger, unifies the accuracy and firepower of the traditional assault rifle with the compact size of the submachine gun (though submachine guns are still used); examples are the French FAMAS and the British SA80.\n\nA recently developed class of firearm is the personal defense weapon or PDW, which is in simplest terms a submachine gun designed to fire ammunitions with ballistic performance similar to rifle cartridges. While a submachine gun is desirable for its compact size and ammunition capacity, its pistol cartridges lack the penetrating capability of a rifle round. Conversely, rifle bullets can pierce light armor and are easier to shoot accurately, but even a carbine such as the Colt M4 is larger and/or longer than a submachine gun, making it harder to maneuver in close quarters. The solution many firearms manufacturers have presented is a weapon resembling a submachine gun in size and general configuration, but which fires a higher-powered armor-penetrating round (often specially designed for the weapon), thus combining the advantages of a carbine and submachine gun. This also earned the PDWs an infrequently used nickname — the submachine carbines. The FN P90 and Heckler & Koch MP7 are most famous examples of PDWs.\n\nFirearms are also categorized by their functioning cycle or \"action\" which describes its loading, firing, and unloading cycle.\n\nThe earliest evolution of the firearm, there are many types of manual action firearms. These can be divided into two basic categories: single shot and repeating.\n\nA single shot firearm can only be fired once per equipped barrel before it must be reloaded or charged via an external mechanism or series of steps. A repeating firearm can be fired multiple times, but can only be fired once with each subsequent pull of the trigger. Between trigger pulls, the firearm's action must be reloaded or charged via an internal mechanism.\n\nA semi-automatic, self-loading, or \"auto loader\" firearm is one that performs all steps necessary to prepare it for firing again after a single discharge, until cartridges are no longer available in the weapon's feed device or magazine. Auto loaders fire one shot with each pull of the trigger. Some people confuse the term with \"fully automatic\" firearms. (See next.) While some semi-automatic rifles may resemble military-style firearms, they are not properly classified \"Assault Weapons\" which refers to those that continue to fire until the finger no longer depressed the trigger.\n\nAn \"automatic\" firearm, or \"fully automatic\", \"fully auto\", or \"full auto\", is generally defined as one that continues to load and fire cartridges from its magazine as long as the trigger is depressed (and until the magazine is depleted of available ammunition. The first weapon generally considered in this category is the Gatling gun, originally a carriage-mounted, crank-operated firearm with multiple rotating barrels that was fielded in the American Civil War. The modern trigger-actuated machine gun began with various designs developed in the late 19th century and fielded in World War I, such as the Maxim gun, Lewis Gun, and MG 08 \"Spandau\". Most automatic weapons are classed as long guns (as the ammunition used is of similar type as for rifles, and the recoil of the weapon's rapid fire is better controlled with two hands), but handgun-sized automatic weapons also exist, generally in the \"submachine gun\" or \"machine pistol\" class.\n\nSelective fire, or \"select fire\", means the capability of a weapon's fire control to be adjusted in either semi-automatic or fully automatic firing modes. The modes are chosen by means of a selector, which varies depending on the weapon's design. Some selective-fire weapons have burst fire mechanisms built in to limit the maximum number of shots fired in fully automatic mode, with most common limits being two or three rounds per trigger pull. The presence of selective-fire modes on firearms allows more efficient use of ammunition for specific tactical needs, either precision-aimed or suppressive fire. This capability is most commonly found on military weapons of the 20th and 21st centuries, most notably the assault rifles.\n\nAssault by firearm resulted in 173,000 deaths, globally, in 2015, up from 128,000 deaths in 1990. Additionally, there were 32,000 unintentional firearm global deaths in 2015. In 2017, there were 15,613 deaths caused by firearms in the US. Additionally, there were 61,599 firearms related incidents in the US in 2017. Over 50% of suicides in the United States are firearm suicides. More than 60% of firearms deaths in the United States are suicide. Firearms are the second leading mechanism of injury deaths, after motor vehicle accidents. To prevent unintentional injury, gun safety training includes education on proper firearm storage and firearm-handling etiquette.\n\nThe first primitive firearms were invented about 1250 A.D. in China when the man-portable fire lance (a bamboo or metal tube that could shoot ignited gunpowder) was combined with projectiles such as scrap metal, broken porcelain, or darts/arrows.\n\nThe earliest depiction of a firearm is a sculpture from a cave in Sichuan, China. The sculpture dates to the 12th century and is of a figure carrying a vase-shaped bombard, with flames and a cannonball coming out of it. The oldest surviving gun, a hand cannon made of bronze, has been dated to 1288 because it was discovered at a site in modern-day Acheng District, Heilongjiang, China, where the \"Yuan Shi\" records that battles were fought at that time. The firearm had a 6.9 inch barrel of a 1-inch diameter, a 2.6 inch chamber for the gunpowder and a socket for the firearm's handle. It is 13.4 inches long and 7.8 pounds without the handle, which would have been made of wood.\n\nThe Europeans and Arabs (first Mamluks) obtained firearms in the 14th century. The Koreans adopted firearms from the Chinese in the 14th century. The Turks, Iranians, (first Aq Qoyunlu and Safavids), and Indians (first Mughals) all got them no later than the 15th century. In each case directly or indirectly from the Europeans. The Japanese did not acquire firearms until the 16th century, and then from the Portuguese rather than the Chinese.\n\nThe development behind firearms accelerated during the 19th and 20th centuries. Breech-loading became more or less a universal standard for the reloading of most hand-held firearms and continues to be so with some notable exceptions (such as mortars). Instead of loading individual rounds into weapons, magazines holding multiple munitions were adopted—these aided rapid reloading. Automatic and semi-automatic firing mechanisms meant that a single soldier could fire many more rounds in a minute than a vintage weapon could fire over the course of a battle. Polymers and alloys in firearm construction made weaponry progressively lighter and thus easier to deploy. Ammunition changed over the centuries from simple metallic ball-shaped projectiles that rattled down the barrel to bullets and cartridges manufactured to high precision. Especially in the past century has particular attention been devoted to accuracy and sighting to make firearms altogether far more accurate than ever before. More than any single factor though, firearms have proliferated due to the advent of mass production—enabling arms manufacturers to produce large quantities of weaponry to a consistent standard.\n\nThe force of a projectile is related to the kinetic energy imparted to it, given by the formula formula_1 where formula_2 is the mass and formula_3 is the velocity of the projectile.\n\nGenerally, kinetic energy can be enhanced in two ways:\n\n\nVelocities of bullets increased with the use of a \"jacket\" of a metal such as copper or copper alloys that covered a lead core and allowed the bullet to glide down the barrel more easily than exposed lead. Such bullets are designated as \"full metal jacket\" (FMJ). Such FMJ bullets are less likely to fragment on impact and are more likely to traverse through a target while imparting less energy. Hence, FMJ bullets impart less tissue damage than non-jacketed bullets that expand. (Dougherty and Eidt, 2009) This led to their adoption for military use by countries adhering to the Hague Convention in 1899.\n\nThat said, the basic principle behind firearm operation remains unchanged to this day. A musket of several centuries ago is still similar in principle to a modern-day assault rifle—using the expansion of gases to propel projectiles over long distances—albeit less accurately and rapidly.\n\nThe Chinese fire lance was the direct predecessor to the modern concept of the firearm. It was not a gun itself, but an addition to the soldiers' spears. Originally it consisted of paper or bamboo barrels that would have incendiary gunpowder within it, that could be lit one time and would project flames at the enemy. Sometimes the Chinese troops would place small projectiles within the barrel that would also be projected when the gunpowder was lit, but most of the explosive force would create flames. Later, the barrel was changed to be made of metal, so that a more explosive gunpowder could be used and put more force into the propulsion of the projectile.\n\nThe original predecessor of all firearms, the Chinese fire lance and hand cannon were loaded with gunpowder and the shot (initially lead shot, later replaced by cast iron) through the muzzle, while a fuse was placed at the rear. This fuse was lit, causing the gunpowder to ignite and propel the cannonball. In military use, the standard hand cannon was tremendously powerful, while also being somewhat useless due to relative inability of the gunner to aim the weapon, or control the ballistic properties of the projectile. Recoil could be absorbed by bracing the barrel against the ground using a wooden support, the forerunner of the stock. Neither the quality or amount of gunpowder, nor the consistency in projectile dimensions were controlled, with resulting inaccuracy in firing due to windage, variance in gunpowder composition, and the difference in diameter between the bore and the shot. The hand cannons were replaced by lighter carriage-mounted artillery pieces, and ultimately the arquebus.\n\nMuzzle-loading muskets (smooth-bored long guns) were among the first firearms developed. The firearm was loaded through the muzzle with gunpowder, optionally some wadding and then a bullet (usually a solid lead ball, but musketeers could shoot stones when they ran out of bullets). Greatly improved muzzleloaders (usually rifled instead of smooth-bored) are manufactured today and have many enthusiasts, many of whom hunt large and small game with their guns. Muzzleloaders have to be manually reloaded after each shot; a skilled archer could fire multiple arrows faster than most early muskets could be reloaded and fired, although by the mid-18th century, when muzzleloaders became the standard small armament of the military, a well-drilled soldier could fire six rounds in a minute using prepared cartridges in his musket. Before then, effectiveness of muzzleloaders was hindered by both the low reloading speed and, before the firing mechanism was perfected, the very high risk posed by the firearm to the person attempting to fire it.\n\nOne interesting solution to the reloading problem was the \"Roman Candle Gun\" with superposed loads. This was a muzzleloader in which multiple charges and balls were loaded one on top of the other, with a small hole in each ball to allow the subsequent charge to be ignited after the one ahead of it was ignited. It was neither a very reliable nor popular firearm, but it enabled a form of \"automatic\" fire long before the advent of the machine gun.\n\nMost early firearms were muzzle-loading. This form of loading has several disadvantages, such as a slow rate of fire and having to expose oneself to enemy fire to reload as the weapon had to be pointed upright so the powder could be poured through the muzzle into the breech followed by the ramming the projectile into the breech. As effective methods of sealing the breech were developed through the development of sturdy, weatherproof, self-contained metallic cartridges, muzzle-loaders were replaced by single-shot breech loaders. Eventually single-shot weapons were replaced by the following repeater type weapons.\n\nMany firearms made in the late 19th century through the 1950s used internal magazines to load the cartridge into the chamber of the weapon. The most notable and revolutionary weapons of this period appeared during the U.S. Civil War and they were the Spencer and Henry repeating rifles. Both used fixed tubular magazines, the former having the magazine in the buttstock and the latter under the barrel which allowed a larger capacity. Later weapons used fixed box magazines that could not be removed from the weapon without disassembling the weapon itself. Fixed magazines permitted the use of larger cartridges and eliminated the hazard of having the bullet of one cartridge butting next to the primer or rim of another cartridge. These magazines are loaded while they are in the weapon, often using a stripper clip. A clip is used to transfer cartridges into the magazine. Some notable weapons that use internal magazines include the Mosin–Nagant, the Mauser Kar 98k, the Springfield M1903, the M1 Garand, and the SKS. Firearms that have internal magazines are usually, but not always, rifles. Some exceptions to this include the Mauser C96 pistol, which uses an internal magazine, and the Breda 30, an Italian light machine gun.\n\nMany modern firearms use what are called detachable or box magazines as their method of chambering a cartridge. Detachable magazines can be removed from the weapon without disassembling the firearms, usually by pushing the magazine release.\n\nA belt or ammunition belt is a device used to retain and feed cartridges into a firearm commonly used on machine guns. Belts were originally composed of canvas or cloth with pockets spaced evenly to allow the belt to be mechanically fed into the gun. These designs were prone to malfunctions due to the effects of oil and other contaminants altering the belt. Later belt designs used permanently connected metal links to retain the cartridges during feeding. These belts were more tolerant to exposure to solvents and oil. Some notable weapons that use belts are the M240, the M249, the M134 Minigun, and the PK Machine Gun.\n\nMatchlocks were the first and simplest firearms firing mechanisms developed. Using the matchlock mechanism, the powder in the gun barrel was ignited by a piece of burning cord called a \"match\". The match was wedged into one end of an S-shaped piece of steel. As the trigger (often actually a lever) was pulled, the match was brought into the open end of a \"touch hole\" at the base of the gun barrel, which contained a very small quantity of gunpowder, igniting the main charge of gunpowder in the gun barrel. The match usually had to be relit after each firing. The main parts to the matchlock firing mechanism are the pan, match, arm and trigger. A benefit of the pan and arm swivel being moved to the side of the gun was it gave a clear line of fire. An advantage to the matchlock firing mechanism is that it did not misfire. However, it also came with some disadvantages. One disadvantage was if it was raining the match could not be kept lit to fire the weapon. Another issue with the match was it could give away the position of soldiers because of the glow, sound, and smell.\n\nThe wheellock action, a successor to the matchlock, predated the flintlock. Despite its many faults, the wheellock was a significant improvement over the matchlock in terms of both convenience and safety, since it eliminated the need to keep a smoldering match in proximity to loose gunpowder. It operated using a small wheel much like that on cigarette lighters which was wound up with a key before use and which, when the trigger was pulled, spun against a flint, creating the shower of sparks that ignited the powder in the touch hole. Supposedly invented by Leonardo da Vinci, the Italian Renaissance man, the wheellock action was an innovation that was not widely adopted due to the high cost of the clockwork mechanism.\n\nThe flintlock action was a major innovation in firearm design. The spark used to ignite the gunpowder in the touch hole was supplied by a sharpened piece of flint clamped in the jaws of a \"cock\" which, when released by the trigger, struck a piece of steel called the \"frizzen\" to create the necessary sparks. (The spring-loaded arm that holds a piece of flint or pyrite is referred to as a cock because of its resemblance to a rooster.) The cock had to be manually reset after each firing, and the flint had to be replaced periodically due to wear from striking the frizzen. (See also flintlock mechanism, snaphance, Miquelet lock) The flintlock was widely used during the 18th and 19th centuries in both muskets and rifles.\nPercussion caps (caplock mechanisms), coming into wide service in the 19th century, were a dramatic improvement over flintlocks. With the percussion cap mechanism, the small primer charge of gunpowder used in all preceding firearms was replaced by a completely self-contained explosive charge contained in a small brass \"cap\". The cap was fastened to the touch hole of the gun (extended to form a \"nipple\") and ignited by the impact of the gun's \"hammer\". (The hammer is roughly the same as the cock found on flintlocks except that it doesn't clamp onto anything.) In the case of percussion caps the hammer was hollow on the end to fit around the cap in order to keep the cap from fragmenting and injuring the shooter.\n\nOnce struck, the flame from the cap in turn ignited the main charge of gunpowder, as with the flintlock, but there was no longer any need to charge the touch hole with gunpowder, and even better, the touch hole was no longer exposed to the elements. As a result, the percussion cap mechanism was considerably safer, far more weatherproof, and vastly more reliable (cloth-bound cartridges containing a premeasured charge of gunpowder and a ball had been in regular military service for many years, but the exposed gunpowder in the entry to the touch hole had long been a source of misfires). All muzzleloaders manufactured since the second half of the 19th century use percussion caps except those built as replicas of the flintlock or earlier firearms.\n\nA major innovation in firearms and light artillery came in the second half of the 19th century when ammunition, previously delivered as separate bullets and powder, was combined in a single metallic (usually brass) cartridge containing a percussion cap, powder, and a bullet in one weatherproof package. The main technical advantage of the brass cartridge case was the effective and reliable sealing of high pressure gasses at the breech, as the gas pressure forces the cartridge case to expand outward, pressing it firmly against the inside of the gun barrel chamber. This prevents the leakage of hot gas which could injure the shooter. The brass cartridge also opened the way for modern repeating arms, by uniting the bullet, gunpowder and primer into one assembly that could be fed reliably into the breech by a mechanical action in the firearm.\n\nBefore this, a \"cartridge\" was simply a premeasured quantity of gunpowder together with a ball in a small cloth bag (or rolled paper cylinder), which also acted as wadding for the charge and ball. This early form of cartridge had to be rammed into the muzzleloader's barrel, and either a small charge of gunpowder in the touch hole or an external percussion cap mounted on the touch hole ignited the gunpowder in the cartridge. Cartridges with built-in percussion caps (called \"primers\") continue to this day to be the standard in firearms. In cartridge-firing firearms, a hammer (or a firing pin struck by the hammer) strikes the cartridge primer, which then ignites the gunpowder within. The primer charge is at the base of the cartridge, either within the rim (a \"rimfire\" cartridge) or in a small percussion cap embedded in the center of the base (a \"centerfire\" cartridge). As a rule, centerfire cartridges are more powerful than rimfire cartridges, operating at considerably higher pressures than rimfire cartridges. Centerfire cartridges are also safer, as a dropped rimfire cartridge has the potential to discharge if its rim strikes the ground with sufficient force to ignite the primer. This is practically impossible with most centerfire cartridges.\n\nNearly all contemporary firearms load cartridges directly into their breech. Some additionally or exclusively load from a magazine that holds multiple cartridges. A magazine is defined as a part of the firearm which exists to store ammunition and assist in its feeding by the action into the breech (such as through the rotation of a revolver's cylinder or by spring-loaded platforms in most pistol and rifle designs). Some magazines, such as that of most centerfire hunting rifles and all revolvers, are internal to and inseparable from the firearm, and are loaded by using a \"clip\". A clip, often mistakingly used to refer to a detachable \"magazine\", is a device that holds the ammunition by the rim of the case and is designed to assist the shooter in reloading the firearm's magazine. Examples include revolver speedloaders, the stripper clip used to aid loading rifles such as the Lee–Enfield or Mauser 98, and the en-bloc clip used in loading the M1 Garand. In this sense, \"magazines\" and \"clips\", though often used synonymously, refer to different types of devices.\n\nMany firearms are \"single shot\": i.e., each time a cartridge is fired, the operator must manually re-cock the firearm and load another cartridge. The classic single-barreled shotgun is a good example. A firearm that can load multiple cartridges as the firearm is re-cocked is considered a \"repeating firearm\" or simply a \"repeater\". A lever-action rifle, a pump-action shotgun, and most bolt-action rifles are good examples of repeating firearms. A firearm that automatically re-cocks and reloads the next round with each trigger pull is considered a semi-automatic or autoloading firearm.\n\nThe first \"rapid firing\" firearms were usually similar to the 19th century Gatling gun, which would fire cartridges from a magazine as fast as and as long as the operator turned a crank. Eventually, the \"rapid\" firing mechanism was perfected and miniaturized to the extent that either the recoil of the firearm or the gas pressure from firing could be used to operate it, thus the operator needed only to pull a trigger (which made the firing mechanisms truly \"automatic\"). An automatic (or \"fully automatic\") firearm is one that automatically re-cocks, reloads, and fires as long as the trigger is depressed. An automatic firearm is capable of firing multiple rounds with one pull of the trigger. The Gatling gun may have been the first automatic weapon, though the modern trigger-actuated machine gun was not widely introduced until the First World War with the German \"Spandau\" and British Lewis Gun. Automatic rifles such as the Browning Automatic Rifle were in common use by the military during the early part of the 20th century, and automatic rifles that fired handgun rounds, known as submachine guns, also appeared in this time. Many modern military firearms have a selective fire option, which is a mechanical switch that allows the firearm be fired either in the semi-automatic or fully automatic mode. In the current M16A2 and M16A4 variants of the U.S.-made M16, continuous fully automatic fire is not possible, having been replaced by an automatic burst of three cartridges (this conserves ammunition and increases controllability). Automatic weapons are largely restricted to military and paramilitary organizations, though many automatic designs are infamous for their use by civilians.\n\n"}
{"id": "14674814", "url": "https://en.wikipedia.org/wiki?curid=14674814", "title": "Fischer Connectors", "text": "Fischer Connectors\n\nFischer Connectors designs and manufactures a range of circular push–pull connectors.\n\nFounded in 1954, the headquarters and manufacturing facility is based in St-Prex, Switzerland, with cable assembly facilities in Europe, North America and Asia Pacific, and subsidiaries and distributors worldwide. Fischer Connectors’ circular push-pull connectors and cable assembly solutions – both electrical and optical – are used in numerous applications.\n\nFisher connectors comply with international quality standards, including ISO 9001, ISO 13485, ISO 14001, OHSAS 18001, REACH and ROHS.\n\nWalter Werner Fischer, a pioneering Swiss engineer, founded the company in 1954 in Morges, Switzerland. The company developed the first sealed connector and, in 1962, took out an international patent on its proprietary push-pull locking system. In 1964 it developed a hermetic connector.\n\nOverseas expansion started with the first subsidiary being established in the UK in 1988, followed by other European countries, North America in 1991 and the first Asian subsidiary in Hong Kong in 2000.\n\nIn 2017, Fischer Connectors became partner to SolarStratos. In the PriceWaterHouseCoopers Study presented at the 18th Swiss Economic Forum in Interlaken on June 1, 2017, Fischer Connectors was designated Swiss Champion 2017 for its Industry 4.0 approach.\n\n"}
{"id": "654374", "url": "https://en.wikipedia.org/wiki?curid=654374", "title": "Fishing light attractor", "text": "Fishing light attractor\n\nA fishing light attractor is a fishing aid which uses lights attached to structure above water or suspended underwater to attract both fish and members of their food chain to specific areas in order to harvest them.\n\nJust as fisherman seek conditions where the chance of catching fish is optimized, fish seek areas where the chance of catching their food is optimal. Most game fish seek waters that are rich in food such as smaller fish, insects or shrimp. And, it follows, that these smaller fish, insects and shrimp congregate where their food is most concentrated.\n\nScientific research shows that all members of this food chain have eyes sensitive to the colors blue and green. This probably evolved because the water absorbs longer wavelengths (Mobley 1994; Hou, 2013). The color of the body of the water is largely based on the constituents inside, combined with the absorption spectrum of the light in water. The colored dissolved organic matter in the water will absorb blue light very fast, then green, then yellow (exponential decay with wavelength), thus give the water a brownish look. Keep in mind that the transmission window of light in the water is very narrow and red light gets absorbed very fast. See above reference for details.\n\nFish and some members of their food chain have color receptors in their eyes optimized for the light of their “space”. Eyes that can see a single space color can detect changes in light intensity. This is equivalent to a world in black, white and shades of gray. In this simplest level of visual information processing, an animal can recognize that something is different in its space—i.e., that there is food or a predator “over there”. Most animals living in a lighted world have an additional visual resource: color vision. By definition, that requires that they have color receptors containing at least two different visual pigments. To efficiently perform this function in water illuminated with light, an aquatic animal would have visual pigments sensitive to the background “space” color and one or more visual pigments offset from this blue-green region, say, in the red or ultraviolet region of the spectrum. This imparts a clear survival advantage to these animals because they can detect not only changes in light intensity but also contrasts in color. Many fish, for example, have two color receptors, one in the blue region of the spectra (425-490 nm) and the other in the near UV (320-380 nm). Insects and shrimp, members of the fish food chain, have blue, green (530 nm) and near UV receptors. In fact, some aquatic animals have up to ten different classes of visual pigment in their eyes. By comparison, humans have three with maximum sensitivities in the blue (442 nm), green (543 nm) and yellow (570 nm). It is the differential responses of these receptor cells that enable color vision.\n\nIt has been known for a long time that a light attracts fish, shrimp and insects at night. But what is the best color for a light attractor? Based on the biology of visual receptors discussed above, the light should be blue or green — the space colors of fish and members of their food chain. However, while blue or green light is desirable it is not essential. Even if the eyes of fish or members of its food chain have color receptors most sensitive to the blue or green, these same receptors have a broad but decreased sensitivity to other colors. So, if a fishing light source is intense enough, other light colors will also attract. For example, a sodium vapor light with its characteristic yellow color will attract fish — if intense enough. A fishing light attractor can also be white light because a portion of its total energy is in the blue to green region.\n\nThe perfect fishing light would have the following properties: 1) high intensity, 2) emit its light in a color similar to the fishes space (blue or green), 3) be powered by a portable electrical supply and 4) be submersible. The last attribute is desirable because significant amounts of light energy from land- or boat-mounted lights are lost by reflection off the surface of the water. No commercial light satisfies all four of these criteria. For example, many high intensity lights such as tungsten-halogen (incandescent), medium pressure mercury or metal-halide discharge lights are so power hungry that they can only be operated for very short periods of time on a battery, thus compromising convenient portability. While LEDs and fluorescent lights draw much less electrical energy, most are not very bright. Further, many of the above lights cannot be submerged in water without risk of electrical shock or damage to the light system.\n\nFishing lights fall into two different groups: those that are portable and those that are permanently mounted. Generally, portable lights are powered by batteries and this sets practical limits to the kind of light that can be used. Most portable light sources are relatively low in light intensity and have short operating times. Lights drawing more than a few tens of watts are not practical. The old classic, a 12 volt automobile incandescent headlight mounted on a Styrofoam float ring, is probably the least expensive and lasts for a few hours before the battery is discharged. Battery-operated fluorescent lamps are three times more efficient in converting electricity to light. Therefore, comparing lamps of similar brightness, they can be operated about three times longer before the battery is discharged. Also, the operating lifetime of fluorescent lights are about ten times longer than incandescent lights. Commercial portable fishing lights based on fluorescent lamps vary widely in intensity. The best use 25-40 watt lamps that emit about 1000–3000 lumens per tube. Costing $100–$200, they are available through the internet, sport stores and catalogs.\n\nPermanent lights are typically powered with 115 volt house current. Placed on poles at the end of a dock or pier, the least expensive lights for outdoor use are security or flood lights using a mercury vapor, high pressure sodium vapor, metal-halide discharge or fluorescent bulb. They cost $25 to $100. While lower cost 115 V AC outdoor flood lights using standard tungsten (incandescent) or tungsten-halogen (quartz) bulbs can also be effective fish attractors, they are energy inefficient. It takes about five 100 watt tungsten lamps to deliver the light equivalent of one security lamp. Security lights are readily available from most hardware or farm supply stores. The fixture includes a photocell controller for automatic dusk-to-dawn operation and comes complete with an appropriate bulb. Most of these lights are bright (6-8 thousand lumens), efficient in converting electricity to light (operated daily for 8 hours, electrical supply costs $40–$100 per year), have long bulb lifetimes (24,000 hours) and stand up well to outside weather conditions. When used as a fishing light, light output can be redirected toward the water by installing a 5” X 10” piece of aluminum flashing or heavy foil bent into a half circle and placed next to the lamp's circular acrylic lens. Flood lights made up of LED lights (Light Emitting Diodes) are an up-and-comer. They are extremely efficient in converting electrical energy to light. Their brightness, arranged as large arrays of LEDs, can now provide up to 36,000 Lumens per fixture, a value quite sufficient to function as a fishing light. Currently available via the internet and some specialty stores, they sell for $120 - $350.\n\nStadium spot lights are energy efficient and their superior brightness illuminates a large area of water. Rated at 250, 400, 1000 and 1500 watts, the high intensity metal-halide discharge lamp, parabola-shaped reflector and light ballast are each sold separately. A complete light fixture and lamp costs about $400–$500. The cost of lamps with different wattage ratings are similar, so most people chose higher wattage lamps. The bulbs in these lamps can emit white, blue-green, green or yellow light. For most fishing waters the lamp color of choice is green, but it should be emphasized that all colors are effective in attracting fish. They are available through specialty light stores. It takes two people to install these big lamps and the installation may also include a switch, timer, heavy gauge wiring and a circuit breaker, thus adding to the cost.\n\nAs mentioned earlier, a significant fraction of the light shining on the surface of the water is lost by reflection and, thus, will not be available to attract fish and members of their food chain. Security lights can be modified to operate submerged in water. Positioning the bulb underwater delivers approximately twice as much light to attract fish. However, the modification must be done professionally as the high voltages that power these lamps can be lethal. Commercially made, submersible fishing lights are available on the internet. The power ballast and lamp housing is mounted on a pole in a dry location. The lamp, potted in a waterproof housing, is connected to the ballast via a waterproof cable. Floating like a fishing line bob, the lamp is positioned underwater by weights on its submerged power supply cord. The bulb is fragile so some manufacturers offer protective covers and hard lenses. However, one unique feature of a submerged, unprotected bulb is that its outer glass envelope gets hot enough during operation to prevent establishment of marine growth on its surface. If the bulb has a protective cover or is not operated every night for at least an hour, occasional manual cleaning is required.\nA permanently fixed fishing light attractor is most effective if it is operated every night. It takes to a week or two for larger fish to discover the location of increasing concentrations of bait fish attracted to the light. Once discovered, the fish return regularly — often arriving at predictable times of the evening.\n\n\"Sulfuric fire fishing\" is a traditional technique practiced in Taiwan where acetylene is burned to attract fish.\n"}
{"id": "2739199", "url": "https://en.wikipedia.org/wiki?curid=2739199", "title": "Gasworks", "text": "Gasworks\n\nA gasworks or gas house is an industrial plant for the production of flammable gas. Many of these have been made redundant in the developed world by the use of natural gas, though they are still used for storage space.\n\nCoal gas was introduced to Great Britain in the 1790s as an illuminating gas by the Scottish inventor William Murdoch.\n\nEarly gasworks were usually located beside a river or canal so that coal could be brought in by barge. Transport was later shifted to railways and many gasworks had internal railway systems with their own locomotives.\n\nEarly gasworks were built for factories in the Industrial Revolution from about 1805 as a light source and for industrial processes requiring gas, and for lighting in country houses from about 1845. Country house gas works are extant at Culzean Castle in Scotland and Owlpen in Gloucestershire.\n\nA gasworks was divided into several sections for the production, purification and storage of gas.\n\nThis contained the retorts in which coal was heated to generate the gas. The crude gas was siphoned off and passed on to the condenser. The waste product left in the retort was coke. In many cases the coke was then burned to heat the retorts or sold as smokeless fuel.\n\nThis consisted of a bank of air-cooled gas pipes over a water-filled sump. Its purpose was to remove tar from the gas by condensing it out as the gas was cooled. Occasionally the condenser pipes were contained in a water tank similar to a boiler but operated in the same manner as the air-cooled variant. The tar produced was then held in a tar well/tank which was also used to store liquor.\n\nAn impeller or pump was used to increase the gas pressure before scrubbing. Exhausters were optional components and could be placed anywhere along the purifying process but were most often placed after the condensers and immediately before the gas entered the gas holders.\n\nA sealed tank containing water through which the gas was bubbled. This removed ammonia and ammonium compounds. The water often contained dissolved lime to aid the removal of ammonia. The water left behind was known as ammonical liquor. Other versions used consisted of a tower, packed with coke, down which water was trickled.\n\nAlso known as an Iron Sponge, this removed hydrogen sulfide from the gas by passing it over wooden trays containing moist ferric oxide. The gas then passed on to the gasholder and the iron sulfide was sold to extract the sulfur. Waste from this process often gave rise to blue billy, a ferrocyanide contaminant in the land which causes problems when trying to redevelop an old gasworks site.\n\nOften only used at large gasworks sites, a benzole plant consisted of a series of vertical tanks containing petroleum oil through which the gas was bubbled. The purpose of a benzole plant was to extract benzole from the gas. The benzole dissolved into the petroleum oil was run through a steam separating plant to be sold separately.\n\nThe gas holder or gasometer was a tank used for storage of the gas and to maintain even pressure in distribution pipes. The gas holder usually consisted of an upturned steel bell contained within a large frame that guided it as it rose and fell depending on the amount of gas contained.\n\nThe by-products of gas-making, such as coke, coal tar, ammonia and sulfur had many uses. For details, see coal gas.\n\nCoal gas is no longer made in the UK but many gasworks sites are still used for storage and metering of natural gas and some of the old gasometers are still in use. Fakenham gasworks dating from 1846 is the only complete, non-operational gasworks remaining in England. Other examples exist at Biggar in Scotland and Carrickfergus in Northern Ireland.\n\nPhotos of Fakenham Gas Works \n\nGasworks were noted for their foul smell and generally located in the poorest areas of metropolitan areas. Cultural remnants of gasworks include many streets named Gas Street or Gas Avenue and groups or gangs known as Gas House Gang, such as the 1934 St. Louis Cardinals baseball team. Ewan McColl's song \"Dirty Old Town\" (about his home town of Salford) famously begins \"Found my love by the gaswork croft…\" (in cover versions often \"I met my love by the gasworks wall…\")\n\nGas was used for many years to illuminate the interior of railway carriages. The New South Wales Government Railways manufactured its own oil-gas for this purpose, together with reticulated coal-gas to railway stations and associated infrastructure. Such works were established at the Macdonaldtown Carriage Sheds, Newcastle, Bathurst, Junee and Werris Creek. These plants followed on from the works of a private supplier which the railway took over in 1884.\n\nGas was also transported in special travelling gas reservoir wagons from the gasworks to stationary reservoirs located at a number of country stations where carriage reservoirs were replenished.\n\nWith the spreading conversion to electric power for lighting buildings and carriages during the 1920s and 1930s, the railway gasworks were progressively decommissioned.\n\nThe Gasworks Newstead site in Brisbane Australia has been a stalwart of the river’s edge since its development in 1863. By 1890, the works were supplying gas to Brisbane streets from Toowong to Hamilton and over the next 100 years, it would grow to supply Brisbane city with the latest in gas technology until it was decommissioned in 1996.\n\nIn March 1866, the Queensland Defence Force placed an official request for town gas connection, evidence of the vital role the gasworks played in the economic development of colonial Brisbane. In fact, the gasworks were considered to be of such importance, that during World War II, genuine fears of attack from Japanese air raids motivated the installation of anti aircraft guns which vigilantly watched over the plant and its employees throughout the war.\n\nThe site itself has been synonymous with economic growth and benefit to Brisbane and Queensland with the success of the gasworks facilitating further development of the Newstead/Teneriffe area to include the James Hardie fibro-cement manufacturing plant, Shell Oil plant, Brisbane Water and Sewerage Depot and even the “Brisbane Gas Company Cookery School” which operated in the 1940s. In 1954, a carbonizing plant was built, giving Brisbane the \"most modern gas producing plant in Australia\", consuming 100 tonnes of coal every eight hours.\n\nDuring its golden years in the late 19th and early 20th centuries, the site also played a vital role in providing employment to aboriginal Australians and many migrant workers arriving there from Europe after the second World War.\n\nThe fine tradition of the Brisbane Gasworks economic and employment-based successes will not be lost or forgotten with the Teneriffe Gasworks Village Development paying homage to the sites history and integrity in its pending urban development.\n\nThe gasholder structure at this site is set to become a hub of a new property development on the site – keeping the structural integrity of the pig iron structure. It will be a true reflection of urban renewal embracing its industrial past.\n\nLocated in South Dunedin, New Zealand, the Dunedin Gasworks Museum consists of a conserved engine house featuring a working boiler house, fitting shop and collection of five stationary steam engines. There are also displays of domestic and industrial gas appliances.\n\nLocated in Athens, Greece Technopolis (Gazi) is a gasworks converted to an exhibition space.\n\nThe Gas Museum in Leicester, UK is operated by The National Gas Museum Trust.\n\nGas Works Park is a public park in Seattle, Washington.\n\nThe Warsaw Gasworks Museum is a museum in Warsaw, Poland.\n\n"}
{"id": "24297943", "url": "https://en.wikipedia.org/wiki?curid=24297943", "title": "Heat and Mass Transfer", "text": "Heat and Mass Transfer\n\nHeat and Mass Transfer is a peer-reviewed scientific journal published by Springer. It serves the circulation of new developments in the field of basic research of heat and mass transfer phenomena, as well as related material properties and their measurements. Thereby applications to engineering problems are promoted. The journal publishes original research reports.\n\nAs of 1995 the title \"\"Wärme- und Stoffübertragung\" was changed to \"Heat and Mass Transfer\".\n\nAmong others, the journal is indexed in Google Scholar, INIS Atomindex, Journal Citation Reports/Science Edition, OCLC, PASCAL, Science Citation Index, Science Citation Index Expanded (SciSearch) and Scopus.\n\n"}
{"id": "6838137", "url": "https://en.wikipedia.org/wiki?curid=6838137", "title": "Hongfujin", "text": "Hongfujin\n\nHongfujin Precision Industry Co., a subsidiary of Foxconn, is a company which manufactures Apple's iPhone 5, iPhone X, iPad Pro, iPod as well as other products for multinational corporations. It is located in Shenzhen, Guangdong, China, and Zhengzhou, Henan, China.\n"}
{"id": "12376743", "url": "https://en.wikipedia.org/wiki?curid=12376743", "title": "IM Flash Technologies", "text": "IM Flash Technologies\n\nIM Flash Technologies, LLC is the semiconductor company founded in January 2006, by Intel Corporation and Micron Technology, Inc. IM Flash produces 3D XPoint used in data centers and high end computers. It has a 300mm wafer fab in Lehi, UT, United States. \n\nIt built a second 300mm wafer fab, IM Flash Singapore, which opened in April 2011. IM Flash took the leading edge in NAND flash scaling by moving to 34 nm design rules in 2008. IM Flash has been able to devise 25-nm NAND chips with 193-nm immersion lithography, plus self-aligned double-patterning (SADP) techniques, where it is widely believed that it is using scanners from ASML Holdings NV and SADP technology. In 2011 IM Flash moved to a 20 nm process– which was the smallest NAND flash technology at the time.\n\nOn July 16, 2018, Micron and Intel announced to cease joint development on 3D XPoint after the 2nd generation technology is finalized, which is expected to complete in the first half of 2019. Technology development beyond the 2nd gen. will be pursued independently by the two companies in order to optimize the technology for their respective product and business needs. The two companies will continue to manufacture memory based on 3D XPoint technology at the Intel-Micron Flash Technologies (IMFT) facility in Lehi, Utah.\n\nOn Oct. 18, 2018, Micron announced the intend to exercise its right to call the remaining interest in the parties' joint venture, IM Flash Technologies, LLC. Micron is to exercise the call option starting Jan. 1, 2019 and the timeline to close the transaction is between six and twelve months after the date Micron exercises the call. At the time of close, Micron expects to pay approximately $1.5 billion in cash for the transaction, dissolving Intel's non-controlling interest in IM Flash as well as IM Flash member debt, which was approximately $1 billion as of Aug. 30, 2018.\n\n\n"}
{"id": "3359456", "url": "https://en.wikipedia.org/wiki?curid=3359456", "title": "Incubator (culture)", "text": "Incubator (culture)\n\nIncubator is a device used to grow and maintain microbiological cultures or cell cultures. The incubator maintains optimal temperature, humidity and other conditions such as the CO (CO) and oxygen content of the atmosphere inside. Incubators are essential for a lot of experimental work in cell biology, microbiology and molecular biology and are used to culture both bacterial as well as eukaryotic cells.\n\nLouis Pasteur used the small opening underneath his staircase as an incubator. Incubators are also used in the poultry industry to act as a substitute for hens. This often results in higher hatch rates due to the ability to control both temperature and humidity. Various brands of incubators are commercially available to breeders.\n\nThe simplest incubators are insulated boxes with an adjustable heater, typically going up to 60 to 65 °C (140 to 150 °F), though some can go slightly higher (generally to no more than 100 °C). The most commonly used temperature both for bacteria such as the frequently used E. coli as well as for mammalian cells is approximately 37 °C (99 °F), as these organisms grow well under such conditions. For other organisms used in biological experiments, such as the budding yeast Saccharomyces cerevisiae, a growth temperature of 30 °C (86 °F) is optimal. \n\nMore elaborate incubators can also include the ability to lower the temperature (via refrigeration), or the ability to control humidity or CO levels. This is important in the cultivation of mammalian cells, where the relative humidity is typically >80% to prevent evaporation and a slightly acidic pH is achieved by maintaining a CO level of 5%.\n\nFrom aiding in hatching chicken eggs to enabling scientists to understand and develop vaccines for deadly viruses, the laboratory incubator has seen numerous applications over the thousands of years it has been in use. The incubator has also provided a foundation for medical advances and experimental work in cellular and molecular biology. \n\nAn incubator is made up of a chamber with a regulated temperature. Some incubators also regulate humidity, gas composition, or ventilation within that chamber. While many technological advances have occurred since the primitive incubators first used in ancient Egypt and China, the main purpose of the incubator has remained unchanged: to create a stable, controlled environment conducive to research, study, and cultivation.\n\nThe earliest incubators were found thousands of years ago in ancient Egypt and China, where they were used to keep chicken eggs warm. Use of incubators revolutionized food production, as it allowed chicks to hatch from eggs without requiring that a hen sit on them, thus freeing the hens to lay more eggs in a shorter period of time. Both early Egyptian and Chinese incubators were essentially large rooms that were heated by fires, where attendants turned the eggs at regular intervals to ensure even heat distribution..\n\nThe incubator received an update in the 16th century when Jean Baptiste Porta drew on ancient Egyptian design to create a more modern egg incubator. While he eventually had to discontinue his work due to the Spanish Inquisition, Rene-Antoine Ferchault de Reaumur took up the challenge in the middle of the 17th century. Reaumur warmed his incubator with a wood stove and monitored its temperature using the Reaumur thermometer, another of his inventions.\n\nIn the 19th century, researchers finally began to recognize that the use of incubators could contribute to medical advancements. They began to experiment to find the ideal environment for maintaining cell culture stocks. These early incubators were simply made up of bell jars that contained a single lit candle. Cultures were placed near the flame on the underside of the jar's lid, and the entire jar was placed in a dry, heated oven.\n\nIn the late 19th century, doctors realized another practical use for incubators: keeping premature or weak infants alive. The first infant incubator, used at a women's hospital in Paris, was heated by kerosene lamps. Fifty years later, Julius H. Hess, an American physician often considered to be the father of neonatology, designed an electric infant incubator that closely resembles the infant incubators in use today.\n\nThe next innovation in incubator technology came in the 1960s, when the CO2 incubator was introduced to the market. Demand came when doctors realized that they could use CO2 incubators to identify and study pathogens found in patients' bodily fluids. To do this, a sample was harvested and placed onto a sterile dish and into the incubator. The air in the incubator was kept at 37 degrees Celsius, the same temperature as the human body, and the incubator maintained the atmospheric carbon dioxide and nitrogen levels necessary to promote cell growth.\n\nAt this time, incubators also began to be used in genetic engineering. Scientists could create biologically essential proteins, such as insulin, with the use of incubators. Genetic modification could now take place on a molecular level, helping to improve the nutritional content and resistance to pestilence and disease of fruits and vegetables.\n\nIncubators serve a variety of functions in a scientific lab. Incubators generally maintain a constant temperature, however additional features are often built in. Many incubators also control humidity. Shaking incubators incorporate movement to mix cultures. Gas incubators regulate the internal gas composition. Some incubators have a means of circulating the air inside of them to ensure even distribution of temperatures. Many incubators built for laboratory use have a redundant power source, to ensure that power outages do not disrupt experiments. Incubators are made in a variety of sizes, from tabletop models, to warm rooms, which serve as incubators for large numbers of samples.\n\n"}
{"id": "26598534", "url": "https://en.wikipedia.org/wiki?curid=26598534", "title": "Indian Financial System Code", "text": "Indian Financial System Code\n\nThe Indian Financial System Code (IFS Code or IFSC) is an alphanumeric code that facilitates electronic funds transfer in India. A code uniquely identifies each bank branch participating in the two main Payment and settlement systems in India: the Real Time Gross Settlement (RTGS) and the National Electronic Fund Transfer (NEFT) systems.\n\nThe IFSC is an 11-character code with the first four alphabetic characters representing the bank name, and the last six characters (usually numeric, but can be alphabetic) representing the branch. The fifth character is 0 (zero) and reserved for future use. Bank IFS Code is used by the NEFT & RTGS systems to route the messages to the destination banks/branches.\nThe format of the IFS Code is shown below.\nBank-wise lists of IFS Codes are available with all the bank-branches participating in inter bank electronic funds transfer. A list of bank-branches participating in NEFT/RTGS and their IFS Code is available on the website of Reserve Bank of India. All the banks have also been advised to print the IFS code of the branch on cheques issued by branches to their customers.\n"}
{"id": "24554091", "url": "https://en.wikipedia.org/wiki?curid=24554091", "title": "List of early medieval watermills", "text": "List of early medieval watermills\n\nThis list of early medieval watermills comprises a selection of European watermills spanning the early Middle Ages, from 500 to 1000 AD.\n\nLargely unaffected from the turbulent political events following the demise of the Western Roman Empire, the importance of watermilling continued to grow under the new Germanic lords. The sharp rise in numbers of early medieval watermills coincided with the appearance of new documentary genres (legal codes, monastic charters, hagiography) which were more inclined to address such a relatively mundane device than the ancient urban-centered literary class had been. This partly explains the relative abundance of medieval literary references to watermills compared to former times.\n\nThe quantitative growth of medieval evidence appears to be more than a mere reflection of the changing nature of surviving sources. By Carolingian times, references to watermills in the Frankish Realm had become \"innumerable\". At the time of the compilation of the Domesday Book (1086), there were an estimated 6,500 watermills in England alone. \n\nBy the early 7th century, watermills were well established in Ireland, and began to spread from the former territory of the empire into the non-romanized parts of Germany a century later. The introduction of the ship mill and tide mill in the 6th century, both of which yet unattested for the ancient period, allowed for a flexible response to the changing water-level of rivers and the Atlantic Ocean, thus demonstrating the technological innovativeness of early medieval watermillers.\n\nBelow the earliest medieval evidence for different types of watermills. This list complements its ancient counterpart.\n\nIn the following, literary, epigraphical and documentary sources referring to watermills and other water-driven machines are listed.\n\nBelow are listed excavated or surveyed watermill sites dated to the early medieval period.\n\nThe following list comprises stray finds of early medieval millstones. Note that there is no way to distinguish millstones driven by water-power from those powered by animals turning a capstan. Most, however, are assumed to derive from watermills.\n\n\n"}
{"id": "22954083", "url": "https://en.wikipedia.org/wiki?curid=22954083", "title": "List of technologies", "text": "List of technologies\n\nHere is a list of significant technological developments, by chronological order and organized by their type and technology just in general that humans have created so far in the world. \n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "58985411", "url": "https://en.wikipedia.org/wiki?curid=58985411", "title": "Loubna Bouarfa", "text": "Loubna Bouarfa\n\nLoubna Bouarfa (born 26 June 1983) is a Dutch, Moroccan data scientist and founder and CEO of OKRA Technologies. \n\nBouarfa grew up in Meknes, Morocco. She then moved at the age of 17 to the University of Technology in Delft to study electrical engineering and specialise in statistical signal processing. In 2007, Bouarfa embarked on a PhD applying Machine Learning to predict surgical workflow and detect anomalies in real time. She applied her research directly to the operating table, predicting error and improving workflows. In her PhD, Bouarfa discovered that AI provides a strong framework for evidence-based medicine, using data to improve outcomes directly in the real-world environment outside of clinical trials and widely used controlled healthcare studies. \n\nBouarfa has founded OKRA Technologies, a company which uses its machine learning platform to improve healthcare outcomes. The OKRA AI engine is used by some of the world's biggest pharmaceutical companies to transform complex datasets into evidence-based predictions, in real time. It was designed to equip Healthcare and Life Sciences professionals with the foresight to improve patient outcomes in the real-world environment. In 2017, OKRA Technologies closed a $4.2 million Series A round.\n\nIn 2016, after several years in academia at the University of Technology in Delft and Imperial College in London, she was eager to implement her research on a commercial scale - this brought her to the Cambridge startup community. \n\nBouarfa holds a number of advisory positions and is a member of the European Commission's High Level Expert Group for Artificial Intelligence.\n\nIn 2017, Bouarfa was recognised by MIT Technology as an Innovator under 35. In January 2018, she was among the finalists for the award of entrepreneur of the Year by Women in IT, and was selected as one of Forbes Top 50 Women in Technology.\n"}
{"id": "14460347", "url": "https://en.wikipedia.org/wiki?curid=14460347", "title": "Martyn Thomas", "text": "Martyn Thomas\n\nProf. Martyn Thomas CBE FREng FIET FRSA (born 1948) is a British independent consultant and software engineer.\n\nMartyn Thomas founded the software engineering company Praxis in 1983, based in Bath, southern England. He has a special interest in safety-critical systems and other high integrity applications. He has acted as an expert witness involving complex software engineering issues.\n\nThomas was born in Salisbury, southern England. He studied biochemistry at University College London, graduating in 1969, when he started working in the field of computing. Between 1969 and 1983, he was employed at universities in London and the Netherlands, at STC working on telecommunications software, and at the South West Universities Regional Computer Centre in Bath.\n\nIn 1983, Thomas founded Praxis with David Bean, where he encouraged the use of formal methods within the company for software development. In 1986, Praxis became the first independent systems house to achieve BS 5750 (later ISO 9001) certification for all its activities. Praxis became internationally recognised as a leader in the use of rigorous software engineering, including formal methods, and grew to around 200 staff.\n\nIn December 1992, Praxis was sold to Deloitte and Touche, an international firm of accountants and management consultants, and Martyn became a Deloitte Consulting international partner whilst remaining chairman and, later, managing director of Praxis. He left Deloitte Consulting in 1997.\n\nHe is currently director of Martyn Thomas Associates Limited and a visiting professor at the Oxford University Department of Computer Science, and the University of Bristol. He lives in London.\n\nNon-executive director of the Health and Safety Executive (HSE),\nIT Livery Company Professor of Information Technology at Gresham College,\nVisiting Professor of Software Engineering at Aberystwyth University, UK,\nFellow at The Royal Academy of Engineering,\nMember at UK Computing Research Committee,\nOwner, Principal Consultant and Expert Witness at MTAL.,\n\nMember of Advisory Council at Foundation for Information Policy Research,\nFellow at British Computer Society,\nChair, Executive Board at DEPLOY Project,\nMember, \"Sufficient Evidence\" study at National Academies / CSTB,\nChair, Steering Committee at DIRC,\nMember of Council at EPSRC,\nMember of Advisory Group at OST Foresight programmes,\nPartner at Deloitte Consulting,\nFounder/Chairman/managing director at Praxis,\nChairman at Praxis Critical Systems,\nDeputy Director at SWURCC,\nSoftware Engineer at STC.\n\nCommander of the Order of the British Empire, CBE,\nFellow of the Royal Academy of Engineering,\nHonorary DSc (Hull),\nHonorary DSc (Edinburgh),\nHonorary DSc (City),\nHonorary DSc (Bath), Dr of Engineering,\nIEE Achievement Medal, Computing and Control,\nWho's Who.\n\n"}
{"id": "38925690", "url": "https://en.wikipedia.org/wiki?curid=38925690", "title": "Micro-Imaging Dust Analysis System", "text": "Micro-Imaging Dust Analysis System\n\nThe Micro-Imaging Dust Analysis System (MIDAS) is one of several instruments on the European Space Agency's \"Rosetta\" mission which studied \"in-situ\" the environment around the active comet 67P/Churyumov–Gerasimenko as it flew into the inner Solar System. MIDAS is an atomic force microscope (AFM) designed to collect dust particles emitted from the comet, and then scan them with a very sharp needle-like tip to determine their 3D structure, size and texture with very high resolution (4 nanometers).\n\nMIDAS is the first instrument capable of imaging the smallest cometary dust particles \"in-situ\". Some interplanetary dust particles collected in the Earth's stratosphere have been shown to have a cometary origin, but their precise provenance is typically unknown. The Stardust mission returned many cometary dust particles, collected during a fast flyby of comet 81P/Wild in aerogel, but these were highly modified, crushed and melted during deceleration and return to Earth. MIDAS was be used primarily in particles a few micrometers in diameter or smaller.\n\nBy collecting and imaging dust particles —tiny pieces of rock, ice and Organic compounds — emitted from comet 67P/Churyumov–Gerasimenko as it passes through the inner Solar System, MIDAS address questions including:\n\n\nSince comets are thought to be ancient, and contain material unchanged since their formation in the early Solar System, these questions will directly help support theories on the Solar System formation. The Principal Investigator is Mark Bentley from The Space Research Institute (IWF) in Austria. The hardware is contributed by universities in Austria, the Netherlands and Germany.\n\nMIDAS is mounted on the nadir panel of the \"Rosetta\" spacecraft and is composed of three main subsystems and an electronics box:\n\nDust collection is performed when close to the comet by opening the shutter and placing one of 61 targets directly behind the funnel. A collimator ensures that only one target at a time can be exposed. The targets are rectangular and in size and are mounted around the circumference of a wheel. Rotating this wheel moves a target from the dust collection (exposure) position to the analysis (scanning) position. Once sufficient dust has been collected, the shutter is closed and the exposed target moved in front of one of 16 cantilevers. Each cantilever is fitted with a sharp tip; since these can be worn down in operation, the sixteen tips provide redundancy for the one year of nominal operation.\n\nEach of the MIDAS cantilevers is piezo-resistive, meaning that its deflection is measured by a change in resistance, not by the reflection of laser light from the cantilever as in many commercial instruments. In this way the need for intricate optical alignment is avoided and the instrument could be made sufficiently robust to withstand launch. Sixteen cantilevers are carried for redundancy, each with a sharp tip of approximately 10 µm long. Four are coated with a cobalt alloy to allow magnetic force microscopy of collected dust.\n\nThe MIDAS targets, on which dust particles are collected, are made of silicon and are rectangular in shape, with a dimension of . Most are coated with a sol gel coating which aids in sticking particles to the targets. Three calibration targets are also carried, which allow calibration of the scanner in XY and in Z, and allow imaging of the tip shape. All 64 targets are mounted on the circumference of a wheel, which rotates to select a sample, to provide coarse Y positioning, and to move samples from the exposure to scanning position. The wheel can also be moved laterally, moving the wheel in front of each of the 16 cantilevers, and allowing for coarse X positioning.\n\nBefore a sample can be scanned, the microscope has to be brought close to the sample. This is achieved by a coarse approach stage driven by a brushed DC motor encased in a pressurised vacuum bellows. This motor drives a spindle which in turn raises or lowers a wheel placed in a spring-loaded wedge, as shown in the diagram. This allows the entire microscope stage to be moved by about 0.5 mm.\n\nThe heart of the instrument is a precision XYZ stage with a travel of approximately in XY and 10 µm in Z directions. Driven by piezo actuators this stage moves the selected cantilever and tip across the sample in a raster pattern, measuring the height of the sample at each point.\n\nThe following table summarises the key characteristics of the instrument:\n\nWhen the spacecraft was taken out of hibernation mode on 20 January 2014, the MIDAS computer software was updated, and the instrument is being calibrated and tested.\n\nAlthough MIDAS has been operated during the long (10 year) cruise phase, it has never been in an environment where significant dust was expected. Thus the only results available to date are reference scans of the calibration and blank targets. Scientific data should be available to the public when the mission starts its primary science phase in November 2014, subject to the usual 6 months proprietary period. Data was made available via the ESA Planetary Science Archive (PSA), where a dedicated Rosetta section is available.\n\n"}
{"id": "43916768", "url": "https://en.wikipedia.org/wiki?curid=43916768", "title": "Ministry of Information (Cambodia)", "text": "Ministry of Information (Cambodia)\n\nThe Ministry of Information () is the government ministry in charge of media and information in Cambodia. The current Minister of Information is Khieu Kanharith.\n\nThe Ministry consists of:\n\n"}
{"id": "3503207", "url": "https://en.wikipedia.org/wiki?curid=3503207", "title": "Multi-core processor", "text": "Multi-core processor\n\nA multi-core processor is a single computing component with two or more independent processing units called cores, which read and execute program instructions. The instructions are ordinary CPU instructions (such as add, move data, and branch) but the single processor can run multiple instructions on separate cores at the same time, increasing overall speed for programs amenable to parallel computing. Manufacturers typically integrate the cores onto a single integrated circuit die (known as a chip multiprocessor or CMP) or onto multiple dies in a single chip package. The microprocessors currently used in almost all personal computers are multi-core.\nA multi-core processor implements multiprocessing in a single physical package. Designers may couple cores in a multi-core device tightly or loosely. For example, cores may or may not share caches, and they may implement message passing or shared-memory inter-core communication methods. Common network topologies to interconnect cores include bus, ring, two-dimensional mesh, and crossbar. Homogeneous multi-core systems include only identical cores; heterogeneous multi-core systems have cores that are not identical (e.g. big.LITTLE have heterogeneous cores that share the same instruction set, while AMD Accelerated Processing Units have cores that don't even share the same instruction set). Just as with single-processor systems, cores in multi-core systems may implement architectures such as VLIW, superscalar, vector, or multithreading.\n\nMulti-core processors are widely used across many application domains, including general-purpose, embedded, network, digital signal processing (DSP), and graphics (GPU).\n\nThe improvement in performance gained by the use of a multi-core processor depends very much on the software algorithms used and their implementation. In particular, possible gains are limited by the fraction of the software that can run in parallel simultaneously on multiple cores; this effect is described by Amdahl's law. In the best case, so-called embarrassingly parallel problems may realize speedup factors near the number of cores, or even more if the problem is split up enough to fit within each core's cache(s), avoiding use of much slower main-system memory. Most applications, however, are not accelerated so much unless programmers invest a prohibitive amount of effort in re-factoring the whole problem.\nThe parallelization of software is a significant ongoing topic of research.\n\nThe terms \"multi-core\" and \"dual-core\" most commonly refer to some sort of central processing unit (CPU), but are sometimes also applied to digital signal processors (DSP) and system on a chip (SoC). The terms are generally used only to refer to multi-core microprocessors that are manufactured on the \"same\" integrated circuit die; separate microprocessor dies in the same package are generally referred to by another name, such as \"multi-chip module\". This article uses the terms \"multi-core\" and \"dual-core\" for CPUs manufactured on the \"same\" integrated circuit, unless otherwise noted.\n\nIn contrast to multi-core systems, the term \"multi-CPU\" refers to multiple physically separate processing-units (which often contain special circuitry to facilitate communication between each other).\n\nThe terms \"many-core\" and \"massively multi-core\" are sometimes used to describe multi-core architectures with an especially high number of cores (tens to thousands).\n\nSome systems use many soft microprocessor cores placed on a single FPGA. Each \"core\" can be considered a \"semiconductor intellectual property core\" as well as a CPU core.\n\nWhile manufacturing technology improves, reducing the size of individual gates, physical limits of semiconductor-based microelectronics have become a major design concern. These physical limitations can cause significant heat dissipation and data synchronization problems. Various other methods are used to improve CPU performance. Some \"instruction-level parallelism\" (ILP) methods such as superscalar pipelining are suitable for many applications, but are inefficient for others that contain difficult-to-predict code. Many applications are better suited to \"thread-level parallelism\" (TLP) methods, and multiple independent CPUs are commonly used to increase a system's overall TLP. A combination of increased available space (due to refined manufacturing processes) and the demand for increased TLP led to the development of multi-core CPUs.\n\nSeveral business motives drive the development of multi-core architectures. For decades, it was possible to improve performance of a CPU by shrinking the area of the integrated circuit (IC), which reduced the cost per device on the IC. Alternatively, for the same circuit area, more transistors could be used in the design, which increased functionality, especially for complex instruction set computing (CISC) architectures. Clock rates also increased by orders of magnitude in the decades of the late 20th century, from several megahertz in the 1980s to several gigahertz in the early 2000s.\n\nAs the rate of clock speed improvements slowed, increased use of parallel computing in the form of multi-core processors has been pursued to improve overall processing performance. Multiple cores were used on the same CPU chip, which could then lead to better sales of CPU chips with two or more cores. For example, Intel has produced a 48-core processor for research in cloud computing; each core has an x86 architecture.\n\nSince computer manufacturers have long implemented symmetric multiprocessing (SMP) designs using discrete CPUs, the issues regarding implementing multi-core processor architecture and supporting it with software are well known.\n\nAdditionally:\n\nIn order to continue delivering regular performance improvements for general-purpose processors, manufacturers such as Intel and AMD have turned to multi-core designs, sacrificing lower manufacturing-costs for higher performance in some applications and systems. Multi-core architectures are being developed, but so are the alternatives. An especially strong contender for established markets is the further integration of peripheral functions into the chip.\n\nThe proximity of multiple CPU cores on the same die allows the cache coherency circuitry to operate at a much higher clock rate than what is possible if the signals have to travel off-chip. Combining equivalent CPUs on a single die significantly improves the performance of cache snoop (alternative: Bus snooping) operations. Put simply, this means that signals between different CPUs travel shorter distances, and therefore those signals degrade less. These higher-quality signals allow more data to be sent in a given time period, since individual signals can be shorter and do not need to be repeated as often.\n\nAssuming that the die can physically fit into the package, multi-core CPU designs require much less printed circuit board (PCB) space than do multi-chip SMP designs. Also, a dual-core processor uses slightly less power than two coupled single-core processors, principally because of the decreased power required to drive signals external to the chip. Furthermore, the cores share some circuitry, like the L2 cache and the interface to the front-side bus (FSB). In terms of competing technologies for the available silicon die area, multi-core design can make use of proven CPU core library designs and produce a product with lower risk of design error than devising a new wider-core design. Also, adding more cache suffers from diminishing returns.\n\nMulti-core chips also allow higher performance at lower energy. This can be a big factor in mobile devices that operate on batteries. Since each core in a multi-core CPU is generally more energy-efficient, the chip becomes more efficient than having a single large monolithic core. This allows higher performance with less energy. A challenge in this, however, is the additional overhead of writing parallel code.\n\nMaximizing the usage of the computing resources provided by multi-core processors requires adjustments both to the operating system (OS) support and to existing application software. Also, the ability of multi-core processors to increase application performance depends on the use of multiple threads within applications.\n\nIntegration of a multi-core chip can lower the chip production yields. They are also more difficult to manage thermally than lower-density single-core designs. Intel has partially countered this first problem by creating its quad-core designs by combining two dual-core ones on a single die with a unified cache, hence any two working dual-core dies can be used, as opposed to producing four cores on a single die and requiring all four to work to produce a quad-core CPU. From an architectural point of view, ultimately, single CPU designs may make better use of the silicon surface area than multiprocessing cores, so a development commitment to this architecture may carry the risk of obsolescence. Finally, raw processing power is not the only constraint on system performance. Two processing cores sharing the same system bus and memory bandwidth limits the real-world performance advantage. In a 2009 report, Dr Jun Ni showed that if a single core is close to being memory-bandwidth limited, then going to dual-core might give 30% to 70% improvement; if memory bandwidth is not a problem, then a 90% improvement can be expected; however, Amdahl's law makes this claim dubious. It would be possible for an application that used two CPUs to end up running faster on a single-core one if communication between the CPUs was the limiting factor, which would count as more than 100% improvement.\n\nThe trend in processor development has been towards an ever-increasing number of cores, as processors with hundreds or even thousands of cores become theoretically possible. In addition, multi-core chips mixed with simultaneous multithreading, memory-on-chip, and special-purpose \"heterogeneous\" (or asymmetric) cores promise further performance and efficiency gains, especially in processing multimedia, recognition and networking applications. For example, a big.LITTLE core includes a high-performance core (called 'big') and a low-power core (called 'LITTLE'). There is also a trend towards improving energy-efficiency by focusing on performance-per-watt with advanced fine-grain or ultra fine-grain power management and dynamic voltage and frequency scaling (i.e. laptop computers and portable media players).\n\nChips designed from the outset for a large number of cores (rather than having evolved from single core designs) are sometimes referred to as manycore designs, emphasising qualitative differences.\n\nThe composition and balance of the cores in multi-core architecture show great variety. Some architectures use one core design repeated consistently (\"homogeneous\"), while others use a mixture of different cores, each optimized for a different, \"heterogeneous\" role.\n\nThe article \"CPU designers debate multi-core future\" by Rick Merritt, EE Times 2008, includes these comments:\nAn outdated version of an anti-virus application may create a new thread for a scan process, while its GUI thread waits for commands from the user (e.g. cancel the scan). In such cases, a multi-core architecture is of little benefit for the application itself due to the single thread doing all the heavy lifting and the inability to balance the work evenly across multiple cores. Programming truly multithreaded code often requires complex co-ordination of threads and can easily introduce subtle and difficult-to-find bugs due to the interweaving of processing on data shared between threads (see thread-safety). Consequently, such code is much more difficult to debug than single-threaded code when it breaks. There has been a perceived lack of motivation for writing consumer-level threaded applications because of the relative rarity of consumer-level demand for maximum use of computer hardware. Although threaded applications incur little additional performance penalty on single-processor machines, the extra overhead of development has been difficult to justify due to the preponderance of single-processor machines. Also, serial tasks like decoding the entropy encoding algorithms used in video codecs are impossible to parallelize because each result generated is used to help create the next result of the entropy decoding algorithm.\n\nGiven the increasing emphasis on multi-core chip design, stemming from the grave thermal and power consumption problems posed by any further significant increase in processor clock speeds, the extent to which software can be multithreaded to take advantage of these new chips is likely to be the single greatest constraint on computer performance in the future. If developers are unable to design software to fully exploit the resources provided by multiple cores, then they will ultimately reach an insurmountable performance ceiling.\n\nThe telecommunications market had been one of the first that needed a new design of parallel datapath packet processing because there was a very quick adoption of these multiple-core processors for the datapath and the control plane. These MPUs are going to replace the traditional Network Processors that were based on proprietary microcode or picocode.\n\nParallel programming techniques can benefit from multiple cores directly. Some existing parallel programming models such as Cilk Plus, OpenMP, OpenHMPP, FastFlow, Skandium, MPI, and Erlang can be used on multi-core platforms. Intel introduced a new abstraction for C++ parallelism called TBB. Other research efforts include the Codeplay Sieve System, Cray's Chapel, Sun's Fortress, and IBM's X10.\n\nMulti-core processing has also affected the ability of modern computational software development. Developers programming in newer languages might find that their modern languages do not support multi-core functionality. This then requires the use of numerical libraries to access code written in languages like C and Fortran, which perform math computations faster than newer languages like C#. Intel's MKL and AMD's ACML are written in these native languages and take advantage of multi-core processing. Balancing the application workload across processors can be problematic, especially if they have different performance characteristics. There are different conceptual models to deal with the problem, for example using a coordination language and program building blocks (programming libraries or higher-order functions). Each block can have a different native implementation for each processor type. Users simply program using these abstractions and an intelligent compiler chooses the best implementation based on the context.\n\nManaging concurrency acquires a central role in developing parallel applications. The basic steps in designing parallel applications are:\n\n\n\n\n\nOn the other hand, on the server side, multi-core processors are ideal because they allow many users to connect to a site simultaneously and have independent threads of execution. This allows for Web servers and application servers that have much better throughput.\n\nVendors may license some software \"per processor\". This can give rise to ambiguity, because a \"processor\" may consist either of a single core or of a combination of cores.\n\nEmbedded computing operates in an area of processor technology distinct from that of \"mainstream\" PCs. The same technological drives towards multi-core apply here too. Indeed, in many cases the application is a \"natural\" fit for multi-core technologies, if the task can easily be partitioned between the different processors.\n\nIn addition, embedded software is typically developed for a specific hardware release, making issues of software portability, legacy code or supporting independent developers less critical than is the case for PC or enterprise computing. As a result, it is easier for developers to adopt new technologies and as a result there is a greater variety of multi-core processing architectures and suppliers.\n\n, multi-core network processing devices have become mainstream, with companies such as Freescale Semiconductor, Cavium Networks, Wintegra and Broadcom all manufacturing products with eight processors. For the system developer, a key challenge is how to exploit all the cores in these devices to achieve maximum networking performance at the system level, despite the performance limitations inherent in an SMP operating system. To address this issue, companies such as 6WIND provide portable packet processing software designed so that the networking data plane runs in a fast path environment outside the OS.\n\nIn digital signal processing the same trend applies: Texas Instruments has the three-core TMS320C6488 and four-core TMS320C5441, Freescale the four-core MSC8144 and six-core MSC8156 (and both have stated they are working on eight-core successors). Newer entries include the Storm-1 family from Stream Processors, Inc with 40 and 80 general purpose ALUs per chip, all programmable in C as a SIMD engine and Picochip with three-hundred processors on a single die, focused on communication applications.\n\n\n\n\nThe research and development of multicore processors often compares many options, and benchmarks are developed to help such evaluations. Existing benchmarks include SPLASH-2, PARSEC, and COSMIC for heterogeneous systems.\n\n\n\n"}
{"id": "237770", "url": "https://en.wikipedia.org/wiki?curid=237770", "title": "Negative resistance", "text": "Negative resistance\n\nIn electronics, negative resistance (NR) is a property of some electrical circuits and devices in which an increase in voltage across the device's terminals results in a decrease in electric current through it.\n\nThis is in contrast to an ordinary resistor in which an increase of applied voltage causes a proportional increase in current due to Ohm's law, resulting in a positive resistance. While a positive resistance consumes power from current passing through it, a negative resistance produces power. Under certain conditions it can increase the power of an electrical signal, amplifying it.\n\nNegative resistance is an uncommon property which occurs in a few nonlinear electronic components. In a nonlinear device, two types of resistance can be defined: 'static' or 'absolute resistance', the ratio of voltage to current formula_1, and \"differential resistance\", the ratio of a change in voltage to the resulting change in current formula_2. The term negative resistance means negative differential resistance (NDR), formula_3. In general, a negative differential resistance is a two-terminal component which can amplify, converting DC power applied to its terminals to AC output power to amplify an AC signal applied to the same terminals. They are used in electronic oscillators and amplifiers, particularly at microwave frequencies. Most microwave energy is produced with negative differential resistance devices. They can also have hysteresis and be bistable, and so are used in switching and memory circuits. Examples of devices with negative differential resistance are tunnel diodes, Gunn diodes, and gas discharge tubes such as neon lamps. In addition, circuits containing amplifying devices such as transistors and op amps with positive feedback can have negative differential resistance. These are used in oscillators and active filters.\n\nBecause they are nonlinear, negative resistance devices have a more complicated behavior than the positive \"ohmic\" resistances usually encountered in electric circuits. Unlike most positive resistances, negative resistance varies depending on the voltage or current applied to the device, and negative resistance devices can have negative resistance over only a limited portion of their voltage or current range. Therefore, there is no real \"negative resistor\" analogous to a positive resistor, which has a constant negative resistance over an arbitrarily wide range of current.\n\nThe resistance between two terminals of an electrical device or circuit is determined by its current–voltage (\"I–V\") curve (characteristic curve), giving the current formula_4 through it for any given voltage formula_5 across it. Most materials, including the ordinary (positive) resistances encountered in electrical circuits, obey Ohm's law; the current through them is proportional to the voltage over a wide range. So the \"I–V\" curve of an ohmic resistance is a straight line through the origin with positive slope. The resistance is the ratio of voltage to current, the inverse slope of the line (in \"I–V\" graphs where the voltage formula_5 is the independent variable) and is constant.\n\nNegative resistance occurs in a few nonlinear (nonohmic) devices. In a nonlinear component the \"I–V\" curve is not a straight line, so it does not obey Ohm's law. Resistance can still be defined, but the resistance is not constant; it varies with the voltage or current through the device. The resistance of such a nonlinear device can be defined in two ways, which are equal for ohmic resistances:\n\n\n\nNegative resistance, like positive resistance, is measured in ohms.\n\nConductance is the reciprocal of resistance. It is measured in siemens (formerly \"mho\") which is the conductance of a resistor with a resistance of one ohm. Each type of resistance defined above has a corresponding conductance\nIt can be seen that the conductance has the same sign as its corresponding resistance: a negative resistance will have a negative conductance while a positive resistance will have a positive conductance.\n\nOne way in which the different types of resistance can be distinguished is in the directions of current and electric power between a circuit and an electronic component. The illustrations below, with a rectangle representing the component attached to a circuit, summarize how the different types work:\nIn an electronic device, the differential resistance formula_14, the static resistance formula_15, or both, can be negative, so there are three categories of devices \"(fig. 2–4 above, and table)\" which could be called \"negative resistances\".\n\nThe term \"negative resistance\" almost always means negative \"differential\" resistance Negative differential resistance devices have unique capabilities: they can act as \"one-port amplifiers\", increasing the power of a time-varying signal applied to their port (terminals), or excite oscillations in a tuned circuit to make an oscillator. They can also have hysteresis. It is not possible for a device to have negative differential resistance without a power source, and these devices can be divided into two categories depending on whether they get their power from an internal source or from their port:\n\n\n\nOccasionally ordinary power sources are referred to as \"negative resistances\" (fig. 3 above). Although the \"static\" or \"absolute\" resistance formula_15 of active devices (power sources) can be considered negative (see Negative static resistance section below) most ordinary power sources (AC or DC), such as batteries, generators, and (non positive feedback) amplifiers, have positive \"differential\" resistance (their source resistance). Therefore, these devices cannot function as one-port amplifiers or have the other capabilities of negative differential resistances.\n\nElectronic components with negative differential resistance include these devices:\n\nElectric discharges through gases also exhibit negative differential resistance, including these devices\n\n\nIn addition, active circuits with negative differential resistance can also be built with amplifying devices like transistors and op amps, using feedback. A number of new experimental negative differential resistance materials and devices have been discovered in recent years. The physical processes which cause negative resistance are diverse, and each type of device has its own negative resistance characteristics, specified by its current–voltage curve.\n\nA point of some confusion is whether ordinary resistance (\"static\" or \"absolute\" resistance, formula_17) can be negative. In electronics, the term \"resistance\" is customarily applied only to passive materials and components – such as wires, resistors and diodes. These cannot have formula_18 as shown by Joule's law formula_19. A passive device consumes electric power, so from the passive sign convention formula_20. Therefore, from Joule's law formula_21. In other words, no material can conduct electric current better than a \"perfect\" conductor with zero resistance. For a passive device to have formula_22 would violate either conservation of energy or the second law of thermodynamics, \"(diagram)\". Therefore, some authors state that static resistance can never be negative.\n\nHowever it is easily shown that the ratio of voltage to current v/i at the terminals of any power source (AC or DC) is negative. For electric power (potential energy) to flow out of a device into the circuit, charge must flow through the device in the direction of increasing potential energy, conventional current (positive charge) must move from the negative to the positive terminal. So the direction of the instantaneous current is \"out\" of the positive terminal. This is opposite to the direction of current in a passive device defined by the passive sign convention so the current and voltage have opposite signs, and their ratio is negative\nThis can also be proved from Joule's law\nThis shows that power can flow out of a device into the circuit if and only if formula_18. Whether or not this quantity is referred to as \"resistance\" when negative is a matter of convention. The absolute resistance of power sources is negative, but this is not to be regarded as \"resistance\" in the same sense as positive resistances. The negative static resistance of a power source is a rather abstract and not very useful quantity, because it varies with the load. Due to conservation of energy it is always simply equal to the negative of the static resistance of the attached circuit \"(right)\".\n\nWork must be done on the charges by some source of energy in the device, to make them move toward the positive terminal against the electric field, so conservation of energy requires that negative static resistances have a source of power. The power may come from an internal source which converts some other form of energy to electric power as in a battery or generator, or from a separate connection to an external power supply circuit as in an amplifying device like a transistor, vacuum tube, or op amp.\n\nA circuit cannot have negative static resistance (be active) over an infinite voltage or current range, because it would have to be able to produce infinite power. Any active circuit or device with a finite power source is \"eventually passive\". This property means if a large enough external voltage or current of either polarity is applied to it, its static resistance becomes positive and it consumes power\n\nTherefore, the ends of the \"I–V\" curve will eventually turn and enter the 1st and 3rd quadrants. Thus the range of the curve having negative static resistance is limited, confined to a region around the origin. For example, applying a voltage to a generator or battery \"(graph, above)\" greater than its open-circuit voltage will reverse the direction of current flow, making its static resistance positive so it consumes power. Similarly, applying a voltage to the negative impedance converter below greater than its power supply voltage \"V\" will cause the amplifier to saturate, also making its resistance positive.\n\nIn a device or circuit with negative differential resistance (NDR), in some part of the \"I–V\" curve the current decreases as the voltage increases:\nThe \"I–V\" curve is nonmonotonic (having peaks and troughs) with regions of negative slope representing negative differential resistance.\n\nPassive negative differential resistances have positive \"static\" resistance; they consume net power. Therefore, the \"I–V\" curve is confined to the 1st and 3rd quadrants of the graph, and passes through the origin. This requirement means (excluding some asymptotic cases) that the region(s) of negative resistance must be limited, and surrounded by regions of positive resistance, and cannot include the origin.\n\nNegative differential resistances can be classified into two types:\n\n\nMost devices have a single negative resistance region. However devices with multiple separate negative resistance regions can also be fabricated. These can have more than two stable states, and are of interest for use in digital circuits to implement multivalued logic.\n\nAn intrinsic parameter used to compare different devices is the \"peak-to-valley current ratio\" (PVR), the ratio of the current at the top of the negative resistance region to the current at the bottom \"(see graphs, above)\":\nThe larger this is, the larger the potential AC output for a given DC bias current, and therefore the greater the efficiency\n\nA negative differential resistance device can amplify an AC signal applied to it if the signal is biased with a DC voltage or current to lie within the negative resistance region of its \"I–V\" curve.\n\nThe tunnel diode circuit \"(see diagram)\" is an example. The tunnel diode \"TD\" has voltage controlled negative differential resistance. The battery formula_30 adds a constant voltage (bias) across the diode so it operates in its negative resistance range, and provides power to amplify the signal. Suppose the negative resistance at the bias point is formula_31. For stability formula_32 must be less than formula_33. Using the formula for a voltage divider, the AC output voltage is\nIn a normal voltage divider, the resistance of each branch is less than the resistance of the whole, so the output voltage is less than the input. Here, due to the negative resistance, the total AC resistance formula_36 is less than the resistance of the diode alone formula_33 so the AC output voltage formula_38 is greater than the input formula_39. The voltage gain formula_40 is greater than one, and increases without limit as formula_32 approaches formula_33.\n\nThe diagrams illustrate how a biased negative differential resistance device can increase the power of a signal applied to it, amplifying it, although it only has two terminals. Due to the superposition principle the voltage and current at the device's terminals can be divided into a DC bias component and an AC component .\nSince a positive change in voltage formula_45 causes a \"negative\" change in current formula_46, the AC current and voltage in the device are 180° out of phase. This means in the AC equivalent circuit \"(right)\", the instantaneous AC current Δ\"i\" flows through the device in the direction of \"increasing\" AC potential Δ\"v\", as it would in a generator. Therefore, the AC power dissipation is \"negative\"; AC power is produced by the device and flows into the external circuit.\nWith the proper external circuit, the device can increase the AC signal power delivered to a load, serving as an amplifier, or excite oscillations in a resonant circuit to make an oscillator. Unlike in a two port amplifying device such as a transistor or op amp, the amplified signal leaves the device through the same two terminals (port) as the input signal enters.\n\nIn a passive device, the AC power produced comes from the input DC bias current, the device absorbs DC power, some of which is converted to AC power by the nonlinearity of the device, amplifying the applied signal. Therefore, the output power is limited by the bias power\nThe negative differential resistance region cannot include the origin, because it would then be able to amplify a signal with no applied DC bias current, producing AC power with no power input. The device also dissipates some power as heat, equal to the difference between the DC power in and the AC power out.\n\nThe device may also have reactance and therefore the phase difference between current and voltage may differ from 180° and may vary with frequency. As long as the real component of the impedance is negative (phase angle between 90° and 270°), the device will have negative resistance and can amplify.\n\nThe maximum AC output power is limited by size of the negative resistance region (formula_49 in graphs above)\n\nThe reason that the output signal can leave a negative resistance through the same port that the input signal enters is that from transmission line theory, the AC voltage or current at the terminals of a component can be divided into two oppositely moving waves, the \"incident wave\" formula_51, which travels toward the device, and the \"reflected wave\" formula_52, which travels away from the device. A negative differential resistance in a circuit can amplify if the magnitude of its reflection coefficient formula_53, the ratio of the reflected wave to the incident wave, is greater than one.\nThe \"reflected\" (output) signal has larger amplitude than the incident; the device has \"reflection gain\". The reflection coefficient is determined by the AC impedance of the negative resistance device, formula_56, and the impedance of the circuit attached to it, formula_57. If formula_58 and formula_59 then formula_60 and the device will amplify. On the Smith chart, a graphical aide widely used in the design of high frequency circuits, negative differential resistance corresponds to points outside the unit circle formula_61, the boundary of the conventional chart, so special \"expanded\" charts must be used.\n\nBecause it is nonlinear, a circuit with negative differential resistance can have multiple equilibrium points (possible DC operating points), which lie on the \"I–V\" curve. An equilibrium point will be stable, so the circuit converges to it within some neighborhood of the point, if its poles are in the left half of the s plane (LHP), while a point is unstable, causing the circuit to oscillate or \"latch up\" (converge to another point), if its poles are on the \"jω\" axis or right half plane (RHP), respectively. In contrast, a linear circuit has a single equilibrium point that may be stable or unstable. The equilibrium points are determined by the DC bias circuit, and their stability is determined by the AC impedance formula_62 of the external circuit.\nHowever, because of the different shapes of the curves, the condition for stability is different for VCNR and CCNR types of negative resistance:\n\n\nFor general negative resistance circuits with reactance, the stability must be determined by standard tests like the Nyquist stability criterion. Alternatively, in high frequency circuit design, the values of formula_70 for which the circuit is stable are determined by a graphical technique using \"stability circles\" on a Smith chart.\n\nFor simple nonreactive negative resistance devices with formula_71 and formula_72 the different operating regions of the device can be illustrated by load lines on the \"I–V\" curve \"(see graphs)\".\n\nThe DC load line (DCL) is a straight line determined by the DC bias circuit, with equation\nwhere formula_74 is the DC bias supply voltage and R is the resistance of the supply. The possible DC operating point(s) (Q points) occur where the DC load line intersects the \"I–V\" curve. For stability\nThe AC load line (\"L\" − \"L\") is a straight line through the Q point whose slope is the differential (AC) resistance formula_75 facing the device. Increasing formula_75 rotates the load line counterclockwise. The circuit operates in one of three possible regions \"(see diagrams)\", depending on formula_75.\n\nIn addition to the passive devices with intrinsic negative differential resistance above, circuits with amplifying devices like transistors or op amps can have negative resistance at their ports. The input or output impedance of an amplifier with enough positive feedback applied to it can be negative. If formula_84 is the input resistance of the amplifier without feedback, formula_85 is the amplifier gain, and formula_86 is the transfer function of the feedback path, the input resistance with positive shunt feedback is\nSo if the loop gain formula_88 is greater than one, formula_89 will be negative. The circuit acts like a \"negative linear resistor\" over a limited range, with \"I–V\" curve having a straight line segment through the origin with negative slope \"(see graphs)\". It has both negative differential resistance and is active\nand thus obeys Ohm's law as if it had a negative value of resistance \"−R\", over its linear range (such amplifiers can also have more complicated negative resistance \"I–V\" curves that do not pass through the origin).\n\nIn circuit theory these are called \"active resistors\". Applying a voltage across the terminals causes a proportional current \"out\" of the positive terminal, the opposite of an ordinary resistor. For example, connecting a battery to the terminals would cause the battery to charge rather than discharge.\n\nConsidered as one-port devices, these circuits function similarly to the passive negative differential resistance components above, and like them can be used to make one-port amplifiers and oscillators with the advantages that:\nThe \"I–V\" curve can have voltage-controlled (\"N\" type) or current-controlled (\"S\" type) negative resistance, depending on whether the feedback loop is connected in \"shunt\" or \"series\".\n\nNegative reactances \"(below)\" can also be created, so feedback circuits can be used to create \"active\" linear circuit elements, resistors, capacitors, and inductors, with negative values. They are widely used in active filters because they can create transfer functions that cannot be realized with positive circuit elements. Examples of circuits with this type of negative resistance are the negative impedance converter (NIC), gyrator, Deboo integrator, frequency dependent negative resistance (FDNR), and generalized immittance converter (GIC).\n\nIf an LC circuit is connected across the input of a positive feedback amplifier like that above, the negative differential input resistance formula_91 can cancel the positive loss resistance formula_92 inherent in the tuned circuit. If formula_93 this will create in effect a tuned circuit with zero AC resistance (poles on the \"jω\" axis). Spontaneous oscillation will be excited in the tuned circuit at its resonant frequency, sustained by the power from the amplifier. This is how feedback oscillators such as Hartley or Colpitts oscillators work. This negative resistance model is an alternate way of analyzing feedback oscillator operation. \"All\" linear oscillator circuits have negative resistance although in most feedback oscillators the tuned circuit is an integral part of the feedback network, so the circuit does not have negative resistance at all frequencies but only near the oscillation frequency.\n\nA tuned circuit connected to a negative resistance which cancels some but not all of its parasitic loss resistance (so formula_94) will not oscillate, but the negative resistance will decrease the damping in the circuit (moving its poles toward the \"jω\" axis), increasing its Q factor so it has a narrower bandwidth and more selectivity. Q enhancement, also called \"regeneration\", was first used in the regenerative radio receiver invented by Edwin Armstrong in 1912 and later in \"Q multipliers\". It is widely used in active filters. For example, RF integrated circuits use \"integrated inductors\" to save space, consisting of a spiral conductor fabricated on chip. These have high losses and low Q, so to create high Q tuned circuits their Q is increased by applying negative resistance.\n\nCircuits which exhibit chaotic behavior can be considered quasi-periodic or nonperiodic oscillators, and like all oscillators require a negative resistance in the circuit to provide power. Chua's circuit, a simple nonlinear circuit widely used as the standard example of a chaotic system, requires a nonlinear active resistor component, sometimes called Chua's diode. This is usually synthesized using a negative impedance converter circuit.\n\nA common example of an \"active resistance\" circuit is the negative impedance converter (NIC) shown in the diagram. The two resistors formula_95 and the op amp constitute a negative feedback non-inverting amplifier with gain of 2. The output voltage of the op-amp is\nSo if a voltage formula_5 is applied to the input, the same voltage is applied \"backwards\" across formula_98, causing current to flow through it out of the input. The current is\nSo the input impedance to the circuit is\nThe circuit converts the impedance formula_98 to its negative. If formula_98 is a resistor of value formula_103, within the linear range of the op amp formula_104 the input impedance acts like a linear \"negative resistor\" of value formula_105. The input port of the circuit is connected into another circuit as if it was a component. An NIC can cancel undesired positive resistance in another circuit, for example they were originally developed to cancel resistance in telephone cables, serving as repeaters.\n\nBy replacing formula_98 in the above circuit with a capacitor , negative capacitances and inductances can also be synthesized. A negative capacitance will have an \"I–V\" relation and an impedance formula_107 of\nwhere formula_109. Applying a positive current to a negative capacitance will cause it to \"discharge\"; its voltage will \"decrease\". Similarly, a negative inductance will have an \"I–V\" characteristic and impedance formula_110 of\nA circuit having negative capacitance or inductance can be used to cancel unwanted positive capacitance or inductance in another circuit. NIC circuits were used to cancel reactance on telephone cables.\n\nThere is also another way of looking at them. In a negative capacitance the current will be 180° opposite in phase to the current in a positive capacitance. Instead of leading the voltage by 90° it will lag the voltage by 90°, as in an inductor. Therefore, a negative capacitance acts like an inductance in which the impedance has a reverse dependence on frequency ω; decreasing instead of increasing like a real inductance Similarly a negative inductance acts like a capacitance that has an impedance which increases with frequency. Negative capacitances and inductances are \"non-Foster\" circuits which violate Foster's reactance theorem. One application being researched is to create an active matching network which could match an antenna to a transmission line over a broad range of frequencies, rather than just a single frequency as with current networks. This would allow the creation of small compact antennas that would have broad bandwidth, exceeding the Chu–Harrington limit.\n\nNegative differential resistance devices are widely used to make electronic oscillators. In a negative resistance oscillator, a negative differential resistance device such as an IMPATT diode, Gunn diode, or microwave vacuum tube is connected across an electrical resonator such as an LC circuit, a quartz crystal, dielectric resonator or cavity resonator with a DC source to bias the device into its negative resistance region and provide power. A resonator such as an LC circuit is \"almost\" an oscillator; it can store oscillating electrical energy, but because all resonators have internal resistance or other losses, the oscillations are damped and decay to zero. The negative resistance cancels the positive resistance of the resonator, creating in effect a lossless resonator, in which spontaneous continuous oscillations occur at the resonator's resonant frequency.\n\nNegative resistance oscillators are mainly used at high frequencies in the microwave range or above, since feedback oscillators function poorly at these frequencies. Microwave diodes are used in low- to medium-power oscillators for applications such as radar speed guns, and local oscillators for satellite receivers. They are a widely used source of microwave energy, and virtually the only solid-state source of millimeter wave and terahertz energy Negative resistance microwave vacuum tubes such as magnetrons produce higher power outputs, in such applications as radar transmitters and microwave ovens. Lower frequency relaxation oscillators can be made with UJTs and gas-discharge lamps such as neon lamps.\n\nThe negative resistance oscillator model is not limited to one-port devices like diodes but can also be applied to feedback oscillator circuits with two port devices such as transistors and tubes. In addition, in modern high frequency oscillators, transistors are increasingly used as one-port negative resistance devices like diodes. At microwave frequencies, transistors with certain loads applied to one port can become unstable due to internal feedback and show negative resistance at the other port. So high frequency transistor oscillators are designed by applying a reactive load to one port to give the transistor negative resistance, and connecting the other port across a resonator to make a negative resistance oscillator as described below.\n\nThe common Gunn diode oscillator \"(circuit diagrams)\" illustrates how negative resistance oscillators work. The diode \"D\" has voltage controlled (\"N\" type) negative resistance and the voltage source formula_112 biases it into its negative resistance region where its differential resistance is formula_113. The choke \"RFC\" prevents AC current from flowing through the bias source. formula_103 is the equivalent resistance due to damping and losses in the series tuned circuit formula_115, plus any load resistance. Analyzing the AC circuit with Kirchhoff's Voltage Law gives a differential equation for formula_116, the AC current\nSolving this equation gives a solution of the form\nThis shows that the current through the circuit, formula_116, varies with time about the DC Q point, formula_121. When started from a nonzero initial current formula_122 the current oscillates sinusoidally at the resonant frequency ω of the tuned circuit, with amplitude either constant, increasing, or decreasing exponentially, depending on the value of α. Whether the circuit can sustain steady oscillations depends on the balance between formula_103 and formula_124, the positive and negative resistance in the circuit:\n\nPractical oscillators are designed in region (3) above, with net negative resistance, to get oscillations started. A widely used rule of thumb is to make formula_129. When the power is turned on, electrical noise in the circuit provides a signal formula_130 to start spontaneous oscillations, which grow exponentially. However, the oscillations cannot grow forever; the nonlinearity of the diode eventually limits the amplitude.\n\nAt large amplitudes the circuit is nonlinear, so the linear analysis above does not strictly apply and differential resistance is undefined; but the circuit can be understood by considering formula_124 to be the \"average\" resistance over the cycle. As the amplitude of the sine wave exceeds the width of the negative resistance region and the voltage swing extends into regions of the curve with positive differential resistance, the average negative differential resistance formula_124 becomes smaller, and thus the total resistance formula_133 and the damping formula_134 becomes less negative and eventually turns positive. Therefore, the oscillations will stabilize at the amplitude at which the damping becomes zero, which is when formula_135.\n\nGunn diodes have negative resistance in the range −5 to −25 ohms. In oscillators where formula_103 is close to formula_124; just small enough to allow the oscillator to start, the voltage swing will be mostly limited to the linear portion of the \"I–V\" curve, the output waveform will be nearly sinusoidal and the frequency will be most stable. In circuits in which formula_103 is far below formula_124, the swing extends further into the nonlinear part of the curve, the clipping distortion of the output sine wave is more severe, and the frequency will be increasingly dependent on the supply voltage.\n\nNegative resistance oscillator circuits can be divided into two types, which are used with the two types of negative differential resistance – voltage controlled (VCNR), and current controlled (CCNR)\n\nMost oscillators are more complicated than the Gunn diode example, since both the active device and the load may have reactance (\"X\") as well as resistance (\"R\"). Modern negative resistance oscillators are designed by a frequency domain technique due to K. Kurokawa. The circuit diagram is imagined to be divided by a \"reference plane\" \"(red)\" which separates the negative resistance part, the active device, from the positive resistance part, the resonant circuit and output load \"(right)\". The complex impedance of the negative resistance part formula_140 depends on frequency \"ω\" but is also nonlinear, in general declining with the amplitude of the AC oscillation current \"I\"; while the resonator part formula_141 is linear, depending only on frequency. The circuit equation is formula_142 so it will only oscillate (have nonzero \"I\") at the frequency \"ω\" and amplitude \"I\" for which the total impedance formula_143 is zero. This means the magnitude of the negative and positive resistances must be equal, and the reactances must be conjugate\nFor steady-state oscillation the equal sign applies. During startup the inequality applies, because the circuit must have excess negative resistance for oscillations to start.\n\nAlternately, the condition for oscillation can be expressed using the reflection coefficient. The voltage waveform at the reference plane can be divided into a component \"V\" travelling toward the negative resistance device and a component \"V\" travelling in the opposite direction, toward the resonator part. The reflection coefficient of the active device formula_146 is greater than one, while that of the resonator part formula_147 is less than one. During operation the waves are reflected back and forth in a round trip so the circuit will oscillate only if\nAs above, the equality gives the condition for steady oscillation, while the inequality is required during startup to provide excess negative resistance. The above conditions are analogous to the Barkhausen criterion for feedback oscillators; they are necessary but not sufficient, so there are some circuits that satisfy the equations but do not oscillate. Kurokawa also derived more complicated sufficient conditions, which are often used instead.\n\nNegative differential resistance devices such as Gunn and IMPATT diodes are also used to make amplifiers, particularly at microwave frequencies, but not as commonly as oscillators. Because negative resistance devices have only one \"port\" (two terminals), unlike two-port devices such as transistors, the outgoing amplified signal has to leave the device by the same terminals as the incoming signal enters it. Without some way of separating the two signals, a negative resistance amplifier is \"bilateral\"; it amplifies in both directions, so it suffers from sensitivity to load impedance and feedback problems. To separate the input and output signals, many negative resistance amplifiers use nonreciprocal devices such as isolators and directional couplers.\n\nOne widely used circuit is the \"reflection amplifier\" in which the separation is accomplished by a \"circulator\". A circulator is a nonreciprocal solid-state component with three ports (connectors) which transfers a signal applied to one port to the next in only one direction, port 1 to port 2, 2 to 3, and 3 to 1. In the reflection amplifier diagram the input signal is applied to port 1, a biased VCNR negative resistance diode \"N\" is attached through a filter \"F\" to port 2, and the output circuit is attached to port 3. The input signal is passed from port 1 to the diode at port 2, but the outgoing \"reflected\" amplified signal from the diode is routed to port 3, so there is little coupling from output to input. The characteristic impedance formula_149 of the input and output transmission lines, usually 50Ω, is matched to the port impedance of the circulator. The purpose of the filter \"F\" is to present the correct impedance to the diode to set the gain. At radio frequencies NR diodes are not pure resistive loads and have reactance, so a second purpose of the filter is to cancel the diode reactance with a conjugate reactance to prevent standing waves.\n\nThe filter has only reactive components and so does not absorb any power itself, so power is passed between the diode and the ports without loss. The input signal power to the diode is\nThe output power from the diode is\nSo the power gain formula_152 of the amplifier is the square of the reflection coefficient\n\nformula_156 is the negative resistance of the diode −r. Assuming the filter is matched to the diode so formula_157 then the gain is\nThe VCNR reflection amplifier above is stable for formula_159. while a CCNR amplifier is stable for formula_160. It can be seen that the reflection amplifier can have unlimited gain, approaching infinity as formula_95 approaches the point of oscillation at formula_124. This is a characteristic of all NR amplifiers, contrasting with the behavior of two-port amplifiers, which generally have limited gain but are often unconditionally stable. In practice the gain is limited by the backward \"leakage\" coupling between circulator ports.\n\nMasers and parametric amplifiers are extremely low noise NR amplifiers that are also implemented as reflection amplifiers; they are used in applications like radio telescopes.\n\nNegative differential resistance devices are also used in switching circuits in which the device operates nonlinearly, changing abruptly from one state to another, with hysteresis. The advantage of using a negative resistance device is that a relaxation oscillator, flip-flop or memory cell can be built with a single active device, whereas the standard logic circuit for these functions, the Eccles-Jordan multivibrator, requires two active devices (transistors). Three switching circuits built with negative resistances are\n\nSome instances of neurons display regions of negative slope conductances (RNSC) in voltage-clamp experiments. The negative resistance here is implied were one to consider the neuron a typical Hodgkin–Huxley style circuit model.\n\nNegative resistance was first recognized during investigations of electric arcs, which were used for lighting during the 19th century. In 1881 Alfred Niaudet had observed that the voltage across arc electrodes decreased temporarily as the arc current increased, but many researchers thought this was a secondary effect due to temperature. The term \"negative resistance\" was applied by some to this effect, but the term was controversial because it was known that the resistance of a passive device could not be negative. Beginning in 1895 Hertha Ayrton, extending her husband William's research with a series of meticulous experiments measuring the \"I–V\" curve of arcs, established that the curve had regions of negative slope, igniting controversy. Frith and Rodgers in 1896 with the support of the Ayrtons introduced the concept of \"differential\" resistance, \"dv/di\", and it was slowly accepted that arcs had negative differential resistance. In recognition of her research, Hertha Ayrton became the first woman voted for induction into the Institute of Electrical Engineers.\n\nGeorge Francis FitzGerald first realized in 1892 that if the damping resistance in a resonant circuit could be made zero or negative, it would produce continuous oscillations. In the same year Elihu Thomson built a negative resistance oscillator by connecting an LC circuit to the electrodes of an arc, perhaps the first example of an electronic oscillator. William Duddell, a student of Ayrton at London Central Technical College, brought Thomson's arc oscillator to public attention. Due to its negative resistance, the current through an arc was unstable, and arc lights would often produce hissing, humming, or even howling noises. In 1899, investigating this effect, Duddell connected an LC circuit across an arc and the negative resistance excited oscillations in the tuned circuit, producing a musical tone from the arc. To demonstrate his invention Duddell wired several tuned circuits to an arc and played a tune on it. Duddell's \"singing arc\" oscillator was limited to audio frequencies. However, in 1903 Danish engineers Valdemar Poulsen and P. O. Pederson increased the frequency into the radio range by operating the arc in a hydrogen atmosphere in a magnetic field, inventing the Poulsen arc radio transmitter, which was widely used until the 1920s.\n\nBy the early 20th century, although the physical causes of negative resistance were not understood, engineers knew it could generate oscillations and had begun to apply it. Heinrich Barkhausen in 1907 showed that oscillators must have negative resistance. Ernst Ruhmer and Adolf Pieper discovered that mercury vapor lamps could produce oscillations, and by 1912 AT&T had used them to build amplifying repeaters for telephone lines.\n\nIn 1918 Albert Hull at GE discovered that vacuum tubes could have negative resistance in parts of their operating ranges, due to a phenomenon called secondary emission. In a vacuum tube when electrons strike the plate electrode they can knock additional electrons out of the surface into the tube. This represents a current \"away\" from the plate, reducing the plate current. Under certain conditions increasing the plate voltage causes a \"decrease\" in plate current. By connecting an LC circuit to the tube Hull created an oscillator, the dynatron oscillator. Other negative resistance tube oscillators followed, such as the magnetron invented by Hull in 1920.\n\nThe negative impedance converter originated from work by Marius Latour around 1920. He was also one of the first to report negative capacitance and inductance. A decade later, vacuum tube NICs were developed as telephone line repeaters at Bell Labs by George Crisson and others, which made transcontinental telephone service possible. Transistor NICs, pioneered by Linvill in 1953, initiated a great increase in interest in NICs and many new circuits and applications developed.\n\nNegative differential resistance in semiconductors was observed around 1909 in the first point-contact junction diodes, called cat's whisker detectors, by researchers such as William Henry Eccles and G. W. Pickard. They noticed that when junctions were biased with a DC voltage to improve their sensitivity as radio detectors, they would sometimes break into spontaneous oscillations. However the effect was not pursued.\n\nThe first person to exploit negative resistance diodes practically was Russian radio researcher Oleg Losev, who in 1922 discovered negative differential resistance in biased zincite (zinc oxide) point contact junctions. He used these to build solid-state amplifiers, oscillators, and amplifying and regenerative radio receivers, 25 years before the invention of the transistor. Later he even built a superheterodyne receiver. However his achievements were overlooked because of the success of vacuum tube technology. After ten years he abandoned research into this technology (dubbed \"Crystodyne\" by Hugo Gernsback), and it was forgotten.\n\nThe first widely used solid-state negative resistance device was the tunnel diode, invented in 1957 by Japanese physicist Leo Esaki. Because they have lower parasitic capacitance than vacuum tubes due to their small junction size, diodes can function at higher frequencies, and tunnel diode oscillators proved able to produce power at microwave frequencies, above the range of ordinary vacuum tube oscillators. Its invention set off a search for other negative resistance semiconductor devices for use as microwave oscillators, resulting in the discovery of the IMPATT diode, Gunn diode, TRAPATT diode, and others. In 1969 Kurokawa derived conditions for stability in negative resistance circuits. Currently negative differential resistance diode oscillators are the most widely used sources of microwave energy, and many new negative resistance devices have been discovered in recent decades.\n\n"}
{"id": "32539851", "url": "https://en.wikipedia.org/wiki?curid=32539851", "title": "NexTraq", "text": "NexTraq\n\nNexTraq is a fleet tracking company based in Atlanta, Georgia, United States. Acquired by Francisco Partners in 2009, NexTraq is a provider of Global Positioning System vehicle management and fleet tracking solutions. The NexTraq Fleet Tracking platform (formerly MARCUS ) is a cloud-based application for service and distribution businesses to optimize fleet operations. The suite of applications—Fleet Dispatch, Fleet Metrics and Fleet Mobile—gives customers the ability to manage their fleet more efficiently. Reporting functionality delivers key performance indicators (KPIs).\n\nOn Oct 30, 2013, NexTraq was acquired by FleetCor Technologies, Inc.\n\nOn June 14, 2017, it was announced that Michelin acquired the company. \n"}
{"id": "27255859", "url": "https://en.wikipedia.org/wiki?curid=27255859", "title": "Noctua (company)", "text": "Noctua (company)\n\nNoctua is an Austrian computer hardware manufacturer of CPU coolers and computer fans primarily for the enthusiast market. The company was founded in 2005 and is a joint venture between Austrian company Rascom Computer Distribution GmbH and the Taiwanese Kolink International Corporation.\n\nNoctua is a highly regarded manufacturer in the enthusiast market and has, according to their official website, received more than 3,000 awards for their products.\n\nThe name \"Noctua\" refers to the scientific name of the little owl, \"Athene noctua\", which in Greek mythology stands for intelligence and wisdom, hence the owl in the logo.\n\n\n"}
{"id": "43098039", "url": "https://en.wikipedia.org/wiki?curid=43098039", "title": "Nuala O'Connor", "text": "Nuala O'Connor\n\nNuala O'Connor is the current President and CEO of the Center for Democracy and Technology (CDT). O'Connor is an expert on technology policy, privacy, and information governance. From 2003–2005, O'Connor served as the First Chief Privacy Officer for the US Department of Homeland Security.\n\nNuala O'Connor was born in Belfast, Northern Ireland in 1966 and grew up in New York City. In 1985, O'Connor attended Princeton University, where she was a member of the Princeton Tigerlilies, Princeton Triangle Club, and Cloister Inn. O'Connor graduated from Princeton University in 1989 with an AB in American Studies and English. O'Connor has also earned an M.Ed in Administration, Planning & Social Policy from Harvard University and a J.D. from Georgetown University Law Center.\n\nCivil and digital rights advocacy groups such as the Electronic Privacy Information Center and the ACLU praised O'Connor for protecting American's privacy rights as the first Chief Privacy Officer for the US Department of Homeland Security, a position in which she was responsible for evaluating the department for privacy impacts and \"mak[ing] sure that privacy is considered and is codified\". In the public sector, O'Connor has experience working on global technology policy for the US Department of Commerce as its Deputy Director of the Office of Policy & Strategic Planning, Chief Privacy Officer, and Chief Counsel for Technology.\n\nO'Connor has held numerous positions in the private sector. At Amazon.com, O'Connor worked as the Vice President of Compliance & Customer Trust and Associate General Counsel for Privacy & Data Protection. At General Electric, O'Connor served as the Global Privacy Leader and was responsible for privacy policy and practices. O'Connor has also worked as legal counsel to DoubleClick, managing class actions and settlements.\n\nCDT's former President & CEO, Leslie Helm, said of O'Connor, \"Nuala is a brilliant choice to lead CDT. She is a passionate advocate for civil liberties, highly expert about the emerging global challenges and fully committed to CDT's mission. She is a bold leader who will guide CDT into its next chapter. I have had the honor of working with CDT's talented and thoughtful team for almost nine years. I am confident that they will thrive with Nuala at the helm.\"\n\nO'Connor is a recipient of numerous awards, including the 2005 IAPP Vanguard Award, the 2010 Executive Women's Forum's Woman of Influence award, and was named to the Federal 100. O'Connor has also been named \"Geek of the Week\" by the Minority Media & Telecom Council. She also serves on a number of boards and advisory committees, including those of the Data Quality Campaign and the National Cyber Security Alliance's StaySafeOnline.\n\n\n"}
{"id": "3112392", "url": "https://en.wikipedia.org/wiki?curid=3112392", "title": "Nuclear reactor core", "text": "Nuclear reactor core\n\nA nuclear reactor core is the portion of a nuclear reactor containing the nuclear fuel components where the nuclear reactions take place and the heat is generated. Typically, the fuel will be low-enriched uranium contained in thousands of individual fuel pins. The core also contains structural components, the means to both moderate the neutrons and control the reaction, and the means to transfer the heat from the fuel to where it is required, outside the core.\n\nInside the core of a typical pressurized water reactor or boiling water reactor are nuclear fuel rods equivalent to the diameter of a large gel type ink-pen, each about 4 m long, which are grouped by the hundreds in bundles called \"fuel assemblies\". Inside each fuel rod, pellets of uranium, or more commonly uranium oxide, are stacked end to end. Also inside the core are control rods, filled with pellets of substances like boron or hafnium or cadmium that readily capture neutrons. When the control rods are lowered into the core, they absorb neutrons, which thus cannot take part in the chain reaction. Conversely, when the control rods are lifted out of the way, more neutrons strike the fissile uranium-235 (U-235) or plutonium-239 (Pu-239) nuclei in nearby fuel rods, and the chain reaction intensifies. The core shroud, also located inside of the reactor, directs the water flow to cool the nuclear reactions inside of the core.\n\nThe heat of the fission reaction is removed by the water, which also acts to moderate the neutron reactions.\n\nAn alternative form of nuclear fuel would be fissile uranium-233 (U-233) made by the neutron-bombardment of the common thorium-232.\n\nThere are also Graphite moderated reactors in use.\n\nOne type uses solid graphite for the neutron moderator and ordinary water for the coolant. See the Soviet-made RBMK nuclear-power reactor. This was the type of reactor involved in the Chernobyl disaster. \n\nIn the advanced gas-cooled reactor, a British design, the core is made of a graphite neutron moderator where the fuel assemblies are located. Carbon dioxide gas acts as a coolant and it circulates through the core, removing heat.\n\nThere have also been several experimental reactors that use graphite for moderation, such as the pebble bed reactor concepts and the molten-salt reactor experiment.\n\n\n"}
{"id": "9940326", "url": "https://en.wikipedia.org/wiki?curid=9940326", "title": "RGX", "text": "RGX\n\nRGX was a body spray owned by Dial Corporation and launched in January 2007. The product was an attempt by Dial to break into the body spray market, currently dominated by AXE and Tag, through a marketing strategy directed at older men. RGX employed the Internet to create buzz via a men's lifestyle website called RGX Life (www.rgxlife.com), followed by television, print, and public relations activities. Television and online banners feature actress Rachel Specter. In 2009, the body spray line was discontinued.\n\nRGX had six fragrance variants:\n\n\nhttp://lifestylemonster.com/\n"}
{"id": "49821823", "url": "https://en.wikipedia.org/wiki?curid=49821823", "title": "RSRCHXchange", "text": "RSRCHXchange\n\nResearch Exchange Ltd is a FinTech company servicing the global asset management industry operating under the trading name RSRCHXchange. Its platform, RSRCHX, is an online marketplace for unbundled financial research. RSRCHX provides asset management firms with a cloud-based repository of reports. Launched in September 2015, RSRCHX incorporates elasticsearch, compliance checks and Commission Sharing Agreement (CSA) and credit card payment administration. RSRCHXchange is one of a number of FinTech start-ups offering products which relate to MiFID II (Markets in Financial Instruments Directive), the European Union financial reforms intended as a response to the financial crisis to improve the functioning of financial markets and enhance investor protection. RSRCHXchange's technology differs from other financial services vendors because its research catalogue is not dependent on RIXML, the industry-developed language for tagging documents. RSRCHXchange specialises in research unbundling, one of the most contentious topics of the regulation.\n\nGlobally $20bn a year is spent on research by asset managers and in the UK alone, the FCA estimates $4.5bn is spent on research. Investment banks being the predominant supplier of external research to most investment managers. This effective oligopoly is the result of bundled commission rates where all services, including research, are paid for at the point of trading exclusively to the executing counterparty. Therefore, the investment banks who enjoy the largest trading volumes have also captured a dominant share of research payments.\n\nThe UK FCA (Financial Conduct Authority) and ESMA (European Securities and Markets Authority) have been debating reforms to the economic structure of investment research for a decade. The dot com boom of 2000 made it abundantly clear to regulators that equity research, investors, fund managers and public companies were too heavily interdependent. The US responded by imposing restrictions on sell side analyst activities, while the UK placed the responsibility on the buy side through the Myners Report (2001), which recommended changes in commission and investment research payments – effectively unbundling. The FCA has consistently argued that unbundling research from dealing commissions would drive both price efficiency and market transparency and at the same time enhance competition.\n\nIn 2006, the introduction of commission sharing agreements (CSAs), which enabled fund managers for the first time to instruct their broker to pay away commissions to other research entities, was a further crucial step towards research unbundling. Given the UK market’s global significance, and the international nature of the brokerage market, this approach has gained traction internationally beyond the UK and represents a significant change in the market for investment research.\n\nMiFID II has the potential to be the most significant catalyst for change in the market for investment research in decades. Crucially, the implementation of MiFID II, which is currently scheduled for January 2018, will require all sell side research to be individually priced and thus effect full price transparency. Investment firms will be required to set and assess research budgets and MiFID II states that that payment must be direct, out of the firm’s own resources, without any undue delay. Full unbundling will finally be achieved as fund management firms will be required to create research payment accounts (RPAs) based on a pre-set budget independent of trading activity. This has met with heavy criticism and lobbying from a number of industry organisations. The MiFID II text also includes further demands of buy side firms which will require buyers and sellers to review and revise the research procurement process. The FCA has already issued a statement of “support” for ESMA’s new policies for investment research and advising asset management firms to start making changes now to prepare themselves for the inevitable regulatory change. While MiFID II’s scope is pan-European only, the regulator recognises that its proposed changes may resonate on a global scale as asset managers are likely to adopt a common system across their global businesses.\n\nCommission sharing agreements (CSA) allow asset management firms to pay out a portion of commissions to research providers instead of the executing broker retaining the entire commission, Commission Sharing Agreements (CSAs) have expanded the content available to investment managers. CSAs have gained traction as a tool giving asset managers greater control over their research payments and enabling them to expand their range of research providers to include independent research providers (IRPs).\n\nCSA penetration rates vary significantly across Europe but are highest in the UK, recently reaching 70%. In the US, the use of Client Commission Arrangements (CSA equivalent) is well-entrenched and these two geographies have leading IRP presence. There are estimates of between 2,000 and 6,000 IRPs. Research has shown that the introduction of CSAs has enabled fund managers to access a broader range of research, providing more valuable inputs to their investment process and thus benefiting the end investor.\n"}
{"id": "10673649", "url": "https://en.wikipedia.org/wiki?curid=10673649", "title": "Robert Cornog", "text": "Robert Cornog\n\nRobert Alden Cornog (July 7, 1912 – July 17, 1998), was a physicist and engineer who helped develop the atomic bomb and missile systems from the Snark to the Minuteman.\n\nA native of Portland, Oregon, who grew up in Iowa City, Cornog earned a bachelor's degree in mechanical engineering at the University of Iowa. After working for the United States Bureau of Reclamation on the Boulder Dam design, he studied at UC Berkeley for his doctorate in physics.\n\nHis graduate student research led to the co-discovery, with Luis Alvarez, of hydrogen and helium of atomic mass 3 (tritium and helium-3). He also assisted Emilio Segrè in the discovery of element 85, astatine.\n\nDuring World War II, Cornog designed magnetic equipment for ships and went to work on the Manhattan Project, successively at UC Berkeley, Princeton University and in Los Alamos, New Mexico. Cornog became chief engineer of the ordnance division of the atomic bomb development team and was involved in the development of the bomb's trigger mechanism.\n\nIn the 1950s, he focused on aerodynamics, nuclear energy and rocket engineering, working on missile systems for several Southern California companies, including Northrop, Space Technology Laboratories and Ramo-Wooldridge Corporation, which became TRW. Also an expert on vacuum technology, Cornog headed Vacuum Enterprises from 1967 to 1974 and managed product development for Torr Vacuum Products until 1984. He held several patents and served as a technical advisor on the film \"Fat Man and Little Boy,\" about the atomic bomb.\n\nEnvisioning peaceful uses for nuclear and space technology, Cornog in 1959 foresaw a world in 40 to 50 years with worldwide color television broadcasts, satellites assembled in space and accurate weather prediction.\n\nCornog was a close associate of rocket pioneer and occultist Jack Parsons. Science fiction author Robert A. Heinlein, a friend, dedicated his novel \"Stranger in a Strange Land\" to Cornog. Donald Kingsbury dedicated his novel \"The Moon Goddess and the Son\" to several people including \"Robert Cornog for discussing the economics of the leoport.\"\n\n\n"}
{"id": "4759545", "url": "https://en.wikipedia.org/wiki?curid=4759545", "title": "Rocket engine nozzle", "text": "Rocket engine nozzle\n\nA rocket engine nozzle is a propelling nozzle (usually of the de Laval type) used in a rocket engine to expand and accelerate the combustion gases produced by burning propellants so that the exhaust gases exit the nozzle at hypersonic velocities.\n\nSimply: the rocket (pumps and a combustion chamber) generates high pressure, a few hundred atmospheres. The nozzle turns the static high pressure high temperature gas into rapidly moving gas at near-ambient pressure.\n\nThe de Laval nozzle was originally developed in the 19th century by Gustaf de Laval for use in steam turbines. It was first used in an early rocket engine developed by Robert Goddard, one of the fathers of modern rocketry. It has since been used in almost all rocket engines, including Walter Theill's implementation, which made possible Germany's V-2 rocket.\n\nThe optimal size of a rocket engine nozzle to be used within the atmosphere is achieved when the exit pressure equals ambient (atmospheric) pressure, which decreases with altitude. For rockets travelling from the Earth to orbit, a simple nozzle design is only optimal at one altitude, losing efficiency and wasting fuel at other altitudes.\n\nJust past the throat, the pressure of the gas is higher than ambient pressure and needs to be lowered between the throat and the nozzle exit by expansion. If the pressure of the jet leaving the nozzle exit is still above ambient pressure, then a nozzle is said to be \"underexpanded\"; if the jet is below ambient pressure, then it is \"overexpanded\".\n\nSlight overexpansion causes a slight reduction in efficiency, but otherwise does little harm. However, if the exit pressure is less than approximately 40% that of ambient, then \"flow separation\" occurs. This can cause jet instabilities that can cause damage to the nozzle or simply cause control difficulties of the vehicle or the engine.\n\nIn some cases it is desirable for reliability and safety reasons to ignite a rocket engine on the ground that will be used all the way to orbit. For optimal liftoff performance, the pressure of the gases exiting nozzle should be at sea-level pressure; however, if a rocket engine is primarily designed for use at high altitudes and is only providing additional thrust to another \"first-stage\" engine during liftoff in a multi-stage design, then designers will usually opt for an overexpanded nozzle (at sea-level) design, making it more efficient at higher altitudes, where the ambient pressure is lower. This was the technique employed on the Space shuttle's main engines, which spent most of their powered trajectory in near-vacuum, while the shuttle's two solid rocket boosters provided the majority of the liftoff thrust.\n\nFor nozzles that are used in vacuum or at very high altitude, it is impossible to match ambient pressure; rather, nozzles with larger area ratio are usually more efficient. However, a very long nozzle has significant mass, a drawback in and of itself. A length that optimises overall vehicle performance typically has to be found. Additionally, as the temperature of the gas in the nozzle decreases, some components of the exhaust gases (such as water vapour from the combustion process) may condense or even freeze. This is highly undesirable and needs to be avoided.\n\nMagnetic nozzles have been proposed for some types of propulsion (for example VASIMR), in which the flow of plasma or ions are directed by magnetic fields instead of walls made of solid materials. These can be advantageous, since a magnetic field itself cannot melt, and the plasma temperatures can reach millions of kelvins. However, there are often thermal design challenges presented by the coils themselves, particularly if superconducting coils are used to form the throat and expansion fields.\n\nThe analysis of gas flow through de Laval nozzles involves a number of concepts and simplifying assumptions:\n\n\nAs the combustion gas enters the rocket nozzle, it is traveling at subsonic velocities. As the throat constricts, the gas is forced to accelerate until at the nozzle throat, where the cross-sectional area is the least, the linear velocity becomes sonic. From the throat the cross-sectional area then increases, the gas expands and the linear velocity becomes progressively more supersonic.\n\nThe linear velocity of the exiting exhaust gases can be calculated using the following equation \n\nSome typical values of the exhaust gas velocity \"v\" for rocket engines burning various propellants are:\n\n\nAs a note of interest, v is sometimes referred to as the \"ideal exhaust gas velocity\" because it based on the assumption that the exhaust gas behaves as an ideal gas.\n\nAs an example calculation using the above equation, assume that the propellant combustion gases are: at an absolute pressure entering the nozzle of p = 7.0 MPa and exit the rocket exhaust at an absolute pressure of p = 0.1 MPa; at an absolute temperature of T = 3500 K; with an isentropic expansion factor of γ = 1.22 and a molar mass of M = 22 kg/kmol. Using those values in the above equation yields an exhaust velocity v = 2802 m/s or 2.80 km/s which is consistent with above typical values.\n\nThe technical literature can be very confusing because many authors fail to explain whether they are using the universal gas law constant R which applies to any ideal gas or whether they are using the gas law constant R which only applies to a specific individual gas. The relationship between the two constants is R = R/M, where R is the universal gas constant, and M is the molar mass of the gas.\n\nThrust is the force that moves a rocket through the air or space. Thrust is generated by the propulsion system of the rocket through the application of Newton's third law of motion: \"For every action there is an equal and opposite reaction\". A gas or working fluid is accelerated out the rear of the rocket engine nozzle, and the rocket is accelerated in the opposite direction. The thrust of a rocket engine nozzle can be defined as:\n\nand for perfectly expanded nozzles, this reduces to:\n\nThe specific impulse formula_4 is the ratio of the thrust produced to the weight flow of the propellants. It is a measure of the fuel efficiency of a rocket engine. In English Engineering units it can be obtained as\n\nIn certain cases, where formula_6 equals formula_7, the formula becomes\n\nIn cases where this may not be so, since for a rocket nozzle formula_6 is proportional to formula_10, it is possible to define a constant quantity that is the vacuum formula_11 for any given engine thus:\n\nand hence:\n\nwhich is simply the vacuum thrust minus the force of the ambient atmospheric pressure acting over the exit plane.\n\nEssentially then, for rocket nozzles, the ambient pressure acting on the engine cancels except over the exit plane of the rocket engine in a rearward direction, while the exhaust jet generates forward thrust.\n\nAs the gas travels down the expansion part of the nozzle, the pressure and temperature decrease, while the speed of the gas increases.\n\nThe supersonic nature of the exhaust jet means that the pressure of the exhaust can be significantly different from ambient pressure – the outside air is unable to equalize the pressure upstream due to the very high jet velocity. Therefore, for supersonic nozzles, it is actually possible for the pressure of the gas exiting the nozzle to be significantly below or very greatly above ambient pressure.\n\nIf the exit pressure is too low, then the jet can separate from the nozzle. This is often unstable, and the jet will generally cause large off-axis thrusts and may mechanically damage the nozzle.\n\nThis separation generally occurs if the exit pressure drops below roughly 30–45% of ambient, but separation may be delayed to far lower pressures if the nozzle is designed to increase the pressure at the rim, as is achieved with the SSME (1–2 psi at 15 psi ambient).\n\nIn addition, as the rocket engine starts up or throttles, the chamber pressure varies, and this generates different levels of efficiency. At low chamber pressures the engine is almost inevitably going to be grossly over-expanded.\n\nThe ratio of the area of the narrowest part of the nozzle to the exit plane area is mainly what determines how efficiently the expansion of the exhaust gases is converted into linear velocity, the exhaust velocity, and therefore the thrust of the rocket engine. The gas properties have an effect as well.\n\nThe shape of the nozzle also modestly affects how efficiently the expansion of the exhaust gases is converted into linear motion. The simplest nozzle shape has a ~15° cone half-angle, which is about 98% efficient. Smaller angles give very slightly higher efficiency, larger angles give lower efficiency.\n\nMore complex shapes of revolution are frequently used, such as Bell nozzles or parabolic shapes. These give perhaps 1% higher efficiency than the cone nozzle and can be shorter and lighter. They are widely used on launch vehicles and other rockets where weight is at a premium. They are, of course, harder to fabricate, so are typically more costly.\n\nThere is also a theoretically optimal nozzle shape for maximal exhaust speed. However, a shorter bell shape is typically used, which gives better overall performance due to its much lower weight, shorter length, lower drag losses, and only very marginally lower exhaust speed.\n\nOther design aspects affect the efficiency of a rocket nozzle. The nozzle's throat should have a smooth radius. The internal angle that narrows to the throat also has an effect on the overall efficiency, but this is small. The exit angle of the nozzle needs to be as small as possible (about 12°) in order to minimize the chances of separation problems at low exit pressures.\n\nA number of more sophisticated designs have been proposed for altitude compensation and other uses.\n\nNozzles with an atmospheric boundary include:\nEach of these allows the supersonic flow to adapt to the ambient pressure by expanding or contracting, thereby changing the exit ratio so that it is at (or near) optimal exit pressure for the corresponding altitude. The plug and aerospike nozzles are very similar in that they are radial in-flow designs but plug nozzles feature a solid centerbody (sometimes truncated) and aerospike nozzles have a \"base-bleed\" of gases to simulate a solid center-body. ED nozzles are radial out-flow nozzles with the flow deflected by a center pintle.\n\nControlled flow-separation nozzles include:\nThese are generally very similar to bell nozzles but include an insert or mechanism by which the exit area ratio can be increased as ambient pressure is reduced.\n\nDual-mode nozzles include:\nThese have either two throats or two thrust chambers (with corresponding throats). The central throat is of a standard design and is surrounded by an annular throat, which exhausts gases from the same (dual-throat) or a separate (dual-expander) thrust chamber. Both throats would, in either case, discharge into a bell nozzle. At higher altitudes, where the ambient pressure is lower, the central nozzle would be shut off, reducing the throat area and thereby increasing the nozzle area ratio. These designs require additional complexity, but an advantage of having two thrust chambers is that they can be configured to burn different propellants or different fuel mixture ratios. Similarly, Aerojet has also designed a nozzle called the \"Thrust Augmented Nozzle\", which injects propellant and oxidiser directly into the nozzle section for combustion, allowing larger area ratio nozzles to be used deeper in an atmosphere than they would without augmentation due to effects of flow separation. They would again allow multiple propellants to be used (such as RP-1), further increasing thrust.\n\nLiquid injection thrust vectoring nozzles are another advanced design that allow pitch and yaw control from un-gimbaled nozzles. India's PSLV calls its design \"Secondary Injection Thrust Vector Control System\"; strontium perchlorate is injected through various fluid paths in the nozzle to achieve the desired control. Some ICBMs and boosters, such as the Titan IIIC and Minuteman II, use similar designs.\n\n\n\n<br>\n"}
{"id": "27341944", "url": "https://en.wikipedia.org/wiki?curid=27341944", "title": "Seismic interferometry", "text": "Seismic interferometry\n\nInterferometry examines the general interference phenomena between pairs of signals in order to gain useful information about the subsurface. Seismic interferometry (SI) utilizes the crosscorrelation of signal pairs to reconstruct the impulse response of a given media. Papers by Keiiti Aki (1957), Géza Kunetz, and Jon Claerbout (1968) helped develop the technique for \"seismic\" applications and provided the framework upon which modern theory is based.\n\nA signal at a location A can be crosscorrelated with a signal at a location B to reproduce a virtual source-receiver pair using seismic interferometry. Crosscorrelation is often considered the key mathematical operation in this approach, but it is also possible to use convolution to come up with a similar result. The crosscorrelation of passive noise measured at a free surface reproduces the subsurface impulse response. As such, it is possible to obtain information about the subsurface with no need for an active seismic source. This method, however, is not limited to passive sources, and can be extended for use with active sources and computer–generated waveforms.\nAs of 2006 the field of seismic interferometry was beginning to change the way geophysicists view noise. Seismic interferometry uses this previously–ignored noise in models of the shallow subsurface. Potential applications include both research and industry.\n\nClaerbout (1968) developed a workflow to apply existing interferometry techniques to investigating the shallow subsurface, although it was not proven until later that seismic interferometry could be applied to real world media. \nThe long term average of random ultrasound waves can reconstruct the impulse response between two points on an aluminum block. However, they had assumed random diffuse noise, limiting interferometry in real world conditions. In a similar case, it was shown that the expressions for uncorrelated noise sources reduce to a single crosscorrelation of observations at two receivers. The interferometric impulse response of the subsurface can be reconstructed using only an extended record of background noise, initially only for the surface and direct wave arrivals.\n\nCrosscorrelations of seismic signals from both active and passive sources at the surface or in the subsurface can be used to reconstruct a valid model of the subsurface. Seismic interferometry can produce a result similar to traditional methods without limitations on the diffusivity of the wavefield or ambient sources. In a drilling application, it is possible to utilize a virtual source to image the subsurface adjacent to a downhole location. This application is increasingly utilized particularly for exploration in subsalt settings.\n\nSeismic interferometry provides for the possibility of reconstructing the subsurface reflection response using the crosscorrelations of two seismic traces. Recent work has mathematically demonstrated applications of crosscorrelation for reconstructing Green's function using wave field reciprocity theorem in a lossless, 3D heterogeneous medium. Traces are most often extended records of passive background noise, but it is also possible to utilize active sources depending on the objective. Seismic interferometry essentially exploits the phase difference between adjacent receiver locations to image the subsurface.\n\nSeismic interferometry consists of simple crosscorrelation and stacking of actual receiver responses to approximate the impulse response as if a virtual source was placed at the location of the applicable receiver. Crosscorrelation of continuous functions in the time domain is presented as Equation 1.\n\n(f 1∗f 2)(t)= ∫ f 1(λ)f 2(λ-t)dλ\n\nWhere the functions are integrated as a function of time at different lag values. In fact, crosscorrelation can be understood conceptually as the traveltime lag associated with waveforms in two discrete receiver locations. Crosscorrelation is similar to convolution where the second function is folded relative to the first.\n\nSeismic interferometry is fundamentally similar to the optical interferogram produced by the interference of a direct and reflected wave passing through a glass lens where intensity is primarily dependent upon the phase component. \nI = 1+2R2 cos[ω(λAr+λrB)]+R^4\n\nWhere:\nIntensity is related to the magnitude of the reflection coefficient (R) and the phase component ω(λAr+λrB). An estimate of the reflectivity distributions can be obtained through the crosscorrelation of the direct wave at a location A with the reflection recorded at a location B where A represents the reference trace. The multiplication of the conjugate of the trace spectrum at A and the trace spectrum at B gives:\n\nФAB =Re^iω(λAr+λrB) + o.t.\n\nWhere:\nФAB = product spectrum\no.t. = additional terms, e.g. correlations of direct-direct, etc. As in the previous case, the product spectrum is a function of phase.\n\nKey: Changes in reflector geometry lead to changes in the correlation result and the reflector geometry can be recovered through the application of a migration kernel. Interpretation of raw interferograms is not normally attempted; crosscorrelated results are generally processed using some form of migration.\n\nIn the simplest case, consider a rotating drill bit at depth radiating energy that is recorded by geophones on the surface. It is possible to assume that the phase of the source wavelet at a given position is random and utilize the crosscorrelation of the direct wave at a location A with a ghost reflection at a location B to image a subsurface reflector without any knowledge regarding the source location. The crosscorrelation of traces A and B in the frequency domain simplifies as:\n\nФ(A, B) = −(Wiω)^2 Re^iω(λArλrB)+o.t.\n\nWhere:\nWi(ω) = frequency domain source wavelet (ith wavelet)\n\nThe crosscorrelation of the direct wave at a location A with a ghost reflection at a location B removes the unknown source term where:\n\nФ(A,B)≈Re^iω(λArλrB)\n\nThis form is equivalent to a virtual source configuration at a location A imaging hypothetical reflections at a location B. Migration of these correlation positions removes the phase term and yields a final migration image at position x where:\n\nm(x) = Σø(A,B,λAx+λxB)\nWhere:\nø(A,B,t) = temporal correlation between locations A and B with lag time t\n\nThis model has been applied to simulate subsurface geometry in West Texas using simulated models including a traditional buried source and a synthetic (virtual) rotating drill bit source to produce similar results. A similar model demonstrated the reconstruction of a simulated subsurface geometry. In this case, the reconstructed subsurface response correctly modeled the relative positions of primaries and multiples. Additional equations can be derived to reconstruct signal geometries in a wide variety of cases.\n\nSeismic interferometry is currently utilized primarily in research and academic settings. In one example, passive listening and the crosscorrelation of long noise traces was used to approximate the impulse response for shallow subsurface velocity analysis in Southern California. Seismic interferometry provided a result comparable to that indicated using elaborate inversion techniques. Seismic interferometry is most often used for the examination of the near surface and is\noften utilized to reconstruct surface and direct waves only. As such, seismic interferometry is commonly used to estimate ground roll to aid in its removal. Seismic interferometry simplifies estimates of shear wave velocity and attenuation in a standing building. Seismic interferometry has been applied to image the seismic scattering and velocity structure of volcanoes.\n\nIncreasingly, seismic interferometry is finding a place in exploration and production. SI can image dipping sediments adjacent to salt domes. Complex salt geometries are poorly resolved using traditional seismic reflection techniques. An alternative method calls for the use of downhole sources and receivers adjacent to subsurface salt features. It is often difficult to generate an ideal seismic signal in a downhole location. Seismic interferometry can virtually move a source into a downhole location to better illuminate and capture steeply dipping\nsediments on the flank of a salt dome. In this case, the SI result was very similar to that obtained using an actual downhole source. Seismic interferometry can locate the position of an unknown source and is often utilized in hydrofrac applications to map the extent of induced fractures. It is possible that interferometric techniques can be applied to timelapse seismic monitoring of subtle changes in reservoir properties in the subsurface.\n\nSeismic interferometry applications are currently limited by a number of factors. Real world media and noise represent limitations for current theoretical development. For example, for interferometry to work noise sources must be uncorrelated and completely surround the region of interest. In addition, attenuation and geometrical spreading are largely neglected and need to be incorporated into more robust models. Other challenges are inherent to seismic interferometry. For example, the source term only drops out in the case of the crosscorrelation of a direct wave at a location A with a ghost reflection at a location B. The correlation of other waveforms can introduce multiples to the resulting interferogram. Velocity analysis and filtering can reduce but not eliminate the occurrence of multiples in a given dataset.\n\nAlthough there have been many advancements in seismic interferometry challenges still remain. One of the biggest remaining challenges is extending the theory to account for real world media and noise distributions in the subsurface. Natural sources typically do not comply with mathematical generalizations and may in fact display some degree of correlation. Additional problems must be addressed before applications of seismic interferometry can become more widespread.\n\n"}
{"id": "146573", "url": "https://en.wikipedia.org/wiki?curid=146573", "title": "Sex toy", "text": "Sex toy\n\nA sex toy is an object or device that is primarily used to facilitate human sexual pleasure, such as a dildo or vibrator. Many popular sex toys are designed to resemble human genitals and may be vibrating or non-vibrating. The term \"sex toy\" can also include BDSM apparatus and sex furniture such as slings; however, it is not applied to items such as birth control, pornography, or condoms. Alternative expressions include adult toy and the dated euphemism marital aid, although \"marital aid\" has a broader sense and is applied to drugs and herbs marketed to supposedly enhance or prolong sex. Sex toys are most commonly sold at a sex shop, but they may also be sold in a pharmacy/chemist store, a pornographic DVD store, or head shop. Today's sex toys are available in almost all countries for male and females.\n\nAnother form of sex toy for both men and women are those for erotic electrostimulation.\n\nErotic electrostimulation refers to the act of using electricity for sexual stimulation. Electrostimulation dates back as early as the mid 1700s. By the mid 1970s, medical transcutaneous electrical nerve stimulation (TENS) machines were widely available. The machines work by stimulating nerve endings with electricity, sending signals of stimulation to the brain. Electrostimulation works off this same principle, when the brain received a signal of stimulation from the genitals, pleasure hormones are released.\n\nErotic furniture is furniture specially shaped for comfort, penetration levels, and stimulation.\n\n\n\nGlass sex toys are commonly made from clear medical grade borosilicate glass (\"hard glass\"). This particular type of safety toughened glass is non-toxic and will withstand extreme temperatures, as well as physical shock without compromising its structural integrity.\n\nThe choice of this high-grade material provides safety in use and the option to heat or chill the toys. Borosilicate glass is also non-porous and can be sterilized to help prevent infection with reuse. The highest quality glass toys can even be put in the dishwasher making them easier to keep clean. As well as their practical qualities, a main selling point of glass sex toys is their visual appeal.\n\nSome glass sex toys vibrate. There are two main ways this can be achieved. Either the toy may have a hole into which a small bullet vibrator can be inserted, or the core of the glass design can be modified to form a standard vibrator. The latter option usually has a plastic cap covering the battery compartment, which will also house any control buttons or switches.\n\nVibrators are vibrating devices intended to stimulate the body. Vibrators come in a range of shapes and sizes, for internal or external use. Some vibrators intended for internal use are phallic in shape. Small vibrators may have a stretchy loop attachment for use as a finger toy or cock ring. Penetrative vibrators usually measure twelve to eighteen cm (five to seven inches) in length and two to five cm (one to two inches) wide often to mimic the size of the average human penis.\n\n\n\n\nNo safety regulations exist in the sex toy industry. The sex toys are sold as novelty items so they do not need to adhere to certain regulations such as reporting the chemicals and materials used in a product. Due to this status, manufacturers are not responsible if their toys are used for any other purpose than being a novelty. Regulations such as REACH do exist, and some sex toys may be compliant to this though, despite that there is no obligation for manufacturers on attaining compliance. A 2006 study conducted by the Greenpeace Netherlands office found high level of phthalates in seven out of eight plastic sex toys tested.\n\nSex toys are classified as novelties in the United States because the Food and Drug Administration has extensive testing and financial requirements in order for sex toys to be classified as medical devices. Therefore, sex toy manufacturers more often choose less complex production by labelling them a novelty, where their listed ingredients do not have to be accurate in chemical composition or percentage of ingredients. Due to the novelty classification, sex toys may contain toxins such as phthalates, which have been banned in children's toys by the CPSC. Phthalates are chemical plasticizers that are added as softeners, to create the malleable and soft effect that many look for in sex toys.\n\nStudies on rodents have revealed that when exposed to very large doses, phthalates can cause damage to the liver, lungs, kidneys, testes and can cause hormonal disruption. The latest research indicates that exposure to these substances can upset the body's ability to regulate hormone production, damage reproduction, can cause liver and kidney defects, and can cause cancer. The most common and external side-effects are rashes, itchiness and irritation to the locations of use.\n\nBefore using a sex toy, owners should take precautions. One should check for tears, rough seams or cracks that could harm the inside of the vagina or anus. Condoms should also be used on porous sex toys and sex toys that are being shared between two or more partners. They should also use appropriate lubricants; silicone lube will break down silicone toys, and oil-based lubes will break down latex condoms.\n\nCleaning sex toys is also very important for sexual health and sex toy safety. Cleaning them will avoid the potential of bacterial infection, transmission of STIs (if shared), or pregnancy (if sperm is present on the toy). Porous sex toys (ridged, flexible, soft and squishy) are difficult to clean and can hide bacteria that multiply and harm the human body. Non-porous toys are easier to clean, being less harmful. When cleaning sex toys, always use warm water and unscented anti-bacterial soap.\n\nNon-porous toys\n\nPorous toys\n\nAnal toys (butt plugs, small dildos, etc.)\n\nIn 2016 the security software company Trend Micro demonstrated that some sex toys are vulnerable to cyberattacks, thus creating the field of onion dildonics. The ethical, legal, and privacy concerns are an area of active research by Sarah Jamie Lewis, amongst others.\n\nSex toys are illegal in India. Selling sex toys is a punishable offense under section 292 of Indian penal code, as sex toys are considered an \"obscene\" product. Besides sex toys, any book, pamphlet, paper, writing, drawing, painting, representation, figure or any other object, is by the way also considered obscene by section 292 if it is lascivious or appeals to the prurient interest. The punishment for the offense is up to two years in prison.\n\nIn Japan, many dildos are created to resemble animals or cartoon characters, such as Hello Kitty, rabbits or dolphins, so that they may be sold as toys, thus avoiding obscenity laws.\n\nIn Malaysia, the sale and importation of sex toys is illegal.\n\nSection 18A of the Sexual Offences Act, 1957, inserted by the Immorality Amendment Act, 1969, prohibited the manufacture or sale of any item \"intended to be used to perform an unnatural sexual act\". The term \"unnatural sexual act\" referred to any sex other than vaginal heterosexual sex, and this prohibition was ostensibly aimed at preventing the use of dildos by lesbians. No longer enforced, the section was repealed by the Criminal Law (Sexual Offences and Related Matters) Amendment Act, 2007.\n\nSex toys and lubricants have become increasingly available in major commercial outlets in the United States. On-shelf displays tend to be more discreet than the offerings on web sites. These items tend to be displayed in the \"sexual health\" sections of stores.\n\nUntil recently, many Southern and some Great Plains states banned the sale of sex toys completely, either directly or through laws regulating \"obscene devices\". In 1999, William H. Pryor, Jr., an assistant attorney general in Alabama commenting on a case involving sex toys and discussing to what end the devices are used, was quoted as saying there is no \"fundamental right for a person to buy a device to produce orgasm\". A federal appeals court upheld Alabama's law prohibiting the sale of sex toys on Valentine's Day, 2007.\n\nIn February 2008, a federal appeals court overturned a Texas statute banning the sales of sex toys, deeming such a statute as violating the Constitution's 14th Amendment on the right to privacy. The appeals court cited \"Lawrence v. Texas\", where the U.S. Supreme Court in 2003 struck down bans on consensual sex between gay couples, as unconstitutionally aiming at \"enforcing a public moral code by restricting private intimate conduct\". Similar statutes have been struck down in Kansas and Colorado.\n\nMarty Klein, author of \"America's War on Sex\" and an advocate for the moral value of sex toys, has described the sex toy bans as a form of erotophobia and genophobia claiming the \"extraordinary erosion of personal liberty, coupled with the massive disrespect of and fear of sexuality is no joke\" and that the \"Supreme Court [of the United States] has declared our orgasms a battlefield, and sex toys another casualty.\"\n\nAs of 2008, it was valued at US$15 billion worldwide, with a growth rate of 30%. 70% of sex toys are manufactured in China. Sex toys are sold in various types of local and online sex shops, at conventions associated with the adult industry, and at parties. However, some items, such as \"hand held massagers\", are sold in mainstream retail outlets such as drugstores.\n\n\n"}
{"id": "18842002", "url": "https://en.wikipedia.org/wiki?curid=18842002", "title": "Shampoo", "text": "Shampoo\n\nShampoo () is a hair care product, originating from the Indian subcontinent, typically in the form of a viscous liquid, that is used for cleaning hair. Less commonly, shampoo is available in bar form, like a bar of soap. Shampoo is used by applying it to wet hair, massaging the product into the hair, and then rinsing it out. Some users may follow a shampooing with the use of hair conditioner.\n\nThe typical reason of using shampoo is to remove the unwanted build-up of sebum in the hair without stripping out so much as to make hair unmanageable. Shampoo is generally made by combining a surfactant, most often sodium lauryl sulfate or sodium laureth sulfate, with a co-surfactant, most often cocamidopropyl betaine in water.\n\nSpecialty shampoos are available for people with dandruff, color-treated hair, gluten or wheat allergies, an interest in using an organic product, and infants and young children (\"baby shampoo\" is less irritating). There are also shampoos intended for animals that may contain insecticides or other medications to treat skin conditions or parasite infestations such as fleas.\n\nThe word \"shampoo\" entered the English language from the Indian subcontinent during the colonial era. It dates to 1762, and is derived from Hindi \"chāmpo\" (चाँपो ), itself derived from the Sanskrit root \"chapayati\" (चपयति, which means to press, knead, soothe).\n\nIn the Indian subcontinent, a variety of herbs and their extracts have been used as shampoos since ancient times. A very effective early shampoo was made by boiling Sapindus with dried Indian gooseberry (amla) and a selection of other herbs, using the strained extract. Sapindus, also known as soapberries or soapnuts, a tropical tree widespread in India, is called \"ksuna\" (Sanskrit: क्षुण) in ancient Indian texts and its fruit pulp contains saponins which are a natural surfactant. The extract of soapberries creates a lather which Indian texts called \"phenaka\" (Sanskrit: फेनक). It leaves the hair soft, shiny and manageable. Other products used for hair cleansing were shikakai (Acacia concinna), hibiscus flowers, ritha (Sapindus mukorossi) and arappu (Albizzia amara). Guru Nanak, the founding prophet and the first Guru of Sikhism, made references to soapberry tree and soap in the 16th century.\n\nCleansing with hair and body massage (champu) during one's daily bath was an indulgence of early colonial traders in India. When they returned to Europe, they introduced the newly learned habits, including the hair treatment they called shampoo.\n\nSake Dean Mahomed, a Bengali traveller, surgeon, and entrepreneur, is credited with introducing the practice of \"champooi\" or \"shampooing\" to Britain. In 1814, Mahomed, with his Irish wife Jane Daly, opened the first commercial \"shampooing\" vapour masseur bath in England, in Brighton. He described the treatment in a local paper as \"The Indian Medicated Vapour Bath (type of Turkish bath), a cure to many diseases and giving full relief when everything fails; particularly Rheumatic and paralytic, gout, stiff joints, old sprains, lame legs, aches and pains in the joints\".\n\nDuring the early stages of shampoo in Europe, English hair stylists boiled shaved soap in water and added herbs to give the hair shine and fragrance. Commercially made shampoo was available from the turn of the 20th century. A 1914 advertisement for Canthrox Shampoo in \"American Magazine\" showed young women at camp washing their hair with Canthrox in a lake; magazine advertisements in 1914 by Rexall featured Harmony Hair Beautifier and Shampoo.\n\nIn 1927, liquid shampoo was invented by German inventor Hans Schwarzkopf in Berlin, whose name created a shampoo brand sold in Europe.\n\nOriginally, soap and shampoo were very similar products; both containing the same naturally derived surfactants, a type of detergent. Modern shampoo as it is known today was first introduced in the 1930s with \"Drene\", the first shampoo using synthetic surfactants instead of soap.\n\nEarly shampoos used in Indonesia were made from the husk and straw (\"merang\") of rice. The husks and straws were burned into ash, and the ashes (which have alkaline properties) are mixed with water to form lather. The ashes and lather were scrubbed into the hair and rinsed out, leaving the hair clean, but very dry. Afterwards, coconut oil was applied to the hair in order to moisturize it.\n\nCertain Native American tribes used extracts from North American plants as hair shampoo; for example the Costanoans of present-day coastal California used extracts from the coastal woodfern, \"Dryopteris expansa\",\n\nBefore quinoa can be eaten the saponin must be washed out from the grain prior to cooking. Pre-Columbian Andean civilizations used this soapy by-product as a shampoo.\n\nShampoo is generally made by combining a surfactant, most often sodium lauryl sulfate or sodium laureth sulfate, with a co-surfactant, most often cocamidopropyl betaine in water to form a thick, viscous liquid. Other essential ingredients include salt (sodium chloride), which is used to adjust the viscosity, a preservative and fragrance. Other ingredients are generally included in shampoo formulations to maximize the following qualities:\n\nMany shampoos are pearlescent. This effect is achieved by the addition of tiny flakes of suitable materials, e.g. glycol distearate, chemically derived from stearic acid, which may have either animal or vegetable origins. Glycol distearate is a wax. Many shampoos also include silicone to provide conditioning benefits.\n\n\nIn the United States, the Food and Drug Administration (FDA) mandates that shampoo containers accurately list ingredients on the products container. The government further regulates what shampoo manufacturers can and cannot claim as any associated benefit. Shampoo producers often use these regulations to challenge marketing claims made by competitors, helping to enforce these regulations. While the claims may be substantiated, however, the testing methods and details of such claims are not as straightforward. For example, many products are purported to protect hair from damage due to ultraviolet radiation. While the ingredient responsible for this protection does block UV, it is not often present in a high enough concentration to be effective. The North American Hair Research Society has a program to certify functional claims based on third-party testing. Shampoos made for treating medical conditions such as dandruff or itchy scalp are regulated as OTC drugs in the US marketplace.\n\nIn the European Union, there is a requirement for the anti-dandruff claim to be substantiated as with any other advertising claim, but it is not considered to be a medical problem.\n\nA number of contact allergens are used as ingredients in shampoos, and contact allergy caused by shampoos is well known. Patch testing can identify ingredients to which patients are allergic, after which a physician can help the patient find a shampoo that is free of the ingredient to which they are allergic. The US bans 11 ingredients from shampoos, Canada bans 587, and the EU bans 1328.\n\nCosmetic companies have developed shampoos specifically for those who have dandruff. These contain fungicides such as ketoconazole, zinc pyrithione and selenium disulfide, which reduce loose dander by killing fungi like \"Malassezia furfur\". Coal tar and salicylate derivatives are often used as well.\nAlternatives to medicated shampoos are available for people who wish to avoid synthetic fungicides. Such shampoos often use tea tree oil, essential oils or herbal extracts.\n\nMany companies have also developed color-protection shampoos suitable for colored hair; some of these shampoos contain gentle cleansers according to their manufacturers.\n\nMany people suffer from eczema on their palms and their head. Some find that wheat or gluten — the protein found in many grains including wheat — is the cause, particularly if they are sensitive to foods containing gluten (citation needed). Shampoo can inadvertently enter the mouth, particularly among children, so all individuals who are on gluten-free diets may prefer to use a gluten-free shampoo. Shampoo manufacturers are starting to recognize this and gluten-free and wheat-free products are now available.\n\nWheat derivatives and ingredients from other gluten grains are commonly used as binders to help shampoo stick together, and are also used as emollients in the form of oils. Following is a list of grain-derived shampoo ingredients. Most of these ingredients do not theoretically contain any intact wheat proteins, but trace amounts may be present due to incomplete processing or contamination. (citation needed)\n\nShampoo for infants and young children is formulated so that it is less irritating and usually less prone to produce a stinging or burning sensation if it were to get into the eyes. For example, Johnson's Baby Shampoo advertises under the premise of \"No More Tears\". This is accomplished by one or more of the following formulation strategies.\nThe distinction in 4 above does not completely surmount the controversy over the use of shampoo ingredients to mitigate eye sting produced by other ingredients, or the use of the products so formulated.\nThe considerations in 3 and 4 frequently result in a much greater multiplicity of surfactants being used in individual baby shampoos than in other shampoos, and the detergency or foaming of such products may be compromised thereby. The monoanionic sulfonated surfactants and viscosity-increasing or foam stabilizing alkanolamides seen so frequently in other shampoos are much less common in the better baby shampoos.\n\nShampoo intended for animals may contain insecticides or other medications for treatment of skin conditions or parasite infestations such as fleas or mange. These must never be used on humans. While some human shampoos may be harmful when used on animals, any human haircare products that contain active ingredients or drugs (such as zinc in anti-dandruff shampoos) are potentially toxic when ingested by animals. Special care must be taken not to use those products on pets. Cats are at particular risk due to their instinctive method of grooming their fur with their tongues.\n\nShampoos that are especially designed to be used on pets, commonly dogs and cats, are normally intended to do more than just clean the pet's coat or skin. Most of these shampoos contain ingredients which act differently and are meant to treat a skin condition or an allergy or to fight against fleas.\n\nThe main ingredients contained by pet shampoos can be grouped in insecticidals, antiseborrheic, antibacterials, antifungals, emollients, emulsifiers and humectants. Whereas some of these ingredients may be efficient in treating some conditions, pet owners are recommended to use them according to their veterinarian's indications because many of them cannot be used on cats or can harm the pet if it is misused.\nGenerally, insecticidal pet shampoos contain pyrethrin, pyrethroids (such as permethrin and which may not be used on cats) and carbaryl. These ingredients are mostly found in shampoos that are meant to fight against parasite infestations.\n\nAntifungal shampoos are used on pets with yeast or ringworm infections. These might contain ingredients such as miconazole, chlorhexidine, providone iodine, ketoconazole or selenium sulfide (which cannot be used on cats).\n\nBacterial infections in pets are sometimes treated with antibacterial shampoos. They commonly contain benzoyl peroxide, chlorhexidine, povidone iodine, triclosan, ethyl lactate, or sulfur.\n\nAntipruritic shampoos are intended to provide relief of itching due to conditions such as atopy and other allergies. These usually contain colloidal oatmeal, hydrocortisone, Aloe vera, pramoxine hydrochloride, menthol, diphenhydramine, sulfur or salicylic acid. These ingredients are aimed to reduce the inflammation, cure the condition and ease the symptoms at the same time while providing comfort to the pet.\n\nAntiseborrheic shampoos are those especially designed for pets with scales or those with excessive oily coats. These shampoos are made of sulfur, salicylic acid, refined tar (which cannot be used on cats), selenium sulfide (cannot be used on cats) and benzoyl peroxide. All these are meant to treat or prevent seborrhea oleosa, which is a condition characterized by excess oils. Dry scales can be prevented and treated with shampoos that contain sulfur or salicylic acid and which can be used on both cats and dogs.\n\nEmollient shampoos are efficient in adding oils to the skin and relieving the symptoms of a dry and itchy skin. They usually contain oils such as almond, corn, cottonseed, coconut, olive, peanut, Persia, safflower, sesame, lanolin, mineral or paraffin oil. The emollient shampoos are typically used with emulsifiers as they help distributing the emollients. These include ingredients such as cetyl alcohol, laureth-5, lecithin, PEG-4 dilaurate, stearic acid, stearyl alcohol, carboxylic acid, lactic acid, urea, sodium lactate, propylene glycol, glycerin, or polyvinylpyrrolidone.\n\nAlthough some of the pet shampoos are highly effective, some others may be less effective for some condition than another. Yet, although natural pet shampoos exist, it has been brought to attention that some of these might cause irritation to the skin of the pet. Natural ingredients that might be potential allergens for some pets include eucalyptus, lemon or orange extracts and tea tree oil. On the contrary, oatmeal appears to be one of the most widely skin-tolerated ingredients that is found in pet shampoos. Most ingredients found in a shampoo meant to be used on animals are safe for the pet as there is a high likelihood that the pets will lick their coats, especially in the case of cats.\n\nPet shampoos which include fragrances, deodorants or colors may harm the skin of the pet by causing inflammations or irritation. Shampoos that do not contain any unnatural additives are known as hypoallergenic shampoos and are increasing in popularity.\n\nSolid shampoos or shampoo bars use as their surfactants soaps or other surfactants formulated as solids. They have the advantage of being spill-proof. They are easy to apply; one may simply rub the bar over wet hair, and work the soaped hair into a low lather.\n\nStiff, non-pourable clear gels to be squeezed from a tube were once popular forms of shampoo, and can be produced by increasing a shampoo's viscosity. This type of shampoo cannot be spilled, but unlike a solid, it can still be lost down the drain by sliding off wet skin or hair.\n\nShampoos in the form of pastes or creams were formerly marketed in jars or tubes. The contents were wet but not completely dissolved. They would apply faster than solids and dissolve quickly.\n\ns are often used in veterinary medicine for various conditions, as well as in humans before some surgical procedures.\n\nClosely associated with environmentalism, the \"no poo\" movement consists of people rejecting the societal norm of frequent shampoo use. Some adherents of the no poo movement use baking soda or vinegar to wash their hair, while others use diluted honey. Other people use nothing, rinsing their hair only with conditioner.\n\nIn the 1970s, ads featuring Farrah Fawcett and Christie Brinkley asserted that it was unhealthy not to shampoo several times a week. This mindset is reinforced by the greasy feeling of the scalp after a day or two of not shampooing. Using shampoo every day removes sebum, the oil produced by the scalp. This causes the sebaceous glands to produce oil at a higher rate, to compensate for what is lost during shampooing. According to Michelle Hanjani, a dermatologist at Columbia University, a gradual reduction in shampoo use will cause the sebum glands to produce at a slower rate, resulting in less grease in the scalp. Although this approach might seem unappealing to some individuals, many people try alternate shampooing techniques like baking soda and vinegar in order to avoid chemicals and ingredients used in many shampoos that make hair greasy over time.\n\n"}
{"id": "18011930", "url": "https://en.wikipedia.org/wiki?curid=18011930", "title": "Site prospecting", "text": "Site prospecting\n\nSite prospecting is the process of evaluating demographic data surrounding potential locations for a business, based on a user-defined trade area or areas.\n\n"}
{"id": "51615806", "url": "https://en.wikipedia.org/wiki?curid=51615806", "title": "Social sorting", "text": "Social sorting\n\nSocial sorting is understood as the breakdown and categorization of group- or person-related raw data into various categories and segments by data manipulators and data brokers. Social sorting involves the key task of separating one group from the other. These groups can be based on income, education, race, ethnicity, profiling, gender, occupation, social status, derived power (social and political) and geographic residence. Depending on the goals of the manipulator raw data is collected and then further evolves into meaningful data in order to be exploited for a specific purpose. For example, the formulation of profiling and predictive policing are all derivations of social sorting.\n\nThe concept is accredited to David Lyon, a sociologist who is best known for his work in surveillance studies. Contemporary times have allowed for the influx and constant growth of data collection especially in the countries of the global north. A prime example of social terming is surveillance. This practice was not always as sophisticated as today. Historically, simple tools such as labor-intensive watchers, book keeping and record keeping acted as the enablers of this form. Surveillance is now done by governments and various organizations. This technological tools that are equipped with surveillance are cameras, records of transactions done at banking machines and point of sale terminals, machine readable passports before boarding, cellular phone calls along with many other examples. This collection of raw data then enables trends to be found and the fostering of predictions.\n\nIn other contextual fashion, social sorting bears much of its resemblance to social stratification. In primitive societies there are evidence of the roots of social sorting where the sexual division of labour was concerned. Women would do most of the gathering where men would concentrate on hunting. It was argued that although this specific task of the woman may point to domestic oppression according to some observers, hunter-gatherer women would not understand this interpretation. The primitive society did show groupings and deployed categorization which perhaps without their own understanding of the understanding of the social construct was still insinuated.\n\nFurther confirmation of social sorting was evident through slavery. The racial divide between whites and blacks manifested for generations. Based on the visual appearance of their skin people of a darker complexion were grouped and made to endure hardships that included beatings, laborious field work, confinement and executions. A prime example of social sorting at work during the African slave era were mullatos were grouped together to perform household and other domestic work.\n\nCriticisms are often directed at the laws, implemented rules, educational system, job employment opportunities and at the government. Questions are asked of the integrity of many socially constructed programs led by private and government institutions. Fairness and equity are thought to be at the forefront of the list of frustrations for many people that are aware of social sorting.\n\nThere are paranoias and indifferences in regards to social sorting. The September 11 attacks and the subsequent war on terror have fueled the desire for categorizing and profiling people. The beneficiaries that are associated with it are evident as it allows for a more transparent viewership. Some researchers such as David Lyon are concerned with the rise of big data as there are many implication on the daily lives of many.\n\nAccording to David Lyon, Canadians are still unaware of the fact that surveillance which goes collaboratively with social sorting is now very much integrated into their daily lives. David Lyon discusses that the systematic routines and attention to personal detail which is encompassed into surveillance. The key criticism involves indifferent treatment to individuals based on their profile. Depending on the details of a person it can lead to the determination of whether the person may end up on a No Fly List.\n\nEmployers have now begun to engage in these screening methods to determine a person’s suitability for a particular job. Law enforcement bodies and insurance companies have now all began to utilize social sorting to their use in order to determine whether their services should be offered or rescinded. The lives of many are directly affected as they are placed into socio-economic niches.\n\nDavid Lyon insinuates that social sorting through surveillance is a modern threat to freedom. Byproducts of social sorting are isolation, segregation and marginalization. Social sorting has called into account issues that primarily involve equity and fairness. Different societies sort in different ways.\n\nOther societies across the world engage in their own forms of social sorting. Some eastern countries such as China and India place much emphasis on an individual’s level of education. Blue collar jobs which at times may be dirty and laborious are often scorned and met with resentment. Low hourly wages, limited to zero prestige and little respect are directed at individuals who are involved in these occupational roles. Furthermore, the perception and low economic advantages hinder the progression of many people. At times, it acts as a domino effect where when one falls other problems come with it. For example, low education may not contribute to a good paying job; this may in turn lead to low income which may in turn lead to residing in a low class neighbourhood.\n\nWilson & McBrier (2005) conducted a longitudinal study based on the theory of minority vulnerability of employees. These constitute to a group of African Americans who work for good financial income in the upper tier for relatively privileged jobs. \"The minority vulnerability thesis, accordingly, maintains that African Americans are more likely to experience layoffs from upper-tier occupations than Whites even when the two groups have similar background socioeconomic statuses, have accumulated similar human-capital credentials, such as educational attainment and commitment to work, and have similar job/labor market characteristics, including union status as well as economic sector of employment. Findings indicate that, after controlling for seniority, African Americans are susceptible to layoffs on a relatively broad and generalized basis that is unstructured by traditional, stratification-based causal factors, namely, background socioeconomic status, human-capital credentials, and job/labor-market characteristics.\"\n\nSchools are accredited differing levels of prestige in comparison to other institutions. Social sorting occurs amongst schools where claims are made about learning experiences and which institutions may be the best for learning all of which are left up to subjection. For example, Princeton, and Harvard are all highly rated prestigious universities for various reasons. These perceptions cause employers and students alike to question the credibility and graduate documentation based on the institution that one attends.\n\nIn 2015, The Data Broker Accountability and Transparency Act was resurrected by four U.S senators that would allow consumers to see and correct personal information held by data brokers and tell those businesses to stop sharing or selling it for marketing purposes.\n\n"}
{"id": "4119243", "url": "https://en.wikipedia.org/wiki?curid=4119243", "title": "Starseed launcher", "text": "Starseed launcher\n\nStarseed is a proposed method of launching interstellar nanoprobes at one-third light speed.\n\nThe launcher uses a 1,000 kilometer long small diameter hollow wire, and uses electrodes lining the hollow wire as an electrostatic accelerator tube, similar to K. Eric Drexler's ideas. The launcher is designed to accelerate its probes to 1/3 the speed of light, about 100,000 kilometers per second, at something on the order of 100 million gravities of acceleration.\n\nKeeping the launch tube straight enough to avoid the probe hitting the tube walls is a major challenge. The launcher would have to be set up in deep space, well away from any planets, to avoid gravitational tidal effects bending the tube too much.\n\nThe \"Starseed Probes\" are proposed to be extremely small (roughly one microgram) nanomachines and nanocomputers. The required launch energy per probe is low due to the low mass, and many nanoprobes would be launched in sequence and rendezvous in flight.\n\n"}
{"id": "14041928", "url": "https://en.wikipedia.org/wiki?curid=14041928", "title": "Sussex trug", "text": "Sussex trug\n\nA Sussex trug is a wooden basket. It is made from a handle and rim of coppiced sweet chestnut wood which is hand-cleft then shaved using a drawknife. The body of the trug is made of five or seven thin boards of cricket bat willow, also hand-shaved with a drawknife. They may have originated in Sussex because of the abundance of chestnut coppice and willows found on the marshes. Nails or pins used are usually copper, to avoid rust.\n\nShapes and sizes became standardised, the most well-known shape being the \"common or garden\" trug ranging in volume from one pint to a bushel. However, there is a diverse range of traditional trugs from garden and oval trugs to the more specialised \"large log\" and \"walking stick\" trugs.\n\nRichard Acres worked as a trug maker in 1485, in Rotherfield in Sussex.\n\nTrugs date back to the 1500s, with active trade in Horsham.\n\nThomas Smith of Herstmonceux, displaying his trugs at the Great Exhibition of 1851, gave the basket wider renown; he was rewarded when Queen Victoria purchased several for members of the Royal family. Further appearances at international exhibitions followed at the 1855 Universal exhibition in Paris; the First International Forestry Exhibition in Edinburgh 1884 and London International Inventions Exhibition.\n\nBy the 1970s, Herstmonceux remained as a significant centre of trug production, with four firms operating in or near that village: Greens of Hailsham, R. Reed, R.W. Rich and Sons, and Thomas Smith and Sons.\n"}
{"id": "2154567", "url": "https://en.wikipedia.org/wiki?curid=2154567", "title": "The Tech Report", "text": "The Tech Report\n\nThe Tech Report is a web site dedicated to covering personal computing technology and culture. The Tech Report specializes in hardware that is most relevant to home users and enthusiasts, and has excelled at analysis with prudent budgets in mind, producing quarterly system build guides at various price points, and occasional price vs. performance scatter plots. Tech Report also has an active online community and a podcast. The website is also a supporter of Stanford's Folding@home and is currently ranked 26th in total production points as of March 22, 2018.\n\nTech Report was founded by Scott 'Damage' Wasson, a Harvard Divinity School graduate, and Andy 'Dr. Evil' Brown. Both started by writing at Ars Technica in 1998. The two later decided to launch their own website. The site eventually grew into a business enterprise with multiple full-time staff members, and continues in that form today.\n\nTech Report was originally located at tech-report.com in 1999. The site was moved to techreport.com in 2003.\n\nOn August 20, 2007 a beta for a new site design was posted in the forums for review by the user community. It has since been moved to live.\n\nLaunching on January 1, 2011, the new site design, TR 3.0 rolled out. Offering a completely new layout and two user switchable colors, blue and white.\n\nOn September 8, 2011 Scott posted an article titled \"Inside the second: A new look at game benchmarking\". This showed GPU manufactures and gamers that frames per second(FPS) are not the only thing that matters in \"smooth\" gameplay, but frame latency has a big part. This encouraged the GPU manufactures to focus on keeping frame latency down across game play through heavy edits to their drivers.\n\nOn December 2, 2015 Scott Wasson, the founder and Editor-In-Chief stepped down as he accepted a role in AMD's graphics division. Wasson subsequently sold the company in March 2018 to Adam Eiberger, the Tech Report's business manager.\n\nA large portion of the main page is dedicated to \"News\" and \"Featured Articles\". Among the news entries are daily \"Shortbread\" posts which offer a summary breakdown of reviews and news offered by other sites. Featured articles are often reviews of newly released PC hardware which has been tested by the site's editors and judged on a number of metrics including performance and value compared to other available hardware. Both the general news posts and the featured articles often feature off-beat humor in the form of wordplay and random pop-culture references, giving evidence to the founders' origins as technology humorists.\n\nAdapting to the general trend of more content for digest, The Tech Report launched their own podcast on February 9, 2008 hosted by Jordan Drake. While the schedule has varied it provides a casual but quite in-depth look back at the topics that made news from a panel of the site staff. Subsequent to 2015 episodes were released irregularly, frequently discussing the release of a new microarchitecture with David Kanter of Real World Technologies.\n\nAs with many technology websites, the Tech Report has developed a large user community, in part through a phpBB-styled forum that is unrestricted in read-only form and open to the public for contribution via simple registration. The forum is primarily structured around computer technology and related topics, but debates also range from politics and religion in the \"opt-in only\" R&P forum to general random chatter in the Back Porch. Contributors to the website also have access to a restricted forum called the Smoky Back Room. Registered users may respond to news topics and other entries posted on the front page in an isolated threaded comments section that automatically attaches to each new entry. Although access to the main page comments is linked into the user database, the discussions are logged separately from the forum area of the site and are not counted toward the user forum statistics.\n"}
{"id": "16991704", "url": "https://en.wikipedia.org/wiki?curid=16991704", "title": "Unicompartmental knee arthroplasty", "text": "Unicompartmental knee arthroplasty\n\nUnicompartmental knee arthroplasty (UKA) is a surgical procedure used to relieve arthritis in one of the knee compartments in which the damaged parts of the knee are replaced. UKA surgery may reduce post-operative pain and have a shorter recovery period than a total knee replacement procedure, particularly in people over 75 years of age. Moreover, UKAs may require a smaller incision, less tissue damage, and faster recovery times.\n\nIn the United States, the procedure constitutes approximately 8% of knee arthroplasties. In comparisons with a more extensive surgical procedure called high tibial osteotomy, UKA has equal or better outcomes.\n\nIn the early 1950s, Duncan C. McKeever theorized that osteoarthritis could be isolated to only one compartment of the knee joint, and that replacement of the entire knee might not be necessary if only one knee compartment were affected. The UKA concept was designed to cause less trauma or damage than traditional total knee replacement by removing less bone and trying to maintain most of the person’s bone and anatomy. The concept was also designed to use smaller implants and thereby keep most of the person’s bone, helping them return to normal function faster.\n\nInitially, UKAs were not always successful, because the implants were poorly designed, people needing the surgery were not thoroughly screened for suitability, and optimal surgical techniques were not developed. Advancements have been made to improve the design of the implants. Also, choosing the best-suited people was emphasized to ensure that surgeons followed the indications and contraindications for partial replacement. Proper selection, following the indications/contraindications, and performing the surgery well are key factors for the success of UKA.\n\nUKA may be suitable for people with moderate joint disease caused by painful osteoarthritis or traumatic injury, a history of unsuccessful surgical procedures or poor bone density that precludes other types of knee surgery. People who may not be eligible for a UKA include those with an active or suspected infection in or about the knee joint, may have a known sensitivity to device materials, have bone infections or disease that result in an inability to support or fixate the new implant to the bone, have inflammatory arthritis, have major deformities that can affect the knee mechanical axis, have neuromuscular disorders that may compromise motor control and/or stability, have any mental neuromuscular disorder, are obese, have lost a severe amount of bone from the shin (tibia) or have severe tibial deformities, have recurring subluxation of the knee joint, have untreated damage to the knee cap and thigh bone joint (patellofemoral joint), have untreated damage to the opposite compartment or the same side of the knee not being replaced by a device, and/or have instability of the knee ligaments such that the postoperative stability the UKA would be compromised.\n\nThe anterior cruciate ligament (ACL) should be intact, although this is debated by clinicians for people who need a medial compartment replacement. For people needing a lateral compartment replacement, the ACL should be intact and is contraindicated for people with ACL-deficient knees because the lateral component has more motion than the medial compartment.\n\nA physical examination and getting the subject’s history is performed before getting surgery. A person with pain in one area of the knee may be a candidate for UKA. However, a person with pain in multiple areas of the knee may not be a good candidate for UKA. The doctor may take some radiographs (e.g., x-rays) to check for degeneration of the other knee compartments and evaluate the knee. The physical exam may also include special tests designed to test the ligaments of the knee and other anatomical structures. Most likely, the surgeon will decide to do a UKA during surgery where he/she can directly see the status of the other compartments.\n\nThe surgeon may choose which type of incision and implant to use for the subject’s knee. During the surgery, the surgeon may align the instruments to determine the amount of bone to remove. The surgeon removes bone from the (tibia) and thigh bone (femur). The surgeon may decide to check if the appropriate amount of bone was removed during the surgery. In order to make sure that the proper size implant is used, a surgeon may choose to use a temporary trial. After making sure the proper size implant is selected, the surgeon will put the implant on the ends of the bone and secure it with pegs. Finally, the surgeon will close the wound with sutures.\n\nThe unicompartmental replacement is a minimally invasive option for people whose arthritis is isolated to either the medial or the lateral compartment. The procedure offers several benefits for patients with a moderately active lifestyle, who have arthritis in just one knee compartment, and who are within normal weight ranges. The surgeon uses an incision of just 3-4 inches; a total knee replacement typically requires an incision of 8-12 inches. The partial replacement does not disrupt the knee cap, which makes for a shorter rehabilitation period. A partial replacement also causes minimal blood loss during the procedure, and results in considerably less post-operative pain. The hospitalization time compared with a total knee replacement is also greatly reduced.\n\nThe potential benefits of UKA include a smaller incision because the UKA implants are smaller than the total knee replacements, and the surgeon may make a smaller incision. This may lead to a smaller scar. Another potential benefit is less post-operative pain because less bone is removed. Also, a quicker operation and shorter recovery period may be a result of less bone being removed during the operation and the soft tissue may sustain less trauma. Also, the rehabilitation process may be more progressive. More specific benefits of UKA are that it may improve range of motion, reduce blood loss during surgery, reduce the person’s time spent in the hospital, and decrease costs.\n\nIn 2018, two of the most significant benefits of UKA or partial knee replacements are:\n1. Partial knee replacement subjects report that their replaced knee feels more like their original non-replaced knee as compared to a total knee replacement\n2, Partial knee replacements leave other options open to further advances. By not replacing the rest of the knee with metal and plastic, if other options exist in years to come for arthritis in these areas then a partial knee replacement does not burn that bridge.\n\nBlood clots (also known as deep vein thrombosis) are a common complication after surgery. However, a doctor may prescribe certain medications to help prevent blood clots. Infection may occur after surgery. However, antibiotics may be prescribed by a doctor to help prevent infections. Individual factors (e.g., anatomy, weight, prior medical history, prior joint surgeries) should be addressed with the surgery subject. The causes of long-term failure of UKAs include polyethylene wear, loosening of the implant, and degeneration of the adjacent knee compartment.\n\nLong term studies reported excellent outcomes for UKA, partly due to subject screening, minimizing the amount of bone that is removed, and using the proper surgical technique. One study found that at a minimum of 10 years follow up time after the initial surgery, the overall survival rate of the implant was 96%. Also, 92% of the people in this study had excellent or good outcome. Another study, reported that at 15 years follow up time after the initial surgery, the overall rate of the implant was 93% and 91% of these people reported good or excellent outcomes.\n\n"}
{"id": "34345294", "url": "https://en.wikipedia.org/wiki?curid=34345294", "title": "United States HRes. 269 on Antonio Meucci", "text": "United States HRes. 269 on Antonio Meucci\n\nThe 107th United States Congress resolution (HRes 269) of June 11, 2002, recognized the contributions of Antonio Meucci. This was interpreted by some as establishing priority for the invention of the telephone to Meucci. The House of Representatives' resolution did not annul or modify any of Bell's patents for the telephone.\n\nDuring the 108th Congress another resolution, SRes 223 was introduced in the United States Senate and died, unenacted.\n\nShortly afterward, the Canadian government passed a similar motion declaring Alexander Graham Bell the inventor of the telephone. The HRes 269 resolution was widely reported by various news media at the time and is still cited by Meucci advocates as proof that he has been acknowledged as the first inventor of the telephone. The resolution has equally been criticized for its factual errors, inaccuracies, biases and distortions.\n\nAs directly quoted in HRes 269 (Ver. EH), and recorded by the US GPO (table and paragraph numbering added for referencing purposes):\n\nOn June 21, 2002, the Canadian House of Commons passed a motion declaring Alexander Graham Bell the inventor of the telephone, in response to the US House resolution.\n\nIn 2003, the distinguished Italian telecommunications inventor and Meucci book author Professor Basilio Catania, interviewed on the parliamentary Bell motion and the congressional Meucci resolution, first commented on Bell's record as an inventor and scientist:\nI am not the kind of man who can make statements without proofs. I did not do it with Meucci and I do not see why I should do it with Bell... ...I can, however, state that the theoretical description of the electrical transmission of speech in Bell's first patent is nearly perfect and appears to me as the first clear treatise ever written.\n\nProfessor Catania later went on to note the straightforwardness of the follow-up parliamentary motion compared to the seaminess of the initial congressional resolution:\nThe Canadian reaction to an unfortunate passage in Resolution No. 269 of the US House of Representatives is quite understandable. In my opinion the insinuation against the morality and scientific stature of Alexander Graham Bell, in the above resolution, was both unnecessary and unproven, though there had been suspicions that Bell might have fished something from Meucci's ideas. Personally, I would have refrained from stating anything that is not fully proven. I do, however, appreciate that, in the Canadian motion pro Bell, nothing [derogatory] is said against Antonio Meucci...\n\nHowever, intellectual property law author R. B. Rockman was more critical in his view of HRes 269. After first reviewing the essential details of \"American Bell Telephone Company v. Globe Telephone Company, Antonio Meucci, et al.\" (31 Fed 728 (SDNY, 1887)), where he noted major inconsistencies in Meucci's various testimonies, the paucity of direct evidence that both the Globe and Meucci brought forward in support of their defenses and the court's definitive rejection of those defenses in favour of Bell, Rockman then compares the\" 'Bell v. Globe and Meucci' \"court decision (of July 19, 1887) with \"HRes 269\":\nI conclude that the comments of Mr. Grosvenor [who wrote a memo highly critical of HRes 269 by comparing it with \"Bell v. Globe\"] are more likely correct in comparison to the statements [within the preamble of HRes 269] by the United States House of Representatives.\n\nRockman then proceeded to dissect and parse the U.S. Government's 1887 legal challenge to Bell's telephone patent, which had been brought by United States Attorney General Augustus H. Garland, a major stock shareholder of the Pan-Electric Company which was the instigator of the suit. Pan-Electric sought to overturn Bell's patent in order to compete against the American Bell Telephone Co.\n\nIt was this very court challenge that is referenced in \"HRes 269's\" preamble, in paragraph No. 9, which inferred an immoral and possibly criminal intent by Bell (\"...the Government of the United States moved to annul the patent issued to Bell on the grounds of fraud and misrepresentation\"), to which Rockland wrote:\nThe United States House of Representatives in its resolution of June 11, 2002 states merely that the government of the United States filed the lawsuit, and that the Supreme Court found the complaint viable and remanded the case for trial. The House of Representatives resolution, in my opinion, leaves out many of the salient details [of the Garland-initiated suit], and presents a disjointed and incorrect view of the invention of the telephone.\n\nThe same paragraph No. 9 of the Meucci resolution also did not mention that the U.S. Attorney General, plus another cabinet member and two senators had been given or owned millions of dollars of stock in Pan-Electric, as revealed by Joseph Pulitzer in the \"New York World\"', a fact which many viewed as a strong incentive for them to try to overturn Bell's patent.\n\nPresident Grover Cleveland subsequently ordered the Attorney General not to 'pursue the matter' after the court case stalled out. One of several biographies on the controversial Attorney General Augustus Garland's involvement in the 'Government case' noted:\nHe did, however, suffer scandal involving the patent for the telephone. The Attorney General's office was intervening in a lawsuit attempting to break Bell's monopoly of telephone technology, but it had come out that Garland owned stock in one of the companies that stood to benefit. This congressional investigation received public attention for nearly a year, and caused his work as attorney general to suffer.\n\nThe 'Government case' became one of the greatest scandals in Grover Cleveland's presidency, and was ended when Cleveland ordered Garland to discontinue the trial.\n\nOther factual errors were also found within the preamble to the HRes 269 resolution; among them were:\n\nIn total Grosvenor listed ten errors in detail found within the congressional Meucci resolution, and was highly critical of both its intent and accuracy. He also asked two salient questions in Section \"C\" of the same report: \"1) Should Congress overrule the US courts and its own committee, which looked at evidence extensively, and without reviewing any evidence in the matter?\", and \"2) Should [the U.S.] Congress pass resolutions on historical facts without checking with legitimate historians or their own library?\" The same document also noted that HRes 269 contradicted the findings of its own Congressional committee's investigation, which had, in 1886, produced a report of 1,278 printed pages.\n\nGrosvenor concluded that: \"The historical facts stated in HR 269 were obtained from highly biased sources, and [were] based on shoddy, cursory research.\"\n\n\nNotes\n\nBibliography\n\n"}
