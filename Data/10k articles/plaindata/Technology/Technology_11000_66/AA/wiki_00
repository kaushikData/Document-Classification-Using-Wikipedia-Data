{"id": "21023991", "url": "https://en.wikipedia.org/wiki?curid=21023991", "title": "ARINC 818", "text": "ARINC 818\n\nARINC 818: Avionics Digital Video Bus (ADVB) is a video interface and protocol standard developed for high bandwidth, low latency, uncompressed digital video transmission in avionics systems. The standard, which was released in January 2007, has been advanced by ARINC and the aerospace community to meet the stringent needs of high performance digital video. The specification was updated and ARINC 818-2 was released in December 2013, adding a number of new features, including link rates up to 32X fibre channel rates, channel-bonding, switching, field sequential color, bi-directional control, and data only links.\n\nIn aircraft, an ever-increasing amount of information is supplied in the form of images, this information passes through a complex video system before reaching cockpit displays. Video systems include: infrared and other wavelength sensors, optical cameras, radar, flight recorders, map/chart systems, synthetic vision, image fusion systems, heads-up displays (HUD) and heads-down primary flight and multifunction displays, video concentrators, and other subsystems. Video systems are used for taxi and take-off assist, cargo loading, navigation, target tracking, collision avoidance, and other critical functions.\n\nARINC 818/ADVB is a Fibre Channel (FC) protocol that builds on FC-AV (Fibre Channel Audio Video, defined in ANSI INCITS 356-2002), which was used extensively on video systems in the F-18 and the C-130AMP. Although FC-AV has been used on numerous programs, each implementation has been unique. ARINC 818 provides an opportunity to standardize high-speed video systems and has since been adopted by a number of high-profile commercial and military aerospace programs, including the A400M, A350XWB, B787, KC-46A, Comac C919, and numerous other programs. ARINC 818 is also common in avionics suites, such as Proline Fusion by Rockwell Collins, the TopDeck by Thales.\n\nARINC 818 is a point-to-point, 8B/10B encoded (or 64B/66B for higher speeds) serial protocol for transmission of video, audio, and data. The protocol is packetized but is video-centric and very flexible, supporting an array of complex video functions including the multiplexing of multiple video streams on a single link or the transmission of a single stream over a dual link. Four different classes of video are defined, from simple asynchronous to stringent pixel synchronous systems.\n\nThe ADVB frame is the basic transport mechanism for ARINC 818. It is important to refer to these packets as “ADVB frames” rather than simply “frames” to eliminate potential confusion with video frames.\n\nThe start of an ADVB frame is signaled by a SOFx 4-byte ordered set and terminated with an EOFx ordered set. Every ADVB frame has a standard Fibre Channel header composed of six 32-bit words. These header words pertain to such things as the ADVB frame origin and intended destination and the ADVB frames position within the sequence. The Source ID field (SID) in the ADVB frame header allows video from each sensor to be distinguished from the other sensors.\n\nThe “payload” contains either video, video parameters or ancillary data. The payload can vary in size, but is limited to 2112 bytes per ADVB frame. To insure data integrity, all ADVB frames have a 32-bit CRC calculated for data between the SOFx and the CRC word. The CRC is the same 32-bit polynomial calculation defined for Fibre Channel.\n\nThe ARINC 818 specification defines a “container” as a set of ADVB frames used to transport video. In other words, a video image and data is encapsulated into a “container” that spans many ADVB frames. The “payload” of each ADVB frame contains either data or video. Within a container, ARINC 818 defines objects that contain certain types of data. That is, certain ADVB frames within the container are part of an object.\n\nAn example of how ARINC 818 transmits color XGA provides a good overview. XGA RGB requires ~141M bytes/s of data transfer (1024 pixels x 3 bytes per pixel x 768 lines x 60 Hz). Adding the protocol overhead and blanking time, a standard link rate of 2.125Gbit/s is required. ARINC 818 “packetizes” video images into Fibre Channel frames. Each FC frame begins with a 4 byte ordered set, called an SOF (Start of Frame), and ends with an EOF (End of Frame), additionally, a 4 byte CRC is included for data integrity. The payload of the first FC frame in a sequence contains embedded header data that accompanies each video image.\n\nEach XGA video line requires 3072 bytes, which exceeds the maximum FC payload length, so each line is divided into two FC frames. Transporting an XGA image requires a “payload” of 1536 FC frames. Additionally, an ADVB header frame is added, making a total of 1537 FC frames. Idle characters are required between FC frames because they are used for synchronization between transmitters and receivers.\n\nAlthough ARINC 818 was developed specifically for avionics applications, the protocol is already being used in sensor fusion applications where multiple sensor outputs are multiplexed onto a single high-speed link. Features added in ARINC 818-2 facilitate using ARINC 818 as a sensor interface.\n\nThe ARINC 818 specification does not mandate which physical layer is to be used and implementations are done using both copper and fiber. Although the majority of implementation use fiber, low-speed implementations of ARINC 818 (1.0625Gbit/s and 2.125Gbit/s) sometimes use copper (twinax or STP). The most commonly, either 850 nm MM fiber (<500m) or 1310 nm SM fiber (up to 10 km) is used, however, in Russia and India, 1310 multi-mode fiber has also been used. ARINC 818 lends itself to applications that require few conductors (slip rings, turrets), low weight (aerospace), EMI resistance, or long distance transmission (aerospace, ships).\n\nARINC 818 is flexible and can accommodate many types of video and data applications. It is the intention of the standard that all implementation be accompanied by a small interface control document (ICD) that defines key parameters of the header such as: link speed, video resolution, color scheme, size of ancillary data, timing classification, or bit-packing schemes. Interoperability is only guaranteed among equipment built to the same ICD.\n\nARINC 818 uses a FC physical layer that can be constructed from any FC compatible 8B/10B SerDes, which are common in large FPGAs such as:<br>\nThe Xilinx Virtex 2 Pro, Xilinx Virtex 5, 6 and 7, Artix 7, Kintex 7<br>\nAltera Cyclone V, ARRIA II GX, ARRIA 5, ARRIA 10, Stratix V and other FPGAs.\n\nARINC 818 transmitters must assemble valid FC frames, including starting and ending ordered sets, headers, and CRC. This can easily be done with VHDL state machines, and many PLD SerDes include built in CRC calculations.\n\nThe flexibility of ARINC 818 allows for receiver implementations using either full image buffers or just display-line buffers. For either, synchronization issues must be considered at the pixel, line, and frame level.\n\nLine buffer or FIFO-based receivers will require that the transmitter adhere to strict line timing requirements of the display. Since the display horizontal scanning must be precise, the arrival time of lines will also need to be precise. ARINC 818 intends that timing parameters such as these be captured in an ICD specific to the video system.\n\nThe authors of ARINC 818 built upon many years of combined experience of using FC to transport different video formats, and key implementation details are included in the specification, including examples of common analog formats.\n\nARINC 818-2, ratified in December 2013, adds features to accommodate higher link rates, support for compression and encryption, networking, and sophisticated display schemes, such as channel bonding used on large area displays (LADs).\n\nLink rates: At the time the original ARINC 818 specification was ratified, the fiber-channel protocol supported link rates up to 8.5 gigabits per second (Gb/s). ARINC 818-2 added rates of 5.0, 6.375 (FC 6x), 12.75 (FC 12x), 14.025 (FC 16x), 21.0375 (FC 24x), and 28.05 (FC 32x) Gb/s. The 6x, 12x, and 24x speeds were added to accommodate the use of high-speed, bi-directional coax with power as a physical medium. The 5 Gbit/s rate was added to accommodate so implementation specific speeds supported by certain FPGAs. The specification also provides for non-standard link rates for bi-directional return path for applications such as camera control where high speed video links are not required.\n\nCompression and Encryption: ARINC 818 was originally envisioned as carrying only uncompressed video and audio. Applications such as high-resolution sensors, UAV/UAS with bandwidth limited downlinks, and data only applications drove the need to compress and/or encrypt a link. Sticking to a philosophy of maximum flexibility, the ARINC 818-2 calls for the ICD to specify implementation details for compression and encryption. The ARINC 818 protocol does not provide a means for compression and encryption, it simply provides flags to indicate that payload is compressed or encrypted.\n\nSwitching: ARINC 818 was designed as a point-to-point protocol. Since many of the newer implementations of the ARINC 818 have multiple displays and or many channels of ARINC 818 (10 or more), switching has become more important. The new specification requires that active switching can only occur between frames. In effect, to prevent broken video frames, the switch must wait until the vertical blanking. Again, the ICD controls the implementation details.\n\nField Sequential Color: A video format code was added to support field sequential color. The color field-sequential mode will typically send each color component in a separate container.\n\nChannel Bonding: To overcome link bandwidth limitations of FPGAs, ARINC 818-2 supports multiple links in parallel. The video frame is broken into smaller segments and transmitted on two or more links. Each link must transmit a complete ADVB frame with header, and the ICD addresses latency and skew between the links.\n\nData-only Links: ARINC 818-2 provides for data-only links, typically used in command-and-control channels, such as those needed for bi-directional camera interfaces. These may employ a standard link rate or a non-standard rate specified by the ICD.\n\nRegions of Interest: The ARINC 818-2 protocol provides a means for defining partial images, tiling, and region-of-interest that are important for high-speed sensors and stereo displays.\n\n\n\n"}
{"id": "397169", "url": "https://en.wikipedia.org/wiki?curid=397169", "title": "Acer Inc.", "text": "Acer Inc.\n\nAcer Inc. ( ; , lit. \"Hongji Corporation Ltd.\"; commonly known as Acer and stylized as acer) is a Taiwanese multinational hardware and electronics corporation, specializing in advanced electronics technology, headquartered in Xizhi, New Taipei City, Taiwan. Acer's products include desktop PCs, laptop PCs (clamshells, 2-in-1s, convertibles and Chromebooks), tablets, servers, storage devices, virtual reality devices, displays, smartphones and peripherals.\n\nAcer also sells gaming PCs and accessories under its Predator brand. In the early 2000s, Acer implemented a new business model, shifting from a manufacturer to a designer, marketer and distributor of products, while performing production processes via contract manufacturers. In 2015, Acer was the sixth-largest personal computer vendor in the world. Currently, in addition to its core IT products business, Acer also has a new business entity that focuses on the integration of cloud services and platforms, and the development of smartphones and wearable devices with value-added IoT applications.\n\nAcer was founded by Stan Shih (施振榮), his wife Carolyn Yeh, and a group of five others as Multitech in 1976, headquartered in Hsinchu City, Taiwan.\n\nThe company began with eleven employees and US$25,000 in capital. Initially, it was primarily a distributor of electronic parts and a consultant in the use of microprocessor technologies. It produced the Micro-Professor MPF-I training kit, then two Apple II clones; the Microprofessor II and III before joining the emerging IBM PC compatible market, and becoming a significant PC manufacturer. The company was renamed Acer in 1987.\n\nIn 1998, Acer reorganized into five groups: Acer International Service Group, Acer Sertek Service Group, Acer Semiconductor Group, Acer Information Products Group, and Acer Peripherals Group. To dispel complaints from clients that Acer competed with its own products and to alleviate the competitive nature of the branded sales vs. contract manufacturing businesses, in 2000 the company spun off the contract business, renaming it Wistron Corporation. The restructuring resulted in two primary units: brand name sales and contract manufacturing. In 2001 the company got rid of its manufacturing units, BenQ and Wistron to focus resources on design and sales.\n\nAcer increased worldwide sales while simultaneously reducing its labor force by identifying and using marketing strategies that best utilized their existing distribution channels. By 2005, Acer employed a scant 7,800 people worldwide. Revenues rose from US$4.9 billion in 2003 to US$11.31 billion in 2006.\n\nAcer's North American market share has slipped over the past few years, while in contrast, the company's European market share has risen.\n\nIn the mid-2000s years, consumer notebooks have been almost the sole growth drivers for the PC industry, and Acer's exceptionally low overheads and dedication to the channel had made it one of the main beneficiaries of this trend. Acer grew quickly in Europe in part by embracing the use of more traditional distribution channels targeting retail consumers when some rivals were pursuing online sales and business customers. In 2007 Acer bought Gateway in the USA and Packard Bell in Europe and became the Number 3 world provider of computers and number 2 for notebooks, and achieved significant improvement in profitability. Acer has been striving to become the world's largest PC vendor, in the belief that the goal can help it achieve economy of scale and garner higher margin. But such a reliance on the high-volume, low-value PC market made Acer exposed when buying habits changed.\n\nIn November 2013 Chairman and CEO J.T. Wang, and President Jim Wong, both resigned due to the company's poor financial performance. Wang had been reportedly due to leave Acer at year end, and was supposed to have been succeeded by Wong. Acer co-founder Stan Shih took over as board chairman and interim president after the departure of Wang and Wong and began to search for new candidates to assume the roles of CEO and president. On 23 December, Acer named Jason Chen, vice president of worldwide sales and marketing at Taiwan Semiconductor Manufacturing, as its new president and CEO, effective 1 January.\n\n\nAcer has 7,000+ employees worldwide, operates in 70 countries, and has approximately 95,000 retail locations spread throughout 160+ countries.\n\nThe Australian subsidiary of Acer is Acer Computer Australia (ACA). The subsidiary was established in 1990, and is currently Australia's third-largest personal computer vendor, behind Hewlett-Packard Australia and Dell Australia and New Zealand. Acer Computer Australia has Australia's highest overall market share in notebook PC and tablet PC sales. The company is also Australia's leading PC vendor in government and education markets. Acer Computer Australia has 480 employees as of 2006.\n\nThe company repairs, assembles and manufacturers laptops and desktops in Sydney.\n\nAcer's EMEA headquarters are located in Lugano, Switzerland. From the late 90's to mid-2000's Acer had computer factories in Europe. The business area was the whole EMEA. In the Netherlands under the name of Acer IMS bv there were two factories: Acer laptop factory in Den Bosch and Acer and IBM desktop factory in Tilburg. Acer also had facilities in Germany under the name of IMS in Ahrensburg and Hamburg.\n\nAcer's subsidiary in India is Acer India (Pvt) Limited, and was incorporated as a wholly owned subsidiary of Acer Computer International, Ltd. in 1999. It is a notable vendor in key segments such as education, desktop computers and low profile notebooks for education. The headquarters is in Bangalore, India.\n\nPT Acer Indonesia is a wholly owned subsidiary of Acer Inc and distributes their products through their main distributor PT Dragon Computer & Communication. Acer is currently the second largest computer vendor in Indonesia. In Q1 2016, Acer recorded >81% market share in Windows tablet in Indonesia.\n\nAcer America Corporation, headquartered in San Jose, California, is a member of the Acer Group. Acer's R&D, engineering, manufacturing, and marketing operations in the United States and Canada are handled by Acer America. The U.S. headquarters was opened with a staff of three in 1985, as Multitech Electronics USA, in Mountain View, California. In 1986, the U.S. headquarters was moved to San Jose, California.\n\n\n\nIn 2005, Acer published its first environmental report, for which the company used the GRI guidelines. All of Acer’s tier-one suppliers have acquired ISO 14001 certification.\n\nIn November 2012, Acer was ranked 4th place out of 15 in Greenpeace’s re-launched Guide to Greener Electronics, with a score of 5.1 points out of 10. The Guide ranks electronics makers according to their policies and practices to reduce their impact on the climate, produce greener products, and make their operations more sustainable.\n\nGreenpeace criticized the company for not setting out targets to reduce greenhouse gas (GHG) emissions as intended in 2010 and for not providing external verification for the GHG emissions it reports for its operations and business travel. It also scored badly on the products criteria receiving no points on product lifecycle while Greenpeace noted that a higher percentage of its products need to meet or exceed Energy Star standards in order for it to score more points.\n\nIt received some praise for launching new products which are free from polyvinyl chloride plastic (PVC) and brominated flame retardants (BFRs) and the company informed Greenpeace\nthat the majority of its products will be PVC/BFR free in the near future. Acer also scored well on chemical management for lobbying for restrictions on organo-halogens and was commended for reporting on GHG emissions from its first-tier suppliers and investigating its second tier.\n\nIn its 2012 report on progress relating to conflict minerals, the Enough Project rated Acer the seventh highest of 24 consumer electronics companies.\n\nAcer has been listed on the DJSI’s Emerging Markets Index since 2014 and on MSCI’s Global Sustainability Indexes since 2015.\n\n\n\n"}
{"id": "36817291", "url": "https://en.wikipedia.org/wiki?curid=36817291", "title": "Archer (company)", "text": "Archer (company)\n\nArcher is a mobile technology company that offers services such as MMS and SMS messaging with the ability to deliver products like mobile coupons, QR codes, Microsoft tags, and mobile bank statements. The company, based in Seattle, WA, was formed from a merger of iLoop Mobile and Lenco Mobile in 2011. Combining Lenco Mobile's international operations and iLoop Mobile's mobile technology, Archer has campaigns running on a global scale. More than 4 billion people worldwide own a mobile device and Archer targets those consumers for companies, agencies and public institutions. Some of the company's work includes infrastructure development in Africa, providing mobile messaging products for clients such as African Bank, Standard Bank, the MTN group, and Vodacom. The company also provided the mobile website for the Obama campaign in the 2008 presidential election.\n\n\n"}
{"id": "2928", "url": "https://en.wikipedia.org/wiki?curid=2928", "title": "Association for Computing Machinery", "text": "Association for Computing Machinery\n\nThe Association for Computing Machinery (ACM) is an international learned society for computing. It was founded in 1947, and is the world's largest scientific and educational computing society. The ACM is a non-profit professional membership group, with more than 100,000 members . Its headquarters are in New York City.\n\nThe ACM is an umbrella organization for academic and scholarly interests in computer science. Its motto is \"Advancing Computing as a Science & Profession\".\n\nThe ACM was founded in 1947 under the name \"Eastern Association for Computing Machinery\", which was changed the following year to the Association for Computing Machinery.\n\nACM is organized into over 171 local chapters and 37 Special Interest Groups (SIGs), through which it conducts most of its activities. Additionally, there are over 500 college and university chapters. The first student chapter was founded in 1961 at the University of Louisiana at Lafayette.\n\nMany of the SIGs, such as SIGGRAPH, SIGPLAN, SIGCSE and SIGCOMM, sponsor regular conferences, which have become famous as the dominant venue for presenting innovations in certain fields. The groups also publish a large number of specialized journals, magazines, and newsletters.\n\nACM also sponsors other computer science related events such as the worldwide ACM International Collegiate Programming Contest (ICPC), and has sponsored some other events such as the chess match between Garry Kasparov and the IBM Deep Blue computer.\n\nACM publishes over 50 journals including the prestigious \"Journal of the ACM\", and two general magazines for computer professionals, \"Communications of the ACM\" (also known as \"Communications\" or \"CACM\") and \"Queue\". Other publications of the ACM include:\n\nAlthough \"Communications\" no longer publishes primary research, and is not considered a prestigious venue, many of the great debates and results in computing history have been published in its pages.\n\nACM has made almost all of its publications available to paid subscribers online at its Digital Library and also has a Guide to Computing Literature. Individual members additionally have access to Safari Books Online and Books24x7. ACM also offers insurance, online courses, and other services to its members.\n\nIn 1997, ACM Press published \"Wizards and Their Wonders: Portraits in Computing\" (), written by Christopher Morgan, with new photographs by Louis Fabian Bachrach. The book is a collection of historic and current portrait photographs of figures from the computer industry.\n\nThe ACM Portal is an online service of the ACM. \nIts core are two main sections: ACM Digital Library and the ACM Guide to Computing Literature.\n\nThe ACM Digital Library is the full-text collection of all articles published by the ACM in its articles, magazines and conference proceedings. The Guide is a bibliography in computing with over one million entries.\nThe ACM Digital Library contains a comprehensive archive starting in the 1950s of the organization's journals, magazines, newsletters and conference proceedings. Online services include a forum called Ubiquity and Tech News digest. There is an extensive underlying bibliographic database containing key works of all genres from all major publishers of computing literature. This secondary database is a rich discovery service known as The ACM Guide to Computing Literature.\n\nACM adopted a hybrid Open Access (OA) publishing model in 2013. Authors who do not choose to pay the OA fee must grant ACM publishing rights by either a copyright transfer agreement or a publishing license agreement.\n\nACM was a \"green\" publisher before the term was invented. Authors may post documents on their own websites and in their institutional repositories with a link back to the ACM Digital Library's permanently maintained Version of Record.\n\nAll metadata in the Digital Library is open to the world, including abstracts, linked references and citing works, citation and usage statistics, as well as all functionality and services. Other than the free articles, the full-texts are accessed by subscription.\n\nThere is also a mounting challenge to the ACM's publication practices coming from the open access movement. Some authors see a centralized peer–review process as less relevant and publish on their home pages or on unreviewed sites like arXiv. Other organizations have sprung up which do their peer review entirely free and online, such as Journal of Artificial Intelligence Research (JAIR), Journal of Machine Learning Research (JMLR) and the Journal of Research and Practice in Information Technology.\n\nIn addition to student and regular members, ACM has several advanced membership grades to recognize those with multiple years of membership and \"demonstrated performance that sets them apart from their peers\".\n\nThe ACM Fellows Program was established by Council of the Association for Computing Machinery in 1993 \"to recognize and honor outstanding ACM members for their achievements in computer science and information technology and for their significant contributions to the mission of the ACM.\" Since March 2015, there are about 958 Fellows out of about 75,000 professional members.\n\nIn 2006, ACM began recognizing two additional membership grades, one which was called Distinguished Members. Distinguished Members (Distinguished Engineers, Distinguished Scientists, and Distinguished Educators) have at least 15 years of professional experience and 5 years of continuous ACM membership and \"have made a significant impact on the computing field\". Note that in 2006 when the Distinguished Members first came out, one of the three levels was called \"Distinguished Member\" and was changed about two years later to \"Distinguished Educator\". Those who already had the Distinguished Member title had their titles changed to one of the other three titles.\n\nAlso in 2006, ACM began recognizing Senior Members. Senior Members have ten or more years of professional experience and 5 years of continuous ACM membership.\n\nWhile not technically a membership grade, the ACM recognizes distinguished speakers on topics in computer science. A distinguished speaker is appointed for a three-year period. There are usually about 125 current distinguished speakers. The ACM website describes these people as 'Renowned International Thought Leaders'. The distinguished speaker program is overseen by a committee \n\nNorman E. Gibbs served as the president of the ACM.\n\nACM has three kinds of chapters: Special Interest Groups, Professional Chapters, and Student Chapters.\n\n, ACM has professional & SIG Chapters in 56 countries.\n\n, there exist ACM student chapters in 41 different countries.\n\nACM and its Special Interest Groups (SIGs) sponsors numerous conferences with 170 hosted worldwide in 2017. ACM Conferences page has an up-to-date complete list while a partial list is shown below. Most of the SIGs also have an annual conference. ACM conferences are often very popular publishing venues and are therefore very competitive. For example, the 2007 SIGGRAPH conference attracted about 30000 visitors, and CIKM only accepted 15% of the long papers that were submitted in 2005.\n\n\nThe ACM is a co–presenter and founding partner of the Grace Hopper Celebration of Women in Computing (GHC) with the Anita Borg Institute for Women and Technology.\n\nSome conferences are hosted by ACM student branches; this includes Reflections Projections, which is hosted by UIUC ACM.. In addition, ACM sponsors regional conferences. Regional conferences facilitate increased opportunities for collaboration between nearby institutions and they are well attended.\n\nFor additional non-ACM conferences, see this list of computer science conferences.\n\nThe ACM presents or co–presents a number of awards for outstanding technical and professional achievements and contributions in computer science and information technology. \n\n\nOver 30 of ACM's Special Interest Groups also award individuals for their contributions with a few listed below.\n\nThe President of ACM for 2018–2020 is Cherri M. Pancake, Professor Emeritus at Oregon State University and Director of the Northwest Alliance for Computational Science and Engineering (NACSE). She is successor of Vicki L. Hanson (2016-2018), Distinguished Professor at the Rochester Institute of Technology and Visiting Professor at the University of Dundee; Alexander L. Wolf (2014–2016), Dean of the Jack Baskin School of Engineering at the University of California, Santa Cruz; Vint Cerf (2012–2014), an American computer scientist who is recognized as one of \"the fathers of the Internet\"; Alain Chesnais (2010–2012), a French citizen living in Toronto, Ontario, Canada, where he runs his company named Visual Transitions; and Dame Wendy Hall of the University of Southampton, UK (2008–2010).\n\nACM is led by a Council consisting of the President, Vice-President, Treasurer, Past President, SIG Governing Board Chair, Publications Board Chair, three representatives of the SIG Governing Board, and seven Members–At–Large. This institution is often referred to simply as \"Council\" in \"Communications of the ACM\".\n\nACM has five \"Boards\" that make up various committees and subgroups, to help Headquarters staff maintain quality services and products. These boards are as follows:\n\nACM-W, the ACM council on women in computing, supports, celebrates, and advocates internationally for the full engagement of women in computing. ACM–W's main programs are regional celebrations of women in computing, ACM-W chapters, and scholarships for women CS students to attend research conferences. In India and Europe these activities are overseen by ACM-W India and ACM-W Europe respectively. ACM-W collaborates with organizations such as the Anita Borg Institute, the National Center for Women & Information Technology (NCWIT), and .\n\nThe ACM-W gives an annual Athena Lecturer Award to honor outstanding women researchers who have made fundamental contributions to computer science. This program began in 2006. Speakers are nominated by SIG officers.\n\nACM's primary partner has been the IEEE Computer Society (IEEE-CS), which is the largest subgroup of the Institute of Electrical and Electronics Engineers (IEEE). The IEEE focuses more on hardware and standardization issues than theoretical computer science, but there is considerable overlap with ACM's agenda. They have many joint activities including conferences, publications and awards. ACM and its SIGs co-sponsor about 20 conferences each year with IEEE-CS and other parts of IEEE. Eckert-Mauchly Award and Ken Kennedy Award, both major awards in computer science, are given jointly by ACM and the IEEE-CS. They occasionally cooperate on projects like developing computing curricula.\n\nACM has also jointly sponsored on events with other professional organizations like the Society for Industrial and Applied Mathematics (SIAM).\n\n\n"}
{"id": "59161710", "url": "https://en.wikipedia.org/wiki?curid=59161710", "title": "BIM Collaboration Format", "text": "BIM Collaboration Format\n\nThe BIM Collaboration Format (BCF) is a structured file format which allows issue tracking with a building information model. BCF is designed primarily for attaching information to collisions and errors connected with specific objects in a model, but can be used for general issue tracking in building construction projects. This allows developers of BIM-supporting software to design interfaces for collaboration between users who access the same model, especially with different software.\n\nThe format was developed by Tekla and Solibri and later adopted as a standard by buildingSMART. Notable software with native support for BCF are Solibri, Tekla with plugins which provide usability for other software.\n\n\n"}
{"id": "5014", "url": "https://en.wikipedia.org/wiki?curid=5014", "title": "Bistability", "text": "Bistability\n\nIn a dynamical system, bistability means the system has two stable equilibrium states. Something that is bistable can be resting in either of two states. These rest states need not be symmetric with respect to stored energy. An example of a mechanical device which is bistable is a light switch. The switch lever is designed to rest in the \"on\" or \"off\" position, but not between the two. Bistable behavior can occur in mechanical linkages, electronic circuits, nonlinear optical systems, chemical reactions, and physiological and biological systems.\n\nIn a conservative force field, bistability stems from the fact that the potential energy has two local minima, which are the stable equilibrium points. By mathematical arguments, a local maximum, an unstable equilibrium point, must lie between the two minima. At rest, a particle will be in one of the minimum equilibrium positions, because that corresponds to the state of lowest energy. The maximum can be visualized as a barrier between them.\n\nA system can transition from one state of minimal energy to the other if it is given enough activation energy to penetrate the barrier (compare activation energy and Arrhenius equation for the chemical case). After the barrier has been reached, the system will relax into the other minimum state in a time called the relaxation time.\n\nBistability is widely used in digital electronics devices to store binary data. It is the essential characteristic of the flip-flop, a circuit widely used in latches and some types of semiconductor memory. A bistable device can store one bit of binary data, with one state representing a \"0\" and the other state a \"1\". It is also used in relaxation oscillators, multivibrators, and the Schmitt trigger.\nOptical bistability is an attribute of certain optical devices where two resonant transmissions states are possible and stable, dependent on the input.\nBistability can also arise in biochemical systems, where it creates digital, switch-like outputs from the constituent chemical concentrations and activities. It is often associated with hysteresis in such systems.\n\nIn the mathematical language of dynamic systems analysis, one of the simplest bistable systems is\n\nThis system describes a ball rolling down a curve with shape formula_2, and has three steady-states: formula_3, formula_4, and formula_5. The middle steady-state formula_6 is unstable, while the other two states are stable. The direction of change of formula_7 over time depends on the initial condition formula_8. If the initial condition is positive (formula_9), then the solution formula_7 approaches 1 over time, but if the initial condition is negative (formula_11), then formula_7 approaches −1 over time. Thus, the dynamics are \"bistable\". The final state of the system can be either formula_3 or formula_14, depending on the initial conditions.\n\nThe appearance of a bistable region can be understood for the model system \nformula_15\nwhich undergoes a supercritical pitchfork bifurcation with bifurcation parameter formula_16.\n\nBistability is key for understanding basic phenomena of cellular functioning, such as decision-making processes in cell cycle progression, cellular differentiation, and apoptosis. It is also involved in loss of cellular homeostasis associated with early events in cancer onset and in prion diseases as well as in the origin of new species (speciation).\n\nBistability can be generated by a positive feedback loop with an ultrasensitive regulatory step. Positive feedback loops, such as the simple X activates Y and Y activates X motif, essentially links output signals to their input signals and have been noted to be an important regulatory motif in cellular signal transduction because positive feedback loops can create switches with an all-or-nothing decision. Studies have shown that numerous biological systems, such as \"Xenopus\" oocyte maturation, mammalian calcium signal transduction, and polarity in budding yeast, incorporate temporal (slow and fast) positive feedback loops, or more than one feedback loop that occurs at different times. Having two different temporal positive feedback loops or “dual-time switches” allows for (a) increased regulation: two switches that have independent changeable activation and deactivation times; and (b) linked feedback loops on multiple timescales can filter noise.\n\nBistability can also arise in a biochemical system only for a particular range of parameter values, where the parameter can often be interpreted as the strength of the feedback. In several typical examples, the system has only one stable fixed point at low values of the parameter. A saddle-node bifurcation gives rise to a pair of new fixed points emerging, one stable and the other unstable, at a critical value of the parameter. The unstable solution can then form another saddle-node bifurcation with the initial stable solution at a higher value of the parameter, leaving only the higher fixed solution. Thus, at values of the parameter between the two critical values, the system has two stable solutions. An example of a dynamical system that demonstrates similar features is\n\nwhere formula_18 is the output, and formula_19 is the parameter, acting as the input.\n\nBistability can be modified to be more robust and to tolerate significant changes in concentrations of reactants, while still maintaining its \"switch-like\" character. Feedback on both the activator of a system and inhibitor make the system able to tolerate a wide range of concentrations. An example of this in cell biology is that activated CDK1 (Cyclin Dependent Kinase 1) activates its activator Cdc25 while at the same time inactivating its inactivator, Wee1, thus allowing for progression of a cell into mitosis. Without this double feedback, the system would still be bistable, but would not be able to tolerate such a wide range of concentrations.\n\nBistability has also been described in the embryonic development of \"Drosophila melanogaster\" (the fruit fly). Examples are anterior-posterior and dorso-ventral axis formation and eye development.\n\nA prime example of bistability in biological systems is that of Sonic hedgehog (Shh), a secreted signaling molecule, which plays a critical role in development. Shh functions in diverse processes in development, including patterning limb bud tissue differentiation. The Shh signaling network behaves as a bistable switch, allowing the cell to abruptly switch states at precise Shh concentrations. \"gli1\" and \"gli2\" transcription is activated by Shh, and their gene products act as transcriptional activators for their own expression and for targets downstream of Shh signaling. Simultaneously, the Shh signaling network is controlled by a negative feedback loop wherein the Gli transcription factors activate the enhanced transcription of a repressor (Ptc). This signaling network illustrates the simultaneous positive and negative feedback loops whose exquisite sensitivity helps create a bistable switch.\n\nBistability can only arise in biological and chemical systems if three necessary conditions are fulfilled: positive feedback, a mechanism to filter out small stimuli and a mechanism to prevent increase without bound.\n\nBistable chemical systems have been studied extensively to analyze relaxation kinetics, non-equilibrium thermodynamics, stochastic resonance, as well as climate change. In bistable spatially extended systems the onset of local correlations and propagation of traveling waves have been analyzed.\n\nBistability is often accompanied by hysteresis. On a population level, if many realisations of a bistable system are considered (e.g. many bistable cells (speciation)), one typically observes bimodal distributions. In an ensemble average over the population, the result may simply look like a smooth transition, thus showing the value of single-cell resolution.\n\nBistability is the ability of a material to present in two stable phases that can both exist within a given range of temperatures but above and below that range only one or the other phase exists.\n\nBistability as applied in the design of mechanical systems is more commonly said to be \"over centre\"—that is, work is done on the system to move it just past the peak, at which point the mechanism goes \"over centre\" to its secondary stable position. The result is a toggle-type action- work applied to the system below a threshold sufficient to send it 'over center' results in no change to the mechanism's state.\n\nSprings are a common method of achieving an \"over centre\" action. A spring attached to a simple two position ratchet-type mechanism can create a button or plunger that is clicked or toggled between two mechanical states. Many ballpoint and rollerball retractable pens employ this type of bistable mechanism.\n\nAn even more common example of an over-center device is an ordinary electric wall switch. These switches are often designed to snap firmly into the \"on\" or \"off\" position once the toggle handle has been moved a certain distance past the center-point.\n\nA ratchet-and-pawl is an elaboration—a multi-stable \"over center\" system used to create irreversible motion. The pawl goes over center as it is turned in the forward direction. In this case, \"over center\" refers to the ratchet being stable and \"locked\" in a given position until clicked forward again; it has nothing to do with the ratchet being unable to turn in the reverse direction.\n\n\n"}
{"id": "5730478", "url": "https://en.wikipedia.org/wiki?curid=5730478", "title": "Canadian Alliance Against Software Theft", "text": "Canadian Alliance Against Software Theft\n\nThe Canadian Alliance Against Software Theft (CAAST) is a Canadian trade group affiliated with the Software Alliance (formerly known as the Business Software Alliance). Its mission statement is to \"reduce software piracy in Canada through education, public policy and enforcement.\" The CAAST was established in 1990.\n\nNote that the web-site and domain formerly owned by CAAST is now owned by some other Florida-based party. Therefore, use extreme caution if you choose to access the web-site.\n\n"}
{"id": "11870490", "url": "https://en.wikipedia.org/wiki?curid=11870490", "title": "Chinese furniture", "text": "Chinese furniture\n\nThe forms of Chinese furniture evolved along three distinct lineages which dates back to 1000 BC, based on \"frame and panel\", \"yoke and rack\" (based on post and rail seen in architecture) and \"bamboo\" construction techniques. Chinese home furniture evolved independently of Western furniture into many similar forms including chairs, tables, stools, cupboards, cabinets, beds and sofas. Until about the 10th century CE the Chinese sat on mats or low platforms using low tables, in typical Asian style, but then gradually moved to using high tables with chairs. \n\nChinese furniture is mostly in plain polished wood, but from at least the Song dynasty the most luxurious pieces often used lacquer to cover the whole or parts of the visible areas. All the various sub-techniques of Chinese lacquerware can be found on furniture, and become increasingly affordable down the social scale, and so widely used, from about the Ming dynasty onwards. carved lacquer furniture was at first only affordable by the imperial family or the extremely rich, but by the 19th century was merely very expensive, and mostly found in smaller pieces or as decorated areas on larger ones. It was especially popular on screens, which were common in China. Lacquer inlaid with mother of pearl was especially a technique used on furniture.\n\nChinese furniture is usually light where possible, anticipating Europe by several centuries in this respect. Practical fittings in metal such as hinges, lock plates, drawer handles and protective plates at edges or feet are used, and often given considerable emphasis, but compared to classic fine European furniture purely decorative metal mounts were rare. From the Qing dynasty furniture made for export, mostly to Europe, became a distinct style, generally made in rather different shapes to suit the destination markets and highly decorated in lacquer and other techniques. \n\nChinese furniture for sitting or lying on was very often used with cushions, but textiles and upholstery are not, until very late historical periods, incorporated into the piece itself in the Western manner. Openwork in carved wood or other techniques is very typical for practical purposes such as chair-backs, and also for decoration. The Ming period is regarded as the \"golden age\" of Chinese furniture, though very few examples of earlier pieces survive. Ming styles have largely set the style for furniture in traditional Chinese style in subsequent periods, though as in other areas of Chinese art, the 18th and 19th centuries saw increasing prosperity used for sometimes excessively elaborated pieces, as wider groups in society were able to imitate court styles.\n\nWhat is now considered the Chinese aesthetic had its origins in China as far back as 1500–1000 BC. The furniture present in some of the artwork from that early period shows woven mats on elevated floors, sometimes accompanied by arm rests, providing seating accompanied by low tables. In this early period both unadorned and intricately engraved and painted pieces were already developing. High chairs, usually single ones, had existed as status symbols, effectively thrones, since at least the Eastern Zhou period (771–256 BCE), but were not used with tables at the same level. \n\nBuddhism, entering China around AD 200, brought with it the idea of (the Buddha) sitting upon a raised platform instead of simply mats. The platform was adopted as an honorific seat for special guests and dignitaries or officials. Longer versions were then used for reclining as well, which eventually evolved into the bed and daybed. Taller versions evolved into higher tables as well. The folding stool also proliferated similarly, after it was adapted from designs developed by nomadic tribes to the North and West, who used them for both their convenience and light weight in many applications such as mounting horses. Later, woven hourglass-shaped stools evolved; a design still in use today throughout China.\n\nSome of the styles now widely regarded as Chinese began appearing more prominently in the Tang dynasty (618–907 AD). It is here that evidence of early versions of the round and yoke back chairs are found, generally used by the elite. By the next two Dynasties (the Northern and Southern Song) the use of varying types of furniture, including chairs, benches, and stools was common throughout Chinese society. Two particular developments were recessed legs and waisted tables. Newer and more complex designs were generally limited to official and higher class use.\n\nIt was from this basis that more modern Chinese furniture developed its distinguishing characteristics. Use of thick lacquer finish and detailed engraving and painted decoration as well as pragmatic design elements would continue to flourish. Significant foreign design influence would not be felt until increased contact with the West began in the 19th century, due to efforts on the part of the ruling elite to limit trade.\n\nDuring the Ming and Qing dynasties previous bans on imports were lifted, allowing for larger quantities and varieties of woods to flood in from other parts of Asia. The use of denser wood led to much finer work, including more elaborate joinery. A Ming Imperial table entirely covered in carved lacquer, now in London, is one of the finest survivals of the period.\n\nChinese furniture traditionally consisted of four distinct categories, all formed by the mid Qing dynasty, but each with its own unique characteristics.\n\nClassic Chinese furniture is typically made of a class of hardwoods, known collectively as \"rosewood\" (紅木, literally \"red wood\"). These woods are denser than water, fine grained, and high in oils and resins. These properties make them dimensionally stable, hardwearing, rot and insect resistant, and when new, highly fragrant. The density and toughness of the wood also allows furniture to be built without the use of glue and nail, but rather constructed from joinery and doweling alone. According to the Chinese industry standards the woods are grouped into eight classes:\n\nFurniture and carving made from these woods are typically referred to, in the market, as \"hongmu furniture\" (紅木家具, literally \"rosewood furniture\"). Due to overlogging for the said furniture, most of the species are either threatened or endangered.\n\nConstruction of traditional wooden Chinese furniture is based primarily of solid wood pieces connected solely using woodworking joints, and rarely using glue or metallic nails. The reason was that the nails and glues used did not stand up well to the vastly fluctuating temperatures and humid weather conditions in most of Central and South-East Asia. Further, the oily and resinous woods used in Chinese furniture generally do not glue well, even when pre-cleaned with modern industrial solvents.\n\nPlatform construction is based on box designs and uses frame-and-panel construction in simple form during earlier periods evolving into more and more modified forms in later periods. While earlier pieces show full frame-and-panel construction techniques, different parts of the construction were modified through the centuries to produce diverse looking pieces which still share the same basic construction. First the panel, originally complete, is subject to cut-out sections, followed by further reduction to what may appear to be simply decorative brackets. Further refinement of the same pattern lead the shape of the decorative brackets being incorporated into the shape of the surrounding frame and simultaneously the two mitered vertical pieces comprising a corner become one solid piece. Pieces start to have small cross-pieces attached to the bottom of the feet rather than a frame that is equal on all sides and finally, with evolution of the complex woodworking joints that allow it, the cross-pieces are removed entirely, leaving a modern table with 3-way mitered corners. Unlike European-derived styles, table designs based on this style will nearly always contain a frame-in-panel top, the panel serving as the tabletop center and the frame sometimes also serving as what would be rails on a European table. Cabinets in this style have a top that does not protrude beyond the sides or front. The critical element in almost all pieces of this type is the mitered joints, especially the 3-way mitered joining of the leg and two horizontal pieces at each corner.\n\nThe Yoke and Rack construction differs critically in the way that the legs of the piece are joined to the horizontal portion (be it tabletop, seat or cabinet carcass) using a type of wedged mortise-and-tenon joint where the end grain of the leg is visible as a circle in the frame of the tabletop. The cross-pieces (stretchers in the western equivalent) are joined through mortise-and-tenon joinery as well. The legs and stretchers are commonly round rather than square or curvilinear. The simplest pieces are simply four splayed legs attached to a solid top, but more complicated pieces contain decorative brackets, drawers and metal latches. Cabinets in this style typically have an overhanging top similar to western-style cabinetry.\n\nBamboo construction style, although historically rooted in pieces made from bamboo, later saw many pieces made from hardwood with patterning to imitate the look of bamboo, or simply in the style of previous pieces made from bamboo. The construction is more similar to the Yoke and Rack style with some apparent crossover.\n\n\n\n"}
{"id": "22887461", "url": "https://en.wikipedia.org/wiki?curid=22887461", "title": "CommunicAsia", "text": "CommunicAsia\n\nCommunicAsia is an information and communications technology (ICT) exhibition and conference held in Singapore. The annual event has taken place since 1979 and is usually held in June. The show customarily runs concurrently with the BroadcastAsia and EnterpriseIT exhibitions and conferences, all of which are operated by Singapore Exhibition Services.\n\nThe CommunicAsia Exhibition is amongst the largest platforms organised for the ICT industry in the Asia-Pacific region. It draws global industry brands to showcase key and emerging technologies. Past exhibitors include LG, Yahoo!, Huawei, Skype, Research in Motion (Blackberry) and Samsung. Attendance is restricted to trade professionals but admission is free.\n\nThe CommunicAsia Summit features roundtable discussions, forums and presentations on a broad range of industry specific subjects. The summit draws participation from top experts and managers from the telecommunications, service provider and satellite sectors as well as senior government functionaries and regulators.\n\nFrom 1979 – 1999 the show was a biennial event. It was held at the World Trade Centre, Singapore, from 1979 up until it moved to Suntec for the 1996 and 1998 shows. In 1999 CommunicAsia was held at Singapore Expo, where it continued to be hosted up until 2010.\n\nCommunicAsia 2009 ran from 16 – 19 June 2009 and the tradeshow featured 1,923 exhibiting companies from 60 countries and regions. The event attracted a total of 54,354 visitors from 100 countries spanning Asia-Pacific, Europe, US and the Middle-East.\n\nSignificant announcements at the event included the launch of two new smartphones by Huawei - the Android-powered U8230 and the C8000, running the Windows Mobile operating system. LG also unveiled its Crystal GD900, the world’s first transparent phone, while Samsung introduced its next era of smartphones in a worldwide launch. Garmin and ASUS also announced a first ever collaboration, the Garmin-ASUS nuvifones G60 and M20, smartphones with location-based capabilities.\n\nThe CommunicAsia 2009 Summit attracted 700 delegates and covered topics such as Mobile Services and Business Models, Network Enablers and Architectures, Satellite Communications Forum, Green Telecoms, Mobile Marketing and Advertising, Mobile TV and Entertainment, IPTV and Next Generation Broadband.\n\nHeld from 15 – 18 June at the Singapore Expo, CommunicAsia 2010 attracted over 55,000 industry visitors, conference speakers and delegates. 55% of visitors came from overseas, mostly from Indonesia, Malaysia and Thailand. \n\nCommunicAsia 2010 witnessed the launch of the Alcatel One Touch Net mobile by Yahoo! and Skype announcing the availability of its interface on three Sony Ericsson Symbian-based smartphones. Other announcements made at the event included the unveiling by Samsung of the Wave and Wave Pro and NTTDOCOMO’s F-04B, a separable phone that allows users to talk and view their phone at the same time. \n\nIt is estimated that deals worth over of $US3.6 billion were signed at the tradeshow.\n\nThe CommunicAsia Summit drew 650 participants and discussed topics including Mobile Value Added Services, Network Security, Getting in on the Cloud, Converged Device Management, Mobile Marketing & Profitability and New Revenue Opportunities. The keynote address was delivered by Josh Silverman, CEO of Skype.\n\nThe 2011 exhibition and conference was held from 21– 24 June at Marina Bay Sands and closed with strong results.\n\nCommunicAsia2012 was once again held at Marina Bay Sands from 19–22 June 2012. \n\nOver 200 debut exhibitors such as AMOS-Spacecom, Calix Networks, Integrasco, Mentum, NovelSat, SLA Mobile and Yokogawa Engineering Asia, shared their expertise and launch their latest products and solutions at the show.\n\nEnterpriseIT, held in conjunction with CommunicAsia, showcased exhibitors comprising international IT systems providers and companies offering enterprise solutions ranging from cloud computing, data centre services, security and M2M software to mobility solutions and video conferencing.\n\nThe show featured notable product launches from the likes of Huawei, Panasonic, Blackmagic Design, among others.\n\nThe event saw Asia's largest satellite representation in a technology trade event, and also offered conferences and meetings with over 350 experts. The show closed with healthy visitor figures on 22 June 2012.\n\nCommunicAsia2013 was held from 18–21 June 2013 at Marina Bay Sands for the third time.\n\nCommunicAsia 2014 will be held from 17-20 June 2014 at Marina Bay Sands.\n\nCommunicAsia 2017 was held from 23-25 May 2017 at Marina Bay Sands.\n\n"}
{"id": "1219747", "url": "https://en.wikipedia.org/wiki?curid=1219747", "title": "Concrete mixer", "text": "Concrete mixer\n\nA concrete mixer (often colloquially called a cement mixer) is a device that homogeneously combines cement, aggregate such as sand or gravel, and water to form concrete. A typical concrete mixer uses a revolving drum to mix the components. For smaller volume works, portable concrete mixers are often used so that the concrete can be made at the construction site, giving the workers ample time to use the concrete before it hardens. An alternative to a machine is mixing concrete by hand. This is usually done in a wheelbarrow; however, several companies have recently begun to sell modified tarps for this purpose.\n\nThe concrete mixer was invented by Columbus, Ohio industrialist Gebhardt Jaeger.\n\nToday's market increasingly requires consistent homogeneity and short mixing times for the industrial production of ready-mix concrete, and more so for precast/prestressed concrete. This has resulted in refinement of mixing technologies for concrete production. Different styles of stationary mixers have been developed, each with its own inherent strengths targeting different parts of the concrete production market. The most common mixers used today fall into three categories:\n\nAll the mixer styles have their own inherent strengths and weaknesses, and all are used throughout the world to varying degrees of popularity.\n\nSpecial concrete transport trucks (in-transit mixers) are made to mix concrete and transport it to the construction site. They can be charged with dry materials and water, with the mixing occurring during transport. They can also be loaded from a \"central mix\" plant; with this process the material has already been mixed prior to loading. The concrete mixing transport truck maintains the material's liquid state through agitation, or turning of the drum, until delivery. The interior of the drum on a concrete mixing truck is fitted with a spiral blade. In one rotational direction, the concrete is pushed deeper into the drum. This is the direction the drum is rotated while the concrete is being transported to the building site. This is known as \"charging\" the mixer. When the drum rotates in the other direction, the Archimedes' screw-type arrangement \"discharges\", or forces the concrete out of the drum. From there it may go onto chutes to guide the viscous concrete directly to the job site. If the truck cannot get close enough to the site to use the chutes, the concrete may be discharged into a concrete pump, connected to a flexible hose, or onto a conveyor belt which can be extended some distance (typically ten or more metres). A pump provides the means to move the material to precise locations, multi-floor buildings, and other distance-prohibitive locations. Buckets suspended from cranes are also used to place the concrete.\nThe drum is traditionally made of steel but on some newer trucks, fibreglass has been used as a weight reduction measure.\n\n\"Rear discharge\" trucks require both a driver and a \"chuteman\" to guide the truck and chute back and forth to place concrete in the manner suitable to the contractor. Newer \"front discharge\" trucks have controls inside the cab of the truck to allow the driver to move the chute in all directions. The first front discharge mixer was designed and built by Royal W. Sims of Holladay, Utah, United States.\n\nConcrete mixers are equipped with two or more axles. Four-, five- and six-axle trucks are the most common, with the number being determined by the load and local legislation governing allowable loads on the road.\n\nThe axles are necessary to distribute the load evenly, allow operation on weight restricted roads, and reduce wear and tear on normal roads. A two- or three-axle truck during the winter when road weight limits are reduced has no usable payload in many jurisdictions. Other areas may require expensive permits to operate.\n\nAdditional axles other than those used for steering (\"steers\") or drivetrain (\"drives\") may be installed between the steers and drives, or behind the drives. Mixers commonly have multiple steering axles as well, which generally result in very large turning radii. To facilitate maneuvering, the additional axles may be \"lift axles\", which allows them to be raised off the ground so that they do not scrub (get dragged sideways across the ground) on tight turns, or increase the vehicle's turning radius. Axles installed behind the drives are known as \"tag axles\" or \"booster axles\", and are often equipped to turn opposite to the steering axle to reduce scrubbing and automatically lift when the truck is put into a reverse gear.\n\nTractor trailer combination mixers where the mixer is installed on a trailer instead of a truck chassis are used in some jurisdictions, such as the province of Quebec where even six-axle trucks would have trouble carrying a useful load.\n\nConcrete mixers generally do not travel far from their plant, as the concrete begins to set as soon as it is in the truck. Many contractors require that the concrete be in place within 90 minutes after loading. If the truck breaks down or for some other reason the concrete hardens in the truck, workers may need to enter the barrel with jackhammers.\n\nStephen Stepanian filed a patent application for the first truck mixer in 1916. \n\nTrucks weigh , and can carry roughly of concrete although many varying sizes of Mixer Truck are currently in use. The most common truck capacity is .\n\nMost concrete mixers in the UK are limited to a speed of .\n\nA variant of standard concrete transportation is the concrete or cement mixing trailer. These small versions of transit-mix trucks are used to supply short loads of concrete. They have a concrete mixing drum with a capacity of between . Cart-aways are usually pulled behind a pick-up truck and batched from smaller batching systems. The mixing trailer system is popular with rental yards and building material locations, which use them to supply ready-mix to their regular customer base.\nMetered concrete trucks or volumetric mobile mixers contain concrete ingredient materials and water to be mixed on the truck at the job site to make and deliver concrete according to the amount needed.\nFor smaller jobs, such as residential repairs, renovations, or hobbyist-scale projects, many cubic yards of concrete are usually not required. Bagged cement is readily available in small-batch sizes, and aggregate and water are easily obtained in small quantities for the small work site. To service this small-batch concrete market, there many types of small portable concrete mixers are available.\n\nA typical portable concrete mixer uses a small revolving drum to mix the components. For smaller jobs the concrete made at the construction site has no time lost in transport, giving the workers ample time to use the concrete before it hardens.\n\nPortable concrete mixers may be powered by gasoline engines, although it is more common that they are powered by electric motors using standard mains current.\n\nThese concrete mixers are further divided based on their loading mechanism. Cement, sand and other aggregates are loaded in a hydraulically operated hopper and then poured into the mixing drum for final mixing. They can be unloaded by tilting the drum. In hand-feed concrete mixers, cement, sand and other aggregates are directly added to the mixing drum manually. These both type of concrete mixers are popular in construction activities in Africa, some Middle Eastern countries and in the Indian subcontinent.\n\nSelf-loading concrete mixers are unique machines designed to batch, mix and transport concrete. They consist of a rotating drum mounted on an operator-driven cab-mounted chassis frame fitted with a loading bucket.\n\nThe operator of the self-loading concrete mixers batches and introduces the ingredients required for mixing concrete (cement, stone aggregates etc.) into the drum using the loading bucket. The drum is usually reversible type, tilt type or a combination of both. A predetermined volume of water is discharged to the drum via a water dispensing unit. The mixture is rotated at mixing speeds within the drum until the concrete discharges via a fitted chute.\n\nSelf-loading concrete mixers are suited for construction sites where concrete batching plants are unavailable, underfoot conditions are not suited for concrete transit mixer trucks or labor availability is scarce or constrained. Applications include urban and rural construction, concrete pavement maintenance, bridge and tunnel construction, township-level highways construction, foundation construction, national defense facilities, construction of high-speed railways, etc.\n\nOperating concrete mixers correctly is one of the biggest safety issues in construction zones. Workers whose tasks are related to concrete processing currently number more than 250,000. Over 10 percent of those workers, 28,000, experienced a job-related injury or illness, and 42 died in just one year.\n\n\n\n"}
{"id": "37411947", "url": "https://en.wikipedia.org/wiki?curid=37411947", "title": "Conveyor pulley", "text": "Conveyor pulley\n\nA conveyor pulley is a mechanical device used to change the direction of the belt in a conveyor system, to drive the belt, and to tension the belt. Modern pulleys are made of rolled shells with flexible end disks and locking assemblies. Early pulley engineering was developed by Josef Sitzwohl in Australia in 1948 and later by Helmuth Lange and Walter Schmoltzi in Germany.\n\nPulleys are made up of several components including the shell, end disk, hub, shaft and locking assembly. The end disk and hub may be on piece. The locking assembly may also be replaced with a hub and bushing on lower tension pulleys. The shell is also referred to as the rim in some parts of the world.\n\nThe pulley shaft is typically sized following CEMA B105.1 in the Americas or AS 1403 in Australia.\n\n\nPulley manufactures in Germany (DDR) in 1962 included Zemag, Lauchhammer, and Köthen.\n"}
{"id": "11312786", "url": "https://en.wikipedia.org/wiki?curid=11312786", "title": "Dichlorofluoromethane", "text": "Dichlorofluoromethane\n\nDichlorofluoromethane or Freon 21 or R 21 is a halomethane or hydrochlorofluorocarbon. It is a colorless and odorless gas.\n\nIts critical point is at 178.5 °C (451.7 K) and 5.17 MPa (51.7 bar). At temperatures from 5 K to 105 K it has one phase in the space group Pbca.\n\nDichlorofluoromethane was used as a propellant and refrigerant, but due to its ozone depletion it has been set to be phased out. It has ozone depletion potential 0.04. Production and consumption has been since 2004 reduced to 15% of level from 1989 and it is to be phased out in 2015 according to Montreal Protocol.\n\n"}
{"id": "5772470", "url": "https://en.wikipedia.org/wiki?curid=5772470", "title": "Digital Serial Interface", "text": "Digital Serial Interface\n\nDigital Serial Interface (DSI) is a protocol for the controlling of lighting in buildings (initially electrical ballasts). It was created in 1991 by Austrian company Tridonic and is based on Manchester-coded 8-bit protocol, data rate of 1200 baud, 1 start bit, 8 data bits (dimming value), 4 stop bits, and is the basis of the more sophisticated protocol Digital Addressable Lighting Interface (DALI).\n\nThe technology uses a single byte to communicate the lighting level (0-255 or 0x00-0xFF). DSI was the first use of digital communication in lighting control, and was the precursor to DALI.\n\n\n\nACN is an ANSI standard protocol adopted by the entertainment industry. It is based on Ethernet and is typically used as the backbone communication between controllers and control segments. To date few fixtures offer native ACN interface. Typically ACN is converted to DMX interface with strings of fixtures.\n\nDALI is an open standard for digital control of lighting. Several companies have adopted the DALI protocol in their product offerings. Even though DALI is an open standard, there are already versions of its implementation emerging in different lighting manufacturers products as they strive to provide a point of difference. DSI is essentially the same technology as DALI in terms of messaging, however, DSI eliminates the individual addressing aspect of each light fitting found in DALI.\n\nDMX is an ANSI standard protocol adopted by the entertainment industry worldwide which is also being offered on many architectural fixtures. DMX is built on ANSI standard RS-485 serial interface.\n\n"}
{"id": "29491804", "url": "https://en.wikipedia.org/wiki?curid=29491804", "title": "Dimensionless momentum-depth relationship in open-channel flow", "text": "Dimensionless momentum-depth relationship in open-channel flow\n\nMomentum for one-dimensional flow in a channel can be given by the expression:\n\nFor open channel flow calculations where momentum can be assumed to be conserved, such as in a hydraulic jump, we can equate the Momentum at an upstream location,\"M\", to that at a downstream location, \"M\", such that: \nIn the unique circumstance where the flow is in a rectangular channel (such as a laboratory flume), we can describe this relationship as unit momentum, by dividing both sides of the equation by the width of the channel. This produces M in terms of ft, and is given by the equation: \n\nMomentum is one of the most important basic definitions in Fluid Mechanics. The conservation of momentum is one of the three fundamental physical principles in both Fluid Mechanics and [Open-channel flow | open channel flow] (the other two are mass conservation and energy conservation). This principle leads to the momentum equation set in three dimensions (x, y and z). With different assumptions, these momentum equations can be simplified to several widely applied forms:\n\nWith Newton’s second law, Newtonian fluids assumption and Stokes hypothesis, the original fluid momentum equations are derived as the Navier–Stokes equations. These equations are classic in Fluid Mechanics, but the nonlinearity in these partial differential equations make them difficult to solve mathematically. As a result, analytical solutions for the Navier–Stokes equations still remain a tough research topic.\n\nFor high Reynolds number flow, the effects of viscosity are negligible. In these cases, with the inviscid assumption, Navier–Stokes equations can be derived as Euler equations. Though they are still nonlinear partial differential equations, the elimination of viscous terms simplifies the problem.\n\nIn some applications, when viscosity, rotationality and compressibility of the fluid can be neglected, the Navier–Stokes equations can be further simplified to the Laplace equation form, which is referred as potential flow.\n\nIn computational fluid dynamics, solving the partial differential momentum equations mentioned above with discretized algebraic equations is the most important procedure to study flow characteristics in different applications.\n\nMomentum also allows us to describe the characteristics of flow when energy is not conserved. HEC-RAS, a widely used computer model developed by the US Army Corps of Engineers for calculating water surface profiles, considers that when flow passes through critical depth, the basic assumption of gradually varied flow required for the Energy Equation is not applicable. Locations where flow may make such a transition include: significant changes in slope, channel geometry (e.g. bridge sections), grade control structures, and the confluence of water bodies. In these instances, HEC-RAS will use a form of the momentum equation to solve for the water surface elevation at an unknown location.\n\nIn addition, momentum flux is one of the parameters to estimate fluid impact on offshore structures. Analysis of momentum flux in coastal regions can provide advisable infrastructure layout planning to minimize the potential hazards from extreme events such as storm surge, hurricane and tsunami (e.g. (Park et al. 2013), (Yeh 2006), (Guard et al. 2005) and (Chanson et al. 2002)).\n\nFor discussion, we will consider an ideal, frictionless, rectangular channel. For each value of q, a unique curve can be produced where \"M\" is shown as a function of depth. As is the case for specific energy, the minimum value of M, M, corresponds to critical depth. For each value of M greater than M, there are two depths that can occur. These are called conjugate depths, and represent supercritical and subcritical alternatives for flow of a given M. Since hydraulic jumps conserve momentum, if the depth at the upstream or downstream end of a hydraulic jump is known, we can determine the unknown depth by drawing a vertical line through the known depth and reading its conjugate. The M-y diagram below shows three M-y curves with unit discharge 10, 15\nand 20 ft/s. It can be observed that the M-y curves shift in positive M axis as the q value increases. From the M-y equation mentioned earlier, as y increases to infinity, the q / gy term would be negligible, and the M value will converge to 0.5y (shown as the black dashed curve in M-y diagram). By taking the derivative dM / dy = 0, we can also obtain the equation of minimum M with different q values:\nBy eliminating the term of q in the equation above with the relationship between q and y (y = ( q / g ) ), and put the resulting equation of y into the original M-y ccg3 c\nequation, we can obtain the characteristic curve of critical M and y (shown as the red dashed curve in M-y diagram):\n\nConjugate depths can be determined from curves like the one above. However, since this curve is unique for q = 20 ft/s, we would have to develop a new curve for each rectangular channel of a given base width (or discharge). If we can establish a dimensionless relationship, we can apply the curve to any problem in which the cross-section is rectangular in shape. To create a dimensionless momentum–depth relationship, we will divide both sides by a normalizing value that will allow us to use a dimensionless relationship between Momentum and Depth for all values of \"q\".\n\nGiven that: \n\nand that: \n\naccording to the Buckingham theorem, with dimensional analysis, we can normalize the relationship between depth and Momentum by dividing both by the value of the critical depth squared and substituting for q to yield:\nIf we let M’ = M/y, and y’ = y/y, this equation becomes: \nBy applying the conversion to dimensionless units described above, the dimensionless momentum–depth diagram is produced below.\nBy close inspection of the dimensionless dnergy–depth diagram, an interesting conclusion can be drawn, which is that M’ is the same function of y’ as E’ is of 1/y’, and vice versa. This is demonstrated in the following chart that compares favorably to the chart of the Dimensionless Energy-Depth Diagram. Note that the only difference between the chart above and the one below is the values of the y-axis are the reciprocal of one another and that the scale has been changed to be consistent with the scale found in the discussion of Dimensionless Energy-Depth. \n\nBecause Energy and Momentum have this reciprocal relationship (found also in the non-dimensionless forms of these relationships), we can use a Dimensionless Energy-Depth Diagram to create a dimensionless momentum–depth diagram, and vice versa.\n\nTo demonstrate the use of a dimensionless momentum–depth diagram in the solution of a simple hydraulic jump problem (hydraulic jump is also very common in other situations. Let’s consider a rectangular channel with a base width of 10 ft, and a flow rate of 100 ft/s, with a tailwater induced downstream depth of 6 ft. What is the depth of flow at the upstream end of the hydraulic jump?\n\nStep 1 – Calculate q: \n\nStep 2 – Calculate y:\nApplying conservation of momentum between position 2 & 3:\n\nIn addition, we can obtain the thrust on the sluice gate as well:\n\n"}
{"id": "20221001", "url": "https://en.wikipedia.org/wiki?curid=20221001", "title": "Draw twister", "text": "Draw twister\n\nA draw twister is a machine used to draw and twist large quantities of polymer fibers. It uses two sets of rollers, where the second set rotates faster than the first, thus drawing the fiber between them. While the fibers are being drawn they are also twisted into thread.\n"}
{"id": "8102", "url": "https://en.wikipedia.org/wiki?curid=8102", "title": "Dysprosium", "text": "Dysprosium\n\nDysprosium is a chemical element with symbol Dy and atomic number 66. It is a rare earth element with a metallic silver luster. Dysprosium is never found in nature as a free element, though it is found in various minerals, such as xenotime. Naturally occurring dysprosium is composed of seven isotopes, the most abundant of which is Dy.\n\nDysprosium was first identified in 1886 by Paul Émile Lecoq de Boisbaudran, but it was not isolated in pure form until the development of ion exchange techniques in the 1950s. Dysprosium has relatively few applications where it cannot be replaced by other chemical elements. It is used for its high thermal neutron absorption cross-section in making control rods in nuclear reactors, for its high magnetic susceptibility in data storage applications, and as a component of Terfenol-D (a magnetostrictive material). Soluble dysprosium salts are mildly toxic, while the insoluble salts are considered non-toxic.\n\nDysprosium is a rare earth element that has a metallic, bright silver luster. It is quite soft, and can be machined without sparking if overheating is avoided. Dysprosium's physical characteristics can be greatly affected by even small amounts of impurities.\n\nDysprosium and holmium have the highest magnetic strengths of the elements, especially at low temperatures. Dysprosium has a simple ferromagnetic ordering at temperatures below . Above , it turns into a helical antiferromagnetic state in which all of the atomic moments in a particular basal plane layer are parallel, and oriented at a fixed angle to the moments of adjacent layers. This unusual antiferromagnetism transforms into a disordered (paramagnetic) state at .\n\nDysprosium metal tarnishes slowly in air and burns readily to form dysprosium(III) oxide:\n\nDysprosium is quite electropositive and reacts slowly with cold water (and quite quickly with hot water) to form dysprosium hydroxide:\n\nDysprosium metal vigorously reacts with all the halogens at above 200 °C:\n\nDysprosium dissolves readily in dilute sulfuric acid to form solutions containing the yellow Dy(III) ions, which exist as a [Dy(OH)] complex:\n\nThe resulting compound, dysprosium(III) sulfate, is noticeably paramagnetic.\n\nDysprosium halides, such as DyF and DyBr, tend to take on a yellow color. Dysprosium oxide, also known as dysprosia, is a white powder that is highly magnetic, more so than iron oxide.\n\nDysprosium combines with various non-metals at high temperatures to form binary compounds with varying composition and oxidation states +3 and sometimes +2, such as DyN, DyP, DyH and DyH; DyS, DyS, DyS and DyS; DyB, DyB, DyB and DyB, as well as DyC and DyC.\n\nDysprosium carbonate, Dy(CO), and dysprosium sulfate, Dy(SO), result from similar reactions. Most dysprosium compounds are soluble in water, though dysprosium carbonate tetrahydrate (Dy(CO)·4HO) and dysprosium oxalate decahydrate (Dy(CO)·10HO) are both insoluble in water. Two of the most abundant dysprosium carbonates, tengerite-(Dy) (Dy(CO)·2–3HO) and kozoite-(Dy) (DyCO(OH)) are known to form via a poorly ordered (amorphous) precursor phase with a formula of Dy(CO)·4HO. This amorphous precursor consists of highly hydrated spherical nanoparticles of 10–20 nm diameter that are exceptionally stable under dry treatment at ambient and high temperatures.\n\nNaturally occurring dysprosium is composed of seven isotopes: Dy, Dy, Dy, Dy, Dy, Dy, and Dy. These are all considered stable, although Dy decays by alpha decay with a half-life of over 1×10 years. Of the naturally occurring isotopes, Dy is the most abundant at 28%, followed by Dy at 26%. The least abundant is Dy at 0.06%.\n\nTwenty-nine radioisotopes have also been synthesized, ranging in atomic mass from 138 to 173. The most stable of these is Dy, with a half-life of approximately 3 years, followed by Dy with a half-life of 144.4 days. The least stable is Dy, with a half-life of 200 ms. As a general rule, isotopes that are lighter than the stable isotopes tend to decay primarily by β decay, while those that are heavier tend to decay by β decay. However, Dy decays primarily by alpha decay, and Dy and Dy decay primarily by electron capture. Dysprosium also has at least 11 metastable isomers, ranging in atomic mass from 140 to 165. The most stable of these is Dy, which has a half-life of 1.257 minutes. Dy has two metastable isomers, the second of which, Dy, has a half-life of 28 ns.\n\nIn 1878, erbium ores were found to contain the oxides of holmium and thulium. French chemist Paul Émile Lecoq de Boisbaudran, while working with holmium oxide, separated dysprosium oxide from it in Paris in 1886. His procedure for isolating the dysprosium involved dissolving dysprosium oxide in acid, then adding ammonia to precipitate the hydroxide. He was only able to isolate dysprosium from its oxide after more than 30 attempts at his procedure. On succeeding, he named the element \"dysprosium\" from the Greek \"dysprositos\" (δυσπρόσιτος), meaning \"hard to get\". The element was not isolated in relatively pure form until after the development of ion exchange techniques by Frank Spedding at Iowa State University in the early 1950s.\n\nWhile dysprosium is never encountered as a free element, it is found in many minerals, including xenotime, fergusonite, gadolinite, euxenite, polycrase, blomstrandine, monazite and bastnäsite, often with erbium and holmium or other rare earth elements. No dysprosium-dominant mineral (that is, with dysprosium prevailing over other rare earths in the composition) has yet been found.\n\nIn the high-yttrium version of these, dysprosium happens to be the most abundant of the heavy lanthanides, comprising up to 7–8% of the concentrate (as compared to about 65% for yttrium). The concentration of Dy in the Earth's crust is about 5.2 mg/kg and in sea water 0.9 ng/L.\n\nDysprosium is obtained primarily from monazite sand, a mixture of various phosphates. The metal is obtained as a by-product in the commercial extraction of yttrium. In isolating dysprosium, most of the unwanted metals can be removed magnetically or by a flotation process. Dysprosium can then be separated from other rare earth metals by an ion exchange displacement process. The resulting dysprosium ions can then react with either fluorine or chlorine to form dysprosium fluoride, DyF, or dysprosium chloride, DyCl. These compounds can be reduced using either calcium or lithium metals in the following reactions:\n\nThe components are placed in a tantalum crucible and fired in a helium atmosphere. As the reaction progresses, the resulting halide compounds and molten dysprosium separate due to differences in density. When the mixture cools, the dysprosium can be cut away from the impurities.\n\nAbout 100 tonnes of dysprosium are produced worldwide each year, with 99% of that total produced in China. Dysprosium prices have climbed nearly twentyfold, from $7 per pound in 2003, to $130 a pound in late 2010. The price increased to $1,400/kg in 2011 but fell to $240 in 2015, largely due to illegal production in China which circumvented government restrictions.\n\nCurrently, most dysprosium is being obtained from the ion-adsorption clay ores of southern China. the Browns Range Project pilot plant, 160 km south east of Halls Creek, Western Australia is producing per annum.\n\nAccording to the United States Department of Energy, the wide range of its current and projected uses, together with the lack of any immediately suitable replacement, makes dysprosium the single most critical element for emerging clean energy technologies - even their most conservative projections predict a shortfall of dysprosium before 2015. As of late 2015, there is a nascent rare earth (including dysprosium) extraction industry in Australia.\n\nDysprosium is used, in conjunction with vanadium and other elements, in making laser materials and commercial lighting. Because of dysprosium's high thermal-neutron absorption cross-section, dysprosium-oxide–nickel cermets are used in neutron-absorbing control rods in nuclear reactors. Dysprosium–cadmium chalcogenides are sources of infrared radiation, which is useful for studying chemical reactions. Because dysprosium and its compounds are highly susceptible to magnetization, they are employed in various data-storage applications, such as in hard disks. Dysprosium is increasingly in demand for the permanent magnets used in electric car motors and wind turbine generators.\n\nNeodymium–iron–boron magnets can have up to 6% of the neodymium substituted by dysprosium to raise the coercivity for demanding applications, such as drive motors for electric vehicles and generators for wind turbines. This substitution would require up to 100 grams of dysprosium per electric car produced. Based on Toyota's projected 2 million units per year, the use of dysprosium in applications such as this would quickly exhaust its available supply. The dysprosium substitution may also be useful in other applications because it improves the corrosion resistance of the magnets.\n\nDysprosium is one of the components of Terfenol-D, along with iron and terbium. Terfenol-D has the highest room-temperature magnetostriction of any known material; which is employed in transducers, wide-band mechanical resonators, and high-precision liquid-fuel injectors.\n\nDysprosium is used in dosimeters for measuring ionizing radiation. Crystals of calcium sulfate or calcium fluoride are doped with dysprosium. When these crystals are exposed to radiation, the dysprosium atoms become excited and luminescent. The luminescence can be measured to determine the degree of exposure to which the dosimeter has been subjected.\n\nNanofibers of dysprosium compounds have high strength and a large surface area. Therefore, they can be used to reinforce other materials and act as a catalyst. Fibers of dysprosium oxide fluoride can be produced by heating an aqueous solution of DyBr and NaF to 450 °C at 450 bars for 17 hours. This material is remarkably robust, surviving over 100 hours in various aqueous solutions at temperatures exceeding 400 °C without redissolving or aggregating.\n\nDysprosium iodide and dysprosium bromide are used in high-intensity metal-halide lamps. These compounds dissociate near the hot center of the lamp, releasing isolated dysprosium atoms. The latter re-emit light in the green and red part of the spectrum, thereby effectively producing bright light.\n\nSeveral paramagnetic crystal salts of dysprosium (Dysprosium Gallium Garnet, DGG; Dysprosium Aluminum Garnet, DAG; Dysprosium Iron Garnet, DyIG) are used in adiabatic demagnetization refrigerators.\n\nThe trivalent dysprosium ion (Dy) has been studied due its downshifting luminescence properties. Dy-doped yttrium aluminium garnet (YAG:Dy) excited in the ultraviolet region of the electromagnetic spectrum results in the emission of photons of longer wavelength in the visible region. This idea is the basis for a new generation of UV-pumped white light emitting diodes.\n\nLike many powders, dysprosium powder may present an explosion hazard when mixed with air and when an ignition source is present. Thin foils of the substance can also be ignited by sparks or by static electricity. Dysprosium fires cannot be put out by water. It can react with water to produce flammable hydrogen gas. Dysprosium chloride fires, however, can be extinguished with water, while dysprosium fluoride and dysprosium oxide are non-flammable. Dysprosium nitrate, Dy(NO), is a strong oxidizing agent and will readily ignite on contact with organic substances.\n\nSoluble dysprosium salts, such as dysprosium chloride and dysprosium nitrate, are mildly toxic when ingested. Based on the toxicity of dysprosium chloride to mice, it is estimated that the ingestion of 500 grams or more could be fatal to a human. The insoluble salts, however, are non-toxic.\n\n"}
{"id": "24032731", "url": "https://en.wikipedia.org/wiki?curid=24032731", "title": "Electrochemical reduction of carbon dioxide", "text": "Electrochemical reduction of carbon dioxide\n\nThe electrochemical reduction of carbon dioxide (ERC) is the conversion of carbon dioxide to more reduced chemical species using electrical energy. The first examples of electrochemical reduction of carbon dioxide are from the 19th century, when carbon dioxide was reduced to carbon monoxide using a zinc cathode. Research in this field intensified in the 1980s following the oil embargoes of the 1970s. Electrochemical reduction of carbon dioxide represents a possible means of producing chemicals or fuels, converting carbon dioxide () to organic feedstocks such as formic acid (HCOOH), methanol (CHOH), ethylene (CH), methane (CH), and carbon monoxide (CO).\n\nIn carbon fixation, plants convert carbon dioxide into sugars, from which many biosynthetic pathways originate. The catalyst responsible for this conversion, RuBisCo, is the most common protein on earth. Some anaerobic organisms employ enzymes to convert CO to carbon monoxide, from which fatty acids can be made.\n\nIn industry, a few products are made from CO, including urea, salicylic acid, methanol, and certain inorganic and organic carbonates. In the laboratory, carbon dioxide is sometimes used to prepare carboxylic acids. No electrochemical process involving CO has been commercialized.\n\nThe electrochemical reduction of carbon dioxide to CO is usually described as:\nThe redox potential for this reaction is similar to that for hydrogen evolution in aqueous electrolytes, thus electrochemical reduction of CO2 is usually competitive with hydrogen evolution reaction.\n\nElectrochemical methods have gained significant attention: 1) at ambient pressure and room temperature; 2) in connection with renewable energy sources (see also solar fuel) 3) competitive controllability, modularity and scale-up is relatively simple. The electrochemical reduction or electrocatalytic conversion of CO can produce value-added chemicals such methane, ethylene, ethane, etc., and the products are mainly dependent on the selected catalysts and operating potentials (applying reduction voltage).\n\nAlthough an electrochemical route to CO (or other chemicals) has not been commercialized, a variety of homogeneous and heterogeneous catalysts have been evaluated. Many such processes are assumed to operate via the intermediacy of metal carbon dioxide complexes. Generally speaking, the processes developed up to 2010 either had poor thermodynamic efficiency (high overpotential), low current efficiency, low selectivity, slow kinetics, and/or poor stability. In 2011, workers from Dioxide Materials and University of Illinois showed that the combination of two catalysts could eliminate the high overpotential More recently, the same group showed that the process was stable for 6 months at over 90% selectivity.\n\n\n"}
{"id": "12769634", "url": "https://en.wikipedia.org/wiki?curid=12769634", "title": "Ephemeralization", "text": "Ephemeralization\n\nEphemeralization, a term coined by R. Buckminster Fuller, is the ability of technological advancement to do \"more and more with less and less until eventually you can do everything with nothing,\" that is, an accelerating increase in the efficiency of achieving the same or more output (products, services, information, etc.) while requiring less input (effort, time, resources, etc.). Fuller's vision was that ephemeralization will result in ever-increasing standards of living for an ever-growing population despite finite resources. The concept has been embraced by those who argue against Malthusian philosophy.\n\nFuller uses Henry Ford's assembly line as an example of how ephemeralization can continuously lead to better products at lower cost with no upper bound on productivity. Fuller saw ephemeralization as an inevitable trend in human development. The progression was from \"compression\" to \"tension\" to \"visual\" to \"abstract electrical\" (i.e., nonsensorial radiation, such as radio waves, x rays, etc.).\n\nLength measurement technologies in human development, for example, started with a compressive measure, such as a ruler. The compressive technique reached an upper limit with a rod. For longer measures, a tensive measure such as a string or rope was used. This reached an upper limit with sagging of the string. Next was a surveyor’s telescope (visual). This reached an upper limit with curvature of the earth. Next was radio triangulation (abstract electrical). The technological progression is a continuing increase in length-measuring ability per pound of instrument, with no apparent upper limit according to Fuller.\n\nFrancis Heylighen and Alvin Toffler have written that ephemeralization, though it may increase our power to solve physical problems, can make non-physical problems worse. According to Heylighen and Toffler, increasing system complexity and information overload make it difficult and stressful for the people who must control the ephemeralized systems. This can negate the advantages of ephemeralization.\n\nThe solution proposed by Heylighen is the integration of human intelligence, computer intelligence, and coordination mechanisms that direct an issue to the cognitive resource (document, person, or computer program) most fit to address it. This requires a distributed, self-organizing system, formed by all individuals, computers and the communication links that connect them. The self-organization can be achieved by algorithms. According to Heylighen, the effect is to superpose the contributions of many different human and computer agents into a collective map that may link the cognitive and physical resources relatively efficiently. The resulting information system could react relatively rapidly and adaptively to requests for guidance or changes in the situation.\n\nIn Heylighen's view, the system could frequently be fed with new information from its myriad human users and computer agents, which it would take into account to offer the human users a list of the best possible approaches to achieve tasks. Heylighen believes near-optimization could be achieved both at the level of the individual who makes the request, and at the level of society which attempts to minimize the conflicts between the desires of its different members and to aim at long term, global progress while as much as possible protecting individual liberty and privacy.\n\n\n"}
{"id": "222349", "url": "https://en.wikipedia.org/wiki?curid=222349", "title": "Flynn's taxonomy", "text": "Flynn's taxonomy\n\nFlynn's taxonomy is a classification of computer architectures, proposed by Michael J. Flynn in 1966. The classification system has stuck, and has been used as a tool in design of modern processors and their functionalities. Since the rise of multiprocessing central processing units (CPUs), a multiprogramming context has evolved as an extension of the classification system.\n\nThe four classifications defined by Flynn are based upon the number of concurrent instruction (or control) streams and data streams available in the architecture.\n\nA sequential computer which exploits no parallelism in either the instruction or data streams. Single control unit (CU) fetches single instruction stream (IS) from memory. The CU then generates appropriate control signals to direct single processing element (PE) to operate on single data stream (DS) i.e., one operation at a time.\n\nExamples of SISD architecture are the traditional uniprocessor machines like older personal computers (PCs; by 2010, many PCs had multiple cores) and mainframe computers.\n\nA single instruction operates on multiple different data streams. Instructions can be executed sequentially, such as by pipelining, or in parallel by multiple functional units.\n\nSingle instruction, multiple threads (SIMT) is an execution model used in parallel computing where single instruction, multiple data (SIMD) is combined with multithreading. This is not a distinct classification in Flynn's taxonomy, where it would be a subset of SIMD. However, Nvidia commonly uses the term in its marketing materials where it argues for the supposed novelty of Nvidia architecture.\n\nMultiple instructions operate on one data stream. This is an uncommon architecture which is generally used for fault tolerance. Heterogeneous systems operate on the same data stream and must agree on the result. Examples include the Space Shuttle flight control computer.\n\nMultiple autonomous processors simultaneously executing different instructions on different data. MIMD architectures include multi-core superscalar processors, and distributed systems, using either one shared memory space or a distributed memory space.\n\nThese four architectures are shown below visually. Each processing unit (PU) is shown for a uni-core or multi-core computer:\n, all the top 10 and most of the TOP500 supercomputers are based on a MIMD architecture.\n\nSome further divide the MIMD category into the two categories below, and even further subdivisions are sometimes considered.\n\nMultiple autonomous processors simultaneously executing the same program (but at independent points, rather than in the lockstep that SIMD imposes) on different data. Also termed \"single process, multiple data\" - the use of this terminology for SPMD is technically incorrect, as SPMD is a parallel execution model and assumes multiple cooperating processors executing a program. SPMD is the most common style of parallel programming. The SPMD model and the term was proposed by Frederica Darema. Gregory F. Pfister was a manager of the RP3 project, and Darema was part of the RP3 team.\n\nMultiple autonomous processors simultaneously operating at least 2 independent programs. Typically such systems pick one node to be the \"host\" (\"the explicit host/node programming model\") or \"manager\" (the \"Manager/Worker\" strategy), which runs one program that farms out data to all the other nodes which all run a second program. Those other nodes then return their results directly to the manager. An example of this would be the Sony PlayStation 3 game console, with its SPU/PPU processor.\n\n"}
{"id": "13793577", "url": "https://en.wikipedia.org/wiki?curid=13793577", "title": "Future Vision Technologies", "text": "Future Vision Technologies\n\nFuture Vision Technologies (FVT), operating from 1991 to 1995, was part of the second wave of companies working to commercialize virtual reality technology. The company was founded by a team out of the Advanced Digital Systems Laboratory in the department of Electrical and Computer Engineering at the University of Illinois at Urbana-Champaign. The three original members, Matt Klapman, David Frerichs, and Kevin Lee, were later joined by John Belmonte. The company ceased to be an active entity when its PC card business was sold to Fujitsu Microelectronics.\n\nThe company produced a number of products which appear to be first of their kind in the market.\n\n"}
{"id": "8973392", "url": "https://en.wikipedia.org/wiki?curid=8973392", "title": "GreenNet", "text": "GreenNet\n\nGreenNet is a not-for-profit Internet service provider based in London, England. It was established in 1985 \"as an effective and cheap way for environmental activists to communicate\". In 1987 the Joseph Rowntree Charitable Trust gave GreenNet a grant to enable it to bring a large number of peace groups online, and \"After a few years they became one of the first internet service providers in Britain\". GreenNet formed an international link with IGC and was a founder member of the Association for Progressive Communications, established in 1990. The registered charity GreenNet Charitable Trust was established in 1994 and owns GreenNet.\n\nGreenNet developed a Fido gateway, GnFido, which allowed access to basic internet facilities such as email using a store and forward system. It provided the only available cheap and accessible internet access for thousands of individuals and organisations in Africa, South Asia and Eastern Europe.\n\nBeginning at 10.15 BST on Thursday 1 August 2013 GreenNet and the Association for Progressive Communications (APC) suffered an extensive DDoS attack. The attack was later described as a \"DNS reflection attack\" also known as a spoofed attack Several sources initially suspected the attack was linked to the Zimbabwean Elections, held a day earlier. GreenNet's services were not fully operational again until 10.30 BST on Thursday 7 August. On 9 August there was a second attack, which, while affecting some systems, allowed GreenNet to discover the site which was being targeted. In October 2013, the target was revealed to be the site of investigative reporter Andrew Jennings.\n\nIn July 2014 Privacy International, GreenNet and five other Internet Service Providers took GCHQ, the UK security service to the Investigatory Powers Tribunal alleging breach of privacy and breaking into their networks. The case ultimately failed, but GCHQ were forced to admit clandestine hacking activities. GreenNet were shortlisted for ISPA's Internet Hero of the year 2015.\n\n"}
{"id": "10783", "url": "https://en.wikipedia.org/wiki?curid=10783", "title": "History of film", "text": "History of film\n\nAlthough the start of the history of film is not clearly defined, the commercial, public screening of ten of Lumière brothers' short films in Paris on 28 December 1895 can be regarded as the breakthrough of projected cinematographic motion pictures. There had been earlier cinematographic results and screenings but these lacked either the quality or the momentum that propelled the cinématographe Lumière into a worldwide success.\n\nSoon film production companies were established all over the world. The first decade of motion picture saw film moving from a novelty to an established mass entertainment industry.\n\nThe earliest films were in black and white, under a minute long and without recorded sound.\n\nDuring the 1890s films became several minutes long and started to consist of several shots. The first film studios were built in 1897. The first rotating camera for taking panning shots was built in 1898. Special effects were introduced and film continuity, involving action moving from one sequence into another, began to be used.\n\nIn the 1900s, continuity of action across successive shots was achieved and the first close-up shot was introduced (some claim D. W. Griffith was the inventor). Most films of this period were what came to be called \"chase films\". The first successful permanent theatre showing only films was \"The Nickelodeon\" in Pittsburgh in 1905. The first feature length multi-reel film was a 1906 Australian production. By 1910, actors began to receive screen credit for their roles, opening the way for the creation of film stars. Regular newsreels were exhibited from 1910 and soon became a popular way for finding out the news. From about 1910, American films had the largest share of the market in Australia and in all European countries except France.\n\nNew film techniques were introduced in this period including the use of artificial lighting, fire effects and low-key lighting (i.e. lighting in which most of the frame is dark) for enhanced atmosphere during sinister scenes. As films grew longer, specialist writers were employed to simplify more complex stories derived from novels or plays into a form that could be contained on one reel and be easier to be understood by the audience an audience that was new to this form of storytelling. Genres began to be used as categories; the main division was into comedy and drama but these categories were further subdivided. During the First World War there was a complex transition for the film industry. The exhibition of films changed from short one-reel programs to feature films. Exhibition venues became larger and began charging higher prices. By 1914, continuity cinema was the established mode of commercial cinema. One of the advanced continuity techniques involved an accurate and smooth transition from one shot to another.\n\nD. W. Griffith had the highest standing among American directors in the industry, because of the dramatic excitement he conveyed to the audience through his films. The American film industry, or \"Hollywood\", as it was becoming known after its new geographical center in Hollywood, a neighborhood in Los Angeles, California, gained the position it has held, more or less, ever since: film factory for the world and exporting its product to most countries. By the 1920s, the United States reached what is still its era of greatest-ever output, producing an average of 800 \"feature\" films annually, or 82% of the global total (Eyman, 1997). During late 1927, Warner's released \"The Jazz Singer\", with the first synchronized dialogue (and singing) in a feature film. By the end of 1929, Hollywood was almost all-talkie, with several competing sound systems (soon to be standardized). Sound saved the Hollywood studio system in the face of the Great Depression (Parkinson, 1995).\n\nThe desire for wartime propaganda created a renaissance in the film industry in Britain, with realistic war dramas. The onset of American involvement in World War II also brought a proliferation of films as both patriotism and propaganda. The House Un-American Activities Committee investigated Hollywood in the early 1950s. During the immediate post-war years the cinematic industry was also threatened by television and the increasing popularity of the medium meant that some film theatres would bankrupt and close. The 1950s was considered a \"Golden Age\" for non-English cinema.\n\n\"Roundhay Garden Scene\" is an 1888 short silent film recorded by French inventor Louis Le Prince. It is believed to be the oldest surviving film in existence, as noted by the \"Guinness Book of Records\".\nThe film \"Sortie de l'usine Lumière de Lyon\" (1895) by French Louis Lumière is considered the \"first true motion picture\".\n\nFilm as an art form has drawn on several earlier traditions in the fields such as (oral) storytelling, literature, theatre and visual arts. Forms of art and entertainment that had already featured moving and/or projected images include:\n\nSome ancient sightings of gods and spirits may have been conjured up by means of (concave) mirrors, camera obscura or unknown projectors. By the 16th century necromantic ceremonies and the conjuring of ghostly apparitions by charlatan \"magicians\" and \"witches\" seemed commonplace. The very first magic lantern shows seem to have continued this tradition with images of death, monsters and other scary figures. Around 1790 this was developed into multi-media ghost shows known as phantasmagoria that could feature mechanical slides, rear projection, mobile projectors, superimposition, dissolves, live actors, smoke (sometimes to project images upon), odors, sounds and even electric shocks. While the first magic lantern images seem to have been intended to scare audiences, soon all sorts of subjects appeared and the lantern was not only used for storytelling but also for education. In the 19th century several new and popular magic lantern techniques were developed, including dissolving views and several types of mechanical slides that created dazzling abstract effects (chromatrope, etc.) or that showed for instance falling snow, or the planets and their moons revolving.\n\nIn the 1890s, films were seen mostly via temporary storefront spaces and traveling exhibitors or as acts in vaudeville programs. A film could be under a minute long and would usually present a single scene, authentic or staged, of everyday life, a public event, a sporting event or slapstick. There was little to no cinematic technique, the film was usually black and white and it was without sound.\nThe novelty of realistic moving photographs was enough for a motion picture industry to blossom before the end of the century, in countries around the world. \"The Cinema\" was to offer a cheaper, simpler way of providing entertainment to the masses. Filmmakers could record actors' performances, which then could be shown to audiences around the world. Travelogues would bring the sights of far-flung places, with movement, directly to spectators' hometowns. Movies would become the most popular visual art form of the late Victorian age.\n\nThe Berlin Wintergarten theater hosted an early movie presentation in front of an audience, shown by the Skladanowsky brothers in 1895. The Melbourne Athenaeum started to screen movies in 1896. Movie theaters became popular entertainment venues and social hubs in the early 20th century, much like cabarets and other theaters.\nUntil 1927, motion pictures were produced without sound. This era is referred to as the silent era of film. To enhance the viewers' experience, silent films were commonly accompanied by live musicians in an orchestra, a theatre organ, and sometimes sound effects and even commentary spoken by the showman or projectionist. In most countries, intertitles came to be used to provide dialogue and narration for the film, thus dispensing with narrators, but in Japanese cinema human narration remained popular throughout the silent era. The technical problems were resolved by 1923.\n\nIllustrated songs were a notable exception to this trend that began in 1894 in vaudeville houses and persisted as late as the late 1930s in film theaters. Live performance or sound recordings were paired with hand-colored glass slides projected through stereopticons and similar devices. In this way, song narrative was illustrated through a series of slides whose changes were simultaneous with the narrative development. The main purpose of illustrated songs was to encourage sheet music sales, and they were highly successful with sales reaching into the millions for a single song. Later, with the birth of film, illustrated songs were used as filler material preceding films and during reel changes.\n\nThe 1914 \"The Photo-Drama of Creation\" was a non-commercial attempt to combine the motion picture with a combination of slides and synchronize the resulting moving picture with audio. The film included hand-painted slides as well as other previously used techniques. Simultaneously playing the audio while the film was being played with a projector was required. Produced by the Watch Tower Bible and Tract Society of Pennsylvania (Jehovah's Witnesses), this eight–hour bible drama was being shown in 80 cities every day and almost eight million people in the United States and Canada saw the presentation.\n\nWithin eleven years of motion pictures, the films moved from a novelty show to an established large-scale entertainment industry. Films moved from a single shot, completely made by one person with a few assistants, towards films several minutes long consisting of several shots, which were made by large companies in something like industrial conditions.\n\nBy 1900, the first motion pictures that can be considered as \"films\" emerged, and film-makers began to introduce basic editing techniques and film narrative.\n\nEarly movie cameras were fastened to the head of a tripod with only simple levelling devices provided. These cameras were effectively fixed during the course of a shot, and the first camera movements were the result of mounting a camera on a moving vehicle. The Lumière brothers shot a scene from the back of a train in 1896.\n\nThe first rotating camera for taking panning shots was built by Robert W. Paul in 1897, on the occasion of Queen Victoria's Diamond Jubilee. He used his camera to shoot the procession in one shot. His device had the camera mounted on a vertical axis that could be rotated by a worm gear driven by turning a crank handle, and Paul put it on general sale the next year. Shots taken using such a \"panning\" head were also referred to as 'panoramas' in the film catalogues.\n\nGeorges Méliès built one of the first film studios in May 1897. It had a glass roof and three glass walls constructed after the model of large studios for still photography, and it was fitted with thin cotton cloths that could be stretched below the roof to diffuse the direct rays of the sun on sunny days. Beginning in 1896, Méliès would go on to produce, direct, and distribute over 500 short films. The majority of these films were short, one-shot films completed in one take. Méliès drew many comparisons between film and the stage, which was apparent in his work. He realized that film afforded him the ability (via his use of time lapse photography) to \"produce visual spectacles not achievable in the theater.\n\n\"The Execution of Mary Stuart\", produced by the Edison Company for viewing with the Kinetoscope, showed Mary Queen of Scots being executed in full view of the camera. The effect was achieved by replacing the actor with a dummy for the final shot. Georges Méliès also utilized this technique in the making of \"Escamotage d'un dame chez Robert-Houdin (The Vanishing Lady)\". The woman is seen to vanish through the use of stop motion techniques.\nThe other basic technique for trick cinematography was the double exposure of the film in the camera. This was pioneered by George Albert Smith in July 1898 in England. The set was draped in black, and after the main shot, the negative was re-exposed to the overlaid scene. His \"The Corsican Brothers\" was described in the catalogue of the Warwick Trading Company in 1900: \"By extremely careful photography the ghost appears *quite transparent*. After indicating that he has been killed by a sword-thrust, and appealing for vengeance, he disappears. A 'vision' then appears showing the fatal duel in the snow.”\n\nG.A. Smith also initiated the special effects technique of reverse motion. He did this by repeating the action a second time, while filming it with an inverted camera, and then joining the tail of the second negative to that of the first. The first films made using this device were \"Tipsy, Topsy, Turvy\" and \"The Awkward Sign Painter\". The earliest surviving example of this technique is Smith's \"The House That Jack Built\", made before September 1900.\n\nCecil Hepworth took this technique further, by printing the negative of the forwards motion backwards frame by frame, so producing a print in which the original action was exactly reversed. To do this he built a special printer in which the negative running through a projector was projected into the gate of a camera through a special lens giving a same-size image. This arrangement came to be called a \"projection printer\", and eventually an \"optical printer\".\n\nThe use of different camera speeds also appeared around 1900 in the films of Robert W. Paul and Hepworth. Paul shot scenes from \"On a Runaway Motor Car through Piccadilly Circus\" (1899) with the camera turning very slowly. When the film was projected at the usual 16 frames per second, the scenery appeared to be passing at great speed. Hepworth used the opposite effect in \"The Indian Chief and the Seidlitz Powder\" (1901). The Chief's movements are sped up by cranking the camera much faster than 16 frames per second. This gives what we would call a \"slow motion\" effect.\n\nThe first films to consist of more than one shot appeared toward the end of the 19th century, a notable example was the French film of the life of Jesus Christ, \"La vie du Christ (The Birth, the Life and the Death of Christ)\", by Alice Guy. These weren't represented as a continuous film, the separate scenes were interspersed with lantern slides, a lecture, and live choral numbers, to increase the running time of the spectacle to about 90 minutes. Another example of this is the reproductions of scenes from the Greco-Turkish war, made by Georges Méliès in 1897. Although each scene was sold separately, they were shown one after the other by the exhibitors. Even Méliès' \"Cendrillon (Cinderella)\" of 1898 contained no action moving from one shot to the next one. To understand what was going on in the film the audience had to know their stories beforehand, or be told them by a presenter.\nReal film continuity, involving action moving from one sequence into another, is attributed to British film pioneer Robert W. Paul's \"Come Along, Do!\", made in 1898 and one of the first films to feature more than one shot. In the first shot, an elderly couple is outside an art exhibition having lunch and then follow other people inside through the door. The second shot shows what they do inside. Paul's 'Cinematograph Camera No. 1' of 1895 was the first camera to feature reverse-cranking, which allowed the same film footage to be exposed several times and thereby to create super-positions and multiple exposures. This technique was first used in his 1901 film \"Scrooge, or, Marley's Ghost\".\n\nThe further development of action continuity in multi-shot films continued in 1899 at the Brighton School in England. In the latter part of that year, George Albert Smith made \"The Kiss in the Tunnel\". This started with a shot from a \"phantom ride\" at the point at which the train goes into a tunnel, and continued with the action on a set representing the interior of a railway carriage, where a man steals a kiss from a woman, and then cuts back to the phantom ride shot when the train comes out of the tunnel. A month later, the Bamforth company in Yorkshire made a restaged version of this film under the same title, and in this case they filmed shots of a train entering and leaving a tunnel from beside the tracks, which they joined before and after their version of the kiss inside the train compartment.\nIn 1900, continuity of action across successive shots was definitively established by George Albert Smith and James Williamson, who also worked in Brighton. In that year Smith made \"As Seen Through a Telescope\", in which the main shot shows street scene with a young man tying the shoelace and then caressing the foot of his girlfriend, while an old man observes this through a telescope. There is then a cut to close shot of the hands on the girl's foot shown inside a black circular mask, and then a cut back to the continuation of the original scene. Even more remarkable is James Williamson's \"Attack on a China Mission Station\" (1900). The first shot shows Chinese Boxer rebels at the gate; it then cuts to the missionary family in the garden, where a fight ensues. The wife signals to British sailors from the balcony, who come and rescue them. The film also used the first \"reverse angle\" cut in film history.\n\nG.A Smith pioneered the use of the close-up shot in his 1900 films \"As Seen Through a Telescope\" and \"Grandma's Reading Glass\". He further developed the ideas of breaking a scene shot in one place into a series of shots taken from different camera positions over the next couple of years, starting with \"The Little Doctors\" of 1901. In a series of films he produced at this time, he also introduced the use of subjective and objective point-of-view shots, the creation of dream-time and the use of reversing. He summed up his work in \"Mary Jane's Mishap\" of 1903, with repeated cuts in to a close shot of a housemaid fooling around, along with superimpositions and other devices, before abandoning film-making to invent the Kinemacolor system of colour cinematography. His films were the first to establish the basics of coherent narrative and what became known as film language, or \"film grammar\".\n\nJames Williamson concentrated on making films taking action from one place shown in one shot to the next shown in another shot in films like \"Stop Thief!\", made in 1901, and many others. He also experimented with the close-up, and made perhaps the most extreme one of all in \"The Big Swallow\", when his character approaches the camera and appears to swallow it. These two film makers of the Brighton School also pioneered the editing of the film; they tinted their work with color and used trick photography to enhance the narrative. By 1900, their films were extended scenes of up to 5 minutes long.\n\nMost films of this period were what came to be called \"chase films\". These were inspired by James Williamson's \"Stop Thief!\" of 1901, which showed a tramp stealing a leg of mutton from a butcher's boy in the first shot, then being chased through the second shot by the butcher's boy and assorted dogs, and finally being caught by the dogs in the third shot. Several British films made in the first half of 1903 extended the chase method of film construction. These included \"An Elopement à la Mode\" and \"The Pickpocket: A Chase Through London\", made by Alf Collins for the British branch of the French Gaumont company, \"Daring Daylight Burglary\", made by Frank Mottershaw at the Sheffield Photographic Company, and \"Desperate Poaching Affray\", made by William Haggar. Haggar in particular innovated the first extant panning shots; the poachers are chased by gamekeepers and police officers and the camera pans along, creating a sense of urgency and speed. His films were also recognised for their intelligent use of depth of staging and screen edges, while film academic Noël Burch praised Haggar's effective use of off-screen space. He was also one of the first film makers to purposefully introduce violence for entertainment; in \"Desperate Poaching Affray\" the villains are seen firing guns at their pursuers.\nOther filmmakers took up all these ideas including the American Edwin S. Porter, who started making films for the Edison Company in 1901. Porter, a projectionist, was hired by Thomas Edison to develop his new projection model known as the Vitascope. Porter wanted to develop a style of filmmaking that would move away from the one-shot short films into a \"story-telling [narrative]\" style. When he began making longer films in 1902, he put a dissolve between every shot, just as Georges Méliès was already doing, and he frequently had the same action repeated across the dissolves. His film, \"The Great Train Robbery\" (1903), had a running time of twelve minutes, with twenty separate shots and ten different indoor and outdoor locations. He used cross-cutting editing method to show simultaneous action in different places. The time continuity in \"The Great Train Robbery\" was actually more confusing than that in the films it was modeled on, but nevertheless it was a greater success than them due to its Wild West violence. \"The Great Train Robbery\" served as one of the vehicles that would launch the film medium into mass popularity.\n\nThe Pathé company in France also made imitations and variations of Smith and Williamson's films from 1902 onwards using cuts between the shots, which helped to standardize the basics of film construction. An influential French film of the period was Méliès's 14-minute-long \"A Trip to the Moon\". It was extremely popular at the time of its release, and is the best-known of the hundreds of films made by Méliès. It was one of the first known science fiction films, and used innovative animation and special effects, including the well-known image of the spaceship landing in the Moon's eye. The sheer volume of Pathé's production led to their filmmakers giving a further precision and polish to the details of film continuity.\n\nThe first use of animation in movies was in 1899, with the production of the short film Matches: An Appeal by British film pioneer Arthur Melbourne-Cooper- a thirty-second long stop-motion animated piece intended to encourage the audience to send matches to British troops fighting the Boer War. The film contains an appeal to send money to Bryant and May who would then send matches to the troops fighting in South Africa. It was shown in December 1899 at The Empire Theatre in London. This film is the earliest known example of stop-motion animation. Little puppets, constructed of matchsticks, are writing the appeal on a black wall. Their movements are filmed frame by frame, movement by movement.\nThe relative sophistication of this piece was not followed up for some time, with subsequent works in animation being limited to short, two or three frame effects, such as appeared in Edwin Stanton Porter's 1902 short \"Fun in a Bakery Shop\", where a lump of dough was made to smile over the course of a three-frame sequence. Works rivaling the British short in length did not appear until 1905, when Edwin Porter made \"How Jones Lost His Roll\", and \"The Whole Dam Family and the Dam Dog\". Both of these films had intertitles which were formed by the letters moving into place from a random scattering to form the words of the titles. This was done by exposing the film one frame at a time, and moving the letters a little bit towards their final position between each exposure. This is what has come to be called \"single frame animation\" or \"object animation\", and it needs a slightly adapted camera that exposes only one frame for each turn of the crank handle, rather than the usual eight frames per turn.\nIn 1906, Albert Edward Smith and James Stuart Blackton at Vitagraph Studios took the next step, and in their \"Humorous Phases of Funny Faces\", what appear to be cartoon drawings of people move from one pose to another. This is done for most of the length of this film by moving jointed cut-outs of the figures frame by frame between the exposures, just as Porter moved his letters. However, there is a very short section of the film where things are made to appear to move by altering the drawings themselves from frame to frame, which is how standard animated cartoons have since been made up to today.\n\nThe technique of single frame animation was further developed in 1907 by Edwin S. Porter in \"The Teddy Bears\" and by J. Stuart Blackton with \"Work Made Easy\". In the first of these the toy bears were made to move, apparently on their own, and in the latter film building tools were made to perform construction tasks without human intervention, by using frame-by-frame animation. The technique got to Europe almost immediately, and Segundo de Chomon and others at Pathé took it further, adding clay animation, in which sculptures were deformed from one thing into another thing frame by frame in \"Sculpture moderne\" (1908), and then Pathé made the next step to the animation of silhouette shapes. Also in France, Émile Cohl fully developed drawn animation in a series of films starting with \"Fantasmagorie\" (1908), in which humans and objects drawn as outline figures went through a series of remarkable interactions and transformations. In the United States the response was from the famous strip cartoon artist Winsor McCay, who drew much more realistic animated figures going through smoother, more naturalistic motion in a series of films starting with the film \"Little Nemo\", made for Vitagraph in 1911. In the next few years various others took part in this development of animated cartoons in the United States and elsewhere.\n\nThe world's first animated feature film was \"El Apóstol\" (1917), made by Italian-Argentine cartoonist Quirino Cristiani utilizing cutout animation. Cristiani also directed the first animated feature film with sound, \"Peludópolis\", released with a vitaphone sound-on-disc synchronization system soundtrack. Unfortunately, a fire that destroyed producer Federico Valle's film studio incinerated the only known copies of the movies, and they are now considered as lost films.\n\nIn 1932, the first short animated film created entirely with technicolor (the trichromatic procedure (green, red, blue), whose use required a triple photographic impression, incorporation of chromatic filters and cameras of enormous dimensions) was Walt Disney's \"Flowers and Trees\", directed by Burt Gillett.\n\nFilms at the time were no longer than one reel, although some multi-reel films had been made on the life of Christ in the first few years of cinema. The first feature length multi-reel film in the world was the 1906 Australian production called \"The Story of the Kelly Gang\".\n\nIt traced the life of the legendary infamous outlaw and bushranger Ned Kelly (1855–1880) and ran for more than an hour with a reel length of approximately 4,000 feet (1,200 m). It was first shown at the Athenaeum Hall in Collins Street, Melbourne, Australia on 26 December 1906 and in the UK in January 1908.\n\nThe first successful permanent theatre showing only films was \"The Nickelodeon\", which was opened in Pittsburgh in 1905. By then there were enough films several minutes long available to fill a programme running for at least half an hour, and which could be changed weekly when the local audience became bored with it. Other exhibitors in the United States quickly followed suit, and within a couple of years there were thousands of these nickelodeons in operation. The American experience led to a worldwide boom in the production and exhibition of films from 1906 onwards.\n\nBy 1907 purpose-built cinemas for motion pictures were being opened across the United States, Britain and France. The films were often shown with the accompaniment of music provided by a pianist, though there could be more musicians. There were also a very few larger cinemas in some of the biggest cities. Initially, the majority of films in the programmes were Pathé films, but this changed fairly quickly as the American companies cranked up production. The programme was made up of just a few films, and the show lasted around 30 minutes. The reel of film, of maximum length , which usually contained one individual film, became the standard unit of film production and exhibition in this period. The programme was changed twice or more a week, but went up to five changes of programme a week after a couple of years. In general, cinemas were set up in the established entertainment districts of the cities. In 1907, Pathé began renting their films to cinemas through film exchanges rather than selling the films outright.\nBy about 1910, actors began to receive screen credit for their roles, and the way to the creation of film stars was opened. Films were increasingly longer, and began to feature proper plots and development.\n\nThe litigation over patents between all the major American film-making companies led to the formation of a trust to control the American film business, with each company in the trust being allocated production quotas (two reels a week for the biggest ones, one reel a week for the smaller). However, although 6,000 exhibitors signed up to the trust, about 2,000 others did not and began to fund new film producing companies. By 1912 the independents had nearly half of the market and the government defeated the trust by initiating anti-trust action at the same time.\n\nIn the early 20th century, before Hollywood, the motion picture industry was based in Fort Lee, New Jersey across the Hudson River from New York City. In need of a winter headquarters, moviemakers were attracted to Jacksonville, Florida due to its warm climate, exotic locations, excellent rail access, and cheaper labor, earning the city the title of \"The Winter Film Capital of the World.\" New York-based Kalem Studios was the first to open a permanent studio in Jacksonville in 1908. Over the course of the next decade, more than 30 silent film companies established studios in town, including Metro Pictures (later MGM), Edison Studios, Majestic Films, King Bee Film Company, Vim Comedy Company, Norman Studios, Gaumont Studios and the Lubin Manufacturing Company. Comedic actor and Georgia native Oliver \"Babe\" Hardy began his motion picture career here in 1914. He starred in over 36 short silent films his first year acting. With the closing of Lubin in early 1915, Oliver moved to New York then New Jersey to find film jobs. Acquiring a job with the Vim Company in early 1915, he returned to Jacksonville in the spring of 1917 before relocating to Los Angeles in October 1917. The first motion picture made in Technicolor and the first feature-length color movie produced in the United States, The Gulf Between, was also filmed on location in Jacksonville in 1917.\n\nJacksonville was especially important to the African American film industry. One notable individual in this regard is the European American producer Richard Norman, who created a string of films starring black actors in the vein of Oscar Micheaux and the Lincoln Motion Picture Company. In contrast to the degrading parts offered in certain white films such as The Birth of a Nation, Norman and his contemporaries sought to create positive stories featuring African Americans in what he termed \"splendidly assuming different roles.\"\n\nJacksonville's mostly conservative residents, however, objected to the hallmarks of the early movie industry, such as car chases in the streets, simulated bank robberies and fire alarms in public places, and even the occasional riot. In 1917, conservative Democrat John W. Martin was elected mayor on the platform of taming the city's movie industry. By that time, southern California was emerging as the major movie production center, thanks in large part to the move of film pioneers like William Selig and D.W. Griffith to the area. These factors quickly sealed the demise of Jacksonville as a major film destination.\n\nAnother factor for the industry's move west was that up until 1913, most American film production was still carried out around New York, but due to the monopoly of Thomas A. Edison, Inc.'s film patents and its litigious attempts to preserve it, many filmmakers moved to Southern California, starting with Selig in 1909. The sunshine and scenery was important for the production of Westerns, which came to form a major American film genre with the first cowboy stars, G.M. Anderson (\"Broncho Billy\") and Tom Mix. Selig pioneered the use of (fairly) wild animals from a zoo for a series of exotic adventures, with the actors being menaced or saved by the animals. Kalem Company sent film crews to places in America and abroad to film stories in the actual places they were supposed to have happened. Kalem also pioneered the female action heroine from 1912, with Ruth Roland playing starring roles in their Westerns.\n\nIn France, Pathé retained its dominant position, followed still by Gaumont, and then other new companies that appeared to cater to the film boom. A film company with a different approach was Film d'Art. This was set up at the beginning of 1908 to make films of a serious artistic nature. Their declared programme was to make films using only the best dramatists, artists and actors. The first of these was \"L'Assassinat du Duc de Guise\" (\"The Assassination of the Duc de Guise\"), a historical subject set in the court of Henri III. This film used leading actors from the Comédie-Française, and had a special accompanying score written by Camille Saint-Saëns. The other French majors followed suit, and this wave gave rise to the English-language description of films with artistic pretensions aimed at a sophisticated audience as \"art films\". By 1910, the French film companies were starting to make films as long as two, or even three reels, though most were still one reel long. This trend was followed in Italy, Denmark, and Sweden.\n\nIn Britain, the Cinematograph Act 1909 was the first primary legislation to specifically regulate the film industry. Film exhibitions often took place in temporary venues and the use of highly flammable cellulose nitrate for film, combined with limelight illumination, created a significant fire hazard. The Act specified a strict building code which required, amongst other things, that the projector be enclosed within a fire resisting enclosure.\n\nRegular newsreels were exhibited from 1910 and soon became a popular way for finding out the news the British Antarctic Expedition to the South Pole was filmed for the newsreels as were the suffragette demonstrations that were happening at the same time. F. Percy Smith was an early nature documentary pioneer working for Charles Urban and he pioneered the use of time lapse and micro cinematography in his 1910 documentary on the growth of flowers.\n\nWith the worldwide film boom, yet more countries now joined Britain, France, Germany and the United States in serious film production. In Italy, production was spread over several centres, with Turin being the first and biggest. There, Ambrosio was the first company in the field in 1905, and remained the largest in the country through this period. Its most substantial rival was Cines in Rome, which started producing in 1906. The great strength of the Italian industry was historical epics, with large casts and massive scenery. As early as 1911, Giovanni Pastrone's two-reel \"La Caduta di Troia (The Fall of Troy)\" made a big impression worldwide, and it was followed by even bigger glasses like \"Quo Vadis?\" (1912), which ran for 90 minutes, and Pastrone's \"Cabiria\" of 1914, which ran for two and a half hours.\nItalian companies also had a strong line in slapstick comedy, with actors like André Deed, known locally as \"Cretinetti\", and elsewhere as \"Foolshead\" and \"Gribouille\", achieving worldwide fame with his almost surrealistic gags.\n\nThe most important film-producing country in Northern Europe up until the First World War was Denmark. The Nordisk company was set up there in 1906 by Ole Olsen, a fairground showman, and after a brief period imitating the successes of French and British filmmakers, in 1907 he produced 67 films, most directed by Viggo Larsen, with sensational subjects like \"Den hvide Slavinde (The White Slave)\", \"Isbjørnenjagt (Polar Bear Hunt)\" and \"Løvejagten (The Lion Hunt)\". By 1910, new smaller Danish companies began joining the business, and besides making more films about the white slave trade, they contributed other new subjects. The most important of these finds was Asta Nielsen in \"Afgrunden (The Abyss)\", directed by Urban Gad for Kosmorama, This combined the circus, sex, jealousy and murder, all put over with great conviction, and pushed the other Danish filmmakers further in this direction. By 1912, the Danish film companies were multiplying rapidly.\n\nThe Swedish film industry was smaller and slower to get started than the Danish industry. Here, the important man was Charles Magnusson, a newsreel cameraman for the Svenskabiografteatern cinema chain. He started fiction film production for them in 1909, directing a number of the films himself. Production increased in 1912, when the company engaged Victor Sjöström and Mauritz Stiller as directors. They started out by imitating the subjects favoured by the Danish film industry, but by 1913 they were producing their own strikingly original work, which sold very well.\n\nRussia began its film industry in 1908 with Pathé shooting some fiction subjects there, and then the creation of real Russian film companies by Aleksandr Drankov and Aleksandr Khanzhonkov. The Khanzhonkov company quickly became much the largest Russian film company, and remained so until 1918.\n\nIn Germany, Oskar Messter had been involved in film-making from 1896, but did not make a significant number of films per year until 1910. When the worldwide film boom started, he, and the few other people in the German film business, continued to sell prints of their own films outright, which put them at a disadvantage. It was only when Paul Davidson, the owner of a chain of cinemas, brought Asta Nielsen and Urban Gad to Germany from Denmark in 1911, and set up a production company, Projektions-AG \"Union\" (PAGU), for them, that a change-over to renting prints began. Messter replied with a series of longer films starring Henny Porten, but although these did well in the German-speaking world, they were not particularly successful internationally, unlike the Asta Nielsen films. Another of the growing German film producers just before World War I was the German branch of the French Éclair company, Deutsche Éclair. This was expropriated by the German government, and turned into DECLA when the war started. But altogether, German producers only had a minor part of the German market in 1914.\n\nOverall, from about 1910, American films had the largest share of the market in all European countries except France, and even in France, the American films had just pushed the local production out of first place on the eve of World War I. So even if the war had not happened, American films may have become dominant worldwide. Although the war made things much worse for European producers, the technical qualities of American films made them increasingly attractive to audiences everywhere.\n\nNew film techniques that were introduced in this period include the use of artificial lighting, fire effects and Low-key lighting (i.e. lighting in which most of the frame is dark) for enhanced atmosphere during sinister scenes.\n\nContinuity of action from shot to shot was also refined, such as in Pathé's \"le Cheval emballé (The Runaway Horse)\" (1907) where cross-cutting between parallel actions is used. D. W. Griffith also began using cross-cutting in the film \"The Fatal Hour\", made in July 1908. Another development was the use of the Point of View shot, first used in 1910 in Vitagraph's \"Back to Nature\". Insert shots were also used for artistic purposes; the Italian film \"La mala planta (The Evil Plant)\", directed by Mario Caserini had an insert shot of a snake slithering over the \"Evil Plant\".\n\nAs films grew longer, specialist writers were employed to simplify more complex stories derived from novels or plays into a form that could be contained on one reel. Genres began to be used as categories; the main division was into comedy and drama, but these categories were further subdivided.\n\nIntertitles containing lines of dialogue began to be used consistently from 1908 onwards, such as in Vitagraph's \"An Auto Heroine; or, The Race for the Vitagraph Cup and How It Was Won\". The dialogue was eventually inserted into the middle of the scene and became commonplace by 1912. The introduction of dialogue titles transformed the nature of film narrative. When dialogue titles came to be always cut into a scene just after a character starts speaking, and then left with a cut to the character just before they finish speaking, then one had something that was effectively the equivalent of a present-day sound film.\n\nThe years of the First World War were a complex transitional period for the film industry. The exhibition of films changed from short one-reel programmes to feature films. Exhibition venues became larger and began charging higher prices.\n\nIn the United States, these changes brought destruction to many film companies, the Vitagraph company being an exception. Film production began to shift to Los Angeles during World War I. The Universal Film Manufacturing Company was formed in 1912 as an umbrella company. New entrants included the Jesse Lasky Feature Play Company, and Famous Players, both formed in 1913, and later amalgamated into Famous Players-Lasky. The biggest success of these years was David Wark Griffith's \"The Birth of a Nation\" (1915). Griffith followed this up with the even bigger \"Intolerance\" (1916), but, due to the high quality of film produced in the US, the market for their films was high.\n\nIn France, film production shut down due to the general military mobilization of the country at the start of the war. Although film production began again in 1915, it was on a reduced scale, and the biggest companies gradually retired from production. Italian film production held up better, although so called \"diva films\", starring anguished female leads were a commercial failure. In Denmark, the Nordisk company increased its production so much in 1915 and 1916 that it could not sell all its films, which led to a very sharp decline in Danish production, and the end of Denmark's importance on the world film scene.\n\nThe German film industry was seriously weakened by the war. The most important of the new film producers at the time was Joe May, who made a series of thrillers and adventure films through the war years, but Ernst Lubitsch also came into prominence with a series of very successful comedies and dramas.\n\nAt this time, studios were blacked out to allow shooting to be unaffected by changing sunlight. This was replaced with floodlights and spotlights. The widespread adoption of irising-in and out to begin and end scenes caught on in this period. This is the revelation of a film shot in a circular mask, which gradually gets larger until it expands beyond the frame. Other shaped slits were used, including vertical and diagonal apertures.\n\nA new idea taken over from still photography was \"soft focus\". This began in 1915, with some shots being intentionally thrown out of focus for expressive effect, as in Mary Pickford starrer \"Fanchon the Cricket\".\n\nIt was during this period that camera effects intended to convey the subjective feelings of characters in a film really began to be established. These could now be done as Point of View (POV) shots, as in Sidney Drew's \"The Story of the Glove\" (1915), where a wobbly hand-held shot of a door and its keyhole represents the POV of a drunken man. The use of anamorphic (in the general sense of distorted shape) images first appears in these years with Abel Gance directed \"la Folie du Docteur Tube (The Madness of Dr. Tube)\". In this film the effect of a drug administered to a group of people was suggested by shooting the scenes reflected in a distorting mirror of the fair-ground type.\n\nSymbolic effects taken over from conventional literary and artistic tradition continued to make some appearances in films during these years. In D. W. Griffith's \"The Avenging Conscience\" (1914), the title \"The birth of the evil thought\" precedes a series of three shots of the protagonist looking at a spider, and ants eating an insect. Symbolist art and literature from the turn of the century also had a more general effect on a small number of films made in Italy and Russia. The supine acceptance of death resulting from passion and forbidden longings was a major feature of this art, and states of delirium dwelt on at length were important as well.\nThe use of insert shots, i.e. close-ups of objects other than faces, had already been established by the Brighton school, but were infrequently used before 1914. It is really only with Griffith's \"The Avenging Conscience\" that a new phase in the use of the Insert Shot starts. As well as the symbolic inserts already mentioned, the film also made extensive use of large numbers of Big Close Up shots of clutching hands and tapping feet as a means of emphasizing those parts of the body as indicators of psychological tension.\n\nAtmospheric inserts were developed in Europe in the late 1910s. This kind of shot is one in a scene which neither contains any of the characters in the story, nor is a Point of View shot seen by one of them. An early example is in Maurice Tourneur directed \"The Pride of the Clan\" (1917), in which there is a series of shots of waves beating on a rocky shore to demonstrate the harsh lives of the fishing folk. Maurice Elvey's \"Nelson; The Story of England's Immortal Naval Hero\" (1919) has a symbolic sequence dissolving from a picture of Kaiser Wilhelm II to a peacock, and then to a battleship.\n\nBy 1914, continuity cinema was the established mode of commercial cinema. One of the advanced continuity techniques involved an accurate and smooth transition from one shot to another. Cutting to \"different\" angles within a scene also became well-established as a technique for dissecting a scene into shots in American films. If the direction of the shot changes by more than ninety degrees, it is called a reverse-angle cutting. The leading figure in the full development of reverse-angle cutting was Ralph Ince in his films, such as \"The Right Girl\" and \"His Phantom Sweetheart\"\n\nThe use of flash-back structures continued to develop in this period, with the usual way of entering and leaving a flash-back being through a dissolve. The Vitagraph company's \"The Man That Might Have Been\" (William J. Humphrey, 1914), is even more complex, with a series of reveries and flash-backs that contrast the protagonist's real passage through life with what might have been, if his son had not died.\n\nAfter 1914, cross cutting between parallel actions came to be used more so in American films than in European ones. Cross-cutting was often used to get new effects of contrast, such as the cross-cut sequence in Cecil B. DeMille's \"The Whispering Chorus\" (1918), in which a supposedly dead husband is having a liaison with a Chinese prostitute in an opium den, while simultaneously his unknowing wife is being remarried in church.\n\nThe general trend in the development of cinema, led from the United States, was towards using the newly developed specifically filmic devices for expression of the narrative content of film stories, and combining this with the standard dramatic structures already in use in commercial theatre. D. W. Griffith had the highest standing amongst American directors in the industry, because of the dramatic excitement he conveyed to the audience through his films. Cecil B. DeMille's \"The Cheat\" (1915), brought out the moral dilemmas facing their characters in a more subtle way than Griffith. DeMille was also in closer touch with the reality of contemporary American life. Maurice Tourneur was also highly ranked for the pictorial beauties of his films, together with the subtlety of his handling of fantasy, while at the same time he was capable of getting greater naturalism from his actors at appropriate moments, as in \"A Girl's Folly\" (1917).\n\nSidney Drew was the leader in developing \"polite comedy\", while slapstick was refined by Fatty Arbuckle and Charles Chaplin, who both started with Mack Sennett's Keystone company. They reduced the usual frenetic pace of Sennett's films to give the audience a chance to appreciate the subtlety and finesse of their movement, and the cleverness of their gags. By 1917 Chaplin was also introducing more dramatic plot into his films, and mixing the comedy with sentiment.\n\nIn Russia, Yevgeni Bauer put a slow intensity of acting combined with Symbolist overtones onto film in a unique way.\n\nIn Sweden, Victor Sjöström made a series of films that combined the realities of people's lives with their surroundings in a striking manner, while Mauritz Stiller developed sophisticated comedy to a new level.\n\nIn Germany, Ernst Lubitsch got his inspiration from the stage work of Max Reinhardt, both in bourgeois comedy and in spectacle, and applied this to his films, culminating in his \"die Puppe\" (\"The Doll\"), \"die Austernprinzessin\" (\"The Oyster Princess\") and \"Madame DuBarry\".\n\nAt the start of the First World War, French and Italian cinema had been the most globally popular. The war came as a devastating interruption to European film industries. The American industry, or \"Hollywood\", as it was becoming known after its new geographical center in California, gained the position it has held, more or less, ever since: film factory for the world and exporting its product to most countries on earth.\n\nBy the 1920s, the United States reached what is still its era of greatest-ever output, producing an average of 800 \"feature\" films annually, or 82% of the global total (Eyman, 1997). The comedies of Charlie Chaplin and Buster Keaton, the swashbuckling adventures of Douglas Fairbanks and the romances of Clara Bow, to cite just a few examples, made these performers' faces well known on every continent. The Western visual norm that would become classical continuity editing was developed and exported – although its adoption was slower in some non-Western countries without strong realist traditions in art and drama, such as Japan.\n\nThis development was contemporary with the growth of the studio system and its greatest publicity method, the star system, which characterized American film for decades to come and provided models for other film industries. The studios' efficient, top-down control over all stages of their product enabled a new and ever-growing level of lavish production and technical sophistication. At the same time, the system's commercial regimentation and focus on glamorous escapism discouraged daring and ambition beyond a certain degree, a prime example being the brief but still legendary directing career of the iconoclastic Erich von Stroheim in the late teens and the 1920s.\n\nDuring late 1927, Warners released \"The Jazz Singer\", which was mostly silent but contained what is generally regarded as the first synchronized dialogue (and singing) in a feature film; but this process was actually accomplished first by Charles Taze Russell in 1914 with the lengthy film \"The Photo-Drama of Creation\". This drama consisted of picture slides and moving pictures synchronized with phonograph records of talks and music. The early sound-on-disc processes such as Vitaphone were soon superseded by sound-on-film methods like Fox Movietone, DeForest Phonofilm, and RCA Photophone. The trend convinced the largely reluctant industrialists that \"talking pictures\", or \"talkies\", were the future. A lot of attempts were made before the success of \"The Jazz Singer\", that can be seen in the List of film sound systems.\n\nThe change was remarkably swift. By the end of 1929, Hollywood was almost all-talkie, with several competing sound systems (soon to be standardized). Total changeover was slightly slower in the rest of the world, principally for economic reasons. Cultural reasons were also a factor in countries like China and Japan, where silents co-existed successfully with sound well into the 1930s, indeed producing what would be some of the most revered classics in those countries, like Wu Yonggang's \"The Goddess\" (China, 1934) and Yasujirō Ozu's \"I Was Born, But...\" (Japan, 1932). But even in Japan, a figure such as the benshi, the live narrator who was a major part of Japanese silent cinema, found his acting career was ending.\n\nSound further tightened the grip of major studios in numerous countries: the vast expense of the transition overwhelmed smaller competitors, while the novelty of sound lured vastly larger audiences for those producers that remained. In the case of the U.S., some historians credit sound with saving the Hollywood studio system in the face of the Great Depression (Parkinson, 1995). Thus began what is now often called \"The Golden Age of Hollywood\", which refers roughly to the period beginning with the introduction of sound until the late 1940s. The American cinema reached its peak of efficiently manufactured glamour and global appeal during this period. The top actors of the era are now thought of as the classic film stars, such as Clark Gable, Katharine Hepburn, Humphrey Bogart, Greta Garbo, and the greatest box office draw of the 1930s, child performer Shirley Temple.\n\nCreatively, however, the rapid transition was a difficult one, and in some ways, film briefly reverted to the conditions of its earliest days. The late '20s were full of static, stagey talkies as artists in front of and behind the camera struggled with the stringent limitations of the early sound equipment and their own uncertainty as to how to utilize the new medium. Many stage performers, directors and writers were introduced to cinema as producers sought personnel experienced in dialogue-based storytelling. Many major silent filmmakers and actors were unable to adjust and found their careers severely curtailed or even ended.\n\nThis awkward period was fairly short-lived. 1929 was a watershed year: William Wellman with \"Chinatown Nights\" and \"The Man I Love\", Rouben Mamoulian with \"Applause\", Alfred Hitchcock with \"Blackmail\" (Britain's first sound feature), were among the directors to bring greater fluidity to talkies and experiment with the expressive use of sound (Eyman, 1997). In this, they both benefited from, and pushed further, technical advances in microphones and cameras, and capabilities for editing and post-synchronizing sound (rather than recording all sound directly at the time of filming).\nSound films emphasized black history and benefited different genres more so than silents did. Most obviously, the musical film was born; the first classic-style Hollywood musical was \"The Broadway Melody\" (1929) and the form would find its first major creator in choreographer/director Busby Berkeley (\"42nd Street\", 1933, \"Dames\", 1934). In France, avant-garde director René Clair made surreal use of song and dance in comedies like \"Under the Roofs of Paris\" (1930) and \"Le Million\" (1931). Universal Pictures begin releasing gothic horror films like \"Dracula\" and \"Frankenstein\" (both 1931). In 1933, RKO Pictures released Merian C. Cooper's classic \"giant monster\" film \"King Kong\". The trend thrived best in India, where the influence of the country's traditional song-and-dance drama made the musical the basic form of most sound films (Cook, 1990); virtually unnoticed by the Western world for decades, this Indian popular cinema would nevertheless become the world's most prolific. (\"See also Bollywood.\")\n\nAt this time, American gangster films like \"Little Caesar\" and Wellman's \"The Public Enemy\" (both 1931) became popular. Dialogue now took precedence over \"slapstick\" in Hollywood comedies: the fast-paced, witty banter of \"The Front Page\" (1931) or \"It Happened One Night\" (1934), the sexual double entrendres of Mae West (\"She Done Him Wrong\", 1933) or the often subversively anarchic nonsense talk of the Marx Brothers (\"Duck Soup\", 1933). Walt Disney, who had previously been in the short cartoon business, stepped into feature films with the first English-speaking animated feature \"Snow White and the Seven Dwarfs\"; released by RKO Pictures in 1937. 1939, a major year for American cinema, brought such films as \"The Wizard of Oz\" and \"Gone with The Wind\".\n\nPreviously, it was believed that color films were first projected in 1909 at the Palace Theatre in London (the main problem with the color being that the technique, created by George Smith, (Kinemacolor) only used two colors: green and red, which were mixed additively). But in fact, it was in 1901 when the first color film in history was created. This untitled film was directed by photographer Edward Raymond Turner and his patron Frederick Marshall Lee. The way they did it was to use black and white film rolls, but have green, red, and blue filters go over the camera individually as it shot. To complete the film, they joined the original footage and filters on a special projector. However, both the shooting of the film and its projection suffered from major unrelated issues that, eventually, sank the idea.\n\nSubsequently, in 1916, the technicolor technique arrived (trichromatic procedure (green, red, blue). Its use required a triple photographic impression, incorporation of chromatic filters and cameras of enormous dimensions). The first audiovisual piece that was completely realized with this technique was the short of Walt Disney \"Flowers and Trees\", directed by Burt Gillett in 1932. Even so, the first film to be performed with this technique will be \"The Vanities Fair\" (1935) by Rouben Mamoulian. Later on, the technicolor was extended mainly in the musical field as \"The Wizard of Oz\" or \"Singing in the rain\", in films such as \"The Adventures of Robin Hood\" or the animation film, \"Snow White and the Seven Dwarfs\".\n\nThe desire for wartime propaganda against the opposition created a renaissance in the film industry in Britain, with realistic war dramas like \"49th Parallel\" (1941), \"Went the Day Well?\" (1942), \"The Way Ahead\" (1944) and Noël Coward and David Lean's celebrated naval film \"In Which We Serve\" in 1942, which won a special Academy Award. These existed alongside more flamboyant films like Michael Powell and Emeric Pressburger's \"The Life and Death of Colonel Blimp\" (1943), \"A Canterbury Tale\" (1944) and \"A Matter of Life and Death\" (1946), as well as Laurence Olivier's 1944 film \"Henry V\", based on the Shakespearean history \"Henry V\". The success of \"Snow White and the Seven Dwarfs\" allowed Disney to make more animated features like \"Pinocchio\" (1940), \"Fantasia\" (1940), \"Dumbo\" (1941) and \"Bambi\" (1942).\n\nThe onset of US involvement in World War II also brought a proliferation of films as both patriotism and propaganda. American propaganda films included \"Desperate Journey\" (1942), \"Mrs. Miniver\" (1942), \"Forever and a Day\" (1943) and \"Objective, Burma!\" (1945). Notable American films from the war years include the anti-Nazi \"Watch on the Rhine\" (1943), scripted by Dashiell Hammett; \"Shadow of a Doubt\" (1943), Hitchcock's direction of a script by Thornton Wilder; the George M. Cohan biopic, \"Yankee Doodle Dandy\" (1942), starring James Cagney, and the immensely popular \"Casablanca\", with Humphrey Bogart. Bogart would star in 36 films between 1934 and 1942 including John Huston's \"The Maltese Falcon\" (1941), one of the first films now considered a classic film noir. In 1941, RKO Pictures released \"Citizen Kane\" made by Orson Welles. It is often considered the greatest film of all time. It would set the stage for the modern motion picture, as it revolutionized film story telling.\n\nThe strictures of wartime also brought an interest in more fantastical subjects. These included Britain's Gainsborough melodramas (including \"The Man in Grey\" and \"The Wicked Lady\"), and films like \"Here Comes Mr. Jordan\", \"Heaven Can Wait\", \"I Married a Witch\" and \"Blithe Spirit\". Val Lewton also produced a series of atmospheric and influential small-budget horror films, some of the more famous examples being \"Cat People\", \"Isle of the Dead\" and \"The Body Snatcher\". The decade probably also saw the so-called \"women's pictures\", such as \"Now, Voyager\", \"Random Harvest\" and \"Mildred Pierce\" at the peak of their popularity.\n\n1946 saw RKO Radio releasing \"It's a Wonderful Life\" directed by Frank Capra. Soldiers returning from the war would provide the inspiration for films like \"The Best Years of Our Lives\", and many of those in the film industry had served in some capacity during the war. Samuel Fuller's experiences in World War II would influence his largely autobiographical films of later decades such as \"The Big Red One\". The Actor's Studio was founded in October 1947 by Elia Kazan, Robert Lewis, and Cheryl Crawford, and the same year Oskar Fischinger filmed \"Motion Painting No. 1\".\n\nIn 1943, \"Ossessione\" was screened in Italy, marking the beginning of Italian neorealism. Major films of this type during the 1940s included \"Bicycle Thieves\", \"Rome, Open City\", and \"La Terra Trema\". In 1952 \"Umberto D\" was released, usually considered the last film of this type.\n\nIn the late 1940s, in Britain, Ealing Studios embarked on their series of celebrated comedies, including \"Whisky Galore!\", \"Passport to Pimlico\", \"Kind Hearts and Coronets\" and \"The Man in the White Suit\", and Carol Reed directed his influential thrillers \"Odd Man Out\", \"The Fallen Idol\" and \"The Third Man\". David Lean was also rapidly becoming a force in world cinema with \"Brief Encounter\" and his Dickens adaptations \"Great Expectations\" and \"Oliver Twist\", and Michael Powell and Emeric Pressburger would experience the best of their creative partnership with films like \"Black Narcissus\" and \"The Red Shoes\".\n\nThe House Un-American Activities Committee investigated Hollywood in the early 1950s. Protested by the Hollywood Ten before the committee, the hearings resulted in the blacklisting of many actors, writers and directors, including Chayefsky, Charlie Chaplin, and Dalton Trumbo, and many of these fled to Europe, especially the United Kingdom.\n\nThe Cold War era zeitgeist translated into a type of near-paranoia manifested in themes such as invading armies of evil aliens, (\"Invasion of the Body Snatchers\", \"The War of the Worlds\"); and communist fifth columnists, (\"The Manchurian Candidate\").\n\nDuring the immediate post-war years the cinematic industry was also threatened by television, and the increasing popularity of the medium meant that some film theatres would bankrupt and close. The demise of the \"studio system\" spurred the self-commentary of films like \"Sunset Boulevard\" (1950) and \"The Bad and the Beautiful\" (1952).\n\nIn 1950, the Lettrists avante-gardists caused riots at the Cannes Film Festival, when Isidore Isou's \"Treatise on Slime and Eternity\" was screened. After their criticism of Charlie Chaplin and split with the movement, the Ultra-Lettrists continued to cause disruptions when they showed their new hypergraphical techniques.\nThe most notorious film is Guy Debord's \"Howls for Sade\" of 1952.\n\nDistressed by the increasing number of closed theatres, studios and companies would find new and innovative ways to bring audiences back. These included attempts to widen their appeal with new screen formats. Cinemascope, which would remain a 20th Century Fox distinction until 1967, was announced with 1953's \"The Robe\". VistaVision, Cinerama, and Todd-AO boasted a \"bigger is better\" approach to marketing films to a dwindling US audience. This resulted in the revival of epic films to take advantage of the new big screen formats. Some of the most successful examples of these Biblical and historical spectaculars include \"The Ten Commandments\" (1956), \"The Vikings\" (1958), \"Ben-Hur\" (1959), \"Spartacus\" (1960) and \"El Cid\" (1961). Also during this period a number of other significant films were produced in Todd-AO, developed by Mike Todd shortly before his death, including Oklahoma! (1955), Around the World in 80 Days (1956), South Pacific (1958) and Cleopatra (1963) plus many more.\n\nGimmicks also proliferated to lure in audiences. The fad for 3-D film would last for only two years, 1952–1954, and helped sell \"House of Wax\" and \"Creature from the Black Lagoon\". Producer William Castle would tout films featuring \"Emergo\" \"Percepto\", the first of a series of gimmicks that would remain popular marketing tools for Castle and others throughout the 1960s.\n\nIn the U.S., a post-WW2 tendency toward questioning the establishment and societal norms and the early activism of the civil rights movement was reflected in Hollywood films such as \"Blackboard Jungle\" (1955), \"On the Waterfront\" (1954), Paddy Chayefsky's \"Marty\" and Reginald Rose's \"12 Angry Men\" (1957). Disney continued making animated films, notably; \"Cinderella\" (1950), \"Peter Pan\" (1953), \"Lady and the Tramp\" (1955), and \"Sleeping Beauty\" (1959). He began, however, getting more involved in live action films, producing classics like \"20,000 Leagues Under the Sea\" (1954), and \"Old Yeller\" (1957). Television began competing seriously with films projected in theatres, but surprisingly it promoted more filmgoing rather than curtailing it.\n\n\"Limelight\" is probably a unique film in at least one interesting respect. Its two leads, Charlie Chaplin and Claire Bloom, were in the industry in no less than three different centuries. In the 19th Century, Chaplin made his theatrical debut at the age of eight, in 1897, in a clog dancing troupe, The Eight Lancaster Lads. In the 21st Century, Bloom is still enjoying a full and productive career, having appeared in dozens of films and television series produced up to and including 2013. She received particular acclaim for her role in \"The King's Speech\" (2010).\n\nFollowing the end of World War II in the 1940s, the following decade, the 1950s, marked a 'golden age' for non-English world cinema, especially for Asian cinema. Many of the most critically acclaimed Asian films of all time were produced during this decade, including Yasujirō Ozu's \"Tokyo Story\" (1953), Satyajit Ray's \"The Apu Trilogy\" (1955–1959) and \"Jalsaghar\" (1958), Kenji Mizoguchi's \"Ugetsu\" (1954) and \"Sansho the Bailiff\" (1954), Raj Kapoor's \"Awaara\" (1951), Mikio Naruse's \"Floating Clouds\" (1955), Guru Dutt's \"Pyaasa\" (1957) and \"Kaagaz Ke Phool\" (1959), and the Akira Kurosawa films \"Rashomon\" (1950), \"Ikiru\" (1952), \"Seven Samurai\" (1954) and \"Throne of Blood\" (1957).\n\nDuring Japanese cinema's 'Golden Age' of the 1950s, successful films included \"Rashomon\" (1950), \"Seven Samurai\" (1954) and \"The Hidden Fortress\" (1958) by Akira Kurosawa, as well as Yasujirō Ozu's \"Tokyo Story\" (1953) and Ishirō Honda's \"Godzilla\" (1954). These films have had a profound influence on world cinema. In particular, Kurosawa's \"Seven Samurai\" has been remade several times as Western films, such as \"The Magnificent Seven\" (1960) and \"Battle Beyond the Stars\" (1980), and has also inspired several Bollywood films, such as \"Sholay\" (1975) and \"China Gate\" (1998). \"Rashomon\" was also remade as \"The Outrage\" (1964), and inspired films with \"Rashomon effect\" storytelling methods, such as \"Andha Naal\" (1954), \"The Usual Suspects\" (1995) and \"Hero\" (2002). \"The Hidden Fortress\" was also the inspiration behind George Lucas' \"Star Wars\" (1977). Other famous Japanese filmmakers from this period include Kenji Mizoguchi, Mikio Naruse, Hiroshi Inagaki and Nagisa Oshima. Japanese cinema later became one of the main inspirations behind the New Hollywood movement of the 1960s to 1980s.\n\nDuring Indian cinema's 'Golden Age' of the 1950s, it was producing 200 films annually, while Indian independent films gained greater recognition through international film festivals. One of the most famous was \"The Apu Trilogy\" (1955–1959) from critically acclaimed Bengali film director Satyajit Ray, whose films had a profound influence on world cinema, with directors such as Akira Kurosawa, Martin Scorsese, James Ivory, Abbas Kiarostami, Elia Kazan, François Truffaut, Steven Spielberg, Carlos Saura, Jean-Luc Godard, Isao Takahata, Gregory Nava, Ira Sachs, Wes Anderson and Danny Boyle being influenced by his cinematic style. According to Michael Sragow of \"The Atlantic Monthly\", the \"youthful coming-of-age dramas that have flooded art houses since the mid-fifties owe a tremendous debt to the Apu trilogy\". Subrata Mitra's cinematographic technique of bounce lighting also originates from \"The Apu Trilogy\". Other famous Indian filmmakers from this period include Guru Dutt, Ritwik Ghatak, Mrinal Sen, Raj Kapoor, Bimal Roy, K. Asif and Mehboob Khan.\n\nThe cinema of South Korea also experienced a 'Golden Age' in the 1950s, beginning with director Lee Kyu-hwan's tremendously successful remake of \"Chunhyang-jon\" (1955). That year also saw the release of \"Yangsan Province\" by the renowned director, Kim Ki-young, marking the beginning of his productive career. Both the quality and quantity of filmmaking had increased rapidly by the end of the 1950s. South Korean films, such as Lee Byeong-il's 1956 comedy \"Sijibganeun nal (The Wedding Day)\", had begun winning international awards. In contrast to the beginning of the 1950s, when only 5 films were made per year, 111 films were produced in South Korea in 1959.\n\nThe 1950s was also a 'Golden Age' for Philippine cinema, with the emergence of more artistic and mature films, and significant improvement in cinematic techniques among filmmakers. The studio system produced frenetic activity in the local film industry as many films were made annually and several local talents started to earn recognition abroad. The premiere Philippine directors of the era included Gerardo de Leon, Gregorio Fernández, Eddie Romero, Lamberto Avellana, and Cirio Santiago.\n\nDuring the 1960s, the studio system in Hollywood declined, because many films were now being made on location in other countries, or using studio facilities abroad, such as Pinewood in the UK and Cinecittà in Rome. \"Hollywood\" films were still largely aimed at family audiences, and it was often the more old-fashioned films that produced the studios' biggest successes. Productions like \"Mary Poppins\" (1964), \"My Fair Lady\" (1964) and \"The Sound of Music\" (1965) were among the biggest money-makers of the decade. The growth in independent producers and production companies, and the increase in the power of individual actors also contributed to the decline of traditional Hollywood studio production.\n\nThere was also an increasing awareness of foreign language cinema in America during this period. During the late 1950s and 1960s, the French New Wave directors such as François Truffaut and Jean-Luc Godard produced films such as \"Les quatre cents coups\", \"Breathless\" and \"Jules et Jim\" which broke the rules of Hollywood cinema's narrative structure. As well, audiences were becoming aware of Italian films like Federico Fellini's \"La Dolce Vita\" and the stark dramas of Sweden's Ingmar Bergman.\n\nIn Britain, the \"Free Cinema\" of Lindsay Anderson, Tony Richardson and others lead to a group of realistic and innovative dramas including \"Saturday Night and Sunday Morning\", \"A Kind of Loving\" and \"This Sporting Life\". Other British films such as \"Repulsion\", \"Darling\", \"Alfie\", \"Blowup\" and \"Georgy Girl\" (all in 1965–1966) helped to reduce prohibitions sex and nudity on screen, while the casual sex and violence of the James Bond films, beginning with \"Dr. No\" in 1962 would render the series popular worldwide.\n\nDuring the 1960s, Ousmane Sembène produced several French- and Wolof-language films and became the \"father\" of African Cinema. In Latin America, the dominance of the \"Hollywood\" model was challenged by many film makers. Fernando Solanas and Octavio Getino called for a politically engaged Third Cinema in contrast to Hollywood and the European auteur cinema.\n\nFurther, the nuclear paranoia of the age, and the threat of an apocalyptic nuclear exchange (like the 1962 close-call with the USSR during the Cuban missile crisis) prompted a reaction within the film community as well. Films like Stanley Kubrick's \"Dr. Strangelove\" and \"Fail Safe\" with Henry Fonda were produced in a Hollywood that was once known for its overt patriotism and wartime propaganda.\n\nIn documentary film the sixties saw the blossoming of Direct Cinema, an observational style of film making as well as the advent of more overtly partisan films like \"In the Year of the Pig\" about the Vietnam War by Emile de Antonio. By the late 1960s however, Hollywood filmmakers were beginning to create more innovative and groundbreaking films that reflected the social revolution taken over much of the western world such as \"Bonnie and Clyde\" (1967), \"The Graduate\" (1967), \"\" (1968), \"Rosemary's Baby\" (1968), \"Midnight Cowboy\" (1969), \"Easy Rider\" (1969) and \"The Wild Bunch\" (1969). \"Bonnie and Clyde\" is often considered the beginning of the so-called New Hollywood.\n\nIn Japanese cinema, Academy Award-winning director Akira Kurosawa produced \"Yojimbo\" (1961), which like his previous films also had a profound influence around the world. The influence of this film is most apparent in Sergio Leone's \"A Fistful of Dollars\" (1964) and Walter Hill's \"Last Man Standing\" (1996). \"Yojimbo\" was also the origin of the \"Man with No Name\" trend.\n\nThe New Hollywood was the period following the decline of the studio system during the 1950s and 1960s and the end of the production code, (which was replaced in 1968 by the MPAA film rating system). During the 1970s, filmmakers increasingly depicted explicit sexual content and showed gunfight and battle scenes that included graphic images of bloody deaths a good example of this is Wes Craven's \"The Last House on the Left\" (1972).\n\nPost-classical cinema is the changing methods of storytelling of the New Hollywood producers. The new methods of drama and characterization played upon audience expectations acquired during the classical/Golden Age period: story chronology may be scrambled, storylines may feature unsettling \"twist endings\", main characters may behave in a morally ambiguous fashion, and the lines between the antagonist and protagonist may be blurred. The beginnings of post-classical storytelling may be seen in 1940s and 1950s film noir films, in films such as \"Rebel Without a Cause\" (1955), and in Hitchcock's \"Psycho\". 1971 marked the release of controversial films like \"Straw Dogs\", \"A Clockwork Orange\", \"The French Connection\" and \"Dirty Harry\". This sparked heated controversy over the perceived escalation of violence in cinema.\n\nDuring the 1970s, a new group of American filmmakers emerged, such as Martin Scorsese, Francis Ford Coppola, George Lucas, Woody Allen, Terrence Malick, and Robert Altman. This coincided with the increasing popularity of the auteur theory in film literature and the media, which posited that a film director's films express their personal vision and creative insights. The development of the auteur style of filmmaking helped to give these directors far greater control over their projects than would have been possible in earlier eras. This led to some great critical and commercial successes, like Scorsese's \"Taxi Driver\", Coppola's \"The Godfather\" films, William Friedkin's \"The Exorcist\", Altman's \"Nashville\", Allen's \"Annie Hall\" and \"Manhattan\", Malick's \"Badlands\" and \"Days of Heaven\", and Polish immigrant Roman Polanski's \"Chinatown\". It also, however, resulted in some failures, including Peter Bogdanovich's \"At Long Last Love\" and Michael Cimino's hugely expensive Western epic \"Heaven's Gate\", which helped to bring about the demise of its backer, United Artists.\n\nThe financial disaster of \"Heaven's Gate\" marked the end of the visionary \"auteur\" directors of the \"New Hollywood\", who had unrestrained creative and financial freedom to develop films. The phenomenal success in the 1970s of Spielberg's \"Jaws\" originated the concept of the modern \"blockbuster\". However, the enormous success of George Lucas' 1977 film \"Star Wars\" led to much more than just the popularization of blockbuster film-making. The film's revolutionary use of special effects, sound editing and music had led it to become widely regarded as one of the single most important films in the medium's history, as well as the most influential film of the 1970s. Hollywood studios increasingly focused on producing a smaller number of very large budget films with massive marketing and promotional campaigns. This trend had already been foreshadowed by the commercial success of disaster films such as \"The Poseidon Adventure\" and \"The Towering Inferno\".\n\nDuring the mid-1970s, more pornographic theatres, euphemistically called \"adult cinemas\", were established, and the legal production of hardcore pornographic films began. Porn films such as \"Deep Throat\" and its star Linda Lovelace became something of a popular culture phenomenon and resulted in a spate of similar sex films. The porn cinemas finally died out during the 1980s, when the popularization of the home VCR and pornography videotapes allowed audiences to watch sex films at home. In the early 1970s, English-language audiences became more aware of the new West German cinema, with Werner Herzog, Rainer Werner Fassbinder and Wim Wenders among its leading exponents.\n\nIn world cinema, the 1970s saw a dramatic increase in the popularity of martial arts films, largely due to its reinvention by Bruce Lee, who departed from the artistic style of traditional Chinese martial arts films and added a much greater sense of realism to them with his Jeet Kune Do style. This began with \"The Big Boss\" (1971), which was a major success across Asia. However, he didn't gain fame in the Western world until shortly after his death in 1973, when \"Enter the Dragon\" was released. The film went on to become the most successful martial arts film in cinematic history, popularized the martial arts film genre across the world, and cemented Bruce Lee's status as a cultural icon. Hong Kong action cinema, however, was in decline due to a wave of \"Bruceploitation\" films. This trend eventually came to an end in 1978 with the martial arts comedy films, \"Snake in the Eagle's Shadow\" and \"Drunken Master\", directed by Yuen Woo-ping and starring Jackie Chan, laying the foundations for the rise of Hong Kong action cinema in the 1980s.\n\nWhile the musical film genre had declined in Hollywood by this time, musical films were quickly gaining popularity in the cinema of India, where the term \"Bollywood\" was coined for the growing Hindi film industry in Bombay (now Mumbai) that ended up dominating South Asian cinema, overtaking the more critically acclaimed Bengali film industry in popularity. Hindi filmmakers combined the Hollywood musical formula with the conventions of ancient Indian theatre to create a new film genre called \"Masala\", which dominated Indian cinema throughout the late 20th century. These \"Masala\" films portrayed action, comedy, drama, romance and melodrama all at once, with \"filmi\" song and dance routines thrown in. This trend began with films directed by Manmohan Desai and starring Amitabh Bachchan, who remains one of the most popular film stars in South Asia. The most popular Indian film of all time was \"Sholay\" (1975), a \"Masala\" film inspired by a real-life dacoit as well as Kurosawa's \"Seven Samurai\" and the Spaghetti Westerns.\n\nThe end of the decade saw the first major international marketing of Australian cinema, as Peter Weir's films \"Picnic at Hanging Rock\" and \"The Last Wave\" and Fred Schepisi's \"The Chant of Jimmie Blacksmith\" gained critical acclaim. In 1979, Australian filmmaker George Miller also garnered international attention for his violent, low-budget action film \"Mad Max\".\n\nDuring the 1980s, audiences began increasingly watching films on their home VCRs. In the early part of that decade, the film studios tried legal action to ban home ownership of VCRs as a violation of copyright, which proved unsuccessful. Eventually, the sale and rental of films on home video became a significant \"second venue\" for exhibition of films, and an additional source of revenue for the film industries.\n\nThe Lucas–Spielberg combine would dominate \"Hollywood\" cinema for much of the 1980s, and lead to much imitation. Two follow-ups to \"Star Wars\", three to \"Jaws\", and three \"Indiana Jones\" films helped to make sequels of successful films more of an expectation than ever before. Lucas also launched THX Ltd, a division of Lucasfilm in 1982, while Spielberg enjoyed one of the decade's greatest successes in \"E.T. the Extra-Terrestrial\" the same year. 1982 also saw the release of Disney's \"Tron\" which was one of the first films from a major studio to use computer graphics extensively. American independent cinema struggled more during the decade, although Martin Scorsese's \"Raging Bull\" (1980), \"After Hours\" (1985), and \"The King of Comedy\" (1983) helped to establish him as one of the most critically acclaimed American film makers of the era. Also during 1983 \"Scarface\" was released, which was very profitable and resulted in even greater fame for its leading actor Al Pacino. Probably the most successful film commercially was Tim Burton's 1989 version of Bob Kane's creation, \"Batman\", which broke box-office records. Jack Nicholson's portrayal of the demented Joker earned him a total of $60,000,000 after figuring in his percentage of the gross.\n\nBritish cinema was given a boost during the early 1980s by the arrival of David Puttnam's company Goldcrest Films. The films \"Chariots of Fire\", \"Gandhi\", \"The Killing Fields\" and \"A Room with a View\" appealed to a \"middlebrow\" audience which was increasingly being ignored by the major Hollywood studios. While the films of the 1970s had helped to define modern blockbuster motion pictures, the way \"Hollywood\" released its films would now change. Films, for the most part, would premiere in a wider number of theatres, although, to this day, some films still premiere using the route of the limited/roadshow release system. Against some expectations, the rise of the multiplex cinema did not allow less mainstream films to be shown, but simply allowed the major blockbusters to be given an even greater number of screenings. However, films that had been overlooked in cinemas were increasingly being given a second chance on home video.\nDuring the 1980s, Japanese cinema experienced a revival, largely due to the success of anime films. At the beginning of the 1980s, \"Space Battleship Yamato\" (1973) and \"Mobile Suit Gundam\" (1979), both of which were unsuccessful as television series, were remade as films and became hugely successful in Japan. In particular, \"Mobile Suit Gundam\" sparked the Gundam franchise of Real Robot mecha anime. The success of \"\" also sparked a Macross franchise of mecha anime. This was also the decade when Studio Ghibli was founded. The studio produced Hayao Miyazaki's first fantasy films, \"Nausicaä of the Valley of the Wind\" (1984) and \"Castle in the Sky\" (1986), as well as Isao Takahata's \"Grave of the Fireflies\" (1988), all of which were very successful in Japan and received worldwide critical acclaim. Original video animation (OVA) films also began during this decade; the most influential of these early OVA films was Noboru Ishiguro's cyberpunk film \"Megazone 23\" (1985). The most famous anime film of this decade was Katsuhiro Otomo's cyberpunk film \"Akira\" (1988), which although initially unsuccessful at Japanese theaters, went on to become an international success.\n\nHong Kong action cinema, which was in a state of decline due to endless Bruceploitation films after the death of Bruce Lee, also experienced a revival in the 1980s, largely due to the reinvention of the action film genre by Jackie Chan. He had previously combined the comedy film and martial arts film genres successfully in the 1978 films \"Snake in the Eagle's Shadow\" and \"Drunken Master\". The next step he took was in combining this comedy martial arts genre with a new emphasis on elaborate and highly dangerous stunts, reminiscent of the silent film era. The first film in this new style of action cinema was \"Project A\" (1983), which saw the formation of the Jackie Chan Stunt Team as well as the \"Three Brothers\" (Chan, Sammo Hung and Yuen Biao). The film added elaborate, dangerous stunts to the fights and slapstick humor, and became a huge success throughout the Far East. As a result, Chan continued this trend with martial arts action films containing even more elaborate and dangerous stunts, including \"Wheels on Meals\" (1984), \"Police Story\" (1985), \"Armour of God\" (1986), \"Project A Part II\" (1987), \"Police Story 2\" (1988), and \"Dragons Forever\" (1988). Other new trends which began in the 1980s were the \"girls with guns\" subgenre, for which Michelle Yeoh gained fame; and especially the \"heroic bloodshed\" genre, revolving around Triads, largely pioneered by John Woo and for which Chow Yun-fat became famous. These Hong Kong action trends were later adopted by many Hollywood action films in the 1990s and 2000s.\n\nThe early 1990s saw the development of a commercially successful independent cinema in the United States. Although cinema was increasingly dominated by special-effects films such as \"\" (1991), \"Jurassic Park\" (1993) and \"Titanic\" (1997), the latter of which became the highest-grossing film of all time at the time up until \"Avatar\" (2009), also directed by James Cameron, independent films like Steven Soderbergh's \"Sex, Lies, and Videotape\" (1989) and Quentin Tarantino's \"Reservoir Dogs\" (1992) had significant commercial success both at the cinema and on home video. Filmmakers associated with the Danish film movement Dogme 95 introduced a manifesto aimed to purify filmmaking. Its first few films gained worldwide critical acclaim, after which the movement slowly faded out.\nMajor American studios began to create their own \"independent\" production companies to finance and produce non-mainstream fare. One of the most successful independents of the 1990s, Miramax Films, was bought by Disney the year before the release of Tarantino's runaway hit \"Pulp Fiction\" in 1994. The same year marked the beginning of film and video distribution online. Animated films aimed at family audiences also regained their popularity, with Disney's \"Beauty and the Beast\" (1991), \"Aladdin\" (1992), and \"The Lion King\" (1994). During 1995, the first feature length computer-animated feature, \"Toy Story\", was produced by Pixar Animation Studios and released by Disney. After the success of Toy Story, computer animation would grow to become the dominant technique for feature length animation, which would allow competing film companies such as DreamWorks Animation and 20th Century Fox to effectively compete with Disney with successful films of their own. During the late 1990s, another cinematic transition began, from physical film stock to digital cinema technology. Meanwhile, DVDs became the new standard for consumer video, replacing VHS tapes.\n\nThe documentary film also rose as a commercial genre for perhaps the first time, with the success of films such as \"March of the Penguins\" and Michael Moore's \"Bowling for Columbine\" and \"Fahrenheit 9/11\". A new genre was created with Martin Kunert and Eric Manes' \"Voices of Iraq\", when 150 inexpensive DV cameras were distributed across Iraq, transforming ordinary people into collaborative filmmakers. The success of \"Gladiator\" led to a revival of interest in epic cinema, and \"Moulin Rouge!\" renewed interest in musical cinema. Home theatre systems became increasingly sophisticated, as did some of the special edition DVDs designed to be shown on them. \"The Lord of the Rings trilogy\" was released on DVD in both the theatrical version and in a special extended version intended only for home cinema audiences.\n\nIn 2001, the \"Harry Potter\" film series began, and by its end in 2011, it had become the highest-grossing film franchise of all time until the Marvel Cinematic Universe passed it in 2015.\n\nMore films were also being released simultaneously to IMAX cinema, the first was in 2002's Disney animation \"Treasure Planet\"; and the first live action was in 2003's \"The Matrix Revolutions\" and a re-release of \"The Matrix Reloaded\". Later in the decade, \"The Dark Knight\" was the first major feature film to have been at least partially shot in IMAX technology.\n\nThere has been an increasing globalization of cinema during this decade, with foreign-language films gaining popularity in English-speaking markets. Examples of such films include \"Crouching Tiger, Hidden Dragon\" (Mandarin), \"Amélie\" (French), \"Lagaan\" (Hindi), \"Spirited Away\" (Japanese), \"City of God\" (Brazilian Portuguese), \"The Passion of the Christ\" (Aramaic), \"Apocalypto\" (Mayan) and \"Inglourious Basterds\" (multiple European languages). Italy is the most awarded country at the Academy Award for Best Foreign Language Film, with 14 awards won, 3 Special Awards and 31 nominations.\n\nIn 2003 there was a revival in 3D film popularity the first being James Cameron's \"Ghosts of the Abyss\" which was released as the first full-length 3-D IMAX feature filmed with the Reality Camera System. This camera system used the latest HD video cameras, not film, and was built for Cameron by Emmy nominated Director of Photography Vince Pace, to his specifications. The same camera system was used to film \"\" (2003), \"Aliens of the Deep\" IMAX (2005), and \"The Adventures of Sharkboy and Lavagirl in 3-D\" (2005).\n\nAfter James Cameron's 3D film \"Avatar\" became the highest-grossing film of all time, 3D films gained brief popularity with many other films being released in 3D, with the best critical and financial successes being in the field of feature film animation such as Universal Pictures/Illumination Entertainment's \"Despicable Me\" and DreamWorks Animation's \"How To Train Your Dragon\", \"Shrek Forever After\" and \"Megamind\". \"Avatar\" is also note-worthy for pioneering highly sophisticated use of motion capture technology and influencing several other films such as \"Rise of the Planet of the Apes\". However the popularity of 3D steadily declined over the subsequent seven years to 2017.\n\n, the largest film industries by number of feature films produced were those of India, the United States, China, Nigeria and Japan. Beginning in 2008 with \"Iron Man\" and \"The Dark Knight\", superhero films have greatly increased in popularity and financial success, with films based on Marvel and DC comics regularly being released every year up to the present.\n\n\n\\ Jones. Based on the book (above); written by Basten & Jones. Documentary, (1998).\n\n"}
{"id": "47690389", "url": "https://en.wikipedia.org/wiki?curid=47690389", "title": "Hook", "text": "Hook\n\nA hook is a tool consisting of a length of material, typically metal, that contains a portion that is curved or indented, such that it can be used to grab onto, connect, or otherwise attach itself onto another object. In a number of uses, one end of the hook is pointed, so that this end can pierce another material, which is then held by the curved or indented portion. \n"}
{"id": "34076003", "url": "https://en.wikipedia.org/wiki?curid=34076003", "title": "Hybrid operating room", "text": "Hybrid operating room\n\nA hybrid operating room is a surgical theatre that is equipped with advanced medical imaging devices such as fixed C-Arms, CT scanners or MRI scanners. These imaging devices enable minimally-invasive surgery. Minimally-invasive surgery is intended to be less traumatic for the patient and minimize incisions on the patient and perform surgery procedure through one or several small cuts.\n\nThough imaging has been a standard part of the OR for a long time in the form of mobile C-Arms, ultrasound and endoscopy, these new minimally-invasive procedures require imaging techniques that can visualize smaller body parts such as thin vessels in the heart muscle and can be facilitated through intraoperative 3D imaging.\n\nHybrid operating rooms are currently used mainly in cardiac, vascular and neurosurgery, but could be suitable for a number of other surgical disciplines.\n\nThe repair of diseased heart valves and the surgical treatment of rhythm disturbances and aortic aneurysms can benefit from the imaging capabilities of a hybrid OR. Hybrid Cardiac Surgery is a widespread treatment for these diseases.\n\nAlso, the shift towards endovascular treatment of abdominal aortic aneurysms pushed the spread of angiographic systems in vascular OR environments. Particularly for complex endografts, a hybrid operating theater should be a basic requirement. Also, it is well-suited for emergency treatment.\n\nSome surgeons do not only verify the placement of complex endografts intraoperatively, they also use their angiography system and the applications it offers for planning the procedure. As anatomy changes between a preoperative CT and intraoperative fluoroscopy because of patient positioning and the insertion of stiff material, a much more precise planning is possible if the surgeon performs an intraoperative rotational angiography, takes an automatic segmentation of the aorta, places markers for the renal arteries and other landmarks in 3D and then overlays the contours on 2D fluoroscopy. This guidance is updated with any change in C-Arm angulation/position or table position.\n\nIn Neurosurgery, applications for hybrid ORs are for example spinal fusion and intracranial aneurysm coiling. In both cases, they have been rated promising to improve outcomes. For spinal fusion procedures, an integration with a navigation system can further improve the workflow. Intraoperative acquisition of a cone beam computed tomgraphy image can also be used to reconstruct three dimensional CT-like images. This may be useful for the applications above and also for confirmation of targeting for placement of ventricular catheters, biopsies, or deep brain stimulation electrodes. Intra-operative MRI is used to guide brain tumor surgery as well as placement of deep brain stimulation electrodes and interstitial laser thermal therapy.\n\nProcedures to diagnose and treat small pulmonary nodules have also recently been performed in hybrid operating rooms. Interventional image guidance thereby offers the advantage of precisely knowing the position of the nodules, particularly in small or ground-glass opaque tumors, metastases, and/or patients with reduced pulmonary function. This allows for a precise navigation in biopsies, and resection in VATS. Most importantly, using interventional imaging in VATS can substitute for the loss of tactile sensing. This new approach also delivers the potential to spare healthy lung tissue by knowing the exact position of the nodule which increases the quality of life for the patient after the operation.\n\nThe process for diagnosis and treatment usually comprises 3 steps:\n\nA hybrid operating room supports steps 2 and 3 (if surgery is performed) of this workflow:\n\nSmall lung nodules identified on a thorax CT need to be examined for malignancy, thus a small portion of sample tissue is taken out in a needle procedure. The needle is advanced through the bronchial tree, or trans-thoracically, towards the position of the nodule. To make sure tissue is captured from the nodule as opposed to accidentally taking healthy lung tissue, imaging modalities such as mobile C-Arms, ultrasound, or bronchoscopes are used. The yield rate of biopsies in small nodules is reported to be between 33–50% in tumors smaller than 3 cm.\n\nTo increase the yield rate, advanced interventional imaging with angiographic C-arms has proven to be beneficial. The advantage of intra-procedural imaging is that the patient and the diaphragm are in exactly the same position during 2D/3D imaging and the actual biopsy. Hence the accuracy is usually much higher than using pre-operative data.\nRotational angiography visualizes the bronchial tree in 3D during the procedure. The air thereby serves as a ‘natural’ contrast agent, thus the nodules are well visible. On this 3D image, using dedicated software, the nodules can be marked, along with a planned needle path for the biopsy (endobronchially or trans-thoracically). These images can then be overlaid on live fluoroscopy. This gives the pulmonologist improved guidance towards the nodules. Yield rates of 90% in nodules of 1–2 cm, and 100% in nodules > 2 cm have been reported with this new approach.\n\nVATS (video-assisted thoracoscopic surgery) is a minimally-invasive technique to resect lung nodules that saves the patient the trauma of a thoracotomy. Thereby, small ports are used to access the pulmonary lobes and introduce a camera on a thoracoscope, along with the necessary instruments. While this procedure speeds up recovery and potentially reduces complications, the loss of natural vision and tactile sensing makes it difficult for the surgeon to locate the nodules, especially in cases of non-superficial, ground-glass opaque, and small lesions. The yield rate for nodules < 1 cm can be below 40% as studies show. As a consequence sometimes more healthy tissue is resected than actually necessary in order to avoid missing (parts of) the lesion. Using advanced intra-operative imaging in the OR helps to precisely locate and resect the lesion in a potentially tissue-sparing and quick fashion. In order to be able to use image guidance during VATS, rotational angiography has to be performed before the introduction of ports, thus before the lobe in question deflates. This way the lesion is visible through the natural contrast of air. In a second step, hook wires, thread needles, or contrast agent (Lipiodol, Iopamidol) are introduced into or next to the lesion to ensure visibility on the angiogram after lung deflation. Then, the conventional part of VATS starts with the introduction of thoracoscopes. The imaging system is used in fluoroscopic mode now, where both the inserted instruments and the previously marked lesion are well visible. A precise resection is now possible. In case contrast agent has been used to mark the lesion, it will also drain into the regional lymph nodes, which then can be resected within the same procedure.\n\nComplex fractures like pelvis fractures, calcaneus or tibia head fractures, etc. need an exact placement of screws and other surgical implants to allow quickest possible treatment of the patients. Minimally invasive surgical approaches result in less trauma for the patient and quicker recovery. However, the risk of malpositionins, revisions and nerval damage cannot be underestimated (Malposition and revision rates of different imaging modalities for percutaneous iliosacral screw fixation following pelvic fractures: a systematic review and meta-analysis). The possibility of the use of an angio system with a spatial resolution of 0.1 mm, the large field of view to image the entire pelvis in one image and the high kW rate allows the surgeon high precision images while not impairing hygiene (floor mounted systems) or access to the patient (CT). Degenerative spine surgery, traumatic spinal fractures, oncologic fractures or scoliosis surgery are other types of surgery that can be optimized in a hybrid OR. The large field of view and the high kW rate allow to optimally image even obese patients. Navigations systems or the use of integrated laser guidance can support and improve the workflow.\n\nAs in other minimally invasive surgery the first laparoscopic surgeons were smiled at and the surgical community did not believe in this new technology. Today it is the gold standard for most surgeries. Starting with a simple appendectomy, partial kidney resections and partial liver resections, etc. The laparoscopic approach is expanding. The image quality, the possibility of imaging the patient in the surgical position and the guidance of the instruments facilitate this approach.(Efficacy of DynaCT for surgical navigation during complex laparoscopic surgery: an initial experience. Partial resection of the kidney, leaving as much healthy tissue, meaning kidney function to the patient has been described some time ago (Nephron sparing surgery for renal tumors: indications, techniques and outcomes.). The challenges the surgeons face is the loss of natural 3D vision and tactile sensing. Through small ports he/she has to rely on the images provided by the endoscope and is unable to feel the tissue. In a hybrid operating room the anatomy can be updated and imaged in real time. 3D images can be fused and/or overlaid on live fluoroscopy or the endoscope. (Real-time image guidance in laparoscopic liver surgery: first clinical experience with a guidance system based on intraoperative CT imaging.) Crucial anatomy like vessels or a tumor can be avoided and complications reduced. Further investigations are under trial at the moment. (Surgical navigation in urology. European perspective)\n\nFor the treatment of trauma patients every minute counts. Patients with severe bleeding after car accidents, explosions, gunshot wounds or aortic dissections, etc. need immediate care due to the life-threatening blood loss. In a hybrid operating room both open and endovascular treatment of the patient can be performed. For example, the tension in the brain due to a severe haemorrhage can be relieved and the aneurysm can be coiled. The concept of placing the emergency patient on an operating table as soon as he/she enters the hospital, if stable perform a trauma scan in the CT or if unstable immediate procedure in the hybrid operating room without having to reposition the patient can save valuable time and reduce risk of further injury.\n\nFluoroscopy is performed with continuous X-ray to guide the progression of a catheter or other devices within the body in live images. To depict even fine anatomic structures and devices, brilliant image quality is required. In particular, in cardiac interventions, imaging the moving heart requires a high frame rate (30f/s, 50 Hz) and high power output (at least 80 kW). Image quality needed for cardiac applications can only be achieved by high powered fixed angiography systems, not with mobile C-Arms.\n\nAngiographic systems provide a so-called acquisition mode, which stores the acquired images automatically on the system to be uploaded into an image archive later. While standard fluoroscopy is predominantly used to guide devices and to re-position the field of view,\ndata acquisition is applied for reporting or diagnostic purposes. In particular, when contrast media is injected, a data acquisition is mandatory, because the stored sequences can be replayed as often as required without re-injection of contrast media. To achieve a sufficient image quality for diagnoses and reporting, the angiographic system uses up to 10 times higher x-ray doses than standard fluoroscopy. Thus, data acquisition should be applied only when truly necessary. Data acquisition serves as a base for advanced imaging techniques such as DSA and rotational angiography.\n\nRotational angiography is a technique to acquire CT-like 3D images intraoperatively with a fixed C-Arm. To do that, the C-Arm is rotated around the patient, acquiring a series of projections that will be reconstructed to a 3D data set.\n\nDigital subtraction angiography (DSA) is a two-dimensional imaging technique for the visualization of blood vessels in the human body (Katzen, 1995).\nFor DSA, the same sequence of a projection is acquired without and then with contrast agent injection through the vessels under investigation. The first image is subtracted from the second to remove background structures such as bones as completely as possible and show the contrast-filled vessels more clearly. As there is a time lag between the acquisition of the first and the second image, motion correction algorithms are necessary to remove movement artifacts.\nAn advanced application of DSA is road mapping. From the acquired DSA sequence, the image frame with maximum vessel opification is identified and assigned to be the so-called road-map mask. This mask is continuously subtracted from live fluoroscopy images to produce real-time subtracted fluoroscopic images overlaid on a static image of the vasculature. The clinical benefit is better visualization of small and complex vascular structures without distracting underlying tissue to support the placement of catheters and wires.\n\nModern angiographic systems are not just used for imaging, but support the surgeon also during the procedure by guiding the intervention based on 3D information acquired either pre-operatively or intra-operatively. Such guidance requires that the 3D information is registered to the patient. This is done using special proprietary software algorithms.\n\n3D images are calculated from a set of projections acquired during a rotation of the C-Arm around the patient. The volume reconstruction is performed on a separate workstation. The C-Arm and the workstation are connected a communicate continuously. For example, when the user virtually rotates the volume on the workstation to view the anatomy from a certain perspective, the parameter of this view can be transmitted to the angio system, which then drives the C-arm to exactly the same perspective for fluoroscopy. In the same way, if the C-arm angulation is changed, this angulation can be transmitted to the workstation which updates the volume to the same perspective as the fluoroscopic view. The software algorithm that stands behind this process is called registration and can also be done with other DICOM images, such as CT or MRT data acquired preoperatively.\n\nThe 3D image itself can be overlaid colour-coded on top of the fluoroscopic image. Any change of the angulations of the C-arm will cause the workstation to re-calculate in real-time the view on the 3D image to match exactly the view of the live 2D fluoroscopy image. Without additional contrast agent injection the surgeon can observe device movements simultaneously with the 3D overlay of the vessel contours in the fluoroscopy image.\nAn alternative way to add information from the workstation to the fluoroscopic image is to overlay, after either manual or automatic segmentation of the anatomical structures of interest in the 3D image, the outline as a contour onto the fluoroscopic image. This provides\nadditional information which is not visible in the fluoroscopic image. Some software available provides landmarks automatically, more can be added manually be the surgeon or a qualified technician. One example is the placement of a fenestrated stentgraft to treat an abdominal aortic aneurysm. The ostia of the renal arteries can be circled on the 3D image and then overlaid on the live fluorscopy. As the marking has been done in 3D, it will update with any change of the fluoroscopy angulation to match the current view.\n\nTrans-Aortic Valve Implantation requires exact positioning of the valve in the aortic root to prevent complications. A good fluoroscopic view is essential, whereby an exact perpendicular angle to the aortic root is considered to be optimal for the implantation. Recently, applications have been released which support the surgeon in selecting this optimal fluoroscopy angulation or even drive the C-arm automatically into the perpendicular view to the aortic root. Some approaches are based on pre-operative CT images, which are used to segment the aorta and calculate optimal viewing angles for valve implantations. CT images must be registered with C-arm CT or fluoroscopic images to transfer the 3D volume to the actual angiographic system. Errors during the registration process might result in diversification from the optimal angulations of the C-arm and must be manually corrected. Additionally, anatomical variations between the acquisition of the pre-operatively CT image and surgery are not accounted for. Patients are generally imaged with hands-up in a CT scanner while surgery is performed with arms aside the patient, which leads to substantial errors. Algorithms purely based on C-arm CT images acquired in the OR by the angiographic system are inherently registered to the patient and show the present anatomy structures. With such an approach, the surgeon does not rely on pre-operative CT images acquired by the radiological department which simplifies the workflow in the OR and reduces errors in the process.\n\nImprovements of the C-Arm technology nowadays also enable perfusion imaging and can visualize parenchymal blood volume in the OR. To do that, rotational angiography (3D-DSA) is combined with a modified injection protocol and a special reconstruction algorithm. The blood flow can then be visualized in the course of time. This can be useful in the treatments of patients suffering from ischemic stroke.\n\nA CT system mounted on rails can be moved into and out of an OR to support complex surgical procedures, such as brain, spine and trauma surgery with additional information through imaging. The Johns Hopkins Bayview Medical Center in Maryland describes that their intra-operative CT usage has a positive impact on patient outcomes by improving safety, decreasing infections and lowering the risks of complications.\n\nMagnetic resonance imaging is used in Neurosurgery:\n\n\nAn MRT system usually requires a lot of space both in the room and around the patient. It is not possible to perform surgery in a regular MRT room. Thus for step 2, there are two solutions of how to use an MR interoperatively, one is a moveable MRT scanner that can be brought in only when imaging is needed, the other is to transport the patient to an MR scanner in an adjacent room during surgery.\n\nNot only the usage of a hybrid operating room is \"hybrid\", but also its role within the hospital system. As it holds an imaging modality, the radiology department could take the lead responsibility for the room for expertise in handling, technical, maintenance, and connectivity reasons. From a patient workflow perspective, the room could be run by their surgical department and should rather be situated next to other surgical facilities, to ensure proper patient care and fast transportation.\n\nInstalling a hybrid OR is a challenge to standard hospital room sizes, as not only the imaging system requires some additional space, but there are also more people in the room as in a normal OR. A team of 8 to 20 people including anasthesiologists, surgeons, nurses, technicians, perfusionists, support staff from device companies etc. can work in such an OR. Depending on the imaging system chosen, a room size of 70 square meters including a control room but excluding a technical room and the preparation areas is recommended. Additional preparations of the room necessary are 2-3mm lead shielding and potentially enforcement of the floor or ceiling to hold the additional weight of the imaging system (approximately 650–1800 kg).\n\nPlanning a hybrid OR requires to involve a considerable number of stakeholders. To ensure a smooth workflow in the room, all parties working there need to state their requirements, which will impact the room design and determining various resources like space, medical, and imaging equipment.\nThis may require professional project management and several iterations in the planning process with the vendor of the imaging system, as technical interdependencies are complex. The result is always an individual solution tailored to the needs and preferences of the interdisciplinary team and the hospital.\n\nIn general, two different light sources are needed in an operating room: the surgical (operating) lights used for open procedures and the ambient lighting for interventional procedures. Particular attention should be paid to the possibility to dim the lights. This is frequently needed during fluoroscopy or endoscopy. For the surgical lights it is most important that they cover the complete area across the operating room table. Moreover, they must not interfere with head heights and collision paths of other equipment. The most frequent mounting position of OR-lights is centrally above the OR table. If a different position is chosen, the lights usually are swivelled in from an area outside the OR table. Because one central axis per light head is necessary, this may lead to at least two central axes and mounting points in order to ensure sufficient illumination of the surgical field. The movement range of the angiography system determines the positioning of the OR lights. Central axes must be outside of moving path and swivel range. This is especially important as devices have defined room height requirements that must be met. In this case, head clearance height for the OR-light may be an issue. This makes lights a critical item in the planning and design process. Other aspects in the planning process of OR lights include avoidance of glare and reflections. Modern OR lights may have additional features, like build in camera and video capabilities. For the illumination of the wound area, a double-arm OR-light system is required. Sometimes even a third light may be required, in cases where more than one surgical activity takes place at the same time, e.g. vein stripping of the legs.\nIn summary, the key topics for planning the surgical light system include:\n\nThe most common imaging modality to be used in hybrid ORs is a C-Arm. Expert consensus rates the performance of mobile C-arms in hybrid ORs as insufficient, because the limited power of the tube impacts image quality, the field of view is smaller for image-intensifier systems than for flat-panel detector systems and the cooling system of mobile C-Arms can lead to overheating after just a few hours, which can be too short for lengthy surgical procedures or for multiple procedures in a row, that would be needed to recover the investment in such a room.\n\nFixed C-Arms do not have these limitations, but require more space in the room. These systems can be mounted either on the floor, the ceiling, or both if a biplane system is chosen. The latter is the system of choice if pediatric cardiologists, electrophysiologists or neurointerventionalists are major users of the room. It is not recommended to implement a biplane system if not clearly required by these clinical disciplines, as ceiling-mounted components may raise hygienic issues: In fact, some hospitals do not allow operating parts directly above the surgical field, because dust may fall in the wound and cause infection. Since any ceiling-mounted system includes moving parts above the surgical field and impairs the laminar airflow, such systems are not the right option for hospitals enforcing highest hygienic standards. (see also and, both German only)\n\nThere are more factors to consider when deciding between ceiling- and floor-mounted systems. Ceiling-mounted systems require substantial ceiling space and, therefore, reduce the options to install surgical lights or booms. Nonetheless, many hospitals choose ceiling-mounted systems because they cover the whole body with more flexibility and – most importantly – without moving the table. The latter is sometimes a difficult and dangerous undertaking during surgery with the many lines and catheters that must also be moved. Moving from a parking to a working position during surgery, however, is easier with a floor-mounted system, because the C-arm just turns in from the side and does not interfere with the anesthesiologist. The ceiling-mounted system, by contrast, during surgery can hardly move to a parking position at the head end without colliding with anesthesia equipment. In an overcrowded environment like the OR, biplane systems add to the complexity and interfere with anesthesia, except for neurosurgery, where anesthesia is not at the head end. Monoplane systems are therefore clearly recommended for rooms mainly used for cardiac surgery.\n\nThe selection of the OR table depends on the primary use of the system. Interventional tables with floating table tops and tilt and cradle compete with fully integrated flexible OR tables. Identification of the right table is a compromise between interventional and surgical requirements. Surgical and interventional requirements may be mutually exclusive. Surgeons, especially orthopedic, general and neurosurgeons usually expect a table with a segmented tabletop for flexible patient positioning. For imaging purposes, a radiolucent tabletop, allowing full body coverage, is required. Therefore, non-breakable carbon fibre tabletops are used.\n\nInterventionalists require a floating tabletop to allow fast and precise movements during angiography. Cardiac and vascular surgeons, in general, have less complex positioning needs, but based on their interventional experience in angiography may be used to having fully motorized movements of the table and the tabletop. For positioning patients on non breakable tabletops, positioning aids are available, i.e. inflatable cushions. Truly floating tabletops are not available with conventional OR tables. As a compromise, floatable angiography tables specifically made for surgery with vertical and lateral tilt are recommended. To further accommodate typical surgical needs, side rails for mounting surgical equipment like retractors or limb holders should be available for the table.\n\nThe position of the table in the room also impacts surgical workflow. A diagonal position in the OR may be considered in order to gain space and flexibility in the room, as well as access to the patient from all sides. Alternatively, a conventional surgery table can be combined with an imaging system if the vendor offers a corresponding integration. The operating room can then be used either with a radiotranslucent but not breakable tabletop that supports 3D imaging, or with a universal breakable tabletop that provides enhanced patient positioning, but restricts 3D imaging. The latter are particularly suited for neuro- or orthopedic surgery, and these integrated solutions recently also became commercially available. If it is planned to share the room for hybrid and open conventional procedures, these are sometimes preferred. They provide greater workflow flexibility because the tabletops are dockable and can be easily exchanged, but require some compromises with interventional imaging.\n\nIn summary, important aspects to be included considered are the position in the room, radiolucency (carbon fiber tabletop), compatibility, and integration of imaging devices with the operating table. Further aspects include table load, adjustable table height, and horizontal mobility (floating) including vertical and lateral tilt. It is important to also have proper accessories available, such as rails for mounting special surgical equipment retractors, camera holder). Free floating angiography tables with tilt and cradle capabilities are best suited for cardiovascular hybrid operating rooms.\n\nX-ray radiation is ionizing radiation, thus exposure is potentially harmful. Compared to a mobile C-Arm, which is classically used in surgery, CT scanners and fixed C-Arms work on a much higher energy level, which induces higher dose. Therefore, it is very important to monitor radiation dose applied in a hybrid OR both for the patient and the medical staff.\n\nThere are a few simple measures to protect people in the OR from scatter radiation, thus lower their dose. Awareness is one critical issue, otherwise the available protection tools might be neglected. Among these tools is protective clothing in the form of a protective apron for the trunk, a protective thyroid shield around the neck and protective glasses. The later may be replaced by a ceiling-suspended lead glass panel. Additional lead curtains can be installed at the table side to protect the lower body region. Even more restrictive rules apply to pregnant staff members.\n\nA very effective measure of both protection to both the staff and the patient of course is applying less radiation. There is always a trade-off between radiation dose and image quality. A higher x-ray dose leads to a clearer picture. Modern software technology can improve image quality during post-processing, such that the same image quality is reached with a lower dose. Image quality thereby is described by contrast, noise, resolution and artifacts. In general, the ALARA principle (as low as reasonably achievable) should be followed. Dose should be as low as possible, but image quality can only be reduced to the level that the diagnostic benefit of the examination is still higher than the potential harm to the patient.\n\nThere are both technical measures taking by x-ray equipment manufacturers to reduce dose constantly and handling options for the staff to reduce dose depending on the clinical application. Among the former is beam hardening. Among the latter are frame rate settings, pulsed fluoroscopy and collimation.\n\nBeam Hardening: X-ray radiation consists of hard and soft particles, i.e. particles with a lot of energy and particles with little energy. Unnecessary exposure is mostly caused by soft particles, as they are to weak to pass through the body and interact with it. Hard particles, by contrast, pass through the patient. A filter in front of the x-ray tube can catch the soft particles, thus hardening the beam. This decreases dose without impacting image quality.\n\nFrame rate: High frame rates (i.e. images acquired per second) are needed to visualize fast motion without stroboscopic effects. However, the higher the frame rate, the higher the radiation dose. Therefore, the frame rate should be chosen according to the clinical need and be as low as reasonably possible. For example, in pediatric cardiology, frame rates of 60 pulses per second are required compared to 0.5 p/s for slowly moving objects. A reduction to half pulse rate reduces dose by about half. The reduction from 30 p/s to 7.5 p/s results in a dose saving of 75%.\n\nWhen using pulsed fluoroscopy, radiation dose is only applied in prespecified intervals of time, thus less dose is used to produce the same image sequence. For the time in between, the last image stored is displayed.\n\nAnother tool for decreasing dose is collimation. It may be that from the field of view provided by the detector, only a small part is interesting for the intervention. The x-ray tube can be shielded at the parts that are not necessary to be visible by a collimator, thus only sending dose to the detector for the body parts in question. Modern C-Arms enable to navigate on acquired images without constant fluoroscopy.\n\n"}
{"id": "3624256", "url": "https://en.wikipedia.org/wiki?curid=3624256", "title": "In kind", "text": "In kind\n\nIn economics and finance, in kind refers to goods, services, and transactions not involving money or not measured in monetary terms. For example: \n"}
{"id": "822216", "url": "https://en.wikipedia.org/wiki?curid=822216", "title": "Inductrack", "text": "Inductrack\n\nInductrack is a passive, fail-safe electrodynamic magnetic levitation system, using only unpowered loops of wire in the track and permanent magnets (arranged into Halbach arrays) on the vehicle to achieve magnetic levitation. The track can be in one of two configurations, a \"ladder track\" and a \"laminated track\". The ladder track is made of unpowered Litz wire cables, and the laminated track is made out of stacked copper or aluminium sheets.\n\nThere are three designs: Inductrack I, which is optimized for high speed operation, Inductrack II, which is more efficient at lower speeds, and Inductrack III, which is intended for heavy loads at low speed.\n\nInductrack (or Inductrak) was invented by a team of scientists at Lawrence Livermore National Laboratory in California, headed by physicist Richard F. Post, for use in maglev trains, based on technology used to levitate flywheels. At constant velocity, power is required only to push the train forward against air and electromagnetic drag. Above a minimum speed, as the velocity of the train increases, the levitation gap, lift force and power used are largely constant. The system can lift 50 times the magnet weight.\n\nThe name \"inductrack\" comes from the word \"inductance\" or \"inductor\"; an electrical device made from loops of wire. As a Halbach magnet array passes over the loops of wire, the sinusoidal variations in the field induce a voltage in the track coils. At low speeds the loops are a largely resistive impedance, and hence the induced currents are highest where the field is changing most quickly, which is around the \"least\" intense parts of the field, thus little lift produced.\n\nHowever, at speed, the impedance of the coils increases, proportionate to speed, and dominates the composite impedance of the coil assemblies. This delays the phase of the current peak so that induced current in the track tends more closely to coincide with the field peaks of the magnet array. The track thus creates its own magnetic field which lines up with and repels the permanent magnets, creating the levitation effect. The track is well modeled as an array of series RL circuits.\n\nWhen neodymium–iron–boron permanent magnets are used, levitation is achieved at low speeds. The test model levitated at speeds above , but Richard Post believes that, on real tracks, levitation could be achieved at \"as little as \". Below the transition speed the magnetic drag increases with vehicle speed; above the transition speed, the magnetic drag \"de\"creases with speed. For example, at the lift to drag ratio is 200:1, far higher than any aircraft but much lower than classic steel on steel rail which reaches 1000:1 (rolling resistance). This occurs because the inductive impedance increases proportionately with speed which compensates for the faster rate of change of the field seen by the coils, thus giving a constant current flow and power consumption for the levitation.\n\nThe Inductrack II variation uses two Halbach arrays, one above and one below the track, to double the magnetic field without substantially increasing the weight or area of the arrays, while also reducing drag at low speeds.\n\nSeveral maglev railroad proposals are based upon Inductrack technology. The U.S. National Aeronautics and Space Administration (NASA) is also considering Inductrack technology for launching space planes. \n\nGeneral Atomics is developing Inductrack technology in cooperation with multiple research partners.\n\nHyperloop Transportation Technologies announced in March of 2016 that they would be using passive Inductrack systems for their titular Hyperloop.\n\n\n"}
{"id": "197213", "url": "https://en.wikipedia.org/wiki?curid=197213", "title": "Lead styphnate", "text": "Lead styphnate\n\nLead styphnate (lead 2,4,6-trinitroresorcinate, CHNOPb ), whose name is derived from styphnic acid, is an explosive used as a component in primer and detonator mixtures for less sensitive secondary explosives. Lead styphnate is only slightly soluble in water and methanol Samples of lead styphnate vary in color from yellow to gold, orange, reddish-brown, to brown. Lead styphnate is known in various polymorphs, hydrates, and basic salts. Normal lead styphnate monohydrate, monobasic lead styphnate, tribasic lead styphnate dihydrate, and pentabasic lead styphnate dehydrate as well as α, β polymorphs of lead styphnate exist.\n\nTwo forms of lead styphnate are six-sided monohydrate crystals and small rectangular crystals. Lead styphnate is particularly sensitive to fire and the discharge of static electricity. When dry, it can readily detonate by static discharges from the human body. The longer and narrower the crystals, the more susceptible lead styphnate is to static electricity. Lead styphnate does not react with metals and is less sensitive to shock and friction than mercury fulminate or lead azide. It is stable in storage, even at elevated temperatures. As with other lead-containing compounds, lead styphnate is toxic owing to heavy metal poisoning.\n\nAlthough never substantiated, lead styphnate may have been discovered by Peter Griess (of Griess test fame) in 1874. In 1919, Edmund Herz first established a preparation of anhydrous normal lead styphnate by the reaction of magnesium styphnate with lead acetate in the presence of nitric acid.\n\nNormal lead styphnate exists as α and β polymorphs, both being monoclinic crystals. The lead centres are seven-coordinate and are bridged via oxygen bridges. The water molecule is coordinated to the metal and is also hydrogen-bonded to the anion. Many of the Pb-O distances are short, indicating some degree of covalency. The styphnate ions lie in approximately parallel\nplanes linked by Pb atoms.\n\nLead styphnate's heat of formation is -835 kJ mol-1. The loss of water leads to the formation of a sensitive anhydrous material with a density of 2.9 g cm. The variation of colors remains unexplained. Lead styphnate has a detonation velocity of 5.2 km/s and an explosion temperature of 265-280 °C after five seconds.\nLead styphnate is mainly used in small arm ammunition for military and commercial applications. It serves as a primary explosive with gunpowder, which will not ignite upon a simple impact. Lead styphnate is also used as primer in microthrusters for small satellite stationkeeping.\n\n"}
{"id": "19163321", "url": "https://en.wikipedia.org/wiki?curid=19163321", "title": "Leak detection", "text": "Leak detection\n\nPipeline leak detection is used to determine if and in some cases where a leak has occurred in systems which contain liquids and gases. Methods of detection include hydrostatic testing, infrared, and laser technology after pipeline erection and leak detection during service.\n\nPipeline networks are the most economic and safest mode of transportation for oil, gases and other fluid products. As a means of long-distance transport, pipelines have to fulfill high demands of safety, reliability and efficiency. If properly maintained, pipelines can last indefinitely without leaks. Most significant leaks that do occur are caused by damage from nearby excavation. If a pipeline is not properly maintained, it can corrode, particularly at construction joints, low points where moisture collects, or locations with imperfections in the pipe. However, these defects can be identified by inspection tools and corrected before they progress to a leak. Other reasons for leaks include accidents, earth movement, or sabotage.\n\nThe primary purpose of leak detection systems (LDS) is to help pipeline controllers to detect and localize leaks. LDS provide alarms and display other related data to the pipeline controllers to assist decision-making. Pipeline leak detection systems can also enhance productivity and system reliability thanks to reduced downtime and inspection time.\n\nAccording to the API document \"RP 1130\", LDS are divided into internally based LDS and externally based LDS. Internally based systems use field instrumentation (for example flow, pressure or fluid temperature sensors) to monitor internal pipeline parameters. Externally based systems use a different set of field instrumentation (for example infrared radiometers or thermal cameras, vapor sensors, acoustic microphones or fiber-optic cables) to monitor external pipeline parameters.\n\nSome countries formally regulate pipeline operation.\n\nThis recommended practice (RP) focuses on the design, implementation, testing and operation of LDS that use an algorithmic approach. The purpose of this recommended practice is to assist the Pipeline Operator in identifying issues relevant to the selection, implementation, testing, and operation of an LDS. \n\nTRFL is the abbreviation for \"Technische Regel für Fernleitungsanlagen\" (Technical Rule for Pipeline Systems). The TRFL summarizes requirements for pipelines being subject of official regulations. It covers pipelines transporting flammable liquids, pipelines transporting liquids that are dangerous for water, and most of the pipelines transporting gas. Five different kinds of LDS or LDS functions are required:\n\nAPI 1155(replaced by API RP 1130) defines the following important requirements for an LDS:\n\nDuring steady-state conditions, the flow, pressures, etc. in the pipeline are (more or less) constant over time. During transient conditions, these variables may change rapidly. The changes propagate like waves through the pipeline with the speed of sound of the fluid. Transient conditions occur in a pipeline for example at start-up, \nif the pressure at inlet or outlet changes (even if the change is small), and when a batch changes, or when multiple products are in the pipeline. Gas pipelines are almost always in transient conditions, because gases are very compressible. Even in liquid pipelines, transient effects cannot be disregarded most of the time. LDS should allow for detection of leaks for both conditions to provide leak detection during the entire operating time of the pipeline.\n\nInternally based systems use field instrumentation (e.g. for flow, pressure and fluid temperature) to monitor internal pipeline parameters which are used to detect possible leaks. System cost and complexity of internally based LDS are moderate because they use existing field instrumentation. This kind of LDS is used for standard safety requirements.\n\nA leak changes the hydraulics of the pipeline, and therefore changes the pressure or flow readings after some time. Local monitoring of pressure or flow at only one point can therefore provide simple leak detection. As it is done locally it requires in principle no telemetry. It is only useful in steady-state conditions, however, and its ability to deal with gas pipelines is limited.\n\nThe acoustic pressure wave method analyses the rarefaction waves produced when a leak occurs. When a pipeline wall breakdown occurs, fluid or gas escapes in the form of a high velocity jet. This produces negative pressure waves which propagate in both directions within the pipeline and can be detected and analyzed. The operating principles of the method are based on the very important characteristic of pressure waves to travel over long distances at the speed of sound guided by the pipeline walls. The amplitude of a pressure wave increases with the leak size. A complex mathematical algorithm analyzes data from pressure sensors and is able in a matter of seconds to point to the location of the leakage with accuracy less than 50 m (164 ft). Experimental data has shown the method's ability to detect leaks less than 3mm (0.1 inch) in diameter and operate with the lowest false alarm rate in the industry – less than 1 false alarm per year.\n\nHowever, the method is unable to detect an ongoing leak after the initial event: after the pipeline wall breakdown (or rupture), the initial pressure waves subside and no subsequent pressure waves are generated. Therefore, if the system fails to detect the leak (for instance, because the pressure waves were masked by transient pressure waves caused by an operational event such as a change in pumping pressure or valve switching), the system will not detect the ongoing leak.\n\nThese methods base on the principle of conservation of mass. In the steady state, the mass flow formula_1 entering a leak-free pipeline will balance the mass flow formula_2 leaving it; any drop in mass leaving the pipeline (mass imbalance formula_3) indicates a leak. Balancing methods measure formula_1 and formula_2 using flowmeters and finally compute the imbalance which is an estimate of the unknown, true leak flow. Comparing this imbalance (typically monitored over a number of periods) against a leak alarm threshold formula_6 generates an alarm if this monitored imbalance. Enhanced balancing methods additionally take into account the change rate of the mass inventory of the pipeline. Names that are used for enhanced line balancing techniques are volume balance, modified volume balance, and compensated mass balance.\n\nThese methods are based on state observers which are designed from fluid mathematical models expressed in state-space representation.\nThese methods can be classified into two types: infinite-dimensional observers and finite-dimensional observers. The first type is based on a couple of quasi-linear hyperbolic partial differential equations: a momentum and a continuity equations that represent the fluid dynamics in a pipeline. The finite-dimensional observers are constructed from a lumped version of the momentum and a continuity equations.\nSeveral types of observers have been used for leak detection, for instance Kalman filters, high gain observers, sliding mode observers\n\nStatistical LDS use statistical methods (e.g. from the field of decision theory) to analyse pressure/flow at only one point or the imbalance in order to detect a leak. This leads to the opportunity to optimise the leak decision if some statistical assumptions hold. A common approach is the use of the hypothesis test procedure\n\nThis is a classical detection problem, and there are various solutions known from statistics.\n\nRTTM means \"Real-Time Transient Model\". RTTM LDS use mathematical models of the flow within a pipeline using basic physical laws such as conservation of mass, conservation of momentum, and conservation of energy. RTTM methods can be seen as an enhancement of balancing methods as they additionally use the conservation principle of momentum and energy. An RTTM makes it possible to calculate mass flow, pressure, density and temperature at every point along the pipeline in real-time with the help of mathematical algorithms. RTTM LDS can easily model steady-state and transient flow in a pipeline. Using RTTM technology, leaks can be detected during steady-state and transient conditions. With proper functioning instrumentation, leak rates may be functionally estimated using available formulas.\n\nE-RTTM stands for \"Extended Real-Time Transient Model\", using RTTM technology with statistical methods. So, leak detection is possible during steady-state and transient condition with high sensitivity, and false alarms will be avoided using statistical methods.\n\nFor the residual method, an RTTM module calculates estimates formula_9, formula_10 for MASS FLOW at inlet and outlet, respectively. This can be done using measurements for pressure and temperature at inlet (formula_11, formula_12) and outlet (formula_13, formula_14). These estimated mass flows are compared with the measured mass flows formula_1, formula_2, yielding the residuals formula_17 and formula_18. These residuals are close to zero if there is no leak; otherwise the residuals show a characteristic signature. In a next step, the residuals are subject of a leak signature analysis. This module analyses their temporal behaviour by extracting and comparing the leak signature with leak signatures in a database (\"fingerprint\"). Leak alarm is declared if the extracted leak signature matches the fingerprint.\n\nExternally based systems use local, dedicated sensors. Such LDS are highly sensitive and accurate, but system cost and complexity of installation are usually very high; applications are therefore limited to special high-risk areas, e.g. near rivers or nature-protection areas.\n\nVideo analytics driven thermal imaging using uncooled microbolometer infrared sensors is emerging as a new and effective method of visualizing, detecting and generating alerts of unplanned surface emissions of liquids and hydrocarbon gas liquids. Detection to alarm generation takes less than 30 seconds. This technology is suitable for above-ground piping facilities, such as pump stations, refineries, storage sites, mines, chemical plants, water crossings, and water treatment plants. The need for new solutions in this area is driven by the fact that more than half of pipeline leaks occur at facilities.\n\nHigh quality thermographic technology accurately measures and visualizes emissivity or infrared radiation (thermal heat) of objects into gray scale imagery without the need for ambient lighting. The monitored petroleum product (e.g. oil) is distinguished from background objects by this heat difference. The addition of an analytic software component, typically optimizable to better address a specific application or environment, enables automated onsite leak analysis, validation and reporting, thereby reducing reliance on man power. A leak appearing within an analytic region (a rule added to the camera) is immediately analyzed for its attributes, including thermal temperature, size, and behaviour (e.g. spraying, pooling, spilling). When a leak is determined to be valid based on set parameters, an alarm notification with leak video is generated and sent to a monitoring station. \n\nOptimal detection distance varies and is influenced by camera lens size, resolution, field of view, thermal detection range and sensitivity, leak size, and other factors. The system's layers of filters and immunity to environmental elements, such as snow, ice, rain, fog and glare, contribute to false alarms reduction. The video monitoring architecture can be integrated onto existing leak detection and repair (LDAR) systems, including SCADA networks, as well as other surveillance systems.\n\nDigital sense cables consist of a braid of semi-permeable internal conductors protected by a permeable insulating moulded braid. An electrical signal is passed through the internal conductors and is monitored by an inbuilt microprocessor inside the cable connector. Escaping fluids pass through the external permeable braid and make contact with the internal semi-permeable conductors. This causes a change in the electrical properties of the cable that is detected by the microprocessor. The microprocessor can locate the fluid to within a 1-metre resolution along its length and provide an appropriate signal to monitoring systems or operators. The sense cables can be wrapped around pipelines, buried sub-surface with pipelines or installed as a pipe-in-pipe configuration.\n\nInfrared thermographic pipeline testing has shown itself to be both accurate and efficient in detecting and locating subsurface pipeline leaks, voids caused by erosion, deteriorated pipeline insulation, and poor backfill. When a pipeline leak has allowed a fluid, such as water, to form a plume near a pipeline, the fluid has a thermal conductance different from the dry soil or backfill. This will be reflected in different surface temperature patterns above the leak location. A high-resolution infrared radiometer allows entire areas to be scanned and the resulting data to be displayed as pictures with areas of differing temperatures designated by differing grey tones on a black & white image or by various colours on a colour image. This system measures surface energy patterns only, but the patterns that are measured on the surface of the ground above a buried pipeline can help show where pipeline leaks and resulting erosion voids are forming; it detects problems as deep as 30 meters below the ground surface.\n\nEscaping liquids create an acoustic signal as they pass through a hole in the pipe. Acoustic sensors affixed to the outside of the pipeline create a baseline acoustic \"fingerprint\" of the line from the internal noise of the pipeline in its undamaged state. When a leak occurs, a resulting low frequency acoustic signal is detected and analysed. Deviations from the baseline \"fingerprint\" signal an alarm.\nNow sensors are having better arrangement with frequency band selection, time delay range selection etc. This makes the graphs more distinct and easy to analyse.There are other ways to detect leakage. Ground geo-phones with filter arrangement are very useful to pinpoint the leakage location. It saves the excavation cost. The water jet in the soil hits the inner wall of soil or concrete. This will create a feeble noise. This noise will decay while coming up on the surface. But the maximum sound can be picked up only over the leakage position. Amplifiers and filter helps to get clear noise. Some types of gases entered into the pipe line will create a range of sounds when leaving the pipe.\n\nThe vapour-sensing tube leak detection method involves the installation of a tube along the entire length of the pipeline. This tube - in cable form - is highly permeable to the substances to be detected in the particular application. If a leak occurs, the substances to be measured come into contact with the tube in the form of vapour, gas or dissolved in water. In the event of a leak, some of the leaking substance diffuses into the tube. After a certain period of time, the inside of the tube produces an accurate image of the substances surrounding the tube. In order to analyse the concentration distribution present in the sensor tube, a pump pushes the column of air in the tube past a detection unit at a constant speed. The detector unit at the end of the sensor tube is equipped with gas sensors. Every increase in gas concentration results in a pronounced \"leak peak\".\n\nAt least two fibre-optic leak detection methods are being commercialized: Distributed Temperature Sensing (DTS) and Distributed Acoustic Sensing (DAS). The DTS method involves the installation of a fibre-optic cable along the length of pipeline being monitored. The substances to be measured come into contact with the cable when a leak occurs, changing the temperature of the cable and changing the reflection of the laser beam pulse, signalling a leak. The location is known by measuring the time delay between when the laser pulse was emitted and when the reflection is detected. This only works if the substance is at a temperature different from the ambient environment. In addition, the distributed fibre-optical temperature-sensing technique offers the possibility to measure temperature along the pipeline. Scanning the entire length of the fibre, the temperature profile along the fibre is determined, leading to leak detection.\n\nThe DAS method involves a similar installation of fiber-optic cable along the length of pipeline being monitored. Vibrations caused by a substance leaving the pipeline via a leak changes the reflection of the laser beam pulse, signaling a leak. The location is known by measuring the time delay between when the laser pulse was emitted and when the reflection is detected. This technique can also be combined with the Distributed Temperature Sensing method to provide a temperature profile of the pipeline.\n\nFlyovers of the pipeline are frequently carried out to either confirm the location or to detect and locate small releases that cannot be identified by other methods. Typically the flyover of the right of way is recorded by video, which may have some image filtering, such as thermal imaging. Larger spills will typically be identified by a \"sheen\" in wetland or an area of dead vegetation around the release location.\n\nFlyovers are typically scheduled and not recommended as a primary leak-detection method. They may be used to rapidly confirm the presence and location of a leak.\n\nBiological methods of leak detection includes the use of dogs, which are more likely to be used once a release has been identified but not located due to its small size; or by landscapers who keep the pipeline right of way clear. \n\nThere are several companies who can provide dogs trained to identify the scent of release. Typically a technician injects a fluid into the pipeline that the scent dogs are trained to track. The dogs will then direct handlers towards a pipeline leak. They are trained to indicated at the strongest concentration therefore their pinpointing abilities can be typically within a meter. It typically takes 24 to 48 hours to mobilise a team, and may take several days to actually locate a release depending on the remoteness of the area.\n\nPipeline rights of way are kept clear by landscapers who are also trained to look for signs of pipeline releases. This is typically a scheduled process and should not be considered a primary form of leak detection.\n\n"}
{"id": "2473135", "url": "https://en.wikipedia.org/wiki?curid=2473135", "title": "Manual override", "text": "Manual override\n\nA manual override (MO) or manual analog override (MAO) is a mechanism wherein control is taken from an automated system and given to the user. For example, a manual override in photography refers to the ability for the human photographer to turn off the automatic aperture sizing, automatic focusing, or any other automated system on the camera.\n\nSome manual overrides can be used to veto an automated system's judgment when the system is in error. An example of this is a printer's ink level detection: in one case, a researcher found that when he overrode the system, up to 38% more pages could be printed at good quality by the printer than the automated system would have allowed.\n\nAutomated systems are becoming increasingly common and integrated into everyday objects such as automobiles and domestic appliances. This development of ubiquitous computing raises general issues of policy and law about the need for manual overrides for matters of great importance such as life-threatening situations and major economic decisions. The loyalty of such autonomous devices then becomes an issue. If they follow rules installed by the manufacturer or required by law and refuse to cede control in some situations then the owners of the devices may feel disempowered, alienated and lacking true ownership.\n\nChina Airlines Flight 140 crashed, causing many deaths, due to a misunderstanding about the manual overrides for the autopilot. The Take-Off/Go Around system had been activated to abort a landing. It was programmed to ignore manual controls in this situation but the human pilots tried to continue the landing. The conflicting control signals from the pilots and autopilot then resulted in the aircraft stalling and crashing. The autopilot for this aircraft type was then reprogrammed so that it would never ignore a manual override.\n\n"}
{"id": "1070542", "url": "https://en.wikipedia.org/wiki?curid=1070542", "title": "Mizuho Information &amp; Research Institute", "text": "Mizuho Information &amp; Research Institute\n\nThe Institute was formed by the merging of DKB Information Systems, Fuji Research Institute Corporation, and IBJ Systems Ltd. (all owned by Mizuho Financial Group) on October 1, 2004.\n\n"}
{"id": "1281230", "url": "https://en.wikipedia.org/wiki?curid=1281230", "title": "Navtex", "text": "Navtex\n\nNavtex (Navigational Telex) is an international automated medium frequency direct-printing service for delivery of navigational and meteorological warnings and forecasts, as well as urgent maritime safety information (MSI) to ships.\n\nNavtex was developed to provide a low-cost, simple, and automated means of receiving this information aboard ships at sea within approximately 370 km (200 nautical miles) off shore.\n\nThere are no user fees associated with receiving navtex broadcasts, as the transmissions are typically transmitted from the National Weather Authority (Italy) or Navy or Coast Guard (as in the US) or national navigation authority (Canada).\n\nWhere the messages contain weather forecasts, an abbreviated format very similar to the shipping forecast is used.\n\nNavtex is a component of the International Maritime Organization/International Hydrographic Organization Worldwide Navigation Warning Service (WWNWS). Navtex is also a major element of the Global Maritime Distress Safety System (GMDSS). SOLAS Convention mandated certain classes of vessels must carry navtex, beginning August 1, 1993.\n\nNavtex transmissions are also called narrow-band direct printing (NBDP). The transmissions are layered on top of SITOR collective B-mode. SITOR-B is a forward error correcting (FEC) broadcast that uses the CCIR 476 character set.\n\nSITOR-B is also used in amateur radio, where it is known as AMTOR-B or AMTOR-FEC. \n\nNavtex broadcasts are primarily made on the medium frequencies of 518 kHz and 490 kHz. The international navtex frequency is 518 kHz, and these broadcasts should always be in English. National transmission of navtex uses 490 kHz specifically for broadcasts in local languages. It is not used in the U.S.\n\nNavtex Marine Safety Information (MSI) national transmissions also take place on HF at 4209.5 kHz using FEC mode.\n\nOther transmission modes (like MT63, or Olivia) with better error correction properties have emerged since navtex was made the standard for maritime information transmissions. Overall, with slightly higher transmitter power most Navtex error correction issues tend to be absent.\n\nNavtex messages are transmitted using binary frequency-shift keying (BFSK) at 100 bit/s and a 170 Hz frequency shift. The characters are encoded using the 7-bit CCIR 476 character set and basic error detection is enabled by employing forward error correction (FEC). This is the same format as the SITOR-B (AMTOR) format.\n\nA navtex message is built on SITOR collective B-mode and consists of:\n\nB is an alpha character identifying the station, and B is an alpha character used to identify the subject of the message. Receivers use these characters to reject messages from certain stations or if the message contains subjects of no interest to the user.\n\nB and B are two-digit numerics identifying individual messages, used by receivers to keep already received messages from being repeated.\n\nFor example, a message containing \"BBBB\" characters of 'FE01' from a U.S. navtex station indicates a weather forecast message from Boston, MA.\n\nNavtex message example:\nZCZC begins the message.\n\nThis character defines the transmitter identity and its associated coverage area.\n\nThe subject indicator character is used by the receiver to identify different classes of messages below. The indicator is also used to reject messages concerning certain optional subjects which are not required by the ship (e.g. LORAN C messages might be rejected in a ship which is not fitted with a LORAN C receiver).\n\nNavtex broadcasts use following subject indicator characters:\n\nNote: Receivers use the B character to identify messages which, because of their importance, can not be rejected (designated by a ).\nThe subject indicator characters B, F and G are normally not used in the United States since the National Weather Service normally includes meteorological warnings in forecast messages. Meteorological warnings are broadcast using the subject indicator character E.\nU.S. Coast Guard District Broadcast Notices to Mariners affecting ships outside the line of demarcation, and inside the line of demarcation in areas where deep draft vessels operate, use the subject indicator character A.\n\nThese two characters define the serial number of each B message type (class). Generally serial numbers start with the numbers '01', however in special circumstances, the numbers begin with '00'. This forces the receiver to print the message.\n\nTime of origin is in the format of \"DDHHmm UTC MMM\" where DD is the date, HH hour, mm minute and MMM three-character abbreviation of month. The time of the transmission of the message is in UTC.\n\nThe full text of the message follows.\n\nThe end of the message is asserted when the characters \"NNNN\" are received.\n\nEach station identifier has a fixed 10-minute time slot, starting with A at 0000UTC. The time slots are repeated at 4 hour intervals. Within each time slot, a mix of navigation warnings, weather forecasts, ice information and other content may be sent, and this is normally according to a structured plan for that specific station. For example, in the first and third time slot they may decide to transmit navigation warnings, and weather forecasts in the others. Normally each NAVAREA or sub-NAVAREA has only one station at each slot.\n\nDetails of all transmitting stations and their schedules may be found at www.pcnavtex.com\n\nNavtex receivers which are approved for GMDSS contain an internal printer and/or a scrollable display, and cost between $800–$1500. A new generation of navtex receivers intended for non-GMDSS applications such as the recreational community is entering the marketplace. These receivers include features such as LCD screens and RS-232 output and have a purchase price in the $300–$500 range. In the UK they can be purchased for £115. There are also a number of navtex engines available that do not have any user interface, and just output decoded data in RS-232 format, either as a simple ASCII data stream, or using the NMEA navtex sentences, or their own proprietary protocol.\n\nThere are also a number of software packages available, such as SeaTTY, Mscan, JNX, Fldigi or JVComm32, allowing messages to be decoded by a PC with a suitable receiver connected to the computer's soundcard. Any general communications receiver capable of audio reception at 518 kHz or 490 kHz single sideband can be used.\n\nSome organisations have gateways through which web users can access the navtex bulletins using a browser:\n\n\n\nNavtex control and monitoring Systems\n\nNavtex decoders\n\nNavtex station lists & information\n"}
{"id": "6369332", "url": "https://en.wikipedia.org/wiki?curid=6369332", "title": "Niagara Tunnel Project", "text": "Niagara Tunnel Project\n\nThe Niagara Tunnel Project was part of a series of major additions to the Sir Adam Beck hydroelectric generation complex in Niagara Falls, Ontario, Canada.\n\nWater delivered by the major new underground tunnel complements other upgrades to the Sir Adam Beck Hydroelectric Generating Stations, resulting in more efficient use of the Niagara River's hydro power.\n\nThe project's new diameter, long tunnel was officially placed into service on 21 March 2013, helping to increase the generating complex's nameplate capacity by 150 megawatts, with the extra power produced enough for approximately 160,000 homes.\n\nFirst constructed in 1922, the initial Sir Adam Beck power generating station, now abbreviated as SAB 1, derived its water supply from a hydro canal connected to the Welland River. However, due to increased power demand in later years, a second generating station, known as SAB 2, was constructed in 1954. It derives its water supply from two diversion tunnels, each about in length. In 1958, a reservoir and the SAB Pump Station were constructed in order to make better use of available water by storing it during periods of low demand and using it in periods of greater demand in order to maximize the efficiency of the stations in regards to electricity supply and demand.\n\nBetween 1996 and 2005, Ontario Power Generation (OPG), which owns and operates the SAB complex, completed a series of major upgrades at the SAB 2 plant, increasing its potential generating nameplate capacity by 194 megawatts. Water delivered through the new Niagara Tunnel Project complements the SAB 2 upgrade, and results in an overall increase to the efficient use of the Niagara River's hydro power.\n\nThe new long tunnel is in diameter, or about four stories in height, and allows an additional diversion of a distant part of the Niagara River to reach the SAB plant complex at a rate of about 500 cubic meters (17,657 cubic feet) of water per second, a flow rate that can fill an Olympic-sized swimming pool in seconds. The Ontario Government considers the Niagara Tunnel Project and the Sir Adam Beck complex as integral to its efforts to close all of the province's coal-fired generating plants as part of its clean and green energy program.\n\nUpon the project's formal opening the Honourable Bob Chiarelli, M.P.P., Ontario’s Minister of Energy, stated that \"This project is a source of pride as an engineering feat and as a practical solution for meeting Ontario's energy needs through clean sources\".\n\nThe Niagara Tunnel was constructed using a Tunnel Boring Machine (TBM), affectionately named \"Big Becky\" in honour of Sir Adam Beck. The TBM machine bored a tunnel about long and about 14.4 metres (47 ft) in diameter under the City of Niagara Falls, Ontario, from the Niagara River to the SAB complex. The bored tunnel was subsequently lined with an impervious membrane and a concrete lining, pre-stressed by high pressure grouting. The massive undertaking created about 1.6 million cubic meters of rock and debris, and was designed for a minimum 90-year life span.\n\nThe HP 471-316 TBM was driven by 15 electric motors totaling about 4.7 megawatts of power (6,375 horsepower), built by the Robbins Company of Solon, Ohio (a suburb of Cleveland), and was the world’s largest hard-rock tunnel boring machine as of 2006. The TBM operated as deep as below ground level to avoid the machine's vibrations being felt at surface level. The design-build contractor for the project was the Austrian construction company Strabag AG, a large construction group with extensive experience in large tunnel construction.\n\nBoring by the Hatch Mott MacDonald Engineering Company started on 15 September 2006 at the north end, located at and ended on 31 March 2011 at the south end, located at .\n\nDue to slower than projected boring progress caused by constant tunnel roof over-breaks, the project completion date was extended to at least December 2013, with a revised total projected cost of $1.6B.\n\nOn 21 March 2013 the Ontario Power Generation Corporation (OPG) and the Ontario Government officially placed the new Niagara tunnel into service with a formal opening ceremony and tour. The OPG reported that the project will be financially rewarding despite being completed four years later than planned and about $500M over its originally estimated cost. OPG's president and CEO Tom Mitchell stated: \"This was a large, complex project that will serve Ontario for more than 100 years\".\n\nThe tunnel project was completed at a total cost $100 million lower than its revised $1.6 billion budget, and with an in-service date of 9 March 2013. The operational start-of-service date was nine months sooner than the project's revised schedule that was drawn up in 2009 due to unexpected and difficult geological conditions encountered in the soft Queenston Shale Formation during the tunnel boring process.\n\nThe new tunnel will provide enough additional water to the Sir Adam Beck generating complex to produce power for approximately 160,000 homes.\n\nThe Niagara Tunnel Project was selected by International Water Power & Dam Construction as the North American Project of the Year for 2013.\n\n\n"}
{"id": "46257702", "url": "https://en.wikipedia.org/wiki?curid=46257702", "title": "North Arm Powder Magazine", "text": "North Arm Powder Magazine\n\nThe North Arm Powder Magazine near Port Adelaide, South Australia was from 1858 to 1906 a secure storage facility for dynamite and gelignite to be used in the construction, mining and quarrying industries.\n\nIt was in Gillman near Port Adelaide at the North Arm of the Port River only 9 m away from the North Arm Bridge and North Arm Road. The explosives were stored in the wooden and slated magazine building and in two dynamite hulks moored in Magazine Creek. One of them was a retired iron dredger, built around 1852, and the other was a former lighter. They were seen as a risk, if they should ever explode, because they were close to the new bridge and coastal settlements and inland towns.\n\nThe North Arm Powder Magazine was built in 1858 by the government. The magazine was built as a lightweight structure on wooden poles due to its location at the tidal creek. Only later this became best practice to mitigate the secondary damage from an explosion by descending débris.\n\nAs it was close to urbanisation, it was suggested to abandon the site already a decade after it had been commissioned. It was taken out of service only in 1906, after its contents had been moved to the new Dry Creek explosives depot. The building was demolished in 1916 and no visible evidence remains.\n"}
{"id": "44137764", "url": "https://en.wikipedia.org/wiki?curid=44137764", "title": "OPS-SAT", "text": "OPS-SAT\n\nOPS-SAT is a CubeSat currently being built by the European Space Agency (ESA) and it is intended to demonstrate the improvements in mission control capabilities that will arise when satellites can fly more powerful on-board computers. It consists of a satellite which contains an experimental computer that is ten times more powerful than any current ESA spacecraft. The OPS-SAT mission has the very clear objective to break the cycle of “has never flown, will never fly” in the area of satellite control. It is going to be the first CubeSat operated directly by ESA.\n\nOPS-SAT shall provide an in-orbit test-bed environment for the deployment of different experiments to test new protocols, new algorithms and new techniques. The satellite is being designed to be robust and no Single point of failure should exist, therefore it shall be always possible to recover the spacecraft if something goes wrong with one of the software experiments. The robustness of the basic satellite itself will allow ESA flight control teams to upload and try out new, innovative control software submitted by experimenters.\n\nAs of April 2017, OPS-SAT is completing its first System Validation Tests, in which the engineering model and ground segments of the spacecraft are brought together.\n\nLaunch date is currently being targeted for 2019.\n\n\n\nThe Experimental Platform is the heart of OPS-SAT. It includes a Dual-core 800 MHz ARM Cortex-A9 processor, an Altera Cyclone V FPGA, 1 GB DDR3 RAM, and an external mass memory device with 8 GB. \n\nESA‘s aim is to remove as many barriers to experimentation as possible. For example, there will be no paper work, ESOC's infrastructure will be ready to do automated tests on the experiments, and aims at reducing the overheads close to zero.\n\nThe NanoSat MO Framework (NMF) is a software framework for nanosatellites based on CCSDS Mission Operations services. An experiment can be developed as an NMF App that can then be installed, started, and stopped. It also includes monitoring and control capabilities for NMF Apps. The software is open-source. \n\nOn Ground, EUD4MO will provide a web-based solution for the monitoring and control of NMF Apps. OPS-SAT experimenters will be able to take control using their web browser.\n\n\n"}
{"id": "24664054", "url": "https://en.wikipedia.org/wiki?curid=24664054", "title": "PICMG 2.10", "text": "PICMG 2.10\n\nPICMG 2.10 is a specification by PICMG that defines the use of the keying mechanisms defined in IEC 1076-4-101 for the J4/P4 connector and in IEEE 1101.10 for handle and cardguide hardware. Backplanes can be designed for 3.3V VIO or 5V VIO operation. These are differentiated by having 'Cadmium Yellow' colored keys for 3.3V or 'Brilliant Blue' color for 5V operation. If the cPCI card operates on a particular VIO voltage the card shall have the respective colored coding key. If the card is compatible with both voltages then it may not have any coding key. Other coding keys exists for use of backplanes and cards that support PICMG 2.5.\n\nAdopted : 10/1/1999\n\nCurrent Revision : 1.0\n"}
{"id": "3548625", "url": "https://en.wikipedia.org/wiki?curid=3548625", "title": "Picket-fencing", "text": "Picket-fencing\n\nPicket fencing is slang for the chopping effect sometimes heard by cell phone users at the edge of a cell's coverage area, or (more likely) by the landline user to whom the cellphone is connected. \"Picket fencing\" refers to the way portions of speech are stripped from the conversation, as if the listener was walking by a picket fence, and hearing a conversation on the other side that changes audibily depending on the position of the pickets relative to the listener.\n\nOriginally from amateur radio, the phrase was used to describe the way an FM transmitter will cut in and out as it nears the capture threshold of a moving receiver or transmitter as it passes through fresnel zones, thus chopping the speech of the transmitting operator. It is not clear if the phrase was intended to describe the loss of the speech, or if it actually referred to the chopping sound itself, which imitates the noise produced by dragging a stiff object across a picket fence.\n"}
{"id": "43982", "url": "https://en.wikipedia.org/wiki?curid=43982", "title": "Plumbing", "text": "Plumbing\n\nPlumbing is any system that conveys fluids for a wide range of applications. Plumbing uses pipes, valves, plumbing fixtures, tanks, and other apparatuses to convey fluids. Heating and cooling (HVAC), waste removal, and potable water delivery are among the most common uses for plumbing, but it is not limited to these applications. The word derives from the Latin for lead, \"plumbum\", as the first effective pipes used in the Roman era were lead pipes.\n\nIn the developed world, plumbing infrastructure is critical to public health and sanitation. Boilermakers and pipefitters are not plumbers, although they work with piping as part of their trade, but their work can include some plumbing.\n\nPlumbing originated during ancient civilizations such as the Greek, Roman, Persian, Indian, and Chinese cities as they developed public baths and needed to provide potable water and wastewater removal, for larger numbers of people. Standardized earthen plumbing pipes with broad flanges making use of asphalt for preventing leakages appeared in the urban settlements of the Indus Valley Civilization by 2700 BC. The Romans used lead pipe inscriptions to prevent water theft.\nThe word \"plumber\" dates from the Roman Empire. The Latin for lead is \"\". Roman roofs used lead in conduits and drain pipes and some were also covered with lead. Lead was also used for piping and for making baths.\n\nPlumbing reached its early apex in ancient Rome, which saw the introduction of expansive systems of aqueducts, tile wastewater removal, and widespread use of lead pipes. With the Fall of Rome both water supply and sanitation stagnated—or regressed—for well over 1,000 years. Improvement was very slow, with little effective progress made until the growth of modern densely populated cities in the 1800s. During this period, public health authorities began pressing for better waste disposal systems to be installed, to prevent or control epidemics of disease. Earlier, the waste disposal system had merely consisted of collecting waste and dumping it on the ground or into a river. Eventually the development of separate, underground water and sewage systems eliminated open sewage ditches and cesspools.\n\nMost large cities today pipe solid wastes to sewage treatment plants in order to separate and partially purify the water, before emptying into streams or other bodies of water. For potable water use, galvanized iron piping was commonplace in the United States from the late 1800s until around 1960. After that period, copper piping took over, first soft copper with flared fittings, then with rigid copper tubing utilizing soldered fittings.\n\nThe use of lead for potable water declined sharply after World War II because of increased awareness of the dangers of lead poisoning. At this time, copper piping was introduced as a better and safer alternative to lead pipes.\n\nThe major categories of plumbing systems or subsystems are:\n\nA \"water pipe\" is a pipe or tube, frequently made of plastic or metal, that carries pressurized and treated fresh water to a building (as part of a municipal water system), as well as inside the building.\n\nFor many centuries, lead was the favoured material for water pipes, because its malleability made it practical to work into the desired shape. (Such use was so common that the word \"plumbing\" derives from \"plumbum\", the Latin word for lead.) This was a source of lead-related health problems in the years before the health hazards of ingesting lead were fully understood; among these were stillbirths and high rates of infant mortality. Lead water pipes were still widely used in the early 20th century, and remain in many households. In addition, lead-tin alloy solder was commonly used to join copper pipes, but modern practice uses tin-antimony alloy solder instead, in order to eliminate lead hazards.\n\nDespite the Romans' common use of lead pipes, their aqueducts rarely poisoned people. Unlike other parts of the world where lead pipes cause poisoning, the Roman water had so much calcium in it that a layer of plaque prevented the water contacting the lead itself. What often causes confusion is the large amount of evidence of widespread lead poisoning, particularly amongst those who would have had easy access to piped water. This was an unfortunate result of lead being used in cookware and as an additive to processed food and drink, for example as a preservative in wine. Roman lead pipe inscriptions provided information on the owner to prevent water theft.\n\nWooden pipes were used in London and elsewhere during the 16th and 17th centuries. The pipes were hollowed-out logs, which were tapered at the end with a small hole in which the water would pass through. The multiple pipes were then sealed together with hot animal fat. They were often used in Montreal and Boston in the 1800s, and built-up wooden tubes were widely used in the USA during the 20th century. These pipes, used in place of corrugated iron or reinforced concrete pipes, were made of sections cut from short lengths of wood. Locking of adjacent rings with hardwood dowel pins produced a flexible structure. About 100,000 feet of these wooden pipes were installed during WW2 in drainage culverts, storm sewers and conduits, under highways and at army camps, naval stations, airfields and ordnance plants.\n\nCast iron and ductile iron pipe was long a lower-cost alternative to copper, before the advent of durable plastic materials but special non-conductive fittings must be used where transitions are to be made to other metallic pipes, except for terminal fittings, in order to avoid corrosion owing to electrochemical reactions between dissimilar metals (see galvanic cell).\n\nBronze fittings and short pipe segments are commonly used in combination with various materials.\n\nThe difference between pipes and tubes is simply in the way it is sized. PVC pipe for plumbing applications and galvanized steel pipe for instance, are measured in IPS (iron pipe size). Copper tube, CPVC, PeX and other tubing is measured nominally, which is basically an average diameter. These sizing schemes allow for universal adaptation of transitional fittings. For instance, 1/2\" PeX tubing is the same size as 1/2\" copper tubing. 1/2\" PVC on the other hand is not the same size as 1/2\" tubing, and therefore requires either a threaded male or female adapter to connect them. When used in agricultural irrigation, the singular form \"pipe\" is often used as a plural.\n\nPipe is available in rigid \"joints\", which come in various lengths depending on the material. Tubing, in particular copper, comes in rigid hard tempered \"joints\" or soft tempered (annealed) rolls. PeX and CPVC tubing also comes in rigid \"joints\" or flexible rolls. The temper of the copper, that is whether it is a rigid \"joint\" or flexible roll, does not affect the sizing.\n\nThe thicknesses of the water pipe and tube walls can vary. Pipe wall thickness is denoted by various schedules or for large bore polyethylene pipe in the UK by the Standard Dimension Ratio (SDR), defined as the ratio of the pipe diameter to its wall thickness. Pipe wall thickness increases with schedule, and is available in schedules 20, 40, 80, and higher in special cases. The schedule is largely determined by the operating pressure of the system, with higher pressures commanding greater thickness. Copper tubing is available in four wall thicknesses: type DWV (thinnest wall; only allowed as drain pipe per UPC), type 'M' (thin; typically only allowed as drain pipe by IPC code), type 'L' (thicker, standard duty for water lines and water service), and type 'K' (thickest, typically used underground between the main and the meter). Because piping and tubing are commodities, having a greater wall thickness implies higher initial cost. Thicker walled pipe generally implies greater durability and higher pressure tolerances.\n\nWall thickness does not affect pipe or tubing size. 1/2\" L copper has the same outer diameter as 1/2\" K or M copper. The same applies to pipe schedules. As a result, a slight increase in pressure losses is realized due to a decrease in flowpath as wall thickness is increased. In other words, 1 foot of 1/2\" L copper has slightly less volume than 1 foot of 1/2 M copper.\n\nWater systems of ancient times relied on gravity for the supply of water, using pipes or channels usually made of clay, lead, bamboo, wood, or stone. Hollowed wooden logs wrapped in steel banding were used for plumbing pipes, particularly water mains. Logs were used for water distribution in England close to 500 years ago. US cities began using hollowed logs in the late 1700s through the 1800s. Today, most plumbing supply pipe is made out of steel, copper, and plastic; most waste (also known as \"soil\") out of steel, copper, plastic, and cast iron.\n\nThe straight sections of plumbing systems are called \"pipes\" or \"tubes\". A pipe is typically formed via casting or welding, whereas a tube is made through extrusion. Pipe normally has thicker walls and may be threaded or welded, while tubing is thinner-walled and requires special joining techniques such as brazing, compression fitting, crimping, or for plastics, solvent welding. These joining techniques are discussed in more detail in the piping and plumbing fittings article.\n\nGalvanized steel potable water supply and distribution pipes are commonly found with nominal pipe sizes from to . It is rarely used today for new construction residential plumbing. Steel pipe has National Pipe Thread (NPT) standard tapered male threads, which connect with female tapered threads on elbows, tees, couplers, valves, and other fittings. Galvanized steel (often known simply as \"galv\" or \"iron\" in the plumbing trade) is relatively expensive, and difficult to work with due to weight and requirement of a pipe threader. It remains in common use for repair of existing \"galv\" systems and to satisfy building code non-combustibility requirements typically found in hotels, apartment buildings and other commercial applications. It is also extremely durable and resistant to mechanical abuse. Black lacquered steel pipe is the most widely used pipe material for fire sprinklers and natural gas.\n\nMost typical single family home systems won't require supply piping larger than due to expense as well as steel piping's tendency to become obstructed from internal rusting and mineral deposits forming on the inside of the pipe over time once the internal galvanizing zinc coating has degraded. In potable water distribution service, galvanized steel pipe has a service life of about 30 to 50 years, although it is not uncommon for it to be less in geographic areas with corrosive water contaminants.\n\nCopper pipe and tubing was widely used for domestic water systems in the latter half of the twentieth century. Demand for copper products has fallen due to the dramatic increase in the price of copper, resulting in increased demand for alternative products including PEX and stainless steel.\n\nPlastic pipe is in wide use for domestic water supply and drain-waste-vent (DWV) pipe. Principal types include:\nPolyvinyl chloride (PVC) was produced experimentally in the 19th century but did not become practical to manufacture until 1926, when Waldo Semon of BF Goodrich Co. developed a method to plasticize PVC, making it easier to process. PVC pipe began to be manufactured in the 1940s and was in wide use for Drain-Waste-Vent piping during the reconstruction of Germany and Japan following WWII. In the 1950s, plastics manufacturers in Western Europe and Japan began producing acrylonitrile butadiene styrene (ABS) pipe. The method for producing cross-linked polyethylene (PEX) was also developed in the 1950s. Plastic supply pipes have become increasingly common, with a variety of materials and fittings employed.\n\nPresent-day water-supply systems use a network of high-pressure pumps, and pipes in buildings are now made of copper, brass, plastic (particularly cross-linked polyethylene called PEX, which is estimated to be used in 60% of single-family homes), or other nontoxic material. Due to its toxicity, most cities moved away from lead water-supply piping by the 1920s in the United States, although lead pipes were approved by national plumbing codes into the 1980s, and lead was used in plumbing solder for drinking water until it was banned in 1986. Drain and vent lines are made of plastic, steel, cast-iron, or lead.\n\nIn addition to lengths of pipe or tubing, pipe fittings are used in plumbing systems, such as valves, elbows, tees, and unions. Pipe and fittings are held in place with pipe hangers and strapping.\n\nPlumbing fixtures are exchangeable devices using water that can be connected to a building's plumbing system. They are considered to be \"fixtures\", in that they are semi-permanent parts of buildings, not usually owned or maintained separately. Plumbing fixtures are seen by and designed for the end-users. Some examples of fixtures include water closets (also known as toilets), urinals, bidets, showers, bathtubs, utility and kitchen sinks, drinking fountains, ice makers, humidifiers, air washers, fountains, and eye wash stations.\n\nThreaded pipe joints are sealed with thread seal tape or pipe dope. Many plumbing fixtures are sealed to their mounting surfaces with plumber's putty.\n\nPlumbing equipment includes devices often hidden behind walls or in utility spaces which are not seen by the general public. It includes water meters, pumps, expansion tanks, back flow preventers, water filters, UV sterilization lights, water softeners, water heaters, heat exchangers, gauges, and control systems.\n\nThere are many tools a plumber needs to do a good plumbing job. While many simple plumbing tasks can be completed with a few common hand held tools, other more complex jobs require specialised tools, designed specifically to make the job easier.\n\nSpecialized plumbing tools include pipe wrenches, flaring pliers, pipe vise, pipe bending machine, pipe cutter, dies, and joining tools such as soldering torches and crimp tools. New tools have been developed to help plumbers fix problems more efficiently. For example, plumbers use video cameras for inspections of hidden leaks or problems, they use hydro jets, and high pressure hydraulic pumps connected to steel cables for trench-less sewer line replacement.\n\nFlooding from excessive rain or clogged sewers may require specialized equipment, such as a heavy duty pumper truck designed to vacuum raw sewage.\n\nBacteria have been shown to live in \"premises plumbing systems\". The latter refers to the \"pipes and fixtures within a building that transport water to taps after it is delivered by the utility\". Community water systems have been known for centuries to spread waterborne diseases like typhoid and cholera, however \"opportunistic premises plumbing pathogens\" have been recognized only more recently; Legionella pneumophila discovered in 1976, Mycobacterium avium, and Pseudomonas aeruginosa are the most commonly tracked bacteria, which people with depressed immunity can inhale or ingest and may become infected with.\nThese opportunistic pathogens can grow for example in faucets, shower heads, water heaters and along pipe walls. Reasons that favor their growth are \"high surface-to-volume ratio, intermittent stagnation, low disinfectant residual, and warming cycles\". A high surface-to-volume ratio, i.e. a relatively large surface area allows the bacteria to form a biofilm, which protects them from disinfection.\n\nMuch of the plumbing work in populated areas is regulated by government or quasi-government agencies due to the direct impact on the public's health, safety, and welfare. Plumbing installation and repair work on residences and other buildings generally must be done according to plumbing and building codes to protect the inhabitants of the buildings and to ensure safe, quality construction to future buyers. If permits are required for work, plumbing contractors typically secure them from the authorities on behalf of home or building owners.\n\nIn Australia, the national governing body for plumbing regulation is the Australian Building Codes Board. They are responsible for the creation of the National Construction Code (NCC), Volume 3 of which, the Plumbing Regulations 2008 and the Plumbing Code of Australia, pertains to plumbing. \n\nEach Government at the state level has their own Authority and regulations in place for licensing plumbers. They are also responsible for the interpretation, administration and enforcement of the regulations outlined in the NCC. These Authorities are usually established for the sole purpose of regulating plumbing activities in their respective states/territories. However, several state level regulation acts are quite outdated, with some still operating on local policies introduced more than a decade ago. This has led to an increase in plumbing regulatory issues not covered under current policy, and as such, many policies are currently being updated to cover these more modern issues. The updates include changed to the minimum experience and training requirements for licensing, additional work standards for new and more specific kinds of plumbing, as well as adopting the Plumbing Code of Australia into state regulations in an effort to standardise plumbing regulations across the country.\n\nIn the United Kingdom the professional body is the Chartered Institute of Plumbing and Heating Engineering (educational charity status) and it is true that the trade still remains virtually ungoverned; there are no systems in place to monitor or control the activities of unqualified plumbers or those home owners who choose to undertake installation and maintenance works themselves, despite the health and safety issues which arise from such works when they are undertaken incorrectly; see \"Health Aspects of Plumbing (HAP)\" published jointly by the World Health Organization (WHO) and the World Plumbing Council (WPC). WPC has subsequently appointed a representative to the World Health Organization to take forward various projects related to Health Aspects of Plumbing.\n\nIn the United States, plumbing codes and licensing are generally controlled by state and local governments. At the national level, the Environmental Protection Agency has set guidelines about what constitutes lead-free plumbing fittings and pipes, in order to comply with the Safe Drinking Water Act.\n\nSome widely used Standards in the United States are:\n\n\n "}
{"id": "37012601", "url": "https://en.wikipedia.org/wiki?curid=37012601", "title": "Real Fábrica de Cristales de La Granja", "text": "Real Fábrica de Cristales de La Granja\n\nReal Fábrica de Cristales de La Granja (\"Royal Factory of Glass and Crystal of La Granja\") was a Spanish royal manufacturing factory built in San Ildefonso (in Segovia, south east on the N 601), in the eighteenth century.\n\nIt was established in 1727 by Philip V of Spain. In that year, funded by the crown, the Catalan artisan Buenaventura Sit installed a small oven which manufactured float glass for the windows and mirrors of the Royal Palace of La Granja de San Ildefonso, which was under construction in the 1720s. Sit had previously worked at Nuevo Baztan where a glass factory failed because of inadequate fuel supplies. At La Granja there was an abundant supply of wood for the factory in the Sierra de Guadarrama.\n\nBartolome Sureda y Miserol, previously director of the Real Fabrica de Porcelana del Buen Retiro, the Real Fabrica de Pano in Guadalajara, and the Real Fabrica de Loza de la Moncloa, became director of the Real Fábrica de Cristales de La Granja in 1822. Glass blowing and glassware production could be viewed at the factory. The wares of the royal factory were exported to the Americas, which caused financial losses to the other countries who exported as well. By 1836, with the royal factory experiencing financial hardship, the Royal Treasury formally took over the facility which, unlike other royal factories, failed to financially support itself.\n\nTo revive the traditions of the Royal Glass Factory, the National Glass Centre Foundation was established in 1982 in the eighteenth-century building. The Ministerial Order of 1989 was formalized by law in 1994, its basic objective being “the promotion, development, education, research and dissemination of craftsmanship and history of glass manufacture artistic and other cultural and scientific activities related to art and art glass.”\n\n"}
{"id": "4770106", "url": "https://en.wikipedia.org/wiki?curid=4770106", "title": "Redheads (matches)", "text": "Redheads (matches)\n\nRedheads is an Australian brand of matches, originally manufactured by Bryant and May in Richmond, Victoria, but now manufactured in Sweden by Swedish Match. It is Australia's top-selling match brand.\n\nReadheads were first produced by Bryant & May in Australia in 1909. The Redhead name refers to the red striking-heads of the matches, which were first sold in Australia in 1946. The logo on the matchbox depicts the head and left shoulder of a redheaded woman, and has had four major updates since its introduction, with a number of special issues also produced.\n\n"}
{"id": "1953581", "url": "https://en.wikipedia.org/wiki?curid=1953581", "title": "Redundancy (engineering)", "text": "Redundancy (engineering)\n\nIn engineering, redundancy is the duplication of critical components or functions of a system with the intention of increasing reliability of the system, usually in the form of a backup or fail-safe, or to improve actual system performance, such as in the case of GNSS receivers, or multi-threaded computer processing.\n\nIn many safety-critical systems, such as fly-by-wire and hydraulic systems in aircraft, some parts of the control system may be triplicated, which is formally termed triple modular redundancy (TMR). An error in one component may then be out-voted by the other two. In a triply redundant system, the system has three sub components, all three of which must fail before the system fails. Since each one rarely fails, and the sub components are expected to fail independently, the probability of all three failing is calculated to be extraordinarily small; often outweighed by other risk factors, such as human error. Redundancy may also be known by the terms \"majority voting systems\" or \"voting logic\".\n\nRedundancy sometimes produces less, instead of greater reliability it creates a more complex system which is prone to various issues, it may lead to human neglect of duty, and may lead to higher production demands which by overstressing the system may make it less safe.\n\nIn computer science, there are four major forms of redundancy, these are:\n\nA modified form of software redundancy, applied to hardware may be:\n\nStructures are usually designed with redundant parts as well, ensuring that if one part fails, the entire structure will not collapse. A structure without redundancy is called fracture-critical, meaning that a single broken component can cause the collapse of the entire structure. Bridges that failed due to lack of redundancy include the Silver Bridge and the Interstate 5 bridge over the Skagit River.\n\nParallel and combined systems demonstrate different level of redundancy. The models are subject of studies in reliability and safety engineering.\n\nThe two functions of redundancy are passive redundancy and active redundancy. Both functions prevent performance decline from exceeding specification limits without human intervention using extra capacity.\n\nPassive redundancy uses excess capacity to reduce the impact of component failures. One common form of passive redundancy is the extra strength of cabling and struts used in bridges. This extra strength allows some structural components to fail without bridge collapse. The extra strength used in the design is called the margin of safety.\n\nEyes and ears provide working examples of passive redundancy. Vision loss in one eye does not cause blindness but depth perception is impaired. Hearing loss in one ear does not cause deafness but directionality is impaired. Performance decline is commonly associated with passive redundancy when a limited number of failures occur.\n\nActive redundancy eliminates performance declines by monitoring the performance of individual devices, and this monitoring is used in voting logic. The voting logic is linked to switching that automatically reconfigures the components. Error detection and correction and the Global Positioning System (GPS) are two examples of active redundancy.\n\nElectrical power distribution provides an example of active redundancy. Several power lines connect each generation facility with customers. Each power line includes monitors that detect overload. Each power line also includes circuit breakers. The combination of power lines provides excess capacity. Circuit breakers disconnect a power line when monitors detect an overload. Power is redistributed across the remaining lines.\n\nCharles Perrow, author of \"Normal Accidents\", has said that sometimes redundancies backfire and produce less, not more reliability. This may happen in three ways: First, redundant safety devices result in a more complex system, more prone to errors and accidents. Second, redundancy may lead to shirking of responsibility among workers. Third, redundancy may lead to increased production pressures, resulting in a system that operates at higher speeds, but less safely.\n\nVoting logic uses performance monitoring to determine how to reconfigure individual components so that operation continues without violating specification limitations of the overall system. Voting logic often involves computers, but systems composed of items other than computers may be reconfigured using voting logic. Circuit breakers are an example of a form of non-computer voting logic.\n\nElectrical power systems use power scheduling to reconfigure active redundancy. Computing systems adjust the production output of each generating facility when other generating facilities are suddenly lost. This prevents blackout conditions during major events such as an earthquake.\n\nThe simplest voting logic in computing systems involves two components: primary and alternate. They both run similar software, but the output from the alternate remains inactive during normal operation. The primary monitors itself and periodically sends an activity message to the alternate as long as everything is OK. All outputs from the primary stop, including the activity message, when the primary detects a fault. The alternate activates its output and takes over from the primary after a brief delay when the activity message ceases. Errors in voting logic can cause both outputs to be active or inactive at the same time, or cause outputs to flutter on and off.\n\nA more reliable form of voting logic involves an odd number of three devices or more. All perform identical functions and the outputs are compared by the voting logic. The voting logic establishes a majority when there is a disagreement, and the majority will act to deactivate the output from other device(s) that disagree. A single fault will not interrupt normal operation. This technique is used with avionics systems, such as those responsible for operation of the Space Shuttle.\n\nEach duplicate component added to the system decreases the probability of system failure according to the formula:-\n\nwhere:\n\nThis formula assumes independence of failure events. That means that the probability of a component B failing given that a component A has already failed is the same as that of B failing when A has not failed. There are situations where this is unreasonable, such as using two power supplies connected to the same socket in such a way that if one power supply failed, the other would too.\n\nIt also assumes that at only one component is needed to keep the system running.\n\n"}
{"id": "2585120", "url": "https://en.wikipedia.org/wiki?curid=2585120", "title": "ReserVec", "text": "ReserVec\n\nReserVec was a computerized reservation system developed by Ferranti Canada for Trans-Canada Airlines (TCA, today's Air Canada) in the late 1950s. It appears to be the first such system ever developed, predating the more famous SABRE system in the US by about two years. Although Ferranti had high hopes that the system would be used by other airlines, no further sales were forthcoming and development of the system ended. Major portions of the transistor-based circuit design were put to good use in the Ferranti-Packard 6000 computer, which would later go on to see major sales in Europe as the ICT 1904.\n\nIn the early 1950s the airline industry was undergoing explosive growth. A serious limiting factor was the time taken to make a single booking, which could take upwards of 90 minutes in total. TCA found their bookings typically involved between three and seven calls to the centralized booking centre in Toronto, where telephone operators would scan flight status displayed on a huge board showing all scheduled flights one month into the future. Bookings past that time could not be made, nor could an agent reliably know anything other than if the flight was full or not – to book two seats was much more complex, requiring the operator to find the \"flight card\" for that flight in a filing cabinet.\n\nIn 1946 American Airlines decided to tackle this problem through automation, introducing the Reservisor, a simple electromechanical computer based on telephone switching systems. Newer versions of the Reservisor included magnetic drum systems for storing flight information further into the future. The ultimate version of the system, the Magnetronic Reservisor, was installed in 1956 and could store data for 2,000 flights a day up to one month into the future. Reservisors were later sold to a number of airlines, as well as Sheraton for hotel bookings, and Goodyear for inventory control.\n\nTCA was aware of the Reservisor, but was unimpressed by its limited capabilities in terms of information it could store, and even more by the failure rate which was essentially \"constant\". Nor did the Reservisor really change the way the reservations system worked; ticket agents still had to call central booking and talk (typically through an intermediary) to a Reservisor operator to answer queries.\n\nTCA asked one of their communications engineers, Lyman Richardson, to study the booking problem, and he quickly came to the opinion that a computerized solution was the only one worth studying. TCA then entered into an agreement to build a prototype system on the University of Toronto's FERUT computer, a surplus Manchester Mark 1 computer they had received in 1952 when the UK's nuclear weapons laboratories had to abandon it after budget cuts.\n\nThe FERUT-based system was demonstrated in 1953 and was a qualified success; while the programmed logic and data storage/retrieval worked well, input/output was a serious bottleneck that seemed to make the system no better than the mechanical Reservisor. Furthermore the Ferut was vacuum tube based, and thus no more reliable than the Reservisor, TCA's major concern prior to the experiment.\n\nRichardson was convinced that the basic concept was sound, and formed a team of himself and several engineers from the university's Computation Center, operating under the aegis of Adalia Ltd., a consulting firm set up by Robert Watson-Watt of radar fame when he moved to Montreal at the end of World War II. They became involved with the newly-forming electronics group at Ferranti Canada, who felt they had a solution to the input/output and reliability problems.\n\nFerranti proposed a new \"transactor\" (terminal) that used a new punched card system. Booking agents at the ticketing offices marked the cards with pencil, for various checkboxes, then inserted it into the transactor which read the marks and punched those codes onto the edge of the card. Cards would then be collected from any number of operators and fed into a normal card reader, which would read them over telephone lines at \"high speed\" directly into the central booking computer. The computer itself would be built using transistor-based logic, thereby eliminating downtime due to tube burnout. Such a system had first been proposed in order to improve the reliability of the DATAR system Ferranti had built for the Canadian Navy, and they were convinced of its practicality.\n\nTCA was interested, and provided $75,000 for the construction of six prototype transactors. In 1957 these were attached to FERUT over telephone lines and the experimental booking program run again. The demonstration was a complete success; users could quickly feed in requests and Ferut was able to book, change, query and cancel flights at speeds that made the Reservisor look terribly slow.\n\nThere was some further development and planning, but in 1959 TCA placed a $2 million ($12 million in year-2000 dollars) contract for a deployment system consisting of 350 transactors and all the communications equipment to support them in the field. Ferranti also won the contract for the computer system itself, although IBM had also been considered. The new machine was based on a 25-bit word, using one bit for parity checking and 24 bits for data, and was equipped with 4,096 words of core memory, later expanded to 8,192 words. Storage consisted of five magnetic drums (one was a spare) with 32,768 25-bit words each, and six tape units. Simple load balancing software routed requests across two CPUs, known as Castor and Pollux, the computer as a whole thus becoming Gemini. An internal TCA contest in late 1960 to name the system as a whole resulted in ReserVec for \"Reservations Electronically Controlled\".\n\nInstallation of the transactors started in April 1961, followed by the computer in the Toronto booking office in August. The system was brought up for testing on October 18th, 1961, connecting additional ticketting offices as the transactors were installed over the next year. By August 1962 the system was complete, and the switch-over from the manual systems to ReserVec was completed on January 24, 1963. Use of ReserVec reduced the head count at the booking office from 230 to 90, and allowed for the sale of thousands of telephone lines formerly needed to reach the human operators. Total turnaround from request to response could be as short as a second, although under load it might drop to two seconds at the worst. The system as a whole could process 10 transactions a second.\n\nIt is interesting to compare the system with SABRE, being deployed at about the same time by American Airlines. SABRE was first started as an experimental effort in 1953, and a formal development contract signed in 1957. The system was first turned on in 1960, and took over booking functions in December 1964. So while the two projects started at the same time, ReserVec was completed almost two years earlier. While the ReserVec cost $4 million, SABRE was ten times that. Equally interesting is that while the SABRE CPU was about ten times faster, ReserVec handled 80-100,000 transactions a day with a maximum two second delay, while SABRE handled only 26,000 with delays of up to three seconds.\n\nUnlike SABRE, however, ReserVec did not store passenger information, which had to be processed manually. In order to address this need, TCA added a second system known as Pioneer, which could link ReserVec's three letter passenger codes with the full passenger records held on a Burroughs D-82 computer (originally designed for US military use). Pioneers were installed only in the Toronto and Montreal offices, smaller offices continued using paper records for user info.\n\nReserVec ran all of TCA's reservations for nine years, with an average downtime of only 120 seconds a year. Originally designed for only 60,000 transactions a day, it was already processing 80 to 100,000 when it was first turned on, and over 600,000 by 1970. Retroactively named ReserVec I, the system was finally replaced at the end of 1970 by a new Univac-based system known as ReserVec II, which featured small computer terminals replacing the punched card systems.\n\nFerranti, now as Ferranti-Packard, tried to sell the machine as-is to other airlines. The US market seemed to be entirely wrapped up by IBM and Univac, but there was no comparable system in Europe, where a number of airlines were looking at the US developments with interest. Gemini could be sold directly into Europe by Ferranti's existing UK-based sales force. Instead, the UK headquarters decided to build their own solution from scratch. In the end, the UK system would never be delivered; it was still being developed when Ferranti decided to sell off their entire computer division after years of losses.\n\nNevertheless, the work did not go to waste. The engineering team convinced Canadian management to support the development of a business computer aimed at the low-end of the mainframe market. They expanded the ReserVec system with additional hardware to directly support multitasking and various changes to make the system highly modular, making it more attractive to a wider variety of users. Sales of the new Ferranti-Packard 6000 were just starting when the UK headquarters used the design to sweeten the deal when selling off their UK computer divisions, handing the design to the ICT who took over production. This was much to the chagrin of the Canadian staff, most of whom quit. The FP-6000 became the ICT 1904, one of a line of similar machines which sold over 3,000 during the 1960s and 70s.\n\n"}
{"id": "6051606", "url": "https://en.wikipedia.org/wiki?curid=6051606", "title": "Safety harness", "text": "Safety harness\n\nA safety harness is a form of protective equipment designed to protect a person, animal, or object from injury or damage. The harness is an attachment between a stationary and non-stationary object and is usually fabricated from rope, cable or webbing and locking hardware. Some safety harnesses are used in combination with a shock absorber, which is used to regulate deceleration when the end of the rope is reached. One example would be bungee jumping.\n\nIn North America, Safety Harness for protection against falls from heights in industrial and construction activities are covered by design performance standards issued by ANSI (American National Standards Institute) in the United States and by CSA (Canadian Standards Association) in Canada. Specifically, the standards issued are ANSI Z359.1 and CSA Z259.10. These standards are updated approximately every four to five years so it is important to ensure the latest version is referenced.\n\nFall Protection Systems\n\nListed below are different types of fall safety equipment and their recommended usage. \n\nClass 1 Body belts (single or double D-ring) are designed to restrain a person in a hazardous work position to prevent fall or to arrest a fall completely within 3 foot of movement (OSHA). Amends must be made to keep the line rigid at all times. A harness should also be used.\n\nClass 2 Chest harnesses are used when there are only limited fall hazards (no vertical free fall hazard), or for retrieving persons such as removal of persons from a tank or a bin. \n\nClass 3 Full body harnesses are designed to arrest the most severe free falls. \n\nClass 4 Suspension belts are independent work supports used to suspend a worker, such as boatswain's chairs or raising or lowering harnesses . \nSafety harness types include:\n\nOccupations that may involve the use of safety harnesses include:\n\nJones & Bartlett. Fire Fighter Skills. 2nd ed. Boston, Toronto, London, Singapore: Jones and Bartlett Publishers, 2009. pp243-244. Print.\n"}
{"id": "31084320", "url": "https://en.wikipedia.org/wiki?curid=31084320", "title": "Shifra Smart Homes", "text": "Shifra Smart Homes\n\nShifra Smart Homes (Arabic: شيفرة للمنازل الذكية )is a home automation provider based out of Dubai in the United Arab Emirates. Shifra Smart Homes provide control systems for lighting, appliances, air conditioning, motorized curtains, Infrared systems, security, video surveillance, multi-room audio, and multi-room video and home theater systems.\n\nShufra Smart Homes provides wireless smart home solutions in the middle east catered to individual residential villas, luxury yachts, as well as real estate developments. The majority of their systems rely on the Z-Wave wireless standard which has characteristics of being an inter-operable, secure, 2-way, wireless mesh based technology created by the Z-Wave Alliance and owned by Sigma Designs.\n\nShifra Smart Homes's main vendors include Honeywell for lighting and appliance control, for curtain control, Global Cache for infrared and RS-232 based control systems, Sonos for multi-room audio, and Acer for Media Center based multimedia solutions\n\n\n"}
{"id": "10746300", "url": "https://en.wikipedia.org/wiki?curid=10746300", "title": "Siemens Gamesa Renewable Energy", "text": "Siemens Gamesa Renewable Energy\n\nSiemens Gamesa Renewable Energy , formerly Gamesa Corporación Tecnológica () and Grupo Auxiliar Metalúrgico, is a Spanish manufacturing company principally involved in the fabrication of wind turbines and the construction of wind farms.\n\nIt was formerly headquartered in Vitoria-Gasteiz, 40 km south of Bilbao, in northern Spain. 2010 it moved to Zamudio, Biscay, a suburb of Bilbao. Gamesa develops, manages and sells wind farms, for which it also supplies wind turbines. It is the market leader in Spain and the fourth largest wind turbine manufacturer in the world (2011). It manufactures the SG7 turbine one of the most powerful turbines in the World.\n\nIn 2016 Siemens Wind Power and Gamesa reached agreement on a 59:41 merger of their wind turbine businesses.\n\nGamesa began operations in 1976 focused at that time on developing new technologies and applying them to emerging activities. These included robotics, microelectronics, aeronautics and the development of composite materials. In 1994, Gamesa Eólica was created as a subsidiary specializing in the manufacture of wind turbines. The company became involved in the development, construction and operations of wind farms in 1995 and completed its first wind farm the following year. Gamesa had a 7-year partnership with Vestas that ended in 2002.\n\nThe Corporation was officially listed on the stock exchange on 31 October 2000 and joined the selective IBEX 35 on 24 April 2001.\n\nSince 2006, the company has focused on technologies associated with sustainable energy, principally wind power. It has divested of its interests in aeronautics, which were sold off to form a new company known as \"Aernnova\", and in services, which were sold off to form a new company known as \"Global Energy Services.\"\n\nAs part of the United Kingdom's move to expand its production of offshore wind energy production, Gamesa has committed to the expenditure of £133.7 million on a production factory and other facilities in the UK, and will also move its offshore wind division headquarters to London.\n\nIn January 2014, Gamesa and French nuclear manufacturer Areva announced a preliminary deal to create a joint venture (Adwen) in the offshore wind power business.\n\nOn 17 July 2016 Siemens AG and Gamesa announced they were to merge their wind businesses, with the two operations forming 59% (Siemens Wind) and 41% (Gamesa) of the resulting company's shareholding, with Siemens offering €3.75 per Gamesa share. The resultant company was to be headquartered in Spain, with an offshore operations headquartered in Hamburg, Germany and Vejle, Denmark. Siemens was reported to have paid €1 billion cash for Gamesa shares. Cost savings between duplicated functions in the two businesses was expected by Gamesa to save c. €230 million in the first year of operation. The combined business would be the largest wind turbine manufacturer worldwide by installed capacity (c. 69GW), exceeding Vestas and GE.\n\nSiemens Gamesa is listed on the Dow Jones Sustainability Index, the FTSE4Good Index which is concerned with corporate social responsibility on the KLD Global Climate 100 Index, and on the Global 100 Index of the 100 most sustainable companies in the world.\n\nIn the US, Gamesa is involved in the construction of the Allegheny Ridge Wind Farm and the Mt Stuart Wind Farm.\n\n"}
{"id": "28339996", "url": "https://en.wikipedia.org/wiki?curid=28339996", "title": "Spaceworthiness", "text": "Spaceworthiness\n\nSpaceworthiness, or aerospaceworthiness, is a property, or ability of a spacecraft to perform to its design objectives and navigate successfully through both the space environment and the atmosphere as a part of a journey to or from space.\n\nAs in airworthiness, the spaceworthiness of a spacecraft depends on at least three basic components:\n\n\nSpaceworthiness is typically maintained through a maintenance program and / or a system of analysis, diagnosis and management of health and reliability of the spacecraft.\n\nSpaceworthiness of launch vehicles and spacecraft is an extension of the concepts of seaworthiness, for boats and ships, and of airworthiness, for aircraft.\n\n"}
{"id": "17096722", "url": "https://en.wikipedia.org/wiki?curid=17096722", "title": "Stirrup jar", "text": "Stirrup jar\n\nA stirrup jar is a style of pottery vessel which originated during the Bronze Age. Such vessels were sometimes decorated with painted designs or ornamentation. Early examples of the stirrup jar have been recovered from Rhodes dating to c. 1200 BC. A number of inscribed Kydonian stirrup jars have been recovered from several archaeological sites on Crete.\n\nStirrup jars were originally intended for use as containers for oil and wine. The early stirrup jars were sparsely decorated and featured a three handled arrangement, disc hole, spout horns, and the shape of a false neck and spout. Traditionally stirrup jars were divided into two versions: a small, fine ware version suggesting a specialized use and a large, coarse ware version.\n\nThe features found suggest that the stirrup jar could have been economically valuable. The arrangement of the stirrup jar suggests a stopper was used to secure the contents. The disc holes and third handle may have been used to secure a tag to the vessel suggesting a commercial importance and resale value. The locations where stirrup jars have been found in reflect the fact that the popularity of this vessel type spread quickly throughout the Aegean and the use of the stirrup jar to identify a specific commodity became important.\n\n\n"}
{"id": "2675737", "url": "https://en.wikipedia.org/wiki?curid=2675737", "title": "SubRip", "text": "SubRip\n\nSubRip is a free software program for Windows which \"rips\" (extracts) subtitles and their timings from video. It is free software, released under the GNU GPL. \"SubRip\" is also the name of the widely used and broadly compatible subtitle text file format created by this software.\n\nUsing optical character recognition, SubRip can extract from live video, video files and DVDs, then record the extracted subtitles and timings as a \"Subrip format\" text file. It can optionally save the recognized subtitles as bitmaps for later subtraction (erasure) from the source video.\n\nIn practice, SubRip is configured with the correct codec for the video source, then trained by the user on the specific text area, fonts, styles, colors and video processing requirements to recognize subtitles. After trial and fine tuning, SubRip can automatically extract subtitles for the whole video source file during its playback. SubRip records the beginning and end times and text for each subtitle in the output text codice_1 file.\n\nSubRip uses AviSynth to extract video frames from source video, and can rip subtitles from all video files supported by that program.\n\nThe SubRip file format, as reported on the Matroska multimedia container format website, is \"perhaps the most basic of all subtitle formats.\" SubRip (SubRip Text) files are named with the extension codice_1, and contain formatted lines of plain text in groups separated by a blank line. Subtitles are numbered sequentially, starting at 1. The timecode format used is hours:minutes:seconds,milliseconds with time units fixed to two zero-padded digits and fractions fixed to three zero-padded digits (00:00:00,000). The fractional separator used is the comma, since the program was written in France.\n\nExample:\n\nUnofficially the format has very basic text formatting, which can be either interpreted or passed through for rendering depending on the processing application. Formatting is derived from HTML tags for bold, italic, underline and color:\nNested tags are allowed; some implementations prefer whole-line formatting only.\n\nThe SubRip codice_1 file format is supported by most software video players listed in \"Comparison of video player software\". For Windows software video players that do not support subtitle playback directly, the VSFilter DirectX filter displays SubRip and other subtitle formats.\nThe SubRip format is supported directly by many subtitle creation/editing tools,\nand some hardware home media players.\nIn August 2008, YouTube added subtitle support to its Flash video player under the \"Closed Captioning\" option - content producers can upload subtitles in SubRip format.\n\nA format originally called WebSRT (Web Subtitle Resource Tracks) was as of October 2010 being specified by the Web Hypertext Application Technology Working Group for the proposed HTML5 codice_5 element. It shared the codice_1 file extension and was \"broadly based on\" (parts of) the SubRip format, but was not fully compatible with SubRip.\nThe prospective format was later renamed WebVTT (Web Video Text Track).\nGoogle's Chrome and Microsoft's Internet Explorer 10 browsers were the first to support tags with WebVTT files for HTML5 videos. Mozilla Firefox implemented WebVTT in its nightly builds (Firefox 24), and as of Firefox 31 (July 24, 2014), Mozilla enabled WebVTT on Firefox by default. The feature had to be enabled in Firefox by going to the \"about:config\" page and setting the value of \"media.webvtt.enabled\" to true. YouTube began supporting WebVTT in April, 2013.\n\nSubRip's default output encoding is configured as Windows-1252. However, output options are also given for many Windows code pages as well Unicode encodings, such as UTF-8 and UTF-16, with or without Byte Order Mark (BOM). Therefore, there's no de facto character encoding standard for codice_1 files, which means that any SubRip file parser must attempt to use Charset detection. Unicode Byte Order Mark (BOM) are typically used to aid detection. \n\nA number of embedded hardware-based players only have support for ASCII or Western European fonts due to the licensing costs associated with the commercial fonts used .\n\n\n\n"}
{"id": "41649516", "url": "https://en.wikipedia.org/wiki?curid=41649516", "title": "Techreturns", "text": "Techreturns\n\nTechreturns Nederland BV is a Dutch company founded with the aim to reduce electronic waste (e-waste). It does this by buying used electronics, especially mobile phones, from consumers and companies to repair them and give them a second life. Some choose to donate mobile phone through organisations such as Masterpeace and Artis Zoo. The company's revenues comes from selling the second hand devices mainly in Asia and Africa where there is a market for affordable, quality electronics. Sales also take place in Europe, especially in the Netherlands through its sister company BeatsNew.\n\nTechreturns is connected to Closing the Loop a foundation collecting e-waste in developing countries for recycling. Involved in this project are also the organisations Fairphone and Text to Change.\n\nRecently, Techreturns has appeared multiple times in the Dutch media, for instance in a discussion about sustainability initiatives by the Dutch government, an annual summary of Dutch sustainable businesses 2013 by MVO Netherlands and in the consumer television programme Kassa on 14-01-14 in a segment about reuse versus recycling in the electronic industry.\n\nTechreturns was founded in 2009, is based in Amsterdam, Netherlands and is a part of Social Enterprise Nederland. Additionally Techreturns has a department in Genova, Italy.\n"}
{"id": "1038665", "url": "https://en.wikipedia.org/wiki?curid=1038665", "title": "Tension-leg platform", "text": "Tension-leg platform\n\nA tension-leg platform (TLP) or extended tension leg platform (ETLP) is a vertically moored floating structure normally used for the offshore production of oil or gas, and is particularly suited for water depths greater than 300 metres (about 1000 ft) and less than 1500 metres (about 4900 ft). Use of tension-leg platforms has also been proposed for wind turbines.\n\nThe platform is permanently moored by means of tethers or tendons grouped at each of the structure's corners. A group of tethers is called a tension leg. A feature of the design of the tethers is that they have relatively high axial stiffness (low elasticity), such that virtually all vertical motion of the platform is eliminated. This allows the platform to have the production wellheads on deck (connected directly to the subsea wells by rigid risers), instead of on the seafloor. This allows a simpler well completion and gives better control over the production from the oil or gas reservoir, and easier access for downhole intervention operations.\n\nTLPs have been in use since the early 1980s. The first tension leg platform was built for Conoco's Hutton field in the North Sea in the early 1980s. The hull was built in the dry-dock at Highland Fabricator's Nigg yard in the north of Scotland, with the deck section built nearby at McDermott's yard at Ardersier. The two parts were mated in the Moray Firth in 1984.\n\nThe Hutton TLP was originally designed for a service life of 25 years in Nord Sea depth of 100 to 1000 metres. It had 16 tension legs. Its weight varied between 46,500 and 55,000 tons when moored to the seabed, but up to 61,580 tons when floating freely. The total area of its living quarters was about 3,500 square metres and accommodated over a 100 cabins though only 40 people were necessary to maintain the structure in place.\n\nThe hull of the Hutton TLP has been separated from the topsides. Topsides have been redeployed to the Prirazlomnoye field in the Barents Sea, while the hull was reportedly sold to a project in the Gulf of Mexico (although the hull has been moored in Cromarty Firth since 2009).\n\nLarger TLPs will normally have a full drilling rig on the platform with which to drill and intervene on the wells. The smaller TLPs may have a workover rig, or in a few cases no production wellheads located on the platform at all.\n\nThe deepest (E)TLPs measured from the sea floor to the surface are:\n\nAlthough the Massachusetts Institute of Technology and the National Renewable Energy Laboratory explored the concept of TLPs for offshore wind turbines in September 2006, architects had studied the idea as early as 2003. Earlier offshore wind turbines cost more to produce, stood on towers dug deep into the ocean floor, were only possible in depths of at most , and generated 1.5 megawatts for onshore units and 3.5 megawatts for conventional offshore setups. In contrast, TLP installation was calculated to cost a third as much. TLPs float, and researchers estimate they can operate in depths between 100 and and farther away from land, and they can generate 5.0 megawatts.\n\nComputer simulations project that in a hurricane TLPs would shift 0.9 m to 1.8 m and the turbine blades would cycle above wave peaks. MIT and NREL researchers say dampers could be used to reduce motion in the event of a natural disaster.\n\nMIT and NREL researchers plan to install a half-scale prototype south of Cape Cod. Sclavounos said, \"We'd have a little unit sitting out there to show that this thing can float and behave the way we're saying it will.\"\n\n\n"}
{"id": "31533319", "url": "https://en.wikipedia.org/wiki?curid=31533319", "title": "Toe board", "text": "Toe board\n\nA roofing toe board is one of the most basic pieces of safety equipment a roofer can use. A toe board is a long piece of 2 inch x 4 inch (a 2x4) wood nailed horizontally along a roof in various places. \n\nMost roofers work in a variety of weather conditions, sometimes severe heat, and resist wearing an apparatus such as a safety harness. As a result of needing both an uncumbered work environment and the need to stay as cool as possible, roofers prefer the toe board due to its freeness of movement. If an accident happens and a roofer loses his/her footing, the 2x4 would stop the roofer from sliding down and/or off the roof.\n\nMore deaths occur in falls than for any other reason in the construction profession.\n\nMore generally, a toe board is a small vertical barrier attached to a raised floor or raised platform. A toe board is like a tiny wall - usually between 4 and 12 inches - whose purpose is to prevent objects or people from falling over, or rolling over, the side of a raised platform, such as preventing a screwdriver dropped on the floor of elevated construction scaffolding from rolling off the side onto people or objects below.\n"}
