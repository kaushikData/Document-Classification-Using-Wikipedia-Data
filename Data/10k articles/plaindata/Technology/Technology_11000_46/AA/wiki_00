{"id": "19813519", "url": "https://en.wikipedia.org/wiki?curid=19813519", "title": "AS 2805", "text": "AS 2805\n\nAS 2805 \"Electronic funds transfer - Requirements for interfaces\" is the Australian standard for financial messaging. It is near-exclusively used in Australia for the operation of card-based financial transactions among banks, automatic teller machines and EFTPOS devices.\n\nIt is closely related to ISO 8583, but pre-dates it by two years (1985 vs 1987).\n\n"}
{"id": "25899022", "url": "https://en.wikipedia.org/wiki?curid=25899022", "title": "Accelerated life testing", "text": "Accelerated life testing\n\nAccelerated life testing is the process of testing a product by subjecting it to conditions (stress, strain, temperatures, voltage, vibration rate, pressure etc.) in excess of its normal service parameters in an effort to uncover faults and potential modes of failure in a short amount of time. By analyzing the product's response to such tests, engineers can make predictions about the service life and maintenance intervals of a product.\n\nIn polymers, testing may be done at elevated temperatures to produce a result in a shorter amount of time than it could be produced at ambient temperatures. Many mechanical properties of polymers have an Arrhenius type relationship with respect to time and temperature (for example, creep, stress relaxation, and tensile properties). If one conducts short tests at elevated temperatures, that data can be used to extrapolate the behavior of the polymer at room temperature, avoiding the need to do lengthy, and hence expensive tests.\n\nALT is primarily used to speed up tests. This is particularly useful in several cases: \nFor instance, a reliability test on circuits that must last years at use conditions (high longevity) would need to yield results in a much shorter time. If the test wanted to estimate how frequently the circuits needed to be replaced, then the category of low failure would also be applicable. Furthermore, if the circuits wore out from gradual use rather than extreme use (such as a large sudden shock), the wear out category would be involved. If a sudden shock was the primary cause of failure, a Highly Accelerated Life Test may be more appropriate.\n\nDesigning a test involves considering what factors affect the test object, what you already know about the test object's behavior, and what you want to learn from the test.\n\nAll factors thought to influence the test object should be involved and tests should be conducted at various levels of each factor. Higher stress levels will speed up the test more however the cause of failure or other response measured must not be changed. For instance, melting components in a circuit would alter why the circuit failed. Increasing the number of tests or the number of test objects in each test generally increases how precisely one can infer the test object's behavior at operating conditions.\n\nA model is an equation that accurately relates a test object's performance to the levels of stress on it. This can be referred to as an acceleration model, with any constants called acceleration factors. The acceleration model is usually related to the types of materials or components tested. A few equations used for acceleration models are the Arrhenius for temperature, Eyring for temperature and humidity, and the Blattau model for temperature cycling.\n\nWhen the model is known in advance the test only needs to identify the parameters for the model, however it is necessary to ensure that the model being used has been well verified. Established models must show agreement between extrapolations from accelerated data and observed data across a range of stress factors.\n\nWhen the appropriate model is not known in advance, or there exist multiple accepted models, the test must estimate what model fits best based on the context of the test and results from testing. Even if two models fit data at high stresses equally well, they may differ by orders of magnitude at lower stresses. This issue can be approached by more tests at a greater range of stresses however the cause of failure must remain unchanged. A possible pre-experiment approach to minimize this is to estimate what data you expect from testing, fit a model to the data, and determine if one would be able to make reliable conclusions if everything went as expected.\n\nInference from the results of an accelerated life test requires being able to relate the test object’s response (lifespan, corrosion, efficiency, etc...) to the levels of applied stress factors over time.\n\nHow one factors in the effect of time depends largely on what one is measuring. For instance, a test that is measuring lifespan may look only at the mean time to failure of the test objects, or it may try to fit a statistical distribution to the data. This is usually referred to as a life distribution, the probability density function of which represents the proportion of products failing at a given time. Several distributions for this purpose are the exponential, Weibull, log-normal, and gamma distributions. In any case, the parameters would be related to the test subjects and the levels of the stress factors being tested.\n\nAs a simplified example, consider a test object with a life distribution that roughly matches a normal distribution. Tests at various stress levels would yield different values for the mean and standard deviation of the distribution. (its parameters) One would then use a known model or attempt to fit a model to relate how each stress factor influenced the distributions parameters. This relation would then be used to estimate the life distribution at operating conditions.\n\nA step stress ALT is a variant of ALT that tests a component at multiple stress levels, one after the other. Components that survive one test are immediately subjected to the next. These are widely modeled under the assumption that survival life of a product depends only on the current level of stress and how many test subjects have failed so far. Step stress ALT can increment low to high, high to low, or through a mix of levels. A step stress ALT test that is interested in extrapolating a life distribution to constant operating conditions must be able to relate the life distribution observed under changing stresses to one of constant stresses.\n\n"}
{"id": "14848579", "url": "https://en.wikipedia.org/wiki?curid=14848579", "title": "Antenna farm", "text": "Antenna farm\n\nAntenna farm or satellite dish farm or just dish farm are terms used to describe an area dedicated to television or radio telecommunications transmitting or receiving antenna equipment, such as C, K or K band satellite dish antennas, UHF/VHF/AM/FM transmitter towers or mobile cell towers. The history of the term \"antenna farm\" is uncertain, but it dates to at least the 1950s.\n\nIn telecom circles, any area with more than three antennas could be referred to as an antenna farm. In the case of an AM broadcasting station (mediumwave and longwave, occasionally shortwave), the multiple mast radiators may all be part of an antenna system for a single station, while for VHF and UHF the site may be under joint management. Alternatively, a single tower with many separate antennas is often called a \"candelabra tower\".\n\nCommercial antenna farms are managed by radio stations, television stations, satellite teleports or military organizations and are mostly very secure facilities with access limited to broadcast engineers, RF engineers or maintenance technicians. This is not only for the physical security of the location (including preventing equipment/metal theft), but also for safety, as there may be a radiation hazard unless stations are powered-down.\n\nWhere terrain allows, mountaintop sites are very attractive for non-AM broadcast stations and others, because it increases the stations' height above average terrain, allowing them to reach further by avoiding obstructions on the ground, and by increasing the radio horizon. With a clearer line of sight in both cases, more signal can be received. While the same is true of a very tall tower, such towers are expensive, dangerous, and difficult to access the top of, and may collect and drop large amounts of ice in winter, or even collapse in a severe ice storm and/or high winds. Multiple small towers also allow stations to have backup facilities co-located on each other's towers for redundancy.\n\nSatellite antenna farms are usually located at remote locations, far away from urban development, especially high rise buildings or airplane flight paths, to avoid and minimize disruption to transmission and reception, and so as to not be an eyesore. Although most radio and TV stations are in fierce competition with each other in their broadcast markets, they will often locate their broadcasting antennas very near each other, and in some cases, will even share land or towers with each other, in the interests of space, land availability, and the cost of putting a transmission building on top of a mountain.\n\nMost metropolitan areas have at least one antenna farm, such as Mount Wilson in greater Los Angeles (seen at right), Sweat Mountain in metro Atlanta, Farnsworth Peak for the Salt Lake Valley, Riverview in Tampa, Florida, Baltimore's Television Hill and Slide Mountain (Mount Rose ski area) in the Reno/Tahoe area. Some cities instead have combined many stations onto one tower, often through diplexers into just one or two antennas, such as atop the Empire State Building in New York, the landmark Sutro Tower of San Francisco, or the huge Miami Gardens tower serving the Miami and Fort Lauderdale region. Cleveland, Ohio has its antenna farm in the suburb of Parma, Ohio due to Parma's high elevation. In central Oklahoma City most of the city's media outlets transmitter and tower facilities are located between the Kilpatrick Turnpike to the south and Interstate 44 to the north, Broadway Extension to the west and Interstate 35 to the east with Britton Road being the central thoroughfare. In addition, all three network affiliates and one of the 3 major radio groups have their studio facilities located within the Oklahoma City tower farm.\n\nIn the Appalachian Mountains of the Eastern United States, Poor Mountain serves most of the FM and TV stations in the Roanoke/Lynchburg market. Holston Mountain in upper East Tennessee is home to most of the FM and TV stations in the Tri-Cities (Bristol, Virginia-Kingsport, Tennessee-Johnson City, Tennessee) DMA. Other examples are Signal Mountain near Chattanooga, Tennessee, Sharp's Ridge in Knoxville, Tennessee, and Paris Mountain in Greenville, South Carolina.\n\nOther examples of co-located towers on mountain peaks in the United States are on Red Mountain in Birmingham, Alabama; Mount Wilson near Los Angeles; the Sutro Tower in Clarendon Heights near Mount Sutro in San Francisco; Lookout Mountain, Colorado near Denver; Cedar Hill between Dallas and Fort Worth; South Mountain Park near Phoenix; Nelson Peak near Salt Lake City; Sandia Crest near Albuquerque, New Mexico. \n\nProbably the most famous broadcast antenna farm of all is the World Trade Center Tower One, on which many of the New York City television and several FM stations had their antennas. All were lost when Twin Towers One and Two collapsed after the September 11 attacks in 2001. Most of those stations now broadcast from their previous home, 200 feet lower, on the Empire State Building.\n\nAntenna farms have often been the source of complaints from local neighborhoods, particularly when a new tower is added. This has been increasingly so for TV stations, which have been pursuing with alacrity the construction of new digital television antennas. Because many of these towers are already full, or were built well before there was the expectation of DTV, many stations have been forced to go through the even greater expense of constructing a new tower.\n\nOne such situation was in Colorado, in the late 1990s and early to mid-2000s. Many of the Denver metropolitan area TV stations already transmitted analog TV from Lookout Mountain, but needed the extra space for more antennas. Additionally, since many people live on Lookout Mountain, there was also the concern about safety, not only from falling ice or even the slight risk of a tower collapse, but also ongoing from the additional RF that it would create. Residents and the city of Golden filed legal objections, including challenging the authority of the Federal Communications Commission (FCC) to override denial of zoning permits by local government (in this case, the Jefferson county commission). The city of Golden also sought to condemn the site, even though it was outside city limits. It was decided that scenic Eldorado Mountain near Boulder might be a better site, but there were also objections that it would ruin the view of that mountain from the valley. Despite other alternatives, the new \"supertower\" was forced on Lookout Mountain by the U.S. Congress, allowing the existing towers to be removed in 2009 after analog shutdown. The site began operating in spring 2008.\n\n\n\n"}
{"id": "41966834", "url": "https://en.wikipedia.org/wiki?curid=41966834", "title": "Criticality (status)", "text": "Criticality (status)\n\nCriticality, is the state of a nuclear chain reacting medium when the chain reaction is just self-sustaining (or critical), that is, when the reactivity is zero. More loosely, the term is used for states in which the reactivity is greater than zero.\n\nIn the context of a nuclear reactor, particularly in a nuclear power plant, criticality refers to the normal operating condition of a reactor, in which nuclear fuel sustains a fission chain reaction. A reactor achieves criticality, (and is said to be critical) when each fission event releases a sufficient number of neutrons to sustain an ongoing series of reactions.\n\nThe International Atomic Energy Agency also defines the \"first criticality date\" as the date when the reactor is made critical for the first time. This is an important milestone in the construction and commissioning of a nuclear power plant. For example, India's Kudunkulam Nuclear Power Plant's second reactor attained criticality on 10 July 2016 by withdrawing the control rods from the reactor, which led to boron dilution to allow neutron concentration to go up, which eventually led to criticality of the reactor. Qinshan 1, China's first domestically designed and constructed nuclear power plant, achieved first criticality on 31 October 1991.\n\n"}
{"id": "894198", "url": "https://en.wikipedia.org/wiki?curid=894198", "title": "Curtain wall (architecture)", "text": "Curtain wall (architecture)\n\nA curtain wall system is an outer covering of a building in which the outer walls are non-structural, utilized to keep the weather out and the occupants in. Since the curtain wall is non-structural, it can be made of lightweight materials, thereby reducing construction costs. When glass is used as the curtain wall, an advantage is that natural light can penetrate deeper within the building. The curtain wall façade does not carry any structural load from the building other than its own dead load weight. The wall transfers lateral wind loads that are incident upon it to the main building structure through connections at floors or columns of the building. A curtain wall is designed to resist air and water infiltration, absorb sway induced by wind and seismic forces acting on the building, withstand wind loads, and support its own dead load weight forces.\n\nCurtain wall systems are typically designed with extruded aluminum framing members, although the first curtain walls were made with steel frames. The aluminum frame is typically infilled with glass, which provides an architecturally pleasing building, as well as benefits such as daylighting. However, the effects of light on visual comfort as well as solar heat gain in a building are more difficult to control when using large amounts of glass infill. Other common infills include: stone veneer, metal panels, louvres, and operable windows or vents.\n\nCurtain walls differ from storefront systems in that they are designed to span multiple floors, taking into consideration design requirements such as: thermal expansion and contraction; building sway and movement; water diversion; and thermal efficiency for cost-effective heating, cooling, and lighting in the building.\n\nBuildings have long been constructed with the exterior walls of the building supporting the load of the entire structure. The development and widespread use of structural steel and later reinforced concrete allowed relatively small columns to support large loads; hence, exterior walls of buildings were no longer required for structural support. The exterior walls could be non-load bearing and thus much lighter and more open than the masonry load-bearing walls of the past. This gave way to increased use of glass as an exterior façade, and the modern-day curtain wall was born.\n\nEarly prototype versions of curtain walls may have existed in buildings of timber construction before the 19th century, should columns have been used to support the building rather than the walls themselves, particularly when large panels of glass infill were involved. When iron began to be used extensively in buildings in late 18th-century Britain such as at Ditherington Flax Mill, and later when buildings of wrought iron and glass such as The Crystal Palace were built, the building blocks of structural understanding were laid for the development of curtain walls.\n\nOriel Chambers (1864) and 16 Cook Street (1866), both built in Liverpool, England, by local architect and civil engineer Peter Ellis, are characterised by their extensive use of glass in their facades. Towards the courtyards they even boasted metal-framed glass curtain walls, which makes them two of the world's first buildings to include this architectural feature. The extensive glass walls allowed light to penetrate further into the building, utilizing more floor space and reducing lighting costs. Oriel Chambers comprises set over five floors without an elevator, which had only recently been invented and was not yet widespread.\n\nAn early example of an all-steel curtain wall used in the classical style is the ' department store on ', Berlin, built in 1901 (since demolished).\n\nSome of the first curtain walls were made with steel mullions, and the polished plate glass was attached to the mullions with asbestos- or fiberglass-modified glazing compound. Eventually silicone sealants or glazing tape were substituted for the glazing compound. Some designs included an outer cap to hold the glass in place and to protect the integrity of the seals. The first curtain wall installed in New York City, in the United Nations Secretariat Building building (Skidmore, Owings, and Merrill, 1952), was this type of construction. Earlier modernist examples are the Bauhaus in Dessau (1926) and the Hallidie Building in San Francisco (1918).\n\nDuring the 1970s, the widespread use of aluminium extrusions for mullions began. Aluminum alloys offer the unique advantage of being able to be easily extruded into nearly any shape required for design and aesthetic purposes. Today, the design complexity and shapes available are nearly limitless. Custom shapes can be designed and manufactured with relative ease. The Omni San Diego Hotel curtain wall in California (developed by JMI Realty, designed by architectural firm Hornberger and Worstel, is an example of a unitized curtain-wall system with integrated sunshades.\n\nThe vast majority of ground-floor curtain walls are installed as long pieces (referred to as \"sticks\") between floors vertically and between vertical members horizontally. Framing members may be fabricated in a shop, but installation and glazing is typically performed at the jobsite.\n\nVery similar to a stick system, a ladder system has mullions which can be split and then either snapped or screwed together consisting of a half box and plate. This allows sections of curtain wall to be fabricated in a shop, effectively reducing the time spent installing the system on site. The drawbacks of using such a system is reduced structural performance and visible joint lines down the length of each mullion.\n\nUnitized curtain walls entail factory fabrication and assembly of panels and may include factory glazing. These completed units are installed on the building structure to form the building enclosure. Unitized curtain wall has the advantages of: speed; lower field installation costs; and quality control within an interior climate-controlled environment. The economic benefits are typically realized on large projects or in areas of high field labor rates.\n\nA common feature in curtain wall technology, the rainscreen principle theorizes that equilibrium of air pressure between the outside and inside of the \"rainscreen\" prevents water penetration into the building. For example, the glass is captured between an inner and an outer gasket in a space called the glazing rebate. The glazing rebate is ventilated to the exterior so that the pressure on the inner and outer sides of the exterior gasket is the same. When the pressure is equal across this gasket, water cannot be drawn through joints or defects in the gasket.\n\nCurtain wall systems must be designed to handle all loads imposed on it as well as keep air and water from penetrating the building envelope.\n\nThe loads imposed on the curtain wall are transferred to the building structure through the anchors which attach the mullions to the building.\n\n\"Dead load\" is defined as the weight of structural elements and the permanent features on the structure. In the case of curtain walls, this load is made up of the weight of the mullions, anchors and other structural components of the curtain wall, as well as the weight of the infill material. Additional dead loads imposed on the curtain wall may include sunshades or signage attached to the curtain wall.\n\n\"Wind load\" is a normal force acting on the building as the result of wind blowing on the building. Wind pressure is resisted by the curtain wall system since it envelops and protects the building. Wind loads vary greatly throughout the world, with the largest wind loads being near the coast in hurricane-prone regions. For each project location, building codes specify the required design wind loads. Often, a wind tunnel study is performed on large or unusually-shaped buildings. A scale model of the building and the surrounding vicinity is built and placed in a wind tunnel to determine the wind pressures acting on the structure in question. These studies take into account vortex shedding around corners and the effects of surrounding topography and buildings.\n\n\"Seismic loads\" in curtain wall system are limited to the interstory drift induced on the building during an earthquake. In most situations, the curtain wall is able to naturally withstand seismic and wind induced building sway because of the space provided between the glazing infill and the mullion. In tests, standard curtain wall systems are typically able to withstand up to three inches (75 mm) of relative floor movement without glass breakage or water leakage.\n\n\"Snow loads\" and live loads are not typically an issue in curtain walls, since curtain walls are designed to be vertical or slightly inclined. If the slope of a wall exceeds 20 degrees or so, these loads may need to be considered.\n\n\"Thermal loads\" are induced in a curtain wall system because aluminum has a relatively high coefficient of thermal expansion. This means that over the span of a couple of floors, the curtain wall will expand and contract some distance, relative to its length and the temperature differential. This expansion and contraction is accounted for by cutting horizontal mullions slightly short and allowing a space between the horizontal and vertical mullions. In unitized curtain wall, a gap is left between units, which is sealed from air and water penetration by gaskets. Vertically, anchors carrying wind load only (not dead load) are slotted to account for movement. Incidentally, this slot also accounts for live load deflection and creep in the floor slabs of the building structure.\n\nAccidental explosions and terrorist threats have brought on increased concern for the fragility of a curtain wall system in relation to blast loads. The bombing of the Alfred P. Murrah Federal Building in Oklahoma City, Oklahoma, has spawned much of the current research and mandates in regards to building response to blast loads. Currently, all new federal buildings in the U.S. and all U.S. embassies built on foreign soil must have some provision for resistance to bomb blasts.\n\nSince the curtain wall is at the exterior of the building, it becomes the first line of defense in a bomb attack. As such, blast resistant curtain walls are designed to withstand such forces without compromising the interior of the building to protect its occupants. Since blast loads are very high loads with short durations, the curtain wall response should be analyzed in a dynamic load analysis, with full-scale mock-up testing performed prior to design completion and installation.\n\nBlast resistant glazing consists of laminated glass, which is meant to break but not separate from the mullions. Similar technology is used in hurricane-prone areas for impact protection from wind-borne debris.\n\n\"Air infiltration\" is the air which passes through the curtain wall from the exterior to the interior of the building. The air is infiltrated through the gaskets, through imperfect joinery between the horizontal and vertical mullions, through weep holes, and through imperfect sealing. The American Architectural Manufacturers Association (AAMA) is an industry trade group in the U.S. that has developed voluntary specifications regarding acceptable levels of air infiltration through a curtain wall.\n\n\"Water penetration\" is defined as water passing from the exterior of the building to the interior of the curtain wall system. Sometimes, depending on the building specifications, a small amount of controlled water on the interior is deemed acceptable. Controlled water penetration is defined as water that penetrates beyond the inner most vertical plane of the test specimen, but has a designed means of drainage back to the exterior. AAMA Voluntary Specifications allow for controlled water penetration while the underlying ASTM E1105 test method would define such water penetration as a failure. To test the ability of a curtain wall to withstand water penetration in the field, an ASTM E1105 water spray rack system is placed on the exterior side of the test specimen, and a positive air pressure difference is applied to the system. This set up simulates a wind driven rain event on the curtain wall to check for field performance of the product and of the installation. Field quality control and assurance checks for water penetration has become the norm as builders and installers apply such quality programs to help reduce the number of water damage litigation suits against their work.\n\nOne of the disadvantages of using aluminum for mullions is that its modulus of elasticity is about one-third that of steel. This translates to three times more deflection in an aluminum mullion compared to a similar steel section under a given load. Building specifications set deflection limits for perpendicular (wind-induced) and in-plane (dead load-induced) deflections. These deflection limits are not imposed due to strength capacities of the mullions. Rather, they are designed to limit deflection of the glass (which may break under excessive deflection), and to ensure that the glass does not come out of its pocket in the mullion. Deflection limits are also necessary to control movement at the interior of the curtain wall. Building construction may be such that there is a wall located near the mullion, and excessive deflection can cause the mullion to contact the wall and cause damage. Also, if deflection of a wall is quite noticeable, public perception may raise undue concern that the wall is not strong enough.\n\nDeflection limits are typically expressed as the distance between anchor points divided by a constant number. A deflection limit of L/175 is common in curtain wall specifications, based on experience with deflection limits that are unlikely to cause damage to the glass held by the mullion. Say a given curtain wall is anchored at 12 foot (144 in) floor heights. The allowable deflection would then be 144/175 = 0.823 inches, which means the wall is allowed to deflect inward or outward a maximum of 0.823 inches at the maximum wind pressure. However, some panels require stricter movement restrictions, or certainly those that prohibit a torque-like motion.\n\nDeflection in mullions is controlled by different shapes and depths of curtain wall members. The depth of a given curtain wall system is usually controlled by the area moment of inertia required to keep deflection limits under the specification. Another way to limit deflections in a given section is to add steel reinforcement to the inside tube of the mullion. Since steel deflects at one-third the rate of aluminum, the steel will resist much of the load at a lower cost or smaller depth.\n\nStrength (or maximum usable stress) available to a particular material is not related to its material stiffness (the material property governing deflection); it is a separate criterion in curtain wall design and analysis. This often affects the selection of materials and sizes for design of the system. The allowable bending strength for certain aluminum alloys, such as those typically used in curtain wall framing, approaches the allowable bending strength of steel alloys used in building construction.\n\nRelative to other building components, aluminum has a high heat transfer coefficient, meaning that aluminum is a very good conductor of heat. This translates into high heat loss through aluminum curtain wall mullions.\nThere are several ways to compensate for this heat loss, the most common way being the addition of thermal breaks. \"Thermal breaks\" are barriers between exterior metal and interior metal, usually made of polyvinyl chloride (PVC). These breaks provide a significant decrease in the thermal conductivity of the curtain wall. However, since the thermal break interrupts the aluminum mullion, the overall moment of inertia of the mullion is reduced and must be accounted for in the structural analysis and deflection analysis of the system.\n\nThermal conductivity of the curtain wall system is important because of heat loss through the wall, which affects the heating and cooling costs of the building. On a poorly performing curtain wall, condensation may form on the interior of the mullions. This could cause damage to adjacent interior trim and walls.\n\nRigid insulation is provided in spandrel areas to provide a higher R-value at these locations.\n\n\"Infill\" refers to the large panels that are inserted into the curtain wall between mullions. Infills are typically glass but may be made up of nearly any exterior building element. Some common infills include metal panels, louvers, a photovoltaic panels.\n\nBy far the most common glazing type, glass can be of an almost infinite combination of color, thickness, and opacity.\nFor commercial construction, the two most common thicknesses are 1/4 inch (6 mm) monolithic and 1 inch (25 mm) insulating glass. Presently, 1/4 inch glass is typically used only in spandrel areas, while insulating glass is used for the rest of the building (sometimes spandrel glass is specified as insulating glass as well). The 1 inch insulation glass is typically made up of two 1/4-inch lites of glass with a 1/2 inch (12 mm) airspace. The air inside is usually atmospheric air, but some inert gases, such as argon or krypton may be used to offer better thermal transmittance values.\n\nIn residential construction, thicknesses commonly used are 1/8 inch (3 mm) monolithic and 5/8 inch (16 mm) insulating glass. Larger thicknesses are typically employed for buildings or areas with higher thermal, relative humidity, or sound transmission requirements, such as laboratory areas or recording studios.\n\nGlass may be used which is transparent, translucent, or opaque, or in varying degrees thereof. \"Transparent\" glass usually refers to \"vision\" glass in a curtain wall. Spandrel or vision glass may also contain translucent glass, which could be for security or aesthetic purposes. \"Opaque\" glass is used in areas to hide a column or spandrel beam or shear wall behind the curtain wall. Another method of hiding spandrel areas is through \"shadow box\" construction (providing a dark enclosed space behind the transparent or translucent glass). Shadow box construction creates a perception of depth behind the glass that is sometimes desired.\n\nFabric is another type of material which is common for curtain walls. Fabric is often much less expensive and serves as a less permanent solution. Unlike glass or stone, fabric is much faster to install, less expensive, and often much easier to modify after it is installed.\nBecause of low density of fabrics total weight of structure is very low then strength consideration of structure is not too important.\n\nThin blocks (3 to 4 inches (75–100 mm)) of stone can be inset within a curtain wall system. The type of stone used is limited only by the strength of the stone and the ability to manufacture it in the proper shape and size. Common stone types used are: calcium silicate, granite, marble, travertine, and limestone. To reduce weight and improve strength, the natural stone may be attached to an aluminum honeycomb backing.\n\nMetal panels can take various forms including aluminum plate; aluminum composite panels consisting of two thin aluminum sheets sandwiching a thin plastic interlayer; copper wall cladding, and panels consisting of metal sheets bonded to rigid insulation, with or without an inner metal sheet to create a sandwich panel. Other opaque panel materials include fiber-reinforced plastic (FRP), stainless steel, and terracotta. Terracotta curtain wall panels were first used in Europe, but only a few manufacturers produce high quality modern terracotta curtain wall panels.\n\nA louver is provided in an area where mechanical equipment located inside the building requires ventilation or fresh air to operate. They can also serve as a means of allowing outside air to filter into the building to take advantage of favorable climatic conditions and minimize the usage of energy-consuming HVAC systems. Curtain wall systems can be adapted to accept most types of louver systems to maintain the same architectural sightlines and style while providing the functionality.\n\nMost curtain wall glazing is fixed, meaning there is no access to the exterior of the building except through doors. However, windows or vents can be glazed into the curtain wall system as well, to provide required ventilation or operable windows. Nearly any window type can be made to fit into a curtain wall system.\n\nFirestopping at the \"perimeter slab edge\", which is a gap between the floor and the curtain wall, is essential to slow the passage of fire and combustion gases between floors. Spandrel areas must have non-combustible insulation at the interior face of the curtain wall. Some building codes require the mullion to be wrapped in heat-retarding insulation near the ceiling to prevent the mullions from melting and spreading the fire to the floor above. The firestop at the perimeter slab edge is considered a continuation of the fire-resistance rating of the floor slab. The curtain wall itself, however, is not ordinarily required to have a rating. This causes a quandary as Compartmentalization (fire protection) is typically based upon \"closed\" compartments to avoid fire and smoke migrations beyond each engaged compartment. A curtain wall by its very nature prevents the completion of the compartment (or envelope). The use of fire sprinklers has been shown to mitigate this matter. As such, unless the building is sprinklered, fire may still travel up the curtain wall, if the glass on the exposed floor is shattered from heat, causing flames to lick up the outside of the building.\n\nFalling glass can endanger pedestrians, firefighters and firehoses below. An example of this is the 1988 First Interstate Tower fire in Los Angeles, California. The fire leapfrogged up the tower by shattering the glass and then consuming the aluminum framing holding the glass. Aluminum's melting temperature is 660 °C, whereas building fires can reach 1,100 °C. The melting point of aluminum is typically reached within minutes of the start of a fire.\n\nFireman knock-out glazing panels are often required for venting and emergency access from the exterior. Knock-out panels are generally fully tempered glass to allow full fracturing of the panel into small pieces and relatively safe removal from the opening.\n\nCurtain walls and perimeter sealants require maintenance to maximize service life. Perimeter sealants, properly designed and installed, have a typical service life of 10 to 15 years. Removal and replacement of perimeter sealants require meticulous surface preparation and proper detailing.\n\nAluminum frames are generally painted or anodized. Care must be taken when cleaning areas around anodized material as some cleaning agents will destroy the finish. Factory applied fluoropolymer thermoset coatings have good resistance to environmental degradation and require only periodic cleaning. Recoating with an air-dry fluoropolymer coating is possible but requires special surface preparation and is not as durable as the baked-on original coating. Anodized aluminum frames cannot be \"re-anodized\" in place but can be cleaned and protected by proprietary clear coatings to improve appearance and durability.\n\nStainless steel curtain walls require no coatings, and embossed, as opposed to abrasively finished, surfaces maintain their original appearance indefinitely without cleaning or other maintenance. Some specially textured matte stainless steel surface finishes are hydrophobic and resist airborne and rain-borne pollutants. This has been valuable in the American Southwest and in the Mideast for avoiding dust, as well as avoiding soot and smoke staining in polluted urban areas.\n\n\n"}
{"id": "1055795", "url": "https://en.wikipedia.org/wiki?curid=1055795", "title": "Dual Scan", "text": "Dual Scan\n\nDual Scan, also known as dual-scan supertwist nematic or DSTN, is an LCD technology in which a screen is divided into two sections which are simultaneously refreshed giving faster refresh rate than traditional passive matrix screens. It is an improved form of supertwist nematic display that offers low power consumption but inferior sharpness and brightness compared to TFT screens. For several years (late '90s to early 2000s), TFT screens were only found in high-end laptops due to them being more expensive and lower-end laptops offering DSTN screens only. This was at a time when the screen was often the most expensive component of laptops. The price difference between a laptop with DSTN and one with TFT could easily be $400 or more. However, TFT gradually became cheaper and has essentially captured the entire market.\n\nDSTN display quality is poor compared to TFT, with visible noise, smearing, much lower contrast and slow response. Such screens are unsuitable for viewing movies.\n"}
{"id": "3155785", "url": "https://en.wikipedia.org/wiki?curid=3155785", "title": "ExtraVision", "text": "ExtraVision\n\nExtraVision was a short-lived teletext service created and operated by the American television network CBS in the early to mid-1980s. It was carried in the vertical blanking interval of the video from local affiliate stations of the CBS network. It featured CBS program information, news, sports, weather, even subtitling for CBS programming (much like page 888 in British/European teletext, and American closed captioning). ExtraVision could also have its pages customized by the local affiliate station carrying it, for such things as program schedules, local community announcements, and station promotions.\n\nExtraVision was discontinued by CBS towards the end of the ’80s, due to the service using the NABTS protocol, which required a quite expensive decoder to receive the service. Also, most of the local CBS affiliates carrying the ExtraVision service did not bother to invest in the computer equipment required to customize pages to carry locally oriented information on the service.\n\nCBS had begun tests in 1979 using the French Antiope system, and again in 1981 in the Los Angeles area. The full ExtraVision service began in 1983 on CBS affiliate WBTV in Charlotte, NC, and went nationwide in 1984. It was cancelled in 1986, a year after teletext had also been abandoned by NBC.\n"}
{"id": "48804800", "url": "https://en.wikipedia.org/wiki?curid=48804800", "title": "EyeClick", "text": "EyeClick\n\nEyeClick is an Israeli-based company known for offering interactive media display solutions used for applications including retail spaces, exhibitions, corporate events and museums. The EyeClick technology is used to transform floors, walls and windows into interactive displays.\n\nEyeClick was founded in 2005 by Ariel Almos who is the current CEO. The company has its headquarters in Ramat Gan, Israel. EyeClick is active in over 80 countries in Europe, the US and Asia, and has partnered with brands such as Coca-Cola, Samsung and Disney. Big companies such as Coca Cola, Microsoft, Volvo, Sony, Burger King, Puma, Bacardi and GE are among the brands that have used the EyeClick technology. The Beijing Olympic Games also used EyeClick. In March 2017, EyeClick won the Gold award at China for its BEAM virtual gaming system.\n\nThe company's products which include EyePlay, EyeStep, EyeWall, EyeBoard and EyeTouch give designers, hospitals, event planners, retailers, media companies and other organizations the ability to showcase rich interactive digital content in public spaces, family entertainment centers, medical centers, museums, malls, airports and chain stores.\n\n"}
{"id": "12941407", "url": "https://en.wikipedia.org/wiki?curid=12941407", "title": "Good regulator", "text": "Good regulator\n\nThe good regulator is a theorem conceived by Roger C. Conant and W. Ross Ashby that is central to cybernetics. It is stated that \"every good regulator of a system must be a model of that system\". That is, any regulator that is maximally successful and simple must be isomorphic with the system being regulated. This result is obtained by considering the entropy of the variation of the output of the controlled system, and shows that, under very general conditions, that the entropy is minimized when there is a mapping from the states of the system to the states of the regulator. The minimum is obtained when the map is an isomorphism, that is, when the regulator models the system.\n\nWith regard to the brain, insofar as it is successful and efficient as a regulator for survival, it must proceed, in learning, by the formation of a model (or models) of its environment.\n\nThe theorem is general enough to apply to all regulating and self-regulating or homeostatic systems. \n\nThe theorem does not explain what it takes for the system to become a good regulator. The problem of creating good regulators is addressed by the ethical regulator theorem, and by the theory of practopoiesis.\n\nWhen restricted to the ODE (ordinary differential equations) subset of control theory, it is referred to as the internal model principle, which was first articulated in 1976 by B. A. Francis and W. M. Wonham. In this form, it stands in contrast to classical control, in that the classical feedback loop fails to explicitly model the controlled system (although the classical controller may contain an implicit model).\n\n\n"}
{"id": "15147539", "url": "https://en.wikipedia.org/wiki?curid=15147539", "title": "Group on Earth Observations", "text": "Group on Earth Observations\n\nThe Group on Earth Observations (or GEO) coordinates international efforts to build a Global Earth Observation System of Systems (GEOSS). It links existing and planned Earth observation systems and supports the development of new ones in cases of perceived gaps in the supply of environment-related information. It aims to construct a global public infrastructure for Earth observations consisting in a flexible and distributed network of systems and content providers.\n\nCommon Earth observation instruments include ocean buoys, meteorological stations and balloons, seismic and Global Positioning System (GPS) stations, remote-sensing satellites, computerized forecasting models and early warning systems. These instruments are used to measure and monitor specific aspects of Earth’s physical, chemical and biological systems.\n\nTo be useful, the raw data collected must be processed, archived, interpreted, and made available via easy-to-use channels in the form of information comprehensible not only by remote sensing experts. Earth observations are vital for policymaking and assessment in many fields.\n\nGEO focuses on facilitating access to Earth observation data for nine priority areas: natural and human-induced disasters, environmental sources of health hazards, energy management, climate change and its impacts, freshwater resources, weather forecasting, ecosystem management, sustainable agriculture, and biodiversity conservation.\n\nGEO was established in February 2005 by the Third Earth Observation Summit in Brussels at the end of a process that started in 2003 with the First Earth Observation Summit in Washington, DC. It was launched in response to calls for action by the 2002 World Summit on Sustainable Development and the Group of Eight (G8) leading industrialized countries. These high-level meetings recognized that international collaboration is essential for exploiting the growing potential of Earth observations to support decision making in an increasingly complex and environmentally stressed world.\n\nGEO is a voluntary partnership of governments and international organizations. It provides a framework within which these partners can develop new projects and coordinate their strategies and investments. As of January 2016, GEO’s membership includes 102 governments including the European Commission. In addition, 92 intergovernmental, international and regional organizations with a mandate in Earth observation or related issues have been recognized as participating organizations (see lists below). Each member and participating organization is represented by a principal and a principal alternate. Members make financial contributions to GEO on a voluntary basis.\n\nGEO is constructing GEOSS on the basis of a 10-Year Strategic Plan from 2016 to 2025. The plan defines a vision statement for GEOSS, its purpose and scope, expected benefits, eight “Societal Benefit Areas” (disaster resiliance, public health surveillance, energy and mineral resources management, sustainable urban development, water resources management, biodiversity and ecosystem sustainability, food security and sustainable agriculture and infrastructure and transport management - with climate as a cross cutting issue), technical and capacity-building priorities, and the GEO governance structure.\nGEO is governed by a plenary consisting of all members and participating Organizations. GEO meets in plenary at least once a year at the level of senior officials and periodically at the ministerial level. Members make decisions at the plenary by consensus.\n\nGEO’s members as of January 2016 include 101 countries and the European Commission:\n\nAlgeria / Argentina / Armenia / Australia / Austria / Bahamas, The / Bahrain / Bangladesh / Belgium / Belize / Brazil / Bulgaria / Burkina Faso / Cameroon / Canada / Central African Republic / Chile / China / Colombia / Congo, Republic of the / Costa Rica / Côte d'Ivoire / Croatia / Cyprus / Czech Republic / Denmark / Ecuador / Egypt / Estonia / Ethiopia / European Commission / Finland / France / Gabon / Georgia / Germany / Ghana / Greece / Guinea, Republic of / Guinea-Bissau / Honduras / Hungary / Iceland / India / Indonesia / Iran / Ireland / Israel / Italy / Japan / Kazakhstan / Korea, Republic of / Latvia / Luxembourg / Madagascar / Malaysia / Mali / Malta / Mauritius / Mexico / Moldova / Morocco / Nepal / Netherlands / New Zealand / Niger / Nigeria / Norway / Pakistan / Panama / Paraguay / Peru / Philippines / Poland / Portugal / Romania / Russian Federation / Senegal / Serbia / Seychelles, Republic of / Slovakia / Slovenia / South Africa / Spain / Sudan / Sweden / Switzerland / Tajikistan / Thailand / Tunisia / Turkey / Uganda / Ukraine / United Arab Emirates / United Kingdom / United States / Uruguay / Uzbekistan / Vietnam / Zimbabwe\n\nAs of January 2016, the participating organizations are:\n\nAARSE: African Association of Remote Sensing of the Environment\nACMAD: African Centre of Meteorological Application for Development\nADIE: Association for the Development of Environmental Information\nAPN: Asia-Pacific Network for Global Change Research\nBelmont Forum \nBiodiversity International \nBSC: Commission on the Protection of the Black Sea Against Pollution \nCATHALAC: Water Center for the Humid Tropics of Latin America and the Caribbean\nConvention on Biological Diversity \nCreative Commons \nCEOS: Committee on Earth Observation Satellites \nCGMS: Coordination Group for Meteorological Satellites \nCIIFEN: International Research Centre on El Niño \nCMO: Caribbean Meteorological Organization \nCODATA: International Council for Science: Committee on Data for Science and Technology \nCOSPAR: Committee on Space Research\nCRTEAN: Regional Centre for Remote Sensing of North Africa States \nEARSeL: European Association of Remote Sensing Laboratories \nECMWF: European Centre for Medium-Range Weather Forecasts \nEEA: European Environment Agency \nEIS-AFRICA: Environmental Information Systems - AFRICA \nEPOS, European Plate Observing System \nESA: European Space Agency \nESIP Federation: Federation of Earth Science Information Partners \nESSL: European Severe Storms Laboratory \nEuropean Union Satellite Centre \nEUMETNET: Network of European Meteorological Services/Composite Observing System \nEUMETSAT: European Organisation for the Exploitation of Meteorological Satellites \nEuroGeoSurveys: The Association of the Geological Surveys of the European Union \nEUREC: The Association of European Renewable Energy Research Centres \nEurisy \nEuroGeoSurveys: The Association of the Geological Surveys of the European Union \nFAO: Food and Agriculture Organization of the United Nations \nFDSN: Federation of Digital Broad-Band Seismograph Networks \nFuture Earth \nGBIF: Global Biodiversity Information Facility \nGCOS: Global Climate Observing System \nGÉANT \nGEM: Global Earthquake Model Foundation\nGLOBE: Global Learning and Observations to Benefit the Environment\nGLOS: Great Lakes Observing System\nGOOS: The Global Ocean Observing System \nGRSS: Geoscience and Remote Sensing Society \nGSDI: Global Spatial Data Infrastructure \nGTOS: Global Terrestrial Observing System \nIAG: International Association of Geodesy \nICA: International Cartographic Association \nICIMOD: International Centre for Integrated mountain Development \nICSU: International Council for Science \nIEEE: Institute of Electrical and Electronics Engineers \nInternational Hydrographic Organisation \nIIASA: International Institute for Applied Systems Analysis \nIISD: International Institute for Sustainable Development \nIISL: International Institute of Space Law \nINCOSE: International Council on Systems Engineering \nIO3C: International Ozone Commission \nIOC: Intergovernmental Oceanographic Commission \nISCGM: International Steering Committee for Global Mapping \nISDE: International Society for Digital Earth \nISPRS: International Society for Photogrammetry and Remote Sensing \nITC: Faculty of Geo-Information Science and Earth Observation \nIUGG: International Union of Geodesy and Geophysics \nIUGS: International Union of Geological Sciences \nIWMI: International Water Management Institute \nMariolopoulous-Kanaginis Foundation for the Environmental Sciences \nMTS: Marine Technology Society \nOGC: Open Geospatial Consortium \nPOGO: Partnership for Observation of the Global Oceans \nRCMRD: Regional Centre for Mapping of Resources for Development \nRDA: Research Data Alliance \nRECTAS: Regional Centre for Training in Aerospace Surveys \nSAON: Sustaining Arctic Observing Networks \nSICA: Central American Commission for the Environment and Development \nSOPAC: South Pacific Applied Geoscience Commission \nSecure World Foundation \nThe World Bank \nNCAR UCAR: University Corporation for Atmospheric Research \nUNCCD: Secretariat of the United Nations Convention to Combat Desertification \nUNECA: United Nations Economic Commission for Africa \nUNEP: United Nations Environment Programme \nUNESCAP: United Nations Economic and Social Commission for Asia and the Pacific \nUNESCO: United Nations Educational, Scientific and Cultural Organisation \nUNFCCC: United Nations Framework Convention on Climate Change \nUNISDR: The United Nations Office for Disaster Risk Reduction \nUNITAR: United Nations Institute for Training and Research \nUNOOSA: United Nations Office for Outer Space Affairs \nUNU-EHS: United Nations University, Institute for Environment and Human Security \nWCRP: World Climate Research Programme \nWDS: ICSU World Data System \nWFPHA: World Federation of Public Health Associations \nWMO: World Meteorological Organization\n\n\n"}
{"id": "46985218", "url": "https://en.wikipedia.org/wiki?curid=46985218", "title": "Guidepoint", "text": "Guidepoint\n\nGuidepoint is an expert network, providing business professionals with opportunities to talk with industry experts who can answer their specialized industry questions. Guidepoint clients consult with experts over the phone, via conference, teleconference, or custom event, or may gather primary research data through a survey, poll, or web-based data offering.\n\nThe company has recruited over 440,000 \"experts\", known as “Guidepoint Advisors”, who come from 110 different business industries.\n\nGuidepoint is based in New York City and has additional offices in Boston, San Francisco, London, Düsseldorf, Singapore, Hong Kong, Shanghai, Seoul, and Tokyo.\n\nGuidepoint was founded by Albert Sebag, Ph.D., previously a bioorganic chemist, where he helped to develop anti-estrogen drugs for treatment of certain cancers. Sebag then moved on to law, earning his J.D. from Boston College Law School. During his legal career, he focused on intellectual property and biotechnology litigation for generic pharmaceutical clients. This experience eventually led Sebag to begin a matching service for oncology patients called Clinical Advisors. Launched in 2003, Clinical Advisors matched cancer patients with the clinical trial that best suited their case.\n\nAs Clinical Advisors expanded their network to include expertise in additional healthcare areas, the company progressed into other industries and sectors. In 2007, the company changed its name to Guidepoint Global to reflect this expanded business model. In 2009, Guidepoint Global acquired Vista Research from Standard & Poor’s, a subsidiary of the company formerly called The McGraw-Hill Companies, an expert network company that was active in Asia and had a strong network of experts in other industries such as technology and telecom.\n\nShortly after rebranding as Guidepoint, “the expert at finding expertise,” and premiering a new logo and website, Guidepoint acquired Innosquared, a Germany-based expert network firm. According to FINAlternatives, Guidepoint is “the only global expert network with a large research team based in Germany and one of only two with research locations in the EMEA region.”\n\nClients use Guidepoint as a resource to find answers to specific industry questions that might only be answered by industry and subject-matter experts. The company essentially operates as a matchmaking service, connecting its users to recruited experts in the field they are looking to learn more about. Experts are categorized into six main industry sectors: Healthcare; Financial and Business Services; Consumer Goods and Services; Energy, Industrials, and Basic Materials; Tech, Media, and Telecom; and Legal and Regulatory.\n\nServices are subscription-based and include hourly phone consultations, in-person events, teleconferences, surveys, custom research reports, and a monthly healthcare data offering called Guidepoint TRACKER, which features data on market share in the therapeutics and medical device sectors, such as the US Breast Implant market.\n\nExperts are pre-screened and required to adhere to compliance policies and procedures.\n\nExpert networks are a longstanding resource for qualitative and primary research. In 2009, IR Magazine reported there were more than 45 expert networks, generating between $400 - $450 million in yearly sales, around 20 percent of the independent research industry.\n\nAccording to Integrity Research, Guidepoint is ranked as the second-largest expert network, with a network of more than 400,000 experts.\n\nGuidepoint’s Terms & Conditions are a complete set of conservative rules that govern every consultation through Guidepoint. They include limitations on the participation of current and former employees of any company that is the subject of any consultation, employees of companies involved in tender offers and IPOs, competitors, government workers, FDA advisory committee members, and clinical trial participants. In addition, Guidepoint conducts background checks on all new experts that joins its network.\n\nIn 2011, Guidepoint hired Catherine Smith, a former senior counsel at the U.S. Securities and Exchange Commission (SEC), to head Guidepoint’s Legal and Compliance departments. During her seven years in the SEC’s Enforcement Division, she was responsible for the investigation and litigation of potential violations of federal securities laws. Smith was an inaugural member of the SEC Enforcement Division’s national Market Abuse Unit and the recipient of the Enforcement Director’s Award. Smith is also a fellow of the Regulatory Compliance Association.\n\nSmith regularly speaks and writes about expert networks and compliance, including for the Regulatory Compliance Association, Boston Security Analysts Society, Private Equity International’s Private Fund Compliance Forum, Private Investment Forum, and The Hedge Fund Law Report.\n\nIn 2010, there were allegations that an expert who provided expert network services gave confidential information to a third party who traded on that information in violation of the federal securities law. That resulted in a strong regulatory focus on the industry and its investment firm clients. Charges were not brought against Guidepoint and now, years later, the investigation seems to be in the past. According to Integrity Research, “Although Guidepoint was mentioned in insider trading complaints, its compliance procedures protected it from accusations of wrong doing.”\n\nGuidepoint donates to and sponsors a number of charitable causes, including Teach for America, Harlem RBI, New York Cares, City Harvest, and the United Way of America.\n\nGuidepoint founder and CEO Albert Sebag is a board member for service organization Repair the World.\n"}
{"id": "44264192", "url": "https://en.wikipedia.org/wiki?curid=44264192", "title": "Herodotus Machine", "text": "Herodotus Machine\n\nThe Herodotus Machine was a machine described by Herodotus, a Greek historian born in Halicarnassus, Caria (modern-day Bodrum, Turkey). Herodotus claims this invention enabled the Ancient Egyptians to construct the pyramids. The contraption supposedly allowed workers to lift heavy building materials. Herodotus is believed to have encountered the device while traveling through Egypt. With limited reference and no true schematics, this machine has stimulated many historians' theories of how the Ancient Egyptians were able to create pyramids.\n\nHerodotus is suspected of having embellished – or made up entirely – some of his historical accounts, but scholars generally accept this particular account as Herodotus provides otherwise reasonable accounts of Egypt and it would have been quite possible for someone living in Parnassus to safely and easily travel to Egypt during Herodotus' lifetime. Trade existed between the Greek City States and the kingdom of Egypt. In Egypt Herodotus is thought to have conversed with locals on the matter.\n\nHerodotus provides a description of the process in Histories.\n\nLeonardo da Vinci is believed to have sketched a machine based on Herodotus' description. Later depictions are premised upon da Vinci's sketches in the \"Codex Madrid\". Visual depictions cannot authoritatively claim to represent Herodotus' machine. Some scholars argue that da Vinci may have had access to ancient texts, since lost, that provided additional details.\n\nSince Herodotus provides more of a description of the components of the design rather than detailed form or usage, many ideas have been put forward and scale models built.\n\n\n"}
{"id": "4047871", "url": "https://en.wikipedia.org/wiki?curid=4047871", "title": "Hexachlorobenzene", "text": "Hexachlorobenzene\n\nHexachlorobenzene, or perchlorobenzene, is an organochloride with the molecular formula CCl. It is a fungicide formerly used as a seed treatment, especially on wheat to control the fungal disease bunt. It has been banned globally under the Stockholm Convention on Persistent Organic Pollutants.\n\nHCB is a white crystalline solid that has negligible solubility in water (0.00000002 M) but sparingly soluble in organic solvents. It is most soluble in halogenated solvents like chloroform (approx 0.03 M), less soluble in esters and hydrocarbons (approx 0.020 M), and even less soluble in short chain alcohols (0.002-0.006 M). Its vapour pressure is 1.09×10 mmHg (1.45 mPa) at 20 °C. Its flash point is 242 °C and it sublimes at 322 °C.\n\nHexachlorobenzene is an animal carcinogen and is considered to be a probable human carcinogen. After its introduction as a fungicide in 1945, for crop seeds, this toxic chemical was found in all food types. Hexachlorobenzene was banned from use in the United States in 1966.\n\nThis material has been classified by the International Agency for Research on Cancer (IARC) as a Group 2B carcinogen (possibly carcinogenic to humans). Animal carcinogenicity data for hexachlorobenzene show increased incidences of liver, kidney (renal tubular tumours) and thyroid cancers. Chronic oral exposure in humans has been shown to give rise to a liver disease (porphyria cutanea tarda), skin lesions with discoloration, ulceration, photosensitivity, thyroid effects, bone effects and loss of hair. Neurological changes have been reported in rodents exposed to hexachlorobenzene. Hexachlorobenzene may cause embryolethality and teratogenic effects. Human and animal studies have demonstrated that hexachlorobenzene crosses the placenta to accumulate in foetal tissues and is transferred in breast milk.\n\nHCB is very toxic to aquatic organisms. It may cause long term adverse effects in the aquatic environment. Therefore, release into waterways should be avoided. It is persistent in the environment. Ecological investigations have found that biomagnification up the food chain does occur. Hexachlorobenzene has a half life in the soil of between 3 and 6 years. Risk of bioaccumulation in an aquatic species is high.\n\nMaterial has relatively low acute toxicity but is toxic because of its persistent and cumulative nature in body tissues in rich lipid content.\n\nIn Anatolia, Turkey between 1955 and 1959, during a period when bread wheat was unavailable, 500 people were fatally poisoned and more than 4,000 people fell ill by eating bread made with HCB-treated seed that was intended for agriculture use. Most of the sick were affected with a liver condition called porphyria cutanea tarda, which disturbs the metabolism of hemoglobin and results in skin lesions. Almost all breastfeeding children under the age of two, whose mothers had eaten tainted bread, died from a condition called \"pembe yara\" or \"pink sore\", most likely from high doses of HCB in the breast milk. In one mother's breast milk the HCB level was found to be 20 parts per million in lipid, approximately 2,000 times the average levels of contamination found in breast-milk samples around the world. Follow-up studies 20 to 30 years after the poisoning found average HCB levels in breast milk were still more than seven times the average for unexposed women in that part of the world (56 specimens of human milk obtained from mothers with porphyria, average value was 0.51 ppm in HCB-exposed patients compared to 0.07 ppm in unexposed controls), and 150 times the level allowed in cow's milk.\n\nIn the same follow-up study of 252 patients (162 males and 90 females, avg. current age of 35.7 years), 20–30 years postexposure, many subjects had dermatologic, neurologic, and orthopedic symptoms and signs. The observed clinical findings include scarring of the face and hands (83.7%), hyperpigmentation (65%), hypertrichosis (44.8%), pinched faces (40.1%), painless arthritis (70.2%), small hands (66.6%), sensory shading (60.6%), myotonia (37.9%), cogwheeling (41.9%), enlarged thyroid (34.9%), and enlarged liver (4.8%). Urine and stool porphyrin levels were determined in all patients, and 17 have at least one of the porphyrins elevated. Offspring of mothers with three decades of HCB-induced porphyria appear normal.\n\n\nCited works\nAdditional references\n"}
{"id": "6328175", "url": "https://en.wikipedia.org/wiki?curid=6328175", "title": "Hume (programming language)", "text": "Hume (programming language)\n\nHume is a functionally based programming language developed at the University of St Andrews and Heriot-Watt University in Scotland since the year 2000. The language name is both an acronym meaning 'Higher-order Unified Meta-Environment' and an honorific to the 18th Century philosopher David Hume. It targets real-time embedded systems, aiming to produce a design that is both highly abstract, yet which will still allow precise extraction of time and space execution costs. This allows programmers to guarantee the bounded time and space demands of executing programs.\n\nHume combines functional programming ideas with ideas from finite state automata. Automata are used to structure communicating programs into a series of \"boxes\", where each box maps inputs to outputs in a purely functional way using high-level pattern-matching. It is structured as a series of levels, each of which exposes different machine properties.\n\nThe Hume language design attempts to maintain the essential properties and features required by the embedded systems domain (especially for transparent time and space costing) whilst incorporating as high a level of program abstraction as possible. It aims to target applications ranging from simple micro-controllers to complex real-time systems such as smartphones. This ambitious goal requires incorporating both low-level notions such as interrupt handling, and high-level ones of data structure abstraction etc. Of course such systems will be programmed in widely differing ways, but the language design should accommodate these varying requirements.\n\nHume is a three-layer language: an outer (static) declaration/metaprogramming layer, an intermediate coordination layer describing a static layout of dynamic processes and the associated devices, and an inner layer describing each process as a (dynamic)\nmapping from patterns to expressions. The inner layer is stateless and purely functional.\n\nRather than attempting to apply cost modeling and correctness proving technology to an existing language framework either directly or by altering a more general language (as with e.g. RTSJ), the approach taken by the Hume designers is to design Hume in such a way that formal models and proofs can definitely be constructed. Hume is structured as a series of overlapping language levels, where each level adds expressibility to the expression semantics, but either loses some desirable property or increases the technical difficulty of providing formal correctness/cost models.\n\nThe interpreter and compiler versions differ a bit.\n\nThe coordination system wires \"boxes\" in a dataflow programming style.\n\nThe expression language is Haskell-like.\n\nThe message passing concurrency system remembers JoCaml's Join patterns or Polyphonic C Sharp chords, but with all channels asynchronous.\n\nThere is a scheduler built-in that continuously checks pattern-matching through all boxes in turn, putting on hold the boxes that cannot copy outputs to busy input destinations.\n\n"}
{"id": "4757213", "url": "https://en.wikipedia.org/wiki?curid=4757213", "title": "Hunting oscillation", "text": "Hunting oscillation\n\nHunting oscillation is a self-oscillation, usually unwanted, about an equilibrium. The expression came into use in the 19th century and describes how a system \"hunts\" for equilibrium. The expression is used to describe phenomena in such diverse fields as electronics, aviation, biology, and railway engineering.\n\nA classical hunting oscillation is a swaying motion of a railway vehicle (often called \"truck hunting\") caused by the coning action on which the directional stability of an adhesion railway depends. It arises from the interaction of adhesion forces and inertial forces. At low speed, adhesion dominates but, as the speed increases, the adhesion forces and inertial forces become comparable in magnitude and the oscillation begins at a critical speed. Above this speed, the motion can be violent, damaging track and wheels and potentially causing derailment. The problem does not occur on systems with a differential because the action depends on both wheels of a wheelset rotating at the same angular rate, although differentials tend to be rare, and conventional trains have their wheels fixed to the axles in pairs instead. Some trains, like the Talgo 350, don't have a differential, yet they are mostly not affected by hunting oscillation, as most of their wheels rotate independently from one another. The wheels of the power car, however, can be affected by hunting oscillation, because the wheels of the power car are fixed to the axles in pairs like in conventional bogies. Less conical wheels and bogies equipped with independent wheels that turn independently from each other and are not fixed to an axle in pairs are cheaper than a suitable differential for the bogies of a train. \n\nThe problem was first noticed towards the end of the 19th century, when train speeds became high enough to encounter it. Serious efforts to counteract it got underway in the 1930s, giving rise to lengthened trucks and the side-damping \"swing hanger\" truck. In the development of the Japanese \"Shinkansen\", less-conical wheels and other design changes were used to extend truck design speeds above . Advances in wheel and truck design based on research and development efforts in Europe and Japan have extended the speeds of steel wheel systems well beyond those attained by the original \"Shinkansen\", while the advantage of back-compatibility keeps such technology dominant over alternatives such as the hovertrain and maglev systems. The speed record for steel-wheeled trains is held by the French TGV, at .\n\nWhile a qualitative description provides some understanding of the phenomenon, deeper understanding inevitably requires a mathematical analysis of the vehicle dynamics. Even then, the results may be only approximate.\n\nA kinematic description deals with the geometry of motion, without reference to the forces causing it, so the analysis begins with a description of the geometry of a wheel set running on a straight track. Since Newton's Second Law relates forces to accelerations of bodies, the forces acting may then be derived from the kinematics by calculating the accelerations of the components. However, if these forces change the kinematic description (as they do in this case) then the results may only be approximately correct.\n\nThis kinematic description makes a number of simplifying assumptions since it neglects forces. For one, it assumes that the rolling resistance is zero. A wheelset (not attached to a train or truck), is given a push forward on a straight and level track. The wheelset starts coasting and never slows down since there are no forces (except downward forces on the wheelset to make it adhere to the track and not slip). If initially the wheelset is centered on the railroad track then the effective diameters of each wheel are the same and the wheelset rolls down the track in a perfectly straight line forever. But if the wheelset is a little off-center so that the effective diameters (or radii) are different, then the wheelset starts to move in a curve of Radius R (depending on these wheelset radii, etc.; to be derived later on). The problem is to use kinematic reasoning to find the trajectory of the wheelset, or more precisely, the trajectory of the center of the wheelset projected vertically on the roadbed in the center of the track. This is a trajectory on the plane of the level earth's surface and plotted on an x-y graphical plot where x is the distance along the railroad and y is the \"tracking error\", the deviation of the center of the wheelset from the straight line of the railway running down the center of the track (midway between the two rails).\n\nTo illustrate that a wheelset trajectory follows a curved path, one may place a nail or screw on a flat table top and give it a push. It will roll in a curved circle because the nail or screw is like a wheelset with extremely different diameter wheels. The head is analogous to a large diameter wheel and the pointed end is like a small diameter wheel. While the nail or screw will turn around in a full circle (and more) the railroad wheelset behaves differently because as soon at it starts to turn in a curve, the effective diameters change in such a way as to decrease the curvature of the path. Note that \"radius\" and \"curvature\" refer to the curvature of the trajectory of the wheelset and not the curvature of the railway since this is perfectly straight track. As the wheelset rolls on, the curvature decreases until the wheels reach the point where their effective diameters are equal and the path is no longer curving. But the trajectory has a slope at this point (it is a straight line which crosses diagonally over the centerline of the track) so that it overshoots the centerline of the track and the effective diameters reverse (the formerly smaller diameter wheel becomes the larger diameter and conversely). This results in the wheelset moving in a curve in the opposite direction. Again it overshoots the centerline and this phenomenon continues indefinitely with the wheelset oscillating from side to side. Note that the wheel flange never makes contact with the rail. In this model, the rails are assumed to always contact the wheel tread along the same line on the rail head which assumes that the rails are knife-edge and only make contact with the wheel tread along a line (of zero width).\n\nThe train stays on the track by virtue of the conical shape of the wheel treads. If a wheelset is displaced to one side by an amount \"y\" (the tracking error), the radius of the tread in contact with the rail on one side is reduced, while on the other side it is increased. The angular velocity is the same for both wheels (they are coupled via a rigid axle), so the larger diameter tread speeds up, while the smaller slows down. The wheel set steers around a centre of curvature defined by the intersection of the generator of a cone passing through the points of contact with the wheels on the rails and the axis of the wheel set. Applying similar triangles, we have for the turn radius:\n\nwhere d is the track gauge, r the wheel radius when running straight and k is the tread taper (which is the slope of tread in the horizontal direction perpendicular to the track).\n\nThe path of the wheel set relative to the straight track is defined by a function y(x) where x is the progress along the track. This is sometimes called the tracking error. Provided the direction of motion remains more or less parallel to the rails, the curvature of the path may be related to the second derivative of y with respect to distance along the track as approximately \n\nIt follows that the trajectory along the track is governed by the equation:\n\nThis is a simple harmonic motion having wavelength:\n\nThis kinematic analysis implies that trains sway from side to side all the time. In fact, this oscillation is damped out below a critical speed and the ride is correspondingly more comfortable. The kinematic result ignores the forces causing the motion. These may be analyzed using the concept of creep (non-linear) but are somewhat difficult to quantify simply, as they arise from the elastic distortion of the wheel and rail at the regions of contact. These are the subject of frictional contact mechanics; an early presentation that includes these effects in hunting motion analysis was presented by Carter. See Knothe for a historical overview.\n\nIf the motion is substantially parallel with the rails, the angular displacement of the wheel set formula_5 is given by:\n\nHence:\n\nThe angular deflection also follows a simple harmonic motion, which lags behind the side to side motion by a quarter of a cycle. In many systems which are characterised by harmonic motion involving two different states (in this case the axle yaw deflection and the lateral displacement), the quarter cycle lag between the two motions endows the system with the ability to extract energy from the forward motion. This effect is observed in \"flutter\" of aircraft wings and \"shimmy\" of road vehicles, as well as hunting of railway vehicles. The kinematic solution derived above describes the motion at the critical speed.\n\nIn practice, below the critical speed, the lag between the two motions is less than a quarter cycle so that the motion is damped out but, above the critical speed, the lag is greater than a quarter cycle so that the motion is amplified.\n\nIn order to estimate the inertial forces, it is necessary to express the distance derivatives as time derivatives. This is done using the speed of the vehicle U, which is assumed constant:\n\nThe angular acceleration of the axle in yaw is:\n\nThe inertial moment (ignoring gyroscopic effects) is:\n\nwhere F is the force acting along the rails and C is the moment of inertia of the wheel set.\n\nthe maximum frictional force between the wheel and rail is given by:\n\nwhere W is the axle load and formula_13 is the coefficient of friction. Gross slipping will occur at a combination of speed and axle deflection given by:\n\nthis expression yields a significant overestimate of the critical speed, but it does illustrate the physical reason why hunting occurs, i.e. the inertial forces become comparable with the adhesion forces above a certain speed. Limiting friction is a poor representation of the adhesion force in this case.\n\nThe actual adhesion forces arise from the distortion of the tread and rail in the region of contact. There is no gross slippage, just elastic distortion and some local slipping (creep slippage). During normal operation these forces are well within the limiting friction constraint. A complete analysis takes these forces into account, using rolling contact mechanics theories.\n\nHowever, the kinematic analysis assumed that there was no slippage at all at the wheel-rail contact. Now it's clear that there is some creep slippage which makes the calculated sinusoidal trajectory of the wheelset (per Klingel's formula) not exactly correct.\n\nIn order to get an estimate of the critical speed, we use the fact that the condition for which this kinematic solution is valid corresponds to the case where there is no net energy exchange with the surroundings, so by considering the kinetic and potential energy of the system, we should be able to derive the critical speed.\n\nLet:\n\nUsing the operator:\n\nthe angular acceleration equation may be expressed in terms of the angular velocity in yaw: formula_17\n\nintegrating:\n\nso the kinetic energy due to rotation is:\nWhen the axle yaws, the points of contact move outwards on the treads so that the height of the axle is lowered. The distance between the support points increases to:\n\n(to second order of small quantities).\nthe displacement of the support point out from the centres of the treads is:\n\nthe axle load falls by\n\nThe work done by lowering the axle load is therefore:\n\nThis is energy lost from the system, so in order for the motion to continue, an equal amount of energy must be extracted from the forward motion of the wheelset.\n\nThe outer wheel velocity is given by:\n\nThe kinetic energy is:\n\nfor the inner wheel it is\n\nwhere m is the mass of both wheels.\n\nThe increase in kinetic energy is:\n\nThe motion will continue at constant amplitude as long as the energy extracted from the forward motion, and manifesting itself as increased kinetic energy of the wheel set at zero yaw, is equal to the potential energy lost by the lowering of the axle load at maximum yaw.\n\nNow, from the kinematics:\n\nbut\n\nThe translational kinetic energy is\n\nThe total kinetic energy is:\n\nThe critical speed is found from the energy balance:\n\nHence the critical speed is given by\n\nThis is independent of the wheel taper, but depends on the ratio of the axle load to wheel set mass. If the treads were truly conical in shape, the critical speed would be independent of the taper. In practice, wear on the wheel causes the taper to vary across the tread width, so that the value of taper used to determine the potential energy is different from that used to calculate the kinetic energy. Denoting the former as a, the critical speed becomes:\n\nwhere a is now a shape factor determined by the wheel wear. This result is derived in from an analysis of the system dynamics using standard control engineering methods.\n\nThe motion of a wheel set is much more complicated than this analysis would indicate. There are additional restraining forces applied by the vehicle suspension and, at high speed, the wheel set will generate additional gyroscopic torques, which will modify the estimate of the critical speed. Conventionally a railway vehicle has stable motion in low speeds, when it reaches to high speeds stability changes to unstable form. The main purpose of nonlinear analysis of rail vehicle system dynamics is to show the view of analytical investigation of bifurcation, nonlinear lateral stability and hunting behavior of rail vehicles in a tangent track. This study contains Bogoliubov method for the analysis\n\nTwo main matters i.e. assuming the body as a fixed support and influence of the nonlinear elements in calculation of the hunting speed are mostly focused in studies. A real railway vehicle has many more degrees of freedom and, consequently, may have more than one critical speed; it is by no means certain that the lowest is dictated by the wheelset motion.\n\nHowever, the analysis is instructive because it shows why hunting occurs. As the speed increases, the inertial forces become comparable with the adhesion forces. That is why the critical speed depends on the ratio of the axle load (which determines the adhesion force) to the wheelset mass (which determines the inertial forces).\n\nAlternatively, below a certain speed, the energy which is extracted from the forward motion is insufficient to replace the energy lost by lowering the axles and the motion damps out; above this speed, the energy extracted is greater than the loss in potential energy and the amplitude builds up.\n\nThe potential energy at maximum axle yaw may be increased by including an elastic constraint on the yaw motion of the axle, so that there is a contribution arising from spring tension. Arranging wheels in bogies to increase the constraint on the yaw motion of wheelsets and applying elastic constraints to the bogie also raises the critical speed. Introducing elastic forces into the equation permits suspension designs which are limited only by the onset of gross slippage, rather than classical hunting. The penalty to be paid for the virtual elimination of hunting is a straight track, with an attendant right-of-way problem and incompatibility with legacy infrastructure.\n\nHunting is a dynamic problem which can be solved, in principle at least, by active feedback control, which may be adapted to the quality of track. However, the introduction of active control raises reliability and safety issues.\n\nShortly after the onset of hunting, gross slippage occurs and the wheel flanges impact on the rails, potentially causing damage to both.\n\nMany road–rail vehicles feature independent axles and suspension systems on each rail wheel. When this is combined with the presence of road wheels on the rail it becomes difficult to use the formulae above. Historically, road–rail vehicle have their front wheels set slightly toe-in which has been found to minimise hunting whilst the vehicle is being driven on-rail.\n\n\nFor general methods dealing with this class of problem, see\n\n"}
{"id": "35087381", "url": "https://en.wikipedia.org/wiki?curid=35087381", "title": "Hydro-slotted perforation", "text": "Hydro-slotted perforation\n\nHydro-slotting perforation technology is the process of opening the productive formation through the casing and cement sheath to produce the oil or gas product flow (intensification, stimulation). The process has been used for industrial drilling since 1980, and involves the use of an underground hydraulic slotting engine (tool, equipment). The technology helps to minimize compressive stress following drilling in the well-bore zone (which reduces the permeability in the zone). \n\nSince ancient times, when there were the first coal mines, it was observed, that increasing the depth of the development the coal tunnel, under the action of overburden pressure, surrounding rocks become harder and little-permeable. To solve this problem they developed a cavern of a certain form in the rock. More modern mining geo-mechanics explain the reason for the occurrence of this effect in relation to drilling wells. During any drilling process in the well there is formed the annular compressive stress conditions around the wellbore zone. The deeper the well, the more overburden pressure, which means the greater the annular compressive stress conditions. On the rocks lying at depths of 3–5 km the compressive stresses may reach up-to 75–125 MPa. In the near-well zone, as a result of concentration these stresses increase and sometimes become equal to double 150–250 MPa. If the tectonic stresses is several times higher than stresses from the weight of rocks, the stresses in the near-well zone may be even greater. \n\nUnder the action of stress conditions and high overburden pressure occurs a significant reduction in permeability in the near wellbore zone, in some cases close to zero. Oil or gas flow can not penetrate to the well. Traditional methods of opening the productive layer formation (cumulative, jet perforation, sand jet perforation, abrasive jetting perforation and other similar methods) did not consider this complicated situation in the near-well zone and therefore was not the effective. Porous and fractured formations are subjected to compression, that deforms the rock mass and reduces its permeability. The greater the depth, the stronger the effect can be.\n\nHydro-slotting perforation is quite different from jet (hydro-jetting or sand-blast) perforation. The energy of working fluid, consisting from water (layer water) and sand (abrasive quartz sand) pressure in the hydraulic engine, is divided into two components: five percent of energy goes to the creation of smooth uniform rectilinear motion of the working rod with the perforator and nozzles (between two and six nozzles) without participation in the process the multimeter tubing or coil-tubing. Ninety-five percent of energy goes to the cutting of continued and geometrically correct deep slots (up to five feet deep and between three and five slots at the same time). Slot length is equal to the length of the working engine shaft, usually .\n\nThe hydro-slotting perforation process does not deform the casing, does not create cracks in the cement, and does not clog-up the borders in the formation.\n\nThe geometry and depth of the slots creates the conditions for occurrence of the effect of unloading the circular stress conditions in the near wellbore zone (from 50 to 100 percent) and accordingly the increase of permeability (up to 30 to 50 percent) in this zone. In addition to this it forms a large area of the penetration ( area for one cut with two nozzles only), that provides a very good hydrodynamic connection of the productive layer with the well. \n\nThe cutting speed may be corrected with the temperature in the borehole, temperature of the working fluid, concentration, flow and pressure. (these components are enough to completely control the depth and length of the cut and thus forming the slots), to instantly cut through the steel casing, through the cement to delve into the productive formation and keep the jets in this state while moving along the borehole, keeping the same depth of cut. At the end of the cutting continuous slot process the engine is set up to the initial position and ready for the next cutting interval. The process of hydro-slotting perforation and the depth of cut is controlled by the working fluid supply, pressure and concentration. The equipment can be operated without lifting on the surface for 11–15 hours. \n\nHydro-slotting perforation is the ecologically safe, environmentally friendly and effective affordable method for intensifying the operation in oil, gas, injection and hydro-geological wells. Now this method is widely used in Azerbaijan, Brazil, China, Eastern and Western Siberia, Jordan, Kazakhstan, Komi Republic, North Caucasus, Russia, Udmurtia, Ukraine, Urals, Uzbekistan and Yemen. The first mention regarding the hydro-slotting perforation in America, was in 1987 at the oil and gas conference in Texas. The first use of hydro-slotting perforation in the United States dates back to 1996, when together with Shell E & P Technology Company, discovered two wells (Abrasive Hydro jet Technology in Albert Load, Michigan). After that the hydro-slotting perforation was highly appreciated by the Department of Geophysics at Stanford University and by Division of Shell Exploration and Production by Shell E&P Technology Company. Hydro-slotting perforation was used in California, Kansas, Michigan, Montana, Nebraska, New York, Pennsylvania, Texas and Wyoming states. In Canada it has been successfully applied in Saskatchewan.\n\nFor opening of any productive layer it is necessary to open the casing, cement sheath and productive layer formation. Geophysics and mining geo-mechanics dictated the next requirements:\n\nIn the early 1970s, the Ministry of Geology of the USSR placed the Government order to scientific research institutions of the Country for the solution of the annular stress conditions and increase the permeability problem in the drilling wells. It was necessary to create the technology of opening the productive layer formation taking into account of uploading the annular stress conditions and increase the permeability in the near wellbore zone. The work to study this problem were assigned to the Institute of Oceanology and VNIMI (St. Petersburg, Russia). During the study there was done hundreds of experiments and mathematical models. It was determined, that if creating a geometrically correct, extended slot, directed along the wellbore and perpendicular to it on the distance from around to , in the zone of around the wellbore, there occurs the unloading of the annular compressive stress conditions from 50 to 100 percent, that are redirected to the far plane of the surface of formed slot, parallel to the wellbore surface. At the same time the permeability in this zone increased 30 to 50 percent. The holes after cumulative, jet perforation, sand jet perforation, abrasive jetting perforation and other similar methods, do not give the effect. The spot perforation did not create a slot in the casing and did not reach the required (unloading effect) depth, because the reverse jet interfered with direct jet and the maximum depth of the hole could not exceed 0.65 feet. When perforation with the movement occurs, the direct jet does not intersect with the reverse jet and depth of cutting can be much more (up to five feet) which is known as the excavation effect. Later it was proven mathematically. \n\nIt was necessary to create a device, that could make the continuation, along the borehole, slots in the casing, cement and go further into the productive formation. The tests with the movement of the multimeter tubing were not been successful, showing it was impossible to create geometrically correct extended slots with moving tubing. It was necessary to create an apparatus, that created a movement of cutting jets by itself, independently from tubing and located on the end of the tubing, directly in the leveled area. independent movement of the cutting jets could only be done mechanically, electrically or hydraulically. After another six months of research and testing it was decided to use mechanics and hydraulics as the base. The first prototype of hydro-slotting perforation device was created in 1972. The technology of hydro-slotting perforation was never sold to anyone. The hydro-slotting perforation technology was transformed into the category of performance techniques (as the technique of conducting the drilling, cumulative perforation, hydraulic fracturing, logging, pumping and so on).\n\nThe finalization of the device (prototype) in the end of 1972 was tasked to the special laboratory of the Research Institute of Oceanology of PSU \"Sevmorgeo\". From the beginning of the work for the revision the existing device was carried out in two directions: hydro-slotting perforation and hydro-mechanical slotting perforation. The second variant differs from the first in that at the beginning the opening of the casing is produced with a circular saw, and then the rock eroded by working fluid (water and sand) jets. The works were done over three years. The work for improvements of the hydro-mechanical slotting perforation were terminated in the result of their further inexpedient. Firstly, it was not necessary to divide the process into two operations: cutting the casing with the circular saw and a further jet-slotting perforation, because the cutting of casing with jet-slotting perforation takes place in a matter of seconds. Second, the mechanism of the circular saw takes up a lot of space in the housing unit, it was impossible to use the energy of working fluid to full power for getting deep slots, the slots get small and not deep (not enough for occurs the unloading of the annular compressive stress conditions and increase the permeability in the near wellbore zone). The further project was focused for finishing the hydro-slotting perforation device only. \n\nIn 1975, the scientific research laboratory of the Research Institute of Oceanology of PSU Sevmorgeo completed the project to improve the prototype of hydro-slotting perforation tool and this tool has been able to operate independently of the tubing movement. The equipment was long, OD, weight and stroke length of only, and it worked on the following principle: the energy of working fluid pressure was divided into two components. Part of energy was used for the motion creation for the working rod with the perforator and nozzles; the other part of the energy was use for the cutting process (creating the continued slots along the wellbore through the casing and cement into the productive formation). The form and depth of the slots allowed the device to perform its main task, unloading the annular stress conditions and increase the permeability. The first practical tests in the wells were successfully made at the end of 1975 on \"Archeda\" field (Volgograd, Russia). \n\n\n\nDuring the period from the date of the first prototype of hydro-slotting perforation tool to present day, the type and technological characteristics of the equipment was significantly improved. The modern underground hydro-slotting equipment represents the devices, capable to instantly cut through the steel casing, through the cement to delve into the productive formation and keep the jets in this state while moving along the borehole, keeping the same depth of cut. Hydro-slotting equipment made of special high-strength materials, long, OD, weight , cutting speed from the point of perforation to per minute, working stroke length ( x x each slot), depth of slots five feet, continued and geometrically correct slots, opening area per cut with four nozzles, can apply streamlined perforators between two and six nozzles, unloading the annular stress conditions in the near wellbore zone 50 to 100 percent, and increase the permeability 30 to 50 percent. The continuous time without lifting to the surface is 11–15 hours (nozzles lifespan ~ 15 hours, perforator ~ seven wells, hydraulic engine ~ 40 wells). \n\nWithout lifting to the surface with the hydro-slotting tool can also:\n\nThe hydro-slotting perforation process does not deform the casing, does not create cracks in the cement and does not clog up the borders in the formation. The process of hydro-slotting perforation is controlled. The cutting speed and depth of cutting may be corrected with the temperature in the borehole, temperature of the working fluid, concentration, flow and pressure. At the end of the cutting process of a continuous slot the engine is set up to the initial position and ready for the next cutting interval. Hydro-slotting perforation sets the perfect geometry for the subsequent fracturing. Hydro-slotting perforation can be applied in any formation: shale, carbonates, sandstone and so on. \n\nFurther improvement of the equipment for hydro-slotting perforation must follow the scientific and technical progress in this technology, not on the way of mindless increase of the holes in the hydro jets pipe. It is necessary to make the underground hydraulic engine for horizontal wells, which must be sealed to prevent the ingress of sand and mud inside and maintain the centerline position relative to the wellbore. It is necessary to make a self-orientation perforator (a particularly important issue of orientation in horizontal wells). For the orientation of the tool it is necessary there is communication with the tool (preferably two-sided) and surface of the well. Taking into account the specific conditions of hydro-slotting perforation process, signaling from the tool and back possibly using ultrasound only. Then the cutting process can be fully controlled from the surface, and it will be possible to change the speed and depth of cutting the slots regardless of the temperature inside the well.\n\nOver the years this method has not undergone much change, but there are many patents on the method of hydro-slotting perforation. With the development of technological progress there has been continuously improved and refined equipment, but patents, regarding the hydro-slotting equipment in full is not so much, there are a few patents on parts.\n\n"}
{"id": "474105", "url": "https://en.wikipedia.org/wiki?curid=474105", "title": "Inerting system", "text": "Inerting system\n\nAn inerting system decreases the probability of combustion of flammable materials stored in a confined space, especially a fuel tank, by maintaining a chemically non-reactive or \"inert\" gas, such as nitrogen, in such a space. \"Inerted\" fuel tanks may be used on land, or aboard ships or aircraft.\n\nThree elements are required to initiate and sustain combustion: an ignition source (heat), fuel and oxygen. Combustion may be prevented by reducing any one of these three elements. If the presence of an ignition source can not be prevented within a fuel tank, then the tank may be made non-ignitable by:\n\n\nAt present, flammable vapors in fuel tanks are rendered inert by replacing the air in the tank with an inert gas, such as nitrogen, nitrogen enriched air, steam or carbon dioxide. This reduces the oxygen concentration of the ullage to below the combustion threshold. Alternate methods based on reducing the ullage fuel-air ratio to below the LFL or increasing the fuel-air ratio to above the UFL have also been proposed.\n\nOil tankers fill the empty space above the oil cargo with inert gas to prevent fire or explosion of hydrocarbon vapors. Oil vapors cannot burn in air with less than 11% oxygen content. The inert gas may be supplied by cooling and scrubbing the flue gas produced by the ship's boilers. Where diesel engines are used, the exhaust gas may not have a low enough oxygen content so fuel-burning inert gas generators may be installed. One-way valves are installed in process piping to the tanker spaces to prevent volatile hydrocarbon vapors or mist from entering other equipment. Inert gas systems have been required on oil tankers since the SOLAS regulations of 1974. The International Maritime Organization (IMO) publishes technical standard IMO-860 describing the requirements for inert gas systems. Other types of cargo such as bulk chemicals may also be carried in inerted tanks, but the inerting gas must be compatible with the chemicals used.\n\nFuel tanks for combat aircraft have long been inerted, as well as self-sealing, but those for transport aircraft, both military and civilian, have not, largely due to cost and weight considerations. Early uses using nitrogen were on the Handley Page Halifax III and VIII, Short Stirling, and Avro Lincoln B.II, which incorporated inerting systems from around 1944. \n\nCleve Kimmel first proposed an inerting system to passenger airlines in the early 1960s. His proposed system for passenger aircraft would have used nitrogen. However, the US Federal Aviation Administration (FAA) refused to consider Kimmel's system after the airlines complained it was impractical. Indeed, early versions of Kimmel's system weighed 2,000 pounds—which would have probably made an aircraft too heavy to fly with passengers on it. However, the FAA did almost no research into making fuel tanks inert for 40 years, even in the face of several catastrophic fuel tank explosions. Instead, the FAA focused on keeping ignition sources out of the fuel tanks.\n\nThe FAA did not consider lightweight inerting systems for commercial jets until the 1996 crash of TWA Flight 800. The crash was blamed on an explosion in the center wing fuel tank of the Boeing 747 used in the flight. This tank is normally used only on very long flights, and little fuel was present in the tank at the time of the explosion. A small amount of fuel in a tank is more dangerous than a large amount, since heat entering the fuel tank with residual fuel causes the fuel to increase in temperature faster and evaporate. This causes the ullage fuel-to-air ratio to increase rapidly and exceed the lower flammability limit. A large quantity of fuel (high mass loading) in the fuel tank can retain the heat energy and slow the fuel evaporation rate. The explosion of a Thai Airways International Boeing 737 in 2001 and a Philippine Airlines 737 in 1990 also occurred in a tank that had residual fuel. The above three explosions occurred on a warm day, in the center wing tank (CWT) that is within the contours of the fuselage. These fuel tanks are located in the vicinity of external equipment that inadvertantly heats the fuel tanks. The National Transportation Safety Board's (NTSB) final report on the crash of the TWA 747 concluded “The fuel air vapor in the ullage of the TWA flight 800 CWT was flammable at the time of the accident.” NTSB identified “Elimination of Explosive Mixture in Fuel tanks in Transport Category Aircraft” as Number 1 item on its Most Wanted List in 1997.\n\nAfter the Flight 800 crash, a 2001 report by an FAA committee stated that U.S. airlines would have to spend US$35 billion to retrofit their existing aircraft fleets with inerting systems that might prevent such future explosions. However, another FAA group developed a nitrogen enriched air (NEA) based inerting system prototype that operated on compressed air supplied by the aircraft’s propulsive engines. Also, the FAA determined that the fuel tank could be rendered inert by reducing the ullage oxygen concentration to 12% rather than previously accepted threshold of 9 to 10%. Boeing commenced testing a derivative system of their own, performing successful test flights in 2003 with several 747 aircraft.\n\nThe new, simplified inerting system was originally suggested to the FAA through public comment. It uses a hollow fiber membrane material that separates supplied air into nitrogen-enriched air (NEA) and oxygen enriched air (OEA). This technology is extensively used for generating oxygen-enriched air for medical purposes. It uses a membrane that preferentially allows the nitrogen molecule (molecular weight 28) to pass through it and not the oxygen molecule (molecular weight 32).\n\nUnlike the inerting systems on military aircraft, this inerting system would run continuously to reduce fuel vapor flammability whenever the aircraft's engines are running; and its goal is to reduce oxygen content within the fuel tank to 12%, lower than normal atmospheric oxygen content of 21%, but higher than that of inerted military aircraft fuel tanks, which is a target of 9% oxygen. This is accomplished by ventilating fuel vapor laden ullage gas out of the tank and into the atmosphere.\n\nAfter what it said was seven years of investigation, the FAA proposed a rule in November 2005, in response to an NTSB recommendation, which would require airlines to \"reduce the flammability levels of fuel tank vapors on the ground and in the air\". This was a shift from the previous 40 years of policy in which the FAA focused only on reducing possible sources of ignition of fuel tank vapors.\n\nThe FAA issued the final rule on 21 July 2008. The rule amends regulations applicable to the design of new airplanes (14CFR§25.981), and introduces new regulations for continued safety (14CFR§26.31–39), Operating Requirements for Domestic Operations (14CFR§121.1117) and Operating Requirements for Foreign Air Carriers (14CFR§129.117). The regulations apply to airplanes certificated after 1 January 1958 of passenger capacity of 30 or more or payload capacity of greater than 7500 pounds. The regulations are performance based and do not require the implementation of a particular method.\n\nThe proposed rule would affect all future fixed-wing aircraft designs (passenger capacity greater than 30), and require a retrofit of more than 3,200 Airbus and Boeing aircraft with center wing fuel tanks, over nine years. The FAA had initially planned to also order installation on cargo aircraft, but this was removed from the order by the Bush administration. Additionally, regional jets and smaller commuter planes would not be subject to the rule, because the FAA does not consider them at high risk for a fuel-tank explosion.\nThe FAA estimated the cost of the program at US$808 million over the next 49 years, including US$313 million to retrofit the existing fleet. It compared this cost to an estimated US$1.2 billion \"cost to society\" from a large airliner exploding in mid-air. The proposed rule came at a time when nearly half of the U.S. airlines' capacity was on carriers that were in bankruptcy.\n\nThe order affects aircraft whose air conditioning units have a possibility of heating up what can be considered a normally empty center wing fuel tank. Some Airbus A320 and Boeing 747 aircraft are slated for \"early action\". Regarding new aircraft designs, the Airbus A380 does not have a center wing fuel tank and is therefore exempt, and the Boeing 787 has a fuel tank safety system that already complies with the proposed rule. The FAA has stated that there have been four fuel tank explosions in the previous 16 years—two on the ground, and two in the air—and that based on this statistic and on the FAA's estimate that one such explosion would happen every 60 million hours of flight time, about 9 such explosions will probably occur in the next 50 years. The inerting systems will probably prevent 8 of those 9 probable explosions, the FAA said.\nBefore the inerting system rule was proposed, Boeing stated that it would install its own inerting system on airliners it manufactures beginning in 2005. Airbus had argued that its planes' electrical wiring made the inerting system an unnecessary expense.\n\n, the FAA had a pending rule to increase the standards of on board inerting systems again. New technologies are being developed by others to provide fuel tank inerting:\n\n(1) The On-Board Inert Gas Generation System (OBIGGS) system, tested in 2004 by the FAA and NASA, with an opinion written by the FAA in 2005. This system is currently in use by many military aircraft types, including the C-17. This system provides the level of safety that the proposed increase in standards by the proposed FAA rules has been written around. Critics of this system cite the high maintenance cost reported by the military.\n\n(2) Three independent research and development firms have proposed new technologies in response to Research & Development grants by the FAA and SBA. The focus of these grants is to develop a system that is superior to OBIGGS that can replace classic inerting methods. None of these approaches has been validated in the general scientific community, nor have these efforts produced commercially available products. All the firms have issued press releases or given non-peer reviewed talks.\n\nTwo other methods in current use to inert fuel tanks are a foam suppressant system and an ullage system. The FAA has decided that the added weight of an ullage system makes it impractical for implementation in the aviation field. Some U.S. Military aircraft still use nitrogen based foam inerting systems, and some companies will ship containers of fuel with an ullage system across rail transportation routes.\n\n\n\n"}
{"id": "6595580", "url": "https://en.wikipedia.org/wiki?curid=6595580", "title": "JackBe", "text": "JackBe\n\nJackBe Corporation was a privately held vendor of enterprise mashup software for real-time intelligence applications. In August 2013 JackBe was acquired by Software AG \n\nJackBe's flagship product is an enterprise mashup platform called Presto, which is used for enterprise mashups, business management dashboards, and real-time intelligence applications.\n\nJackBe’s main product, Presto, is an enterprise mashup platform. Presto provides real-time intelligence through functionality for self-service, on-demand data integration, and business dashboards.\n\nJackBe launched a cloud computing-based version of its Presto product in March 2010. It is hosted on Amazon EC2. Jackbe launched Mashup Sites for SharePoint (MSS) in July 2010\nJackbe announced an Enterprise App Depot in 2010 as a platform for creating internal application directories. The Enterprise App Depot is aimed at non-developers (business users), allowing them to create new business applications and then share the applications with other users. Industry analyst Joe McKendrick described the Enterprise App Store as a \"cool idea\" on ZDNet.\n\n\n"}
{"id": "10700197", "url": "https://en.wikipedia.org/wiki?curid=10700197", "title": "John Innes compost", "text": "John Innes compost\n\nJohn Innes compost is a set of four formulae for growing media, developed at the former John Innes Horticultural Institution (JIHI), now the John Innes Centre, in the 1930s and released into the public domain.\n\nThe scientists who developed the formulae were William Lawrence and John Newell. The director at the time was Daniel Hall\nLawrence started to investigate the whole procedure of making seed and potting composts following a major disaster in 1933 with \"Primula sinensis\" seedlings, an important experimental plant for JIHI geneticists.\n\nAfter hundreds of trials, Lawrence and Newell arrived at two basic composts, a base fertiliser for use in the potting compost and a standard feed. The formulae of these as yet unnamed composts were published in 1938.\n\nThese composts originally provided a sterile and well balanced growing medium for the experimental plant material needed at the institute.\n\nThe Institution made the formulae generally available, but never manufactured the composts for sale nor benefited financially from their production.\nThe name ‘John Innes Compost’ was allotted in 1938–39; the horticultural retail trade in the composts made ‘John Innes’ a household name, but JIHI received no financial benefit from them.\n\nThe formulae contain loam, peat, sand or grit, and fertiliser in varying ratios for specific purposes.\nThe composts are \"soil-based\".\n\n"}
{"id": "17770721", "url": "https://en.wikipedia.org/wiki?curid=17770721", "title": "L-3 SmartDeck", "text": "L-3 SmartDeck\n\nSmartDeck - is a fully integrated cockpit system originally developed by L-3 Avionics Systems. and acquired in 2010 by Esterline CMC Electronics through an exclusive licensing agreement.\n\nSmartDeck is one of the many systems available today known as a “glass cockpit.” Popularized by large transport category aircraft in the 1980s, the glass cockpit is a high technology cockpit configuration in which the traditional flight instruments and gauges are replaced by computer screens that combine information into an organized and user friendly format. As computer technology advances, glass cockpit systems are declining in cost and becoming available in smaller general aviation aircraft. These technologies are often able to offer pilots more flight information than would be available in a conventional style cockpit and many feature a high level of automation that can aid the pilot in navigation and system monitoring.\n\nL-3 created SmartDeck as an alternative to other glass cockpit systems currently on the market. The major design objectives of integration and ease of use were achieved by designing the menu structure with a “three-clicks-or-less” philosophy similar to the Apple iPod and by incorporating navigation, weather, traffic and terrain avoidance, communication, flight controls, engine monitoring and enhanced vision into one cockpit system. This is achieved by combining a number of L-3’s situational awareness technologies into the system.\n\nAt the National Business Aviation Association annual convention in October 2010, CMC Electronics announced that it had acquired the SmartDeck technology from L-3 and L-3 ceased all development. CMC has continued the development and, as of March 2012, was expecting to announce a launch customer in the near future.\n\n\nThe user interface for a basic SmartDeck system consists of one primary flight display (PFD), one multi-function display (MFD), one flight display controller (FDC), and a center console unit (CCU) display system. Other components include two air data attitude and heading reference systems (ADAHRS), two data concentrators, two magnetometers, two WAAS GPS receivers, two nav/com radios with a PS Engineering audio panel, a transponder and the S-TEC Intelliflight 1950 Integrated Digital Flight Control System (DFCS). SmartDeck interfaces with the L-3 Avionics SkyWatch collision avoidance system, Landmark terrain awareness warning system (TAWS B), Stormscope lightning detection system and IRIS Infrared Imaging System, among other avionics technologies. The SmartDeck system is customizable for different customers and platforms.\n\nSmartDeck features a high level of redundancy that offers added safety in the event of a system failure. The dual ADAHRS continuously compare flight data and alert the pilot if the difference between the two units exceeds a predefined tolerance; during an ADAHRS miscompare, both flight displays will act as PFDs and the discrepancy will be highlighted. This is known as reversionary mode, a condition in which both screens combine all the standard PFD information with a number of key MFD functions.\n\nEach component in the system is connected via a dual IEEE 1394 interface, also known as Firewire. This high speed connection interface is common on high-speed computers and is also used on military aircraft such as the F-22 Raptor and F-35 Lightning II. Users can monitor the system health on the MFD during flight and will be notified in the event of a failed connection; however, the system will continue to function normally as long as part of the redundant network connections remain linked.\n\nThe chief purpose of the SmartDeck Primary Flight Display is to provide the attitude, airspeed, altitude, turn rate, vertical speed and course information available in the standard six pack of a conventional cockpit. In addition, the PFD gives autopilot mode information, abbreviated engine parameters, glide slope and localizer information and winds aloft. Quick reference true airspeed, ground speed, density altitude, outside air temperature, bearing, ground track, DME data, and time en route data are also displayed on the PFD. Dedicated buttons along the bottom of the PFD are used to change the reference bugs for indicated airspeed, course, heading, altitude and vertical speed as well as the barometer setting and source for navigation information. The reference bug settings also control the autopilot and flight director.\n\nSmartDeck's PFD is also equipped with synthetic vision, a 3D rendering of obstacles, terrain and airports that allows the pilot to see \"through\" weather and darkness. The image moves in real time with the aircraft and presents a clear view of the outside environment.\n\nSmartDeck’s MFD contains a host of flight information available on a number of “pages” dedicated to different functions. Each page features its own menu and submenus that are used to control the display options. The amount of information available on each screen is customizable and much of the information can be combined onto one page to decrease the need for frequently changing screens.\n\nThe map page is displayed for the majority of a routine flight on the MFD to aid the pilot in navigation and to assist with situational awareness. A moving map can be displayed in a VFR or IFR format on the MFD with an aircraft icon that represents the aircraft’s present position. A number of selectable options allow the pilot to easily customize the detail level of the moving map. Selectable map overlays include:\n\n\nAdditionally, pilot selectable traffic, weather and terrain information is available on dedicated thumbnails or overlaid on the map. A thumbnail overlay for an enhanced vision display is also available.\n\nDuring instrument approaches or while performing SIDs and STARs, a chart overlay option is available on the map page. Chart overlay gives aircraft position on the designated Jeppessen chart in lieu of the map. This function allows the pilot to maintain additional situational awareness throughout the approach and departure phases of the flight.\n\nThe auxiliary page combines a large amount of aircraft system data into one easy to navigate page. The various submenus of the auxiliary page display aircraft systems, such as engine parameters and electrical; system health, which displays connections of different components; and subsystems, like GPS or transponder functionality. Also available on the Aux page are normal, abnormal and emergency checklists, aircraft performance charts and a setup page for customization of the PFD and MFD screens. Checklist progress is maintained when switching to other pages giving the pilot quick access to procedures without hindering safe navigation.\n\nThe SmartDeck CCU is a smaller display screen used for entering flight plan data, obtaining airport information, and entering nav/com frequencies or transponder codes. SmartDeck is the only glass cockpit system in the light aircraft market that includes a display dedicated to such functions. Because radio frequencies, flight plan data and airport info can also be manipulated on the MFD, SmartDeck provides a “feature in use” annunciation if the user is accessing or modifying information in two places at once.\n\nWhen airways or instrument approaches are loaded into a flight plan, the CCU will automatically change to the appropriate navigation frequencies as the flight progresses. The system displays the location identifier next to communication frequencies when selected from the database and identifies the Morse code ID for navigation frequencies. A save feature allows up to 30 flight plans with as many as 100 waypoints to be saved on the unit.\n\nThe S-TEC Intelliflight 1950 DFCS is the integrated autopilot used with SmartDeck. It is a two-axis attitude-based digital autopilot with a flight director. Autopilot controls are located on the CCU and include heading, nav, approach, indicated airspeed hold, vertical speed hold, and altitude hold buttons. With the autopilot engaged, the system can fly full instrument approaches and holds automatically as well as pilot created holds using the “place hold” function. After the desired mode is activated, autopilot parameters such as vertical speed and heading are selected using dedicated buttons along the bottom of the PFD and changed with a concentric control knob on the Flight Data Controller. The various autopilot modes include:\n\n\n\nSmartDeck has received Technical Standard Order (TSO) Authorization and Supplemental Type Certification (STC) from the FAA. The system was certified in a Cirrus SR22. A limited STC is available through aftermarket dealers for installation on the Cirrus SR22 G2 model aircraft. L-3 was also awarded the development phase for Cirrus’ new “Cirrus Vision SF50”. Later in the program, Cirrus decided to switch to a similar system by Garmin, prompting L-3 to sue them for $18M.\n\nFollowing FAA certification, SmartDeck will compete directly with the Garmin G1000, Avidyne Entegra, Chelton FlightLogic and the Collins Pro Line series.\n\n"}
{"id": "3253952", "url": "https://en.wikipedia.org/wiki?curid=3253952", "title": "List of civilian radiation accidents", "text": "List of civilian radiation accidents\n\nThis article lists notable civilian accidents involving radioactive materials or involving ionizing radiation from artificial sources such as x-ray tubes and particle accelerators. Accidents related to nuclear power that involve fissile materials are listed at List of civilian nuclear accidents. Military accidents are listed at List of military nuclear accidents.\n\nIn listing civilian radiation accidents, the following criteria have been followed:\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "10553773", "url": "https://en.wikipedia.org/wiki?curid=10553773", "title": "Marine chronometer", "text": "Marine chronometer\n\nA marine chronometer is a timepiece that is precise and accurate enough to be used as a portable time standard; it can therefore be used to determine longitude by means of celestial navigation. When first developed in the 18th century, it was a major technical achievement, as accurate knowledge of the time over a long sea voyage is necessary for navigation, lacking electronic or communications aids. The first true chronometer was the life work of one man, John Harrison, spanning 31 years of persistent experimentation and testing that revolutionized naval (and later aerial) navigation and enabling the Age of Discovery and Colonialism to accelerate.\n\nThe term \"chronometer\" was coined from the Greek words \"chronos\" (meaning time) and \"meter\" (meaning counter) in 1714 by Jeremy Thacker, an early competitor for the prize set by the Longitude Act in the same year. It has recently become more commonly used to describe watches tested and certified to meet certain precision standards. Timepieces made in Switzerland may display the word \"chronometer\" only if certified by the COSC (Official Swiss Chronometer Testing Institute).\n\nTo determine a position on the Earth's surface, it is necessary and sufficient to know the latitude, longitude, and altitude. Altitude considerations can naturally be ignored for vessels operating at sea level. Until the mid-1750s, accurate navigation at sea out of sight of land was an unsolved problem due to the difficulty in calculating longitude. Navigators could determine their latitude by measuring the sun's angle at noon (i.e., when it reached its highest point in the sky, or culmination) or, in the Northern Hemisphere, to measure the angle of Polaris (the North Star) from the horizon (usually during twilight). To find their longitude, however, they needed a time standard that would work aboard a ship. Observation of regular celestial motions, such as Galileo's method based on observing Jupiter's natural satellites, was usually not possible at sea due to the ship's motion. The lunar distances method, initially proposed by Johannes Werner in 1514, was developed in parallel with the marine chronometer. The Dutch scientist Gemma Frisius was the first to propose the use of a chronometer to determine longitude in 1530.\n\nThe purpose of a chronometer is to measure accurately the time of a known fixed location, for example Greenwich Mean Time (GMT). This is particularly important for navigation. Knowing GMT at local noon allows a navigator to use the time difference between the ship's position and the Greenwich Meridian to determine the ship's longitude. As the Earth rotates at a regular rate, the time difference between the chronometer and the ship's local time can be used to calculate the longitude of the ship relative to the Greenwich Meridian (defined as 0°) using spherical trigonometry. In modern practice, a nautical almanac and trigonometric sight-reduction tables permit navigators to measure the Sun, Moon, visible planets, or any of 57 selected stars for navigation at any time that the horizon is visible.\n\nThe creation of a timepiece which would work reliably at sea was difficult. Until the 20th century, the best timekeepers were pendulum clocks, but both the rolling of a ship at sea and the up to 0.2% variations in the gravity of Earth made a simple gravity-based pendulum useless both in theory and in practice.\n\nChristiaan Huygens, following his invention of the pendulum clock in 1656, made the first attempt at a marine chronometer in 1673 in France, under the sponsorship of Jean-Baptiste Colbert. In 1675, Huygens, who was receiving a pension from Louis XIV, invented a chronometer that employed a balance wheel and a spiral spring for regulation, instead of a pendulum, opening the way to marine chronometers and modern pocket watches and wristwatches. He obtained a patent for his invention from Colbert, but his clock remained imprecise at sea. Huygens' attempt in 1675 to obtain an English patent from Charles II stimulated Robert Hooke, who claimed to have conceived of a spring-driven clock years earlier, to attempt to produce one and patent it. During 1675 Huygens and Hooke each delivered two such devices to Charles, but none worked well and neither Huygens nor Hooke received an English patent. It was during this work that Hooke formulated what is known as Hooke's Law.\n\nThe first published use of the term was in 1684 in \"Arcanum Navarchicum\", a theoretical work by Kiel professor Matthias Wasmuth. This was followed by a further theoretical description of a chronometer in works published by English scientist William Derham in 1713. Derham's principal work, \"Physico-theology, or a demonstration of the being and attributes of God from his works of creation\", also proposed the use of vacuum sealing to ensure greater accuracy in the operation of clocks. Attempts to construct a working marine chronometer were begun by Jeremy Thacker in England in 1714, and by Henry Sully in France two years later. Sully published his work in 1726 with \"Une Horloge inventée et executée par M. Sulli\", but neither his nor Thacker's models were able to resist the rolling of the seas and keep precise time while in shipboard conditions.\n\nIn 1714, the British government offered a longitude prize for a method of determining longitude at sea, with the awards ranging from £10,000 to £20,000 (£2 million to £4 million in 2018 terms) depending on accuracy. John Harrison, a Yorkshire carpenter, submitted a project in 1730, and in 1735 completed a clock based on a pair of counter-oscillating weighted beams connected by springs whose motion was not influenced by gravity or the motion of a ship. His first two sea timepieces H1 and H2 (completed in 1741) used this system, but he realised that they had a fundamental sensitivity to centrifugal force, which meant that they could never be accurate enough at sea. Construction of his third machine, designated H3, in 1759 included novel circular balances and the invention of the bi-metallic strip and caged roller bearings, inventions which are still widely used. However, H3's circular balances still proved too inaccurate and he eventually abandoned the large machines.\n\nHarrison solved the precision problems with his much smaller H4 chronometer design in 1761. H4 looked much like a large five-inch (12 cm) diameter pocket watch. In 1761, Harrison submitted H4 for the £20,000 longitude prize. His design used a fast-beating balance wheel controlled by a temperature-compensated spiral spring. These features remained in use until stable electronic oscillators allowed very accurate portable timepieces to be made at affordable cost. In 1767, the Board of Longitude published a description of his work in \"The Principles of Mr. Harrison's time-keeper\".\n\nIn France, 1748, Pierre Le Roy invented the detent escapement characteristic of modern chronometers. In 1766, Pierre Le Roy created a revolutionary chronometer that incorporated a detent escapement, the temperature-compensated balance and the isochronous balance spring: Harrison showed the possibility of having a reliable chronometer at sea, but these developments by Le Roy are considered by Rupert Gould to be the foundation of the modern chronometer. The innovations of Le Roy made the chronometer a much more accurate piece than had been anticipated.\n\nFerdinand Berthoud in France, as well as Thomas Mudge in Britain also successfully produced marine timekeepers. Although none were simple, they proved that Harrison's design was not the only answer to the problem. The greatest strides toward practicality came at the hands of Thomas Earnshaw and John Arnold, who in 1780 developed and patented simplified, detached, \"spring detent\" escapements, moved the temperature compensation to the balance, and improved the design and manufacturing of balance springs. This combination of innovations served as the basis of marine chronometers until the electronic era.\nThe new technology was initially so expensive that not all ships carried chronometers, as illustrated by the fateful last journey of the East Indiaman \"Arniston\", shipwrecked with the loss of 372 lives. However, by 1825, the Royal Navy had begun routinely supplying its vessels with chronometers.\n\nIt was common for ships at the time to observe a time ball, such as the one at the Royal Observatory, Greenwich, to check their chronometers before departing on a long voyage. Every day, ships would anchor briefly in the River Thames at Greenwich, waiting for the ball at the observatory to drop at precisely 1pm. This practice was in small part responsible for the subsequent adoption of Greenwich Mean Time as an international standard. (Time balls became redundant around 1920 with the introduction of radio time signals, which have themselves largely been superseded by GPS time.) In addition to setting their time before departing on a voyage, ship chronometers were also routinely checked for accuracy while at sea by carrying out lunar or solar observations. In typical use, the chronometer would be mounted in a sheltered location below decks to avoid damage and exposure to the elements. Mariners would use the chronometer to set a so-called hack watch, which would be carried on deck to make the astronomical observations. Though much less accurate (and expensive) than the chronometer, the hack watch would be satisfactory for a short period of time after setting it (i.e., long enough to make the observations).\n\nAlthough industrial production methods began revolutionizing watchmaking in the middle of the 19th century, chronometer manufacture remained craft-based much longer. Around the turn of the 20th century, Swiss makers such as Ulysse Nardin made great strides toward incorporating modern production methods and using fully interchangeable parts, but it was only with the onset of World War II that the Hamilton Watch Company in the United States perfected the process of mass production, which enabled it to produce thousands of its Hamilton Model 21 and Model 22 chronometers of World War Two for the United States Navy & Army and other Allied navies. Despite Hamilton's success, chronometers made in the old way never disappeared from the marketplace during the era of mechanical timekeepers. Thomas Mercer Chronometers still makes chronometers to the present day.\n\nWithout their accuracy and the accuracy of the feats of navigation that marine chronometers enabled, it is arguable that the ascendancy of the Royal Navy, and by extension that of the British Empire, would not have occurred; the formation of the empire by wars and conquests of colonies abroad took place in a period in which British vessels had reliable navigation due to the chronometer, while their Portuguese, Dutch, and French opponents did not. For example: the French were well established in India and other places before Britain, but were defeated by naval forces in the Seven Years' War.\n\nThe most complete international collection of marine chronometers, including Harrison's H1 to H4, is at the Royal Observatory, Greenwich, in London, UK.\n\nThe crucial problem was to find a resonator that remained unaffected by the changing conditions met by a ship at sea. The balance wheel, harnessed to a spring, solved most of the problems associated with the ship's motion. Unfortunately, the elasticity of most balance spring materials changes relative to temperature. To compensate for ever-changing spring strength, the majority of chronometer balances used bi-metallic strips to move small weights toward and away from the centre of oscillation, thus altering the period of the balance to match the changing force of the spring. The balance spring problem was solved with a nickel-steel alloy named Elinvar for its invariable elasticity at normal temperatures. The inventor was Charles Édouard Guillaume, who won the 1920 Nobel Prize for physics in recognition for his metallurgical work.\n\nThe escapement serves two purposes. First, it allows the train to advance fractionally and record the balance's oscillations. At the same time, it supplies minute amounts of energy to counter tiny losses from friction, thus maintaining the momentum of the oscillating balance. The escapement is the part that ticks. Since the natural resonance of an oscillating balance serves as the heart of a chronometer, chronometer escapements are designed to interfere with the balance as little as possible. There are many constant-force and detached escapement designs, but the most common are the spring detent and pivoted detent. In both of these, a small detent locks the escape wheel and allows the balance to swing completely free of interference except for a brief moment at the centre of oscillation, when it is least susceptible to outside influences. At the centre of oscillation, a roller on the balance staff momentarily displaces the detent, allowing one tooth of the escape wheel to pass. The escape wheel tooth then imparts its energy on a second roller on the balance staff. Since the escape wheel turns in only one direction, the balance receives impulse in only one direction. On the return oscillation, a passing spring on the tip of the detent allows the unlocking roller on the staff to move by without displacing the detent. The weakest link of any mechanical timekeeper is the escapement's lubrication. When the oil thickens through age or temperature or dissipates through humidity or evaporation, the rate will change, sometimes dramatically as the balance motion decreases through higher friction in the escapement. A detent escapement has a strong advantage over other escapements as it needs no lubrication. An impulse from the escape wheel to the impulse roller is nearly dead-beat, meaning little sliding action needing lubrication. Chronometer escape wheels and passing springs are typically gold due to the metal's lower slide friction over brass and steel.\n\nChronometers often included other innovations to increase their efficiency and precision. Hard stones such as ruby and sapphire were often used as jewel bearings to decrease friction and wear of the pivots and escapement. Diamond was often used as the cap stone for the lower balance staff pivot to prevent wear from years of the heavy balance turning on the small pivot end. Until the end of mechanical chronometer production in the third quarter of the 20th century, makers continued to experiment with things like ball bearings and chrome-plated pivots.\n\nMarine chronometers always contain a maintaining power which keeps the chronometer going while it is being wound, and a power reserve to indicate how long the chronometer will continue to run without being wound. Marine chronometers are the most accurate portable mechanical clocks ever made, achieving a precision of around a 0.1 second per day or less than one minute per year. This is accurate enough to locate a ship's position within 1–2 miles (2–3 km) after a month's sea voyage.\n\nIn strictly horological terms, \"rating\" a chronometer means that prior to the instrument entering service, the average rate of gaining or losing per day is observed and recorded on a rating certificate which accompanies the instrument. This daily rate is used in the field to correct the time indicated by the instrument to get an accurate time reading. Even the best-made chronometer with the finest temperature compensation etc. exhibits two types of error, (1) random and (2) consistent. The quality of design and manufacture of the instrument keeps the random errors small. In principle, the consistent errors should be amenable to elimination by adjustment, but in practice it is not possible to make the adjustment so precisely that this error is completely eliminated, so the technique of rating is used. The rate will also change while the instrument is in service due to e.g. thickening of the oil, so on long expeditions the instrument's rate would be periodically checked against accurate time determined by astronomical observations.\n\nShips and boats commonly use electronic aids to navigation, mostly the Global Navigation Satellite Systems. However celestial navigation, which requires the use of a precise chronometer, is still a requirement for certain international mariner certifications such as Officer in Charge of Navigational Watch, and Master and Chief Mate deck officers, \nand supplements offshore yachtmasters on long-distance private cruising yachts. \nModern marine chronometers can be based on quartz clocks that are corrected periodically by GPS signals or radio time signals (see radio clock). These quartz chronometers are not always the most accurate quartz clocks when no signal is received, and their signals can be lost or blocked. However, there are quartz movements, even in wrist watches such as the Omega Marine Chronometer, that are accurate to within 5 or 20 seconds per year.\nAt least one quartz chronometer made for advanced navigation utilizes multiple quartz crystals which are corrected by a computer using an average value, in addition to GPS time signal corrections.\n\n\n"}
{"id": "5375682", "url": "https://en.wikipedia.org/wiki?curid=5375682", "title": "Memory disambiguation", "text": "Memory disambiguation\n\nMemory disambiguation is a set of techniques employed by high-performance out-of-order execution microprocessors that execute memory access instructions (loads and stores) out of program order. The mechanisms for performing memory disambiguation, implemented using digital logic inside the microprocessor core, detect true dependencies between memory operations at execution time and allow the processor to recover when a dependence has been violated. They also eliminate spurious memory dependencies and allow for greater instruction-level parallelism by allowing safe out-of-order execution of loads and stores.\n\nWhen attempting to execute instructions out of order, a microprocessor must respect true dependencies between instructions. For example, consider a simple true dependence:\nIn this example, the codice_1 instruction on line 2 is dependent on the codice_1 instruction on line 1 because the register R1 is a source operand of the addition operation on line 2. The codice_1 on line 2 cannot execute until the codice_1 on line 1 completes. In this case, the dependence is static and easily determined by a microprocessor, because the sources and destinations are registers. The destination register of the codice_1 instruction on line 1 (codice_6) is part of the instruction encoding, and so can be determined by the microprocessor early on, during the decode stage of the pipeline. Similarly, the source registers of the codice_1 instruction on line 2 (codice_6 and codice_9) are also encoded into the instruction itself and are determined in decode. To respect this true dependence, the microprocessor's scheduler logic will issue these instructions in the correct order (instruction 1 first, followed by instruction 2) so that the results of 1 are available when instruction 2 needs them.\n\nComplications arise when the dependence is not statically determinable. Such non-static dependencies arise with memory instructions (loads and stores) because the location of the operand may be indirectly specified as a register operand rather than directly specified in the instruction encoding itself.\nHere, the store instruction writes a value to the memory location specified by the value in the address (R2+2), and the load instruction reads the value at the memory location specified by the value in address (R4+4). The microprocessor cannot statically determine, prior to execution, if the memory locations specified in these two instructions are different, or are the same location, because the locations depend on the values in R2 and R4. If the locations are different, the instructions are independent and can be successfully executed out of order. However, if the locations are the same, then the load instruction is dependent on the store to produce its value. This is known as an ambiguous dependence.\n\nExecuting loads and stores out of order can produce incorrect results if a dependent load/store pair was executed out of order. Consider the following code snippet, given in MIPS assembly:\n\nAssume that the scheduling logic will issue an instruction to the execution unit when all of its register operands are ready. Further assume that registers codice_10 and codice_11 are ready: the values in codice_10 and codice_11 were computed a long time ago and have not changed. However, assume codice_14 is not ready: its value is still in the process of being computed by the codice_15 (integer divide) instruction. Finally, assume that registers codice_10 and codice_11 hold the same value, and thus all the loads and stores in the snippet access the same memory word.\n\nIn this situation, the codice_18 instruction on line 2 is not ready to execute, but the codice_19 instruction on line 3 is ready. If the processor allows the codice_20 instruction to execute before the codice_21, the load will read an old value from the memory system; however, it should have read the value that was just written there by the codice_21. The load and store executed out of program order, but there was a memory dependence between them that was violated.\n\nSimilarly, assume that register codice_23 \"is\" ready. The codice_24 instruction on line 4 is also ready to execute, and it may execute before the preceding codice_19 on line 3. If this occurs, the codice_19 instruction will read the \"wrong\" value from the memory system, since a later store instruction wrote its value there before the load executed.\n\nMemory dependencies come in three flavors:\n\n\nThe three dependencies are shown in the preceding code segment (reproduced for clarity):\n\n\nModern microprocessors use the following mechanisms, implemented in hardware, to resolve ambiguous dependences and recover when a dependence was violated.\n\nValues from store instructions are not committed to the memory system (in modern microprocessors, CPU cache) when they execute. Instead, the store instructions, including the memory address and store data, are buffered in a store queue until they reach the retirement point. When a store retires, it \"then\" writes its value to the memory system. This avoids the WAR and WAW dependence problems shown in the code snippet above where an earlier load receives an incorrect value from the memory system because a later store was allowed to execute before the earlier load.\n\nAdditionally, buffering stores until retirement allows processors to speculatively execute store instructions that follow an instruction that may produce an exception (such as a load of a bad address, divide by zero, etc.) or a conditional branch instruction whose direction (taken or not taken) is not yet known. If the exception-producing instruction has not executed or the branch direction was predicted incorrectly, the processor will have fetched and executed instructions on a \"wrong path.\" These instructions should not have been executed at all; the exception condition should have occurred before any of the speculative instructions executed, or the branch should have gone the other direction and caused different instructions to be fetched and executed. The processor must \"throw away\" any results from the bad-path, speculatively-executed instructions when it discovers the exception or branch misprediction. The complication for stores is that any stores on the bad or mispredicted path should not have committed their values to the memory system; if the stores had committed their values, it would be impossible to \"throw away\" the commit, and the memory state of the machine would be corrupted by data from a store instruction that should not have executed.\n\nThus, without store buffering, stores cannot execute until all previous possibly-exception-causing instructions have executed (and not caused an exception) and all previous branch directions are known. Forcing stores to wait until branch directions and exceptions are known significantly reduces the out-of-order aggressiveness and limits ILP (Instruction level parallelism) and performance. With store buffering, stores can execute ahead of exception-causing or unresolved branch instructions, buffering their data in the store queue but not committing their values until retirement. This prevents stores on mispredicted or bad paths from committing their values to the memory system while still offering the increased ILP and performance from full out-of-order execution of stores.\n\nBuffering stores until retirement avoids WAW and WAR dependencies but introduces a new issue. Consider the following scenario: a store executes and buffers its address and data in the store queue. A few instructions later, a load executes that reads from the same memory address to which the store just wrote. If the load reads its data from the memory system, it will read an old value that would have been overwritten by the preceding store. The data obtained by the load will be incorrect.\n\nTo solve this problem, processors employ a technique called store-to-load forwarding using the store queue. In addition to buffering stores until retirement, the store queue serves a second purpose: forwarding data from completed but not-yet-retired (\"in-flight\") stores to later loads. Rather than a simple FIFO queue, the store queue is really a Content-Addressable Memory (CAM) searched using the memory address. When a load executes, it searches the store queue for in-flight stores to the same address that are logically earlier in program order. If a matching store exists, the load obtains its data value from that store instead of the memory system. If there is no matching store, the load accesses the memory system as usual; any preceding, matching stores must have already retired and committed their values. This technique allows loads to obtain correct data if their producer store has completed but not yet retired.\n\nMultiple stores to the load's memory address may be present in the store queue. To handle this case, the store queue is priority encoded to select the \"latest\" store that is logically earlier than the load in program order. The determination of which store is \"latest\" can be achieved by attaching some sort of timestamp to the instructions as they are fetched and decoded, or alternatively by knowing the relative position (slot) of the load with respect to the oldest and newest stores within the store queue.\n\nModern out-of-order CPUs can use a number of techniques to detect a RAW dependence violation, but all techniques require tracking in-flight loads from execution until retirement. When a load executes, it accesses the memory system and/or store queue to obtain its data value, and then its address and data are buffered in a load queue until retirement. The load queue is similar in structure and function to the store queue, and in fact in some processors may be combined with the store queue in a single structure called a load-store queue, or LSQ. The following techniques are used or have been proposed to detect RAW dependence violations:\n\nWith this technique, the load queue, like the store queue, is a CAM searched using the memory access address, and keeps track of all in-flight loads. When a store executes, it searches the load queue for completed loads from the same address that are logically later in program order. If such a matching load exists, it must have executed before the store and thus read an incorrect, old value from the memory system/store queue. Any instructions that used the load's value have also used bad data. To recover if such a violation is detected, the load is marked as \"violated\" in the retirement buffer. The store remains in the store queue and retirement buffer and retires normally, committing its value to the memory system when it retires. However, when the violated load reaches the retirement point, the processor flushes the pipeline and restarts execution from the load instruction. At this point, all previous stores have committed their values to the memory system. The load instruction will now read the correct value from the memory system, and any dependent instructions will re-execute using the correct value.\n\nThis technique requires an associative search of the load queue on every store execution, which consumes circuit power and can prove to be a difficult timing path for large load queues. However, it does not require any additional memory (cache) ports or create resource conflicts with other loads or stores that are executing.\n\nWith this technique, load instructions that have executed out-of-order are re-executed (they access the memory system and read the value from their address a second time) when they reach the retirement point. Since the load is now the retiring instruction, it has no dependencies on any instruction still in-flight; all stores ahead of it have committed their values to the memory system, and so any value read from the memory system is guaranteed to be correct. The value read from memory at re-execution time is compared to the value obtained when the load first executed. If the values are the same, the original value was correct and no violation has occurred. If the re-execution value differs from the original value, a RAW violation has occurred and the pipeline must be flushed because instructions dependent on the load have used an incorrect value.\n\nThis technique is conceptually simpler than the load queue search, and it eliminates a second CAM and its power-hungry search (the load queue can now be a simple FIFO queue). Since the load must re-access the memory system just before retirement, the access must be very fast, so this scheme relies on a fast cache. No matter how fast the cache is, however, the second memory system access for every out-of-order load instruction does increase instruction retirement latency and increases the total number of cache accesses that must be performed by the processor. The additional retire-time cache access can be satisfied by re-using an existing cache port; however, this creates port resource contention with other loads and stores in the processor trying to execute, and thus may cause a decrease in performance. Alternatively, an additional cache port can be added just for load disambiguation, but this increases the complexity, power, and area of the cache. Some recent work (Roth 2005) has shown ways to filter many loads from re-executing if it is known that no RAW dependence violation could have occurred; such a technique would help or eliminate such latency and resource contention.\n\nA minor benefit of this scheme (compared to a load-queue search) is that it will not flag a RAW dependence violation and trigger a pipeline flush if a store that would have caused a RAW dependence violation (the store's address matches an in-flight load's address) has a data value that matches the data value already in the cache. In the load-queue search scheme, an additional data comparison would need to be added to the load-queue search hardware to prevent such a pipeline flush.\n\nCPUs that fully support out-of-order execution of loads and stores must be able to detect RAW dependence violations when they occur. However, many CPUs avoid this problem by forcing all loads and stores to execute in-order, or by supporting only a limited form of out-of-order load/store execution. This approach offers lower performance compared to supporting full out-of-order load/store execution, but it can significantly reduce the complexity of the execution core and caches.\n\nThe first option, making loads and stores go in-order, avoids RAW dependences because there is no possibility of a load executing before its producer store and obtaining incorrect data. Another possibility is to effectively break loads and stores into two operations: address generation and cache access. With these two separate but linked operations, the CPU can allow loads and stores to access the memory system only once all previous loads and stores have had their address generated and buffered in the LSQ. After address generation, there are no longer any ambiguous dependencies since all addresses are known, and so dependent loads will not be executed until their corresponding stores complete. This scheme still allows for some \"out-of-orderness\"—the address generation operations for any in-flight loads and stores can execute out-of-order, and once addresses have been generated, the cache accesses for each load or store can happen in any order that respects the (now known) true dependences.\n\nProcessors that fully support out-of-order load/store execution can use an additional, related technique, called memory dependence prediction, to attempt to predict true dependences between loads and stores \"before\" their addresses are known. Using this technique, the processor can prevent loads that are predicted to be dependent on an in-flight store from executing before that store completes, avoiding a RAW dependence violation and thus avoiding the pipeline flush and the performance penalty that is incurred. See the memory dependence prediction article for more details.\n\n"}
{"id": "3281646", "url": "https://en.wikipedia.org/wiki?curid=3281646", "title": "Mesh analysis", "text": "Mesh analysis\n\nMesh analysis (or the mesh current method) is a method that is used to solve planar circuits for the currents (and indirectly the voltages) at any place in the electrical circuit. Planar circuits are circuits that can be drawn on a plane surface with no wires crossing each other. A more general technique, called loop analysis (with the corresponding network variables called loop currents) can be applied to any circuit, planar or not. Mesh analysis and loop analysis both make use of Kirchhoff’s voltage law to arrive at a set of equations guaranteed to be solvable if the circuit has a solution. Mesh analysis is usually easier to use when the circuit is planar, compared to loop analysis.\n\nMesh analysis works by arbitrarily assigning mesh currents in the essential meshes (also referred to as independent meshes). An essential mesh is a loop in the circuit that does not contain any other loop. Figure 1 labels the essential meshes with one, two, and three.\n\nA mesh current is a current that loops around the essential mesh and the equations are set solved in terms of them. A mesh current may not correspond to any physically flowing current, but the physical currents are easily found from them. It is usual practice to have all the mesh currents loop in the same direction. This helps prevent errors when writing out the equations. The convention is to have all the mesh currents looping in a clockwise direction. Figure 2 shows the same circuit from Figure 1 with the mesh currents labeled.\n\nSolving for mesh currents instead of directly applying Kirchhoff's current law and Kirchhoff's voltage law can greatly reduce the amount of calculation required. This is because there are fewer mesh currents than there are physical branch currents. In figure 2 for example, there are six branch currents but only three mesh currents.\n\nEach mesh produces one equation. These equations are the sum of the voltage drops in a complete loop of the mesh current. For problems more general than those including current and voltage sources, the voltage drops will be the impedance of the electronic component multiplied by the mesh current in that loop.\n\nIf a voltage source is present within the mesh loop, the voltage at the source is either added or subtracted depending on if it is a voltage drop or a voltage rise in the direction of the mesh current. For a current source that is not contained between two meshes, the mesh current will take the positive or negative value of the current source depending on if the mesh current is in the same or opposite direction of the current source. The following is the same circuit from above with the equations needed to solve for all the currents in the circuit.\n\nformula_1\nOnce the equations are found, the system of linear equations can be solved by using any technique to solve linear equations.\nThere are two special cases in mesh current: currents containing a supermesh and currents containing dependent sources.\n\nA supermesh occurs when a current source is contained between two essential meshes. The circuit is first treated as if the current source is not there. This leads to one equation that incorporates two mesh currents. Once this equation is formed, an equation is needed that relates the two mesh currents with the current source. This will be an equation where the current source is equal to one of the mesh currents minus the other. The following is a simple example of dealing with a supermesh.\n\nformula_2\n\nA dependent source is a current source or voltage source that depends on the voltage or current of another element in the circuit. When a dependent source is contained within an essential mesh, the dependent source should be treated like an independent source. After the mesh equation is formed, a dependent source equation is needed. This equation is generally called a constraint equation. This is an equation that relates the dependent source’s variable to the voltage or current that the source depends on in the circuit. The following is a simple example of a dependent source.\n\nformula_3\n\n"}
{"id": "7661723", "url": "https://en.wikipedia.org/wiki?curid=7661723", "title": "Molten salt", "text": "Molten salt\n\nMolten salt is salt which is solid at standard temperature and pressure (STP) but enters the liquid phase due to elevated temperature. A salt that is normally liquid even at STP is usually called a room temperature ionic liquid, although technically molten salts are a class of ionic liquids.\n\nMolten salts have a variety of uses. Molten chloride salt mixtures are commonly used as baths for various alloy heat treatments, such as annealing and martempering of steel. Cyanide and chloride salt mixtures are used for surface modification of alloys such as carburizing and nitrocarburizing of steel. Cryolite (a fluoride salt) is used as a solvent for aluminium oxide in the production of aluminium in the Hall-Héroult process. Fluoride, chloride, and hydroxide salts can be used as solvents in pyroprocessing of nuclear fuel. Molten salts (fluoride, chloride, and nitrate) can also be used as heat transfer fluids as well as for thermal storage. This thermal storage is commonly used in molten salt power plants.\n\nA commonly used thermal salt is the eutectic mixture of 60% sodium nitrate and 40% potassium nitrate, which can be used as liquid between 260-550 °C. It has a heat of fusion of 161 J/g, and a heat capacity of 1.53 J/(g K).\n\nExperimental salts using lithium may have a melting point of 116 °C while still having a heat capacity of 1.54 J/(g K).\nSalts may cost $1,000 per ton, and a typical plant may use 30,000 tons of salt.\n\nRegular table salt has a melting point of 800 °C and a heat of fusion of 520 J/g.\n\nAmbient temperature molten salts are present in the liquid phase at standard conditions for temperature and pressure. Examples of such salts include \"N\"-ethylpyridinium bromide and aluminium chloride mix, discovered in 1951 and ethylammonium nitrate discovered by Paul Walden. Other ionic liquids take advantage of asymmetrical quaternary ammonium cations like alkylated imidazolium ions, and large, branched anions like the bistriflimide ion.\n\n\n\nC.F. Baes, \"The chemistry and thermodynamics of molten salt reactor fuels\", Proc. AIME Nuclear Fuel Reprocessing Symposium, Ames, Iowa, USA, 1969 (August 25)\n"}
{"id": "34342785", "url": "https://en.wikipedia.org/wiki?curid=34342785", "title": "Nasal EPAP", "text": "Nasal EPAP\n\nNasal expiratory positive airway pressure (Nasal EPAP) is a treatment for obstructive sleep apnea (OSA) and snoring.\n\nContemporary EPAP devices have two small valves that allow air to be drawn in through each nostril, but not exhaled; the valves are held in place by adhesive tabs on the outside of the nose. The mechanism by which EPAP may work is not clear; it may be that the resistance to nasal exhalation leads to a buildup in CO which in turn increases respiratory drive, or that resistance to exhalation generates pressure that forces the upper airway to open wider.\n\nIn OSA it appears to be effective to reduce but not eliminate apnea for people with mild to moderate OSA (Apnea–hypopnea index < 30) and for people who cannot tolerate CPAP, but within those groups it is not clear why some respond and others do not, and the evidence consists of small clinical trials with follow-up no longer than one year. As of 2015 there was evidence from one small trial that it may be useful in children with OSA. It has shown evidence of reducing snoring as well.\n\nEPAP is unlikely to be effective in people with significant nasal obstruction. Side effects that emerged during clinical trials included difficulty breathing, difficulty falling or staying asleep, dry mouth, nasal congestion, headache, difficulty putting on or removing the device, and anxiety.\n\nVentus Medical received FDA 510K clearance for marketing the Provent EPAP device for OSA in 2009. Ventus received 510K clearance for an EPAP device to treat snoring in June 2012. Ventus was acquired by Theravent Inc. in 2013.\n\nCalik MW. Treatments for Obstructive Sleep Apnea. J Clin Outcomes Manag. 2016 Apr;23(4):181-192. \n"}
{"id": "29501338", "url": "https://en.wikipedia.org/wiki?curid=29501338", "title": "Noise generator", "text": "Noise generator\n\nA noise generator is a circuit that produces electrical noise (i.e., a random signal). Noise generators are used to test signals for measuring noise figure, frequency response, and other parameters. Noise generators are also used for generating random numbers.\n\nThere are several circuits used for noise generation. For example, temperature-controlled resistors, temperature-limited vacuum diodes, zener diodes, and gas discharge tubes. A source that can be switched on and off (\"gated\") is beneficial for some test methods.\n\nNoise generators usually rely on a fundamental noise process such as thermal noise or shot noise.\n\nThermal noise can be a fundamental standard. A resistor at a certain temperature has a thermal noise associated with it. A noise generator might have two resistors at different temperatures and switch between the two resistors. The resulting output power is low. (For a 1 kΩ resistor at room temperature and a 10 kHz bandwidth, the RMS noise voltage is 400 nV.)\n\nIf electrons flow across a barrier, then they have discrete arrival times. Those discrete arrivals exhibit shot noise. The output noise level of a shot noise generator is easily set by the DC bias current. Typically, the barrier in a diode is used.\n\nDifferent noise generator circuits use different methods of setting the DC bias current.\n\nOne common noise source was a thermally-limited (\"saturated-emission\") hot-cathode vacuum tube diode. These sources could serve as white noise generators from a few kilohertz through UHF and were available in normal radio tube glass envelopes. Flicker (1/\"f\") noise limited application at lower frequencies; electron transit time limited application at higher frequencies. The basic design was a diode vacuum tube with a heated filament. The temperature of the cathode (filament) sets the anode (plate) current that determines the shot noise; see Richardson equation. The anode voltage is set large enough to collect all the electrons emitted by the filament. If the plate voltage were too low, then there would be space charge near the filament that would affect the noise output. For a calibrated generator, care must be taken so that the shot noise dominates the thermal noise of the tube's plate resistance and other circuit elements.\n\nLong, thin, hot-cathode gas-discharge glass tubes fitted with a normal bayonet light bulb mount for the filament and an anode top cap, were used for SHF frequencies and diagonal insertion into a waveguide. They were filled with a pure inert gas such as neon because mixtures made the output temperature-dependent. Their burning voltage was under 200 V, but they needed optical priming (pre-ionizing) by a 2-Watt incandescent lamp prior to ignition by an anode voltage spike in the 5-kV range.\n\nOne miniature thyratron found an additional use as a noise source, when operated as a diode (grid tied to cathode) in a transverse magnetic field.\n\nAnother possibility is using the collector current in a transistor.\n\nReverse-biased diodes in breakdown can also be used as shot noise sources. Voltage regulator diodes are common, but there are two different breakdown mechanisms, and they have different noise characteristics. The mechanisms are the Zener effect and avalanche breakdown.\n\nReverse-biased diodes and bipolar transistor base-emitter junctions that breakdown below about 7 volts primarily exhibit the Zener effect; the breakdown is due to internal field emission. The junctions are thin, and the electric field is high. Zener breakdown is shot noise. The flicker (1/\"f\") noise corner can be below 10 Hz.\n\nThe noise generated by zener diodes is a simple shot noise.\n\nFor breakdown voltages greater than 7 volts, the semiconductor junction width is thicker and primary breakdown mechanism is an avalanche. The noise output is more complicated. There is excess noise (i.e., noise over and above the simple shot noise) because there is avalanche multiplication. \n\nFor higher power output noise generators, amplification is needed. For broadband noise generators, that amplification can be difficult to achieve. One method uses avalanche multiplication within the same barrier that generates the noise. In an avalanche, one carrier collides with other atoms and knocks free new carriers. The result is that for each carrier that starts across a barrier, several carriers synchronously arrive. The result is a wide-bandwidth high-power source. Conventional diodes can be used in breakdown.\n\nThe avalanche breakdown also has multistate noise. The noise output power randomly switches among several output levels. Multistate noise looks somewhat like flicker (1/\"f\") noise. The effect is process dependent, but it can be minimized. Diodes may also be selected for low multistate noise.\n\nA commercial example of an avalanche diode noise generator is the Agilent 346C that covers 10 MHz to 26.5 GHz.\n\n\n"}
{"id": "508665", "url": "https://en.wikipedia.org/wiki?curid=508665", "title": "OpenCores", "text": "OpenCores\n\nOpenCores is a community developing digital open-source hardware through electronic design automation, with a similar ethos as the free software movement. OpenCores hopes to eliminate redundant design work and slash development costs. A number of companies have been reported as adopting OpenCores IP in chips, or as adjuncts to EDA tools. OpenCores is also cited from time to time in the electronics press as an example of open source in the electronics hardware community.\n\nOpenCores has always been a commercially owned organization. In 2015, the core active users of OpenCores established the independent Free and Open Source Silicon Foundation (FOSSi), and registered the libreCores.org website as the basis for all future development, independent of commercial control.\n\nDamjan Lampret, one of the founders of OpenCores, stated on his website that it began in 1999. The first public record of the new website and its objectives was in \"EE Times\" in 2000. Then \"CNET News\" reported in 2001. Through the following years it was supported by advertising and sponsorship, including by Flextronics.\n\nIn mid-2007 an appeal was put out for a new backer, and that November, Swedish design house ORSoC AB agreed to take over maintenance of the OpenCores website.\n\n\"EE Times\" reported in late 2008 that OpenCores had passed the 20,000 subscriber mark. In October 2010 it reached 95,000 registered users and had approximately 800 projects. In July 2012 it reached 150,000 registered users.\n\nDuring 2015, ORSoC AB formed a joint venture with KNCMiner AB to develop bitcoin mining machines. As this became the primary focus of the business, they were able to spend less time with the opencores.org project. In response to the growing lack of commitment, the core OpenRISC development team set up the Free and Open Source Silicon Foundation (FOSSi), and registered the libreCores.org website as the basis for all future development, independent of commercial control.\n\nIn the absence of a widely accepted open source hardware license, the components produced by the OpenCores initiative use several different software licenses. The most common is the GNU LGPL, which states that any modifications to a component must be shared with the community, while one can still use it together with proprietary components. The less restrictive 3-clause BSD license is also used in some hardware projects, while the GNU GPL is often used for software components, such as models and firmware.\n\nThe library will consist of design elements from central processing units, memory controllers, peripherals, motherboards, and other components. Emerging semiconductor manufacturers could use the information and license designs for free.\n\nThe emphasis is on digital modules called \"cores\", commonly known as IP Cores. The components are used for creating both custom integrated circuits (ASICs) and FPGAs.\n\nThe cores are implemented in the hardware description languages Verilog, VHDL or SystemC which may be synthesized to either silicon or gate arrays.\n\nThe project aims at using a common non-proprietary system bus named Wishbone, and most components are nowadays adapted to this bus.\n\nAmong the components created by OpenCores contributors are:\n\n\nIn April 2011 OpenCores opened donations for a new project to develop a complete system on a chip design based on the OpenRISC processor and implement it into an ASIC-component. OpenCores affiliated with OpenCores, for example OpenSPARC and LEON.\n\n\n"}
{"id": "24619003", "url": "https://en.wikipedia.org/wiki?curid=24619003", "title": "PICMG 1.0", "text": "PICMG 1.0\n\nPICMG 1.0 is a PICMG specification that defines a CPU form factor and corresponding backplane connectors for PCI-ISA passive backplanes. This standard moves components typically located on the motherboard (i.e. memory, CPUs and chipset components) to a single plug-in card. PICMG 1.0 CPU Cards look much like standard ISA cards with extra gold finger connections for the ISA bus and the root PCI bus. The \"motherboard\" is replaced with a simple \"passive backplane\" that has only PCI and ISA connectors attached to it. These backplane connections include a dedicated system slot of the PICMG 1.0 CPU and various connections for standard ISA and PCI peripheral cards. This backplane is simple and robust, with a very low likelihood of failure, given its passive nature. This allows a much lower Mean Time to Repair than classic computer motherboard approaches, as electronics associated with CPUs can be replaced without having to remove peripheral devices.\n\nAdopted : 10/10/1994\n\nCurrent Revision : 2.0\n\n\nAdopted : 5/25/1995\n\nCurrent Revision : 1.1\n"}
{"id": "43051116", "url": "https://en.wikipedia.org/wiki?curid=43051116", "title": "Quality Technology Services", "text": "Quality Technology Services\n\nQTS Realty Trust, Inc. (popularly known as Quality Technology Services or QTS) is a real estate investment trust that invests in carrier-neutral data centers and provides colocation and peering services. As of December 31, 2017, the company owned 25 operating data center facilities comprising over 5.7 million net rentable square feet. The company has over 1,100 customers including enterprises, network operators, cloud providers, and supporting service providers.\n\nIn 2003, Chad Williams founded the company with the purchase of a data center in Kansas.\n\nIn October 2005, the company acquired Deltacom's e^deltacom business unit and its data center in Suwanee, Georgia for $26 million.\n\nIn October 2006, the company acquired a 960,000 square foot data center in Atlanta as well as Globix Hosting LLC for a total of $161 million. The company also acquired NTT USA LLC, which owned a 130,000 square foot facility in the New York City area.\n\nIn December 2007, the company acquired the customers of First National Technology Solutions. The company also acquired 120,000 square feet of data center and office technology space in Silicon Valley.\n\nIn April 2008, the company expanded into Florida.\n\nIn 2011, the company acquired a data center in Lenexa, Kansas.\n\nIn January 2013, the company acquired Herakles LLC and its 92,000-square-foot data center in San Francisco.\n\nIn February 2013, the company acquired 40 acres in Irving, Texas for construction of a data center facility.\n\nIn October 2013, the company became a public company via an initial public offering.\n\nIn April 2014, the company acquired the former Chicago Sun-Times plant with plans to convert it into a data center.\n\nIn June 2015, the company acquired Carpathia Hosting for $326 million.\n\nIn June 2016, the company acquired a data center campus in New Jersey from DuPont Fabros Technology for $125 million.\n\nIn February 2017, the company acquired a 260,000 square foot data center in Irving, Texas for $50 million.\n\nIn May 2017, the company acquired a 3.4-acre parcel next to its Atlanta facility for $1 million.\n\n \n"}
{"id": "14358884", "url": "https://en.wikipedia.org/wiki?curid=14358884", "title": "Reflector (photography)", "text": "Reflector (photography)\n\nIn photography and cinematography, a reflector is an improvised or specialised reflective surface used to redirect light towards a given subject or scene.\n\nApart from certain highly specialized components found in enlargers, projectors and scanners, photographic reflectors fall into two main groups:\n\nSimilar to a domestic lampshade, these reflectors are fixed to an artificial light source (for example, a filament bulb or flash tube) to direct and shape the otherwise scattered light, reflecting it off their concave inner surfaces and directing it towards the scene to be photographed. Although there are a large number of variants, the most common types are:\n\n\nThe reflector factor is the ratio of the illumination provided by a lamp fitted within a reflector to the illumination provided without any reflector fitted. A matte reflector will typically have a reflector factor of around 2, due to its more diffuse effect, while a polished or metallic-finished reflector may have a factor of up to 6.\n\nAlso known as plane reflectors, \"flats\" or bounce boards, this kind of reflector is located independent of a light source; the light is reflected off its surface, either to achieve a broader light source, or control shadows and highlights, or both. This kind of reflector generally has a very low reflectivity factor that varies widely according to surface texture and colour. As a result, it is most commonly used to control contrast in both artificial and natural lighting, in place of a fill light or \"kick\" light. In this case, light \"spilling\" from the main ambient or key light illuminating a scene is reflected back into the scene with a varying degrees of precision and intensity, according to the chosen reflective surface and its position relative to the scene.\n\nReflectors may also be used as a means of increasing the size of the main light source, which may (or may not) retain a direct path to the scene. By positioning a board reflector close to a light source, its effective size can be increased by \"bouncing\" the light off it. A very common example of this technique is the traditional umbrella reflector, invented by George Larson , typically having a gold, silver or matte white interior onto which a lamp fitted with a circular reflector is projected, providing a broad, soft illumination. The lamp faces away from the scene to be photographed, allowing only reflected light to be thrown forward.\nTypes of Reflector.\n1 Dispersive reflector.\n2 Holophane.\n3 optical combination\n4 concentrating\n5 Angle.\n\nReflectors vary enormously in size, colour, reflectivity and portability. In tabletop still life photography, small mirrors and card stock are used extensively, both to reduce lighting contrast and create highlights on reflective subjects such as glassware and jewelry. Larger-scale subjects such as motor vehicles require the use of huge \"flats\", often requiring specialised motorized winches to position them accurately. Location photography calls for much more portable materials and a large range of lightweight, folding reflectors are commercially available in a variety of colors.\n\nPhotographers make regular use of walls, ceilings and even entire rooms as reflectors, especially with the interior of buildings which may lack sufficient available light. Often known as \"bounce flash\" photography, but equally common with Tungsten lights in cinematography, this technique was pioneered by Subrata Mitra in 1956.\n\nThe area to be photographed is lit by walls off-camera, which then provide illumination similar to that of a large window. When \"bounced\" off a ceiling, the lighting resembles that of fluorescent tubes. As this very broad, flat lighting is more typical of an overcast day outdoors, a more realistic interior illumination is achieved by reducing the power of additional lighting relative to the available light, so that either source may act as a fill to the other. Hence bounce lighting may provide either the primary or secondary (fill) light source, depending on its intensity.\n\nWalls also make ideal reflectors outdoors, reflecting sunlight back upon a subject and reducing shadows (and hence overall contrast) according to the color, size and proximity of the wall. A more readily available alternative is the portable, lightweight, collapsible reflector, commercially available in a range of sizes and colors, or improvised using a sheet of card stock or even a bed sheet. Stands may be erected to retain these reflectors, although it is often much more convenient and practical to have an assistant hold and move them.\n\n"}
{"id": "41732397", "url": "https://en.wikipedia.org/wiki?curid=41732397", "title": "Room Key", "text": "Room Key\n\nRoom Key is a hotel metasearch engine launched in January 2012. Room Key is a joint venture founded by Choice Hotels International, Hilton Worldwide, Hyatt Hotels Corporation, InterContinental Hotels Group, Marriott International, and Wyndham Worldwide or their respective affiliates. , there are over 45,000 hotels listed on Room Key, which span 150 countries and include over 60 well-known brands.\n\nRoom Key's founder, John Davis, previously founded Pegasus Solutions, the first hotel distribution switch technology which streamlined global distribution systems and Internet connectivity for hotels worldwide.\n\nThe company is headquartered in Dallas, Texas. Its interim CEO is Steve Sickel.\n\nRoom Key’s history traces to 2010 with talks between hoteliers. In 2011, Room Key acquired hotelicopter, a Charlottesville, VA-based firm, and its technology platform, which provided the technology foundation for Roomkey.com. Room Key launched its beta site and added Best Western International as its first commercial partner in January 2012. In March 2012, Room Key launched international websites in the United Kingdom, Canada, Australia and New Zealand.\n\nRoom Key added Preferred Hotel Group as a commercial partner in March 2012 and WorldHotels in April 2012. Room Key exited beta and fully launched in May 2012.\n\nIn September 2012, Room Key added La Quinta Inns & Suites, Millennium & Copthorne Hotels and Leading Hotels of the World as commercial partners. Carlson Rezidor Hotel Group was added as a commercial partner in November 2012.\n\nIn September 2014, Davis stepped down as CEO of Room Key. Steve Sickel was named interim CEO.\n\nRoom Key is owned by its founding partners: Choice Hotels International, Hilton Worldwide, Hyatt Hotels Corporation, InterContinental Hotels Group, Marriott International, and Wyndham Worldwide or their respective affiliates. Along with its founding partners’ properties, Room Key’s website includes properties from commercial partners. Commercial partners do not have an ownership stake in Room Key. Room Key also has partnerships with several central reservation systems (CRS) including Sabre SynXis, Trust and TravelClick, which enable the listing of thousands of additional hotels on Room Key’s website.\n\nWhen a Room Key user attempts to book a room with any of Room Key’s founding or commercial partners, the customer is directed to the partner’s site and books a room directly with the hotel.\n\n"}
{"id": "416779", "url": "https://en.wikipedia.org/wiki?curid=416779", "title": "Scenic design", "text": "Scenic design\n\nScenic design (also known as scenography, stage design, set design, or production design) is the creation of theatrical, as well as film or television scenery. Scenic designers come from a variety of artistic backgrounds, but in recent years, are mostly trained professionals, holding a B.F.A. or M.F.A. degrees in theater arts. Scenic designers design sets and scenery that aim to support the overall artistic goals of the production.\n\nA designer looks at the details searching for evidence through research to produce conceptual ideas that’s best toward supporting the content and values with visual elements. The subject of, “How do we generate creative ideas?” is a very legitimate question. The most consuming part of expanding our horizons toward scenic concepts is much more than witnessing creativity, and creative people. It starts with us opening our mind to the possibilities. To have an attitude toward learning, seeking, and engaging in creativity and to be willing to be adventurous, inquisitive and curious. Our imagination is highly visual. Whether outside or inside, colorful trees or concerts, star lit skies or the architecture of a great building, scenic design is a process of discovery. Discovering what will best clarify and support the setting, environment, atmosphere, ambience, & world that is being created.\n\nThe scenic designer works with the director and other designers to establish an overall visual concept for the production and design the stage environment. They are responsible for developing a complete set of design drawings that include the following:\nAll of these required drawing elements can be easily created from one accurate 3-D CAD model of the set design.\n\nThe scenic designer is responsible for collaborating with the theatre director and other members of the production design team to create an environment for the production and then communicating the details of this environment to the technical director, production manager, charge scenic artist and prop master. Scenic designers are responsible for creating scale models of the scenery, renderings, paint elevations and scale construction drawings as part of their communication with other production staff.\n\nIn Europe and Australia, scenic designers take a more holistic approach to theatrical design and will often be responsible not only for scenic design but costume, lighting and sound and are referred to as theatre designers or scenographers or production designers.\n\nNotable scenic designers, past and present, include: Alban Piot, Adolphe Appia, Boris Aronson, Alexandre Benois, Alison Chitty, Antony McDonald, Barry Kay, Caspar Neher, Cyro Del Nero, Aleksandra Ekster, Daniil Lider, David Borovsky, David Gallo, Edward Gordon Craig, Es Devlin, Ezio Frigerio, Franco Colavecchia, Christopher Gibbs, Franco Zeffirelli, George Tsypin, Howard Bay, Inigo Jones, Jean-Pierre Ponnelle, Jo Mielziner, John Lee Beatty, Josef Svoboda, Ken Adam, Léon Bakst, Luciano Damiani, Maria Björnson, Ming Cho Lee, Motley, Natalia Goncharova, Nathan Altman, Nicholas Georgiadis, Oliver Smith, Ralph Koltai, Neil Patel, Robert Brill, Robert Wilson, Russell Patterson, Brian Sidney Bembridge, Santo Loquasto, Sean Kenny, Todd Rosenthal, Robin Wagner, Tony Walton, Roger Kirk.\n\n\n\n"}
{"id": "40775658", "url": "https://en.wikipedia.org/wiki?curid=40775658", "title": "See What You Print", "text": "See What You Print\n\nSWYP (See What You Print) is a printer concept developed by technology product design firm Artefact. The concept was released in 2011. It features simplified interactions and a touch screen that shows the user exactly what the print output will be.\n\nThe SWYP concept was the recipient of two industry awards. It received a Braun Prize Silver Award in 2012. It also received an IXDA Interaction Award for disruptive interaction design.\n"}
{"id": "10167815", "url": "https://en.wikipedia.org/wiki?curid=10167815", "title": "Shape moiré", "text": "Shape moiré\n\nShape moiré is one type of moiré patterns demonstrating the phenomenon of moiré magnification. 1D shape moiré is the particular simplified case of 2D shape moiré. One-dimensional patterns may appear when superimposing an opaque layer containing tiny horizontal transparent lines on top of a layer containing a complex shape which is periodically repeating along the vertical axis. \n\nShape moiré is sometimes referred as band moiré. The opaque layer with transparent lines is called the revealing layer. The layer containing the periodically repeating shapes is called the base layer. The period of shapes in the base layer is denoted as \"p\". The period of transparent lines in the revealing layer is denoted as \"p\". The periods of both layers must be sufficiently close. The superimposition image reveals the shapes of the base layer stretched along the vertical axis. The magnified shapes appear periodically along the vertical axis. The dimensions along the horizontal axis are not changed. If the complex shape of the base layer is a sequence of symbols (e.g. a horizontal text) compressed along the vertical axis, then the superimposition of the revealing layer can restore the original proportions of these symbols. The size along the vertical axis, \"p\", of the magnified optical shape is expressed by the following formula:\nNegative values of \"p\" signify mirrored appearance (the magnified shapes will be inverted along the vertical axis) of the stretched shapes.\n\nWhen the revealing layer is moved along the vertical axis, the magnified shapes move along the vertical axis at a faster speed. The speedup factor is expressed by the following formula:\nNegative values of \"v\" / \"v\" signify the movement of optical shapes in reverse direction.\n\nWhen \"p\" > \"p\", the magnified shapes appear normally, but they move in reverse direction compared to the movement of the revealing layer. See the figure below:\n\nWhen \"p\" < \"p\", the magnified shapes appear inverted along the vertical axis, but they move in the same direction as the revealing layer. See the figure below:\n\nLine moiré can be considered as a particular case of shape moiré when the shape embedded in the base layer is simply a straight or curved line.\n\n"}
{"id": "18399611", "url": "https://en.wikipedia.org/wiki?curid=18399611", "title": "Shift-invariant system", "text": "Shift-invariant system\n\nA shift invariant system is the discrete equivalent of a time-invariant system, defined such that if \"y\"(\"n\") is the response of the system to \"x\"(\"n\"), then \"y\"(\"n\"–\"k\") is the response of the system to \"x\"(\"n\"–\"k\"). That is, in a shift-invariant system the contemporaneous response of the output variable to a given value of the input variable does not depend on when the input occurs; time shifts are irrelevant in this regard.\n\nBecause digital systems need not be causal, some operations can be implemented in the digital domain that cannot be implemented using discrete analog components. Digital filters that require finite numbers of future values can be implemented while the analog counterparts cannot.\n\n\n"}
{"id": "43058667", "url": "https://en.wikipedia.org/wiki?curid=43058667", "title": "Shin Sung-chul", "text": "Shin Sung-chul\n\nShin was educated in South Korea and the United States. He received his secondary education in Kyunggi High School. After he received B.S. degree in 1975 in the field of applied physics at Seoul National University, M.S. degree in 1977 in the field of condensed matter physics at KAIST, he carried out research at Korea Research Institute of Standards and Science till 1980. Then, he decided to continue his study in the United States and received his Ph.D. degree in 1984 in the field of materials physics at Northwestern University.\n\nAfter Shin received his Ph.D. degree, he joined Eastman Kodak's Research Lab as a senior researcher and carried out research till 1989. At that time, the South Korean government urged scientists abroad to come back to South Korea and contribute to the development of national science and technology and nurturing young scientists and engineers.\n\nShin replied to the call and joined KAIST in 1989 as an assistant professor in the department of physics. In 2009, Shin became a chair professor of KAIST. During 22 years in KAIST, he held various academic positions. He served as a founding director of the Korea Institute for Advance Study (1996), director of Center for Nanospinics of Spintronic Materials (1998-2005), founding director of Institute of Nano Science & Technology (2001~2003) in KAIST. He also has held various academic positions of many other organizations such as a President of the Korean Magnetics Society(2009~2010), a Chairman of ICM2012(2009~2012), and a President of Korean Physical Society (2011~2012).\nShin published over 320 journal papers, holds 37 patents, delivered 160 international and national invited academic talks, and supervised over 80 Postdoctoral, M.S., Ph.D. students.\n\nIn addition to academic positions, Shin has held various administrative positions, made contributions in promoting science and technology and provided advice on science and technology policy to the government. He served as Vice-Dean of Student Affairs, Director of International Cooperation Division, and Dean of Planning as well as Vice President (2004-2005) in KAIST. He also served as a board member of Expo Science Park, a president of Daedeok Club (Opinion Leaders’ Club for Science and Technology in Daeduck Science Town) (2002-2008), emceed a science program broadcast by KBS, a major Korean television broadcasting network, provided advice on science policy to the government, including Ministry of Science and Technology and Ministry of Science, ICT and Future Planning, and delivered more than 140 public lectures. Shin was appointed as a member of PACST (Presidential Advisory Council on Science & Technology)]. The role of PACST is to provide advice and consultations to the president of South Korea on issues related to science and technology. Shin is also serving as the chair of Committee for Future Strategy, PACST.\n\nThe South Korean government founded DGIST as a research institute in 2004 and changed the related law to allow DGIST offer degree programs in 2008. Therefore, DGIST changed its system from a research institute to a university and opened graduate program in March 2011. The South Korean government appointed Shin as the founding president of this new university. He has been serving as the founding president of DGIST since 2011.\n\n\n"}
{"id": "2242641", "url": "https://en.wikipedia.org/wiki?curid=2242641", "title": "Shock sensitivity", "text": "Shock sensitivity\n\nShock sensitivity is a comparative measure of the sensitivity to sudden compression (by impact or blast) of an explosive chemical compound. Determination of the shock sensitivity of a material intended for practical use is one important aspect of safety testing of explosives. A variety of tests and indices are in use, of which one of the more common is the Rotter Impact Test with results expressed as FoI (Figure of Insensitivity.) At least four other impact tests are in common use, while various \"gap tests\" are used to measure sensitivity to blast shock. Julius-Peters KG is a notable German company which manufactures testing apparatus for these tests.\n\nA few materials such as nitrogen triiodide cannot be touched at all without detonating, and so are of purely academic interest. Some other compounds with a high sensitivity to shock, such as nitroglycerin and acetone peroxide, may detonate from a firm jolt and so cannot be legally transported in pure form. Acetone peroxide is often used by amateurs and terrorists as a means to detonate other explosives as well as acting as the main blasting agent, often resulting in injuries or death to those who underestimate its sensitivity. A number of methods are known to desensitize nitroglycerine so that it can be transported for medical uses, and it is also incorporated into other less sensitive explosives, such as dynamites and gelignites.\n\nMany practical commercial materials of intermediate sensitivity, such as gelignites and water gel explosives, can be safely handled as they will not explode from casual shocks such as being dropped or lightly knocked by a tool. However they may explode if struck forcefully by a metal tool, and would certainly explode in the barrel if they were used in an artillery shell. Reliable initiation of such materials requires the small explosion of a detonator.\n\nStill less sensitive materials such as blasting agents like ANFO, or shell fillings like Composition B, are so insensitive that the impulse from the detonator must be amplified by an explosive booster charge to secure reliable detonation. Some polymer bonded explosives — especially those based on TATB — are designed for use in insensitive munitions, which are unlikely to detonate even if struck by another explosive weapon.\n"}
{"id": "48966643", "url": "https://en.wikipedia.org/wiki?curid=48966643", "title": "Sid Boyum", "text": "Sid Boyum\n\nSid Boyum (1914-1991) was an industrial photographer, sculptor and graphic artist in Madison, Wisconsin, United States. Much of his work falls into the category of outsider art. Today, Boyum is best known for his public sculptures scattered throughout the Schenk-Atwood-Starkweather-Yahara Neighborhood on Madison's east side.\n\nIn addition to these publicly exhibited works, dozens of other totemic concrete sculptures still crowd the back yard of Boyum's former home and studio, now abandoned. Efforts are underway to preserve Boyum's home and art. Each year from 1963 to 1989, The Wisconsin State Journal commissioned Boyum to draw a different full-page, poster commemorating the opening of the Wisconsin fishing season. During his lifetime, Boyum also produced thousands of photographs (including a number of whimsical self-portraits), 16-millimeter films, drawings, paintings and bas-relief works.\n\nBoyum was a close friend, collaborator and influence on other Wisconsin artists and collectors, including Baraboo's Tom Every (A.K.A. \"Doctor Evermor\"), creator of the Forevertron and Alex Jordan, Jr creator of the House on the Rock.\n\n"}
{"id": "45232441", "url": "https://en.wikipedia.org/wiki?curid=45232441", "title": "Small molecule sensors", "text": "Small molecule sensors\n\nSmall molecule sensors are an effective way to detect the presence of metal ions in solution. Although many types exist, most small molecule sensors comprise a subunit that selectively binds to a metal that in turn induces a change in a fluorescent subunit. This change can be observed in the small molecule sensor's spectrum, which can be monitored using a detection system such as a microscope or a photodiode. Different probes exist for a variety of applications, each with different dissociation constants with respect to a particular metal, different fluorescent properties, and sensitivities. They show great promise as a way to probe biological processes by monitoring metal ions at low concentrations in biological systems. Since they are by definition small and often capable of entering biological systems, they are conducive to many applications for which other more traditional bio-sensing are less effective or not suitable.\n\nMetal ions are essential to virtually all biological systems and hence studying their concentrations with effective probes is highly advantageous. Since metal ions are key to the causes of cancer, diabetes, and other diseases, monitoring them with probes that can provide insight into their concentrations with spatial and temporal resolution is of great interest to the scientific community. There are many applications that one can envision for small molecule sensors. It has been shown that one can use them to differentiate effectively between acceptable and harmful concentrations of mercury in fish. Further, since some types of neurons uptake zinc during their operation, these probes can be used as a way to track activity in the brain and could serve as an effective alternative to functional MRI. One can also track and quantify the growth of a cell, such as a fibroblast, that uptakes metal ions as it constructs itself. Numerous other biological processes can be tracked using small molecule sensors as many change metal concentrations as they occur, which can then be monitored. Still, the sensor must be tailored for its specific environment and sensing requirements. Depending on the application, the metal sensor should be selective for a certain type of metal, and especially needs to be able to bind its target metal with greater affinity than metals that naturally exist at high concentrations within the cell . Further, they should provide a response with a strong modulation in fluorescent spectrum and hence provide a high signal to noise ratio. Finally, it is essential that a sensor is not toxic to the biological system in which it is used.\n\nMost detection mechanisms involved in small molecule sensors comprise some modulation in the fluorescent behavior of the sensing molecule upon binding the target metal. When a metal coordinates to such a sensor, it may either enhance or reduce the original fluorescent emission. The former is known as the Chelation Enhancement Fluorescence effect (CHEF), while the latter is called the Chelation Enhancement Quenching effect (CHEQ). By changing the intensity of emission at different wavelengths, the resulting fluorescent spectrum may attenuate, amplify, or shift upon the binding and dissociation of a metal. This shift in spectra can be monitored using a detector such as a microscope or a photodiode.\nListed below are some examples of mechanisms by which emission is modulated. Their participation in CHEQ or CHEF is dependent on the metal and small molecule sensor in question.\n\n\nFluorophores are essential to our measurement of the metal binding event, and indirectly, metal concentration. There are many types, all with different properties that make them advantageous for different applications. Some work as small metal sensors completely on their own while others must be complexed with a subunit that can chelate or bind a metal ion. Rhodamine for example undergoes a conformation change upon the binding of a metal ion. In so doing it switches between a colorless, non-fluorescent spirocyclic form to a fluorescent, pink open cyclic form. Quinoline based sensors have been developed that form luminescent complexes with Cd(II) and fluorescent ones with Zn(II). It is hypothesized to function by changing its lowest luminescent state from n–* to –* when coordinating to a metal. \nWhen the Dansyl group DNS binds to a metal, it loses a sulfonamide hydrogen, causing fluorescence quenching via a PET or reverse PET mechanism in which an electron is transferred either to or from the metal that is bound.\n\nZinc is one of the most common metal ions in biological systems. Small molecule sensors for it include:\n\nCopper is a biologically important metal to detect. It has many sensors developed for it including:\n\nIron is used a great deal in biological systems, a fact that is well known due to its role in Hemoglobin. For it, there are many small molecule sensors including:\n\nCobalt sensors have been made that capitalize on the breaking of C-O bonds by Co(II) in a fluorescent probe known as Cobalt Probe 1 (CP1).\n\nMercury is a toxic heavy metal, and as such it is important to be able to detect it in biological systems. Sensors include:\n\n"}
{"id": "57410739", "url": "https://en.wikipedia.org/wiki?curid=57410739", "title": "SqueezeNet", "text": "SqueezeNet\n\nSqueezeNet is the name of a deep neural network that was released in 2016. SqueezeNet was developed by researchers at DeepScale, University of California, Berkeley, and Stanford University. In designing SqueezeNet, the authors' goal was to create a smaller neural network with fewer parameters that can more easily fit into computer memory and can more easily be transmitted over a computer network.\n\nSqueezeNet was originally released on February 22, 2016. This original version of SqueezeNet was implemented on top of the Caffe deep learning software framework. Shortly thereafter, the open-source research community ported SqueezeNet to a number of other deep learning frameworks. On February 26, 2016, Eddie Bell released a port of SqueezeNet for the Chainer deep learning framework. On March 2, 2016, Guo Haria released a port of SqueezeNet for the Apache MXNet framework. On June 3, 2016, Tammy Yang released a port of SqueezeNet for the Keras framework. In 2017, companies including Baidu, Xilinx, Imagination Technologies, and Synopsys demonstrated SqueezeNet running on low-power processing platforms such as smartphones, FPGAs, and custom processors.\n\nAs of 2018, SqueezeNet ships \"natively\" as part of the source code of a number of deep learning frameworks such as PyTorch, Apache MXNet, and Apple CoreML. In addition, 3rd party developers have created implementations of SqueezeNet that are compatible with frameworks such as TensorFlow. Below is a summary of frameworks that support SqueezeNet.\n\nSqueezeNet was originally described in a paper entitled \"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size.\" AlexNet is a deep neural network that has 240MB of parameters, and SqueezeNet has just 5MB of parameters. However, it's important to note that SqueezeNet is not a \"squeezed version of AlexNet.\" Rather, SqueezeNet is an entirely different DNN architecture than AlexNet. What SqueezeNet and AlexNet have in common is that both of them achieve approximately the same level of accuracy when evaluated on the ImageNet image classification validation dataset.\n\nModel compression (e.g. quantization and pruning of model parameters) can be applied to a deep neural network after it has been trained. In the SqueezeNet paper, the authors demonstrated that a model compression technique called Deep Compression can be applied to SqueezeNet to further reduce the size of the parameter file from 5MB to 500KB. Deep Compression has also been applied to other DNNs such as AlexNet and VGG.\n"}
{"id": "13218926", "url": "https://en.wikipedia.org/wiki?curid=13218926", "title": "Standard Commands for Programmable Instruments", "text": "Standard Commands for Programmable Instruments\n\nThe Standard Commands for Programmable Instruments (SCPI; often pronounced \"skippy\") defines a standard for syntax and commands to use in controlling programmable test and measurement devices, such as automatic test equipment and electronic test equipment.\n\nSCPI was defined as an additional layer on top of the specification \"Standard Codes, Formats, Protocols, and Common Commands\". The standard specifies a common syntax, command structure, and data formats, to be used with all instruments. It introduced generic commands (such as codice_1 and codice_2) that could be used with any instrument. These commands are grouped into subsystems. SCPI also defines several classes of instruments. For example, any controllable power supply would implement the same codice_3 base functionality class. Instrument classes specify which subsystems they implement, as well as any instrument-specific features.\n\nThe physical hardware communications link is not defined by SCPI. While it was originally created for the IEEE-488.1 (GPIB) bus, SCPI can also be used with RS-232, RS-422, Ethernet, USB, VXIbus, HiSLIP, etc.\n\nSCPI commands are ASCII textual strings, which are sent to the instrument over the physical layer (e.g., IEEE-488.1). Commands are a series of one or more keywords, many of which take parameters. In the specification, keywords are written codice_4: The entire keyword can be used, or it can be abbreviated to just the uppercase portion. Responses to query commands are typically ASCII strings. However, for bulk data, binary formats can be used.\n\nThe SCPI specification consists of four volumes: Volume 1: \"Syntax and Style\", Volume 2: \"Command Reference\", Volume 3: \"Data Interchange Format\", Volume 4: \"Instrument Classes\". The specification was originally released as non-free printed manuals, then later as a free PDF file.\n\nFirst released in 1990, SCPI originated as an additional layer for IEEE-488. IEEE-488.1 specified the physical and electrical bus, and IEEE-488.2 specified protocol and data format, but neither specified instrument commands. Different manufacturers, and even different models, of the same type of instrument would use different command sets. SCPI created a standard which could be common across all manufacturers and models. It requires use of the IEEE-488.2 data formats, but does not mandate the IEEE-488.1 bus.\n\nIn 2002-2003, the \"SCPI Consortium\" voted to become part of the \"IVI Foundation\" (Interchangeable Virtual Instruments).\n\nIn 1987, IEEE introduced IEEE 488.2-1987 specification \"Standard Codes, Formats, Protocols, and Common Commands\", it was later revised in 1992 as IEEE 488.2-1992.\n\nWhile IEEE 488.2 provided a device-independent syntax, there was still no standard for instrument-specific commands. Commands to control the same class of instrument, e.g., multimeters, would vary between manufacturers and even models. The United States Air Force, and later Hewlett-Packard, recognized this problem. In 1989, HP developed their TML language which was the forerunner to SCPI.\n\nThe IEC developed their own standards in parallel with the IEEE, with IEC 60625-2-1993 (IEC 625). In 2004, the IEEE and IEC combined their respective standards into a \"dual logo\" IEEE/IEC standard IEC 60488-2-2004,\" Part 2: Codes, Formats, Protocols and Common Commands\", replaces IEEE 488.2-1992 and IEC 60625-2-1993.\n\nSCPI commands to an instrument may either perform a \"set\" operation (e.g. switching a power supply on) or a \"query\" operation (e.g. reading a voltage). Queries are issued to an instrument by appending a question-mark to the end of a command. Some commands can be used for both setting and querying an instrument. For example, the data-acquisition mode of an instrument could be set by using the codice_5 command or it could be queried by using the codice_6 command. Some commands can both set and query an instrument at once. For example, the codice_7 command runs a self-calibration routine on some equipment, and then returns the results of the calibration.\n\nSimilar commands are grouped into a hierarchy or \"tree\" structure. For example, any instruction to read a measurement from an instrument will begin with \"codice_2\". Specific sub-commands within the hierarchy are nested with a colon (codice_9) character. For example, the command to \"Measure a DC voltage\" would take the form codice_10, and the command to \"Measure an AC current\" would take the form codice_11.\n\nThe command syntax shows some characters in a mixture of upper and lower case. Abbreviating the command to only sending the upper case has the same meaning as sending the upper and lower case command.\n\nFor example, the command “codice_12” would set an RS-232 serial communications interface to 2400 bit/s. This could also alternatively be abbreviated “codice_13”. The query command “codice_14” or “codice_15” would instruct the instrument to report its current baud rate.\n\nMultiple commands can be issued to an instrument in a single string. They are made of simple commands separated by a semicolon character (codice_16). For example, the command to \"Measure a DC voltage then measure an AC current\" would be issued as codice_17.\n\nSimple commands which start with a colon (codice_9) are interpreted with respect to the root of the command tree. Otherwise, they refer implicitly to the last node of the previous command (unless they already begin with an asterisk). For example,\n\nis a shorthand for the message\n\nSome commands require an additional argument. Arguments are given after the command, and are separated by a space. For example, the command to set the trigger mode of an instrument to \"normal\" may be given as \"codice_19\". Here, the word \"codice_20\" is used as the argument to the \"codice_21\" command.\n\nFor commands that accept integer arguments, values may be specified in multiple computer number formats: decimal, hexadecimal, octal, binary. The last three formats are defined by IEEE 488.2, which SCPI is based upon. Decimal numbers (radix 10) aren't prefixed, Hexadecimal numbers (radix 16) are prefixed with codice_22 or codice_23, octal numbers (radix 8) with codice_24 or codice_25, and binary numbers (radix 2) with codice_26 or codice_27. Hexadecimal digits may use either uppercase letters (ABCDEF), or lowercase letters (abcdef), or mixed case letters (aBcDeF). For octal, the letter \"Q\" was chosen instead of the letter \"O\" to minimize the visual confusion with the number \"0\" (zero).\n\nThe following argument examples are numerically equivalent:\n\n\n"}
{"id": "33834820", "url": "https://en.wikipedia.org/wiki?curid=33834820", "title": "TechChange", "text": "TechChange\n\nTechChange is a US social enterprise which provides courses on the use of technology in addressing social and global challenges. It is a registered benefit corporation based in Washington, DC and was founded in the summer of 2010.\n\nTechChange delivers online certificate courses on a number of topics including technology for emergency management, mobile phones for international development, social media for social change, social entrepreneurship, digital organizing, open government and more.\n\nTechChange also partners with international development, humanitarian, and peacebuilding organizations such as USAID, US State Department, World Bank and United Nations Foundation to deliver online educational content. In 2012, TechChange partnered with the mHealth Alliance, a joint program hosted by the United Nations Foundation, to create the first online certificate course in mHealth.\n\nTechChange produces educational animations for international development topics and tools. In April 2013, a TechChange animation on M-Pesa, the popular mobile money transfer program was featured as the United States Agency for International Development video of the week.\n\nTechChange has been featured in a variety of publications for their new approaches to online learning and capacity building including the \"New York Times\", \"PBS NewsHour\", \"The Economist\", Fast Company, \"Chronicle of Higher Education\", \"Stanford Social Innovation Review\", \"The Guardian\", and Dowser.org.\n"}
{"id": "1326176", "url": "https://en.wikipedia.org/wiki?curid=1326176", "title": "Telecoms &amp; Internet converged Services &amp; Protocols for Advanced Networks", "text": "Telecoms &amp; Internet converged Services &amp; Protocols for Advanced Networks\n\nThe Telecoms & Internet converged Services & Protocols for Advanced Networks (TISPAN) is a standardization body of ETSI, specializing in fixed networks and Internet convergence. It was formed in 2003 from the amalgamation of the ETSI bodies Telecommunications and Internet Protocol Harmonization Over Networks (TIPHON) and Services and Protocols for Advanced Networks (SPAN).\n\nTISPAN's focus is to define the European view of the Next Generation Networking (NGN), though TISPAN also includes much participation from regions outside Europe. \n\nTISPAN NGN Release 1 was published in December 2005 and contained the architectural foundations and basic specifications required in support of PSTN replacement. The TISPAN NGN architecture is based on sharing common components between cooperating subsystems. The TISPAN NGN architecture complies with the general reference model for next generation networks defined in ITU-T Recommendation Y.2011 [1] and is therefore layered with a service stratum and a transport stratum. Each of these layers is further decomposed into sub-systems that perform specific roles within the overall architecture. This allows new subsystems to be added over time to cover new demands and service classes. By making network resources, applications, and user equipment common to all subsystems, it ensures mobility of users, terminals and services as much as possible, even across administrative boundaries. A key subsystem is based on the architectures of 3rd Generation Partnership Project (3GPP) IP Multimedia Subsystem (IMS). TISPAN has been working with 3GPP to extend the IMS architecture with capabilities required in support of wire-line access.\n\nTISPAN NGN Release 2 was finalized early 2008, and added support for IPTV services and Business Communications over the IMS.\n\nSince early 2008, TISPAN has begun work on the third release of its NGN specifications with prime focus on IPTV enhancements, Content Delivery Networks (CDN) and home networking. In 2011, TISPAN published the specification of a functional architecture for Content Delivery Networks (CDN) and is now working on the specification of the protocols applicable to the reference points identified in this architecture (See ETSI TS 182 019)\n\nThe ETSI website on Next Generation Networking states:\n\"Standards for fixed NGN were developed by the now closed ETSI technical committee TISPAN. The TC has adopted the 3GPP™ core IMS specifications using Internet (SIP) protocols to allow features such as Presence, IPTV, Messaging, and Conferencing to be delivered irrespective of the network in use. Maintenance of NGN standards are now the responsibility of TC NTECH.\"\n\n"}
{"id": "37792009", "url": "https://en.wikipedia.org/wiki?curid=37792009", "title": "Twine (device)", "text": "Twine (device)\n\nTwine is a stand-alone device that uses sensors to detect parts of its environment and that connects to a wifi network to communicate. Rules loaded into the Twine can test for sensor conditions and, based on logic, send messages through email or SMS, make an HTTP request, or light a LED. It can act as a data logger.\n\nThe device was created by Supermechanical in the USA from funding raised on Kickstarter. Their original goal was for $35,000 yet they raised $556,541 from 3,966 backers on January 3, 2012. \nThe product successfully shipped in November 2012.\n"}
{"id": "613980", "url": "https://en.wikipedia.org/wiki?curid=613980", "title": "Union of Shop, Distributive and Allied Workers", "text": "Union of Shop, Distributive and Allied Workers\n\nThe Union of Shop, Distributive and Allied Workers (USDAW) is a trade union in the United Kingdom, consisting of over 433,000 members nationwide. Usdaw members work in a variety of occupations and industries including: shopworkers, factory and warehouse workers, drivers, call centres, clerical workers, milkround and dairy process, butchers and meat packers, catering, laundries, chemical processing, home shopping and pharmaceutical. The retail sector employs around 2.77 million people.\n\nThe union was formed in 1947 by the merger of the National Union of Distributive and Allied Workers and the National Union of Shop Assistants, Warehousemen and Clerks.\n\nUSDAW has a managerial and supervisory section called Sata - the Supervisory, Administrative and Technical Association. This is a special section of the union representing employees in middle and lower management or with administrative or technical responsibilities.\n\nUSDAW is the recognised trade union for workers employed by Tesco; consequently, it has the biggest recognition agreement in the private sector and more than 160,000 (over a third) of its members work for Tesco. Other companies that have signed partnership agreements with USDAW include Morrisons, Sainsbury's, The Co-operative Group, Primark, Asda, Argos and Boots.\n\nUSDAW produces a quarterly membership magazine for members, \"Arena\", as well as a bimonthly magazine for union activists, \"Network\".\n"}
{"id": "239926", "url": "https://en.wikipedia.org/wiki?curid=239926", "title": "Water treatment", "text": "Water treatment\n\nWater treatment is any process that improves the quality of water to make it more acceptable for a specific end-use. The end use may be drinking, industrial water supply, irrigation, river flow maintenance, water recreation or many other uses, including being safely returned to the environment. Water treatment removes contaminants and undesirable components, or reduces their concentration so that the water becomes fit for its desired end-use.\n\nTreatment for drinking water production involves the removal of contaminants from raw water to produce water that is pure enough for human consumption without any short term or long term risk of any adverse health effect. Substances that are removed during the process of drinking water treatment include suspended solids, bacteria, algae, viruses, fungi, and minerals such as iron and manganese.\n\nThe processes involved in removing the contaminants include physical processes such as settling and filtration, chemical processes such as disinfection and coagulation and biological processes such as slow sand filtration.\n\nMeasures taken to ensure water quality not only relate to the treatment of the water, but to its conveyance and distribution after treatment. It is therefore common practice to keep residual disinfectants in the treated water to kill bacteriological contamination during distribution.\n\nWorld Health Organization (WHO) guidelines are a general set of standards intended to apply where better local standards are not implemented. More rigorous standards apply across Europe, the USA and in most other developed countries. followed throughout the world for drinking water quality requirements.\n\nA combination selected from the following processes is used for municipal drinking water treatment worldwide:\n\nTechnologies for potable water and other uses are well developed, and generalized designs are available from which treatment processes can be selected for pilot testing on the specific source water. In addition, a number of private companies provide patented technological solutions for the treatment of specific contaminants. Automation of water and waste-water treatment is common in the developed world. Source water quality through the seasons, scale, and environmental impact can dictate capital costs and operating costs. End use of the treated water dictates the necessary quality monitoring technologies, and locally available skills typically dictate the level of automation adopted.\nWastewater treatment is the process that removes the majority of the contaminants from wastewater or sewage and produces both a liquid effluent suitable for disposal to the natural environment and a sludge. Biological processes can be employed in the treatment of wastewater and these processes may include, for example, aerated lagoons, activated sludge or slow sand filters. To be effective, sewage must be conveyed to a treatment plant by appropriate pipes and infrastructure and the process itself must be subject to regulation and controls. Some wastewaters require different and sometimes specialized treatment methods. At the simplest level, treatment of sewage and most wastewaters is carried out through separation of solids from liquids, usually by sedimentation. By progressively converting dissolved material into solids, usually a biological floc, which is then settled out, an effluent stream of increasing purity is produced.\n\nTwo of the main processes of industrial water treatment are \"boiler water treatment\" and \"cooling water treatment\". A large amount of proper water treatment can lead to the reaction of solids and bacteria within pipe work and boiler housing. Steam boilers can suffer from scale or corrosion when left untreated. Scale deposits can lead to weak and dangerous machinery, while additional fuel is required to heat the same level of water because of the rise in thermal resistance. Poor quality dirty water can become a breeding ground for bacteria such as \"Legionella\" causing a risk to public health.\n\nWith the proper treatment, a significant proportion of industrial on-site wastewater might be reusable. This can save money in three ways: lower charges for lower water consumption, lower charges for the smaller volume of effluent water discharged and lower energy costs due to the recovery of heat in recycled wastewater.\n\nCorrosion in low pressure boilers can be caused by dissolved oxygen, acidity and excessive alkalinity. Water treatment therefore should remove the dissolved oxygen and maintain the boiler water with the appropriate pH and alkalinity levels.\nWithout effective water treatment, a cooling water system can suffer from scale formation, corrosion and fouling and may become a breeding ground for harmful bacteria. This reduces efficiency, shortens plant life and makes operations unreliable and unsafe.\n\nWater supplied to domestic properties may be further treated before use, often using an in-line treatment process. Such treatments can include water softening or ion exchange. Many proprietary systems also claim to remove residual disinfectants and heavy metal ions.\n\nSaline water can be treated to yield fresh water. Two main processes are used, reverse osmosis or distillation. Both methods require more energy than water treatment of local surface waters, and are usually only used in coastal areas or where water such as groundwater has high salinity.\n\nLiving away from drinking water supplies often requires some form of portable water treatment process. These can vary in complexity from the simple addition of a disinfectant tablet in a hiker's water bottle through to complex multi-stage processes carried by boat or plane to disaster areas.\n\nSome industries such as the production of silicon wafers, space technology and many high quality metallurgical process require ultrapure water. The production of such water typically involves many stages, and can include reverse osmosis, ion exchange and several distillation stages using solid tin apparatus.\n\nEarly water treatment methods still used included sand filtration and chlorination. The first documented use of sand filters to purify the water supply dates to 1804, when the owner of a bleachery in Paisley, Scotland, John Gibb, installed an experimental filter, selling his unwanted surplus to the public. This method was refined in the following two decades, and it culminated in the first treated public water supply in the world, installed by the Chelsea Waterworks Company in London in 1829.\n\nAppropriate technology options in water treatment include both community-scale and household-scale point-of-use (POU) or self-supply designs. Such designs may employ solar water disinfection methods, using solar irradiation to inactivate harmful waterborne microorganisms directly, mainly by the UV-A component of the solar spectrum, or indirectly through the presence of an oxide photocatalyst, typically supported TiO in its anatase or rutile phases. Despite progress in SODIS technology, military surplus water treatment units like the ERDLator are still frequently used in developing countries. Newer military style Reverse Osmosis Water Purification Units (ROWPU) are portable, self-contained water treatment plants are becoming more available for public use.\n\nFor waterborne disease reduction to last, water treatment programs that research and development groups start in developing countries must be sustainable by the citizens of those countries. This can ensure the efficiency of such programs after the departure of the research team, as monitoring is difficult because of the remoteness of many locations.\n\nWater treatment plants can be significant consumers of energy. In California, more than 4% of the state's electricity consumption goes towards transporting moderate quality water over long distances, treating that water and treating sewage to a high standard. In areas with high quality water sources which flow by gravity to the point of consumption, and where sewage flow and treatment can be undertaken using gravity systems, costs will be much lower.\nMuch of the energy requirements are in pumping. Processes that avoid the need for pumping tend to have overall low energy demands. Those water treatment technologies that have very low energy requirements including trickling filters, slow sand filters, gravity aqueducts.\n\nA notable example that combines both wastewater treatment and drinking water treatment is NEWater in Singapore. NEWater is a technology practised in Singapore that converts wastewater to potable water. More specifically, it is treated wastewater (sewage) that has been purified using dual-membrane (via microfiltration and reverse osmosis) and ultraviolet technologies, in addition to conventional water treatment processes. The water is potable and is consumed by humans, but is mostly used by industries requiring high purity water. The total capacity of the plants is about 75,700 m/day. Some 6% of this is used for indirect potable use, equal to about 1% of Singapore's potable water requirement of 14 m/s. The rest is used at wafer fabrication plants and other non-potable applications in industries in Woodlands, Tampines, Pasir Ris, and Ang Mo Kio.\n\nThe Safe Drinking Water Act requires the U.S. Environmental Protection Agency (EPA) to set standards for drinking water quality in public water systems (entities that provide water for human consumption to at least 25 people for at least 60 days a year). Enforcement of the standards is mostly carried out by state health agencies. States may set standards that are more stringent than the federal standards.\n\nEPA has set standards for over 90 contaminants organized into six groups: microorganisms, disinfectants, disinfection byproducts, inorganic chemicals, organic chemicals and radionuclides.\n\nEPA also identifies and lists unregulated contaminants which may require regulation. The \"Contaminant Candidate List\" is published every five years, and EPA is required to decide whether to regulate at least five or more listed contaminants.\n\nLocal drinking water utilities may apply for low interest loans, to make facility improvements, through the Drinking Water State Revolving Fund.\n\nEPA and state environmental agencies set wastewater standards under the Clean Water Act. Point sources must obtain surface water discharge permits through the National Pollutant Discharge Elimination System (NPDES). Point sources include industrial facilities, municipal governments (sewage treatment plants and storm sewer systems), other government facilities such as military bases, and some agricultural facilities, such as animal feedlots.\n\nEPA sets basic national wastewater standards:\nThese standards are incorporated into the permits, which may include additional treatment requirements developed on a case-by-case basis. NPDES permits must be renewed every five years. EPA has authorized 46 state agencies to issue and enforce NPDES permits. EPA regional offices issues permits for the rest of the country.\n\nWastewater discharges to groundwater are regulated by the Underground Injection Control Program under the Safe Drinking Water Act.\n\nFinancial assistance for improvements to sewage treatment facilities is available to state and local governments through the Clean Water State Revolving Fund, a low interest loan program.\n\n\n"}
{"id": "9406780", "url": "https://en.wikipedia.org/wiki?curid=9406780", "title": "World Programming System", "text": "World Programming System\n\nThe World Programming System, also known as WPS Analytics or WPS, is a software product developed by a company called World Programming. WPS allows users to create, edit and run programs written in the language of SAS.\n\nThe program was the subject of a lawsuit by SAS Institute. The EU Court of Justice ruled in favor of World Programming, stating that the copyright protection does not extend to the software functionality, the programming language used and the format of the data files used by the program. It stated that there is no copyright infringement when a company which does not have access to the source code of a program studies, observes and tests that program to create another program with the same functionality.\n\nWPS can use programs written in the language of SAS without the need for translating them into any other language. In this regard WPS is compatible with the SAS system. WPS is a language interpreter able to process the language of SAS and produce similar results.\n\nWPS is available to run on Mainframe z/OS, Windows, macOS, Linux, Linux for Arm8 64-bit (AArch64), PowerLinux, POWER/System p/pSeries (LE), Solaris, AIX and Linux on Mainframe System z.\n\nOn all supported platforms, programs written in the language of SAS can be executed from a WPS command line interface, often referred to as running in batch mode.\n\nWPS can also be used from a graphical user interface known as the WPS Workbench for managing, editing and running programs written in the language of SAS. The WPS Workbench user interface is based on Eclipse.\n\nWPS version 4 (released in March 2018) introduced a drag-and-drop workflow canvas providing interactive blocks for data retrieval, blending and preparation, data discovery and profiling, predictive modelling powered by machine learning algorithms, model performance validation and scorecards.\n\nWPS version 3 (released in February 2012) provides a new client/server architecture that allows the WPS Workbench GUI to execute SAS programs on remote server installations of WPS in a network or cloud. The resulting output, data sets, logs, etc., can then all be viewed and manipulated from inside the Workbench as if the workloads had been executed locally. SAS programs do not require any special language statements to use this feature.\n\nIn 2010 World Programming defended its use of the language of SAS in the High Court of England and Wales in \"SAS Institute Inc. v World Programming Ltd\".\n\n\nGartner recognized World Programming in their Cool Vendors in Data Science, 2014 Report.\n\n"}
