{"id": "608675", "url": "https://en.wikipedia.org/wiki?curid=608675", "title": "30 Hudson Street", "text": "30 Hudson Street\n\n30 Hudson Street, also known as Goldman Sachs Tower, is a , 42-story building in Jersey City, New Jersey. It is the tallest building in New Jersey. Completed in 2004, the tower was designed by César Pelli. It houses offices, a cafeteria, a health unit, and a full-service fitness facility including a physical therapy clinic. Provident Bank of New Jersey and Così outlets are also located on the ground level, and open to the general public.\n\nThe building is in the Exchange Place area close to a PATH station and is accessible by the Hudson-Bergen Light Rail at the Essex Street and Exchange Place stops.\n\nThe tower sits on the waterfront overlooking the Hudson River and Lower Manhattan and is visible from all five of the New York City boroughs. On a clear day, the building may be visible from as far away as Highlands, New Jersey 40 miles south and Bear Mountain, New York 48 miles north.\n\nOriginally intended to be a dedicated use building for Goldman Sachs' middle and back office units, lower than projected staffing levels at the bank following the global financial crisis forced Goldman to seek occupancy from other tenants to avoid forgone rental income. Royal Bank of Canada currently shares the space, with plans for other professional service firms to take occupancy as well in the near future.\n\nOriginally the tower was meant to be the centerpiece of an entire Goldman Sachs campus at Exchange Place, which was to include a training center, a university, and a large hotel complex. Many of the company's Manhattan-based equity traders refused to move away from Wall Street, delaying the occupation of the building's top 13 floors, which remained vacant until early 2008.\n\nOnce a derelict and mostly industrial part of Jersey City, the Exchange Place area forms part of New Jersey's Gold Coast, a revitalized strip of land along the formerly industrial west bank of the Hudson. Economic development in the 2000s spurred large-scale residential, commercial, and office development along the waterfront.\n\nAlthough the location was largely rejected by the company's financial executives, 4,000 Goldman Sachs employees made the move to the building, including much of the company's real estate, technology, operations, and administrative departments. The building is certified under LEED-NC Version 2.0 of the U.S. Green Building Council.The building has been surrounded by pedestrian protective scaffolding since 2010.\n\nThe company completed construction of another tower in 2010 to house the bulk of their sales and trading departments. It is located at 200 West Street in Lower Manhattan just north of Brookfield Place (formerly the World Financial Center), almost directly across the water from 30 Hudson. Under their \"Venice strategy\", Goldman Sachs works in conjunction with NY Waterway to shuttle workers between the two buildings on private ferries.\n\nThe building was used in 2016 by the Bernie Sanders presidential campaign to symbolize Goldman Sachs and Hillary Clinton's ties to the company.\n\n\nNotes\n"}
{"id": "3508232", "url": "https://en.wikipedia.org/wiki?curid=3508232", "title": "Aftershave", "text": "Aftershave\n\nAftershave is a liquid product applied to skin after shaving. It contains an antiseptic agent such as denatured alcohol, stearate citrate or witch hazel to prevent infection of cuts, as well as to act as an astringent to reduce skin irritation. Menthol is used in some varieties as well to numb damaged skin, and it is an ingredient that shaving cream manufacturers have started including in their formulations, too.\n\nAn alcohol-based aftershave usually causes an immediate stinging sensation after applying it post-shave, with effects sometimes lasting several minutes, but most commonly only for seconds. For this reason, a market consisting of highly differentiated products has been created—some using alcohols, some not.\n\nSome aftershaves use fragrance or essential oil to enhance scent. Moisturizers—natural and artificial, are often touted as able to soften the skin.\n\nAftershave is sometimes mistakenly referred to as eau de cologne due to the very similar nature of the two products. Some aftershave manufacturers encourage using their fragranced aftershave as if it were cologne, in order to increase sales by encouraging consumers to use it in a more versatile manner, rather than just after a shaving session. Some aftershaves were inspired by a cologne.\n\nEarly aftershaves included witch-hazel and bay rum, and have been documented in shaving guides. Both still are sold as aftershaves.\n\n"}
{"id": "20838163", "url": "https://en.wikipedia.org/wiki?curid=20838163", "title": "Alberta Taciuk process", "text": "Alberta Taciuk process\n\nThe Alberta Taciuk process (ATP; known also as the AOSTRA Taciuk process) is an above-ground dry thermal retorting technology for extracting oil from oil sands, oil shale and other organics-bearing materials, including oil contaminated soils, sludges and wastes. The technology is named after its inventor William Taciuk and the Alberta Oil Sands Technology and Research Authority.\n\nThe research and development of the ATP technology started in 1970. In 1975, its inventor, William Taciuk, formed the UMATAC Industrial Processes (now part of Polysius) to further its development. The first ATP pilot plant was constructed in 1977.\n\nThe ATP was originally developed for pyrolysis of oil sand. However, its first commercial application in 1989 was dedicated to the environmental remediation of contaminated soils. From 1999 to 2004, ATP technology was used for shale oil extraction at the Stuart Oil Shale Plant in Australia. During that time, of shale oil was extracted before the owner, Southern Pacific Petroleum Pty Ltd went into receivership. The subsequent owner, Queensland Energy Resources closed and dismantled the plant.\n\nIn 2002, Estonian company Viru Keemia Grupp tested this technology; however, it was not taken into use.\n\nThe ATP is an above-ground oil-shale retorting technology classified as a hot recycled solids technology. The distinguishing feature of the ATP is that the drying and pyrolysis of the oil shale or other feed, as well as the combustion, recycling, and cooling of spent materials and residues, all occur within a single rotating multi-chamber horizontal retort. Its feed consists of fine particles.\n\nIn its shale-oil applications, fine particles (less than in diameter) are fed into the preheat tubes of the retort, where they are dried and preheated to indirectly by hot shale ash and hot flue gas. In the pyrolysis zone, oil shale particles are mixed with hot shale ash and the pyrolysis is performed at temperatures between and . The resulting shale oil vapor is withdrawn from the retort through a vapour tube and recovered by condensation in other equipment. The char residues, mixed with ash, are moved to the combustion zone, and burnt at about to form shale ash. Part of the ash is delivered to the pyrolysis zone, where its heat is recycled as a hot solid carrier; the other part is removed and cooled in the cooling zone with the combustion gases by heat transfer to the feed oil shale.\n\nThe advantages of the ATP technology for shale oil extraction lie in its simple and robust design, energy self-sufficiency, minimal process water requirements, ability to handle fine particles, and high oil yields. It is particularly suited for processing materials with otherwise low oil yield. The mechanical transfer of solids through the machine does not involve moving parts and it achieves improved process efficiencies through solid-to-solid heat transfer. Most of the process energy (over 80%) is produced by combustion of char and produced oil shale gas; external energy inputs are minimal. The oil yields are about 85–90% of Fischer Assay. The organic carbon content of the process residue (spent shale) is less than 3%. The process produces only small amounts of contaminated water with low concentrations of phenols. These advantages also apply to its oil sands applications, including increased oil yield, a simplified process flow, reduction of bitumen losses to tailings, elimination of the need for tailing ponds, improvement in energy efficiency compared with the hot water extraction process, and elimination of requirements for chemical and other additives.\n\nA complication of the ATP is that retorting operations can reach temperatures at which carbonate minerals within the shale decompose, increasing greenhouse gas emissions.\n\nAs of 2008, ATP was used by the United States Environmental Protection Agency at a PCB-contaminated site near Buffalo, New York, and at the Waukegan Harbor, Illinois.\n\nUMATAC Industrial Processes runs a 5 tons of oil shale per hour pilot processor in Calgary, Alberta for large scale tests of different oil shales. The Fushun Mining Group of China has built a 250 tonnes per hour ATP plant that began commissioning in 2010. Jordan Energy and Mining Ltd plans to use the ATP technology for extracting oil from Al Lajjun and Attarat oil shale deposits in Jordan.\n\n"}
{"id": "1764276", "url": "https://en.wikipedia.org/wiki?curid=1764276", "title": "Autobahn", "text": "Autobahn\n\nThe Autobahn ( , plural ') is the federal controlled-access highway system in Germany. The official German term is (plural ', abbreviated \"BAB\"), which translates as \"federal motorway\". The literal meaning of the word \"Bundesautobahn\" is \"Federal Auto(mobile) Track\".\n\nGerman \"autobahnen\" have no federally mandated speed limit for some classes of vehicles. However, limits are posted (and enforced) in areas that are urbanised, substandard, accident-prone, or under construction. On speed-unrestricted stretches, an advisory speed limit (\"\") of applies. While going faster is not illegal as such in the absence of a speed limit, it can cause an increased liability in the case of an accident; courts have ruled that an \"ideal driver\" who is exempt from absolute liability for \"inevitable\" tort under the law would not exceed \"Richtgeschwindigkeit\".\n\nA 2008 estimate reported that 52% of the autobahn network had only the advisory speed limit, 15% had temporary speed limits due to weather or traffic conditions, and 33% had permanent speed limits. Measurements from the German State of Brandenburg in 2006 showed average speeds of on a 6-lane section of autobahn in free-flowing conditions.\n\nGermany's autobahn network has a total length of about in 2017, and a density of 36 motorway kilometres per thousand square kilometer (Eurostat) which ranks it among the most dense and longest controlled-access systems in the world, and fifth in density within the EU in 2016 (Netherlands 66, Finland 3). Longer similar systems can be found in the United States () and in China (). However both the U.S. and China have an area nearly 30 times bigger than Germany, which demonstrates the high density of Germany's highway system.\n\nOnly federally built controlled-access highways with certain construction standards including at least two lanes per direction are called \"Bundesautobahn\". They have their own white-on-blue signs and numbering system. In the 1930s, when construction began on the system, the official name was \"Reichsautobahn\". Various other controlled-access highways exist on the federal \"(Bundesstraße),\" state \"(Landesstraße),\" district, and municipal level but are not part of the \"Autobahn\" network and are officially referred to as \"Kraftfahrstraße\" (with rare exceptions, like A 995 Munich-Giesing–Brunntal). These highways are considered \"autobahnähnlich\" (autobahn-like) and are sometimes colloquially called \"Gelbe Autobahn\" (yellow autobahn) because most of them are \"Bundesstraßen\" (federal highways) with yellow signs. Some controlled-access highways are classified as \"Bundesautobahn\" in spite of not meeting the autobahn construction standard (for example the A 62 near Pirmasens).\n\nSimilar to high-speed motorways in other countries, autobahns have multiple lanes of traffic in each direction, separated by a central barrier with grade-separated junctions and access restricted to motor vehicles with a top speed of more than . Nearly all exits are to the right. The earliest motorways were flanked by shoulders about in width, constructed of varying materials; right-hand shoulders on many autobahns were later retrofitted to in width when it was realized cars needed the additional space to pull off the autobahn safely. In the postwar years, a thicker asphaltic concrete cross-section with full paved hard shoulders came into general use. The top design speed was approximately in flat country but lower design speeds were used in hilly or mountainous terrain. A flat-country autobahn that was constructed to meet standards during the Nazi period, could support the speed of up to on curves.\n\nThe current autobahn numbering system in use in Germany was introduced in 1974. All autobahns are named by using the capital letter A, which simply stands for \"Autobahn\" followed by a blank and a number (for example A 8). The main autobahns going all across Germany have a single digit number. Shorter autobahns that are of regional importance (e.g. connecting two major cities or regions within Germany) have a double digit number (e.g. A 24, connecting Berlin and Hamburg). The system is as follows:\n\n\nThere are also some very short autobahns built just for local traffic (e.g. ring roads or the A 555 from Cologne to Bonn) that usually have three digits for numbering. The first digit used is similar to the system above, depending on the region. East-west routes are always even-numbered, north-south routes are always odd-numbered.\n\nThe north-south autobahns are generally numbered using odd numbers from west to east; that is to say, the more easterly roads are given higher numbers. Similarly, the east-west routes are numbered using even numbers from north (lower numbers) to south (higher numbers).\n\nThe idea for the construction of the autobahn was first conceived in the mid-1920s during the days of the Weimar Republic, but the construction was slow, and most projected sections did not progress much beyond the planning stage due to economic problems and a lack of political support. One project was the private initiative \"HaFraBa\" which planned a \"car-only road\" crossing Germany from Hamburg in the north via central Frankfurt am Main to Basel in Switzerland. Parts of the \"HaFraBa\" were completed in the late 1930s and early 1940s, but construction eventually was halted by World War II. The first public road of this kind was completed in 1932 between Cologne and Bonn and opened by Konrad Adenauer (Lord Mayor of Cologne and future Chancellor of West Germany) on 6 August 1932. Today, that road is the Bundesautobahn 555. This road was not yet called \"Autobahn\" and lacked a centre median like modern motorways, but instead was termed a \"Kraftfahrstraße\" (\"motor vehicle road\") with two lanes each direction without intersections, pedestrians, bicycles, or animal-powered transportation.\n\nJust days after the 1933 Nazi takeover, Adolf Hitler enthusiastically embraced an ambitious autobahn construction project, appointing Fritz Todt, the Inspector General of German Road Construction, to lead it. By 1936, 130,000 workers were directly employed in construction, as well as an additional 270,000 in the supply chain for construction equipment, steel, concrete, signage, maintenance equipment, etc. In rural areas, new camps to house the workers were built near construction sites. The job creation program aspect was not especially important because full employment was almost reached by 1936. The autobahns were not primarily intended as major infrastructure improvement of special value to the military as often stated. Their military value was limited as all major military transports in Germany were done by train to save fuel. The propaganda ministry turned the construction of the autobahns into a major media event that attracted international attention.\n\nThe autobahns formed the first limited-access, high-speed road network in the world, with the first section from Frankfurt am Main to Darmstadt opening in 1935. This straight section was used for high-speed record attempts by the Grand Prix racing teams of Mercedes-Benz and Auto Union until a fatal accident involving popular German race driver Bernd Rosemeyer in early 1938. The world record of set by Rudolf Caracciola on this stretch just prior to the accident remains one of the highest speeds ever achieved on a public motorway. A similar intent in the 1930s existed for a ten-kilometre stretch of what is today Bundesautobahn 9 just south of Dessau—called the \"Dessauer Rennstrecke\"—had bridges with no piers, meant for land speed record cars like the Mercedes-Benz T80 to have made a record attempt in January 1940, abandoned due to the outbreak of World War II in Europe four months earlier.\n\nDuring World War II, the median strips of some autobahns were paved over to allow their conversion into auxiliary airstrips. Aircraft were either stashed in numerous tunnels or camouflaged in nearby woods. However, for the most part during the war, the autobahns were not militarily significant. Motor vehicles, such as trucks, could not carry goods or troops as quickly or in as much bulk and in the same numbers as trains could, and the autobahns could not be used by tanks as their weight and caterpillar tracks damaged the road surface. The general shortage of petrol in Germany during much of the war, as well as the low number of trucks and motor vehicles needed for direct support of military operations, further decreased the autobahn's significance. As a result, most military and economic freight was carried by rail. After the war, numerous sections of the autobahns were in bad shape, severely damaged by heavy Allied bombing and military demolition. Furthermore, thousands of kilometres of autobahns remained unfinished, their construction brought to a halt by 1943 due to the increasing demands of the war effort.\n\nIn West Germany (FRG), most existing autobahns were repaired soon after the war. During the 1950s, the West German government restarted the construction program. It invested in new sections and in improvements to older ones. Finishing the incomplete sections took longer, with some stretches opened to traffic by the 1980s. Some sections cut by the Iron Curtain in 1945 were only completed after German reunification in 1990. Others were never completed, as more advantageous routes were found. Some of these incomplete sections to this very day stretch across the landscape forming a unique type of modern ruin, often easily visible on satellite photographs.\n\nThe autobahns of East Germany (GDR) were neglected in comparison to those in West Germany after 1945. East German autobahns were used primarily for GDR military traffic and for state-owned farming or manufacturing vehicles. The speed limit on the GDR autobahns was 100 km/h; however, lower speed limits were frequently encountered due to poor or quickly changing road conditions. The speed limits on the GDR autobahns were rigorously enforced by the Volkspolizei, whose patrol cars were frequently found hiding under camouflage tarpaulins waiting for speeders.\n\nThe last four kilometres of remaining original \"Reichsautobahn\", a section of A 11 northeast of Berlin near Gartz built in 1936—the westernmost remainder of the never-finished Berlinka—are scheduled for replacement around 2015. Roadway condition is described as \"deplorable\"; the 25-metre-long concrete slabs, too long for proper expansion, are cracking under the weight of the traffic as well as the weather.\n\nThe first autobahn in Austria was the West Autobahn from Wals near Salzburg to Vienna. Building started by command of Adolf Hitler shortly after the \"Anschluss\" in 1938. It extended the \"Reichsautobahn 26\" from Munich (the present-day A8), however only including the branch-off of the planned Tauern Autobahn was opened to the public on 13 September 1941. Construction works discontinued the next year and were not resumed until 1955.\n\nThere are sections of the former German \"Reichsautobahn\" system in the former eastern territories of Germany, \"i.e.\" East Prussia, Farther Pomerania and Silesia; these territories became parts of Poland and the Soviet Union with the implementation of the Oder–Neisse line after World War II. Parts of the planned autobahn from Berlin to Königsberg (the \"Berlinka\") were completed as far as Stettin (Szczecin) on 27 September 1936. After the war, they were incorporated as the A6 autostrada of the Polish motorway network. A single-carriageway section of the \"Berlinka\" east of the former \"Polish Corridor\" and the Free City of Danzig opened in 1938; today it forms the Polish S22 expressway from Elbląg (Elbing) to the border with the Russian Kaliningrad Oblast, where it is continued by the R516 regional road. Also on 27 September 1936, a section from Breslau (Wrocław) to Liegnitz (Legnica) in Silesia was inaugurated, which today is part of the Polish A4 autostrada, followed by the (single vehicle) \"Reichsautobahn 9\" from Bunzlau (Bolesławiec) to Sagan (Żagań) the next year, today part of the Polish A18 autostrada.\n\nAfter the German occupation of Czechoslovakia, plans for a motorway connecting Breslau with Vienna via Brno (Brünn) in the \"Protectorate of Bohemia and Moravia\" were carried out from 1939 until construction works discontinued in 1942. A section of the former \"Strecke 88\" near Brno is today part of the R52 expressway of the Czech Republic.\n\n, Germany's autobahn network has a total length of about 12,993 km. From 2009 Germany has embarked on a massive widening and rehabilitation project, expanding the lane count of many of its major arterial routes, such as the A5 in the southwest and A8 going east-west.\n\nMost sections of Germany's autobahns are modern, containing two or three, sometimes four lanes in addition to an emergency lane (hard shoulder). A few other sections remain in an old state, with two lanes, no emergency lane, and short slip-roads and ramps.\n\nThe motorway density in Germany is 36 kilometers per thousand square kilometer in 2016, close to the small near countries ( Netherlands, Belgium, Luxembourg, Switzerland, Slovenia).\n\nAbout 16,000 emergency telephones are distributed at regular intervals all along the autobahn network, with triangular stickers on the armco barriers pointing the way to the nearest one. Despite the increasing use of mobile phones, there are still some 700 calls made each day on average.\n\nFor the Emergency service or Roadside assistance to come to the right location, the road kilometre must be given as part of the emergency call.\n\nFor breaks during longer journeys, parking sites, rest areas and truck stops are distributed over the complete Autobahn network. Parking on the autobahn is prohibited in the strictest terms outside these designated areas. There is a distinction between \"managed\" and \"unmanaged\" rest areas. (German: \"bewirtschaftet\" / \"unbewirtschaftet\").\n\nA managed rest area (German: \"Autobahnraststätte\" or \"Raststätte\" for short) usually also includes a filling station, charging station, lavatories, toilets and baby changes. Many rest areas also have restaurants, shops, public telephones, internet access and a playground. Some have hotels. Mandated every 50 km or so, rest areas are usually open all night.\n\nBoth kinds of rest areas are directly on the autobahn, with their own exits, and any service roads connecting them to the rest of the road network are usually closed to general traffic. The autobahn must not be left at rest areas.\n\nRest areas and truck stops are marked several times, starting several kilometres in advance, and with larger signs that often include icons announcing what kinds of facilities travellers can expect.\n\nGermany's autobahns are famous for being among the few public roads in the world without blanket speed limits for cars and motorcycles. As such, they are important German cultural identifiers, \"... often mentioned in hushed, reverential tones by motoring enthusiasts and looked at with a mix of awe and terror by outsiders.\" Some speed limits are implemented on different autobahns.\n\nCertain limits are imposed on some classes of vehicles:\n\nAdditionally, speed limits are posted at most on- and off-ramps and interchanges and other danger points like sections under construction or in need of repair.\n\nWhere no general limit is required, the advisory speed limit is , referred to in German as the \"Richtgeschwindigkeit\". The advisory speed is not enforceable; however, being involved in an accident driving at higher speeds can lead to the driver being deemed at least partially responsible due to \"increased operating danger\" (\"Erhöhte Betriebsgefahr\").\n\nThe Federal Road Research Institute (\"Bundesanstalt für Straßenwesen\") solicited information about speed regulations on autobahns from the sixteen States and reported the following, comparing the years 2006 and 2008:\nExcept at construction sites, the general speed limits, where they apply, are usually between and ; construction sites usually have a speed limit of but the limit may be as low as . In rare cases, sections may have limits of , or on one ramp . Certain stretches have lower speed limits during wet weather. Some areas have a speed limit of in order to reduce noise pollution during overnight hours (usually 10pm – 6am) or because of increased traffic during daytime (6am – 8pm).\nSome limits were imposed to reduce pollution and noise. Limits can also be temporarily put into place through dynamic traffic guidance systems that display the according message. More than half of the total length of the German autobahn network has no speed limit, about one third has a permanent limit, and the remaining parts have a temporary or conditional limit.\n\nSome cars with very powerful engines can reach speeds of well over . Major German car manufacturers, except Porsche, follow a \"gentlemen's agreement\" by electronically limiting the top speeds of their cars—with the exception of some top of the range models or engines—to . These limiters can be deactivated, so speeds up to might arise on the German autobahn, but due to other traffic, such speeds are generally not attainable except during certain times like between 10 p.m. and 6 a.m. or on Sundays (when trucks drivers have to rest by law). Furthermore, there are certain autobahn sections which are known for having light traffic, making such speeds attainable during most days (especially some of those located in Eastern Germany). Most unlimited sections of the autobahn are located outside densely populated areas.\n\nVehicles with a top speed less than (such as quads, low-end microcars, and agricultural/construction equipment) are not allowed to use the autobahn, nor are motorcycles and scooters with low engine capacity regardless of top speed (mainly applicable to mopeds which are typically limited to 25 or 45 km/h anyway). To comply with this limit, heavy-duty trucks in Germany (e.g. mobile cranes, tank transporters etc.) often have a maximum design speed of (usually denoted by a round black-on-white sign with \"62\" on it), along with flashing orange beacons to warn approaching cars that they are travelling slowly. There is no general minimum speed but drivers are not allowed to drive at an unnecessarily low speed as this would lead to significant traffic disturbance and an increased collision risk.\n\nGerman national speed limits have a historical association with war-time restrictions and deprivations, the Nazi era, and the Soviet era in East Germany. \"Free driving for free citizens\" (\"freie Fahrt für freie Bürger\"), a slogan promoted by the German Auto Club since the 1970s, is a popular slogan among those opposing autobahn speed restrictions. Tarek Al-Wazir, head of the Green Party in Hesse, and currently the Hessian Transport Minister has stated that \"the speed limit in Germany has a similar status as the right to bear arms in the American debate... At some point, a speed limit will become reality here, and soon we will not be able to remember the time before. It's like the smoking ban in restaurants.\"\n\nThe Weimar Republic had no federally required speed limits. The first crossroads-free road for motorized vehicles only, now A555 between Bonn and Cologne, had a limit when it opened in 1932. In October 1939, the Nazis instituted the first national maximum speed limit, throttling speeds to in order to conserve gasoline for the war effort. After the war, the four Allied occupation zones established their own speed limits until the divided East German and West German republics were constituted in 1949; initially, the Nazi speed limits were restored in both East and West Germany.\n\nIn December 1952 the West German legislature voted to abolish \"all\" national speed limits, seeing them as Nazi relics, reverting to State-level decisions. National limits were reestablished incrementally. The urban limit was enacted in 1956, effective in 1957. The limit on rural roads—except autobahns—became effective in 1972.\n\nJust prior to the 1973 oil crisis, Germany, Switzerland, and Austria<ref name=\"ÖAMTC/Sicherheit\"></ref> all had no general speed restriction on autobahns. During the crisis, like other nations, Germany imposed temporary speed restrictions; for example, on autobahns effective 13 November 1973. Automakers projected a 20% plunge in sales, which they attributed in part to the lowered speed limits. The 100 km/h limit championed by Transportation Minister Lauritz Lauritzen lasted 111 days. Adjacent nations with unlimited speed autobahns, Austria and Switzerland, imposed permanent limits after the crisis.\n\nHowever, after the crisis eased in 1974, the upper house of the German parliament, which was controlled by conservative parties, successfully resisted the imposition of a permanent mandatory limit supported by Chancellor Brandt. The upper house insisted on a recommended limit until a thorough study of the effects of a mandatory limit could be conducted. Accordingly, the Federal Highway Research Institute conducted a multiple-year experiment, switching between mandatory and recommended limits on two test stretches of autobahn. In the final report issued in 1977, the Institute stated the mandatory speed limit could reduce the autobahn death toll but there would be economic impacts, so a political decision had to be made due to the trade-offs involved. At that time, the Federal Government declined to impose a mandatory limit. The fatality rate trend on the German autobahn mirrored those of other nations' motorways that imposed a general speed limit.\n\nIn the mid-1980s, acid rain and sudden forest destruction renewed debate on whether or not a general speed limit should be imposed on autobahns. A car's fuel consumption increases with high speed, and fuel conservation is a key factor in reducing air pollution. Environmentalists argued that enforcing limits of limit on autobahns and on other rural roads would save lives as well as the forest, reducing the annual death toll by 30% (250 lives) on autobahns and 15% (1,000 lives) on rural roads; the German motor vehicle death toll was about 10,000 at the time.<ref name=\"http://www.bast.de 2014\"></ref> The Federal Government sponsored a large-scale experiment with a speed limit in order to measure the impact of reduced speeds on emissions and compliance. Afterward, again, the Federal Government declined to impose a mandatory limit, deciding the modest measured emission reduction would have no meaningful effect on forest loss. By 1987 all restrictions on test sections had been removed, even in Hesse where the state government was controlled by a \"red-green\" coalition.\n\nPrior to German reunification in 1990, eastern German states focused on restrictive traffic regulation such as a autobahn speed limit and of on other rural roads. Within two years after the opening, availability of high-powered vehicles and a 54% increase in motorized traffic led to a doubling of annual traffic deaths, despite \"interim arrangements [which] involved the continuation of the speed limit of on autobahns and of outside cities\". An extensive program of the four \"E\"s (enforcement, education, engineering, and emergency response) brought the number of traffic deaths back to pre-unification levels after a decade of effort while traffic regulations were conformed to western standards (e.g., freeway advisory limit, on other rural roads, and ).\n\nIn 1993, the Social democratic-Green Party coalition controlling the State of Hesse experimented with a limit on autobahns and on other rural roads. These limits were attempts to reduce ozone pollution.\n\nDuring his term of office (1998 to 2005) as Chancellor of Germany, Gerhard Schröder opposed an autobahn speed limit, famously referring to Germany as an \"Autofahrernation\" (a \"nation of drivers\").\n\nIn October 2007, at a party congress held by the Social Democratic Party of Germany, delegates narrowly approved a proposal to introduce a blanket speed limit of on all German autobahns. While this initiative is primarily a part of the SPD's general strategic outline for the future and, according to practices, not necessarily meant to affect immediate government policy, the proposal had stirred up a debate once again; Germany's chancellor since 2005, Angela Merkel, and leading cabinet members expressed outspoken disapproval of such a measure.\n\nIn 2008, the Social Democratic-Green Party coalition controlling Germany's smallest State, the paired City-State of Bremen & Bremerhaven, imposed a 120-kilometre-per-hour (75 mph) limit on its last of speed-unlimited autobahn in hopes of leading other States to do likewise.\n\nIn 2011, the first ever Green minister-president of any German state, Winfried Kretschmann of Baden-Württemberg initially argued for a similar, state-level limit. However, Baden-Württemberg is an important location for the German motor industry, including the headquarters of Daimler AG and Porsche; the ruling coalition ultimately decided a state-level limit on its of speed-unlimited roads—arguing for nationwide speed limit instead.\n\nIn 2014, the conservative-liberal ruling coalition of Saxony confirmed its rejection of a general speed limit on autobahns, instead advocating dynamic traffic controls where appropriate. Between 2010 and 2014 in the State of Hesse, transportation ministers Dieter Posch and his successor Florian Rentsch, both members of the Free Democratic Party, removed or raised speed limits on several sections of autobahn following regular 5-year reviews of speed limit effectiveness; some sections just prior to the installation of Tarek Al-Wazir (Green Party) as Transportation Minister in January 2014 as part of an uneasy CDU-green coalition government. In 2015, the left-green coalition government of Thuringia declared that a general autobahn limit was a Federal matter; Thuringia would not unlaterally impose a general Statewide limit, although the Thuringian environmental minister had recommended a limit.\n\nIn late 2015, Winfried Hermann, Baden-Württemberg's Green minister of transportation, promised to impose a trial speed limit of on about 10% of the state's autobahns beginning in May 2016. However, the ruling green-social democratic coalition lost its majority in the March 2016 elections; while Mr Hermann retained his post in the new Green – Christian Democratic government, he put aside preparations for a speed limit due to opposition from his new coalition partners.\n\nIn 2014, autobahns carried 31% of motorized road traffic while accounting for 11% of Germany's traffic deaths. The autobahn fatality rate of 1.6 deaths per billion travel-kilometres compared favorably with the 4.6 rate on urban streets and 6.5 rate on rural roads.\n\nBetween 1970 and 2010, overall German road fatalities decreased by almost 80% from 19,193 to 3,648; over the same time period, autobahn deaths halved from 945 to 430 deaths. Statistics for 2013 show total German traffic deaths had declined to the lowest count ever recorded: 3,340 (428 on autobahns); a representative of the Federal Statistical Office attributed the general decline to harsh winter weather that delayed the start of the motorcycle-riding season. In 2014, there was a total of 3,377 road fatalities, while autobahn deaths dropped to 375.\n<nowiki>*</nowiki> per 1,000,000,000 travel-kilometres\n\nIn 2012, the leading cause of autobahn accidents was \"excessive speed (for conditions)\": 6,587 so-called \"speed related\" crashes claimed the lives of 179 people, which represents almost half (46.3%) of 387 autobahn fatalities that year. However, \"excessive speed\" does not mean that a speed limit has been exceeded, but that police determined at least one party travelled too fast for existing road or weather conditions. On autobahns 22 people died per 1,000 injury crashes; a lower rate than the 29 deaths per 1,000 injury accidents on conventional rural roads, which in turn is five times higher than the risk on urban roads—speeds are higher on rural roads and autobahns than urban roads, increasing the severity potential of a crash.\n\nA few countries publish the safety record of their motorways; the Federal Highway Research Institute<ref name=\"http://www.bast.de IRTAD\"></ref> provided IRTAD statistics for the year 2012:\nFor example, a person yearly traversing on regular roads and on motorways has an approximately chance of dying in a car accident on a German road in any particular year ( on an autobahn), compared to in Czech Republic, in Denmark, or in the United States.\n\nHowever, there are many differences between countries in their geography, economy, traffic growth, highway system size, degree of urbanization and motorization, and so on.\n\nThe Federal government does not regularly measure or estimate travel speeds. One study reported in a transportation engineering journal offered historical perspective on the increase in travel speeds over a decade, as shown below.\n\n\"Source\": Kellermann, G: Geschwindigkeitsverhalten im Autobahnnetz 1992. Straße+Autobahn, Issue 5/1995.\n\nThe Federal Environmental Office reported that, on a free-flowing section in 1992, the recorded average speed was with 51% of drivers exceeding the recommended speed.\n\nIn 2006, speeds were recorded using automated detection loops in the State of Brandenburg at two points: on a six-lane section of A9 near Niemegk with a advisory speed limit; and on a four-lane section of A10 bypassing Berlin near Groß Kreutz with a mandatory limit. The results are shown below:\nAt peak times on the \"free-flowing\" section of A9, over 60% of road users exceeded the recommended maximum speed, more than 30% of motorists exceeded , and more than 15% exceeded —in other words the so-called \"85th-percentile speed\" was in excess of 170 km/h.\n\nOn 1 January 2005, a new system came into effect for mandatory tolls \"(Mautpflicht)\" on heavy trucks (those weighing more than 12 t) while using the German autobahn system (\"LKW-Maut\"). The German government contracted with a private company, \"Toll Collect GmbH\", to operate the toll collection system, which has involved the use of vehicle-mounted transponders and roadway-mounted sensors installed throughout Germany. The toll is calculated depending on the toll route, as well as based on the pollution class of the vehicle, its weight and the number of axles on the vehicles. Certain vehicles, such as emergency vehicles and buses, are exempt from the toll. An average user is charged €0.15 per kilometre, or about $0.31 per mile (Toll Collect, 2007).\n\nDriving in Germany is regulated by the Straßenverkehrs-Ordnung (\"road traffic regulations\", abbreviated StVO). Enforcement on the federal Autobahnen is handled by each state's Highway Patrol (\"Autobahnpolizei\"), often using unmarked police cars and motorcycles and usually equipped with video cameras, thus allowing easier enforcement of laws such as tailgating. Notable laws include the following.\n\n\n\n\"\", \"\", and \"Burnout Dominator\" use autobahn as one of their tracks. \"Euro Truck Simulator\", \"German Truck Simulator\", and \"Euro Truck Simulator 2\" features the Autobahn in its open world map. \"Burnout 3: Takedown\" named them as Alpine while \"Burnout Dominator\" divided them into two (Autobahn and Autobahn Loop).\n\"\" also had a track that had the player drive across different sections of the autobahn. The entire game world of \"\" is set on the autobahn.\nOn \"Gran Turismo 5\" and \"Gran Turismo 6\", a trophy is awarded to those who have driven the same distance as the autobahn total length. In December 2010 video game developer Synetic GmbH and Conspiracy Entertainment released the title \"Alarm für Cobra 11 – Die Autobahnpolizei\" featuring real world racing and mission-based gameplay. It is taken from the popular German television series about a two-man team of \"Autobahnpolizei\" first set in Berlin then later in North Rhine-Westphalia.\n\n\n\n"}
{"id": "2316598", "url": "https://en.wikipedia.org/wiki?curid=2316598", "title": "Barbara Nitke", "text": "Barbara Nitke\n\nBarbara Nitke (born 1950) is a New York City art photographer who specializes in the subject of human sexual relations. She has worked extensively in the porn and BDSM communities.\n\nNitke was born in Lynchburg, Virginia in 1950 and grew up in Virginia and Alaska. She is currently a fashion/art photographer, and is on the faculty of the School of Visual Arts in New York.\n\nHailed by The Village Voice for her quest \"to find humanity in marginal sex\", Nitke has gained worldwide attention for her affecting and powerful photographs chronicling relationships between consenting adults engaged in sadomasochistic activities.\n\nHer documentation of sexuality began in 1982 on the sets of pornographic films, towards the end of the Golden Age of Porn. She worked in the industry for twelve years as a set photographer on hundreds of adult films. Her photographs from this period reveal the combination of boredom, surrealism, vulnerability and dissociation inherent in the X-rated world. During that time she also became active as a photographer on mainstream television and movie sets, work which she continues today.\n\nIn 1991, after the hardcore porn business moved to Los Angeles, she began shooting on the New York sets of fetish and SM movies, which had become the fastest growing segment of the adult film industry.\n\nThree years later she attended her first meeting of The Eulenspiegel Society, the oldest SM support and educational group in the country, to see a presentation by underground photographer Charles Gatewood. The couples she met in the SM scene fascinated her, and she began photographing them in 1994. They became the focus of her book, \"Kiss of Fire: A Romantic View of Sadomasochism\" (2003). It was among the first mainstream publications to examine the subject of BDSM.\n\nNitke ran a successful Kickstarter campaign to produce her second book, \"Barbara Nitke: American Ecstasy\" (2012), a memoir in pictures and words of her hardcore porn days.\n\nIn 2001, Nitke filed a lawsuit, along with co-plaintiff the National Coalition for Sexual Freedom, challenging the constitutionality of the Communications Decency Act, a federal statute prohibiting the publication of obscenity on the Internet. The case was called \"Nitke v. Ashcroft\", then later changed to \"Nitke v. Gonzales\".\n\nNitke and the NCSF argued that while the Supreme Court's decision in \"Miller v. California\" defines obscenity according to community standards, the Internet does not permit publishers to restrict the dissemination of their speech based on geography. Therefore, the plaintiffs claimed, a person posting sexually explicit material on the Internet could be found criminally liable according to the standards of the most restrictive community in the country. This, Nitke said, would chill her freedom of speech and therefore violate her First Amendment rights.\n\nA three-judge panel of the United States District Court for the Southern District of New York conducted a trial, and in 2005 found that Nitke and the NCSF had presented insufficient evidence that the variation in community standards is substantial enough to chill the plaintiffs' speech. On March 20, 2006, the Supreme Court affirmed that ruling without opinion.\n\n\n\n\n"}
{"id": "45424610", "url": "https://en.wikipedia.org/wiki?curid=45424610", "title": "Desmond Keegan", "text": "Desmond Keegan\n\nDesmond Keegan is an Irish academic.\n\nHe attended the University College Dublin where he pursued a BA in Classical European Civilization and his MA in Medieval European Civilization. Keegan laid the foundations differentiating Distance Education from regular study. In 1979 he initiated the international journal \"Distance Education\", which is now in its 36th year of publication. This was the first publication of its kind to focus on Distance Education as a new area of scholarship. Keegan proposed some of the fundamental issues that are still outstanding. Some of the issues include \"The Role of Time Synchronous Technology\", \"Access Equity and Social Impact of Distance Education\", \"Didactics or Skills Required by Learners and Teachers in using Electronic Technology\" and \"The Market and Willingness of Students to Partake in Electronic Classrooms.\"\n\nKeegan's doctoral thesis was published by Groom Helm named Foundations of Distance Education in 1986. In 1990 the second edition was published by Routledge followed by the third in 1996. This was translated into Italian in 1994 and Chinese in 1997. His work was also chosen as a set text for the Open University of the United Kingdom for the MA in Open and Distance Education in 1997. In his extended work in 1996 he laid the foundation of Distance Education different from regular study.\n\nHe has published ten books on distance education and related subjects:\n\nKeegan, D. (1984) (with Börje Holmberg and David Sewart) \"Distance Education: international perspectives\". London: Croom Helm, (1988) London and New York: Routledge (2nd edition).\n\nKeegan, D. (1985) (with Francesco Lata) \"L’università a distanza. Riflessioni e proposte per un nuovo modello di università. (The distance university. Reflections and proposals for a new university model)\". Milano: Franco Angeli.\n\nKeegan, D. (1986) \"Foundations of distance education.\" London: Croom Helm (first edition), (1990) London and New York:Routledge (2nd edition), (1996) London and New York: Routledge (3rd edition).\n\nKeegan, D. (1993) \"Theoretical principles of distance education.\" London and New York; Routledge.\n\nKeegan, D. (1994) (with Keith Harry and Magnus John) \"Distance education: new perspectives\". London and New York; Routledge.\n\nKeegan, D. (1994) \"The industrialisation of teaching and learning: Otto Peters on distance education\". London and New York; Routledge.\n\nKeegan, D. (1997) \"Distance education in the European Union.\" Brussels: European Commission.\n\nKeegan, D. (1995) (with Gérard Weidenfeld) \"L’enseignnement à distance à l’aube du troisième millénaire. (Distance education at the dawn of the 3rd millennium)\". Futuroscope: CNED.\n\nKeegan, D. (2000) \"Yuancheng Jiaoyu Yahjiu. (The study of distance education)\". Shijiazhuang: Hebei Scientific and Technology Press.\n\nKeegan, D. (2000) \"Distance training: taking stock at a time of change.\" London and New York: Routledge.\n\nHe also published 35 technical analyses of distance education, e-learning, and mobile learning.\n\nVarious online publications between 1992 and 2003 include:\n\nKeegan entered the field of distance education in 1978 when he was appointed Head of School at the South Australian government College of External Studies (SACES)\n\nIn 1980 he founded an international journal on distance education. Today in its 36th year it remains a leading journal in the field.\n\nIn 1992 he founded a series of academic volumes on the theory and practice of distance education, called the \"Routledge Studies in Distance Education Series.\"20 volumes were published between 1992-2003. Professor A. Tait of the Open University of the United Kingdom, became joint executive editor during the course of the Series\n\nKeegan’s major theoretical contribution on the theory and practice of distance education was termed ‘the reintegration of the teaching acts’. It focused on the characteristic of distance education of the separation of the teacher and the learner, contrary to nearly 2000 years of history of education in the West. The daily work of the distance educator is to reintegrate the teaching acts shattered by the benefits of distance education.\nHe carried out a census study of distance education in the European Union on 1 January 1996 counting every student enrolled in every distance education institution in every one of the, then, 15 member states, with 889.893 enrolled in government distance training institutions, 997.967 in correspondence schools and colleges, 462.784 in the 5 open universities and 151.192 in distance education courses from conventional universities for a total of 2.350.795 students. This showed that distance education was the chosen form of training for over 2.000.000 European students per year and at an average enrolment fee of between €100 and €1000 per year was a largely unknown and unstudied billion euro European training industry. This research was published by the European Commission in Brussels.\n\nKeegan also contributed to the development of a new form of distance education which he called ‘mobile learning’ (distance education on smartphones and small tablets). In 1999 he developed his first project in the field for the European Commission and this was funded in early 2000\n\n"}
{"id": "7829039", "url": "https://en.wikipedia.org/wiki?curid=7829039", "title": "Digital program insertion", "text": "Digital program insertion\n\nDigital program insertion (DPI) allows cable headends and broadcast affiliates to insert locally generated commercials and short programs into remotely distributed regional programs before they are delivered to home viewers.\n\nDigital program insertion also refers to a specific technology which allows an MPEG transport stream to be spliced into a currently flowing MPEG transport stream seamlessly and with little or no artifacts. The controlling signaling used to initiate an MPEG is referred to as an SCTE-35 message. The communication API between MPEG splicers and content delivery servers or ad insertion servers is referred to as SCTE30 messages.\n\n"}
{"id": "15114778", "url": "https://en.wikipedia.org/wiki?curid=15114778", "title": "Digital ticket", "text": "Digital ticket\n\nA digital ticket is a virtual instance of a ticket which represents the digitization of rights to claim goods or services.\n\nA digital ticket must fulfill the following criteria:\n\nIn addition, another three requirements are also important for digital tickets, they are:\n\n\n\nBesides the criteria mentioned above, there are still several features that should be concerned, such as anonymity, transferability and repetitive usability.\nThe ticket is first issued by the service provider or issuer. The ownership of a ticket may change after it was issued, by transferring the ticket. Either the issuer or owner of ticket might view the status of the ticket. Finally, it is redeemed by the current owner at the service provider.\nFrom creator's point of view, each digital ticket has certain structure, this could be expressed in a multilayer architecture depicted as follows:\nLayer 1\n\nCommon ticket properties that do not depend on the ticket type:\n\nLayer 2\n\nLayer 3\nDepending on the purpose of the ticket, it may be transferred. During the transfer process, the ticket should be visible to both parties involved. After the transfer process is done, the ownership of ticket has changed. The history of transfer should be recorded in either the ticket itself or the central database.\n\nAbility to view the ticket is important to both the service provider and the owner of the ticket. The owner needs to know what his ticket actually is and the service provider needs to verify the ticket during redemption. The view could be achieved by properly designed hardware.\n\nA digital ticket always has certain value that could be redeemed at service provider. Normally after redeeming, the ticket is cleaned. Some tickets work for a period, and will only be deleted after this period. In the special case when the ticket isn't given away after redeeming, it is called a pass.\n\nIn order to make an implementation of the digital ticket system, a combination of two paradigms can be used. The first is the account-based system, which relies on central storage and network connections. The second is the smartcard-based system, which uses decentralized storage to store and transfer the ticket.\n\nIn an account-based system for tickets, the rights of the tickets are managed in accounts. Ticket changes in accounts can be made by communicating with a so-called account manager through a network. The trust in these systems can be seen from the service provider's and user's perspective, in which the former generally manages the whole system. This leads to an imbalanced trust relationship. Two other disadvantages of these systems are the need for protection of accounts against malicious users and the relatively large efforts that need to be done to store all the accounts for both the users and the service provider.\n\nGenerally, the storage and maintaining tasks of the account are assigned to the service provider. This leads to costs and efforts on his side. In some cases these systems could be shared by different service providers, but a need to make general agreements remains. Since the service provider normally has full control over the accounts, tickets could be deleted or altered and after that refuse to fulfill the service the initial ticket stands for.\n\nTypically ID and password are used in account-based systems to authenticate a user. This does not prevent fraud by the service provider. Several digital-cash systems deal with this by having a secret key generated at the user's PC, which remains out of hands of the service provider. This, however, is not sufficient when tickets are redeemed at the real service provider.\n\nSince the account management is completely in control of the service providers, unwanted actions such as copying tickets can easily be detected and traced back by them.\n\nIn smart card-based systems, tickets are stored on a smart card and are circulated by putting two smart cards in a reader and completing the transaction. The smart card itself takes care of the calculations that need to be done for safe transferring.\n\nTickets are stored on the smart cards. The smart cards can be provided by both the users and the service providers. The performance of current smart cards is limited, which makes asynchronous trading difficult. Different service providers are likely to use different standards, which makes it mandatory to have a different smart cards for different kinds of tickets.It is very useful to passengers.\n\nA secret key can be implemented in the smart card, which makes it possible to carry the card around and redeem a ticket without using a network connection. When the service provider distributes and issues the private keys on these cards, fraud from malicious service providers is still an issue. This also makes it hard for different service providers\nto share a smart card.\n\nStorage is generally maintained by the service provider. The smart card needs to be protected against multiplication. However, if the system is broken security is completely lost.\n\n\n"}
{"id": "14226794", "url": "https://en.wikipedia.org/wiki?curid=14226794", "title": "Doktoringenieur", "text": "Doktoringenieur\n\nThe Doktoringenieur (acronym Dr.-Ing., also \"Doktor der Ingenieurwissenschaften\") is the German engineering doctorate degree, comparable to the Doctor of Engineering, Engineering Doctorate, Doctor of Science (Engineering), Doctor of Science (Technology) or a PhD in Engineering or Architecture.\n\nIt was first introduced in 1899, in the context of the centenary of the Technical University of Berlin, at the Prussian Institutes of Technology. The other German states adopted it in the following years. In contrast to the other historic doctoral degrees (e.g. \"Dr. phil.\", \"Dr. jur.\" or \"Dr. med.\"), the Doktoringenieur was not titled in Latin but German, and therefore written with dash (Dr.-Ing.).\n\nIn the field of mathematics, computer science and natural sciences, some universities offer the choice between \"Dr.-Ing.\" and \"Dr. rer. nat.\" based on the primary focus of the dissertation. If the contributions focus slightly more on applied scientific engineering a \"Dr.-Ing.\" is given, while a \"Dr. rer. nat.\" is preferred if the dissertation contains more theoretical scientific contributions. It may be worth noting that a German doctorate is usually a research doctorate and is awarded in the context of the so-called promotion that also requires a dissertation.\n\nIt should not be confused with a Dutch double title \"dr. ing.\", indicating one holds a doctorate (Dr.) as well an engineer's degree (Ing.) from a Dutch polytechnic (i.e. Hogeschool).\n\n"}
{"id": "25686459", "url": "https://en.wikipedia.org/wiki?curid=25686459", "title": "Elegant glass", "text": "Elegant glass\n\nElegant glass is high quality glassware created in the United States during the Depression Era. It was sold for high prices in department stores and given as wedding gifts. When new, Elegant glass would cost more than its oft-confused counterpart, Depression glass, because it was at least partially handmade, had a cleaner finish, and more vibrant colors. From the 1920s through the 1950s, Elegant glass was an alternative to fine china. Most of the Elegant glassware manufacturers closed by the end of the 1950s, and cheap glassware and imported china took its place.\n\nElegant glass was at least partially hand made during production. Elegant glass manufacturers produced vibrant colors that varied far more than Depression Glass. Shades of red, blue, green, amber, yellow, smoke, amethyst, and pink were produced. An easy way to compare the difference in color quality is to take a look at a piece of cobalt Elegant glass and place it alongside a piece of cobalt Depression Glass. The intensity of the former piece is quite evident. Pressed Elegant glass was fire polished to get rid of the flaws in the glass. The normal flaws found in pressed glass – straw marks, raised seams, etc. were removed. The base of bowls, platters, etc. was ground so it would sit evenly on a table. Many patterns of Elegant glass were embellished with acid etching, cutting, enamel decoration, gold encrustation, platinum and gold trim.\n\nElegant glass was sold in the finer stores (never given away). It was also marketed as wedding patterns. It was offered as an alternative to china and crystal which were still imported due to manufacturing costs and were incredibly expensive. Many consumers purchased Elegant glass and placed it on display, only using it for very special occasions.\n\nElegant glass patterns had a wide range of items available including:\n\nTableware included plates, bowls, platters, sherbets, salt and pepper shakers, compotes, creamers, sugar bowls, epergnes, mayonnaise bowls, place holders, baskets, candy dishes, cruets, bells, candlesticks, cheese stands, bread and butter plates, baskets, bon bons, jam/jelly jars, tidbit trays, nut dishes, celery dishes, pickle dishes, lamps, cracker jars, oil and vinegar bottles, marmalade jars, and vases.\n\nBarware includes card trays, milk pitchers, jugs, cigarette holders, coasters, cordial glasses, cocktails glasses, decanters, bitters bottles, ice buckets, water goblets, wine glasses, ashtrays, and muddlers.\n\nCompanies that made Elegant glass and the patterns they produced.\n\n\n\nCompanies and artists that designed acid etching, cutting, enamel decoration, gold encrustation, platinum and gold trim but did not create glass.\n\nCambridge\nDiamond Glass Company\n\nNew Martinsville Glass Company\n\n\nElegant Glass Identification:\n"}
{"id": "1792122", "url": "https://en.wikipedia.org/wiki?curid=1792122", "title": "Extreme Engineering", "text": "Extreme Engineering\n\nExtreme Engineering is a documentary television series that aired on the Discovery Channel and the Science Channel. The program featured futuristic and ongoing engineering projects. The series' last season aired in July 2011. Danny Forster first hosted the series in season 4 and has been the host since season 6.\n\n\"Engineering the Impossible\" was a 2-hour special, created and written by Alan Lindgren and produced by Powderhouse Productions for the Discovery Channel. It focused on three incredible, yet physically possible, engineering projects: the Gibraltar Bridge, the 170-story Millennium Tower and the over Freedom Ship. This program won the Beijing International Science Film Festival Silver Award, and earned Discovery's second-highest weeknight rating for 2002. After the success of this program, Discovery commissioned Powderhouse to produce the first season of the 10-part series, \"Extreme Engineering\", whose episodes were written by Alan Lindgren, Ed Fields and several other Powderhouse writer-producers. Like \"Engineering the Impossible\", the first season of \"Extreme Engineering\" focused on extreme projects of the future. Season 2 (and all seasons since) featured projects already in construction around the world.\n\nSeason 2 was the first season produced in HDTV for HD Theater.\nPowderhouse Productions produced six episodes for season 4 with host Danny Forster. It currently airs under the \"Build It Bigger\" name on HD Theater, The Science Channel, and Discovery Channel.\n\n"}
{"id": "25694361", "url": "https://en.wikipedia.org/wiki?curid=25694361", "title": "Final ASP", "text": "Final ASP\n\nFinal ASP is a Microsoft Exchange Hosting partner based in Memphis, Tennessee.\n\nThe Microsoft Application provider was founded in 2003 by owner Casey Condo. Final ASP was the first Microsoft partner to host products in Tennessee.\nAs of January 2010, Final ASP serves more than 400 clients in the U.S. with more than $1 million in annual revenue.\n\nIn September 2009, Final ASP launched a spin-off Internet marketing company called Magnetic SEO.\n\n"}
{"id": "41211319", "url": "https://en.wikipedia.org/wiki?curid=41211319", "title": "Fuel cell auxiliary power unit", "text": "Fuel cell auxiliary power unit\n\nA fuel cell auxiliary power unit (FC-APU) is a fuel cell based auxiliary power unit on a vehicle that provides energy for functions other than propulsion. They are mainly used in trucking, aviation, marine and recreational vehicles.\n\nIn 2010 there were globally 3,100 fuel cell APU shipments.\n\nAround 300,000 refrigerator trucks with auxiliary power units are on the road in the United States. In recent years, truck and fuel cell manufacturers have teamed up to create, test and demonstrate a fuel cell APU that eliminates nearly all emissions and uses diesel fuel more efficiently. In 2008, a DOE sponsored partnership between Delphi Electronics and Peterbilt demonstrated that a fuel cell could provide power to the electronics and air conditioning of a Peterbilt Model 386 under simulated \"idling\" conditions for 10 hours. Delphi has said the 5 kW system for Class 8 trucks will be released in 2012, at an $8000–9000 price tag that would be competitive with other \"midrange\" two-cylinder diesel APUs, should they be able to meet those deadlines and cost estimates.\n\n\n"}
{"id": "39092570", "url": "https://en.wikipedia.org/wiki?curid=39092570", "title": "Germanane", "text": "Germanane\n\nGermanane is a single-layer crystal composed of germanium with one hydrogen bonded in the z-direction for each atom. In material science, great interest is shown in related single layered materials, such as graphene, composed of carbon, and silicene, composed of silicon. Such materials represent a new generation of semiconductors with potential applications in computer chips and solar cells. Germanane’s structure is similar to graphane, and therefore graphene. Bulk germanium does not adopt this structure. Germanane has been produced in a two-step route starting with calcium germanide. From this material, the calcium is removed by de-intercalation with HCl to give a layered solid with the empirical formula GeH. The Ca sites in Zintyl-phase CaGe2 interchange with the H atoms in the HCl solution, which leaves us with GeH and CaCl2.\n\nGermanane's electron mobility is predicted to be more than ten times that of silicon and five times more than conventional germanium. Hydrogen-doped germanane is chemically and physically stable when exposed to air and water.\n\nGermanane has a “direct band gap,” easily absorbing and emitting light, and potentially useful for optoelectronics. (Conventional silicon and germanium have indirect band gaps, reducing light absorption or emission.) In addition, the Ge atoms have higher spin-orbit coupling (as compared to C in graphene/graphane) which can allow us to explore the quantum spin Hall effect.\n\nResearchers at the University of Groningen in the Netherlands and the University of Ioannina in Greece, have reported on the first field effect transistor fabricated with germanane, highlighting its promising electronic and optoelectronic properties. Germanane FET's show transport in both electron and hole doped regimes with on/off current ratio of up to 10(10) and carrier mobilities of 150 cm (V.s)(70 cm (V.s)) at 77 K (room temperature). A significant enhancement of the device conductivity under illumination with 650 nm red laser is observed.\n\n"}
{"id": "21150165", "url": "https://en.wikipedia.org/wiki?curid=21150165", "title": "Growth of photovoltaics", "text": "Growth of photovoltaics\n\nWorldwide growth of photovoltaics has been an exponential curve between 1992–2017. During this period of time, photovoltaics (PV), also known as solar PV, evolved from a niche market of small scale applications to a mainstream electricity source. When solar PV systems were first recognized as a promising renewable energy technology, programs, such as feed-in tariffs, were implemented by a number of governments in order to provide economic incentives for investments. For several years, growth was mainly driven by Japan and pioneering European countries. As a consequence, cost of solar declined significantly due to experience curve effects like improvements in technology and economies of scale.\n\nExperience curves describe that the price of a thing decreases with the sum-total ever produced. PV growth increased even more rapidly when production of solar cells and modules started to ramp up in the USA with their Million Solar Roofs project, and when renewables were added to China's 2011 five-year-plan for energy production. Since then, deployment of photovoltaics has gained momentum on a worldwide scale, particularly in Asia but also in North America and other regions, where solar PV by 2015–17 was increasingly competing with conventional energy sources as grid parity has already been reached in about 30 countries.\n\nProjections for photovoltaic growth are difficult and burdened with many uncertainties. Official agencies, such as the International Energy Agency consistently increased their estimates over the years, but still fell short of actual deployment.\n\nHistorically, the United States was the leader of installed photovoltaics for many years, and its total capacity amounted to 77 megawatts in 1996—more than any other country in the world at the time. Then, Japan was the world's leader of produced solar electricity until 2005, when Germany took the lead and by 2016 had a capacity of over 40 gigawatts. However, in 2015, China became world's largest producer of photovoltaic power, and in 2017 became the first country to surpass the 100 GW of cumulative installed PV capacity. China is expected to be the leader in installed PV capacity, and along with India and US, it is forecasted to be the largest market for solar PV installations in the coming decade.\n\nBy the end of 2017, cumulative photovoltaic capacity reached about 401 gigawatts (GW), estimated to be sufficient to supply 2.1% of global electricity demand. Solar contributed 8%, 7.4% and 7.1% to the respective annual domestic consumption in Italy, Greece and Germany. The European Photovoltaic Industry Association, a solar industry trade group, claims installed worldwide capacity will more than double or even triple to more than 500 GW between 2016 and 2020; by 2050, it claims solar power will become the world's largest source of electricity. Such an achievement would require PV capacity to grow to 4,600 GW, of which more than half was forecast to be deployed in China and India.\n\nNameplate capacity denotes the peak power output of power stations in unit watt prefixed as convenient, to e.g. kilowatt (kW), megawatt (MW) and gigawatt (GW). Because power output for variable renewable sources is unpredictable, however, using nameplate capacity as a metric significantly overstates a source's average generation. Thus, capacity is typically multiplied by a suitable capacity factor, which takes into account varying conditions - weather, nighttime, latitude, maintenance, etc. to give energy planners an idea of a source's value to the public. In addition, depending on context, the stated peak power may be prior to a subsequent conversion to alternating current, e.g. for a single photovoltaic panel, or include this conversion and its loss for a grid connected photovoltaic power station. Worldwide, the average solar PV capacity factor is 11%.\n\nWind power has different characteristics, e.g. a higher capacity factor and about four times the 2015 electricity production of solar power. Compared with wind power, photovoltaic power production correlates well with power consumption for air-conditioning in warm countries. a handful of utilities have started combining PV installations with battery banks, thus obtaining several hours of dispatchable generation to help mitigate problems associated with the duck curve after sunset.\n\nFor a complete history of deployment over the last two decades, also see section \"History of deployment\".\n\nIn 2017, photovoltaic capacity increased by 95 GW, with a 34% growth year-on-year of new installations. Cumulative installed capacity exceeded 401 GW by the end of the year, sufficient to supply 2.1 percent of the world's total electricity consumption.\n\nAs of 2018, Asia was the fastest growing region, with almost 75% of global installations. China alone accounted for more than half of worldwide deployment in 2017. In terms of cumulative capacity, Asia was the most developed region with more than half of the global total of 401 GW in 2017. Europe continued to decline as a percentage of the global PV market. In 2017, Europe represented 28% of global capacity, the Americas 19% and Middle East 2%.\n\nSolar PV covered 3.5% and 7% of European electricity demand and peak electricity demand, respectively in 2014.\n\nSince the 1950s, when the first solar cells were commercially manufactured, there has been a succession of countries leading the world as the largest producer of electricity from solar photovoltaics. First it was the United States, then Japan, followed by Germany, and currently China.\n\nThe United States, where modern solar PV was invented, led installed capacity for many years. Based on preceding work by Swedish and German engineers, the American engineer Russell Ohl at Bell Labs patented the first modern solar cell in 1946. It was also there at Bell Labs where the first practical c-silicon cell was developed in 1954. Hoffman Electronics, the leading manufacturer of silicon solar cells in the 1950s and 1960s, improved on the cell's efficiency, produced solar radios, and equipped Vanguard I, the first solar powered satellite launched into orbit in 1958.\n\nIn 1977 US-President Jimmy Carter installed solar hot water panels on the White House promoting solar energy and the National Renewable Energy Laboratory, originally named \"Solar Energy Research Institute\" was established at Golden, Colorado. In the 1980s and early 1990s, most photovoltaic modules were used in stand-alone power systems or powered consumer products such as watches, calculators and toys, but from around 1995, industry efforts have focused increasingly on developing grid-connected rooftop PV systems and power stations. By 1996, solar PV capacity in the US amounted to 77 megawatts–more than any other country in the world at the time. Then, Japan moved ahead.\n\nJapan took the lead as the world's largest producer of PV electricity, after the city of Kobe was hit by the Great Hanshin earthquake in 1995. Kobe experienced severe power outages in the aftermath of the earthquake, and PV systems were then considered as a temporary supplier of power during such events, as the disruption of the electric grid paralyzed the entire infrastructure, including gas stations that depended on electricity to pump gasoline. Moreover, in December of that same year, an accident occurred at the multibillion-dollar experimental Monju Nuclear Power Plant. A sodium leak caused a major fire and forced a shutdown (classified as INES 1). There was massive public outrage when it was revealed that the semigovernmental agency in charge of Monju had tried to cover up the extent of the accident and resulting damage. Japan remained world leader in photovoltaics until 2004, when its capacity amounted to 1,132 megawatts. Then, focus on PV deployment shifted to Europe.\n\nIn 2005, Germany took the lead from Japan. With the introduction of the Renewable Energy Act in 2000, feed-in tariffs were adopted as a policy mechanism. This policy established that renewables have priority on the grid, and that a fixed price must be paid for the produced electricity over a 20-year period, providing a guaranteed return on investment irrespective of actual market prices. As a consequence, a high level of investment security lead to a soaring number of new photovoltaic installations that peaked in 2011, while investment costs in renewable technologies were brought down considerably. In 2016 Germany's installed PV capacity was over the 40 GW mark.\n\nChina surpassed Germany's capacity by the end of 2015, becoming the world's largest producer of photovoltaic power. China's rapid PV growth continued in 2016 – with 34.2 GW of solar photovoltaics installed. The quickly lowering feed in tariff rates at the end of 2015 motivated many developers to secure tariff rates before mid-year 2016 – as they were anticipating further cuts (correctly so). During the course of the year, China announced its goal of installing 100 GW during the next Chinese Five Year Economic Plan (2016–2020). China expected to spend ¥1 trillion ($145B) on solar construction during that period. Much of China's PV capacity was built in the relatively less populated west of the country whereas the main centres of power consumption were in the east (such as Shanghai and Beijing). Due to lack of adequate power transmission lines to carry the power from the solar power plants, China had to curtail its PV generated power.\n\nThe average price per watt dropped drastically for solar cells in the decades leading up to 2017. While in 1977 prices for crystalline silicon cells were about $77 per watt, average spot prices in August 2018 were as low as $0.13 per watt or nearly 600 times less than forty years ago. Prices for thin-film solar cells and for c-Si solar panels were around $.60 per watt. Module and cell prices declined even further after 2014 \"(see price quotes in table)\".\n\nThis price trend was seen as evidence supporting Swanson's law (an observation similar to the famous Moore's Law) that states that the per-watt cost of solar cells and panels fall by 20 percent for every doubling of cumulative photovoltaic production. A 2015 study showed price/kWh dropping by 10% per year since 1980, and predicted that solar could contribute 20% of total electricity consumption by 2030.\nIn its 2014 edition of the \"Technology Roadmap: Solar Photovoltaic Energy\" report, the International Energy Agency (IEA) published prices for residential, commercial and utility-scale PV systems for eight major markets as of 2013 \"(see table below)\". However, DOE's SunShot Initiative report states lower prices than the IEA report, although both reports were published at the same time and referred to the same period. After 2014 prices fell further. For 2014, the SunShot Initiative modeled U.S. system prices to be in the range of $1.80 to $3.29 per watt. Other sources identified similar price ranges of $1.70 to $3.50 for the different market segments in the U.S. In the highly penetrated German market, prices for residential and small commercial rooftop systems of up to 100 kW declined to $1.36 per watt (€1.24/W) by the end of 2014. In 2015, Deutsche Bank estimated costs for small residential rooftop systems in the U.S. around $2.90 per watt. Costs for utility-scale systems in China and India were estimated as low as $1.00 per watt. As of May 2017, a residential 5 kW-system in Australia cost on average about AU$1.25, or US$0.93 per watt.\n\nThere were significant advances in conventional crystalline silicon (c-Si) technology in the years leading up to 2017. The falling cost of the polysilicon since 2009, that followed after a period of severe shortage \"(see below)\" of silicon feedstock, pressure increased on manufacturers of commercial thin-film PV technologies, including amorphous thin-film silicon (a-Si), cadmium telluride (CdTe), and copper indium gallium diselenide (CIGS), lead to the bankruptcy of several thin-film companies that had once been highly touted. The sector faced price competition from Chinese crystalline silicon cell and module manufacturers, and some companies together with their patents were sold below cost.\n\nIn 2013 thin-film technologies accounted for about 9 percent of worldwide deployment, while 91 percent was held by crystalline silicon (mono-Si and multi-Si). With 5 percent of the overall market, CdTe held more than half of the thin-film market, leaving 2 percent to each CIGS and amorphous silicon.\n\n\n\n\nIn the early 2000s, prices for polysilicon, the raw material for conventional solar cells, were as low as $30 per kilogram and silicon manufacturers had no incentive to expand production.\n\nHowever, there was a severe silicon shortage in 2005, when governmental programmes caused a 75% increase in the deployment of solar PV in Europe. In addition, the demand for silicon from semiconductor manufacturers was growing. Since the amount of silicon needed for semiconductors makes up a much smaller portion of production costs, semiconductor manufacturers were able to outbid solar companies for the available silicon in the market.\n\nInitially, the incumbent polysilicon producers were slow to respond to rising demand for solar \napplications, because of their painful experience with over-investment in the past. Silicon prices sharply rose to about $80 per kilogram, and reached as much as $400/kg for long-term contracts and spot prices. In 2007, the constraints on silicon became so severe that the solar industry was forced to idle about a quarter of its cell and module manufacturing capacity—an estimated 777 MW of the then available production capacity. The shortage also provided silicon specialists with both the cash and an incentive to develop new technologies and several new producers entered the market. Early responses from the solar industry focused on improvements in the recycling of silicon. When this potential was exhausted, companies have been taking a harder look at alternatives to the conventional Siemens process.\n\nAs it takes about three years to build a new polysilicon plant, the shortage continued until 2008. Prices for conventional solar cells remained constant or even rose slightly during the period of silicon shortage from 2005 to 2008. This is notably seen as a \"shoulder\" that sticks out in the and it was feared that a prolonged shortage could delay solar power becoming competitive with conventional energy prices without subsidies.\n\nIn the meantime the solar industry lowered the number of grams-per-watt by reducing wafer thickness and kerf loss, increasing yields in each manufacturing step, reducing module loss, and raising panel efficiency. Finally, the ramp up of polysilicon production alleviated worldwide markets from the scarcity of silicon in 2009 and subsequently lead to an overcapacity with sharply declining prices in the photovoltaic industry for the following years.\n\nAs the polysilicon industry had started to build additional large production capacities during the shortage period, prices dropped as low as $15 per kilogram forcing some producers to suspend production or exit the sector. Prices for silicon stabilized around $20 per kilogram and the booming solar PV market helped to reduce the enormous global overcapacity from 2009 onwards. However, overcapacity in the PV industry continued to persist. In 2013, global record deployment of 38 GW (updated EPIA figure) was still much lower than China's annual production capacity of approximately 60 GW. Continued overcapacity was further reduced by significantly lowering solar module prices and, as a consequence, many manufacturers could no longer cover costs or remain competitive. As worldwide growth of PV deployment continued, the gap between overcapacity and global demand was expected in 2014 to close in the next few years.\n\nIEA-PVPS published in 2014 historical data for the worldwide utilization of solar PV module production capacity that showed a slow return to normalization in manufacture in the years leading up to 2014. The utilization rate is the ratio of production capacities versus actual production output for a given year. A low of 49% was reached in 2007 and reflected the peak of the silicon shortage that idled a significant share of the module production capacity. As of 2013, the utilization rate had recovered somewhat and increased to 63%.\n\nAfter anti-dumping petition were filed and investigations carried out, the United States imposed tariffs of 31 percent to 250 percent on solar products imported from China in 2012. A year later, the EU also imposed definitive anti-dumping and anti-subsidy measures on imports of solar panels from China at an average of 47.7 percent for a two-year time span.\n\nShortly thereafter, China, in turn, levied duties on U.S. polysilicon imports, the feedstock for the production of solar cells. In January 2014, the Chinese Ministry of Commerce set its anti-dumping tariff on U.S. polysilicon producers, such as Hemlock Semiconductor Corporation to 57%, while other major polysilicon producing companies, such as German Wacker Chemie and Korean OCI were much less affected. All this has caused much controversy between proponents and opponents and was subject of debate.\n\nDeployment figures on a global, regional and nationwide scale are well documented since the early 1990s. While worldwide photovoltaic capacity grew continuously, deployment figures by country were much more dynamic, as they depended strongly on national policies. A number of organizations release comprehensive reports on PV deployment on a yearly basis. They include annual and cumulative deployed PV capacity, typically given in watt-peak, a break-down by markets, as well as in-depth analysis and forecasts about future trends.\n\nDue to the exponential nature of PV deployment, most of the overall capacity has been installed in the years leading up to 2017 \"(see pie-chart)\". Since the 1990s, each year has been a record-breaking year in terms of newly installed PV capacity, except for 2012. Contrary to some earlier predictions, early 2017 forecasts were that 85 gigawatts would be installed in 2017. Near end-of-year figures however raised estimates to 95 GW for 2017-installations.<ref name=\"Est-Global-PV-2016/17\">Global Solar Market Demand Expected To Reach 100 Gigawatts In 2017, Says SolarPower Europe, CleanTechnica, 27 October 2017</ref>\n\nWorldwide growth of solar PV capacity was an exponential curve between 1992 and 2017. Tables below show global cumulative nominal capacity by the end of each year in megawatts, and the year-to-year increase in percent. In 2014, global capacity was expected to grow by 33 percent from 139 to 185 GW. This corresponded to an exponential growth rate of 29 percent or about 2.4 years for current worldwide PV capacity to double. Exponential growth rate: P(t) = Pe, where \"P\" is 139 GW, growth-rate \"r\" 0.29 (results in doubling time \"t\" of 2.4 years).\n\nThe following table contains data from multiple different sources. For 1992–1995: compiled figures of 16 main markets \"(see section All time PV installations by country),\" for 1996–1999: BP-Statistical Review of world energy (Historical Data Workbook) for 2000–2013: EPIA Global Outlook on Photovoltaics Report\n\n\n"}
{"id": "7574868", "url": "https://en.wikipedia.org/wiki?curid=7574868", "title": "Irma Wyman", "text": "Irma Wyman\n\nIrma M. Wyman (January 31, 1928 - November 17, 2015) was an early computer engineer and the first woman to become vice president of Honeywell, Inc. She was a systems thinking tutor and was the first female CIO of Honeywell.\n\nIn 1945, Wyman received a Regents Scholarship and was accepted into the College of Engineering at the University of Michigan as one of seven female students. To supplement her scholarship, she worked as a switchboard operator and waitress.\n\nAt the time, women in engineering programs received little encouragement and support. While her grades qualified her for membership in Tau Beta Pi, the engineering honor society, she received only a \"Women's Badge\", since the society did not admit women at the time. Wyman graduated with a Bachelor of Science/EM degree in 1949.\n\nWhile still a junior in college, Wyman worked on a missile guidance project at the Willow Run Research Center. To calculate trajectory, they used mechanical calculators. She visited the U.S. Naval Proving Ground where Grace Hopper was working on similar problems and discovered they were using a prototype of a programmable Mark II computer developed at Harvard University. She became interested in computers and later recalled that \"I became an enthusiastic pioneer in this new technology and it led to my life's career.\"\n\nAfter graduation, she joined a start-up company that was eventually acquired by Honeywell Information Systems. She moved to Minneapolis and began a long management career at Honeywell, eventually serving as Chief Information Officer. She became vice president of Honeywell Corporate Information Management (CIM) before retiring in 1990.\n\nWyman then began a second career as archdeacon in the Minnesota Diocese of the Episcopal Church where she coached servant leadership, retiring again after ten years as Archdeacon of the Diocese of Minnesota.\n\nWyman supported research and planning as a thought leader in futures studies. As an aside to this, she contended to an interviewer in 1979, that\n\nWyman endowed the Irma M. Wyman Scholarship at the University of Michigan's Center for the Education of Women to support women in engineering, computer science and related fields. Irma's persistent advocacy for women in computer science reflects those of her early career mentor:\n\n\n\"We never get a second chance to make a first impression.\" (1983–1987)\n\n"}
{"id": "1775224", "url": "https://en.wikipedia.org/wiki?curid=1775224", "title": "Jetronic", "text": "Jetronic\n\nJetronic is a trade name of a fuel injection technology for automotive petrol engines, developed and marketed by Robert Bosch GmbH from the 1960s onwards. Bosch licensed the concept to many automobile manufacturers. There are several variations of the technology offering technological development and refinement.\n\nAnalog fuel injection, 'D' is from meaning pressure. Inlet manifold depression (vacuum) is measured using a pressure sensor located in, or connected to the intake manifold, in order to calculate the duration of fuel injection pulses. Originally, this system was called Jetronic, but the name D-Jetronic was later created as a retronym to distinguish it from newer versions.\n\nD-Jetronic was a precursor of modern common rail systems as it had constant pressure fuel delivery to the injectors and pulsed injections, albeit grouped (2 groups of injectors pulsed together) rather than sequential (individual injector pulses) as on later systems.\n\nD-Jetronic used analogue circuitry, with no microprocessor nor digital logic, the ECU utilised about 25 transistors to perform all of the processing. The lack of processing power and the unavailability of solid-state sensors meant that the vacuum sensor was a rather expensive precision instrument, rather like a barometer, with brass bellows inside to measure the manifold pressure. \n\nAlthough conceptually similar to most later systems with individual electrically controlled injectors per cylinder, and pulse-width modulated fuel delivery, the fuel pressure was not modulated by manifold pressure, and the injectors were fired only once per 2 revolutions on the engine (with half of the injectors being fired each revolution).\n\nThe system was last used (with a Lucas designed timing mechanism and Lucas labels super-imposed on some components) on the Jaguar V12 engine (XJ12 and XJ-S) from 1975 until 1979.\n\nMechanical fuel injection, 'K' stands for , meaning \"continuous\". Commonly called 'Continuous Injection System (CIS) in the USA. K-Jetronic is different from pulsed injection systems in that the fuel flows continuously from all injectors, while the fuel pump pressurises the fuel up to approximately 5 bar (73.5 psi). The volume of air taken in by the engine is measured to determine the amount of fuel to inject. This system has no lambda loop or lambda control. K-Jetronic debuted in the 1973.5 Porsche 911T in January 1973, and was later installed into a number of Porsche, Volkswagen, Audi, BMW, Mercedes-Benz, Rolls-Royce, Bentley, Lotus, Ferrari, Nissan, Peugeot, Renault, Volvo, Saab, DeLorean, TVR and Ford automobiles. The final car to use K-Jetronic was the 1994 Porsche 911 Turbo 3.6.\n\nFuel is pumped from the tank to a large control valve called a \"fuel distributor\", which divides the single fuel supply line from the tank into smaller lines, one for each injector. The fuel distributor is mounted atop a control vane through which all intake air must pass, and the system works by varying fuel volume supplied to the injectors based on the angle of the air vane in the air flow meter, which in turn is determined by the volume of air passing the vane, and by the control pressure. The control pressure is regulated with a mechanical device called the control pressure regulator (CPR) or the warm-up regulator (WUR). Depending on the model, the CPR may be used to compensate for altitude, full load, and/or a cold engine. The injectors are simple spring-loaded check valves with nozzles; once fuel system pressure becomes high enough to overcome the counterspring, the injectors begin spraying.\n\nFirst introduced in the Volvo 265 in 1976. A variant of K-Jetronic with closed-loop lambda control, also named Ku-Jetronic, the letter u denominating USA. The system was developed to comply with U.S.A. state of California's California Air Resources Board exhaust emission regulations, and later replaced by KE-Jetronic.\n\nElectronically controlled mechanical fuel injection. The engine control unit (ECU) may be either analog or digital, and the system may or may not have closed-loop lambda control. The system is based on the K-Jetronic mechanical system, with the addition of an electro-hydraulic actuator, essentially a fuel injector inline with the fuel return. Instead of injecting fuel into the intake, this injector allows fuel to bypass the fuel distributor, which varies the fuel pressure supplied to the mechanical injection components based on several inputs (engine speed, air pressure, coolant temperature, throttle position, lambda etc.) via the ECU. With the electronics disconnected, this system will operate as a K-Jetronic system.\n\nCommonly known as 'CIS-E' in the USA. The later KE3 (CIS-E III) variant features knock sensing capabilities.\n\nAnalog fuel injection. L-Jetronic was often called Air-Flow Controlled (AFC) injection to further separate it from the pressure-controlled D-Jetronic — with the 'L' in its name derived from , meaning 'air'. In the system, air flow into the engine is measured by a moving vane (indicating engine load) known as the volume air flow sensor (VAF) — referred to in German documentation as the \"LuftMengenMesser\" or LMM. L-Jetronic used custom-designed integrated circuits, resulting in a simpler and more reliable engine control unit (ECU) than the D-Jetronic's.\n\nL-Jetronic was used heavily in 1980s-era European cars, as well as BMW K-Series motorcycles. Licensing some of Bosch's L-Jetronic concepts and technologies, Lucas, Hitachi Automotive Products, NipponDenso, and others produced similar fuel injection systems for Asian car manufacturers. L-Jetronic manufactured under license by Japan Electronic Control Systems was fitted to the 1980 Kawasaki Z1000-H1, the world's first production fuel injected motorcycle. Despite physical similarity between L-Jetronic components and those produced under license by other manufacturers, the non-Bosch systems should not be called L-Jetronic, and the parts are usually incompatible.\n\nThis is a simplified and more modern variant of L-Jetronic. The ECU was much cheaper to produce due to more modern components, and was more standardised than the L-Jetronic ECUs. As per L-Jetronic, a vane-type airflow sensor is used. Compared with L-Jetronic, the fuel injectors used by LE-Jetronic have a higher impedance. Three variants of LE-Jetronic exist: LE1, the initial version. LE2 (1984–), featured cold start functionality integrated in the ECU, which does not require the cold start injector and thermo time switch used by older systems. LE3 (1989–), featuring miniaturised ECU with hybrid technology, integrated into the junction box of the mass airflow meter.\n\nThe same as LE1-Jetronic and LE2-Jetronic respectively, but with closed-loop lambda control. Initially designed for the US market.\n\nDigital fuel injection, introduced for California bound 1982 Volvo 240 models. The 'LH' stands for - the hotwire anemometer technology used to determine the mass of air into the engine. This air mass meter is called HLM2 (\"Hitzdraht-LuftMassenmesser\" 2) by Bosch. The LH-Jetronic was mostly used by Scandinavian car manufacturers, and by sports and luxury cars produced in small quantities, such as Porsche 928. The most common variants are LH 2.2, which uses an Intel 8049 (MCS-48) microcontroller, and usually a 4 kB programme memory, and LH 2.4, which uses a Siemens 80535 microcontroller (a variant of Intel's 8051/MCS-51 architecture) and 32 kB programme memory based on the 27C256 chip. LH-Jetronic 2.4 has adaptive lambda control, and support for a variety of advanced features; including fuel enrichment based on exhaust gas temperature (ex. Volvo B204GT/B204FT engines). Some later (post-1995) versions contain hardware support for first generation diagnostics according to ISO 9141 (a.k.a. OBD-II) and immobiliser functions. \n\nDigital fuel injection. This system features one centrally positioned fuel injection nozzle. In the US, this kind of single-point injection was marketed as 'throttle body injection' (TBI, by GM), or 'central fuel injection' (CFI, by Ford).\n\nMono-Jetronic is different from all other known single-point systems, in that it only relies on a throttle position sensor for judging the engine load. There are no sensors for air flow, or intake manifold vacuum. Mono-Jetronic always had adaptive closed-loop lambda control, and due to the simple engine load sensing, it is heavily dependent on the lambda sensor for correct functioning.\n\nThe ECU uses an Intel 8051 microcontroller, usually with 16 kiB of program memory and without advanced on-board diagnostics (OBD-II became a requirement in model-year 1996.)\n\n\n"}
{"id": "51214265", "url": "https://en.wikipedia.org/wiki?curid=51214265", "title": "Kris Rinne", "text": "Kris Rinne\n\nKris Rinne is a technology person and retired Senior VP of network technology at AT&T Labs. She was an inductee to the 2013 Wireless Hall of Fame and the 2014 Women in Technology International Hall of Fame. She has been described as a key person in wireless technologies for her AT&T work.\n"}
{"id": "33517500", "url": "https://en.wikipedia.org/wiki?curid=33517500", "title": "LabLynx", "text": "LabLynx\n\nLabLynx, Inc. is a privately owned, funded, and managed American corporation that develops, supports, and markets laboratory information management system (LIMS) solutions. Its primary offerings over the years have included ELab, webLIMS, and their successor, LabLynx LIMS. The company’s primary clients include laboratories in the agriculture, clinical, environmental, forensics, health care, and manufacturing industries, including government agencies. The company is known for introducing one of the first browser-based LIMS products in 1997 and being in the laboratory informatics industry for decades.\n\nBefore LabLynx was a company, it was a LIMS product offered by Atlanta Systems Consultants, Inc. (ASC). Formed in 1992, ASC’s LabLynx division later began work on a laboratory information management system designed specifically for a web browser. ASC demonstrated its new Internet Explorer-based LabLynx LIMS at Pittcon in 1997, among the first browser-based LIMS to appear at the time. The company again showcased LabLynx at Pittcon in 1998 and soon after picked up a major LIMS-based contract with the U.S. Customs Service.\n\nBy July 2000, the LabLynx division of ASC separated to become its own incorporated entity. And while ASC eventually ceased to exist in 2005, LabLynx, Inc. went on to diversify its offerings. The LabLynx’s browser-based LIMS previously demonstrated at Pittcon in 1997 expanded to become ELab, which in 2001 took on an application service provider (ASP) model of distribution. In 2004 LabLynx released a browser-based tool called openLIMS, which gave consultants and end-users the ability \"to build custom LIMS solutions that are geared to the exact operational needs of many different laboratories.\"\n\nOn June 19, 2006, LabLynx established the Laboratory Informatics Institute, an open membership group with the purpose of advancing the field of laboratory informatics and shaping the standards associated with it.\n\nIn 2011, LabLynx was involved in an initiative to standardize and structure the transmission of laboratory data that first originates in a LIMS or LIS and then moves to a person's or population of people's electronic health records. This laboratory results interface (LRI) pilot began in August 2011 and included collaborations with the supported open source project mdDigest and the U.S. Office of the National Coordinator for Health Information Technology (ONC).\n\nIn February 2015, LabLynx released HealthCloudPOL, a cloud-based laboratory information system (LIS) for the physician office laboratory (POL).\n\nSince transitioning from Atlanta Systems Consultants, Inc. to LabLynx, Inc. in 2000, LabLynx has become increasingly active in the laboratory informatics community. Projects that LabLynx has started or been involved in within the community include:\n\n\nLabLynx products past and present include:\n\n\n"}
{"id": "14078178", "url": "https://en.wikipedia.org/wiki?curid=14078178", "title": "Lever Bank Bleach Works", "text": "Lever Bank Bleach Works\n\nLever Bank Bleach Works was a Bleach Works at Ladyshore, near Little Lever, Bolton. The works was owned by Thomas Ridgway & Sons. Former British Prime Minister Tony Blair would appear to be a direct descendent of this family.\n\nThe works was located between the Manchester Bolton & Bury Canal to the north, and the River Irwell to the south, in the area known as Ladyshore in the Irwell Valley.\n\nAccessed from Ladyshore Bridge, over the canal, the cobbled pathway is still very much evident. Stanchions across the river still stand although the bridge/pipework they supported no longer exists. A weir was built on the nearby River Irwell and water diverted through a small channel into a reservoir. High quality stone walls are still evident along the banks of the river.\n\nThe weir collapsed in June 2012.\n\n"}
{"id": "16713163", "url": "https://en.wikipedia.org/wiki?curid=16713163", "title": "List of Intel manufacturing sites", "text": "List of Intel manufacturing sites\n\nThe following is a list of Intel's manufacturing and assembly/test sites. Processors are manufactured in semiconductor fabrication plants (\"fabs\") which are then sent to assembly and testing sites before delivery to customers. Approximately 75% of Intel's semiconductor fabrication is performed in the USA.\n\nOpened in 1968 in Mountain View, California.\n\nOpened in 1968 in Santa Clara, California.\n\nThe Intel Fab 3 building in Livermore, California on North Mines Road. The plant opened in 1972 and began making wafers in April 1973. Fab 3 closed its doors in 1991. It was the first plant outside of the Santa Clara area, and is where the famous Bunny Suits were first introduced.\n\nThe Intel Fab 4 building was the first Intel wafer manufacturing plant outside of Silicon Valley and the first Intel facility in what is now known as Oregon's Silicon Forest. Located in Aloha, Oregon, production began in 1976 to process 3 inch wafers. The plant was decommissioned in 1996 and was later demolished in 2016. \n\nThe Intel Fab 5 building is located in Aloha, Oregon and was previously a development and then production facility. Currently Fab 5 is inactive.\n\nThe Intel Fab 6 was the first silicon wafer manufacturing facility in Arizona. Located in Chandler, Ground breaking was in early 1980. Fab 6 was in operation from 1980 until 2000. Key architecture was the 286 microprocessor.\n\nThe Intel Fab 7 building is located in Rio Rancho, New Mexico. Production began in 1980 with a focus on flash memory chips and by the time production stopped in 2002 it was producing 0.35 micron-6 inch wafers. Later in 2005, $105 million was invested to temporarily turn Fab 7 into a testing facility.\n\nThe Intel Fab 8 facility is located in Jerusalem, Israel and was Intel's first Fab outside of the United States. Production began in 1985 and ended in 2008 and at the time was Intel's last 6 inch wafer fab. The building was then converted into a die prep facility to support nearby Fab 28in 2009.\n\nThe Intel Fab 9 facility is located in Rio Rancho, New Mexico and production began in 1987. Eventually this facility was expanded to merge with Fab 11 in 1999.\n\nLeixlip, Ireland\n\nRio Rancho, New Mexico\n\nLeixlip, Ireland\n\nThe Intel Fab 15 building is located in Aloha, Oregon. Previously it was a development fab named D1A before construction began on D1B in 1994. Production continued until 2003 when it was converted to an assembly and test facility.\n\nThe Intel Fab 16 building was planned to open in Ft. Worth, Texas in 1999, but was eventually cancelled in 2003.\n\nHudson, Massachusetts\n\nHillsboro, Oregon\n\nRio Rancho, New Mexico\n\nChandler, Arizona\n\nThe Intel Fab 23 facility was located at Colorado Springs, Colorado. The site was originally purchased from Rockwell International in 2000 but due to lack of demand, and financial reasons Intel later put the site up for sale in 2007. The El Paso County government then bought the site in 2011 after there were no offers made and re-purposed the offices.\n\nThe Intel D2 building was located in Santa Clara, California and opened in 1989. It was decommissioned in 2009 and then converted into a data center.\n\n"}
{"id": "398138", "url": "https://en.wikipedia.org/wiki?curid=398138", "title": "List of computer-animated films", "text": "List of computer-animated films\n\nA computer-animated film is a feature film that has been computer-animated to appear three-dimensional. While traditional 2D animated films are now made primarily with the help of computers, the technique to render realistic 3D computer graphics (CG) or 3D computer-generated imagery (CGI), is unique to computers.\n\nThis is a list of theatrically released feature films that are entirely computer-animated.\n\nRelease date listed is the \"first\" public theatrical screening of the completed film. This may mean that the dates listed here may \"not\" be representative of when the film came out in your resident country, or when it was widely released in the United States.\n\nThe country or countries listed reflects the places where the production companies for each title are based. This means that the countries listed for a film might not reflect the location where the film was shot or the countries where the film received a theatrical release. If a title is a multi-country production, the country listed first corresponds with the production company that had the most significant role in the film's creation.\n\n\n"}
{"id": "2898015", "url": "https://en.wikipedia.org/wiki?curid=2898015", "title": "List of the largest software companies", "text": "List of the largest software companies\n\nMany lists exist that provide an overview of large software companies, often called \"independent software vendors\" (\"ISVs\"), in the world. The lists differ by methodology of composition and consequently show substantial differences in both listed companies and ranking of those companies.\n\nThe Forbes Global 2000 is an annual ranking of the top 2000 public companies in the world by \"Forbes\" magazine, based on a mix of four metrics: sales, profit, assets and market value. The Forbes list for software companies includes only pure play (or nearly pure play) software companies and excludes manufacturers, consumer electronics companies, conglomerates, IT consulting firms, and computer services companies even if they have large software divisions. For example, IBM would likely rank #3 in 2017 if its software business unit had been a separate company. The omission of more diversified firms can be useful to investors who are seeking either more or less exposure in the software sector.\n\nThe top 10 companies in the 2017 Forbes list for the \"Software & Programming\" industry are listed in the following table:\n\nAll values listed in the table are in billion US$, Forbes lists market capitalization as of February 2017.\n\n"}
{"id": "960581", "url": "https://en.wikipedia.org/wiki?curid=960581", "title": "Load-bearing wall", "text": "Load-bearing wall\n\nA load-bearing wall or bearing wall is a wall that is an active structural element of a building, that is, it bears the weight of the elements above said wall, resting upon it by conducting its weight to a foundation structure. The materials most often used to construct load-bearing walls in large buildings are concrete, block, or brick.\n\nLoad-bearing walls are one of the earliest forms of construction. The development of the flying buttress in Gothic architecture allowed structures to maintain an open interior space, transferring more weight to the buttresses instead of to central bearing walls. The Notre Dame Cathedral is an example of a load-bearing wall structure with flying buttresses.\n\nIn housing, load-bearing walls are most common in the light construction method known as \"platform framing\", and each load-bearing wall sits on a wall sill plate which is mated to the lowest base plate. The birth of the skyscraper era, the concurrent rise of steel as a more suitable framing system first designed by William Le Baron Jenney, and the limitations of load-bearing construction in large buildings led to a decline in the use of load-bearing walls in large-scale, commercial structures.\n\nA load-bearing wall or bearing wall is a wall that is an active structural element of a building, that is, it bears the weight of the elements above said wall, resting upon it by conducting its weight to a foundation structure. The materials most often used to construct load-bearing walls in large buildings are concrete, block, or brick. By contrast, a curtain wall provides no significant structural support beyond what is necessary to bear its own materials or conduct such loads to a bearing wall.\n\nLoad-bearing walls are one of the earliest forms of construction. The development of the flying buttress in Gothic architecture allowed structures to maintain an open interior space, transferring more weight to the buttresses instead of to central bearing walls. The Notre Dame Cathedral is an example of a load-bearing wall structure with flying buttresses.\n\nDepending on the type of building and the number of floors, load-bearing walls are gauged to the appropriate thickness to carry the weight above them. Without doing so, it is possible that an outer wall could become unstable if the load exceeds the strength of the material used, potentially leading to the collapse of the structure. The primary function of this wall is to enclose or divide space of the building to make it more functional and useful. It provides privacy, affords security, and gives protection against heat, cold, sun or rain.\n\nIn housing, load-bearing walls are most common in the light construction method known as \"platform framing\", and each load-bearing wall sits on a wall sill plate which is mated to the lowest base plate. The sills are bolted to the masonry or concrete foundation.\n\nThe \"top plate\" or \"ceiling plate\" is the top of the wall, which sits just below the platform of the next floor (at the ceiling). The \"base plate\" or \"floor plate\" is the bottom attachment point for the wall studs. Using a top plate and a bottom plate, a wall can be constructed while it lies on its side, allowing for end-nailing of the studs between two plates, and then the finished wall can be tipped up vertically into place atop the wall sill; this not only improves accuracy and shortens construction time, but also produces a stronger wall.\n\nDue to the immense weight of skyscrapers, the base and walls of the lower floors must be extremely strong. Pilings are used to anchor the building to the bedrock underground. For example, the Burj Khalifa, the world's tallest building as well as the world's tallest structure, uses specially treated and mixed reinforced concrete. Over of concrete, weighing more than were used to construct the concrete and steel foundation, which features 192 piles, with each pile being 1.5 m diameter × 43 m long ( × ) and buried more than deep.\n"}
{"id": "2369267", "url": "https://en.wikipedia.org/wiki?curid=2369267", "title": "Lumibrite", "text": "Lumibrite\n\nLumibrite (SrAl2O4 + Eu + Dy - Strontium aluminate doped with Europium and Dysprosium) is a luminous paint used on some models of Seiko, Citizen, Lorus, and Pulsar watches.\n\nThis is a new kind of luminescent paint that glows brighter for longer than the previous generation of luminescent paints, and is environmentally friendly (non radioactive).\n\nLuminous paint\n"}
{"id": "36416250", "url": "https://en.wikipedia.org/wiki?curid=36416250", "title": "Mechanical connections", "text": "Mechanical connections\n\nMechanical rebar connections, also known as mechanical splices, are used to join lengths of rebar together. Building code requirements for masonry construction now require longer lap lengths for some bar sizes. \nThese changes mean more congestion in masonry cells and more difficult construction. To alleviate this problem, builders can either use open-cell blocks, which are more expensive, or use mechanical rebar couplers to eliminate the lap. Reinforcing bar coupling has all of the features desirable in a rebar joining system combined with unequalled simplicity of installation. Couplers are designed to splice the same diameter bars where one bar is free to move and can be rotated.\n\nBenefits of using a mechanical rebar connection include:\n\n\n"}
{"id": "3536583", "url": "https://en.wikipedia.org/wiki?curid=3536583", "title": "Mena Grabowski Trott", "text": "Mena Grabowski Trott\n\nMena Grabowski Trott (born Philomena Frances Grabowski on 16 September 1977, now Mena Grabowski Lazar) is a co-founder of Six Apart, creator of Movable Type and TypePad. The company name originates from the fact that Trott and co-founder/ex-husband Benjamin Trott were born six days apart.\n\nTrott was president of Six Apart. She remains on the Board of Directors after the merger which resulted in the formation of Say Media. She made her first efforts in weblogging at dollarshort.org in 2001.\n\nMovable Type was originally developed by Mena Trott and Benjamin Trott during a period of unemployment in late 2001 for Mena's personal blogging use.\n\nTrott was named one of the People of the Year by PC Magazine in 2004. That same year, she was named a member of the TR100 by MIT Technology Review magazine, as one of the top 100 innovators in the world under the age of 35.\n\nMena Trott was operating the blog \"The Sew Weekly\" at sewweekly.com. The blog encouraged people to \"sew one garment a week\". Each week, Mena developed a theme, such as \"tickled pink\" (garments made from pink fabric) and \"celebrating mothers\" (garments which our mothers wore). That blog project appears to have come to an end as of December 2012.\n\n"}
{"id": "6975823", "url": "https://en.wikipedia.org/wiki?curid=6975823", "title": "Ministry of Higher Education, Science and Technology (Dominican Republic)", "text": "Ministry of Higher Education, Science and Technology (Dominican Republic)\n\nThe Ministerio de Educación Superior, Ciencia y Tecnología (MESCYT) (Ministry of Higher Education, Science and Technology) is a government institution that regulates higher education in the Dominican Republic. It was formerly known as the \"Consejo Nacional de Educación Superior\" (CONES) (National Council of Higher Education). The current minister is Alejandrina Germán, who entered the position in August 2016.\n\n\n"}
{"id": "57042496", "url": "https://en.wikipedia.org/wiki?curid=57042496", "title": "Mobile Passport", "text": "Mobile Passport\n\nMobile Passport is a mobile app that enables travelers entering the United States to submit their passport information and customs declaration form to U.S. Customs and Border Protection (CBP) via smartphone or tablet and go through the inspections process using an expedited lane. Mobile Passport is available to U.S. passport holders and Canadian passport holders with a B1 and B2 visa when entering the United States. \nThe app is available on iOS and Android devices and is operational at 24 airports and the Port Everglades cruise port in Fort Lauderdale, Florida.\nThe use of Mobile Passport operations have increased threefold from 2016 to 2017.\n\nThe Mobile Passport app was developed by Airside Mobile, Inc, an organization started by former executives at the Transportation Security Administration (TSA), Hans Miller and Adam Tsao. Miller and Tsao partnered with Florian and Christina Scholochow in 2010. \n\nMobile Passport operations were launched in Atlanta at the Hartsfield-Jackson International Airport in 2016 and are now available at 24 U.S. airports and 1 U.S. cruise port. The Mobile Passport app is authorized by CBP and sponsored by the Airports Council International-North America, Boeing, and the Port of Everglades.\nAirside Mobile, Inc. secured a Series A funding of $6M in the fall of 2017.\n\nDuring the customs process at the Federal Inspection Service (FIS) area of a U.S. airport, travelers arriving from international locations typically wait in long lines before presenting passports and paperwork and verbally answering questions made by CBP officials. U.S. passport holders and Canadian passport holders with a B1 or B2 visa who have downloaded the Mobile Passport app can expedite this process by submitting information regarding their passport and trip details via their mobile device to CBP officials, then access an expedited line.\nThe Mobile Passport app can be downloaded from the Apple App Store or Google Play Store on to an iOS or Android device and complete their digital passport profile(s) before they travel. A pre-approval is not required and the app is for free. \nTo set up a digital passport profile, travelers may use the document scanner feature within the Mobile Passport app to capture their passport information, and then verify that it is correct. Or, they may manually enter the information directly on the app. The name must appear exactly as it does on the passport. For instance, under Surname, users have to enter their last name. Under Given Names, travelers have to enter their first name [SPACE] middle name [SPACE] middle name, and so on. All hyphens and apostrophes should be replaced by a [SPACE]. To complete the passport profile, users have to touch the box at the bottom of the screen. The camera on the mobile device will open up to allow a “selfie” to be taken of the respective passport holder. According to CBP, group submissions may be made using the Mobile Passport app on one mobile device for all family members of the same household traveling together provided that each family member is a U.S. citizen with a valid U.S. passport or a Canadian citizen with both a valid Canadian passport and B1 or B2 visa status. The Mobile Passport app will not work for U.S. Legal Permanent Residents nor will they be able to process through the designated Mobile Passport lane upon arrival in the United States. \nUpon landing or docking in the United States, and instead of filling out a traditional customs declaration paper form, Mobile Passport users may power up their mobile device, turn on Wi-Fi or data, select all of the passport profiles for their traveling party, and complete the New Trip section on the Mobile Passport app. Within a few moments, CBP will review the submission and send the traveler a digital Encrypted Quick Response (QR) code receipt(s), which expires after four hours. This provides travelers enough time between landing or docking and approaching the FIS area where they may access the designated Mobile Passport lanes. Mobile Passport users will be required to show their physical passport(s) and QR code receipt(s), and briefly talk to a CBP officer. \nTravelers using Mobile Passport may access their mobile devices to use the Mobile Passport app, when processing through the FIS area, but they may not conduct phone calls.\nPersonal data is encrypted and saved is on the device as a profile and transmitted securely to CBP for review, similar to using an Automated Passport Control kiosk. The Mobile Passport app offers an option to store the user profile on the device or to delete it after the trip. \nBecause the Mobile Passport app is used upon entry to the United States, the port of entry is defined as the location where travelers will initially enter the U.S. and undergo screening by CBP. Travelers who would like to use the Mobile Passport app should make sure they are arriving in the U.S. at one of the airports or cruise ports participating with Mobile Passport. \n\n"}
{"id": "45647471", "url": "https://en.wikipedia.org/wiki?curid=45647471", "title": "National Engineering Forum", "text": "National Engineering Forum\n\nThe National Engineering Forum (NEF) is an American movement based on the idea that the U.S. engineering enterprise fuels national security and economic prosperity, but that the nation’s engineers face a series of challenges threatening their profession’s sustainability. The movement is aimed at finding solutions to those challenges, identified by NEF as the 3C’s - capacity, capability and competitiveness:\n\n\nCurrently, NEF spotlights American engineers and the 3C’s via its newsletter and website, and acclaims engineering advancements on Twitter. \n\nIn 2012, Lockheed Martin launched the National Engineering Forum, and then engaged the Council on Competitiveness and the National Academy of Engineering, which share a common vision for transforming the way we perceive, experience, and prioritize engineering in this country. NEF's initial focus included a five-year regional dialogue tour of key engineering hubs throughout the nation, including: New York; Knoxville, Tennessee; Albuquerque, New Mexico; Los Angeles; Columbus, Ohio; Houston; San Diego; Seattle; Detroit; Raleigh-Durham, North Carolina; Pittsburgh; Chicago; Boston; Atlanta; Phoenix; Madison, Wisconsin; Orlando, Florida; Stillwater, Oklahoma and Greenville, South Carolina.\n\nIn addition to sharing engineering news and features in its regular newsletter, in 2014 NEF released a report entitled Engineering our Nation's Future. \n\n\n"}
{"id": "6736017", "url": "https://en.wikipedia.org/wiki?curid=6736017", "title": "National Technical Museum (Prague)", "text": "National Technical Museum (Prague)\n\nThe National Technical Museum () (NTM) in Prague is the largest institution dedicated to preserving information and artifacts related to the history of technology in the Czech Republic. The museum has large exhibits representing approximately 15% of its total collection. The museum also manages substantial archives consisting of approximately 3,500 linear shelf meters of archival material and about 250,000 books. The museum was founded in 1908 and has been in its current location (adjacent to Letná Park) since 1941.\n\nThe collections originate from the professional engineering school (founded in 1717 in Prague), continued by the Prague Polytechnical Institute (founded in 1806) and finally the opening of the Czech industrial museum, founded by Vojtěch Náprstek in 1874. Further parts of his collections were transferred to the museum later in the 20th century.\nThe modern origin is the Technical Museum of Bohemia, opened to the public in 1910 in the Schwarzenberg Palace, renamed the Czechoslovakian Technical Museum in 1918 (with the founding of the state of Czechoslovakia) and moved to its present building in 1942. It was designed by Milan Babuška, the winner of an architectural competition in Functionalist style. The collection moved out during Nazi occupation, then was gradually returned, only fully occupying the main building in the 1990s.\n\nIn 1995 three new galleries were opened: Industrial Design, Single Building, and Fine Arts. Industrial design particularly featured early Czech motorcycles and light fittings, and their processes of manufacture. The Single Building Gallery is used for quarterly exhibitions of models and designs of buildings which have some topical relevance such as an anniversary. The Fine Arts gallery features paintings and sculptures from the 17th to 20th century of subjects related to industry, science and technology.\n\nIn 2001 the museum opened a Railroad Museum that contains about 100 railway vehicles.\n\nThe 2002 European floods caused damage to some 200 cubic metres of documents at the museum - in a location separate to the main building. Work involved in drying and restoring the materials continued until 2013.\n\nThe National Technical Museum was closed from September 2006 to February 2011 for extensive renovations.\n\nAviation pioneerJan Kašpar donated the aircraft, with which he had flown his flight from Pardubice to Velká Chuchle in 1911, to the National Technical Museum in Prague, where it is still on display among many other historic aircraft.\n\n\n"}
{"id": "4907609", "url": "https://en.wikipedia.org/wiki?curid=4907609", "title": "Noren", "text": "Noren\n\nNoren (暖簾) are traditional Japanese fabric dividers hung between rooms, on walls, in doorways, or in windows. They usually have one or more vertical slits cut from the bottom to nearly the top of the fabric, allowing for easier passage or viewing. \"Noren\" are rectangular and come in many different materials, sizes, colors, and patterns.\n\nNoren were originally used to protect a house from wind, dust, and rain, as well as to keep a house warm on cold days and to provide shade on hot summer days. They can also be used for decorative purposes or for dividing a room into two separate spaces.\n\nExterior \"noren\" are traditionally used by shops and restaurants as a means of protection from sun, wind, and dust, and for displaying a shop's name or logo. Names are often Japanese characters, especially kanji, but may be mon emblems, Japanese rebus monograms, or abstract designs. Noren designs are generally traditional to complement their association with traditional establishments, but modern designs also exist. Interior noren are often used to separate dining areas from kitchens or other preparation areas, which also prevents smoke or smells from escaping.\n\nBecause a \"noren\" often features the shop name or logo, the word in Japanese may also refer to a company's brand value. Most notably, in Japanese accounting, the word \"noren\" is used to describe the goodwill of a company after an acquisition.\n\nSentō (commercial bathhouses) also place \"noren\" across their entrances, typically blue in color for men and red for women with the kanji 湯 (yu, lit. hot water) or the corresponding hiragana ゆ. They are also hung in the front entrance to a shop to signify that the establishment is open for business, and they are always taken down at the end of the business day.\n\n"}
{"id": "34930742", "url": "https://en.wikipedia.org/wiki?curid=34930742", "title": "Painting with Fire", "text": "Painting with Fire\n\nPainting with Fire (PWF) is the name given to an immersion process for creating torch fired enamel jewelry. This process is the focal point of torch fired enamel jewelry workshops taught by Barbara A. Lewis, written about in her book, and discussed in \"Belle Armoire Jewelry\", \"Handcrafted Jewelry\", \"Bead Trends\", \"Stringing\" and \"Bead Unique\". \n\nHistorically, enameling is the application of glass-on-metal (See vitreous enamel). Traditional enameling methods, such as Cloisonné and Grisaille, require expensive kilns and often years of training and experience. \n\nThe torch firing of enamel, a process that requires a fuel source such as propane or map gas, is inexpensive and accessible to the jewelry artist who has neither the time nor the financial resources to create a traditional enamel studio.\n\nThe predominant process for producing torch fired enamel jewelry involves placing a cold and pre-washed metal piece (typically copper) on a tripod, heating the piece with a hand-held gas-fueled torch and sifting enamel onto the heated metal. This method, while less expensive than kiln-fired enameling, can be slow and pose significant safety concerns.\nMore than 40 years ago, Joseph Spencer of Safety Harbor, Florida, pioneered Multi-Torch Fired Enameling Barbara Lewis, a long-time ceramic artist and student of Spencer, has applied Spencer’s process to develop the Painting with Fire (PWF) Immersion Process.\n\nUnlike the usual tripod-based torch firing methods, the PWF Immersion Process uses a mounted, inexpensive stationary torch and heating the unwashed cold metal in the flame using a welding tig rod (stainless steel mandrel). The heated metal is then immersed directly into the powdered enamel (Thompson Enamel 80 mesh opaque or transparent), then reheating and repeating the immersion process three times – a total of no more than 60–90 seconds per piece. Using Lewis’ patent pending Bead Pulling Station, the thrice-coated enamel bead is then gently pulled from the mandrel and allowed to fall into a simple bread pan filled with garden vermiculite.\n\nOther kiln and torch firing processes for producing enamel jewelry have typically been limited to pure copper or expensive fine silver. These other methods can be used to enamel sterling silver, but only after completing the laborious process of depletion gilding. With the PWF method, if using transparent enamels in cool colors (blue or green), there is no requirement to heat and dip the oxidized silver into pickling acid. The immersion method involves the same three-times heating and immersion of the sterling piece, attached to the stainless steel mandrel, and coating with transparent enamel.\n\nLewis’ PWF method also pioneered the enameling of lightweight iron filigree beads. The PWF immersion method allows for artistic variations using multiple combinations of opaque and transparent enamels or reducing the oxygen to create smoky hazes. These variations and other applications of the PWF method are discussed in a \"ning\" network. The PWF immersion method is the subject of \"Torch Fired Enamel Jewelry – A Workshop in Painting with Fire\".\n\n"}
{"id": "2620537", "url": "https://en.wikipedia.org/wiki?curid=2620537", "title": "Powers-Samas", "text": "Powers-Samas\n\nPowers-Samas was a British company which sold unit record equipment. \n\nIn 1915 Powers Tabulating Machine Company established European operations through the Accounting and Tabulating Machine Company of Great Britain Limited, in 1929 renamed Powers-Samas Accounting Machine Limited (Samas, full name Societe Anonyme des Machines a Statistiques, had been the Power's sales agency in France, formed in 1922). The informal reference \"Acc and Tab\" would persist.\n\nDuring the Second World War it produced large numbers of Typex cipher machines, derived from the German Enigma, for use by the British armed forces and other government departments. In 1959 it merged with the competing British Tabulating Machine Company (BTM) to form International Computers and Tabulators (ICT).\n\nPowers-Samas machines detected the holes in punched cards mechanically, unlike IBM equipment where holes in punched cards are detected by electrical circuits. Pins that could drop through round holes in punched cards were connected to linkages and their displacement when a hole was present actuated other parts of the machine to produce the desired results.\n\nSetting up a machine involved building a suitable network of linkages. According to one user, this was achieved by locating above the reading block, in contact with the tops of the matrix pins, a removable Y-shaped 'connection box' (equivalent to the Hollerith plug board) which was hard-wired to the job. The box had at the base as many rods as were needed to read the positions within the used data fields, so that, when forced down, appropriate features of the machine - printheads, counters or control links were physically set as a reaction to the moving tops of the connecting box rods. Thus while many connection wires were straight-through, some sensed holes needed to allow multiple actuation, while some multiple code-punching needed to be combined to achieve a single purpose. Designing the system, including setting up the tabulator, was the sales engineers job, while soldering the 'conn-box' forest of cranked rods to meet the design requirement was down to the skill of the Powers Engineer who was thus the doyen of the machine room.\n\nPowers-Samas used a variety of card sizes and formats, including 21, 36, 40, 45, 65 and 130 column cards. The 40-column card, measuring 4.35 by 2 inches, was the most common.\n\n"}
{"id": "3411777", "url": "https://en.wikipedia.org/wiki?curid=3411777", "title": "Quality of experience", "text": "Quality of experience\n\nQuality of Experience (QoE, less frequently QoX or QX) is a measure of the delight or annoyance of a customer's experiences with a service (e.g., web browsing, phone call, TV broadcast). QoE focuses on the entire service experience; it is a holistic concept, similar to the field of User Experience, but with its roots in telecommunication. QoE is an emerging multidisciplinary field based on social psychology, cognitive science, economics, and engineering science, focused on understanding overall human quality requirements.\n\nIn 2013, within the context of the COST Action \"QUALINET\", QoE has been defined as:The degree of delight or annoyance of the user of an application or service. It results from the fulfillment of his or her expectations with respect to the utility and / or enjoyment of the application or service in the light of the user’s personality and current state.This definition has been adopted in 2016 by the International Telecommunication Union in Recommendation ITU-T P.10. Before, various definitions of QoE had existed in the domain, with the above-mentioned definition now finding wide acceptance in the community.\n\nQoE has historically emerged from Quality of Service (QoS), which attempts to objectively measure service parameters (such as packet loss rates or average throughput). QoS measurement is most of the time not related to a customer, but to the media or network itself. QoE however is a purely subjective measure from the user’s perspective of the overall quality of the service provided, by capturing people’s aesthetic and hedonic needs.\n\nQoE looks at a vendor's or purveyor's offering from the standpoint of the customer or end user, and asks, \"What mix of goods, services, and support, do you think will provide you with the perception that the total product is providing you with the experience you desired and/or expected?\" It then asks, \"Is this what the vendor/purveyor has actually provided?\" If not, \"What changes need to be made to enhance your total experience?\" In short, QoE provides an assessment of human expectations, feelings, perceptions, cognition and satisfaction with respect to a particular product, service or application.\n\nQoE is a blueprint of all human subjective and objective quality needs and experiences arising from the interaction of a person with technology and with business entities in a particular context. Although QoE is perceived as subjective, it is an important measure that counts for customers of a service. Being able to measure it in a controlled manner helps operators understand what may be wrong with their services and how to improve them.\n\nQoE aims at taking into consideration every factor that contributes to a user's perceived quality of a system or service. This includes system, human and contextual factors. The following so-called \"influence factors\" have been identified and classified by Reiter et al.:\nStudies in the field of QoE have typically focused on system factors, primarily due to its origin in the QoS and network engineering domains. Through the use of dedicated test laboratories, the context is often sought to be kept constant. However, studies investigating context and human factors have become more popular. Research has shown that human factors account for observed variations in multimedia quality ratings, including socio-cultural and economic background as well as user expectations.\n\nQoE is strongly related to but different from the field of User Experience (UX), which also focuses on users' experiences with services. Historically, QoE has emerged from telecommunication research, while UX has its roots in Human–Computer Interaction. Both fields can be considered multi-disciplinary. In contrast to UX, the goal of improving QoE for users was more strongly motivated by economic needs.\n\nWechsung and De Moor identify the following key differences between the fields:\nAs a measure of the end-to-end performance at the service level from the user's perspective, QoE is an important metric for the design of systems and engineering processes. This is particularly relevant for video services because due to their high traffic demands, bad network performance may highly affect the user's experience. So, when designing systems, the expected output, i.e. the expected QoE, is often taken into account also as a system output metric and optimization goal.\n\nTo measure this level of QoE, human ratings can be used. The Mean Opinion Score (MOS) is a widely used measure for assessing the quality of media signals. It is a limited form of QoE measurement, relating to a specific media type, in a controlled environment and without explicitly taking into account user expectations. The MOS as an indicator of experienced quality has been used for audio and speech communication, as well as for the assessment of quality of Internet video, television and other multimedia signals, and web browsing. Due to inherent limitations in measuring QoE in a single scalar value, the usefulness of the MOS is often debated.\n\nSubjective quality evaluation processes require a lot of human resources, establishing it as a time-consuming process. Objective evaluation methods can provide quality results faster, but require dedicated machine resources and sophisticated apparatus configurations, and may not be as precise as human measurements. Towards this, objective evaluation methods may be based on and make use of multiple metrics.\n\nQoE metrics are often measured at the end devices and can conceptually be seen as the remaining quality after the distortion introduced during the preparation of the content and the delivery through the network, until it reaches the decoder at the end device. There are several elements in the media preparation and delivery chain, and some of them may introduce distortion. This causes degradation of the content, and several elements in this chain can be considered as ”QoE-relevant“ for the offered services. The causes of degradation are applicable for any multimedia service, that is, not exclusive to video or speech. Typical degradations occur at the encoding system (compression degradation), transport network, access network (e.g., packet loss or packet delay), home network (e.g. WiFi performance) and end device (e.g. decoding performance).\n\nAs engineers typically work with QoS parameters, the concept of QoE in engineering is also known as “Perceived Quality of Service” (PQoS), in the sense of the QoS as it is finally perceived by the end-user.\n\nSeveral QoE-centric network management and bandwidth management solutions have been proposed, which aim to improve the QoE delivered to the end-users.\n\nWhen managing a network, QoE fairness may be taken into account in order to keep the users sufficiently satisfied (i.e., high QoE) in a fair manner. From a QoE perspective, network resources and multimedia services should be managed in order to guarantee specific QoE levels instead of classical QoS parameters, which are unable to reflect the actual delivered QoE. A pure QoE-centric management is challenged by the nature of the Internet itself, as the Internet protocols and architecture were not originally designed to support today's complex and high demanding multimedia services.\n\nAs an example for an implementation of QoE management, network nodes can become QoE-aware by estimating the status of the multimedia service as perceived by the end-users. This information can then be used to improve the delivery of the multimedia service over the network and proactively improve the users' QoE. This can be achieved, for example, via traffic shaping. QoE management gives the service provider and network operator the capability to minimize storage and network resources by allocating only the resources that are sufficient to maintain a specific level of user satisfaction.\n\nAs it may involve limiting resources for some users or services in order to increase the overall network performance and QoE, the practice of QoE management requires that net neutrality regulations are considered.\n"}
{"id": "1157601", "url": "https://en.wikipedia.org/wiki?curid=1157601", "title": "Rake (tool)", "text": "Rake (tool)\n\nA rake (Old English \"raca\", cognate with Dutch \"raak\", German \"Rechen\", from the root meaning \"to scrape together\", \"heap up\") is a broom for outside use; a horticultural implement consisting of a toothed bar fixed transversely to a handle, and used to collect leaves, hay, grass, etc., and in gardening, for loosening the soil, light weeding and levelling, removing dead grass from lawns, and generally for purposes performed in agriculture by the harrow.\n\nLarge mechanized versions of rakes are used in farming, called hay rakes, are built in many different forms (e.g. star-wheel rakes, rotary rakes, etc.). Nonmechanized farming may be done with various forms of a hand rake.\n\nAs weeding was a constant issue in the ancient and medieval Chinese agricultural process, it led to the invention of the weeding rake. The invention of the Chinese weed rake is derived from the invention of the Chinese harrow. In the Chinese agricultural text \"Qimin Yaoshu\" written by the Northern Wei Dynasty official Jia Sixie. Harrows were originally called iron-teeth rakes due to its shaping. According to its shape, the Chinese harrow was divided into three sub-classifications: Strip rake, Y-shaped rake, and square rake. The harrows seen in the murals of the Wei and Jin Dynasties are strip rakes.\n\nModern hand-rakes usually have steel, plastic, or bamboo teeth or tines, though historically they have been made with wood or iron. The handle is often made of wood or metal. Some rakes are two-sided and made with dull blades in the shapes of slight crescents, used for removing dead grass (\"thatch\") from lawns. When rakes have longer teeth, they may be arranged in the shape of an old-style folding fan.\n\nIf a rake lies in the ground with the teeth facing upwards, as shown on the top picture, and someone accidentally steps on the teeth, the rake's handle can swing rapidly upwards, colliding with the victim's face. This is often seen in slapstick comedy and cartoons, such as \"Tom and Jerry\" and \"The Simpsons\" episode \"Cape Feare\", wherein a series of rakes become what Sideshow Bob describes as his \"arch-nemesis\". There is a Russian saying \"to step on the same rake\" (), which means \"to repeat the same silly mistake\", also the word \"rake\" () in Russian slang means \"troubles\".\n\nFor conditioning and dethatching soil as well as moving larger pieces of debris. Most weeds have weaker and shallower roots than grass and thus dethatching along with (afterward) necessary sunlight, fertilizer and seed, and if later necessary any remedial chemicals, makes for a good crop of grass. Larger tools (or lawnmower attachments) are more often used for large areas of de-thatching or soil preparation. However the action of making the soil bare and exposed to sun is not good and worms do not like it. It should be protected with straw afterward. Soil aeration tools do not remove weed but prepare soil without exposure.\n\nThere are pros and cons to each. Plastic rakes are generally lighter weight and lower cost. Because they can be fabricated in widths of greater dimensions they are more suitable for leaves which have recently been deposited. Metal tined rakes are better suited for spring raking when the debris is often wet or rotted and can best be collected when the metal tines penetrate to the thatch layer.\n\n"}
{"id": "20698132", "url": "https://en.wikipedia.org/wiki?curid=20698132", "title": "Refrigerated van", "text": "Refrigerated van\n\nA refrigerated van (also called a refrigerated wagon) is a railway goods wagon with cooling equipment. Today they are designated by the International Union of Railways (UIC) as Class I.\n\nThe first wagons were cooled with ice that had been cut in winter from special pools or lakes. It was Gustavus Swift who succeed in the winter of 1877 for the first time in developing an efficient cooling system for railway wagons for Chicago businesses and meat producers. It circulated air through the ice and then through the entire wagon in order to cool it down. This system was the basis of the success of the Union Stock Yard, the Chicago slaughterhouses. The cooled wagons made it possible for the first time to transport the meat of slaughtered animals to the whole of the USA. Later, manufactured ice was used, but this rapidly gave way to other means of cooling; the simplest was the substitution of normal (water) ice by dry ice. With the increasing reliability of combustion engines, engine-powered refrigerator vans emerged. There are even vans whose cooling is achieved by the evaporation of liquid gas.\n\nCompared with machine-cooled vans, ice-cooled wagons have the disadvantage of uneven temperature control, because the cooling effect is only achieved by air circulation. On the other hand, machine-cooled wagons are expensive to maintain and operate, but can be set to the desired temperature and maintained at that temperature throughout the entire journey. They are also better suited to transporting goods at deep-freeze temperatures of around , whereas evaporators and ice-cooling are more suited to maintaining temperatures of around . Banana <nowiki>transport wagons</nowiki> with gas evaporators have an optimum internal temperature of In addition to proper refrigerated vans, there are covered wagons with thermal insulation and, in some cases, even those are equipped with refrigerating sets. These wagons can only operate at temperatures between , where a constant internal temperature is desired.\n\nIn Asia, Chinese and Kazakh railways are experimenting with using refrigerated rail cars to transport sensitive consumer electronic goods manufactured in China to European markets. The products are transported in climate-controlled rail cars from factory towns in the interior of China to Almaty, Kazakhstan, and from there by air to Europe. The use of insulate climate-controlled cars makes it possible for these kinds of goods to be transported by rail even in winter, which had not been done previously.\n\nIn long-distance trains in the former Eastern Block countries, refrigerator trains were used that comprised a refrigeration plant wagon, a guards van and several refrigerated vans.\n\nMost food is transported by road nowadays due to the shorter journey times. The stock of refrigerated vans owned by railway companies has therefore shrunk considerably. Most refrigerated vans in Europe today are operated by Interfrigo. These wagons are easy to tell apart externally: white vans are normal refrigerated wagons, blue ones with white stripes along the side are machine-cooled refrigerator vans.\n\nAlmost all refrigerated vans currently in service are built to UIC standard classes. The two-axled wagons have the same overall dimensions as the covered goods wagons of classes Gbs or Hbfs.\n\n1) Refrigerated and insulated vans / Refrigerated vans with cooling equipment\n\n\n"}
{"id": "22388400", "url": "https://en.wikipedia.org/wiki?curid=22388400", "title": "Rocket mass heater", "text": "Rocket mass heater\n\nA rocket mass heater (also termed rocket stove mass heater) is a space heating system developed from the rocket stove, a type of efficient wood-burning stove, and the masonry heater. Its fundamental characteristics are an insulated combustion chamber where fuel (generally wood) is burned with high efficiency at extremely high temperatures, and a large thermal mass in contact with the exhaust gases which absorbs most of the generated heat before the gases are released to the atmosphere. According to various reports a rocket mass heater can reduce fuel consumption by 80 - 90% compared to \"conventional\" stoves. \n\nAn internal vertical insulated chimney, the combustion chamber, ensures an efficient high-temperature burn and creates enough draft to push exhaust gases through the rest of the system. Flue gases are cooled to a relatively low temperature within the thermal store, approximately 50 °C (122 °F), and in some designs steam within these gases condenses into liquid, releasing the associated latent heat of condensation which further increases the efficiency in the manner of a condensing (gas) boiler.\n\nThe rocket (cooking) stove was developed by the Aprovecho Institute, and described in 1982. A rocket stove is a cooking appliance optimized to provide relatively high heat utilization and low emissions.\n\nIanto Evans of the Cob Cottage Company published his how-to book 'Rocket Mass Heaters' in 2006; however, the first prototype was built in the 1980s. Although in some sense simply a variation on a masonry heater, most rocket mass heaters are distinct in producing immediate radiant heat (from the metal \"burn\" barrel), in being constructed of much cheaper materials (usually a cob mass, 55 gallon steel drum, and small brick firebox), and in requiring less robust a base to be constructed on, since a rocket mass heater's weight is distributed over a larger area. Moreover, masonry heaters have been used since pre-historic times and have evolved into various designs in Europe, Russia, and China. The key principle is the incorporation of a large thermal mass built of masonry which absorbs heat from exhausting combustion products directed in a sinuous path through channels embedded in the masonry.\n\nIn the earliest and most popular form, wood is gravity-fed into a \"J-shaped\" combustion chamber, from where the hot gases enter a heavily insulated fire-brick or ceramic refractory vertical secondary combustion chamber, the exhaust from which then passes along horizontal metal ducting embedded within a massive cob thermal store. The thermal store is large enough to retain heat for many hours and may form part of the structure of the building. They have proved to be popular with natural buildings and within permaculture designs; they are normally self-built and are not yet recognized by all building codes which regulate the design and construction of heating systems within buildings.\n\nMore recent alternate developments have instituted a batch-fed horizontal firebox which feeds into the vertical heat riser or secondary combustion chamber. This type is termed the \"batch box rocket\" and is described on Peter van den Berg's website. Also, the horizontal duct flow through the mass may be replaced by a massive \"bell\" of larger cross section where the hot gases slow and stratify and the cooled gases exit near the bottom as in some masonry heaters. These developments may be used in different combinations in specific instances. \nThis section describes the components of a J-style RMH.\nThe fuel feed receives fuel vertically.\nThe thermal mass which encloses the exhaust duct and absorbs heat from the hot combustion gases is normally made of “cob,” which is a clay and sand mixture reinforced with straw. The thermal mass is often sculpted into a bench which becomes an integral architectural feature of the home, radiating thermal energy into the space for more than 24 hours after the wood fuel is exhausted.\n\n\n\n\n"}
{"id": "2572665", "url": "https://en.wikipedia.org/wiki?curid=2572665", "title": "Roycroft", "text": "Roycroft\n\nRoycroft was a reformist community of craft workers and artists which formed part of the Arts and Crafts movement in the United States. Elbert Hubbard founded the community in 1895, in the village of East Aurora, New York, near Buffalo. Participants were known as Roycrofters. The work and philosophy of the group, often referred to as the Roycroft movement, had a strong influence on the development of American architecture and design in the early 20th century.\n\nThe name \"Roycroft\" was chosen after the printers, Samuel and Thomas Roycroft, who made books in London from about 1650–1690. And beyond this, the word \"roycroft\" had a special significance to Elbert Hubbard, meaning \"King's Craft\". In guilds of early modern Europe, king's craftsmen were guild members who had achieved a high degree of skill and therefore made things for the King. The Roycroft insignia was borrowed from the monk Cassiodorus, a 13th-century bookbinder and illuminator.\n\nElbert Hubbard had been influenced by the ideas of William Morris on a visit to England. He was unable to find a publisher for his book \"Little Journeys\", so inspired by Morris's Kelmscott Press, decided to set up his own private press to print the book himself, founding Roycroft Press.\n\nHis championing of the Arts and Crafts approach attracted a number of visiting craftspeople to East Aurora, and they formed a community of printers, furniture makers, metalsmiths, leathersmiths, and bookbinders. A quotation from John Ruskin formed the Roycroft \"creed\":\n\nA belief in working with the head, hand and heart and mixing enough play with the work so that every task is pleasurable and makes for health and happiness.\n\nThe inspirational leadership of Hubbard attracted a group of almost 500 people by 1910, and millions more knew of him through his essay \"A Message to Garcia\".\n\nThe Roycroft Press is also credited for publishing partner publications, such as Carl Lothar Bredemeier's The Buffalo Magazine for Arts in 1920.\n\nIn 1915 Hubbard and his wife, noted suffragist Alice Moore Hubbard, died in the sinking of RMS \"Lusitania\", and the Roycroft community went into a gradual decline. Following Elbert's death, his son Bert took over the business. In attempts to keep his father's business afloat, Bert proposed selling Roycroft's furniture through major retailers. Sears & Roebuck eventually agreed to carry the furniture, but this was only a short lived success.\n\nFourteen original Roycroft buildings are located in the area of South Grove and Main Street in East Aurora. Known as the \"Roycroft Campus\", this rare survival of an art colony was awarded National Historic Landmark status in 1986.\n\nThe Elbert Hubbard Roycroft Museum, housed in the George and Gladys Scheidemantel House, in East Aurora is the main collection and research centre for the work of the Roycrofters.\n\n\n\n\n"}
{"id": "10566027", "url": "https://en.wikipedia.org/wiki?curid=10566027", "title": "Rupture disc", "text": "Rupture disc\n\nA rupture disc, also known as a pressure safety disc, burst disc, bursting disc, or burst diaphragm, is a non-reclosing pressure relief device that, in most uses, protects a pressure vessel, equipment or system from overpressurization or potentially damaging vacuum conditions. \n\nA rupture disc is a type of sacrificial part because it has a one-time-use membrane that fails at a predetermined differential pressure, either positive or vacuum. The membrane is usually made out of metal, but nearly any material (or different materials in layers) can be used to suit a particular application. Rupture discs provide instant response (within milliseconds) to an increase or decrease in system pressure, but once the disc has ruptured it will not reseal. It is not possible to set an accurate pressure value at which the disc will burst. \nMajor advantages of the application of rupture discs compared to using pressure relief valves include leak-tightness and cost. \n\nRupture discs are commonly used in petrochemical, aerospace, aviation, defense, medical, railroad, nuclear, chemical, pharmaceutical, food processing and oil field applications. They can be used as single protection devices or as a backup device for a conventional safety valve; if the pressure increases and the safety valve fails to operate (or can't relieve enough pressure fast enough), the rupture disc will burst. Rupture discs are very often used in combination with safety relief valves, isolating the valves from the process, thereby saving on valve maintenance and creating a leak-tight pressure relief solution. It is sometimes possible, and preferable for highest reliability though at higher initial cost, to avoid the use of emergency pressure relief devices by developing an intrinsically safe mechanical design that provides containment in all cases. \n\nAlthough commonly manufactured in disc form, the devices also are manufactured as rectangular panels (rupture panels or vent panels). Device sizes range from under to at least , depending upon the industry application. Rupture discs and vent panels are constructed from carbon steel, stainless steel, hastelloy, graphite, and other materials, as required by the specific use environment.\n\nRupture discs are widely accepted throughout industry and specified in most global pressure equipment design codes (ASME, PED, etc.). Rupture discs can be used to specifically protect installations against unacceptably high pressures or can be designed to act as one-time valves or triggering devices to initiate with high reliability and speed a sequence of actions required.\n\nThe dome is weakened by precision cut scores in the material during manufacturing, often using laser technology.\nSo-called forward acting rupture discs have the overpressure on the concave side (inside) of the dome. Rupture of the membrane takes place when tensile forces exceed the ultimate tensile stress of the membrane material. Reverse acting rupture discs have the overpressure on the convex side (outside) of the dome. Stresses cause the dome to buckle after which it fails. The relevant material property that determines failure is the Young's modulus. Correct installation is essential. If installed upside down, the reliability of the overpressure safety device is compromised.\n\nBlowout panels, also called blow-off panels, areas with intentionally weakened structure, are used in enclosures, buildings or vehicles where a sudden overpressure may occur. By failing in a predictable manner, they channel the overpressure or pressure wave in the direction where it causes controlled, directed minimal harm, instead of causing a catastrophic failure of the structure. Blow-off panels are used in ammunition compartments of some tanks to protect the crew in case of ammunition explosion, turning a catastrophic kill into mere firepower kill. An alternative example is a deliberately weakened wall in a room used to store compressed gas cylinders; in the event of a fire or other accident, the tremendous energy stored in the (possibly flammable) compressed gas is directed into a \"safe\" direction, rather than potentially collapsing the structure in a similar manner to a thermobaric weapon.\n\nBlowout panels are installed in several modern tanks, including the M1 Abrams, and have in the past been considered as a possible solution to magazine explosions on battleships.\n\nIn military ammunition storage, blowout panels are included in the design of the bunkers which house explosives. Such bunkers are designed, typically, with concrete walls on four sides, and a roof made of a lighter material covered with earth. In some cases this lighter material is wood, though metal sheeting is also employed. The design is such that if an explosion or fire in the ammunition bunker (also called a locker) were to occur, the force of the blast would be directed vertically, and away from other structures and personnel.\n\nSome models of gene gun also use a rupture disc, but not as a safety device. Instead, their function is part of the normal operation of the device, allowing for precise pressure-based control of particle application to a sample. In these devices, the rupture disc is designed to fail within an optimal range of gas pressure that has been empirically associated with successful particle integration into tissue or cell culture. Different disc strengths can be available for some gene gun models.\n\n"}
{"id": "848494", "url": "https://en.wikipedia.org/wiki?curid=848494", "title": "Shock tube", "text": "Shock tube\n\nThe shock tube is an instrument used to replicate and direct blast waves at a sensor or a model in order to simulate actual explosions and their effects, usually on a smaller scale. Shock tubes (and related impulse facilities such as shock tunnels, expansion tubes, and expansion tunnels) can also be used to study aerodynamic flow under a wide range of temperatures and pressures that are difficult to obtain in other types of testing facilities. Shock tubes are also used to investigate compressible flow phenomena and gas phase combustion reactions. More recently, shock tubes have been used in biomedical research to study how biological specimens are affected by blast waves.\n\nA shock wave inside a shock tube may be generated by a small explosion (blast-driven) or by the buildup of high pressures which cause diaphragm(s) to burst and a shock wave to propagate down the shock tube (compressed-gas driven).\n\nAn early study of compression driven shock tubes was published in 1899 by French scientist Paul Vieille, though the apparatus was not called a shock tube until the 1940s. In the 1940s, interest revived and shock tubes were increasingly used to study the flow of fast moving gases over objects, the chemistry and physical dynamics of gas phase combustion reactions. In 1966, Duff and Blackwell described a type of shock tube driven by high explosives. These ranged in diameter from 0.6 to 2 m and in length from 3 m to 15 m. The tubes themselves were constructed of low-cost materials and produced shock waves with peak dynamic pressures of 7 MPa to 200 MPa and durations of a few hundred microseconds to several milliseconds.\n\nBoth compression-driven and blast-driven shock tubes are currently used for scientific as well as military applications. Compressed-gas driven shock tubes are more easily obtained and maintained in laboratory conditions; however, the shape of the pressure wave is different from a blast wave in some important respects and may not be suitable for some applications. Blast-driven shock tubes generate pressure waves that are more realistic to free-field blast waves. However, they require facilities and expert personnel for handling high explosives. Also, in addition to the initial pressure wave, a jet effect caused by the expansion of compressed gases (compression-driven) or production of rapidly expanding gases (blast-driven) follows and may transfer momentum to a sample after the blast wave has passed. More recently, laboratory scale shock tubes driven by fuel-air mixtures have been developed that produce realistic blast waves and can be operated in more ordinary laboratory facilities. Because the molar volume of gas is much less, the jet effect is a fraction of that for compressed-gas driven shock tubes. To date, the smaller size and lower peak pressures generated by these shock tubes make them most useful for preliminary, nondestructive testing of materials, validation of measurement equipment such as high speed pressure transducers, and for biomedical research as well as military applications.\n\nA simple shock tube is a tube, rectangular or circular in cross-section, usually constructed of metal, in which a gas at low pressure and a gas at high pressure are separated using some form of diaphragm. See, for instance, texts by Soloukhin, Gaydon and Hurle, and Bradley. The diaphragm suddenly bursts open under predetermined conditions to produce a wave propagating through the low pressure section. The shock that eventually forms increases the temperature and pressure of the test gas and induces a flow in the direction of the shock wave. Observations can be made in the flow behind the incident front or take advantage of the longer testing times and vastly enhanced pressures and temperatures behind the reflected wave.\n\nThe low-pressure gas, referred to as the driven gas, is subjected to the shock wave. The high pressure gas is known as the driver gas. The corresponding sections of the tube are likewise called the driver and driven sections. The driver gas is usually chosen to have a low molecular weight, (e.g., helium or hydrogen) for safety reasons, with high speed of sound, but may be slightly diluted to 'tailor' interface conditions across the shock. To obtain the strongest shocks the pressure of the driven gas is well below atmospheric pressure (a partial vacuum is induced in the driven section before detonation).\n\nThe test begins with the bursting of the diaphragm. Several methods are commonly used to burst the diaphragm. \n\nThe bursting diaphragm produces a series of pressure waves, each increasing the speed of sound behind them, so that they compress into a shock propagating through the driven gas. This shock wave increases the temperature and pressure of the driven gas and induces a flow in the direction of the shock wave but at lower velocity than the lead wave. Simultaneously, a rarefaction wave, often referred to as the Prandtl-Meyer wave, travels back in to the driver gas.\n\nThe interface, across which a limited degree of mixing occurs, separates driven and driver gases is referred to as the contact surface and follows, at a lower velocity, the lead wave.\n\nA 'Chemical Shock Tube' involves separating driver and driven gases by a pair of diaphragms designed to fail after pre-determined delays with an end 'dump tank' of greatly increased cross-section. This allows an extreme rapid reduction (quench) in temperature of the heated gases.\n\nIn addition to measurements of rates of chemical kinetics shock tubes have been used to measure dissociation energies and molecular relaxation rates they have been used in aerodynamic tests. The fluid flow in the driven gas can be used much as a wind tunnel, allowing higher temperatures and pressures therein replicating conditions in the turbine sections of jet engines. However, test times are limited to a few milliseconds, either by the arrival of the contact surface or the reflected shock wave.\n\nThey have been further developed into shock tunnels, with an added nozzle and dump tank. The resultant high temperature hypersonic flow can be used to simulate atmospheric re-entry of spacecraft or hypersonic craft, again with limited testing times.\n\nShock tubes have been developed in a wide range of sizes. The size and method of producing the shock wave determine the peak and duration of the pressure wave it produces. Thus, shock tubes can be used as a tool used to both create and direct blast waves at a sensor or an object in order to imitate actual explosions and the damage that they cause on a smaller scale, provided that such explosions do not involve elevated temperatures and shrapnel or flying debris. Results from shock tube experiments can be used to develop and validate numerical model of the response of a material or object to an ambient blast wave without shrapnel or flying debris. Shock tubes can be used to experimentally determine which materials and designs would be best suited to the job of attenuating ambient blast waves without shrapnel or flying debris. The results can then be incorporated into designs to protect structures and people that might be exposed to an ambient blast wave without shrapnel or flying debris. Shock tubes are also used in biomedical research to find out how biological tissues are affected by blast waves.\n\nThere are alternatives to the classical shock tube; for laboratory experiments at very high pressure, shock waves can also be created using high-intensity short-pulse lasers.\n\n\n"}
{"id": "48566641", "url": "https://en.wikipedia.org/wiki?curid=48566641", "title": "Stone industry", "text": "Stone industry\n\nStone industry refers to the part of the primary sector of the economy, similar to the mining industry, but concerned with excavations of stones, in particular granite, marble, slate and sandstone. Other products of the industry include crushed stone and dimension stone.\n\nStone industry is one of the oldest in the world. Creation of stone tools (microliths industry) in the region of South Africa has been dated to about 60,000-70,000 years ago. Granite and marble mining existing as far back as Ancient Egypt. Crushed stone was used extensively by the first great road building civilizations, such as Ancient Greece and Ancient Rome.\n\n"}
{"id": "50245", "url": "https://en.wikipedia.org/wiki?curid=50245", "title": "Sugar beet", "text": "Sugar beet\n\nA sugar beet is a plant whose root contains a high concentration of sucrose and which is grown commercially for sugar production. In plant breeding it is known as the Altissima cultivar group of the common beet (\"Beta vulgaris\"). Together with other beet cultivars, such as beetroot and chard, it belongs to the subspecies \"Beta vulgaris\" subsp. \"vulgaris.\" Its closest wild relative is the sea beet (\"Beta vulgaris\" subsp. \"maritima\").\n\nIn 2013, Russia, France, the United States, Germany, and Turkey were the world's five largest sugar beet producers. In 2010–2011, North America and Europe did not produce enough sugar from sugar beets to meet overall demand for sugar and were all net importers of sugar. The US harvested of sugar beets in 2008. In 2009, sugar beets accounted for 20% of the world's sugar production.\n\nThe sugar beet has a conical, white, fleshy root (a taproot) with a flat crown. The plant consists of the root and a rosette of leaves. Sugar is formed by photosynthesis in the leaves and is then stored in the root.\n\nThe root of the beet contains 75% water, about 20% sugar, and 5% pulp. The exact sugar content can vary between 12 and 21% sugar, depending on the cultivar and growing conditions. Sugar is the primary value of sugar beet as a cash crop. The pulp, insoluble in water and mainly composed of cellulose, hemicellulose, lignin, and pectin, is used in animal feed. The byproducts of the sugar beet crop, such as pulp and molasses, add another 10% to the value of the harvest.\n\nSugar beets grow exclusively in the temperate zone, in contrast to sugarcane, which grows exclusively in the tropical and subtropical zones. The average weight of sugar beet ranges between . Sugar beet foliage has a rich, brilliant green color and grows to a height of about . The leaves are numerous and broad and grow in a tuft from the crown of the beet, which is usually level with or just above the ground surface.\n\nModern sugar beets date back to mid-18th century Silesia where the king of Prussia subsidised experiments aimed at processes for sugar extraction. In 1747, Andreas Marggraf isolated sugar from beetroots and found them at concentrations of 1.3–1.6%. He also demonstrated that sugar could be extracted from beets that was identical with sugar produced from sugarcane. His student, Franz Karl Achard, evaluated 23 varieties of mangelwurzel for sugar content and selected a local strain from Halberstadt in modern-day Saxony-Anhalt, Germany. Moritz Baron von Koppy and his son further selected from this strain for white, conical tubers. The selection was named \"weiße schlesische Zuckerrübe\", meaning white Silesian sugar beet, and boasted about a 6% sugar content. This selection is the progenitor of all modern sugar beets.\n\nA royal decree led to the first factory devoted to sugar extraction from beetroots being opened in Kunern, Silesia (now Konary, Poland) in 1801. The Silesian sugar beet was soon introduced to France, where Napoleon opened schools specifically for studying the plant. He also ordered that be devoted to growing the new sugar beet. This was in response to British blockades of cane sugar during the Napoleonic Wars, which ultimately stimulated the rapid growth of a European sugar beet industry. By 1840, about 5% of the world's sugar was derived from sugar beets, and by 1880, this number had risen more than tenfold to over 50%. The sugar beet was introduced to North America after 1830, with the first commercial production starting in 1879 at a farm in Alvarado, California. The sugar beet was also introduced to Chile by German settlers around 1850.\n\n\"The beet-root, when being boiled, yields a juice similar to syrup of sugar, which is beautiful to look at on account of its vermilion color.\" This was written by 16th-century scientist, Olivier de Serres, who discovered a process for preparing sugar syrup from the common red beet. However, because crystallized cane sugar was already available and provided a better taste, this process never caught on. This story characterizes the history of the sugar beet. The competition between beet sugar and sugarcane for control of the sugar market plays out from the first extraction of a sugar syrup from a garden beet into the modern day.\n\nThe use of sugar beets for the extraction of crystallized sugar dates to 1747, when Andreas Sigismund Marggraf, professor of physics in the Academy of Science of Berlin, discovered the existence of a sugar in vegetables similar in its properties to that obtained from sugarcane. He found the best of these vegetable sources for the extraction of sugar was the white beet. Despite Marggraf’s success in isolating pure sugar from beets, their commercial manufacture for sugar did not take off until the early 19th century. Marggraf's student and successor Franz Karl Achard began selectively breeding sugar beet from the 'White Silesian' fodder beet in 1784. By the beginning of the 19th century, his beet was about 5–6% sucrose by (dry) weight, compared to around 20% in modern varieties. Under the patronage of Frederick William III of Prussia, he opened the world's first beet sugar factory in 1801, at Cunern (Polish: Konary) in Silesia.\n\nThe work of Achard soon attracted the attention of Napoleon Bonaparte, who appointed a commission of scientists to go to Silesia to investigate Achard's factory. Upon their return, two small factories were constructed near Paris. Although these factories were not altogether a success, the results attained greatly interested Napoleon. Thus, when two events, the blockade of Europe by the British Navy and the Haitian Revolution against his brother-in-law, made the importation of cane sugar untenable, Napoleon seized the opportunity offered by beet sugar to address the shortage. In 1811, Napoleon issued a decree appropriating one million francs for the establishment of sugar schools, and compelling the farmers to plant a large acreage to sugar beets the following year. He also prohibited the further importation of sugar from the Caribbean effective in 1813.\n\nThe number of mills increased considerably during the 1820s and 1830s, reaching a peak of 543 in 1837. The number was down to 382 in 1842, producing about 22.5 million kg of sugar during that year.\n\nAs a result of the French advances in sugar beet production and processing made during the Napoleonic Wars, the beet sugar industry in Europe developed rapidly. A new tax levied in Germany in 1810 prompted the experimentation to increase the sugar content of the beet. This was because the tax assessed the value of the sugar beet crop based on the unprocessed weight of the sugar beet rather than the refined sugar produced from them. By 1812, Frenchman Jean-Baptiste Quéruel, working for the industrialist Benjamin Delessert, devised a process of sugar extraction suitable for industrial application. By 1837, France had become the largest sugar beet producer in the world, a position it continued to hold in the world even into 2010. By 1837, 542 factories in France were producing 35,000 tonnes of sugar. However, by 1880, Germany became the largest producer of sugar from sugar beet in the world, since the German factories processed most of the sugar beets grown in eastern France.\n\nBy the 1850s, sugar beet production had reached Russia and Ukraine. This was made possible by the protection of the sugar beet industry by bounties, or subsidies, paid to beet sugar producers upon the export of their sugar by their respective governments. The protection provided to the sugar beet industry by these bounties caused drastic damage to the cane sugar industry and their grip on the British sugar market. The result was a reduction in the production of cane sugar, molasses and rum until 1915. During World War I, the widespread conflict destroyed large tracts of land that had served as sugar beet producers and repurposed much of the remaining sugar beet land for grain production. This resulted in a shortage that revived the shrinking cane sugar industry.\n\nThe first attempts at sugar beet cultivation were pursued by abolitionists in New England. The \"Beet Sugar Society of Philadelphia\" was founded in 1836 and promoted home-produced beet sugar as an alternative to the slave-produced cane sugar from the West Indies or sugar imported from Asia (called \"free sugar\" because it was grown without using slavery), but which tasted \"awful\". However, this movement failed, perhaps most due to the unpopularity of abolitionists at the time, at least until the Civil War, when these associations would become irrelevant and only the economic feasibility of the industry remained.\n\nIn the 1850s, an attempt was made in Utah by the LDS Church-owned Deseret Manufacturing Company to grow and process sugar beets, that failed for several reasons. First, the beet seeds they imported from France were not able to produce much sugar in the heavily salinized soil of Utah. Second, the cost of importing the beet seed from France ate up any possibility for profit. Finally, none of the people running the factory knew how to properly use the chemicals to separate the sugar from the beet pulp.\n\nThe first successful sugar beet factory was built by E. H. Dyer at Alvarado, California (now Union City), in 1870, but did not see any profit until 1879. The factory survived on subsidies it gained, since the abolitionist stigma that had held back the development of a sugar beet industry had been erased with the Civil War.\nAfter this first success in Alvarado, the sugar beet industry expanded rapidly. In 1889, Arthur Stayner and others were able to convince LDS Church leaders to back a second attempt, leading to the Utah-Idaho Sugar Company. By 1914, the sugar beet industry in the United States matched the production of its European counterparts. The largest producers of beet sugar in the United States would remain California, Utah, and Nebraska until the outbreak of World War II. Many sugar beet farmers in California were Japanese Americans; when they were interned during World War II, California's beet sugar production also shifted inland to states such as Idaho, Montana, North Dakota, and Utah. In many of the regions where new sugar beet farms were started during the war, farmers were unfamiliar with beet sugar cultivation, so they hired Japanese workers from internment camps who were familiar with sugar beet production to work on the farms.\n\nSugar beets are grown in 11 states and represent 55% of the US sugar production as compared to sugar cane which is grown in 4 states and accounts for 45% of US sugar production.\n\nSugar beets were not grown on a large scale in the United Kingdom until the mid-1920s, when 17 processing factories were built, following war-time shortages of imported cane sugar. Before World War I, with its far-flung empire, the United Kingdom simply imported the sugar from the cheapest market. However, World War I had created a shortage in sugar, prompting the development of a domestic market. The first sugar beet processing factory was built at Lavenham in Suffolk in 1860, but failed after a few years without the government support its counterparts on the continent received. The Dutch built the first successful factory at Cantley in Norfolk in 1912, and it was moderately successful since, because of its Dutch backing, it received Dutch bounties.\n\nSugar beet seed from France was listed in the annual catalogues of Gartons Agricultural Plant Breeders from that firm's inception in 1898 until the first of their own varieties was introduced in 1909. In 1915, the British Sugar Beet Society was formed to create an example of a domestic sugar beet industry for the purpose of obtaining government financing. Twelve years later, in 1927, they succeeded. The sugar beet industry in the United Kingdom was finally subsidized providing stability to the domestic industry that had experienced volatile shifts in profits and losses in the years since 1915.\n\nReferences to the sugar manufacturing from beets in Russia are dating back to 1802. Jacob Esipov has built a first Russian commercial factory producing sugar from beets in the Tula province.\n\nDuring the Soviet period, some particularly impressive advancements were made in seed development, of which the most useful was the development of a frost-resistant sugar beet, further expanding the growing range of the sugar beet.\n\nThe sugar beet, like sugarcane, needs a peculiar soil and a unique climate for its successful cultivation. The most important requirement is the soil must contain a large supply of plant food, be rich in humus, and have the property of retaining a great deal of moisture. A certain amount of alkali is not necessarily detrimental, as sugar beets are not especially susceptible to injury by some alkali. The ground should be fairly level and well-drained, especially where irrigation is practiced.\n\nWhile the physical character is of secondary importance, as generous crops are grown in sandy soil as well as in heavy loams, still the ideal soil is a sandy loam, i.e., a mixture of organic matter, clay and sand. A subsoil of gravel, or the presence of hard-pan, is not desirable, as cultivation to a depth of from is necessary to produce the best results.\n\nClimatic conditions, temperature, sunshine, rainfall and winds have an important bearing upon the success of sugar beet agriculture. A temperature ranging from during the growing months is most favorable. In the absence of adequate irrigation, of rainfall are necessary to raise an average crop. High winds are harmful, as they generally crust the land and prevent the young beets from coming through the ground. The best results are obtained along the coast of southern California, where warm, sunny days succeeded by cool, foggy nights seem to meet sugar beet's favored growth conditions. Sunshine of long duration but not of great intensity is the most important factor in the successful cultivation of sugar beets. Near the equator, the shorter days and the greater heat of the sun sharply reduce the sugar content in the beet.\n\nIn high elevation regions such as those of Colorado and Utah, where the temperature is high during the daytime, but where the nights are cool, the quality of the sugar beet is excellent. In Michigan, the long summer days from the relatively high latitude (the Lower Peninsula, where production is concentrated, lies between the 41st and 46th parallels North) and the influence of the Great Lakes result in satisfactory climatic conditions for sugar beet culture. Sebewaing, Michigan lies in the Thumb region of Michigan; both the region and state are major sugar beet producers. Sebewaing is home to one of three Michigan Sugar Company factories. The town sponsors an annual Michigan Sugar Festival.\n\nTo cultivate beets successfully, the land must be properly prepared. Deep ploughing is the first principle of beet culture. It allows the roots to penetrate the subsoil without much obstruction, thereby preventing the beet from growing out of the ground, besides enabling it to extract considerable nourishment and moisture from the lower soil. If the latter is too hard, the roots will not penetrate it readily and, as a result, the plant will be pushed up and out of the earth during the process of growth. A hard subsoil is impervious to water and prevents proper drainage. It should not be too loose, however, as this allows the water to pass through more freely than is desirable. Ideally, the soil should be deep, fairly fine and easily penetrable by the roots. It should also be capable of retaining moisture and at the same time admit of a free circulation of air and good drainage. Sugar beet crops exhaust the soil rapidly. Crop rotation is recommended and necessary. Normally, beets are grown in the same ground every third year, peas, beans or grain being raised the other two years.\nIn most temperate climates, beets are planted in the spring and harvested in the autumn. At the northern end of its range, growing seasons as short as 100 days can produce commercially viable sugar beet crops. In warmer climates, such as in California's Imperial Valley, sugar beets are a winter crop, planted in the autumn and harvested in the spring. In recent years, Syngenta has developed the so-called tropical sugar beet. It allows the plant to grow in tropical and subtropical regions. Beets are planted from a small seed; of beet seed comprises 100,000 seeds and will plant over of ground ( will plant about .\n\nUntil the latter half of the 20th century, sugar beet production was highly labor-intensive, as weed control was managed by densely planting the crop, which then had to be manually thinned two or three times with a hoe during the growing season. Harvesting also required many workers. Although the roots could be lifted by a plough-like device which could be pulled by a horse team, the rest of the preparation was by hand. One laborer grabbed the beets by their leaves, knocked them together to shake free loose soil, and then laid them in a row, root to one side, greens to the other. A second worker equipped with a beet hook (a short-handled tool between a billhook and a sickle) followed behind, and would lift the beet and swiftly chop the crown and leaves from the root with a single action. Working this way, he would leave a row of beets that could be forked into the back of a cart.\n\nToday, mechanical sowing, herbicide application for weed control, and mechanical harvesting have displaced this reliance on manual farm work. A root beater uses a series of blades to chop the leaf and crown (which is high in nonsugar impurities) from the root. The beet harvester lifts the root, and removes excess soil from the root in a single pass over the field. A modern harvester is typically able to cover six rows at the same time. The beets are dumped into trucks as the harvester rolls down the field, and then delivered to the factory. The conveyor then removes more soil.\n\nIf the beets are to be left for later delivery, they are formed into clamps. Straw bales are used to shield the beets from the weather. Provided the clamp is well built with the right amount of ventilation, the beets do not significantly deteriorate. Beets that freeze and then defrost, produce complex carbohydrates that cause severe production problems in the factory. In the UK, loads may be hand examined at the factory gate before being accepted.\n\nIn the US, the fall harvest begins with the first hard frost, which arrests photosynthesis and the further growth of the root. Depending on the local climate, it may be carried out over the course of a few weeks or be prolonged throughout the winter months. The harvest and processing of the beet is referred to as \"the campaign\", reflecting the organization required to deliver the crop at a steady rate to processing factories that run 24 hours a day for the duration of the harvest and processing (for the UK, the campaign lasts about five months). In the Netherlands, this period is known as \"de bietencampagne\", a time to be careful when driving on local roads in the area while the beets are being grown, because the naturally high clay content of the soil tends to cause slippery roads when soil falls from the trailers during transport.\n\nThe world harvested of sugar beets in 2013. The world's largest producer was the United States, with a harvest. The average yield of sugar beet crops worldwide was 58.2 tonnes per hectare.\n\nThe most productive sugar beet farms in the world, in 2010, were in Chile, with a nationwide average yield of 87.3 tonnes per hectare.\n\nImperial Valley (California) farmers have achieved yields of about 160 tonnes per hectare and over 26 tonnes sugar per hectare. Imperial Valley farms benefit from high intensities of incident sunlight and intensive use of irrigation and fertilizers.\n\nThe sugar industry in the EU came under bureaucratic pressure in 2006 and ultimately resulted in the loss of 20,000 jobs, although many factories, as detailed in a later 2010 EU audit, were found to have been mistakenly shut down, as they were profitable without government intervention. Western Europe, and Eastern Europe did not produce enough sugar from sugar beets to meet overall demand for sugar in 2010–2011, and were net importers of sugar.\n\nAfter they are harvested, beets are typically transported to a factory. In the UK, beets are transported by a hauler, or by a tractor and a trailer by local farmers. Railways and boats are no longer used. Some beets were carried by rail in the Republic of Ireland, until the complete shutdown of Irish Sugar beet production in 2006.\n\nEach load is weighed and sampled before it gets tipped onto the reception area, typically a \"flat pad\" of concrete, where it is moved into large heaps. The beet sample is checked for\n\nFrom these elements, the actual sugar content of the load is calculated and the grower's payment determined.\n\nThe beet is moved from the heaps into a central channel or gulley, where it is washed towards the processing plant.\n\n\nAfter reception at the processing plant, the beet roots are washed, mechanically sliced into thin strips called cossettes, and passed to a machine called a diffuser to extract the sugar content into a water solution.\n\nDiffusers are long vessels of many metres in which the beet slices go in one direction while hot water goes in the opposite direction. The movement may either be caused by a rotating screw or the whole rotating unit, and the water and cossettes move through internal chambers. The three common designs of diffuser are the horizontal rotating 'RT' \"(Raffinerie Tirlemontoise\", manufacturer), inclined screw 'DDS' (\"De Danske Sukkerfabrikker\"), or vertical screw \"Tower\". Modern tower extraction plants have a processing capacity of up to per day. A less-common design uses a moving belt of cossettes, with water pumped onto the top of the belt and poured through. In all cases, the flow rates of cossettes and water are in the ratio one to two. Typically, cossettes take about 90 minutes to pass through the diffuser, the water only 45 minutes. These countercurrent exchange methods extract more sugar from the cossettes using less water than if they merely sat in a hot water tank. The liquid exiting the diffuser is called raw juice. The colour of raw juice varies from black to a dark red depending on the amount of oxidation, which is itself dependent on diffuser design.\n\nThe used cossettes, or pulp, exit the diffuser at about 95% moisture, but low sucrose content. Using screw presses, the wet pulp is then pressed down to 75% moisture. This recovers additional sucrose in the liquid pressed out of the pulp, and reduces the energy needed to dry the pulp. The pressed pulp is dried and sold as animal feed, while the liquid pressed out of the pulp is combined with the raw juice, or more often introduced into the diffuser at the appropriate point in the countercurrent process. The final byproduct, vinasse, is used as fertilizer or growth substrate for yeast cultures.\n\nDuring diffusion, a portion of the sucrose breaks down into invert sugars. These can undergo further breakdown into acids. These breakdown products are not only losses of sucrose, but also have knock-on effects reducing the final output of processed sugar from the factory. To limit (thermophilic) bacterial action, the feed water may be dosed with formaldehyde and control of the feed water pH is also practiced. Attempts at operating diffusion under alkaline conditions have been made, but the process has proven problematic. The improved sucrose extraction in the diffuser is offset by processing problems in the next stages.\n\n\nCarbonatation is a procedure which removes impurities from raw juice before it undergoes crystallization. First, the juice is mixed with hot milk of lime (a suspension of calcium hydroxide in water). This treatment precipitates a number of impurities, including multivalent anions such as sulfate, phosphate, citrate and oxalate, which precipitate as their calcium salts and large organic molecules such as proteins, saponins and pectins, which aggregate in the presence of multivalent cations. In addition, the alkaline conditions convert the simple sugars, glucose and fructose, along with the amino acid glutamine, to chemically stable carboxylic acids. Left untreated, these sugars and amines would eventually frustrate crystallization of the sucrose.\n\nNext, carbon dioxide is bubbled through the alkaline sugar solution, precipitating the lime as calcium carbonate (chalk). The chalk particles entrap some impurities and absorb others. A recycling process builds up the size of chalk particles and a natural flocculation occurs where the heavy particles settle out in tanks (clarifiers). A final addition of more carbon dioxide precipitates more calcium from solution; this is filtered off, leaving a cleaner, golden light-brown sugar solution called thin juice.\n\nBefore entering the next stage, the thin juice may receive soda ash to modify the pH and sulphitation with a sulfur-based compound to reduce colour formation due to decomposition of monosaccharides under heat.\n\nThe thin juice is concentrated via multiple-effect evaporation to make a thick juice, roughly 60% sucrose by weight and similar in appearance to pancake syrup. Thick juice can be stored in tanks for later processing, reducing the load on the crystallization plant.\n\nThick juice is fed to the crystallizers. Recycled sugar is dissolved into it, and the resulting syrup is called mother liquor. The liquor is concentrated further by boiling under a vacuum in large vessels (the so-called vacuum pans) and seeded with fine sugar crystals. These crystals grow as sugar from the mother liquor forms around them. The resulting sugar crystal and syrup mix is called a \"massecuite\", from \"cooked mass\" in French. The massecuite is passed to a centrifuge, where the High Green syrup is removed from the massecuite by centrifugal force. After a predetermined\ntime, water is then sprayed into the centrifuge via a spray bar to wash the sugar crystals which produces Low Green syrup. The centrifuge then spins at very high speed to partially dry the crystals the machine then slows down and a plough shaped arm is deployed which ploughs out the sugar from the sides of the centrifuge from the top to the bottom onto conveying plant underneath where it is transported into a rotating granulator where it is dried using warm air.\n\nThe high green syrup is fed to a raw sugar vacuum pan from which a second batch of sugar is produced. This sugar (\"raw\") is of lower quality with more colour and impurities, and is the main source of the sugar dissolved again into the mother liquor. The syrup from the raw (Low green syrup) is boiled for a long time in AP Pans and sent to slowly flow around a series of about eight crystallisers. From this, a very low-quality sugar crystal is produced (known in some systems as \"AP sugar\") that is also redissolved. The syrup separated is molasses, which still contains sugar, but contains too much impurity to undergo further processing economically. The molasses is stored on site and is added to dried beet pulp to make animal feed. Some is also sold in bulk tankers.\n\nActual procedures may vary from the above description, with different recycling and crystallisation processes.\n\nIn a number of countries, notably the Czech Republic and Slovakia, beet sugar is used to make a rum-like distilled spirit called \"Tuzemak\". On the Åland Islands, a similar drink is made under the brand name \"Kobba Libre\". In some European countries, especially in the Czech Republic and Germany, beet sugar is also used to make rectified spirit and vodka.\n\nAn unrefined sugary syrup is produced directly from the sugar beet. This thick, dark syrup is produced by cooking shredded sugar beet for several hours, then pressing the resulting mash and concentrating the juice produced until it has a consistency similar to that of honey and in the Czech Republic, beet sugar is used to make a rum-like distilled spirit all Czechs know as their rum, an alcoholic beverage called Tuzemák, formerly called Tuzemský rum (English: domestic rum).\n\n\nAn unrefined sugary syrup can be produced directly from sugar beet. This thick, dark syrup is produced by cooking shredded sugar beet for several hours, then pressing the resulting mash and concentrating the juice produced until it has a consistency similar to that of honey. No other ingredients are used. In Germany, particularly the Rhineland area, this sugar beet syrup (called \"Zuckerrüben-Sirup\" or \"Zapp\" in German) is used as a spread for sandwiches, as well as for sweetening sauces, cakes and desserts.\n\nCommercially, if the syrup has a dextrose equivalency (DE) above 30, the product has to be hydrolyzed and converted to a high-fructose syrup, much like high-fructose corn syrup, or isoglucose syrup in the EU.\n\nMany road authorities in North America use desugared beet molasses as de-icing or anti-icing products in winter control operations. The molasses can be used directly, combined with liquid chlorides and applied to road surfaces, or used to treat the salt spread on roads. Molasses can be more advantageous than road salt alone because it reduces corrosion and lowers the freezing point of the salt-brine mix, so the de-icers remain effective at lower temperatures. The addition of the liquid to rock salt has the additional benefits that it reduces the bounce and scatter of the rock salt, keeping it where it is needed, and reduces the activation time of the salt to begin the melting process.\n\nBetaine can be isolated from the byproducts of sugar beet processing. Production is chiefly through chromatographic separation, using techniques such as the \"simulated moving bed\".\n\nUridine can be isolated from sugar beet.\n\nBP and Associated British Foods plan to use agricultural surpluses of sugar beet to produce biobutanol in East Anglia in the United Kingdom.\n\nThe feedstock-to-yield ratio for sugarbeet is 56:9. Therefore, it takes 6.22 kg of sugar beet to produce 1 kg of ethanol (approximately 1.27 l at room temperature).\n\nSugar beets are an important part of a crop rotation cycle.\n\nSugar beet plants are susceptible to \"Rhizomania\" (\"root madness\"), which turns the bulbous tap root into many small roots, making the crop economically unprocessable. Strict controls are enforced in European countries to prevent the spread, but it is already endemic in some areas. It is also susceptible to the beet leaf curl virus, which causes crinkling and stunting of the leaves.\n\nContinual research looks for varieties with resistance, as well as increased sugar yield. Sugar beet breeding research in the United States is most prominently conducted at various USDA Agricultural Research Stations, including one in Fort Collins, Colorado, headed by Linda Hanson and Leonard Panella; one in Fargo, North Dakota, headed by John Wieland; and one at Michigan State University in East Lansing, Michigan, headed by J. Mitchell McGrath.\n\nOther economically important members of the Chenopodioideae subfamily:\n\nIn the United States, genetically modified sugar beets, engineered for resistance to glyphosate, a herbicide marketed as Roundup, were developed by Monsanto as a genetically modified crop. In 2005, the US Department of Agriculture-Animal and Plant Health Inspection Service (USDA-APHIS) deregulated glyphosate-resistant sugar beets after it conducted an environmental assessment and determined glyphosate-resistant sugar beets were highly unlikely to become a plant pest. Sugar from glyphosate-resistant sugar beets has been approved for human and animal consumption in multiple countries, but commercial production of biotech beets has been approved only in the United States and Canada. Studies have concluded the sugar from glyphosate-resistant sugar beets has the same nutritional value as sugar from conventional sugar beets. After deregulation in 2005, glyphosate-resistant sugar beets were extensively adopted in the United States. About 95% of sugar beet acres in the US were planted with glyphosate-resistant seed in 2011.\n\nWeeds may be chemically controlled using glyphosate without harming the crop. After planting sugar beet seed, weeds emerge in fields and growers apply glyphosate to control them. Glyphosate is commonly used in field crops because it controls a broad spectrum of weed species and has a low toxicity. A study from the UK suggests yields of genetically modified beet were greater than conventional, while another from the North Dakota State University extension service found lower yields. The introduction of glyphosate-resistant sugar beets may contribute to the growing number of glyphosate-resistant weeds, so Monsanto has developed a program to encourage growers to use different herbicide modes of action to control their weeds.\n\nIn 2008, the Center for Food Safety, the Sierra Club, the Organic Seed Alliance and High Mowing Seeds filed a lawsuit against USDA-APHIS regarding their decision to deregulate glyphosate-resistant sugar beets in 2005. The organizations expressed concerns regarding glyphosate-resistant sugar beets' ability to potentially cross-pollinate with conventional sugar beets. U.S. District Judge Jeffrey S. White, US District Court for the Northern District of California, revoked the deregulation of glyphosate-resistant sugar beets and declared it unlawful for growers to plant glyphosate-resistant sugar beets in the spring of 2011. Believing a sugar shortage would occur USDA-APHIS developed three options in the environmental assessment to address the concerns of environmentalists. In 2011, a federal appeals court for the Northern district of California in San Francisco overturned the ruling. In July 2012, after completing an environmental impact assessment and a plant pest risk assessment the USDA deregulated Monsanto's Roundup Ready sugar beets.\n\nThe sugar beet genome has been sequenced and a reference genome sequence has been generated. The genome size of the sugar beet is approximately 731 Megabases, and sugar beet DNA is packaged in 18 chromosomes (2n=2x=18).\n\nCrop wild beet populations (\"B. vulgaris\" ssp. \"maritima\") have been sequenced as well, allowing for identification of the resistance gene \"Rz2\" in the wild progenitor. \"Rz2\" confers resistance to rhizomania, commonly known as the sugar beet root madness disease.\n\n"}
{"id": "2324639", "url": "https://en.wikipedia.org/wiki?curid=2324639", "title": "Syngenta", "text": "Syngenta\n\nSyngenta AG is a Swiss-based global company that produces agrochemicals and seeds. As a biotechnology company, it conducts genomic research. It was formed in 2000 by the merger of Novartis Agribusiness and Zeneca Agrochemicals. Syngenta is the world’s largest crop chemical producer it ranked third in seeds and biotechnology sales.\nSales in 2015 were approximately US$13.4 billion, over half of which were in emerging markets. It is owned by ChemChina, a Chinese state-owned enterprise.\n\nBased in Basel, Switzerland, Syngenta was formed in 2000 by the merger of Novartis Agribusiness and Zeneca Agrochemicals. Its roots are considerably older.\n\nNovartis was formed of the 1996 merger of the three Swiss companies: Geigy, which has roots back to 1758; Sandoz Laboratories which was founded in 1876; and Ciba, founded in 1884. Ciba and Geigy had merged in 1971 and had concentrated mainly on crop protection in its agro division, Sandoz more on seeds.\nZeneca Agrochemicals was part of AstraZeneca, and formerly of Imperial Chemical Industries. ICI was formed in the UK in 1926. Two years later, work began at the Agricultural Research Station at Jealotts Hill near Bracknell.\n\nIn 2004, Syngenta Seeds purchased Garst, the North American corn and soybean business of Advanta, as well as Golden Harvest Seeds. On 5 December 2004, the European Union ended a six-year moratorium when it approved imports of two varieties of genetically modified corn sold by Monsanto and its Swiss rival, Syngenta.\n\nIn 2005, Syngenta opposed a Swiss ban on genetically engineered organisms. On 28 November 2005, Switzerland enacted a five-year ban on the farming of genetically modified crops, underscoring the problems facing the European Commission and biotech companies like Syngenta, Bayer and Monsanto as they try to overcome consumer doubts about safety.\n\nIn 2014, Monsanto sought to acquire Syngenta for a reported $40 billion, but Syngenta rejected the offer. Since April 2015 Monsanto and Syngenta had been working with their investment banks Morgan Stanley and Goldman Sachs respectively on a deal. The U.S. Treasury tried to stop the deal for tax inversion. Syngenta's Board of Directors rejected an even better offer by Monsanto during August 2015, and Monsanto withdrew from the negotiations on 26 August.\n\nAccording to the Swiss business weekly, , many Syngenta shareholders were enraged, both by the company's refusal to enter into takeover negotiations with Monsanto, and with the subsequent offer withdrawal. This resulted in a group of investors responding by creating the \"Alliance of Critical Syngenta Shareholders\", which urged the Board to \"evaluate all options for value creation without prejudice\".\n\nIn a recent interview, Chairman Michel Demaré was asked whether Syngenta could remain independent. He responded, \"If you have the patience to wait for cycles to materialize, then it would be possible. But in these circumstances, where our shareholders have a kind of a benchmark share price, what they think this company is worth, it is very difficult to say that we can deliver this in the next twelve months.\" He thereby acknowledged that the company needs to be sold, especially given industry consolidation, which is creating larger competitors.\n\nThe failed Monsanto buyout caused Syngenta shares to increase by nearly 40%. In February 2016, ChemChina, a Chinese state-owned enterprise, offered to purchase Syngenta for $43 billion (480 Swiss francs per share), a deal which the company \"unanimously recommended to shareholders”. In April 2017, the Federal Trade Commission, the Committee on Foreign Investment in the United States, and the European Commissioner for Competition approved of the acquisition, allowing the largest foreign takeover in Chinese history to proceed. To secure approval, ChemChina agreed to divest from pesticide production of paraquat, abamectin, and chlorothalonil. As an additional condition for the acquisition, 67 percent of the shareholders of Syngenta had to offer their shares to ChemChina. According to a press release, over 80 percent of shareholders agreed to the takeover by May 4, 2017. Therefore, the transaction is planned to close on June 7, 2017.\n\nThe following is an illustration of the company's mergers, acquisitions, spin-offs and historical predecessors:\n\nSyngenta has eight primary product lines which it develops, markets and sells worldwide;\nIts five product lines for pesticides are selective herbicides, non-selective herbicides, fungicides, insecticides and seed care. \nThree product lines for seed products include corn and soya, other field crops and vegetables. In 2014, sales from crop protection products accounted for US $11.381 billion, i.e. 75% of total sales. Field crop seeds include both hybrid seeds and genetically engineered seeds, some of which enter the food chain and become part of genetically modified food. According to Syngenta, in the US their \"proprietary triple stack corn seeds expanded to represent around 25 percent of units sold.\" In 2010, the US EPA approved insecticidal trait stacks including Syngenta's AGRISURE VIPTERA™ gene, which offers resistance to certain corn pests. Syngenta cross-licenses its proprietary genes with Dow AgroSciences and thus is able to include Dow's Herculex I and Herculex RW insect resistance traits in its seeds. It sells a VMAX soybean that is resistant to glyphosate herbicide.\n\nKey Syngenta brands include Actara (Thiamethoxam), Agrisure (corn with Viptera trait), Alto (Cyproconazole), Amistar (azoxystrobin), Avicta, Axial, Bicep II, Bravo, Callisto, Celest, Cruiser (TMX, Thiamethoxam), Dividend, Dual, Durivo, Elatus, Fusilade, Force, Golden Harvest, Gramoxone, Hilleshoeg, Karate, Northrup-King (NK), Proclaim, Revus, Ridomil, Rogers, Score, Seguris, S&G, Tilt, Topik, Touchdown, Vertimec and Vibrance.\n\nAtrazine is an herbicide of the triazine class. Atrazine is used to prevent pre- and post-emergence broadleaf weeds in crops such as maize (corn) and sugarcane and on turf, such as golf courses and residential lawns.\n\nIt is one of the most widely used herbicides in US and Australian agriculture. Atrazine has been banned in the European Union. There has been controversy over atrazine's effects on amphibians, but the EPA has concluded \"that atrazine does not adversely affect amphibian gonadal development\".\n\nThe European Commission decided to suspend use of the company's insecticide Cruiser (thiamethoxam, TMX) on crops pollinated by bees.\nSyngenta together with Bayer is challenging this ban in court.\n\nSyngenta's predecessor, Ciba-Geigy, introduced the insecticide Galecron chlordimeform in 1966, and it was removed from the market in 1988. In 1976, Ciba-Geigy told regulatory authorities that it was temporarily withdrawing chlordimeform because ongoing long-term toxicology studies - particularly studies to determine if long-term exposure could cause cancer - showed that it was causing cancer, and that it has already started to monitor its workers' exposure and had found chlordimeform and its metabolites in the urine of its workers.\nCiba-Geigy then applied for, and was granted, permission to market Galecron at lower doses for use only on cotton. However, as further long term monitoring data was obtained, regulators banned chlordimeform in 1988. In a 1995 class action in the US, Ciba-Geigy agreed to cover costs for employee health monitoring and treatment. In 2005, Syngenta reported that employee health monitoring was continuing at the company's Monthey, Switzerland site.\n\nLike many agriculture companies, Syngenta also works in the biofuel space. In 2011, it announced the corn trait ENOGEN to reduce substantially the consumption of water and energy versus conventional corn. Several ethanol producers plan to process such improved corn. For example, Syngenta has signed a commercial agreement with Three Rivers Energy, LLC of Coshocton, Ohio, US to use grain featuring Enogen trait technology following the 2014 corn harvest.\n\nIn 2007, Queensland University in Australia contracted with Syngenta to research different inputs for biofuels as a renewable energy source.\n\nSyngenta is led by CEO J. Erik Fyrwald. The other Directors are Vinita Bali, Stefan Borgas, Gunnar Brock, David Lawrence, Eleni Gabre-Madhin, Eveline Saupper, Jacques Vincent, and Jürg Witmer. Syngenta is listed on both the Swiss stock exchange and in New York.\nSyngenta employs over 28,000 people in over 90 countries.\n\nIn 2001, the United States Patent and Trademark Office ruled in favor of Syngenta which had filed a suit against Bayer for patent infringement on a class of neonicotinoid insecticides. The following year Syngenta filed suits against Monsanto and other companies claiming infringement of its U.S. biotechnology patents covering genetically modified corn and cotton. In 2004, it again filed a suit against Monsanto, claiming antitrust violations related to the U.S. biotech corn seed market, and Monsanto countersued. Monsanto and Syngenta settled all litigation in 2008.\n\nSyngenta was defendant in a class action lawsuit by the city of Greenville, Illinois concerning the adverse effects of atrazine in human water supplies. The suit was settled for $105 million in May 2012. A similar case involving six states has been in federal court since 2010.\n\nIn the US, Syngenta is facing lawsuits from farmers and shipping companies regarding Viptera genetically modified corn. The plaintiffs in nearly 30 states contend that Syngenta's introduction of Viptera drove down US grain market prices, leading to financial harm, and that Syngenta acted irresponsibly by doing too little to enable shipping companies to export the grain to approved ports. Before Viptera's 2010 introduction Syngenta secured all US and NCGA-recommended export approvals, but none from China. China had imported little to no US grain prior to 2010, and at the time was not considered a major partner, but it became a major partner in 2010, when it dramatically increased US grain imports. For three years, China imported U.S. Viptera grain without formal approval. In November 2013, Chinese officials destroyed a U.S. grain shipment containing Viptera grain, started rejecting all US shipments with the GM grain, but continued to accept it from all countries other than the US. That same year, US corn market prices dropped $4 per bushel, causing over $2.9B in losses, with just over half of that loss occurring prior to China's November rejection. China later approved the GM corn in 2014 but US corn grain market prices have not rebounded. Syngenta lost the first lawsuit to reach trial, in Kansas on June 23, 2017, and was ordered to pay the farmers $217 million. However, Syngenta has stated it would appeal the verdict.\n\nOn 21 October 2007, a Brazilian peasant organization, the Landless Workers' Movement (), led a group of landless farmers in an occupation of one of the company's seed research farms, in protest against genetically-engineered (\"genetically modified\") vegetables and in hopes of obtaining land for landless families to cultivate. After the occupation had begun, a team from NF Security arrived in a minibus and a fight with gunfire ensued. A protestor and a security guard were killed, and some protesters and security guards were wounded.\n\nThe Brazilian police investigation completed in November 2007 blamed the confrontation and death of the protestor on nine employees and the owner of NF Security; the leader of MST was blamed for trespassing. The inquiry found that the protester was fatally shot in the abdomen and in the leg. The security guard was shot in the head. Eight others were injured, five of them landless.\n\nThe Civil Court of Cascavel granted an order for the repossession of the site on 20 December 2007 and on 12 June 2008, the remaining MST members left the Santa Teresa site they had been occupying. On 14 October 2008, Syngenta donated the 123-hectare station to the Agronomy Institute of Paraná (IAPAR) for research into biodiversity, recovery of degraded areas and agriculture production systems, as well as environmental education programs.\n\nCourt case\n\nIn November 2015, Judge Pedro Ivo Moreiro, of the 1st Civil Court of Cascavel, ruled that Syngenta must pay compensation to the family of Valmir Mota de Oliveira (\"Keno\"), who was killed in the attack, and to Isabel Nascimento dos Santos who was injured. In his sentence the judge stated that \"to refer to what happened as a confrontation is to close one’s eyes to reality, since […] there is no doubt that, in truth, it was a massacre disguised as repossession of property\". The version of events put forward by Syngenta was rejected by the Court. In May 2010 Syngenta was condemned by the IV Permanent People's Tribunal for human rights violations in Brazil.\n\nThere has been a long running conflict between Syngenta and University of California at Berkeley biologist Tyrone Hayes.\n\nAccording to an article in the 10 February 2014, issue of \"The New Yorker,\" Syngenta's public-relations team took steps to discredit Hayes, whose research is purported to suggest that the Syngenta-produced chemical atrazine was responsible for abnormal development of reproductive organs in frogs. The article states that the company paid third-party critics to write articles discrediting Hayes's work, planned to have his wife investigated, and planted hostile audience members at scientific talks given by Hayes.\n\nDuring a 21 February 2014 interview conducted on \"Democracy Now\", Hayes reiterated the claims. After the interview aired, Syngenta denied targeting Hayes or making any threats, calling those statements \"uncorroborated and intentionally damaging\" and demanding a retraction and public apology from Hayes and \"Democracy Now\".\n\nIn 2010 Syngenta forwarded an ethics complaint to the University of California Berkeley, complaining that Hayes had been sending sexually explicit and harassing e-mails to Syngenta scientists. Legal counsel from the university responded that Hayes had acknowledged sending letters having \"unprofessional and offensive\" content, and that he had agreed not to use similar language in future communications.\n\nThe issue has been described as \"one of the weirdest feuds in the history of science”, by Dashka Slater in her 2012 profile of Hayes in Mother Jones magazine.\n\nSyngenta is in the transparency register of the European Union as a registered lobbyist. For 2017, it declared a €1,500,000 - €1,750,000 expenditure of lobbying in European institutions.\n\nSyngenta's contributions to US federal candidates, parties, and outside groups totaled $140,822 during the 2018 election cycle, ranking it 20th on the list of companies in its sector. Its lobbying expenditures in the US during 2018 were $770,000, ranking it 7th in its sector.\n\nThe objectives and goals of the Syngenta Foundation are \"to work with rural communities in the Semi-arid regions of the world and improve their livelihoods.\" This non-profit organization supports sustainable food security projects in a number of countries.\nThe Syngenta Foundation addressed the World Food Day Symposium in 2005 as an output of the Millennium Ecosystem Report.\n\nIn 2007, Syngenta's Canadian division was named one of Canada's Top 100 Employers, as published in Maclean's magazine, one of only a handful of agribusiness firms to receive this honour.\nIn October 2008, Syngenta Crop Protection Canada, Inc. was recognized as one of Waterloo Area's Top Employers, as announced in the Waterloo Region Record, Guelph Mercury and Cambridge Times. In 2011, Syngenta was named among the top 10 employers in biotechnology by Science magazine. The 2011 Dow Jones Sustainability Index named Syngenta one of the best performing chemical companies worldwide. Syngenta was one of five chemical companies in the World and Europe indices based on economic, social and environmental performance.\n\n"}
{"id": "29324165", "url": "https://en.wikipedia.org/wiki?curid=29324165", "title": "Thermization", "text": "Thermization\n\nThermization, also spelled thermisation, is a method of sanitizing raw milk with low heat. \"Thermization is a generic description of a range of subpasteurization heat treatments (57 to 68°C × 10 to 20 s) that markedly reduce the number of spoilage bacteria in milk with minimal heat damage.\" The process is not used on other food products, and is similar to pasteurization but uses lower temperatures, allowing the milk product to retain more of its original taste. In Europe, there is a distinction between cheeses made of thermized milk and raw-milk cheeses. However, the United States' Food and Drug Administration (FDA) places the same regulations on all unpasteurized cheeses. As a result, cheeses from thermized milk must be aged for 60 days or more before being sold in the United States, the same restriction placed on raw-milk cheeses by the FDA.\n\nThermization involves heating milk at temperatures of around for 15 seconds, while pasteurization involves heating milk at for 15 seconds or at for 30 minutes. Thermization is used to extend the keeping quality of raw milk (the length of time that milk is suitable for consumption) when it cannot be immediately used in other products, such as cheese. Thermization can also be used to extend the storage life of fermented milk products by inactivating microorganisms in the product.\n\nThermization inactivates psychrotrophic bacteria in milk and allows the milk to be stored below for three days, or stored at for seven days. Later, the milk may be given stronger heat treatment to be preserved longer. Cooling thermized milk before reheating is necessary to delay/prevent the outgrowth of bacterial spores. When the milk is first heated, spores can begin to germinate, but their growth can be halted or delayed when the milk is refrigerated, depending on the microorganisms' growth requirements. Germinated spores are sensitive to subsequent heating, however since germination is not a homogeneous process, not all spores will germinate or be inactivated by subsequent heating.\n"}
{"id": "46587931", "url": "https://en.wikipedia.org/wiki?curid=46587931", "title": "Valenbisi", "text": "Valenbisi\n\nValenbisi is the name of a bicycle sharing system in Valencia inaugurated on June 21, 2010. It is similar to the Vélo'v service in Lyon or Vélib' in Paris, and using the same bicycles and stations as used in Dublin, Vienna, and Brussels. Its purpose is to cover the small and medium daily routes within the city in a climate-friendly way, eliminating the pollution, roadway noise, and traffic congestion that motor vehicles create.\nThe city council and JCDecaux manage and maintain the system. To use it, users must acquire a yearly membership. Currently the network consists of more than 275 stations to lend and return over 2750 bicycles distributed throughout the system. The stations are situated through most of the flat areas of the city with a distance of around 300 to 400 metres between each one, with many situated next to public transport stops to allow for intermodal use. Metro Stations usually have signs pointing to the locations of nearest Valenbisi stations.\nThe bikes can be lent from, and returned to, any station in the system, making it suitable for one-way travel. Each station has between 15 and 30 parking slots to fix and lock bicycles, but in highly transited areas many stations may be close together.\n\nUse of the system is based on membership, and users can subscribe online or by visiting a service office. The Valenbisi member cards are only sent to addresses in Valencia in an attempt to prevent tourists from using the system.\n\nBike stations have generally replaced on-street car or motorcycle parking spaces, though others were placed on large pedestrian areas. Each station includes a long series of docks for bikes, with a computerized pylon at one end for completing transactions.\n\nThe system is paid for mostly by local car drivers with an on-street parking control system, distributed throughout much of the densely populated inner city. This money, about €1 million annually, is paid to the system operator. The yearly user fee is €29,12.\n\n"}
{"id": "6206730", "url": "https://en.wikipedia.org/wiki?curid=6206730", "title": "Vehicular ad hoc network", "text": "Vehicular ad hoc network\n\nVehicular ad hoc networks (VANETs) are created by applying the principles of mobile ad hoc networks (MANETs) – the spontaneous creation of a wireless network for Vehicle-to-vehicle (V2V) data exchange – to the domain of vehicles. VANETs were first mentioned and introduced in 2001 under \"car-to-car ad hoc mobile communication and networking\" applications, where networks can be formed and information can be relayed among cars. It was shown that vehicle-to-vehicle and vehicle-to-roadside communications architectures will co-exist in VANETs to provide road safety, navigation, and other roadside services. \nVANETs are a key part of the intelligent transportation systems (ITS) framework. Sometimes, VANETs are referred as Intelligent Transportation Networks\n\nWhile, in the early 2000s, VANETs were seen as a mere one-to-one application of MANET principles, they have since then developed into a field of research in their own right. By 2015, the term VANET became mostly synonymous with the more generic term inter-vehicle communication (IVC), although the focus remains on the aspect of spontaneous networking, much less on the use of infrastructure like Road Side Units (RSUs) or cellular networks.\n\nVANETs support a wide range of applications – from simple one hop information dissemination of, e.g., cooperative awareness messages (CAMs) to multi-hop dissemination of messages over vast distances.\nMost of the concerns of interest to mobile ad hoc networks (MANETs) are of interest in VANETs, but the details differ. Rather than moving at random, vehicles tend to move in an organized fashion. The interactions with roadside equipment can likewise be characterized fairly accurately. And finally, most vehicles are restricted in their range of motion, for example by being constrained to follow a paved highway.\n\nExample applications of VANETs are:\n\nVANETs can use any wireless networking technology as their basis. The most prominent are short range radio technologies like WLAN (either standard Wi-Fi or ZigBee). In addition, cellular technologies or LTE can be used for VANETs. The latest technology for this wireless networking is visible light communication [VLC] (Infrared transmission and reception).\n\nPrior to the implementation of VANETs on the roads, realistic computer simulations of VANETs using a combination of Urban Mobility simulation and Network simulation are necessary. Typically open source simulator like SUMO (which handles road traffic simulation) is combined with a network simulator like TETCOS NetSim, or NS-2 to study the performance of VANETs.\n\nMajor standardization of VANET protocol stacks is taking place in the U.S., in Europe, and in Japan, corresponding to their dominance in the automotive industry.\n\nIn the U.S., the IEEE 1609 WAVE Wireless Access in Vehicular Environments protocol stack builds on IEEE 802.11p WLAN operating on seven reserved channels in the 5.9 GHz frequency band.\nThe WAVE protocol stack is designed to provide multi-channel operation (even for vehicles equipped with only a single radio), security, and lightweight application layer protocols.\nWithin the IEEE Communications Society, there is a Technical Subcommittee on Vehicular Networks & Telematics Applications (VNTA). The charter of this committee is to actively promote technical activities in the field of vehicular networks, V2V, V2R and V2I communications, standards, communications-enabled road and vehicle safety, real-time traffic monitoring, intersection management technologies, future telematics applications, and ITS-based services.\n\nIn the US, the systems will use a region of the 5.9 GHz band set aside by the United States Congress, the unlicensed frequency also used by Wi-Fi. The US V2V standard, commonly known as WAVE (\"Wireless Access for Vehicular Environments\"), builds upon the lower-level IEEE 802.11p standard, as early as 2004.\n\nThe European Commission Decision 2008/671/EC harmonises the use of the 5 875-5 905 MHz frequency band for transport safety ITS applications. In Europe V2V is standardised as ETSI ITS, a standard also based on IEEE 802.11p. C-ITS, cooperative ITS, is also a term used in EU policy making, closely linked to ITS-G5 and V2V.\n\nV2V is also known as VANET (vehicular ad hoc network). It is a variation of MANET (Mobile ad hoc network), with the emphasis being now the node is the vehicle. In 2001, it was mentioned in a publication that ad hoc networks can be formed by cars and such networks can help overcome blind spots, avoid accidents, etc. The infrastructure also participates in such systems, then referred to as V2X (vehicle-to-everything). Over the years, there have been considerable research and projects in this area, applying VANETs for a variety of applications, ranging from safety to\nnavigation and law enforcement.\n\nIn 1999 the US Federal Communications Commission (FCC) allocated 75 MHz in the spectrum of 5.850-5.925 GHz for intelligent transport systems.\n\nAs of 2016, V2V is under threat from cable television and other tech firms that want to take away a big chunk of the radio spectrum currently reserved for it and use those frequencies for high-speed internet service. V2V's current share of spectrum was set aside by the government in 1999. The auto industry is trying to retain all it can saying that it desperately needs the spectrum for V2V. The Federal Communications Commission has taken the side of the tech companies with the National Traffic Safety Board supporting the position of the auto industry. Internet service providers who want the spectrum claim that self-driving cars will make extensive use of V2V unnecessary. The auto industry said it is willing to share the spectrum if V2V service is not slowed or disrupted; the FCC plans to test several sharing schemes.\n\nResearch in VANETs started as early as 2000, in universities and research labs, having evolved from researchers working on wireless ad hoc networks. Many have worked on media access protocols, routing, warning message dissemination, and VANET application scenarios. V2V is currently in active development by General Motors, which demonstrated the system in 2006 using Cadillac vehicles. Other automakers working on V2V include Toyota, BMW, Daimler, Honda, Audi, Volvo and the Car-to-Car communication consortium.\n\nSince then the United States Department of Transportation (USDOT) has been working with a range of stakeholders on V2X. In 2012 a pre-deployment project was implemented in Ann Arbor, Michigan. 2800 vehicles covering cars, motorcycles, buses and HGV of different brands took part using equipment by different manufacturers. The US National Highway Traffic Safety Administration (NHTSA) saw this model deployment as proof that road safety could be improved and that WAVE standard technology was interoperable. In August 2014 NHTSA published a report arguing vehicle-to-vehicle technology was technically proven as ready for deployment. In April 2014 it was reported that U.S. regulators were close to approving V2V standards for the U.S. market. On 20 August 2014 the NHTSA published an Advance Notice of Proposed Rulemaking (ANPRM) in the Federal Register, arguing that the safety benefits of V2X communication could only be achieved, if a significant part of the vehicles fleet was equipped. Because of the lacking immediate benefit for early adopters the NHTSA proposed a mandatory introduction. On 25 June 2015 the US House of Representatives held a hearing on the matter, where again the NHTSA, as well as other stakeholders argued the case for V2X.\n\nIn the EU the ITS Directive 2010/40/EU was adopted in 2010. It aims to assure that ITS applications are interoperable and can operate across national borders, it defines priority areas for secondary legislation, which cover V2X and requires technologies to be mature. In 2014 the European Commission's industry stakeholder \"C-ITS Deployment Platform\" started working on a regulatory framework for V2X in the EU. It identified key approaches to an EU-wide V2X security Public Key infrastructure (PKI) and data protection, as well as facilitating a mitigation standard to prevent radio interference between ITS-G5 based V2X and CEN DSRC-based road charging systems. The European Commission recognised ITS-G5 as the initial communication technology in its 5G Action Plan and the accompanying explanatory document, to form a communication environment consisting of ITS-G5 and cellular communication as envisioned by EU Member States. Various pre-deployment projects exist at EU or EU Member State level, such as SCOOP@F, the Testfeld Telematik, the digital testbed Autobahn, the Rotterdam-Vienna ITS Corridor, Nordic Way, COMPASS4D or C-ROADS. Further projects are under preparation.\n\n\n\n"}
{"id": "1952367", "url": "https://en.wikipedia.org/wiki?curid=1952367", "title": "Wedge", "text": "Wedge\n\nA wedge is a triangular shaped tool, and is a portable inclined plane, and one of the six classical simple machines. It can be used to separate two objects or portions of an object, lift up an object, or hold an object in place. It functions by converting a force applied to its blunt end into forces perpendicular (normal) to its inclined surfaces. The mechanical advantage of a wedge is given by the ratio of the length of its slope to its width. Although a short wedge with a wide angle may do a job faster, it requires more force than a long wedge with a narrow angle.\n\nPerhaps the first example of a wedge is the hand axe, also see biface and Olorgesailie. Wedges have been around for thousands of years, they were first made of simple stone. A hand axe is made by chipping stone, generally flint, to form a bifacial edge, or wedge. A wedge is a simple machine that transforms lateral force and movement of the tool into a transverse splitting force and movement of the workpiece. The available power is limited by the effort of the person using the tool, but because power is the product of force and movement, the wedge amplifies the force by reducing the movement. This amplification, or mechanical advantage is the ratio of the input speed to output speed. For a wedge this is given by 1/tanα, where α is the tip angle. The faces of a wedge are modeled as straight lines to form a sliding or prismatic joint.\n\nThe origin of the wedge is not known. In ancient Egyptian quarries, bronze wedges were used to break away blocks of stone used in construction. Wooden wedges that swelled after being saturated with water, were also used. Some indigenous peoples of the Americas used antler wedges for splitting and working wood to make canoes, dwellings and other objects.\n\nWedges are used to lift heavy objects, separating them from the surface upon which they rest.\n\nConsider a block that is to be lifted by a wedge. As the wedge slides under the block, the block slides up the sloped side of a wedge. This lifts the weight \"F\" of the block. The horizontal force \"F\" needed to lift the block is obtained by considering the velocity of the wedge \"v\" and the velocity of the block \"v\". If we assume the wedge does not dissipate or store energy, then the power into the wedge equals the power out.\n\nor\nThe velocity of the block is related to the velocity of the wedge by the slope of the side of the wedge. If the angle of the wedge is \"α\" then\nwhich means that the mechanical advantage\nThus, the smaller the angle \"α\" the greater the ratio of the lifting force to the applied force on the wedge. This is the mechanical advantage of the wedge. This formula for mechanical advantage applies to cutting edges and splitting operations as well as to lifting.\n\nThey can also be used to separate objects, such as blocks of cut stone. Splitting mauls and splitting wedges are used to split wood along the grain. A narrow wedge with a relatively long taper used to finely adjust the distance between objects is called a shim, and is commonly used in carpentry.\n\nThe tips of forks and nails are also wedges, as they split and separate the material into which they are pushed or driven; the shafts may then hold fast due to friction.\n\nThe blade is a compound inclined plane, consisting of two inclined planes placed so that the planes meet at one edge. When the edge where the two planes meet is pushed into a solid or fluid substance it overcomes the resistance of materials to separate by transferring the force exerted against the material into two opposing forces normal to the faces of the blade.\n\nThe blade's first known use by humans was the sharp edge of a flint stone that was used to cleave or split animal tissue, e.g. cutting meat. The use of iron or other metals led to the development of knives for those kinds of tasks. The blade of the knife allowed humans to cut meat, fibers, and other plant and animal materials with much less force than it would take to tear them apart by simply pulling with their hands. Other examples are plows, which separate soil particles, scissors which separate fabric, axes which separate wood fibers, and chisels and planes which separate wood.\n\nWedges, saws and chisels can separate thick and hard materials, such as wood, solid stone and hard metals and they do so with much less force, waste of material, and with more precision, than crushing, which is the application of the same force over a wider area of the material to be separated.\n\nOther examples of wedges are found in drill bits, which produce circular holes in solids. The two edges of a drill bit are sharpened, at opposing angles, into a point and that edge is wound around the shaft of the drill bit. When the drill bit spins on its axis of rotation, the wedges are forced into the material to be separated. The resulting cut in the material is in the direction of rotation of the drill bit while the helical shape of a bit allows the removal of the cut material.\n\nWedges can also be used to hold objects in place, such as engine parts (poppet valves), bicycle parts (stems and eccentric bottom brackets), and doors. A wedge-type door stop (door wedge) functions largely because of the friction generated between the bottom of the door and the wedge, and the wedge and the floor (or other surface).\n\nThe mechanical advantage of a wedge can be calculated by dividing the height of the wedge by the wedge's width:\n\nThe more acute, or narrow, the angle of a wedge, the greater the ratio of the length of its slope to its width, and thus the more mechanical advantage it will yield.\n\nHowever, in an elastic material such as wood, friction may bind a narrow wedge more easily than a wide one. This is why the head of a splitting maul has a much wider angle than that of an axe.\n\n"}
{"id": "23082223", "url": "https://en.wikipedia.org/wiki?curid=23082223", "title": "Yuchanyan", "text": "Yuchanyan\n\nYuchanyan is an early Neolithic cave site in Dao County (Daoxian), Hunan, China. The site yielded sherds of ceramic vessels and other artifacts which were dated by analysis of charcoal and bone collagen, giving a date range of 17,500 to 18,300 years old for the pottery. The pottery specimens may be the oldest known examples of pottery.\n\nThe cave yielded fragmentary remains of 2 or more ceramic vessels, in addition to large amounts of ash, a rich animal bone assemblage, cobble and flake artifacts, bone tools, and shell tools. The artifacts indicate that the cave was a Late Paleolithic foragers' camp. Here we report on the radiocarbon ages of the sediments based on analyses of charcoal and bone collagen. The best-preserved charcoal and bone samples were identified by prescreening in the field and laboratory. The dates range from around 21,000 to 13,800 cal BP. The age of the ancient pottery ranges between 18,300 and 15,430 cal BP. Charcoal and bone collagen samples located above and below one of the fragments produced dates of around 18,000. These ceramic potsherds therefore provide some of the earliest evidence for pottery making in China.\n"}
