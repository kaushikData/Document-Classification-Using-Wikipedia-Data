{"id": "45471618", "url": "https://en.wikipedia.org/wiki?curid=45471618", "title": "Acoustic liner", "text": "Acoustic liner\n\nAircraft engines, typically turbofans, use acoustic liners to damp engine noise. Liners are applied on the internal walls of the engine nacelle, both in the intake and by-pass ducts, and use Helmholtz resonance principle for the dissipation of incident acoustic energy.\n\nAn acoustic liner is a sandwich panel made by:\nThe lower half of a liner features dedicated internal slots to allow liquid drainage in order to prevent ice formation or fire hazards. From an acoustic perspective, this implies that the upper acoustic panel is a \"locally reacting liner\", whereas the lower one is a \"non-locally reacting liner\".\n\nAcoustic liners can be distinguished by their internal configuration on the base of the number of honeycomb cell layers:\nPorous layers (e.g. the septum and the face-sheet) can be for example a perforate plate, a wire mesh or a felt-metal. The honeycomb can be of aluminum or glass fiber and the cells' size is selected small enough to ensure an acoustic plane wave in the cell for the whole frequency of interest. The face-sheet and back-skin can be either metallic or in carbon fiber. In order to have a cylindrical barrel, panel parts are structurally joined together leading to a partial loss of the acoustic area known as splice. Current state-of-the-art of acoustic are made without splices and are known as zero-splice liners. For example, the evolution of the splices width on the Airbus family ranges from three splices of about 15 cm for the Airbus A320 to a zero-splice liner for the Airbus A380.\n\nThe acoustic liners' performance can be verified in dedicated experimental test rigs, by means of virtual prototypes or by means of ground tests on full-scale engines. Both these type of tests and simulations allow to determine the acoustic attenuation in the far-field. Moreover, the acoustic performance is linked to the acoustic impedance which can be measured with one of the following techniques:\nAll these methodologies measure the liner's normal impedance. However, only flow duct facilities and the in-situ method allow the measurement of the impedance in presence of the grazing flow which can affect the impedance itself. Moreover, the in-situ method is the only one able to measure the impedance directly on full scale acoustic liners.\n"}
{"id": "41245695", "url": "https://en.wikipedia.org/wiki?curid=41245695", "title": "Amazon Prime Air", "text": "Amazon Prime Air\n\nAmazon Prime Air is a conceptual drone-based delivery system currently in development by Amazon.\n\nOn December 1, 2013, Amazon.com CEO Jeff Bezos revealed plans for Amazon Prime Air in an interview on \"60 Minutes\". Amazon Prime Air is planned to use multirotor Miniature Unmanned Air Vehicle (Miniature UAV) technology to autonomously fly individual packages to customers within 30 minutes of ordering. To qualify for 30 minute delivery, the order must be less than 5 pounds (2.25 kg), must be small enough to fit in the cargo box that the craft will carry, and must have a delivery location within a 10-mile (16 km) radius of a participating Amazon order fulfillment center.\n\nIn the FAA Modernization and Reform Act of 2012, Congress issued the Federal Aviation Administration a deadline of September 30, 2015 to accomplish a \"safe integration of civil unmanned aircraft systems into the national airspace system.\" In August 2016 commercial use of UAV technology was legalized by the United States Congress.\n\nIn March 2015, the FAA granted Amazon permission to begin U.S. testing of a prototype under a waiver to the then regulations. Amazon reported that the vehicle cleared for use was obsolete. In April 2015, the FAA allowed Amazon to begin testing current models. In the interim, Amazon had begun testing at a Canadian site close to the United States border.\n\nCurrent U.S. regulations required drones fly no higher than 400 ft (122 m), no faster than 100 mph (161 km/h), and remain within the pilot's line of sight. Amazon has stated it intends to move towards operating above 200 ft (61 m) and beneath 500 ft (152 m), with 500 ft. Amazon has stated it plans to fly drones weighing an up to 55 lb (25 kg) within a 10 mi (16 km) radius of its warehouses, at speeds of up to 50 mph (80.5 km/h) with packages weighing up to 5 lb (2.26 kg) in tow.\n\nAmazon has patented a beehive-like structure to house delivery drones in cities, allowing Amazon to move from large single-story warehouses that temporarily store packages before they are shipped.\n\nFulfillment centres designed to accommodate drone deliveries and operations within a certain radius, are currently required. This was announced in a video released by Amazon. On December 15, 2016, Amazon began its first publicly available trial of Amazon Prime Air to those within several miles of Amazon's depot in Cambridge, England.\n\nOn December 7, 2016, Amazon successfully delivered a Prime Air parcel to Cambridge, England. Amazon had built a Prime Air fulfillment center in the Cambridge area. Amazon posted a video on their official YouTube channel, on December 14, 2016 of the delivery.\n\nAmazon Prime Air drones will be connected to the internet to allow for flight control management, and communication between drones. It has been reported by Brookings that Amazon's data collection usage from drones has not been disclosed. The Pittsburgh Journal of Technology Law and Policy, have reported proposed data collection from Amazon Prime Air drones as including; automated object detection, GPS surveillance, gigapixel cameras, and enhanced image resolution.\n\nAutonomous vehicles also fall under the Prime Air division's responsibility. There is little news about this potential direction, which would seem to make great sense for Amazon, since the last mile of delivery is the most expensive, and human drivers are the primary cost; but there is little news about this since an article about an Amazon patent for reversible lanes.\n\n"}
{"id": "18506594", "url": "https://en.wikipedia.org/wiki?curid=18506594", "title": "Ann Dowling", "text": "Ann Dowling\n\nDame Ann Patricia Dowling (born 15 July 1952) is a British mechanical engineer who researches combustion, acoustics and vibration, focusing on efficient, low-emission combustion and reduced road vehicle and aircraft noise. Dowling is a Deputy Vice-Chancellor and Professor of Mechanical Engineering, and from 2009 to 2014 she was Head of the Department of Engineering at the University of Cambridge, where she was the first female professor in 1993. She is also the President of the Royal Academy of Engineering, and the Academy's first female president.\n\nDowling was educated at Ursuline Convent School, Westgate, Kent and the University of Cambridge (as a member of Girton College), where after studying mathematics at undergraduate level, and following a summer job at the Royal Aircraft Establishment, she was awarded a PhD degree in 1978. Dowling's doctorate was in aeroacoustics, specifically on the Concorde noise problem.\n\nDowling's research career has been at University of Cambridge starting as a research fellow in 1977 but she has held visiting posts at the Massachusetts Institute of Technology (Jerome C Hunsaker Visiting Professor, 1999) and at the California Institute of Technology (Moore Distinguished Scholar 2001).\nDowling is one of four main panel chairs for the Research Excellence Framework.\n\nOn 3 February 2012, the oil company BP announced that Dowling was to become a non-executive director with immediate effect. She has been a non-executive board member at BIS since February 2014.\n\nDowling was elected a Fellow of the Royal Academy of Engineering (FREng) in 1996 and a Fellow of the Royal Society in 2003. Her FRS nomination reads: .\n\nIn February 2013 Dowling was listed as one of the 100 most powerful women in the United Kingdom by \"Woman's Hour\" on BBC Radio 4. and was the subject of an episode of \"The Life Scientific\" in 2012.\n\nDowling is a Patron of the Women's Engineering Society (WES).\n\nIn January 2014 Dowling was nominated for election as President of the Royal Academy of Engineering. She took up the position in September 2014, and in that role chaired a 2015 review (the 'Dowling Review') of business–university research collaboration, published in July 2015.\n\nShe was appointed a CBE in 2002 and promoted to DBE in 2007. In 2016 she was appointed to the Order of Merit.\n\nThe James Watt International Gold Medal was awarded to her in 2016 by the Institution of Mechanical Engineers \"for her work associated with efficient, low emission combustion; and understanding, modelling and reducing the noise from cars, helicopters and fixed-wing aircraft\"\n\nIn November 2017, Dowling was elected to the Chinese Academy of Engineering alongside 18 foreign individuals including Bill Gates and L. Rafael Reif.\n\n"}
{"id": "48892446", "url": "https://en.wikipedia.org/wiki?curid=48892446", "title": "Automatic bug fixing", "text": "Automatic bug fixing\n\nAutomatic bug-fixing is the automatic repair of software bugs without the intervention of a human programmer. It is also commonly referred to as \"automatic patch generation\", \"automatic bug repair\", or \"automatic program repair\". The typical goal of such techniques is to automatically generate correct patches to eliminate bugs in software programs without causing software regression.\n\nAutomatic bug fixing is made according to a specification of the expected behavior which can be for instance a formal specification or a test suite.\n\nA test-suite – the input/output pairs specify the functionality of the program, possibly captured in assertions can be used as a test oracle to drive the search. This oracle can in fact be divided between the \"bug oracle\" that exposes the faulty behavior, and the \"regression oracle\", which encapsulates the functionality any program repair method must preserve. Note that a test suite is typically incomplete and does not cover all possible cases. Therefore, it is often possible for a validated patch to produce expected outputs for all inputs in the test suite but incorrect outputs for other inputs. The existence of such validated but incorrect patches is a major challenge for generate-and-validate techniques. Recent successful automatic bug-fixing techniques often rely on additional information other than the test suite, such as information learned from previous human patches, to further identify correct patches among validated patches.\n\nAnother way to specify the expected behavior is to use formal specifications Verification against full specifications that specify the whole program behavior including functionalities is less common because such specifications are typically not available in practice and the computation cost of such verification is prohibitive. For specific classes of errors, however, implicit partial specifications are often available. For example, there are targeted bug-fixing techniques validating that the patched program can no longer trigger overflow errors in the same execution path.\n\nGenerate-and-validate approaches compile and test each candidate patch to collect all validated patches that produce expected outputs for all inputs in the test suite. Such a technique typically starts with a test suite of the program, i.e., a set of test cases, at least one of which exposes the bug. An early generate-and-validate bug-fixing systems is GenProg. The effectiveness of generate-and-validate techniques remains controversial, because they typically do not provide patch correctness guarantees. Nevertheless, the reported results of recent state-of-the-art techniques are generally promising. For example, on systematically collected 69 real world bugs in eight large C software programs, the state-of-the-art bug-fixing system Prophet generates correct patches for 18 out of the 69 bugs.\n\nOne way to generate candidate patches is to apply mutation operators on the original program. Mutation operators manipulate the original program, potentially via its abstract syntax tree representation, or a more coarse-grained representation such as operating at the statement-level or block-level. Earlier genetic improvement approaches operate at the statement level and carry out simple delete/replace operations such as deleting an existing statement or replacing an existing statement with another statement in the same source file. Recent approaches use more fine-grained operators at the abstract syntax tree level to generate more diverse set of candidate patches.\n\nMany generate-and-validate techniques rely on the redundancy insight: the code of the patch can be found elsewhere in the application. This idea was introduced in the Genprog system, where two operators, addition and replacement of AST nodes, were based on code taken from elsewhere (i.e. adding an existing AST node). This idea has been validated empirically, with two independent studies that have shown that a significant proportion of commits (3%-17%) are composed of existing code.\n\nUsing fix templates is an alternative way to generate candidate patches. Fix templates are typically predefined program mutation rules for fixing specific classes of bugs. Examples of fix templates include inserting a conditional statement to check whether the value of a variable is null to fix null pointer exception, and changing an integer constant by one to fix off-by-one errors. Fix templates are therefore often adopted by targeted techniques. It is also possible to automatically mine fix templates for generate-and-validate approches.\n\nRepair techniques exist that are based on symbolic execution. For example, Semfix uses symbolic execution to extract a repair constraint. Angelix introduced the concept of angelic forest in order to deal with multiline patches.\nUnder certain assumptions, it is possible to state the repair problem as a synthesis problem.\n\nTargeted automatic bug-fixing techniques generate repairs for specific classes of errors such as null pointer exception integer overflow , buffer overflow , memory leak , etc.. Such techniques often use empirical fix templates to fix bugs in the targeted scope. For example, insert a conditional statement to check whether the value of a variable is null or insert missing memory deallocation statements. Comparing to generate-and-validate techniques, targeted techniques tend to have better bug-fixing accuracy but a much narrowed scope.\n\nMachine learning techniques can improve the effectiveness of automatic bug-fixing systems. One example of such techniques learns from past successful patches from human developers collected from open source repositories in GitHub and SourceForge. It then use the learned information to recognize and prioritize potentially correct patches among all generated candidate patches. Alternatively, patches can be directly mined from existing sources. Example approaches include mining patches from donor applications or from QA web sites.\n\nThere are multiple usages of automatic bug fixing:\n\nIn essence, automatic bug fixing is a search activity, whether deductive-based or heuristic-based. The search space of automatic bug fixing is composed of all edits that can be possibly made to a program. There have been studies to understand the structure of this search space. Qi et al. showed that the original fitness function of Genprog is not better than random search to drive the search. Martinez et al. explored the imbalance between possible repair actions, showing its significant impact on the search. Long et al.'s study indicated that correct patches can be considered as sparse in the search space and that incorrect overfitting patches are vastly more abundant (see also discussion about overfitting below).\n\nAutomatic bug-fixing techniques that rely on a test suite do not provide patch correctness guarantees, because the test suite is incomplete and does not cover all cases. A weak test suite may cause generate-and-validate techniques to produce validated but incorrect patches that have negative effects such as eliminating desirable functionalities, causing memory leaks, and introducing security vulnerabilities. \n\nSometimes, in test-suite based program repair, tools generate patches that pass the test suite, yet are actually incorrect, this is known as the \"overfitting\" problem. \"Overfitting\" in this context refers to the fact that the patch overfits to the test inputs. There are different kinds of overfitting: incomplete fixing means that only some buggy inputs are fixed, regression introduction means some previously working features are broken after the patch (because they were poorly tested). Early prototypes for automatic repair suffered a lot from overfitting: on the Manybugs C benchmark, Qi et al. reported that 104/110 of plausible GenProg patches were overfitting; on the Defects4J Java benchmark, Martinez et al. reported that 73/84 plausible patches as overfitting. In the context of synthesis-based repair, Le et al. obtained more than 80% of overfitting patches. \n\nAnother limitation of generate-and-validate systems is the search space explosion. For a program, there are a large number of statements to change and for each statement there are a large number of possible modifications. State-of-the-art systems address this problem by assuming that a small modification is enough for fixing a bug, resulting in a search space reduction.\n\nThe limitation of approaches based on symbolic analysis is that real world programs are often converted to intractably large formulas especially for modifying statements with side effects.\n\nAutomatic bug-fixing is an active research topic in computer science. There are many implementations of various bug-fixing techniques especially for C and Java programs. Note that most of these implementations are research prototypes for demonstrating their techniques, i.e., it is unclear whether their current implementations are ready for industrial usage or not.\n\nGenProg and ClearView are two earliest generate-and-validate bug-fixing tools published at 2009. The benchmark collected by GenProg authors contains 69 real world defects and it is widely used to evaluate many other bug-fixing tools in C as well. The state-of-the-art tool evaluated on GenProg benchmark is Prophet, generating correct patches for 18 out of 69 defects.\n\n\nis evaluated on 105 cases and reported to generate repairs for 55 cases. A\nlater study shows that only 69 cases out of the 105 evaluated cases correspond\nto bugs and correct patches for only 2 cases out of the 69 cases are generated.\n\n\n\n\n"}
{"id": "12385791", "url": "https://en.wikipedia.org/wiki?curid=12385791", "title": "Bankgiro", "text": "Bankgiro\n\nBankgiro is a Norwegian giro system used by all banks in the country, managed by Bankenes Betalingssentral (BBS). The system allows arbitrary transactions between private accounts in all Norwegian accounts using Norwegian krone. It is optimised for online banking, though it is also available via the post (Brevgiro), using a telephone (Telegiro) or based on automation, including the services Avtalegiro, Autogiro and eFaktura.\n\nThe system dates back to 1973 when the commercial banks and the saving banks created a common system that could compete with the Postgiro system used by the postal bank, Postbanken. Use of Bankgiro was free until 1985. In the late 1990s, Postgiro was bought out by BBS and Postbanken went over to using Bankgiro.\n\n"}
{"id": "40459873", "url": "https://en.wikipedia.org/wiki?curid=40459873", "title": "Blackman–Tukey transformation", "text": "Blackman–Tukey transformation\n\nIn electrical and electronic engineering, transformation from time domain to another domain such as frequency domain is used to focus more on details of the waveform. Many of the details can be analyzed much easier when this transformation is done to waveform, when it goes to another domain. Different methods are present to do transformation from time domain to frequency domain; one of them is the Blackman–Tukey transformation method that also uses Fast Fourier transform. To meet the filter specification according to requirements, several methods are used, including windowing effect.\n\nStatically estimation is to determine the expected value(s) of statistical expected values of statistical quantities. Statistical estimation also tries to find the expected values. The expected values are those values that we expect among the random values, derived from samples of population in probability (group of subset). Because we have problem here, and in time series analysis, discrete data obtained as a function of time is usually available rather than samples of population or group of subsets taken simultaneously. The difficulty is commonly avoided by the process which is named ergodic process, that changes with time and probability gets involved with it, and it's not always periodic at all portions of time.\n\nThis method has some procedures such as:\n\nAutocorrelation function makes the wave smoothed rather than the averaging several waveforms. This function is set to window, the corresponding waveform toward its extremes.\nComputation gets faster if more data is correlated and if memory capacity of the system increases then overlap save sectioning technique would be applied. If the autocorrelation function in Blackman–Tukey is computed using FFT then it will name fast correlation method for spectral estimation.\n\n"}
{"id": "4379834", "url": "https://en.wikipedia.org/wiki?curid=4379834", "title": "British Rail flying saucer", "text": "British Rail flying saucer\n\nThe British Rail flying saucer, officially known simply as \"space vehicle\", was a proposed spacecraft designed by Charles Osmond Frederick.\n\nThe flying saucer originally started as a proposal for a lifting platform. However, the project was revised and edited, and by the time the patent was filed had become a large passenger craft for interplanetary travel.\n\nThe craft was to be powered by nuclear fusion, using laser beams to produce pulses of nuclear energy in a generator in the centre of the craft, at a rate of over 1000 Hz to prevent resonance, which could damage the vehicle. The pulses of energy would then have been transferred out of a nozzle into a series of radial electrodes running along the underside of the craft, which would have converted the energy into electricity that would then pass into a ring of powerful electromagnets (the patent describes using superconductors if possible). These magnets would accelerate subatomic particles emitted by the fusion reaction, providing lift and thrust. This general design was used in several fusion rocket studies.\n\nA layer of thick metal running above the fusion reactor would have acted as a shield to protect the passengers above from the radiation emitted from the core of the reactor. The entire vehicle would be piloted in such a way that the acceleration and deceleration of the craft would have simulated gravity in zero gravity conditions.\n\nA patent application was filed by Jensen and Son on behalf of British Rail on 11 December 1970 and granted on 21 March 1973.\n\nThe patent lapsed in 1976 due to non-payment of renewal fees.\n\nThe patent first came to the attention of the media when it was featured in \"The Guardian\" on 31 May 1978, in a story by Adrian Hope of the \"New Scientist\" magazine. There was a further mention in \"The Daily Telegraph\" on 11 July 1982, during the silly season. \"The Railway Magazine\" mentioned it in its May 1996 issue, saying that the passengers would have been \"fried\" anyway.\n\nWhen the patent was rediscovered in 2006, it gained widespread publicity in the British press. A group of nuclear scientists examined the designs and declared them to be unworkable, expensive and very inefficient. Michel van Baal of the European Space Agency claimed \"I have had a look at the plans, and they don't look very serious to me at all\", adding that many of the technologies used in the craft, such as nuclear fusion and high temperature superconductors, had not yet been discovered, while Colin Pillinger, the scientist in charge of the Beagle 2 probe, was quoted as saying \"If I hadn't seen the documents I wouldn't have believed it\".\n"}
{"id": "17410326", "url": "https://en.wikipedia.org/wiki?curid=17410326", "title": "Charge transfer switch", "text": "Charge transfer switch\n\nA charge transfer switch OR CTS charge pump is a charge pump that offers better low-voltage performance and \"a better voltage pumping gain and a higher output voltage\" than previous charge pumps such as the Dickson charge pump.\n\n"}
{"id": "11498147", "url": "https://en.wikipedia.org/wiki?curid=11498147", "title": "Chicago P.D. (TV series)", "text": "Chicago P.D. (TV series)\n\nChicago P.D. is an American police procedural drama television series created by Dick Wolf and Matt Olmstead as the second installment of Dick Wolf's \"Chicago\" franchise. The series premiered on NBC as a mid-season replacement on January 8, 2014. The show follows the uniformed patrol officers and the Intelligence Unit of the 21st District of the Chicago Police Department as they pursue the perpetrators of the city's major street offenses.\n\nOn May 9, 2018, NBC renewed the series for a sixth season. The season premiered on September 26, 2018.\n\n\"Chicago P.D.\" follows the Chicago Police Department uniformed cops who patrol the beat and go head-to-head with the city's street crimes and the detectives of the Intelligence Unit that combats the city's major offenses, including organized crime, drug trafficking, and high-profile murders.\n\n\n\nAt the 2016 Television Critics Association winter press tour, NBC Entertainment president Jennifer Salke revealed that the network had discussions with Dick Wolf about a fourth series in the \"Chicago\" franchise centered on the legal system. The working title was \"Chicago Law\". Salke has since confirmed that the series was officially in development. The series will be a spin-off of \"Chicago P.D.\" by introducing Assistant State's Attorney (ASA) characters in the 21st episode of \"P.D.\"'s third season. Philip Winchester was the first to be cast. Nazneen Contractor joined the series in March 2016 and the title was changed to \"Chicago Justice\". \"Rocky\" alum Carl Weathers will play Cook County State's Attorney Mark Jefferies, while Joelle Carter will also star. Lorraine Toussaint reprised her role as defense attorney Shambala Green, who appeared in seven episodes of \"Law & Order\". On May 12, 2016, NBC gave \"Chicago Justice\" a series order. On September 28, it was reported that Jon Seda would join the cast and that his character Antonio would become an investigator for the DA's office. The series premiered on March 1, 2017. NBC canceled the series after one season. In July 2017, it was announced that Seda would rejoin \"Chicago P.D.\" full time.\n\nOn March 27, 2013, it was reported that NBC was considering plans for a spin-off of \"Chicago Fire\". Deadline Hollywood revealed that the proposed spin-off would involve the Chicago Police Department, and would be created and executive produced by Dick Wolf, Derek Haas, Michael Brandt, and Matt Olmstead. On May 10, 2013, NBC picked up the show for the 2013–14 United States network television schedule. On May 12, 2013, the show was announced as an unscheduled midseason replacement. On October 18, 2013, NBC announced that it would premiere on January 8, 2014, at 10:00 p.m. EST, after \"\".\n\nOn November 9, 2015, NBC renewed the series for a fourth season, which premiered on September 21, 2016. On May 10, 2017, NBC renewed the series for a fifth season, which premiered on September 27, 2017.\n\nThe series is filmed entirely in Chicago. The exterior of the station house is the Old Maxwell Street Police Station (943 West Maxwell Street) and is the same location that was used on the series \"Hill Street Blues\". It is located about half a mile from the firehouse location of \"Chicago Fire\" at 1360 S. Blue Island Ave.\n\nTania Raymonde was cast in the planned series as a police officer named Nicole. Kelly Blatz was also cast as a young but seasoned Officer Elam. The new characters were introduced in the penultimate episode of \"Chicago Fire\"'s first season, which aired on May 15, 2013. Scott Eastwood was set to portray Officer Jim Barnes in the \"Chicago Fire\" season finale and the proposed spin-off. Eastwood also co-starred with Tania Raymonde in \"Texas Chainsaw 3D\". \"\" Melissa Sagemiller would portray Detective Willhite, a member of the Chicago P.D. Intelligence Unit. Other cast members include Jason Beghe as the Intelligence team Sergeant Hank Voight and Jon Seda as Detective Antonio Dawson. LaRoyce Hawkins was the only Chicago area actor originally cast in May.\n\nAs the show went into pre-production, the cast began to change. On June 13, 2013, it was announced that Melissa Sagemiller would no longer be a part of the show and Jesse Lee Soffer officially joined the cast as a series regular. His character was introduced in the second season of \"Chicago Fire\". On August 23, 2013, Patrick Flueger and \"One Tree Hill\" star Sophia Bush joined the cast as Officer Adam Ruzek and Detective Erin Lindsay respectively. Marina Squerciati joined the cast on August 28, 2013. On August 30, Elias Koteas became a regular. Archie Kao was announced as a regular on September 27, 2013.\n\nOn October 21, 2013, Stella Maeve was cast in a recurring role as Nadia, a pretty 18-year-old escort who is addicted to heroin and goes through a very difficult withdrawal. Sydney Tamiia Poitier will guest star in five episodes as a detective, who will eventually crossover on to \"Chicago Fire\". On December 20, 2013, it was announced that both Eastwood and Raymonde had departed the series over creative differences.\n\nOn September 2016, it was reported that series regular Jon Seda would move to the new spin-off \"Chicago Justice\" as regular. After \"Justice\"'s cancellation, in July 2017 it was disclosed that Seda would move back to \"Chicago P.D.\" as series regular for season 5\n\nIn May 2017, it was announced that Sophia Bush would depart the series after four seasons\n\nIn July 2017, Tracy Spiridakos was promoted to series regular as Det. Hailey Upton, after a two-episode guest stint in season 4.\n\nIt came to light in November 2017 that Bush's exit was due to Jason Beghe's behavior behind the scenes. Bush's claims lead NBC to open up an investigation into Beghe's behavior\n\nAiring at 10 p.m. Wednesdays, \"Chicago P.D.\" is earning a 2.3 rating, 7 share in adults 18–49 and 9.4 million viewers overall this season, for gains versus last season of +10% in 18–49 and +14% in total viewers. \"Chicago P.D.\" has finished 1 or tied for No. 1 among ABC, CBS and NBC in the time period with five of its last six original broadcasts through Feb. 2.\n\nSince its premiere, \"Chicago P.D.\" has received mixed reviews from critics. Metacritic, which assigns a normalized rating out of 100 to reviews from mainstream critics, gives the drama a 50% rating based on 21 reviews, indicating \"mixed or average reviews\". Rotten Tomatoes gives season 1 a rating of 57% based on reviews from 28 critics.\n\nRay Rahman of \"Entertainment Weekly\" gave the drama a favorable review: \"It's hard to imagine the series capturing the compelling, can't-watch-just-one magic that makes the \"Law & Order\" franchise so marathonable, but it moves just fast enough to keep you from changing the channel in search of an SVU re-run.\" Alessandra Stanley of \"The New York Times\" also gave the series a positive review when it premiered on January 8: \"\"Chicago P.D.\" is, in many ways, a throwback to an earlier, male-dominated era of crime shows, yet it carves out room for strong female characters who are good at their jobs and taken seriously by their colleagues.\" Other critics expressed their disappointment in the police drama such as Robert Bianco of \"USA Today\": \"When did Wolf's work, which used to show some grace and wit, become this ugly, plodding and crass?\"\n\n\n"}
{"id": "5674131", "url": "https://en.wikipedia.org/wiki?curid=5674131", "title": "Color-tagged structure", "text": "Color-tagged structure\n\nA color-tagged structure is a structure in the United States which has been classified by a color to represent the severity of damage or the overall condition of the building. The exact definition for each color may be different at local levels.\n\nA \"red-tagged\" structure has been severely damaged to the degree that the structure is too dangerous to inhabit. Similarly, a structure is \"yellow-tagged\" if it has been moderately damaged to the degree that its habitability is limited (only during the day, for example). A \"green-tagged\" structure may mean the building is either undamaged or has suffered slight damage, although differences exist at local levels when to use a green tag.\n\nTagging is performed by government building officials, or, occasionally during disasters, by engineers deputized by the building official. Natural disasters such as earthquakes, floods and mudslides are among the most common causes of a building being red-, yellow- or green-tagged. Usually, after such incidents, the local government body responsible for enforcing the building safety code examines the affected structures and tags them as appropriate.\n\nIn some areas of the United States, buildings are marked with a rectangular sign that is red with a white border and a white \"X\". Such signs provide the same information as \"red-tagging\" a building. Tagging structures in these ways can warn firefighters and others about hazardous buildings before the buildings are entered.\n"}
{"id": "15755367", "url": "https://en.wikipedia.org/wiki?curid=15755367", "title": "Components in Electronics", "text": "Components in Electronics\n\nComponents in Electronics is a trade publication for the electronics industry. Owned by Specialist Business Media, \"Components in Electronics\" has been in print since 1984.\n\n\"Components In Electronics\" was first launched in 1984; Initially an advertising only publication its publisher, Hannover Press, decided two years later to create a new vehicle with a full editorial content. The team at that time included Tony Worton (publisher) who had formerly worked at electronics titles including \"New Electronics\", along with Alan Fredericks and Sean Stewart (both sales). The title was re-launched in October 1986, with Nick Foot as Editor. One year later, another permanent editor was employed and the same editorial team carried the magazine through to January 1998.\n\n\"CIE\" was originally printed on a King Size paper format the shape of a vinyl LP record. This meant that every page could include a combination of both editorial and advertising. The magazine focused entirely on electronic components, rather than production or test and measurement issues. In style, the editorial content was a mix of technical articles, case studies and new product information.\n\nBy the mid-90s the circulation stood at around 20,000 copies were distributed to designers and specifiers. A spin-off and annual Distsribution Directory was published around this time focusing on UK distribution.\n\nIn 1998, Jenny Ross, who had worked in a number of important production roles since the launch of the title, assumed the role of publisher. Following on from Ross, Shan Millie became publisher in February 2004 staying until December 2005. Alex McLachlan succeeded Millie taking over the role in December 2005 and remains the current publisher.\n\nIn around 2000 Paul Marsh became Editor followed in the early 2000s by Dulcie Elliot.\n\nLate 2006 saw a redesign of the print magazine. The King Size format was replaced by the ISO A4 size.\n\nIn September 2007, Neil Tyler took over as Editor followed by Joe Bush in 2014. Amy Wallington became editor in December 2015 and is currently still in the position. \n\n\"CIE\" is a Specialist Business Media publication. \n\n\"CIE\" provides a mix of industry news, market analysis and technological trends. It is read by electronics design engineers, specifiers and buyers.\n\nThe publication and website also covers established and emerging standards, new products, applications and technologies in the following groups:\n\n\n\n"}
{"id": "6449908", "url": "https://en.wikipedia.org/wiki?curid=6449908", "title": "Consumer fireworks", "text": "Consumer fireworks\n\nConsumer fireworks are fireworks sold to the general public for use. They are generally weaker in explosive power compared to professional displays.\n\nConsumer fireworks are often quite small and can be classified into three groups: daytime, nighttime, and novelty fireworks.\n\nDaytime fireworks include most bottle rockets, smoke balls, firecrackers, and other fireworks that emit very little or no light. Some examples of daytime consumer fireworks include:\n\nNovelty fireworks typically produce a much weaker explosion and sound. In some Countries and areas where fireworks are illegal to use, they still allow these small, low grade fireworks to be used. A few examples include:\n\n\nConsumer fireworks can be used with a variety of tools. One set of tools has to do with basic ignition, such as lighters, matches, and punks also known as a 'port fire'. By using a rack, one can ignite a series of different fireworks to create a scene. These sometimes allow for the finales seen at professional fireworks displays to be created using consumer fireworks. Racks can be used with multiple types of fireworks, such as aerial shells, fountains, Roman candles, and the newest class of fireworks, 500 gram repeaters. Other tools are involved with the setup of fireworks for later display, such as shovels, various hand tools, and spare visco fuses. The true scope of tools used with consumer fireworks is limited only by the displayer's imagination.\n\nThere are several ways by which fireworks can be ignited. The most basic of these is simply flame from a match, lighter or other device that emits flames. Another way to light fireworks is using a device called a punk. A punk is a long, thin piece of wood covered in a substance that burns very slowly, producing only heat, with no flame. A port fire is a smoldering compound as a powder compressed in a stiff paper tube. The most complicated method used to ignite consumer fireworks is to use electronic ignition. This is the preferred method of many professional pyrotechnicians worldwide because of the vast improvement in operator safety. There are a few electronic ignition (often called \"e-fire\") systems that use readily available materials.\n\nLaws regarding consumer fireworks vary between countries and states.\n\nIn Finland, fireworks (other than novelties and sparklers) are usually sold between Christmas and New Years. It is legal to sell fireworks all year around (predominantly through online retailers). They may only be used on New Year's Eve from 6pm to 2am the following morning.\n\nIn Germany, fireworks are available for purchase from most stores in the 3 days prior to New Year's Eve, allowing families to have their own celebrations in their backyard.\n\nIn Iceland, fireworks are sold from December 28–31 and again on January 6.\n\nIn Ireland, the import of consumer fireworks is strictly controlled, and the use of them must be overseen by a professional fireworks operator. This has led to considerable smuggling of fireworks from Northern Ireland, where regulations governing the sale of fireworks are more permissive.\n\nIn the Netherlands, fireworks (other than category 1) cannot be sold to anyone under the age of 16. They are only sold the last three days up till and including December 31. If one of those days is a Sunday, the sale period is moved to the 28 up till and including 30 December. Fireworks may only be fired between 6pm on New Year's Eve to 2am the following day.\n\nIn Norway, fireworks cannot be sold to anyone under the age of 18. Class 1, 2 and 3 fireworks are for sale from December 27 to December 31. They may only be fired between 18:00h and 02:00h on New Year's Eve.\n\nIn the United Kingdom, fireworks cannot be sold to anyone under the age of 18 and are not permitted to be set off between 11pm and 7am. Exceptions are made for New Year, Bonfire Night (5 November), the Chinese New Year and Diwali. Fireworks are available from specialist stores year round and their use is also permitted throughout the year within the specified time limits. The sale of Categories 1 (Indoor), 2 (Garden) and 3 (Display) are available to the general public — with Category 4 (Professional) being restricted to permit holders.\n\nIn Belgium, fireworks (except category 1) cannot be sold to anyone under the age of 16. Fireworks are available for purchase year round, but may only be used by police permission.\n\nIn Poland, fireworks cannot be sold to anyone under the age of 18. Fireworks are available for purchase year round, but are primarily sold before New Year's Eve.\n\nFireworks laws vary across province. They make some noise, they sparkle, and according to all the experts, they are dangerous. However, it is legal in Canada to purchase a variety of consumer fireworks like Roman candles and star wheels.\n\nThe Western provinces, Such as Alberta and B.C. draw the line when it comes to home explosives.\n\nIn Alberta, it varies when it comes Fireworks and Explosives, the province last year considered a province-wide ban on fireworks (though the ban would have given cities wiggle-room to pass laws allowing certain displays).\n\nIn Calgary, there’s a total ban, meaning you can’t buy or set off fireworks anywhere, while Edmonton allows it, but only if you get a permit first.\n\nIn British Columbia, Fireworks on the West Coast aren’t even in the discussion for Canada Day, or Victoria Day for that matter.\n\nIn the Metro Vancouver, you can buy fireworks in Vancouver, Burnaby, West Vancouver and North Vancouver, as long as you’re within a week of Halloween (and with a permit), and Canada day, but you’re out of luck in Surrey, Richmond, Langley, and Abbotsford, and much of the lower mainland.\n\nIn Vancouver there is a tradition to light things off on Halloween.\n\nIn Ontario, Fireworks are sold around Canada Day and Victoria Day without a permit, and some GTA municipalities now allow fireworks on the Hindu festival of Diwali. In Toronto itself, you might get away with lighting them off on Chinese New Year, and it’s possible to score a permit for other occasions.\n\nIn Quebec St. Jean-Baptiste Day is a major fireworks day, but not necessarily the backyard variety. Fireworks are forbidden year round on the Island of Montreal, though allowed in much of the rest of the province.\n\nIn Atlantic Canada, expect Prince Edward Island, All Fireworks are legal a can be used all year round.\n\nIn the United States, the laws governing consumer fireworks vary widely from state to state, or from county to county. It is common for consumers to cross state and county lines in order to purchase types of fireworks which are outlawed in their home-jurisdictions. Fireworks laws in urban areas typically limit sales or use by dates or seasons. Municipalities may have stricter laws than their counties or states do.\n\nThe Consumer Product Safety Commission defines what fireworks may be considered consumer fireworks. Consumer fireworks in the United States are limited to 500 grams (this was previously 200 grams until recent years) of composition, and firecrackers may have up to 50 milligrams of flash powder. Reloadable shells are limited to 1.75\" in diameter, and shells in pre-fused tubes are limited to 2\". Any fireworks that exceed these limits are not considered consumer fireworks and need an ATF license.\n\nThe American Pyrotechnics Association maintains a directory of state laws pertaining to fireworks.\n\nOne state  — Massachusetts— bans the sale and use of all consumer fireworks, including novelties and sparklers.\n\nTwo states  — Illinois and Vermont  — permits the sale and use of only wire or wood stick sparklers and other novelties.\n\nThe following states allow the sale and use of non-aerial and non-explosive fireworks (also called \"safe and sane\") like novelties, fountains and sparklers, etc.: Arizona, California, Colorado, Connecticut, Delaware, Florida, Idaho, Maryland, Minnesota, New York, New Jersey, North Carolina, Oregon, Rhode Island, Virginia, Wisconsin, and the District of Columbia.\n\nDelaware Governor John Carney signed into law a bill on May 20, 2018 allowing sparklers and non-explosive, non-airborne novelty items. The use is allowed only on July 4 and December 31 each year with sales limited to the 30 days preceding their allowed use.\nNew York Governor Andrew Cuomo signed into law a bill on November 21, 2014 allowing sparklers, party poppers, cone fountains and other novelty items to be sold, possessed, and used in the state of New York outside of New York City. Local governments must approve the sale and use of fireworks before people can legally use them. Although the use of fireworks is allowed year-round, the sale of fireworks, by registered businesses, is limited to a period of June 1 through July 5 and December 26 through January 2 each year.\n\nMichigan legislators have passed and enrolled a bill that was signed by Governor Rick Snyder on December 13, 2011 allowing the sale and use of all consumer fireworks, however, sellers must pay a fee ($600–1000) to sell higher-power consumer fireworks, and a tax will be added to fireworks purchases.\n\nCalifornia has very specific requirements for the types of consumer fireworks that can be sold to and used by residents. Even then each city can and often does place restrictions on sale and use. The use of firecrackers is permitted according to the FBGOC.\n\nIn Minnesota and Florida only consumer fireworks that do not explode or fly through the air are permitted to be sold to and used by residents.\n\nThe following states permit the sale of all or most types of consumer fireworks to residents: Alabama, Alaska, Arkansas, Iowa, Indiana, Kansas, Kentucky, Louisiana, Maine, Michigan, Mississippi, Missouri, Montana, Nebraska, New Hampshire, New Mexico, North Dakota, Ohio, Oklahoma, Pennsylvania, South Carolina, South Dakota, Tennessee, Texas, Utah, Washington, West Virginia, Wyoming (varies by county) and Georgia. Many of these states have selling seasons around the 4th of July and/or Christmas and New Year's Eve. Some of these states also allow local laws or regulations to further restrict the types permitted or the selling seasons.\n\nMissouri permits all types of consumer fireworks to be sold to residents with two selling seasons; June 20 – July 10 and December 20 – January 2. South Carolina permits all types of consumer fireworks except small rockets less than ½” in diameter and 3” long to be sold and used by residents year round.\n\nTwo states — Hawaii and Nevada — allow each county to establish its own regulations.\n\nFor example, Clark County, Nevada, where Las Vegas is located, allows residents to purchase and use only non-explosive and non-aerial consumer fireworks during the 4th of July, while other counties permit all types of consumer fireworks.\n\nIn Maine fireworks can only be sold to people over 21 at firework only stores except for sparklers that can be sold at stores that sell other items except for propane dealers or other explosives\n\nMany states have stores with all types of consumer fireworks that sell to non-residents with the provision they are to remove the purchased fireworks from that state. This is why there are many stores selling fireworks in states like Ohio, Florida, Missouri, New Hampshire, Nevada and Wisconsin with all types of consumer fireworks, even though residents are limited or prohibited from buying or using those very same consumer fireworks unless they have the appropriate licenses and/or permits.\n\nMany Native American Tribes have consumer fireworks stores on reservation lands that are exempt from state and local authority and will sell to people that are not in the tribe.\n\nIn Australia, Type 1 fireworks are permitted to be sold to the public. For anything that has a large explosion or gets airborne, users need to register for a Type 2 Licence. The Australian Capital Territory allows fireworks to be sold to residents 18 years or older during the week leading up to the Queen's Birthday long weekend for personal purposes. A similar allowance is made in the Northern Territory in the days leading up to Northern Territory Day (July 1). The types of fireworks allowed for sale is restricted to quieter fireworks, which can only be used during the long weekend and only at the address provided to the seller.\n\nIn New Zealand fireworks cannot be sold to anyone under the age of 18 (previously 14), and may only be sold for the four days leading up to and including November 5. However, fireworks are able to be used at any time of the year (i.e.: there is no time restriction on when fireworks can be used, even though there is a restriction on the sales time of fireworks). The types of fireworks available to the public are multi-shot \"cakes\", Roman candles, single shot shooters, ground and wall spinners, fountains, cones, sparklers, and various novelties, such as smoke bombs and pharaoh's serpents. sales of fireworks have become increasingly restricted in recent years. Skyrockets, and other fireworks where the firework itself flies, were banned in 1994. Firecrackers and Bangers were banned in 1993. As of 2007, sparklers may only be purchased no more than 50 at a time in packs with other fireworks. This is due to the popularity of sparkler bombs.\nIn Chile fireworks are restricted for consumer use and can only be seen in professional shows. The prohibition came at the request of organizations dealing with burnt children, many of the accidents being caused by unsupervised use of fireworks.\n"}
{"id": "28152761", "url": "https://en.wikipedia.org/wiki?curid=28152761", "title": "Dr. Scholl's", "text": "Dr. Scholl's\n\nDr. Scholl's is a footwear and foot care brand owned by Bayer in the North and Latin American markets, and by Aurelius AG in its remaining markets worldwide. The brand in the North and Latin American markets was owned by Merck & Co. through its predecessor Schering-Plough until they sold it to Bayer on 1 October 2014. Aurelius AG bought the brand for all other markets from Reckitt Benckiser on 21 July 2014.\n\nPodiatrist William Mathias Scholl started the brand in 1906, in Chicago, United States. The original company expanded globally to design and patent over 1000 foot care products, and became a member of the Fortune 500 in 1971.\n\nAfter William Scholl died in 1968, Schering-Plough bought the brand. Under parent Merck & Co., Schering-Plough imports the product line from China, and has a North American distribution agreement with the Brown Shoe Company. In 1984, Schering-Plough sold the global brand and non-North American operations to European Home Products, who manufacture and distrubute footwear and foot care products under the Scholl brand.\n"}
{"id": "30830351", "url": "https://en.wikipedia.org/wiki?curid=30830351", "title": "Dream Logic", "text": "Dream Logic\n\n\"Dream Logic\" is the fifth episode of the second season of the American science fiction drama television series \"Fringe\", and the 25th episode overall. It was written by Josh Singer and directed by Paul A. Edwards. The episode follows several people seemingly dreaming while still awake, leading the Fringe team to investigate the dangerous side effects of a sleep study.\n\nOn its initial American broadcast on October 15, 2009 on the Fox network, \"Dream Logic\" was watched by an estimated 5.78 million viewers. It received mixed reviews, with multiple critics noting it was considerably worse in quality than the previous week's episode while at the same time praising the case's ties to Peter's past as well as the return of Sam Weiss.\n\nIn Seattle a man named Greg Leiter (Jim Thorburn) hallucinates that his boss and coworkers are demons, leading Greg to attack and murder the boss. Greg is hospitalized and falls asleep for sixteen hours; when Olivia (Anna Torv) and Peter (Joshua Jackson) interview him in the hospital, he tells them his boss was a demon out of a bad dream before suffering a seizure and having his hair turn white. Walter (John Noble) posits that Greg died from \"acute exhaustion\". However, believing Seattle to be like the mental institution, Walter desires to go home to Boston and run tests on the corpse from there.\n\nOlivia and Peter learn Greg was being treated for a sleep disorder, and that his dreams had involved demons until they stopped several months ago. Another hallucinating victim turns up in Seattle and dies. The Fringe team discover that both victims had a brain–computer interface chip attached to their thalamus, the part of the brain controlling dreams. Broyles (Lance Reddick) and Nina (Blair Brown) reveal new information leading to the sleep researcher Dr. Nayak (Ravi Kapoor) who implanted the chips. Another victim named Diana (Jovanna Huguet) hallucinates at a restaurant and kills a coworker before similarly dying of exhaustion.\n\nOlivia and Peter first suspect Dr. Nayak's research assistant Zach (Jarrett Knowles) but find him dead. Back in Boston Walter believes the chips lead to mind control and tests this on the FBI agent assigned to him while Peter and Olivia are away. However, during these tests Walter soon changes his theory; the dreams are being stolen from their hosts to cause a \"high\" in Dr. Nayak, who is receiving them and has two personalities. Peter and Olivia shut down the dream equipment before Nayak kills another victim, but the doctor dies in the process. The final scene shows Peter dreaming about his childhood when Walter kidnaps him, an event Peter normally has no memory of; Peter wakes up confused but still unaware of what his father did.\n\nIn a sideplot, Olivia is grieving for her partner Charlie, whom she discovered in previous episode was murdered by a shapeshifter. Sam Weiss (Kevin Corrigan) helps her work through it by giving her a \"project\" that requires her to collect business cards from people wearing the color red. She is told to grab random letters from the names, that once unscrambled read \"you're gonna be fine\".\n\n\"Dream Logic\" was written by co-executive producer Josh Singer and was directed by cinematographer Paul A. Edwards. It was filmed in August 2009. Shooting partly took place in the basement of a semi-functioning mental hospital in Vancouver, leading actress Anna Torv to remark during filming, \"I heard this is where they kept dead bodies. Bad vibes in this place.\" Actor Joshua Jackson noted that the hospital's \"patients are known to wander through a shot. This has the potential for being a very interesting day.\"\n\n\"Dream Logic\" featured a guest appearance by recurring guest star Kevin Corrigan, as well as one time guest actors Ravi Kapoor, Jim Thorburn, Travis Schuldt, Drew Nelson, and Alex Zahara.\n\n\"Dream Logic\" featured the song \"From the Beginning\" by the progressive rock trio Emerson, Lake & Palmer. Dr. Nayak's lab assistant is named Zack Miller, which was perceived by some media outlets to be a reference to frequent \"Fringe\" writing partners Zack Stentz and Ashley Edward Miller. In the final scene set in the young Peter's bedroom in the parallel universe, a poster depicts the Space Shuttle \"Challenger\" in its 11th mission (1984). However, in the prime universe the \"Challenger\" exploded in its 10th mission two years later. One viewer noted this as yet another sign that the parallel universe is more technologically advanced than ours, as they began their space program earlier than ours.\n\nOn its initial American broadcast on October 15, 2009, an estimated 5.78 million viewers watched \"Dream Logic\", helping it earn a 3.5/6 ratings share among all households and a 2.2/6 ratings share in the important 18–49 demographic.\n\nAOL TV writer Jane Boursaw wrote, \"The whole storyline with the creepy dual-personality doc and the mind-control-dreams was good, though I sort of knew the doc must be involved from the beginning. I also wonder how it fits in with the alternate universe.\" The A.V. Club's Zack Handlen graded the episode with a B-. Ken Tucker from \"Entertainment Weekly\" believed that \"in some ways, this was one of the less-distinctive, more \"X-Files\"-ish episodes of Fringe on Thursday night... But as usual, there was another narrative layer at work here\" involving a grieving Olivia. IGN's Ramsey Isler rated the episode 7.4/10, explaining the case-of-the-week failed to have \"excitement, surprise, and suspense\" and lacked \"what makes a typical \"Fringe\" episode unique and intriguing\". While calling certain parts \"kind of boring\", Isler did however praise the episode's directing, acting, sets, and other technical aspects. He concluded his review by noting the best part of \"Dream Logic\" was the ending depicting a young Peter, \"It's a haunting scene, and it adds yet another stone in this long pathway to revelation that the Fringe team is building up to... But besides that great little scene at the end, I thought this was an average episode. Granted, \"Fringe\"s 'average' episodes are still better than a lot of the other stuff on TV.\"\n\nMTV writer Josh Wigler thought the episode \"nearly put [him] to sleep,\" as it was \"bland\" and \"a bit of a dud\". The episode however did lead Wigler to \"start dig[ging]\" Sam Weiss, and also to enjoying Olivia's subplot, but believed the episode's monster-of-the-week could have been used to actually parallel her trauma; Wigler appreciated how the Fringe case tied into Walter and Peter's backstories. He concluded, \"It wasn't bad enough to make me concerned for the future of \"Fringe\" or anything, but it certainly didn't keep me on the edge of my seat waiting for the next crazy twist like last week. I'm still loving this season, but tonight's episode was one of the weaker outings.\" Newsarama's Chanel Reeder stated \"Dream Logic\" \"certainly put the brakes on the speed that \"Fringe\" had gained in the previous\" episode, but praised the connection with Peter's past as \"one of the most interesting underlying parts\". Reeder's favorite part was Sam Weiss, calling him \"a fantastic dimension to the show\". Josie Kafka of Open Salon was more positive than other reviewers, and in particular highlighted the episode's humor, \"Almost all of the Cambridge scenes were funny: Astrid and Walter have a great rapport, especially when there's a rube in the room for them to play with.\" She concluded \"I liked this one, more for the great Peter/Olivia and Walter/Astrid stuff than the plot of the week,\" and gave the episode \"Three out of four anagrams\".\n\nAfter the episode's broadcast, \"Popular Mechanics\" published an article analyzing the science depicted. They concluded that the Brain–computer interface (BCI) chips, like the ones used in the episode to tie the victims' brains to the computer used by Dr. Laxmeesh Nayak, have also been used on real-life human subjects, though not in relation to controlling sleep cycles. However, the article continued that it is \"not currently possible\" for BCI chips to \"facilitate direct transfer of understandable information from one person's brain to another\", nor is it possible for the chips to \"directly read another person's thoughts or dreams\" and steal them. In addition, \"Popular Mechanics\" noted that Walter's theory of the victims' deaths being caused by exhaustion is \"pure bunk\", as there are many long-term experiments that have safely tested humans' deprivation of REM sleep. According to one scientist interviewed, though chips shown in \"Dream Logic\" cause hallucinations, paranoid thoughts, and a disconnect from reality, these traits are \"not even remotely possible\" outside of fiction.\n\nDirector Paul A. Edwards submitted \"Dream Logic\" for consideration in the Outstanding Directing for a Drama Series category at the 62nd Primetime Emmy Awards. He did not receive a nomination.\n\n"}
{"id": "22647055", "url": "https://en.wikipedia.org/wiki?curid=22647055", "title": "Dry carpet cleaning", "text": "Dry carpet cleaning\n\nDry carpet cleaning involves the use of specialized machines to clean carpets with recently developed chemical technologies that permit no-moisture or \"very low moisture\" (VLM) cleaning, resulting in carpet beautification, and removal of stains, dirt, grit, sand, and allergens. Clean carpets are recognized by manufacturers as being more visually pleasing, potentially longer-lasting and probably healthier than poorly maintained carpets.\n\nCarpet cleaning is reportedly widely misunderstood, and chemical developers have only within recent decades created new carpet care technologies. Particularly, encapsulation and other green technologies work better, are easier to use, require less training, save more time and money, and lead to less re-soiling than prior methods. Dry carpet cleaning can also aid in achieving U.S. Green Building Council Leadership in Energy and Design (LEED) certification.\n\nDry carpet cleaning systems are more accurately known as \"very low moisture\" (VLM) systems, relying on dry compounds complemented by application cleaning solutions, and are growing significantly in market share due in part to their very rapid drying time, a significant factor for 24-hour commercial installations. Dry-cleaning and \"very low moisture\" systems are also often faster and less labor-intensive than wet-extraction systems.\n\nPre-treatments, pre-conditioners, or \"traffic-lane cleaners\", which are detergents or emulsifiers that break the binding of soils to carpet fibers over a short period of time, are commonly sprayed onto carpet prior to the primary use of the dry-cleaning system. One chemical dissolves the greasy films that bind soils and prevent effective soil removal by vacuuming. The solution may add a solvent like d-limonene, petroleum byproducts, glycol ethers, or butyl agents. The amount of time the pretreatment \"dwells\" in the carpet should be less than 15 minutes, due to the thorough carpet brushing common to these \"very low moisture\" systems, which provides added agitation to ensure the pretreatment works fully through the carpet.\n\nAn absorbent, 98% biodegradable cleaning compound may be spread evenly over carpet and brushed or scrubbed in. For small areas, a household hand brush can work such a compound into carpet pile; dirt and grime is attracted to the compound, which is then vacuumed off, leaving carpet immediately clean and dry. For commercial applications, a specially designed cylindrical counter-rotating brushing system is used, without a vacuum cleaner. Machine scrubbing is more typical, in that hand scrubbing generally cleans only the top third of carpet.\n\nIn the 1990s, new polymers began literally encapsulating (crystallizing) soil particles into dry residues on contact, in a process now regarded by the industry as a growing, up-and-coming technology; working like \"tiny sponges\", the deep-cleaning compound crystals dissolve and absorb dirt prior to its removal from the carpet. Cleaning solution is applied by rotary machine, \"CRB\" counter rotating brush, brush applicator, or compression sprayer. Dry residue is vacuumable immediately, either separately or from a built-in unit of the cleaning system machine. According to \"ICS Cleaning Specialist\", evidence suggests encapsulation improves carpet appearance, compared to other systems; and it is favorable in terms of high-traffic needs, operator training, equipment expense, and lack of wet residue. Encapsulation carpet cleaning also keeps carpets cleaner for longer periods of time compared to other methods. Encapsulation also avoids the drying time of carpet shampoos, making the carpet immediately available for use.\n\nThe use of encapsulation to create a crystalline residue that can be immediately vacuumed (as opposed to the dry powder residue of wet-cleaning systems, which generally requires an additional day before vacuuming)is a newer technology that has recently become an accepted method for commercial and residential carpet deep cleaning.\n\nAfter cleaning product in solution is deposited onto the surface as mist, a round buffer or \"bonnet\" scrubs the mixture with rotating motion. This industry machine resembles a floor buffer, with an absorbent spin pad that attracts soil and is rinsed or replaced repeatedly. The bonnet method is not strictly dry-cleaning and involves short drying time, making it a solution for a deep cleaning of dirt or odor as considered suitable for valuable carpet. To reduce pile distortion, the absorbent pad should be kept well-lubricated with cleaning solution.\n\nA d-limonene based cleaner is pre-sprayed upon the carpet to be cleaned. The product is given a dwell time of 5–10 minutes. The carpet is then extracted using an acid rinse solution through a hot water extraction machine. Triple dry strokes are then performed to ensure a low dry time. While this process is not strictly dry cleaning and involves a 1-4 hour dry time, it cleans deep into the fibers.\n"}
{"id": "57091785", "url": "https://en.wikipedia.org/wiki?curid=57091785", "title": "EU medical device regulation", "text": "EU medical device regulation\n\nThe European Union Medical Device Regulation (Council Regulation 2017/745 of 5 April 2017 concerning medical devices, OJ No L 117/1 of 2017-05-05) to repeals the existing directives on medical devices: Medical Devices Directive [93/42/EEC] and Active Implantable Medical Device Directive [90/385/EEC]. The regulation was published on 5 May 2017 and comes into force on 25 May 2020. Currently approved medical devices will have a transition time of three years (until 26 May 2020) to meet the new MDR requirements.\n"}
{"id": "52094170", "url": "https://en.wikipedia.org/wiki?curid=52094170", "title": "Elastic and inelastic collisions apparatus", "text": "Elastic and inelastic collisions apparatus\n\nThe elastic and inelastic collisions apparatus is a large apparatus to study elastic and inelastic collisions.\n\nIt consists of a large frame carrying two beams from which two rows of six and two wooden balls, respectively, are suspended from pairs of strings. The instrument was often used with two elastic balls (of ivory) or inelastic balls (of wet clay), of equal or different mass. By changing the parameters of the experiments such as height of fall and mass, one could conduct a systematic investigation of collision-related phenomena. For example, when the row of balls is struck by one of the outermost balls, the row of balls remains motionless and the impulse is fully transmitted to the ball at the opposite end, which rebounds. As it falls back, the cycle continues in the opposite direction. This apparatus was illustrated by Jean-Antoine Nollet in \"Leçons de physique expérimentale\" (Paris, 1743-1748). In his description, Nollet claims to have merely altered and developed a model previously used by Edme Mariotte. The most sophisticated devices for studying elastic and inelastic collisions were built by Willem Jacob 's Gravesande and Petrus van Musschenbroek.\n\nThe instrument is held in the Lorraine collections of the Museo Galileo in Florence.\n"}
{"id": "1490736", "url": "https://en.wikipedia.org/wiki?curid=1490736", "title": "Elektro", "text": "Elektro\n\nElektro is the nickname of a robot built by the Westinghouse Electric Corporation in its Mansfield, Ohio facility between 1937 and 1938. Seven feet tall (2.1 m), weighing 265 pounds (120.2 kg), humanoid in appearance, he could walk by voice command, speak about 700 words (using a 78-rpm record player), smoke cigarettes, blow up balloons, and move his head and arms. Elektro's body consisted of a steel gear, cam and motor skeleton covered by an aluminum skin. His photoelectric \"eyes\" could distinguish red and green light. He was on exhibit at the 1939 New York World's Fair and reappeared at that fair in 1940, with \"Sparko\", a robot dog that could bark, sit, and beg to humans.\n\nElektro toured North America in 1950 in promotional appearances for Westinghouse, and was displayed at Pacific Ocean Park in Venice, California in the late 1950s and early 1960s. He also appeared as \"Thinko\", in \"Sex Kittens Go to College\" (1960). In the 1960s, his head was given to Harold Gorsuch, a retiring Westinghouse engineer.\n\nIn 1992, the dance band Meat Beat Manifesto produced the song \"Original Control (Version 2)\" which prominently featured snippets of Elektro's monologues, quoting lines such as \"I am Elektro\" and \"My brain is bigger than yours\".\n\nElektro survived the scrap pile and is currently the property of the Mansfield Memorial Museum. In 2013, a replica of Elektro was exhibited at The Henry Ford Museum in Dearborn, MI.\n\nA \"Mansfield Museum of Circuitry and Robotics\" and anecdotes about Elektro and Sparko are briefly but touchingly presented in the beginning of the novel \"The Man from Primrose Lane\" by James Renner, 2012, Sarah Crichton Books, an imprint of Farrar, Straus and Giroux.\n\n"}
{"id": "54625595", "url": "https://en.wikipedia.org/wiki?curid=54625595", "title": "EuroFIRST PIRATE", "text": "EuroFIRST PIRATE\n\nThe EuroFirst Passive Infrared Airborne Track Equipment (PIRATE) is the Forward looking infrared(FLIR) / Infra-red search and track (IRST) for the Eurofighter Typhoon. It is produced by the EuroFIRST consortium consisting of Leonardo S.p.A. of Italy (lead contractor), THALES Land & Joint Systems of the UK (system technical authority), and TECNOBIT of Spain. The system is mounted on the port side of the fuselage, forward of the windscreen and provides passive and thus undetectable and unjammable means of long range surveillance. In addition the system has been shown to locate stealth aircraft at a \"significant distance\" with further improvements in detection through software updates. PIRATE detects heat caused on a aircraft's skin caused by air friction.\n\nIn 1989 Thorn-EMI and FIAR, and GEC Avionics and Pilkington and Ferranti competed for the tender to develop an IRST system for the Eurofighter. In 1991, Germany decided not to participate in the development due to cost issues but reserved the right to use the system. In September 1992, Thorn-EMI (now Thales) finally won the tender with Italy's FIAR (now Leonardo) and Spain's Eurotronica for the infrared sensor of the Eurofighters, based on the Air Defence Alerting Device (ADAD), and went on to form the EuroFirst consortium. The flight tests were begun in 2001 by testing the FLIR function in the case of flights between Turin and Sardinia. Later on, the FLIR was also used to coordinate the integration into the Eurofighter's display elements. The first Eurofighter Typhoon equipped with PIRATE was in a tranche 1 block 5 and delivered to the Italian Aeronautica Militare in August 2007. \n\nPIRATE is a passive, rotating, cooled infrared sensor with high resolution, which is also referred to as an imaging infrared sensor. PIRATE operates in two IR bands, 3–5 and 8–11 micrometres. The externally visible dome is only the azimutally rotatable sensor head, which contains the tilting mirror. Both elements have been stabilized, the maximum azimuth and elevation range is probably 150 ° × 60 °. The sensor consists of a single Line-Replaceable Item (LRI), which in turn contains four subsystems: The sensor head module with the stabilized sensor head, the detector, the servo control processor, and the electronics for signal matching. The data processing computer to discover and tracking targets, the video processor to the imaging display of the infrared image on the HUD, HMD or MHDD. The sensor is connected to the MIL-STD-1553 to and connect to a STANEG 2910 bus. PowerPC processors were used for the signal processing. The sensor is 680 × 591 × 300 mm (L x W x H), and weighs kilograms.\n\nPIRATE is fundamentally a Track-While-Scan infrared search and track (IRST) system which also performs conventional navigation FLIR functions. It can track and prioritized up to 500 targets simultaneously. In the air-air role, the equipment passively detects, tracks, classifies and prioritizes multiple airborne targets under all aspects, look-up, look-down and co-altitude conditions. The IRST functions are fully integrated into the aircraft attack and identification sub-system and can be diverted to specific search and track areas as a result of commands on the communication bus. The pilot has several selectable operating modes depending upon the particular combat mission requirement. These images are displayed on the Head Up Display (HUD), Multifunction Head Down Display (MHDD) or the pilots helmet visor. \nPIRATE is capable of operating in each one of the following IRST modes :\n\n\n\nPIRATE is also providing IR TV imaging to cockpit displays in accordance with the following FLIR modes:\n\nThe range of the sensor is a closely guarded secret by the EuroFirst consortium but confirmed to be more than 74 km .The RAND Corporation reports ranges of 50 nm (93 km) to a subsonic target from the front and up to 90 nm from the rear of a subsonic target. However, the weather conditions will affect the performance of the infrared-based target search and target tracking significantly. In 2013, the detection range of PIRATE was said to be further increased by software updates. \n"}
{"id": "10930690", "url": "https://en.wikipedia.org/wiki?curid=10930690", "title": "Finn Haldorsen", "text": "Finn Haldorsen\n\nFinn Haldorsen (5 September 1934 – February 4, 2005) was a Norwegian-born businessman.\n\nFinn Haldorsen was the youngest of 13 children born to Haldor Haldorsen and Anne Serine Haldorsen in\nRubbestadneset, county of Hordaland, Norway. After completing schooling on Bømlo he attended college in Bryne, Norway. Following his military service, he went to work at a steel factory in Manchester, England. He started studying mechanical engineering at the University of Cardiff in Wales. After receiving his honours degree in 1961, he returned to Norway to start work at Wichmann Motorfabrikk AS, his father’s engine factory.\n\nHaldorsen was best known for starting the Rubb Group in Norway in 1968. Rubb Group was named for his home town of Rubbestadneset. Haldorsen's goal was to build a fabric covered building that would withstand the harsh Norwegian climate. His success with this effort lead to him moving to England to start Rubb Buildings Ltd. in 1977. When this company was firmly established under the leadership of Bill Wood, he moved to Maine, and established Rubb Inc. in the United States which today is under the leadership of David C. Nickerson.\n\nHaldorsen was married, and had 3 sons and 1 daughter.\n\n"}
{"id": "50959753", "url": "https://en.wikipedia.org/wiki?curid=50959753", "title": "GCM NES", "text": "GCM NES\n\nGCM NES is a core banking and financial software suite developed by GCM for use by retail banks.\n\nIt includes functions for universal banking, core banking, payments, money markets, insurance, securities processing, financial inclusion, treasury operations. There are also modules that deal with capital markets and the insurance business.\n\nNES offers a suite of products, which are periodically evaluated by independent research firms like Forrester. The list of products include.\n\n\n"}
{"id": "39775526", "url": "https://en.wikipedia.org/wiki?curid=39775526", "title": "GainSpan", "text": "GainSpan\n\nGainSpan, a San Jose, California-based semiconductor company, designs and markets wireless connectivity products. It offers Wi-Fi chips, software, and embedded Wi-Fi modules. The company provides Wi-Fi technology for the residential housing, healthcare, and smart energy industries. \n\nEngineers from Intel Corporation created GainSpan in September 2006 with the goal of reducing the power consumption of traditional Wi-Fi. GainSpan was the first company to optimize Wi-Fi chips for low power consumption and to apply new power management techniques to target long battery life applications.\n\n"}
{"id": "945024", "url": "https://en.wikipedia.org/wiki?curid=945024", "title": "Groupe Bull", "text": "Groupe Bull\n\nBull SAS (also known as Groupe Bull, Bull Information Systems, or simply Bull) is a French-owned computer company headquartered in Les Clayes-sous-Bois, in the western suburbs of Paris. The company has also been known at various times as Bull General Electric, Honeywell Bull, CII Honeywell Bull, and Bull HN. Bull was founded in 1931, as H.W. Egli - Bull, to capitalize on the punched card technology patents of Norwegian engineer Fredrik Rosing Bull (1882–1925). After a reorganization in 1933, with new owners coming in, the name was changed to Compagnie des Machines Bull (CMB).\n\nThe company has undergone many takeovers and mergers since its formation. In particular, it has had various ownership relations with General Electric, Honeywell, and NEC from the 1960s to the 1980s; and with Motorola, Debeka, and France Télécom more recently. It acquired Honeywell Information Systems in the late 1980s, and later also had a share of Zenith Data Systems and Packard Bell. Bull was nationalised in 1982 and was merged with most of the rest of the French computer industry. In 1994, the company was re-privatised.\n\nBull has a worldwide presence in more than 100 countries, and is particularly active in the defense, finance, health care, manufacturing, public and telecommunication sectors.\n\nIn August 2014, the French IT company Atos announced that it had acquired a controlling stake in Bull SA through a tender offer launched in May. Atos announced plans in October, 2014 to buy out or squeeze out the remaining share and bondholders.\n\nBull launched the Hoox m2, the first integrally secured European smartphone, which in June 2014 was approved for use with data classified as 'Restricted Information' ('Diffusion Restreinte') by the Agence nationale de la sécurité des systèmes d'information (ANSSI). The Hoox range of secure mobiles and smartphones ensures confidentiality of voice, SMS, e-mail and data communication.\n\nOn 31 July 1919, a Norwegian engineer named Fredrik Rosing Bull filed a patent for a \"combined sorter-recorder-tabulator of punch cards\" machine that he had developed with financing from the Norwegian insurance company Storebrand. Storebrand integrated his device into its operations in 1921. The following year Bull sold his second machine to the Danish insurer Hafnia who had learned of the technology through an article in an insurance trade magazine. At the time of Bull's death of cancer in 1925 at the age of 43, a dozen of his machines had been sold to different companies throughout Europe. The commercial and technical development of the machines continued under the direction of Bull's childhood friend and long-time collaborator Reidar Knutsen along with his brother Kurt Andréas Knutsen.\n\nAs the business grew several outside investors were brought in, leading to the incorporation of the company H.W. Egli Bull in 1931. In 1933, more investors joined and the company changed its name to Compagnie des Machines Bull, a name it would keep until 1964.\n\n\n\n\nGroupe Bull:\n\nAmesys, a Groupe Bull subsidiary specializing in defense and aerospace-related systems and software, became embroiled in controversy in 2011 when it was revealed that it had sold an internet monitoring system to the Muammar Gaddafi regime of Libya in 2007. The \"Eagle System\" was used by the Gaddafi regime to spy on citizens and foreign journalists. On 12 March 2013 Reporters Without Borders named Amesys as one of five \"Corporate Enemies of the Internet\" and “digital era mercenaries” for selling products that have been or are being used by governments to violate human rights and freedom of information. A judicial inquiry was opened by the French government in May 2012 following allegations of complicity in torture by the International Federation for Human Rights (FIDH). In March 2012 Groupe Bull divested itself of the Eagle System, selling it for the sum of 4 million euros to Nexa Technologies, a company run by a former Amesys CEO.\n\n"}
{"id": "4135179", "url": "https://en.wikipedia.org/wiki?curid=4135179", "title": "Growler (electrical device)", "text": "Growler (electrical device)\n\nA growler is an electrical device used for testing insulation of a motor for shorted coils. A growler consists of a coil of wire wrapped around an iron core and connected to a source of alternating current. When placed on the armature or stator core of a motor the growler acts as the primary of a transformer and the armature coils act as the secondary. A \"feeler\", a thin strip of steel (hacksaw blade) can be used as the short detector.\n\nThe alternating magnetic flux set up by the growler passes through the windings of the armature coil, generating an alternating voltage in the coil. A short in the coil creates a closed circuit that will act like the secondary coil of a transformer, with the growler acting like the primary coil. This will induce an alternating current in the shorted armature that will in turn cause an alternating magnetic field to encircle the shorted armature coil. A flat, broad, flexible piece of metal containing iron is used to detect the magnetic field generated by a shorted armature. A hacksaw blade is commonly used as a feeler. The alternating magnetic field induced by a shorted armature is strong at the surface of the armature, and when the feeler is lightly touched to the iron core of an armature winding, small currents are induced in the feeler that generate a third alternating magnetic field surrounding the feeler. \n\nWith the growler energized, the feeler is moved from slot to slot. When the feeler is moved over a slot containing the shorted coil, the alternating magnetic field will alternately attract and release the feeler, causing it to vibrate in synch with the alternating current. A strong vibration of the feeler accompanied by a growling noise indicated that the coil is shorted.\n\nAlong with the standard application the growler can be used:\n\n"}
{"id": "6313696", "url": "https://en.wikipedia.org/wiki?curid=6313696", "title": "Guizhou WZ-2000", "text": "Guizhou WZ-2000\n\nThe Guizhou WZ-2000, also known as the WuZhen-2000 and previously the WZ-9, is a multi-purpose \nUnmanned combat air vehicle (UCAV) developed by Guizhou Aviation Industry Group (GAIC) in the People's Republic of China.\n\nDevelopment on the WZ-2000 began in 1999. A mock up of the WZ-2000 was publicly displayed at the 2000 Zhuhai Airshow, with a more accurate model on display at the 2002 event. First flight was on 26 December 2003, and there is sketchy news that an improved version, possibly designated the WZ-2000B, was due to fly at the end of 2005.\n\nThe UAV is powered by a single WS-11 turbofan engine which sits on top of the empennage between the two V-shaped tail fins. The fins are canted at approximately 40°. The sensor package includes thermal imaging camera, synthetic aperture radar, with images transmitted via a satellite communications antenna in the nose bulge.\n\n"}
{"id": "16740988", "url": "https://en.wikipedia.org/wiki?curid=16740988", "title": "Hempcrete", "text": "Hempcrete\n\nHempcrete or Hemplime is bio-composite material, a mixture of hemp hurds (shives) and lime (possibly including natural hydraulic lime, sand, pozzolans) used as a material for construction and insulation. It is marketed under names like Hempcrete, Canobiote, Canosmose, and Isochanvre. Hempcrete is easier to work with than traditional lime mixes and acts as an insulator and moisture regulator. It lacks the brittleness of concrete and consequently does not need expansion joints. The result is a lightweight insulating material ideal for most climates as it combines insulation and thermal mass.\n\nHempcrete has been used in France since the early 1990’s to construct non-weight bearing insulating infill walls, as hempcrete does not have the requisite strength for constructing foundation and is instead supported by the frame. France continues to be an avid user of hempcrete; it is growing in popularity annually.\n\nLike other plant products, hemp absorbs CO from the atmosphere as it grows, retaining the carbon and releasing the oxygen. Theoretically 165 kg of carbon can be absorbed and locked up by 1 m of hempcrete wall during manufacture.\n\nThe typical compressive strength is around 1 MPa, around 1/20 that of residential grade concrete. It is a low density material and resistant to crack under movement thus making it highly suitable for use in earthquake-prone areas. Hempcrete walls must be used together with a frame of another material that supports the vertical load in building construction, as hempcrete's density is 15% that of traditional concrete.\n\nLimecrete, Ltd. (UK) reports a fire resistance rating of 1 hour per British/EU standards.\n\nIn the United States, permits are needed for the use of hemp in building. \n\n\n"}
{"id": "15040625", "url": "https://en.wikipedia.org/wiki?curid=15040625", "title": "Hydrogen valve", "text": "Hydrogen valve\n\nA hydrogen valve is a special type of valve that is used for hydrogen at very low temperatures or high pressures in hydrogen storage or for example hydrogen vehicles.\nHigh pressure ball valves up to 6000 psig (413 bar) at 250 degrees F (121 degrees C) and flow coefficients from 4.0 to 13.8.\n\nValves used in industrial hydrogen and oxygen applications, such as petrochemical processes, are often made of inconel.\n\n"}
{"id": "39416632", "url": "https://en.wikipedia.org/wiki?curid=39416632", "title": "I.am+", "text": "I.am+\n\ni.am+ is a Hollywood-based company founded by musician Will.i.am in 2013 with the mission of creating wearable products that combine fashion and technology. In 2016, i.am+ acquired Israeli machine learning company Sensiya.\n\nIn 2012 the i.am+ camera for the iPhone 4 \n\ndial is a SIM-enabled smartwatch currently available in the UK exclusively through Three. Featuring a voice-enabled AI named AneedA, the smartwatch is the first of its kind with a conversational operating system. The dial is also unique in that it does not need to be tethered to a smart phone and can send calls and SMS messages independently. Included with the dial is a music streaming service with over 20 million songs.\n\ni.am+ EPs are high-end Bluetooth headphones. The iconic circular form was designed to echo their namesake vinyl records. The EPs feature a woven fabric cable and a magnetic clip so they can be worn around the neck when not in use.\n\nThe EPs were replaced with the 2nd generation of bluetooth headphones (now called i.am+ BUTTONS). i.am+ BUTTONS launched in November 2016. \n\nIn July 2017, i.am+ purchased Wink from Flextronics for $38.7M. \n\nIn January 2018 i.am+ acquired Swedish earbuds startup Earin.\n\nIn October 2018, iam+ announced a new platform agnostic voice assistant called Omega.\n"}
{"id": "51002505", "url": "https://en.wikipedia.org/wiki?curid=51002505", "title": "IEEE Rebooting Computing", "text": "IEEE Rebooting Computing\n\nIEEE Rebooting Computing is a global initiative launched by IEEE that proposes to rethink the concept of computing through a holistic look at all aspects of computing, from the device itself to the user interface. As part of its work, IEEE Rebooting Computing provides access to various resources like conferences and educational events, feature and scholarly articles, reports, and videos.\n\nIEEE Future Directions Committee established an \"IEEE Rebooting Computing\" working group in late 2012 with the broad vision of \"rebooting\" the entire field of computer technology. The activities of this working group are carried out by the IEEE Rebooting Computing Committee, a team of volunteers from ten participating IEEE Societies and Councils, in conjunction with IEEE Future Directions staff members.\n\nThe term \"rebooting computing\" was coined by IEEE Life Fellow, Peter Denning, as part of an early U.S. National Science Foundation-sponsored project focused on revamping computer education.\n\nIn order to achieve its goal of rebooting computing, IEEE Rebooting Computing hosted four invitation-only summits between 2013 and 2015 in Washington, D.C., and Santa Cruz, California. These summits addressed the future of computing from a holistic point of view.\n\nIn 2014, IEEE Rebooting Computing adopted its new logo, consisting of an exploding infinity symbol. The logo is intended to suggest the absence of limits for future computing technology.\n\nIEEE Rebooting Computing announced the signing of a Memorandum of Understanding (MOU) with the International Technology Roadmap for Semiconductors (ITRS) in March 2015. This led in May 2016 to the formation of the \"IEEE International Roadmap for Devices and Systems\" (IRDS), which incorporated the previous mission of ITRS in semiconductor device fabrication and expanded it to encompass alternative technologies, computer architectures, and system applications.\n\nIn September 2015, IEEE Rebooting Computing announced support for the National Strategic Computing Initiative (NSCI). Established under Executive Order 13072 issued by U.S. President Barack Obama in July 2015, the NSCI calls for a coordinated Federal strategy in high-performance computing (HPC) research, development, and deployment.\n\nIn October 2015, the National Nanotechnology Initiative (NNI), an interagency program of the U.S. government, announced a \"Nanotechnology-Inspired Grand Challenge in Future Computing\". A key document cited by NNI as part of this grand challenge is a white paper, co-sponsored by IEEE Rebooting Computing and ITRS, entitled \"Sensible Machines\".\n\nIEEE Rebooting Computing aims to help return the computing industry to exponential computer-performance scaling, which stalled in 2005 due to energy inefficiencies of CMOS-based classical computers. Historically, computer processing power doubled every 18 months due to increasing densities of transistors per semiconductor unit. To alleviate challenges brought on by limitations in computer architectures and sustain regular processing performance gains, there was a move toward instruction-level parallelism and superscalar microprocessors. However, with rising costs associated with greater power consumption brought on by this approach signaling the end of Moore's Law, IEEE introduced the IEEE Rebooting Computing initiative.\n\nIncorporating three fundamental pillars of rebooting computing, including energy efficiency, security, and Human Computer Interface (HCI), the initiative seeks to overcome setbacks and challenges relating to the deceleration of computational power and capacity. In turn, these efforts may also be applied in other technology sectors, such as the Internet of Things.\n\nWith the goal of identifying new directions in computing and aiding industry in returning to historical exponential scaling of computer performance, IEEE Rebooting Computing encompasses a variety of activities, products, and services. Among these efforts are an online web portal, technical community, publications, conferences, and events. IEEE Rebooting Computing also maintains a collaborative partnership with IRDS, as well as responding to and participating in national and international initiatives, the NSCI and the \"Nanotechnology Inspired Grand Challenge for Future Computing\".\n\nThe web portal is the primary online home for IEEE Rebooting Computing. The website provides relevant news, information, and resources to users, such as articles authored by IEEE experts and third-party publications. It also includes access to a list of both IEEE-sponsored and general industry conferences and events, videos, and historical data from IEEE Rebooting Computing's past summits.\n\nIEEE Technical Communities are virtual communities for practitioners, subject matter experts, researchers, and other technology professionals interested in specific topic areas. Open to any interested individual, the IEEE Rebooting Computing Technical Community serves as a venue for the distribution and dissemination of news, announcements, and other information from those societies and councils taking part in the IEEE Rebooting Computing initiatiive. IEEE membership is not required to become a member of the IEEE Rebooting Computing Technical Community.\n\nIEEE Rebooting Computing sponsors, co-sponsors, and takes part in a variety of technology conferences and events worldwide. Conference and event programming is designed to stimulate discussion of existing and emerging technologies, including challenges, benefits, and opportunities. Typically lasting anywhere from a single day to a week or more, conference and event programming generally encompasses keynote addresses, panel discussions, paper presentations, poster sessions, tutorials, and workshops in one or more tracks.\n\nDuring its first several years, the initiative's flagship event series was its Rebooting Computing Summits. The inaugural IEEE Rebooting Computing Summit was held in December 2013 in Washington, D.C. The event drew business and industry, government, and academic representatives both from the U.S. and internationally for a variety of plenary lectures and brainstorming sessions.\n\nBased on the first event, a second IEEE Rebooting Computing Summit was held in May 2014 in Santa Cruz, California. Following a similar format to the first summit, a group of invited business and trade, academia, and government experts took part in discussing neuromorphic engineering, approximate computing, and adiabatic / reversible computing.\n\nWith the first two summits serving as the event's basis, IEEE Rebooting Computing held a third summit in October 2014, in Scotts Valley, California. The theme for the third summit was \"Rethinking Structures of Computation\", and focused on the topics of parallel computing, security, approximation, and Human-Computer Interaction. As part of the event, attendees took part in plenary talks, a poster session, and heard details of a new government initiative in future computing research.\n\nA fourth IEEE Rebooting Computing Summit (RCS4), with a theme of \"Roadmapping the Future of Computing: Discovering How We May Compute\" was held in December 2015, in Washington D.C. The event included plenary talks and breakout groups in the three tracks of \"Probabilistic/Approximate Computing\", \"Neuromorphic Computing\", and \"Beyond CMOS/3D Computing\", with a fourth track on \"Superconducting Computing\". The summit also hosted speakers from other programs promoting future computing, both governmental and industrial, including DARPA, Intelligence Advanced Research Projects Activity (IARPA), ITRS, NSCI, Office of Science and Technology Policy (OSTP), and Semiconductor Research Corporation.\n\nA larger, open conference, the IEEE International Conference on Rebooting Computing (ICRC 2016), was held in October 2016, in San Diego, California. The goal of ICRC 2016 was to discover and foster novel methodologies to reinvent computing technology, including new materials and physics, devices and circuits, system and network architectures, and algorithms and software. Proceedings of the event have been published by IEEE, and videos of many of the presentations are available online. The second conference in this series, ICRC 2017, was held in November 2017 in Washington, DC, as part of IEEE Rebooting Computing Week. A third conference in this series, ICRC 2018, is being planned for November 2018.\n\nIn November 2017, IEEE Rebooting Computing also sponsored a distinct one-day summit, following ICRC, which addressed similar topics but with a somewhat different focus and audience. This Industry Summit featured plenary presentations by industry, government, and academic leaders on what we can expect for new computer technologies in coming decades. For example, this featured a new public announcement from IBM Research on a breakthrough in quantum computing technology. Other topics of interest included artificial intelligence, machine learning, memory-driven computing, and heterogeneous computing.\n\nIn June 2015, IEEE Rebooting Computing held the first-ever Low-Power Image Recognition Competition (LPIRC). Held as a one-day workshop as during the 2015 Design Automation Conference in San Francisco, California, the competition aimed to assess the state of low-power approaches to object detection in images. The competition fielded competitors from four different countries and included teams from Carnegie Mellon, Rice University, and Tsinghua University and Huawei.\n\nBefore the competition, training data was released for detection from the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC). Source code of the referee system was released to the public in March 2015. For the competition, an intranet was established for the contestants to retrieve provided image files from and return answers to the competition's referee system. Teams were given 10 minutes to process images, which were ranked by detection accuracy and energy usage.\n\nA second LPIRC was held during the June 2016 Design Automation Conference in Austin, Texas. A third LPIRC was held in July 2017 as part of the Computer Vision and Pattern Recognition Conference in Honolulu, Hawaii. A fourth LPIRC is being planned for 2018.\n\nAs part of the initiative's work, IEEE Rebooting Computing members and societies regularly publish papers, manuscripts, journals and magazines, and other documents. Among the various IEEE publications IEEE Rebooting Computing contributes to or features articles from on its web portal are \"Computer Magazine\"; \"IEEE Journal on Emerging and Selected Topics in Circuits and Systems\"; \"IEEE Journal on Exploratory Solid-State Computational Devices and Circuits\"; \"IEEE Solid-State Circuits Magazine\"; \"IEEE Spectrum\"; and \"Proceedings of the IEEE\".\n\nIn December 2015, \"Computer Magazine\" published a special issue on rebooting computing, with members of the IEEE Rebooting Committee as guest editors and contributors. In November 2016, the Italian online magazine \"Mondo Digitale\" published an article entitled \"Rebooting Computing: Developing a Roadmap for the Future of the Computer Industry.\" In March 2017, \"Computing in Science and Engineering\" published a special issue on \"The End of Moore's Law\", addressing alternative approaches to maintaining exponential growth in performance, even as classic device scaling may be ending.\n\nIEEE Rebooting Computing also contributes to a variety of trade publications and news outlets, such as \"EE Times\" and \"Scientific Computing\".\n\nIEEE Rebooting Computing has multi-society participation from a cross-section of IEEE societies with interest in numerous aspects of computing, including circuits and systems design; architectures; design automation; magnetics; nanotechnology; reliability; and superconductors.\n\nIEEE Societies and Councils taking part in the IEEE Rebooting Computing initiative are:\n\nIEEE Rebooting Computing has established a collaborative relationship with the ITRS. The two organizations initiated an exchange of information in 2014. Following the signing of a formal collaboration agreement, IEEE Rebooting Computing and ITRS arranged and held joint international workshops in 2015 with the objective of identifying computer performance scaling challenges and establishing a roadmap to successfully restart computer performance scaling. IEEE Rebooting Computing further collaborated with ITRS on a new effort, known as ITRS 2.0, that extends beyond traditional Moore's Law scaling of chips to include roadmaps covering systems and applications.\n\nITRS Chairman Paolo Gargini said, \"The ITRS shares IEEE Rebooting Computing’s mission to restore computing to its historic exponential performance scaling trends so our society and future societies can benefit. Our agreement will ensure we help fundamentally shift the computer industry’s focus, resources, time and attention on to new possibilities for computational performance.\"\n\nOn May 4, 2016, IEEE announced the launch of the \"International Roadmap for Devices and Systems\" (IRDS), operating as part of the IEEE Standards Association's (IEEE-SA) Industry Connections program. IRDS is sponsored by IEEE Rebooting Computing in consultation with the IEEE Computer Society and ITRS. IRDS will provide guidance on future trends in computer systems, architectures, software, chips, and other components across the entire computer industry, and is modeled on ITRS roadmaps that have previously guided the semiconductor industry during the Moore’s Law era. The first IRDS Roadmap is scheduled for release in the first quarter of 2018 on the IRDS Web Portal. This follows preliminary presentation of the roadmap components at the IRDS Conference in Washington DC in November 2017, as part of the Rebooting Computing Week set of events.\n\nThrough its summits, other educational efforts, and engagement with government, IEEE Rebooting Computing initiative has begun to influence both the technology industry and national policy efforts. The initiative plans to extend this current sphere of influence by creating and releasing the IRDS \"Roadmap of Future Computing\". This proposed roadmap will include development of performance benchmarks and standards for new classes of computer systems.\n\nAddressing roadblocks in future high-performance computing, also known as exascale computing, is a key area of focus for IEEE Rebooting Computing. The initiative has been actively pursuing and aiding the industry in making progress toward possible solutions such as specialized chip architectures, millivolt switches, and 3D integrated circuits, as noted by Dr. Erik DeBenedictis of Sandia National Laboratories in \"Power Problems Threaten to Strangle Exascale Computing\".\n\nIn February 2015, IEEE Rebooting Computing Senior Program Director Bichlien Hoang and co-author Sin-Kuen Hawkins were received a \"Best Presentation Award\" for their paper, \"How Will Rebooting Computing Help IoT\". Presented at the 18th International Conference on Intelligence in Next Generation Networks (ICIN 2015) in Paris, France, the paper described IEEE Rebooting Computing's approach to addressing technical challenges generated by IoT other key computing trends.\n\nMedia coverage of IEEE Rebooting Computing's efforts has increased. In May 2016, a New York Times feature article on the technological and economic implications of the ending of Moore’s Law quoted IEEE Rebooting Co-Chair, Professor Thomas M. Conte of the Georgia Institute of Technology as saying, \"The end of Moore’s Law is what led to this. Just relying on the semiconductor industry is no longer enough. We have to shift and punch through some walls and break through some barriers.\"\n\nAmong other publications reporting on IEEE Rebooting Computing activities are \"EE Times\"; \"HPCWire\"; \"IEEE Spectrum\"; \"Inside HPC\"; \"Scientific Computing\"; \"SiliconANGLE\"; and \"VR World\".\n\n\n"}
{"id": "44236305", "url": "https://en.wikipedia.org/wiki?curid=44236305", "title": "Jean Dollfus", "text": "Jean Dollfus\n\nJean Dollfus (September 25 1800 – 21 May 1887) was a French industrialist who grew a textile company, Dollfus-Meig et Cie (D.M.C.), in Mulhouse. Dollfus was a leading figure in a philanthropic society which constructed a company town that sold houses at cost to the town's workers. Dollfus also helped publish an encyclopedia of needlework.\n\nJean Dollfus was born in Mulhouse, France, in 1800, the son of Daniel Dollfus and Anne Marie Mieg. He was born into a family that owned a textile business established in the 18th century. His parents wrote their surname as Dollfus-Mieg, and Daniel used this name to re-brand his uncle's textile company as Dollfus-Mieg & Compagnie, or D.M.C., in 1800. Whilst studying in Leeds, Jean Dollfus found out about Mercerised cotton. This was a new technique of chemically treating cotton to increase not only its strength but also its appearance, a discovery that he would apply to the textile business.\n\nDollfus was a leading member of the \"Societe Industrielle de Mulhouse\". In 1851 he published a letter to them where he advocated free trade. Dollfus noted that the French cotton trade was stagnant whilst between 1830 and 1850 the British had doubled their consumption of raw cotton. Dollfus believed that taxes levied by the French in order to protect French workers were in fact preventing industrial expansion.\nIn 1852 the \"Societe Industrielle de Mulhouse\" began construction on \"cités ouvrières\", or worker's towns. This philanthropic endeavor was not led by a single company but involved numerous leading citizens, although Dollfus was credited with leading the work. Henry Roberts is credited with the inspiration and Emile Muller drew up the designs. Construction continued for the next 45 years. The development was so novel and admirable that Napoleon III made 30,000 francs available to assist the project and Dollfus became the Mayor of Mulhouse from 1863 to 1869. By 1885 the society had constructed 1,060 workers' houses in Mulhouse and had sold 775 of them at cost to their occupants. Each of these occupants, after about 15 years, owned a small house with a small garden.\nDollfus commissioned Pierre-Auguste Renoir to paint a copy of a painting by Delacroix. The painting was completed by 1876 and was not a true copy, as Renoir had adapted the colours and brush work to an impressionist style. This painting remained in the Dollfus family until 1911 and it is now housed in the Worcester Art Museum in Massachusetts.\n\nIn 1884 Dollfus signed an agreement with Thérèse de Dillmont, a textile teacher and writer. She came from Vienna to work with him. She wrote an that was translated into 17 languages. This book had product placement as it recommended products from Dollfus' company, and established Dollfus' company as a publisher of textile patterns. After Dillmont died, the brand was continued and in 2004 a Russian translation of Dillmont's book was published.\n\nDollfus died in 1887 and his business continued under the leadership of his grandchildren.\n"}
{"id": "15437100", "url": "https://en.wikipedia.org/wiki?curid=15437100", "title": "List of industry trade groups in the United States", "text": "List of industry trade groups in the United States\n\nThis is a list of notable industry trade groups in the United States.\n"}
{"id": "96558", "url": "https://en.wikipedia.org/wiki?curid=96558", "title": "Maxwell's demon", "text": "Maxwell's demon\n\nIn the philosophy of thermal and statistical physics, Maxwell's demon is a thought experiment created by the physicist James Clerk Maxwell in 1867 in which he suggested how the second law of thermodynamics might hypothetically be violated. In the thought experiment, a demon controls a small door between two chambers of gas. As individual gas molecules approach the door, the demon quickly opens and shuts the door so that only fast molecules are passed into one of the chambers, while only slow molecules are passed into the other. Because faster molecules are hotter, the demon's behaviour causes one chamber to warm up and the other to cool down, thereby decreasing entropy and violating the second law of thermodynamics. This thought experiment has provoked debate and theoretical work on the relation between thermodynamics and information theory extending to the present day, with a number of scientists arguing that theoretical considerations rule out any practical device violating the second law in this way.\n\nThe thought experiment first appeared in a letter Maxwell wrote to Peter Guthrie Tait on 11 December 1867. It appeared again in a letter to John William Strutt in 1871, before it was presented to the public in Maxwell's 1872 book on thermodynamics titled \"Theory of Heat\".\n\nIn his letters and books, Maxwell described the agent opening the door between the chambers as a \"finite being\". William Thomson (Lord Kelvin) was the first to use the word \"demon\" for Maxwell's concept, in the journal \"Nature\" in 1874, and implied that he intended the mediating, rather than malevolent, connotation of the word.\n\nThe second law of thermodynamics ensures (through statistical probability) that two bodies of different temperature, when brought into contact with each other and isolated from the rest of the Universe, will evolve to a thermodynamic equilibrium in which both bodies have approximately the same temperature. The second law is also expressed as the assertion that in an isolated system, entropy never decreases.\n\nMaxwell conceived a thought experiment as a way of furthering the understanding of the second law. His description of the experiment is as follows:\n\nIn other words, Maxwell imagines one container divided into two parts, \"A\" and \"B\". Both parts are filled with the same gas at equal temperatures and placed next to each other. Observing the molecules on both sides, an imaginary demon guards a trapdoor between the two parts. When a faster-than-average molecule from \"A\" flies towards the trapdoor, the demon opens it, and the molecule will fly from \"A\" to \"B\". Likewise, when a slower-than-average molecule from \"B\" flies towards the trapdoor, the demon will let it pass from \"B\" to \"A\". The average speed of the molecules in \"B\" will have increased while in \"A\" they will have slowed down on average. Since average molecular speed corresponds to temperature, the temperature decreases in \"A\" and increases in \"B\", contrary to the second law of thermodynamics. A heat engine operating between the thermal reservoirs \"A\" and \"B\" could extract useful work from this temperature difference.\n\nThe demon must allow molecules to pass in both directions in order to produce only a temperature difference; one-way passage only of faster-than-average molecules from \"A\" to \"B\" will cause higher temperature and pressure to develop on the \"B\" side.\n\nSeveral physicists have presented calculations that show that the second law of thermodynamics will not actually be violated, if a more complete analysis is made of the whole system including the demon. The essence of the physical argument is to show, by calculation, that any demon must \"generate\" more entropy segregating the molecules than it could ever eliminate by the method described. That is, it would take more thermodynamic work to gauge the speed of the molecules and selectively allow them to pass through the opening between \"A\" and \"B\" than the amount of energy gained by the difference of temperature caused by this.\n\nOne of the most famous responses to this question was suggested in 1929 by Leó Szilárd, and later by Léon Brillouin. Szilárd pointed out that a real-life Maxwell's demon would need to have some means of measuring molecular speed, and that the act of acquiring information would require an expenditure of energy. Since the demon and the gas are interacting, we must consider the total entropy of the gas and the demon combined. The expenditure of energy by the demon will cause an increase in the entropy of the demon, which will be larger than the lowering of the entropy of the gas.\n\nIn 1960, Rolf Landauer raised an exception to this argument. He realized that some measuring processes need not increase thermodynamic entropy as long as they were thermodynamically reversible. He suggested these \"reversible\" measurements could be used to sort the molecules, violating the Second Law. However, due to the connection between thermodynamic entropy and information entropy, this also meant that the recorded measurement must not be erased. In other words, to determine whether to let a molecule through, the demon must acquire information about the state of the molecule and either discard it or store it. Discarding it leads to immediate increase in entropy but the demon cannot store it indefinitely: In 1982, Charles Bennett showed that, however well prepared, eventually the demon will run out of information storage space and must begin to erase the information it has previously gathered. Erasing information is a thermodynamically irreversible process that increases the entropy of a system. Although Bennett had reached the same conclusion as Szilard’s 1929 paper, that a Maxwellian demon could not violate the second law because entropy would be created, he had reached it for different reasons. Regarding Landauer's principle, the minimum energy dissipated by deleting information was experimentally measured by Eric Lutz \"et al.\" in 2012. Furthermore, Lutz \"et al.\" confirmed that in order to approach the Landauer's limit, the system must asymptotically approach zero processing speed.\n\nJohn Earman and John D. Norton have argued that Szilárd and Landauer's explanations of Maxwell's demon begin by assuming that the second law of thermodynamics cannot be violated by the demon, and derive further properties of the demon from this assumption, including the necessity of consuming energy when erasing information, etc. It would therefore be circular to invoke these derived properties to defend the second law from the demonic argument. Bennett later acknowledged the validity of Earman and Norton's argument, while maintaining that Landauer's principle explains the mechanism by which real systems do not violate the second law of thermodynamics.\n\nAlthough the argument by Landauer and Bennett only answers the consistency between the second law of thermodynamics and the whole cyclic process of the entire system of a Szilard engine (a composite system of the engine and the demon), a recent approach based on the non-equilibrium thermodynamics for small fluctuating systems has provided deeper insight on each information process with each subsystem. From this viewpoint, the measurement process is regarded as a process where the correlation (mutual information) between the engine and the demon increases, and the feedback process is regarded as a process where the correlation decreases. If the correlation changes, thermodynamic relations as the second law of thermodynamics and the fluctuation theorem for each subsystem should be modified, and for the case of external control a second-law like inequality and a generalized fluctuation theorem with mutual information are satisfied. These relations suggest that we need extra thermodynamic cost to increase correlation (measurement case), and in contrast we can apparently violate the second law up to the consumption of correlation (feedback case). For more general information processes including biological information processing, both inequality and equality with mutual information hold.\n\nReal-life versions of Maxwellian demons occur, but all such \"real demons\" have their entropy-lowering effects duly balanced by increase of entropy elsewhere. Molecular-sized mechanisms are no longer found only in biology; they are also the subject of the emerging field of nanotechnology. Single-atom traps used by particle physicists allow an experimenter to control the state of individual quanta in a way similar to Maxwell's demon.\n\nIf hypothetical mirror matter exists, Zurab Silagadze proposes that demons can be envisaged, \"which can act like perpetuum mobiles of the second kind: extract heat energy from only one reservoir, use it to do work and be isolated from the rest of ordinary world. Yet the Second Law is not violated because the demons pay their entropy cost in the hidden (mirror) sector of the world by emitting mirror photons.\"\n\nIn the February 2007 issue of \"Nature\", David Leigh, a professor at the University of Edinburgh, announced the creation of a nano-device based on the Brownian ratchet popularized by Richard Feynman. Leigh's device is able to drive a chemical system out of equilibrium, but it must be powered by an external source (light in this case) and therefore does not violate thermodynamics.\n\nPreviously, researchers including Nobel Prize winner Fraser Stoddart, created ring-shaped molecules called rotaxanes which could be placed on an axle connecting two sites, \"A\" and \"B\". Particles from either site would bump into the ring and move it from end to end. If a large collection of these devices were placed in a system, half of the devices had the ring at site \"A\" and half at \"B\", at any given moment in time.\n\nLeigh made a minor change to the axle so that if a light is shone on the device, the center of the axle will thicken, restricting the motion of the ring. It only keeps the ring from moving, however, if it is at \"A\". Over time, therefore, the rings will be bumped from \"B\" to \"A\" and get stuck there, creating an imbalance in the system. In his experiments, Leigh was able to take a pot of \"billions of these devices\" from 50:50 equilibrium to a 70:30 imbalance within a few minutes.\n\nIn 2009 Mark G. Raizen developed a laser atomic cooling technique which realizes the process Maxwell envisioned of sorting individual atoms in a gas into different containers based on their energy. The new concept is a one-way wall for atoms or molecules that allows them to move in one direction, but not go back. The operation of the one-way wall relies on an irreversible atomic and molecular process of absorption of a photon at a specific wavelength, followed by spontaneous emission to a different internal state. The irreversible process is coupled to a conservative force created by magnetic fields and/or light. Raizen and collaborators proposed using the one-way wall in order to reduce the entropy of an ensemble of atoms. In parallel, Gonzalo Muga and Andreas Ruschhaupt independently developed a similar concept. Their \"atom diode\" was not proposed for cooling, but rather for regulating the flow of atoms. The Raizen Group demonstrated significant cooling of atoms with the one-way wall in a series of experiments in 2008. Subsequently, the operation of a one-way wall for atoms was demonstrated by Daniel Steck and collaborators later in 2008. Their experiment was based on the 2005 scheme for the one-way wall, and was not used for cooling. The cooling method realized by the Raizen Group was called \"single-photon cooling\", because only one photon on average is required in order to bring an atom to near-rest. This is in contrast to other laser cooling techniques which use the momentum of the photon and require a two-level cycling transition.\n\nIn 2006, Raizen, Muga, and Ruschhaupt showed in a theoretical paper that as each atom crosses the one-way wall, it scatters one photon, and information is provided about the turning point and hence the energy of that particle. The entropy increase of the radiation field scattered from a directional laser into a random direction is exactly balanced by the entropy reduction of the atoms as they are trapped by the one-way wall.\n\nThis technique is widely described as a \"Maxwell's demon\" because it realizes Maxwell's process of creating a temperature difference by sorting high and low energy atoms into different containers. However, scientists have pointed out that it is not a true Maxwell's demon in the sense that it does not violate the second law of thermodynamics; it does not result in a net decrease in entropy and cannot be used to produce useful energy. This is because the process requires more energy from the laser beams than could be produced by the temperature difference generated. The atoms absorb low entropy photons from the laser beam and emit them in a random direction, thus increasing the entropy of the environment.\n\nIn 2014, Pekola et al. demonstrated an experimental realization of a Szilárd engine. Only a year later and based on an earlier theoretical proposal, the same group presented the first experimental realization of an autonomous Maxwell’s demon, which extracts microscopic information from a system and reduces its entropy by applying feedback. The demon is based on two capacitively coupled single-electron devices, both integrated on the same electronic circuit. The operation of the demon is directly observed as a temperature drop in the system, with a simultaneous temperature rise in the demon arising from the thermodynamic cost of generating the mutual information. In 2016, Pekola et al. demonstrated a proof-of-principle of an autonomous demon in coupled single-electron circuits, showing a way how to cool critical elements in a circuit with information as a fuel. Pekola et al. have also proposed that a simple qubit circuit, e.g., made of a superconducting circuit, could provide a basis to study a quantum Szilard's engine.\n\nDaemons in computing, generally processes that run on servers to respond to users, are named for Maxwell's demon.\n\nHistorian Henry Brooks Adams in his manuscript \"The Rule of Phase Applied to History\" attempted to use Maxwell's demon as a historical metaphor, though he misunderstood and misapplied the original principle. Adams interpreted history as a process moving towards \"equilibrium\", but he saw militaristic nations (he felt Germany pre-eminent in this class) as tending to reverse this process, a Maxwell's demon of history. Adams made many attempts to respond to the criticism of his formulation from his scientific colleagues, but the work remained incomplete at Adams' death in 1918. It was only published posthumously.\n\nSociologist Pierre Bourdieu incorporated Maxwell's demon into his work, \"Raisons Pratiques\" as a metaphor for the socioeconomic inequality among students, as maintained by the school system, the economy, and families.\n\nA machine powered by Maxwell's demon plays a role in Thomas Pynchon's novel \"The Crying of Lot 49\".\n\nThe demon is mentioned several times in \"The Cyberiad\", a series of short stories by the noted science fiction writer Stanisław Lem. In the book the demon appears both in its original form and in a modified form where it uses its knowledge of all particles in the box in order to surmise general (but unfocused and random) facts about the rest of the universe.\n\nThe demon is implied in Larry Niven's short story \"Unfinished Story Nr 2\", within the context of a world of magic, depending on local concentrations of 'manna', a prerequirement for magic such that magic is no longer possible after manna has been locally depleted.\n\n\n"}
{"id": "7081715", "url": "https://en.wikipedia.org/wiki?curid=7081715", "title": "Micropower", "text": "Micropower\n\nMicropower describes the use of very small electric generators and prime movers or devices to convert heat or motion to electricity, for use close to the generator. The generator is typically integrated with microelectronic devices and produces \"several watts of power or less.\" These devices offer the promise of a power source for portable electronic devices which is lighter weight and has a longer operating time than batteries.\n\nThe components of any turbine engine — the gas compressor, the combustion chamber, and the turbine rotor — are fabricated from etched silicon, much like integrated circuits. The technology holds the promise of ten times the operating time of a battery of the same weight as the micropower unit, and similar efficiency to large utility gas turbines. Researchers at Massachusetts Institute of Technology have thus far succeeded in fabricating the parts for such a micro turbine out of six etched and stacked silicon wafers, and are working toward combining them into a functioning engine about the size of a U.S. quarter coin.\n\nResearchers at Georgia Tech have built a micro generator 10 mm wide, which spins a magnet above an array of coils fabricated on a silicon chip. The device spins at 100,000 revolutions per minute, and produces 1.1 watts of electrical power, sufficient to operate a cell phone. Their goal is to produce 20 to 50 watts, sufficient to power a laptop computer.\n\nScientists at Lehigh University are developing a hydrogen generator on a silicon chip that can convert methanol, diesel, or gasoline into fuel for a microengine or a miniature fuel cell.\n\nProfessor Sanjeev Mukerjee of Northeastern University's chemistry department is developing fuel cells for the military that will burn hydrogen to power portable electronic equipment, such as night vision goggles, computers, and communication equipment. In his system, a cartridge of methanol would be used to produce hydrogen to run a small fuel cell for up to 5,000 hours. It would be lighter than rechargeable batteries needed to provide the same power output, with a longer run time. Similar technology could be improved and expanded in future years to power automobiles.\n\nThe National Academies' National Research Council recommended in a 2004 report that the U.S. Army should investigate such micropower sources for powering electronic equipment to be carried by soldiers in the future, since batteries sufficient to power the computers, sensors, and communications devices would add considerable weight to the burden of infantry soldiers.\n\nThe Future Warrior Concept of the U.S. Army envisions a 2- to 20-watt micro turbine fueled by a liquid hydrocarbon being used to power communications and wearable heating/cooling equipment for up to six days on 10 ounces of fuel.\n\nProfessor Orest Symko of the University of Utah physics department and his students developed Thermal Acoustic Piezo Energy Conversion (TAPEC), devices of a cubic inch (16 cubic centimeters), or so, which convert waste heat into acoustic resonance and then into electricity. It would be used to power microelectromechanical systems, or MEMS. The research was funded by the U.S. Army. Symko was to present a paper at the Acoustical Society of America. June 8, 2007. Researchers at MIT developed the first micro-scale piezoelectric energy harvester using thin film PZT in 2005. Arman Hajati and Sang-Gook Kim invented the Ultra Wide-Bandwidth micro-scale piezoelectric energy harvesting device by exploiting the nonlinear stiffness of a doubly clamped microelectromechanical systems (MEMS) resonator. The stretching strain in a doubly clamped beam shows a nonlinear stiffness, which provides a passive feedback and results in amplitude-stiffened Duffing mode resonance.\n\nProfessor Zhong Lin Wang of the Georgia Institute of Technology said his team of investigators had developed a \"nanometer-scale generator ... based on arrays of vertically aligned zinc oxide nanowires that move inside a \"zigzag\" plate electrode.\" Built into shoes, it could generate electricity from walking to power small electronic devices. It could also be powered by blood flow to power biomedical devices. Per an account of the device which appeared in the journal Science, bending of the zinc oxide nanowire arrays produces an electric field by the piezoelectric properties of the material. The semiconductor properties of the device create a Schottky barrier with rectifying capabilities. The generator is estimated to be 17% to 30% efficient in converting mechanical motion into electricity. This could be used to power biomedical devices that have wireless transmission capabilities for data and control. A later development was to grow hundreds of such nanowires on a substrate that functioned as an electrode. On top of this was placed a silicon electrode covered with a series of platinum ridges. Vibration of the top electrode caused the generation of direct current. A report by Wang was to appear in the August 8, 2007 issue of the journal \"Nano Letters,\" saying that such devices could power implantable biomedical devices. The device would be powered by flowing blood or a beating heart. It could function while immersed in body fluids, and would get its energy from ultrasonic vibrations. Wang expects that an array of the devices could produce 4 watts per cubic centimeter. Goals for further development are to increase the efficiency of the array of nanowires, and to increase the lifetime of the device, which as of April 2007 was only about one hour. By November 2010 Wang and his team were able to produce 3 volts of potential and as much as 300 nanoamperes of current, an output level 100 times greater than was possible a year earlier, from an array measuring about 2 cm by 1.5 cm.\n\nThe windbelt is a micropower technology invented by Shawn Frayne. It is essentially an aeolian harp, except that it exploits the motion of the string produced by aeroelastic flutter to create a physical oscillation that can be converted to electricity. It avoids the losses inherent in rotating wind powered generators. Prototypes have produced 40 milliwatts in a 16 km/h wind. Magnets on the vibrating membrane generate currents in stationary coils.\n\nPiezoelectric nanofibers in clothing could generate enough electricity from the wearer's body movements to power small electronic devices, such as iPods or some of the electronic equipment used by soldiers on the battlefield, based on research by University of California, Berkeley Professor Liwei Lin and his team. One million such fibers could power an iPod, and would be altogether as large as a grain of sand. Researchers at Stanford University are developing \"eTextiles\" — batteries made of fabric — that might serve to store power generated by such technology.\n\nThermal resonator technology allows generation of power from the daily change of temperature, even when there is no instantaneous temperature difference as needed for thermoelectric generation, and no sunlight as needed for photovoltaic genĚeration. A phase change material such as octadecane is selected which can change from solid to liquid when the ambient temperature changes a few degrees celsius. In a small demonstration device created by chemical engineering professor Michael Strano and seven others at MIT, a 10 degree celsius daily change produced 350 millivolts and 1.3 milliwatts. The power levels envisioned could power sensors and communication devices.\n\n\n"}
{"id": "14546679", "url": "https://en.wikipedia.org/wiki?curid=14546679", "title": "Mound system", "text": "Mound system\n\nA mound system is an alternative to the traditional rural septic system drain field. The mound system is an engineered drain field used in areas where septic systems are more prone to failure due to having extremely permeable or impermeable soils, soil with shallow cover over porous bedrock, and soils that have a high seasonal water table.\n\nThe primary waste liquids cleaning and purification action in a drain field is performed by a biofilm in the loose fill surrounding the perforated drain tile. If the soil permeability is too low, liquid is not absorbed fast enough, resulting in surface ponding of unsanitary liquids. If the soil permeability is too high, or is exposed to fractured bedrock, the wastewater quickly penetrates down to the water table before the biofilm has time to purify the water leading to contamination of the aquifer. In either situation, the mound system provides an ideal habitat for the biofilm and has the correct permeability to assure slow absorption of effluent into the mound before exiting as purified water into the surrounding environment.\n\nThe mound system was originally designed in the 1940s by the North Dakota College of Agriculture. It was known as the Nodak Disposal System. In 1976 the University of Wisconsin studied the mound systems under the university's Waste Management Project. This project published the first design manual indicating the appropriate site conditions and design criteria for mounds. In 2000 a new manual was released.\n\nThe mound system includes a septic tank, a dosing chamber, and a mound. Waste from homes is sent to the septic tank where the solid fraction settles to the bottom of the tank. The effluent is sent to a second tank called a \"dosing chamber,\" from which the effluent is distributed to the mound at a metered rate (in \"doses\"). Wastewater is partially treated as it moves through the mound sand. Final treatment and disposal occurs in the soil beneath the mound. The mound system can also better handle the effluent since it doesn't all come into the mound at once allowing it to clean the effluent better which helps to keep the system from failing.\n\nThe absorption mound is built in layers at different depths. The depths of these layers are determined by the depth of the limiting layer of the soil, which may be a seasonal water table, bedrock, fragipan, or glacial till. Standards created by Ohio State University state that 24 inches of natural soil should be above the limiting layer in the soil. A layer of specifically sized sand is placed on top of the natural soil so that the natural soil and sand reach a depth of 4 feet. The distribution pipes that are fed by the dosing chamber are placed on top of the sand in layers of gravel. Then construction fabric and soil are placed on top of the gravel to help keep the pipes from freezing. The top layer of soil also allows the mound to be planted with grass or non-woody plants in order to control erosion \n\nWhen installing a mound system, the soil in the area where the mound is to be placed will not be compacted or disturbed. Any trees that are on the area are cut away at ground level, and the roots and stumps left in place. The surface of the area for the mound is then roughened up with a chisel plough. This prepares the area for the sand. Work is done from upslope of the mound area so that the ground down slope of the mound does not get compacted. Tyler tables are used to help determine the area of a mound.\nTime dosing is an important aspect in the functioning of the mound system. Research done by Darby on sand filters showed that short frequent doses of effluent onto sand filters with orifices that are closely spaced helps to improve effluent quality. By contrast, demand dosing releases large amounts of effluent at once, which passes through the sand very rapidly. This does not give the biota the proper amount of time to clean the effluent.\n\n\n"}
{"id": "34728514", "url": "https://en.wikipedia.org/wiki?curid=34728514", "title": "NACA Report No. 964", "text": "NACA Report No. 964\n\nNACA Report No. 964 - The effects of variations in Reynolds number between 3.0 x 10 and 25.0 x 10 upon the aerodynamic characteristics of a number of NACA 6-series airfoil sections was published by the United States National Advisory Committee for Aeronautics in 1950. It contained a series of graphs showing the resulting lift and drag of several NACA 6-series airfoil sections from tests performed in a variable-density wind tunnel, in which the Reynolds number (RN) was set at three different values.\n\nResults are presented of an investigation made to determine the two-dimensional lift and drag characteristics of nine NACA 6-series airfoil sections at RN of 15.0, 20.0, and 25.0 million. Also presented are data from NACA Rep. 824 for the same airfoils at RN of 3.0, 6.0, and 9.0 million. The airfoils selected represent sections having variations in the airfoil thickness, thickness form, and camber. The characteristics of an airfoil with a split flap were determined in one instance, as was the effect of surface roughness. Qualitative explanations in terms of flow behavior are advanced for the observed types of scale effect.\n\nTwo-dimensional aerodynamic data obtained at RN of 3.0, 6.0 and 9.0 million are now generally available for a large number of systematically derived NACA airfoil sections. This range of RN is sufficient to satisfy engineering needs for many practical applications, but the recent trends toward both very large and very high-speed aircraft have emphasized the necessity for aerodynamic data at higher values. An investigation has accordingly been made of the aerodynamic characteristics of a number of systematically varied NACA 6-series airfoils at RN of 15.0, 20.0 and 25.0 million. The results of this investigation at high RN together with those for the same airfoils at lower RN are presented in Report No. 964. \nThe NACA 63 series was chosen as the basic group for investigation because these airfoils appear to offer good low-speed characteristics with a minimum of compromise from consideration of the high-speed characteristics. In all cases, only lift and drag were measured.\n\nAll the tests were made in the Langley two-dimensional low-turbulence pressure tunnel. The test section of this tunnel measures 3 x 7.5 feet, and the model completely spanned the 3-foot dimension. Seals were installed between the ends of the model and the tunnel walls to prevent air leakage.\n\nThe reaction of the minimum drag coefficient of smooth airfoils to increasing RN is attributed to the relative strengths of two interacting boundary-layer changes. A thinning of the boundary layer with increasing RN gives a gradual decrease of minimum drag. As the RN is increased beyond a certain value, however, the transition point begins to move forward and the drag increases.\nThe flow conditions of the thicker airfoils are more favorable for delaying the forward movement of transition.\n\nThe addition of standard roughness to the NACA 63-009 section causes a large increase in the minimum drag at all RN, but increasing the RN has a favorable effect in reducing the drag.\n\nIncreasing the RN from 9.0 to 15.0 million resulted in the almost complete disappearance of the low-drag range of all the airfoils except that of 18% thickness.\n\nThe angle of zero lift of the cambered airfoils showed almost no variation with RN.\nThroughout the range of RN of this investigation, the values of the lift-curve slope for the smooth sections are very close to that predicted by thin-airfoil theory (2π per radian, or 0.110 per degree).\nThe effects of increasing RN on the maximum lift showed two general trends: for airfoils of 12% thickness or less, the maximum lift remains relatively constant over the lower range of RN. Increasing the RN, however, causes a rapid increase in maximum lift; the 18% thick airfoils demonstrated a steady increase of maximum lift as RN is increased.\n\nThe 18% thick section had a type of maximum-lift variation with the RN that was entirely different from the thinner sections. Any comparison of airfoil maximum-lift characteristics can be made only if the data for the group of airfoils under consideration are available at the same RN. The choice of an optimum airfoil for maximum lift for a given application must be determined from data corresponding to the operating RN of the application.\n\n"}
{"id": "2227994", "url": "https://en.wikipedia.org/wiki?curid=2227994", "title": "Oxygen tank", "text": "Oxygen tank\n\nAn oxygen tank is an oxygen storage vessel, which is either held under pressure in gas cylinders, or as liquid oxygen in a cryogenic storage tank.\n\nOxygen tanks are used to store gas for:\n\n\nBreathing oxygen is delivered from the storage tank to users by use of the following methods: oxygen mask, nasal cannula, full face diving mask, diving helmet, demand valve, oxygen rebreather, built in breathing system (BIBS), oxygen tent, and hyperbaric oxygen chamber.\n\nContrary to popular belief scuba divers very rarely carry oxygen tanks. The vast majority of divers breathe air or nitrox stored in a diving cylinder. A small minority breathe trimix, heliox or other exotic gases. Some of these may carry pure oxygen for accelerated decompression or as a component of a rebreather. Some shallow divers, particularly naval divers, use oxygen rebreathers or have done so historically.\n\nOxygen is rarely held at pressures higher than , due to the risks of fire triggered by high temperatures caused by adiabatic heating when the gas changes pressure when moving from one vessel to another. Medical use liquid oxygen \"airgas\" tanks are typically .\n\nAll equipment coming into contact with high pressure oxygen must be \"oxygen clean\" and \"oxygen compatible\", to reduce the risk of fire. \"Oxygen clean\" means the removal of any substance that could act as a source of ignition. \"Oxygen compatible\" means that internal components must not burn readily or degrade easily in a high pressure oxygen environment.\n\nIn some countries there are legal and insurance requirements and restrictions on the use, storage and transport of pure oxygen. Oxygen tanks are normally stored in well-ventilated locations, far from potential sources of fire and concentrations of people.\n\n"}
{"id": "12408344", "url": "https://en.wikipedia.org/wiki?curid=12408344", "title": "Personal care", "text": "Personal care\n\nPersonal care or toiletries are consumer products used in personal hygiene and for beautification.\n\nSubsectors of personal care include personal hygiene and cosmetics.\n\nThere is a distinction between personal hygienic items, which are necessities, and cosmetics, which are luxury goods solely used for beautification. In practice, such sundries are often intermixed in retail store aisles.\n\nPersonal care includes products as diverse as cleansing pads, colognes, cotton swabs, cotton pads, deodorant, eye liner, facial tissue, hair clippers, lip gloss, lipstick, lip balm, lotion, makeup, hand soap, facial cleanser, body wash, nail files, pomade, perfumes, razors, shaving cream, moisturizer, talcum powder, toilet paper, toothpaste, facial treatments, wet wipes, and shampoo.\n\nTypical toiletries offered at many hotels include:\n\nSome of the major corporations in the personal care industry are:\n\nOther corporations, such as pharmacies (e.g. CVS/pharmacy, Walgreens) primarily retail in personal care rather than manufacture personal care products themselves.\n\n"}
{"id": "50375026", "url": "https://en.wikipedia.org/wiki?curid=50375026", "title": "Power-to-X", "text": "Power-to-X\n\nPower-to-X (also P2X and P2Y) has two meanings. First, power-to-X refers to a number of electricity conversion, energy storage, and reconversion pathways that utilize surplus electric power, typically during periods where fluctuating renewable energy generation exceeds load. Second, power-to-X refers to conversion technologies that allow for the decoupling of power from the electricity sector for use in other sectors (such as transport or chemicals), possibly using power that has been provided by additional investments in generation. The term power-to-x is widely used in Germany and may have originated there.\n\nThe X in the terminology can refer to one of the following: power-to-ammonia, power-to-chemicals, power-to-fuel, power-to-gas, power-to-heat, power-to-hydrogen, power-to-liquid, power-to-methane, power-to-mobility, power-to-power, and power-to-syngas.\n\nCollectively power-to-X schemes which use surplus power fall under the heading of flexibility measures and are particularly useful in energy systems with high shares of renewable generation and/or with strong decarbonization targets. A large number of pathways and technologies are encompassed by the term. In 2016 the German government funded a €30million first-phase research project into power-to-X options.\n\nSurplus electric power can be converted to other forms of energy for storage and reconversion. Direct current electrolysis (efficiency 80–85% at best) can be used to produce hydrogen which can, in turn, be converted to methane (CH) via methanation. Another possibility is converting the hydrogen, along with CO to methanol. Both these fuels can be stored and used to produce electricity again, hours to months later. Reconversion technologies include gas turbines, CCGT plant, and fuel cells. Power-to-power refers to the round-trip reconversion efficiency. For hydrogen storage, the round-trip efficiency remains limited at 35–50%. Electrolysis is expensive and power-to-gas processes need substantial full-load hours (say 30%) to be economic. Grid-dedicated battery storage is not normally considered a power-to-X concept.\n\nHydrogen and methane can also be used as downstream fuels, feed into the natural gas grid, or used to make or synthetic fuel. Alternatively they can be used as a chemical feedstock, as can ammonia ().\n\nPower-to-heat involves contributing to the heat sector, either by resistance heating or via a heat pump. Resistance heaters have unity efficiency, and the corresponding coefficient of performance (COP) of heat pumps is 2–5. Back-up immersion heating of both domestic hot water and district heating offers a cheap way of using surplus renewable energy and will often displace carbon-intensive fossil fuels for the task. Large-scale heat pumps in district heating systems with thermal energy storage are an especially attractive option for power-to-heat: they offer exceptionally high efficiency for balancing excess wind and solar power, and they can be profitable investments.\n\nPower-to-mobility refers to the charging of battery electric vehicles (EV). Given the expected uptake of EVs, dedicated dispatch will be required. As vehicles are idle for most of the time, shifting the charging time can offer considerable flexibility: the charging window is a relatively long 8–12hours, whereas the charging duration is around 90minutes. The EV batteries can also be discharged to the grid to make them work as electricity storage devices, but this causes additional wear to the battery.\n\nHeat pumps with hot water storage and electric vehicles have been found to have higher potential on reduction of emissions and fossil fuel use than several other power-to-X or electricity storage schemes for using surplus wind and solar power.\n\n"}
{"id": "2667560", "url": "https://en.wikipedia.org/wiki?curid=2667560", "title": "Pressure Equipment Directive (EU)", "text": "Pressure Equipment Directive (EU)\n\nThe Pressure Equipment Directive (PED) 2014/68/EU (formerly 97/23/EC) of the EU sets out the standards for the design and fabrication of pressure equipment (\"pressure equipment\" means steam boilers, pressure vessels, piping, safety valves and other components and assemblies subject to pressure loading) generally over one litre in volume and having a maximum pressure more than 0.5 bar gauge. It also sets the administrative procedures requirements for the \"conformity assessment\" of pressure equipment, for the free placing on the European market without local legislative barriers. It has been mandatory throughout the EU since 30 May 2002, with 2014 revision fully effective as of 19 July 2016. This is enacted in the UK as the Pressure Equipment Regulations (PER).\nThe set out standards and regulations regarding pressure vessels and boilers safety is also very close to the US standards defined by the American Society of Mechanical Engineers (ASME). This enables most international inspection agencies to provide both verification and certification services to assess compliance to the different pressure equipment directives.\n\n\n\nDirective 97/23/EC was fully superseded by directive 2014/68/EU from 20 July 2016 onwards. Article 13 of the new directive (classification of pressure equipment) became effective 1 June 2015, replacing article 9 of directive 97/23/EC.\n\n\n"}
{"id": "658183", "url": "https://en.wikipedia.org/wiki?curid=658183", "title": "Process control", "text": "Process control\n\nAutomatic process control in continuous production processes is a combination of control engineering and chemical engineering disciplines that uses industrial control systems to achieve a production level of consistency, economy and safety which could not be achieved purely by human manual control. It is implemented widely in industries such as oil refining, pulp and paper manufacturing, chemical processing and power generating plants.\n\nThere is a wide range of size, type and complexity, but it enables a small number of operators to manage complex processes to a high degree of consistency. The development of large automatic process control systems was instrumental in enabling the design of large high volume and complex processes, which could not be otherwise economically or safely operated.\n\nThe applications can range from controlling the temperature and level of a single process vessel, to a complete chemical processing plant with several thousand control loops.\n\nEarly process control breakthroughs came most frequently in the form of water control devices. Ktesibios of Alexandria is credited for inventing float valves to regulate water level of water clocks in the 3rd Century BC. In the 1st Century AD, Heron of Alexandria invented a water valve similar to the fill valve used in modern toilets.\n\nLater process controls inventions involved basic physics principles. In 1620, Cornlis Drebbel invented a bimetallic thermostat for controlling the temperature in a furnace. In 1681, Denis Papin discovered the pressure inside a vessel could be regulated by placing weights on top of the vessel lid. In 1745, Edmund Lee created the fantail to improve windmill efficiency; a fantail was a smaller windmill placed 90° of the larger fans to keep the face of the windmill pointed directly into the oncoming wind.\n\nWith the dawn of the Industrial Revolution in the 1760s, process controls inventions were aimed to replace human operators with mechanized processes. In 1784, Oliver Evans created a water-powered flourmill which operated using buckets and screw conveyors. Henry Ford applied the same theory in 1910 when the assembly line was created to decrease human intervention in the automobile production process.\n\nFor continuously variable process control it was not until 1922 that a formal control law for what we now call PID control or three-term control was first developed using theoretical analysis, by Russian American engineer Nicolas Minorsky. Minorsky was researching and designing automatic ship steering for the US Navy and based his analysis on observations of a helmsman. He noted the helmsman steered the ship based not only on the current course error, but also on past error, as well as the current rate of change; this was then given a mathematical treatment by Minorsky.\nHis goal was stability, not general control, which simplified the problem significantly. While proportional control provided stability against small disturbances, it was insufficient for dealing with a steady disturbance, notably a stiff gale (due to steady-state error), which required adding the integral term. Finally, the derivative term was added to improve stability and control.\n\nProcess control of large industrial plants has evolved through many stages. Initially, control would be from panels local to the process plant. However this required a large manpower resource to attend to these dispersed panels, and there was no overall view of the process. The next logical development was the transmission of all plant measurements to a permanently-manned central control room. Effectively this was the centralisation of all the localised panels, with the advantages of lower manning levels and easier overview of the process. Often the controllers were behind the control room panels, and all automatic and manual control outputs were transmitted back to plant. However, whilst providing a central control focus, this arrangement was inflexible as each control loop had its own controller hardware, and continual operator movement within the control room was required to view different parts of the process.\n\nWith the coming of electronic processors and graphic displays it became possible to replace these discrete controllers with computer-based algorithms, hosted on a network of input/output racks with their own control processors. These could be distributed around plant, and communicate with the graphic display in the control room or rooms. The distributed control system was born.\n\nThe introduction of DCSs allowed easy interconnection and re-configuration of plant controls such as cascaded loops and interlocks, and easy interfacing with other production computer systems. It enabled sophisticated alarm handling, introduced automatic event logging, removed the need for physical records such as chart recorders, allowed the control racks to be networked and thereby located locally to plant to reduce cabling runs, and provided high level overviews of plant status and production levels.\n\nThe accompanying diagram is a general model which shows functional manufacturing levels in a large process using processor and computer-based control.\n\nReferring to the diagram;\n\n\nTo determine the fundamental model for any process, the inputs and outputs of the system are defined differently than for other chemical processes. The balance equations are defined by the control inputs and outputs rather than the material inputs. The control model is a set of equations used to predict the behavior of a system and can help determine what the response to change will be.\n\n\nProcesses can be characterized as one or more of the following forms:\n\n\nThe fundamental building block of any industrial control system is the control loop, which controls just one process variable. An example is shown in the accompanying diagram, where the flow rate in a pipe is controlled by a PID controller, assisted by what is effectively a cascaded loop in the form of a valve servo-controller to ensure correct valve positioning.\n\nSome large systems may have several hundreds or thousands of control loops. In complex processes the loops are interactive, so that the operation of one loop may affect the operation of another. The system diagram for representing control loops is a Piping and instrumentation diagram.\n\nCommonly used controllers are programmable logic controller (PLC), Distributed Control System (DCS) or SCADA.\n\nA further example is shown. If a control valve were used to hold level in a tank, the level controller would compare the equivalent reading of a level sensor to the level setpoint and determine whether more or less valve opening was necessary to keep the level constant. A cascaded flow controller could then calculate the change in the valve position.\n\nThe economic nature of many products manufactured in batch and continuous processes require highly efficient operation due to thin margins. The competing factor in process control is that products must meet certain specifications in order to be satisfactory. These specifications can come in two forms: a minimum and maximum for a property of the material or product, or a range within which the property must be. All loops are susceptible to disturbances and therefore a buffer must be used on process set points to ensure disturbances do not cause the material or product to go out of specifications. This buffer comes at an economic cost (i.e. additional processing, maintaining elevated or depressed process conditions, etc.).\n\nProcess efficiency can be enhanced by reducing the margins necessary to ensure product specifications are met. This can be done by improving the control of the process to minimize the effect of disturbances on the process. The efficiency is improved in a two step method of narrowing the variance and shifting the target. Margins can be narrowed through various process upgrades (i.e. equipment upgrades, enhanced control methods, etc.). Once margins are narrowed, an economic analysis can be done on the process to determine how the set point target is to be shifted. Less conservative process set points lead to increased economic efficiency. Effective process control strategies increase the competitive advantage of manufacturers who employ them.\n\n\n"}
{"id": "55764851", "url": "https://en.wikipedia.org/wiki?curid=55764851", "title": "RNase H-dependent PCR", "text": "RNase H-dependent PCR\n\nRNase H-dependent PCR (rhPCR) is a modification of the standard PCR technique. In rhPCR, the primers are designed with a removable amplification block on the 3’ end. Amplification of the blocked primer is dependent on the cleavage activity of a hyperthermophilic archaeal Type II RNase H enzyme during hybridization to the complementary target sequence. This RNase H enzyme possesses several useful characteristics that enhance the PCR. First, it has very little enzymatic activity at low temperature, enabling a “hot start PCR” without modifications to the DNA polymerase. Second, the cleavage efficiency of the enzyme is reduced in the presence of mismatches near the RNA residue. This allows for reduced primer dimer formation, detection of alternative splicing variants, ability to perform multiplex PCR high higher numbers of PCR primers, and the ability to detect single-nucleotide polymorphisms.\n\nrhPCR primers consist of three sections. 1) The 5’ DNA section, equivalent in length and melting temperature (Tm) requirements to a standard PCR primer, is extended after cleavage by the RNase HII enzyme. 2) A single RNA base provides the cleavage site for the RNase HII. 3) A short 3’ extension of four or five bases followed by a blocker (usually a short, non-extendable molecule like a propanediol) prevents extension by a DNA polymerase until removal. A rhPCR reaction begins with the primers and template free in solution (Figure 1). While free in solution, these primers are not deblocked by the RNase HII enzyme, as they must be in an RNA:DNA heteroduplex with the template to be cleaved. Once bound to the template, the rhPCR primers are cleaved by the thermostable RNase HII enzyme. This removes the block, allowing for the DNA polymerase to extend off of the primers. The cycling of the PCR reaction continues the process. rhPCR primers are designed so that after cleavage by the RNase H2 enzyme, the Tm of the primers are still greater than the annealing temperature of PCR reaction. These primers can be used in both 5’ nuclease Taqman and SYBR Green types of quantitative PCR. \n\nrhPCR can be used for quantitative PCR and medical or environmental laboratories:\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "41665", "url": "https://en.wikipedia.org/wiki?curid=41665", "title": "Return loss", "text": "Return loss\n\nIn telecommunications, return loss is the loss of power in the signal returned/reflected by a discontinuity in a transmission line or optical fiber. This discontinuity can be a mismatch with the terminating load or with a device inserted in the line. It is usually expressed as a ratio in decibels (dB);\n\nReturn loss is related to both standing wave ratio (SWR) and reflection coefficient (Γ). Increasing return loss corresponds to lower SWR. Return loss is a measure of how well devices or lines are matched. A match is good if the return loss is high. A high return loss is desirable and results in a lower insertion loss.\n\nReturn loss is used in modern practice in preference to SWR because it has better resolution for small values of reflected wave.\n\nProperly, loss quantities, when expressed in decibels, should be positive numbers. However, return loss has historically been expressed as a negative number, and this convention is still widely found in the literature.\n\nThe correct definition of return loss is the difference in dB between the incident power sent towards the Device Under Test (DUT) and the power reflected, resulting in a positive sign:\n\nHowever taking the ratio of reflected to incident power results in a negative sign for return loss;\n\nReturn loss with a positive sign is identical to the magnitude of Γ when expressed in decibels but of opposite sign. That is, return loss with a negative sign is more properly called reflection coefficient. The S-parameter \"S\" from two-port network theory is frequently also called return loss, but is actually equal to Γ.\n\nCaution is required when discussing increasing or decreasing return loss since these terms strictly have the opposite meaning when return loss is defined as a negative quantity.\n\nIn metallic conductor systems, reflections of a signal traveling down a conductor can occur at a discontinuity or impedance mismatch. The ratio of the amplitude of the reflected wave \"V\" to the amplitude of the incident wave \"V\" is known as the reflection coefficient formula_4.\n\nWhen the source and load impedances are known values, the reflection coefficient is given by\n\nwhere \"Z\" is the impedance toward the source and \"Z\" is the impedance toward the load.\n\nReturn loss is the negative of the magnitude of the reflection coefficient in dB. Since power is proportional to the square of the voltage, return loss is given by,\n\nwhere the vertical bars indicate magnitude. Thus, a large positive return loss indicates the reflected power is small relative to the incident power, which indicates good impedance match from source to load.\n\nWhen the actual transmitted (incident) power and the reflected power are known (i.e., through measurements and/or calculations), then the return loss in dB can be calculated as the difference between the incident power \"P\" (in dBm) and the reflected power \"P\" (in dBm),\n\nIn optics (particularly in fiberoptics) a loss that takes place at discontinuities of refractive index, especially at an air-glass interface such as a fiber endface. At those interfaces, a fraction of the optical signal is reflected back toward the source. This reflection phenomenon is also called \"Fresnel reflection loss,\" or simply \"Fresnel loss.\"\n\nFiber optic transmission systems use lasers to transmit signals over optical fiber, and a high optical return loss (ORL) can cause the laser to stop transmitting correctly. The measurement of ORL is becoming more important in the characterization of optical networks as the use of wavelength-division multiplexing increases. These systems use lasers that have a lower tolerance for ORL, and introduce elements into the network that are located in close proximity to the laser.\n\nwhere formula_10 is the reflected power and formula_11 is the incident, or input, power.\n\n\n\n\n"}
{"id": "322550", "url": "https://en.wikipedia.org/wiki?curid=322550", "title": "Standard Performance Evaluation Corporation", "text": "Standard Performance Evaluation Corporation\n\nThe Standard Performance Evaluation Corporation (SPEC) is an American non-profit corporation that aims to \"produce, establish, maintain and endorse a standardized set\" of performance benchmarks for computers.\n\nSPEC was founded in 1988. SPEC benchmarks are widely used to evaluate the performance of computer systems; the test results are published on the SPEC website.\n\nSPEC evolved into an umbrella organization encompassing four diverse groups; Graphics and Workstation Performance Group (GWPG), the High Performance Group (HPG), the Open Systems Group (OSG) and the newest, the Research Group (RG).\n\nMembership in SPEC is open to any interested company or entity that is willing to commit to SPEC's standards. It allows:\n\n\nThe list of members is available on SPEC's membership page;.\n\n\nThe benchmarks aim to test \"real-life\" situations. There are several benchmarks testing Java scenarios, from simple computation (SPECjbb) to a full system with Java EE, database, disk, and network (SPECjEnterprise).\n\nThe SPEC CPU suites test CPU performance by measuring the run time of several programs such as the compiler GCC, the chemistry program gamess, and the weather program WRF. The various tasks are equally weighted; no attempt is made to weight them based on their perceived importance. An overall score is based on a geometric mean.\n\nMeasuring and comparing combined performance of CPU, memory and compiler.\n\n\nMeasuring performance of an OpenGL 3D graphics system, tested with various rendering tasks from several popluar 3D-intensive real applications on a given system.\n\nThe SPEC OMP (OpenMP) is the first one for evaluating performance based on OpenMP applications, for measuring the performance of SMP (Shared memory Multi-Processor, i.e. UMA) systems.\n\nevaluates the performance of server side Java by emulating a three-tier client/server system (with emphasis on the middle tier).\n\nA multi-tier benchmark for measuring the performance of Java 2 Enterprise Edition (J2EE) technology-based application servers.\n\nSPEC SFS is for measuring file server throughput and response time supporting both NFS and SMB protocol access.\n\n\n\n\n\nSPEC benchmarks are written in a portable programming language (usually C, C#, Java or Fortran), and the interested parties may compile the code using whatever compiler they prefer for their platform, but may not change the code. Manufacturers have been known to optimize their compilers to improve performance of the various SPEC benchmarks. SPEC has rules that attempt to limit such optimizations.\n\nIn order to use a benchmark, a license has to be purchased from SPEC; the costs vary from test to test with a typical range from several hundred to several thousand dollars. This pay-for-license model might seem to be in violation of the GPL as the benchmarks include software such as GCC that is licensed by the GPL. However, the GPL does not require software to be distributed for free, only that recipients be allowed to redistribute any GPLed software that they receive; the license agreement for SPEC specifically exempts items that are under \"licenses that require free distribution\", and the files themselves are placed in a separate part of the overall software package.\n\nSPEC attempts to create an environment where arguments are settled by appeal to notions of technical credibility, representativeness, or the \"level playing field\". SPEC representatives are typically engineers with expertise in the areas being benchmarked. Benchmarks include \"run rules\", which describe the conditions of measurement and documentation requirements. Results that are published on SPEC's website undergo a peer review by members' performance engineers.\n\n"}
{"id": "47474262", "url": "https://en.wikipedia.org/wiki?curid=47474262", "title": "Syllabical and Steganographical Table", "text": "Syllabical and Steganographical Table\n\nThe work was written by P.R. Wouves. It was thought by Philadelphia book dealer W.D. De Witt that this author's name was a play on words and a pseudonym for Benjamin Franklin. He thought that the \"P.R\" stood for Poor Richard, a pseudonym Franklin used for \"Richard Saunders\" as the author of \"Poor Richard's Almanack.\" Franklin had a reputation for using ciphers and codes in secret communications. The printer for this work was developed by Benjamin Franklin Bache, the grandson of Benjamin Franklin.\n\nThe work was of two pages and a chart. The two pages were title pages which were in English and French. Printed on the back of these pages were information and instructions for the chart that was within. This chart consisted of a large single sheet of paper of a table. It contained 62 alphabetical columns of 6,138 two-letter combinations, numbered from 1 to 99 so that words could be translated into numbers. The table chart sheet was normally kept folded for carrying around. The folded chart then was opened for using, as one would do with a folded road map.\n\nThe chart table was to be used to put into code a message that was to be kept secret from all others except the recipient. It was a cipher system. The idea behind this was that anything written in the Latin alphabet could be put into a numerical code of a set of numbers. It was initially set up for English and French but could also be used for Spanish, Portuguese, Dutch, or Italian. Wouves's chart for a secret correspondence method was only intended for certain of his closest friends. He was working on steganography. The initial preliminary table was printed in 1797 as a partial work. There never was an official printed edition of his total completed work.\n\nThe sheet of the chart table when open is wide by deep. It has a copyright from Pennsylvania District number 192 issued to author \"P.R. Wouves\" on November 9, 1787. According to US Army cryptographer William F. Friedman the table is of significant historical importance for what it represents.\n"}
{"id": "52638692", "url": "https://en.wikipedia.org/wiki?curid=52638692", "title": "Targeted alpha-particle therapy", "text": "Targeted alpha-particle therapy\n\nTargeted alpha-particle therapy (or TAT) is an in-development method of targeted radionuclide therapy of various cancers. It employs radioactive substances which undergo alpha decay to treat diseased tissue at close proximity. It has the potential to provide highly targeted treatment, especially to microscopic tumour cells. Targets include leukemias, lymphomas, gliomas, melanoma, and peritoneal carcinomatosis. As in diagnostic nuclear medicine, appropriate radionuclides can be chemically bound to a targeting biomolecule which carries the combined radiopharmaceutical to a specific treatment point.\n\nIt has been said that \"α-emitters are indispensable with regard to optimisation of strategies for tumour therapy\".\n\nThe primary advantage of alpha particle (α) emitters over other types of radioactive sources is their very high linear energy transfer (LET) and relative biological effectiveness (RBE). Beta particle (β) emitters such as Yttrium-90 can travel considerable distances beyond the immediate tissue before depositing their energy, while alpha particles deposit their energy in 70–100 μm long tracks.\n\nAlpha particles are more likely than other types of radiation to cause double-strand breaks to DNA molecules, which is one of several effective causes of cell death.\n\nSome α emitting isotopes such as Ac and Bi are only available in limited quantities from Th decay, although cyclotron production is feasible.\n\nThe ARRONAX cyclotron can produce At by irradiation of Bi.\n\nThough many α-emitters exist, useful isotopes would have a sufficient energy to cause damage to cancer cells, and a half-life that is long enough to provide a therapeutic dose without remaining long enough to damage healthy tissue.\n\nSeveral radionuclides have been studied for use in immunotherapy. Though β-emitters are more popular, in part due to their availability, trials have taken place involving Ac, At, Pb and Bi.\n\nTreatment of peritoneal carcinomas has promising early results limited by availability of α-emitters compared to β-emitters.\n\nRa was the first α-emitter approved by the FDA in the United States for treatment of bone metastases from prostate cancer, and is a recommended treatment in the UK by NICE. In a phase III trial comparing Ra to a placebo, survival was significantly improved.\n\nEarly trials of Ac and Bi have shown evidence of anti-tumour activity in Leukaemia patients.\n\nPhase I trials on melanomas have shown Bi is effective in causing tumour regression.\n\nThe short path length of alpha particles in tissue, which makes them well suited to treatment of the above types of disease, is a negative when it comes to treatment of larger bodies of solid tumour by intravenous injection. However, potential methods to solve this problem of delivery exist, such as direct intratumoral injection and anti-angiogenic drugs.\n\n"}
{"id": "21971683", "url": "https://en.wikipedia.org/wiki?curid=21971683", "title": "Thermal bag", "text": "Thermal bag\n\nA thermal bag is a type of thermally insulated shipping container in the form of a bag which can be carried, usually made of thermally insulating materials and sometimes a refrigerant gel. It is used to help maintain the temperature of its contents, keeping cold items cold, and hot items hot.\n\nInsulated bags have been in use for many years in industry, medical/pharmaceutical use, food delivery, lunch bags, etc. Several designs have been available.\n\nCommercial thermal shopping bags, to carry temperature-sensitive purchases home without breaking the cold chain, were first introduced by grocery and other shops in Europe in the mid 1980s. A thermal bag to keep pizzas being delivered hot was invented by Ingrid Kosar in 1983, and is commonly used now. A cool box is very similar in concept, but typically larger and in the form of a rigid box.\n\nThermal pharmaceutical bags are designed to transport temperature-sensitive medications, protecting them from damaging temperatures, shocks, and light. Many vaccines are delicate biological substances that can lose part or all of their effectiveness if they are frozen, allowed to get too hot, or exposed to bright light. Such vaccines must be kept within a specified temperature range, typically , from manufacture to use. According to the World Health Organization, at least 7% of temperature-sensitive medical products suffer significant degradation in potency in transit.\n\nThermal bags are usually manufactured with varying quantities of the following materials:\n\n\n"}
{"id": "3188645", "url": "https://en.wikipedia.org/wiki?curid=3188645", "title": "Toilet papering", "text": "Toilet papering\n\nToilet papering (also called TP-ing, House Wrapping or Yard Rolling) is the act of covering an object, such as a tree, house, or another structure with toilet paper. This is typically done by throwing numerous toilet paper rolls in such a way that they unroll in midair and thus fall on the targeted object in multiple streams. Toilet papering can be an initiation, a joke, a prank, or an act of revenge. It is common in the United States and frequently takes place on Halloween, April Fools' Day, or after the completion of school events such as graduation or homecoming football game.\n\nWhile few jurisdictions in the United States have statutes specifically against toilet papering, some police departments cite perpetrators on the grounds of littering, trespassing, disorderly conduct, or criminal mischief, especially when the homeowner's property is damaged.\n\nThrowing toilet paper is a component of the audience participation activities associated with showings of \"The Rocky Horror Picture Show\". Rolls of toilet paper are customarily thrown in a reference to Scott brand toilet paper after a character shouts the phrase \"Great Scott!\" in response to the entrance of character Dr. Everett Scott.\n\nFollowing the Chicago Blackhawks' Stanley Cup victory in 2015, a group of fans celebrated by toilet papering the home of Blackhawks head coach Joel Quenneville, in Hinsdale, Illinois.\n\n"}
{"id": "1894270", "url": "https://en.wikipedia.org/wiki?curid=1894270", "title": "Toleware", "text": "Toleware\n\nThe term tôle, derived from the French \"tôle peinte\", \"painted sheet metal\", is synonymous in English usage with japanning on tin, such as the tôle shades for bouilotte lamps and other candle shades, and trays and lidded canisters, in which stenciling and gilding often features, almost always on a black ground. Pontypool and Usk in South Wales made a reputation for tôle imitating Japanese lacquer starting in the early 19th century.\n\nIn the collectibles and antique industry, toleware refers to kitchen-related objects created from metal, typically tin or thin steel, and are often in decorative styles such as Arts and Crafts and Pennsylvania Dutch. Decorative painting on these items is common but not necessary. This style of decorative art spread from Europe to the United States in the 18th century, and was popular in US kitchens in the 18th and 19th centuries.\n\nIn the field of handicrafts, tole painting on metal objects is a popular amateur pastime.\n\nToleware is featured on United States Postal Service 5-cent Make up stamp.\n\n"}
{"id": "374594", "url": "https://en.wikipedia.org/wiki?curid=374594", "title": "Wilhelm Schickard", "text": "Wilhelm Schickard\n\nWilhelm Schickard (22 April 1592 – 24 October 1635) was a German professor of Hebrew and Astronomy who became famous in the second part of the 20th century after Dr. Franz Hammer, a biographer (along with Max Caspar) of Johannes Kepler, claimed that the drawings of a calculating clock, predating the public release of Pascal's calculator by twenty years, had been discovered in two unknown letters written by Schickard to Johannes Kepler in 1623 and 1624.\n\nDr. Hammer asserted that because these letters had been lost for three hundred years, Blaise Pascal had been called and celebrated as the inventor of the mechanical calculator in error during all this time.\n\nAfter careful examination it was found that Schikard's drawings had been published at least once per century starting from 1718, that his machine was not complete and required additional wheels and springs and that it was designed around a \"single tooth\" carry mechanism that didn't work properly when used in calculating clocks.\n\nSchickard's machine was the first of several designs of \"direct entry\" calculating machines in the 17th century (including the designs of Blaise Pascal, Tito Burattini, Samuel Morland and René Grillet). The Schickard machine was particularly notable for its integration of an ingenious system of rotated Napier's rods for multiplication with a first known design for an adding machine, operated by rotating knobs for input, and with a register of rotated numbers showing in windows for output. Taton has argued that Schickard's work had no impact on the development of mechanical calculators. However, whilst there can be debate about what constitutes a \"mechanical calculator\" later devices, such as Moreland's multiplying and adding instruments when used together, Caspar Schott's Cistula, René Grillet's machine arithmétique, and Claude Perrault's rhabdologique at the end of the century, and later, the Bamberger Omega developed in the early 20th Century, certainly followed the same path pioneered by Schickard with his ground breaking combination of a form of Napier's rods and adding machine designed to assist multiplication.\n\nSchickard was born in Herrenberg and educated at the University of Tübingen, receiving his first degree, B.A. in 1609 and M.A. in 1611. He studied theology and oriental languages at Tübingen until 1613. In 1613 he became a Lutheran minister continuing his work with the church until 1619 when he was appointed professor of Hebrew at the University of Tübingen.\n\nSchickard was a universal scientist and taught biblical languages such as Aramaic as well as Hebrew at Tübingen. In 1631 he was appointed professor of astronomy at the University of Tübingen. His research was broad and included astronomy, mathematics and surveying. He invented many machines such as one for calculating astronomical dates and one for Hebrew grammar. He made significant advances in mapmaking, producing maps that were far more accurate than previously available.\n\nHe was, among his other skills, a renowned wood and copperplate engraver.\n\nWilhelm Schickard died of the bubonic plague in Tübingen, on 23 or 24 October 1635. In 1651, Giovanni Riccioli named the lunar crater Schickard after him.\n\nIn 1625 Schickard, a Christian Hebraist, published an influential treatise, \"Mishpat ha-melek, Jus regium Hebraeorum\" (Title in both Hebrew and Latin: The King's Law) in which he uses the Talmud and rabbinical literature to analyze ancient Hebrew political theory. Schickard argues that the Bible supports monarchy.\n\nAround 1621, Schickard built a machine based on gears for doing simplified multiplications involved in Johannes Kepler's calculations of the orbit of the Moon. In 1623 and 1624, in two letters that he sent to Kepler, reported his design and construction of what he referred to as an “arithmeticum organum” (“arithmetical instrument”) that he has invented, but which would later be described as a Rechenuhr (calculating clock). The machine was designed to assist in all the four basic functions of arithmetic (addition, subtraction, multiplication and division). Amongst its uses, Schickard suggested it would help in the laborious task of calculating astronomical tables. The machine could add and subtract six-digit numbers, and indicated an overflow of this capacity by ringing a bell. The adding machine in the base was primarily provided to assist in the difficult task of adding or multiplying two multi-digit numbers. To this end an ingenious arrangement of rotatable Napier's bones were mounted on it. It even had an additional \"memory register\" to record intermediate calculations. Whilst Schickard noted that the adding machine was working his letters mention that he had asked a professional, a clockmaker named Johann Pfister to build a finished machine. Regrettably it was destroyed in a fire either whilst still incomplete, or in any case before delivery. Schickard abandoned his project soon after. He and his entire family were wiped out in 1635 by bubonic plague during the Thirty Years' War.\n\nSchickard's machine used clock wheels which were made stronger and were therefore heavier, to prevent them from being damaged by the force of an operator input. Each digit used a display wheel, an input wheel and an intermediate wheel. During a carry transfer all these wheels meshed with the wheels of the digit receiving the carry.\n\nThe Institute for Computer Science at the University of Tübingen is called the Wilhelm-Schickard-Institut für Informatik in his honor.\n\nThere has been a long-standing question about who should be given priority of invention of the mechanical calculator. Schickard's mechanism was chronologically earlier but was never able to be used and appears to have had serious design flaws. Pascal's design was slightly later but functioned superbly.\n\nIn 1718 an early biographer of Kepler, Michael Gottlieb Hansch, had published letters from Schickard that described the calculating machine, and his priority was also mentioned in an 1899 publication, the \"Stuttgarter Zeitschrift für Vermessungswesen\". In 1957, Franz Hammer, one of Kepler's biographers, announced that Schickard's drawings of this previously unknown calculating clock predated Pascal's work by twenty years.\n\nBruno von Freytag-Löringhoff built a replica of Schickard's machine in 1960, but had to improve on the design of the carry mechanism: \nPascal's invention was almost certainly independent, as \"it is almost certain that Pascal would not have known of Schickard's machine.\" Pascal realized that a single-tooth gear would only be adequate for a carry that only needs to propagate a few places. For more digits, the force required to propagate extended carries would damage such gears.\n\nThe two machines were essentially different in that Pascal's machine was designed primarily for addition and (with the use of complementary numbers) for subtraction. The adding machine in Schickard's design may have jammed in the unusual case of a carry being required across too many dials, but it could smoothly subtract by reversing the motion of the input dials, in a way that was not possible in the Pascaline. (Experiments with replicas show that in the event of a jam when a carry is attempted across more than (say) three dials, it is obvious to the operator who may intervene to assist the machine to perform the additional carries. This is not as efficient as with the Pascaline, but it is not a fatal deficiency.) The Schickard adding machine also has provision for an audible warning when an output was too large for the available dials. This was not provided for in the Pascaline.\n\nPascal tried to create a smoothly functioning adding machine for use by his father initially, and later for commercialisation, while the adding machine in Schickard's design appears to have been introduced to assist in multiplication (through the calculation of partial products using Napier's rods, a process that can also be used to assist division).\n\n"}
{"id": "373172", "url": "https://en.wikipedia.org/wiki?curid=373172", "title": "Winemaking", "text": "Winemaking\n\nWinemaking or vinification is the production of wine, starting with the selection of the fruit, its fermentation into alcohol, and the bottling of the finished liquid. The history of wine-making stretches over millennia. The science of wine and winemaking is known as oenology. A winemaker may also be called a vintner. The growing of grapes is viticulture and there are many varieties of grapes\n\nWinemaking can be divided into two general categories: still wine production (without carbonation) and sparkling wine production (with carbonation — natural or injected). Red wine, white wine, and rosé are the other main categories. Although most wine is made from grapes, it may also be made from other plants, see fruit wine. Other similar light alcoholic drinks (as opposed to beer or spirits) include mead, made by fermenting honey and water, and kumis, made of fermented mare's milk.\n\nThere are five basic stages to the wine making process which begins with harvesting or picking. After the harvest, the grapes are taken into a winery and prepared for primary ferment. At this stage red wine making diverges from white wine making.\nRed wine is made from the must (pulp) of red or black grapes and fermentation occurs together with the grape skins, which give the wine its color.\nWhite wine is made by fermenting juice which is made by pressing crushed grapes to extract a juice; the skins are removed and play no further role. Occasionally white wine is made from red grapes; this is done by extracting their juice with minimal contact with the grapes' skins.\nRosé wines are either made from red grapes where the juice is allowed to stay in contact with the dark skins long enough to pick up a pinkish color (maceration or saignée) or by blending red wine with white wine. White and rosé wines extract little of the tannins contained in the skins.\n\nTo start primary fermentation yeast may be added to the must for red wine or may occur naturally as ambient yeast on the grapes or in the air. Yeast may be added to the juice for white wine. During this fermentation, which often takes between one and two weeks, the yeast converts most of the sugars in the grape juice into ethanol (alcohol) and carbon dioxide. The carbon dioxide is lost to the atmosphere.\n\nAfter the primary fermentation of red grapes the free run wine is pumped off into tanks and the skins are pressed to extract the remaining juice and wine. The press wine is blended with the free run wine at the winemaker's discretion. The wine is kept warm and the remaining sugars are converted into alcohol and carbon dioxide.\n\nThe next process in the making of red wine is malo-lactic conversion. This is a bacterial process which converts \"crisp, green apple\" malic acid to \"soft, creamy\" lactic acid softening the taste of the wine. Red wine is sometimes transferred to oak barrels to mature for a period of weeks or months; this practice imparts oak aromas and some tannin to the wine. The wine must be settled or clarified and adjustments made prior to bottling.\n\nThe time from harvest to drinking can vary from a few months for Beaujolais nouveau wines to over twenty years for wine of good structure with high levels of acid, tannin or sugar. However, only about 10% of all red and 5% of white wine will taste better after five years than it will after just one year.\nDepending on the quality of grape and the target wine style, some of these steps may be combined or omitted to achieve the particular goals of the winemaker. Many wines of comparable quality are produced using similar but distinctly different approaches to their production; quality is dictated by the attributes of the starting material and not necessarily the steps taken during vinification.\n\nVariations on the above procedure exist. With sparkling wines such as Champagne, an additional, \"secondary\" fermentation takes place inside the bottle, dissolving trapped carbon dioxide in the wine and creating the characteristic bubbles. Sweet wines or off-dry wines are made by arresting fermentation before all sugar has been converted into ethanol and allowing some residual sugar to remain. This can be done by chilling the wine and adding sulphur and other allowable additives to inhibit yeast activity or sterile filtering the wine to remove all yeast and bacteria. In the case of sweet wines, initial sugar concentrations are increased by harvesting late (late harvest wine), freezing the grapes to concentrate the sugar (ice wine), allowing or encouraging botrytis cinerea fungus to dehydrate the grapes or allowing the grapes to raisin either on the vine or on racks or straw mats. Often in these high sugar wines, the fermentation stops naturally as the high concentration of sugar and rising concentration of ethanol retard the yeast activity. Similarly in fortified wines, such as port wine, high proof neutral grape spirit (brandy) is added to arrest the ferment and adjust the alcohol content when the desired sugar level has been reached. In other cases the winemaker may choose to hold back some of the sweet grape juice and add it to the wine after the fermentation is done, a technique known in Germany as süssreserve.\n\nThe process produces wastewater, pomace, and lees that require collection, treatment, and disposal or beneficial use.\n\nSynthetic wines, engineered wines or fake wines, are a product that do not use grapes at all and start with water and ethanol and then adds acids, amino acids, sugars, and organic compounds.\n\nThe quality of the grapes determines the quality of the wine more than any other factor. Grape quality is affected by variety as well as weather during the growing season, soil minerals and acidity, time of harvest, and pruning method. The combination of these effects is often referred to as the grape's \"terroir\".\n\nGrapes are usually harvested from the vineyard from early September until early November in the northern hemisphere, and mid February until early March in the southern hemisphere. In some cool areas in the southern hemisphere, for example Tasmania, harvesting extends into May.\n\nThe most common species of wine grape is Vitis vinifera, which includes nearly all varieties of European origin.\n\nHarvest is the picking of the grapes and in many ways the first step in wine production. Grapes are either harvested mechanically or by hand. The decision to harvest grapes is typically made by the winemaker and informed by the level of sugar (called °Brix), acid (TA or Titratable Acidity as expressed by tartaric acid equivalents) and pH of the grapes. Other considerations include phenological ripeness, berry flavor, tannin development (seed color and taste). Overall disposition of the grapevine and weather forecasts are taken into account.\n\nMechanical harvesters are large tractors that straddle grapevine trellises and, using firm plastic or rubber rods, strike the fruiting zone of the grapevine to dislodge the grapes from the rachis. Mechanical harvesters have the advantage of being able to cover a large area of vineyard land in a relatively short period of time, and with a minimum investment of manpower per harvested ton. A disadvantage of mechanical harvesting is the indiscriminate inclusion of foreign non-grape material in the product, especially leaf stems and leaves, but also, depending on the trellis system and grapevine canopy management, may include moldy grapes, canes, metal debris, rocks and even small animals and bird nests. Some winemakers remove leaves and loose debris from the grapevine before mechanical harvesting to avoid such material being included in the harvested fruit. In the United States mechanical harvesting is seldom used for premium winemaking because of the indiscriminate picking and increased oxidation of the grape juice. In other countries (such as Australia and New Zealand), mechanical harvesting of premium winegrapes is more common because of general labor shortages.\nManual harvesting is the hand-picking of grape clusters from the grapevines. In the United States, some grapes are picked into one- or two-ton bins for transport back to the winery. Manual harvesting has the advantage of using knowledgeable labor to not only pick the ripe clusters but also to leave behind the clusters that are not ripe or contain bunch rot or other defects. This can be an effective first line of defense to prevent inferior quality fruit from contaminating a lot or tank of wine.\nDestemming is the process of separating stems from the grapes. Depending on the winemaking procedure, this process may be undertaken before crushing with the purpose of lowering the development of tannins and vegetal flavors in the resulting wine. Single berry harvesting, as is done with some German Trockenbeerenauslese, avoids this step altogether with the grapes being individually selected.\n\nCrushing is the process when gently squeezing the berries and breaking the skins to start to liberate the contents of the berries. Destemming is the process of removing the grapes from the rachis (the stem which holds the grapes). In traditional and smaller-scale wine making, the harvested grapes are sometimes crushed by trampling them barefoot or by the use of inexpensive small scale crushers. These can also destem at the same time. However, in larger wineries, a mechanical crusher/destemmer is used.\nThe decision about destemming is different for red and white wine making. Generally when making white wine the fruit is only crushed, the stems are then placed in the press with the berries. The presence of stems in the mix facilitates pressing by allowing juice to flow past flattened skins. These accumulate at the edge of the press.\nFor red winemaking, stems of the grapes are usually removed before fermentation since the stems have a relatively high tannin content; in addition to tannin they can also give the wine a vegetal aroma (due to extraction of 2-methoxy-3-isopropylpyrazine which has an aroma reminiscent of green bell peppers.) On occasion, the winemaker may decide to leave them in if the grapes themselves contain less tannin than desired. This is more acceptable if the stems have 'ripened' and started to turn brown.\nIf increased skin extraction is desired, a winemaker might choose to crush the grapes after destemming. Removal of stems first means no stem tannin can be extracted. In these cases the grapes pass between two rollers which squeeze the grapes enough to separate the skin and pulp, but not so much as to cause excessive shearing or tearing of the skin tissues. In some cases, notably with \"delicate\" red varietals such as Pinot noir or Syrah, all or part of the grapes might be left uncrushed (called \"whole berry\") to encourage the retention of fruity aromas through partial carbonic maceration.\nMost red wines derive their color from grape skins (the exception being varieties or hybrids of non-vinifera vines which contain juice pigmented with the dark Malvidin 3,5-diglucoside anthocyanin) and therefore contact between the juice and skins is essential for color extraction.\nRed wines are produced by destemming and crushing the grapes into a tank and leaving the skins in contact with the juice throughout the fermentation (maceration). It is possible to produce white (colorless) wines from red grapes by the fastidious pressing of uncrushed fruit. This minimizes contact between grape juice and skins (as in the making of \"Blanc de noirs\" sparkling wine, which is derived from Pinot noir, a red vinifera grape.)\n\nMost white wines are processed without destemming or crushing and are transferred from picking bins directly to the press. This is to avoid any extraction of tannin from either the skins or grapeseeds, as well as maintaining proper juice flow through a matrix of grape clusters rather than loose berries. In some circumstances winemakers choose to crush white grapes for a short period of skin contact, usually for three to 24 hours. This serves to extract flavor and tannin from the skins (the tannin being extracted to encourage protein precipitation without excessive Bentonite addition) as well as potassium ions, which participate in bitartrate precipitation (cream of tartar). It also results in an increase in the pH of the juice which may be desirable for overly acidic grapes. This was a practice more common in the 1970s than today, though still practiced by some Sauvignon blanc and Chardonnay producers in California.\n\nIn the case of rosé wines, the fruit is crushed and the dark skins are left in contact with the juice just long enough to extract the color that the winemaker desires. The must is then pressed, and fermentation continues as if the winemaker was making a white wine.\n\nYeast is normally already present on the grapes, often visible as a powdery appearance of the grapes. The primary, or alcoholic fermentation can be done with this natural yeast, but since this can give unpredictable results depending on the exact types of yeast that are present, cultured yeast is often added to the must. One of the main problems with the use of wild ferments is the failure for the fermentation to go to completion, that is some sugar remains unfermented. This can make the wine sweet when a dry wine is desired. Frequently wild ferments lead to the production of unpleasant acetic acid (vinegar) production as a by product.\nDuring the primary fermentation, the yeast cells feed on the sugars in the must and multiply, producing carbon dioxide gas and alcohol. The temperature during the fermentation affects both the taste of the end product, as well as the speed of the fermentation. For red wines, the temperature is typically 22 to 25 °C, and for white wines 15 to 18 °C.\nFor every gram of sugar that is converted, about half a gram of alcohol is produced, so to achieve a 12% alcohol concentration, the must should contain about 24% sugars. The sugar percentage of the must is calculated from the measured density, the must weight, with the help of a specialized type of hydrometer called a saccharometer. If the sugar content of the grapes is too low to obtain the desired alcohol percentage, sugar can be added (chaptalization). In commercial winemaking, chaptalization is subject to local regulations.\n\nAlcohol of more than 12% can be achieved by using yeast that can withstand high alcohol. Some yeasts can produce 18% alcohol in the wine however extra sugar is added to produce a high alcohol content.\n\nDuring or after the alcoholic fermentation, a secondary, or malolactic fermentation can also take place, during which specific strains of bacteria (lactobacter) convert malic acid into the milder lactic acid. This fermentation is often initiated by inoculation with desired bacteria.\n\nPressing is the act of applying pressure to grapes or pomace in order to separate juice or wine from grapes and grape skins. Pressing is not always a necessary act in winemaking; if grapes are crushed there is a considerable amount of juice immediately liberated (called free-run juice) that can be used for vinification. Typically this free-run juice is of a higher quality than the press juice. Pressed juice is typically lesser in quality due to the release and increase of total phenolic compounds, as well as browning index and the C6-alcohol levels. These compounds are responsible for the herb-like taste perceived in wine with pressed grapes. However, most wineries do use presses in order to increase their production (gallons) per ton, as pressed juice can represent between 15%-30% of the total juice volume from the grape.\n\nPresses act by positioning the grape skins or whole grape clusters between a rigid surface and a movable surface and slowly decrease the volume between the two surfaces. Modern presses dictate the duration and pressure at each press cycle, usually ramping from 0 Bar to 2.0 Bar. Sometimes winemakers choose pressures which separate the streams of pressed juice, called making \"press cuts.\" As the pressure increases the amount of tannin extracted from the skins into the juice increases, often rendering the pressed juice excessively tannic or harsh. Because of the location of grape juice constituents in the berry (water and acid are found primarily in the mesocarp or pulp, whereas tannins are found primarily in the exocarp, or skin, and seeds), pressed juice or wine tends to be lower in acidity with a higher pH than the free-run juice.\nBefore the advent of modern winemaking, most presses were basket presses made of wood and operated manually. Basket presses are composed of a cylinder of wooden slats on top of a fixed plate, with a moveable plate that can be forced downward (usually by a central ratcheting threaded screw.) The press operator would load the grapes or pomace into the wooden cylinder, put the top plate in place and lower it until juice flowed from the wooden slats. As the juice flow decreased, the plate was ratcheted down again. This process continued until the press operator determined that the quality of the pressed juice or wine was below standard, or all liquids had been pressed. Since the early 1990s, modern mechanical basket presses have been revived through higher-end producers seeking to replicate the gentle pressing of the historical basket presses. Because basket presses have a relatively compact design, the press cake offers a relatively longer pathway for the juice to travel before leaving the press. It is believed by advocates of basket presses that this relatively long pathway through the grape or pomace cake serves as a filter to solids that would otherwise affect the quality of the press juice.\n\nWith red wines, the must is pressed after primary fermentation, which separates the skins and other solid matter from the liquid. With white wine, the liquid is separated from the must before fermentation . With rose, the skins may be kept in contact for a shorter period to give color to the wine, in that case the must may be pressed as well. After a period in which the wine stands or ages, the wine is separated from the dead yeast and any solids that remained (called lees), and transferred to a new container where any additional fermentation may take place.\n\nPigeage is a French winemaking term for the traditional grape stomping in open fermentation tanks. To make certain types of wine, grapes are put through a crusher and then poured into open fermentation tanks. Once fermentation begins, the grape skins are pushed to the surface by carbon dioxide gases released in the fermentation process. This layer of skins and other solids is known as the cap. As the skins are the source of the tannins, the cap needs to be mixed through the liquid each day, or \"punched,\" which traditionally is done by stomping through the vat.\n\n\"Cold stabilization\" is a process used in winemaking to reduce tartrate crystals (generally potassium bitartrate) in wine. These tartrate crystals look like grains of clear sand, and are also known as \"wine crystals\" or \"wine diamonds\". They are formed by the union of tartaric acid and potassium, and may appear to be [sediment] in the wine, though they are not. During the cold stabilizing process after fermentation, the temperature of the wine is dropped to close to freezing for 1–2 weeks. This will cause the crystals to separate from the wine and stick to the sides of the holding vessel. When the wine is drained from the vessels, the tartrates are left behind. They may also form in wine bottles that have been stored under very cold conditions.\n\nDuring the secondary fermentation and aging process, which takes three to six months, the fermentation continues very slowly. The wine is kept under an airlock to protect the wine from oxidation. Proteins from the grape are broken down and the remaining yeast cells and other fine particles from the grapes are allowed to settle. Potassium bitartrate will also precipitate, a process which can be enhanced by cold stabilization to prevent the appearance of (harmless) tartrate crystals after bottling. The result of these processes is that the originally cloudy wine becomes clear. The wine can be racked during this process to remove the lees.\n\nThe secondary fermentation usually takes place in large stainless steel vessels with a volume of several cubic meters, oak barrels or glass demijohns (also referred to as carboys), depending on the goals of the winemakers. Unoaked wine is fermented in a barrel made of stainless steel or other material having no influence in the final taste of the wine. Depending on the desired taste, it could be fermented mainly in stainless steel to be briefly put in oak, or have the complete fermentation done in stainless steel. Oak could be added as chips used with a non-wooden barrel instead of a fully wooden barrel. This process is mainly used in cheaper wine.\n\nAmateur winemakers often use glass carboys in the production of their wine; these vessels (sometimes called \"demijohns\") have a capacity of . The kind of vessel used depends on the amount of wine that is being made, the grapes being used, and the intentions of the winemaker.\n\nMalolactic fermentation occurs when lactic acid bacteria metabolize malic acid and produce lactic acid and carbon dioxide. This is carried out either as an intentional procedure in which specially cultivated strains of such bacteria are introduced into the maturing wine, or it can happen by chance if uncultivated lactic acid bacteria are present.\n\nMalolactic fermentation can improve the taste of wine that has high levels of malic acid, because malic acid, in higher concentration, generally causes an unpleasant harsh and bitter taste sensation, whereas lactic acid is more gentle and less sour. Lactic acid is an acid found in dairy products. Malolactic fermentation usually results in a reduction in the amount of total acidity of the wine. This is because malic acid has two acid radicals (-COOH) while lactic acid has only one. However, the pH should be monitored and not allowed to rise above a pH of 3.55 for whites or a pH of 3.80 for reds. pH can be reduced roughly at a rate of 0.1 units per 1 gram/litre of tartaric acid addition.\n\nThe use of lactic acid bacteria is the reason why some chardonnays can taste \"buttery\" due to the production of diacetyl by the bacteria. Most red wines go through complete malolactic fermentation, both to lessen the acid of the wine and to remove the possibility that malolactic fermentation will occur in the bottle.\nWhite wines vary in the use of malolactic fermentation during their making. Lighter aromatic wines such as Riesling, generally do not go through malolactic fermentation. The fuller white wines such as barrel fermented chardonnay, are more commonly put through malolactic fermentation. Sometimes a partial fermentation, for example, somewhere less than 50% might be employed.\n\nWhether the wine is aging in tanks or barrels, tests are run periodically in a laboratory to check the status of the wine. Common tests include Brix, pH, titratable acidity, residual sugar, free or available sulfur, total sulfur, volatile acidity and percent alcohol. Additional tests include those for the crystallization of cream of tartar (potassium hydrogen tartrate) and the precipitation of heat unstable protein; this last test is limited to white wines. These tests may be performed throughout the making of the wine as well as prior to bottling. In response to the results of these tests, a winemaker can decide on appropriate remedial action, for example the addition of more sulfur dioxide. Sensory tests will also be performed and again in response to these a winemaker may take remedial action such as the addition of a protein to soften the taste of the wine.\nBrix (°Bx) is one measure of the soluble solids in the grape juice and represents not only the sugars but also includes many other soluble substances such as salts, acids and tannins, sometimes called total dissolved solids (TDS). Because sugar is the dominant compound in grape juice, these units are effectively a measure of sugar level. The level of sugar in the grapes determines the final alcohol content of the wine as well as indirect index of grape maturity. °Bx is measured in grams per hundred grams of solution, so 20 °Bx means that 100 grams of juice contains 20gm of dissolved compounds. There are other common measures of sugar content of grapes, specific gravity, Oechsle (Germany) and Baumé (France). °Bx is usually measured with a refractometer while the other methods use a hydrometer which measures specific gravity. Generally, hydrometers are a cheaper alternative. In the French Baumé (Be° or Bé° for short) one Be° corresponds approximately to one percent alcohol. One Be° is equal to 1.8 °Bx, that is 1.8 grams of sugar per one hundred grams. Therefore, to achieve one percent alcohol the winemaker adds sugar at a rate of 1.8 grams per 100 ml (18 grams per liter) — a practice known as chaptalization, which is illegal in some countries and in California.\n\nVolatile acidity test verifies if there is any steam distillable acids in the wine. Mainly present is acetic acid (the dominant component of vinegar), but lactic, butyric, propionic, and formic acid can also be found. Usually the test checks for these acids in a cash still, but there are other methods available such as HPLC, gas chromatography and enzymatic methods. The amount of volatile acidity found in sound grapes is negligible, because it is a by-product of microbial metabolism. Because acetic acid bacteria require oxygen to grow, eliminating any air in wine containers as well as addition of sulfur dioxide (SO) will limit their growth. Rejecting moldy grapes also prevents possible problems associated with acetic acid bacteria. Use of sulfur dioxide and inoculation with a low-V.A. producing strain of \"Saccharomyces\" may deter acetic acid producing yeast. A relatively new method for removal of volatile acidity from a wine is reverse osmosis. Blending may also help—a wine with high V.A. can be filtered (to remove the microbe responsible) and blended with a low V.A. wine, so that the acetic acid level is below the sensory threshold.\n\nSulphur dioxide can be readily measured with relatively simple laboratory equipment. There are several methods available; a typical test involves acidification of a sample with phosphoric acid, distillation of the liberated SO, and capture by hydrogen peroxide solution. The SO and peroxide react to form sulphuric acid, which is then titrated with NaOH to an end point with an indicator, and the volume of NaOH required is used to calculate the SO level. This method has inaccuracies associated with red wine, inefficient condensers, and excessive aspiration rate, although the results are reproducible, having an accuracy with just a 2.5-5% error, which is sufficient to control the level of sulphur dioxide in wine.\n\nDifferent batches of wine can be mixed before bottling in order to achieve the desired taste. The winemaker can correct perceived inadequacies by mixing wines from different grapes and batches that were produced under different conditions. These adjustments can be as simple as adjusting acid or tannin levels, to as complex as blending different varieties or vintages to achieve a consistent taste.\n\nFining agents are used during winemaking to remove tannins, reduce astringency and remove microscopic particles that could cloud the wines. The winemakers decide on which fining agents are used and these may vary from product to product and even batch to batch (usually depending on the grapes of that particular year).\n\nGelatin [gelatine] has been used in winemaking for centuries and is recognized as a traditional method for wine fining, or clarifying. It is also the most commonly used agent to reduce the tannin content. Generally no gelatin remains in the wine because it reacts with the wine components, as it clarifies, and forms a sediment which is removed by filtration prior to bottling.\n\nBesides gelatin, other fining agents for wine are often derived from animal products, such as micronized potassium casseinate (casein is milk protein), egg whites, egg albumin, bone char, bull's blood, isinglass (Sturgeon bladder), PVPP (a synthetic compound), lysozyme, and skim milk powder.\n\nSome aromatized wines contain honey or egg-yolk extract.\n\nNon-animal-based filtering agents are also often used, such as bentonite (a volcanic clay-based filter), diatomaceous earth, cellulose pads, paper filters and membrane filters (thin films of plastic polymer material having uniformly sized holes).\n\nThe most common preservative used in winemaking is sulfur dioxide (SO), normally added in one of the following forms: liquid sulfur dioxide, sodium or potassium metabisulphite. Another useful preservative is potassium sorbate.\n\nSulfur dioxide has two primary actions, firstly it is an anti microbial agent and secondly an anti oxidant. In the making of white wine it can be added prior to fermentation and immediately after alcoholic fermentation is complete. If added after alcoholic ferment it will have the effect of preventing or stopping malolactic fermentation, bacterial spoilage and help protect against the damaging effects of oxygen. Additions of up to 100 mg per liter (of sulfur dioxide) can be added, but the available or free sulfur dioxide should be measured by the aspiration method and adjusted to 30 mg per liter. Available sulfur dioxide should be maintained at this level until bottling. For rose wines smaller additions should be made and the available level should be no more than 30 mg per liter.\n\nIn the making of red wine, sulfur dioxide may be used at high levels (100 mg per liter) prior to ferment to assist in color stabilization. Otherwise, it is used at the end of malolactic ferment and performs the same functions as in white wine. However, small additions (say, ) should be used to avoid bleaching red pigments and the maintenance level should be about 20 mg/L. Furthermore, small additions (say 20 mg per liter) may be made to red wine after alcoholic ferment and before malolactic ferment to overcome minor oxidation and prevent the growth of acetic acid bacteria.\n\nWithout the use of sulfur dioxide, wines can readily suffer bacterial spoilage no matter how hygienic the winemaking practice.\n\nPotassium sorbate is effective for the control of fungal growth, including yeast, especially for sweet wines in bottle. However, one potential hazard is the metabolism of sorbate to geraniol which is a potent and unpleasant by-product. The production of geraniol occurs only if sorbic acid is present during malo-lactic fermentation. To avoid this, either the wine must be sterile bottled or contain enough sulfur dioxide to inhibit the growth of bacteria. Sterile bottling includes the use of filtration.\n\nSome winemakers practice natural wine making where no preservative is added. Once the wine is bottled and corked, the bottles are put into refrigeration with temperatures near .\n\nFiltration in winemaking is used to accomplish two objectives, clarification and microbial stabilization. In clarification, large particles that affect the visual appearance of the wine are removed. In microbial stabilization, organisms that affect the stability of the wine are removed therefore reducing the likelihood of re-fermentation or spoilage.\nThe process of clarification is concerned with the removal of particles; those larger than for coarse polishing, particles larger than 1–4 micrometers for clarifying or polishing. Microbial stabilization requires a filtration of at least 0.65 micrometers for yeast retention and 0.45 µm for bacteria retention. However, filtration at this level may lighten a wine's color and body. Microbial stabilization does not imply sterility, i.e. eliminating (removing) or killing (deactivating) of all forms of life and other biological agents. It simply means that a significant amount of yeast and bacteria has been removed to a harmless level for the wine stability.\n\nClarification of the wine can take place naturally by putting the wine into refrigeration at . The wine takes about a month to settle and it is clear. No chemicals are needed.\n\nA final dose of sulfite is added to help preserve the wine and prevent unwanted fermentation in the bottle. The wine bottles then are traditionally sealed with a cork, although alternative wine closures such as synthetic corks and screwcaps, which are less subject to cork taint, are becoming increasingly popular. The final step is adding a capsule to the top of the bottle which is then heated for a tight seal.\n\nTraditionally known as a \"vintner\", a winemaker is a person engaged in making wine. They are generally employed by wineries or .\n\nList of top 15 wine producing countries by volume. (Volume in thousands of hectoliters)\n\n\n\n"}
