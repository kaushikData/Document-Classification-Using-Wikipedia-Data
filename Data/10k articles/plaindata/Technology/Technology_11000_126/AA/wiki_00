{"id": "27562676", "url": "https://en.wikipedia.org/wiki?curid=27562676", "title": "1&amp;1 Internet", "text": "1&amp;1 Internet\n\n1&1 IONOS, founded in 1988, is a web hosting company owned by United Internet, a German Internet company. Its CEO is Achim Weiss. The company is one of the world's largest web hosting companies, employing over 7000 people worldwide, with offices in 10 countries and with data centers in Europe and in Lenexa, Kansas.\n\n\n\nThe company offers domain registration, cloud servers, virtual private servers (VPS), and dedicated servers. The company offers shared hosting packages on the Linux and Windows platforms, and it offers a drag-and-drop template product, MyWebsite. \n\nCustomers are only able to complete orders on the UK website if they have a UK address, other customers may have German or USA Kansas hosting depending which website is used for ordering. There is a site available for France, Spain, and Mexico as well. \n\nIn 2006, the company estimated that it hosted more websites than any other company in Germany and the United Kingdom, and that it hosted the seventh-largest number of websites among web hosting companies in the United States.\n\nThe Company's world headquarters is located in Montabaur, Germany. The U.S. headquarters is located in Chesterbrook, Pennsylvania. \n\n"}
{"id": "896483", "url": "https://en.wikipedia.org/wiki?curid=896483", "title": "Absolute (perfumery)", "text": "Absolute (perfumery)\n\nUsed in perfumery and aromatherapy, absolutes are similar to essential oils. They are concentrated, highly aromatic, oily mixtures extracted from plants. Whereas essential oils are produced by distillation, boiling or pressing, absolutes are produced through solvent extraction, or more traditionally, through enfleurage.\n\nFirst, plant material is extracted with a hydrocarbon solvent, such as hexane, to yield a concrete. The concrete is then extracted with ethanol. The ethanol extract is cooled (e.g., to -15 °C) to solidify waxes, and cold filtered to yield a liquid extract. When the ethanol evaporates, an oil — the absolute — is left behind.\n\nTraditionally, the absolute was obtained by enfleurage, where the resulting pommade was extracted with ethanol to yield the absolute.\n\nSome raw materials are either too delicate or too inert to be steam-distilled and can only yield their aroma through solvent extraction. Examples of these are jasmine and beeswax. Absolutes in demand include rose, jasmine, tuberose, jonquil, ylang-ylang, mimosa, boronia, lavender, lavandin, geranium, clary sage, violet, oak moss, tonka bean.\n\nRose oil, jasmine absolute, tuberose absolute, orris root oil, ambrette seeds oil, angelica root oil, and orange flower oil are valuable and expensive fragrance and flavor ingredients.\n\nResidual solvents may remain in the absolutes. Therefore, some absolutes are considered undesirable for aromatherapy.\n\n"}
{"id": "42874426", "url": "https://en.wikipedia.org/wiki?curid=42874426", "title": "AliExpress", "text": "AliExpress\n\nAliExpress is an online retail service based in China that is owned by Alibaba. Launched in 2010, it is made up of small businesses in China and elsewhere, such as Singapore, that offer products to international online buyers. It is the most visited e-commerce website in Russia and was the 10th most popular website in Brazil.\nIt facilitates small businesses to sell to customers all over the world, and one can find almost anything for sale. It is more accurately compared to eBay, as sellers are independent, using AliExpress as a host to sell to consumers and other businesses.\n\nSellers on AliExpress can be either companies or individuals. AliExpress is different from Amazon because it acts only as an e-commerce platform and doesn’t sell products directly to consumers. It directly connects Chinese businesses with buyers. The main difference from the Chinese shopping site Taobao is that AliExpress is aimed primarily at international buyers mostly in the United States, Russia, Brazil and Spain.\nAlibaba is using AliExpress to expand its reach outside of Asia and challenge online giants like Amazon and eBay.\nAliExpress is using affiliate marketing to find new consumers.\n\nAliExpress started as a business-to-business buying and selling portal. It has since expanded to business-to-consumer, consumer-to-consumer, cloud computing, and payment services, as well. AliExpress is currently available in the languages English, Spanish, Dutch, French, Italian, Polish, Portuguese and Russian. Customers outside of the country boundaries for these languages are automatically served the English version of the service.\n\nAliExpress doesn't allow \"customers\" in mainland China to buy from the platform, even though all \"retailers\" are mostly Chinese.\n\nAliExpress has a popular affiliate marketing program where partners are rewarded for sending visitors to the site with a commission on sales.\n"}
{"id": "3927666", "url": "https://en.wikipedia.org/wiki?curid=3927666", "title": "Business intelligence software", "text": "Business intelligence software\n\nBusiness intelligence software is a type of application software designed to retrieve, analyze, transform and report data for business intelligence. The applications generally read data that has been previously stored, often - though not necessarily - in a data warehouse or data mart.\n\nThe first comprehensive business intelligence systems were developed by IBM and Siebel (currently acquired by Oracle) in the period between 1970 and 1990. At the same time, small developer teams were emerging with attractive ideas, and pushing out some of the products companies still use nowadays.\n\nIn 1988, specialists and vendors organized a Multiway Data Analysis Consortium in Rome, where they considered making data management and analytics more efficient, and foremost available to smaller and financially restricted businesses. By 2000, there were many professional reporting systems and analytic programs, some owned by top performing software producers in the United States of America.\n\nIn the years after 2000, business intelligence software producers became interested in producing universally applicable BI systems which don’t require expensive installation, and could hence be considered by smaller and midmarket businesses which could not afford on premise maintenance. These aspirations emerged in parallel with the cloud hosting trend, which is how most vendors came to develop independent systems with unrestricted access to information.\n\nFrom 2006 onwards, the positive effects of cloud-stored information and data management transformed itself to a completely mobile-affectioned one, mostly to the benefit of decentralized and remote teams looking to tweak data or gain full visibility over it out of office. As a response to the large success of fully optimized uni-browser versions, vendors have recently begun releasing mobile-specific product applications for both Android and iOS users. Cloud-hosted data analytics made it possible for companies to categorize and process large volumes of data, which is how we can currently speak of unlimited visualization, and intelligent decision making.\n\nThe key general categories of business intelligence applications are:\n\nExcept for spreadsheets, these tools are provided as standalone applications, suites of applications, components of Enterprise resource planning systems, application programming interfaces or as components of software targeted to a specific industry. The tools are sometimes packaged into data warehouse appliances.\n\n\n\n"}
{"id": "5324835", "url": "https://en.wikipedia.org/wiki?curid=5324835", "title": "Button copy", "text": "Button copy\n\nButton copy is a deprecated physical design of road signs in the United States. Round plastic retroreflective buttons made of transparent plastic are placed in rows following the contours of sign legend elements, usually painted white, such as letters, numbers, arrows, and borders. In daylight, the buttons visually blend with the white sign legend elements and so are minimally conspicuous. At night, light from each approaching vehicle's headlamps strikes the retroreflective buttons and is reflected back towards the eyes of the vehicle's driver. Thus the sign is made sufficiently conspicuous and legible for adequately fast and accurate recognition and interpretation by drivers.\nButton copy is no longer manufactured; technologically it has been supplanted by retroreflective sheeting made by various manufacturers in numerous colors and grades. As state departments of transportation increasingly stopped specifying button copy signs in favor of signs made with sheeting, it became uneconomic to maintain production of the materials and supplies to make button copy signs. The last state to specify button copy signs was Arizona, which switched to sheeting in 2000.\n\n"}
{"id": "8098634", "url": "https://en.wikipedia.org/wiki?curid=8098634", "title": "CEN/TC 125", "text": "CEN/TC 125\n\nCEN/TC 125 (CEN Technical Committee 125) is a technical decision making body within the CEN system working on standardization in the field of Masonry, including natural stone, in the European Union.\n\n"}
{"id": "30306484", "url": "https://en.wikipedia.org/wiki?curid=30306484", "title": "Cognitive infocommunications", "text": "Cognitive infocommunications\n\nCognitive infocommunications (CogInfoCom) investigates the link between the research areas of infocommunications and the cognitive sciences, as well as the various engineering applications which have emerged as the synergic combination of these sciences.\n\nThe primary goal of CogInfoCom is to provide a systematic view of how cognitive processes can co-evolve with infocommunications devices so that the capabilities of the human brain may not only be extended through these devices, irrespective of geographical distance, but may also interact with the capabilities of any artificially cognitive system. This merging and extension of cognitive capabilities is targeted towards engineering applications in which artificial and/or natural cognitive systems are enabled to work together more effectively.\n\nTwo important dimensions of cognitive infocommunications are the mode of communication and the type of communication.\nThe mode of communication refers to the actors at the two endpoints of communication:\n\n\nThe type of communication refers to the type of information that is conveyed between the two communicating entities, and the way in which this is done:\n\n\n\nThe first draft definition of CogInfoCom was given in \"Cognitive Infocommunications: CogInfoCom\". The definition was finalized based on the paper with the joint participation of the Startup Committee at the 1st International Workshop on Cognitive Infocommunications, held in Tokyo, Japan in 2010. A recent overview and further information can be found in, and in the two special issues on CogInfoCom which have been published since then, and at the official website of CogInfoCom.\n"}
{"id": "30871703", "url": "https://en.wikipedia.org/wiki?curid=30871703", "title": "CompTIA", "text": "CompTIA\n\nThe Computing Technology Industry Association (CompTIA), a non-profit trade association, issuing professional certifications for the information technology (IT) industry. It is considered one of the IT industry's top trade associations. Based in Downers Grove, Illinois, CompTIA issues vendor-neutral professional certification in over 120 countries. The organization releases over 50 industry studies annually to track industry trends and changes. Over 2.2 million people have earned CompTIA certifications since the association was established.\n\nCompTIA was created in 1982 as the Association of Better Computer Dealers (ABCD). ABCD later changed its name to the Computing Technology Industry Association.\n\nIn 2010, CompTIA moved into its world headquarters in Downers Grove, Illinois. The building was designed to meet LEED CI Certification standards.\n\nThe CompTIA portal moved to a hybrid version of the open-access model in April 2014 with exclusive content for due-paying members. The move expanded the organization's reach to engage a broader, more diverse set of members and within a year, CompTIA's membership grew from 2,050 members to more than 50,000 in 2015. By the close of 2016, the organization boasted more than 100,000 members worldwide.\n\nCompTIA launched the Dream IT program in 2014 to provide resources for girls and women in the United States interested in the IT industry. In October 2015, the program was expanded into the UK.\n\nSkillsboost, CompTIA's online resource for schools, was launched in June 2015. It contained resources for students, parents and teachers to promote the importance of computer skills. CompTIA held its first annual ChannelCon Vendor Summit in 2015. The Vendor Summit is exclusive to people attending ChannelCon, the industry's premier conference for collaboration, education and networking. It addresses issues within the IT industry. \n\nIn January 2017, CompTIA launched an IT professional association built on its acquisition of the Association of Information Technology Professionals.\n\nCompTIA administers its vendor-neutral certification exams through Pearson VUE testing centers.\n\nThe CompTIA \"IT Fundamentals\" certification covers foundational IT concepts, basic IT literacy, and terminology and concepts of the IT industry. It is considered the first step toward the A+ certification. \n\n\n\"Advanced Security Practitioner certification\" (CASP) is a certification intended to follow Security+. The CompTIA Advanced Security Practitioner certification was accredited by the International Organization for Standardization (ISO) and the American National Standards Institute (ANSI) on December 13, 2011. The CASP exam will certify that the successful candidate has the technical knowledge and skills required to conceptualize, design, and engineer secure solutions across complex enterprise environments. In March 2013, the Department of Defense approved the certification a baseline certification accepted for Information Assurance Technical Level III, IS Manager Level II and IA Systems Architect and Engineer Levels I and II.\n\n\"Certified Document Imaging Architect\", or \"CDIA+\", is a certification for competency in document imaging, document management, and enterprise content management.\n\nThe \"Certified Technical trainer\" or \"CTT+\" certification is a vendor-neutral certification that is applicable to training professionals in all industries. Originally administered in 2001 through The Chauncey Institute, the CTT program was acquired by CompTIA and renamed as CTT+. It was created through a collaboration of the Information Technology Training Association, Inc. (ITTA) and the Computer Education Management Association (CedMA).\n\nThe CompTIA \"Healthcare IT Technician\" certificate focused on IT in the healthcare industry and was aimed at IT professionals who install and maintain electronic health record systems. It was retired in 2017.\n\nIn January 2018, CompTIA released stackable certifications:\n\n\n\n\n\n\nThe State & Local Government and Education (SLED) division of CompTIA is a consortium of executives from leading technology companies \"focused on advancing the interests of the IT industry in the SLED market\". \n\nCompTIA established a 501(c)(3) charitable foundation called Creating IT Futures. In 2012, Creating IT Futures worked with the Wounded Warrior Project to provide 5,000 CompTIA certification testing vouchers to injured military veterans.\n\nPreviously, CompTIA marketed its flagship A+, Network+ and Security+ certifications as being valid for a lifetime. In January 2011, CompTIA changed the status of these certifications so that they would expire every three years. Under this proposal, certified individuals would have to re-certify for the exams or pay a yearly maintenance fee for a CEU (Continuing Education Units) system. CompTIA modified the guidelines so that only certificates received after January 1, 2011, would need to be renewed every three years; however, a certain amount of documented hours geared towards use of the certification will automatically renew the certification. The un-expirable certificates, issued before 2011, are officially called Good-for-Life, and getting a more updated (and expirable) certification does not replace the Good-for-Life one – the professional can have both.\n\n\n"}
{"id": "291003", "url": "https://en.wikipedia.org/wiki?curid=291003", "title": "Digital Compact Cassette", "text": "Digital Compact Cassette\n\nThe Digital Compact Cassette (DCC) is a magnetic tape sound recording format introduced by Philips and Matsushita in late 1992 and marketed as the successor to the standard analog Compact Cassette. It was also a direct competitor to Sony's MiniDisc (MD), but neither format toppled the then-ubiquitous analog cassette despite their technical superiority. Another competing format, the Digital Audio Tape (DAT) had by 1992 also failed to sell in large quantities (although it was established in recording studios)—DCC was envisaged as a less expensive alternative to DAT. DCC shares a similar form factor to analog compact cassettes, and DCC recorders can play back either type of cassette. This backward compatibility allowed users to adopt digital recording without rendering their existing tape collections completely obsolete.\n\nDCC signalled the parting of ways of Philips and Sony, who had worked together successfully on the standard Compact Disc, CD-ROM, and CD-i before. Based on the success of Digital Audio Tape in professional environments, both companies saw a market for a new consumer-oriented digital audio recording system that would be less expensive and perhaps less fragile. Sony decided to create the entirely new MiniDisc format (based on their experience with magneto-optical recording and Compact Disc) while Philips decided on a tape format that was compatible with their earlier analog Compact Cassette format.\n\nDCC, initially referred to as S-DAT (stationary-head digital audio tape, as opposed to R-DAT—rotary-head digital audio tape), was developed in cooperation with Matsushita, and the first DCC recorders, were introduced at the CES in Chicago in May, 1992 and the consumer electronics show in Amsterdam in September 1992. At that time, not only Philips and Technics (brand of Matsushita) announced DCC recorders but also other brands such as Grundig and Marantz (both related to Philips at the time).\n\nMore recorders and players were introduced by Philips and other manufacturers in the following years, including some portable players and recorders as well as in-dash DCC/radio combinations for automotive use.\n\nAt the HCC-dagen computer fair in Utrecht, Netherlands, on November 24, 25, and 26, 1995, Philips presented the DCC-175 portable recorder that can be connected to an IBM-compatible PC using the \"PC-link\" cable. This was the only DCC recorder that can be connected to and controlled by a computer, and it was only ever available in the Netherlands.\n\nPhilips marketed the DCC format in Europe, the United States, and Japan. According to the newspaper article that announced the demise of DCC, DCC was more popular than MiniDisc in Europe (especially in the Netherlands).\n\nDCC was quietly discontinued in October 1996 after Philips admitted it had failed at achieving any significant market penetration with the format, and unofficially conceded victory to Sony. However, the MiniDisc format had not done very well either; the price of both systems had been too high for the younger market, and audiophiles rejected MD and DCC because in their opinion, the lossy compression deteriorated the audio quality too much.\n\nIn 2019 a documentary called DCC...There is still music left to write, will be released. All parties involved, inventing, creating, managing and rejuvenating the DCC format will finally be able to share the true story about DCC.\n\nDCC uses a 9-track magneto-resistive (MR) head for playback. The head is fixed to the mechanism of the recorder, unlike rotary heads that are used in helical scan systems such as DAT or VHS to increase head-to-tape speed. Because of the reduced number of moving parts, DCC decks are less sensitive to shock and vibration. And because of the dimensions that are so similar to analog compact cassettes, existing auto-reverse audio cassette recorder mechanisms can easily be adapted for use in DCC recorders simply by mounting a DCC head instead of only an analog head. In fact, Philips did this during development.\n\nMagneto-resistive heads do not use iron so they do not build up residual magnetism. They never need to be demagnetized, and if a cassette demagnetizer is used on MR heads, they are easily damaged or destroyed.\n\nVarious head assemblies were used, according to the service manuals:\n\nThe tape speed of DCC is the same as for analog compact cassettes: per second, DCCs use tape that is the same width as that from analog compact cassettes: 1/8 of an inch (3.175 mm). The tape that is used in production cassettes is chromium dioxide- or cobalt-doped ferric-oxide, 3-4 µm thick in a total tape thickness of 12 µm, identical to the tape that was widely in use for video tapes.\n\nNine heads are used to read/write half the width of the tape; the other half of the width are used for the B-side. Eight of these tracks contain audio data, the ninth track is used for auxiliary information such as song titles and track markers, as well as markers to make the player switch from side A to side B (with or without winding towards the end of the tape first) and end-of-tape markers.\n\nThe (theoretical) maximum capacity of a DCC tape is 120 minutes, compared to 3 hours for DAT; however, no 120-minute tapes were ever produced. Also, because of the time needed for the mechanism to switch direction, there is always a short interruption in the audio between the two sides of the tape. DCC recorders can record from digital sources that use the S/PDIF standard, at sample rates of 32 kHz, 44.1 kHz or 48 kHz, or they can record from analog sources at 44.1 kHz.\n\nBecause of the low tape speed, the achievable bit rate of DCC is limited. To compensate, DCC uses an audio compression codec based on MPEG-1 Audio Layer I (MP1) and termed PASC (precision adaptive sub-band coding). MPEG and PASC use digital filters to convert the audio into 32 frequency subbands, and then use adaptive allocation and scaling to decide how many bits should be assigned to represent each frequency band. When decoding, the subband bit stream is used to synthesize an uncompressed bit stream again. PASC lowers the typical bitrate of a CD recording of approximately 1.4 megabits per second to 384 kilobits per second, a compression ratio of around 4:1. The difference in quality between PASC and the 5:1 compression used by early versions of ATRAC in the original MiniDisc is largely a subjective matter.\n\nAfter adding system information (such as emphasis settings, SCMS information, time code) and adding Reed-Solomon error correction bits to the 384 kbit/s data stream, followed by 8b/10b encoding, the resulting bit rate is 768 kbit/s, which is recorded onto the eight data tracks at 96 kbit/s per track in a checkered pattern. According to the Philips webpage, it is possible for a DCC recorder to recover all missing data from a tape even if one of the 8 audio tracks is completely unreadable, or if all tracks are unreadable for 1.45 mm (about 0.03 seconds).\n\nOn prerecorded tapes, the information about album artist, album title, and track titles and lengths is recorded in the auxiliary ninth track continuously for the length of the entire tape. This makes it possible for players to recognize immediately what the tape position is and how to get to any of the other tracks (including which side of the tape to turn to), as soon as a tape is inserted and playback is started, regardless of whether the tape was rewound before inserting or not.\n\nOn user tapes, a track marker is recorded at the beginning of every track, so that it is possible to skip and repeat tracks automatically. The markers are automatically recorded when a silence is detected during an analog recording, or when a track marker is received in the S/PDIF signal of a digital input source (this track marker is automatically generated by CD players). It is possible to remove these markers (to \"merge tracks\"), or add extra markers (to \"split tracks\") without rerecording the audio. Furthermore, it is possible to add markers afterwards that will signal the end of the tape or the end of the tape side, so that during playback, the player will stop the mechanism or fast-forward to the end of the A-side or will switch from A-side to B-side immediately.\n\nOn later generations of recorders, it is possible to make a third tape type, referred to by service documentation as \"super-user tapes\". The DCC-730 and DCC-951 make it possible to enter title information for each track, which is recorded on the auxiliary track after the start-of-track marker. Because the title information is only stored in one place, so unlike prerecorded tapes where users can see the names of all tracks on a tape, it is not possible to see tracks names of any other track than the one that is currently playing.\n\nThe three tape types (prerecorded, standard-user, and super-user) are compatible with all recorders and it is impossible (and unnecessary) to recognize the difference between a standard-user tape and a super-user tape without playing it. There are some interesting minor compatibility problems with text on super-user tapes; for example:\n\nAll DCC recorders use the SCMS copy-protection system, which uses two bits in the S/PDIF digital audio stream and on tape to differentiate between protected vs. unprotected audio, and between original vs. copy:\n\nAnalog recording is not restricted: tapes recorded from analog source are marked \"unprotected\". The only limitation to analog recording on DCC as compared to that on DAT recorders is that the A/D converter is fixed to a sample frequency of 44.1 kHz. On the DCC-175 portable recorder it is possible to circumvent the SCMS protection by copying audio to the hard disk and then back to another tape, using the DCC Studio program.\n\nDCCs are similar to compact cassettes, except that there are no \"bulges\" where the tape-access holes are located. The top side of a DCC is flat and there are no access holes for the hubs on there (they are not required because auto-reverse is a standard feature on all DCC decks), so this side can be used for a bigger label than can be used on an analog compact cassette. A spring-loaded metal shutter similar to the shutters on 3.5 inch floppy disks and MiniDiscs covers the tape access holes and locks the hubs while the cassette is not in use. Cassettes provide several extra holes and indentations so that DCC recorders can tell a DCC apart from an analog compact cassette, and so they can tell what the length of a DCC tape is. Also, there is a sliding marker on the DCC to enable and disable recording. Unlike the break-away notches on analog compact cassettes and VHS tapes, this marker makes it easier to make a tape recordable again, and unlike on analog compact cassettes, the marker protects the entire tape rather than just one side.\n\nThe cases that DCCs come in generally do not have the characteristic folding mechanism of those for analog compact cassettes. Instead, DCC cases tend to be simply plastic boxes that are open on one of the short sides. The front side has a hole that is almost the size of the cassette, so that any label on the cassette is exposed even when the cassette is in its case. This allows the user to slide the cassette into and out of the case with one hand, and it reduced production costs, especially for prerecorded cassettes, because a label is needed only for the cassette rather than for the case. Format partner Matsushita does, however, produce blank cassettes (under their Panasonic brand) with a clam-shell-style case. Because DCCs have no \"bulges\" near the tape access holes, there is more space in the case behind the cassette to insert, for example, a booklet for a prerecorded tape, or a folded up card on which users could write the contents of the tape. In spite of the differences, the outside measurements of the standard DCC cases are exactly identical to the cases of analog compact cassettes, so they can be used in existing storage systems. The Matsushita-designed clam-shell case is slightly thinner than an analog compact cassette case is.\n\nThere is only one DCC recorder that has the capability of being connected to and controlled by a computer: the DCC-175. It is a portable recorder that was developed by Marantz in Japan (unlike most of the other Philips recorders which were developed in the Netherlands and Belgium), and looks very similar to the other portables available from Philips and Marantz at the time: the DCC-134 and the DCC-170. The DCC-175 was sold only in the Netherlands, and was available separately or in a package with the \"PC-link\" data cable which can be used to connect the recorder to a parallel port of an IBM-compatible PC. Only small quantities of both recorder and cable were made, leaving many people searching for one or both at the time of the demise of DCC.\n\nThe DCC-175 Service Manual shows that in the recorder, the cable was connected to the I²S bus that carries the PASC bitstream, and it is also connected to a dedicated serial port of the microcontroller, to allow the PC to control the mechanism and to read and write auxiliary information such as track markers and track titles. The parallel port connector of the cable contains a custom chip created especially for this purpose by Philips Key Modules, as well as a standard RAM chip. Philips made no detailed technical information available to the public about the custom chip and therefore it is impossible for people who own a DCC-175 but no PC-link cable to make their own version of the PC-link cable.\n\nThe PC-link cable package included software consisting of:\n\nPhilips also provided a DOS backup application via their BBS, and later on they provided an upgrade to the DCC Studio software to fix some bugs and provide better compatibility with Windows 95 which had come out just before the release of the DCC-175. The software also works with Windows 98 but not with any later versions of Windows.\n\nThe backup programs for DOS as well as Windows does not support long file names which had been introduced by Windows 95 just a few months before the release. Also, because the tape runs at its usual speed and data rate, it takes 90 minutes to record approximately 250 megabytes of uncompressed data. Other backup media common in those days are faster, have more capacity, and support long file names, so the DCC backup programs are relatively uninteresting for users.\n\nThe DCC Studio application, however, was a useful application that makes it possible to copy audio from tape to hard disk and vice versa, regardless of the SCMS status of the tape. This makes it possible to circumvent SCMS with DCC Studio. The program also allows users to manipulate the PASC audio files that were recorded to hard disk in various ways: they can change equalization settings, cut/copy and paste track fragments, and place and move audio markers and name those audio markers from the PC keyboard. It is possible to record a mix tape by selecting the desired tracks from a list, and moving the tracks around in a playlist. Then the user can click on the record button to copy the entire playlist back to DCC tape, while simultaneously recording markers (such as reverse and end-of-tape) and track titles. It is not necessary to record the track titles and tape markers separately (as you would do with a stationary recorder), and thanks to the use of a PC keyboard, it is possible to use characters in song titles that are not available when using a stationary machine's remote control.\n\nThe DCC Studio program uses the recorder as playback and recording device. Most PCs of that era do not have a sound card and none is needed either. Working with the PASC data directly without the need to compress and decompress, it also saves a lot of hard disk space, and most computers in that time would have had a hard time compressing and decompressing PASC data in real time anyway. However, many users complained that they would have liked to have the possibility of using uncompressed WAV audio files with the DCC Studio program, and Philips responded by mailing a floppy disk to registered users, containing programs to convert a WAV file to PASC and vice versa. Unfortunately this software is extremely slow (it takes several hours to compress a few minutes of PCM music in a WAV file to PASC) but it was soon discovered that the PASC files are simply MPEG-1 Audio Layer I files that use an under-documented padding feature of the MPEG standard to make all frames the same length, so then it became easy to use other MPEG decoding software to convert PASC to PCM and vice versa\n\nAfter YouTuber Techmoan released a video in 2014 about the DCC format, it started a revival of interest in the format across the globe. The next year (2015) the DCCMuseum opened its doors in Redondo Beach, California. The DCCMuseum has kept most of the momentum going by releasing new albums in 2017 and 2018 (in cooperation With Jeremy Heiden and Causing Change Media) and is curated by Ralf Porankiewicz AKA DrDCC. \n\nThere is only one DCC recorder that has the capability of being connected to and controlled by a computer: the DCC-175. It is a portable recorder that was developed by Marantz in Japan (unlike most of the other Philips recorders which were developed in the Netherlands and Belgium), and looks very similar to the other portables available from Philips and Marantz at the time: the DCC-134 and the DCC-170. The DCC-175 was sold only in the Netherlands, and was available separately or in a package with the \"PC-link\" data cable which can be used to connect the recorder to a parallel port of an IBM-compatible PC. Only small quantities of both recorder and cable were made, leaving many people searching for one or both at the time of the demise of DCC.\n\nThe DCC-175 Service Manual shows that in the recorder, the cable was connected to the I²S bus that carries the PASC bitstream, and it is also connected to a dedicated serial port of the microcontroller, to allow the PC to control the mechanism and to read and write auxiliary information such as track markers and track titles. The parallel port connector of the cable contains a custom chip created especially for this purpose by Philips Key Modules, as well as a standard RAM chip. Philips made no detailed technical information available to the public about the custom chip and therefore it is impossible for people who own a DCC-175 but no PC-link cable to make their own version of the PC-link cable.\n\nThe PC-link cable package included software consisting of:\n\n\nThe technology of using stationary MR heads was later developed by OnStream for use as a data storage media for computers. MR heads are now also commonly used in hard disks, although hard disks now use the GMR variant, whereas DCCs use the earlier AMR.\n\nA derivative technology developed originally for DCC is now being used for filtering beer. Silicon wafers with micrometer-scale holes are ideal for separating yeast particles from beer. The beer flows through the silicon wafer leaving the yeast particles behind, which results in a very clear beer. The manufacturing process for the filters was originally developed for the read/write heads of DCC decks.\n\n\n"}
{"id": "23754541", "url": "https://en.wikipedia.org/wiki?curid=23754541", "title": "Disgorger", "text": "Disgorger\n\nA disgorger is used in coarse fishing to remove a fish hook from deep inside the mouth of a fish that is not possible to reach using fingers alone. It is also known as the Unhooker or Hook remover. It is typically made of plastic or metal and is used on smaller fish. Larger fish that are hooked deep inside the mouth can be unhooked using a pair of forceps or a pair of long-nose pliers.\n\nA disgorger is used by slipping the end over a tight line and sliding down to the bend of the hook. A push is then needed to remove the hook. Under the pressure of the line, the hook will tighten against the end of the disgorger and can be removed from the mouth.\n"}
{"id": "35016780", "url": "https://en.wikipedia.org/wiki?curid=35016780", "title": "ETAOI keyboard", "text": "ETAOI keyboard\n\nETAOI keyboard is a text input software for touch screen-reliant devices. It is based on ETAOI method for coding.\n\nThe keyboard uses five keys. Characters and signs are typed using single and multiple taps, or slides. There are unique combinations of taps and slides for each character.\n\nWith no upper limit on the number of keys on which each combination is based, the keyboard can be used for typing an unlimited variety of characters. Combinations encompassing up to three keys are used for typing in Latin-derived alphabets.\n\nETAOI name is derived from the 5 most frequently used letters in English: e, t, a, o, i. These characters are typed using just single taps.\n\nThe same efficiency-driven approach has been followed when assigning meaningful combinations of taps and slides to all other letters, numbers and signs: the most frequently used ones are also the quickest to produce. This constitutes a radical departure from the QWERTY layout, where the position of specific letters remains unrelated to the frequency of their use.\n\nBecause to its novel conceptual basis, ETAOI is particularly well suited for use with screens of limited size. Once mastered, its five keys allow for touch typing (typing without looking) commonly associated with physical keyboards. Top recorded speeds are currently 55 words per minute.\n\nWith all the keys presented as just one horizontal line placed at the screen’s bottom edge, the keyboard allows the viewing area to be used much more effectively as it usually takes only a third of the space required by QWERTY keyboard variants. The keys’ larger size results in a significantly reduced number of typing errors.\n\nOther benefits include immediate access to characters other than letters without the need to recall additional screens, by simply applying the relevant combinations of taps and slides. In the same manner, the software can be used to quickly produce letters specific to only some Latin-derived alphabets. Languages currently supported are Spanish, German, French, Italian and Polish.\n\nThe keyboard also includes a suite of editing tools with in-built commands allowing to precisely select, copy and paste text.\n\nGiven its novelty, ETAOI keyboard is accompanied by a wide range of training materials. These aim to reduce the initial learning times to about 40–60 minutes.\n\n\"ETAOI alphabet\" is the recommended starting training programme. It helps the user memorize all the combinations, from the most frequently encountered to the least common ones.\n\n\"ETAOI speed\" helps the user learn how to type quicker, with the training material consisting of the most popular words, as well as pangrams (sentences using all letters of the alphabet).\n\nThe keyboard is currently available on the Google Play store and includes versions for smart phones and tablet computers.\n"}
{"id": "38845187", "url": "https://en.wikipedia.org/wiki?curid=38845187", "title": "Eclipse windmill", "text": "Eclipse windmill\n\nThe Eclipse windmill was one of the more successful designs of windmill used to pump water in the nineteenth century United States. It was invented by Leonard Wheeler, a Presbyterian minister who was working among the Ojibwe on the south shore of Lake Superior. Wheeler perfected the device on his missionary homestead for nearly two decades, unknown to the larger technological world. In 1866 health issues forced him to move to Beloit, Wisconsin, then a bustling industrial city, where he was persuaded to patent the basic function of the device. Although Wheeler died before he could witness the success of his invention, his sons carried on the legacy. Some of the companies that succeeded the original Eclipse Windmill Company remain viable in the 21st century.\n\nLeonard Hemenway Wheeler was born in Shrewsbury, MA, on April 18, 1811. His mother died when he was one month old. He was raised by his father and an aunt in Bridport, VT. He attended college at Middlebury College, where he graduated in 1837, and Andover Theological Seminary, where he graduated in 1840. Around this time he met and married Harriet Wood, similarly devout in her religious beliefs. Since Leonard was already committed to missionary work among the Ojibwe, the couple soon found themselves setting up a household at La Pointe, Wisconsin on Madeline Island in Wisconsin Territory.\n\nWheeler eventually became committed to the notion of helping the Ojibwe learn the agricultural skills needed to sustain themselves, especially since the fur trade, upon which they had been economically dependent for several generations, was declining dramatically. The island was not conducive to farming, so Wheeler moved to the mainland, and established a home and mission, naming it Odanah, the Ojibwe word for village. Many Ojibwe (there were around 1000 in the Chequamegon Bay region) were already semi-permanent residents of that location, because of wild rice fields at the mouth of the Bad River. The Ojibwe name of the location was Kietiganing.\n\nWheeler originally built his first windmill for the purpose of pumping water out of a deep ravine up to his home. He continually improved it over time, and it was eventually used for a variety of farming purposes such as grinding corn.\n\nThe hard winter life in the far north woods eventually took a toll on Wheeler. In 1866 he moved his family to Beloit, where many of his professional colleagues lived and where he was able to find friendship and support for his family while he sought out an alternative career.\n\nAfter moving to Beloit a cousin from Indiana, Samuel Chipman, became interested in Wheeler's windmill. Chipman persuaded Wheeler to work on a patent and provided financial support while Wheeler did so. On September 10, 1867, US Patent No, 68674 was issued. The patent had three claims.\n\nThe principal claim was for a regulating mechanism to keep wheel pointed at an optimum angle based on both the wind speed and direction. The main design problem for windmills is to capture as much power as possible in light wind, but not be damaged or destroyed by high-speed winds. In prior art, windmills had a simple tail vane which kept the wheel pointed directly at the wind, a simple design still in use in decorative garden windmills. Wheeler developed a mechanism that pointed the wheel into the wind at low to moderate wind speeds, but turned the wheel to point obliquely to the wind at high speeds. This was accomplished by the means of a secondary vane which shifted angles, held in an optimum position by weights through a series of pulleys. which operated against the main directional vane to optimize the angle.\n\nAn additional benefit to Wheeler's invention, which became second patent claim, was that by a simple shift in one of the hanging weights the motion of the wheel could be stopped altogether. His third claim was for a means by which to mount the vertical axis for the entire mechanism with a hollow shaft so that the ropes operating the regulating pulleys could pass down to the weights without restricting the rotational angle of the platform.\n\nIn an 1868 letter to his elderly father (still in Vermont), Wheeler described the device and his hopes for it this way:\n\nYou will like to know, perhaps, how I succeeded with the mill. I have spent a good deal of time on it, experimenting and getting it into good shape. It now suits, so far as the general plan of it is concerned. Two points have especially engaged my attention: 1: To get the mill so as to run nicely. 2 : To simplify and improve the construction of it, so that it will be strong and cheaply built. I have, as yet, put up but two hundred-dollar mills, both of which drive pumps in wells fifty feet deep, and give good satisfaction. Practical mechanics and machinists are much pleased with it, — it is so simple in its construction and accomplishes such important ends. It is self-regulating and will take care of itself in any wind, however gusty and strong it may blow. For deep wells on our prairies to raise water for watering stock, it is a mill, we think, which will be much wanted. It is not yet remunerative, the balance has been out of pocket thus far, but I hope the scale will turn it my favor before long. I suffer, as many inventors do, in not having the means to bring it rapidly into notice. But, as in every new thing, we must be contented to move slowly and wait patiently for results. The mill in its present form, is used only to work pumps. I hope soon to get up another form for driving machinery. I feel quite confident I can get one up in a shape that will give better satisfaction than anything now out, though it must be confessed there are not a few difficulties in the way. So you see I continue to keep busy, and, having something of the versatility common to the Yankees, if I can not do one thing I turn my hand to another. I shall never be able to preach any more, but if I can get up a useful mill and provide for the wants of my family, one part of the end of life will be answered.\n\nAs Wheeler mentions in the letter, a key aspect of his device, although not part of the patent, was the use of inexpensive interchangeable parts, lowering the overall manufacturing costs.\n\nAt first the windmill was manufactured by L. H. Wheeler and Sons. In February 1872 Wheeler died. In 1873 Wheeler's sons, along with S. T. Merrill and C. B. Salmon, organized the Eclipse Wind Mill Company and the company began to grow steadily. At first the wheel design used four vanes, but this was changed to a wheel with a dense pattern of fins, a signature of the Eclipse, and was manufactured in several diameters from 8 1/2 to 14 feet.\n\nThe Centennial International Exhibition of 1876, the first official World's Fair in the United States, was held in Philadelphia, Pennsylvania, from May 10 to November 10, 1876, to celebrate the 100th anniversary of the signing of the Declaration of Independence in Philadelphia. About 10 million visitors attended, equivalent to about 20% of the population of the United States at the time. Hundreds of machines and devices were exhibited, including a special exhibit of windmills, among which were three models by Eclipse as well as by the United States Wind-Engine & Pump Company (Batavia, Illinois), E. Stover & Brothers (Freeport, Illinois) and Gammon & Deering (Chicago, Illinois). The Eclipse windmill received the following citation:\n\nCommended as a thoroughly efficient, well-made engine, economical in work. We find a strong solid wheel varying in size from ten feet to sixteen feet diameter. The special claim is for a small controllable side vane, which acts as an overbalance and draws the sail away from the wind in a storm. It is prevented from acting unfavorably in a light wind, because it must first overcome the leverage of a weighted arm, which can be made adjustable by moving the weight nearer or farther from the centre. The turn-table travels on four friction balls running on a grooved surface. Table casting is made in one piece with the cap or bed plate, and has flanges on under side to receive the heads of the posts. The end of the piston-rod has a ball attachment which keeps it in place. There is no method for taking up the wear of this ball.\n\nIn 1880 Merrill and Salmon retired, and the company was renamed the Eclipse Wind Engine Company, with W. H. Wheeler as president. Twice thereafter the manufacturing plant was rebuilt, and Wheeler added friction clutch and gas engine products.\n\nIn the early 1890s, Charles Morse bought shares in the Eclipse Wind Engine Company. Eventually Morse merged the interests of Eclipse with those of the Fairbanks Company as well as other enterprises, organizing the combined company known as Fairbanks, Morse & Co..\n\nAt the turn of the 20th Century, the products of Fairbanks, Morse & Co., including the various models of the Eclipse Windmill, were sold all over the world. Eclipse windmills at the time were one of the top two brands in the United States and were also manufactured extensively in Germany. When the patent rights expired in 1901, many competing windmill companies copied the design.\n\nAfter World War I, the Fairbanks Morse company no longer manufactured the classic wood-fin Eclipse. However, they did produce several steel windmills that carried the Eclipse brand name.\n\nEclipse windmills are restored and protected by preservationists, two examples being:\n\n\n"}
{"id": "16051502", "url": "https://en.wikipedia.org/wiki?curid=16051502", "title": "European Hygienic Engineering and Design Group", "text": "European Hygienic Engineering and Design Group\n\nEHEDG, the (European Hygienic Engineering and Design Group) is a European-based non-governmental organization devoted to the advancement of hygienic design and food engineering.\nEHEDG is a consortium of equipment manufacturers, food industries, research institutes and public health authorities, founded in 1989 with the aim to promote hygiene during the processing and packing of food products.\n\nEuropean legislation requires that handling, preparation, processing, packaging, etc. of food is done hygienically, with hygienic machinery in hygienic premises (the food hygiene directive, the machine directive and the food contact materials directive).\n\nHow to comply with these requirements, however, is left to the industry. EHEDG provides practical guidance on hygienic engineering aspects to help complying to these requirements. As food safety does not end at the borders of Europe, the EHEDG actively promotes global harmonization of guidelines and standards.\n\nEHEDG publicise guidelines (approx. 25) on different aspects of hygienic design and provides certification for industries on equipment used in the food industry.\n\nTogether with EFFoST and Wageningen University, EHEDG runs the website Food-Info, which provides general information on foods to the public.\n\nThe Executive Committee consists of a President, Past President and twelve Members-At-Large.\n\nEHEDG is headquartered at the VDMA in Frankfurt am Main, Germany.\n\n"}
{"id": "50523514", "url": "https://en.wikipedia.org/wiki?curid=50523514", "title": "Gboard", "text": "Gboard\n\nGboard is a virtual keyboard app developed by Google for Android and iOS devices. It was first released on iOS in May 2016, followed by a release on Android in December 2016, debuting as a major update to the already-established Google Keyboard app on Android.\n\nGboard features Google Search, including web results and predictive answers, easy searching and sharing of GIF and emoji content, a predictive typing engine suggesting the next word depending on context, and multilingual language support. Updates to the keyboard have enabled additional functionality, including GIF suggestions, options for a dark color theme or adding a personal image as the keyboard background, support for voice dictation, next-phrase prediction, and hand-drawn emoji recognition. At the time of its launch on iOS, the keyboard only offered support for the English language, with more languages being gradually added in the following months, whereas on Android, the keyboard supported more than 100 languages at the time of release.\n\nIn August 2018 Gboard passed 1 billion installs on the Google Play Store, making it one of the most popular Android apps.\n\nGboard was first launched on the iOS operating system in May 2016, followed by a release on Android in December 2016, debuting as a major update for the already-established Google Keyboard app.\n\nIn August 2018 Gboard surpassed 1 billion installs on Android making it one of the most popular apps on the platform. This is measured by the Google Play Store and includes downloads by users as well as pre-installed instances of the app.\n\nGboard is a virtual keyboard app. It features Google Search, including web results and predictive answers, easy searching and sharing of GIF and emoji content, and a predictive typing engine suggesting the next word depending on context. At its May 2016 launch on iOS, Gboard only supported the English language, while it supported \"more than 100 languages\" at the time of its launch on the Android platform. Google states that Gboard will add more languages \"over the coming months\".\n\nGboard also supports one-handed mode on Android after its May 2016 update. This functionality was added to the app when it was branded as Google Keyboard.\n\nAn update for the iOS app released in August 2016 added French, German, Italian, Portuguese, and Spanish languages, as well as offering \"smart GIF suggestions\", where the keyboard will suggest GIFs relevant to text written. The keyboard also offers new options for a dark theme or adding a personal image from the camera roll as the keyboard's background. Another new update in March 2018 added Croatian, Czech, Danish, Dutch, Finnish, Greek, Polish, Romanian, Balochi, Swedish, Catalan, Hungarian, Malay, Russian, Latin American Spanish, and Turkish languages, along with support for voice dictation, enabling users to \"long press the mic button on the space bar and talk\". In April 2017, Google significantly increased the amount of Indian languages supported on Gboard, adding 11 new languages, bringing the total number of supported Indian languages to 22.\n\nIn June 2017, the Android app was updated to support recognition of hand-drawn emoji and the ability to predict whole phrases rather than single words. The functionality is expected to come to the iOS app at a later time.\n\nThe Virtual keyboard of Android i.e. Gboard also supports sending emojis that looks like you using Bitmoji.\n\n\"The Wall Street Journal\" praised the keyboard, particularly the integrated Google search feature. However, it was noted that the app does not currently support integration with other apps on the device, meaning that queries such as \"Buy Captain America movie tickets\" sends users to the web browser rather than an app for movie tickets installed on their phone. \"The Wall Street Journal\" also praised the predictive typing engine, stating that it \"blows past most competitors\" and \"it gets smarter with use\". They also discovered that Gboard \"cleverly suggests emojis as you type words\". It was noted that there was the lack of a one-handed mode, as well as lack of options for changing color or size of keys, writing that \"If you're looking to customize a keyboard, Gboard isn't for you.\"\n\n"}
{"id": "1994676", "url": "https://en.wikipedia.org/wiki?curid=1994676", "title": "Gobby", "text": "Gobby\n\nGobby is a free software collaborative real-time editor available on Windows and Unix-like platforms. (It runs on Mac OS X using Apple's X11.app.) It was initially released in June 2005 by the 0x539 dev group. (The hexadecimal value 0x539 is equal to 1337 in decimal.)\nGobby uses GTK+ for its GUI widgets.\n\nIt features a client-server architecture which supports multiple documents in one session, document synchronisation on request, password protection and an IRC-like chat for communication out of band. Users can choose a colour to highlight the text they have written in a document. Gobby is fully Unicode-aware, provides syntax highlighting for most programming languages and has basic Zeroconf support.\n\nA dedicated server called Sobby is also provided, together with a script which could format saved sessions for the web (e.g. to provide logs of meetings with a collaboratively prepared transcript). The collaborative editing protocol is named Obby, and there are other implementations that use this protocol (e.g. Rudel, a plugin for GNU Emacs). Gobby 0.5 replaces Sobby with a new server called infinoted.\n\nVersion 0.4.0 featured fully encrypted connections and further usability enhancements.\n\nVersions numbered 0.4.9x are preview releases for version 0.5.0. The most noticeable improvement is undo support, using the adOPTed algorithm for concurrency control.\n\n\n"}
{"id": "8554282", "url": "https://en.wikipedia.org/wiki?curid=8554282", "title": "Grove Field", "text": "Grove Field\n\nGrove Field is a public airport located three miles (5 km) north of the central business district of Camas, a city in Clark County, Washington, United States. It is located near Lacamas Lake which has a seaplane base. Due to the closing of Evergreen Field, many aircraft have moved to Grove Field.\n\nGrove Field covers an area of which contains one runway (7/25) with a 2,710 x 40 ft (826 x 12 m) asphalt pavement. For the 12-month period ending May 31, 2007, the airport had 10,000 general aviation aircraft operations, an average of 27 per day. At that time there were 86 aircraft based at this airport: 99% single-engine and 1% multi-engine. Flight training, aircraft rental and minor maintenance services are available at Grove. \n\nThe air field is named for Ward Grove who built the airfield in 1945. Grove moved to Vancouver, Washington in 1924 and learned to fly in Portland, Oregon at the Rankin Flying School operated by Tex Rankin. During World War II, Grove taught flying in California and returned to Clark County, Washington after the war.The Port of Camas-Washougal bought the airstrip from him in 1961 and in 1984, the Port Commission voted to name the field in his honor. Grove died in 1993. A fire on October 6, 2014, did $1 million in damage to 10 hangars at the airport.\n\n"}
{"id": "2063535", "url": "https://en.wikipedia.org/wiki?curid=2063535", "title": "Homogeneous charge compression ignition", "text": "Homogeneous charge compression ignition\n\nHomogeneous charge compression ignition (HCCI) is a form of internal combustion in which well-mixed fuel and oxidizer (typically air) are compressed to the point of auto-ignition. As in other forms of combustion, this exothermic reaction releases energy that can be transformed in an engine into work and heat.\n\nHCCI combines characteristics of conventional gasoline engine and diesel engines. Gasoline engines combine \"homogeneous charge\" (HC) with \"spark ignition\" (SI), abbreviated as HCSI. Diesel engines combine \"stratified charge\" (SC) with \"compression ignition\" (CI), abbreviated as SCCI.\n\nAs in HCSI, HCCI injects fuel during the intake stroke. However, rather than using an electric discharge (spark) to ignite a portion of the mixture, HCCI raises density and temperature by compression until the entire mixture reacts spontaneously.\n\nStratified charge compression ignition also relies on temperature and density increase resulting from compression. However, it injects fuel later, during the compression stroke. Combustion occurs at the boundary of the fuel and air, producing higher emissions, but allowing a leaner and higher compression burn, producing greater efficiency.\n\nControlling HCCI requires microprocessor control and physical understanding of the ignition process. HCCI designs achieve gasoline engine-like emissions with diesel engine-like efficiency.\n\nHCCI engines achieve extremely low levels of oxides of nitrogen emissions () without a catalytic converter. Hydrocarbons (unburnt fuels and oils) and carbon monoxide emissions still require treatment to meet automobile emissions control regulations.\n\nRecent research has shown that the hybrid fuels combining different reactivities (such as gasoline and diesel) can help in controlling HCCI ignition and burn rates. RCCI, or reactivity controlled compression ignition, has been demonstrated to provide highly efficient, low emissions operation over wide load and speed ranges..This HCCI system can be highly efficient with the help of free piston engine\n\nHCCI engines have a long history, even though HCCI has not been as widely implemented as spark ignition or diesel injection. It is essentially an Otto combustion cycle. HCCI was popular before electronic spark ignition was used. One example is the hot-bulb engine which used a hot vaporization chamber to help mix fuel with air. The extra heat combined with compression induced the conditions for combustion. Another example is the \"diesel\" model aircraft engine.\n\nA mixture of fuel and air ignites when the concentration and temperature of reactants is sufficiently high. The concentration and/or temperature can be increased in several different ways:\n\n\nOnce ignited, combustion occurs very quickly. When auto-ignition occurs too early or with too much chemical energy, combustion is too fast and high in-cylinder pressures can destroy an engine. For this reason, HCCI is typically operated at lean overall fuel mixtures.\n\n\n\nHCCI is more difficult to control than other combustion engines, such as SI and diesel. In a typical gasoline engine, a spark is used to ignite the pre-mixed fuel and air. In Diesel engines, combustion begins when the fuel is injected into pre-compressed air. In both cases, combustion timing is explicitly controlled. In an HCCI engine, however, the homogeneous mixture of fuel and air is compressed and combustion begins whenever sufficient pressure and temperature are reached. This means that no well-defined combustion initiator provides direct control. Engines must be designed so that ignition conditions occur at the desired timing. To achieve dynamic operation, the control system must manage the conditions that induce combustion. Options include the compression ratio, inducted gas temperature, inducted gas pressure, fuel-air ratio, or quantity of retained or re-inducted exhaust. Several control approaches are discussed below.\n\nTwo compression ratios are significant. The \"geometric compression ratio\" can be changed with a movable plunger at the top of the cylinder head. This system is used in diesel model aircraft engines. The \"effective compression ratio\" can be reduced from the geometric ratio by closing the intake valve either very late or very early with variable valve actuation (variable valve timing that enables the Miller cycle). Both approaches require energy to achieve fast response. Additionally, implementation is expensive, but is effective. The effect of compression ratio on HCCI combustion has also been studied extensively.\n\nHCCI's autoignition event is highly sensitive to temperature. The simplest temperature control method uses resistance heaters to vary the inlet temperature, but this approach is too slow to change on a cycle-to-cycle frequency. Another technique is \"fast thermal management\" (FTM). It is accomplished by varying the intake charge temperature by mixing hot and cold air streams. It is fast enough to allow cycle-to-cycle control. It is also expensive to implement and has limited bandwidth associated with actuator energy.\n\nExhaust gas is very hot if retained or re-inducted from the previous combustion cycle or cool if recirculated through the intake as in conventional EGR systems. The exhaust has dual effects on HCCI combustion. It dilutes the fresh charge, delaying ignition and reducing the chemical energy and engine output. Hot combustion products conversely increase gas temperature in the cylinder and advance ignition. Control of combustion timing HCCI engines using EGR has been shown experimentally.\n\n\"Variable valve actuation\" (VVA) extends the HCCI operating region by giving finer control over the temperature-pressure-time envelope within the combustion chamber. VVA can achieve this via either:\n\n\nWhile electro-hydraulic and camless VVA systems offer control over the valve event, the componentry for such systems is currently complicated and expensive. Mechanical variable lift and duration systems, however, although more complex than a standard valvetrain, are cheaper and less complicated. It is relatively simple to configure such systems to achieve the necessary control over the valve lift curve.\n\nAnother means to extend the operating range is to control the onset of ignition and the heat release rate by manipulating the fuel itself. This is usually carried out by blending multiple fuels \"on the fly\" for the same engine. Examples include blending of commercial gasoline and diesel fuels, adopting natural gas or ethanol \". This can be achieved in a number of ways:\n\n\nCompression Ignition Direct Injection (CIDI) combustion is a well-established means of controlling ignition timing and heat release rate and is adopted in diesel engine combustion. Partially Pre-mixed Charge Compression Ignition (PPCI) also known as Premixed Charge Compression Ignition (PCCI) is a compromise offering the control of CIDI combustion with the reduced exhaust gas emissions of HCCI, specifically lower soot. The heat release rate is controlled by preparing the combustible mixture in such a way that combustion occurs over a longer time duration making it less prone to knocking. This is done by timing the injection event such that a range of air/fuel ratios spread across the combustion cylinder when ignition begins. Ignition occurs in different regions of the combustion chamber at different times - slowing the heat release rate. This mixture is designed to minimize the number of fuel-rich pockets, reducing soot formation. The adoption of high EGR and diesel fuels with a greater resistance to ignition (more \"gasoline like\") enable longer mixing times before ignition and thus fewer rich pockets that produce soot and \n\nIn a typical ICE, combustion occurs via a flame. Hence at any point in time, only a fraction of the total fuel is burning. This results in low peak pressures and low energy release rates. In HCCI however, the entire fuel/air mixture ignites and burns over a much smaller time interval, resulting in high peak pressures and high energy release rates. To withstand the higher pressures, the engine has to be structurally stronger. Several strategies have been proposed to lower the rate of combustion and peak pressure. Mixing fuels, with different autoignition properties, can lower the combustion speed.\nHowever, this requires significant infrastructure to implement. Another approach uses dilution (i.e. with exhaust gases) to reduce the pressure and combustion rates (and output).\n\nIn the divided combustion chamber approach, there are two cooperating combustion chambers: a small auxiliary and a big main.<br>\nA high compression ratio is used in the auxiliary combustion chamber. <br>\nA moderate compression ratio is used in the main combustion chamber wherein a homogeneous air-fuel mixture is compressed / heated near, yet below, the auto-ignition threshold.<br>\nThe high compression ratio in the auxiliary combustion chamber causes the auto-ignition of the homogeneous lean air-fuel mixture therein (no spark plug required); the burnt gas bursts - through some \"transfer ports\", just before the TDC - into the main combustion chamber triggering its auto-ignition.<br>\nThe engine needs not be structurally stronger.\n\nIn ICEs, power can be increased by introducing more fuel into the combustion chamber. These engines can withstand a boost in power because the heat release rate in these engines is slow. However, in HCCI engines increasing the fuel/air ratio results in higher peak pressures and heat release rates. In addition, many viable HCCI control strategies require thermal preheating of the fuel, which reduces the density and hence the mass of the air/fuel charge in the combustion chamber, reducing power. These factors make increasing the power in HCCI engines challenging.\n\nOne technique is to use fuels with different autoignition properties. This lowers the heat release rate and peak pressures and makes it possible to increase the equivalence ratio. Another way is to thermally stratify the charge so that different points in the compressed charge have different temperatures and burn at different times, lowering the heat release rate and making it possible to increase power.\nA third way is to run the engine in HCCI mode only at part load conditions and run it as a diesel or SI engine at higher load conditions.\n\nBecause HCCI operates on lean mixtures, the peak temperature is much lower than that encountered in SI and diesel engines. This low peak temperature reduces the formation of , but it also leads to incomplete burning of fuel, especially near combustion chamber walls. This produces relatively high carbon monoxide and hydrocarbon emissions. An oxidizing catalyst can remove the regulated species, because the exhaust is still oxygen-rich.\n\nEngine knock or pinging occurs when some of the unburnt gases ahead of the flame in an SI engine spontaneously ignite. This gas is compressed as the flame propagates and the pressure in the combustion chamber rises. The high pressure and corresponding high temperature of unburnt reactants can cause them to spontaneously ignite. This causes a shock wave to traverse from the end gas region and an expansion wave to traverse into the end gas region. The two waves reflect off the boundaries of the combustion chamber and interact to produce high amplitude standing waves, thus forming a primitive thermoacoustic device where the resonance is amplified by the increased heat release during the wave travel similar to a Rijke tube.\n\nA similar ignition process occurs in HCCI. However, rather than part of the reactant mixture igniting by compression ahead of a flame front, ignition in HCCI engines occurs due to piston compression more or less simultaneously in the bulk of the compressed charge. Little or no pressure differences occur between the different regions of the gas, eliminating any shock wave and knocking, but the rapid pressure rise is still present and desirable from the point of seeking maximum efficiency from near-ideal isochoric heat addition.\n\nComputational models for simulating combustion and heat release rates of HCCI engines require detailed chemistry models. This is largely because ignition is more sensitive to chemical kinetics than to turbulence/spray or spark processes as are typical in SI and diesel engines. Computational models have demonstrated the importance of accounting for the fact that the in-cylinder mixture is actually in-homogeneous, particularly in terms of temperature. This in-homogeneity is driven by turbulent mixing and heat transfer from the combustion chamber walls. The amount of temperature stratification dictates the rate of heat release and thus tendency to knock. This limits the usefulness of considering the in-cylinder mixture as a single zone, resulting in the integration of 3D computational fluid dynamics codes such as Los Alamos National Laboratory's KIVA CFD code and faster solving probability density function modelling codes.\n\nAs of 2017, no HCCI engines were produced at commercial scale. However, several car manufacturers had functioning HCCI prototypes.\n\n\nTo date, few prototype engines run in HCCI mode, but HCCI research has resulted in advancements in fuel and engine development. Examples include:\n\n\n\n\n"}
{"id": "57825535", "url": "https://en.wikipedia.org/wiki?curid=57825535", "title": "ISO 9847", "text": "ISO 9847\n\nISO 9847, \"Solar energy -- Calibration of field pyranometers by comparison to a reference pyranometer,\" is an ISO standard for the calibration of pyranometers.\n"}
{"id": "53893494", "url": "https://en.wikipedia.org/wiki?curid=53893494", "title": "Institute of Food and Radiation Biology", "text": "Institute of Food and Radiation Biology\n\nInstitute of Food and Radiation Biology is a government research institute that studies food preservation methods through irradiation. The institute is part of the Bangladesh Atomic Energy Commission.\n\nThe institute was established on 1974 as the Irradiation and Pest Control Research Institute. In 1979 the institute was moved to Atomic Energy Research Establishment and renamed Institute of Food and Radiation Biology. The institute has an irradiation facility that offers its services at subsidized rate to Bangladeshi companies. The institute has a Cobalt-60 gamma irradiator that is operated by the Gamma Source Division.\n"}
{"id": "41257045", "url": "https://en.wikipedia.org/wiki?curid=41257045", "title": "Interaction Design Foundation", "text": "Interaction Design Foundation\n\nThe Interaction Design Foundation is a non-profit educational organization which produces open content and Open Access educational materials online with the stated goal of \"democratizing education by making world-class educational materials free for anyone, anywhere.\" \nMaterials are learning materials at graduate level targeted at both industry and academia in the fields of interaction design, computer science, user experience, information architecture, and design.\nThe centerpiece of the Interaction-Design.org is the peer reviewed \"Encyclopedia of Human-Computer Interaction\", which currently number 40+ textbooks written by 100+ leading designers and professors as well as commentaries and HD video interviews shot around the world. \nThe website features professional and academic textbooks, video lectures, a conference calendar, and a comprehensive bibliography of the most authoritative publications within the design of interactive technology.\n\nIn June 2013, the Interaction Design Foundation launched a 4 year 35,000 mile bike tour, named \"Share the Knowledge Tour\", to raise awareness of the rising cost of education - with weekly events on university campuses.\n\nFinancial sponsors include the German software company SAP. Authors include Harvard professor Clayton Christensen and New York Times bestselling author, Robert Spence who invented the \"magnifying glass\" visualization that is familiar to anyone with an iPhone or iMac, and Stu Card who performed the research that led to the computer mouse's commercial introduction by Xerox.\n\nThe Executive Board currently include Don Norman, Ken Friedman, Bill Buxton, Irene Au, Michael Arent, Daniel Rosenberg, Jonas Lowgren and Olof Schybergson.\n\n\n"}
{"id": "14843", "url": "https://en.wikipedia.org/wiki?curid=14843", "title": "Interstellar travel", "text": "Interstellar travel\n\nInterstellar travel is the term used for crewed or uncrewed travel between stars or planetary systems. Interstellar travel will be much more difficult than interplanetary spaceflight; the distances between the planets in the Solar System are less than 30 astronomical units (AU)—whereas the distances between stars are typically hundreds of thousands of AU, and usually expressed in light-years. Because of the vastness of those distances, interstellar travel would require a high percentage of the speed of light; huge travel time, lasting from decades to millennia or longer; or a combination of both.\n\nThe speeds required for interstellar travel in a human lifetime far exceed what current methods of spacecraft propulsion can provide. Even with a hypothetically perfectly efficient propulsion system, the kinetic energy corresponding to those speeds is enormous by today's standards of energy development. Moreover, collisions by the spacecraft with cosmic dust and gas can produce very dangerous effects both to passengers and the spacecraft itself.\n\nA number of strategies have been proposed to deal with these problems, ranging from giant arks that would carry entire societies and ecosystems, to microscopic space probes. Many different spacecraft propulsion systems have been proposed to give spacecraft the required speeds, including nuclear propulsion, beam-powered propulsion, and methods based on speculative physics.\n\nFor both crewed and uncrewed interstellar travel, considerable technological and economic challenges need to be met. Even the most optimistic views about interstellar travel see it as only being feasible decades from now. However, in spite of the challenges, if or when interstellar travel is realised, a wide range of scientific benefits is expected.\n\nMost interstellar travel concepts require a developed space logistics system capable of moving millions of tons to a construction / operating location, and most would require gigawatt-scale power for construction or power (such as Star Wisp or Light Sail type concepts). Such a system could grow organically if space-based solar power became a significant component of Earth's energy mix. Consumer demand for a multi-terawatt system would automatically create the necessary multi-million ton/year logistical system.\n\nDistances between the planets in the Solar System are often measured in astronomical units (AU), defined as the average distance between the Sun and Earth, some . Venus, the closest other planet to Earth is (at closest approach) 0.28 AU away. Neptune, the farthest planet from the Sun, is 29.8 AU away. As of January 2018, Voyager 1, the farthest man-made object from Earth, is 141.5 AU away.\n\nThe closest known star, Proxima Centauri, is approximately away, or over 9,000 times farther away than Neptune.\n\nBecause of this, distances between stars are usually expressed in light-years, defined as the distance that a light photon travels in a year. Light in a vacuum travels around per second, so this is some or in a year. Proxima Centauri is 4.243 light-years away.\n\nAnother way of understanding the vastness of interstellar distances is by scaling: One of the closest stars to the Sun, Alpha Centauri A (a Sun-like star), can be pictured by scaling down the Earth–Sun distance to . On this scale, the distance to Alpha Centauri A would be .\n\nThe fastest outward-bound spacecraft yet sent, Voyager 1, has covered 1/600 of a light-year in 30 years and is currently moving at 1/18,000 the speed of light. At this rate, a journey to Proxima Centauri would take 80,000 years.\n\nA significant factor contributing to the difficulty is the energy that must be supplied to obtain a reasonable travel time. A lower bound for the required energy is the kinetic energy formula_1 where formula_2 is the final mass. If deceleration on arrival is desired and cannot be achieved by any means other than the engines of the ship, then the lower bound for the required energy is doubled to formula_3.\n\nThe velocity for a manned round trip of a few decades to even the nearest star is several thousand times greater than those of present space vehicles. This means that due to the formula_4 term in the kinetic energy formula, millions of times as much energy is required. Accelerating one ton to one-tenth of the speed of light requires at least (world energy consumption 2008 was 143,851 terawatt-hours), without factoring in efficiency of the propulsion mechanism. This energy has to be generated onboard from stored fuel, harvested from the interstellar medium, or projected over immense distances.\n\nA knowledge of the properties of the interstellar gas and dust through which the vehicle must pass is essential for the design of any interstellar space mission. A major issue with traveling at extremely high speeds is that interstellar dust may cause considerable damage to the craft, due to the high relative speeds and large kinetic energies involved. Various shielding methods to mitigate this problem have been proposed. Larger objects (such as macroscopic dust grains) are far less common, but would be much more destructive. The risks of impacting such objects, and methods of mitigating these risks, have been discussed in the literature, but many unknowns remain and, owing to the inhomogeneous distribution of interstellar matter around the Sun, will depend on direction travelled. Although a high density interstellar medium may cause difficulties for many interstellar travel concepts, interstellar ramjets, and some proposed concepts for decelerating interstellar spacecraft, would actually benefit from a denser interstellar medium.\n\nThe crew of an interstellar ship would face several significant hazards, including the psychological effects of long-term isolation, the effects of exposure to ionizing radiation, and the physiological effects of weightlessness to the muscles, joints, bones, immune system, and eyes. There also exists the risk of impact by micrometeoroids and other space debris. These risks represent challenges that have yet to be overcome.\n\nIt has been argued that an interstellar mission that cannot be completed within 50 years should not be started at all. Instead, assuming that a civilization is still on an increasing curve of propulsion system velocity and not yet having reached the limit, the resources should be invested in designing a better propulsion system. This is because a slow spacecraft would probably be passed by another mission sent later with more advanced propulsion (the incessant obsolescence postulate). On the other hand, Andrew Kennedy has shown that if one calculates the journey time to a given destination as the rate of travel speed derived from growth (even exponential growth) increases, there is a clear minimum in the total time to that destination from now. Voyages undertaken before the minimum will be overtaken by those that leave at the minimum, whereas voyages that leave after the minimum will never overtake those that left at the minimum.\n\nThere are 59 known stellar systems within 40 light years of the Sun, containing 81 visible stars. The following could be considered prime targets for interstellar missions:\n\nExisting and near-term astronomical technology is capable of finding planetary systems around these objects, increasing their potential for exploration\n\nSlow interstellar missions based on current and near-future propulsion technologies are associated with trip times starting from about one hundred years to thousands of years. These missions consist of sending a robotic probe to a nearby star for exploration, similar to interplanetary probes such as used in the Voyager program. By taking along no crew, the cost and complexity of the mission is significantly reduced although technology lifetime is still a significant issue next to obtaining a reasonable speed of travel. Proposed concepts include Project Daedalus, Project Icarus, Project Dragonfly, Project Longshot, and more recently Breakthrough Starshot.\n\nNear-lightspeed nano spacecraft might be possible within the near future built on existing microchip technology with a newly developed nanoscale thruster. Researchers at the University of Michigan are developing thrusters that use nanoparticles as propellant. Their technology is called \"nanoparticle field extraction thruster\", or nanoFET. These devices act like small particle accelerators shooting conductive nanoparticles out into space.\n\nMichio Kaku, a theoretical physicist, has suggested that clouds of \"smart dust\" be sent to the stars, which may become possible with advances in nanotechnology. Kaku also notes that a large number of nanoprobes would need to be sent due to the vulnerability of very small probes to be easily deflected by magnetic fields, micrometeorites and other dangers to ensure the chances that at least one nanoprobe will survive the journey and reach the destination.\n\nGiven the light weight of these probes, it would take much less energy to accelerate them. With onboard solar cells, they could continually accelerate using solar power. One can envision a day when a fleet of millions or even billions of these particles swarm to distant stars at nearly the speed of light and relay signals back to Earth through a vast interstellar communication network.\n\nAs a near-term solution, small, laser-propelled interstellar probes, based on current CubeSat technology were proposed in the context of Project Dragonfly.\n\nIn crewed missions, the duration of a slow interstellar journey presents a major obstacle and existing concepts deal with this problem in different ways. They can be distinguished by the \"state\" in which humans are transported on-board of the spacecraft.\n\nA generation ship (or world ship) is a type of interstellar ark in which the crew that arrives at the destination is descended from those who started the journey. Generation ships are not currently feasible because of the difficulty of constructing a ship of the enormous required scale and the great biological and sociological problems that life aboard such a ship raises.\n\nScientists and writers have postulated various techniques for suspended animation. These include human hibernation and cryonic preservation. Although neither is currently practical, they offer the possibility of sleeper ships in which the passengers lie inert for the long duration of the voyage.\n\nA robotic interstellar mission carrying some number of frozen early stage human embryos is another theoretical possibility. This method of space colonization requires, among other things, the development of an artificial uterus, the prior detection of a habitable terrestrial planet, and advances in the field of fully autonomous mobile robots and educational robots that would replace human parents.\n\nInterstellar space is not completely empty; it contains trillions of icy bodies ranging from small asteroids (Oort cloud) to possible rogue planets. There may be ways to take advantage of these resources for a good part of an interstellar trip, slowly hopping from body to body or setting up waystations along the way.\n\nIf a spaceship could average 10 percent of light speed (and decelerate at the destination, for manned missions), this would be enough to reach Proxima Centauri in forty years. Several propulsion concepts have been proposed that might be eventually developed to accomplish this (see § Propulsion below), but none of them are ready for near-term (few decades) developments at acceptable cost.\n\nAssuming faster-than-light travel is impossible, one might conclude that a human can never make a round-trip farther from Earth than 20 light years if the traveler is active between the ages of 20 and 60. A traveler would never be able to reach more than the very few star systems that exist within the limit of 20 light years from Earth. This, however, fails to take into account relativistic time dilation. Clocks aboard an interstellar ship would run slower than Earth clocks, so if a ship's engines were capable of continuously generating around 1 g of acceleration (which is comfortable for humans), the ship could reach almost anywhere in the galaxy and return to Earth within 40 years ship-time (see diagram). Upon return, there would be a difference between the time elapsed on the astronaut's ship and the time elapsed on Earth.\n\nFor example, a spaceship could travel to a star 32 light-years away, initially accelerating at a constant 1.03g (i.e. 10.1 m/s) for 1.32 years (ship time), then stopping its engines and coasting for the next 17.3 years (ship time) at a constant speed, then decelerating again for 1.32 ship-years, and coming to a stop at the destination. After a short visit, the astronaut could return to Earth the same way. After the full round-trip, the clocks on board the ship show that 40 years have passed, but according to those on Earth, the ship comes back 76 years after launch.\n\nFrom the viewpoint of the astronaut, onboard clocks seem to be running normally. The star ahead seems to be approaching at a speed of 0.87 light years per ship-year. The universe would appear contracted along the direction of travel to half the size it had when the ship was at rest; the distance between that star and the Sun would seem to be 16 light years as measured by the astronaut.\n\nAt higher speeds, the time on board will run even slower, so the astronaut could travel to the center of the Milky Way (30,000 light years from Earth) and back in 40 years ship-time. But the speed according to Earth clocks will always be less than 1 light year per Earth year, so, when back home, the astronaut will find that more than 60 thousand years will have passed on Earth.\n\nRegardless of how it is achieved, a propulsion system that could produce acceleration continuously from departure to arrival would be the fastest method of travel. A constant acceleration journey is one where the propulsion system accelerates the ship at a constant rate for the first half of the journey, and then decelerates for the second half, so that it arrives at the destination stationary relative to where it began. If this were performed with an acceleration similar to that experienced at the Earth's surface, it would have the added advantage of producing artificial \"gravity\" for the crew. Supplying the energy required, however, would be prohibitively expensive with current technology.\n\nFrom the perspective of a planetary observer, the ship will appear to accelerate steadily at first, but then more gradually as it approaches the speed of light (which it cannot exceed). It will undergo hyperbolic motion. The ship will be close to the speed of light after about a year of accelerating and remain at that speed until it brakes for the end of the journey.\n\nFrom the perspective of an onboard observer, the crew will feel a gravitational field opposite the engine's acceleration, and the universe ahead will appear to fall in that field, undergoing hyperbolic motion. As part of this, distances between objects in the direction of the ship's motion will gradually contract until the ship begins to decelerate, at which time an onboard observer's experience of the gravitational field will be reversed.\n\nWhen the ship reaches its destination, if it were to exchange a message with its origin planet, it would find that less time had elapsed on board than had elapsed for the planetary observer, due to time dilation and length contraction.\n\nThe result is an impressively fast journey for the crew.\n\nAll rocket concepts are limited by the rocket equation, which sets the characteristic velocity available as a function of exhaust velocity and mass ratio, the ratio of initial (\"M\", including fuel) to final (\"M\", fuel depleted) mass.\n\nVery high specific power, the ratio of thrust to total vehicle mass, is required to reach interstellar targets within sub-century time-frames. Some heat transfer is inevitable and a tremendous heating load must be adequately handled.\n\nThus, for interstellar rocket concepts of all technologies, a key engineering problem (seldom explicitly discussed) is limiting the heat transfer from the exhaust stream back into the vehicle.\n\nA type of electric propulsion, spacecraft such as Dawn use an ion engine. In an ion engine, electric power is used to create charged particles of the propellant, usually the gas xenon, and accelerate them to extremely high velocities. The exhaust velocity of conventional rockets is limited by the chemical energy stored in the fuel’s molecular bonds, which limits the thrust to about 5 km/s. They produce a high thrust (about 10⁶ N), but they have a low specific impulse, and that limits their top speed. By contrast, ion engines have low force, but the top speed in principle is limited only by the electrical power available on the spacecraft and on the gas ions being accelerated. The exhaust speed of the charged particles range from 15 km/s to 35 km/s.<ref name=\"http://www.iflscience.com\"></ref>\n\nNuclear-electric or plasma engines, operating for long periods at low thrust and powered by fission reactors, have the potential to reach speeds much greater than chemically powered vehicles or nuclear-thermal rockets. Such vehicles probably have the potential to power solar system exploration with reasonable trip times within the current century. Because of their low-thrust propulsion, they would be limited to off-planet, deep-space operation. Electrically powered spacecraft propulsion powered by a portable power-source, say a nuclear reactor, producing only small accelerations, would take centuries to reach for example 15% of the velocity of light, thus unsuitable for interstellar flight during a single human lifetime.\n\nFission-fragment rockets use nuclear fission to create high-speed jets of fission fragments, which are ejected at speeds of up to . With fission, the energy output is approximately 0.1% of the total mass-energy of the reactor fuel and limits the effective exhaust velocity to about 5% of the velocity of light. For maximum velocity, the reaction mass should optimally consist of fission products, the \"ash\" of the primary energy source, so no extra reaction mass need be bookkept in the mass ratio.\n\nBased on work in the late 1950s to the early 1960s, it has been technically possible to build spaceships with nuclear pulse propulsion engines, i.e. driven by a series of nuclear explosions. This propulsion system contains the prospect of very high specific impulse (space travel's equivalent of fuel economy) and high specific power.\n\nProject Orion team member Freeman Dyson proposed in 1968 an interstellar spacecraft using nuclear pulse propulsion that used pure deuterium fusion detonations with a very high fuel-burnup fraction. He computed an exhaust velocity of 15,000 km/s and a 100,000-tonne space vehicle able to achieve a 20,000 km/s delta-v allowing a flight-time to Alpha Centauri of 130 years. Later studies indicate that the top cruise velocity that can theoretically be achieved by a Teller-Ulam thermonuclear unit powered Orion starship, assuming no fuel is saved for slowing back down, is about 8% to 10% of the speed of light (0.08-0.1c). An atomic (fission) Orion can achieve perhaps 3%-5% of the speed of light. A nuclear pulse drive starship powered by fusion-antimatter catalyzed nuclear pulse propulsion units would be similarly in the 10% range and pure matter-antimatter annihilation rockets would be theoretically capable of obtaining a velocity between 50% to 80% of the speed of light. In each case saving fuel for slowing down halves the maximum speed. The concept of using a magnetic sail to decelerate the spacecraft as it approaches its destination has been discussed as an alternative to using propellant, this would allow the ship to travel near the maximum theoretical velocity. Alternative designs utilizing similar principles include Project Longshot, Project Daedalus, and Mini-Mag Orion. The principle of external nuclear pulse propulsion to maximize survivable power has remained common among serious concepts for interstellar flight without external power beaming and for very high-performance interplanetary flight.\n\nIn the 1970s the Nuclear Pulse Propulsion concept further was refined by Project Daedalus by use of externally triggered inertial confinement fusion, in this case producing fusion explosions via compressing fusion fuel pellets with high-powered electron beams. Since then, lasers, ion beams, neutral particle beams and hyper-kinetic projectiles have been suggested to produce nuclear pulses for propulsion purposes.\n\nA current impediment to the development of \"any\" nuclear-explosion-powered spacecraft is the 1963 Partial Test Ban Treaty, which includes a prohibition on the detonation of any nuclear devices (even non-weapon based) in outer space. This treaty would, therefore, need to be renegotiated, although a project on the scale of an interstellar mission using currently foreseeable technology would probably require international cooperation on at least the scale of the International Space Station.\n\nAnother issue to be considered, would be the g-forces imparted to a rapidly accelerated spacecraft, cargo, and passengers inside (see Inertia negation).\n\nFusion rocket starships, powered by nuclear fusion reactions, should conceivably be able to reach speeds of the order of 10% of that of light, based on energy considerations alone. In theory, a large number of stages could push a vehicle arbitrarily close to the speed of light. These would \"burn\" such light element fuels as deuterium, tritium, He, B, and Li. Because fusion yields about 0.3–0.9% of the mass of the nuclear fuel as released energy, it is energetically more favorable than fission, which releases <0.1% of the fuel's mass-energy. The maximum exhaust velocities potentially energetically available are correspondingly higher than for fission, typically 4–10% of c. However, the most easily achievable fusion reactions release a large fraction of their energy as high-energy neutrons, which are a significant source of energy loss. Thus, although these concepts seem to offer the best (nearest-term) prospects for travel to the nearest stars within a (long) human lifetime, they still involve massive technological and engineering difficulties, which may turn out to be intractable for decades or centuries.\n\nEarly studies include Project Daedalus, performed by the British Interplanetary Society in 1973–1978, and Project Longshot, a student project sponsored by NASA and the US Naval Academy, completed in 1988. Another fairly detailed vehicle system, \"Discovery II\", designed and optimized for crewed Solar System exploration, based on the DHe reaction but using hydrogen as reaction mass, has been described by a team from NASA's Glenn Research Center. It achieves characteristic velocities of >300 km/s with an acceleration of ~1.7•10 \"g\", with a ship initial mass of ~1700 metric tons, and payload fraction above 10%. Although these are still far short of the requirements for interstellar travel on human timescales, the study seems to represent a reasonable benchmark towards what may be approachable within several decades, which is not impossibly beyond the current state-of-the-art. Based on the concept's 2.2% burnup fraction it could achieve a pure fusion product exhaust velocity of ~3,000 km/s.\n\nAn antimatter rocket would have a far higher energy density and specific impulse than any other proposed class of rocket. If energy resources and efficient production methods are found to make antimatter in the quantities required and store it safely, it would be theoretically possible to reach speeds of several tens of percent that of light. Whether antimatter propulsion could lead to the higher speeds (>90% that of light) at which relativistic time dilation would become more noticeable, thus making time pass at a slower rate for the travelers as perceived by an outside observer, is doubtful owing to the large quantity of antimatter that would be required.\n\nSpeculating that production and storage of antimatter should become feasible, two further issues need to be considered. First, in the annihilation of antimatter, much of the energy is lost as high-energy gamma radiation, and especially also as neutrinos, so that only about 40% of \"mc\" would actually be available if the antimatter were simply allowed to annihilate into radiations thermally. Even so, the energy available for propulsion would be substantially higher than the ~1% of \"mc\" yield of nuclear fusion, the next-best rival candidate.\n\nSecond, heat transfer from the exhaust to the vehicle seems likely to transfer enormous wasted energy into the ship (e.g. for 0.1\"g\" ship acceleration, approaching 0.3 trillion watts per ton of ship mass), considering the large fraction of the energy that goes into penetrating gamma rays. Even assuming shielding was provided to protect the payload (and passengers on a crewed vehicle), some of the energy would inevitably heat the vehicle, and may thereby prove a limiting factor if useful accelerations are to be achieved.\n\nMore recently, Friedwardt Winterberg proposed that a matter-antimatter GeV gamma ray laser photon rocket is possible by a relativistic proton-antiproton pinch discharge, where the recoil from the laser beam is transmitted by the Mössbauer effect to the spacecraft.\n\nRockets deriving their power from external sources, such as a laser, could replace their internal energy source with an energy collector, potentially reducing the mass of the ship greatly and allowing much higher travel speeds. Geoffrey A. Landis has proposed for an interstellar probe, with energy supplied by an external laser from a base station powering an Ion thruster.\n\nA problem with all traditional rocket propulsion methods is that the spacecraft would need to carry its fuel with it, thus making it very massive, in accordance with the rocket equation. Several concepts attempt to escape from this problem:\n\nIn 1960, Robert W. Bussard proposed the Bussard ramjet, a fusion rocket in which a huge scoop would collect the diffuse hydrogen in interstellar space, \"burn\" it on the fly using a proton–proton chain reaction, and expel it out of the back. Later calculations with more accurate estimates suggest that the thrust generated would be less than the drag caused by any conceivable scoop design. Yet the idea is attractive because the fuel would be collected \"en route\" (commensurate with the concept of \"energy harvesting\"), so the craft could theoretically accelerate to near the speed of light. The limitation is due to the fact that the reaction can only accelerate the propellant to 0.12c. Thus the drag of catching interstellar dust and the thrust of accelerating that same dust to 0.12c would be the same when the speed is 0.12c, preventing further acceleration.\n\nA light sail or magnetic sail powered by a massive laser or particle accelerator in the home star system could potentially reach even greater speeds than rocket- or pulse propulsion methods, because it would not need to carry its own reaction mass and therefore would only need to accelerate the craft's payload. Robert L. Forward proposed a means for decelerating an interstellar light sail in the destination star system without requiring a laser array to be present in that system. In this scheme, a smaller secondary sail is deployed to the rear of the spacecraft, whereas the large primary sail is detached from the craft to keep moving forward on its own. Light is reflected from the large primary sail to the secondary sail, which is used to decelerate the secondary sail and the spacecraft payload. In 2002, Geoffrey A. Landis of NASA's Glen Research center also proposed a laser-powered, propulsion, sail ship that would host a diamond sail (of a few nanometers thick) powered with the use of solar energy. With this proposal, this interstellar ship would, theoretically, be able to reach 10 percent the speed of light.\n\nA magnetic sail could also decelerate at its destination without depending on carried fuel or a driving beam in the destination system, by interacting with the plasma found in the solar wind of the destination star and the interstellar medium.\n\nThe following table lists some example concepts using beamed laser propulsion as proposed by the physicist Robert L. Forward:\n\nThe following table is based on work by Heller, Hippke and Kervella.\n\nAchieving start-stop interstellar trip times of less than a human lifetime require mass-ratios of between 1,000 and 1,000,000, even for the nearer stars. This could be achieved by multi-staged vehicles on a vast scale. Alternatively large linear accelerators could propel fuel to fission propelled space-vehicles, avoiding the limitations of the Rocket equation.\n\nScientists and authors have postulated a number of ways by which it might be possible to surpass the speed of light, but even the most serious-minded of these are highly speculative.\n\nIt is also debatable whether faster-than-light travel is physically possible, in part because of causality concerns: travel faster than light may, under certain conditions, permit travel backwards in time within the context of special relativity. Proposed mechanisms for faster-than-light travel within the theory of general relativity require the existence of exotic matter and it is not known if this could be produced in sufficient quantity.\n\nIn physics, the Alcubierre drive is based on an argument, within the framework of general relativity and without the introduction of wormholes, that it is possible to modify a spacetime in a way that allows a spaceship to travel with an arbitrarily large speed by a local expansion of spacetime behind the spaceship and an opposite contraction in front of it. Nevertheless, this concept would require the spaceship to incorporate a region of exotic matter, or hypothetical concept of negative mass.\n\nA theoretical idea for enabling interstellar travel is by propelling a starship by creating an artificial black hole and using a parabolic reflector to reflect its Hawking radiation. Although beyond current technological capabilities, a black hole starship offers some advantages compared to other possible methods. Getting the black hole to act as a power source and engine also requires a way to convert the Hawking radiation into energy and thrust. One potential method involves placing the hole at the focal point of a parabolic reflector attached to the ship, creating forward thrust. A slightly easier, but less efficient method would involve simply absorbing all the gamma radiation heading towards the fore of the ship to push it onwards, and let the rest shoot out the back.\n\nWormholes are conjectural distortions in spacetime that theorists postulate could connect two arbitrary points in the universe, across an Einstein–Rosen Bridge. It is not known whether wormholes are possible in practice. Although there are solutions to the Einstein equation of general relativity that allow for wormholes, all of the currently known solutions involve some assumption, for example the existence of negative mass, which may be unphysical. However, Cramer \"et al.\" argue that such wormholes might have been created in the early universe, stabilized by cosmic string. The general theory of wormholes is discussed by Visser in the book \"Lorentzian Wormholes\".\n\nThe Enzmann starship, as detailed by G. Harry Stine in the October 1973 issue of \"Analog\", was a design for a future starship, based on the ideas of Robert Duncan-Enzmann. The spacecraft itself as proposed used a 12,000,000 ton ball of frozen deuterium to power 12–24 thermonuclear pulse propulsion units. Twice as long as the Empire State Building and assembled in-orbit, the spacecraft was part of a larger project preceded by interstellar probes and telescopic observation of target star systems.\n\nProject Hyperion, one of the projects of Icarus Interstellar.\n\nNASA has been researching interstellar travel since its formation, translating important foreign language papers and conducting early studies on applying fusion propulsion, in the 1960s, and laser propulsion, in the 1970s, to interstellar travel.\n\nThe NASA Breakthrough Propulsion Physics Program (terminated in FY 2003 after a 6-year, $1.2-million study, because \"No breakthroughs appear imminent.\") identified some breakthroughs that are needed for interstellar travel to be possible.\n\nGeoffrey A. Landis of NASA's Glenn Research Center states that a laser-powered interstellar sail ship could possibly be launched within 50 years, using new methods of space travel. \"I think that ultimately we're going to do it, it's just a question of when and who,\" Landis said in an interview. Rockets are too slow to send humans on interstellar missions. Instead, he envisions interstellar craft with extensive sails, propelled by laser light to about one-tenth the speed of light. It would take such a ship about 43 years to reach Alpha Centauri if it passed through the system without stopping. Slowing down to stop at Alpha Centauri could increase the trip to 100 years, whereas a journey without slowing down raises the issue of making sufficiently accurate and useful observations and measurements during a fly-by.\n\nThe 100 Year Starship (100YSS) is the name of the overall effort that will, over the next century, work toward achieving interstellar travel. The effort will also go by the moniker 100YSS. The 100 Year Starship study is the name of a one-year project to assess the attributes of and lay the groundwork for an organization that can carry forward the 100 Year Starship vision.\n\nHarold (\"Sonny\") White from NASA's Johnson Space Center is a member of Icarus Interstellar, the nonprofit foundation whose mission is to realize interstellar flight before the year 2100. At the 2012 meeting of 100YSS, he reported using a laser to try to warp spacetime by 1 part in 10 million with the aim of helping to make interstellar travel possible.\n\n\nA few organisations dedicated to interstellar propulsion research and advocacy for the case exist worldwide. These are still in their infancy, but are already backed up by a membership of a wide variety of scientists, students and professionals.\n\nThe energy requirements make interstellar travel very difficult. It has been reported that at the 2008 Joint Propulsion Conference, multiple experts opined that it was improbable that humans would ever explore beyond the Solar System. Brice N. Cassenti, an associate professor with the Department of Engineering and Science at Rensselaer Polytechnic Institute, stated that at least 100 times the total energy output of the entire world [in a given year] would be required to send a probe to the nearest star.\n\nAstrophysicist Sten Odenwald stated that the basic problem is that through intensive studies of thousands of detected exoplanets, most of the closest destinations within 50 light years do not yield Earth-like planets in the star's habitable zones. Given the multi-trillion-dollar expense of some of the proposed technologies, travelers will have to spend up to 200 years traveling at 20% the speed of light to reach the best known destinations. Moreover, once the travelers arrive at their destination (by any means), they will not be able to travel down to the surface of the target world and set up a colony unless the atmosphere is non-lethal. The prospect of making such a journey, only to spend the rest of the colony's life inside a sealed habitat and venturing outside in a spacesuit, may eliminate many prospective targets from the list.\n\nMoving at a speed close to the speed of light and encountering even a tiny stationary object like a grain of sand will have fatal consequences. For example, a gram of matter moving at 90% of the speed of light contains a kinetic energy corresponding to a small nuclear bomb (around 30kt TNT).\n\nExplorative high-speed missions to Alpha Centauri, as planned for by the Breakthrough Starshot initiative, are projected to be realizable within the 21st century. It is alternatively possible to plan for unmanned slow-cruising missions taking millennia to arrive. These probes would not be for human benefit in the sense that one can not foresee whether there would be anybody around on earth interested in then back-transmitted science data. An example would be the Genesis mission, which aims to bring unicellular life, in the spirit of directed panspermia, to habitable but otherwise barren planets. Comparatively slow cruising Genesis probes, with a typical speed of formula_5, corresponding to about formula_6, can be decelerated using a magnetic sail. Unmanned missions not for human benefit would hence be feasible.\n\nIn February 2017, NASA announced that its Spitzer Space Telescope had revealed seven Earth-size planets in the TRAPPIST-1 system orbiting an ultra-cool dwarf star 40 light-years away from our solar system. Three of these planets are firmly located in the habitable zone, the area around the parent star where a rocky planet is most likely to have liquid water. The discovery sets a new record for greatest number of habitable-zone planets found around a single star outside our solar system. All of these seven planets could have liquid water – the key to life as we know it – under the right atmospheric conditions, but the chances are highest with the three in the habitable zone.\n\n\n\n"}
{"id": "6111026", "url": "https://en.wikipedia.org/wiki?curid=6111026", "title": "K-NFB Reader", "text": "K-NFB Reader\n\nThe K-NFB Reader (an acronym for Kurzweil — National Federation of the Blind Reader) is a handheld electronic reading device for the blind. It was developed in a partnership between Ray Kurzweil and National Federation of the Blind.\n\nThe original version of the reader was composed of a digital camera and a PDA, which contained specialised OCR software and speech synthesizers to read the scanned material aloud. It was released at a price of $3,495.\n\nThe software was later ported to the Symbian operating system, to be used on Nokia N82 camera phones, with a new price of $1,595.\n\nIn 2014, an iOS port was released at a price of $99. An Android version was released shortly after.\n\n\n"}
{"id": "7883908", "url": "https://en.wikipedia.org/wiki?curid=7883908", "title": "Kliek", "text": "Kliek\n\nA kliek (pronunciation: \"cleek\") is a heavy curved bat to play kolf with.\n\n\n<br>\n"}
{"id": "203848", "url": "https://en.wikipedia.org/wiki?curid=203848", "title": "League for Programming Freedom", "text": "League for Programming Freedom\n\nLeague for Programming Freedom (LPF) was founded in 1989 by Richard Stallman to unite free software developers as well as developers of proprietary software to fight against software patents and the extension of the scope of copyright. Their logo is the Statue of Liberty holding a floppy disk and tape spool.\n\nAmong other initiatives, the League started the \"Burn all GIFs\" campaign in opposition to the actions of Unisys in enforcing their patent on LZW compression used by CompuServe when creating the image format.\n\nThe League produced a newsletter, \"Programming Freedom\" 11 issues in 1991-1995. These primary source materials chronicle the work of the organization.\n\nThe single event that had the most influence on the creation of the League was Apple's lawsuits against Microsoft about supposed copyrights violations of the look and feel of the Macintosh in the development of Windows. After the lawsuit ended, the League went dormant, to be resurrected by those who were increasingly troubled by the enforcement of software patents.\n\nIn September 2009, LPF President Dean Anderson sent a notice to former members announcing the return of the LPF and reviving its membership, with plans for an election on 12 May 2010.\n\n"}
{"id": "39790668", "url": "https://en.wikipedia.org/wiki?curid=39790668", "title": "Lettering guide", "text": "Lettering guide\n\nA lettering guide template is a special type of template used to write uniform characters. It consists of a sheet of plastic or other material with cut-outs of letters, numbers, and other shapes used especially for creating technical drawings. For decades they have been essential for lettering a drawing nameplate so text and other designs could be made quickly and uniformly.\n\nAlthough they have been superseded by the use of computers, during the greater part of the last century they were used to relatively ease the lettering process in the creation of technical drawings. They were an indispensable tool for architects and technical illustrators in general, for labeling their drawings and plans but also for the description of projects, in which it was good practice to use a lettering template to achieve uniform and well-written text.\n\nA lettering template could also be used by people illiterate or semi-illiterate to learn to type, or improve their handwriting. In the course of political history some politicians, such as Bettino Craxi, have used them to help people with writing difficulties. They distributed cardboard templates with the sequence of characters of their last name, so they could be easily written during the voting process.\n\n"}
{"id": "75251", "url": "https://en.wikipedia.org/wiki?curid=75251", "title": "List of walls", "text": "List of walls\n\nSee List of fortifications for a list of notable fortified structures. \nFor city walls in particular, see List of cities with defensive walls (regional list: List of town walls in England and Wales).\n\n\n\n\n\n\n"}
{"id": "9387798", "url": "https://en.wikipedia.org/wiki?curid=9387798", "title": "Lorenz beam", "text": "Lorenz beam\n\nThe Lorenz beam was blind-landing radio navigation system developed by C. Lorenz AG in Berlin. The first system had been installed in 1932 at Berlin-Tempelhof Central Airport, followed by Dübendorf in Switzerland (1934) and others all over the world. The Lorenz company referred to it simply as the Ultrakurzwellen-Landefunkfeuer, German for \"ultra-short-wave landing radio beacon\", or LFF. Prior to the World War II the Germans had deployed the landing aid system at many Luftwaffe airfields in and outside Germany and equipped most of their bombers with the radio equipment needed to use it. The RAF continued using the system as late as 1960 at RAF Ternhill, under the name Standard Beam Approach (SBA).\n\nThe basic idea behind the short-range LFF system was later developed into a long-range system for air navigation known as Elektra. Further development produced a system that worked over very long distances, hundreds or thousands of kilometres, known as Sonne (or often, Elektra-Sonnen) that allowed aircraft and U-Boats to take fixes far into the Atlantic. The British captured Sonne receivers and maps and started to use it for their own navigation under the name Consol.\n\nThe blind approach navigation system was developed starting in 1932 by Dr. Ernst Kramar of the Lorenz company. It was adopted by Deutsche Lufthansa in 1934 and sold around the world. The Lorenz company was founded in 1880 by Carl Lorenz and is now part of ITT. \n\nLorenz used a single radio transmitter at 33.33 MHz (Anflugfunkfeuer) and three antennas placed in a line parallel to the end of the runway. The center antenna was always powered, while the other two were short circuited by a mechanical rotary switch turned by a simple motor. This resulted in a \"kidney\" shaped broadcast pattern centered on one of the two \"side\" antennas depending on which antenna had been short-circuited. The contacts on the switch were set so that one antenna was shorted for the time to be considered a \"Dot\" by a morse operator and the other as a \"Dash\". The signal could be detected for some distance off the end of the runway, as much as 30 km. The Lorenz obtained a sharper beam than could be created by an aerial array by having two lobes of signal.\nA pilot approaching the runway would tune his radio to the broadcast frequency and listen for the signal. If he heard a series of dots, he knew he was off the runway centerline to the left (the \"dot-sector\") and had to turn to the right to line up with the runway. If he was to the right, he would hear a series of dashes instead (the \"dash-sector\"), and turned left. The key to the operation of the system was an area in the middle where the two signals overlapped. The dots of the one signal \"filled in\" the dashes of the other, resulting in a steady tone known as the \"equi-signal\". By adjusting his path until he heard the equi-signal, the pilot could align his aircraft with the runway for landing.\n\nTwo small marker beacons were also used: one 300 m off the end of runway, the \"HEZ\" (Haupteinflugzeichen), and another 3 km away, the \"VEZ\" (Voreinflugzeichen), both were broadcast on 38 MHz and modulated at 1700 and 700 Hz, respectively. These signals were broadcast directly upward, and would be heard briefly as the aircraft flew over them. To approach the runway, the pilot would fly to a published altitude and then use the main directional signals to line up with the runway and started flying toward it. When he flew over the , he would start descending on a standard glide slope, continuing to land or abort at the , depending on whether or not he could see the runway.\n\nLorenz could fly a plane down a straight line with relatively high accuracy, enough so that the aircraft could then find the runway visually in all but the worst conditions. However it required a fairly constant monitoring of the radio by the pilot, who would often also be tasked with talking to the local control tower. In order to ease the workload, Lorenz later introduced a cockpit indicator that could listen to the signals and display the direction to the runway centerline as an arrow telling the pilot which direction to turn. The indicator also included two neon lamps to indicate when the aircraft crossed over each of the marker beacons. Later derivatives of the system had signals of equal length in the pattern left-right-silence, to operate a visual indicator in the cabin.\n\nThe Lorenz system was similar to the Diamond-Dunmore equi-signal radio guidance system, developed by the US Bureau of Standards in the early 1930s.\n\nIn the Second World War the Lorenz beam principle was used by the German Luftwaffe as the basis of a number of blind bombing aids, notably Knickebein ('crooked leg') and the X-Gerät ('X-Apparatus'), in their bombing offensive against English cities during the winter of 1940/41. Knickebein was very similar to LFF, modifying it only slightly to be more highly directional and work over much longer distance. Using the same frequencies allowed their bombers to use the already-installed LFF receivers, although a second receiver was needed in order to pinpoint a single location.\n\nThe X-Gerät involved cross-beams of the same characteristics but on different frequencies, which would both enable the pilot to calculate his speed (from the time between crossing the Fore Cross Signal and crossing the Main Cross Signal), and indicate when he should drop his payload. The calculation was performed by a mechanical computer. Lorenz modified this system to create the Viktoria/Hawaii lateral guidance system for the V-2 rocket.\n\nWhen the British discovered the existence of the 'Knickebein' system, they rapidly jammed it, however the 'X-Gerät' was not successfully jammed for quite some time. A later innovation by the Germans was the 'Baedeker' or 'Taub' modification, which used supersonic modulation. This was so quickly jammed that the Germans practically gave up on the use of beam-bombing systems, with the exception of the 'FuGe 25A', which operated for a short time towards the end of Operation Steinbock, known as the \"Baby Blitz\".\n\nA further operational drawback of the system was that bombers had to follow a fixed course between the beam transmitter station and the target; once the beam had been detected, defensive measures were made more effective by knowledge of the course.\n\n'Sonne' (Eng. 'Sun') was a derivation of Lorenz used by the Luftwaffe for long-range navigation out over the Atlantic using transmitters in Occupied Europe, and another in neutral Spain, and after its existence had been discovered by the British, under the direction of R. V. Jones it was allowed to continue in use, un-jammed, because it was felt that it was actually more useful to RAF Coastal Command than it was to the Germans. In British use the German system was named 'Consol', and it remained un-jammed for the period of the war.\n\nThe long range version developed by the Germans during the war was used by many countries for civilian purposes after the war, mostly under its English name Consol. Transmitters were installed in the US, the UK and the USSR.\n\nThe reason the Lorenz beam principle was necessary, with its overlapping beams, was because the sharpness of a beam increases approximately logarithmically with the length of the aerial array with which it is generated. A law of diminishing returns operates, such that to attain the sharpness achieved by the Lorenz system with a single beam (approximately 1 mile wide over a range of two hundred miles), an array of prohibitive size would be required.\n\n\n"}
{"id": "2953383", "url": "https://en.wikipedia.org/wiki?curid=2953383", "title": "Lyman James Briggs", "text": "Lyman James Briggs\n\nLyman James Briggs (May 7, 1874 – March 25, 1963) was an American engineer, physicist and administrator. He was a director of the National Bureau of Standards during the Great Depression and chairman of the Uranium Committee before America entered the Second World War. The Lyman Briggs College at Michigan State University is named in his honor.\n\nBriggs was born on a farm in Assyria, Michigan, near Battle Creek, Michigan. He was the eldest of two brothers in a family that descended from Clement Briggs, who arrived in America in 1621 on the \"Fortune\", the first ship to follow the \"Mayflower\". He grew up in an outdoor life with duties to attend such as would be found on an active farm in the late 19th century. He went to the Briggs School built by his grandfather and later was a teacher there.\n\nBriggs entered Michigan Agricultural College (now Michigan State University) in East Lansing, Michigan, entering by examination at age 15. Michigan State was a Land Grant college, so courses were taught in agriculture and mechanical arts. He majored in agriculture, but by graduation time in 1893 his interests had moved on to mechanical engineering and physics. He next entered the University of Michigan in Ann Arbor, Michigan, completing a master's degree in physics in 1895. From there he entered Johns Hopkins University in Baltimore, Maryland, and began work on his PhD.\n\nIn 1896 Briggs married Katharine Cook whom he met as an undergraduate at Michigan Agricultural College. Lyman and Katharine Cook Briggs had two children, a boy, Albert (known as \"Bertie\") and a girl, Isabel. Albert died in infancy, and Isabel would eventually marry Clarence Myers and go on to generate the Myers–Briggs Type Indicator with her mother ().\n\nIn 1896 he also joined the US Department of Agriculture in Washington, D.C. While in Washington, he also continued his research at Johns Hopkins under Henry Augustus Rowland. Although he spent time working with the newly discovered Roentgen Rays, he ultimately graduated in 1903 with a Ph.D. in agriculture with a dissertation \"On the absorption of water vapor and of certain salts in aqueous solution by quartz\". He was also elected to the Cosmos Club the same year.\n\nIn Briggs' first professional position he was put in charge of the Physics Laboratory (later the Bureau of Soils) of the US Department of Agriculture. He was one of a new breed of multi-disciplinary scientists studying the biology and ecology of plant life. His research work was concentrated on water retention of soils and he was a founder of the science of soil physics. In 1906 he devised a soil classification technique called the moisture equivalent based on centrifuging, which is now thought of as the first Pedotransfer function. In the same year he also organized a biophysical laboratory that later became the Bureau of Plant Industry. Briggs worked with Homer Leroy Shantz on the effect of environment on the water uptake by plants, and was an early contributor to ecology.\n\nBriggs was detailed by an Executive Order to the Department of Commerce's Bureau of Standards in 1917 due to mobilization pressures of World War I. There he developed an artificial horizon device for naval vessels with John Hayford which established a stable zenith independent of the roll of the vessel for the aiming of naval guns. This allowed for the roll of the ship to be observed, so that the firing of the guns could be timed with the roll of the ship. The device was so successful it found its way into the control rooms of most naval vessels. A confidential report called the Hayford–Briggs report was given to the Navy, but never published.\n\nIn 1920 Briggs officially left the Department of Agriculture and joined the National Bureau of Standards, where he was chief of the Engineering Physics Division (later the Mechanics and Sound Division). He appointed Hugh L. Dryden to head the Aerodynamics Physics Section, and together they pioneered research in the aerodynamics of airfoils moving near the speed of sound in an airstream. This work had significant application in developing blade forms for aircraft propellers.\n\nHe also retained an interest in navigational devices, and with Paul R. Heyl invented the Heyl–Briggs earth inductor compass. The compass used a spinning electric coil subjected to the magnetic field of the Earth to determine the bearing of an airplane in relation to the Earth's magnetic field. For this invention, they received the Magellan Medal of the American Philosophical Society in 1922. This type of compass was used by Admiral Byrd in his flight to the North Pole and by Charles Lindbergh on his 1927 trans-Atlantic flight.\n\nIn 1926 Briggs was appointed assistant director for research and testing by National Bureau of Standards Director George Kimball Burgess. On Burgess's death in 1932, Briggs was nominated by US President Herbert C. Hoover to Burgess as director of the National Bureau of Standards. However, none of Hoover's nominations were acted on by the US Congress before he left office. After Franklin D. Roosevelt took office as president in 1933 he was pressed to name \"a good Democrat\" as director of the National Bureau of Standards. Roosevelt, not wishing to make a patronage appointment, replied, \"I haven't the slightest idea whether Briggs is a Republican or a Democrat; all I know is that he is the best qualified man for the job.\" \n\nBriggs took over the Bureau during difficult times. It was the height of the depression and his first task was to reduce costs 50%. He managed to save the jobs of about 2/3 of the career employees by putting many on part-time employment and transferring others to the American Standards Association while they continued their work at the bureau. He emphasized doing work with direct economic impact and got money from the Works Progress Administration to hire unemployed mathematicians to develop math tables. Due to Briggs outstanding persuasive powers, he managed to get Congress to increase its appropriation for the Bureau in 1935, and many of the employees that were let go were re-hired.\n\nIn 1939, President Franklin Roosevelt called on Briggs, by then aged 65, to head the Advisory Committee on Uranium to investigate the fission of uranium, as a result of the Einstein–Szilárd letter. Even though Roosevelt had sanctioned a project, progress was slow and was not directed exclusively towards military applications. Eugene Wigner said that \"We often felt we were swimming in syrup\". Boris Pregel said \"It is wonder that after so many blunders and mistakes anything was accomplished at all\". Leó Szilárd believed that the project was delayed for a least a year by the short-sightedness and sluggishness of the authorities. At the time Briggs was not well and was due to undergo a serious operation. He was unable to take the energetic action that was often needed.\n\nMeanwhile, in the United Kingdom German refugees Otto Frisch and Rudolf Peierls under professor Marcus Oliphant made a breakthrough, indicating that it would be possible to make a bomb from purified U-235. From June 1940, copies of British progress reports were sent to Briggs via a British contact in Washington, Ralph H. Fowler. In March 1941 a British committee of Nobel Prize–winning scientists, called the MAUD Committee, concluded that an atomic bomb was \"not only feasible, it was inevitable\". They also pointed out that a large part of a laboratory in Berlin had been devoted to nuclear research. A copy of the MAUD Committee's interim report was sent to the Briggs in the USA because Britain lacked the resources to undertake such a large and urgent program on its own. Britain also wished to move its key research facilities to safety across the Atlantic. The MAUD Committee issued another report giving technical details on the design and costs on 15 July 1941.\n\nBritain was at war and felt an atomic bomb should have the highest priority, especially because the Germans might soon have one; but the US was not at war at that time and many Americans did not want to get involved. One of the members of the MAUD Committee, Marcus Oliphant flew to the United States in late August 1941 in an unheated bomber to find out why the United States was ignoring the MAUD Committee's findings. Oliphant said that: \"The minutes and reports had been sent to Lyman Briggs, who was the Director of the Uranium Committee, and we were puzzled to receive virtually no comment. I called on Briggs in Washington, only to find out that this inarticulate and unimpressive man had put the reports in his safe and had not shown them to members of his committee. I was amazed and distressed.\"\n\nOliphant then met the whole Uranium Committee. Samuel K. Allison was a new committee member, a talented experimentalist and a protégé of Arthur Compton at the University of Chicago. \"Oliphant came to a meeting\", Allison recalls, \"and said 'bomb' in no uncertain terms. He told us we must concentrate every effort on the bomb and said we had no right to work on power plants or anything but the bomb. The bomb would cost 25 million dollars, he said, and Britain did not have the money or the manpower, so it was up to us.\" Allison was surprised that Briggs had kept the committee in the dark.\n\nOliphant visited other physicists to galvanise the USA into action. As a result, in December 1941 Vannevar Bush, director of the powerful Office of Scientific Research and Development, undertook to launch a full-scale effort to develop atomic bombs. As the scale of the project became clearer, it came under direct military control as the Manhattan Project.\n\nBriggs sought new business for the Bureau. In 1939 he sent Secretary of Commerce Daniel C. Roper a list of services the Bureau could provide in the event of armed conflict in Europe. By 1942 90% of the Bureau's activities were classified work for the war effort. Some of the Bureau's activities were the non-rotating proximity fuze, guided missile developments (see the Bat), establishment of a Radio Propagation Laboratory, critical materials research on optical glass which Germany had previously supplied, on quartz and synthetic rubber and measurement and calibration services. Briggs changed the Bureau's culture from one of open access to one of secrecy.\n\nBriggs retired from the Bureau in 1945, at the age of 72. He was appointed director emeritus of NBS after working for 49 years in federal government. Bureau employees erected a bronze sundial in his honor through their Employees Welfare Association. At his request the names of the first three directors of Bureau are cast onto the rim of the instrument: Samuel Wesley Stratton, George Kimball Burgess, and Lyman James Briggs.\n\nIn 1948 Briggs received the Medal of Merit from US President Harry Truman for his distinguished work in connection with World War II.\n\nAt the request of Secretary of Commerce Henry A. Wallace, he wrote a 180-page account on NBS war research that was published in 1949.\n\nIn his retirement Briggs returned to research, establishing a laboratory for studying fluids under negative pressure at the National Bureau of Standards. This topic was directly related to his earlier research in the water uptake of plants. In one famous experiment he measured the negative pressure (or tension) that would break a column of water held in a capillary tube by capillary action. It turned out that at room temperature the maximum attainable tension in water was 250 bar and in mercury nearly 500 bar. This was published in several papers between (1950–1953), and the 1950 paper remains a classic and is still regularly cited in the literature on metastable water.\n\nBriggs' love for baseball triggered another piece of research. During World War II the government had mandated that rubber in baseballs be replaced by cork. Complaints about the new balls lead Briggs to test them, and in 1945 he demonstrated that the new baseballs were inferior. This was done by addressing the issue of whether or not a pitched baseball could curve out of the plane of the pitch. With the help of two pitchers from the Washington Senators baseball club and his 1917 wind tunnel he studied the effect of spin and speed on the trajectory and established the relationship between the amount of spin and the curvature of the ball (\"see\" curveball). To measure the spin, he attached a lightweight tape to the ball and counted the number of twists in the tape. This was a popular topic in newspapers and is probably the most widely known of his research.\n\nAnother of Briggs many interests was the National Geographic Society and in 1934 he chaired the Society's Committee on Research and Exploration. During this time he instrumented two stratospheric balloon flights, the second of which broke the existing record for altitude in 1936. During retirement he became more active in the Society, and lead an expedition to study the solar eclipse in Brazil in 1947. Briggs often wrote articles for the National Geographic Magazine.\n\nBriggs died March 25, 1963, aged 88, after a diverse life of scientific exploration and service. He is remembered for his range of interests. Briggs was almost universally liked, and had a reputation for even headedness and serenity. Edward U. Condon, Briggs' successor at the Bureau said: \"\"Briggs should always be remembered as one of the great figures in Washington during the first half of the century, when the Federal Government was slowly and stumblingly groping towards a realization of the important role science must play in the full future development of human society\".\"\n\nIn 2007, Michigan State University honored the Lyman Briggs school, named for Briggs, by allowing it to become the Lyman Briggs College.\n\nPositions:\n\nHonorary doctorates by the following institutions:\n\nBriggs received the following honors:\n\nServed as president of:\n\n\n\n\n"}
{"id": "9018311", "url": "https://en.wikipedia.org/wiki?curid=9018311", "title": "Machmeter", "text": "Machmeter\n\nA Machmeter is an aircraft pitot-static system flight instrument that\nshows the ratio of the true airspeed to the speed of sound, \na dimensionless quantity called Mach number. This is shown on a Machmeter as a decimal fraction.\nAn aircraft flying at the speed of sound is flying\nat a Mach number of one, expressed as \"Mach 1\".\n\nAs an aircraft in transonic flight approaches the speed of sound, \nit first reaches its critical mach number, where air flowing\nover low-pressure areas of its surface locally reaches the\nspeed of sound, forming shock waves. The indicated airspeed\nfor this condition changes with ambient temperature, \nwhich in turn changes with altitude.\nTherefore, indicated airspeed is not entirely adequate to\nwarn the pilot of the impending problems. Mach number is\nmore useful, and most high-speed aircraft are limited to a maximum operating Mach number, also known as M.\n\nFor example, if the M is Mach 0.83, then at where the speed of sound under standard conditions is , the true airspeed at M is . The speed of sound increases with air temperature, so at Mach 0.83 at where the air is much warmer than at , the true airspeed at M would be .\n\nModern electronic Machmeters use information from an air data computer system which makes calculations using inputs from a pitot-static system. Some older mechanical Machmeters use an altitude aneroid and an airspeed capsule which together convert pitot-static pressure into Mach number. The Machmeter suffers from instrument and position errors.\n\nIn subsonic flow the Mach meter can be calibrated according to:\n\nwhere:\n\nWhen a shock wave forms across the pitot tube the required formula is derived from the Rayleigh Supersonic Pitot equation, and is solved iteratively:\n\nwhere:\n\nNote that the inputs required are total pressure and static pressure. Air temperature input is not required.\n\n\n"}
{"id": "30299046", "url": "https://en.wikipedia.org/wiki?curid=30299046", "title": "Markus Neteler", "text": "Markus Neteler\n\nMarkus Neteler received his degree in Physical Geography and Landscape Ecology from the University of Hannover, Germany, in 1999 where he worked as a researcher and teaching assistant for two years.\n\nFrom 2001-2007, he was a researcher at Bruno Kessler Foundation (FBK) (formerly ITC-irst), Trento, Italy. In the period 2005-2007, while preserving the work of FBK researcher, he worked as the same figure for the Centro di Ecologia Alpina of Trento (Italy). From 2008 to 2016, he worked at the Edmund Mach Foundation (FEM) - San Michele all'Adige (Trento, Italy), as the coordinator of the GIS and Remote Sensing unit. In 2016 he moved to Bonn, Germany, where he co-founded the Mundialis company.\n\nHis main interests are remote sensing for environmental risk assessment and Free Software GIS development, especially GRASS GIS (of which he is the coordinator since 1999).\n\nHe co-authored two books on the use of the free and open source software GRASS GIS and several scientific papers on GIS.\n\nHe is a founding member of the GRASS Anwender-Vereinigung e.V. (Germany), the Open Source Geospatial Foundation (OSGeo, USA) and GFOSS.it the Italian association for geospatial free and open source software. In September 2006, he was honored with the Sol Katz award for Geospatial Free and Open Source Software (GFOSS) for his commitment to the GRASS project coordination. In 2010 he received his PhD in Natural Sciences (Dr. rer. nat.) in Physical Geography.\n\n\n"}
{"id": "1095698", "url": "https://en.wikipedia.org/wiki?curid=1095698", "title": "Mill (grinding)", "text": "Mill (grinding)\n\nA mill is a device that breaks solid materials into smaller pieces by grinding, crushing, or cutting. Such comminution is an important unit operation in many processes. There are many different types of mills and many types of materials processed in them. Historically mills were powered by hand (e.g., via a hand crank), working animal (e.g., horse mill), wind (windmill) or water (watermill). Today they are usually powered by electricity.\n\nThe grinding of solid materials occurs through mechanical forces that break up the structure by overcoming the interior bonding forces. After the grinding the state of the solid is changed: the grain size, the grain size disposition and the grain shape.\n\nMilling also refers to the process of breaking down, separating, sizing, or classifying aggregate material. For instance rock crushing or grinding to produce uniform aggregate size for construction purposes, or separation of rock, soil or aggregate material for the purposes of structural fill or land reclamation activities. Aggregate milling processes are also used to remove or separate contamination or moisture from aggregate or soil and to produce \"dry fills\" prior to transport or structural filling.\n\nGrinding may serve the following purposes in engineering:\n\nIn spite of a great number of studies in the field of fracture schemes there is no formula known which connects the technical grinding work with grinding results. To calculate the needed grinding work against the grain size changing three semi-empirical models are used. These can be related to the Hukki relationship between particle size and the energy required to break the particles. In stirred mills, the Hukki relationship does not apply and instead, experimentation has to be performed to determine any relationship.\n\n\n\n\nwith \"W\" as grinding work in kJ/kg, \"c\" as grinding coefficient, \"d\" as grain size of the source material and \"d\" as grain size of the ground material.\nA reliable value for the grain sizes \"d\" and \"d\" is \"d\". This value signifies that 80% (mass) of the solid matter has a smaller grain size. \nThe Bond's grinding coefficient for different materials can be found in various literature. To calculate the KICK's and Rittinger's coefficients following formulas can be used\n\nformula_4\n\nformula_5\n\nwith the limits of Bond's range: upper \"d\" = 50 mm and lower \"d\" = 0.05 mm.\n\nTo evaluate the grinding results the grain size disposition of the source material (1) and of the ground material (2) is needed. Grinding degree is the ratio of the sizes from the grain disposition. There are several definitions for this characteristic value:\n\n\n\nIn materials processing a grinder is a machine for producing fine particle size reduction through attrition and compressive forces at the grain size level. See also crusher for mechanisms producing larger particles. In general, grinding processes require a relatively large amount of energy; for this reason, an experimental method to measure the energy used locally during milling with different machines was recently proposed.\n\nA typical type of fine grinder is the ball mill. A slightly inclined or horizontal rotating cylinder is partially filled with balls, usually stone or metal, which grind material to the necessary fineness by friction and impact with the tumbling balls. Ball mills normally operate with an approximate ball charge of 30%. Ball mills are characterized by their smaller (comparatively) diameter and longer length, and often have a length 1.5 to 2.5 times the diameter. The feed is at one end of the cylinder and the discharge is at the other. Ball mills are commonly used in the manufacture of Portland cement and finer grinding stages of mineral processing, one example being the Sepro tyre drive Grinding Mill. Industrial ball mills can be as large as 8.5 m (28 ft) in diameter with a 22 MW motor, drawing approximately 0.0011% of the total world's power (see List of countries by electricity consumption). However, small versions of ball mills can be found in laboratories where they are used for grinding sample material for quality assurance.\n\nThe power predictions for ball mills typically use the following form of the Bond equation:\nwhere\n\nA rotating drum causes friction and attrition between steel rods and ore particles. But note that the term 'rod mill' is also used as a synonym for a slitting mill, which makes rods of iron or other metal. Rod mills are less common than ball mills for grinding minerals.\n\nThe rods used in the mill, usually a high-carbon steel, can vary in both the length and the diameter. However, the smaller the rods, the larger is the total surface area and hence, the greater the grinding efficiency\n\nAutogenous or autogenic mills are so-called due to the self-grinding of the ore: a rotating drum throws larger rocks of ore in a cascading motion which causes impact breakage of larger rocks and compressive grinding of finer particles. It is similar in operation to a SAG mill as described below but does not use steel balls in the mill. Also known as ROM or \"Run Of Mine\" grinding.\n\nSAG is an acronym for Semi-Autogenous Grinding. SAG mills are autogenous mills but use grinding balls like a ball mill. A SAG mill is usually a primary or first stage grinder. SAG mills use a ball charge of 8 to 21%. The largest SAG mill is 42' (12.8m) in diameter, powered by a 28 MW (38,000 HP) motor. A SAG mill with a 44' (13.4m) diameter and a power of 35 MW (47,000 HP) has been designed.\n\nAttrition between grinding balls and ore particles causes grinding of finer particles. SAG mills are characterized by their large diameter and short length as compared to ball mills. The inside of the mill is lined with lifting plates to lift the material inside the mill, where it then falls off the plates onto the rest of the ore charge. SAG mills are primarily used at gold, copper and platinum mines with applications also in the lead, zinc, silver, alumina and nickel industries.\n\nA rotating drum causes friction and attrition between rock pebbles and ore particles. May be used where product contamination by iron from steel balls must be avoided. Quartz or silica is commonly used because it is inexpensive to obtain.\n\nA high pressure grinding roll, often referred to as HPGRs or roller press, consists out of two rollers with the same dimensions, which are rotating against each other with the same circumferential speed. The special feeding of bulk material through a hopper leads to a material bed between the two rollers. The bearing units of one roller can move linearly and are pressed against the material bed by springs or hydraulic cylinders. The pressures in the material bed are greater than 50 MPa (7,000 PSI). In general they achieve 100 to 300 MPa. By this the material bed is compacted to a solid volume portion of more than 80%.\n\nThe roller press has a certain similarity to roller crushers and roller presses for the compacting of powders, but purpose, construction and operation mode are different.\n\nExtreme pressure causes the particles inside of the compacted material bed to fracture into finer particles and also causes microfracturing at the grain size level. Compared to ball mills HPGRs achieve a 30 to 50% lower specific energy consumption, although they are not as common as ball mills since they are a newer technology.\n\nA similar type of intermediate crusher is the edge runner, which consists of a circular pan with two or more heavy wheels known as mullers rotating within it; material to be crushed is shoved underneath the wheels using attached plow blades.\n\nAnother type of fine grinder commonly used is the buhrstone mill, which is similar to old-fashioned flour mills.\n\nA VSI mill throws rock or ore particles against a wear plate by slinging them from a spinning center that rotates on a vertical shaft. This type of mill uses the same principle as VSI Crusher\n\nTower mills, often called vertical mills, stirred mills or regrind mills, are a more efficient means of grinding material at smaller particle sizes, and can be used after ball mills in a grinding process. Like ball mills, grinding (steel) balls or pebbles are often added to stirred mills to help grind ore, however these mills contain a large screw mounted vertically to lift and grind material. In tower mills, there is no cascading action as in standard grinding mills. Stirred mills are also common for mixing quicklime (CaO) into a lime slurry. There are several advantages to the tower mill: low noise, efficient energy usage, and low operating costs.\n\n"}
{"id": "27863316", "url": "https://en.wikipedia.org/wiki?curid=27863316", "title": "NYLUG", "text": "NYLUG\n\nNYLUG (New York Linux Users Group) is a LUG (Linux User Group) based out of New York City. NYLUG supports all things Linux and FLOSS in the greater New York City area.\n\nNYLUG meets on a monthly basis, and features a speaker or speakers who give presentations of interest to the NYLUG membership. These presentations are generally either technical or related to FLOSS. NYLUG's first presentation was in January, 1999 and NYLUG has been in continuous operation since then. As such, NYLUG is the oldest LUG in New York City.\n\nNYLUG also runs a number of popular mailing lists and an IRC channel.\n\nIn addition to monthly presentations, NYLUG holds monthly workshops and occasional social events for its membership.\n\nA significant number of members of NYLUG were involved in planning for the first DebConf in the United States - which was held in New York City in August 2010.\n\n\n"}
{"id": "193688", "url": "https://en.wikipedia.org/wiki?curid=193688", "title": "Nail clipper", "text": "Nail clipper\n\nA nail clipper (also called nail clippers, a nail trimmer, a nail cutter or nipper type) is a hand tool used to trim fingernails, toenails and hangnails.\n\nNail clippers are usually made of stainless steel but can also be made of plastic and aluminium. Two common varieties are the plier type and the compound lever type. Most nail clippers usually come with another tool attached, which is used to clean dirt out from under nails. A nail clipper often has a miniature file fixed to it to allow rough edges of nails to be manicured. A nail file allows for removal of any excess nail that is jagged or has been missed. Nail clippers occasionally come with a pocket knife or a nail catcher; the multi-purpose nail clipper was invented by Hungarian inventor David Gestetner. The nail clipper consists of a head which may be concave or convex. Specialized nail clippers which have convex clipping ends are intended for trimming toenails, while concave clipping ends are for fingernails. The cutting head may be manufactured to be parallel or perpendicular to the principal axis of the cutter. Cutting heads which are parallel to the principal axis are made to address accessibility issues involved with cutting toenails.\n\nPrior to the invention of the modern nail clipper, people would use small knives to trim or pare their nails. Descriptions of nail trimming in literature date as far back as the 8th century BC. The Book of Deuteronomy exhorts in 21:12 that a man, should he wish to take a captive as a wife, \"shall bring her home to [his] house, and she shall shave her head and trim her nails\". A reference is made in Horace's \"Epistles\", written circa 20 BC, to \"A close-shaven man, it's said, in an empty barber's booth, penknife in hand, quietly cleaning his nails.\"\n\nThe first United States patent for an improvement in a finger-nail clipper was filed in 1875 by Valentine Fogerty and in the United Kingdom, Hungarian inventor David Gestetner. Other subsequent patents for an improvement in finger-nail clippers are those in 1876 by William C. Edge, and in 1878 by John H. Hollman. Filings for finger-nail clippers include, in 1881, those of Eugene Heim and Celestin Matz, in 1885 by George H. Coates (for a finger-nail \"cutter\"), and in 1905 by Chapel S. Carter with a later patent in 1922. Around 1913, Carter was secretary of the H. C. Cook Company of Ansonia, Connecticut, which was incorporated in 1903 as the H. C. Cook Machine Company by Henry C. Cook, Lewis I. Cook, and Chapel S. Carter. Around 1928, Carter was president of the company when, he claimed, about 1896, the \"Gem\"-brand finger nail clipper made its first appearance.\n\nIn 1947, William E. Bassett (who started the W. E. Bassett Company in 1939) developed the \"Trim\"-brand nail clipper, the first made using modern (at the time) manufacturing methods using the superior jaw-style design that had been around since the 19th century, but adding two nibs near the base of the file to prevent lateral movement, replacing the pinned rivet with a notched rivet, and adding a thumb-swerve in the lever.\n\n"}
{"id": "5047019", "url": "https://en.wikipedia.org/wiki?curid=5047019", "title": "National Chicken Council", "text": "National Chicken Council\n\nThe National Chicken Council (NCC) is a non-profit trade association based in Washington, D.C. that represents the interests of the United States chicken industry to the United States Congress and United States federal agencies.\n\nThe members of the NCC include chicken producers and processors, poultry distributors, and industry firms. Chicken producers and processors in the NCC account for approximately 95 percent of the chickens produced in the United States. Issues important to the council include biosecurity in the poultry industry and avian influenza. The council sponsors EatChicken.com, a website providing chicken recipes, cooking tips, food safety information and propaganda. In October 2011, Lampkin Butts, president and CEO of Sanderson Farms, was named the Chairman of the National Chicken Council. He will serve for one year.\n\n"}
{"id": "748887", "url": "https://en.wikipedia.org/wiki?curid=748887", "title": "Oak (wine)", "text": "Oak (wine)\n\nOak is used in winemaking to vary the color, flavor, tannin profile and texture of wine. It can be introduced in the form of a barrel during the fermentation or aging periods, or as free-floating chips or staves added to wine fermented in a vessel like stainless steel. Oak barrels can impart other qualities to wine through evaporation and low level exposure to oxygen.\n\nIn early wine history, the amphora was the vessel of choice for the storage and transportation of wine. Due to the perishable nature of wood material it is difficult to trace the usage of barrels in history. The Greek historian Herodotus noted that ancient Mesopotamians used barrels made of palm wood to transport wine along the Euphrates. Palm is a difficult material to bend and fashion into barrels, however, and wine merchants in different regions experimented with different wood styles to find a better wood source. The use of oak has been prevalent in winemaking for at least two millennia, first coming into widespread use during the time of the Roman Empire. In time, winemakers discovered that beyond just storage convenience, wine kept in oak barrels took on properties that improved it by making it softer and, in some cases, better-tasting.\n\nThe porous nature of an oak barrel allows evaporation and oxygenation to occur in wine but typically not at levels that would cause oxidation or spoilage. The typical 59-gallon (225-liter) barrel can lose anywhere from 5 to 6 gallons (21 to 25 liters) (of mostly alcohol and water) in a year through evaporation. This allows the wine to concentrate its flavor and aroma compounds. Small amounts of oxygen are allowed to pass through the barrel and act as a softening agent upon the wine's tannins.\n\nThe chemical properties of oak can have a profound effect on wine. Phenols within the wood interact to produce vanilla type flavors and can give the impression of tea notes or sweetness. The degree of \"toast\" on the barrel can also impart different properties affecting the tannin levels as well as the aggressive wood flavors. The hydrolyzable tannins present in wood, known as ellagitannins, are derived from lignin structures in the wood. They help protect the wine from oxidation and reduction.\nWines can be barrel fermented in oak or placed in oak after fermentation for a period of aging or maturation. Wine matured in oak receives more oak flavors and properties than wine fermented in oak because yeast cells present in fermentation interact with and \"latch on\" to oak components. When dead yeast cells are removed as lees some oak properties go with them. \n\nCharacteristics of white wines fermented in oak include a pale color and extra silky texture. White wines fermented in steel and matured in oak will have a darker coloring due to heavy phenolic compounds still present. Flavor notes commonly used to describe wines exposed to oak include caramel, cream, smoke, spice and vanilla. Chardonnay is a varietal with very distinct flavor profiles when fermented in oak, which include coconut, cinnamon and cloves notes. The \"toastiness\" of the barrel can bring out varying degrees of mocha and toffee notes in red wine.\n\nThe length of time a wine spends in the barrel is dependent on the varietal and finished style the winemaker desires. The majority of oak flavoring is imparted in the first few months the wine is in contact with oak, while longer term exposure adds light barrel aeration, which helps precipitate phenolic compounds and quickens the aging process. New World Pinot noir may spend less than a year in oak. Premium Cabernet Sauvignon may spend two years. The very tannic Nebbiolo grape may spend four or more years in oak. High end Rioja producers will sometimes age their wines up to ten years in American oak to get a desired earthy cedar and herbal character.\n\nThe species of oak typically used for American oak production is the \"Quercus alba\" which is a white oak species that is characterized by its relatively fast growth, wider grains and lower wood tannins. It is found in most of the Eastern United States as well as Missouri, Minnesota and Wisconsin where many wine barrels are from. In Oregon the \"Quercus garryana\" white oak has started to gain usage due to its closer similarities to European oak. \n\nIn France, both the \"Quercus robur\" (common oak) and \"Quercus petraea\" (white oak) are considered apt for wine making, however, the latter is considered far superior for its finer grain and richer contribution of aromatic components like vanillin and its derivates, methyl-octalactone and tannins, as well as phenols and volatile aldehydes.\nFrench oak typically comes from one or more primary forests: Allier, Limousin, Nevers, Tronçais and Vosges. The wood from each of these forests has slightly different characteristics. Many winemakers utilize barrels made from different cooperages, regions and degrees of toasting in blending their wines to enhance the complexity of the resulting wine.\n\nItalian winemakers have had a long history of using Slavonian oak from the \"Quercus robur\" which is known for its tight grain, low aromatics and medium level tannins. Slavonian oak tends to be used in larger barrel sizes (with less surface area relative to volume) with the same barrels reused for many more years before replacement. Prior to the Russian Revolution, \"Quercus petraea\" oak from the Baltic/European states especially from Hungary was the most highly sought after wood for French winemaking. The trees in the Hungarian Zemplén Mountains grow slower in the volcanic soil and smaller, creating fine tight grain which sequentially lends itself to a very delicate extraction.\n\nThe hemicellulose in the Hungarian oak breaks down more easily, and conveys an exceptional selection of toasted, vanilla, sugary, woody, spicy and caramel-like flavors – imparting these aromas with less intensity, and slower than American or French oak.\n\nMany winemakers favor the softer, smoother, creamier texture that Hungarian oak offers their wines. French winemakers preferred to use Hungarian barrels until the early 20th century, then – because of world wars, supply cut – the French wine industry was forced to find its own source in France, similar to the unique quality, legendary Hungarian Zemplén oak.\n\nHowever, after the fall of the Iron Curtain, the cooperages from France again became major consumers of the exclusive Quercus petraea/Sessile Hungarian Oak trees originating in the Zemplén Mountain Forest. \n\nThe Russian oak from the Adygey region along the Black Sea is being explored by French winemakers as a cheaper alternative to French and Hungarian oak. Canadian wineries have been experimenting with the use of Canadian oak, which proponents describe as a middle ground between American and French oak even though it is the same species as American oak. \n\nOak trees are typically between 80–120 years old prior to harvesting with the ideal conditions being a cool climate in a dense forest region that gives the trees opportunity to mature slowly and develop a tighter grain. Typically one tree can provide enough wood for two barrels. The trees are typically harvested in the winter months when there is less sap in the trunk.\n\nAmerican oak tends to be more intensely flavored than French oak with more sweet and vanilla overtones due to the American oak having two to four times as many lactones. Winemakers choose American oak typically for bold, powerful reds, base wines for \"assemblage\", or for warm climate Chardonnays. Besides being derived from different species, a major difference between American and French oak comes from the preparation of the wood. The tighter grain and less watertight nature of French oak obliges coopers to split the wood along the grain. The wood is then aged or \"seasoned\" for 24 to 36 months in the open air, in a so-called wood-yard. \nEven though American coopers may use a kiln-dry method to season the wood, almost all others will season American oak in exactly the same way as French. Open air seasoning has the advantage of leaching undesirable chemical components and bitter tannins, mellowing the oak in a manner that kiln-dry methods are incapable of replicating. \nEven though sun, rain, and wind may suffice in most cases to season oak, in drier climates coopers - such as Tonelería Nacional - apply up to 2000 mm (80 in) of water a year to their wood stacks in order to facilitate the seasoning process.\n\nSince French oak must be split, only 20 to 25% of the tree can be utilized; American oak may be serrated, which makes it at least twice as economical.\nIts more pronounced oxidation and a quicker release of aromas help wines to lose their astringency and harshness faster; which makes this the wood of choice for shorter maturations - six to ten months. Because of American oak’s modest tannin contribution, the perfect first fill is a wine with abundant tannins and good texture; it allows the fruit to interact harmoniously with the wood, which contributes a wide array of complex aromas and soft, yet very palatable tannins. \n\nFrench oak, on the other hand, generates silky and transparent tannins, which transmit a sensation of light sweetness combined with fruity flavors that persist in the mouth. Spices and toasted almond are noteworthy, combined with flavors of ripe red fruit in red wines, and notes of peach, exotic fruits and floral aromas like jasmine and rose in whites, depending on the grape variety employed.\n\n \nWine barrels, especially those made of oak, have long been used as containers in which wine is aged. Aging in oak typically imparts desirable vanilla, butter and spice flavors to wine. The size of the barrel plays a large role in determining the effects of oak on the wine by dictating the ratio of surface area to volume of wine with smaller containers having a larger impact. The most common barrels are the Bordeaux barriques style which hold followed by the Burgundy style barrel which hold . Some New World wine makers are now also using the larger hogshead barrel. Larger barrels are also traditionally used in parts of Italy such as Barolo, as well as the south of France. \n\nNew barrels impart more flavors than do previously used barrels. Over time many of the oak properties get \"leached\" out of the barrel with layers of natural deposits left from the wine building up on the wood to where after 3 to 5 vintages there may be little or no oak flavors imparted on the wine. In addition, oxygen transport through the oak and into the wine, which is required for maturation, becomes severely limited after 3–5 years. The cost of barrels varies due to the supply and demand market economy and can change with different features that a cooperage may offer. As of late 2007 the price for a standard American oak barrel was US$600 to 800, French oak US$1200 and up, and Eastern European US$600. Due to the expense of barrels, several techniques have been devised in an attempt to save money. One is to shave the inside of used barrels and insert new thin inner staves that have been toasted.\n\nBarrels are constructed in cooperages. The traditional method of European coopers has been to hand-split the oak into staves (or strips) along the grain. After the oak is split, it is allowed to \"season\" or dry outdoors while exposed to the elements. This process can take anywhere from 10 to 36 months during which time the harshest tannins from the wood are leached out. These tannins are visible as dark gray and black residue left on the ground once the staves are removed. The longer the wood is allowed to season the softer the potential wine stored in the barrels may be but this can add substantially to the cost of the barrel. In some American cooperage the wood is dried in a kiln instead of outdoor seasoning. While this method is much faster, it does not soften the tannins quite as much as outdoor seasoning.\n\nThe staves are then heated, traditionally over an open fire, and, when pliable, are bent into the desired shape of the barrel and held together with iron rings. Instead of fire, a cooper may use steam to heat up the staves but this tends to impart less \"toastiness\" and complexity to the resulting wine. Following the traditional, hand worked style, a cooper is typically able to construct one barrel in a day's time. Winemakers can order barrels with the wood on the inside of the barrel having been lightly charred or toasted with fire, medium toasted, or heavily toasted. Typically the \"lighter\" the toasting the more oak flavor and tannins that are imparted. Heavy toast or \"charred\" which is typical treatment of barrels in Burgundy wine have an added dimension from the char that medium or light toasted barrels do not impart. Heavy toasting dramatically reduces the coconut note lactones, even in American oak, but create a high carbon content that may reduce the coloring of some wines. During the process of toasting, the furanic aldehydes in the wood reach a higher level of concentration. This produces the \"roasted\" aroma in the wine. The toasting also enhances the presences of vanillin and the phenol eugenol which creates smokey and spicy notes that in some wines are similar to the aromatics of oil of cloves.\n\nAlthough oak barrels have long been used by winemakers, many wineries now use oak wood chips for aging wine more quickly and also adding desired woody aromas along with vanilla flavors. It is a common misconception that oak imparts butter flavors to wine. This is not so. The butter flavors come from lactic acid, naturally present in the wine, converted during malolactic fermentation to diacetyl. This process reverses itself, although the addition of sulfur dioxide prevents this, and the diacetyl remains. Oak chips can be added during fermentation or during aging. In the latter case, they are generally placed into fabric sacks and placed into the aging wine. The diversity of chips available gives winemakers numerous options. Oak chips have the benefit of imparting intense oak flavoring in a matter of weeks while traditional oak barrels would need a year or more to convey similar intensity. Critics claim that the oak flavoring from chips tend to be one-dimensional and skewed towards the vanilla extract with the wines still lacking some of the physical benefits that barrel oak imparts. The use of oak powder is also less common than chips, although they are a very practical alternative if oak character is to be introduced during fermentation. Oak planks or staves are sometimes used, either during fermentation or aging. Wines made from these barrel alternatives typically do not age as well as wines that are matured in barrels. Improvements in micro-oxygenation have allowed winemakers to better mimic the gentle aeration of oak barrels in stainless steel tanks with oak chips.\n\nPrior to 2006, the practice of using oak chips was outlawed in the European Union. In 1999, the Bordeaux court of appeals fined four wineries, including third growth Chateau Giscours, over $13,000 USD for the use of oak chips in their wine.\n\nThroughout history other wood types, including chestnut, pine, redwood, and black locust, have been used in crafting winemaking vessels, particularly large fermentation vats. However none of these wood types possess the compatibility with wine that oak has demonstrated in combining its water tight, yet slightly porous, storage capabilities with the unique flavor and texture characteristic that it can impart to the wine that it is in contact with. Chestnut is very high in tannins and is too porous as a storage barrel and must be coated with paraffin to prevent excessive wine loss through evaporation. Redwood is too rigid to bend into the smaller barrel shapes and imparts an unpleasant flavor. Black Locust imparts a yellow tint to the wine. Other hardwoods like apple and cherry wood have an off putting smell. Austrian winemakers have a history of using black locust barrels. Historically, chestnut was used by Beaujolais, Italian and Portuguese wine makers. Some Rhône winemakers still use paraffin coated chestnut barrels but the coating minimizes any effect from the wood making its function similar to a neutral concrete vessel. In Chile there are traditions for using barrels made of rauli wood but it is beginning to fall out of favor due to the musky scent it imparts on wine.\n\n\n"}
{"id": "46743384", "url": "https://en.wikipedia.org/wiki?curid=46743384", "title": "Peerio", "text": "Peerio\n\nPeerio is a cross-platform end-to-end encrypted application that provides secure messaging, file sharing, and cloud file storage. Peerio is available as an application for iOS, Android, macOS, Windows, and Linux. Peerio (Legacy) was originally released on 14 January 2015, and was replaced by Peerio 2 on 15 June 2017.\n\nMessages and user files stored on the Peerio cloud are protected by end-to-end encryption, meaning the data is encrypted in a way that cannot be read by third parties, such as Peerio itself or its service providers. Security is provided by a single permanent key-password, which in Peerio is called an \"Account Key\".\n\nThe company, Peerio Technologies Inc., was founded in 2014 by Vincent Drouin. The intent behind Peerio is to provide a security program that is easier to use than the current PGP standard.\n\nPeerio allows users to share encrypted messages and files in direct messages or groups that Peerio calls \"rooms\".\n\nPeerio \"rooms\" are offered as a team-oriented group chat, allowing administrative functionality to add and remove other users from the group chat.\n\nPeerio allows users to store encrypted files online, offering limited cloud storage for free with optional paid upgrades.\n\nPeerio messages and files persist between logins and hardware, differing from ephemeral encrypted messaging apps which do not retain message or file history between logins or different devices.\n\nPeerio supports application based multi-factor authentication.\n\nPeerio allows users to share animated GIFs.\n\nPeerio utilizes end-to-end encryption and is applied by default to all message and file data. End-to-end encryption is intended to encrypt data in a way that only the sender and intended recipients are able to decrypt, and thus read, the data.\n\nTaken from Peerio's privacy policy:\n\n\"Peerio utilizes the NaCl (pronounced \"salt\") cryptographic framework, which itself uses the following cryptographic primitives:\nAdditionally, Peerio uses \"scrypt\" for memory-hard key derivation and \"BLAKE2s\" is used for various hashing operations.\n\nFor in-transit encryption, Peerio Services use Transport Layer Security (TLS) with best-practice cipher suite configuration, including support for perfect forward secrecy (PFS). You can view a detailed and up-to-date independent review of Peerio’s TLS configuration on SSL Labs.\"\n\nPrior to Peerio's initial release, the software was audited by the German security firm Cure53, which found only non-security related bugs, all of which were fixed prior to the applications release. \n\nAccording to Peerio's website, the application was also audited in March 2017 by Cure53.\n\nPeerio is partly open source and publishes code publicly on GitHub\n\nPeerio offers a bug bounty, offering cash rewards for anyone who reports security vulnerabilities.\n\nThe first iteration of Peerio, Peerio (Legacy), was developed by Nadim Kobeissi and Florencia Herra-Vega and was released on 14 January 2015 and was closed on 8 January 2018.\n\nPeerio (Legacy) was a free application, available for Android, iOS, Windows, macOS, Linux, and as a Google Chrome extension. It offered end-to-end encryption, which is enabled by default. The encryption used the miniLock open-source security standard, which was also developed by Kobeissi. \n\nOn 15 June 2017, Peerio 2 was launched as the successor to Peerio (Legacy). According to the company's blog, Peerio 2 is purported to be a \"radical overhaul\" of the original application's core technology. Claimed benefits in comparison to Peerio (Legacy) include increased speed, support for larger file transfers (up to 7000GB), and a re-designed user interface. Peerio also stated a added focus towards businesses looking for encrypted team collaboration software. \n"}
{"id": "39396914", "url": "https://en.wikipedia.org/wiki?curid=39396914", "title": "PhET Interactive Simulations", "text": "PhET Interactive Simulations\n\nPhET Interactive Simulations, a project at the University of Colorado Boulder, is a non-profit open educational resource (OER) project founded in 2002 by Nobel Laureate Carl Wieman. PhET began with Wieman's vision to improve the way science is taught and learned. Their stated mission is \"To advance science and math literacy and education worldwide through free interactive simulations.\"\n\nThe project acronym \"PhET\" originally stood for \"Physics Education Technology,\" but PhET soon expanded to other disciplines. The project now designs, develops, and releases over 125 free interactive simulations for educational use in the fields of physics, chemistry, biology, earth science, and mathematics. The simulations have been translated into over 65 different languages, including Spanish, Chinese, German, and Arabic; and in 2011, the PhET website received over 25 million visitors.\n\nIn October 2011, PhET Interactive Simulations was chosen as the 2011 Microsoft Education Tech Award laureate. The Tech Awards, presented by The Tech Museum of Innovation, honor innovators from around the world for technology benefitting humanity.\n\nAfter winning the Nobel prize in 2001, Wieman became particularly involved with efforts at improving science education and has conducted educational research on science instruction. He helped write Physics 2000 to provide simulations to explain his work in creating the Bose-Einstein Condensate. As he gave public lectures, some incorporating simulations, he noticed that \"often the simulations would be the primary thing people would remember from my talk. Based on their questions and comments, it appeared that they consistently learned the physics represented in the simulations.\" He then used money from a grant from the National Science Foundation Distinguished Teaching Scholars program, the Kavli Foundation, and a portion of his Nobel Prize money to found PhET to improve the way that physics is taught and learned. The PhET simulations differ from the Physics 2000 ones because users can interact with the simulation to change conditions whereas the Physics 2000 simulations are just videos.\n\nIn 2007, Wieman moved to Vancouver, British Columbia while retaining 20% faculty position at the University of Colorado Boulder. The current director of PhET is Dr. Katherine Perkins, who has been with PhET since January 2003. Perkins hopes that the simulations’ accessibility and interactive nature will increase scientific literacy and promote student engagement in the classroom.\n\nPhET Interactive Simulations is part of the University of Colorado Boulder which is a member of the Association of American Universities. The team changes over time and has about 16 members consisting of professors, post-doctoral students, researchers, education specialists, software engineers (sometimes contractors), educators, and administrative assistants. The current director of PhET is Dr. Katherine Perkins.\n\nPhET Interactive Simulations incorporates research-based practices on effective teaching to enhance the learning of science and mathematics concepts. The simulations are designed to be flexible so that they can be used as lecture demonstrations, labs, or homework activities. They use an intuitive, game-like environment where students can learn through scientist-like exploration within a simplified environment, where dynamic visual representations make the invisible visible, and where science ideas are connected to real-world phenomena.\n\nA PhET simulation starts with three to five people including a content expert (scientist), a teacher, an educational researcher, and a professional software developer. The design begins with identifying specific learning goals that have proven to be conceptually difficult based on teachers' experiences in the classroom. The simulation design, look and feel is storyboarded, discussed, and then finally \"coded.\" Each simulation is user tested through interviews with students and in classrooms, re-worked as needed and re-tested, before released on the PhET website.\n\nAlong with testing every simulation, the PhET team performs education research on their simulations. They have shown in their research that when students explore simulations in addition to traditional labs, student concept understanding improves.\n\nWhile PhET Interactive Simulations develops the simulations, it is primarily teachers and publishers who develop the educational activities which use the simulations, sharing these with the community. Contributors on the PhET site follow Open Education Practices (OEP), enabling teachers to use or adapt the activities freely. \nOther Open Education Resource organizations that provide ideas and reviews include: \n\nProfessional organizations also provide ideas for using PhET simulations. In the JCE Chemical Education Xchange (ChemEd X), members have blogged about how using PhET can help with specific topics like Stoichiometry Resources, First Week Excitement, PHYSICS 2000, and Adding Inquiry to Atomic Theory.\n\nThe National Science Foundation has provided grants for several organizations to study PhET use:\n\nOther research grants:\n"}
{"id": "14146415", "url": "https://en.wikipedia.org/wiki?curid=14146415", "title": "Pirelli Internetional Award", "text": "Pirelli Internetional Award\n\nThe Pirelli Internetional Award was first offered in 1996, as the first international multimedia competition for the communication of science & technology conducted entirely on the internet. Since then, annual awards have been granted to the best multimedia presentations focussing on themes involving the diffusion of science and technology. The multimedia presentations must deal with either physics, chemistry, mathematics, life sciences, or the enabling information and communication technologies that empower multimedia itself. According to Marco Tronchetti Provera, President of the Pirelli Group, the award was established in the belief that the diffusion of social, economic and technological advances are as important as their discovery. As the name indicates, the award is sponsored by the Pirelli Corporation.\n\nAn international jury of notable people including Nobel Prize laureates reviews the top entries.\n\nWith an overall budget prize of 105,000 euro (about US$ 130,000), awards are granted in the following major categories: physics, chemistry, mathematics, life sciences, and information and communications technology.\n\nThis category rewards the best multimedia works coming from the field of physics and amounts to an award of 15,000 euro.\n\nThis category rewards the best multimedia works coming from the field of chemistry and amounts to an award of 15,000 euro.\n\nThis category rewards the best multimedia work coming from the field of mathematics and amounts to an award of 15,000 euro.\n\nThis category rewards the best multimedia work coming from the field of life sciences and amounts to an award of 15,000 euro.\n\nThis special category prize rewards those multimedia works which represent a relevant contribution to Information and Communications Technology by means of a product, process, or service, and is deemed to be of particular significance to the jury in that the Pirelli Internetional Award is predicated in large degree to contributions such as these. The Information and Communications Technology prize amounts to 15,000 euro.\n\nThe Top Pirelli Prize is the supreme recognition that the international jury grants a multimedia work which best embodies the philosophy of the Pirelli Internetional Award. In fact, the Top Pirelli Prize is considered the overall winner. It amounts to an additional 10,000 euro on top of the monetary award granted in any of the five regular categories. The Top Pirelli Prize was first awarded in 2001 (five years after the inception of the Pirelli Award) to Prof. Robert C. Michelson for his work on the Entomopter, a biologically inspired insect-like aerial robot.\n\nWinners of the Pirelli Award (First Prize for best from any institution, and Top Pirelli Prize):\n\n1996 First Prize: Elisa Manacorda (Italy) for GalileoNet.it\n\n1997 First Prize: Marco Ziegler (Switzerland) for sgich1.unifr.ch/visu.html\n\n1998 First Prize: Jurgen Renn (Germany) for the Institute and Museum of the History of Science\n\n1999 First Prize: Ericsson Medialab (Sweden) for Warriors of the Net\n\n2000 First Prize: Roberto Fieschi (INFM, Italy) for maccw.sns.it/technet\n\n2001 First Prize and Top Pirelli Prize: Robert C. Michelson (Georgia Institute of Technology, USA) for the Entomopter Project\n\n2002 First Prize and Top Pirelli Prize: Howard Hughes Medical Institute (USA) for GVirtual Labs\n\n2003 First Prize and Top Pirelli Prize: David Grubin Productions, Thirteen/WNET and Docstar (USA) for Secret Life of the Brain\n\n2004 First Prize and Top Pirelli Prize: Patrick J. Lynch (Yale University, USA) for the Yale-New Haven Medical Center\n\n2005 First Prize and Top Pirelli Prize: Roeland van der Marel and Gijs Verdoes Kleijn (Space Telescope Science Institute, Netherlands/USA) and Educational Web Adventures (Netherlands/USA) for Black Holes: Gravity's Relentless Pull\n\n2006 First Prize and Top Pirelli Prize: New Index (Norway) for the Interactive Mobile Whiteboard\n\n2007 First Prize and Top Pirelli Prize: Space Telescope Institute/Google (USA) for Google Sky\n\nIn a July 2, 2008 communique, author and manager of the Pirelli Internetional Award, Massimo Armeni, announced that Pirelli would no longer be conducting the award on an annual basis, however no indication was given as to when the next competition for the award would be announced.\n\n"}
{"id": "22534140", "url": "https://en.wikipedia.org/wiki?curid=22534140", "title": "Powder deaerator", "text": "Powder deaerator\n\nA Powder Deaerator (also powder compactor or powder densifier) is a working apparatus for deaerating and compacting / densifying of dry, fine-grained powders. The machine removes excess air and open spaces in the powder, leaving it a more solid, compact, material.\n\nPowder Deaerators consist of two parallel drums – a filter drum and a pressure drum – which rotate in opposite directions. The drums are driven via drive motor and spur gear. The filter drum is coated with a porous sinter metal layer. It is connected to a vacuum line via a hollow shaft creating a negative pressure within the filter drum. An adjusting device serves for the setting of the gap width between the rollers. On the adjusting device there are spring assemblies to generate the necessary pressure.\n\nThe material is aspirated (sucked in) and held on the filter drum by a vacuum, where it is drawn in the space between the filter and pressure drums. The combination of vacuum and pressure causes a deaerating and densification of the product. At the end of the densification process, the densified product is stripped off the filter drum by knives so it can leave the machine.\n\nDeaerators can be used for the deaerating and densification of all powders and other fine-piece bulk materials. The aim of the application is to raise the bulk density and/or improve the handling properties of a product. Deaerators are also used for the dosing and precompaction in granulation processes. Typical products which can be handled are silicic acid, carbon black, pigments, aluminium oxide, magnesium oxide, etc. The product temperatures can reach up to 100 °C.\n\nDeaerators achieve savings in package, transport and storage capacity by significantly reducing the powder volume. Explosive products can also be compacted as well.\n\n"}
{"id": "1182245", "url": "https://en.wikipedia.org/wiki?curid=1182245", "title": "Project delivery method", "text": "Project delivery method\n\nA project delivery method is a system used by an agency or owner for organizing and financing design, construction, operations, and maintenance services for a structure or facility by entering into legal agreements with one or more entities or parties.\n\nCommon project delivery methods include:\n\n\nThough DBB is now used for most private projects and the majority of public projects, it has not historically been the predominant delivery method of choice. The master builders of centuries past acted both as designers and constructors for both public and private clients. In the United States, Zane's Post Road in Ohio and the IRT in New York City were both originally developed under more integrated delivery methods, as were most infrastructure projects until 1933. Integrated Project Delivery offers a new delivery method to remove considerable waste from the construction process while improving quality and a return to more collaborative methods from the past. \nIn an effort to assist industry professionals with the selection of appropriate project delivery systems, construction management researchers have prepared a Procurement Method and Contract Selection Model, which can be used for high level decision making for construction projects on a case by case basis. .\n\nThere are two key variables which account for the bulk of the variation between delivery methods:\n\nWhen the various service providers are segmented, the owner has the most amount of control, but this control is costly and doesn't give each provider an incentive to optimize its contribution for the next service. When there is tight integration amongst providers, each step of the delivery is undertaken with future activities in mind, resulting in cost savings, but limiting the owner's influence throughout the project.\n\nThe owner's direct financing of a project simply means that the owner directly pays the providers for their services. In the case of a facility with a consistent revenue stream, indirect financing becomes possible: rather than be paid by the owner, the providers are paid with the revenue collected from the facility's operation.\n\nIndirect financing risks being mistaken for privatization. Though the providers do have a concession to operate and collect revenue from a facility that they built and financed, the structure itself remains the property of the owner (usually a government agency in the case of public infrastructure).\n"}
{"id": "1564205", "url": "https://en.wikipedia.org/wiki?curid=1564205", "title": "Real-time computer graphics", "text": "Real-time computer graphics\n\nReal-time computer graphics or real-time rendering is the sub-field of computer graphics focused on producing and analyzing images in real time. The term can refer to anything from rendering an application's graphical user interface (GUI) to real-time image analysis, but is most often used in reference to interactive 3D computer graphics, typically using a graphics processing unit (GPU). One example of this concept is a video game that rapidly renders changing 3D environments to produce an illusion of motion.\n\nComputers have been capable of generating 2D images such as simple lines, images and polygons in real time since their invention. However, quickly rendering detailed 3D objects is a daunting task for traditional Von Neumann architecture-based systems. An early workaround to this problem was the use of sprites, 2D images that could imitate 3D graphics.\n\nDifferent techniques for rendering now exist, such as ray-tracing and rasterization. Using these techniques and advanced hardware, computers can now render images quickly enough to create the illusion of motion while simultaneously accepting user input. This means that the user can respond to rendered images in real time, producing an interactive experience.\n\nThe goal of computer graphics is to generate computer-generated images, or frames, using certain desired metrics. One such metric is the number of frames generated in a given second. Real-time computer graphics systems differ from traditional (i.e., non-real-time) rendering systems in that non-real-time graphics typically rely on ray tracing. In this process, millions or billions of rays are traced from the camera to the world for detailed rendering—this expensive operation can take hours or days to render a single frame.\n\nReal-time graphics systems must render each image in less than 1/30th of a second. Ray tracing is far too slow for these systems; instead, they employ the technique of z-buffer triangle rasterization. In this technique, every object is decomposed into individual primitives, usually triangles. Each triangle gets positioned, rotated and scaled on the screen, and rasterizer hardware (or a software emulator) generates pixels inside each triangle. These triangles are then decomposed into atomic units called fragments that are suitable for displaying on a display screen. The fragments are drawn on the screen using a color that is computed in several steps. For example, a texture can be used to \"paint\" a triangle based on a stored image, and then shadow mapping can alter that triangle's colors based on line-of-sight to light sources.\nReal-time graphics optimizes image quality subject to time and hardware constraints. GPUs and other advances increased the image quality that real-time graphics can produce. GPUs are capable of handling millions of triangles per frame, and current DirectX 11/OpenGL 4.x class hardware is capable of generating complex effects, such as shadow volumes, motion blurring, and triangle generation, in real-time. The advancement of real-time graphics is evidenced in the progressive improvements between actual gameplay graphics and the pre-rendered cutscenes traditionally found in video games. Cutscenes are typically rendered in real-time—and may be interactive. Although the gap in quality between real-time graphics and traditional off-line graphics is narrowing, offline rendering remains much more accurate.\n\nReal-time graphics are typically employed when interactivity (e.g., player feedback) is crucial. When real-time graphics are used in films, the director has complete control of what has to be drawn on each frame, which can sometimes involve lengthy decision-making. Teams of people are typically involved in the making of these decisions.\n\nIn real-time computer graphics, the user typically operates an input device to influence what is about to be drawn on the display. For example, when the user wants to move a character on the screen, the system updates the character's position before drawing the next frame. Usually, the display's response-time is far slower than the input device—this is justified by the immense difference between the (fast) response time of a human being's motion and the (slow) perspective speed of the human visual system. This difference has other effects too: because input devices must be very fast to keep up with human motion response, advancements in input devices (e.g., the current Wii remote) typically take much longer to achieve than comparable advancements in display devices.\n\nAnother important factor controlling real-time computer graphics is the combination of physics and animation. These techniques largely dictate what is to be drawn on the screen—especially \"where\" to draw objects in the scene. These techniques help realistically imitate real world behavior (the temporal dimension, not the spatial dimensions), adding to the computer graphics' degree of realism.\n\nReal-time previewing with graphics software, especially when adjusting lighting effects, can increase work speed. Some parameter adjustments in fractal generating software may be made while viewing changes to the image in real time.\n\nThe graphics rendering pipeline (\"rendering pipeline\" or simply \"pipeline\") is the foundation of real-time graphics. Its main function is to render a two-dimensional image in relation to a virtual camera, three-dimensional objects (an object that has width, length, and depth), light sources, lighting models, textures and more.\n\nThe architecture of the real-time rendering pipeline can be divided into conceptual stages: application, geometry and rasterization. \n\nThe application stage is responsible for generating \"scenes\", or 3D settings that are drawn to a 2D display. This stage is implemented in software that developers optimize for performance. This stage may perform processing such as collision detection, speed-up techniques, animation and force feedback, in addition to handling user input.\n\nCollision detection is an example of an operation that would be performed in the application stage. Collision detection uses algorithms to detect and respond to collisions between (virtual) objects. For example, the application may calculate new positions for the colliding objects and provide feedback via a force feedback device such as a vibrating game controller.\n\nThe application stage also prepares graphics data for the next stage. This includes texture animation, animation of 3D models, animation via transforms, and geometry morphing. Finally, it produces primitives (points, lines, and triangles) based on scene information and feeds those primitives into the geometry stage of the pipeline.\n\nThe geometry stage manipulates polygons and vertices to compute what to draw, how to draw it and where to draw it. Usually, these operations are performed by specialized hardware or GPUs. Variations across graphics hardware mean that the \"geometry stage\" may actually be implemented as several consecutive stages.\n\nBefore the final model is shown on the output device, the model is transformed onto multiple spaces or coordinate systems. Transformations move and manipulate objects by altering their vertices. \"Transformation\" is the general term for the four specific ways that manipulate the shape or position of a point, line or shape.\n\nIn order to give the model a more realistic appearance, one or more light sources are usually established during transformation. However, this stage cannot be reached without first transforming the 3D scene into view space. In view space, the observer (camera) is typically placed at the origin. If using a right-handed coordinate system (which is considered standard), the observer looks in the direction of the negative z-axis with the y-axis pointing upwards and the x-axis pointing to the right.\n\nProjection is a transformation used to represent a 3D model in a 2D space. The two main types of projection are orthographic projection (also called parallel) and perspective projection. The main characteristic of orthographic projection is that parallel lines remain parallel after the transformation. Perspective projection utilizes the concept that if the distance between the observer and model increases, the model appears smaller than before. Essentially, perspective projection mimics human sight.\n\nClipping is the process of removing primitives that are outside of the view box in order to facilitate the rasterizer stage. Once those primitives are removed, the primitives that remain will be drawn into new triangles that reach the next stage.\n\nThe purpose of screen mapping is to find out the coordinates of the primitives during the clipping stage.\n\nThe rasterizer stage applies color and turns the graphic elements into pixels or picture elements.\n\nComputer animation has been around since the 1940s and 1950s, but it was not until the 1970s and 1980s that 3D techniques were implemented. \n\nThe first step towards 3D graphics was taken in 1972 by Edwin Catmull and Fred Parke. Their implementation featured a computer-generated hand and face that was created using wire-frame imagery. Up until 1975, wire-frame imagery was the only digital technology used to create 3D images. \n\n3D graphics have reached the point where animated humans look almost entirely realistic. Eventually, humans may not be able to tell the difference between filmed humans and animated humans.\n\nThe film, Beowulf (2007 film), showcases 3D graphics that get close to fooling the human eye. The film was created using 3D motion capture technology.\n\n\n"}
{"id": "867009", "url": "https://en.wikipedia.org/wiki?curid=867009", "title": "Rockwell Collins", "text": "Rockwell Collins\n\nRockwell Collins is a multinational corporation company headquartered in Cedar Rapids, Iowa providing avionics and information technology systems and services to government agencies and aircraft manufacturers. The company was merged with UTC Aerospace Systems on November 27, 2018, and now operates as Collins Aerospace. \n\nArthur Collins founded Collins Radio Company in 1933 in Cedar Rapids, Iowa. It designed and produced both shortwave radio equipment and equipment for the burgeoning AM Broadcast industry. Collins was solicited by the military, the scientific community and by the larger AM radio stations for special equipment. Collins supplied the equipment to establish a communications link with the South Pole expedition of Rear Admiral Richard Byrd in 1933.\n\nIn 1936, Collins had begun production of the 12H audio console, 12X portable field announcers box, the 300E and 300F broadcast transmitters. Throughout World War II, the 212A1 and 212B1 replaced the 12H design. Collins became the principal supplier of radio and navigation equipment used in the military, where uncompromising performance was required.\n\nIn the post war years, the Collins Radio Company expanded its work in all phases of the communications field while broadening its technology. This moved Arthur Collins into a more active role as CEO guiding department leaders holding significant responsibilities. New developments such as flight control instruments, radio communication devices and satellite voice transmissions created great opportunities in the marketplace. Collins Radio Company provided communications for the United States' role in the Space Race, including equipment for astronauts to communicate with earth stations and equipment to track and communicate with spacecraft. Collins communications equipment was used for Projects Mercury, Gemini and Apollo, providing voice communication for every American astronaut traveling through space. In 1973, the U.S. Skylab Program used Collins equipment to provide communication from the astronauts to earth.\n\nAfter facing financial difficulties, the Collins Radio Company was purchased by Rockwell International in 1973. In 2001 the avionics division of Rockwell International was spun off to form the current Rockwell Collins, Inc., retaining its name. Rockwell Collins is highly concentrated in the defense and commercial avionics markets and no longer markets receivers to the public. The Collins mechanical filter is still in production and does, however, find consumer and commercial use.\n\nOn April 28, 2000, Rockwell International Corp and its Rockwell Collins unit agreed to acquire Sony Corp's Sony Trans Com (Irvine, Calif) for undisclosed terms. Sony had purchased the business from Sundstrand Corp. in 1989. On December 20, 2000, Rockwell Collins expanded its services to commercial and executive aviation in Mercosur countries.\n\nThe company has acquired several companies, including Hughes-Avicom's in-flight entertainment business, Intertrade Ltd., Flight Dynamics, K Systems, Inc. (Kaiser companies), Communication Solutions, Inc., Airshow, Inc., NLX (Simulation Business) in 2003, portions of Evans & Sutherland, TELDIX GmbH, IP Unwired, Anzus Inc. in 2006, Information Technology & Applications Corporation in 2007, Athena Technologies, Datapath Inc. (divested in 2014), SEOS Displays Ltd., Air Routing International in 2010, Computing Technologies for Aviation (CTA) in 2011, ARINC in 2014, and BE Aerospace in 2017.\n\nThe company is among the major suppliers of in-flight entertainment on board aircraft. Rockwell Collins' key competitors in this industry include Panasonic Avionics Corporation, Thales Group, and JetBlue's IFE subsidiary LiveTV.\n\nAs of 2010, the company employs over 20,000 people and has an annual turnover of 4.665 billion US dollars. Its non-executive chairman is Anthony Carbone following the retirement of Clayton M. Jones.\nIn September 2012, Kelly Ortberg was appointed as president of the company. In August 2013, Kelly Ortberg was appointed CEO of Rockwell Collins.\n\nOn September 4, 2017, United Technologies of Farmington, Connecticut agreed to acquire the company for $30 billion. The transaction closed on November 26, 2018.\n\nIn the mid-1930s, the Collins Radio Company constructed and sold transmitters and audio mixing consoles to the broadcast industry.\n\nIn 1939, the model 12 Speech Input Console, in addition to the 26C limiter amplifier, was licensed to Canadian Marconi Co. for both sales in Canada and His Majesties Service for the war effort. Collins success in constructing broadcast transmitters continued to grow, selling well over a thousand up to the start of World War II. During World War II, Collins expertise grew in higher power transmitters producing designs which ran well over 15 kilowatts of RF power on a continuous basis. After the war a limited number of AM transmitters were produced called the 300G and remain the finest in low power AM transmitters (300W) ever produced.\n\nCollins remained an important manufacturer of AM and FM broadcast radio transmitters for the commercial market surviving the drastic cost cutting market of the 1960s and 1970s. The transmitter line was later sold to Continental Electronics, which continued to produce a number of Collins designs under its own nameplate before phasing them out in the 1980s.\n\nCollins produced several shortwave transmitters to the commercial market. A \"30\" Series production catered to the growing need of state highway patrol agencies and Department of Commerce aviation needs. During World War II, Collins produced high power transmitters for aircraft, notably the ART-13 equipped with automatic tuning circuits, which represented an important enhancement for airborne radio communications.\n\nAfter World War II, Collins supported both broadcast and the growing post-war amateur radio market. The United States Coast Guard Cutter USCGC Courier was employed as seagoing relay station for Voice of America programming using two Collins 207B-1 transmitters.\nAmateur radio transmitters included the 32V-1, -2, and -3, KWS-1 and the rack mounted KW-1.\n\nAround 1947, the company introduced their first amateur radio receiver, the 75A-1 (called the 75A). This set achieved excellent stability for the time due to high build quality and the use of a permeability tuned oscillator (PTO) in its second conversion stage. It was one of the few double conversion superheterodynes on the market and covered only the amateur bands.\n\nWith the experience gained in the design of the 75A-1, Collins released the 51J-1 receiver, a general coverage HF set covering .5 to 30 MHz. It would be produced in somewhat updated versions (51J-2, 51J-3, 51J-4) for about a decade. It was known as the R-388 and was used in multiple receiver diversity RTTY installations.\n\nThe 75A amateur line was updated throughout the early 50s, finishing with the 75A-4, which was released in 1955. The Collins mechanical filter was introduced to consumers in the 75A-3, and the 75A-4 was one of the first receivers marketed specifically as a single sideband receiver. \n\nAround 1950, Collins began designing the R-390 (.5–30 MHz) for the US military. This was intended to be a receiver of the highest performance available, with the ruggedness and serviceability required for military duty. It featured direct mechanical digital frequency readout. The set is composed of several modules for easy field repair—a bad module could simply be swapped out and repaired later, or junked. Sets built during the original 1951 contract cost the government about $2500 each and around 16,000 were produced.\n\nConcurrently, Collins developed the R-389, a long-wave version with fewer than 1000 made. The R-391, another variant of the R-390, allowed choice of 8 different auto-tuned channels.\n\nThree years later, Collins delivered the R-390A to the military. About 54,000 were produced and the set was a military workhorse until the 1970s. Like the R-390, it can outperform many modern radios, to the point that it was designated top secret until the late 1960s.\n\nIn 1958 Collins replaced the 75A series with the much smaller 75S series, part of the S/Line, discussed in the next section. These featured mechanical filters, very accurate frequency readout, and excellent stability. At the request of the US government, Collins designed the 51S-1 general coverage set, which was essentially (in intended use) a physically smaller replacement for the 51J series. It was not intended as a replacement for the higher performance R-390A, and unlike the R-390A, it was extensively marketed for commercial use.\n\nCollins produced a few high performance solid state receivers in the 1970s, such as the 651S-1. Like their tube predecessors, these are coveted by collectors today.\n\nWith the introduction of the S/Line in 1958, Collins moved from designing individual products that could be \"used\" together, to ones that were designed to \"integrate and operate\" together, in various combinations, as a system. They were the first equipment maker to take this approach. Collins was also the first to introduce a compact HF transceiver, the KWM-1, the year before. Together, these two innovations put Collins temporarily ahead of its competition and set the stage for other manufacturers and the next generation of amateur (and military) HF radio equipment.\n\nThe 75S-1 receiver and 32S-1 transmitter, comprising the heart of the S/Line, operated separately or together to transceive. The units included crystal bandpass filters and a new compact PTO design that provided stable, highly linear tuning across 200 kHz band segments. The S/Line tuning dial mechanism was unique when introduced. It used concentric dials and a gear mechanism that provided precise dial resolution, better than 1 kHz.\nWithin a few years Collins had introduced additional S/Line components, including the 30S-1 kilowatt power amplifier, the 30L-1 desktop power amplifier, and the 62S-1 transverter, which provided coverage of the 6 meter (50 MHz) and 2 meter (144 MHz) amateur bands. The KWM-2 transceiver replaced the KWM-1 using many of the S/Line's design features and matching its styling. Other accessories included speakers, microphones and control consoles.\n\nIllustrating the uniqueness of their new, smaller units in the market, Collins advertisements in the 1950s and early 1960s emphasized the S/Line's physical styling and size as often as they did its performance.\n\nCollins continued to improve the S/Line, first introducing the S-2, then the S-3 units, the 75S-3 (and -3A, -3B and -3C) receiver, and the 32S-3 and -3A transmitters. The -3A and -3C units were identical to the -3 and -3B units, respectively, except they provided an extra set of heterodyne oscillator crystals enabling them to cover extra bands – useful for military, amateur and MARS operation, where operation just outside the regular amateur bands was necessary.\nAmong amateur radio operators, the S/Line established its reputation as perhaps the most solidly engineered equipment available – and the most costly. As a result, S/Line equipment, and the A-Line and other predecessors, are restored, prized, and operated on the air by collectors today.\n\nCollins continued to produce the S/Line well into the late 1970s and after its acquisition by Rockwell.\n\nIn 1978, with the move to solid state design, the S/Line came to an end after a two decade production run. The KWM-380 transceiver was introduced the next year – a break with the past both in its use of transistors and digital technology, and its styling. It would be Collins’ final entry in the amateur radio market until it was discontinued in the mid-1980s.\n\nIn the 1960s, the company designed and sold C-System computerized message switching equipment, built an intranet, and began implementing computer storage of design data for circuit boards and assemblies. They had a goal of automating all functions from parts ordering and inventory to factory scheduling to generation of maintenance provisioning.\n\nWith products technically successful and far ahead of their time in many respects, Mr. Collins continued to invest in development at a rate that could not be supported by sales when a downturn occurred, and began to have financial problems.\n\nIn 1991, Rockwell sold its Richardson, TX based Network Transmission Systems division to Alcatel.\n\nIn 2008, Rockwell Collins acquired Athena Technologies for $107 million.\n\nIn April 2017, Rockwell Collins entered the aircraft cabin interiors market through the acquisition of B/E Aerospace for $8.3 billion. Based in Wellington, Florida, B/E products included seating, food and beverage preparation and storage equipment, lighting and oxygen systems, and modular galley and lavatory systems for commercial airliners and business jets. B/E benefits of rival Zodiac Aerospace delivery troubles. Retrofit opportunities are provided by its $12 billion installed base. B/E shareholders received 20% of the new Rockwell which then had $8.1 billion in revenues and $1.9 billion in pre-tax earnings with nearly 30,000 employees.\n\nRockwell Collins filed for regulatory approval for its intended acquisition of B/E Aerospace, a manufacturer and supplier of aircraft cabin interior products, including cabin seating, lighting, oxygen systems, food and beverage preparation and storage equipment, galleys and lavatories, before the Philippine Competition Commission since the latter has a branch in the Philippines operating a manufacturing plant in Tanauan, Batangas.\n\nAs a result of the acquisition, a newly created direct or indirect subsidiary of Rockwell, Quarterback Merger Sub Corp., will merge with and into B/E Aerospace, with the latter surviving the merger as a direct or indirect subsidiary of Rockwell Collins.\n\nRockwell Collins has five main divisions:\n\nThe CS division services the commercial airline industry and business aircraft, providing navigation, communication, Synthetic vision, other cockpit products such as autoland autopilots, and cabin products such as In Flight Entertainment (IFE). The GS division services primarily the US government and military, but also provides some products and services to foreign governments with close ties to the United States. Notable government related projects that Rockwell Collins has involvement with are Common Avionics Architecture System (CAAS), Joint Tactical Radio System (JTRS), Tactical Targeting Network Technology (TTNT), Defense Advanced GPS Receiver (DAGR), and Future Combat Systems. The I&SS division is an amalgamation of (IB) International Business organization whose responsibility is sales, engineering and human resource of personnel outside of North America and Service Solutions who provides support services such as customer support, simulation and training and technical publications. I&SS provide a common service to both CS and GS divisions and its formation was announced on the Rockwell Collins press release web page on February 19, 2010.\n\nThe Donald R. Beall Advanced Technology Center (ATC) is a research and development center within Rockwell Collins that focuses on creating, identifying, and maturing technologies targeted at driving business growth. ATC maintains a portfolio that balances shorter term deliverables focused on core and adjacent markets with technologies for long term growth. ATC has three departments: Advanced Radio Systems, Communications and Navigation Systems, and Embedded Information Systems.\n\nAs with several other brands of vintage radio equipment, there is an active community of Collins radio enthusiasts, with clubs, web sites and on-line discussions dedicated to restoring and operating the equipment. The Collins Collectors Association (CCA) and the Collins Radio Association (CRA) are two examples of such organizations.\n\nGroups of Collins users also organize meetings, gatherings at hamfests and regularly scheduled on-air discussions called “nets”.\n\n\n\n"}
{"id": "33365207", "url": "https://en.wikipedia.org/wiki?curid=33365207", "title": "Servo bandwidth", "text": "Servo bandwidth\n\nServo bandwidth is the maximum trackable sinusoidal frequency of amplitude A, with tracking achieved at or before 10% of A amplitude is reached. The servo bandwidth indicates the capability of the servo to follow rapid changes in the commanded input. It is usually specified as a frequency in Hertz or radian/sec.\n\nBandwidth of systems is generally defined to be the frequency at which the system's amplitude is formula_1 times the signal amplitude. But if we apply same logic to servo systems it is difficult to analyze and develop a system to a sufficiently accurate specification. This is because of ambiguity with regard to frequency at which the amplitude should go to formula_1.\n\nA simple and sound definition can be sought regarding this.\nLet us say we want to design a position servo control system with following specifications:\n\nThe above definition is not enough to design a practical control system. The definitions above have inherent problems with regard to what amplitude the manufacturer should take to design the servo with 10 Hz bandwidth.\nIf the manufacturer takes the amplitude to be ±20° and rise time for this amplitude to be 0.025 sec (10 Hz sinusoid)\nand some other manufacturer takes amplitude to be ±50°, the acceleration requirements calculated by two will be very different.\n\nThis leads us to understand that giving servo bandwidth alone with no amplitude specification is almost useless. Also defining the bandwidth as per normal bandwidth definition does not help (ambiguity with regard to frequency at which the amplitude should go to formula_3.\n\n"}
{"id": "95261", "url": "https://en.wikipedia.org/wiki?curid=95261", "title": "Sheet music", "text": "Sheet music\n\nSheet music is a handwritten or printed form of music notation that uses modern musical symbols to indicate the pitches (melodies), rhythms or chords of a song or instrumental musical piece. Like its analogs – printed books or pamphlets in English, Arabic or other languages – the medium of sheet music typically is paper (or, in earlier centuries, papyrus or parchment), although the access to musical notation since the 1980s has included the presentation of musical notation on computer screens and the development of scorewriter computer programs that can notate a song or piece electronically, and, in some cases, \"play back\" the notated music using a synthesizer or virtual instruments.\n\nUse of the term \"sheet\" is intended to differentiate written or printed forms of music from sound recordings (on vinyl record, cassette, CD), radio or TV broadcasts or recorded live performances, which may capture film or video footage of the performance as well as the audio component. In everyday use, \"sheet music\" (or simply \"music\") can refer to the print publication of commercial sheet music in conjunction with the release of a new film, TV show, record album, or other special or popular event which involves music. The first printed sheet music made with a printing press was made in 1473.\n\nSheet music is the basic form in which Western classical music is notated so that it can be learned and performed by solo singers or instrumentalists or musical ensembles. Many forms of traditional and popular Western music are commonly learned by singers and musicians \"by ear\", rather than by using sheet music (although in many cases, traditional and pop music may also be available in sheet music form).\n\nThe term \"score\" is a common alternative (and more generic) term for sheet music, and there are several types of scores, as discussed below. The term \"score\" can also refer to theatre music, orchestral music or songs written for a play, musical, opera or ballet, or to music or songs written for a television programme or film; for the last of these, see Film score.\n\nSheet music from the 20th and 21st century typically indicates the title of the song or composition on a title page or cover, or on the top of the first page, if there is no title page or cover. If the song or piece is from a movie, Broadway musical, or opera, the title of the main work from which the song/piece is taken may be indicated.\n\nIf the songwriter or composer is known, her or his name is typically indicated along with the title. The sheet music may also indicate the name of the lyric-writer, if the lyrics are by a person other than one of the songwriters or composers. It may also the name of the arranger, if the song or piece has been arranged for the publication. No songwriter or composer name may be indicated for old folk music, traditional songs in genres such as blues and bluegrass, and very old traditional hymns and spirituals, because for this music, the authors are often unknown; in such cases, the word \"Traditional\" is often placed where the composer's name would ordinarily go.\n\nThe type of musical notation varies a great deal by genre or style of music. In most classical music, the melody and accompaniment parts (if present) are notated on the lines of a staff using round note heads. In classical sheet music, the staff typically contains:\n\n\nMost songs and pieces from the Classical period (ca. 1750) onward indicate the piece's tempo using an expression—often in Italian—such as \"Allegro\" (fast) or \"Grave\" (slow) as well as its dynamics (loudness or softness). The lyrics, if present, are written near the melody notes. However, music from the Baroque era (ca. 1600–1750) or earlier eras may have neither a tempo marking nor a dynamic indication. The singers and musicians of that era were expected to know what tempo and loudness to play or sing a given song or piece due to their musical experience and knowledge. In the contemporary classical music era (20th and 21st century), and in some cases before (such as the Romantic period in German-speaking regions), composers often used their native language for tempo indications, rather than Italian (e.g., \"fast\" or \"schnell\") or added metronome markings (e.g., = 100 beats per minute).\n\nThese conventions of classical music notation, and in particular the use of English tempo instructions, are also used for sheet music versions of 20th and 21st century popular music songs. Popular music songs often indicate both the tempo and genre: \"slow blues\" or \"uptempo rock\". Pop songs often contain chord names above the staff using letter names (e.g., C Maj, F Maj, G7, etc.), so that an acoustic guitarist or pianist can improvise a chordal accompaniment.\n\nIn other styles of music, different musical notation methods may be used. In jazz, while most professional performers can read \"classical\"-style notation, many jazz tunes are notated using chord charts, which indicate the chord progression of a song (e.g., C, A7, d minor, G7, etc.) and its form. Members of a jazz rhythm section (a piano player, jazz guitarist and bassist) use the chord chart to guide their improvised accompaniment parts, while the \"lead instruments\" in a jazz group, such as a saxophone player or trumpeter, use the chord changes to guide their solo improvisation. Like popular music songs, jazz tunes often indicate both the tempo and genre: \"slow blues\" or \"fast bop\".\n\nProfessional country music session musicians typically use music notated in the Nashville Number System, which indicates the chord progression using numbers (this enables bandleaders to change the key at a moment's notice). Chord charts using letter names, numbers, or Roman numerals (e.g., I–IV–V) are also widely used for notating music by blues, R&B, rock music and heavy metal musicians. Some chord charts do not provide any rhythmic information, but others use slashes to indicate beats of a bar and rhythm notation to indicate syncopated \"hits\" that the songwriter wants all of the band to play together. Many guitar players and electric bass players learn songs and note tunes using tablature, which is a graphic representation of which frets and strings the performer should play. \"Tab\" is widely used by rock music and heavy metal guitarists. Singers in many popular music styles learn a song using only a lyrics sheet, learning the melody \"by ear\" from the recording.\n\nSheet music can be used as a record of, a guide to, or a means to perform, a song or piece of music. Sheet music enables instrumental performers who are able to read music notation (a pianist, orchestral instrument players, a jazz band, etc.) or singers to perform a song or piece. In classical music, authoritative musical information about a piece can be gained by studying the written sketches and early versions of compositions that the composer might have retained, as well as the final autograph score and personal markings on proofs and printed scores.\n\nComprehending sheet music requires a special form of literacy: the ability to read music notation. An ability to read or write music is not a requirement to compose music. There have been a number of composers and songwriters who have been capable of producing music without the capacity themselves to read or write in musical notation, as long as an amanuensis of some sort is available to write down the melodies they think of. Examples include the blind 18th-century composer John Stanley and the 20th-century songwriters Lionel Bart, Irving Berlin and Paul McCartney. As well, in traditional music styles such as the blues and folk music, there are many prolific songwriters who could not read music, and instead played and sang music \"by ear\".\n\nThe skill of sight reading is the ability of a musician to perform an unfamiliar work of music upon viewing the sheet music for the first time. Sight reading ability is expected of professional musicians and serious amateurs who play classical music, jazz and related forms. An even more refined skill is the ability to look at a new piece of music and hear most or all of the sounds (melodies, harmonies, timbres, etc.) in one's head without having to play the piece or hear it played or sung. Skilled composers and conductors have this ability, with Beethoven being a noted historical example.\n\nClassical musicians playing orchestral works, chamber music, sonatas and singing choral works ordinarily have the sheet music in front of them on a music stand when performing (or held in front of them in a music folder, in the case of a choir), with the exception of solo instrumental performances of solo pieces, concertos, or solo vocal pieces (art song, opera arias, etc.), where memorization is expected. In jazz, which is mostly improvised, sheet music (called a \"lead sheet\" in this context) is used to give basic indications of melodies, chord changes, and arrangements. Even when a jazz band has a lead sheet, chord chart or arranged music, many elements of a performance are improvised.\n\nHandwritten or printed music is less important in other traditions of musical practice, however, such as traditional music and folk music, in which singers and instrumentalists typically learn songs \"by ear\" or from having a song or tune taught to them by another person. Although much popular music is published in notation of some sort, it is quite common for people to learn a song by ear. This is also the case in most forms of western folk music, where songs and dances are passed down by oral – and aural – tradition. Music of other cultures, both folk and classical, is often transmitted orally, though some non-Western cultures developed their own forms of musical notation and sheet music as well.\n\nAlthough sheet music is often thought of as being a platform for new music and an aid to composition (i.e., the composer \"writes\" the music down), it can also serve as a visual record of music that already exists. Scholars and others have made transcriptions to render Western and non-Western music in readable form for study, analysis and re-creative performance. This has been done not only with folk or traditional music (e.g., Bartók's volumes of Magyar and Romanian folk music), but also with sound recordings of improvisations by musicians (e.g., jazz piano) and performances that may only partially be based on notation. An exhaustive example of the latter in recent times is the collection \"The Beatles: Complete Scores\" (London: Wise Publications, 1993), which seeks to transcribe into staves and tablature all the songs as recorded by the Beatles in instrumental and vocal detail.\n\nModern sheet music may come in different formats. If a piece is composed for just one instrument or voice (such as a piece for a solo instrument or for \"a cappella\" solo voice), the whole work may be written or printed as one piece of sheet music. If an instrumental piece is intended to be performed by more than one person, each performer will usually have a separate piece of sheet music, called a \"part\", to play from. This is especially the case in the publication of works requiring more than four or so performers, though invariably a \"full score\" is published as well. The sung parts in a vocal work are not usually issued separately today, although this was historically the case, especially before music printing made sheet music widely available.\n\nSheet music can be issued as individual pieces or works (for example, a popular song or a Beethoven sonata), in collections (for example works by one or several composers), as pieces performed by a given artist, etc.\n\nWhen the separate instrumental and vocal parts of a musical work are printed together, the resulting sheet music is called a \"score\". Conventionally, a score consists of musical notation with each instrumental or vocal part in vertical alignment (meaning that concurrent events in the notation for each part are orthographically arranged). The term \"score\" has also been used to refer to sheet music written for only one performer. The distinction between \"score\" and \"part\" applies when there is more than one part needed for performance.\n\nScores come in various formats.\n\nA full score is a large book showing the music of all instruments or voices in a composition lined up in a fixed order. It is large enough for a conductor to be able to read while directing orchestra or opera rehearsals and performances. In addition to their practical use for conductors leading ensembles, full scores are also used by musicologists, music theorists, composers and music students who are studying a given work.\n\nA miniature score is like a full score but much reduced in size. It is too small for use in a performance by a conductor, but handy for studying a piece of music, whether it be for a large ensemble or a solo performer. A miniature score may contain some introductory remarks.\n\nA study score is sometimes the same size as, and often indistinguishable from, a miniature score, except in name. Some study scores are octavo size and are thus somewhere between full and miniature score sizes. A study score, especially when part of an anthology for academic study, may include extra comments about the music and markings for learning purposes.\n\nA piano score (or piano reduction) is a more or less literal transcription for piano of a piece intended for many performing parts, especially orchestral works; this can include purely instrumental sections within large vocal works (see \"vocal score\" immediately below). Such arrangements are made for either piano solo (two hands) or piano duet (one or two pianos, four hands). Extra small staves are sometimes added at certain points in piano scores for two hands to make the presentation more complete, though it is usually impractical or impossible to include them while playing.\n\nAs with vocal score (below), it takes considerable skill to reduce an orchestral score to such smaller forms because the reduction needs to be not only playable on the keyboard but also thorough enough in its presentation of the intended harmonies, textures, figurations, etc. Sometimes markings are included to show which instruments are playing at given points.\n\nWhile piano scores are usually not meant for performance outside of study and pleasure (Franz Liszt's concert transcriptions of Beethoven's symphonies being one group of notable exceptions), ballets get the most practical benefit from piano scores because with one or two pianists they allow the ballet to do many rehearsals at a much lower cost, before an orchestra has to be hired for the final rehearsals. Piano scores can also be used to train beginning conductors, who can conduct a pianist playing a piano reduction of a symphony; this is much less costly than conducting a full orchestra. Piano scores of operas do not include separate staves for the vocal parts, but they may add the sung text and stage directions above the music.\n\nA part is an extraction from the full score of a particular instrument's part. It is used by orchestral players in performance, where the full score would be too cumbersome. However, in practice, it can be a substantial document if the work is lengthy, and a particular instrument is playing for much of its duration.\nA vocal score (or, more properly, piano-vocal score) is a reduction of the full score of a vocal work (e.g., opera, musical, oratorio, cantata, etc.) to show the vocal parts (solo and choral) on their staves and the orchestral parts in a piano reduction (usually for two hands) underneath the vocal parts; the purely orchestral sections of the score are also reduced for piano. If a portion of the work is \"a cappella\", a piano reduction of the vocal parts is often added to aid in rehearsal (this often is the case with \"a cappella\" religious sheet music).\n\nPiano-vocal scores serve as a convenient way for vocal soloists and choristers to learn the music and rehearse separately from the orchestra. The vocal score of a musical typically does not include the spoken dialogue, except for cues. Piano-vocal scores are used to provide piano accompaniment for the performance of operas, musicals and oratorios by amateur groups and some small-scale professional groups. This may be done by a single piano player or by two piano players. With some 2000s-era musicals, keyboardists may play synthesizers instead of piano.\n\nThe related but less common choral score contains the choral parts with reduced accompaniment.\n\nThe comparable organ score exists as well, usually in association with church music for voices and orchestra, such as arrangements (by later hands) of Handel's \"Messiah\". It is like the piano-vocal score in that it includes staves for the vocal parts and reduces the orchestral parts to be performed by one person. Unlike the vocal score, the organ score is sometimes intended by the arranger to substitute for the orchestra in performance if necessary.\n\nA collection of songs from a given musical is usually printed under the label vocal selections. This is different from the vocal score from the same show in that it does not present the complete music, and the piano accompaniment is usually simplified and includes the melody line.\n\nA short score is a reduction of a work for many instruments to just a few staves. Rather than composing directly in full score, many composers work out some type of short score while they are composing and later expand the complete orchestration. An opera, for instance, may be written first in a short score, then in full score, then reduced to a vocal score for rehearsal. Short scores are often not published; they may be more common for some performance venues (e.g., band) than in others. Because of their preliminary nature, short scores are the principal reference point for those composers wishing to attempt a 'completion' of another's unfinished work (e.g. Movements 2 thru 5 of Gustav Mahler's 10th Symphony or the third Act of Alban Berg's opera \"Lulu\").\n\nAn open score is a score of a polyphonic piece showing each voice on a separate staff. In Renaissance or Baroque keyboard pieces, open scores of four staves were sometimes used instead of the more modern convention of one staff per hand. It is also sometimes synonymous with full score (which may have more than one part per staff).\n\nScores from the Baroque period (1600-1750) are very often in the form of a bass line in the bass clef and the melodies played by instrument or sung on an upper stave (or staves) in the treble clef. The bass line typically had figures written above the bass notes indicating which intervals above the bass (e.g., chords) should be played, an approach called \"figured bass\". The figures indicate which intervals the harpsichordist, pipe organist or lute player should play above each bass note.\nA lead sheet specifies only the melody, lyrics and harmony, using one staff with chord symbols placed above and lyrics below. It is commonly used in popular music and in jazz to capture the essential elements of song without specifying the details of how the song should be arranged or performed.\n\nA chord chart (or simply, chart) contains little or no melodic information at all but provides fundamental harmonic information. Some chord charts also indicate the rhythm that should be played, particularly if there is a syncopated series of \"hits\" that the arranger wants all of the rhythm section to perform. Otherwise, chord charts either leave the rhythm blank or indicate slashes for each beat.\n\nThis is the most common kind of written music used by professional session musicians playing jazz or other forms of popular music and is intended for the rhythm section (usually containing piano, guitar, bass and drums) to improvise their accompaniment and for any improvising soloists (e.g., saxophone players or trumpet players) to use as a reference point for their extemporized lines.\n\nA fake book is a collection of jazz songs and tunes with just the basic elements of the music provided. There are two types of fake books: (1) collections of lead sheets, which include the melody, chords, and lyrics (if present), and (2) collections of songs and tunes with only the chords. Fake books that contain only the chords are used by rhythm section performers (notably chord-playing musicians such as electric guitarists and piano players and the bassist) to help guide their improvisation of accompaniment parts for the song. Fake books with only the chords can also be used by \"lead instruments\" (e.g., saxophone or trumpet) as a guide to their improvised solo performances. Since the melody is not included in chord-only fake books, lead instrument players are expected to know the melody.\n\nA tablature (or tab) is a special type of musical score – most typically for a solo instrument – which shows \"where\" to play the pitches on the given instrument rather than \"which\" pitches to produce, with rhythm indicated as well. Tabulature is widely used in the 2000s for guitar and electric bass songs and pieces in popular music genres such as rock music and heavy metal music. This type of notation was first used in the late Middle Ages, and it has been used for keyboard (e.g., pipe organ) and for fretted string instruments (lute, guitar).\n\nMusical notation was developed before parchment or paper were used for writing. The earliest form of musical notation can be found in a cuneiform tablet that was created at Nippur, in Sumer (today's Iraq) in about 2,000 BC. The tablet represents fragmentary instructions for performing music, that the music was composed in harmonies of thirds, and that it was written using a diatonic scale.\n\nA tablet from about 1,250 BC shows a more developed form of notation. Although the interpretation of the notation system is still controversial, it is clear that the notation indicates the names of strings on a lyre, the tuning of which is described in other tablets. Although they are fragmentary, these tablets represent the earliest notated melodies found anywhere in the world.\nAncient Greek musical notation was in use from at least the 6th century BC until approximately the 4th century AD; several complete compositions and fragments of compositions using this notation survive. The notation consists of symbols placed above text syllables. An example of a complete composition is the Seikilos epitaph, which has been variously dated between the 2nd century BC to the 1st century AD.\n\nIn Ancient Greek music, three hymns by Mesomedes of Crete exist in manuscript. One of the oldest known examples of music notation is a papyrus fragment of the Hellenic era play \"Orestes\" (408 BC) has been found, which contains musical notation for a choral ode. Ancient Greek notation appears to have fallen out of use around the time of the Decline of the Roman Empire.\n\nBefore the 15th century, Western music was written by hand and preserved in manuscripts, usually bound in large volumes. The best-known examples of Middle Ages music notation are medieval manuscripts of monophonic chant. Chant notation indicated the notes of the chant melody, but without any indication of the rhythm. In the case of Medieval polyphony, such as the motet, the parts were written in separate portions of facing pages. This process was aided by the advent of mensural notation, which also indicated the rhythm and was paralleled by the medieval practice of composing parts of polyphony sequentially, rather than simultaneously (as in later times). Manuscripts showing parts together in score format were rare and limited mostly to organum, especially that of the Notre Dame school. During the Middle Ages, if an Abbess wanted to have a copy of an existing composition, such as a composition owned by an Abbess in another town, she would have to hire a copyist to do the task by hand, which would be a lengthy process and one that could lead to transcription errors.\n\nEven after the advent of music printing in the mid-1400s, much music continued to exist solely in composers' hand-written manuscripts well into the 18th century.\n\nThere were several difficulties in translating the new printing press technology to music. In the first printed book to include music, the \"Mainz Psalter\" (1457), the music notation (both staff lines and notes) was added in by hand. This is similar to the room left in other incunabulae for capitals. The psalter was printed in Mainz, Germany by Johann Fust and Peter Schöffer, and one now resides in Windsor Castle and another at the British Library. Later, staff lines were printed, but scribes still added in the rest of the music by hand. The greatest difficulty in using movable type to print music is that all the elements must line up – the note head must be properly aligned with the staff. In vocal music, text must be aligned with the proper notes (although at this time, even in manuscripts, this was not a high priority).\n\nMusic engraving is the art of drawing music notation at high quality for the purpose of mechanical reproduction. The first machine-printed music appeared around 1473, approximately 20 years after Gutenberg introduced the printing press. In 1501, Ottaviano Petrucci published \"Harmonice Musices Odhecaton A\", which contained 96 pieces of printed music. Petrucci's printing method produced clean, readable, elegant music, but it was a long, difficult process that required three separate passes through the printing press. Petrucci later developed a process which required only two passes through the press. But it was still taxing since each pass required very precise alignment for the result to be legible (i.e., so that the note heads would be correctly lined up with the staff lines). This was the first well-distributed printed polyphonic music. Petrucci also printed the first tablature with movable type. Single impression printing, in which the staff lines and notes could be printed in one pass, first appeared in London around 1520. Pierre Attaingnant brought the technique into wide use in 1528, and it remained little changed for 200 years.\nA common format for issuing multi-part, polyphonic music during the Renaissance was \"partbooks\". In this format, each voice-part for a collection of five-part madrigals, for instance, would be printed separately in its own book, such that all five part-books would be needed to perform the music. The same partbooks could be used by singers or instrumentalists. Scores for multi-part music were rarely printed in the Renaissance, although the use of score format as a means to compose parts simultaneously (rather than successively, as in the late Middle Ages) is credited to Josquin des Prez.\n\nThe effect of printed music was similar to the effect of the printed word, in that information spread faster, more efficiently, at a lower cost, and to more people than it could through laboriously hand-copied manuscripts. It had the additional effect of encouraging amateur musicians of sufficient means, who could now afford sheet music, to perform. This in many ways affected the entire music industry. Composers could now write more music for amateur performers, knowing that it could be distributed and sold to the middle class.\n\nThis meant that composers did not have to depend solely on the patronage of wealthy aristocrats. Professional players could have more music at their disposal and they could access music from different countries. It increased the number of amateurs, from whom professional players could then earn money by teaching them. Nevertheless, in the early years, the cost of printed music limited its distribution. Another factor that limited the impact of printed music was that in many places, the right to print music was granted by the monarch, and only those with a special dispensation were allowed to do so, giving them a monopoly. This was often an honour (and economic boon) granted to favoured court musicians or composers.\n\nMechanical plate engraving was developed in the late sixteenth century. Although plate engraving had been used since the early fifteenth century for creating visual art and maps, it was not applied to music until 1581. In this method, a mirror image of a complete page of music was engraved onto a metal plate. Ink was then applied to the grooves, and the music print was transferred onto paper. Metal plates could be stored and reused, which made this method an attractive option for music engravers. Copper was the initial metal of choice for early plates, but by the eighteenth century, pewter became the standard material due to its malleability and lower cost.\n\nPlate engraving was the methodology of choice for music printing until the late nineteenth century, at which point its decline was hastened by the development of photographic technology. Nevertheless, the technique has survived to the present day and is still occasionally used by select publishers such as G. Henle Verlag in Germany.\n\nAs musical composition increased in complexity, so too did the technology required to produce accurate musical scores. Unlike literary printing, which mainly contains printed words, music engraving communicates several different types of information simultaneously. To be clear to musicians, it is imperative that engraving techniques allow absolute precision. Notes of chords, dynamic markings, and other notation line up with vertical accuracy. If text is included, each syllable matches vertically with its assigned melody. Horizontally, subdivisions of beats are marked not only by their flags and beams, but also by the relative space between them on the page. The logistics of creating such precise copies posed several problems for early music engravers, and have resulted in the development of several music engraving technologies.\n\nIn the 19th century, the music industry was dominated by sheet music publishers. In the United States, the sheet music industry rose in tandem with blackface minstrelsy. The group of New York City-based music publishers, songwriters and composers dominating the industry was known as \"Tin Pan Alley\". In the mid-19th century, copyright control of melodies was not as strict, and publishers would often print their own versions of the songs popular at the time. With stronger copyright protection laws late in the century, songwriters, composers, lyricists, and publishers started working together for their mutual financial benefit. New York City publishers concentrated on vocal music. The biggest music houses established themselves in New York City, but small local publishers – often connected with commercial printers or music stores – continued to flourish throughout the country. An extraordinary number of East European immigrants became the music publishers and songwriters on Tin Pan Alley-the most famous being Irving Berlin. Songwriters who became established producers of successful songs were hired to be on the staff of the music houses.\n\nThe late-19th century saw a massive explosion of parlor music, with ownership of, and skill at playing the piano becoming \"de rigueur\" for the middle-class family. In the late-19th century, if a middle-class family wanted to hear a popular new song or piece, they would buy the sheet music and then perform the song or piece in an amateur fashion in their home. But in the early 20th century the phonograph and recorded music grew greatly in importance. This, joined by the growth in popularity of radio broadcasting from the 1920s on, lessened the importance of the sheet music publishers. The record industry eventually replaced the sheet music publishers as the music industry's largest force.\n\nIn the late 20th and into the 21st century, significant interest has developed in representing sheet music in a computer-readable format (see music notation software), as well as downloadable files. Music OCR, software to \"read\" scanned sheet music so that the results can be manipulated, has been available since 1991.\n\nIn 1998, virtual sheet music evolved further into what was to be termed digital sheet music, which for the first time allowed publishers to make copyright sheet music available for purchase online. Unlike their hard copy counterparts, these files allowed for manipulation such as instrument changes, transposition and MIDI (Musical Instrument Digital Interface) playback. The popularity of this instant delivery system among musicians appears to be acting as a catalyst of new growth for the industry well into the foreseeable future.\n\nAn early computer notation program available for home computers was Music Construction Set, developed in 1984 and released for several different platforms. Introducing concepts largely unknown to the home user of the time, it allowed manipulation of notes and symbols with a pointing device such as a mouse; the user would \"grab\" a note or symbol from a palette and \"drop\" it onto the staff in the correct location. The program allowed playback of the produced music through various early sound cards, and could print the musical score on a graphics printer.\n\nMany software products for modern digital audio workstation and scorewriters for general personal computers support generation of sheet music from MIDI files, by a performer playing the notes on a MIDI-equipped keyboard or other MIDI controller or by manual entry using a mouse or other computer device.\n\nIn 1999, Harry Connick, Jr. invented a system and method for coordinating music display among players in an orchestra. Connick's invention is a device with a computer screen which is used to show the sheet music for the musicians in an orchestra instead of the more commonly used paper. Connick uses this system when touring with his big band, for instance. In the classical music world, some string quartet groups use computer screen-based parts. There are several advantages to computer-based parts. Since the score is on a computer screen, the user can adjust the contrast, brightness and even the size of the notes, to make reading easier. In addition, some systems will do \"page turns\" using a foot pedal, which means that the performer does not have to miss playing music during a page turn, as often occurs with paper parts.\n\nOf special practical interest for the general public is the Mutopia project, an effort to create a library of public domain sheet music, comparable to Project Gutenberg's library of public domain books. The International Music Score Library Project (IMSLP) is also attempting to create a virtual library containing all public domain musical scores, as well as scores from composers who are willing to share their music with the world free of charge.\n\nSome scorewriter computer programs have a feature that is very useful for composers and arrangers: the ability to \"play back\" the notated music using synthesizer sounds or virtual instruments. Due to the high cost of hiring a full symphony orchestra to play a new composition, before the development of these computer programs, many composers and arrangers were only able to hear their orchestral works by arranging them for piano, organ or string quartet. While a scorewiter program's playback will not contain the nuances of a professional orchestra recording, it still conveys a sense of the tone colors created by the piece and of the interplay of the different parts.\n\n\n\n"}
{"id": "3492525", "url": "https://en.wikipedia.org/wiki?curid=3492525", "title": "Signal integrity", "text": "Signal integrity\n\nSignal integrity or SI is a set of measures of the quality of an electrical signal. In digital electronics, a stream of binary values is represented by a voltage (or current) waveform. However, digital signals are fundamentally analog in nature, and all signals are subject to effects such as noise, distortion, and loss. Over short distances and at low bit rates, a simple conductor can transmit this with sufficient fidelity. At high bit rates and over longer distances or through various mediums, various effects can degrade the electrical signal to the point where errors occur and the system or device fails. Signal integrity engineering is the task of analyzing and mitigating these effects. It is an important activity at all levels of electronics packaging and assembly, from internal connections of an integrated circuit (IC), through the package, the printed circuit board (PCB), the backplane, and inter-system connections. While there are some common themes at these various levels, there are also practical considerations, in particular the interconnect flight time versus the bit period, that cause substantial differences in the approach to signal integrity for on-chip connections versus chip-to-chip connections.\n\nSome of the main issues of concern for signal integrity are ringing, crosstalk, ground bounce, distortion, signal loss, and power supply noise.\n\nSignal integrity primarily involves the electrical performance of the wires and other packaging structures used to move signals about within an electronic product. Such performance is a matter of basic physics and as such has remained relatively unchanged since the inception of electronic signaling. The first transatlantic telegraph cable suffered from severe signal integrity problems, and analysis of the problems yielded many of the mathematical tools still used today to analyze signal integrity problems, such as the telegrapher's equations. Products as old as the Western Electric crossbar telephone exchange (circa 1940), based on the wire-spring relay, suffered almost all the effects seen today - the ringing, crosstalk, ground bounce, and power supply noise that plague modern digital products.\n\nOn printed circuit boards, signal integrity became a serious concern when the transition (rise and fall) times of signals started to become comparable to the propagation time across the board. Very roughly speaking, this typically happens when system speeds exceed a few tens of MHz. At first, only a few of the most important, or highest speed, signals needed detailed analysis or design. As speeds increased, a larger and larger fraction of signals needed SI analysis and design practices. In modern (> 100 MHz) circuit designs, essentially all signals must be designed with SI in mind.\n\nFor ICs, SI analysis became necessary as an effect of reduced design rules. In the early days of the modern VLSI era, digital chip circuit design and layout were manual processes. The use of abstraction and the application of automatic synthesis techniques have since allowed designers to express their designs using high-level languages and apply an automated design process to create very complex designs, ignoring the electrical characteristics of the underlying circuits to a large degree. However, scaling trends (see Moore's law) brought electrical effects back to the forefront in recent technology nodes. With scaling of technology below 0.25 µm, the wire delays have become comparable or even greater than the gate delays. As a result, the wire delays needed to be considered to achieve timing closure. In nanometer technologies at 0.13 µm and below, unintended interactions between signals (e.g. crosstalk) became an important consideration for digital design. At these technology nodes, the performance and correctness of a design cannot be assured without considering noise effects.\n\nMost of this article is about SI in relation to modern electronic technology - notably the use integrated circuits and printed circuit board technology. Nevertheless, the principles of SI are not exclusive to the signalling technology used. SI existed long before the advent of either technology, and will do so as long as electronic communications persist.\n\nSignal integrity problems in modern integrated circuits (ICs) can have many drastic consequences for digital designs: \nThe cost of these failures is very high, and includes photomask costs, engineering costs and\nopportunity cost due to delayed product introduction. Therefore, electronic design automation (EDA) tools have been developed to analyze, prevent, and correct these problems.\nIn integrated circuits, or ICs, the main cause of signal integrity problems is crosstalk.\nIn CMOS technologies, this is primarily due to coupling capacitance, but in general it may be caused by mutual inductance, substrate coupling, non-ideal gate operation, and other sources. The fixes normally involve changing the sizes of drivers and/or spacing of wires.\n\nIn analog circuits, designers are also concerned with noise that arise from physical sources, such as thermal noise, flicker noise, and shot noise. These noise sources on the one hand present a lower limit to the smallest signal that can be amplified, and on the other, define an upper limit to the useful amplification.\n\nIn digital ICs, noise in a signal of interest arises primarily from coupling effects from switching of other signals. Increasing interconnect density has led to each wire having neighbors that are physically closer together, leading to increased crosstalk between neighboring nets. As circuits have continued to shrink in accordance with Moore's law, several effects have conspired to make noise problems worse:\n\nThese effects have increased the interactions between signals and decreased the noise immunity of\ndigital CMOS circuits. This has led to noise being a significant problem for digital ICs that must be considered by every digital chip designer prior to tape-out. There are several concerns that must be mitigated:\n\nTypically, an IC designer would take the following steps for SI verification:\n\nModern signal integrity tools for IC design perform all these steps automatically, producing reports that give a design a clean bill of health, or a list of problems that must be fixed. However, such tools generally are not applied across an entire IC, but only selected signals of interest.\n\nOnce a problem is found, it must be fixed. Typical fixes for IC on-chip problems include:\n\nEach of these fixes may possibly cause other problems. This type of issue must be addressed as part of design flows and design closure. Re-analysis after design changes is a prudent measure.\n\nOn-die termination (ODT) or Digitally Controlled Impedance (DCI) is the technology where the termination resistor for impedance matching in transmission lines is located within a semiconductor chip, instead of a separate, discrete device mounted on a circuit board.\nThe closeness of the termination from the receiver shorten the stub between the two, thus improving the overall signal integrity.\n\nFor wired connections, it is important to compare the interconnect flight time to the bit period to decide whether an impedance matched or unmatched connection is needed.\n\nThe channel flight time (delay) of the interconnect is roughly per () of FR-4 stripline (the propagation velocity depends on the dielectric and the geometry). Reflections of previous pulses at impedance mismatches die down after a few bounces up and down the line (i.e. on the order of the flight time). At low bit rates, the echoes die down on their own, and by midpulse, they are not a concern. Impedance matching is neither necessary nor desirable. There are many circuit board types other than FR-4, but usually they are more costly to manufacture.\n\nThe gentle trend to higher bit rates accelerated dramatically in 2004, with the introduction by Intel of the PCI-Express standard. Following this lead, the majority of chip-to-chip connection standards underwent an architectural shift from parallel buses to serializer/deserializer (SERDES) links called \"lanes.\" Such serial links eliminate parallel bus clock skew and reduce the number of traces and resultant coupling effects but these advantages come at the cost of a large increase in bit rate on the lanes, and shorter bit periods.\n\nAt multigigabit/s data rates, link designers must consider reflections at impedance changes (e.g. where traces change levels at vias, see Transmission lines), noise induced by densely packed neighboring connections (crosstalk), and high-frequency attenuation caused by the skin effect in the metal trace and dielectric loss tangent. Examples of mitigation techniques for these impairments are a redesign of the via geometry to ensure an impedance match, use of differential signaling, and preemphasis filtering, respectively.\n\nAt these new multigigabit/s bit rates, the bit period is shorter than the flight time; echoes of previous pulses can arrive at the receiver on top of the main pulse and corrupt it. In communication engineering this is called intersymbol interference (ISI). In signal integrity engineering it is usually called eye closure (a reference to the clutter in the center of a type of oscilloscope trace called an eye diagram). When the bit period is shorter than the flight time, elimination of reflections using classic microwave techniques like matching the electrical impedance of the transmitter to the interconnect, the sections of interconnect to each other, and the interconnect to the receiver, is crucial. Termination with a source or load is a synonym for matching at the two ends. The interconnect impedance that can be selected is constrained by the impedance of free space (), a geometric form factor and by the square root of the relative dielectric constant of the stripline filler (typically FR-4, with a relative dielectric constant of ~4). Together, these properties determine the trace's characteristic impedance. is a convenient choice for single-end lines, and for differential.\n\nAs a consequence of the low impedance required by matching, PCB signal traces carry much more current than their on-chip counterparts. This larger current induces crosstalk primarily in a magnetic or inductive mode as opposed to a capacitive mode. To combat this crosstalk, digital PCB designers must remain acutely aware of not only the intended signal path for every signal, but also the path of returning signal current for every signal. The signal itself and its returning signal current path are equally capable of generating inductive crosstalk. Differential trace pairs help to reduce these effects.\n\nA third difference between on-chip and chip-to-chip connection involves the cross-sectional size of the signal conductor, namely that PCB conductors are much larger (typically or more in width). Thus, PCB traces have a small series resistance (typically 0.1 Ω/cm) at DC. The high frequency component of the pulse is however attenuated by additional resistance due to the skin effect and dielectric loss tangent associated with the PCB material.\n\nThe main challenge often depends on whether the project is a cost-driven consumer application or a performance-driven infrastructure application. They tend to require extensive post-layout verification (using an EM simulator) and pre-layout design optimization (using SPICE and a channel simulator), respectively.\n\nThe noise levels on a trace/network is highly dependent on the routing topology selected. In a point-to-point topology, the signal is routed from the transmitter directly to the receiver (this is applied in PCIe, RapidIO, GbE, DDR2/DDR3/DDR4 DQ/DQS etc.). A point-to-point topology has the least SI-problems since there is no large impedance matches being introduced by line T's (a two-way split of a trace).\n\nFor interfaces where multiple pachages are receiving from the same line, (for example with a backplane configuration), the line must be split at some point to service all receivers. Some stubs and impedance missmatches are deemed to occur. Multipackage intefraces include BLVDS, DDR2/DDR3/DDR4 C/A bank, RS485 and CAN Bus. There are two main multipachage topologies: Tree and fly-by.\n\n\nThere are special purpose EDA tools\nthat help the engineer perform all these steps on each signal in a design, pointing out problems or verifying the design is ready for manufacture. In selecting which tool is best for a particular task, one must consider characteristics of each such as capacity (how many nodes or elements), performance (simulation speed), accuracy (how good are the models), convergence (how good is the solver), capability (non-linear versus linear, frequency dependent versus frequency independent etc.), and ease of use.\n\nAn IC package or PCB designer removes signal integrity problems through these techniques: \n\nEach of these fixes may possibly cause other problems. This type of issue must be addressed as part of design flows and design closure.\n\n\n"}
{"id": "6500706", "url": "https://en.wikipedia.org/wiki?curid=6500706", "title": "Silva compass", "text": "Silva compass\n\nSilva Sweden AB is an outdoors products company, most known for their high-grade compasses and other navigational equipment including GPS tools, mapping software, and altimeters for aircraft. They also offer a marine range. The company's founders - Gunnar Tillander, Alvar Kjellström, Alvid Kjellström, and Björn Kjellström - invented the hugely popular orienteering or protractor compass used around the world for outdoors navigation. \n\nSilva Sweden AB is a company created and based in Sweden that exports worldwide, and operates in numerous countries. They have marketing companies in Stockholm, Sweden, Mantes-la-Ville, France, Friedrichsdorf, Germany, and Livingston, Scotland.\n\nSilva Sweden AB created their first compass in 1928, and established their company in 1933.<br>\nBelow is a Chronological timeline of important dates in the history of Silva Sweden AB: \n\n\nAfter the founding of Silva USA in 1946 and Silva Ltd. in Canada two years later, both affiliates were later acquired by Johnson Wax Associates, later Johnson Camping, Inc., and by 1985, Johnson Worldwide Associates (JWA). From 1980, JWA had imported Swedish-made compasses manufactured by Silva Production AB for sale in North America. \n\nIn 1996, a decision by Silva Production AB to end sole distribution of its Swedish-made \"Silva\"-brand compasses by Silva USA led to a court battle the following year between JWA and Silva Production AB (Silva Sweden AB).\n\nIn 1998, JWA and Silva Production AB of Sweden reached a settlement whereby JWA retained the exclusive right to sell compasses under its \"Silva\" trademark in North America, made by unnamed manufacturers. JWA also retained the North American rights to some product names such as \"Ranger\", \"Polaris\", \"1, 2, 3\" and others commonly used and recognized in the U.S. and Canadian markets and made popular during the time Silva Production AB was manufacturing Swedish-made \"Silva\" compasses for JWA in North America. JWA was eventually renamed Johnson Outdoors, Inc.\n\nFor its part, Silva Production AB/Silva Group retained the right to manufacture and sell compasses, GPS tools, and other navigational products under its \"Silva\" trademark outside the United States and Canada, as well as market its Swedish-made compasses and GPS tools in North America under the \"Brunton\", \"Elite\", and \"Nexus\" brands. The Swedish firm also retained the right to state on \"Nexus\" packaging and in the \"Nexus\" catalog that \"Nexus\" compasses were made by Silva Production AB, but did not retain the right to advertise this fact.\n\nIn 2009 Fiskars sold Brunton Inc. to Fenix Outdoor AB of Sweden, and in consequence, Silva Production AB stopped exporting Silva of Sweden compasses to North America under the \"Brunton\" and \"Nexus\" brands, and halted further imports of Brunton products to markets outside North America under the Silva brand. Currently, Silva of Sweden AB no longer distributes any of its compass products to the USA or Canadian markets.\n\nIn 2006, Silva was acquired by the Finnish Fiskars Group. Silva Sweden AB retained its own corporate identity as an outdoor products manufacturer within Fiskars under its Gerber Legendary Blades Division.\n\nIn 2009, Fiskars sold Brunton Inc. to a Swedish company, Fenix Outdoor AB. After divestiture, Brunton closed out its \"Nexus\" and \"Elite\" compass lines and discontinued the \"Brunton 54LU\" compass, all of which were relabeled Silva of Sweden products, and discontinued imports of the \"Silva Multi-Navigator\" GPS sold in North America as the \"Brunton MNS\". These actions left Silva of Sweden without a North American distributor for its Swedish-made compasses and GPS tools. As a result, Silva of Sweden compasses and GPS products are no longer available in North America. \n\nIn 2011, Fiskars sold Silva Sweden AB to Karnell AB, a Swedish investment group.\n\nToday, the \"Silva Group\" consists of the parent company, now called Silva Sweden AB, together with its subsidiaries \"Silva Ltd.\" in the United Kingdom, \"Silva France\", \"Silva Deutschland\", and \"Silva Far East\". The core activities of the Silva Group consist of design, development, manufacture of compasses for land and sea and sales of compasses, GPS and outdoor instruments, headlamps, binoculars and other electronic navigation equipment.\n\nAt its production facility in Haninge, Sweden, and in mainland China, Silva of Sweden AB manufactures a wide variety of portable compasses for recreational, hiking, scientific, and marine uses. The company produces a range of models, from simple protractor or baseplate compasses like the \"Field\", \"Expedition 3\" (formerly the \"Ranger 3\"), and \"Expedition 4\" to more sophisticated sighting compasses such as the \"Expedition Model 54\" and the Silva \"SightMaster\" line of surveying compasses. \n\nSilva's \"Expedition 15T\", \"Expedition TDCL\", and \"Expedition S\" are modernized versions of the popular Type 15 \"Silva Ranger\", itself a development of the \"Model 1939\" that incorporated a liquid-damped capsule with a sighting mirror that doubled as a protective cover. In keeping with its origins as orienteering compass manufacturers, Silva also offers its Orienteering Series of \"Jet\", \"Spectra\", and \"Race\" models optimized for orienteering and adventure racing competition. \n\nSilva has a long history of supplying variants of their general-use compasses to various military forces of the world, including the defence forces of Great Britain, Denmark, Australia, New Zealand, and Sweden. These include the Silva \"Expedition 3 Military\", \"Expedition 4 Military\", \"Expedition 4B Military\", \"Expedition 15T\", \"Expedition 15TDCL\", and \"Expedition 54 Military\" NATO compasses with dials in both mils and degrees and optional tritium lighting (all standard models have luminous lighting; models with a 'B' (beta) suffix are fitted with self-illuminating tritium capsules.\n\nIn 2000, Silva introduced the Silva \"Multi-Navigator\" navigation tool, which combined a GPS receiver with an electronic compass, barometer, and altimeter. The \"Multi-Navigator\" was sold in North America under the Brunton brand as the \"Brunton MNS\". The \"Multi-Navigator\" was followed in 2004 by the \"Silva Atlas\" navigation tool which featured a greyscale map display. Both the \"Multi-Navigator\" and the \"Atlas\" failed to capture a significant portion of the highly competitive GPS market, and were withdrawn from the market in 2009.\n\nBesides compasses and GPS tools, Silva manufactures several other types of outdoor gear and navigational equipment, including weather/altimeter/temperature/wind meters, headlamps, binoculars, and orienteering accessories.\n\n\n"}
{"id": "40760945", "url": "https://en.wikipedia.org/wiki?curid=40760945", "title": "Talk-down aircraft landing", "text": "Talk-down aircraft landing\n\nA talk-down landing may be attempted in the event of the death or incapacitation of an aircraft pilot. It involves a passenger or other unqualified person flying the aircraft to a landing with assistance from radioed instructions either from the ground or a nearby aircraft.\n\n\nThere is no record of a talk-down landing of a large commercial aircraft. There have, however, been incidents where qualified pilots travelling as passengers or flight attendants on commercial flights have taken the co-pilot's seat to assist the pilot.\n\nIn the 1957 film \"Zero Hour!\", the pilot and co-pilot of an airliner both become seriously ill from food poisoning. A man who has not flown for 10 years and has no familiarity with large aircraft is given instructions from the airport and safely lands the plane. The 1980 comedy film \"Airplane!\" spoofs the same plot with a passenger on a jet airliner being talked down to a safe landing after both pilots contract food poisoning. \"Airport 1975\" released in 1974 has a plot where a Boeing 747 has a mid-air collision with a small plane that leaves the crew dead or badly injured and a stewardess is instructed to fly the aircraft to avoid mountains and on other flight actions, but she does not actually land it. In the 1997 film \"Turbulence\", a stewardess successfully lands a Boeing 747 under instruction by radio after both pilots are killed. In the 1999 film \"Airspeed (film)\", a 13-year-old girl lands a private jet while given instructions from the airport.\nIn 2010, British hypnotist and illusionist Derren Brown presented a programme on Channel 4 television entitled \"Derren Brown: Hero at 30,000 Feet\". It showed Brown putting a man through a series of challenges culminating in him traveling on a plane where the pilot had supposedly been incapacitated. The man, who had not been on a plane in ten years, boarded a flight traveling from Leeds to Jersey, where he had been told that a fake game-show presented by Brown was to be filmed. The flight crew, stewards and stewardesses were real, but the rest of the passengers were actors. During the flight, the cabin crew announced that the captain had been taken ill and asked for a volunteer to land the plane. At the last minute, the man volunteered. While walking to the front of the plane he was placed into a trance by Brown. After the plane landed, the man was placed into a cockpit flight simulator and woken up. He was talked through landing procedures by a person identified to him as an air traffic controller and completed the challenge successfully.\n\nA 2007 edition of the science entertainment television program \"MythBusters\", entitled \"Air Plane Hour\", aimed to determine whether an untrained civilian could be instructed how to successfully land a plane over the radio with the aid of a NASA flight simulator. For their first test, the two presenters who had no flight experience attempted to land a plane unaided. One came in at a steep angle and too fast, and the simulated plane flipped over and broke apart; the other stalled and crash-landed in a field. They then repeated the exercise with instructions by radio from a licensed pilot and both were able to land safely.\n"}
{"id": "2214142", "url": "https://en.wikipedia.org/wiki?curid=2214142", "title": "Tshwane University of Technology", "text": "Tshwane University of Technology\n\nTshwane University of Technology (TUT) is a higher education institution in South Africa that came into being through a merger of three technikons — Technikon Northern Gauteng, Technikon North-West and Technikon Pretoria.\n\nAs the number of students registering annually grows rapidly, records show that Tshwane University of Technology caters for approximately 60,000 students and it has become the largest residential higher education institution in South Africa. \n\nIt comprises up to six satellite campuses. These campuses include Pretoria, Soshanguve, Ga-Rankuwa, Witbank (eMalahleni), Mbombela and Polokwane campuses. Two faculties, namely the Faculties of Science and The Arts, have dedicated campuses in the Pretoria city centre.\n\nThere were 48,078 students enrolled for the year 2012 at the Tshwane University of Technology. It was estimated, for the year 2014, that the number of first year student applications the university received were around 80,000.\n\nIn 2010 Webometrics ranked the university the 15th best in South Africa and 5662th in the world.\n\nThe United Nations Educational, Scientific and Cultural Organization (UNESCO) ranks the university's Department of Journalism as one of twelve Potential Centres of Excellence in Journalism Training in Africa.\n\n\n"}
{"id": "1324296", "url": "https://en.wikipedia.org/wiki?curid=1324296", "title": "Utility pole", "text": "Utility pole\n\nA utility pole is a column or post used to support overhead power lines and various other public utilities, such as electrical cable, fiber optic cable, and related equipment such as transformers and street lights. It can be referred to as a transmission pole, telephone pole, telecommunication pole, power pole, hydro pole, telegraph pole, or telegraph post, depending on its application. A stobie pole is a multi-purpose pole made of two steel joists held apart by a slab of concrete in the middle, generally found in South Australia.\n\nElectrical wires and cables are routed overhead on utility poles as an inexpensive way to keep them insulated from the ground and out of the way of people and vehicles. Utility poles can be made of wood, metal, concrete, or composites like fiberglass. They are used for two different types of power lines; \"subtransmission lines\" which carry higher voltage power between substations, and \"distribution lines\" which distribute lower voltage power to customers.\n\nThe first poles were used in 1816 by the telegraph inventor Sir Francis Ronalds who set up eight miles of overhead cable in Hammersmith. Utility poles were first used in the mid-19th century in America with telegraph systems, starting with Samuel Morse who attempted to bury a line between Baltimore and Washington, D.C., but moved it above ground when this system proved faulty. Today, underground distribution lines are increasingly used as an alternative to utility poles in residential neighborhoods, due to poles' perceived ugliness.\n\nUtility poles are commonly used to carry two types of electric power lines: \"distribution lines\" (or \"feeders\") and \"subtransmission lines\". Distribution lines carry power from local substations to customers. They generally carry voltages from 4.6 to 33 kilovolts (kV) for distances up to 30 miles, and include transformers to step the voltage down from the primary voltage to the lower secondary voltage used by the customer. A service drop carries this lower voltage to the customer's premises.\n\nSubtransmission lines carry higher voltage power from regional substations to local substations. They usually carry 46 kV, 69 kV, or 115 kV for distances up to 60 miles. 230 kV lines are often supported on H-shaped towers made with two or three poles. Transmission lines carrying voltages of above 230 kV are usually not supported by poles, but by metal pylons (known as transmission towers in the US).\n\nFor economic or practical reasons, such as to save space in urban areas, a distribution line is often carried on the same poles as a subtransmission line but mounted under the higher voltage lines; a practice called \"underbuild\". Telecommunication cables are usually carried on the same poles that support power lines; poles shared in this fashion are known as joint-use poles, but may have their own dedicated poles.\n\nThe standard utility pole in the United States is about long and is buried about in the ground. However, poles can reach heights of or more to satisfy clearance requirements. They are typically spaced about apart in urban areas, or about in rural areas, but distances vary widely based on terrain. Joint-use poles are usually owned by one utility, which leases space on it for other cables. In the United States, the National Electrical Safety Code, published by the Institute of Electrical and Electronics Engineers (IEEE) (not to be confused with the National Electrical Code published by the National Fire Protection Association [NFPA]), sets the standards for construction and maintenance of utility poles and their equipment.\n\nMost utility poles are made of wood, pressure-treated with some type of preservative for protection against rot, fungi and insects. Southern yellow pine is the most widely used species in the United States; however, many species of long straight trees are used to make utility poles, including Douglas fir, jack pine, lodgepole pine, western red cedar, and Pacific silver fir.\n\nTraditionally, the preservative used was creosote, but due to environmental concerns, alternatives such as pentachlorophenol, copper naphthenate and borates are becoming widespread in the United States. In the United States, standards for wood preservative materials and wood preservation processes, along with test criteria, are set by ANSI, ASTM, and American Wood Protection Association (AWPA) specifications. Despite the preservatives, wood poles decay and have a life of approximately 25 to 50 years depending on climate and soil conditions, therefore requiring regular inspection and remedial preservative treatments. Woodpecker damage to wood poles is the most significant cause of pole deterioration in the U.S.\n\nOther common utility pole materials are steel and concrete, with composites (such as fibreglass) also becoming more prevalent. One particular patented utility pole variant used in Australia is the Stobie pole, made up of two vertical steel posts with a slab of concrete between them.\n\nIn southern Switzerland along various lakes, telephone poles are made of granite. Starting in the early 1900s, these 18-foot (5 m) poles were originally used for telegraph wires and later for telephone wires. Because they are made of granite, the poles last indefinitely.\n\nOn poles carrying both electrical and communications wiring, the electric power distribution lines and associated equipment are mounted at the top of the pole above the communication cables, for safety. The vertical space on the pole reserved for this equipment is called the \"supply space\". The wires themselves are usually uninsulated, and supported by insulators, commonly mounted on a horizontal crossarm. Power is transmitted using the three-phase system, with three wires, or phases, labeled \"A\", \"B\", and \"C\".\n\nSubtransmission lines comprise only these 3 wires, plus sometimes an overhead ground wire (OGW), also called a \"static line\" or a \"neutral\", suspended above them. The OGW acts like a lightning rod, providing a low resistance path to ground thus protecting the phase conductors from lightning.\n\nDistribution lines use two systems, either grounded-wye (\"Y\" on electrical schematics) or delta (Greek letter \"Δ\" on electrical schematics). A delta system requires only a conductor for each of the three phases. A grounded-wye system requires a fourth conductor, the neutral, whose source is the center of the \"Y\" and is grounded. However, \"spur lines\" branching off the main line to provide power to side streets often carry only one or two phase wires, plus the neutral. A wide range of standard distribution voltages are used, from 2,400 V to 34,500 V. On poles near a service drop, there is a pole-mounted step-down distribution transformer to provide the required mains voltage. In North America, service drops provide 240/120V split-phase power for residential and light commercial service, using cylindrical single-phase transformers. In Europe and most other countries, 230V three phase (230Y400) service drops are used. The transformer's primary is connected to the distribution line through protective devices called fuse cutouts. In the event of an overload, the fuse melts and the device pivots open to provide a visual indication of the problem. They can also be opened manually by linemen using a long insulated rod called a hot stick to disconnect the transformer from the line.\n\nThe pole may be grounded with a heavy bare copper or copper-clad steel wire running down the pole, attached to the metal pin supporting each insulator, and at the bottom connected to a metal rod driven into the ground. Some countries ground every pole while others only ground every fifth pole and any pole with a transformer on it. This provides a path for leakage currents across the surface of the insulators to get to ground, preventing the current from flowing through the wooden pole which could cause a fire or shock hazard. It provides similar protection in case of flashovers and lightning strikes. A surge arrester (also called a lightning arrester) may also be installed between the line (ahead of the cutout) and the ground wire for lightning protection. The purpose of the device is to conduct extremely high voltages present on the line directly to ground.\n\nIf uninsulated conductors touch due to wind or fallen trees, the resultant sparks can start wildfires. To reduce this problem, aerial bundled conductors are being introduced.\n\nThe communications cables are attached below the electric power lines, in a vertical space along the pole designated the \"communications space\". The communications space is separated from the lowest electrical conductor by the \"communication worker safety zone\", which provides room for workers to maneuver safely while servicing the communication cables, avoiding contact with the power lines.\n\nThe most common communication cables found on utility poles are copper or fibre optic cable (FOC) for telephone lines and coaxial cable for cable television (CATV). Coaxial or optical fibre cables linking computer networks are also increasingly found on poles in urban areas. The cable linking the telephone exchange to local customers is a thick cable lashed to a thin supporting cable, containing hundreds of twisted pair subscriber lines. Each twisted pair line provides a single telephone circuit or local loop to a customer. There may also be fibre optic cables interconnecting telephone exchanges. Like electrical distribution lines, communication cables connect to service drops when used to provide local service to customers.\n\nUtility poles may also carry other equipment such as street lights, supports for traffic lights and overhead electric trolley wires, and cellular network antennas. They can also carry fixtures and decorations specific for certain holidays or events specific to the city where they are located.\n\nSolar panels mounted on utility poles may power auxiliary equipment where the expense of a power line connection is unwanted.\n\nStreetlights and holiday fixtures are powered directly from secondary distribution.\n\nThe primary purpose of pole attachment hardware is to secure the cable and associated aerial plant facilities to poles and to help facilitate necessary plant rearrangements. An aerial plant network requires high-quality reliable hardware to\nFunctional performance requirements common to pole line hardware for utility poles made of wood, steel, concrete, or Fiber-Reinforced Composite (FRC) materials are contained in Telcordia GR-3174, \"Generic Requirements for Hardware Attachments for Utility Poles\".\n\n\n\nIn some countries, such as the United Kingdom, utility poles have sets of brackets arranged in a standard pattern up the pole to act as hand and foot holds so that maintenance and repair workers can climb the pole to work on the lines. In the United States, such steps have been determined to be a public hazard and are no longer allowed on new poles. Linemen may use climbing spikes called gaffs to ascend wooden poles without steps on them. In the UK, boots fitted with steel loops that go around the pole (known as \"Scandinavian Climbers\") are also used for climbing poles. In the US, linemen use bucket trucks for the vast majority of poles that are accessible by vehicle.\n\nThe poles at the end of a straight section of utility line where the line ends or angles off in another direction are called \"dead-end\" poles in the United States. Elsewhere they may be referred to as anchor or termination poles. These must carry the lateral tension of the long straight sections of wire. They are usually made with heavier construction. The power lines are attached to the pole by horizontal strain insulators, either placed on crossarms (which are either doubled, tripled, or replaced with a steel crossarm, to provide more resistance to the tension forces) or attached directly to the pole itself.\n\nDead-end and other poles that support lateral loads have guy-wires to support them. The guys always have strain insulators inserted in their length to prevent any high voltages caused by electrical faults from reaching the lower portion of the cable that is accessible by the public. In populated areas, guy wires are often encased in a yellow plastic or wood tube reflector attached to their lower end, so that they can be seen more easily, reducing the chance of people and animals walking into them or vehicles crashing into them.\n\nAnother means of providing support for lateral loads is a push brace pole, a second shorter pole that is attached to the side of the first and runs at an angle to the ground. If there is no space for a lateral support, a stronger pole, e.g. a construction of concrete or iron, is used.\n\nIn 1844, the United States Congress granted Samuel Morse $30,000 to build a 40-mile telegraph line between Baltimore, Maryland and Washington, D.C. Morse began by having a lead-sheathed cable made. After laying seven miles underground, he tested it. He found so many faults with this system that he dug up his cable, stripped off its sheath, bought poles and strung his wires overhead. On February 7, 1844, Morse inserted the following advertisement in the Washington newspaper: \"Sealed proposals will be received by the undersigned for furnishing 700 straight and sound chestnut posts with the bark on and of the following dimensions to wit: 'Each post must not be less than eight inches in diameter at the butt and tapering to five or six inches at the top. Six hundred and eighty of said posts to be 24 feet in length, and 20 of them 30 feet in length.'\"\n\nOne of the early Bell System lines was the Washington DC–Norfolk line which was, for the most part, square-sawn tapered poles of yellow pine probably treated to refusal with creosote. \"Treated to refusal\" means that the manufacturer forces preservatives into the wood, until it refuses to accept more, but performance is not guaranteed.\nSome of these were still in service after 80 years. The building of pole lines was resisted in some urban areas in the late 19th century, and political pressure for undergrounding remains powerful in many countries.\n\nIn Eastern Europe, Russia, and third world countries, many utility poles still carry bare communication wires mounted on insulators not only along railway lines, but also along roads and sometimes even in urban areas. Errant traffic being uncommon on railways, their poles are usually less tall. In the United States electricity is predominately carried on unshielded aluminum conductors wound around a solid steel core and affixed to rated insulators made from glass, ceramic, or poly. Telephone, CATV, and fibre optic cables are generally attached directly to the pole without insulators.\n\nIn the United Kingdom, much of the rural electricity distribution system is carried on wooden poles. These normally carry electricity at 11 or 33 kV (three phases) from 132 kV substations supplied from pylons to distribution substations or pole-mounted transformers. Wooden poles have been used for 132kv for a number of years from the early 1980s one is called the trident they are usually used on short sections, though the line from Melbourne, Cambs to near Buntingford, Herts is quite long. The conductors on these are bare metal connected to the posts by insulators. Wood poles can also be used for low voltage distribution to customers.\n\nToday, utility poles may hold much more than the uninsulated copper wire that they originally supported. Thicker cables holding many twisted pair, coaxial cable, or even fibre-optic, may be carried. Simple analogue repeaters or other outside plant equipment have long been mounted against poles, and often new digital equipment for multiplexing/demultiplexing or digital repeaters may now be seen. In many places, as seen in the illustration, providers of electricity, television, telephone, street light, traffic signal and other services share poles, either in joint ownership or by renting space to each other. In the United States, ANSI standard 05.1.2008 governs wood pole sizes and strength loading. Utilities that fall under the Rural Electrification Act must also follow the guidelines set forth in RUS Bulletin 1724E-150 (from the US Department of Agriculture) for pole strength and loading.\n\nSteel utility poles are becoming more prevalent in the United States thanks to improvements in engineering and corrosion prevention coupled with lowered production costs. However, premature failure due to corrosion is a concern when compared to wood. The National Association of Corrosion Engineers or NACE is developing inspection, maintenance, and prevention procedures similar to those used on wood utility poles to identify and prevent decay.\n\nBritish Telecom posts are usually marked with the following information:\n\nThe date on the pole is applied by the manufacturer and refers to the date the pole was \"preserved\" (treated to withstand the elements).\nIn the United States, utility poles are marked with information concerning the manufacturer, pole height, ANSI strength class, wood species, original preservative, and year manufactured (vintage) in accordance with ANSI standard O5.1.2008. This is called branding, as it is usually burned into the surface; the resulting mark is sometimes called the \"birth mark\". Although the position of the brand is determined by ANSI specification, it is essentially just below \"eye level\" after installation. A rule of thumb for understanding a pole's brand is the manufacturer's name or logo at the top with a two-digit date beneath (sometimes preceded by a month).\n\nBelow the date is a two-character wood species abbreviation and one- to three-character preservative. Some wood species may be marked \"SP\" for southern pine, \"WC\" for western cedar, or \"DF\" for Douglas fir. Common preservative abbreviations are \"C\" for creosote, \"P\" for pentachlorophenol, and \"SK\" for chromated copper arsenate (originally referred to salts type K). The next line of the brand is usually the pole's ANSI class, used to determine maximum load; this number ranges from 10 to H6 with a smaller number meaning higher strength. The pole's height (from butt to top) in 5-foot increments is usually to the right of the class separated by a hyphen, although it is not uncommon for older brands to have the height on a separate line. The pole brand is sometimes an aluminum tag nailed in place.\n\nBefore the practice of branding, many utilities would set a 2- to 4-digit date nail into the pole upon installation. The use of date nails went out of favor during World War II due to war shortages but is still used by a few utilities. These nails are considered valuable to collectors, with older dates being more valuable, and unique markings such as the utilities' name also increasing the value. However, regardless of the value to collectors, all attachments on a utility pole are the property of the utility company, and unauthorized removal is a felony.\n\nA practice in some areas is to place poles on coordinates upon a grid. The pole at right is a Delmarva Power pole located in a rural area of the state of Maryland in the United States. The lower two tags are the \"X\" and \"Y\" coordinates along said grid. Just as in a coordinate plane used in geometry, X increases as one travels east and Y increases as one travels north. The upper two tags are specific to the subtransmission section of the pole; the first refers to the route number, the second to the specific pole along the route.\nHowever, not all power lines follow the road. In the British region of East Anglia, EDF Energy Networks often add the Ordnance Survey Grid Reference coordinates of the pole or substation to the name sign.\n\nIn some areas, utility pole name plates may provide valuable coordinate information: a poor man's GPS.\nA pole route (or pole line in the US) is a telephone link or electrical power line between two or more locations by way of multiple uninsulated wires suspended between wooden utility poles. This method of link is common especially in rural areas where burying the cables would be expensive. Another situation in which pole routes were extensively used were on the railways to link signal boxes. Traditionally, prior to around 1965, pole routes were built with open wires along non-electrical operated railways; this necessitated insulation when the wire passed over the pole, thus preventing the signal from becoming attenuated.\n\nAt electrical operated railways, pole routes were usually not built as too much jamming from the overhead wire would occur. To accomplish this, cables were separated using spars with insulators spaced along them; in general four insulators were used per spar. Only one such pole route still exists on the UK rail network, in the highlands of Scotland. There was also a long section in place between Wymondham, Norfolk and Brandon in Suffolk, United Kingdom; however, this was de-wired and removed during March 2009.\n\nUtility poles are used by birds for nesting and to rest on. Utility poles and related structures are regarded by some to be a form of visual pollution. Many lines are placed underground for this reason, in places of high population density or scenic beauty that justify the expense. Architects design some pylons to be pretty, thus avoiding visual pollution.\n\nSome chemicals used to preserve wood poles including creosote and pentachlorophenol are toxic and have been found in the environment.\n\nHistorically, pole-mounted transformers were filled with a polychlorinated biphenyl (PCB) liquid. PCBs persist in the environment and have adverse effects on animals.\n\n\n"}
{"id": "42095650", "url": "https://en.wikipedia.org/wiki?curid=42095650", "title": "Valley Preferred Cycling Center", "text": "Valley Preferred Cycling Center\n\nThe Valley Preferred Cycling Center (VPCC), also known as the Lehigh Valley Velodrome or simply T-Town, is a professional cycling center and a velodrome located in Breinigsville, Pennsylvania. It serves as the Lehigh Valley's main track cycling stadium. The velodrome is operated by a non-profit [[501(c)(3)]https://www.mcall.com/news/breaking/mc-nws-velodrome-marty-nothstein-leave-20180530-story.html] organization \"Velodrome Fund., Inc\" that promotes competitive cycling, youth fitness, and adult wellness activities for the Lehigh Valley. Over the years, the velodrome hosted various cycling championships. VPCC is the home of the [[World Series of Bicycling]]. The Velodrome annually hosts the [[USA Cycling]] Elite Nationals qualifying event. The center also features a Cycling Hall of Fame. Over the past 40 years, the center introduced tens of thousands of people to cycling, producing over 140 national champions, seven world champions, and three Olympic medalist. [[Marty Nothstein]], a three-time world champion in track events and an Olympic gold and silver medalist, is no longer the executive director of Valley Preferred Cycling Center.<https://www.mcall.com/news/breaking/mc-nws-velodrome-marty-nothstein-leave-20180530-story.html></ref>\n\n[[File:Cyclists on the valley preferred cycling center track 2.jpg|thumb|left|T-Town Elite [[cyclists]] during routine practice.]]\nThe velodrome started as an initiative by [[Robert Rodale]], a publisher and an Olympian, in the early 1970s. Rodale became interested in cycling while competing in the [[Pan-American Games]] in Winnipeg, Canada in [[1967 Pan American Games|1967]]. In 1974 construction broke ground on the plot of land that was owned by Bob Rodale and his wife, Ardath. The first race was held on October 12, 1975. The Velodrome was originally called the \"Lehigh County Velodrome\" or simply \"T-town\" (due to its close proximity to [[Trexlertown]]). The center underwent a number of renovations which added rest rooms, seats for the fans, a podium, showers, and changing rooms.\n\nIn 1995, the center underwent a major $2.5M renovations in preparation for the [[1996 Summer Olympics]] cycling trials. In 2008, extensive repairs and resurfacing were done to the Velodrome.\n\nIn 2007, Valley Preferred Health Network bought the naming rights to the center, and the velodrome's name became Valley Preferred Cycling Center.\n\nIn 2008, the town of [[Breinigsville]] offered land to expand the center to include a hall of fame. Today, the center is part of a 103-acre Bob Rodale Cycling and Fitness Park.\n\nThe center hosted various competitions over the years including the [[UCI Track Cycling World Cup]] and the [[UCI Juniors Track World Championships]]. It's the home of the [[World Series of Bicycling]] and the annual [[USA Cycling]] Elite Nationals qualifying event. Additionally many other smaller regional, national championships and international competitions also take place. Most recently, the VPCC hosted the 2016 [[USA Cycling]] [[United States National Track Championships|Eilte and Junior National Track Championships]].\n\nVPCC offers a variety of free or low-cost community cycling programs designed to introduce the public to the sport of track cycling, including Try the Track, the spring and fall Bicycling Racing League, and Air Products Development Program.\n\nThe velodrome is outdoor and uncovered. The track is 1093.6 ft (333.3 m) in length with a concrete surface. The track has 30-degree banked turns and 12.5-degree straightaways. At the bottom of the track is an 8 feet (2.4 m) concrete apron. Time trial lines are painted on the track as well.\n\n[[File:Valley preferred cycling center seating.jpg|thumb|right|Main seating area.]]\nThe Lehigh Valley Velodrome houses a \"Cycling Hall of Fame\". Members of the hall of fame include:\n\nThe VeloFest is the largest cycling marketplace in the United States. It's held twice a year in May and October on the infield of the Valley Preferred Cycling Center. The flea market features 100s of vendors with 1000s of cycling enthusiasts visiting each year.\n\n[[Category:Velodromes in the United States]]\n[[Category:Sports venues in Pennsylvania]]\n[[Category:Sports in Allentown, Pennsylvania]]\n[[Category:1975 establishments in Pennsylvania]]\n[[Category:History of cycling]]\n[[Category:Cycling museums and halls of fame]]\n[[Category:Sports halls of fame]]\n[[Category:Halls of fame in Pennsylvania]]\n[[Category:Buildings and structures in Lehigh County, Pennsylvania]]\n[[Category:Sports venues completed in 1975]]"}
