{"id": "8018240", "url": "https://en.wikipedia.org/wiki?curid=8018240", "title": "AN/APQ-13", "text": "AN/APQ-13\n\nThe AN/APQ-13 radar was an American ground scanning radar developed by Bell Laboratories, Western Electric, and MIT as an improved model of the airborne H2X radar, itself developed from the first ground scanning radar, the British H2S radar. It was used on B-29s during World War II in the Pacific theater for high altitude area bombing, search and navigation. Computation for bombing could be performed by an impact predictor. A range unit permitted a high degree of accuracy in locating beacons. The radome was carried on the aircraft belly between the bomb bays and was partially retractable on early models. The radar operated at a frequency of 9375 ± 45 megahertz and used a superheterodyne receiver.\n\nThe AN/APQ-13 radar was the first military radar converted to civilian use as a weather warning radar. About 30 systems were converted, starting in late 1945. They were installed at military bases.\n\nThe last operational APQ-13 was removed from the Fort Sill, Oklahoma post weather station in October 1977 for display at what is now the National Museum of the United States Air Force. The museum intended to display it in its original configuration as a navigation and bombing radar, but would note the radar's much longer history as an operational weather radar.\n\nThe AN/APQ-13 weather radars were generally replaced by the AN/CPS-9, which was specifically designed as a weather radar.\n"}
{"id": "2655811", "url": "https://en.wikipedia.org/wiki?curid=2655811", "title": "Accel (interbank network)", "text": "Accel (interbank network)\n\nAccel (formerly known as ACCEL/Exchange) is an interbank network. It connects over 412,000 \nautomatic teller machines (ATMs) in all 50 states in the United States. The network also has a small number of ATMs in certain U.S. Air Force Bases around the world, and over 3.3 million electronic funds transfer at point-of-sale locations.\n\nOperating in Canada as The Exchange, it provides 3300 ATMs to customers of 170 financial institutions, including smaller banks like National Bank of Canada and Canadian Western Bank, credit unions and online banks.\n\nThe U.S. division is managed by Fiserv.\n\n\n"}
{"id": "10899583", "url": "https://en.wikipedia.org/wiki?curid=10899583", "title": "Adaptive equipment", "text": "Adaptive equipment\n\nAdaptive equipment are devices that are used to assist with completing activities of daily living.\n\nBathing, dressing, grooming, toileting, and feeding are self-care activities that are including in the spectrum of activities of daily living (ADLs).\nJennifer McLaughlin Maly a P.T./ D.P.T. in her article located in the journal Exceptional Parent gives a more complete definition of adaptive equipment: \"Typically, a piece of adaptive equipment is utilized to increase a child's function. Examples of adaptive equipment or assistive technology are wheelchairs, lifts, standing frames, gait trainers, augmentative communication devices, bath chairs, and recreational items such as swings or tricycles.\"\n\nA growing market for adaptive equipment is in the use of mobility vans. In this case, adaptive equipment, also known as assistive technology, can help a person with a disability operate a motor vehicle when otherwise they would not be able to.\n\nMobility adaptive equipment are used in cases where a disease or accident leaves an individual's motor functions hindered or unusable. if an individual suffers from restricted motor functions, there are Equipment and technology that can assist in regaining some or all mobility.\n\nA manual or motorized wheelchair is a chair with attached wheels that allow a person who can not walk, due to illness, injury, or disability, to move around.\n\nCrutches are devices used to transfer the bodies load from the lower body to the upper body. Crutches are used when a person lower body is not completely immobilized but impaired.\n\nProsthetic devices are artificial devices used to replace a missing body part caused from either an illness, accident, or a birth defect.\n\nOrthotic devices, or orthoses, are devices used to align, brace, or correct deformities. Orthoses also help to improve the movement of ones joints, spine, or limbs.\n\nSensory/Neurological adaptive equipment are used in cases where an individual lacks proper stimulation of a sense. For instance, individuals who are either blind, mute, deaf, or a combination of them.\n\nHearing aids are devices used by partially deaf individual to regain a portion of hearing by amplifying sound.\n\nBraille is a system a raised bumps that allow blind individuals to read text with their fingers. Braille is a code of language and not a language in itself.\n\nAssistive listening devices (ADL) are devices used to amplify sounds an individual wants to hear, especially in areas with lots of back ground noise. ADLs can be used with hearing aids and cochlear implants to improve the individuals hearing.\n\nAugmentative and Alternative Communication (AAC) devices are used to help individuals with communication disorder to express themselves to others. The devices can carry from picture boards to computer assisted speech.\n\nAlerting Devices are assistive device that connect with door bells, telephones, and other alarming device. These devices add a specific alarm based on one's disability. For instance, a deaf individual can have a door bell that blinks a light instead of a noise to indicate someone is at the door.\n"}
{"id": "53274321", "url": "https://en.wikipedia.org/wiki?curid=53274321", "title": "Alliance for Biosecurity", "text": "Alliance for Biosecurity\n\nThe Alliance for Biosecurity is a consortium of companies that develop products to respond to national security threats, including bioterrorism pathogens and emerging infectious diseases. It is headquartered in Washington DC.\n\nThe United States faces risks to national security posed by the danger of bioterrorism or a destabilizing infectious disease pandemic. The vulnerability is considered severe because many of the vaccines and medicines that would be needed to protect people do not currently exist. The Alliance for Biosecurity is a group of pharmaceutical and biotechnology companies that work to create preventive measures and treatments for severe infectious diseases.\n\nWithin the U.S. federal government, the Biomedical Advanced Research and Development Authority (BARDA) and the Project BioShield Special Reserve Fund (SRF) provide funding to research, develop, and procure a medicines to control epidemics.\n\nThe Alliance for Biosecurity was formed in 2005. Its purpose was to build a partnership between government and private sector biotechnology and pharmaceutical companies working in the biodefense space. The Center for Biosecurity, a nonprofit multidisciplinary organization of physicians public health professionals and scientists, was an organizer of the alliance and participates in it. Together, the two groups have provided congressional testimony and authored letters to Congress.\n\nIn April 2018, the alliance conducted a national poll about biosecurity. Seventy-three percent of the 1,612 Americans polled said they would support a congressional decision to increase funding to address biosecurity needs and capabilities. The poll was conducted, in part, to measure support for biosecurity funding because reauthorization of the Pandemic and All-Hazards Preparedness Act (PAHPA) is due by September 30, 2018. PAHPA is a law that improved the federal government's medical and public health preparedness for national security threats. Examples of threats include the spread of infectious diseases or chemical, biological, radiological or nuclear (CBRN) attacks.\n\nIn 2018, Congress passed the annual Labor, Health and Human Services, and Education appropriations bill before the end of the fiscal year for the first time in over 20 years. Congress also passed a Department of Defense appropriations bill before the end of the fiscal year for the first time in 10 years. The alliance supported passage of both bills. Key funding in the bills included:\n\n\nThe Alliance for Biosecurity is a coalition of biopharmaceutical companies and laboratory/academic partners that promotes a strong public-private partnership to ensure medical countermeasures are available to protect public health and enhance national health security. The Alliance advocates for public policies and funding to support the rapid development, production, stockpiling, and distribution of critically needed medical countermeasures.\n\nThe alliance has supported the following legislation:\nThe alliance also gives out awards to Congress. For example, in October 2017 it awarded eight Members of Congress, such as Maryland Congressman Dutch Ruppersberger, with its \"Congressional Biosecurity Champion Award,\" which honors elected officials who work to improve how the U.S. can prevent and fight biosecurity threats.\n\nThe Alliance for Biosecurity is made up of the following companies and university research labs:\n\n"}
{"id": "30876233", "url": "https://en.wikipedia.org/wiki?curid=30876233", "title": "Aspect ratio (image)", "text": "Aspect ratio (image)\n\nThe aspect ratio of an image describes the proportional relationship between its width and its height. It is commonly expressed as two numbers separated by a colon, as in \"16:9\". For an \"x\":\"y\" aspect ratio, no matter how big or small the image is, if the width is divided into \"x\" units of equal length and the height is measured using this same length unit, the height will be measured to be \"y\" units.\n\nFor example, in a group of images that all have an aspect ratio of 16:9, one image might be 16 inches wide and 9 inches high, another 16 centimeters wide and 9 centimeters high, and a third might be 8 yards wide and 4.5 yards high. Thus, aspect ratio concerns the \"relationship\" of the width to the height, not an image's actual size.\n\nThe most common aspect ratios used today in the presentation of films in cinemas are 1.85:1 and 2.39:1. Two common videographic aspect ratios are 4:3 (1.:1), the universal video format of the 20th century, and (1.:1), universal for high-definition television and European digital television. Other cinema and video aspect ratios exist, but are used infrequently.\n\nIn still camera photography, the most common aspect ratios are 4:3, 3:2, and more recently found in consumer cameras, 16:9. Other aspect ratios, such as 5:3, 5:4, and 1:1 (square format), are used in photography as well, particularly in medium format and large format.\n\nWith television, DVD and Blu-ray Disc, converting formats of unequal ratios is achieved by enlarging the original image to fill the receiving format's display area and cutting off any excess picture information (zooming and cropping), by adding horizontal mattes (letterboxing) or vertical mattes (pillarboxing) to retain the original format's aspect ratio, by stretching (hence distorting) the image to fill the receiving format's ratio, or by scaling by different factors in both directions, possibly scaling by a different factor in the center and at the edges (as in \"Wide Zoom mode\").\n\nIn motion picture formats, the physical size of the film area between the sprocket perforations determines the image's size. The universal standard (established by William Dickson and Thomas Edison in 1892) is a frame that is four perforations high. The film itself is 35 mm wide (1.38 in), but the area between the perforations is 24.89 mm × 18.67 mm (0.980 in × 0.735 in), leaving the de facto ratio of 4:3, or 1.:1.\n\nWith a space designated for the standard optical soundtrack, and the frame size reduced to maintain an image that is wider than tall, this resulted in the Academy aperture of 22 mm × 16 mm (0.866 in × 0.630 in) or 1.375:1 aspect ratio.\n\nThe motion picture industry convention assigns a value of 1.0 to the image’s height; an anamorphic frame (since 1970, 2.39:1) is often incorrectly described (rounded) as 2.40:1 or 2.40 (\"two-four-oh\"). After 1952, a number of aspect ratios were experimented with for anamorphic productions, including 2.66:1 and 2.55:1. A SMPTE specification for anamorphic projection from 1957 (PH22.106-1957) finally standardized the aperture to 2.35:1. An update in 1970 (PH22.106-1971) changed the aspect ratio to 2.39:1 in order to make splices less noticeable. This aspect ratio of 2.39:1 was confirmed by the most recent revision from August 1993 (SMPTE 195-1993).\n\nIn American cinemas, the common projection ratios are 1.85:1 and 2.39:1. Some European countries have 1.:1 as the wide screen standard. The \"Academy ratio\" of 1.375:1 was used for all cinema films in the sound era until 1953 (with the release of George Stevens' \"Shane\" in 1.:1). During that time, television, which had a similar aspect ratio of 1.:1, became a perceived threat to movie studios. Hollywood responded by creating a large number of wide-screen formats: CinemaScope (up to 2.:1), Todd-AO (2.20:1), and VistaVision (initially 1.50:1, now 1.:1 to 2.00:1) to name just a few. The \"flat\" 1.85:1 aspect ratio was introduced in May 1953, and became one of the most common cinema projection standards in the U.S. and elsewhere.\n\nThe goal of these various lenses and aspect ratios was to capture as much of the frame as possible, onto as large an area of the film as possible, in order to fully utilize the film being used. Some of the aspect ratios were chosen to utilize smaller film sizes in order to save film costs while other aspect ratios were chosen to use larger film sizes in order to produce a wider higher resolution image. In either case the image was squeezed horizontally to fit the film's frame size and avoid any unused film area.\n\nDevelopment of various film camera systems must ultimately cater to the placement of the frame in relation to the lateral constraints of the perforations and the optical soundtrack area. One clever wide screen alternative, VistaVision, used standard 35 mm film running sideways through the camera gate, so that the sprocket holes were above and below frame, allowing a larger horizontal negative size per frame as only the vertical size was now restricted by the perforations. There were even a limited number of projectors constructed to also run the print-film horizontally. Generally, however, the 1.50:1 ratio of the initial VistaVision image was optically converted to a vertical print (on standard four-perforation 35 mm film) to show with the standard projectors available at theaters, and was then masked in the projector to the US standard of 1.85:1. The format was briefly revived by Lucasfilm in the late 1970s for special effects work that required larger negative size (due to image degradation from the optical printing steps necessary to make multi-layer composites). It went into obsolescence largely due to better cameras, lenses, and film stocks available to standard four-perforation formats, in addition to increased lab costs of making prints in comparison to more standard vertical processes. (The horizontal process was also adapted to 70 mm film by IMAX, which was first shown at the Osaka '70 Worlds Fair.)\n\nSuper 16 mm film was frequently used for television production due to its lower cost, lack of need for soundtrack space on the film itself (as it is not projected but rather transferred to video), and aspect ratio similar to 16:9 (the native ratio of Super 16 mm is 15:9). It also can be blown up to 35 mm for theatrical release and therefore is sometimes used for feature films.\n\nSquare displays are rarely used in devices and monitors. Nonetheless, video consumption on social apps has grown rapidly and led to the emergence of new video formats more suited to mobile devices that can be held in horizontal and vertical orientations. In that sense, square video was popularized by mobile apps such as Instagram and has since been supported by other major social platforms including Facebook and Twitter. It can fill nearly twice as much screen space compared to 16:9 format (when the device is held differently while viewing from how video was recorded).\n\n4:3 (1.:1) (generally read as \"Four-Three\", \"Four-by-Three\", or \"Four-to-Three\") for standard television has been in use since the invention of moving picture cameras and many computer monitors used to employ the same aspect ratio. 4:3 was the aspect ratio used for 35 mm films in the silent era. It is also very close to the 1.375:1 Academy ratio, defined by the Academy of Motion Picture Arts and Sciences as a standard after the advent of optical sound-on-film. By having TV match this aspect ratio, movies originally photographed on 35 mm film could be satisfactorily viewed on TV in the early days of the medium (i.e. the 1940s and the 1950s). Since the start of the 21st century broadcasters worldwide are phasing out the 4:3 standard entirely, as manufacturers started to favor the 16:9 aspect ratio of all modern high-definition television sets and broadcast cameras.\n\n16:9 (1.:1) (generally named as \"Sixteen-by-Nine\", \"Sixteen-Nine\", and \"Sixteen-to-Nine\") is the international standard format of HDTV, non-HD digital television and analog widescreen television PALplus. Japan's Hi-Vision originally started with a 5:3 (= 15:9) ratio but converted when the international standards group introduced a wider ratio of 5⅓ to 3 (= 16:9). Many digital video cameras have the capability to record in 16:9, and 16:9 is the only widescreen aspect ratio natively supported by the DVD standard. DVD producers can also choose to show even wider ratios such as 1.85:1 and 2.39:1 within the 16:9 DVD frame by hard matting or adding black bars within the image itself. However, it was used often in British TVs in the United Kingdom in the 1990s before the 21st century.\n\nWhen cinema attendance dropped, Hollywood created widescreen aspect ratios in order to differentiate the film industry from TV. Being one of the most common the 1.85:1 ratio.\n\nThe 2:1 aspect ratio was first used in the 1950's for the RKO Superscope format.\n\nSince 1998, cinematographer Vittorio Storaro has advocated for a format named \"Univisium\" that uses an 2:1 format. It is designed to be a compromise between the cinema 2.39:1 aspect ratio and the HD-TV broadcast 16:9 ratio. Univisium has gained little traction in the theatrical film market, but has recently been used by Netflix and Amazon Video for productions such as \"House of Cards\" and \"Transparent\", respectively. This aspect ratio is standard on the acquisition formats mandated by these content platforms and is not necessarily a creative choice.\n\nMoreover, some mobile devices, such as the LG G6, LG V30, Huawei Mate 10 Pro, Google Pixel 2 XL and OnePlus 5T, are embracing the 2:1 format (advertised as 18:9), as well as the Samsung Galaxy S8, Samsung Galaxy Note 8, Samsung Galaxy S9 and Samsung Galaxy Note 9 with an slightly similar 18.5:9 format. The Apple iPhone X also has a similar screen ratio of 19.5:9 (2.16:1).\n\nAnamorphic format is the cinematography technique of shooting a widescreen picture on standard 35 mm film or other visual recording media with a non-widescreen native aspect ratio. When projected, image have an aproximated 2.35:1 or 2.39:1 aspect ratio.\n\nAnother trend arising from the massive use of smartphones is Vertical video (9:16), that is intended for viewing in portrait mode. It was popularized by Snapchat and is also now being adopted by Twitter and Facebook.\n\nOften, screen specifications are given by their diagonal length. The following formulae can be used to find the height (\"h\"), width (\"w\") and area (\"A\"), where \"r\" stands for ratio, written as a fraction, and \"d\" for diagonal length.\n"}
{"id": "20978769", "url": "https://en.wikipedia.org/wiki?curid=20978769", "title": "Bulging factor", "text": "Bulging factor\n\nBulging factor is an engineering term describing the geometry of out-of plane deformations of the surface of a crack on a pressurized fuselage structure. It is used in evaluating the damage tolerance of airframe fuselages.\n\nThe single curved geometry and pressure differential causes a longitudinal crack to bulge out or protrude from the original shape. This change in geometry, or “bulging effect”, significantly increases the stress intensity factor at the crack tips. The effects of this loading condition can trigger different types of failure mechanisms. \n\nFor the case of unstiffened shell structures, the bulging factor can be defined as the ratio of stress-intensity (SIF) of a curved shell to the stress-intensity factor of a flat panel:\n\nformula_1\n\nThe representation of this phenomenon becomes rather complex due to the biaxial and internal pressure load and structural configuration.\n\n"}
{"id": "8869168", "url": "https://en.wikipedia.org/wiki?curid=8869168", "title": "Business Journalist of the Year Awards", "text": "Business Journalist of the Year Awards\n\nThe Business Journalist of the Year Awards are widely recognised as the most important global awards for business writers and broadcasters. They are the only awards for business writers that are open to journalists of all nationalities, and the only awards to cover the entire spectrum of business and financial reporting.\n\nThe Business Journalist of the Year Awards (first presented in 1999) were created because business information had become a global commodity paying little heed to national borders. In a world where goods, services, money and knowledge move ever more freely from one domain to another, the source of business information is no longer relevant - only its quality matters. That's the spirit these awards were created in, and the spirit in which they have grown.\n\nThe Business Journalist of the Year Awards are judged exclusively by journalists and governed by the World's leading business editors. The Editors' Committee comprises Martin Dickson, Deputy Editor of the \"Financial Times\"; Robert Peston, business editor at the BBC; Hugo Dixon, editor-in-chief and chairman of Breakingviews; Jesse Lewis, managing editor of \"The Wall Street Journal Europe\"; and Rik Kirkland, former managing editor of \"Fortune\".\n\n"}
{"id": "40495186", "url": "https://en.wikipedia.org/wiki?curid=40495186", "title": "Canaan Creative", "text": "Canaan Creative\n\nCanaan Creative, known simply as Canaan, is an computer hardware manufacturer based in Beijing, China. It produces application-specific integrated circuits (ASICs) for use in bitcoin mining.\n\nFounded by N.G. Zhang in 2013, Canaan invented the first Bitcoin-specific ASICs. Canaan is known for using open source software in its products.\n\nIn 2010, while still pursuing a Ph.D. degree in electrical engineering, Ngzhang discovered Bitcoin. In November 2011, Ngzhang announced on Bitcointalk the world's first dedicated bitcoin mining hardware named \"Icarus\" using FPGA technology.\n\nThe follow-up product came in May 2012 with the second FPGA-based design, the Lancelot.\n\nIn 2012, upon the release of Icarus, the Bitcoin community and industry gave much attention to the project. Then Ngzhang founded the Avalon Project as the brand to launch bitcoin mining hardware and Avalon ASIC semiconductors. And, because the software around the chips is open source, many engineers joined the project.\n\nAvalon is a Canaan Creative brand and product line offering both bitcoin mining machines and semiconductor chips under the same name. The project was originally created by Zhang Ng, known also as \"ngzhang\".\n\nBy January 2013, the Avalon Project successfully developed the world's first dedicated bitcoin mining ASIC chip, the Avalon1 A3256. A month later, Avalon announced a product, the Avalon1 3module which used the Avalon1 A3256 chip. The Avalon1 had the equivalent SHA-256 algorithm calculating power as the most powerful graphics card at the time, AMD's HD5970 Test's performed showed that the chip was 110 times more powerful than the previous FPGA calculating power. This also meant that use of graphics card \"GPU\" bitcoin mining had also been surpassed with the Avalon1.\n\nIn August 2013, the Avalon Project launched its second generation of dedicated bitcoin mining ASIC chips, the Avalon2 A3255. Soon after, the \"Avalon2 Single\" hardware product using a single Avalon2 A3255 chip, was released. Unlike past machines, the Avalon2 does not operate in standalone mode. During the design of the Avalon2, a bottleneck in mining performance was realized with a standalone-only design. Therefore, the Avalon2 also is operated with an external controller so multiple Avalon2 Single units can be operated in parallel to reduce peripheral costs. The mining power in Bitcoin shifted from a single user's running individual mining machines to running Bitcoin mining farms.\n\nIn April 2014, the Avalon Project launched the third generation dedicated bitcoin mining chip the Avalon3 A3233 after 1 year and 3 months from its predecessor. In that time, the Bitcoin industry grew around the world. The Avalon Project also launched the Bitcoin hardware product, the Avalon3 using the Avalon3 A3233 ASIC chip and an open source Web-based management software called AMS (Avalon Management System).\n\nIn August 2014, Avalon released the Avalon nano with a single A3233 chip on a small unit one can simply plug into a USB port for power and mining.\n\nBy September 2014, the Avalon Project entered advanced IC (Integrated circuit) design by releasing the Avalon3 A3222 chip using a 28 nm ASIC chip manufacturing process.\n\nIn October 2014, the Avalon4.0 mining hardware was released using 40 Avalon A3222 chips to bring 1T or 1 Terahashes per second SHA-256 Hashing power.\n\nThe latest hardware release came in October 2015 is the Avalon6 which uses 80 updated A3218 Avalon Avalon ASIC chips to achieve a 3.5T hash rate.\n\nIn August 2018, Canaan announced a TV set that supposedly called \"AvalonMiner Inside\" which is a TV set with processing power of 2.8 trillion hashes per second.\n\nIn October 2018, Canaan announced the creation of a KPU or knowledge processing unit with an official title of Kendryte. Other products Canaan is working on include a \"cloud computing heater\" and a mobile mining farm called an \"Avalon Box\".\n\nThe company attempted a reverse acquisition for $466 Millions USD by Shandong Luyitong (LYT), publicly traded company on the Shenzhen stock market. After the acquisition, Canaan Creative would have been a wholly owned sudsidiary at that point.\n\nPublic reports stated that, \"Luyitong [would have] paid roughly ¥1bn (approximately $152m) in cash, and issued 81 million shares at an average price per share of ¥24.57 ($3.74), representing an additional ¥1.99bn (roughly $303m).\" Reports say that Canaan's \"strong sales projections fueled the valuation, and that it anticipated a net profit of $27m for 2016, with expectations of $39m and $53m in 2017 and 2018, respectively.\"\n\nThe attempted deal was an investment in Canaan's future as noted by founder, N.G. Zhang's statement, \"This acquisition and investment provides us the necessary resources and autonomy to make even better products and innovations that our customers expect.\"\n\nHowever, in the end, the deal did not happen as noted by Coindesk. \"While Canaan spent time and resources pulling together this deal, those efforts are not wasted. They are contributions to our future. Internally Canaan strengthened its management structure, refreshed our public image, and have our product pipelines flowing to deliver solid reliable and efficient bitcoin technology solutions worldwide,\" said N.G. Zhang, Canaan's CEO.\n\n\n"}
{"id": "43495665", "url": "https://en.wikipedia.org/wiki?curid=43495665", "title": "Cash on delivery", "text": "Cash on delivery\n\nCash on delivery (COD), sometimes called collect on delivery, is the sale of goods by mail order where payment is made on delivery rather than in advance. If the goods are not paid for, they are returned to the retailer. Originally, the term applied only to payment by cash but as other forms of payment have become more common, the word \"cash\" has sometimes been replaced with the word \"collect\" to include transactions by checks, credit cards or debit cards.\n\nThe advantages of COD for online or mail order retailers are:\n\nThe principal disadvantage for retailers is that many more orders will be returned as buyers are less committed to the purchase than if they had paid in advance.\n\nMost operators impose a limit on the amount of money that can be collected per delivery or per day using COD services. Limits may be higher for non-cash payments. Canada Post, for instance, applies a limit of C$1000 for cash, but C$5,000 for payment by check or money order.\n\nIn some countries COD remains a popular option with internet-based retailers, since it is far easier to set up for small businesses and does not require the purchaser to have a credit card. Many small businesses prefer cash payment to credit card payment, as it avoids credit card processing fees. Some shops also offer discounts if paid in cash, because they can offer a better price to the consumer.\n\nThe overwhelming majority of e-shopping transactions in the Middle East are COD. 60% of online transactions in the UAE and Middle East are done by cash on delivery and this has also led to the growth of courier companies offering a COD service.\n\n"}
{"id": "69794", "url": "https://en.wikipedia.org/wiki?curid=69794", "title": "Cryptogram", "text": "Cryptogram\n\nA cryptogram is a type of puzzle that consists of a short piece of encrypted text. Generally the cipher used to encrypt the text is simple enough that the cryptogram can be solved by hand. Frequently used are substitution ciphers where each letter is replaced by a different letter or number. To solve the puzzle, one must recover the original lettering. Though once used in more serious applications, they are now mainly printed for entertainment in newspapers and magazines. \n\nOther types of classical ciphers are sometimes used to create cryptograms. An example is the book cipher where a book or article is used to encrypt a message.\n\nThe \"Cryptogram\" is also the name of the periodic publication of the American Cryptogram Association (ACA), which contains a large number of cryptographic puzzles.\n\nThe ciphers used in cryptograms were not originally created for entertainment purposes, but for real encryption of military or personal secrets.\n\nThe first use of the cryptogram for entertainment purposes occurred during the Middle Ages by monks who had spare time for intellectual games. A manuscript found at Bamberg states that Irish visitors to the court of Merfyn Frych ap Gwriad (died 844), king of Gwynedd in Wales were given a cryptogram which could only be solved by transposing the letters from Latin into Greek. Around the thirteenth century, the English monk Roger Bacon wrote a book in which he listed seven cipher methods, and stated that \"a man is crazy who writes a secret in any other way than one which will conceal it from the vulgar.\" In the 19th century Edgar Allan Poe helped to popularize cryptograms with many newspaper and magazine articles.\n\nWell-known examples of cryptograms in contemporary culture are the syndicated newspaper puzzles Cryptoquip and Cryptoquote, from King Features.\n\nIn a public challenge, writer J.M. Appel announced on September 28, 2014, that the table of contents page of his short story collection, \"Scouting for the Reaper\", also doubled as a cryptogram, and he pledged an award for the first to solve it.\n\nCryptograms based on substitution ciphers can often be solved by frequency analysis and by recognizing letter patterns in words, such as one letter words, which, in English, can only be \"i\" or \"a\" (and sometimes \"o\"). Double letters, apostrophes, and the fact that no letter can substitute for itself in the cipher also offer clues to the solution. Occasionally, cryptogram puzzle makers will start the solver off with a few letters.\n\nWhile the Cryptogram has remained popular, over time other puzzles similar to it have emerged. One of these is the Cryptoquote, which is a famous quote encrypted in the same way as a Cryptogram. A more recent version, with a biblical twist, is CodedWord. This puzzle makes the solution available only online where it provides a short exegesis on the biblical text. Yet a third is the Cryptoquiz. This puzzle starts off at the top with a category (unencrypted). For example, \"Flowers\" might be used. Below this is a list of encrypted words which are related to the stated category. The person must then solve for the entire list to finish the puzzle. Yet another type involves using numbers as they relate to texting to solve the puzzle.\n\nThe Zodiac Killer sent four cryptograms to police while he was still active. Despite much research and many investigations, only one of these has been translated, which was of no help in identifying the serial killer.\n\nFiction novels featuring cryptograms include \"The Davinci Code\" by Dan Brown and complex biblical cryptograms in \"Decoding the Phoenix\" by MD TChaves. In non-fiction, \"The Mammoth Book of Secret Codes and Cryptograms: Over 600 Mystery Codes to Be Cracked!\" by Elonka Dunin and many other crypto games reached mainstream prominence following the success of movies such as \"The Davinci Code\" and \"National Treasure: Book of Secrets\" starring Nicolas Cage. \n\n"}
{"id": "13068104", "url": "https://en.wikipedia.org/wiki?curid=13068104", "title": "DITA Open Toolkit", "text": "DITA Open Toolkit\n\nDITA Open Toolkit (DITA-OT) is an open-source publishing engine for XML content authored in the Darwin Information Typing Architecture (DITA).\n\nThe toolkit's extensible plug-in mechanism allows users to add their own transformations and customize the default output, which includes: \n\nOriginally developed by IBM and released to open source in 2005, the distribution packages contain Ant, Apache FOP, Java, Saxon, and Xerces.\n\nMost DITA authoring tools and DITA CMSs integrate the DITA-OT, or parts of it, into their publishing workflows.\n\nStandalone tools have also been developed to run the DITA-OT via a graphical user interface instead of the command line.\n\n"}
{"id": "38240436", "url": "https://en.wikipedia.org/wiki?curid=38240436", "title": "Deck railing", "text": "Deck railing\n\nDeck railing is a guard rail to prevent people falling from decks, stairs and balconies of buildings. Over time, many different styles of deck railing have been developed.\n\nThe most common residential deck railing design is built on-site using pressure treated lumber, with the vertical balusters regularly spaced to meet building code. Cable railings typically use stainless steel cables strung horizontally. Glass balusters and glass panels open the view while still providing safety. With the increasing popularity of composite lumber for decking, manufacturers are providing composite railing components. Other options include wrought iron and sheet steel, into which custom designs can be cut.\n\nBuilding code varies on the national, state, county and municipal level. Most areas in the world that use a variation of the International Building Code require a guard rail if there is a difference of 30\" or more between platforms. Other common requirements are that no space on the railing be greater than that through which a 4\" sphere could pass and that the railing assembly be able to withstand a load of 50 pounds per square foot.\n\nThe typical deck railing is generally built from pressure treated lumber. Posts on a deck are also typically pressure treated wood and standard sizes are 4x4, 6x6, and 8x8. These posts give structural support to the railing assembly and are the most critical part for the safety of the guard rail assembly. In between the posts, two 2x4s are attached to the posts with screws for the best connection. The lower board is placed 3.5\" from the top of the finish deck to the bottom of the board. The top board is placed with the top at 35\" from the deck. Then the vertical 2x2 pressure treated wood balusters are installed spaced regularly every 3.5\". Then a 2x6 is installed horizontally across the top of the posts and to 2x4. The 2x6 should be fastened with screws to the posts and 2x4 boards for the most rigidity.\n\nMountain laurel handrail, glass baluster systems, metal baluster systems, and composite railing systems all install in a similar manner. The differences is in the type of baluster installed. All four of these deck railings can be built using pressure treated lumber, another wood like cedar, or composite lumber to provide the structure.\n\nWrought iron and other metal railing systems that do not come in ready-to-install kits will usually require a skilled blacksmith as much welding will be required. These sections are typically built off-site in a workshop under controlled conditions, so that installation on the job site can be as speedy as possible.\n\n\n\n`"}
{"id": "16252355", "url": "https://en.wikipedia.org/wiki?curid=16252355", "title": "Delivery Multimedia Integration Framework", "text": "Delivery Multimedia Integration Framework\n\nDMIF, or Delivery Multimedia Integration Framework, is a uniform interface between the application and the transport, that allows the MPEG-4 application developer to stop worrying about that transport. DMIF was defined in MPEG-4 Part 6 (ISO/IEC 14496-6) in 1999. DMIF defines two interfaces: the DAI (DMIF/Application Interface) and the DNI (DMIF-Network Interface). A single application can run on different transport layers when supported by the right DMIF instantiation.\nMPEG-4 DMIF supports the following functionalities:\n\nDMIF expands upon the MPEG-2 DSM-CC standard (ISO/IEC 13818-6:1998) to enable the convergence of interactive, broadcast and conversational multimedia into one specification which will be applicable to set tops, desktops and mobile stations. The DSM-CC work was extended as part of the ISO/IEC 14496-6, with the \"DSM-CC Multimedia Integration Framework (DMIF)\". DSM-CC stands for \"Digital Storage Media - Command and Control\". DMIF was also a name of working group within Moving Picture Experts Group. The acronym \"DSM-CC\" was replaced by \"Delivery\" (Delivery Multimedia Integration Framework) in 1997.\n"}
{"id": "52507690", "url": "https://en.wikipedia.org/wiki?curid=52507690", "title": "Documentation testing", "text": "Documentation testing\n\nDocumentation testing is part of non-functional testing of a product. It may be a type of black box testing that ensures that documentation about how to use the system matches with what the system does, providing proof that system changes and improvement have been documented.\n\nDocumentation testing includes the plans, results, and testing of a system or system component. It includes test case specifications, test plans, test procedures, test reports, and test logs. It is about the testing of all the documents stating, defining, explaining and reporting or validating requirements, procedures followed and results. Documentation testing starts with the beginning of the very first software process to be most cost effective. Documentation testing includes checking the spelling and grammar to review any ambiguity or inconsistency between what functionality it performs and what it is supposed to do.\n\nProduct documentation is a critical part of the final product. Poor documentation can affet the product or company reputation.\n\nDocumentation is about the testing of all the documents created prior and after the testing of software. Any delay in the testing of the document will increase the cost. Some common artifacts about software development and testing can be specified as test cases, test plans, requirements, and traceability matrices.\n\nFour key areas for testing a document include instructions, examples, messages, and samples. Instructions will be needed to step by step execute the test scenarios for looking errors or their omission. Further examples can be provided to elaborate the GUI components, syntax, commands and interfaces to show executed outputs or pin points. Inconsistencies also needed to be taken care of with errors as they can confuse the users and these ambiguities will cause much damage if the user of the system will be a novice user. Examples will be needed in case of any problem that occurs to the user, particularly novice users who Novice Users will check the documentation for any confusion.\n\nDocumentation problems can be handled in formal ways just the same way as the coding problems. Defect reporting tools and tracking tools are the common solutions for handling defects just like as they are handled in code.\n\n"}
{"id": "13330867", "url": "https://en.wikipedia.org/wiki?curid=13330867", "title": "Electrical technologist", "text": "Electrical technologist\n\nElectrical technologists are people who apply electrical theory on the job. Their knowledge and skill lies between that of electrical engineers and general electrical trades persons. In North America they train in a three-year diploma programs at colleges or universities. Specializations within the field include instrumentation, power, telecommunications, programming and electronic controls. Electrical technologists are employed by utilities, engineering drafting/design companies, industry, and construction companies.\n"}
{"id": "28837853", "url": "https://en.wikipedia.org/wiki?curid=28837853", "title": "Ellenroad Ring Mill Engine", "text": "Ellenroad Ring Mill Engine\n\nThe Ellenroad Ring Mill Engine is a preserved stationary steam engine in Milnrow, Greater Manchester. It powered the Ellenroad Ring Mill from 1917, and after the mill's closure the engine is still worked under steam as a museum display.\n\nAt 3000 hp, the twin tandem compound steam engine is possibly the most powerful of the type in preservation. The two engines are named \"Victoria\" and \"Alexandra\", multiple ropes around the flywheel drove the line shafts on each floor of the mill which in turn drove the ring spinning frames.\n\nIn addition to the mill engine, the museum also houses in operational condition the original Ellenroad mill pilot generator engine and sprinkler pump, the Whitelees Beam Engine, and the Marsden Engine. The museum trust also owns the surviving components of the Fern Mill Engine, which it hopes eventually to restore to working condition.\n\n\n"}
{"id": "46630", "url": "https://en.wikipedia.org/wiki?curid=46630", "title": "Embedded system", "text": "Embedded system\n\nAn embedded system is a programmed controlling and operating system with a dedicated function within a larger mechanical or electrical system, often with real-time computing constraints. It is \"embedded\" as part of a complete device often including hardware and mechanical parts. Embedded systems control many devices in common use today. Ninety-eight percent of all microprocessors are manufactured components of embedded systems.\n\nExamples of properties of typical embedded computers when compared with general-purpose counterparts are low power consumption, small size, rugged operating ranges, and low per-unit cost. This comes at the price of limited processing resources, which make them significantly more difficult to program and to interact with. However, by building intelligence mechanisms on top of the hardware, taking advantage of possible existing sensors and the existence of a network of embedded units, one can both optimally manage available resources at the unit and network levels as well as provide augmented functions, well beyond those available. For example, intelligent techniques can be designed to manage power consumption of embedded systems.\n\nModern embedded systems are often based on microcontrollers (i.e. CPUs with integrated memory or peripheral interfaces), but ordinary microprocessors (using external chips for memory and peripheral interface circuits) are also common, especially in more complex systems. In either case, the processor(s) used may be types ranging from general purpose to those specialized in certain class of computations, or even custom designed for the application at hand. A common standard class of dedicated processors is the digital signal processor (DSP).\n\nSince the embedded system is dedicated to specific tasks, design engineers can optimize it to reduce the size and cost of the product and increase the reliability and performance. Some embedded systems are mass-produced, benefiting from economies of scale.\n\nEmbedded systems range from portable devices such as digital watches and MP3 players, to large stationary installations like traffic lights, factory controllers, and largely complex systems like hybrid vehicles, MRI, and avionics. Complexity varies from low, with a single microcontroller chip, to very high with multiple units, peripherals and networks mounted inside a large chassis or enclosure.\n\nOne of the very first recognizably modern embedded systems was the Apollo Guidance Computer, developed ca. 1965 by Charles Stark Draper at the MIT Instrumentation Laboratory. At the project's inception, the Apollo guidance computer was considered the riskiest item in the Apollo project as it employed the then newly developed monolithic integrated circuits to reduce the size and weight. An early mass-produced embedded system was the Autonetics D-17 guidance computer for the Minuteman missile, released in 1961. When the Minuteman II went into production in 1966, the D-17 was replaced with a new computer that was the first high-volume use of integrated circuits.\n\nSince these early applications in the 1960s, embedded systems have come down in price and there has been a dramatic rise in processing power and functionality. An early microprocessor for example , the Intel 4004 (released in 1971), was designed for calculators and other small systems but still required external memory and support chips. In 1978 National Engineering Manufacturers Association released a \"standard\" for programmable microcontrollers, including almost any computer-based controllers, such as single board computers, numerical, and event-based controllers.\n\nAs the cost of microprocessors and microcontrollers fell it became feasible to replace expensive knob-based analog components such as potentiometers and variable capacitors with up/down buttons or knobs read out by a microprocessor even in consumer products. By the early 1980s, memory, input and output system components had been integrated into the same chip as the processor forming a microcontroller. Microcontrollers find applications where a general-purpose computer would be too costly.\n\nA comparatively low-cost microcontroller may be programmed to fulfill the same role as a large number of separate components. Although in this context an embedded system is usually more complex than a traditional solution, most of the complexity is contained within the microcontroller itself. Very few additional components may be needed and most of the design effort is in the software. Software prototype and test can be quicker compared with the design and construction of a new circuit not using an embedded processor.\n\nEmbedded systems are commonly found in consumer, cooking, industrial, automotive, medical, commercial and military applications.\n\nTelecommunications systems employ numerous embedded systems from telephone switches for the network to cell phones at the end user.\nComputer networking uses dedicated routers and network bridges to route data.\n\nConsumer electronics include MP3 players, mobile phones, video game consoles, digital cameras, GPS receivers, and printers.\nHousehold appliances, such as microwave ovens, washing machines and dishwashers, include embedded systems to provide flexibility, efficiency and features. Advanced HVAC systems use networked thermostats to more accurately and efficiently control temperature that can change by time of day and season. Home automation uses wired- and wireless-networking that can be used to control lights, climate, security, audio/visual, surveillance, etc., all of which use embedded devices for sensing and controlling.\n\nTransportation systems from flight to automobiles increasingly use embedded systems.\nNew airplanes contain advanced avionics such as inertial guidance systems and GPS receivers that also have considerable safety requirements.\nVarious electric motors — brushless DC motors, induction motors and DC motors — use electric/electronic motor controllers.\nAutomobiles, electric vehicles, and hybrid vehicles increasingly use embedded systems to maximize efficiency and reduce pollution.\nOther automotive safety systems include anti-lock braking system (ABS), Electronic Stability Control (ESC/ESP), traction control (TCS) and automatic four-wheel drive.\n\nMedical equipment uses embedded systems for vital signs monitoring, electronic stethoscopes for amplifying sounds, and various medical imaging (PET, SPECT, CT, and MRI) for non-invasive internal inspections. Embedded systems within medical equipment are often powered by industrial computers.\n\nEmbedded systems are used in transportation, fire safety, safety and security, medical applications and life critical systems, as these systems can be isolated from hacking and thus, be more reliable, unless connected to wired or wireless networks via on-chip 3G cellular or other methods for IoT monitoring and control purposes. For fire safety, the systems can be designed to have greater ability to handle higher temperatures and continue to operate. In dealing with security, the embedded systems can be self-sufficient and be able to deal with cut electrical and communication systems.\n\nA new class of miniature wireless devices called motes are networked wireless sensors. Wireless sensor networking, WSN, makes use of miniaturization made possible by advanced IC design to couple full wireless subsystems to sophisticated sensors, enabling people and companies to measure a myriad of things in the physical world and act on this information through IT monitoring and control systems. These motes are completely self-contained, and will typically run off a battery source for years before the batteries need to be changed or charges.\n\nEmbedded Wi-Fi modules provide a simple means of wirelessly enabling any device that communicates via a serial port.\n\nEmbedded systems are designed to do some specific task, rather than be a general-purpose computer for multiple tasks. Some also have real-time performance constraints that must be met, for reasons such as safety and usability; others may have low or no performance requirements, allowing the system hardware to be simplified to reduce costs.\n\nEmbedded systems are not always standalone devices. Many embedded systems consist of small parts within a larger device that serves a more general purpose. For example, the Gibson Robot Guitar features an embedded system for tuning the strings, but the overall purpose of the Robot Guitar is, of course, to play music. Similarly, an embedded system in an automobile provides a specific function as a subsystem of the car itself.\nThe program instructions written for embedded systems are referred to as firmware, and are stored in read-only memory or flash memory chips. They run with limited computer hardware resources: little memory, small or non-existent keyboard or screen.\n\nEmbedded systems range from no user interface at all, in systems dedicated only to one task, to complex graphical user interfaces that resemble modern computer desktop operating systems.\nSimple embedded devices use buttons, LEDs, graphic or character LCDs (HD44780 LCD for example) with a simple menu system.\n\nMore sophisticated devices that use a graphical screen with touch sensing or screen-edge buttons provide flexibility while minimizing space used: the meaning of the buttons can change with the screen, and selection involves the natural behavior of pointing at what is desired. Handheld systems often have a screen with a \"joystick button\" for a pointing device.\n\nSome systems provide user interface remotely with the help of a serial (e.g. RS-232, USB, I²C, etc.) or network (e.g. Ethernet) connection. This approach gives several advantages: extends the capabilities of embedded system, avoids the cost of a display, simplifies BSP and allows one to build a rich user interface on the PC. A good example of this is the combination of an embedded web server running on an embedded device (such as an IP camera) or a network router. The user interface is displayed in a web browser on a PC connected to the device, therefore needing no software to be installed.\n\nEmbedded processors can be broken into two broad categories. Ordinary microprocessors (μP) use separate integrated circuits for memory and peripherals. Microcontrollers (μC) have on-chip peripherals, thus reducing power consumption, size and cost. In contrast to the personal computer market, many different basic CPU architectures are used since software is custom-developed for an application and is not a commodity product installed by the end user. Both Von Neumann as well as various degrees of Harvard architectures are used. RISC as well as non-RISC processors are found. Word lengths vary from 4-bit to 64-bits and beyond, although the most typical remain 8/16-bit. Most architectures come in a large number of different variants and shapes, many of which are also manufactured by several different companies.\n\nNumerous microcontrollers have been developed for embedded systems use. General-purpose microprocessors are also used in embedded systems, but generally, require more support circuitry than microcontrollers.\n\nPC/104 and PC/104+ are examples of standards for \"ready-made\" computer boards intended for small, low-volume embedded and ruggedized systems, mostly x86-based. These are often physically small compared to a standard PC, although still quite large compared to most simple (8/16-bit) embedded systems. They often use DOS, Linux, NetBSD, or an embedded real-time operating system such as MicroC/OS-II, QNX or VxWorks. Sometimes these boards use non-x86 processors.\n\nIn certain applications, where small size or power efficiency are not primary concerns, the components used may be compatible with those used in general purpose x86 personal computers. Boards such as the VIA EPIA range help to bridge the gap by being PC-compatible but highly integrated, physically smaller or have other attributes making them attractive to embedded engineers. The advantage of this approach is that low-cost commodity components may be used along with the same software development tools used for general software development. Systems built in this way are still regarded as embedded since they are integrated into larger devices and fulfill a single role. Examples of devices that may adopt this approach are ATMs and arcade machines, which contain code specific to the application.\n\nHowever, most ready-made embedded systems boards are not PC-centered and do not use the ISA or PCI buses. When a system-on-a-chip processor is involved, there may be little benefit to having a standardized bus connecting discrete components, and the environment for both hardware and software tools may be very different.\n\nOne common design style uses a small system module, perhaps the size of a business card, holding high density BGA chips such as an ARM-based system-on-a-chip processor and peripherals, external flash memory for storage, and DRAM for runtime memory. The module vendor will usually provide boot software and make sure there is a selection of operating systems, usually including Linux and some real time choices. These modules can be manufactured in high volume, by organizations familiar with their specialized testing issues, and combined with much lower volume custom mainboards with application-specific external peripherals.\n\nImplementation of embedded systems has advanced so that they can easily be implemented with already-made boards that are based on worldwide accepted platforms. These platforms include, but are not limited to, Arduino and Raspberry Pi.\n\nA common array for very-high-volume embedded systems is the system on a chip (SoC) that contains a complete system consisting of multiple processors, multipliers, caches and interfaces on a single chip. SoCs can be implemented as an application-specific integrated circuit (ASIC) or using a field-programmable gate array (FPGA).\n\nEmbedded systems talk with the outside world via peripherals, such as:\n\nAs with other software, embedded system designers use compilers, assemblers, and debuggers to develop embedded system software. However, they may also use some more specific tools:\n\nSoftware tools can come from several sources:\n\nAs the complexity of embedded systems grows, higher level tools and operating systems are migrating into machinery where it makes sense. For example, cellphones, personal digital assistants and other consumer computers often need significant software that is purchased or provided by a person other than the manufacturer of the electronics. In these systems, an open programming environment such as Linux, NetBSD, OSGi or Embedded Java is required so that the third-party software provider can sell to a large market.\n\nEmbedded systems are commonly found in consumer, cooking, industrial, automotive, medical applications.\nSome examples of embedded systems are MP3 players, mobile phones, video game consoles, digital cameras, DVD players, and GPS. Household appliances, such as microwave ovens, washing machines and dishwashers, include embedded systems to provide flexibility and efficiency.\n\nEmbedded debugging may be performed at different levels, depending on the facilities available. The different metrics that characterize the different forms of embedded debugging are: does it slow down the main application, how close is the debugged system or application to the actual system or application, how expressive are the triggers that can be set for debugging (e.g., inspecting the memory when a particular program counter value is reached), and what can be inspected in the debugging process (such as, only memory, or memory and registers, etc.).\n\nFrom simplest to most sophisticated they can be roughly grouped into the following areas:\n\nUnless restricted to external debugging, the programmer can typically load and run software through the tools, view the code running in the processor, and start or stop its operation. The view of the code may be as HLL source-code, assembly code or mixture of both.\n\nBecause an embedded system is often composed of a wide variety of elements, the debugging strategy may vary. For instance, debugging a software- (and microprocessor-) centric embedded system is different from debugging an embedded system where most of the processing is performed by peripherals (DSP, FPGA, and co-processor).\nAn increasing number of embedded systems today use more than one single processor core. A common problem with multi-core development is the proper synchronization of software execution. In this case, the embedded system design may wish to check the data traffic on the busses between the processor cores, which requires very low-level debugging, at signal/bus level, with a logic analyzer, for instance.\n\nReal-time operating systems (RTOS) often supports tracing of operating system events. A graphical view is presented by a host PC tool, based on a recording of the system behavior. The trace recording can be performed in software, by the RTOS, or by special tracing hardware. RTOS tracing allows developers to understand timing and performance issues of the software system and gives a good understanding of the high-level system behaviors. Commercial tools like RTXC Quadros or IAR Systems exists.\n\nEmbedded systems often reside in machines that are expected to run continuously for years without errors, and in some cases recover by themselves if an error occurs. Therefore, the software is usually developed and tested more carefully than that for personal computers, and unreliable mechanical moving parts such as disk drives, switches or buttons are avoided.\n\nSpecific reliability issues may include:\n\nA variety of techniques are used, sometimes in combination, to recover from errors—both software bugs such as memory leaks, and also soft errors in the hardware:\n\nFor high volume systems such as portable music players or mobile phones, minimizing cost is usually the primary design consideration. Engineers typically select hardware that is just “good enough” to implement the necessary functions.\n\nFor low-volume or prototype embedded systems, general purpose computers may be adapted by limiting the programs or by replacing the operating system with a real-time operating system.\n\nThere are several different types of software architecture in common use.\n\nIn this design, the software simply has a loop. The loop calls subroutines, each of which manages a part of the hardware or software. Hence it is called a simple control loop or control loop.\n\nSome embedded systems are predominantly controlled by interrupts. This means that tasks performed by the system are triggered by different kinds of events; an interrupt could be generated, for example, by a timer in a predefined frequency, or by a serial port controller receiving a byte.\n\nThese kinds of systems are used if event handlers need low latency, and the event handlers are short and simple. Usually, these kinds of systems run a simple task in a main loop also, but this task is not very sensitive to unexpected delays.\n\nSometimes the interrupt handler will add longer tasks to a queue structure. Later, after the interrupt handler has finished, these tasks are executed by the main loop. This method brings the system close to a multitasking kernel with discrete processes.\n\nA nonpreemptive multitasking system is very similar to the simple control loop scheme, except that the loop is hidden in an API. The programmer defines a series of tasks, and each task gets its own environment to “run” in. When a task is idle, it calls an idle routine, usually called “pause”, “wait”, “yield”, “nop” (stands for \"no operation\"), etc.\n\nThe advantages and disadvantages are similar to that of the control loop, except that adding new software is easier, by simply writing a new task, or adding to the queue.\n\nIn this type of system, a low-level piece of code switches between tasks or threads based on a timer (connected to an interrupt). This is the level at which the system is generally considered to have an \"operating system\" kernel. Depending on how much functionality is required, it introduces more or less of the complexities of managing multiple tasks running conceptually in parallel.\n\nAs any code can potentially damage the data of another task (except in larger systems using an MMU) programs must be carefully designed and tested, and access to shared data must be controlled by some synchronization strategy, such as message queues, semaphores or a non-blocking synchronization scheme.\n\nBecause of these complexities, it is common for organizations to use a real-time operating system (RTOS), allowing the application programmers to concentrate on device functionality rather than operating system services, at least for large systems; smaller systems often cannot afford the overhead associated with a \"generic\" real-time system, due to limitations regarding memory size, performance, or battery life. The choice that an RTOS is required brings in its own issues, however, as the selection must be done prior to starting to the application development process. This timing forces developers to choose the embedded operating system for their device based upon current requirements and so restricts future options to a large extent. The restriction of future options becomes more of an issue as product life decreases. Additionally the level of complexity is continuously growing as devices are required to manage variables such as serial, USB, TCP/IP, Bluetooth, Wireless LAN, trunk radio, multiple channels, data and voice, enhanced graphics, multiple states, multiple threads, numerous wait states and so on. These trends are leading to the uptake of embedded middleware in addition to a real-time operating system.\n\nA microkernel is a logical step up from a real-time OS. The usual arrangement is that the operating system kernel allocates memory and switches the CPU to different threads of execution. User mode processes implement major functions such as file systems, network interfaces, etc.\n\nIn general, microkernels succeed when the task switching and intertask communication is fast and fail when they are slow.\n\nExokernels communicate efficiently by normal subroutine calls. The hardware and all the software in the system are available to and extensible by application programmers.\n\nIn this case, a relatively large kernel with sophisticated capabilities is adapted to suit an embedded environment. This gives programmers an environment similar to a desktop operating system like Linux or Microsoft Windows, and is therefore very productive for development; on the downside, it requires considerably more hardware resources, is often more expensive, and, because of the complexity of these kernels, can be less predictable and reliable.\n\nCommon examples of embedded monolithic kernels are embedded Linux and Windows CE.\n\nDespite the increased cost in hardware, this type of embedded system is increasing in popularity, especially on the more powerful embedded devices such as wireless routers and GPS navigation systems. Here are some of the reasons:\n\nIn addition to the core operating system, many embedded systems have additional upper-layer software components. These components consist of networking protocol stacks like CAN, TCP/IP, FTP, HTTP, and HTTPS, and also included storage capabilities like FAT and flash memory management systems. If the embedded device has audio and video capabilities, then the appropriate drivers and codecs will be present in the system. In the case of the monolithic kernels, many of these software layers are included. In the RTOS category, the availability of the additional software components depends upon the commercial offering.\n\n\n"}
{"id": "43667043", "url": "https://en.wikipedia.org/wiki?curid=43667043", "title": "Flowgorithm", "text": "Flowgorithm\n\nFlowgorithm is a graphical authoring tool which allows users to write and execute programs using flowcharts. The approach is designed to emphasize the algorithm rather than the syntax of a specific programming language. The flowchart can be converted to several major programming languages. Flowgorithm was created at Sacramento State University.\n\nThe name is a portmanteau of \"flowchart\" and \"algorithm\".\n\nFlowgorithm can interactively translate flowchart programs into source code written in other programming languages. As the user steps through their flowchart, the related code in the translated program is automatically highlighted. The following programming languages are supported:\nBesides English, Flowgorithm supports other spoken languages. These are:\nFlowgorithm combines the classic flowchart symbols and those used by SDL diagrams. The color of each shape is shared by the associated generated code and the console window. The colors can be changed to several built-in themes. \n\nThe image below has the solution for 99 Bottles of Beer. A function is used to return a string that either contains the singular \"bottle\" or plural \"bottles\" depending on the value of the parameter.\n\nOther educational programming languages include:\n\n"}
{"id": "29717622", "url": "https://en.wikipedia.org/wiki?curid=29717622", "title": "Foucault knife-edge test", "text": "Foucault knife-edge test\n\nThe Foucault knife-edge test was described in 1858 by French physicist Léon Foucault to measure conic shapes of optical mirrors, with error margins measurable in fractions of wavelengths of light (or Angstroms, millionths of an inch, or nanometers). It is commonly used by amateur telescope makers for figuring small astronomical mirrors. Its relatively simple, inexpensive apparatus can produce measurements more cost-effectively than most other testing techniques.\n\nIt measures mirror surface dimensions by reflecting light into a knife edge at or near the mirror's centre of curvature. In doing so, it only needs a tester which in its most basic 19th century form consists of a light bulb, a piece of tinfoil with a pinhole in it, and a razor blade to create the knife edge. The testing device is adjustable along the X-axis (knife cut direction) across the Y-axis (optical axis), and must have measurable adjustment to 0.001 inch (25 µm) or better along lines parallel to the optical axis. According to Texereau it amplifies mirror surface defects by a factor of one million, making them easily accessible to study and remediation.\n\nThe mirror to be tested is placed vertically in a stand. The Foucault tester is set up at the distance of the mirror's radius of curvature (radius R is twice the focal length.) with the pinhole to one side of the centre of curvature (a short vertical slit parallel to the knife edge can be used instead of the pinhole). The tester is adjusted so that the returning beam from the pinhole light source is interrupted by the knife edge.\n\nViewing the mirror from behind the knife edge shows a pattern on the mirror surface. If the mirror surface is part of a perfect sphere, the mirror appears evenly lighted across the entire surface. If the mirror is spherical but with defects such as bumps or depressions, the defects appear greatly magnified in height. If the surface is paraboloidal, the mirror usually looks like a doughnut or lozenge although the exact appearance depends on the exact position of the knife edge.\n\nIt is possible to calculate how closely the mirror surface resembles a perfect parabola by placing a Couder mask, Everest pin stick (after A. W. Everest) or other zone marker over the mirror. A series of measurements with the tester, finding the radii of curvature of the zones along the optical axis of the mirror (Y-axis). These data are then reduced and graphed against an ideal parabolic curve.\n\nA number of other tests are used which measure the mirror at the center of curvature. Some telescope makers use a variant of the Foucault test called a Ronchi test that replaces the knife edge with a grating (similar to a very coarse diffraction grating) comprising fine parallel wires, an etching on a glass plate, a photograph negative or computer printed transparency. Ronchi test patterns are matched to those of standard mirrors or generated by computer.\n\nOther variants of the Foucault test include the Gaviola or Caustic test which can measure mirrors of fast f/ratio more accurately than the Foucault test which is limited to about (λ/8) wavelength accuracy on small and medium-sized mirrors. The Caustic test is capable of measuring larger mirrors and achieving a (λ/20) wave peak to valley accuracy by using a testing stage which is adjusted from side to side so as to measure each zone of each side of the mirror from the center of its curvature.\n\nThe \"Dall null test\" uses a plano-convex lens placed a short distance in front of the pinhole. With the correct positioning of the lens, a parabolic mirror appears flat under testing instead of doughnut-shaped so testing is much easier and zonal measurements are not needed.\n\nThere are a number of interferometric tests which have been used including the Michelson-Twyman and the Michelson method, both published in 1918, the Lenouvel method and the Fizeau method. Interferometric testing has been made more affordable in recent years by affordable lasers, digital cameras (such as webcams), and computers, but remains primarily an industrial methodology.\n\n\n\n"}
{"id": "9465204", "url": "https://en.wikipedia.org/wiki?curid=9465204", "title": "Guard tour patrol system", "text": "Guard tour patrol system\n\nA guard tour patrol system is a system for logging the rounds of employees in a variety of situations such as security guards patrolling property, technicians monitoring climate-controlled environments, and correctional officers checking prisoner living areas. It helps ensure that the employee makes his or her appointed rounds at the correct intervals and can offer a record for legal or insurance reasons. Such systems have existed for many years using mechanical watchclock-based systems (watchman clocks/guard tour clocks/patrol clocks). Computerized systems were first introduced in Europe in the early 1980s, and in North America in 1986. Modern systems are based on handheld data loggers and RFID sensors.\nThe system provides a means to record the time when the employee reaches certain points on their tour. Checkpoints or watchstations are commonly placed at the extreme ends of the tour route and at critical points such as vaults, specimen refrigerators, vital equipment, and access points. Some systems are set so that the interval between stations is timed so if the employee fails to reach each point within a set time, other staff are dispatched to ensure the employee's well-being.\nAn example of a modern set-up might work as follows: the employee carries a portable electronic sensor (PES) or electronic data collector which is activated at each checkpoint. Checkpoints can consist of iButton semiconductors, magnetic strips, proximity microchips such as RFIDs or NFC- or optical barcodes. The data collector stores the serial number of the checkpoint with the date and time. Later, the information is downloaded from the collector into a computer where the checkpoint's serial number will have an assigned location (i.e. North Perimeter Fence, Cell Number 1, etc.). Data collectors can also be programmed to ignore duplicate checkpoint activations that occur sequentially or within a certain time period. Computer software used to compile the data from the collector can print out summaries that pinpoint missed checkpoints or patrols without the operator having to review all the data collected. Because devices can be subject to misuse, some have built-in microwave, g-force, and voltage detection.\n\nIt combines readers, tags and software.\n\nThe first Guard tour system were the touch readers with software. Upon further development, more working modes for the readers became available. Such as RFID and GPS. And the communication of readers and software was connected with USB cables or download stations. For USB connection, the \"Pogo Pin\" connection is very popular. Because the contacts with gold-plating are very stable and waterproof.\n\nNewer, light-weight guard touring systems utilize QR codes or barcodes rather than expensive electronic components. A mobile phone app is used to scan (take a photo) of the QR code which creates a time stamp in the system.\n\nThe reader needs to read the tags to record the information, such as the time and tag's ID. Then upload the information to software to get the report.\n\nThere are three types of guard patrol software. They are desktop, local network client-server, and web-based versions. \nThe desktop version can only work on one computer.\nThe local network client server type can work using the local area network.\nThe web-based version can work everywhere with internet access.\nIn the analog age, the device used for this purpose was the watchclock. Watchclocks often had a paper or light cardboard disk placed inside for each 24-hour period. The user would carry the clock to each checkpoint, where a numbered key could be found (typically chained in place). The key would be inserted into the clock where it would imprint the disk. At the end of the shift or 24-hour period an authorized person (usually a supervisor) would unlock the watchclock and retrieve the disk.\nAs development of guard tour system, the device can work with more functions. Such as send data real-time by GPRS to software and GPS location and tracking mode.\nIn software, we set up the Patrol Department, Patrol Route, Guard, Checkpoint, Event and Patrol Plan in general, depending on the software purchased. The software will then have specific tours set for officers to complete, being able to indicate whether the inspection was completed properly or not, with the ability to note a specific temperature of an inspection, or make any kind of notes necessary. Guard Tour software systems seem to be becoming the norm in tracking tours for officers. Examples of this software would be the Guard Tour System by Silvertrac Software, SequriX, Trackforce, QR-Patrol, Kugadi, GigaTrak TrackTik, PatrolLIVE, Orna, Guarnic. GuardMetrics and Youtility\n\nNew touring solutions rely on cloud-based Software as a Service (SaaS) combined with mobile or fixed on-site devices. These offer the advantages of lower installation and maintenance costs, forgoing the need for hardware, software upgrades, data backups and computer maintenance. On-site systems need all the usual software patches, backups and periodic hardware replacement. In operation, the role of the watchclock system, described above, has largely been replaced by some combination of GPS, RFID/NFC, or QR coded labels. Users prove that they have visited particular locations or performed tasks by scanning these tags or via GPS generated maps. These technologies result in lower costs, while increasing the flexibility of the systems to handle changes or new uses. This is important when routes change, or if a solution is needed on short notice. Tag-based touring systems typically utilize a mobile phone or tablet app to scan the tags and then upload that information along with a time stamp, phone's location information, and optionally other information the guard enters into the app on the phone. These systems provide instant access to tour information as it is uploaded by the application or device carried by the user, rather than requiring the officer to return to an upload station.\n\nAlthough this technology was initially developed for the security market, there are other uses. Some include:\n\n\nFor routes which have significant outdoor exposure GPS units have proven to be an effective means of tracking security and law enforcement patrol behavior. GPS systems do not function in the most vulnerable areas such as indoors or underground. Accordingly, systems using assisted GPS have been developed.\n\n"}
{"id": "40799663", "url": "https://en.wikipedia.org/wiki?curid=40799663", "title": "High-frequency vibrating screens", "text": "High-frequency vibrating screens\n\nHigh frequency vibrating screens are the most important screening machines primarily utilised in the mineral processing industry. They are used to separate feeds containing solid and crushed ores down to approximately 200μm in size, and are applicable to both perfectly wetted and dried feed. The frequency of the screen is mainly controlled by an electromagnetic vibrator which is mounted above and directly connected to the screening surface. Its high frequency characteristics differentiates it from a normal vibrating screen. The high frequency vibrating screens usually operates at an inclined angle, traditionally varying between 0 and 25 degrees and can go up to a maximum of 45 degrees. Besides, it should operate at a low stroke and has a frequency ranging from 1500 - 7200 RPM.\n\nPre-treatment of the feed is often required before the use of the high frequency screen as the apertures in the screen may become blocked easily.\n\nHigh frequency screens have become more standardized and widely adopted in materials classification processes. It allows efficient cuts and fine separations, which can provide high purity and precise sizing control of the product (for sizes of fine particles up to 0.074-1.5mm). Common industrial applications include dewatering of materials, processing of powder in coal, ores and minerals, wood pelleting, fractionated reclaimed asphalt pavement, food, pharmaceutical and chemical industry. Fineness of the products and system capacities varies over a huge range between different models, to satisfy individual application requirements.\n\nMost commonly, high frequency screens are used to separate \"reclaimed\" asphalt pavement (RAP) into multiple sizes and fractions, which allow producers to take full advantage of the recycled materials. RAP is a recycle material that is reused in new pavement construction; any recycled products are worth as much as what they replace. As compared to conventional screening methods which are limited to producing unacceptable sizes in the products, high frequency screens can produce more efficient sizing to obtain a finer product. Another advantage of using high frequency screens to recycle the reclaimed materials is the available aggregate and oil that can be reused, and reducing the amount of new material required. Therefore, the capital cost for the process is lowered while maintaining a high quality of the asphalt mixture. Moreover, high frequency screen applies intensive vibration directly onto the screen media, such high RPM allows asphalt pavement material to achieve a higher stratification and separate at a faster rate.\n\nIn mineral processing such as ferrous metals ore (e.g. iron, tin, tungsten, tantalum etc.) and nonferrous metals ores (e.g. lead, zinc, gold, silver and industrial sand etc.), high frequency screens have a crucial role. After the ores get comminuted, high frequency screens such as Sepro-Sizetec Screens are used as a classifier which selects materials size that is small enough to enter the next stage for recovery. For example, the closed grinding circuit (e.g. recirculating network with ball mill). Firstly, it screens out the coarse particles and recirculates them back to the grinding mill machine. Then, the fine grain material will be unloaded timely, avoiding over crushing caused by re-grinding. The benefits of using high frequency screens in mineral processing can meet the requirement of fineness easily for recovery and is able to achieve a smaller size separation, reducing capacity needed for comminution stage and overall energy consumption. Hence, improving the grade of the final product and providing a better recovery and screening efficiency.\n\nThe high frequency vibrating screens achieves a high efficiency of separation and differs from its counterparts since it breaks down the surface tension between particles. Also the high level of RPMs contributes to increasing the stratification of material so they separate at a much higher rate. Separation cannot take place without stratification. Furthermore, since the screen vibrates vertically, there is a ‘popcorn effect’ whereby the coarser particles are lifted higher and finer particles stay closer to the screen, thus increases the probability of separation. In some high frequency vibrating screens the flow rate of the feed can be controlled, this is proportional to the ‘popcorn effect’; if the flow rate lowers, the effect is also decreased. Limitations of the high frequency vibrating screen are that the fine screens are very fragile and are susceptible to becoming blocked very easily. Over time the separation efficiency will drop and the screen will need to be replaced.\n\nAn alternative to the high frequency vibrating screens is the rotary sifter. A rotary sifter uses a screen which rotates in a circular motion and the finer particles are sifted through the apertures. It is also generally used for finger separations; between 12mm to 45μm particle size. The rotary sifter will usually be chosen based on the nature of the substance being separated; whey, yeast bread mix, cheese powder, fertilizers. The rotary sifter is often preferred in the non-metallurgical industry and operates in a way to achieve a dust and noise free environment. The limitation for the rotary sifter is that it cannot handle a high capacity compared to the high frequency vibrating screen. Both equipment, however, achieve a high screening efficiency.\n\nConventional and general design for a high frequency vibrating screen consists of mainframe, screen web, eccentric bock, electric motor, rub spring and coupler. The two most common types of vibrators which induce the high frequency vibrations are hydraulic or electric vibrators, these electric vibrators are either electric motors or solenoids. Common designs for screening decks are either single or double deck. In addition, another feature of high frequency vibrating screens are the static side plates which provide benefits such as smaller support structure, less noise, longer life, and hence less maintenance. In industry, the screens are operated at a tiled angle up till 40 º. The high frequency (1500 – 7200 rpm) and low amplitude (1.2 – 2.0 mm) characteristics leads to the vertical-elliptical movement that rapidly transports oversized particles down the screen. Creating a thin bed of particles, this improves the efficiency and capacity of the screen.\n\nStationary screens are typically used in plants and not moved around. In the mineral processing industry, equipment often has to be moved to different sites depending on the jobs taken up by a company. Mobile screens thus are another viable design for companies who have to move their equipment often. These include wheel-mounted and track-mounted plants which allow for easy transportation and movement of the screens. Typical mobile screen designs are shown in the diagrams on left.\n\nThe screening performance is affected significantly by various factors such as equipment capacity and angle of inclination, in which the performance can be measured by screening efficiency and flux of the product.\n\nFlux is defined as the amount of a desired component (undersize material) that has carried over the screening media from the feed per time per unit area. Screening efficiency is expressed as the ratio of the amount of material that actually passes through the aperture, divided by the amount in the feed that theoretically should pass. Commercially perfect screening is considered to be 95% efficient if the process is operated with appropriate feed concentration and size particles. Generally, a suitable particle size difference between sieving and feed should be no more than 30%. High screening efficiency can reduce the qualified gain content in cyclic loading and screening and thus increasing the processing capacity of the mill.\n\nThe equipment capacity is almost directly proportional to screen width. This means that by increasing the length, there will be additional chances for passage, and will usually lead to increase in transmission and efficiency. In general, the standard size of screen length should be two to three times the width. However, certain special situations such as restricted space may require a different design.\n\nAngle of inclination can be designed based on the desired mineral grain. For example, wet sieving angle is generally around 25 ± 2 ° for concentrator. Increasing the slope of a screen will effectively reduce the aperture by the cosine of the angle of inclination. At the same time, the materials also move across the screen faster which leads to more rapid stratification. However, the performance tends to decrease after a certain point since the slope of the deck is too high and most particles will remain on the oversized stream instead of passing through the aperture, thus, lower flux is yielded.\n\nTable below presents relationship between inclined angle with desired product flux and efficiency.\n\nThe purpose of the vibrating screen is that particles are introduced to the gaps in the screens repeatedly. The frequency of the screen must be high enough so that it prevents the particles from blocking the apertures and the maximum height of the particle trajectory should occur when the screen surface is at its lowest point. Based on the principle, there is an optimum frequency and amplitude of vibration \nTransmission refers to the fraction of desired particle that passes through the apertures in the screen. At low frequency, screening efficiency is high but blinding is severe. Blinding will decrease as frequency increases but the particles will have difficulty going through the apertures. When designing a high frequency vibrating screen, an optimum point of frequency and amplitude must be chosen, depending on the specific applications.\n\nThe separation efficiency is simply a measure of the amount of material removed by the screen compared to the theoretical amount that should have been removed. Screen efficiency can be obtained using different equation, which depends on whether the desired product is the oversize or undersize fraction from the screen.\n\nThe screen efficiency based on the oversize (E) is given by: \n\nThe screen efficiency based on the undersize (E) is then given by:\n\nwhere Q(o) is the mass flow rate of solid in the screen overflow, Q(f) is the mass flow rate of solid feed, Q(u) is the mass flow rate of solid in the screen underflow, M(o) is the mass fraction of undersize in the overflow, <br>M(f) is the mass fraction of undersize in the feed, M(u) is the mass fraction of undersize in the underflow.\n\nThe overall efficiency (E) is given by:\n\nIn the process of sizing minerals there are often rules of thumbs that need to be followed in order to achieve maximum efficiency in the separation.\n\nThe selection on the screen type will be based on the materials that the equipment will be used to process. A significant problem occurs with screens because if the screen is not suitable for the material fed to the screen, the materials will blind the apertures and regular maintenance will be required. Different types of screens have been developed to counter this problem. An example is the \"self-cleaning\" wire; these wires are free to vibrate and so resistance to blinding will increase. The particles will be shaken off the wires and apertures. However, there will be a trade-off with screening efficiency.\n\nThe high frequency vibrating screens will often be used as a secondary screener as its purpose is to separate the finer minerals. This not only ensures good separation efficiency, it will also help to maintain the life-time of the screen. Blinding can occur significantly if particle sizes are not within the screens’ designed criteria.\n\nAnother problem that is often encountered is that the particles clump together due to the moist. This will result in the undesired particle size that is not allowed to pass through the apertures into the product stream. It is recommended that screening at less than around 5mm aperture size is normally performed on perfectly dry materials. A heated screen deck may be used to evaporate the moist in the feed. It will also break the surface tension between the screen wire and the particles. An alternative is to run the feed through a dryer before entering the high frequency vibrating screen.\n\nHigh frequency vibrating screens are widely used in many industrial process, thus there will be high quantity of waste product released into the environment. It is important that these waste streams are treated, since the untreated waste will cause damage to the environment over a sustained period of time.\n\nAn established post-treatment system is classification processing. In this system, the waste streams are separated into different types of waste materials. The types of waste materials are classified into recyclable materials, hazardous materials, organic materials, inorganic materials. Generally, waste materials are separated using mechanical separation and manual separation. Mechanical separations are used for separating metals and other materials that may be harmful to the environment, and also to prepare the waste stream for manual separations. Manual separation have two types of sorting which are positive sorting and negative sorting. Positive sorting collects reusable waste such as recyclable and organic materials while negative sorting collects unusable waste such as hazardous and inorganic materials. After this separation process, the recyclable materials are transferred for reuse. The organic wastes are often treated using chemical processes (e.g. combustion, pyrolysis etc.) or biological treatment (microbial decomposition). The products obtained from these waste organic materials are in the form of Refuse-Derived Fuel. RDF can be used in many ways to generate electricity or even used alongside with traditional sources of fuel in coal power plants. The rest of the hazardous and unwanted inorganic wastes are transferred to landfill to be disposed. These post-treatment processes are crucial to sustain the environment.\n\nThe research on high frequency screens has led to new developments in the field which enhance the operation and performance of the equipment. These new developments include the stacking of up to 5 individual screen decks placed on top of the other and operating in parallel. A divider system splits the feed slurry to each Stack Sizer® screen, then to each screen deck on the machine. Each screen deck has an undersize and oversize collection pan which will respectively go into their common outlet. The stacking of the machines thus allows more production while using less space. Another new development is the fabrication of Polyweb urethane screen surfaces that have openings as fine as 45 µm and open areas from 35% - 45%. This leads to the screen being able to separate finer particles. The screens can be used for both wet and dry applications and urethane formulation is still an ongoing process. Thus, research and development is still being invested in high frequency screening equipment to improve the overall separation efficiency and also to lower costs.\n\nTo further optimize the performance for high frequency vibrating equipment, a \"variable speed\" hydraulic vibrator is being developed and used to drive the screen decks. It utilizes fluid hydraulic force which then can be converted into rotary power in order to generate high frequency vibration. This modification allows equipment to operate at higher frequency range, up to 8200 RPM, compared to the conventional electric vibrators. Besides that, the induced vibration also creates an excellent condition for separating finer particles and improves the contacting probability for the materials. Another variation that could be applied to the equipment is the \"rotary tensioning system\", in which it helps to provide a quicker screen media change. Therefore, multiple applications can be achieved by single equipment, as with different size of feed material can be deal by replacing screens in a very small downtime. Hence, it improves the economic benefits of plants.\n"}
{"id": "2384938", "url": "https://en.wikipedia.org/wiki?curid=2384938", "title": "High Blast Explosive", "text": "High Blast Explosive\n\nHigh Blast Explosive, or HBX, is an explosive used as a bursting charge in missile warheads, mines, depth bombs, depth charges, and torpedoes.\n\nIt was developed during World War II as a desensitized modification of Torpex explosives.<ref name=\"Ordnance Technical Data Sheet U.S. BOMB, 325-350-LB, DEPTH BOMB, MK 54/MK 54 Mod 1\"> \"Ordnance Technical Data Sheet U.S. BOMB, 325-350-LB, DEPTH BOMB, MK 54/MK 54 Mod 1\"</ref>\n\nIt is an aluminized (powdered aluminum) explosive having the same order of sensitivity as Composition B.\n\nTests indicate that it is about 98% to 100% as powerful as Torpex, that it is definitely less sensitive than Torpex in both laboratory impact and bullet impact, that it is slightly more sensitive in these respects than TNT, and that is be about the same order as Composition B.\n\nA difficulty with HBX is that it produces gas and builds up pressure in the case during stowage. It was discovered that adding calcium chloride to the mixture will absorb all the moisture and eliminate the production of gas.\n\nThere are three types of HBX explosives: HBX-1, HBX-3, and H-6. Below is each type's \"Grade A\" composition based on weight:\n"}
{"id": "24237990", "url": "https://en.wikipedia.org/wiki?curid=24237990", "title": "In Mortal Hands", "text": "In Mortal Hands\n\nIn Mortal Hands: A Cautionary History of the Nuclear Age is a 2009 book by Stephanie Cooke. The book explains why nuclear energy failed to develop in the way its planners hoped, and explores the relationship between the military and civilian sides of nuclear energy. In the book, Cooke argues that we are not close to solving the nuclear waste problem, and that \"the billions spent by government on nuclear over the past sixty years crowded out other energy options\". The book suggests that there are practical reasons why nuclear reactors are unlikely to provide a solution to the global climate change problem.\n\n\"In Mortal Hands\" has been the subject of several media interviews with Cooke.\n\nStephanie Cooke has written about the nuclear industry since the 1980s. She is currently an editor for the Energy Intelligence Group publication, \"Nuclear Intelligence Weekly\" and is a contributor to the \"Bulletin of the Atomic Scientists\".\n\n\n"}
{"id": "9012608", "url": "https://en.wikipedia.org/wiki?curid=9012608", "title": "Institute of Molecular Pathology and Immunology of the University of Porto", "text": "Institute of Molecular Pathology and Immunology of the University of Porto\n\nThe Institute of Molecular Pathology and Immunology of the University of Porto, best known by its acronym IPATIMUP (), is a Portuguese non-profit institution of public utility dedicated to the health sciences research. An associate laboratory of the University of Porto, Porto, since November 23, 2000, it is headed by the notable Portuguese researcher Manuel Sobrinho Simões.\n\nIPATIMUP's major lines of action are the prevention and early diagnosis of stomach cancer or precocious lesions, and diagnosis quality improvement of malignant neoplasia and pre-malignant lesions. The numerous published papers and important results related to gastric and esophagical cancer make this one of the (if not the) top-level cancer-related research institutions in Portugal and in Europe. But as a scientific teaching-associated institution, its main goals also are:\n\n\n"}
{"id": "47403", "url": "https://en.wikipedia.org/wiki?curid=47403", "title": "Instrumentation", "text": "Instrumentation\n\nInstrumentation is a collective term for measuring instruments used for indicating, measuring and recording physical quantities, and has its origins in the art and science of scientific instrument-making. \n\nThe term instrumentation may refer to a device or group of devices used for direct reading thermometers or, when using many sensors, may become part of a complex industrial control system in such as manufacturing industry, vehicles and transportation. Instrumentation can be found in the household as well; a smoke detector or a heating thermostat are examples.\n\nThe history of instrumentation can be divide into several phases. \nElements of industrial instrumentation have long histories. Scales for comparing weights and simple pointers to indicate position are ancient technologies. Some of the earliest measurements were of time. One of the oldest water clocks was found in the tomb of the ancient Egyptian pharaoh Amenhotep I, buried around 1500 BCE. Improvements were incorporated in the clocks. By 270 BCE they had the rudiments of an automatic control system device. \n\nIn 1663 Christopher Wren presented the Royal Society with a design for a \"weather clock\". A drawing shows meteorological sensors moving pens over paper driven by clockwork. Such devices did not become standard in meteorology for two centuries. The concept has remained virtually unchanged as evidenced by pneumatic chart recorders, where a pressurized bellows displaces a pen. Integrating sensors, displays, recorders and controls was uncommon until the industrial revolution, limited by both need and practicality.\n\nEarly systems used direct process connections to local control panels for control and indication, which from the early 1930s saw the introduction of pneumatic transmitters and automatic 3-term (PID) controllers.\n\nThe ranges of pneumatic transmitters were defined by the need to control valves and actuators in the field. Typically a signal ranged from 3 to 15 psi (20 to 100kPa or 0.2 to 1.0 kg/cm2) as a standard, was standardized with 6 to 30 psi occasionally being used for larger valves. \nTransistor electronics enabled wiring to replace pipes, initially with a range of 20 to 100mA at up to 90V for loop powered devices, reducing to 4 to 20mA at 12 to 24V in more modern systems. A transmitter is a device that produces an output signal, often in the form of a 4–20 mA electrical current signal, although many other options using voltage, frequency, pressure, or ethernet are possible. The transistor was commercialized by the mid-1950s.\n\nInstruments attached to a control system provided signals used to operate solenoids, valves, regulators, circuit breakers, relays and other devices. Such devices could control a desired output variable, and provide either remote or automated control capabilities.\n\nEach instrument company introduced their own standard instrumentation signal, causing confusion until the 4-20 mA range was used as the standard electronic instrument signal for transmitters and valves. This signal was eventually standardized as ANSI/ISA S50, “Compatibility of Analog Signals for Electronic Industrial Process Instruments\", in the 1970s. The transformation of instrumentation from mechanical pneumatic transmitters, controllers, and valves to electronic instruments reduced maintenance costs as electronic instruments were more dependable than mechanical instruments. This also increased efficiency and production due to their increase in accuracy. Pneumatics enjoyed some advantages, being favored in corrosive and explosive atmospheres.\n\nIn the early years of process control, process indicators and control elements such as valves were monitored by an operator that walked around the unit adjusting the valves to obtain the desired temperatures, pressures, and flows. As technology evolved pneumatic controllers were invented and mounted in the field that monitored the process and controlled the valves. This reduced the amount of time process operators were needed to monitor the process. Later years the actual controllers were moved to a central room and signals were sent into the control room to monitor the process and outputs signals were sent to the final control element such as a valve to adjust the process as needed. These controllers and indicators were mounted on a wall called a control board. The operators stood in front of this board walking back and forth monitoring the process indicators. This again reduced the number and amount of time process operators were needed to walk around the units. The most standard pneumatic signal level used during these years was 3-15 psig.\n\nProcess control of large industrial plants has evolved through many stages. Initially, control would be from panels local to the process plant. However this required a large manpower resource to attend to these dispersed panels, and there was no overall view of the process. The next logical development was the transmission of all plant measurements to a permanently-manned central control room. Effectively this was the centralisation of all the localised panels, with the advantages of lower manning levels and easier overview of the process. Often the controllers were behind the control room panels, and all automatic and manual control outputs were transmitted back to plant. \n\nHowever, whilst providing a central control focus, this arrangement was inflexible as each control loop had its own controller hardware, and continual operator movement within the control room was required to view different parts of the process. With coming of electronic processors and graphic displays it became possible to replace these discrete controllers with computer-based algorithms, hosted on a network of input/output racks with their own control processors. These could be distributed around plant, and communicate with the graphic display in the control room or rooms. The distributed control concept was born. \n\nThe introduction of DCSs and SCADA allowed easy interconnection and re-configuration of plant controls such as cascaded loops and interlocks, and easy interfacing with other production computer systems. It enabled sophisticated alarm handling, introduced automatic event logging, removed the need for physical records such as chart recorders, allowed the control racks to be networked and thereby located locally to plant to reduce cabling runs, and provided high level overviews of plant status and production levels.\n\nIn some cases the sensor is a very minor element of the mechanism. Digital cameras and wristwatches might technically meet the loose definition of instrumentation because they record and/or display sensed information. Under most circumstances neither would be called instrumentation, but when used to measure the elapsed time of a race and to document the winner at the finish line, both would be called instrumentation.\n\nA very simple example of an instrumentation system is a mechanical thermostat, used to control a household furnace and thus to control room temperature. A typical unit senses temperature with a bi-metallic strip. It displays temperature by a needle on the free end of the strip. It activates the furnace by a mercury switch. As the switch is rotated by the strip, the mercury makes physical (and thus electrical) contact between electrodes.\n\nAnother example of an instrumentation system is a home security system. Such a system consists of \nsensors (motion detection, switches to detect door openings), simple algorithms to detect intrusion, local control (arm/disarm) and remote monitoring of the system so that the police can be summoned. Communication is an inherent part of the design.\n\nKitchen appliances use sensors for control.\n\nModern automobiles have complex instrumentation. In addition to displays of engine rotational speed and vehicle linear speed, there are also displays of battery voltage and current, fluid levels, fluid temperatures, distance traveled and feedbacks of various controls (turn signals, parking brake, headlights, transmission position). Cautions may be displayed for special problems (fuel low, check engine, tire pressure low, door ajar, seat belt unfastened). Problems are recorded so they can be reported to diagnostic equipment. Navigation systems can provide voice commands to reach a destination. Automotive instrumentation must be cheap and reliable over long periods in harsh environments. There may be independent airbag systems which contain sensors, logic and actuators. Anti-skid braking systems use sensors to control the brakes, while cruise control affects throttle position. A wide variety of services can be provided via communication links as the OnStar system. Autonomous cars (with exotic instrumentation) have been demonstrated.\n\nEarly aircraft had a few sensors. \"Steam gauges\" converted air pressures into needle deflections that could be interpreted as altitude and airspeed. A magnetic compass provided a sense of direction. The displays to the pilot were as critical as the measurements.\n\nA modern aircraft has a far more sophisticated suite of sensors and displays, which are embedded into avionics systems. The aircraft may contain inertial navigation systems, global positioning systems, weather radar, autopilots, and aircraft stabilization systems. Redundant sensors are used for reliability. A subset of the information may be transferred to a crash recorder to aid mishap investigations. Modern pilot displays now include computer displays including head-up displays.\n\nAir traffic control radar is distributed instrumentation system. The ground portion transmits an electromagnetic pulse and receives an echo (at least). Aircraft carry transponders that transmit codes on reception of the pulse. The system displays aircraft map location, an identifier and optionally altitude. The map location is based on sensed antenna direction and sensed time delay. The other information is embedded in the transponder transmission.\n\nAmong the possible uses of the term is a collection of laboratory test equipment controlled by a computer through an IEEE-488 bus (also known as GPIB for General Purpose Instrument Bus or HPIB for Hewlitt Packard Instrument Bus). Laboratory equipment is available to measure many electrical and chemical quantities. Such a collection of equipment might be used to automate the testing of drinking water for pollutants.\n\nInstrumentation is used to measure many parameters (physical values). These parameters include:\n\n\n\n\nInstrumentation engineering is the engineering specialization focused on the principle and operation of measuring instruments that are used in design and configuration of automated systems in electrical, pneumatic domains etc and the control of quantities being measured.\nThey typically work for industries with automated processes, such as chemical or manufacturing plants, with the goal of improving system productivity, reliability, safety, optimization and stability.\nTo control the parameters in a process or in a particular system, devices such as microprocessors, microcontrollers or PLCs are used, but their ultimate aim is to control the parameters of a system.\n\nInstrumentation engineering is loosely defined because the required tasks are very domain dependent. An expert in the biomedical instrumentation of laboratory rats has very different concerns than the expert in rocket instrumentation. Common concerns of both are the selection of appropriate sensors based on size, weight, cost, reliability, accuracy, longevity, environmental robustness and frequency response. Some sensors are literally fired in artillery shells. Others sense thermonuclear explosions until destroyed. Invariably sensor data must be recorded, transmitted or displayed. Recording rates and capacities vary enormously. Transmission can be trivial or can be clandestine, encrypted and low-power in the presence of jamming. Displays can be trivially simple or can require consultation with human factors experts. Control system design varies from trivial to a separate specialty.\n\nInstrumentation engineers are responsible for integrating the sensors with the recorders, transmitters, displays or control systems, and producing the Piping and instrumentation diagram for the process. They may design or specify installation, wiring and signal conditioning. They may be responsible for calibration, testing and maintenance of the system.\n\nIn a research environment it is common for subject matter experts to have substantial instrumentation system expertise. An astronomer knows the structure of the universe and a great deal about telescopes - optics, pointing and cameras (or other sensing elements). That often includes the hard-won knowledge of the operational procedures that provide the best results. For example, an astronomer is often knowledgeable of techniques to minimize temperature gradients that cause air turbulence within the telescope.\n\nInstrumentation technologists, technicians and mechanics specialize in troubleshooting, repairing and maintaining instruments and instrumentation systems.\n\nCurrent Loop (4-20mA) - Electrical\n\nHART - Data signalling often overlaid on a current loop.\n\nFoundation Fieldbus - Data signalling\n\nProfibus - Data signalling\n\nRalph Müller (1940) stated \"That the history of physical science is largely the history of instruments and their intelligent use is well known. The broad generalizations and theories which have arisen from time to time have stood or fallen on the basis of accurate measurement, and in several instances new instruments have had to be devised for the purpose. There is little evidence to show that the mind of modern man is superior to that of the ancients. His tools are incomparably better.\"\n\nDavis Baird has argued that the major change associated with Floris Cohens identification of a \"fourth big scientific revolution\" after World War II is the development of scientific instrumentation, not only in chemistry but across the sciences. In chemistry, the introduction of new instrumentation in the 1940s was \"nothing less than a scientific and technological revolution\" in which classical wet-and-dry methods of structural organic chemistry were discarded, and new areas of research opened up.\n\nAs early as 1954, W A Wildhack discussed both the productive and destructive potential inherent in process control.\nThe ability to make precise, verifiable and reproducible measurements of the natural world, at levels that were not previously observable, using scientific instrumentation, has \"provided a different texture of the world\". This instrumentation revolution fundamentally changes human abilities to monitor and respond, as is illustrated in the examples of DDT monitoring and the use of UV spectrophotometry and gas chromatography to monitor water pollutants.\n\n"}
{"id": "43271081", "url": "https://en.wikipedia.org/wiki?curid=43271081", "title": "Iron founder", "text": "Iron founder\n\nAn iron founder (also iron-founder or ironfounder) is a worker in molten ferrous metal, generally working within an iron foundry. \n\nHistorically the appellation \"founder\" was given to the supervisor of a blast furnace, and persons who made castings in iron or other heavy metal. The term is also often applied to the company or works in which an iron foundry operates.\n\n"}
{"id": "13168288", "url": "https://en.wikipedia.org/wiki?curid=13168288", "title": "Jackup rig", "text": "Jackup rig\n\nA jackup rig or a self-elevating unit is a type of mobile platform that consists of a buoyant hull fitted with a number of movable legs, capable of raising its hull over the surface of the sea. The buoyant hull enables transportation of the unit and all attached machinery to a desired location. Once on location the hull is raised to the required elevation above the sea surface supported by the sea bed. The legs of such units may be designed to penetrate the sea bed, may be fitted with enlarged sections or footings, or may be attached to a bottom mat. Generally jackup rigs are not self-propelled and rely on tugs or heavy lift ships for transportation.\n\nJackup platforms are used as exploratory drilling platforms and offshore and wind farm service platforms. Jackup platforms have been the most popular and numerous of various mobile types in existence. The total number of jackup drilling rigs in operation numbered about 540 at the end of 2013.\n\nJackup rigs are so named because they are self-elevating with three, four, six and even eight movable legs that can be extended (“jacked”) above or below the hull. Jackups are towed or moved under self propulsion to the site with the hull lowered to the water level, and the legs extended above the hull. The hull is actually a water-tight barge that floats on the water’s surface. When the rig reaches the work site, the crew jacks the legs downward through the water and into the sea floor (or onto the sea floor with mat supported jackups). This anchors the rig and holds the hull well above the waves.\n\nAn early design was the DeLong platform that Leon B. DeLong designed. In 1949 he started his own company, DeLong Engineering & Construction Company. In 1950 he constructed the \"DeLong Rig No. 1\" for Magnolia Petroleum, consisting of a barge with six legs. In 1953 DeLong entered into a joint-venture with McDermott, which built the \"DeLong-McDermott No.1\" in 1954 for Humble Oil.This was the first mobile offshore drilling platform. This barge had ten legs which had \"spud cans\" to prevent them from digging into the seabed too deep. When DeLong-McDermott was taken over by the Southern Natural Gas Company which formed The Offshore Company the platform was called \"Offshore No. 51\".\n\nIn 1954, Zapata Offshore, owned by George H. W. Bush ordered the \"Scorpion\". It was designed by R. G. LeTourneau and featured three electro-mechanically-operated lattice type legs. Built on the shores of the Mississippi river by the LeTourneau Company, it was launched in December 1955. The \"Scorpion\" was put into operation in May 1956 off Port Aransas, Texas. The second, also designed by LeTourneau, was called \"Vinegaroon\". \n\nA jackup rig is a barge fitted with long support legs that can be raised or lowered. The jackup is maneuvered (self-propelled or by towing) into location with its legs up and the hull floating on the water. Upon arrival at the work location, the legs are jacked down onto the seafloor. Then \"preloading\" takes place, where the weight of the barge and additional ballast water are used to drive the legs securely into the sea bottom so they will not penetrate further while operations are carried out. After preloading, the jacking system is used to raise the entire barge above the water to a predetermined height or \"air gap\", so that wave, tidal and current loading acts only on the relatively slender legs and not on the barge hull.\n\nModern jacking systems use a rack and pinion gear arrangement where the pinion gears are driven by hydraulic or electric motors and the rack is affixed to the legs.\n\nJackup rigs can only be placed in relatively shallow waters, generally less than of water. However, a specialized class of jackup rigs known as premium or ultra-premium jackups are known to have operational capability in water depths ranging from 150 to 190 meters (500 to 625 feet).\n\nThis type of rig is commonly used in connection with oil and/or natural gas drilling. There are more jackup rigs in the worldwide offshore rig fleet than other type of mobile offshore drilling rig. Other types of offshore rigs include semi-submersibles (which float on pontoon-like structures) and drillships, which are ship-shaped vessels with rigs mounted in their center. These rigs drill through holes in the drillship hulls, known as moon pools.\n\nThis type of rig is commonly used in connection with offshore wind turbine installation.\n\n\"Jackup rigs\" can also refer to specialized barges that are similar to an oil and gas platform but are used as a base for servicing other structures such as offshore wind turbines, long bridges, and drilling platforms.\n\n"}
{"id": "57100123", "url": "https://en.wikipedia.org/wiki?curid=57100123", "title": "Libre Computer Project", "text": "Libre Computer Project\n\nThe Libre Computer Project is an effort initiated by Shenzhen Libre Technology Co., Ltd., with the goal of producing standards-compliant single-board computers (SBC) and upstream software stack to power them.\n\nLibre Computer Project uses crowd-funding on Indiegogo and Kickstarter to market their SBC designs.\n\nActive Libre Computer SBC designs include:\n\nThe ROC-RK3328-CC \"Renegade\" board was funded on Indiegogo and features the following specifications:\n\n\nThe AML-S905X-CC \"Le Potato\" board was funded on Kickstarter on 24 July 2017 and features the following specifications:\n\n\nNOTE: GPIO Header Pin 11 or HDMI CEC is selectable by onboard jumper. They can not be used at the same time since they share the same pad.\n\nThe \"Tritium\" board was funded on Kickstarter on 13 January 2018 with the following specifications:\n\nLibre Computer is focused on upstream support in open-source software using standardized API interfaces. This includes Linux, u-boot, LibreELEC RetroArch, and more. A variety of open-source operating systems may be used on Libre Computer boards, including Linux and Android. Few to no binary blobs are used to boot and operate the boards.\n\nSchematics and 2D silkscreen are available for all hardware. Design files are based on non-disclosure materials from SoC vendors.\n\n\nSome of these devices should be under but not all.\n"}
{"id": "40013149", "url": "https://en.wikipedia.org/wiki?curid=40013149", "title": "List of topics in space", "text": "List of topics in space\n\nList of Topics in Space; topics as related to outer space.\n\n\n"}
{"id": "7187928", "url": "https://en.wikipedia.org/wiki?curid=7187928", "title": "Machinima, Inc.", "text": "Machinima, Inc.\n\nMachinima, Inc. is a multiplatform online entertainment network operated by Warner Bros. Digital Networks. Machinima's programming has had widespread availability across multiple platforms such as YouTube, Twitter, Facebook, Twitch.tv, and Amazon Prime Video. The company was founded in January 2000 by Hugh Hancock and was headquartered in Los Angeles, California.\n\nIt originated as a hub for its namesake, machinima, which uses and manipulates video-game technology to create animation, as well as featuring articles on machinima and content about film and technology. The website initially helped to bring attention to machinima as an art form and to encourage productions based on game engines other than those of id Software's first-person shooter computer game series \"Quake\". Over time, the website's focus shifted to general entertainment programming centered around video game culture, comic books and fandom.\n\nOn October 12, 2016, sources told media sites that Warner Bros. was nearing a deal to acquire Machinima and its branded properties.<ref name=\"Variety 10/2016\"></ref> On November 17, 2016, Warner Bros. confirmed the news, thus making Machinima a wholly owned subsidiary of Warner Bros. Digital Networks.<ref name=\"Variety 11/2016\"></ref>\n\nIn December 1999, id Software released \"Quake III Arena\". According to Paul Marino, executive director of the Academy of Machinima Arts & Sciences, film makers who had been using prior versions of the \"Quake\" series to record animated videos, then called \"\"Quake\" movies\", were initially excited, but the enthusiasm dampened when id announced that, in an attempt to curtail cheating in multiplayer games, it would take legal action against anyone who released details of \"Quake III\"s networking code, which was included in the game's game demo file format. This precluded the use of custom demo-editing tools that had facilitated the creation of videos that used the older \"Quake\" and \"Quake II\" demo file formats, slowing the release of new \"Quake\" movies. Another contributing factor to this decline was that the self-referential nature of the gaming-related situations and commentary of \"Quake\" movies was losing novelty. Marino explained bluntly that \"the joke was getting old\". Therefore, the \"Quake\" movie community needed to reinvent itself.\nIn January 2000, Hugh Hancock launched Machinima.com, a resource for video makers who used computer and video games as a medium. The site's name was foreign to the \"Quake\" movie community. The term \"machinima\" was originally \"machinema\", a portmanteau of \"machine\" and \"cinema\". However, Hancock had misspelled the term in a previous email, and the new name stuck because he and Anthony Bailey, who had worked on \"Quake done Quick\", liked the now-embedded reference to anime.\n\nThe site opened with multiple articles, interviews, and tutorials, and was soon able to acquire exclusive releases of new productions. One such work, \"Quad God\", was the first to use \"Quake III Arena\" and the first to be released in a conventional video file format instead of a demo file format exclusive to a certain game. The switch to conventional media offended some machinima producers, but \"Quad God\", by Tritin Films helped to introduce machinima to a wider audience and to solidify Machinima.com's launch. Matt Kelland, Dave Morris, and Dave Lloyd called the release of \"Quad God\" \"a key moment in the development of machinima. In turn, as Machinima.com became more popular throughout 2000, other game engines, such as that of \"Unreal Tournament\", became the basis of new productions and the focus of new software tools for machinima.\n\nOn 30 January 2006, Hancock announced his resignation as editor-in-chief of Machinima.com and that control of the site would be transferred to the staff of Machinima, Inc. Among the reasons cited for the change were differences in approach to the site and a desire to devote more time to Strange Company's 2006 machinima production \"BloodSpell\". Hancock called the decision \"possibly the biggest step I've taken since I founded Strange Company nearly nine years ago\".\n\nTowards the end of 2010, Machinima revamped its website and removed the forums (wanting users to use the Facebook page instead), and the ability to upload videos. Since the revamp of their website, Machinima has shifted focus away from actual machinima content. The network now focuses on gamer lifestyle and entertainment programming, broadcasting solely through their YouTube channels.\n\nIn January 2012, Machinima discontinued podcast feeds on iTunes without an in-feed announcement. Back episodes remain available but no new episodes have appeared on the feed since moving to YouTube exclusively. In June 2012, Machinima partnered with Meteor Entertainment to promote Hawken, a highly anticipated free-to-play online game which was later released in December 2012.\n\nIn the same month, Microsoft announced the inclusion of Machinima programming on Xbox Live during the 2012 Electronic Entertainment Expo. In May 2012, Google invested $35 million into Machinima. It was the first time Google has openly backed a content company by taking an equity stake. In December 2012, Machinima.com announced it was letting go of 23 staff from its workforce. Machinima said the lay-offs were due to re-organizing as part of its global growth strategy, but were still hiring in other key divisions whilst these layoffs were happening.\n\nIn early 2014, the main channel briefly returned to uploading original machinima series and movies. In March 2014, Warner Bros. led an $18 million round of funding for Machinima. Around the same time, founder Allen DeBevoise stepped down as CEO and became the new chairman. Former Ovation COO Chad Gutsein was installed in his place. In November 2014, Machinima announced plans to rebrand their network, with a revamped logo and new tagline.\n\nIn February 2015, the company raised an additional $42 million in funding led by Warner Bros. Machinima said that the additional funding would be used to accelerate growth through more investments in content and technology to better serve the firms audiences, advertisers, creators and distributors.\n\nOn October 12, 2016, sources told media sites that Warner Bros. was nearing a deal to acquire Machinima and its branded properties. On November 17, 2016, Warner Bros. confirmed the news, thus making Machinima a wholly owned subsidiary of Warner Bros. Digital Networks.\n\nOn February 14, 2018, after being integrated into Warner Bros. Digital Networks, Machinima unveiled a new logo and plans to shift its programming back towards gamer-centric content, and away from the Multi-channel network model. \n\nIn November 2018, \"Deadline.com\" reported that Warner Bros.' new parent company AT&T was preparing to reorganize Machinima into Otter Media—owner of the multi-channel network Fullscreen, and Rooster Teeth Productions—which similarly produces gaming-oriented content and series. Otter was previously a joint venture between AT&T and Chernin Group; AT&T bought out Chernin's stake shortly after its purchase of Time Warner.\n\nMachinima's content was primarily hosted on various YouTube channels. Content uploaded onto these channels are either produced in-house or by signed directors. Machinima has also utilized social media platforms to provide fans with featured uploads, interactive questions, and live event coverage.\n\n\"Inside Gaming\" was the main editorial brand of Machinima. Coverage of gaming news, previews, and reviews was provided for more than 600,000 weekly viewers through daily and weekly shows on its YouTube channel hosted by then employee Adam Kovic under the alias \"The Dead Pixel\". He was often seen in Halo 3-themed machinima form in his lava-red Recon helmet.\n\n\"Inside Gaming\" is the successor to Machinima's discontinued segment, \"Inside Halo\", which was less successful because of the lack of news surrounding the Halo series. \"Inside Halo\" was developed and hosted by \"Soda God\" who alternated weekly hosting with Adam Kovic who became the only host. Eventually an official co-host, Matt Dannevik, joined Kovic on the set of \"Inside Gaming Daily\"; he was laid off in December 2012. Producers Bruce Greene and James Willems regularly co-hosted with Kovic, and have started their own YouTube channel under \"Inside Gaming\". \"Inside Gaming\" also hosted its own annual awards show, the \"Inside Gaming Awards\" in Los Angeles. The awards show celebrates the biggest developers and achievements in the video-games industry, and features top gaming choices by viewers and the staff of \"Inside Gaming\". Categories in which games are selected include, among others: Game of the Year, Best Online Multiplayer, and Best Original Games.\n\nOn January 26, 2015, Inside Gaming employees Adam Kovic, Bruce Greene, Lawrence Sonntag, Joel Rubin, Sean Poole, James Willems, and Matt Peake announced that they were leaving Machinima. The group is now known as Funhaus, a subsidiary of Rooster Teeth Productions. On April 9, 2015, Matt Dannevik announced in a video that he would be returning to Machinima and taking over the Inside Gaming channel, with help from other members of Machinima. The channel, and the show, has since become inactive.\n\n\"ETC News\", otherwise known as simply \"ETC\", was Machinima's entertainment news show that started in 2010. The name originally stood for \"Entertainment, Technology, Culture.\" It was originally hosted by Khail Anonymous, who later left the company in 2014 and currently works for \"Yahoo! News\". It was most recently hosted by Machinima employees Ricky Hayberg and Eliot Dewberry, before their departure from the company. Originally airing on the primary Machinima channel the show was moved to its own channel on June 30, 2016, though Ricky and Eliot still worked for Machinima and the show was still owned by them. \n\n\"ETC Daily\" was the main show, with weekly shows such as \"TechNewsDay\" (originally Tech Tuesday, but renamed in order to remove the inherent deadline) for technology news, \"Weekly Weird News\" for exploring odd headlines and going deeper into \"crazy\" news stories, News Dump for covering film news, \"T.U.G.S.\" (\"The Totally Uninformed Gaming Show\") for gaming news meant to satirize what the hosts considered to be game journalists who were far too soft on gaming companies, though the show was later moved to the specialized gaming channel, ETC Party Time. Outside of news, the channel hosted the \"ETC Podcast\", where the hosts interviewed creators such as Dan Harmon & Justin Roiland, Mike Shinoda, Tony Hale, Ed Skrein, Elijah Wood, Kill The Noise, Verne Troyer, Alicia Malone, Kristian Harloff & Mark Ellis, and Dillon Francis. \n\nOn June 27, 2018 the duo officially cut ties with ETC News and launched a new YouTube channel known as \"Internet Today\", taking many shows from ETC and continuing them there. \"ETC Daily\" was renamed to \"Internet Today\" and \"T.U.G.S.\" was officially canceled. The \"ETC Podcast\" was reworked into \"Idiots Watching Anime\", a full series version of a few episodes they did of the \"ETC Podcast\" where the two hosts watched a few episodes of \"Dragon Ball Z\", a show they were both unfamiliar with, and discussed it with the friends who suggested the episodes.\n\nMachinima previously held livestreams on the \"Machinima Live\" YouTube channel. Currently, the network streams on Twitch, with their channel hosting gameplay events, convention coverage, and more. \"Machinima Live\" also had 24-hour live streams, such as one that took place in 2010 after the release of \"\". Machinima staff, directors, and guests took part in playing the game in four-hour shifts in attempt to reach 15th prestige. A similar event also occurred after the release of \"\".\n\nIn 2012, Machinima branched out to competitive side of gaming with the inclusion of \"Machinima VS\", a channel featuring event coverage from some top-ranked players, teams, and casters. It serves as Machinima's esports channel. The channel has since become inactive.\n\nOn December 7, 2009, the \"Machinima Respawn\" channel was launched as Machinima's gameplay focused channel. It had a host of shows about games and related topics as well as the show \"Respawn\" hosted by Adam Montoya, Scott Robison, and Shaun Hutchinson. \"Inbox\" was a later show that gained a cult following for its comedy and the funny personalities of the hosts Scott Fisher and Scott Robison. At one point \"Machinima Respawn\" was the most subscribed channel on YouTube. Due to budget cuts at Machinima, the lack of views of the newer programming on \"Respawn\" in later years, and the departure of Scott Robison, Shaun Hutchinson and Adam Montoya along with Scott Fisher, \"Machinima Respawn\" was discontinued, not having been active since February 22, 2015. It has since been absorbed into Machinima Realm, which was later renamed \"Realm Games\".\n\nOriginally launched as \"Machinima Realm\", \"Realm Games\" is Machinima's gameplay-focused channel. Originally only focusing on MMO's and Real-time strategy games among other genres, it has since become Machinima's primary hub for all their gameplay content after the closure of \"Machinima Respawn.\" It is currently being managed by Machinima employee Shane, otherwise known as Shibby2142, and hosts a variety of content focusing on several different games, mostly \"Overwatch\" and \"League of Legends\" recently.\n\nOn May 18, 2009, Machinima released \"Terminator Salvation: The Machinima Series\", an animated web series set before the video game and leading to the events of the film, comprising six episodes. The series set in 2016 and follows Blair Williams (voiced by Moon Bloodgood) is fighting the war against the machines in downtown Los Angeles, while tracking down the computer hacker named Laz Howard (voiced by Cam Clarke) and trying to pursue him to join sides with the resistance. The series was created using real-time computer animation from the video game. It was distributed by Warner Premiere, produced by Wonderland Sound and Vision and The Halcyon Company.\n\nOn April 11, 2011, Machinima aired \"\", a live action series produced by Warner Bros. Digital Distribution, Warner Bros. Interactive Entertainment and Warner Premiere featuring Michael Jai White, Darren Shahlavi, and Jeri Ryan. Based on the Mortal Kombat series, \"Legacy\" succeeds the short film \"\", which takes place in an alternate universe. This series was aired exclusively on the Machinima YouTube channel and served over 60 million combined views.\n\n\"Bite Me\" was a two-season, web series released on December 31, 2010, about three gamers as they find themselves in the midst of a real life zombie outbreak. Relying only on the knowledge and skills they have gain from years of gaming, they drop the controller and pick up anything that can be used as a weapon. For the first season of the show, Machinima partnered with Microsoft and Capcom and accumulated over 14 million viewers. The second season was released on March 6, 2012, and was also aired on FEARnet, a horror cable network.\n\nOn May 23 of 2014, Machinima aired \"\", a live action Street Fighter series produced by Capcom and created by Joey Ansah and Christian Howard who made the popular short film \",\" reprising their roles as Ken and Akuma from the film.\n\nIn 2014, Machinima announced that they would air a three-part animated series titled \"\" which would serve as a companion to the animated movie, \"\". In May 2015, before the series even debuted, Machinima and DC Entertainment revealed that it had been renewed for a ten-episode second season to air in 2016. The first season launched on June 8, 2015, over two weeks before the movie was released. Series creator Sam Liu would later report that the series was shelved and that he had \"moved on to other projects.\"\n\nIn May 2015 it was revealed that Machinima, in co-development with Blue Ribbon Content and DC Entertainment is developing a live action adaptation of the DC Comics’ cult-favorite classic Dial H for Hero, called \"#4Hero\". It is described as VFX-heavy action-comedy about a young woman named Nellie Tribble who gets her powers from a smartphone app that allows her to instantly become a Super Hero for a short period of time. Her super powers are dictated by whatever is trending on social media at that moment.\n\n\"DC’s Hero Project\" is a contest show developed by Machinima, Blue Ribbon Content, and DC Entertainment. The show is about finding \"the next great creator for the world of DC Comics\". It is described as a contest between eight competitors who compete in elimination challenges to develop a live-action short video based on their interpretations of the characters from DC Comics’ Starman series. One of the confirmed judges will be bestselling writer, and DC Entertainment Chief Creative Officer, Geoff Johns.\n\n\"Street Fighter: Resurrection\" streamed exclusively on go90 in March 2016.\n\n\"Transformers: Combiner Wars\" is an animated series created in partnership with Hasbro for go90. It is the first part of the \"Prime Wars Trilogy\", a branch of the Transformers franchise based on the \"\" continuity family. It is based on the \"Combiner Wars\" toys as well as the story line from IDW Publishing's \"The Transformers\".\n\nMachinima developed a premium channel to feature quality content produced by network content creators along with major production companies and Hollywood studios known as \"Machinima Prime\". Weekly shows that run on \"Prime\" include \"Life on the Road\", \"XARM\", \"Prank Lab\", and \"Halo 4: Forward Unto Dawn\". After a year long hiatus, the channel was relaunched on August 17, 2016 and rebranded as \"Primr\", with all previously uploaded videos and series being made private. Some of the short films uploaded on the channel have since been reinstated, while its series remain hidden. The channel has since become inactive.\n\nHalo 4: Forward Unto Dawn\n\nXARM\n\nPrank Lab\n\nTH3 cLAN\n\nTainted Love\n\nHappy Hour was a block (and later a channel) focusing on animation. Eventually, the channel and block were shelved, with the channel being inactive since August 18, 2015, with no new episodes from any of its exclusive shows since July 28, 2015. The shows moved from the primary Machinima channel that are still being continued were moved back after Happy Hour's discontinuation. However, the channel resurfaced on September 7, 2016, with the premiere of the fourth season of \"Happy Hour Saloon\", an animated series that parodies video games. It stopped uploading content again on November 11, 2016.\n\nBattlefield Friends\n\nArby 'n' the Chief\n\nSanity Not Included\n\nSonic For Hire\n\nTwo Best Friends Funtime Adventures\n\nMachinima, Inc. has expanded on to many websites and platforms to distribute its content currently maintaining the main website, eleven YouTube channels, an iPhone/iPad application, an Android application, Facebook applications, Facebook/Twitter/Google+ feeds, and a newsletter that distributes news content.\n\nPreviously, Machinima had an application on both the Xbox One and Xbox 360. Both consoles have since removed the Machinima app from their App Store.\n\nMachinima uses a variety of social networking services including Facebook and Twitter as distribution platforms for its productions. It is integrated with Apple IOS and Microsoft Xbox Live service. Machinima frequently posts content on various social networks core to the concept of sharing and generating hits for Machinima videos.\n\nMachinima's partnership with Google includes Google's £30 million investment in Machinima.\n\nAs a Multi Channel Network, Machinima has over 5,000 partners worldwide who are contracted to produce video content under the Machinima brand. The company has been criticised for the use of perpetual contracts. Ben Vacas, known to the YouTube community as 'Braindeadly', attracted media attention in January 2013 over contractual issues with Machinima. Under the terms of his contract, Machinima was permitted to place advertisements on Vacas's videos and in return he would receive a percentage of the profits generated. However, the contract also disclosed that it existed \"in perpetuity\" meaning Machinima would hold the rights to any content created by Vacas, published on his partnered YouTube channel, in his lifetime, a detail Vacas failed to read.\n\nIn January 2014, Machinima was alleged to be paying its YouTube video partners for showing Xbox One content. According to reports, the content must be at least 30 seconds long and the Xbox One must be mentioned by name. An accompanying legal agreement also states that the partner \"may not say anything negative or disparaging about Machinima, Xbox One, or any of its games\". Additionally, the agreement states that the video producer must keep the details of the promotional agreement confidential, or they do not qualify for the promotional payment. Videos participating in this promotion tag their videos with the tag: XB1M13. Microsoft claims that it had no knowledge of the promotion.\n\nMachinima has faced criticism from YouTube members and viewers for a lack of transparency with its associates, placing advertisements on their associate channels' videos without permission, and a lack of transparency on the revenues side. One associate member, Clash, also criticised Machinima for insensitively placing an ad on a video dedicated to his ailing dog.\n\nOn September 2, 2015, Machinima agreed to settle Federal Trade Commission charges that it engaged in deceptive advertising by paying “influencers” to post YouTube videos endorsing Microsoft’s Xbox One system and several games. The FTC claimed that the influencers failed to adequately disclose that they were being paid for their seemingly objective opinions. Under the proposed settlement, Machinima was prohibited from similar deceptive conduct in the future, and it was required to ensure its influencers clearly disclosed when they are compensated for their endorsements. According to the FTC’s complaint, Machinima and its influencers were part of an Xbox One marketing campaign managed by Microsoft’s advertising agency, Starcom MediaVest Group. Machinima guaranteed Starcom that the influencer videos would be viewed at least 19 million times.\n\nOn May 9, 2018, \"Arby 'N' The Chief\" creator Jon Graham tweeted that most of the series was removed from Machinima's YouTube channel for unspecified reasons.\n\n\n\n"}
{"id": "53615175", "url": "https://en.wikipedia.org/wiki?curid=53615175", "title": "Meniga", "text": "Meniga\n\nMeniga is a software company founded in Reykjavik, Iceland in 2009 by Georg Ludviksson (CEO) and brothers Asgeir Asgeirsson (CTO) and Viggo Asgeirsson (CHRO).\n\nMeniga provides digital banking solutions, and was one of the first companies to use PFM software in Europe. Meniga was founder in 2009, and has offices in London, Reykjavik and Stockholm. \n\nThe company name is derived from a popular children's song, Eniga Meniga by Icelandic author and musician . The song is a playful ode to the song author's money problems.\n\nMeniga works with many banks in Europe including Santander, Commerzbank, CSOB, Intesa Sanpaolo, ING, and mBank. In 2016, the company was working with more than 60 banks. In November 2017, Meniga partnered with Spanish banking concern IberCaja. \n"}
{"id": "8779491", "url": "https://en.wikipedia.org/wiki?curid=8779491", "title": "Meowth's Party", "text": "Meowth's Party\n\nMeowth's Party is the 7th ending theme song of the Pokemon anime series. It was used in the original Japanese versions of episodes 117 through 141. It first gained exposure internationally, when it was recreated as an interactive technical demo showcased at the 2000 Nintendo Space World convention, demonstrating the GameCube's graphic capabilities. In the demo, Team Rocket's Meowth is seen entertaining guests at a party with his red guitar. The demo featured Pokémon from the \"Red\", \"Blue\" (\"Green\" in Japan), \"Gold\" and \"Silver\" versions of the Pokémon video games. The demo was created by the same team who created \"Pokémon Stadium\" for the Nintendo 64.\n\nMeowth is seen dancing with his associates, Team Rocket members Jessie (Musashi in the original Japanese version) and James (Kojiro), and their two Pokémon, Arbok and Weezing (Matadogasu). In both the ending sequence, and the tech demo, he is voiced by Inuko Inuyama, who voices the character in the Japanese version of the anime series (where the character is known as \"Nyarth\"). In the United States, Kids' WB! aired the ending in its original Japanese after one episode, and aired an English dub of the song in the next. The English dub also features Maddie Blaustein as Meowth, Rachel Lillis as Jessie, and Eric Stuart as James, who were voicing their respective characters in the series at the time. Neither ending was ever aired again on the network.\n\nThe animation for both versions was composed in 3-D computer graphics, with the exception of Jessie and James, who, in the ending sequence, remain as 2-D cels in the style of \"Paper Mario\" and \"PaRappa the Rapper\".\n\nMeowth sings the song at his party in the \"Pokémon\" episode titled \"Pichu Bros. in Party Panic\", a special episode only available for viewing on the GameCube game Pokémon Channel. Unlike the rest of the episode, this section uses 3-D computer graphics and Meowth's actions are selected randomly for each viewing.\n\nIn \"Super Smash Bros. Melee\", there is a trophy of Meowth holding a red guitar. The character model is the same as the Meowth from \"Meowth's Party\". The description also mentions \"Meowth's Party\" and even gives a brief explanation about it.\n\nMeowth's Party (ニャースのパーティ \"Nyarth's Party\") is a mini CD single that was released on October 27, 1999 in Japan. The lyrics were by Akihito Toda, the songs were composed and arranged by Hirokazu Tanaka.\n\nAmazon.com has a listing with information on the single but it is usually out of stock on it. Packaged with the single is a small poster with a screenshot from the ending sequence and the lyrics to the songs. There is also a 41 second long fifth track that isn't listed on the packaging.\n\n\n"}
{"id": "57378137", "url": "https://en.wikipedia.org/wiki?curid=57378137", "title": "Ministry of Testing", "text": "Ministry of Testing\n\nMinistry of Testing, also referred to as the MoT, is a global software testing community that was founded by Rosie Sherry, who was longlisted for most influential woman in UK tech by Computer Weekly in 2017. MoT started out as a UK-based internet forum for software testers and quickly grew into an independent business that provides software testing conferences and Meetups around the world, and an online learning platform dedicated to the craft of software testing .\n\nMembers of the Ministry of Testing community consist of software testers and those working in software quality. The community created by Ministry of Testing aims to get its participants sharing innovative practises and ideas around software testing. Computer Scientists at the University of Maryland used Ministry of Testing (along with organisations) to recruit software testers for a study into identifying vulnerabilities in software.\n\nMinistry of Testing’s first conference, named TestBash, was first held in Cambridge. Events have been described as having a strong community atmosphere and using innovative conference engagement methods, such as The UnExpo and 99-Second Talks.\nTestBash software testing conferences are largely informal events, with talks addressing areas of innovation across of testing, quality and working in software development. There are now multiple TestBash conferences taking place in 7 cities annually around the world. In 2018 the first TestBash focusing specifically on technical testing and automation will be held, and named Test.bash();.\n\nThe Dojo is a learning platform dedicated to software testing. MoT co-create all the learning materials and courses on their learning platform with active software testers in their community.\n"}
{"id": "34658245", "url": "https://en.wikipedia.org/wiki?curid=34658245", "title": "Multi-image", "text": "Multi-image\n\nMulti-image is the now largely obsolete practice and business of using 35mm slides (diapositives) projected by single or multiple slide projectors onto one or more screens in synchronization with an audio voice-over or music track. Multi-image productions are also known as multi-image slide presentations, slide shows and diaporamas and are a specific form of multimedia or audio-visual production.\n\nOne of the hallmarks of multi-image was the use of the wide screen panorama. Precisely overlapping slides were placed in slide mounts with soft-edge density masks; when the resulting images were projected, the images would blend seamlessly on the screen to create the panorama. By cutting and dissolving between images in the projectors, animation effects were created in the panorama format.\n\nThe term \"multi-image\" is sometimes used to describe digital photo image computer programs that combine or change images on-screen, for photo montages, and image stitching.\n\nMulti-image presentations were a unique form of communication to audiences of various sizes, to meet a variety of communication and entertainment needs. The use of projected photographic images such as lantern slides for entertainment and instruction dates to the early 1800s. Others, such as L. Frank Baum had a traveling show (1908) that included slides, film, and actors describing the Land of Oz. Throughout the years improvements in technology took place and applications for multi-image continued to expand. During the 1960s, automated synchronized audio and slides modules became more common and found use in instructional environments.\n\nIn general, multi-image can be defined as being:\n\nMulti-image as a business thrived during the 1970s and 1980s. Multi-image presentations ranged from single projector shows run by projector-viewer to large events for business meetings and conventions where multiple shows would be presented and often were rear-projected by 24 or more projectors.\n\nCreating and presenting multi-image productions involved a relatively large number of specialized skills, equipment, and facilities to produce. During the height of multi-image, a number of types of businesses were directly engaged in the industry which employed thousands of specialists that ranged from producers and designers, writers, artists, typesetters, photographers, photo lab technicians, audio technicians, programmers, staging specialists as well as others associated with these disciplines.\nA professional organization, the Association for Multi-image International (AMI), was created and had numerous active chapters around the world. The AMI held an annual convention and multi-image competition. Local chapters of AMI in various cities also held regional competitions.\n\nAn entire industry grew around supplying the tools and equipment needed to supply and support multi-image production.\n\nDriven by changes in technology and by economic considerations, multi-image has been replaced by video presentations and by readily available computer based technologies such as laptop computers running PowerPoint and projecting through digital projectors. Visual presentation and photo and graphics editing software programs have allowed a wider range of communicators quick, flexible, and easy access to the tools and technologies needed to create presentations. Digital photography has reduced the need for laboratory services and complex equipment. The expansion and ease of use of desktop computing brought a close to the multi-image industry.\n\nThe art and business of multi-image drew from many older existing technologies. When a multi-image project was initiated, various overall management roles were required to provide direction, planning, and project management. These roles generally involved the activities similar to those found in other media industries, such as creative, visual, and technical directors, producers, production managers, and writers.\n\nOften individuals with the various production skills would fill these roles as well as performing their production roles. Multi-image productions in general were deadline driven and it was common that the production process would be non-linear, allowing for multiple activities to take place with overlapping roles.\n\nThe visual quality of a presentation was based on using photographs or artwork created for use in the presentation as source material. 35mm slides could be used directly as they were originally taken. Often the original photographs were masked or duplicated for positioning and sizing. In some instances, 35mm cameras modified for pin-registration were used to create animated sequences.\n\nThrough the use of multiple projectors set up to project onto a screen area, over-projected images could be designed as animations. Typical multi-image animation effects included fading from one photograph or graphic to another, progressively building text to form a completed statement, inserting images or graphics into frames or windows on screen, step by step movement of images across the screen area, and superimposing text or images onto a background. The visual effects were synchronized to music or voice and based on the capabilities and limitations of the slide projectors and dissolve units.\n\nArtwork produced for multi-image presentations in general was based on one of two forms of top-lit reflective copy artwork:\n\nFlat art was created by using a variety of standard graphic illustration techniques, using pen and ink, airbrush, paints, from clip art, colored paper, transfer lettering such as Letraset, and by copying from existing materials on a copy stand or on optical slide camera. Cel art, such as found in cel animation, was also used.\n\nArt used in optical slide printing was based on the use of high-contrast photomechanical materials such as photomechanical transfers, black and white type, rubylith and other materials. Phototypesetting was done by a variety of means including the Visual Graphics Corporation PhotoTypositor and the Compugraphic EditWriter at companies that specialized in providing typography. When making artwork for multi-image presentations, photographs, transparencies, and film images or continuous tone film or airbrushed masks could also be used as part of the camera-ready artwork.\n\nThe color or image separated layers of artwork were pin-registered using a variety of standard pin systems including 1/4-inch, Oxberry, and Acme. The artwork was created on a light table or animation disc to provide back lighting.\n\nOften the art was created based on a grid system. A copy of the optical slide camera grid or reticle was used to align the artwork elements. The use of the grid for alignment could be used to accurately position images and art elements throughout the process from the creation of the artwork to the final projected images. This process also allowed layers of the artwork to be interchangeable from slide to slide, images or graphic elements could be carefully placed on the screen relative to other images, and for combining parts of images or graphics by separate slides as in an animated sequence.\n\nThe audio track of a multi-image show provided a framework for the timing of the presentation and for the sequencing and animations of the slides. These were produced generally on -inch audio tape on multi-track tape recorders such as models by Tascam, TEAC, Sony, Fostex and Crown, which allowed for having two tracks or channels for stereo sound and one for the synchronization or click track which was used to encode and playback the signals for the dissolve units. The audio and synchronization tracks were normally separated by a blank track to prevent any carryover of the synchronization cues into the audio playback.\nAudio editing of the music or voice-over was done manually to create a scratch track, usually with a cutting block and tape. Once the audio edits were completed, the final version would be copied onto another tape; either to  inch, cassette or other format so that there tape used to run the presentation would be a fresh uncut tape.\n\nAs productions became more sophisticated, 16- and 24-track recording processes were used to create elaborate soundtracks and four-channel surround sound for large business theatre environments. In productions such as the Maritz-produced car announcement shows for GM and Ford Motor Company, 16-track recorders were used for playback onsite. These 2-inch showtapes would contain extra tracks to support the vocals for a live cast onstage, as well as additional string-section support for the live orchestra and a click track to cue the conductor, in order to maintain synchronization between the cast, orchestra and on-screen visuals.\n\nCompleted artwork was copied under top lights on a pin-registered camera on a high contrast film such as Kodaline or Kodalith to create 35mm mattes used in the optical slide printing process. When needed, pin-registered positive mattes known as countermattes were made by contact printing from the mattes. When a set of mattes and countermates were completed, the optical slide camera was used to assemble the separate images onto a single frame of film by making multiple exposures through the mattes. Accuracy in positioning the separate elements of the slide is made possible by the film sprockets, which were also used to position the finished color slides and masks in the slide mounts.\n\nCopy cameras and optical slide cameras usually had motorized movement of the camera on the column toward and away from the table to allow for resizing the image on the camera on the Z-axis.\n\nFully functional optical slide cameras had compounds for positioning both artwork and the mattes relative to the lens and camera head. A compound could move left to right (horizontal) X-axis and top to bottom (vertical) Y-axis movement as well as rotate (theta-axis). Compounds were often motorized with micro-stepping motors to allow for smooth programmable and repeatable positioning. Higher end optical slide cameras had the ability to create and save programs.\n\nLight sources for copying artwork generally were quartz halogen lights. Backlighting was generally provided by the use a darkroom photographic enlarger colorhead modified for use with the camera. This form of controlled light provide colored light balanced for the color sensitivity of the film used and provided color light for the separate exposures that are made through the mattes.\n\nThe optical slide camera could be used to create a number of types of slides and special effects commonly used in multi-image presentations:\n\nSlides created by these methods and combinations of these methods were made to align and animate when projected, creating what was considered the visual multi-image experience.\n\nFilm processing requires specific facilities, equipment, and skill needed to maintain consistent results. Film processors for developing transparency film require maintaining processing control which was based on good practices in chemical mixing and storage, accurate time, temperature and agitation during the process and the use of control strips. Control strips were run with the film and read on a densitometer to determine their variation from a standard so that corrections could be made\n\nThe use of computer graphics replaced much of the manual activity in creating the artwork and converting the art into slides. Service bureaus offering production work from large workstations such as Dicomed and Genigraphics dominated larger markets. Smaller producers using desktop software such as Photoshop, Persuasion, Harvard Graphics and PowerPoint allowed many slide producers to quickly create slides which were imaged on film recorders such as Management Graphics, Lasergraphics, Polaroid, Celco, and CCG.\n\nCompleted slides were mounted into pin-registered slide mounts. Three or more pieces of film could be mounted into a slide mount which allowed slides to contain the image film chip and masks to allow for inserting and over-projecting. Slides were edited and arranged for programming on light tables.\n\nThere were two basic slide projector programming controls: a set of instructions to position the slide in the projector and a set of instructions for the slide projector lamp. These controls would be used to define the cues. The cues would often designate an action for more than one projector such as with a dissolve between two slides which would require simultaneously fading up on one lamp and fading down another lamp. Similar commands were used to control motion-picture projectors, as well as auxiliary controls for lighting and effects, etc.\n\nThrough synchronizing the audio and the visual effects and combining visual slide effects, the programming process could create a single continuous show that was saved to memory. Once the programming was completed, the most common way to preserve the programming information was to record the programming cues onto the audio track so that the show could be run synchronized with the audio playback. In some situations, data would be contained in the programming computer (stored either on an internal hard disk drive or a floppy disk), and the events would be triggered by a timecode track on the audio tape.\n\nAs multi-image programming devices progressed to digital computers and became more sophisticated in the late 1970s, more programming features were added. Complex looping effects, independent cycling allowing background animation over foreground effects, comprehensive control of motion picture projectors, control of (and by) video devices and other peripheral devices, and the use of SMPTE timecode for synchronization became commonplace. Multiply-exposed optical effects and the use of computer-generated imagery allowed the medium to emerge, briefly, as an art form. The use of multitrack audio playback enhanced the experience and provided for surround sound.\n\nCompleted multi-image productions were presented in a variety of venues ranging from temporary one-on-one settings to semi-permanent world's fair pavilions and museum exhibitions. Large multi-projector multi-image presentations required adequate projection space which was often from behind the screen for rear-screen projection. Setting up large show required control over the room lighting and often involved drapes and scaffolds.\n\nThe actual presentations often were considered to be business theater, incorporating special effects such as pyrotechnics, breakaway screens, live entertainment, even breakaway screens such as having a vehicle crash through the screen to introduce a new model truck.\n\n\n"}
{"id": "21530", "url": "https://en.wikipedia.org/wiki?curid=21530", "title": "Nitroglycerin", "text": "Nitroglycerin\n\nNitroglycerin (NG), also known as nitroglycerine, trinitroglycerin (TNG), nitro, glyceryl trinitrate (GTN), or 1,2,3-trinitroxypropane, is a dense, colorless, oily, explosive liquid most commonly produced by nitrating glycerol with white fuming nitric acid under conditions appropriate to the formation of the nitric acid ester. Chemically, the substance is an organic nitrate compound rather than a nitro compound, yet the traditional name is often retained. Invented in 1847, nitroglycerin has been used as an active ingredient in the manufacture of explosives, mostly dynamite, and as such it is employed in the construction, demolition, and mining industries. Since the 1880s, it has been used by the military as an active ingredient, and a gelatinizer for nitrocellulose, in some solid propellants, such as cordite and ballistite.\n\nNitroglycerin is a major component in double-based smokeless gunpowders used by reloaders. Combined with nitrocellulose, hundreds of powder combinations are used by rifle, pistol, and shotgun reloaders.\n\nIn medicine for over 130 years, nitroglycerin has been used as a potent vasodilator (dilation of the vascular system) to treat heart conditions, such as angina pectoris and chronic heart failure. Though it was previously known that these beneficial effects are due to nitroglycerin being converted to nitric oxide, a potent venodilator, the enzyme for this conversion was not discovered to be mitochondrial aldehyde dehydrogenase (ALDH2) until 2002. Nitroglycerin is available in sublingual tablets, sprays, and patches.\n\nNitroglycerin was the first practical explosive produced that was stronger than black powder. It was first synthesized by the Italian chemist Ascanio Sobrero in 1847, working under Théophile-Jules Pelouze at the University of Turin. Sobrero initially called his discovery \"pyroglycerine\" and warned vigorously against its use as an explosive.\n\nNitroglycerin was later adopted as a commercially useful explosive by Alfred Nobel, who experimented with safer ways to handle the dangerous compound after his younger brother, Emil Oskar Nobel, and several factory workers were killed in an explosion at the Nobels' armaments factory in 1864 in Heleneborg, Sweden.\n\nOne year later, Nobel founded Alfred Nobel and Company in Germany and built an isolated factory in the Krümmel hills of Geesthacht near Hamburg. This business exported a liquid combination of nitroglycerin and gunpowder called \"Blasting Oil\", but this was extremely unstable and difficult to handle, as evidenced in numerous catastrophes. The buildings of the Krümmel factory were destroyed twice.\n\nIn April 1866, three crates of nitroglycerin were shipped to California for the Central Pacific Railroad, which planned to experiment with it as a blasting explosive to expedite the construction of the Summit Tunnel through the Sierra Nevada Mountains. One of the crates exploded, destroying a Wells Fargo company office in San Francisco and killing 15 people. This led to a complete ban on the transportation of liquid nitroglycerin in California. The on-site manufacture of nitroglycerin was thus required for the remaining hard-rock drilling and blasting required for the completion of the First Transcontinental Railroad in North America.\n\nLiquid nitroglycerin was widely banned elsewhere, as well, and these legal restrictions led to Alfred Nobel and his company's developing dynamite in 1867. This was made by mixing nitroglycerin with diatomaceous earth (\"Kieselgur\" in German) found in the Krümmel hills. Similar mixtures, such as \"dualine\" (1867), \"lithofracteur\" (1869), and \"gelignite\" (1875), were formed by mixing nitroglycerin with other inert absorbents, and many combinations were tried by other companies in attempts to get around Nobel's tightly held patents for dynamite.\n\nDynamite mixtures containing nitrocellulose, which increases the viscosity of the mix, are commonly known as \"gelatins\".\n\nFollowing the discovery that amyl nitrite helped alleviate chest pain, the physician William Murrell experimented with the use of nitroglycerin to alleviate angina pectoris and to reduce the blood pressure. He began treating his patients with small diluted doses of nitroglycerin in 1878, and this treatment was soon adopted into widespread use after Murrell published his results in the journal \"The Lancet\" in 1879. A few months before his death in 1896, Alfred Nobel was prescribed nitroglycerin for this heart condition, writing to a friend: \"Isn't it the irony of fate that I have been prescribed nitro-glycerin, to be taken internally! They call it Trinitrin, so as not to scare the chemist and the public.\" The medical establishment also used the name \"glyceryl trinitrate\" for the same reason.\n\nLarge quantities of nitroglycerin were manufactured during World War I and World War II for use as military propellants and in military engineering work. During World War I, HM Factory, Gretna, the largest propellant factory in Britain, produced about 800 tonnes of cordite RDB per week. This amount required at least 336 tonnes of nitroglycerin per week (assuming no losses in production). The Royal Navy had its own factory at the Royal Navy Cordite Factory, Holton Heath, in Dorset, England. A large cordite factory was also built in Canada during World War I. The Canadian Explosives Limited cordite factory at Nobel, Ontario, was designed to produce of cordite per month, requiring about 286 tonnes of nitroglycerin per month.\n\nIn its pure form, nitroglycerin is a contact explosive, with physical shock causing it to explode, and it degrades over time to even more unstable forms. This makes nitroglycerin highly dangerous to transport or use. In its undiluted form, it is one of the world's most powerful explosives, comparable to the more recently developed RDX and PETN.\n\nEarly in its history, liquid nitroglycerin was found to be \"desensitized\" by cooling it to about . At this temperature, nitroglycerin freezes, contracting upon solidification. Thawing it out can be extremely sensitizing, especially if impurities are present or the warming is too rapid. Chemically \"desensitizing\" nitroglycerin is possible to a point where it can be considered about as \"safe\" as modern high explosives, such as by the addition of roughly 10 to 30% ethanol, acetone, or dinitrotoluene. (The percentage varies with the desensitizing agent used.) Desensitization requires extra effort to reconstitute the \"pure\" product. Failing this, desensitized nitroglycerin must be assumed to be substantially more difficult to detonate, possibly rendering it useless as an explosive for practical application.\n\nA serious problem in the use of nitroglycerin results from its high freezing point of . Solid nitroglycerin is much less sensitive to shock than the liquid, a common feature in explosives. In the past, nitroglycerin was often shipped in the frozen state, but this resulted in a high number of accidents during the thawing process just before its use. This disadvantage is overcome by using mixtures of nitroglycerin with other polynitrates. For example, a mixture of nitroglycerin and ethylene glycol dinitrate freezes at .\n\nNitroglycerin and any diluents can certainly deflagrate (burn). The explosive power of nitroglycerin derives from detonation: energy from the initial decomposition causes a strong pressure wave that detonates the surrounding fuel. This is a self-sustained shock wave that propagates through the explosive medium at 30 times the speed of sound as a near-instantaneous pressure-induced decomposition of the fuel into a white-hot gas. Detonation of nitroglycerin generates gases that would occupy more than 1,200 times the original volume at ordinary room temperature and pressure. The heat liberated raises the temperature to about . This is entirely different from deflagration, which depends solely upon available fuel regardless of pressure or shock. The decomposition results in much higher ratio of energy to gas moles released compared to other explosives, making it one of the hottest detonating high explosives.\n\nNitroglycerin can be produced by acid-catalyzed nitration of glycerol (glycerin).\n\nThe industrial manufacturing process often reacts glycerol with a nearly 1:1 mixture of concentrated sulfuric acid and concentrated nitric acid. This can be produced by mixing white fuming nitric acid—a quite expensive pure nitric acid in which the oxides of nitrogen have been removed, as opposed to red fuming nitric acid, which contains nitrogen oxides—and concentrated sulfuric acid. More often, this mixture is attained by the cheaper method of mixing fuming sulfuric acid, also known as oleum—sulfuric acid containing excess sulfur trioxide—and azeotropic nitric acid (consisting of about 70% nitric acid, with the rest being water).\n\nThe sulfuric acid produces protonated nitric acid species, which are attacked by glycerol's nucleophilic oxygen atoms. The nitro group is thus added as an ester C−O−NO and water is produced. This is different from an electrophilic aromatic substitution reaction in which nitronium ions are the electrophile.\n\nThe addition of glycerol results in an exothermic reaction (i.e., heat is produced), as usual for mixed-acid nitrations. If the mixture becomes too hot, it results in a runaway reaction, a state of accelerated nitration accompanied by the destructive oxidation of organic materials by the hot nitric acid and the release of poisonous nitrogen dioxide gas at high risk of an explosion. Thus, the glycerin mixture is added slowly to the reaction vessel containing the mixed acid (not acid to glycerin). The nitrator is cooled with cold water or some other coolant mixture and maintained throughout the glycerin addition at about , much below which the esterification occurs too slowly to be useful. The nitrator vessel, often constructed of iron or lead and generally stirred with compressed air, has an emergency trap door at its base, which hangs over a large pool of very cold water and into which the whole reaction mixture (called the charge) can be dumped to prevent an explosion, a process referred to as drowning. If the temperature of the charge exceeds about (actual value varying by country) or brown fumes are seen in the nitrator's vent, then it is immediately drowned.\n\nThe main use of nitroglycerin, by tonnage, is in explosives such as dynamite and in propellants.\n\nNitroglycerin is an oily liquid that may explode when subjected to heat, shock, or flame.\n\nAlfred Nobel developed the use of nitroglycerin as a blasting explosive by mixing nitroglycerin with inert absorbents, particularly \"Kieselguhr\", or diatomaceous earth. He named this explosive dynamite and patented it in 1867. It was supplied ready for use in the form of sticks, individually wrapped in greased waterproof paper. Dynamite and similar explosives were widely adopted for civil engineering tasks, such as in drilling highway and railroad tunnels, for mining, for clearing farmland of stumps, in quarrying, and in demolition work. Likewise, military engineers have used dynamite for construction and demolition work.\n\nNitroglycerin was also used as an ingredient in military propellants for use in firearms.\n\nNitroglycerin has been used in conjunction with hydraulic fracturing, a process used to recover oil and gas from shale formations. The technique involves displacing and detonating nitroglycerin in natural or hydraulically induced fracture systems, or displacing and detonating nitroglycerin in hydraulically induced fractures followed by wellbore shots using pelletized TNT.\n\nNitroglycerin has an advantage over some other high explosives that on detonation it produces practically no visible smoke. Therefore, it is useful as an ingredient in the formulation of various kinds of smokeless powder.\n\nIts sensitivity has limited the usefulness of nitroglycerin as a military explosive, and less sensitive explosives such as TNT, RDX, and HMX have largely replaced it in munitions. It remains important in military engineering, and combat engineers still use dynamite.\n\nAlfred Nobel then developed ballistite, by combining nitroglycerin and guncotton. He patented it in 1887. Ballistite was adopted by a number of European governments, as a military propellant. Italy was the first to adopt it. The British government and the Commonwealth governments adopted cordite instead, which had been developed by Sir Frederick Abel and Sir James Dewar of the United Kingdom in 1889. The original Cordite Mk I consisted of 58% nitroglycerin, 37% guncotton, and 5.0% petroleum jelly. Ballistite and cordite were both manufactured in the forms of \"cords\".\n\nSmokeless powders were originally developed using nitrocellulose as the sole explosive ingredient. Therefore, they were known as single-base propellants. A range of smokeless powders that contains both nitrocellulose and nitroglycerin, known as double-base propellants, were also developed. Smokeless powders were originally supplied only for military use, but they were also soon developed for civilian use and were quickly adopted for sports. Some are known as sporting powders. Triple-base propellants contain nitrocellulose, nitroglycerin, and nitroguanidine, but are reserved mainly for extremely high-caliber ammunition rounds such as those used in tank cannons and naval artillery.\n\nBlasting gelatin, also known as gelignite, was invented by Nobel in 1875, using nitroglycerin, wood pulp, and sodium or potassium nitrate. This was an early, low-cost, flexible explosive.\n\nNitroglycerin belongs to a group of drugs called nitrates, which includes many other nitrates like isosorbide dinitrate (Isordil) and isosorbide mononitrate (Imdur, Ismo, Monoket). These agents all exert their effect by being converted to nitric oxide in the body by mitochondrial aldehyde dehydrogenase (ALDH2), and nitric oxide is a potent natural vasodilator.\n\nIn medicine, nitroglycerin is used for angina pectoris, a painful symptom of ischemic heart disease caused by inadequate flow of blood and oxygen to the heart and as a potent antihypertensive agent. Nitroglycerin corrects the imbalance between the flow of oxygen and blood to the heart. At low doses, nitroglycerin dilates veins more than arteries, thereby reducing preload (volume of blood in the heart after filling); this is thought to be its primary mechanism of action. By decreasing preload, the heart has less blood to pump, which decreases oxygen requirement since the heart does not have to work as hard. Additionally, having a smaller preload reduces the ventricular transmural pressure (pressure exerted on the walls of the heart), which decreases the compression of heart arteries to allow more blood to flow through the heart. At higher doses, it also dilates arteries, thereby reducing afterload (decreasing the pressure against which the heart must pump). Improved myocardial oxygen demand vs oxygen delivery ratio leads to the following therapeutic effects during episodes of angina pectoris: subsiding of chest pain, decrease of blood pressure, increase of heart rate, and orthostatic hypotension. Patients experiencing angina when doing certain physical activities can often prevent symptoms by taking nitroglycerin 5 to 10 minutes before the activity. Overdoses may generate methemoglobinemia.\n\nNitroglycerin is available in tablets, ointment, solution for intravenous use, transdermal patches, or sprays administered sublingually. Some forms of nitroglycerin last much longer in the body than others. Continuous exposure to nitrates has been shown to cause the body to stop responding normally to this medicine. Experts recommend that the patches be removed at night, allowing the body a few hours to restore its responsiveness to nitrates. Shorter-acting preparations of nitroglycerin can be used several times a day with less risk of developing tolerance. Nitroglycerin was first used by William Murrell to treat angina attacks in 1878, with the discovery published that same year.\n\nInfrequent exposure to high doses of nitroglycerin can cause severe headaches known as \"NG head\" or \"bang head\". These headaches can be severe enough to incapacitate some people; however, humans develop a tolerance to and dependence on nitroglycerin after long-term exposure. Withdrawal can (rarely) be fatal; withdrawal symptoms include chest pain and heart problems and if unacceptable may be treated with re-exposure to nitroglycerin or other suitable organic nitrates.\n\nFor workers in nitroglycerin (NTG) manufacturing facilities, the effects of withdrawal sometimes include \"Sunday heart attacks\" in those experiencing regular nitroglycerin exposure in the workplace, leading to the development of tolerance for the venodilating effects. Over the weekend, the workers lose the tolerance, and when they are re-exposed on Monday, the drastic vasodilation produces a fast heart rate, dizziness, and a headache, this is referred to as \"Monday disease.\"\n\nPeople can be exposed to nitroglycerin in the workplace by breathing it in, skin absorption, swallowing it, or eye contact. The Occupational Safety and Health Administration has set the legal limit (permissible exposure limit) for nitroglycerin exposure in the workplace as 0.2 ppm (2 mg/m) skin exposure over an 8-hour workday. The National Institute for Occupational Safety and Health has set a recommended exposure limit of 0.1 mg/m skin exposure over an 8-hour workday. At levels of 75 mg/m, nitroglycerin is immediately dangerous to life and health.\n\n"}
{"id": "4155979", "url": "https://en.wikipedia.org/wiki?curid=4155979", "title": "Orangi Pilot Project", "text": "Orangi Pilot Project\n\nThe Orangi Pilot Project (; abbreviated OPP) collectively designates three Pakistani Non-governmental organisations working together, having emerged from a socially innovative project carried out in 1980s in the squatter areas of Orangi Town, Karachi, Pakistan. It was initiated by Akhtar Hameed Khan, and involved the local residents solving their own sanitation problems. Innovative methods were used to provide adequate low cost sanitation, health, housing and microfinance facilities.\n\nCurrently OPP designates three organisations, borne out of the original OPP in 1989 : OPP-RTI (Research and Training Institute), OPP-OCT (Orangi Charitable Trust, involved in microfinance) and OPP-KHASDA (Karachi Health and Social Development Association, involved in health activities). A fourth organisation, OPP-RDT (Rural Development Trust) was merged with OPP-RTI in 2012.\n\nThe project also comprised a number of programs, including a people's financed and managed Low-Cost Sanitation Program; a Housing Program; a Basic Health and Family Planning Program; a Program of Supervised Credit for Small Family Enterprise Units; an education Program; and a Rural development Program in the nearby villages.\n\nToday, the project encompasses much more than the neighbourhood level problems. The research and development programmes under the institutions developed by the project now covers wider issues related to the areas all over Karachi.\n\nIts director until 2013 was Perween Rahman, who was murdered on 13 March 2013.\n\nOrangi was a squatter community, and did not qualify for government aid due to their \"unofficial\" status. With endogenous research, the community was able to make an affordable sanitation system for the treatment of sewage, which helped to reduce the spread of disease. The system was created and paid for by the local community, who would not have had access to a sewer system otherwise.\n\nThe programme proved so successful that it was adopted by the communities across developing countries. After the success of the initial phase, the program was expanded into four autonomous groups.\n\nDr Akhtar Hameed Khan (1914–1999) was the founder and first Director of the project, and through his dynamic and innovative skills managed to bring modern sanitation to the squatter community of 1 million people. He had previously organised farmers' cooperatives and rural training centres and had served as an adviser to various development projects in Pakistan.\n\nComparing the OPP with his earlier Comilla project, Akhtar Hameed Khan commented: \n\nHowever, both projects followed the same research and extension methods.\n\n\"OPP-OCT discovered that this growing settlement of Orangi was full of the enterprising spirit. The most impressive demonstration of the spirit of enterprises is the creation of employment everywhere in the lanes; inside the homes there are around twenty thousand family units, shops workshops, peddlers and vendors. In response to the dual challenge of inflation and recession, the residents have invented working family, modifying homes into workshops, promoting the women from mere dependents to economic partners and wage earners, abandoning the dominant patriarchal pattern with surprising speed.\n\nOPP’s research revealed two significant factors; first, there was unlimited demand for products and services of these family units. Second, the family units were extremely competitive (on account of very low over heads and very cheap and docile labour). The working family units of Orangi were completely integrated with the main Karachi markets. In fact many units are supplying goods to famous firms, who just put their labels and make big profits. What is required is to support their initiatives.\n\nResearch further revealed that the production and employment in urban as well as rural areas could easily be increased provided the credit is accessible, as there was no shortage of market demand or productive labour. But they would not get credit at reasonable rate, because banks were inaccessible to them. The lack of bank credit forced them to buy raw materials at exorbitant prices while they had to sell their products at depressed prices and forego expansion.\n\nOn the basis of the research findings, Orangi Pilot Project (OPP) decided to arrange access to credit to these micro enterprises. For this Orangi Pilot Project – Orangi Charitable Trust (OPP – OCT) was established in 1989 as an independent and autonomous institution in Orangi, a low income settlement of over one million people. The main objective is to support people effort in their economic development by providing credit in urban and rural areas.\"\n\n\n\n"}
{"id": "30058253", "url": "https://en.wikipedia.org/wiki?curid=30058253", "title": "Oriental Powder Company", "text": "Oriental Powder Company\n\nOriental Powder Company was a gunpowder manufacturer with mills located on the Presumpscot River in Gorham and Windham, Maine. The company was one of the four largest suppliers to Union forces through the American Civil War.\n\nThe Presumpscot River dropped 16 feet at Gambo Falls where the river formed the border between the towns of Windham and Gorham. Sebago Lake formed a large natural reservoir upstream of Gambo giving the falls an unusually reliable water supply with comparatively minor flow peaking from storm runoff. Early European settlers built a sawmill powered by the falls. In 1824 the sawmill was converted to gunpowder manufacture by Edmund Fowler and Lester Laflin. Their Gorham Powder Company became known among the local population as the Gambo powder mills. Lester Laflin was a grandson of American Revolutionary War gunpowder manufacturer Matthew Laflin. The Laflin family manufactured gunpowder in Massachusetts for several generations. When Lester came east to Maine, his first cousins traveled west to build gunpowder mills in New York and Chicago. Lester, his partner, and their mill foreman drowned on Sebago Lake on 22 June 1827.\nFollowing an explosion killing seven employees on 19 July 1828, the Gambo Falls mill was enlarged by Oliver Whipple concurrently with construction of locks for the Cumberland and Oxford Canal. Whipple had been manufacturing gunpowder in Lowell, Massachusetts since 1818. The new canal provided reliable transportation from Portland harbor for sulfur from Sicily and saltpeter from India and from Sebago Lake for charcoal and lumber from forests to the north. Whipple's Gambo mill used the lumber to manufacture kegs holding as much as 25 pounds of powder. Kegs of gunpowder were shipped to Portland in canal boats when possible, but moved in horse-drawn sleighs when the canal was frozen. Canal boats carried about 25 tons, and sometimes sailed all the way to Boston when weather was favorable.\n\nAfter plant explosions killed one employee each in 1835, 1847, 1849, 1850, and 1851, a major explosion on 12 October 1855 killed seven employees, including Whipple's brother and son, injured five more and destroyed a canal boat and parts of the mill. Manufacture of gunpowder in response to orders avoided the hazard of storing powder inventories until orders were received, but required water power on demand. The canal lock system controlled outflow from Sebago Lake; and, as a shipper interested in the well-being of its customer, canal management was receptive to regulating water releases to meet needs of the powder mill.\n\nGilbert Grafton Newhall of Salem, Massachusetts, purchased the property in early 1855 to manufacture powder for Crimean War belligerents, and organized Oriental Powder Company to repair the damage and construct new facilities. A charcoal house, saltpeter refinery, wheel mills, press mills, kernelling mills, glazing mills, and storehouses were dispersed along both banks of the river and canal for a mile upstream of Gambo to minimize damage during infrequent explosions. Charcoal was manufactured from dried, debarked alder packed into cast iron retorts. Charcoal was made from willow, poplar or maple when alder was unavailable. Crude saltpeter was dissolved in hot water in kettles holding as much as . Impurities were skimmed from the surface or settled to the bottom so a hot, saturated solution could be decanted for crystallization. Moist saltpeter crystals were mixed with appropriate amounts of sulfur and charcoal by heavy rotating wheels to form a cake which was then cut unto smaller pieces in bronze- or zinc-toothed kernelling mills. The kernels were sieved into desired sizes and dried prior to being tumbled with graphite which reduced tendency for the finished grains to stick together during storage. Intermediate products were transported between the dispersed production and storage facilities in wooden wheelbarrows constructed with no iron parts and pushed over plank walkways by workmen going barefoot in the summer or wearing shoes without iron nails during colder weather. Newhall gave his name to a small company town of employee residences built near Gambo Falls. Company offices were in Portland, Maine to avoid business disruption by the periodic explosions at the manufacturing facilities.\n\nBy 1860, Oriental was one of four companies making 69% of United States gunpowder sales. The larger DuPont and Hazard Powder Company mills each provided approximately one-third of the Union gunpowder supply for the civil war. The fourth major supplier was Lester Laflin's cousins' mill which later evolved into the early smokeless powder manufacturer Laflin & Rand. The DuPont mill was uncomfortably close to the battle line and considered potentially vulnerable to sabotage from southern sympathizers in the slave state of Delaware.\n\nThe federal government purchased as much powder as Oriental Powder Company could produce through the war years. Wartime production included large cast hexagonal powder grains for Rodman guns; and an Columbiad was installed at Gambo to test this specialized powder. Production increased to 1250 tons per year as accidental explosions killed one employee on 8 July 1861, three on 7 July 1862, and another on 14 November 1863. Demand for gunpowder declined when the civil war ended, but picked up briefly during the Franco-Prussian War and Russo-Turkish War. Oriental Powder Company was ranked 4th (after DuPont, Laflin & Rand, and Hazard) among the six companies of the United States Gunpowder Trade Association popularly known as the powder trust.\n\nCanal boats were unable to compete with rail service and the canal was unused after the Portland and Ogdensburg Railway reached Gambo Falls (by then called Newhall) in 1871. Oriental Powder Company employees assumed control of the former canal dam at Sebago Lake when the canal locks ceased operations; but water users in Westbrook, Maine, were unsatisfied with timing of water releases. Court action was initiated in January 1877, following a drought. Legal maneuvering continued for several months between Oriental Powder Company and Westbrook water users while water releases were controlled by whichever side mustered a larger number of employees at the dam. Legislation enacted in February 1878 effectively passed control of water releases from Sebago Lake to water users in Westbrook.\n\nAs smaller Maine gunpowder mills went out of business following the civil war, Oriental Powder Company acquired the assets of the North Buckfield Powder Mill in 1880, the Warren Powder Mill in 1887, and the Camden Powder Mill in 1892. Machinery was salvaged from these mills, and a subsidiary Oriental Powder Company of Pennsylvania began manufacturing rock blasting powder at Fairchance, Pennsylvania in 1902. As telegraph became available for sales communications, the powder company reduced the risk to manufacturing facilities by building magazines for powder inventories near distant rail distribution centers like Chicago and Salt Lake City.\n\nLocal supplies of charcoal and keg staves became more expensive as forest resources were exhausted. The rock blasting powder market for mining and railroad construction fluctuated with financial panics. The Newhall mill ceased operations on 1 June 1893 as smokeless powder and dynamite became preferred for traditional uses of gunpowder. Eastern Dynamite Company was formed in 1895 and began manufacturing dynamite at Newhall. Four employees died in three fatal explosions through the final decade of explosives manufacture. March 19, 1904, was the last of 32 recorded blasts claiming 46 lives along the river. Mill operations after the 1904 explosion were limited to manufacturing wood flour shipped elsewhere for mixing with nitroglycerine to form dynamite. Production of wood flour continued into the 1950s after ownership passed to the Atlas Powder Company in 1912. The civil war Columbiad remained at Newhall until scrapped during World War II.\n"}
{"id": "28002580", "url": "https://en.wikipedia.org/wiki?curid=28002580", "title": "Peter Schwartze", "text": "Peter Schwartze\n\nPeter Heinrich Schwartze (born on 23 May 1931 in Bad Salzuflen) is a German neurophysiologist, systems scientist and cyberneticist well known in the ex-German Democratic Republic. \nSchwartze graduated the medical University in Leipzig, Germany in 1957 and specialized in Neurophysiology. He studied and worked at the universities of Rostock, Greifswald and Leipzig. He became Doctor Habilitatus of the University Leipzig, Germany in 1968 and Professor of Pathophysiology in 1978 and served as the Director of the Carl Ludwig Institute of Physiology between 1980 and 1992 as successor of .\n\nProf. Schwartze also served as member of the East German Parliament between 1980-1990, in the Cultural Association fraction.\n\nSchwartze studied the vestibular apparatus, the air-righting reflex and related spinal reflexes for over 30 years. He published hundreds of scientific reports (mostly in German journals) and a number of scientific and text books on issues of brain development, vestibulo-ocular reflexes and cybernetics.) He served as vice president of the Society for Experimental Research between 1978 and 81 and also served as member of the editorial board of many journals such as Pediatrics and Related Topics Journal and International Tinnitus Journal.\n\n"}
{"id": "2097807", "url": "https://en.wikipedia.org/wiki?curid=2097807", "title": "Pitot-static system", "text": "Pitot-static system\n\nA pitot-static system is a system of pressure-sensitive instruments that is most often used in aviation to determine an aircraft's airspeed, Mach number, altitude, and altitude trend. A pitot-static system generally consists of a pitot tube, a static port, and the pitot-static instruments. Other instruments that might be connected are air data computers, flight data recorders, altitude encoders, cabin pressurization controllers, and various airspeed switches. Errors in pitot-static system readings can be extremely dangerous as the information obtained from the pitot static system, such as altitude, is potentially safety-critical. Several commercial airline disasters have been traced to a failure of the pitot-static system.\nThe pitot-static system of instruments uses the principle of air pressure gradient. It works by measuring pressures or pressure differences and using these values to assess the speed and altitude. These pressures can be measured either from the static port (static pressure) or the pitot tube (pitot pressure). The static pressure is used in all measurements, while the pitot pressure is used only to determine airspeed.\n\nThe pitot pressure is obtained from the pitot tube. The pitot pressure is a measure of ram air pressure (the air pressure created by vehicle motion or the air ramming into the tube), which, under ideal conditions, is equal to stagnation pressure, also called total pressure. The pitot tube is most often located on the wing or front section of an aircraft, facing forward, where its opening is exposed to the relative wind. By situating the pitot tube in such a location, the ram air pressure is more accurately measured since it will be less distorted by the aircraft's structure. When airspeed increases, the ram air pressure is increased, which can be translated by the airspeed indicator.\n\nThe static pressure is obtained through a static port. The static port is most often a flush-mounted hole on the fuselage of an aircraft, and is located where it can access the air flow in a relatively undisturbed area. Some aircraft may have a single static port, while others may have more than one. In situations where an aircraft has more than one static port, there is usually one located on each side of the fuselage. With this positioning, an average pressure can be taken, which allows for more accurate readings in specific flight situations. An alternative static port may be located inside the cabin of the aircraft as a backup for when the external static port(s) are blocked. A pitot-static tube effectively integrates the static ports into the pitot probe. It incorporates a second coaxial tube (or tubes) with pressure sampling holes on the sides of the probe, outside the direct airflow, to measure the static pressure. When the aircraft climbs, static pressure will decrease.\n\nSome pitot-static systems incorporate single probes that contain multiple pressure-transmitting ports that allow for the sensing of air pressure, angle of attack, and angle of sideslip data. Depending on the design, such air data probes may be referred to as 5-hole or 7-hole air data probes. Differential pressure sensing techniques can be used to produce angle of attack and angle of sideslip indications.\n\nThe pitot-static system obtains pressures for interpretation by the pitot-static instruments. While the explanations below explain traditional, mechanical instruments, many modern aircraft use an air data computer (ADC) to calculate airspeed, rate of climb, altitude and Mach number. In some aircraft, two ADCs receive total and static pressure from independent pitot tubes and static ports, and the aircraft's flight data computer compares the information from both computers and checks one against the other. There are also \"standby instruments\", which are back-up pneumatic instruments employed in the case of problems with the primary instruments.\n\nThe airspeed indicator is connected to both the pitot and static pressure sources. The difference between the pitot pressure and the static pressure is called dynamic pressure. The greater the dynamic pressure, the higher the airspeed reported. A traditional mechanical airspeed indicator contains a pressure diaphragm that is connected to the pitot tube. The case around the diaphragm is airtight and is vented to the static port. The higher the speed, the higher the ram pressure, the more pressure exerted on the diaphragm, and the larger the needle movement through the mechanical linkage.\n\nThe pressure altimeter, also known as the barometric altimeter, is used to determine changes in air pressure that occur as the aircraft's altitude changes. Pressure altimeters must be calibrated prior to flight to register the pressure as an altitude above sea level. The instrument case of the altimeter is airtight and has a vent to the static port. Inside the instrument, there is a sealed aneroid barometer. As pressure in the case decreases, the internal barometer expands, which is mechanically translated into a determination of altitude. The reverse is true when descending from higher to lower altitudes.\n\nAircraft designed to operate at transonic or supersonic speeds will incorporate a machmeter. The machmeter is used to show the ratio of true airspeed in relation to the speed of sound. Most supersonic aircraft are limited as to the maximum Mach number they can fly, which is known as the \"Mach limit\". The Mach number is displayed on a machmeter as a decimal fraction.\n\nThe variometer, also known as the vertical speed indicator (VSI) or the vertical velocity indicator (VVI), is the pitot-static instrument used to determine whether or not an aircraft is flying in level flight. The vertical speed specifically shows the rate of climb or the rate of descent, which is measured in feet per minute or meters per second. The vertical speed is measured through a mechanical linkage to a diaphragm located within the instrument. The area surrounding the diaphragm is vented to the static port through a calibrated leak (which also may be known as a \"restricted diffuser\"). When the aircraft begins to increase altitude, the diaphragm will begin to contract at a rate faster than that of the calibrated leak, causing the needle to show a positive vertical speed. The reverse of this situation is true when an aircraft is descending. The calibrated leak varies from model to model, but the average time for the diaphragm to equalize pressure is between 6 and 9 seconds.\n\nThere are several situations that can affect the accuracy of the pitot-static instruments. Some of these involve failures of the pitot-static system itself—which may be classified as \"system malfunctions\"—while others are the result of faulty instrument placement or other environmental factors—which may be classified as \"inherent errors\".\n\nA blocked pitot tube is a pitot-static problem that will only affect airspeed indicators. A blocked pitot tube will cause the airspeed indicator to register an increase in airspeed when the aircraft climbs, even though actual airspeed is constant. (As long as the drain hole is also blocked, as the air pressure would otherwise leak out to the atmosphere.) This is caused by the pressure in the pitot system remaining constant when the atmospheric pressure (and static pressure) are decreasing. In reverse, the airspeed indicator will show a decrease in airspeed when the aircraft descends. The pitot tube is susceptible to becoming clogged by ice, water, insects or some other obstruction. For this reason, aviation regulatory agencies such as the U.S. Federal Aviation Administration (FAA) recommend that the pitot tube be checked for obstructions prior to any flight. To prevent icing, many pitot tubes are equipped with a heating element. A heated pitot tube is required in all aircraft certificated for instrument flight except aircraft certificated as Experimental Amateur-Built.\n\nA blocked static port is a more serious situation because it affects all pitot-static instruments. One of the most common causes of a blocked static port is airframe icing. A blocked static port will cause the altimeter to freeze at a constant value, the altitude at which the static port became blocked. The vertical speed indicator will read zero and will not change at all, even if vertical speed increases or decreases. The airspeed indicator will reverse the error that occurs with a clogged pitot tube and cause the airspeed to be read less than it actually is as the aircraft climbs. When the aircraft is descending, the airspeed will be over-reported. In most aircraft with unpressurized cabins, an alternative static source is available and can be selected from within the cockpit.\n\nInherent errors may fall into several categories, each affecting different instruments. \"Density errors\" affect instruments metering airspeed and altitude. This type of error is caused by variations of pressure and temperature in the atmosphere. A \"compressibility error\" can arise because the impact pressure will cause the air to compress in the pitot tube. At standard sea level pressure altitude the calibration equation (see calibrated airspeed) correctly accounts for the compression so there is no compressibility error at sea level. At higher altitudes the compression is not correctly accounted for and will cause the instrument to read greater than equivalent airspeed. A correction may be obtained from a chart. Compressibility error becomes significant at altitudes above and at airspeeds greater than . \"Hysteresis\" is an error that is caused by mechanical properties of the aneroid capsules located within the instruments. These capsules, used to determine pressure differences, have physical properties that resist change by retaining a given shape, even though the external forces may have changed. \"Reversal errors\" are caused by a false static pressure reading. This false reading may be caused by abnormally large changes in an aircraft's pitch. A large change in pitch will cause a momentary showing of movement in the opposite direction. Reversal errors primarily affect altimeters and vertical speed indicators.\n\nAnother class of inherent errors is that of position error. A position error is produced by the aircraft's static pressure being different from the air pressure remote from the aircraft. This error is caused by the air flowing past the static port at a speed different from the aircraft's true airspeed. Position errors may provide positive or negative errors, depending on one of several factors. These factors include airspeed, angle of attack, aircraft weight, acceleration, aircraft configuration, and in the case of helicopters, rotor downwash. There are two categories of position errors, which are \"fixed errors\" and \"variable errors\". Fixed errors are defined as errors which are specific to a particular model of aircraft. Variable errors are caused by external factors such as deformed panels obstructing the flow of air, or particular situations which may overstress the aircraft.\n\nLag errors are caused by the fact that any changes in the static or dynamic pressure outside the aircraft require a finite amount of time to make their way down the tubing and affect the gauges. This type of error depends on the length and diameter of the tubing as well as the volume inside the gauges. Lag error is only significant around the time when the airspeed or altitude are changing. It is not a concern for steady level flight.\n\n\n\n\n"}
{"id": "24985139", "url": "https://en.wikipedia.org/wiki?curid=24985139", "title": "Remote Sensing Systems", "text": "Remote Sensing Systems\n\nRemote Sensing Systems (RSS) is a private research company founded in 1974 by Frank Wentz. It processes microwave data from a variety of NASA satellites. Most of their research is supported by the Earth Science Enterprise program. The company is based in Santa Rosa, California.\n\nThey are a widely cited source of data, on the satellite temperature record. Their data is one source of evidence for global warming. Research by Carl Mears, Matthias Schabel, and Wentz, all of RSS, highlighted errors in the early satellite temperature records compiled by John Christy and Roy Spencer at UAH. The UAH data had previously showed no significant temperature trend, bringing the derived satellite data into closer agreement with surface temperature trends, radiosonde data and computer models. The UAH data is now closer to the RSS data but differences remain, for example the Lower Troposphere global average trend since 1979, RSS currently have +0.133K/decade while UAH have 0.140K/decade, while the mid troposphere difference is even more marked at 0.079K/decade and 0.052K/decade respectively. However, in a recent online YouTube video, Dr. Carl Mears, a senior scientist with the team behind the satellite data, explained how he believes his data set needed correction. \n\nI would have to say that the surface data seems that it’s more accurate, because a number of groups analyze the surface data, including some who set out to prove the other ones wrong, and they all get more or less the same answer.\nIn June 2017, version 4 of the TLT was released and this substantially revised upwards the trend from 1979 by 36% from .135K per decade to .184K per decade.\n\nhttp://www.nature.com/nature/journal/v394/n6694/abs/394661a0.html Wentz, Frank J., and Matthias C. Schabel. \"Effects of orbital decay on satellite-derived lower-tropospheric temperature trends.\" Nature 394.6694 (1998): 661-664. \nhttp://www.nature.com/nature/journal/v403/n6768/abs/403414a0.html Wentz, Frank J., and Matthias C. Schabel. \"Precise climate monitoring using complementary satellite data sets.\" Nature 403.6768 (2000): 414-416.\n\n"}
{"id": "24134222", "url": "https://en.wikipedia.org/wiki?curid=24134222", "title": "Ridge-post framing", "text": "Ridge-post framing\n\nRidge-post framing is an old type of timber framing.\nThe ridge board of their roof is not carried by king posts based on tie beams, but the ridge posts are based on the ground work. The German term for this construction is \"Firstständerhaus\". The free-standing posts in the interior of the house and the posts in the gable or lateral walls were originally called \"Firstsäule\" (ridge columns). On a purlin roof the ridge posts carry the ridge purlin. On the latter are hung the sloping rafters to which the roof is fixed. This type of \"Firstständerhaus\" was predominantly built around the 15th century in Baden region.\n\n"}
{"id": "12321977", "url": "https://en.wikipedia.org/wiki?curid=12321977", "title": "Shale oil extraction", "text": "Shale oil extraction\n\nShale oil extraction is an industrial process for unconventional oil production. This process converts kerogen in oil shale into shale oil by pyrolysis, hydrogenation, or thermal dissolution. The resultant shale oil is used as fuel oil or upgraded to meet refinery feedstock specifications by adding hydrogen and removing sulfur and nitrogen impurities.\n\nShale oil extraction is usually performed above ground (\"ex situ\" processing) by mining the oil shale and then treating it in processing facilities. Other modern technologies perform the processing underground (on-site or \"in situ\" processing) by applying heat and extracting the oil via oil wells.\n\nThe earliest description of the process dates to the 10th century. In 1684, Great Britain granted the first formal extraction process patent. Extraction industries and innovations became widespread during the 19th century. The industry shrank in the mid-20th century following the discovery of large reserves of conventional oil, but high petroleum prices at the beginning of the 21st century have led to renewed interest, accompanied by the development and testing of newer technologies.\n\nAs of 2010, major long-standing extraction industries are operating in Estonia, Brazil, and China. Its economic viability usually requires a lack of locally available crude oil. National energy security issues have also played a role in its development. Critics of shale oil extraction pose questions about environmental management issues, such as waste disposal, extensive water use, waste water management, and air pollution.\n\nIn the 10th century, the Arabian physician Masawaih al-Mardini (Mesue the Younger) wrote of his experiments in extracting oil from \"some kind of bituminous shale\". The first shale oil extraction patent was granted by the British Crown in 1684 to three people who had \"found a way to extract and make great quantities of pitch, tarr, and oyle out of a sort of stone\". Modern industrial extraction of shale oil originated in France with the implementation of a process invented by Alexander Selligue in 1838, improved upon a decade later in Scotland using a process invented by James Young. During the late 19th century, plants were built in Australia, Brazil, Canada, and the United States. The 1894 invention of the Pumpherston retort, which was much less reliant on coal heat than its predecessors, marked the separation of the oil shale industry from the coal industry.\n\nChina (Manchuria), Estonia, New Zealand, South Africa, Spain, Sweden, and Switzerland began extracting shale oil in the early 20th century. However, crude oil discoveries in Texas during the 1920s and in the Middle East in the mid 20th century brought most oil shale industries to a halt. In 1944, the US recommenced shale oil extraction as part of its Synthetic Liquid Fuels Program. These industries continued until oil prices fell sharply in the 1980s. The last oil shale retort in the US, operated by Unocal Corporation, closed in 1991. The US program was restarted in 2003, followed by a commercial leasing program in 2005 permitting the extraction of oil shale and oil sands on federal lands in accordance with the Energy Policy Act of 2005.\n\n, shale oil extraction is in operation in Estonia, Brazil, and China. In 2008, their industries produced about 930,000 metric tonnes (17,700 barrels per day) of shale oil. Australia, the US, and Canada have tested shale oil extraction techniques via demonstration projects and are planning commercial implementation; Morocco and Jordan have announced their intent to do the same. Only four processes are in commercial use: Kiviter, Galoter, Fushun, and Petrosix.\n\nShale oil extraction process decomposes oil shale and converts its kerogen into shale oil—a petroleum-like synthetic crude oil. The process is conducted by pyrolysis, hydrogenation, or thermal dissolution. The efficiencies of extraction processes are often evaluated by comparing their yields to the results of a Fischer Assay performed on a sample of the shale.\n\nThe oldest and the most common extraction method involves pyrolysis (also known as \"retorting\" or destructive distillation). In this process, oil shale is heated in the absence of oxygen until its kerogen decomposes into condensable shale oil vapors and non-condensable combustible oil shale gas. Oil vapors and oil shale gas are then collected and cooled, causing the shale oil to condense. In addition, oil shale processing produces spent oil shale, which is a solid residue. Spent shale consists of inorganic compounds (minerals) and char—a carbonaceous residue formed from kerogen. Burning the char off the spent shale produces oil shale ash. Spent shale and shale ash can be used as ingredients in cement or brick manufacture. The composition of the oil shale may lend added value to the extraction process through the recovery of by-products, including ammonia, sulfur, aromatic compounds, pitch, asphalt, and waxes.\n\nHeating the oil shale to pyrolysis temperature and completing the endothermic kerogen decomposition reactions require a source of energy. Some technologies burn other fossil fuels such as natural gas, oil, or coal to generate this heat and experimental methods have used electricity, radio waves, microwaves, or reactive fluids for this purpose. Two strategies are used to reduce, and even eliminate, external heat energy requirements: the oil shale gas and char by-products generated by pyrolysis may be burned as a source of energy, and the heat contained in hot spent oil shale and oil shale ash may be used to pre-heat the raw oil shale.\n\nFor \"ex situ\" processing, oil shale is crushed into smaller pieces, increasing surface area for better extraction. The temperature at which decomposition of oil shale occurs depends on the time-scale of the process. In \"ex situ\" retorting processes, it begins at and proceeds more rapidly and completely at higher temperatures. The amount of oil produced is the highest when the temperature ranges between . The ratio of oil shale gas to shale oil generally increases along with retorting temperatures. For a modern \"in situ\" process, which might take several months of heating, decomposition may be conducted at temperatures as low as . Temperatures below are preferable, as this prevents the decomposition of lime stone and dolomite in the rock and thereby limits carbon dioxide emissions and energy consumption.\n\nHydrogenation and thermal dissolution (reactive fluid processes) extract the oil using hydrogen donors, solvents, or a combination of these. Thermal dissolution involves the application of solvents at elevated temperatures and pressures, increasing oil output by cracking the dissolved organic matter. Different methods produce shale oil with different properties.\n\nIndustry analysts have created several classifications of the technologies used to extract shale oil from oil shale.\n\n\"By process principles\": Based on the treatment of raw oil shale by heat and solvents the methods are classified as pyrolysis, hydrogenation, or thermal dissolution.\n\n\"By location\": A frequently used distinction considers whether processing is done above or below ground, and classifies the technologies broadly as \"ex situ\" (displaced) or \"in situ\" (in place). In \"ex situ\" processing, also known as above-ground retorting, the oil shale is mined either underground or at the surface and then transported to a processing facility. In contrast, \"in situ\" processing converts the kerogen while it is still in the form of an oil shale deposit, following which it is then extracted via oil wells, where it rises in the same way as conventional crude oil. Unlike \"ex situ\" processing, it does not involve mining or spent oil shale disposal aboveground as spent oil shale stays underground.\n\n\"By heating method\": The method of transferring heat from combustion products to the oil shale may be classified as direct or indirect. While methods that allow combustion products to contact the oil shale within the retort are classified as \"direct\", methods that burn materials external to the retort to heat another material that contacts the oil shale are described as \"indirect\"\n\n\"By heat carrier\": Based on the material used to deliver heat energy to the oil shale, processing technologies have been classified into gas heat carrier, solid heat carrier, wall conduction, reactive fluid, and volumetric heating methods. Heat carrier methods can be sub-classified as direct or indirect.\n\nThe following table shows extraction technologies classified by heating method, heat carrier and location (\"in situ\" or \"ex situ\").\n\n\"By raw oil shale particle size\": The various \"ex situ\" processing technologies may be differentiated by the size of the oil shale particles that are fed into the retorts. As a rule, gas heat carrier technologies process oil shale lumps varying in diameter from , while solid heat carrier and wall conduction technologies process fines which are particles less than in diameter.\n\n\"By retort orientation\": \"Ex-situ\" technologies are sometimes classified as vertical or horizontal. Vertical retorts are usually shaft kilns where a bed of shale moves from top to bottom by gravity. Horizontal retorts are usually horizontal rotating drums or screws where shale moves from one end to the other. As a general rule, vertical retorts process lumps using a gas heat carrier, while horizontal retorts process fines using solid heat carrier.\n\n\"By complexity of technology\": \"In situ\" technologies are usually classified either as \"true in situ\" processes or \"modified in situ\" processes. \"True in situ\" processes do not involve mining or crushing the oil shale. \"Modified in situ\" processes involve drilling and fracturing the target oil shale deposit to create voids in the deposit. The voids enable a better flow of gases and fluids through the deposit, thereby increasing the volume and quality of the shale oil produced.\n\nInternal combustion technologies burn materials (typically char and oil shale gas) within a vertical shaft retort to supply heat for pyrolysis. Typically raw oil shale particles between and in size are fed into the top of the retort and are heated by the rising hot gases, which pass through the descending oil shale, thereby causing decomposition of the kerogen at about . Shale oil mist, evolved gases and cooled combustion gases are removed from the top of the retort then moved to separation equipment. Condensed shale oil is collected, while non-condensable gas is recycled and used to carry heat up the retort. In the lower part of the retort, air is injected for the combustion which heats the spent oil shale and gases to between and . Cold recycled gas may enter the bottom of the retort to cool the shale ash. The Union A and Superior Direct processes depart from this pattern. In the Union A process, oil shale is fed through the bottom of the retort and a pump moves it upward. In the Superior Direct process, oil shale is processed in a horizontal, segmented, doughnut-shaped traveling-grate retort.\n\nInternal combustion technologies such as the Paraho Direct are thermally efficient, since combustion of char on the spent shale and heat recovered from the shale ash and evolved gases can provide all the heat requirements of the retort. These technologies can achieve 80-90% of Fischer assay yield. Two well-established shale oil industries use internal combustion technologies: Kiviter process facilities have been operated continuously in Estonia since the 1920s, and a number of Chinese companies operate Fushun process facilities.\n\nCommon drawbacks of internal combustion technologies are that the combustible oil shale gas is diluted by combustion gases and particles smaller than can not be processed. Uneven distribution of gas across the retort can result in blockages when hot spots cause particles to fuse or disintegrate.\n\nHot recycled solids technologies deliver heat to the oil shale by recycling hot solid particles—typically oil shale ash. These technologies usually employ rotating kiln or fluidized bed retorts, fed by fine oil shale particles generally having a diameter of less than ; some technologies use particles even smaller than . The recycled particles are heated in a separate chamber or vessel to about and then mixed with the raw oil shale to cause the shale to decompose at about . Oil vapour and shale oil gas are separated from the solids and cooled to condense and collect the oil. Heat recovered from the combustion gases and shale ash may be used to dry and preheat the raw oil shale before it is mixed with the hot recycle solids.\n\nIn the Galoter and Enefit processes, the spent oil shale is burnt in a separate furnace and the resulting hot ash is separated from the combustion gas and mixed with oil shale particles in a rotating kiln. Combustion gases from the furnace are used to dry the oil shale in a dryer before mixing with hot ash. The TOSCO II process uses ceramic balls instead of shale ash as the hot recycled solids. The distinguishing feature of the Alberta Taciuk Process (ATP) is that the entire process occurs in a single rotating multi–chamber horizontal vessel.\n\nBecause the hot recycle solids are heated in a separate furnace, the oil shale gas from these technologies is not diluted with combustion exhaust gas. Another advantage is that there is no limit on the smallest particles that the retort can process, thus allowing all the crushed feed to be used. One disadvantge is that more water is used to handle the resulting finer shale ash.\n\nThese technologies transfer heat to the oil shale by conducting it through the retort wall. The shale feed usually consists of fine particles. Their advantage lies in the fact that retort vapors are not combined with combustion exhaust. The Combustion Resources process uses a hydrogen–fired rotating kiln, where hot gas is circulated through an outer annulus. The Oil-Tech staged electrically heated retort consists of individual inter-connected heating chambers, stacked atop each other. Its principal advantage lies in its modular design, which enhances its portability and adaptability. The Red Leaf Resources EcoShale In-Capsule Process combines surface mining with a lower-temperature heating method similar to \"in situ\" processes by operating within the confines of an earthen structure. A hot gas circulated through parallel pipes heats the oil shale rubble. An installation within the empty space created by mining would permit rapid reclamation of the topography.\nA general drawback of conduction through a wall technologies is that the retorts are more costly when scaled-up due to the resulting large amount of heat conducting walls made of high-temperature alloys.\n\nIn general, externally generated hot gas technologies are similar to internal combustion technologies in that they also process oil shale lumps in vertical shaft kilns. Significantly, though, the heat in these technologies is delivered by gases heated outside the retort vessel, and therefore the retort vapors are not diluted with combustion exhaust. The Petrosix and Paraho Indirect employ this technology. In addition to not accepting fine particles as feed, these technologies do not utilize the potential heat of combusting the char on the spent shale and thus must burn more valuable fuels. However, due to the lack of combustion of the spent shale, the oil shale does not exceed and significant carbonate mineral decomposition and subsequent CO generation can be avoided for some oil shales. Also, these technologies tend to be the more stable and easier to control than internal combustion or hot solid recycle technologies.\n\nKerogen is tightly bound to the shale and resists dissolution by most solvents. Despite this constraint, extraction using especially reactive fluids has been tested, including those in a supercritical state. Reactive fluid technologies are suitable for processing oil shales with a low hydrogen content. In these technologies, hydrogen gas (H) or hydrogen donors (chemicals that donate hydrogen during chemical reactions) react with coke precursors (chemical structures in the oil shale that are prone to form char during retorting but have not yet done so). Reactive fluid technologies include the IGT Hytort (high-pressure H) process, donor solvent processes, and the Chattanooga fluidized bed reactor. In the IGT Hytort oil shale is processed in a high-pressure hydrogen environment. The Chattanooga process uses a fluidized bed reactor and an associated hydrogen-fired heater for oil shale thermal cracking and hydrogenation. Laboratory results indicate that these technologies can often obtain significantly higher oil yields than pyrolysis processes. Drawbacks are the additional cost and complexity of hydrogen production and high-pressure retort vessels.\n\nSeveral experimental tests have been conducted for the oil-shale gasification by using plasma technologies. In these technologies, oil shale is bombarded by radicals (ions). The radicals crack kerogen molecules forming synthetic gas and oil. Air, hydrogen or nitrogen are used as plasma gas and processes may operate in an arc, plasma arc, or plasma electrolysis mode. The main benefit of these technologies is processing without using water.\n\n\"In situ\" technologies heat oil shale underground by injecting hot fluids into the rock formation, or by using linear or planar heating sources followed by thermal conduction and convection to distribute heat through the target area. Shale oil is then recovered through vertical wells drilled into the formation. These technologies are potentially able to extract more shale oil from a given area of land than conventional \"ex situ\" processing technologies, as the wells can reach greater depths than surface mines. They present an opportunity to recover shale oil from low-grade deposits that traditional mining techniques could not extract.\n\nDuring World War II a modified \"in situ\" extraction process was implemented without significant success in Germany. One of the earliest successful \"in situ\" processes was underground gasification by electrical energy (Ljungström method)—a process exploited between 1940 and 1966 for shale oil extraction at Kvarntorp in Sweden. Prior to the 1980s, many variations of the \"in situ\" process were explored in the United States. The first modified \"in situ\" oil shale experiment in the United States was conducted by Occidental Petroleum in 1972 at Logan Wash, Colorado. Newer technologies are being explored that use a variety of heat sources and heat delivery systems.\n\nWall conduction \"in situ\" technologies use heating elements or heating pipes placed within the oil shale formation. The Shell in situ conversion process (Shell ICP) uses electrical heating elements for heating the oil shale layer to between over a period of approximately four years. The processing area is isolated from surrounding groundwater by a freeze wall consisting of wells filled with a circulating super-chilled fluid. Disadvantages of this process are large electrical power consumption, extensive water use, and the risk of groundwater pollution. The process was tested since the early 1980s at the Mahogany test site in the Piceance Basin. of oil were extracted in 2004 at a testing area.\n\nIn the CCR Process proposed by American Shale Oil, superheated steam or another heat transfer medium is circulated through a series of pipes placed below the oil shale layer to be extracted. The system combines horizontal wells, through which steam is passed, and vertical wells, which provide both vertical heat transfer through refluxing of converted shale oil and a means to collect the produced hydrocarbons. Heat is supplied by combustion of natural gas or propane in the initial phase and by oil shale gas at a later stage.\n\nThe Geothermic Fuels Cells Process (IEP GFC) proposed by Independent Energy Partners extracts shale oil by exploiting a high-temperature stack of fuel cells. The cells, placed in the oil shale formation, are fueled by natural gas during a warm-up period and afterward by oil shale gas generated by its own waste heat.\n\nExternally generated hot gas \"in situ\" technologies use hot gases heated above-ground and then injected into the oil shale formation. The Chevron CRUSH process, which was researched by Chevron Corporation in partnership with Los Alamos National Laboratory, injects heated carbon dioxide into the formation via drilled wells and to heat the formation through a series of horizontal fractures through which the gas is circulated. General Synfuels International has proposed the Omnishale process involving injection of super-heated air into the oil shale formation. Mountain West Energy's In Situ Vapor Extraction process uses similar principles of injection of high-temperature gas.\n\nExxonMobil's \"in situ\" technology (ExxonMobil Electrofrac) uses electrical heating with elements of both wall conduction and volumetric heating methods. It injects an electrically conductive material such as calcined petroleum coke into the hydraulic fractures created in the oil shale formation which then forms a heating element. Heating wells are placed in a parallel row with a second horizontal well intersecting them at their toe. This allows opposing electrical charges to be applied at either end.\n\nThe Illinois Institute of Technology developed the concept of oil shale volumetric heating using radio waves (radio frequency processing) during the late 1970s. This technology was further developed by Lawrence Livermore National Laboratory. Oil shale is heated by vertical electrode arrays. Deeper volumes could be processed at slower heating rates by installations spaced at tens of meters. The concept presumes a radio frequency at which the skin depth is many tens of meters, thereby overcoming the thermal diffusion times needed for conductive heating. Its drawbacks include intensive electrical demand and the possibility that groundwater or char would absorb undue amounts of the energy. Radio frequency processing in conjunction with critical fluids is being developed by Raytheon together with CF Technologies and tested by Schlumberger.\n\nMicrowave heating technologies are based on the same principles as radio wave heating, although it is believed that radio wave heating is an improvement over microwave heating because its energy can penetrate farther into the oil shale formation. The microwave heating process was tested by Global Resource Corporation. Electro-Petroleum proposes electrically enhanced oil recovery by the passage of direct current between cathodes in producing wells and anodes located either at the surface or at depth in other wells. The passage of the current through the oil shale formation results in resistive Joule heating.\n\nThe dominant question for shale oil production is under what conditions shale oil is economically viable. According to the United States Department of Energy, the capital costs of a \"ex-situ\" processing complex are $3–10 billion. The various attempts to develop oil shale deposits have succeeded only when the shale-oil production cost in a given region is lower than the price of petroleum or its other substitutes. According to a survey conducted by the RAND Corporation, the cost of producing shale oil at a hypothetical surface retorting complex in the United States (comprising a mine, retorting plant, upgrading plant, supporting utilities, and spent oil shale reclamation), would be in a range of $70–95 per barrel ($440–600/m), adjusted to 2005 values. Assuming a gradual increase in output after the start of commercial production, the analysis projects a gradual reduction in processing costs to $30–40 per barrel ($190–250/m) after achieving the milestone of . The United States Department of Energy estimates that the \"ex-situ\" processing would be economic at sustained average world oil prices above $54 per barrel and \"in-situ\" processing would be economic at prices above $35 per barrel. These estimates assume a return rate of 15%. Royal Dutch Shell announced in 2006 that its Shell ICP technology would realize a profit when crude oil prices are higher than $30 per barrel ($190/m), while some technologies at full-scale production assert profitability at oil prices even lower than $20 per barrel ($130/m).\n\nTo increase the efficiency of oil shale retorting and by this the viability of the shale oil production, researchers have proposed and tested several co-pyrolysis processes, in which other materials such as biomass, peat, waste bitumen, or rubber and plastic wastes are retorted along with the oil shale. Some modified technologies propose combining a fluidized bed retort with a circulated fluidized bed furnace for burning the by-products of pyrolysis (char and oil shale gas) and thereby improving oil yield, increasing throughput, and decreasing retorting time.\n\nOther ways of improving the economics of shale oil extraction could be to increase the size of the operation to achieve economies of scale, use oil shale that is a by-product of coal mining such as at Fushun China, produce specialty chemicals as by Viru Keemia Grupp in Estonia, co-generate electricity from the waste heat and process high grade oil shale that yields more oil per shale processed.\n\nA possible measure of the viability of oil shale as an energy source lies in the ratio of the energy in the extracted oil to the energy used in its mining and processing (Energy Returned on Energy Invested, or EROEI). A 1984 study estimated the EROEI of the various known oil shale deposits as varying between 0.7–13.3; Some companies and newer technologies assert an EROEI between 3 and 10. According to the World Energy Outlook 2010, the EROEI of \"ex-situ\" processing is typically 4 to 5 while of \"in-situ\" processing it may be even as low as 2.\n\nTo increase the EROEI, several combined technologies were proposed. These include the usage of process waste heat, e.g. gasification or combustion of the residual carbon (char), and the usage of waste heat from other industrial processes, such as coal gasification and nuclear power generation.\n\nThe water requirements of extraction processes are an additional economic consideration in regions where water is a scarce resource.\n\nMining oil shale involves a number of environmental impacts, more pronounced in surface mining than in underground mining. These include acid drainage induced by the sudden rapid exposure and subsequent oxidation of formerly buried materials, the introduction of metals including mercury into surface-water and groundwater, increased erosion, sulfur-gas emissions, and air pollution caused by the production of particulates during processing, transport, and support activities. In 2002, about 97% of air pollution, 86% of total waste and 23% of water pollution in Estonia came from the power industry, which uses oil shale as the main resource for its power production.\n\nOil-shale extraction can damage the biological and recreational value of land and the ecosystem in the mining area. Combustion and thermal processing generate waste material. In addition, the atmospheric emissions from oil shale processing and combustion include carbon dioxide, a greenhouse gas. Environmentalists oppose production and usage of oil shale, as it creates even more greenhouse gases than conventional fossil fuels. Experimental \"in situ\" conversion processes and carbon capture and storage technologies may reduce some of these concerns in the future, but at the same time they may cause other problems, including groundwater pollution. Among the water contaminants commonly associated with oil shale processing are oxygen and nitrogen heterocyclic hydrocarbons. Commonly detected examples include quinoline derivatives, pyridine, and various alkyl homologues of pyridine (picoline, lutidine).\n\nWater concerns are sensitive issues in arid regions, such as the western US and Israel's Negev Desert, where plans exist to expand oil-shale extraction despite a water shortage. Depending on technology, above-ground retorting uses between one and five barrels of water per barrel of produced shale-oil. A 2008 programmatic environmental impact statement issued by the US Bureau of Land Management stated that surface mining and retort operations produce of waste water per of processed oil shale. \"In situ\" processing, according to one estimate, uses about one-tenth as much water.\nEnvironmental activists, including members of Greenpeace, have organized strong protests against the oil shale industry. In one result, Queensland Energy Resources put the proposed Stuart Oil Shale Project in Australia on hold in 2004.\n\n"}
{"id": "27375401", "url": "https://en.wikipedia.org/wiki?curid=27375401", "title": "Sind ibn Ali", "text": "Sind ibn Ali\n\nAbu al-Tayyib Sanad ibn Ali al-Yahudi (died c. 864 C.E.), was an eighth-century Iraqi Jewish astronomer, translator, mathematician and engineer employed at the court of the Abbasid caliph Al-Ma'mun. A later convert to Islam, Sanad's father was a learned Jewish astronomer who lived and worked in Baghdad. \n\nHe is known to have translated and modified the Zij al-Sindhind. The Zij al-Sindhind was the first astronomical table ever introduced in the Muslim World. As a mathematician Sanad ibn ʿAlī was a colleague of al-Khwarizmi and worked closely with Yaqūb ibn Tāriq together they calculated the diameter of the Earth and other astronomical bodies. He also wrote a commentary on \"Kitāb al-ğabr wa-l-muqābala\" and helped prove the works of al-Khwarizmi. The decimal point notation to the Arabic numerals was introduced by Sanad ibn Ali.\n\nAccording to Ibn Abi Usaibia: the Banū Mūsā brothers out of sheer professional jealousy kept him away from Abbasid Caliph al-Mutawakkil at his new capital Samarra and had caused Sanad ibn ʿAlī to be sent away to Baghdad. Both Ja'far Muhammad ibn Mūsā ibn Shākir and Ahmad ibn Mūsā ibn Shākir delegated the work of digging a great canal instead to Al-Farghani and thus ignoring Sanad ibn ʿAlī, the better engineer. Al-Farghani committed a great error, making the beginning of the canal deeper than the rest and water never reached the new garrison of \"Al-Ja'fariya\". News of this greatly angered al-Mutawakkil and the two Banū Mūsā brothers were saved from severe punishment only by the gracious willingness of Sanad ibn ʿAlī, to vouch the corrections of Al-Farghani's calculations thus risking his own welfare and possibly his life.\n"}
{"id": "5309", "url": "https://en.wikipedia.org/wiki?curid=5309", "title": "Software", "text": "Software\n\nComputer software, or simply software, is a collection of data or computer instructions that tell the computer how to work. This is in contrast to physical hardware, from which the system is built and actually performs the work. In computer science and software engineering, computer software is all information processed by computer systems, programs and data. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media. Computer hardware and software require each other and neither can be realistically used on its own.\n\nAt the lowest programming level, executable code consists of machine language instructions supported by an individual processor—typically a central processing unit (CPU) or a graphics processing unit (GPU). A machine language consists of groups of binary values signifying processor instructions that change the state of the computer from its preceding state. For example, an instruction may change the value stored in a particular storage location in the computer—an effect that is not directly observable to the user. An instruction may also invoke one of many input or output operations, for example displaying some text on a computer screen; causing state changes which should be visible to the user. The processor executes the instructions in the order they are provided, unless it is instructed to \"jump\" to a different instruction, or is interrupted by the operating system. , most personal computers, smartphone devices and servers have processors with multiple execution units or multiple processors performing computation together, and computing has become a much more concurrent activity than in the past.\n\nThe majority of software is written in high-level programming languages. They are easier and more efficient for programmers because they are closer to natural languages than machine languages. High-level languages are translated into machine language using a compiler or an interpreter or a combination of the two. Software may also be written in a low-level assembly language, which has strong correspondence to the computer's machine language instructions and is translated into machine language using an assembler.\n\nAn outline (algorithm) for what would have been the first piece of software was written by Ada Lovelace in the 19th century, for the planned Analytical Engine. She created proofs to show how the engine would calculate Bernoulli Numbers. Because of the proofs and algorithm, she is considered the first computer programmer.\n\nThe first theory about software—prior to creation of computers as we know them today—was proposed by Alan Turing in his 1935 essay \"On Computable Numbers, with an Application to the Entscheidungsproblem\" (decision problem).\n\nThis eventually led to the creation of the academic fields of computer science and software engineering; Both fields study software and its creation. Computer science is the theoretical study of computer and software (Turing's essay is an example of computer science), whereas software engineering is the application of engineering and development of software.\n\nHowever, prior to 1946, software was not yet the programs stored in the memory of stored-program digital computers, as we now understand it. The first electronic computing devices were instead rewired in order to \"reprogram\" them.\n\nOn virtually all computer platforms, software can be grouped into a few broad categories.\n\nBased on the goal, computer software can be divided into:\n\n\n\nProgramming tools are also software in the form of programs or applications that software developers (also known as\n\"programmers, coders, hackers\" or \"software engineers\") use to create, debug, maintain (i.e. improve or fix), or otherwise support software. \n\nSoftware is written in one or more programming languages; there are many programming languages in existence, and each has at least one implementation, each of which consists of its own set of programming tools. These tools may be relatively self-contained programs such as compilers, debuggers, interpreters, linkers, and text editors, that can be combined together to accomplish a task; or they may form an integrated development environment (IDE), which combines much or all of the functionality of such self-contained tools. IDEs may do this by either invoking the relevant individual tools or by re-implementing their functionality in a new way. An IDE can make it easier to do specific tasks, such as searching in files in a particular project. Many programming language implementations provide the option of using both individual tools or an IDE.\n\nUsers often see things differently from programmers. People who use modern general purpose computers (as opposed to embedded systems, analog computers and supercomputers) usually see three layers of software performing a variety of tasks: platform, application, and user software.\n\n\nComputer software has to be \"loaded\" into the computer's storage (such as the hard drive or memory). Once the software has loaded, the computer is able to \"execute\" the software. This involves passing instructions from the application software, through the system software, to the hardware which ultimately receives the instruction as machine code. Each instruction causes the computer to carry out an operation—moving data, carrying out a computation, or altering the control flow of instructions.\n\nData movement is typically from one place in memory to another. Sometimes it involves moving data between memory and registers which enable high-speed data access in the CPU. Moving data, especially large amounts of it, can be costly. So, this is sometimes avoided by using \"pointers\" to data instead. Computations include simple operations such as incrementing the value of a variable data element. More complex computations may involve many operations and data elements together.\n\nSoftware quality is very important, especially for commercial and system software like Microsoft Office, Microsoft Windows and Linux. If software is faulty (buggy), it can delete a person's work, crash the computer and do other unexpected things. Faults and errors are called \"bugs\" which are often discovered during alpha and beta testing. Software is often also a victim to what is known as software aging, the progressive performance degradation resulting from a combination of unseen bugs.\n\nMany bugs are discovered and eliminated (debugged) through software testing. However, software testing rarely—if ever—eliminates every bug; some programmers say that \"every program has at least one more bug\" (Lubarsky's Law). In the waterfall method of software development, separate testing teams are typically employed, but in newer approaches, collectively termed agile software development, developers often do all their own testing, and demonstrate the software to users/clients regularly to obtain feedback. Software can be tested through unit testing, regression testing and other methods, which are done manually, or most commonly, automatically, since the amount of code to be tested can be quite large. For instance, NASA has extremely rigorous software testing procedures for many operating systems and communication functions. Many NASA-based operations interact and identify each other through command programs. This enables many people who work at NASA to check and evaluate functional systems overall. Programs containing command software enable hardware engineering and system operations to function much easier together.\n\nThe software's license gives the user the right to use the software in the licensed environment, and in the case of free software licenses, also grants other rights such as the right to make copies.\n\nProprietary software can be divided into two types:\n\n\nOpen source software, on the other hand, comes with a free software license, granting the recipient the rights to modify and redistribute the software.\n\nSoftware patents, like other types of patents, are theoretically supposed to give an inventor an exclusive, time-limited license for a \"detailed idea (e.g. an algorithm) on how to implement\" a piece of software, or a component of a piece of software. Ideas for useful things that software could \"do\", and user \"requirements\", are not supposed to be patentable, and concrete implementations (i.e. the actual software packages implementing the patent) are not supposed to be patentable either—the latter are already covered by copyright, generally automatically. So software patents are supposed to cover the middle area, between requirements and concrete implementation. In some countries, a requirement for the claimed invention to have an effect on the physical world may also be part of the requirements for a software patent to be held valid—although since \"all\" useful software has effects on the physical world, this requirement may be open to debate. Meanwhile, American copyright law was applied to various aspects of the writing of the software code.\n\nSoftware patents are controversial in the software industry with many people holding different views about them. One of the sources of controversy is that the aforementioned split between initial ideas and patent does not seem to be honored in practice by patent lawyers—for example the patent for Aspect-Oriented Programming (AOP), which purported to claim rights over \"any\" programming tool implementing the idea of AOP, howsoever implemented. Another source of controversy is the effect on innovation, with many distinguished experts and companies arguing that software is such a fast-moving field that software patents merely create vast additional litigation costs and risks, and actually retard innovation. In the case of debates about software patents outside the United States, the argument has been made that large American corporations and patent lawyers are likely to be the primary beneficiaries of allowing or continue to allow software patents.\n\nDesign and implementation of software varies depending on the complexity of the software. For instance, the design and creation of Microsoft Word took much more time than designing and developing Microsoft Notepad because the latter has much more basic functionality.\n\nSoftware is usually designed and created (aka coded/written/programmed) in integrated development environments (IDE) like Eclipse, IntelliJ and Microsoft Visual Studio that can simplify the process and compile the software (if applicable). As noted in a different section, software is usually created on top of existing software and the application programming interface (API) that the underlying software provides like GTK+, JavaBeans or Swing. Libraries (APIs) can be categorized by their purpose. For instance, the Spring Framework is used for implementing enterprise applications, the Windows Forms library is used for designing graphical user interface (GUI) applications like Microsoft Word, and Windows Communication Foundation is used for designing web services. When a program is designed, it relies upon the API. For instance, a Microsoft Windows desktop application might call API functions in the .NET Windows Forms library like \"Form1.Close()\" and \"Form1.Show()\" to close or open the application. Without these APIs, the programmer needs to write these functionalities entirely themselves. Companies like Oracle and Microsoft provide their own APIs so that many applications are written using their software libraries that usually have numerous APIs in them.\n\nData structures such as hash tables, arrays, and binary trees, and algorithms such as quicksort, can be useful for creating software.\n\nComputer software has special economic characteristics that make its design, creation, and distribution different from most other economic goods.\n\nA person who creates software is called a programmer, software engineer or software developer, terms that all have a similar meaning. More informal terms for programmer also exist such as \"coder\" and \"hacker\"although use of the latter word may cause confusion, because it is more often used to mean someone who illegally breaks into computer systems.\n\nA great variety of software companies and programmers in the world comprise a software industry. Software can be quite a profitable industry: Bill Gates, the co-founder of Microsoft was the richest person in the world in 2009, largely due to his ownership of a significant number of shares in Microsoft, the company responsible for Microsoft Windows and Microsoft Office software products - both market leaders in their respective product categories.\n\nNon-profit software organizations include the Free Software Foundation, GNU Project and Mozilla Foundation. Software standard organizations like the W3C, IETF develop recommended software standards such as XML, HTTP and HTML, so that software can interoperate through these standards.\n\nOther well-known large software companies include Google, IBM, TCS, Infosys, Wipro, HCL Technologies, Oracle, Novell, SAP, Symantec, Adobe Systems, Sidetrade and Corel, while small companies often provide innovation.\n"}
{"id": "2788463", "url": "https://en.wikipedia.org/wiki?curid=2788463", "title": "Subscriber loop carrier", "text": "Subscriber loop carrier\n\nA subscriber loop carrier or subscriber line carrier (SLC) provides telephone exchange-like telephone interface functionality. An SLC remote terminal is typically located in an area with a high density of telephone subscribers, such as a residential neighborhood, that is remote from the telephone company's central office. Two or four T1 circuits (depending on the configuration) connect the SLC remote terminal to the central office terminal (COT), in the case of a universal subscriber loop carrier (USLC). An integrated subscriber loop carrier (ISLC) has its T-spans terminating directly in time division switching equipment in the telephone exchange.\n\nOne system serves up to 96 customers. This configuration is more efficient than the alternative of having separate copper pairs between each service termination point (the subscriber's location) and the central telephone exchange. \n\nThese systems are generally installed in cabinets that have some form of uninterruptible power supply or other backup battery arrangements, and sometimes with additional equipment such as remote DSLAMs. \n\n"}
{"id": "11717153", "url": "https://en.wikipedia.org/wiki?curid=11717153", "title": "TabletKiosk", "text": "TabletKiosk\n\nTabletKiosk is a manufacturer of enterprise-grade Tablet PCs and UMPCs located in Torrance, California, United States. All mobile computers produced by TabletKiosk fall into the slate category, featuring touchscreen or pen (active digitizer) input, in lieu of integrated or convertible keyboards. Current products include the Sahara Slate PC i500 series, designed in-house at TabletKiosk's Taiwan R&D facility. Early generations of the eo brand of UMPC (Ultra-Mobile PC) were designed in collaboration with outside designers and the TabletKiosk team, while the fourth generation of this brand, the eo a7400 is designed exclusively inhouse.\n\nTabletKiosk is a wholly owned subsidiary of Sand Dune Ventures, based in Torrance, California.\n\nIn 2006, TabletKiosk delayed shipment of its \"eo\" brand tablet after discovering problems with the device's fan.\n\nSoftBrands announced in 2007 that it would use TabletKiosk's Sahara Slate PC line to distribute SoftBrands software to hotel companies.\n\nParkland Memorial Hospital in Dallas, Texas, United States has patients visiting its emergency department fill in their details using a TabletKiosk machine.,\n\nIn 2013, Healthcare Global named the Sahara Slate PC i500 as One of the Top 10 Mobile Tablets For Healthcare Professionals.\n\n\n"}
{"id": "5595643", "url": "https://en.wikipedia.org/wiki?curid=5595643", "title": "Teletype Corporation", "text": "Teletype Corporation\n\nThe Teletype Corporation, a part of American Telephone and Telegraph Company's Western Electric manufacturing arm since 1930, came into being in 1928 when the Morkrum-Kleinschmidt Company changed its name to the name of its trademark equipment. Teletype Corporation, of Skokie, Illinois, was responsible for the research, development and manufacture of data and record communications equipment, but it is primarily remembered for the manufacture of electromechanical teleprinters.\n\nBecause of the nature of its business, as stated in the corporate charter, Teletype Corporation was allowed a unique mode of operation within Western Electric. It was organized as a separate entity, and contained all the elements necessary for a separate corporation. Teletype's charter permitted the sale of equipment to customers outside the AT&T Bell System, which explained their need for a separate sales force. The primary customer outside of the Bell System was the United States Government.\n\nThe Teletype Corporation continued in this manner until January 8, 1982, the date of settlement of \"United States v. AT&T\", a 1974 United States Department of Justice antitrust suit against AT&T. At that time, Western Electric was fully absorbed into AT&T as AT&T Technologies, and the Teletype Corporation became AT&T Teletype. The last vestiges of what had been the Teletype Corporation ceased in 1990, bringing to a close the dedicated teleprinter business.\nOne of the three Teletype manufacturing buildings in Skokie remains in use as a parking garage for a shopping center. Every other floor of the building has been removed. The other two buildings were demolished.\n\nThe Teletype Corporation had its roots in the Morkrum Company. In 1902, electrical engineer Frank Pearne approached Joy Morton, head of Morton Salt, seeking a sponsor for Pearne's research into the practicalities of developing a printing telegraph system. Joy Morton needed to determine whether this was worthwhile and so consulted mechanical engineer Charles Krum, who was vice president of the Western Cold Storage Company, which was run by Morton’s brother Mark Morton. Krum was interested in helping Pearne, so space was set up in a laboratory in the attic of Western Cold Storage. Frank Pearne lost interest in the project after a year, and left to become a teacher. Krum was prepared to continue Pearne’s work, and in August 1903 a patent was filed for a \"typebar page printer\".\n\nIn 1904, Krum filed a patent for a \"type wheel printing telegraph machine\" which was issued in August 1907.\n\nIn 1906, the Morkrum Company was formed, with the company name combining the Morton and Krum names and reflecting the financial assistance provided by Joy Morton. This is the time when Charles Krum's son, Howard Krum, joined his father in this work. It was Howard who developed and patented the start-stop synchronizing method for code telegraph systems, which made possible the practical teleprinter.\n\nIn 1908, a working teleprinter was produced, called the Morkrum Printing Telegraph, which was field tested with the Alton Railroad.\n\nIn 1910, the Morkrum Company designed and installed the first commercial teletypewriter system on Postal Telegraph Company lines between Boston and New York City using the \"Blue Code Version\" of the Morkrum Printing Telegraph.\n\nIn 1925, the Morkrum Company and the Kleinschmidt Electric Company merged to form the Morkrum-Kleinschmidt Company.\n\nIn December 1928, the company changed its name to the less cumbersome \"Teletype Corporation\".\n\nIn 1930, the Teletype Corporation was purchased by the American Telephone and Telegraph Company for $30,000,000 in stock and became a subsidiary of the Western Electric Company. While other principals in the Teletype Corporation retired, Howard Krum stayed on as a consultant.\n\n\nIn 1916, Kleinschmidt filed a patent application for a type-bar page printer This printer utilized Baudot code but did not utilize the start-stop synchronization technology that Howard Krum had previously patented. The type-bar printer was intended for use on multiplex circuits, and its printing was controlled from a local segment on a receiving distributor of the sunflower type. In 1919, Kleinschmidt appeared to be concerned chiefly with development of multiplex transmitters for use with this printer.\n\n\n\nTeletype models and their dates:\n\nThe Teletype Model 15 is a Baudot code page printer; the mainstay of U.S. military communications in World War II. A reliable, heavy-duty machine with a cast frame. In 1930, Sterling Morton, Howard L. Krum, and Edward E. Kleinschmidt filed an application for a U.S. patent covering the commercial form of the Model 15 page printer. Approximately 200,000 Model 15 teleprinters were built. The Model 15 stands out as one of a few machines that remained in production for many years, remaining in production until 1963, a total of 33 years of continuous production. The production run was stretched somewhat by World War II — the Model 28 was scheduled to replace the Model 15 in the mid-1940s, but Teletype built so many factories to produce the Model 15 during World War II that it was more economical to continue mass production of the Model 15. The Model 15, in its \"receive only\" configuration with no keyboard, was the classic \"news Teletype\" until the 1950s, when the news wire services began to move to TeleTypeSetter feeds. Some radio stations still use a recording of the sound of one of these machines as background during news broadcasts.\n\nThe Teletype Model 19 is a Model 15 with an integrated paper tape perforator and a Model 14 Transmitter-Distributor.\n\nThe Teletype Model 20 is an upper/lower case Type Bar Page Printer available as a receive only machine or a send-receive machine with four rows of keys, using a six-bit code for TeleTypeSetter (TTS) use.\n\nThe Teletype Model 26 is a Baudot code page printer; a lower-cost machine using a typewheel. The platen and paper moved while typing, like a manual typewriter.\n\n\nThe Teletype Model 28 is a product line of page printers, typing and non-typing tape perforator and tape reperforators, fixed-head single contact and pivoted head multi-contact transmitter-distributors, and receiving selector equipment. Regarded as the most rugged machines that the Teletype Corporation built, this line of teleprinters used an exchangeable type box for printing, and a sequential selector \"Stunt Box\" to mechanically initiate non-printing functions within the typing unit of the page printer, electrically control functions within the page printer and electrically control external equipment. The Teletype Corporation introduced the Model 28 as a commercial product in 1953, after this product had originally been designed for the US Military.\n\nStarting with the Model 28, Teletype page printer model numbers were often modified by letters indicating the configuration. The configurations, in increasing order of equipment level and cost, were:\n\nNot all models came in all three configurations. Teletype Corporation documents suffixed the configuration to the model number, e.g., \"Model 33 ASR\" (Model 33 Automatic Send and Receive). In contrast, some customers and users tended to place the configuration before the model number, e.g., \"ASR-33\".\n\nThe U.S. military had their own system of identifying the various models, often identifying various improvements, included options / features, etc. The TT-47/UG was the first Model 28 KSR, and while Teletype's designation for the basic machine remained the same over the next 20+ years, the TT-47/UG took on suffixes to identify the specific version. The last TT-47/UG was the TT-47L/UG. The U.S. Navy also assigned some \"set\" designations using the standard Army/Navy system, such as the AN/UGC-5, a Teletype Model 28 ASR which has a keyboard, printer, tape punch and reader facilities all in one cabinet.\n\nThe Teletype Model 29 is a six-bit machine using an IBM BCD code. It began as a replacement for Model 20, but apparently there was no market for such a machine, so it was repurposed for IDP (Integrated Data Processing) use.\n\nThe Teletype Model 31 is a compact and light-weight five-level start-stop tape printer designed for mobile operation in the air and on the sea.\n\nDataspeed was the Bell System name for a family of high speed paper tape systems used with DataPhone modems. Type 1\nwas 5-level, 1050 wpm. Type 2 was 5-8 level, 1050 wpm. Type 4 was 8 level with automatic error detection and correction\nby retransmitting blocks of data received in error. Type 5 was 8 level 750 wpm using a modem that was very inexpensive\nas a transmitter; hence the system was popular for data collection applications.\n\n\nThe Teletype Model 32 and Teletype Model 33 are low-cost teleprinters, all-mechanical in design, with many plastic parts; both used a typewheel for printing. They were produced in ASR, KSR and RO versions and introduced as a commercial product in 1963 after being originally designed for the US Navy. The Model 33 is an ASCII teleprinter designed for light-duty office use. The Model 32 is a Baudot variant of the Model 33. Both were less rugged and less expensive than earlier Teletype machines. The Model 33 ASR was ubiquitous as a console device, with its paper tape reader/punch used as an I/O device, in the early minicomputer era. Over 600,000 Model 32 and 33 machines were manufactured.\n\nThe Teletype Model 35 is a 110 baud terminal that utilizes a serial input / output eight-level 11 unit code signal consisting of a start bit, seven information bits, an even parity bit and two stop bits. The Model 35 was produced in ASR, KSR and RO versions. The Model 35 handles 1963 and USASI X3.4-1968 ASCII Code and prints 63 graphics of this code with letters in upper case on an 8.5 wide inch page using a typebox. The Model 35 interface will accept DC current (20 ma or 60 ma). An optional modem interface provides for operation over voice-grade channels. The modem transmits asynchronously in serial format, compatible with Bell System 101, 103 and 113 data sets or their equivalent.\n\nThe Teletype Model 35 ASR is 38.5 inches high, 40 inches wide and 24 inches deep. The Teletype Model 35 KSR and RO are 38.5 inches high, 24 inches wide and 24 inches deep. This machine uses a standard 115 VAC 60 Hz single phase synchronous motor. The recommended operating environment is a temperature of 40 to 110 Fahrenheit, a relative humidity of 2 to 95 percent and an altitude of 0 to 10,000 feet. Lubrication maintenance is recommended every 1500 hours of operation or every six months, whichever occurs first. The printing paper is an 8.44 inch by 4.5 inch diameter roll. Ribbons are 0.5 inch wide by 60 yards long, with plastic spools and eyelets for proper ribbon reverse operation.\n\nEach 1ESS switch included a retractable rack-mounted 35-type TTY and a duplicate remote TTY.\n\nThe Teletype Inktronic Terminal is an electronic, high-speed teleprinter that uses electrostatic deflection to print characters at 1200 words-per-minute. The Inktronic terminal prints an 80 column page using forty jets to print 63 alphanumeric characters in two columns on an 8.5 inch roll of paper. The Inktronic terminal was produced in KSR and RO versions. The KSR version can generate 128 ASCII code combinations while the RO version was available as a Baudot or ASCII printer. An ASR version was planned but not produced. When the Inktronic terminal proved unreliable and difficult to maintain, it was withdrawn from production.\n\nThe Teletype Model 37 is a 150 baud terminal that utilizes a serial input / output 10 unit code signal consisting of a start bit, seven information bits, an even parity bit and a stop bit. The Model 37 was produced in ASR, KSR and RO versions. The Model 37 handles USASI X3.4-1968 ASCII Code and prints 94 graphics of this code with letters in both upper and lower case, with the option to print in two colors. The Model 37 uses a six-row removable typebox with provisions for 96 type pallet positions. When the Shift-Out feature is included, the six-row typebox is replaced with a seven-row typebox allowing 112 pallet positions, or it can be replaced with an eight-row typebox allowing 128 type pallet positions. The Model 37 interface meets the requirements of EIA RS-232-B. The Model 37 has a recommended maintenance interval of every six months or every 1500 hours. The Model 37 is 36.25 inches high. The Model 37 ASR and KSR are 27.5 inches deep. The Model 37 RO is 24.25 inches deep. The Model 37 ASR weighs 340 pounds. The Model 37 KSR and RO weighs approximately 185 pounds.\n\nThe 4100 Paper Tape Equipment consists of the 4110 series of synchronous paper tape readers (CX), 4120 series of synchronous punches (BRPE), 4130 series of asynchronous readers (DX) and the 4140 series of asynchronous punches (DRPE).\n\nThe CX readers operate at 107 characters per second, with two fixed-level versions for reading either 6-level or 8-level fully punched or chadless paper tape, and an adjustable-level version for reading 5, 6, 7 or 8-level paper tape. All CX version readers were packaged for table-top use and have a parallel contact interface which is wired directly to the reader tape sensing contacts.\n\nThe BRPE tape punches operate at 110 characters per second, with two adjustable-level versions for reading either 5 or 8-level paper tape or 6, 7 or 8-level paper tape. A fixed-level 6-level paper tape punch was also available. All BRPE version paper tape punches were packaged for table-top use and have a parallel contact interface which is wired directly to the punch electromagnetic coils.\n\nThe DX paper tape readers operate at any speed up to 360 characters per second via external timing, except EIA. EIA tape readers operate at 120 characters per second if internal timing is used, and up to 314 characters per second if external timing is used. All of the DX paper tape readers are adjustable-level, reading 5, 6, 7 or 8-level fully punched or chadless paper tape. The DX series of paper tape readers were available in table-top and rack mount models, with an optional verifier data output for use by an external verifier logic to guarantee the accuracy of each character read.\n\nThe DRPE tape punches operate at any speed up to 240 characters per second with an adjustable-level version for reading 6, 7 or 8-level paper tape and a fixed-level 6-level version. All DRPE version paper tape punches were packaged in table-top and rack-mount models, with an optional verifier logic which read each character immediately after it was punched, compared it to the character received and provided an error output pulse if they did not agree.\n\nThe Teletype Model 38 is a 110 baud terminal that utilizes a serial input / output 11 unit code signal consisting of a start bit, seven information bits, an even parity bit and two stop bits. The Model 38 was produced in ASR, KSR and RO versions. The Model 38 handles USASI X3.4-1968 ASCII Code and prints 94 graphics of this code with letters in both upper and lower case, with the option to print in two colors on a pin-fed, 14-7/8 inch wide page. The Model 38 interface will accept either DC current (20 ma or 60 ma) or EIA RS-232-C. A built-in modem interface provides for operation over voice-grade channels. The modem transmits asynchronously in serial format, compatible with Bell System 101, 103, 108 and 113 data sets or their equivalent.\n\nThe Teletype Model 42 and Teletype Model 43 are electronic teleprinters providing character-at-a-time keyboard-printer send receive operation. The Teletype Model 42 is the Baudot variant of the Model 43, which is an ASCII teleprinter. The Model 43 has two transmission speeds, 100 words-per-minute and 300 words-per-minute, selectable by the operator to match the distant station speed. The Teletype Model 43 printer could print up to 80 characters per line using the friction feed printer option. Model 43 sprocket feed printers print messages with up to 132 characters per line using 12 inch wide sprocket feed paper. The tractor feed printer will print messages with up to 80 characters per line using 12 inch sprocket feed paper.\n\nThe Teletype Dataspeed 40 combined electronic CRTs and high speed printer terminals and were networked in many different applications through the use of \"cluster controllers\" and digital data service units. The Dataspeed 40 interfaced a synchronous modem usually running at 2400, 4800 or 9600 baud. The trademarked term \"Dataspeed\" originated with a series of high speed paper tape terminals that sent and received oil-treated punched paper tape at 1050 words per minute. AT&T used the Dataspeed 40 terminals internally for their Switching Control Center System and similar purposes. The Dataspeed 40 was also sold commercially and used for a variety of purposes and displaced some older and slower speed Teletype equipment.\n\nAN/FGC-5 electronic four-channel time-division multiplex system, using vacuum tube technology. AN/UGC-1 transistorized four-channel multiplex and AN/UGC-3 sixteen channel multiplex. ADIS automatic data interchange system for the Federal Aviation Agency, handling weather data. BDIS automatic switching system for the F.A.A., handling flight plan data. AIDS a similar data switching system for New York Telephone Co.\n\n"}
{"id": "8561726", "url": "https://en.wikipedia.org/wiki?curid=8561726", "title": "The Design of Everyday Things", "text": "The Design of Everyday Things\n\nThe Design of Everyday Things is a best-selling book by cognitive scientist and usability engineer Donald Norman about how design serves as the communication between object and user, and how to optimize that conduit of communication in order to make the experience of using the object pleasurable. One of the main premises of the book is that although people are often keen to blame themselves when objects appear to malfunction, it is not the fault of the user but rather the lack of intuitive guidance that should be present in the design.\n\nThe book was published in 1988 with the title The Psychology of Everyday Things. Norman said his academic peers liked that title, but believed the new title better conveyed the content of the book and better attracted interested readers. It is often referred to by the initialisms POET and DOET.\n\nNorman uses case studies to describe the psychology behind what he deems good and bad design, and proposes design principles. The book spans several disciplines including behavioral psychology, ergonomics, and design practice.\n\nA major update of the book, The Design of Everyday Things: Revised and Expanded Edition, was published in 2013.\n\nIn the book, Norman introduced the term \"affordance\" as it applied to design, adding a perceptual dimension to James J. Gibson's concept of the same name. Examples of affordances are flat plates on doors meant to be pushed, small finger-size push-buttons, and long and rounded bars we intuitively use as handles. As Norman used the term, the plate or button \"affords\" pushing, while the bar or handle affords pulling. Norman discussed door handles at length.\n\nHe also popularized the term \"user-centered design\", which he had previously referred to in \"User Centered System Design\" in 1986. He used the term to describe design based on the needs of the user, leaving aside what he deemed secondary issues like aesthetics. User-centered design involves \"simplifying the structure of tasks\", \"making things visible\", \"getting the mapping right\", \"exploiting the powers of constraint\", \"designing for error\", \"explaining affordances\" and seven stages of action. He goes to great lengths to define and explain these terms in detail, giving examples following and going against the advice given and pointing out the consequences.\n\nOther topics of the book include:\n\n\nAfter a group of industrial designers felt affronted after reading an early draft, Norman rewrote the book to make it more sympathetic to the profession.\n\n"}
{"id": "9610271", "url": "https://en.wikipedia.org/wiki?curid=9610271", "title": "UCL Department of Science and Technology Studies", "text": "UCL Department of Science and Technology Studies\n\nThe UCL Department of Science and Technology Studies (STS) is an academic department in University College London, London, England. It is part of UCL's Faculty of Mathematics and Physical Sciences. The department offers academic training at both undergraduate and graduate (MSc and MPhil/PhD) levels.\n\nThe department received its current name in 1994. It had been the \"Department of History and Philosophy of Science\" from 1938 to 1994, and the \"Department of History and Method of Science\" from 1921 to 1938.\n\nUniversity College London was the first UK university to offer single honours undergraduate degrees in this interdisciplinary subject, launching its BSc in history and philosophy of science in 1993. Two related BSc degrees followed shortly thereafter. At UCL, science and technology studies (abbreviated \"STS\") includes three specialist research clusters: \"history of science,\" \"philosophy of science,\" and \"science, culture, and democracy\".\n\nThe department offices are located in Gordon Square, Bloomsbury, London.\n\n"}
{"id": "47296822", "url": "https://en.wikipedia.org/wiki?curid=47296822", "title": "Wistarburg Glass Works", "text": "Wistarburg Glass Works\n\nThe Wistarburg Glass Works (sometimes spelled Wistarburgh Glass Works; also known as the United Glass Company) was the first successful glass factory and joint-venture enterprise in the Thirteen Colonies, referred to as Colonial America. Caspar Wistar founded the glass works company in 1739. He began by recruiting experienced glass artisans from Europe, and built homes for the workers along with a mansion for the factory's foreman. Wistar also had a company store built near the factory.\n\nThe village that developed around the factory adopted Wistar's name, and became known as Wistarburg. The village was in Salem County, New Jersey, in the township of Alloway. Wistar's factory produced thousands of glass bottles per year, as well as window glass. Wistar was a friend of Benjamin Franklin, and made glass globes for Franklin's electricity-producing machines used for scientific research. Wistar's son inherited the business and his son, Wistar's grandson, eventually gained control of the company, but owing to his mismanagement it closed.\n\nCaspar Wistar (1696–1752) emigrated from the Palitinate region of Germany to Philadelphia in 1717. There he began making brass buttons, which he would take with him on sales visits to Salem County, New Jersey. On one such trip he noticed that the township of Alloway had an abundant supply of the necessary materials for glass manufacture – white sand, clay, wood, and accessible waterways of the nearby Deep Run and Alloway Creek rivers for transporting raw materials in and finished products out. He was thus encouraged to establish a glass factory there. Wistar's was not the first to be built in colonial America. Earlier establishments had been setup at Jamestown, Virginia, in 1607 and 1621. They were founded to export glass items to England, but ultimately failed, having produced little, if any, glass.\n\nWistar's button-making business proved to be a success, and together with his gains from speculating in land allowed him to accumulate sufficient capital to fund a new enterprise. He began his glass-making venture in 1737 by buying of land about from Salem, New Jersey, and commenced construction of his factory – which became the first commercially successful glass factory in America. He arranged in January 1738 to lease of land containing of wood from John Ladd, a local landowner. Wistar then recruited four experienced glass makers from the Palitinate region of Germany – C. Halter, S. Griessmeyer, J. Wentzel, and J. Halter – to make the factory operational. He organized a joint venture with them, dividing profits in exchange for the art of glass making. The four artisans were to teach this art to the Wistars and nobody else in the colonies. Wistar had arranged for their journey to America, and they arrived in Philadelphia from Rotterdam on the ship \"Two Sisters\" in September 1738.\n\nWistar had houses built near the factory to rent for the key artisans and other workers that were usually indentured immigrants. He also had a mansion constructed for the factory's foreman, which also served as a lodging and office for Wistar while on his visits to the factory from his home in Philadelphia. A company store was constructed for the workers' needs. Products could be purchased on credit against the glass a worker would make in the future; a bookkeeper was employed to keep track of the store accounts and housing rents. The store also served the people of the village of Alloway, and was the center of community life.\n\nWistar entered into a profit-sharing arrangement with the four German artisans who had helped to make his new factory operational. To ensure their continuing support he set up a joint venture between five members, the first cooperative manufacturing joint-venture business in America and the first long-term successful glasshouse. The 1739 company began trading as the United Glass Company, which consisted of three individual joint ventures. Wistar was the general manager and primary share holder, with a two-thirds majority ownership in each of the joint ventures. The four Germans held one-third ownership and shared the expenses, assets, and profits of the three individual joint ventures. The first entity under the umbrella enterprise was held between Wistar and Wentzel, the second between Wistar and C. Halter, and the third between Wistar, Griesmeyer and J. Halter. The joint venture arrangement remained in effect until Wistar's death in 1752.\n\nWistar's factory produced about 15,000 glass bottles per year made in the Waldglas style, which had been a way of making glass in Europe since the Middle Ages. It was an inexpensive traditional method whereby the main materials of wood ash and sand produced a greenish-yellow glass. The factory produced window glass and was the first American supplier for the thirteen colonies. The Glass House (as it was often called) also produced rum flasks and tableware. Wistar had unlimited access to white silica sand, as well as other necessary materials, and the company's success was further aided by New Jersey's low taxation; Wistar was easily able to obtain more assets for the business.\n\nWistar lived in Philadelphia, and was aware that the British did not allow the manufacture of certain items that were in competition with those imported from Britain. He therefore downplayed the profits from his glass business, so as to avoid arousing the suspicions of the British. Wistar ran his business affairs from Philadelphia, spending little time in Alloway. He sold the manufactured glass products through his store in Philadelphia, the Wistarburg company store, and his son Richard's store in New York City.\n\nWistar was friends with Benjamin Franklin of Philadelphia and made glass globes for Franklin's electricity producing machines, used for scientific research into electricity. Franklin built several of his machines, using the Wistarburg glass globes, for Cadwallader Colden and Lewis Evans, for which they paid him between ten and twelve pounds each. The Wistarburg Glass Works also made glass tubes for David Rittenhouse to use in his experiments on electricity.\n\nWistar died of dropsy in Philadelphia on March 21, 1752, and his son Richard took over the glass works. Richard also mostly ran the business from Philadelphia, but worked on increasing sales and expanding the company's product range. Just like his father, he relied on hired artisans to run the glass works factory in Alloway. The glass enterprise continued into the 1770s, but struggled owing to deforestation around Wistarburg. Wood from the trees was needed as fuel to run the glass furnaces, and when supplies began to run short some of the company's key artisans resigned and went to start their own glass company in Glassboro, New Jersey, where there were new uncut forests and plenty of wood. Richard also had personal problems that occupied much of his time, and financial difficulties caused by the Revolutionary War; in an attempt to make ends meet, he sold a major portion of his father's New Jersey real estate. Richard died in 1781, and his son John took over the business. John failed to devote the effort necessary to run the glass works, allowing it to go into serious decline and ultimately it went out of business. The final year of the enterprise is disputed by scholars, but it is known that in 1793 the property that the factory was on was divided up among Richard's heirs and sold in parcels.\n\n"}
