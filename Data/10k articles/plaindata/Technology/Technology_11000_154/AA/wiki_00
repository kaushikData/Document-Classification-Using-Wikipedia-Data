{"id": "29758209", "url": "https://en.wikipedia.org/wiki?curid=29758209", "title": "4-Dimethylaminophenylpentazole", "text": "4-Dimethylaminophenylpentazole\n\n4-Dimethylaminophenylpentazole is an unstable, explosive compound that contains the rare pentazole ring, which is composed of five nitrogen atoms. The electron donating effect of the 4-dimethylamino substituent on the phenyl ring makes this compound one of the more stable of the phenylpentazoles. At room temperature, its chemical half-life is only a few hours, although storage is possible at cryogenic temperatures. The compound was first prepared in 1956 along with other substituted phenylpentazoles. Studies have been conducted on various other derivatives, though necessarily limited by the instability of these compounds. Some more highly substituted derivatives, such as 2,6-dihydroxy-4-dimethylaminophenylpentazole, are slightly more stable but conversely, more difficult to make. Current research has focused on forming transition metal complexes of these pentazole derivatives, as the pentazole ring should be stabilised by bonding to the metal centre.\n"}
{"id": "25469171", "url": "https://en.wikipedia.org/wiki?curid=25469171", "title": "Advanced Digital Broadcast", "text": "Advanced Digital Broadcast\n\nAdvanced Digital Broadcast (ADB) is a company which provides and integrates software, system and service solutions to pay-TV operators, content producers and property owners.\n\nADB's global headquarters is located in Eysins, Switzerland; they have research and development facilities in Poland and in Italy and an Operations division in Taipei, Taiwan. ADB has a local offices in several countries in Europe, the United States and Asia.\n\nFounded in 1995, ADB initially focused on developing and marketing software for digital TV processors and expanded its business to the design and manufacture of digital TV equipment in 1997. The company sold its first set-top box in 1997 and since then has been delivering a number of set-top boxes, and Gateway devices, together with advanced software platforms. ADB has sold over 60 million devices worldwide to cable, satellite, IPTV and broadband operators. ADB employs over 500 people, of which 70% are in engineering functions.\n\nIn 1995, ADB was founded by Andrew Rybicki with an initial focus on developing and marketing software for advanced digital TV processors.\n\nIn 1997, ADB started designing and manufacturing of digital TV equipment. ADB designed its first commercial set-to box, and established its dedicated R&D facility in Poland and corporate headquarters in Taiwan.\n\nBetween 1998 and 2000 offices were opened in Australia, and Spain, and ADB sold its one millionth set-top box.\n\nIn 2001, ADB announced the development of an open standard set-top box middleware solution based on the Multimedia Home Platform (MHP) specification and established its worldwide headquarters in Geneva, Switzerland. The company sold its two millionth set-top box.\n\nIn 2002, the company opened its Americas headquarters in Chicago (since moved to Denver). and became the world's first set-top box provider to launch an MHP digital receiver, the i-CAN 3000, in Finland.\n\nIn 2005, ADB Group was floated on the Swiss Stock Exchange (SWX)\n\nIn 2015, ADB rebranded and changed the company logo.\n"}
{"id": "20993959", "url": "https://en.wikipedia.org/wiki?curid=20993959", "title": "Airborne particulate radioactivity monitoring", "text": "Airborne particulate radioactivity monitoring\n\nContinuous particulate air monitors (CPAMs) have been used for years in nuclear facilities to assess airborne particulate radioactivity (APR). In more recent times they may also be used to monitor people in their homes for the presence of manmade radioactivity. These monitors can be used to trigger alarms, indicating to personnel that they should evacuate an area. This article will focus on CPAM use in nuclear power plants, as opposed to other nuclear fuel-cycle facilities, or laboratories, or public-safety applications.\n\nIn nuclear power plants, CPAMs are used for measuring releases of APR from the facility, monitoring levels of APR for protection of plant personnel, monitoring the air in the reactor containment structure to detect leakage from the reactor systems, and to control ventilation fans, when the APR level has exceeded a defined threshold in the ventilation system.\n\nCPAMs use a pump to draw air through a filter medium to collect airborne particulate matter that carries very small particles of radioactive material; the air itself is not radioactive. The particulate radioactive material might be natural, e.g., radon decay products (\"progeny\", e.g., Pb), or manmade, usually fission or activation products (e.g., Cs), or a combination of both. There are also \"gas monitors\" which pass the air through a sample chamber volume which is viewed continuously by a radiation detector. Radionuclides that occur in the gaseous form (e.g., Kr) are not collected on the CPAM filter to any appreciable extent, so that a separate monitoring system is needed to assess these nuclide concentrations in the sampled air. These gas monitors are often placed downstream of a CPAM so that any particulate matter in the sampled air is collected by the CPAM and thus will not contaminate the gas monitor's sample chamber.\n\nIn monitoring, the region of deposition of this material onto the filter medium is \"continuously\" viewed by a radiation detector, concurrent with the collection. This is as opposed to a sampling system\",\" in which the airborne material is collected by pumping air, usually at a much higher volumetric flowrate than a CPAM, through a collection medium for some period of time, but there is no continuous radiation detection; the filter medium is removed \"periodically\" from the sampler and taken to a separate radiation detection system for analysis.\n\nIn general, sampling has better detection sensitivity for low levels of airborne radioactivity, due to the much larger total volume of air passing through the filter medium over the sampling interval (which may be on the order of hours), and also due to the more sophisticated forms of quantitative analysis available once the filter medium is removed from the sampler. On the other hand, monitoring with CPAMs provides nearly real-time airborne radioactivity level indication. It is common practice to refer to \"sampled\" air even when discussing a CPAM, i.e., as opposed to \"monitored\" air, which would, strictly, be more correct.\n\nThere are two major types of CPAMs, fixed-filter and moving-filter. In the former, the filter medium does not move while the airborne material is collected. The latter type has two main variants, the rectangular deposition area (“window”) and the circular window. In both types of CPAM the sampled air is pulled (not pushed) by a pump through the piping of the monitor up to the structure that holds the filter medium. It is important to note that CPAM pumps are specially designed to maintain a constant volumetric flowrate.\n\nAs the air passes through the collection medium (usually a form of filter paper), particulate matter is deposited onto the filter in either a rectangular or circular pattern, depending on the instrument's design, and then the air continues on its way out of the monitor. The \"entire\" deposition area, regardless of its geometric shape, is assumed to be viewed by a radiation detector of a type appropriate for the nuclide in question.\n\nMoving-filter monitors are often used in applications where loading of the filter medium with dust is an issue; this dust loading reduces the air flow over time. The moving-filter collection medium (“tape”) is assumed to move across the deposition area at a constant, known rate. This rate is often established in such a way that a roll of the filter tape will last about one month; a typical filter movement rate is about one inch per hour.\n\nThe rectangular-window moving filter monitor will be denoted as RW, and the circular, CW. Fixed filter is FF.\n\nCPAMs are used to monitor the air effluents from nuclear facilities, notably power reactors. Here the objective is to assess the amount of certain radionuclides released from the facility. Real-time measurement of the very low concentrations released by these facilities is difficult; a more-reliable measurement of the \"total\" radioactivity released over some time interval (days, perhaps weeks) may in some cases be an acceptable approach. In effluent monitoring, a sample of the air in the plant stack is withdrawn and pumped (pulled) down to the CPAM location. This sampled air in many cases must travel a considerable distance through piping. Extracting and transporting the particulates for the CPAM to measure in such a way that the measurement is representative of what is being released from the facility is challenging.\n\nIn the USA there are effluent monitoring requirements in both 10CFR20 and 10CFR50; Appendix B to the former and Appendix I to the latter are especially important. 10CFR50 Appendix A states:\n\nAlso in the USA, Regulatory Guide 1.21, \"Measuring, Evaluating, and Reporting Radioactivity in Solid Wastes and Releases of Radioactive Materials in Liquid and Gaseous Effluents from Light-Water-Cooled Nuclear Power Plants\" is highly relevant to this CPAM application.\n\nFor occupational exposure (inhalation) assessment, CPAMs may be used to monitor the air in some volume, such as a compartment in a nuclear facility where personnel are working. A difficulty with this is that, unless the air in the compartment is uniformly mixed, the measurement made at the monitor location may not be representative of the concentration of radioactive material in the air that the workers are breathing. For this application the CPAM may be physically placed directly in the occupied compartment, or it may extract sampled air from the HVAC system that serves that compartment.\nThe following portions of 10CFR20 are relevant to the requirement for occupational exposure CPAM applications in the USA: 10CFR20.1003 (definition of Airborne Radioactivity Area), 1201, 1204, 1501, 1502, 2103.\n\nRadiation monitors in general have a number of process-control applications in nuclear power plants; a major CPAM application in this area is the monitoring of the air intake for the plant control room. In the event of an accident, high levels of airborne radioactivity could be brought into the control room by its HVAC system; the CPAM monitors this air and is intended to detect high concentrations of radioactivity and shut down the HVAC flow when necessary.\n\nFor use in the USA, standard 10CFR50 Appendix A states:\n\nThis defines a requirement for monitoring the air intake for the control room, such that the exposure limits, including for inhalation exposure, shall not be exceeded. CPAMs are often used for this.\n\nLeakage from the so-called \"reactor coolant pressure boundary\" is required to be monitored in USA nuclear power plants. Monitoring the airborne particulate radioactivity in the reactor containment structure is an acceptable method to meet this requirement, and so CPAMs are used. It is the case that when primary coolant escapes into the containment structure, certain noble gas nuclides become airborne, and subsequently decay into particulate nuclides. One of the most common of these pairs is Kr and Rb; the latter is detected by the CPAM. Relating the observed CPAM response to the Rb back to a leakage rate from the primary system is far from trivial.\n\nThe regulatory basis for this CPAM application is found in 10CFR50:\n\nFor use in the USA, standard 10 CFR 50, Appendix A, \"General Design Criteria for Nuclear Power Plants,\" Criterion 30, \"Quality of reactor coolant pressure boundary,\" requires that means be provided for detecting and, to the extent practical, identifying the location of the source of reactor coolant leakage. The specific attributes of the reactor coolant leakage detection systems are outlined in Regulatory Positions 1 through 9 of Regulatory Guide 1.45.\n\nFor use in the USA, standard 10 CFR 50.36, \"Technical Specifications,\" paragraph (c)(2)(ii)(A), specifies that a Limiting Condition for Operation be established for installed instrumentation that is used to detect and indicate in the control room a significant abnormal degradation of the reactor coolant pressure boundary. This instrumentation is required by Specification 3.4.15, \"RCS Leakage Detection Instrumentation.\"\n\nStep changes in reactor coolant leakage can be detected with moving filter media to satisfy the quantitative requirements of USNRC Regulatory Guide 1.45. [See description for US Patent Number 5343046 (1994).] The mathematical method is highly detailed and it focuses on time-dependent viewable collected activity, rather than concentration, as f(t). The method, among other features, yields the desired fixed-filter degenerate case (filter paper velocity = 0.) The method was first put into use in the 1990s at a nuclear power plant in the United States. Though originally derived for dominant Kr-88/Rb-88 in leaked reactor coolant, it has been expanded to include Xe-138/Cs-138 and can be modified by replication to include any N similar pairings. Further refinements to the mathematical methodology have been made by the inventor; these developments obviate the described patented collimator apparatus for making quantitative assessment of leak rate step change when rectangular collection grids are employed.\n\nThe response of the monitor is sensitive to the half-life of the nuclide being collected and measured. It is useful to define a \"long-lived\" (LL) nuclide to have negligible decay during the measurement interval. On the other hand, if the decay cannot be ignored, the nuclide is considered \"short-lived\" (SL). In general, for the monitor response models discussed below, the LL response can be obtained from the SL response by taking limits of the SL equation as the decay constant approaches zero. If there is any question about which response model to use, the SL expressions will \"always\" apply; however, the LL equations are considerably simpler and so should be used when there is no question about the half-life (e.g., Cs is LL).\n\nThe output of the radiation detector is a random sequence of pulses, usually processed by some form of \"ratemeter,\" which continuously estimates the rate at which the detector is responding to the radioactivity deposited on the filter medium. There are two fundamental types of ratemeters, analog and digital. The ratemeter output is called the countrate, and it varies with time.\n\nRatemeters of both types have the additional function of \"smoothing\" the output countrate estimate, i.e., reducing its variability. (This process is more correctly termed \"filtering.\") Ratemeters must make a tradeoff between this necessary variance reduction and their response time; a smooth output (small variance) will tend to lag behind an increase in the true pulse rate. The significance of this lag depends on the application of the monitor.\n\nEven when the filter medium is clean, that is, before the pump is started that pulls the air through the filter, the detector will respond to the ambient \"background\" radiation in the vicinity of the monitor. The countrate that results from deposited radioactivity is called the \"net\" countrate, and is obtained by subtracting this background countrate from the dynamically-varying countrate that is observed once the pump is started. The background is usually assumed to be constant.\n\nThe countrate of the monitor varies dynamically, so that a measurement time interval must be specified. Also, these are integrating devices, meaning that some finite time is required to accumulate radioactivity onto the filter medium. The input to the monitor is, in general, a time-dependent concentration in air of the specified nuclide. However, for the calculations given below, this concentration will be held constant over that interval.\n\nSince concentrations resulting from physical events tend to vary with time, due to dilution processes and/or a nonconstant source term (airborne radioactivity emission rate), it is not realistic to hold the concentration constant for significant lengths of time. Thus, measurement intervals on the order of several hours are not plausible for the purposes of these calculations.\n\nThere are situations in which a nuclide deposited on the CPAM filter decays into another nuclide, and that second nuclide remains on the filter. This \"parent-progeny\" or decay chain situation is especially relevant to so-called \"radon-thoron\" (RnTn) or natural airborne radioactivity. The mathematical treatment described in this article does not consider this situation, but it can be treated using matrix methods (see Ref [11]).\n\nAnother issue is the fact that in a power reactor context it would be unusual for a CPAM to be collecting only a single particulate nuclide; more likely there would be a mixture of fission product and activation product nuclides. The modeling discussed in this article considers only one nuclide at a time. However, since the radiation emitted by each nuclide is independent of the others, so that the nuclides present on the filter medium do not interact with each other, the monitor response is the linear combination of the individual responses. Thus the overall CPAM response to a mixture is just the superposition (i.e., the sum) of the individual responses.\n\nCPAMs use either a Geiger tube, for \"gross beta-gamma\" counting, or a NaI(Tl) crystal, often for simple single-channel gamma spectroscopy. (In this context, \"gross\" means a measurement that does not attempt to find the specific nuclides in the sample.) Plastic scintillators are also popular. Essentially, in power reactor applications, beta and gamma are the radiations of interest for particulate monitoring.\n\nIn other fuel-cycle applications, such as nuclear reprocessing, alpha detection is of interest. In those cases, the interference from other isotopes such as RnTn is a major problem, and more sophisticated analysis, such as the use of HPGe detectors and multichannel analyzers, are used where spectral information, such as is used for Radon compensation, is required.\n\nRadioiodine (especially I) monitoring is often done using a particulate-monitor setup, but with an activated charcoal collection medium, which can adsorb some iodine vapors as well as particulate forms. Single-channel spectroscopy is usually specified for iodine monitors.\n\nDetailed mathematical models that describe the dynamic, time-dependent countrate response of these monitors in a very general manner are presented in and will not be repeated here. For the purpose of this article, a few useful results from that paper will be summarized. The objective is to predict the net countrate of a CPAM for a single, specific manmade nuclide, for a given set of conditions. That predicted response can be compared to the expected background and/or interferences (nuclides other than the one sought), to assess the monitor’s detection capability. The response predictions can also be used to calculate alarm setpoints that correspond to appropriate limits (such as those in 10CFR20) on the concentration of airborne radioactivity in the sampled air.\n\nThe parameters used in these models are summarized in this list:\n\n\n\"Line loss\" refers to the losses of particulate matter in transit from a sampling point to the monitor; thus the concentration measured would be somewhat lower than that in the original sampled air. This factor is meant to compensate for these losses. Sampling lines are specifically designed to minimize these losses, for example, by making bends gradual as opposed to right-angled. These lines (pipes) are needed since in many applications the CPAM cannot be physically located directly in the sampled air volume, such as a nuclear power plant's main stack, or the ventilation air intake for the plant control room.\n\n\"Emission abundance\" refers to the fact that the disintegration of any given nucleus of the isotope of interest in the CPAM analysis may not result in the emission of the radiation being detected (e.g., a beta particle or gamma ray). Thus, overall there will be some fraction of the disintegrations that emit the radiation of interest (e.g. the 662 keV gamma ray of Cs is emitted in about 85% of the disintegrations of Cs nuclei).\n\nThe response models are based on the consideration of the sources and losses of the deposited radioactivity on the filter medium. Taking the simplest case, the FF monitor, this leads to a differential equation which expresses the rate of change of the monitor countrate:\nThe first term accounts for the source of radioactivity from the sampled air, and the second term is the loss due to the decay of that radioactivity. A convenient way to express the solution to this equation uses the scalar convolution integral, which results in\n\nThe last term accounts for any initial activity on the filter medium, and is usually set to zero (clean filter at time zero). The initial countrate of the monitor, before the concentration transient begins, is only that due to ambient background. If radon progeny are present, they are assumed to be at equilibrium and generating a constant countrate that adds to the ambient background’s countrate.\n\nVarious solutions for the time-dependent FF countrate follow directly, once a concentration time-dependence \"Q(t)\" has been specified. Note that the monitor flowrate \"F\" is assumed constant; if it isn't, and its time-dependence is known, then that \"F(t)\" would need to be placed inside the integral. Also note that the time variable in all the models is measured from the instant the concentration in the sampled air begins to increase.\n\nFor the moving-filter CPAMs, the above expression is a starting point, but the models are considerably more complicated, due to (1) the loss of material as the filter medium moves away from the detector's field of view and (2) the differing lengths of time that parts of the filter medium have been exposed to the sampled air. The basic modeling approach is to break down the deposition regions into small differential areas and then consider how long each such area receives radioactive material from the air.\n\nThe resulting expressions are integrated across the deposition region to find the overall response. The RW solution consists of two double integrals, while the CW response solution consists of three triple integrals. A very important consideration in these models is the \"transit time,\" which is the time required for a differential area to traverse the window along its longest dimension. As a practical matter, the transit time is the time required for \"all\" differential elements that were in the deposition window at time zero to leave the window.\n\nThis figure shows contours of constant activity on a CW deposition area, after the transit time has expired. The filter moves from left to right, and the activity increases from left to right. The differential areas on the diameter have been in the deposition window the longest, and at the far right, have been in the window, accumulating activity, for the full transit time.\n\nFinally, to illustrate the complexity of these models, the RW response for time less than the transit time is\n\\dot C_{RW} (t)\\,\\,\\, = \\,\\,\\,\n"}
{"id": "38324933", "url": "https://en.wikipedia.org/wiki?curid=38324933", "title": "Applications of nanotechnology", "text": "Applications of nanotechnology\n\nThe 2000s have seen the beginnings of the applications of nanotechnology in commercial products, although most applications are limited to the bulk use of passive nanomaterials. Examples include titanium dioxide and zinc oxide nanoparticles in sunscreen, cosmetics and some food products; silver nanoparticles in food packaging, clothing, disinfectants and household appliances such as Silver Nano; carbon nanotubes for stain-resistant textiles; and cerium oxide as a fuel catalyst. As of March 10, 2011, the Project on Emerging Nanotechnologies estimated that over 1300 manufacturer-identified nanotech products are publicly available, with new ones hitting the market at a pace of 3–4 per week.\n\nNanotechnology is being used in developing countries to help treat disease and prevent health issues. The umbrella term for this kind of nanotechnology is Nanomedicine.\n\nNanotechnology is also being applied to or developed for application to a variety of industrial and purification processes. Purification and environmental cleanup applications include the desalination of water, water filtration, wastewater treatment, groundwater treatment, and other nanoremediation. In industry, applications may include construction materials, military goods, and nano-machining of nano-wires, nano-rods, few layers of graphene, etc. Also, recently a new field arisen from the root of Nanotechnology is called Nanobiotechnology. Nanobiotechnology is the biology-based, application-oriented frontier area of research in the hybrid discipline of Nanoscience and biotechnology with an equivalent contribution.\n\n"}
{"id": "3095430", "url": "https://en.wikipedia.org/wiki?curid=3095430", "title": "Association for Manufacturing Technology", "text": "Association for Manufacturing Technology\n\nThe Association for Manufacturing Technology (AMT) is a trade association based in McLean, Virginia, in the United States. It was founded as the National Machine Tool Builders' Association (NMTBA) in 1902. It represents and promotes the interests of American providers of manufacturing machinery and equipment.\n\nThe AMT’s most visible activity is its management of the International Manufacturing Technology Show (IMTS), which is held on even-numbered years at McCormick Place in Chicago, Illinois.\n\nThe AMT is a committee-driven organization that services the manufacturing community through volunteer participation of its membership. Examples of activities fostered by the AMT include:\n\n\nAMT also supports their membership with technology centers and representative offices in China, India, Poland, Mexico and Brazil.\n\n\n"}
{"id": "1455693", "url": "https://en.wikipedia.org/wiki?curid=1455693", "title": "Barrington Moore Jr.", "text": "Barrington Moore Jr.\n\nBarrington Moore Jr. (12 May 1913 – 16 October 2005) was an American political sociologist, and the son of forester Barrington Moore. He is famous for his \"Social Origins of Dictatorship and Democracy: Lord and Peasant in the Making of the Modern World\" (1966), a comparative study of modernization in Britain, France, the United States, China, Japan, Russia, Germany, and India. His many other works include \"Reflections on the Causes of Human Misery\" (1972) and an analysis of rebellion, \"Injustice: the Social Basis of Obedience and Revolt\" (1978).\n\nHe graduated from Williams College, Massachusetts, where he received a thorough education in Latin and Greek and in history. He also became interested in political science, and was elected to Phi Beta Kappa. In 1941, Moore obtained his Ph.D. in sociology from Yale University. He worked as a policy analyst for the government, in the Office of Strategic Services (OSS) and at the Department of Justice. He met Herbert Marcuse, a lifelong friend, and also his future wife, Elizabeth Ito, at the OSS. His wife died in 1992. They had no children.\n\nHis academic career began in 1945 at the University of Chicago, in 1948 he went to Harvard University, joining the \"Russian Research Center\" in 1951. He was emerited in 1979. Moore published his first book, \"Soviet Politics\" in 1950 and \"Terror and Progress, USSR\" in 1954. In 1958 his book of six essays on methodology and theory, \"Political Power and Social Theory\", attacked the methodological outlook of 1950s social science. His students at Harvard included comparative social scientists Theda Skocpol, and Charles Tilly.\n\nMoore's groundbreaking work \"Social Origins of Dictatorship and Democracy\" (1966), was the cornerstone to what is now called comparative historical analysis in the social sciences. In that work he studied the conditions for the sociogenesis of democratic, fascist and communist regimes, looking especially at the ways in which industrialization and the pre-existing agrarian regimes interacted to produce those different political outcomes. He drew particular attention to the violence which preceded the development of democratic institutions.\n\nMoore lists five conditions for the development of Western-style democracy (through a \"bourgeois revolution\"):\n\nMoore's concern was the transformation of pre-industrial agrarian social relations into \"modern\" ones. He highlighted what he called \"three routes to the modern world\" - the liberal democratic, the fascist, and the communist - each deriving from the timing of industrialization and the social structure at the time of transition.\n\nIn the simplest sense, \"Social Origins\" can be summarized with his famous statement \"No bourgeois, no democracy\" though taking that idea at face value undercuts and misinterprets the nuances of his argument.\n\nMoore also directly addressed the Japanese transition to modernity through fascism and the communist path in China, while implicitly remarking on Germany and Russia.\nOne can see Moore's theme of the bourgeoisie again here - in the states that became democratic, there was a strong bourgeoisie. In Japan and China, the bourgeoisie was weak, and allied with the elites or peasants to create fascism or communism, respectively.\n\nThe wide range of critical response to \"Social Origins\" was examined by Jon Wiener in the journal \"History and Theory.\" \n\nIn 1965, Moore, Herbert Marcuse, and Robert Paul Wolff each authored an essay on the concept of tolerance and the three essays were collected in the book \"A Critique of Pure Tolerance\". The title was a play on the title of Immanuel Kant's book \"Critique of Pure Reason\". In the book Moore argues that academic research and society in general should adopt a strictly scientific and secular outlook and approach theories and conjectures with empirical verification.\n\n\n"}
{"id": "14627460", "url": "https://en.wikipedia.org/wiki?curid=14627460", "title": "Base isolation", "text": "Base isolation\n\nBase isolation, also known as seismic base isolation or base isolation system, is one of the most popular means of protecting a structure against earthquake forces. It is a collection of structural elements which should substantially decouple a superstructure from its substructure resting on a shaking ground thus protecting a building or non-building structure's integrity.\n\nBase isolation is one of the most powerful tools of earthquake engineering pertaining to the passive structural vibration control technologies. It is meant to enable a building or non-building structure to survive a potentially devastating seismic impact through a proper initial design or subsequent modifications. In some cases, application of base isolation can raise both a structure's seismic performance and its seismic sustainability considerably. Contrary to popular belief base isolation does not make a building earthquake proof.\n\n\"Base isolation system\" consists of \"isolation units\" with or without \"isolation components\", where:\n\n\nIsolation units could consist of shear or sliding units.\n\nThis technology can be used for both new structural design and seismic retrofit. In process of seismic retrofit, some of the most prominent U.S. monuments, e.g. Pasadena City Hall, San Francisco City Hall, Salt Lake City and County Building or LA City Hall were mounted on \"base isolation systems\". It required creating rigidity diaphragms and moats around the buildings, as well as making provisions against overturning and P-Delta Effect.\n\nBase isolation is also used on a smaller scale—sometimes down to a single room in a building. Isolated raised-floor systems are used to safeguard essential equipment against earthquakes. The technique has been incorporated to protect statues and other works of art—see, for instance, Rodin's \"Gates of Hell\" at the National Museum of Western Art in Tokyo's Ueno Park. \nThrough the George E. Brown, Jr. Network for Earthquake Engineering Simulation (NEES), researchers are studying the performance of base isolation systems. The project, a collaboration among researchers at University of Nevada, Reno; University of California, Berkeley; University of Wisconsin, Green Bay; and the University at Buffalo is conducting a strategic assessment of the economic, technical, and procedural barriers to the widespread adoption of seismic isolation in the United States. NEES resources have been used for experimental and numerical simulation, data mining, networking and collaboration to understand the complex interrelationship among the factors controlling the overall performance of an isolated structural system. This project involves earthquake shaking table and hybrid tests at the NEES experimental facilities at the University of California, Berkeley, and the University at Buffalo, aimed at understanding ultimate performance limits to examine the propagation of local isolation failures (e.g., bumping against stops, bearing failures, uplift) to the system level response. These tests will include a full-scale, three-dimensional test of an isolated 5-story steel building on the E-Defense shake table in Miki, Hyogo, Japan.\n\nAn adaptive base isolation system includes a tunable isolator that can adjust its properties based on the input to minimize the transferred vibration. Magnetorheological fluid dampers and isolators with Magnetorheological elastomer have been suggested as adaptive base isolators.\n\n"}
{"id": "298585", "url": "https://en.wikipedia.org/wiki?curid=298585", "title": "Beetle bank", "text": "Beetle bank\n\nA beetle bank, in agriculture and horticulture, is a form of biological pest control. It is a strip planted with grasses (bunch grasses) and/or perennial plants, within a crop field or a garden, that fosters and provides habitat for beneficial insects, birds, and other fauna that prey on pests.\n\nBeetle banks are typically made up from plants such as sunflowers, \"Vicia faba\", \"Centaurea cyanus\", coriander, borage, \"Muhlenbergia\", \"Stipa\", and buckwheats (\"Eriogonum spp.\"). Beetle banks are used to reduce or replace the use of insecticides, and can also serve as habitat for birds and beneficial rodents. For example, insects such as \"Chrysoperla carnea\" and the Ichneumon fly can prey on pests. The concept was developed by the Game & Wildlife Conservation Trust in collaboration with the University of Southampton.\n\nOther important benefits can be providing habitat for pollinators and endangered species. If using local native plants, endemic and indigenous flora and fauna restoration ecology is supported.\n\nAccording to a March 2005 draft entry for the \"Oxford English Dictionary\", the term first came into use in the early 1990s, with published examples including the August 22, 1992 issue of the \"New Scientist\" and an October 12, 1994 reference in \"The Guardian\" society section:\n\n\n"}
{"id": "1269039", "url": "https://en.wikipedia.org/wiki?curid=1269039", "title": "Bioaugmentation", "text": "Bioaugmentation\n\nBiological augmentation is the addition of archaea or bacterial cultures required to speed up the rate of degradation of a contaminant. Organisms that originate from contaminated areas may already be able to break down waste, but perhaps inefficiently and slowly.\n\nBioaugmentation usually requires studying the indigenous varieties present in the location to determine if biostimulation is possible. If the indigenous variety do not have the metabolic capability to perform the remediation process, exogenous varieties with such sophisticated pathways are introduced.\n\nBioaugmentation is commonly used in municipal wastewater treatment to restart activated sludge bioreactors. Most cultures available contain microbial cultures, already containing all necessary microorganisms (\"B. licheniformis\", \"B. thuringiensis\", \"P. polymyxa\", \"B. stearothermophilus\", \"Penicillium\" sp., \"Aspergillus\" sp., \"Flavobacterium\", \"Arthrobacter\", \"Pseudomonas\", \"Streptomyces\", \"Saccharomyces\", \"Triphoderma\", etc.). Activated sludge systems are generally based on microorganisms like bacteria, protozoa, nematodes, rotifers, and fungi, which are capable of degrading biodegradable organic matter. There are many positive outcomes from the use of bioaugmentation, such as the improvement in efficiency and speed of the process of breaking down substances and the reduction of toxic particles in an area.\n\nBioaugmentation is favorable in contaminated soils that have undergone bioremediation, but still pose an environmental risk. This is because microorganisms that were originally in the environment did not accomplish their task during bioremediation when it came to breaking down chemicals in the contaminated soil. The failure of original bacteria can be caused by environmental stresses, as well as changes in the microbial population due to mutation rates. When microorganisms are added, they are potentially more suited to the nature of the new contaminant, meanwhile the older microorganisms are similar to the older pollution and contamination. However, this is merely one of many factors; site size is also a very important determinant. In order to see whether bioaugmentation should be implemented, the overall setting must be considered. Also, some highly specialized microorganisms are not capable of adapting to certain site settings. Availability of certain microorganism types (as used for bioremediation) may also be a problem. Although bioaugmentation may appear to be a perfect solution for contaminated soil, it can have drawbacks. For example, the wrong type of bacteria can result in potentially clogged aquifers, or the remediation result may be incomplete or unsatisfactory.\nAt sites where soil and groundwater are contaminated with chlorinated ethenes, such as tetrachloroethylene and trichloroethylene, bioaugmentation can be used to ensure that the \"in situ\" microorganisms can completely degrade these contaminants to ethylene and chloride, which are non-toxic. Bioaugmentation is typically only applicable to bioremediation of chlorinated ethenes, although there are emerging cultures with the potential to biodegrade other compounds including BTEX, chloroethanes, chloromethanes, and MTBE. The first reported application of bioaugmentation for chlorinated ethenes was at Kelly Air Force Base, TX. Bioaugmentation is typically performed in conjunction with the addition of electron donor (biostimulation) to achieve geochemical conditions in groundwater that favor the growth of the dechlorinating microorganisms in the bioaugmentation culture.\n\nIncluding more microbes into an environment is beneficial to the speed of the cleanup duration. The interaction and competitions of two compounds influence the performance that a microorganism, original or new, could have. This can be tested by placing a soil that favors the new microbes into the area and then looking at the performance. The results will show if the new microorganism can perform well enough in that soil with other microorganisms. This helps to determine the correct amount of microbes and indigenous substances that are needed in order to optimize performance and create a co-metabolism. \n'Bioaugmentation Cultures.\n\nAn example of how bioaugmentation has improved an environment, is in the coke plant wastewater in China. Coal in China is used as a main energy source and the contaminated water contains harmful toxic contaminants like ammonia, thiocyanate, phenols and other organic compounds, such as mono- and polycyclic nitrogen-containing aromatics, oxygen and sulfur-containing heterocyclics and polynuclear aromatic hydrocarbons. Previous measures to treat this problem was an aerobic-anoxic-oxic system, solvent extractions, stream stripping, and biological treatment. Bioaugmentation has been reported to remove 3-chlorobenzoate, 4-methyl benzoate, toluene, phenol, and chlorinated solvents.\n\nThe anaerobic reactor was packed with semi-soft media, which were constructed by plastic ring and synthetic fiber string. The anoxic reactor is a completely mixed reactor while the oxic reactor is a hybrid bioreactor in which polyurethane foam carriers were added. Water from anoxic reactor, odic reactor and sedimentation tank were used and had mix-ins of different amount of old and developed microbes with .75 concentration and 28 degree Celsius. The rate of contaminant degradation depended on the amount of microbe concentration. In the enhanced microbial community indigenous microorganisms broke down the contaminants in the coke plant wastewater, such as pyridines, and phenolic compounds. When indigenous heterotrophic microorganisms were added, they converted many large molecular compounds into smaller and simpler compounds, which could be taken from more biodegradable organic compounds. This proves that bioaugmentation could be used as a tool for the removal of unwanted compounds that are not properly removed by conventional biological treatment system. When bioaugmentation is combined with A1–A2–O system for the treatment of coke plant wastewater it is very powerful.\n\nIn the petroleum industry, there is a large problem with how oilfield drilling pit is disposed of. Many used to simply place dirt over the pit, but it is far more productive and economically beneficial to use bioaugmentation. With the use of advanced microbes, drilling companies can actually treat the problem in the oilfield pit instead of transferring the waste around. Specifically, polycyclic aromatic hydrocarbons can be metabolized by some bacteria, which significantly reduces environmental damage from drilling activities. Given suitable environmental conditions, microbes are placed in the oilpit to break down hydrocarbons and alongside are other nutrients. Before treatment there was a total petroleum hydrocarbon (TPH) level of 44,880 ppm, which within just 47 days the TPH was lowered to a level of 10,000 ppm to 6,486 ppm.\n\nThere have been many instances where bioaugmentation had deficiencies in its process. Examples include Goldstein et al., 1985; Stephenson and Stephenson, 1992; Bouchez et al., 2000; Vogel and Walter, 2001; Wagner-Döbler, 2003. Many of these problems occurred because the microbial ecology issues were not taken into consideration in order to map the performance of the bioaugmentation. It is crucial to consider the microbes' ability to withstand the conditions in the microbial community to be placed in. In many of the cases that have failed, only the microbes' ability to break down compounds was considered and less their fitness in existing communities and the resulting competitive stress. It is better to identify the existing communities before looking at the strains needed to break down pollutants.\n\n\n"}
{"id": "2774201", "url": "https://en.wikipedia.org/wiki?curid=2774201", "title": "Boiler explosion", "text": "Boiler explosion\n\nA boiler explosion is a catastrophic failure of a boiler. As seen today, boiler explosions are of two kinds. One kind is a failure of the pressure parts of the steam and water sides. There can be many different causes, such as failure of the safety valve, corrosion of critical parts of the boiler, or low water level. Corrosion along the edges of lap joints was a common cause of early boiler explosions.\n\nThe second kind is a fuel/air explosion in the furnace, which would more properly be termed a firebox explosion. Firebox explosions in solid-fuel-fired boilers are rare, but firebox explosions in gas or oil-fired boilers are still a potential hazard.\n\nThere are many causes for boiler explosions such as poor water treatment causing scaling and over heating of the plates, low water level, a stuck safety valve, or even a furnace explosion that in turn, if severe enough, can cause a boiler explosion. Poor operator training resulting in neglect or other mishandling of the boiler has been a frequent cause of explosions since the beginning of the industrial revolution. In the late 19th and early 20th century, the inspection records of various sources in the U.S., UK, and Europe showed that the most frequent cause of boiler explosions was weakening of boilers through simple rusting, by anywhere from 2 to 5 times more than all other causes.\n\nBefore materials science, inspection standards, and quality control caught up with the rapidly growing boiler manufacturing industry, a significant number of boiler explosions were directly traceable to poor design, workmanship, and undetected flaws in poor quality materials. The alarming frequency of boiler failures in the U.S. due to defects in materials and design were attracting the attention of international engineering standards organizations, such as the ASME, which established their first Boiler Testing Code in 1884. The boiler explosion that caused the Grover Shoe Factory disaster in Brockton, Massachusetts on March 10, 1905 resulted in 58 deaths and 117 injuries, and inspired the state of Massachusetts to publish its first boiler laws in 1908.\n\nSeveral written sources provide a concise description of the causes of boiler explosions:\n\n\"The principal causes of explosions, in fact the only causes, are deficiency of strength in the shell or other parts of the boilers, over-pressure and over-heating. Deficiency of strength in steam boilers may be due to original defects, bad workmanship, deterioration from use or mismanagement.\"\n\n\"Cause.-Boiler explosions are always due to the fact that some part of the boiler is, for some reason, too weak to withstand the pressure to which it is subjected. This may be due to one of two causes: Either the boiler is not strong enough to safely carry its proper working pressure, or else the pressure has been allowed to rise above the usual point by the sticking of the safety valves, or some similar cause\"\n\nWhile deterioration and mishandling are probably the most common \"causes\" of boiler explosions, the actual \"mechanism\" of a catastrophic boiler failure was not well documented until extensive experimentation was undertaken by U.S. boiler inspectors in the early 20th century. Several different attempts were made to cause a boiler to explode by various means, but one of the most interesting experiments demonstrated that in certain circumstances, if a sudden opening in the boiler allowed steam to escape too rapidly, water hammer could cause destruction of the entire pressure vessel:\n\n\"\"A cylindrical boiler was tested and withstood a steam pressure of 300 pounds without injury.\" \" When the [discharge] valve was suddenly opened at a pressure of 235 pounds the boiler gave way, the iron being twisted and torn into fragments and thrown in all directions. The reason for this was that the sudden rush of steam from the boiler into the discharge pipe reduced the pressure in the boiler very rapidly. This reduction of pressure caused the sudden formation of a great quantity of steam within the water, and the heavy mass of water being thrown with great violence toward the opening whence the steam was being withdrawn, struck the portions of the boiler near that opening and caused the fracture.\"\"\n\nBoiler explosions are common in sinking ships once the superheated boiler touches cold sea water, as the sudden cooling of the superheated metal causes it to crack; for instance, when the \"SS Ben Lomond\" was torpedoed by a U-boat, the torpedoes and resulting boiler explosion caused the ship to go down in two minutes, leaving Poon Lim as the only survivor in a complement of 54 crew.\n\nBoiler explosions are of a particular danger in (locomotive-type) fire tube boilers because the top of the firebox (crown sheet) must be covered with some amount of water at all times; or the heat of the fire can weaken the crown sheet or crown stays to the point of failure, even at \"normal working pressure\".\n\nThis was the cause of the Gettysburg Railroad firebox explosion near Gardners, Pennsylvania in 1995, where low water allowed the front of the crown sheet to overheat until the regular crown stays pulled through the sheet, releasing a great deal of steam and water under full boiler pressure into the firebox. Fortunately, the crown sheet design included several alternating rows of button-head safety stays, which limited the failure of the crown sheet to the first 5 or 6 rows of conventional stays, preventing a collapse of the entire crown sheet.\n\nHowever, this type of failure is not limited to railway engines, as locomotive-type boilers have been used for traction engines, portable engines, skid engines used for mining or logging, stationary engines for sawmills and factories, for heating, and as package boilers providing steam for other processes. In all applications, maintaining the proper water level is essential for safe operation.\n\nMany shell-type boilers carry a large bath of liquid water which is heated to a higher temperature and pressure (enthalpy) than boiling water would be at atmospheric pressure. During normal operation, the liquid water remains in the bottom of the boiler due to gravity, steam bubbles rise through the liquid water and collect at the top for use until saturation pressure is reached, then the boiling stops. If some pressure is released, boiling begins again, and so on.\n\nIf steam is released normally, say by opening a throttle valve, the bubbling action of the water remains moderate and relatively dry steam can be drawn from the highest point in the vessel.\n\nIf steam is released more quickly, the more vigorous boiling action that results can throw a fine spray of droplets up as \"wet steam\" which can cause damage to piping, engines, turbines and other equipment downstream.\n\nIf a large crack or other opening in the boiler vessel allows the internal pressure to drop very suddenly, the heat energy remaining in the water will cause even more of the liquid to flash into steam bubbles, which then rapidly displace the remaining liquid. The potential energy of the escaping steam and water are now transformed into work, just as they would have done in an engine; with enough force to peel back the material around the break, severely distorting the shape of the plate which was formerly held in place by stays, or self-supported by its original cylindrical shape. The rapid release of steam and water can provide a very potent blast, and cause great damage to surrounding property or personnel.\n\nThe rapidly expanding steam bubbles can also perform work by throwing large \"slugs\" of water inside the boiler in the direction of the opening, and at astonishing velocity. A fast-moving mass of water carries a great deal of kinetic energy (from the expanding steam), and in collision with the shell of the boiler results in a violent destructive effect. This can greatly enlarge the original rupture, or tear the shell in two.\n\nMany plumbers and steamfitters are aware of this phenomenon, which is called \"water hammer\". A few ounce \"slug\" of water passing through a steam line at high velocity and striking a 90 degree elbow can instantly fracture a fitting that is otherwise capable of handling several times the normal static pressure. It can then be understood that a few hundred, or even a few thousand pounds of water \"moving at the same velocity\" inside a boiler shell can easily blow out a tube sheet, collapse a firebox, even toss the entire boiler a surprising distance through reaction as the water exits the boiler, like the recoil of a heavy cannon firing a ball.\n\nSeveral accounts of the SL-1 experimental reactor accident vividly describe the incredibly powerful effect of water hammer on a pressure vessel: \"\"The expansion caused by this heating process caused water hammer as water was accelerated upwards toward the reactor vessel head, producing approximately 10,000 pounds per square inch (69,000 kPa) of pressure on the head of the reactor vessel when water struck the head at 160 feet per second (50 m/s)\" \"This extreme form of water hammer propelled control rods, shield plugs, and the entire reactor vessel upward. A later investigation concluded that the 26,000-pound (12,000 kg) vessel had jumped 9 feet 1 inch (2.77 m) and the upper control rod drive mechanisms had struck the ceiling of the reactor building prior to settling back into its original location\"\"\n\nA steam locomotive operating at 350 psi (2.4 MPa) would have a temperature of about 225 °C, and a specific enthalpy of 963.7 kJ/kg. Since standard pressure saturated water has a specific enthalpy of just 418.91 kJ/kg, the difference between the two specific enthalpies, 544.8 kJ/kg, is the total energy expended in the explosion. So in the case of a large locomotive which can hold as much as 10,000 kg of water at a high pressure and temperature state, this explosion would have an theoretical energy release equal to about 1160 kg of TNT.\n\nIn the case of a firebox explosion, these typically occur after a burner flameout. Oil fumes, natural gas, propane, coal, or any other fuel can build up inside the combustion chamber. This is especially of concern when the vessel is hot; the fuels will rapidly volatize due to the temperature. Once the lower explosive limit (LEL) is reached, any source of ignition will cause an explosion of the vapors.\n\nA fuel explosion within the confines of the firebox may damage the pressurized boiler tubes and interior shell, potentially triggering structural failure, steam or water leakage, and/or a secondary boiler shell failure and steam explosion.\n\nA common form of minor firebox \"explosion\" is known as \"drumming\" and can occur with any type of fuel. Instead of the normal \"roar\" of the fire, a rhythmic series of \"thumps\" and flashes of fire below the grate and through the firedoor indicate that the combustion of the fuel is proceeding through a rapid series of detonations, caused by an inappropriate air/fuel mixture with regard to the level of draft available. This usually causes no damage in locomotive type boilers, but can cause cracks in masonry boiler settings if allowed to continue.\n\nThe plates of early locomotive boilers were joined by simple overlapping joints. This practice was satisfactory for the annular joints, running around the boiler, but in longitudinal joints, along the length of the boiler, the overlap of the plates diverted the boiler cross-section from its ideal circular shape. Under pressure the boiler strained to reach, as nearly as possible, the circular cross-section. Because the double-thickness overlap was stronger than the surrounding metal, the repeated bending and release caused by the variations in boiler pressure caused internal cracks, or grooves (deep pitting), along the length of the joint. The cracks offered a starting point for internal corrosion, which could hasten failure. It was eventually found that this internal corrosion could be reduced by using plates of sufficient size so that no joints were situated below the water level. Eventually the simple lap seam was replaced by the single or double butt-strap seams, which do not suffer from this defect.\n\nDue to the constant expansion and contraction of the firebox a similar form of \"stress corrosion\" can take place at the ends of staybolts where they enter the firebox plates, and is accelerated by poor water quality. Often referred to as \"necking\", this type of corrosion can reduce the strength of the staybolts until they are incapable of supporting the firebox at normal pressure.\n\nGrooving (deep, localized pitting) also occurs near the waterline, particularly in boilers that are fed with water that has not been de-aerated or treated with oxygen scavenging agents. All \"natural\" sources of water contain dissolved air, which is released as a gas when the water is heated. The air (which contains oxygen) collects in a layer near the surface of the water and greatly accelerates corrosion of the boiler plates in that area.\n\nThe intricate shape of a locomotive firebox, whether made of soft copper or of steel, can only resist the steam pressure on its internal walls if these are supported by stays attached to internal girders and the outer walls. They are liable to fail through fatigue (because the inner and outer walls expand at different rates under the heat of the fire), from corrosion, or from wasting as the heads of the stays exposed to the fire are burned away. If the stays fail the firebox will explode inwards. Regular visual inspection, internally and externally, is employed to prevent this. Even a well-maintained firebox will fail explosively if the water level in the boiler is allowed to fall far enough to leave the top plate of the firebox uncovered. This can occur when crossing the summit of the hill, as the water flows to the front part of the boiler and can expose the firebox crown sheet. The majority of locomotive explosions are firebox explosions caused by such crown sheet uncovering.\n\nThe Pennsylvania was a side wheeler steamboat which suffered a boiler explosion in the Mississippi River and sank at Ship Island near Memphis, Tennessee, on June 13, 1858. Of the 450 passengers on board more than 250 died, including Henry Clemens, the younger brother of the author Mark Twain.\n\nSS Ada Hancock, a small steamboat used to transfer passengers and cargo to and from the large coastal steamships that stopped in San Pedro Harbor in the early 1860s, suffered disaster when its boiler exploded violently in San Pedro Bay, the port of Los Angeles, near Wilmington, California on April 27, 1863 killing twenty-six people and injuring many others of the fifty-three or more passengers on board.\n\nThe steamboat \"Sultana\" was destroyed in an explosion on 27 April 1865, resulting in the greatest maritime disaster in United States history. An estimated 1,549 passengers were killed when three of the ship's four boilers exploded and the \"Sultana\" burned and sank not far from Memphis, Tennessee.\n\nAnother US Civil War Steamboat explosion was the Steamer \"Eclipse\" on January 27, 1865, which was carrying members of the 9th Indiana Artillery. One official Records report mentions the disaster reports 10 killed and 68 injured; a later report mentions that 27 were killed and 78 wounded. Fox's Regimental Losses reports 29 killed.\n\nThe stationary steam engines used to power machinery first came to prominence during the industrial revolution, and in the early days there were many boiler explosions from a variety of causes. One of the first investigators of the problem was William Fairbairn, who helped establish the first insurance company dealing with the losses such explosions could cause. He also established experimentally that the hoop stress in a cylindrical pressure vessel like a boiler was twice the longitudinal stress. Such investigations helped him and others explain the importance of stress concentrations in weakening boilers.\n\nModern boilers are designed with redundant pumps, valves, water level monitors, fuel cutoffs, automated controls, and pressure relief valves. In addition, the construction must adhere to strict engineering guidelines set by the relevant authorities. The NBIC, ASME, and others attempt to ensure safe boiler designs by publishing detailed standards. The result is a boiler unit which is less prone to catastrophic accidents.\n\nAlso improving safety is the increasing use of \"package boilers.\" These are boilers which are built at a factory then shipped out as a complete unit to the job site. These typically have better quality and fewer issues than boilers which are site assembled tube-by-tube. A package boiler only needs the final connections to be made (electrical, breaching, condensate lines, etc.) to complete the installation.\n\nIn steam locomotive boilers, as knowledge was gained by trial and error in early days, the explosive situations and consequent damage due to explosions were inevitable. However, improved design and maintenance markedly reduced the number of boiler explosions by the end of the 19th century. Further improvements continued in the 20th century.\n\nOn land-based boilers, explosions of the pressure systems happened regularly in stationary steam boilers in the Victorian era, but are now very rare because of the various protections provided, and because of regular inspections compelled by governmental and industry requirements.\n\nWater heaters can explode with surprising violence when their safety devices fail.\n\nA steam explosion can occur in any kind of a water heater, where a sufficient amount of energy is delivered and the steam created exceeds the strength of the vessel. When the heat delivery is sufficiently rapid, a localized superheating can occur, resulting in a water hammer destroying the vessel. The SL-1 nuclear reactor accident is a good example.\n\nHewison (1983) gives a comprehensive account of British boiler explosions, listing 137 between 1815 and 1962. It is noteworthy that 122 of these were in the 19th century and only 15 in the 20th century.\n\nBoiler explosions generally fell into two categories. The first is the breakage of the boiler barrel itself, through weakness/damage or excessive internal pressure, resulting in sudden discharge of steam over a wide area. Stress corrosion cracking at the lap joints was a common cause of early boiler explosions, probably caused by caustic embrittlement. The water used in boilers was not often closely controlled, and if acidic, could corrode the wrought iron boiler plates. Galvanic corrosion was an additional problem where copper and iron were in contact.\nBoiler plates have been thrown up to a quarter of a mile (Hewison, Rolt). The second type is the collapse of the firebox under steam pressure from the adjoining boiler, releasing flames and hot gases into the cab. Improved design and maintenance almost totally eliminated the first type, but the second type is always possible if the engineer and fireman do not maintain the water level in the boiler.\n\nBoiler barrels could explode if the internal pressure became too high. To prevent this, safety valves were installed to release the pressure at a set level. Early examples were spring-loaded, but John Ramsbottom invented a tamper-proof valve which was universally adopted. The other common cause of explosions was internal corrosion which weakened the boiler barrel so that it could not withstand normal operating pressure. In particular, grooves could occur along horizontal seams (lap joints) below water level. Dozens of explosions resulted, but were eliminated by 1900 by the adoption of butt joints, plus improved maintenance schedules and regular hydraulic testing.\n\nFireboxes were generally made of copper, though later locomotives had steel fireboxes. They were held to the outer part of the boiler by stays (numerous small supports). Parts of the firebox in contact with full steam pressure have to be kept covered with water, to stop them overheating and weakening. The usual cause of firebox collapses is that the boiler water level falls too low and the top of the firebox (crown sheet) becomes uncovered and overheats. This occurs if the fireman has failed to maintain water level or the level indicator (gauge glass) is faulty. A less common reason is breakage of large numbers of stays, due to corrosion or unsuitable material.\n\nThroughout the 20th century, two boiler barrel failures and thirteen firebox collapses occurred in the UK. The boiler barrel failures occurred at Cardiff in 1909 and Buxton in 1921; both were caused by misassembly of the safety valves causing the boilers to exceed their design pressures. Of the 13 firebox collapses, four were due to broken stays, one to scale buildup on the firebox, and the rest were due to low water level.\n\n\n\n"}
{"id": "35828623", "url": "https://en.wikipedia.org/wiki?curid=35828623", "title": "Business reporting", "text": "Business reporting\n\nBusiness reporting or enterprise reporting refers to both \"the public reporting of operating and financial data by a business enterprise,\" and \"the regular provision of information to decision-makers within an organization to support them in their work.\" It is a fundamental part of the larger movement towards improved business intelligence and knowledge management. Implementation often involves extract, transform, and load (ETL) procedures in coordination with a data warehouse and then using one or more reporting tools. Reports can be distributed in print form, via email or accessed via a corporate intranet.\n\nWith the expansion of information technology there has been an increase in the production of unified reports which join different views of an organization in one place. This reporting process involves querying data sources with different logical models to produce a human readable report. For example, a decision maker may need to query a human resources databases and a capital improvements databases to show how efficiently space is being used across an entire corporation.\n\nReporting can also be used for verification and cross-checks. Audit teams like FINRA and SEC adhere to reports for all business firms. Standard Business Reporting is a group of international programs instigated by a number of governments with the end of make business the centre when it comes to managing business-to-government reporting obligations.\n\n"}
{"id": "46593693", "url": "https://en.wikipedia.org/wiki?curid=46593693", "title": "Cable cars in Chicago", "text": "Cable cars in Chicago\n\nIn 1900, Chicago already had the second largest cable car network in the country (and, arguably, the city would grow to have the largest streetcar network in the world in a few decades). In 1900, there were three private companies operating of double track routes radiating out from the downtown area. State of the art technology when the first line opened in 1882, by 1900 electric traction had proven superior and in 1906 all cable routes were changed to electrical power. In 2015 most were part of Chicago Transit Authority bus routes.\n\nIn the 1850s Chicago was growing rapidly and local transportation was a problem. Flat and low, drainage was poor and the roads were often muddy and near impassible for foot and horse traffic.\n\nIn 1859 the Illinois state legislature incorporated the Chicago City Railway (CCR) and the North Chicago Street Railroad (NCSR), to provide rail horsecar service in Chicago. In 1861 the Chicago West Division Railway was incorporated. The three companies served different parts of the city, defined by the Chicago River, and were not in competition with each other. By 1880 all three had main routes with feeder lines.\n\nIn 1882 the CCR opened cable lines to the south on State St. and Wabash-Cottage Grove Ave. Immediately successful, the State St. line would be extended to 63rd St. by 1887 and the Cottage Grove Ave. line to 71st St. by 1890.\n\nIn 1886 the NCSR put a cable line on Clark St. and parallel 5th Ave. (now Wells St.) into service. In 1889 a branch on Lincoln Ave. opened, and the last branch, on Clybourn Ave., opened in 1891.\n\nIn 1890 the re-organized West Chicago Street Railroad (WCSR) opened their first lines, to the northwest on Milwaukee Ave. Shortly afterword a line strait west on Madison Ave. opened. In 1893 two more routes would open, southwest on Blue Island Ave and south on Halsted St.\n\nIn 1892 the Chicago City Council allowed the CCR to electrify three horse lines outside of downtown, two years later many North and West lines were electrified. In 1896 the first downtown electrification was permitted, in 1906 all cable service was converted to electric traction.\n\nThe cable cars did not suffer much from the elements, and the harsher winters of the US Midwest and East Coast were no problem for them. As with some other cities using cable cars the problem in generally flat Chicago was not one of grades, but of traffic volume due to the density of the city.\n\nAs in other cities the cable cars did not completely replace the horsecars, but they rather created a transportation backbone. In fact, even as the horse lines were being converted to trolleys, the electrical cars had to be pulled by grip cars through the downtown, due to the lack of trolley wires there.\n\nThe passenger numbers caused a different approach than many other cities. Some single cars were used, but on most lines grip cars pulled trains of up to three trailers (reduced to two by law in the 1890s).\n\nMost grip cars were short and open. Four different types of grips were used, one by each company and the WCSR's south and southwest lines using a fourth. CCR used a grip that could grip either side of the cable, allowing the grip car to operate in either direction. NCSR and WCSR grip cars could operate in one direction only. None of the grips could be used on other lines. Approximately 700 grip cars were in service.\n\nBoth the NCSR and WCSR operated large combination grip cars, with an open front and closed back sections. These cars also could pull trailers.\n\nTrailers started as short two axle cars similar to horsecars, and were built by the operator. Later, longer two truck cars would be built by vendors. Open summer and closed winter cars were used, with two car trains the norm, there were between two and four trailers for each grip.\n\nSwitching directions on a cable train can be very difficult. Each track can go in one direction only, and the grip car has to be at the head of the train. Turning around a loop was common, at the end of most lines there were loops. In the downtown area the loops went around several blocks, increasing the area the line would otherwise serve. Equipment and operating differences prevented common track use between most routes, in 1900 there were six separate loops in use.\n\nThe Chicago River separates the downtown from the North and West sides. Heavy river traffic required moveable bridges, and long delays. Cable cannot be used on moveable bridges, and the delays would have stopped the whole system, so the NCCR leased and refurbished the city's LaSalle St. tunnel under the river, the WCCR would use the similar Washington St. tunnel for its first two lines. For the WCCR's two Southwest lines the company dug a tunnel next to Van Buren St. at their own expense.\n\nAll three companies used similar infrastructures, with large steam boilers and reciprocating engines driving long endless cables through conduits. At their peak there were 13 powerhouses driving 34 cables. Different cables could run at different speeds, the CCR's loop originally ran at (increased to in 1892) while outlying cables could operate at .\n\nThroughout cable operations both politics and business were very corrupt in many cities, including Chicago. Some politicians expected not only political support but also bribes. Dummy companies were created to extort the operators, and property owners often conspired to sell their consent to the routes. The CCR, well managed and first in operation, was affected least, while the North and West companies, controlled by robber baron Charles Tyson Yerkes, were involved in some unscrupulous business practices.\n\nIn 1900 the lowering of the river exposed the tops of all three tunnels, making them hazards to navigation. In 1906 all three tunnels under the river were closed for construction, cutting cable service to the North and West. This was when the changeover to electricity ordered by the Chicago City Council in 1905 occurred. The last cable powered train was on the CCR Cottage Grove Ave. line on October 21, 1906.\n\nIn 1900 the Chicago City Railway was the largest cable operator in the country. Incorporated on February 14, 1859, it was well managed and progressive from its beginning. In 1880 their president had inspected the successful San Francisco lines, and felt cable could be used in Chicago. In 1882 they opened the first cable lines outside of San Francisco. They then built lines past the built up areas, making land along the route more valuable. Development followed the lines, making more traffic.\n\nWhen first opened the State St. and Wabash - Cottage Grove Avenue lines both used a slow speed () three block loop. This could not handle the traffic, in 1892 the Cottage Grove Avenue line started using a new two block loop directly east of the original, which was rebuilt two years later. Trains of both lines ran opposite each other on Wabash Ave.\n\nBecause CCR grip cars were bi-directional, trains could be reversed onto the opposite track, and did not need a loop. It also meant that a train could stop and return without going to the end of the line. The Cottage Grove line had runs reversing at 39th St. and at the end of the line.\n\nIn 1887 the CCR carried 70,000 to 100,000 passengers a day on approximately 150 trains. By 1892, after both lines had been lengthened, 300 trains were scheduled daily. Three powerhouses pulled thirteen cables.\n\nIn 1906 CCR electrified its State St. line on July 22, and the Wabash-Cottage Grove Ave. line on October 21, the last day of cable service in Chicago.\n\nOn February 1, 1914, the CCR began operating as part of the Chicago Surface Lines (CSL).\n\nThe North Chicago Street Railroad was the smallest of the three companies. Incorporated in 1859 as the North Chicago Street Railway, a horse-car system, it was badly damaged by the Great Chicago Fire of 1871. Little improvement was done until 1885, when a Philadelphia syndicate controlled by Charles Tyson Yerkes reorganized it as the North Chicago Street Railroad. In 1886 it began converting to cable.\n\nAll the NCSR's lines entered downtown through the LaSalle St. tunnel and used a six block loop. The Clybourn Ave. line was the only place where single combination cars were used. The end of that route at had a turntable, rather than the loops that the other lines used.\n\nThe NCSR had up to 177 grip cars and many more trailers. Three powerhouses pulled 9 cables.\n\nOn May 24, 1899, the NCSR and WCSR were combined into the Chicago Union Traction Co., which would go into receivership on April 22, 1903 and was bought by the Chicago Railway Co. (CR) on January 25, 1908. On February 1, 1914, the Chicago Railway Co. began operating as part of the Chicago Surface Lines.\n\nThe West Chicago Street Railroad was incorporated in 1861 as the Chicago West Division Railway, in 1885 the Chicago Passenger Railway opened as a competitor. In 1887 the two were combined and reorganized by Charles Tyson Yerkes as the West Chicago Street Railroad. This put the NCSR. and WCSR under the same ownership, it began cable service in 1890, the last of the three companies to do so.\n\nA northwest and west line used another tunnel under the river on Washington St. to get to a two block loop, a four block loop was later added. A south and southwest line terminated west of the river until the privately built Van Buren St. tunnel opened in 1894, an eight block loop was used. The northwest Milwaukee Ave. line used single combination cars, all other lines used short grip cars with trailers.\n\nThe WCSR had 230 grip cars and several times many trailers. Six powerhouses pulled 12 cables.\n\nOn May 24, 1899 the WCSR, like the NCSR, was combined into the CUT, which would be bought by the CR on January 25, 1908. On February 1, 1914, the CR began operating as part of the Chicago Surface Lines.\n\nA CCR station from 1893 at 5529 South Lake Park Avenue survives in 2015. It currently serves as the home of the Hyde Park Historical Society. A shop building from 1902 and streetcar barns from 1906 remained in service in 2014 at the Chicago Transit Authority's 77th St. and Vincennes Ave. yard.\n\nA NCSR powerhouse at LaSalle and Illinois Streets remained in 2012.\n\nA WCSR altered powerhouse at Jefferson and Washington Streets and a car barn on Blue Island Ave. near Western Avenue remained in 2016.\n\n\n"}
{"id": "29510292", "url": "https://en.wikipedia.org/wiki?curid=29510292", "title": "Construction barrel", "text": "Construction barrel\n\nConstruction barrels (officially known as \"drums\" in the United States) are traffic control devices used to channel motor vehicle traffic through construction sites or to warn motorists of construction activity near the roadway. They are used primarily in the United States, but are occasionally used in Canada and Mexico. They are an alternative to traffic cones which are smaller and easily hit by vehicles. Drums tend to command more respect from drivers than cones as they are larger, more visible, and give the appearance of being formidable obstacles.\n\nConstruction barrels are typically bright orange and have four alternating white and orange reflective bands. However some regions, such as the province of Ontario, Canada, uses black stripes instead of white. Most have a rubber base that prevents the barrel from tipping over during high winds or when struck by a vehicle. Construction barrels have a handle at the top so they can be easily picked up and carried. The handle also allows crews to install barricade lights to increase visibility. The product makes up a $90 million industry in the United States .\n\nUntil the late 1980s, construction crews typically used 55-gallon steel drums to guide traffic through construction areas. They were painted orange and white and filled with sand or water to keep them in place. Because the drums were steel and weighed down with sand or water, extensive damage would occur to vehicles striking them. Plastic barrels that are commonly seen on American roadways today began emerging in the late 1970s and 1980s; steel 55-gallon drums were largely phased out by the 1990s and may no longer be used as traffic control devices in the US. \n\nBy 1981, the drums were mainly a two-piece plastic design that included the top piece of the drum and a base that was filled with sandbags. The same year, an updated version of the invention was released by PSS; it included a flange to allow sandbag placement on the outside of the drum which made it easier to maneuver. In 1985, PSS released the modern-day version of the construction barrel, the LifeGard® drum . The LifeGard® utilized the sidewall of a recycled truck tire at its base to keep the drum securely in place on the roadway. This design is the most common one in use today.\n\n"}
{"id": "40627582", "url": "https://en.wikipedia.org/wiki?curid=40627582", "title": "Coplanar waveguide", "text": "Coplanar waveguide\n\nCoplanar waveguide is a type of electrical planar transmission line which can be fabricated using printed circuit board technology, and is used to convey microwave-frequency signals. On a smaller scale, coplanar waveguide transmission lines are also built into monolithic microwave integrated circuits. Conventional coplanar waveguide (CPW) consists of a single conducting track printed onto a dielectric substrate, together with a pair of return conductors, one to either side of the track. All three conductors are on the same side of the substrate, and hence are \"coplanar\". The return conductors are separated from the central track by a small gap, which has an unvarying width along the length of the line. Away from the cental conductor, the return conductors usually extend to an indefinite but large distance, so that each is notionally a semi-infinite plane.\n\nConductor-backed coplanar waveguide (CBCPW) is a common variant which has a ground plane covering the entire back-face of the substrate. The ground-plane serves as a third return conductor.\n\nCoplanar waveguide was invented in 1969 by Cheng P. Wen, primarily as a means by which non-reciprocal components such as gyrators and isolators could be incorporated in planar transmission line circuits.\n\nThe electromagnetic wave carried by a coplanar waveguide exists partly in the dielectric substrate, and partly in the air above it. In general, the dielectric constant of the substrate will be different (and greater) than that of the air, so that the wave is travelling in an inhomogeneous medium. In consequence CPW will not support a true TEM wave; at non-zero frequencies, both the E and H fields will have longitudinal components (a hybrid mode).\n\nNonreciprocal gyromagnetic devices depend on the microwave signal presenting a rotating (circularly polarized) magnetic field to a statically magnetized ferrite body. CPW is designed to produce just such a rotating magnetic field in the two slots between the central and side conductors.\n\nThe dielectric substrate has no direct effect on the magnetic field of a microwave signal travelling along the CPW line. For the magnetic field, the CPW is then symmetrical in the plane of the metalization, between the substrate side and the air side. Consequently, currents flowing along parallel paths on opposite faces of each conductor (on the air-side and on the substrate-side) are subject to the same inductance, and the overall current tends to be divided equally between the two faces.\n\nConversely, the substrate \"does\" affect the electric field, so that the substrate side contributes a larger capacitance across the slots than does the air side. Electric charge can accumulate or be depleted more readily on the substrate face of the conductors than on the air face. As a result, at those points on the wave where the current reverses direction, charge will spill over the edges of the metalization between the air face and the substrate face. This secondary current over the edges gives rise to a longitudinal (parallel with the line), magnetic field in each of the slots, which is in quadrature with the vertical (normal to the substrate surface) magnetic field associated with the main current along the conductors.\n\nIf the dielectric constant of the substrate is much greater than unity, then the magnitude of the longitudinal magnetic field approaches that of the vertical field, so that the combined magnetic field in the slots approaches circular polarization.\n\nCoplanar waveguides play an important role in the field of solid state quantum computing, e.g. for the coupling of microwave photons to a superconducting qubit. In particular the research field of circuit quantum electrodynamics was initiated with coplanar waveguide resonators as crucial elements that allow for high field strength and thus strong coupling to a superconducting qubit by confining a microwave photon to a volume that is much smaller than the cube of the wavelength. To further enhance this coupling, superconducting coplanar waveguide resonators with extremely low losses were applied. (The quality factors of such superconducting coplanar resonators at low temperatures can exceed 10 even in the low-power limit.) Coplanar resonators can also be employed as quantum buses to couple multiple qubits to each other.\n\nAnother application of coplanar waveguides in solid state research is for studies involving magnetic resonance, e.g. for electron spin resonance spectroscopy or for magnonics.\n\nCoplanar waveguide resonators have also been employed to characterize the material properties of (high-T) superconducting thin films.\n\n\n"}
{"id": "42517624", "url": "https://en.wikipedia.org/wiki?curid=42517624", "title": "Crealogix", "text": "Crealogix\n\nCrealogix Group (own spelling: CREALOGIX) is a Swiss software company headquartered in Zurich, Switzerland, which develops and implements Financial Technology (FinTech) solutions for banks. The focus is placed on customer needs for mobility, security, personalised advice and education within a comprehensive user experience.Established in 1996, the Group has over 500 employees worldwide and offices in Switzerland (Zurich, Berne, Bubikon), Germany (Stuttgart, Puchheim, Jever, Coburg), United Kingdom (London, Winchester), Singapore and Barcelona. The shares of Crealogix Holding AG (CLXN) are traded on the SIX Swiss Exchange.\n\nThe company was founded in 1996 in Switzerland by Bruno Richle (Executive Chairman), Dr. Richard Dratva (Chief Strategy Officer, Vice Chairman), Peter Süsstrunk (Senior Management) and Daniel Hiltebrand. The IPO followed in 2000. Crealogix changed from an initial service provider to a software product manufacterer, providing solutions in the fields of online and mobile banking & payments, as well as digital learning & education. Additionally the company has developed a focus on the financial industry. In 2011 Crealogix acquired the e-banking business unit of 'abaXX' from Cordys Deutschland AG. In 2012, the entire AdviceManager product business was acquired from the German company C1 FinCon. In 2014, the company established new locations in London, Vienna and Singapore. In January 2015, it acquired the company MBA Systems in UK. Thomas Avedik, the former head of the 'Swiss Digital Banking' business unit, was appointed CEO of the Crealogix Group as of 1 January 2016. In January 2018, the Group acquired 100 percent of the shares of Innofis, a Barcelona-based digital banking provider. A short time later, it takes over ELAXY Business Solution & Services completely from the German IT service provider Fiducia & GAD IT AG. With the overall takeover, Crealogix exercised an option that was already agreed within the framework of the initial investment in 2015. The investments in ELAXY Financial Software & Solutions remain unchanged; Crealogix holds 80% and Fiducia & GAD continues to hold 20%.\n"}
{"id": "11787686", "url": "https://en.wikipedia.org/wiki?curid=11787686", "title": "Daly detector", "text": "Daly detector\n\nA Daly detector is a gas-phase ion detector that consists of a metal \"doorknob\", a scintillator (phosphor screen) and a photomultiplier. It was named after its inventor Norman Richard Daly. Daly detectors are typically used in mass spectrometers.\n\nIons that hit the doorknob release secondary electrons. A high voltage (about ) between the doorknob and the scintillator accelerates the electrons onto the phosphor screen, where they are converted to photons. These photons are detected by the photomultiplier.\n\nThe advantage of the Daly detector is that the photomultiplier can be separated by a window, which lets the photons through from the high vacuum of the mass spectrometer, thus preventing an otherwise possible contamination and extending life span of the detector. The Daly detector also allows a higher acceleration after the field-free region of a time-of-flight mass spectrometer flight tube, which can improve the sensitivity for heavy ions.\n\nNorman Daly was awarded 6 patents in the years 1962–1973 relating to ion detection and mass spectrometers, from his work at the United Kingdom Atomic Energy Authority.\n"}
{"id": "7179232", "url": "https://en.wikipedia.org/wiki?curid=7179232", "title": "Design Piracy Prohibition Act", "text": "Design Piracy Prohibition Act\n\nThe Design Piracy Prohibition Act, , , and , were bills of the same name introduced in the United States Congress that would have amended Title 17 of the United States Code to provide \"sui generis\" protection to fashion designs for a period of three years. The Acts would have extend protection to \"the appearance as a whole of an article of apparel, including its ornamentation,\" with \"apparel\" defined to include \"men's, women's, or children's clothing, including undergarments, outerwear, gloves, footwear, and headgear;\" \"handbags, purses, and tote bags;\" belts, and eyeglass frames. In order to receive the three-year term of protection, the designer would be required to register with the U.S. Copyright Office within three months of going public with the design.\n\nH.R. 2511 was introduced July 13, 2011 by Representative Robert Goodlatte [R-VA6] with thirteen co-sponsors. On August 25, 2011, the U.S. House Committee on the Judiciary referred the Bill to the U.S. House Subcomittee on Courts, the Internet, and Intellectual Property.\nhttp://www.govtrack.us/congress/bill.xpd?bill=h112-2511\n\nH.R. 2033 was introduced April 25, 2007 by Representative Bill Delahunt with fourteen co-sponsors. On May 4, 2007 the U.S. House Committee on the Judiciary referred the Bill to the U.S. House Subcomittee on Courts, the Internet, and Intellectual Property. A hearing was held February 14, 2008, but the bill never made it out of the subcommittee.\n\nS. 1957 was introduced on August 2, 2007, in Washington, D.C. by Senator Charles Schumer (D-NY) with ten co-sponsors. The bill was referred to the Senate Committee on the Judiciary but progressed no further.\n\nH.R. 2196 was introduced on April 30, 2009, by Representative Delahunt and twenty-three co-sponsors. The bill was referred to the House Committee on the Judiciary on the same day and then stalled in committee.\n\nCurrently, fashion may only be protected by copyright to the extent that its shape is non-utilitarian enough to qualify as a creative \"sculpture,\" or to the extent that a design, pattern, or image on the clothing qualifies as \"pictorial\" or \"graphic.\" While current laws against counterfeit goods do provide some protection for designers, this is so only when the trademark is used and not when merely the design is copied under a different label. In addition, fashion may be protected by design patents if the requirements for patentability are met. To be patentable an ornamental design must be new, original and non-obvious. The United States Patent and Trademark Office website (www.uspto.gov) has a searchable database of patents, and includes patents on apparel in class D2, carrying articles in class D3, and eyeglass frames in class D16. Technological advances to the means of textile and garment production, as well as increases in the number of distribution channels and the availability of cheap labor in emerging economies have enabled those who would copy these designs to do so quickly and inexpensively. Legislation targeting design piracy has already been enacted in Europe, India, and Japan.\n\nCritics claim that, contrary to the bill's claims, the bill will actually harm independent fashion designers. The majority of independent designers do not have the litigation funds to effectively challenge big business should they be accused of copyright infringement. Furthermore, because distributors of accused designs can be penalized as well as the designer, distributors of clothing will become very wary of new designs unless the designer has adequate funds, influence, and power to hire skilled and effective lawyers. Pattern companies frequently utilize prevailing trends; so they too are vulnerable. Because of the legal risks of producing fashion patterns, fewer people will sew their own clothing, and fabric and sewing stores will suffer losses as well. As evidence of the bill's hypocrisy, critics point to how one of the most vocal supporters of the bill, Diane von Fürstenberg, was recently caught copying and distributing a piece of clothing originally designed by an independent Canadian designer. Critics also argue that the industry is already thriving commercially and encourages innovation. They point attention to the concept that originality in fashion design is too insubstantial for copyright law to distinguish protected elements from non-protected elements, and that extending copyright protection would stifle independent designers while giving powerful, big-business fashion houses a near-monopoly.\n\nH.R. 5055 was introduced March 30, 2006 by Representative Robert W. Goodlatte (R-Va.), with six co-sponsors from both parties. The bill was referred by the U.S. House Committee on the Judiciary to the U.S. House Subcomittee on Courts, the Internet, and Intellectual Property.\n\nThe subcommittee held a hearing on the bill on July 27, 2006, at which there was disagreement among legal experts as well as representatives of the fashion industry as to whether there was a need for copyright protection. Proponents of the Act claimed that new technology threatened American designers' ability to compete with the products of lower-cost countries, because the distribution of images of new designs and the automation of copying and manufacturing could occur within hours. They additionally pointed out that the United States was the exception among western nations in failing to protect designs.\n\nS.3728 was introduced on August 5, 2010, by Senator Chuck Schumer with ten co-sponsors. On December 1, 2010, the Senate Committee on the Judiciary voted unanimously for the bill to proceed to the Senate floor. This is the furthest that any of the design bills has progressed since 2006.\n\nUnder the IDPPPA, a copy of a design would have infringed if it was found to be \"substantially identical\" to the original work with little to no changes to set that design apart. Penalties for false representation would have been increased from $500 to $5,000 and from $1,000 to $10,000. \"Apparel\" items that would be protected by this Act include women's, men's, and children's clothing as well as luggage, handbags, wallets and eyeglass frames. A \"fashion design\" under the IDPPPA would be defined as an entire article of apparel including its embellishment and also includes elements of the original apparel that are the creative work of the original designer and are unique.\n\nSupporters argue that this act would create more protection for fashion designers. Opponents have argued that the bill would \"bring more lawyers into every step of the design process,\" outlaw \"inspiration and creativity,\" prevent \"unrestricted use of works in the public domain,\" and \"slow down the fast-paced design process.\" Some designers have supported the IDPPPA for protecting their current and future fashion designs. For example, Kurt Courtney of the AAFA has praised the bill as a \"great compromise and a product of hard work,\" but added that its effects will largely be seen in court cases involving the bill.\n\nS.3523 was introduced on September 10, 2012, by Senator Chuck Schumer with ten co-sponsors. On September 20, 2012, the Senate Committee on the Judiciary voted for the bill to proceed to the Senate floor without amendment.\n\n\n"}
{"id": "44235740", "url": "https://en.wikipedia.org/wiki?curid=44235740", "title": "ESpace", "text": "ESpace\n\neSpace is a software company established in Egypt. It was founded in the year 2000 when eight graduates from Computer Science Department, Alexandria University decided that instead of seeking a job opportunity in any multinational company or software house, they would rather start their own company. Inspired by the startup fever that took place in the Silicon Valley at the time, they started their own company which became a leader in the field of Web Scalability Solutions.\n\neSpace, the Alexandria-based software developer juggernaut had partnered with the Egyptian government to create elections.eg to help facilitate elections in Egypt.\n\neSpace in partnership with Google, has set up a site in anticipation of the Turkish house elections, scheduled for 30 March.\n"}
{"id": "9842121", "url": "https://en.wikipedia.org/wiki?curid=9842121", "title": "Electric heating pouch", "text": "Electric heating pouch\n\nAccording to the U.S. Patent & Trademark Office, electric heating pouches are medical apparatus, namely, electric heating devices for curative treatment.\n\nThe U.S. Food and Drug Administration (FDA) classifies the heating pouch as a class II medical device.\n\nHeating pouches are sometimes mislabeled as \"heating pads\". Heating pads are, by definition, flat, whereas heating pouches are contoured or curved.\n\nIn order to achieve the geometric design, three uniquely made heating pads are placed within a PVC pouch to form a scoop-like shape.\n\nHeating pouches are made specially for human body joints; specifically the shoulder, elbow, hip, knee, ankle, and wrist or hand. The curvature of the pouch enables the user to completely cover these joints, whereas flat heating pads cannot conform to the curved joints.\n"}
{"id": "1624595", "url": "https://en.wikipedia.org/wiki?curid=1624595", "title": "Electrolytic process", "text": "Electrolytic process\n\nAn electrolytic process is the use of electrolysis industrially to refine metals or compounds at a high purity and low cost. Some examples are the Hall-Héroult process used for aluminium, or the production of hydrogen from water. Electrolysis is usually done in bulk using hundreds of sheets of metal connected to an electric power source. In the production of copper, these pure sheets of copper are used as starter material for the cathodes, and are then lowered into a solution such as copper sulfate with the large anodes that are cast from impure (97% pure) copper. The copper from the anodes are electroplated on to the cathodes, while any impurities settle to the bottom of the tank. This forms cathodes of 99.999% pure copper.\n\n"}
{"id": "207040", "url": "https://en.wikipedia.org/wiki?curid=207040", "title": "First aid kit", "text": "First aid kit\n\nA first aid kit is a collection of supplies and equipment that is used to give medical treatment. There is a wide variation in the contents of first aid kits based on the knowledge and experience of those putting it together, the differing first aid requirements of the area where it may be used and variations in legislation or regulation in a given area.\n\nThe international standard for first aid kits is that they should be identified with the ISO graphical symbol for first aid (from ISO 7010) which is an equal white cross on a green background.\n\nFirst aid kits can be assembled in almost any type of container, and this will depend on whether they are commercially produced or assembled by an individual. Standard kits often come in durable plastic boxes, fabric pouches or in wall mounted cabinets. The type of container will vary depending on purpose, and they range in size from wallet sized through to large box.\n\nIt is recommended that all kits are in a clean, waterproof container to keep the contents safe and aseptic. Kits should also be checked regularly and restocked if any items are damaged or are out of date.\n\nThe International Organization for Standardization (ISO) sets a standard for first aid kits of being green, with a white cross, in order to make them easily recognizable to anyone requiring first aid.\nThe ISO only endorse the use of the green background and white cross, and this has been adopted as standard across many countries and regions, including the entire EU. First aid kits are sometimes marked (by an individual or organisation) with a red cross on white background, but use of this symbol by anyone but the International Committee of the Red Cross (ICRC) or associated agency is illegal under the terms of the First Geneva Convention, which designates the red cross as a protected symbol in all countries signatory to it. One of the few exceptions is in North America, where despite the passing of the First Geneva convention in 1864, and its ratification in the United States in 1881, Johnson & Johnson has used the red cross as a mark on its products since 1887 and registered the symbol as a U.S. trademark for medicinal and surgical plasters in 1905.\n\nSome first aid kits may also feature the Star of Life, normally associated with emergency medical services, but which are also used to indicate that the service using it can offer an appropriate point of care. Though not supported by the ISO, a white cross on red back ground is also widely recognised as a first aid symbol. However, for very small medical institutions and domestic purposes, the white cross on a plain green background is preferred.\n\nCommercially available first aid kits available via normal retail routes have traditionally been intended for treatment of minor injuries only. Typical contents include adhesive bandages, regular strength pain medication, gauze and low grade disinfectant. \n\nSpecialized first aid kits are available for various regions, vehicles or activities, which may focus on specific risks or concerns related to the activity. For example, first aid kits sold through marine supply stores for use in watercraft may contain seasickness remedies.\n\nFirst aid treats the ABCs as the foundation of good treatment. For this reason, most modern commercial first aid kits (although not necessarily those assembled at home) will contain a suitable infection barrier for performing artificial respiration as part of cardiopulmonary resuscitation, examples include:\n\nAdvanced first aid kits may also contain items such as:\n\nSome first aid kits, specifically those used by event first aiders and emergency services, include bottled oxygen for resuscitation and therapy.\n\nThe common kits mostly found in the homes may contain:\n\nTrauma injuries, such as bleeding, bone fractures or burns, are usually the main focus of most first aid kits, with items such as bandages and dressings being found in the vast majority of all kits.\n\nThe use of personal protective equipment or PPE will vary by kit, depending on its use and anticipated risk of infection. The adjuncts to artificial respiration are covered above, but other common infection control PPE includes:\n\n\nMedication can be a controversial addition to a first aid kit, especially if it is for use on members of the public. It is, however, common for personal or family first aid kits to contain certain medications. Dependent on scope of practice, the main types of medicine are life saving medications, which may be commonly found in first aid kits used by paid or assigned first aiders for members of the public or employees, painkillers, which are often found in personal kits, but may also be found in public provision and lastly symptomatic relief medicines, which are generally only found in personal kits.\n\nLife saving\n\nPain killers\n\nSymptomatic relief\n\n\nTopical medications\n\nBesides the regular uses for first aid kits, they can be helpful in wilderness or survival situations. First aid kits can make up a part of a survival kit or a mini survival kit in addition to other tools.\n\nIn the United States, the Occupational Safety and Health Administration (OSHA) requires all job sites and workplaces to make available first aid equipment for use by injured employees\nWhile providing regulations for some industries such as logging\n\nin general the regulation lack specifics on the contents of the first aid kit. This is understandable, as the regulation covers every means of employment, and different jobs have different types of injuries and different first-aid requirements. However, in a non-mandatory section,\n\nas the basis for the \"suggested\" minimum contents of a first aid kit. Another source for modern first aid kit information is United States Forest Service Specification 6170-6\n, which specifies the contents of several different-sized kits, intended to serve groups of differing size.\n\nIn general, the type of first aid facilities required in a workplace are determined by many factors, such as:\n\nAs the understanding of first aid and lifesaving measures has advanced, and the nature of public health risks has changed, the contents of first aid kits have changed to reflect prevailing understandings and conditions. For example, earlier US Federal specifications\nfor first aid kits included incision/suction-type snakebite kits and mercurochrome antiseptic. There are many historic components no longer used today, of course; some notable examples follow. As explained in the article on snakebite, the historic snakebite kit is no longer recommended. Mercurochrome was removed in 1998 by the US FDA from the generally recognized as safe category due to concerns over its mercury content. Another common item in early 20th century first aid kits, picric acid gauze for treating burns, is today considered a hazardous material due to its forming unstable and potentially explosive picrates when in contact with metal. Examples of modern additions include the CPR face shields and specific body-fluid barriers included in modern kits, to assist in CPR and to help prevent the spread of bloodborne pathogens such as HIV.\n\n\n"}
{"id": "40258204", "url": "https://en.wikipedia.org/wiki?curid=40258204", "title": "Furniture Manufacturing Eco Museum in Tainan", "text": "Furniture Manufacturing Eco Museum in Tainan\n\nThe Furniture Manufacturing Eco Museum in Tainan () is a museum in Rende District, Tainan, Taiwan.\n\nThe museum building was originally the manufacturing building for the Yongxing Furniture built in 1958. The factory building was converted to museum after 50 years of its furniture manufacturing processes due to its relocation to Mainland China. Planning and renovation works were done in three years and the museum was officially opened in May 2005.\n\n\nThe museum is accessible within walking distance southwest from Bao'an Station of the Taiwan Railway Administration.\n\n"}
{"id": "8466952", "url": "https://en.wikipedia.org/wiki?curid=8466952", "title": "Giveaway of the Day", "text": "Giveaway of the Day\n\nGiveaway of the Day (GOTD) is a website that offers commercially licensed Windows (and occasionally Mac) software for free without software updates or technical support. Each software offered on the site is free only on a specific date, and protection software included with the download will only activate the free license if the software is installed on the correct date. Though the project prohibits commercial usage, individual software publishers may allow otherwise in the end user license agreement for their own software. The site was launched on 24 October 2006.\n\nGiveawayoftheday.com is ranked #6,444 in the world according to Alexa traffic rankings on October 4, 2017. The site is ranked #6,190 in the US, where it is estimated that 19% of its visitors are located. It is also popular in Germany, where it is ranked #2,481.\n\nCategories of software on Giveaway of the Day include system tools, shell replacements, video and photo editing tools, office add-ons, file format converters, screen-savers and others. Software titles offered by Giveaway of the Day are free if downloaded from the Giveaway of the Day website.\n\nInitially GOTD used a technique to protect the installation package that led to a situation where titles previously featured on Giveaway of the Day became available on warez sites. Some websites provided protection circumvention guides. In July 2012 GOTD team announced a new release of the wrapper, integrating Themida software developed by Oreans Technologies into the wrapper as a protection mechanism.\n\nAt the end of the 2008 Giveaway announced in the blog their plans to launch similar project for Mac products. Even though the idea was highly appreciated and supported by the users community the project team didn’t step forward with it. Сurrently GOTD occasionally offers Mac applications together with giveaways for Windows (usually by the same developer).\n\nOn 11 December 2006 a new site was launched, Game Giveaway of the Day, which gave away games each day. Later, these giveaways were restricted to weekends. Since 29 April 2009, the beta website has been giving away games on a monthly basis.\n\n\n"}
{"id": "432097", "url": "https://en.wikipedia.org/wiki?curid=432097", "title": "Hoe (tool)", "text": "Hoe (tool)\n\nA hoe is an ancient and versatile agricultural and horticultural hand tool used to shape soil, remove weeds, clear soil, and harvest root crops. Shaping the soil includes piling soil around the base of plants (hilling), digging narrow furrows (drills) and shallow trenches for planting seeds or bulbs. Weeding with a hoe includes agitating the surface of the soil or cutting foliage from roots, and clearing soil of old roots and crop residues. Hoes for digging and moving soil are used to harvest root crops such as potatoes.\n\nThere are many kinds of hoes of varied appearances and purposes. Some have multiple functions while others have singular and specific functionality.\n\nThere are two general types of hoe: draw hoes for shaping soil and scuffle hoes for weeding and aerating soil.\n\nA draw hoe has a blade set at approximately a right angle to the shaft. The user chops into the ground and then pulls (draws) the blade towards them. Altering the angle of the handle can cause the hoe to dig deeper or more shallowly as the hoe is pulled. A draw hoe can easily be used to cultivate soil to a depth of several inches. A typical design of draw hoe, the \"eye hoe\", has a ring in the head through which the handle is fitted. This design has been used since Roman times.\n\nA scuffle hoe is used to scrape the surface of the soil, loosen the top inch or so, and to cut the roots of, remove, and disrupt the growth of weeds efficiently. These are primarily of two different designs: the Dutch hoe and the hoop hoe.\n\nThe term \"hand hoe\" most commonly refers to any type of light-weight, short-handled hoe, although it may be used simply to contrast hand-held tools against animal or machine pulled tools.\n\n\n\nHoes resembling neither draw nor scuffle hoes include:\n\nHoes are an ancient technology, predating the plough and perhaps preceded only by the digging stick. In Sumerian mythology, the invention of the hoe was credited to Enlil, the chief of the council of gods. The hand-plough (\"mr\") was depicted in predynastic Egyptian art, and hoes are also mentioned in ancient documents like the Code of Hammurabi (ca. 18th century BC) and the Book of Isaiah (c. 8th century BC).\n\nThe human damage caused by long-term use of short-handled hoes, which required the user to bend over from the waist to reach the ground, and caused permanent, crippling lower back pain to farm workers, resulted, after struggle led by César Chávez with political help from Governor Jerry Brown in the California Supreme Court declaring the short-handled hoe to be an unsafe hand tool that was banned under California law in 1975.\n\nOver the past fifteen or twenty years, hoes have become increasingly popular tools for professional archaeologists. While not as accurate as the traditional trowel, the hoe is an ideal tool for cleaning relatively large open areas of archaeological interest. It is faster to use than a trowel, and produces a much cleaner surface than an excavator bucket or shovel-scrape, and consequently on many open-area excavations the once-common line of kneeling archaeologists trowelling backwards has been replaced with a line of stooping archaeologists with hoes.\n\n\n\n"}
{"id": "22689366", "url": "https://en.wikipedia.org/wiki?curid=22689366", "title": "HyTest Ltd", "text": "HyTest Ltd\n\nHyTest Ltd is a producer of monoclonal antibodies and antigens for the diagnostic industry and research communities all around the world. HyTest is based in Turku, Finland. The company was established in 1994 and has become the global market leader in supplying certain reagents. HyTest is known for its investments in research and development. HyTest supplies for example Troponin I antibodies and HyTest's Troponin complex has been chosen by AACC cTnI Standardization Subcommittee for international reference material which is available from NIST.\n\nSome of HyTest's key products:\n\nHyTest has a certified ISO 9001:2015 quality system\n\n\n"}
{"id": "9853620", "url": "https://en.wikipedia.org/wiki?curid=9853620", "title": "In-circuit test", "text": "In-circuit test\n\nIn-circuit test (ICT) is an example of white box testing where an electrical probe tests a populated printed circuit board (PCB), checking for shorts, opens, resistance, capacitance, and other basic quantities which will show whether the assembly was correctly fabricated. It may be performed with a bed of nails type test fixture and specialist test equipment, or with a fixtureless in-circuit test setup.\n\nA bed of nails tester is a traditional electronic test fixture which has numerous pins inserted into holes in an epoxy phenolic glass cloth laminated sheet (G-10) which are aligned using tooling pins to make contact with test points on a printed circuit board and are also connected to a measuring unit by wires. Named by analogy with a real-world bed of nails, these devices contain an array of small, spring-loaded pogo pins; each pogo pin makes contact with one node in the circuitry of the DUT (device under test). By pressing the DUT down against the bed of nails, reliable contact can be quickly and simultaneously made with hundreds or even thousands of individual test points within the circuitry of the DUT. The hold-down force may be provided manually or by means of a vacuum or a mechanical presser, thus pulling the DUT downwards onto the nails.\n\nDevices that have been tested on a bed of nails tester may show evidence of this after the process: small dimples (from the sharp tips of the Pogo pins) can often be seen on many of the soldered connections of the PCB.\n\nBed of nails fixtures require a mechanical assembly to hold the PCB in place. Fixtures can hold the PCB with either a vacuum or pressing down from the top of the PCB. Vacuum fixtures give better signal reading versus the press-down type. On the other hand, vacuum fixtures are expensive because of their high manufacturing complexity. Moreover, vacuum fixtures cannot be used on bed-of-nails systems that are used in automated production lines, where the board is automatically loaded to the tester by a handling mechanism.\nThe bed of nails or fixture, as generally termed, is used together with an in-circuit tester. Fixtures with a grid of 0.8 mm for small nails and test point diameter 0.6 mm are theoretically possible without using special constructions. But in mass production, test point diameters of 1.0 mm or higher are normally used to minimise contact failures leading to lower remachining costs.\n\nThis technique of testing PCBs is being slowly superseded by boundary scan techniques (silicon test nails), automated optical inspection, and built-in self-test, due to shrinking product sizes and lack of space on PCB's for test pads. Nevertheless ICT is used in mass production to detect failures before doing end-of-line test and producing scrap.\n\nIn-circuit testing has been known to cause mechanical failures such as capacitor flex cracking and pad cratering. This typically occurs on a bed of nails tester if there is excessive board flexure due to poor support placement or high probe forces. It can be challenging to optimize for ideal support locations and probe forces without spending resources designing and building an ICT fixture. Current methods typically employ strain gaging or similar techniques to monitor board flexure. More recently, some have looked at finite element simulation to proactively design or adjust an ICT fixture to avoid these mechanical failure modes. This approach can be implemented as part of a design for manufacturability methodology to provide rapid feedback on ICT design and reduce costs.\n\n\nWhile in-circuit testers are typically limited to testing the above devices, it is possible to add additional hardware to the test fixture to allow different solutions to be implemented. Such additional hardware includes:\n\n\nWhile in-circuit test is a very powerful tool for testing PCBs, it has these limitations:\n\n\nThe following are related technologies and are also used in electronic production to test for the correct operation of Electronics Printed Circuit boards\n\n\n"}
{"id": "51464542", "url": "https://en.wikipedia.org/wiki?curid=51464542", "title": "Inovallée", "text": "Inovallée\n\nInovallée (contraction in French for the words innovation and valley) is a science park located at Meylan and Montbonnot-Saint-Martin near Grenoble in France.\n\nCreated in 1972 with the acronym ZIRST, it becomes Inovallée in 2005 and houses primarily companies in the fields of information and communications technology. In 2014, there are more than 362 companies and 11,174 people working in the park.\n\nInovallée also benefits from the close proximity of the Université Grenoble Alpes, Grenoble Institute of Technology and the Polygone Scientifique.\n\nThe major companies located at Inovallée are Xerox Research Centre Europe, Orange labs, Dolphin Integration, Mirantis, Schneider Electric, Salesforce.com and also the French Institute for Research in Computer Science and Automation.\n\n"}
{"id": "21319016", "url": "https://en.wikipedia.org/wiki?curid=21319016", "title": "Ken Cook", "text": "Ken Cook\n\nKen Cook is president and co-founder of the Environmental Working Group (EWG), a non-profit, non-partisan organization dedicated to protecting human health and the environment. Cook has written dozens of articles, opinion pieces, and reports on environmental, public health, and agricultural topics. He has been an active lobbyist for over 20 years.\nCook has been named as one of Washington's Top Lobbyists by \"The Hill\" and \"The Huffington Post\". He has made multiple appearances on \"The News Hour With Jim Lehrer\", CBS's \"60 Minutes\", National Public Radio, and the evening newscasts of ABC, NBC, CBS, and CNN among other programs. \n\nCook started the Environmental Working Group in 1993, and since its inception they have provided research geared to reforming agriculture policy, advancing conservation techniques and environmental protection. At the onset of debate over the 1995 Farm Bill, the EWG compiled information on who received the money set aside for conservation efforts. The data sets \"sift[ed] through a complex web of corporations, partnerships and other business entities,\" which allowed the USDA to assign specific dollar amounts to the individuals behind the businesses. \n\nIn the 1990s, EWG’s research was a major factor in the passage of the landmark pesticide reform law, the Food Quality Protection Act. EWG was among the first to draw attention to the health threat posed by the weed-killer atrazine and conducted the first extensive tests for the chemical in tap water in 29 Midwest cities. In the last several years Cook and EWG have been in the forefront of national and state campaigns to require the labeling of foods that contain genetically engineered ingredients.\n\nA front-page profile in the \"Omaha World Herald\" in 1996 said, \"Cook's fingerprints can be found on nearly two decades of U.S. farm law.\" In 2000, Progressive Farmer named Cook one of agriculture's most influential leaders in the 20th Century, alongside advocates like Rachel Carson and Aldo Leopold. \n\nCook has appeared as himself in the documentaries \"King Corn\" (2007), \"The World According to Monsanto\" (2008), \"A Place at the Table\" (2012), and \"Pricele$$\" (2012).\n\nCook is a board member of Food Policy Action (and founding chairman), Organic Voices and the Amazon Conservation Team and a former member of the board of the Organic Center\n\nCook graduated from the University of Missouri in Columbia, Missouri with an M.S. in Soil Science. He is married to environmental leader Deb Callahan. Ken is a native of Affton, Missouri and went to Vianney High School.\n\nCook also has a B.A. in history and a B.S. in agriculture.\n\n"}
{"id": "14954762", "url": "https://en.wikipedia.org/wiki?curid=14954762", "title": "List of national highways of Japan", "text": "List of national highways of Japan\n\nThis list of national highways of Japan contains every national route in Japan.\n"}
{"id": "11817652", "url": "https://en.wikipedia.org/wiki?curid=11817652", "title": "Loupe", "text": "Loupe\n\nA loupe ( ) is a simple, small magnification device used to see small details more closely. Unlike a magnifying glass, a loupe does not have an attached handle, and its focusing lens(es) are contained in an opaque cylinder or cone or fold into an enclosing housing that protects the lenses when not in use. Loupes are also called hand lenses.\n\nThree basic types of loupes exist:\n\nLoupes are used in a number of industries, notably the jewelry trade, watchmaking, photography, printing, dentistry, education and ophthalmology. Loupes are also used in academia and life sciences, such as geology and biology. Amateur naturalists may also find a hand lens or a loupe a useful tool when looking at or identifying species. They are also used in numismatics and stamp collecting.\n\nJewelers typically use a monocular, handheld loupe in order to magnify gemstones and other jewelry that they wish to inspect. A 10× magnification is good to use for inspecting jewelry and hallmarks and is the Gemological Institute of America's standard for grading diamond clarity. Stones will sometimes be inspected at higher magnifications than 10×, although the depth of field, which is the area in focus, becomes too small to be instructive. The accepted standard for grading diamonds is therefore that inclusions and blemishes visible at 10× impact the clarity grade.\n\nLoupes are employed to assist watchmakers in assembling mechanical watches. Many aspects require the use of the loupe, in particular the assembly of the watch mechanism itself, the assembly and details of the watch dial, as well as the formation of the watch strap and installation of precious stones onto the watch face. Some families like Kruder which were into watchmaking and working with glass started producing high-quality loupes in addition to their watches\n\nAnalog (film) photographers use loupes to review, edit or analyze negatives and slides on a light table. Typical magnifications for viewing slides full-frame depend on image format; 35 mm frames (24×36 mm slides to 38×38 mm superslides) are best viewed at ca. 5×, while ca. 3× is optimal for viewing medium format slides (6×4.5 cm / 6×6 cm / 6×7 cm). Often, a 10× loupe is used to examine critical sharpness. Photographers using large format cameras also use a loupe to view the ground glass image to aid in focusing. DSLR camera users also use loupes to help to identify dust and other particles on the sensor, in preparation for sensor cleaning.\n\nOffset and flexographic printing see frequent use of loupes in order to carefully analyze how ink lies on paper. Strippers use loupes in order to register film separations to one another. Pressmen use them to check registration of colors, estimate dot-gain, and diagnose issues with roller pressure and chemistry based on the shape of individual dots and rosettes.\n\nDental loupes aid dentists, hygienists, and dental therapists to devise accurate diagnoses of oral conditions and enhance surgical precision when completing treatment. Additionally, loupes can improve dentists' posture which can decrease occupational strain.\n\nDental caries, also known as cavities, are most accurately identified by visual and tactile examination of a clean, dry tooth. Magnification enables dentists to improve their ability to differentiate between a stain and a cavity. Cavities are rated and scored based on their visual presentation. If magnification is too high diagnosis becomes difficult due to the small field of view. Ideal magnification for diagnostic purposes is up to 2×. Treatment of dental caries, periodontal disease, and pulpal disease are all aided by magnification.\n\nThe dental specialty of endodontics has performed the vast majority of research regarding magnification in dentistry. Because the identification of accessory canals in addition to the primary pulp canals is essential to complete nonsurgical root canal therapy, magnification provides dentists enhanced visualization to locate and treat more obscured canals.\n\nTreatment of periodontal disease is achieved by removing calculus deposits, plaque and therefore bacteria which causes inflammation and subsequently bone destruction. In severe cases, surgery to reduce pocket depth is indicated. Periodontists and hygienists must visualize plaque and calculus to remove it. Magnification can assist dentists and hygienists with identification and removal of plaque and calculus in addition to improving visualization for periodontal surgery.\nAs dental professionals use both hands in performing dental procedures, dental loupes are binocular and usually take the form of a pair of glasses. Some dental loupes are flip-type, which take the form of two small cylinders, one in front of each lens of the glasses. Other types are inset within the lens of the glasses. A typical magnification for use in dentistry is 2.5×, but dental loupes can be anywhere in the range from 2× to 8×.\n\nTogether with proper access to the oral cavity, light is an important part of performing precision dentistry. Because a dentist's head often eclipses the overhead dental lamp, loupes may be fitted with a light source. Loupe-mounted lights used to be fed by fiber optic cables that connected to either a wall-mounted or table-top light source. Newer models feature a more convenient LED lamp within the loupe-mounted light and an electric cord coming from either the conventional wall-mounted or table-top light source or a belt clip rechargeable battery pack. Options for loupe-mounted cameras and video recorders are also available.\n\nSurgeons in many specialties commonly use loupes when doing surgery on delicate structures. The loupes used by surgeons are mounted in the lenses of glasses and are custom made for the individual surgeon, taking into account their corrected vision, interpupillary distance and desired focal distance. Multiple magnification powers are available. They are most commonly used in otolaryngology, neurosurgery, plastic surgery, cardiac surgery, orthopedic surgery, and vascular surgery.\n\nThe loupe (hand lens) is a vital geological field tool used to identify small mineral crystals and structures in rocks.\n\nLoupes are used by professional and amateur field biologists for help identifying species in field situations where a full-sized microscope is impractical, but the ability to observe small morphological characteristics is desired. Many floras and diagnostic keys for identifying plant or animal species recommend the use of a loupe, because taxa may be separated by minute details like the presence of hairs, shapes of hairs and glands.\n\nScientists in the meteoritics and planetary science field as well as private collectors of meteorites use loupes as one of their primary tools in meteorite classification and study, and also for simple examination in the field during meteorite recovery.\n\nDue to the extremely small size of many modern surface-mount components used in compact electronics, engineers often use a loupe to inspect the completed circuit board for manufacturing defects such as solder bridging and missing or misaligned components. While soldering or reworking surface-mount components by hand a loupe can be used for identifying and aligning parts. Due to the ever-decreasing size of electronic components, magnification is increasingly required for circuit assembly.\n\nTattoo artists use loupes to inspect the quality of their needle tips. They do this to ensure the least amount of damage possible to the skin surface.\n\nLoupes are an essential tool in both numismatics, the study of currency, and the related practice of coin collection. Coin collectors frequently employ loupes for better evaluation of the quality of their coins, since identifying surface wear is vital when attempting to classify the grade of a coin. Uncirculated coins (coins without wear) can command a substantial premium over coins with slight wear. This wear cannot always be seen with the naked eye. Numismatists can also employ loupes to identify some counterfeit coins that would pass a naked-eye visual inspection.\n\nStamp collectors employ loupes to improve their ability to evaluate the quality of their stamps. Identifying surface, adhesive, and perforation is vital when attempting to classify the grade of a stamp. Mint stamps (stamps without cancellations) with no wear can command a substantial premium over mint stamps with slight wear. This wear cannot always be seen with the naked eye. Practitioners of philately, the study of postage and revenue stamps, also employ loupes to identify counterfeit stamps that would pass naked-eye inspections.\n\nSharpeners of knives or other edged or pointed tools may use a loupe to inspect the tool and ensure correct angle, straightness of the bevel, a sharp edge and/or point, and a polished finish.\n\n"}
{"id": "35778547", "url": "https://en.wikipedia.org/wiki?curid=35778547", "title": "MDNX", "text": "MDNX\n\nMDNX was a private telecommunications company located in Bracknell and London. In December 2013 MDNX acquired the entire issued share capital of Easynet, a global provider of managed networking, hosting and cloud integration services, from LDC. The combined business went by the name of Easynet. Its CEO was Mark Thompson. The company was acquired by Interoute in September 2015.\n\nMDNX's origins lie in the former Siemens service group company, 'Solution1'. Thompson and colleague Wayne Churchill joined Solution1 in 2009, and decided to set up MDNX. Within months MDNX bought out 'Solution1' along with its customer base, acquired the retail section of \"Viatel\", gaining fibre network and hosting sites, and CI-Net with cloud and wireless capability. The new company brand launched in 2010.\n\nIn March 2012 MDNX was one of 12 companies that successfully bid for the provision of services to the UK Public Services Network.\n\nIn August 2012 MDNX acquired Octium (trading as Griffin and Iconnyx).\n\nIn December 2013 MDNX acquired Easynet.\n\nIn September 2015 Interoute announced agreement to acquire MDNX (trading as Easynet) for £402m.\n\n"}
{"id": "29651059", "url": "https://en.wikipedia.org/wiki?curid=29651059", "title": "Meebox", "text": "Meebox\n\nMeebox, (also stylized as Meeb[ ]x) is a Mexican company specializing in the design and manufacturing of computers and other consumer electronics. Meebox has operations in Latin America and the United States. It was the first Mexican company to manufacture a full functioned tablet computer. and is one of only three Mexican companies which manufacture tablet PCs. In September 2011, Mexican telecom giant Telmex began selling Meebox tablet computers for use with the Telcel 3G wireless internet network. In 2012, Honda Motor Corporation of Japan began using Meebox tablets and computers for point of sales units and business management, becoming the first major foreign business client of Meebox.\n\nMeebox produces a wide range of consumer electronics and parts including, desktop tower units, LCD displays, solar panels, netbooks, laptop computers, webcams, speakers, RAM memory, DVD drives, surge protectors, mice, cables, keyboards, adapters, headphones, and point of sales and display units for commercial clients.\n\nIn addition to the aforementioned products, most of Meebox's sales come from its two distinctive products: An All-in-one PC, the Meebox touch and a Windows powered tablet pc, the Meebox Slate.*Meebox Slate:\n"}
{"id": "6742061", "url": "https://en.wikipedia.org/wiki?curid=6742061", "title": "Megaproject", "text": "Megaproject\n\nA megaproject is an extremely large-scale investment project. \nAccording to the \"Oxford Handbook of Megaproject Management\", \"Megaprojects are large-scale, complex ventures that typically cost $1 billion or more, take many years to develop and build, involve multiple public and private stakeholders, are transformational, and impact millions of people\". However, $1 billion is not a constraint in defining megaprojects; in some contexts a relative approach is needed, such as in developing countries, where a much smaller project (such as one with a $100 million budget) could constitute a megaproject. Therefore, a more general definition is \"Megaprojects are temporary endeavours (i.e. projects) characterized by: large investment commitment, vast complexity (especially in organizational terms), and long-lasting impact on the economy, the environment, and society\".\n\nAccording to the European Cooperation in Science and Technology (COST), megaprojects are characterized both by \"extreme complexity (both in technical and human terms) and by a long record of poor delivery\". Megaprojects attract a lot of public attention because of substantial impacts on communities, environment, and budgets, and the high costs involved. Megaprojects can also be defined as \"initiatives that are physical, very expensive, and public\". Bent Flyvbjerg, a professor at the Saïd Business School of the University of Oxford says that globally, megaprojects make up 8 percent of total GDP.\n\nCare in the project development process is required to reduce any possible optimism bias and strategic misrepresentation, as a curious paradox exists in which more and more megaprojects are being proposed despite their consistently poor performance against initial forecasts of budget, schedule, and benefits.\n\nMegaprojects are often affected by corruption leading to higher cost and lower benefit.\n\nMegaprojects include bridges, tunnels, highways, railways, airports, seaports, power plants, dams, wastewater projects, Special Economic Zones, oil and natural gas extraction projects, public buildings, information technology systems, aerospace projects, weapons systems, large-scale sporting events and, more recently, mixed use waterfront redevelopments; however, the most common megaprojects are in the categories of hydroelectric facilities, nuclear power plants, and large public transportation projects. Megaprojects can also include large-scale high-cost initiatives in scientific research and infrastructure, such as the sequencing of the human genome, a significant global advance in genetics and biotechnology.\n\nAccording to Bent Flyvbjerg, \"As a general rule of thumb, 'megaprojects' are measured in billions of dollars, 'major projects' in hundreds of millions, and 'projects' in millions and tens of millions.\"\n\nThe logic on which many of the typical megaprojects are built is collective benefits; for example electricity for everybody (who can pay), road access (for those that have cars), etc. They may also serve as the means for opening frontiers. Megaprojects have undergone a wide criticism for their top down planning processes and for their ill effects on certain communities. Large scale projects often advantage one group of people while disadvantaging another, for instance, the Three Gorges Dam in China is the largest hydroelectric project in the world, but required the displacement of 1.2 million farmers. In the 1970s, the Highway revolts involved urban activists opposing government plans to demolish buildings in freeway routes that would disadvantage the urban working class to benefit commuters. Anti-nuclear protests against proposed nuclear power plants in the United States and Germany prevented developments due to environmental and social concerns.\n\nMore recently, new types of megaprojects have been identified that no longer follow the old models of being singular and monolithic in their purposes, but have become quite flexible and diverse, such as waterfront redevelopment schemes that seem to offer something to everybody. However, just like the old megaprojects, the new ones also foreclose \"upon a wide variety of social practices, reproducing rather than resolving urban inequality and disenfranchisement\". Because of their plethora of land uses \"these mega-projects inhibit the growth of oppositional and contestational practices\". The collective benefits that are often the underlying logic of a mega-project, are here reduced to an individualized form of public benefit.\n\nBent Flyvbjerg argues that policymakers are attracted to megaprojects for four reasons:\n\n\nProponents of infrastructure-based development advocate for funding large-scale projects to create long-term economic benefits. Investing in megaprojects in order to stimulate the general economy has been a popular policy measure since the economic crisis of the 1930s. Recent examples are the 2008–2009 Chinese economic stimulus program, the 2008 European Union stimulus plan, and the American Recovery and Reinvestment Act of 2009.\n\nMegaprojects often raise capital based on expected returns—though projects often go overbudget and over time, and market conditions like commodity prices can change. Concern at cost overruns is often expressed by critics of megaprojects during the planning phase. Bent Flyvbjerg has noted the existence of incentives to overstate income, underestimate costs, and exaggerate future social and economic benefits due to lack of accountability and risk-sharing mechanisms. If the megaproject is delivered in a country with relevant corruption the likelihood and magnitude of having overbudgets increases. \n\nOne of the most challenging aspects of megaprojects is obtaining sufficient funding. Alan Altshuler and David Luberoff have found that creative and politically adept political leadership is required to secure resources as well as generate public support, mollify critics, and manage conflict through many years of planning, authorization and implementation. Other challenges faced by those planning megaprojects include laws and regulations that empower community groups, contested information and methodologies, high levels of uncertainty, avoiding impacts on neighborhoods and the environment, and attempting to solve a wicked problem.\n\nA megaproject's economic failures (and successes) have common characteristics. According to John Cunningham, assessing their success, or failure, can be accomplished by benchmarking their performance against these common characteristics in order to mitigate failure and turn a troubled megaproject around.\n\n\n"}
{"id": "14218425", "url": "https://en.wikipedia.org/wiki?curid=14218425", "title": "Metadesign", "text": "Metadesign\n\nMetadesign (or meta-design) is an emerging conceptual framework aimed at defining and creating social, economic and technical infrastructures in which new forms of collaborative design can take place. It consists of a series of practical design-related tools for achieving this.\n\nAs a methodology, its aim is to nurture emergence of the previously unthinkable as possibilities or prospects through the collaboration of designers within interdisciplinarity 'metadesign' teams. Inspired by the way living systems work, this new field aims to help improve the way we feed, clothe, shelter, assemble, communicate and live together.\n\nMetadesign has been initially put forward as an industrial design approach to complexity theory and information systems by Dutch designer Andries Van Onck in 1963, while at Ulm School of Design (later at Politecnico di Milano and Rome and Florence ISIA). Since then, several different design, creative and research approaches have used the name \"Metadesign\", ranging from Humberto Maturana and Francisco Varela's biological approach, to Gerhard Fischer's and Elisa Giaccardi's techno-social approach, and Paul Virilio's techno-policital approach.\n\nLater on, a very active group was present at Politecnico di Milano, and several different universities and graduate programs began applying Metadesign in design teaching around the world generally based at Van Onck's approach, further developed at Politecnico di Milano. Nevertheless, there's a very active, but widely dispersed, group that base their activities at Maturana and Varela's approach.\n\nMore recently, some efforts have been made to systematize Metadesign as a structured creative process, such as (1) Fischer's and Giaccardi's and (2) Caio Vassão's academic works, among several others, based on a much wider reference frame, ranging from post-structuralist philosophy, Neil Postman's media ecology, Christopher Alexander's pattern languages and deep ecology.\n\nThis variety of approaches is justified by the myriad interpretations that can be derived from the etymological structure of the term.\n\nThe Greek word 'meta' originally meant 'beside' or 'after' but is now also used to imply the possibility of change or transformation, including self-transformation. Metadesign can therefore allude to a possible design practice that (re)designs itself (see Maturana and Varela's term autopoiesis). The idea of Metadesign acknowledges that future uses and problems cannot be completely anticipated at design time. Aristotle's influential theory of design defined it by saying that the 'cause' of design was its final state. This teleological perspective is similar to the orthodox idea of an economic payback at the point of sale, rather than successive stages when the product could be seen to achieve high levels of perceived value, throughout the whole design cycle. Some supporters of metadesign hope that it will extend the traditional notion of system design beyond the original development of a system by allowing users to become co-designers.\n\nBy harnessing creative teamwork within a suitable co-design framework, some metadesigners have sought to catalyse changes at a behavioural level. However, as Einstein said, \"We can't solve problems by using the same kind of thinking we used when we created them\". This points to a need for appropriate innovation at all levels, including the metaphorical language that serves to sustain a given paradigm. In practical terms this adds considerable complexity to the task of managing actions and outcomes. What may be so neatly described as 'new knowledge', in practical terms, exists as an interpersonal and somatic web of tacit knowledge that needs to be interpreted and applied by many collaborators. This tends to reduce the semantic certainty of roles, actions and descriptors within a given team, making it necessary to rename particular shared experiences that seem inappropriately defined. In other instances it may be necessary to invent new words to describe perceived gaps in what can be discussed within a prevailing vernacular. Humberto Maturana's work on distributed language and the field of biosemiotics is germane to this task. Some researchers have used bisociation in order to create an auspicious synergy of benign synergies. In aspiring to this outcome, metadesign teams will cultivate auspicious 'diversities-of-diversities'. It suggests that metadesign would offer a manifold ethical space. In this respect, related approaches include what Arthur Koestler (1967) called holarchy, or what John Dewey and John Chris Jones have called 'creative democracy'.\n\nRegarding a wide range of applications and contexts, Vassão has argued that Metadesign can be understood as a set of four \"conceptual tools\", utilizing Gilles Deleuze's understanding of the term \"tool\":\n\nVassão has argued that, in all different approaches to metadesign, the presence of these conceptual tools can be verified.\n\n"}
{"id": "28670861", "url": "https://en.wikipedia.org/wiki?curid=28670861", "title": "Missha", "text": "Missha\n\nMissha () is a South Korea-based skincare and cosmetics manufacturer, retailer and a franchise business. It is part of Able C&C Co., Ltd.\n\nIn August 2009, the luxury Missha Homme Urban Soul line was launched with higher quality and are more expensive than the existing two men's cosmetic lines.\n\nIn August 2012, as part of Lotte Department Store's expansion programme into China, a replica of Seoul’s main shopping district Myeong-dong was featured in its new store in Tianjin, with outlets of Missha, The Face Shop and Skin Food.\n\nMissha products range from makeup, skin care, to body and hair products; such as All-Around Safe Block Soft Finish Sun Milk SPF 50, Cool Fitting Body Gel and Hot Burning Body Gel.\n\nMissha is also known for making high quality, lower cost duplications of more expensive brands. For example, the Missha Time Revolution First Treatment Essence is a popular duplication of luxury brand essence SK-II, and the Missha Time Revolution Night Repair Science Activator Ampoule is a popular duplicate of the Estee Lauder Advanced Night Repair.\n\n\n\n\n\n\nOn August 1, 2011 it was announced that South Korean boyband duo TVXQ signed on as advertisement models for Missha in Korea and started their official promotions that month. On choosing TVXQ, Able-CNC Marketing said \"TVXQ’s powerful and luxurious image coupled with their passion for bringing new ideas to the stage perfectly fits the brand image of ‘Missha’ to a T. With TVXQ, Missha aims to become a global cosmetic brands in conjunction with the Hallyu wave, and thus become leaders of the international market\".\n\nOn 14 August 2012, Missha launched a new limited-edition TVXQ perfume set and had a special fan signing event at the Missha store in Myeong-dong. On September 10, 2012, Missha announced that the duo had again signed an exclusive contract with them to represent the brand worldwide and that their advertisements would soon be featured in some 1,000 stores across Asia.\n\n\nOn August 22, 2007, a court ruled Missha was guilty of trademark infringement against Mary Quant Cosmetics. Missha's original flower logo was determined to be too similar to the Mary Quant Daisy logo. Missha was fined and the ruling stated that \"Although Missha’s trademark combines a diagram and letters and has different colors compared to the trademark of the Mary Quant Cosmetics, there are reasonable concerns that the identical shapes of the diagrams could hurt the brand recognition of the plaintiff and may confuse customers\". The company was required to change its logo, because under Korean copyright law, companies cannot use symbols that are similar in shape to a previously-registered trademark but only differ in color.\n\nOn January 2, 2015, all 20 Missha stores in Hong Kong and Macau were closed and all staff left the stores the same day. Various notices were posted at the stores, stating that \"Missha is no longer available to serve you. We apologize for any inconvenience caused\" and \"under construction\".\n\nAccording to a local newspaper, Apple Daily, an employee received a WhatsApp message from a head-office colleague at about noon, saying that \"The boss is gone. Everyone can leave.\" As reported, Missha owes the former staff over HK$1 million for salary and severance payments.\n\n"}
{"id": "2768512", "url": "https://en.wikipedia.org/wiki?curid=2768512", "title": "Optoelectronic plethysmography", "text": "Optoelectronic plethysmography\n\nOptoelectronic plethysmography (OEP) is a method to evaluate ventilation through an external measurement of the chest wall surface motion.\n\nA number of small reflective markers are placed on the thoraco-abdominal surface by hypoallergenic adhesive tape. A system for human motion analysis measures the three-dimensional coordinates of these markers and the enclosed volume is computed by connecting the points to form triangles.\n\nFrom OEP it is thus possible to obtain volume variations of the entire chest wall and its different compartments. The chest wall can be modeled as being composed of three different compartments: pulmonary rib cage (RCp), abdominal rib cage (RCa), and the abdomen (AB). This model is the most appropriate for the study of chest wall kinematics in the majority of conditions, including exercise. It takes into consideration the fact that the lung- and diaphragm-apposed parts of the rib cage (RCp and RCa, respectively) are exposed to substantially different pressures on their inner surface during inspiration, that the diaphragm acts directly only on RCa, and that non-diaphragmatic inspiratory muscles act largely on RCp. Abdominal volume change is defined as the volume swept by the abdominal wall.\n\nOptoelectronic plethysmography can be used following different measurement protocols, specifically developed for different applications and different experimental and clinical situations. In the arrangement designed for the analysis in sitting and standing positions, 89 markers are arranged on the thoraco-abdominal surface.\n\nOptoelectronic plethysmography can be used also in supine and prone positions.\n\nOEP was used to study chest wall kinematics in healthy subjects during exercise, patients with Chronic Obstructive Pulmonary Disease, patients with neuromuscular disorders and in Intensive Care Unit.\n\nThe validation of the method was obtained by comparing the lung volume changes obtained by Volumetric and Flow measuring Spirometers and chest wall total volumes by optoelectronic plethysmography during different maneuvers.\n\nThis method has been developed at the Bioengineering Department of the Politecnico di Milano university by Andrea Aliverti and collaborators.\n\n"}
{"id": "35638495", "url": "https://en.wikipedia.org/wiki?curid=35638495", "title": "Other World Computing", "text": "Other World Computing\n\nOther World Computing (OWC) is an American computer hardware company and online store for Mac upgrades and accessories located at MacSales.com that was founded in 1988.\n\nIn 1988, at age 14, Larry O'Connor began LRO Enterprises, a printer ribbon re-inking business, in his family's barn. A year later, LRO Enterprises reorganized into LRO Computer Sales and began selling computer memory chips via America Online. The company moved into its first facility in Woodstock, Illinois and hired its first employees.\n\nIn 1992, LRO Computer Sales shifted focus to computers by offering hard drives to its customers. In 1993, LRO Computer Sales incorporated in the state of Illinois under the name New Concepts Development Corporation (NCDC). The company then moved into a 2,500-square-foot office space, which expanded to about 6,500-square feet over the next eight years. In 1994, O'Connor renamed LRO Computers Sales \"Other World Computing\" (OWC), which operates doing business as NCDC. OWC shipped its first OWC-branded acceleration products in 1995 followed by the introduction of the Mercury G3 ZIF upgrade line in 1999.\n\nOWC expanded and introduced the Mercury Classic Elite line of external storage and offered an iPod case. OWC announced a portable FireWire drive and a FireWire/USB combination product in 2003.\n\nIn 2003, OWC released a line of external storage products; the Mercury Extreme product line with a G4/1.33 GHz processor upgrade, the fastest Macintosh processor to that date; an extra high-capacity NuPower replacement battery compatible with PowerBook G3 FireWire (2000/Pismo) and PowerBook G3 Lombard (1999/Bronze Keyboard) models; and the OWC Neptune line of external 7200RPM FireWire storage solutions. OWC launched FasterMac.net, a Macintosh-only Internet access service that provided dialup access throughout the U.S. specifically for Macintosh computer users in 2003.\n\nIn 2004, the company also began offering an iPod battery replacement program and introduced the miniStack line of drives to complement Apple's Mac mini. In 2006, OWC introduced the first Dual-HD external FireWire drive RAID available up to 1.5 TB and became the first third party company with memory modules and upgrade kits for the Intel-based Mac Pro that met Apple specifications and first to introduce a Quad Interface external hard drive combining FireWire 800, FireWire 400, USB 2.0, and eSATA connection options in one product – the OWC Mercury Elite-AL Pro Quad Interface.\n\nIn January 2007, OWC announced it would be the US distributor of the Axiotron Modbook.\nOWC also introduced the OWC Mercury Rack Pro line and the OWC Blu-ray internal and external drives. In April 2009, OWC expanded its storage line with the OWC Mercury Elite Pro Qx2, a desktop hardware RAID storage product.\n\nIn 2008, OWC moved into a new corporate headquarters designed to platinum Leadership in Energy and Environmental Design standards. \n\nIn October 2009, a Vestas V39-500kW wind turbine started generating more electricity than OWC needed to run the facility, view OWC's think green efforts. OWC said it was the first technology manufacturer/distributor in the U.S. to become totally on-site wind powered.\n\nOther World Computing was on the \"Inc.\" magazine 5000 \"Fastest-Growing Privately Owned Companies\" and \"Computer and Electronics Top 100\" list from 2007 through 2013.\nIn 2010, OWC announced the Mercury Extreme SSD line of 2.5\" SATA solid state drives. The OWC Data Doubler, for adding a second internal drive to MacBook, MacBook Pro, Mac mini, and iMac computers was also introduced as was the OWC Slim eSATA ExpressCard Adapter, which adds an eSATA port to Mac and PC notebooks.\n\nIn 2011, sales revenue was reported as $88.3 million with about 137 employees.\n\nOWC adopted solar power as an energy source at its two largest locations in Woodstock and Austin, TX. The Woodstock, IL solar system will generate 265,000 kWh per year, when combined with power from the wind turbine it brings the Woodstock headquarters’ total alternative power generation capacity to over one million kilowatts. It also means that over the course of a year, OWC produces more power than it consumes.\n\nA similar but smaller array is on the roof of OWC's Austin,TX building. Energized at the beginning of 2014, 160 solar panels generate approximately one-third of the power consumed by the three-story building, including the majority of power that is consumed by OWC.\n\nOWC markets upgrade kits for iMac, Macbook Pro, Macbook Air, Mac mini, MacBook and Mac Pro.\nThe Data-Doubler installation kit allows customers to add a second 2.5\" SATA hard disk drive or solid state drive to the optical drive bay of a Mac mini, MacBook, or MacBook Pro. The optical drive can then be repurposed as an external drive.\nOWC designs and manufactures solid state drives.\nMaxRAM is a line of memory upgrades for Apple products.\n"}
{"id": "54535694", "url": "https://en.wikipedia.org/wiki?curid=54535694", "title": "Passion Dust", "text": "Passion Dust\n\nPassion Dust Intimacy Capsules are a novelty cosmetic product, introduced in 2017, that consists of capsules full of small, glittering particles that are intended to be inserted into the vagina before sex. The product is advertised as making the female genitalia \"look, feel and taste soft, sweet and magical\", but its use may carry serious health risks.\n\nPassion Dust was invented by Lola-Butterflie Von-Kerius, an American who sells the product from her home via the Internet under the label Pretty Woman Inc. According to her website, the \"passion dust\" is made of gelatin, starch-based edible glitter, gum arabic, zea mays starch and vegetable stearate.\n\nFollowing reports of the viral success of Passion Dust in 2017, gynecologists interviewed by news media warned that the use of Passion Dust may carry serious health risks. The particles may disturb the bacterial balance of the vagina, and the starch and gelatin they contain may encourage the growth of harmful bacteria and fungi, which may cause infections such as bacterial vaginosis or vaginal yeast infection, and inflammation of the vagina. The particles may scratch the vaginal mucosa, allowing infection of the vaginal walls, and they may also migrate up through the cervix to cause similar damage to the lining of the uterus. The website of Pretty Woman Inc. states that the product is harmless.\n"}
{"id": "1227787", "url": "https://en.wikipedia.org/wiki?curid=1227787", "title": "Pierre Victor Auger", "text": "Pierre Victor Auger\n\nPierre Victor Auger (14 May 1899 – 25 December 1993) was a French physicist, born in Paris. He worked in the fields of atomic physics, nuclear physics, and cosmic ray physics. He is famous for being one of the discoverers of the Auger effect, named after him.\n\nPierre's father was chemistry professor Victor Auger. Pierre Auger was a student at the École normale supérieure in Paris from 1919 to 1922, the year when he passed the agrégation of physics. He then joined the physical chemistry laboratory of the faculté des sciences of the University of Paris under the direction of Jean Perrin to work there on the photoelectric effect.\n\nIn 1926 he obtained his doctorate in physics from the University of Paris. In 1927, he was named assistant to the faculté des sciences of Paris and, at the same time, adjoint chief of service to l'Institut de biologie physico-chimique. Chief of work to faculty in 1934 and general secretary of the annual tables of the constants in 1936, he was named university lecturer in physics to the faculty on the first of November 1937. He was charged with, until 1940, the course on the experimental bases of the quantum theory within the chair of theoretical physics and astrophysics. He was also adjoint director of the laboratory of physical chemistry. He then occupied the chair of quantum physics and relativity of the faculté des sciences of Paris.\n\nAt the end of World War II, he was named director of higher education from 1945 to 1948, which permitted him to introduce the first chair of genetics at the Sorbonne, conferred upon Boris Ephrussi.\n\nThe process where Auger electrons are emitted from atoms is used in Auger electron spectroscopy to study the elements on the surface of materials. This method was named after him, despite the fact that Lise Meitner discovered the process a few years before in 1922.\n\nIn his work with cosmic rays, he found that the cosmic radiation events were coincident in time meaning that they were associated with a single event, an air shower. He estimated that the energy of the incoming particle that creates large air showers must be at least 10 electronvolts (eV) = 10 particles of 10 eV (critical energy in air) and a factor of ten for energy loss from traversing the atmosphere.\n\n\n\n\n\n\n"}
{"id": "25959056", "url": "https://en.wikipedia.org/wiki?curid=25959056", "title": "Preservation development", "text": "Preservation development\n\nPreservation development is a model of real-estate development that addresses farmland preservation. It shares many attributes with conservation development, with the addition of strategies for maintaining and operating productive agriculture and silviculture, often in perpetuity. A preservation development is a master planned community that allows limited, carefully designed development (typically housing) on a working farm, while placing the majority of productive land under a system of easements and community governance to ensure a continuity of farming and environmental stewardship.\n\nPreservation development is not a formal planning approach, but an example of goal-oriented environmental planning. Particular characteristics of the land, local market and local agricultural norms influence the tools to be deployed in each case. The successful project should, however, aim to meet several goals:\n\nPreservation development was developed in the 1980s in response to rapid farmland loss due to urban sprawl around Boston. Robert Baldwin, Sr. devised the system of interlocking \"farmbelt\" and \"greenbelt\" easements. The system, and associated design and community governance tools, was refined through the 1990s on projects around New England. In 2005, this model was expanded into the Southeast, beginning with the 2,300 acre (931 ha) Bundoran Farm, in Charlottesville, Virginia.\n\nIn the United States, most land is conserved by a combination of charitable giving and tax incentives. Parcels with ecological, historic or scenic value may be voluntarily placed under conservation easement, which prohibits or significantly limits future development of the land. The landowner may be directly compensated for the easement (Purchase of Development Rights), or the future-development rights may be considered a donation, subject to tax credits offsetting income taxes due. In some US states, the tax credits may be sold to generate income from the transaction. In a few localities, future-development rights may be sold or traded (Transferable development rights), and redeployed in urbanizing areas.\n\nPreservation development is a market-based approach, and does not rely on taxpayer funding or charitable donation. The landowner sells the land. Development and land protections are enacted simultaneously, and the resulting subdivided parcels are sold to individuals. The value of each parcel is increased by adjacency and access to the conserved land, which allows development density significantly below that allowed by zoning.\n\nPreservation development is a type of sustainable development wherein the natural carrying capacity of land is considered not only in terms of development but also in agricultural capacity and ecological service. Rather than maximizing development, developers seek a Triple Bottom Line (TBL) balance between social, environmental and economic factors.\n\nNew Urbanism and Smart Growth promote density, interconnectivity and access to transit as desirable goals of urban planning. Both approaches privilege development in infill locations and brownfields. Preservation Development's focus on greenfield sites with active agriculture and forestry places has placed it outside the mainstream of either movement.\n\nSince 2001, however, New Urbanist planners Duany / Plater-Zyberk have promoted a \"transect\" zoning approach, recognizing the need to extend Smart Growth approaches to highly urbanized and rural locations. These new codes address development pressure in exurban locations, as does Preservation Development. In this context, Preservation Development is an appropriate settlement pattern for the two or three lowest-density landscape types on the transect, and insufficiently dense for the other categories.\n\nSome communities with zoning influenced by tenets of Smart Growth have embraced Preservation Development as an additional tool for managing exurban growth.\n\n"}
{"id": "54382181", "url": "https://en.wikipedia.org/wiki?curid=54382181", "title": "Sabamobil", "text": "Sabamobil\n\nSabamobil was a magnetic tape audio cartridge format, made by SABA and came to the market in 1964. It used already available four-track ¼ inch tape on 3-inch reels (= 7.62 cm), with two mono channels per side, using a tape speed of 3¾ IPS (~ 9.5 cm/s), and was compatible with reel-to-reel audio tape recording except the against remove secured ends of the tape in the reel. The cartridge could be opened without the need of any tools by removing two holding clamps. Tape head and capstan were placed between the reels.\n\nIn the US the player was offered for US$ 136, which would correspond to $ of today, a cassette was $14 (with inflation of today $ ) and the adapter for installation in car was $45. The model TK-R12 also had an builtin medium frequency AM-broadcast receiver and could also be operated portable with five D-type batteries. The drive assembly had no drive belts. It appeared in the following year of the introduction of the Compact Cassette and lost its market shares soon to 8-track and Compact Cassette, which both came in smaller cartridges.\n\nA similar technic to reuse standard 3-inch reels was the design of the dictation machine \"Philips Norelco EL3581\", but with rearranged tracks and slower tape speed.\n\n"}
{"id": "15004087", "url": "https://en.wikipedia.org/wiki?curid=15004087", "title": "Society for Information Display", "text": "Society for Information Display\n\nThe Society for Information Display (SID) is an industry organization for displays, generally electronic displays such as televisions and computer monitors. SID was founded in 1962. Its main activities are publishing technical journals and running \"Display Week\", its main conference, held in May or June each year. SID publications include the \"Journal of the Society for Information Display\", published monthly, the \"Digest of Technical Papers\" from SID's annual conference, \"Information Display\" magazine, proceedings from other conferences such as the Vehicle Displays and Interfaces Symposium, Asia Display, and International Display Workshops. In addition, local chapters in the Americas, Europe, and throughout Asia have meetings frequently, including lectures by display technologists, and are sometimes offered as webcasts. The SID Board of Directors grants several SID awards based upon outstanding achievements and significant contributions.\nIn early 1960s, IRE (Institute of Radio Engineers. Later IRE was merged with AIEE to form IEEE in January, 1963.) declined to create a new Section devoted solely to electronic information displays. This caused founders from IRE to start their own society, Society for Information Display. \nSID's founding meeting was called by Dr. Luxenberg September 29, 1962 at UCLA in Boelter Hall. \nDue to emergence of new display technologies and their successful commercialization, such as CRT, Plasma Display, LCD Display and OLED Displays, SID has expanded its scale in membership as well as the attendance in Display Week, its symposium and exhibition. \n\nThe headquarter of SID is located in Campbell, California.\n\nSID has 5 society officers as follows.\n\nThe Best in Show award is intended to honor the most significant advances in display technology and systems, products, prototypes, and manufacturing processes presented by exhibitors during Display Week.The following factors will be used by the award committee to evaluate the exhibit material:\n\nThe Display Industry Awards committee of SID will select the 2013 Best in Show award winners based on the self-nomination forms available to all organizations exhibiting during Display Week 2013 and on an onsite review of the nominated exhibits. Final selection will be done at the actual exhibition. Blue Ribbon awards will be presented to the winners at the SID Awards Lunch on Wednesday during Display Week. (The winners will be informed on Tuesday night, prior to the lunch on Wednesday). Winners will be encouraged to mount the blue ribbons in a visible location in their exhibit booths. Engraved bronze plaques will be provided to the three winners after Display Week.\n\nBest Prototype Award are given to the best/most disruptive prototype in the peer-reviewed I-Zone at Display Week.\n\nDisplay Industry Awards are given to the best new display component, display application, and display of the year, industry-wide\n\n\n\n\nICDM is the International Committee for Display Metrology, part of SID’s Definitions and Standards Committee charged with setting standards for display metrology. ICDM is largely focused on the production of the Information Display Measurements Standard, which has about 140 display measurements covering every area of displays. The standard includes nearly every display technology for multiple user types, from display manufacturers to consumers.\n"}
{"id": "413204", "url": "https://en.wikipedia.org/wiki?curid=413204", "title": "Storm drain", "text": "Storm drain\n\nA storm drain, storm sewer (U.S. and Canada), surface water drain/sewer (United Kingdom), or stormwater drain (Australia and New Zealand) is infrastructure designed to drain excess rain and ground water from impervious surfaces such as paved streets, car parks, parking lots, footpaths, sidewalks, and roofs. Storm drains vary in design from small residential dry wells to large municipal systems.\n\nDrains receive water from street gutters on most motorways, freeways and other busy roads, as well as towns in areas with heavy rainfall that leads to flooding, and coastal towns with regular storms. Even gutters from houses and buildings can connect to the storm drain. Many storm drainage systems are gravity sewers that drain untreated storm water into rivers or streams—so it is unacceptable to pour hazardous substances into the drains.\n\nStorm drains often cannot manage the quantity of rain that falls in heavy rains or storms. Inundated drains can cause basement and street flooding. In many areas require detention tanks inside a property that temporarily hold runoff in heavy rains and restrict outlet flow to the public sewer. This reduces the risk of overwhelming the public sewer. Some storm drains mix stormwater (rainwater) with sewage, either intentionally in the case of combined sewers, or unintentionally.\n\nSeveral related terms are used differently in American and British English:\n\nThere are two main types of stormwater drain (highway drain or road gully in the UK) inlets: side inlets and grated inlets. Side inlets are located adjacent to the curb (curb) and rely on the ability of the opening under the back stone or lintel to capture flow. They are usually depressed at the invert of the channel to improve capture capacity.\n\nMany inlets have gratings or grids to prevent people, vehicles, large objects or debris from falling into the storm drain. Grate bars are spaced so that the flow of water is not impeded, but sediment and many small objects can also fall through. However, if grate bars are too far apart, the openings may present a risk to pedestrians, bicyclists, and others in the vicinity. Grates with long narrow slots parallel to traffic flow are of particular concern to cyclists, as the front tire of a bicycle may become stuck, causing the cyclist to go over the handlebars or lose control and fall. Storm drains in streets and parking areas must be strong enough to support the weight of vehicles, and are often made of cast iron or reinforced concrete.\n\nSome of the heavier sediment and small objects may settle in a catch basin, or sump, which lies immediately below the outlet, where water from the top of the catch basin reservoir overflows into the sewer proper. The catchbasin serves much the same function as the \"trap\" in household wastewater plumbing in trapping objects.\n\nIn the United States, unlike the plumbing trap, the catch basin does not necessarily prevent sewer gases such as hydrogen sulfide and methane from escaping. However, in the United Kingdom, where they are called gully pots, they are designed as true water-filled traps and do block the egress of gases and rodents.\n\nMost catchbasins contain stagnant water during drier parts of the year and can, in warm countries, become mosquito breeding grounds. Larvicides or disruptive larval hormones, sometimes released from \"mosquito biscuits\", have been used to control mosquito breeding in catch basins. Mosquitoes may be physically prevented from reaching the standing water or migrating into the sewer proper by the use of an \"inverted cone filter\". Another method of mosquito control is to spread a thin layer of oil on the surface of stagnant water, interfering with the breathing tubes of mosquito larvae.\n\nThe performance of catch basins at removing sediment and other pollutants depends on the design of the catchbasin (for example, the size of the sump), and on routine maintenance to retain the storage available in the sump to capture sediment. Municipalities typically have large vacuum trucks that perform this task.\n\nCatch basins act as the first-line pretreatment for other treatment practices, such as retention basins, by capturing large sediments and street litter from urban runoff before it enters the storm drainage pipes.\n\nPipes can come in many different cross-sectional shapes (rectangular, square, bread-loaf-shaped, oval, inverted pear-shaped, egg shaped, and most commonly, circular). Drainage systems may have many different features including waterfalls, stairways, balconies and pits for catching rubbish, sometimes called Gross Pollutant Traps (GPTs). Pipes made of different materials can also be used, such as brick, concrete, high-density polyethylene or galvanized steel. Fibre reinforced plastic is being used more commonly for drain pipes and fittings.\n\nMost drains have a single large exit at their point of discharge (often covered by a grating) into a canal, river, lake, reservoir, sea or ocean. Other than catchbasins, typically there are no treatment facilities in the piping system. Small storm drains may discharge into individual dry wells. Storm drains may be interconnected using slotted pipe, to make a larger dry well system. Storm drains may discharge into man-made excavations known as recharge basins or retention ponds.\n\nStorm drains are often unable to manage the quantity of rain that falls during heavy rains and/or storms. When storm drains are inundated, basement and street flooding can occur. Unlike catastrophic flooding events, this type of urban flooding occurs in built-up areas where man-made drainage systems are prevalent. Urban flooding is the primary cause of sewer backups and basement flooding, which can affect properties repeatedly.\n\nClogged drains also contribute to flooding by the obstruction of storm drains. Communities or cities can help reduce this by cleaning leaves from the storm drains to stop ponding or flooding into yards. Snow in the winter can also clog drains when there is an unusual amount of rain in the winter and snow is plowed atop storm drains.\n\nRunoff into storm sewers can be minimized by including \"sustainable urban drainage systems\" (UK term) or \"low impact development\" or \"green infrastructure\" practices (US terms) into municipal plans. To reduce stormwater from rooftops, flows from eaves troughs (rain gutters and downspouts) may be infiltrated into adjacent soil, rather than discharged into the storm sewer system. Storm water runoff from paved surfaces can be directed to unlined ditches (sometimes called swales or bioswales) before flowing into the storm sewers, again to allow the runoff to soak into the ground. Permeable paving materials can be used in building sidewalks, driveways and in some cases, parking lots, to infiltrate a portion of the stormwater volume.\n\nMany areas require that properties have detention tanks that temporarily hold rainwater runoff, and restrict the outlet flow to the public sewer. This lessens the risk of overburdening the public sewer during heavy rain. An overflow outlet may also connect higher on the outlet side of the detention tank. This overflow prevents the detention tank from completely filling. Restricting water flow and temporarily holding the water in a detention tank public this way makes it far less likely for rain to overwhelm the sewers.\n\nThe \"first flush\" from urban runoff can be extremely dirty. Storm water may become contaminated while running down the road or other impervious surface, or from lawn chemical run-off, before entering the drain.\n\nWater running off these impervious surfaces tends to pick up gasoline, motor oil, heavy metals, trash and other pollutants from roadways and parking lots, as well as fertilizers and pesticides from lawns. Roads and parking lots are major sources of nickel, copper, zinc, cadmium, lead and polycyclic aromatic hydrocarbons (PAHs), which are created as combustion byproducts of gasoline and other fossil fuels. Roof runoff contributes high levels of synthetic organic compounds and zinc (from galvanized gutters). Fertilizer use on residential lawns, parks and golf courses is a significant source of nitrates and phosphorus.\n\nSeparation of undesired runoff can be achieved by installing devices within the storm sewer system. These devices are relatively new and can only be installed with new development or during major upgrades. They are referred to as oil-grit separators (OGS) or oil-sediment separators (OSS). They consist of a specialized manhole chamber, and use the water flow and/or gravity to separate oil and grit.\n\nCatch basins are commonly designed with a sump area below the outlet pipe level—a reservoir for water and debris that helps prevent the pipe from clogging. Unless constructed with permeable bottoms to let water infiltrate into underlying soil, this subterranean basin can become a mosquito breeding area, because it is cool, dark, and retains stagnant water for a long time. Combined with standard grates, which have holes large enough for mosquitoes to enter and leave the basin, this is a major problem in mosquito control.\n\nBasins can be filled with concrete up to the pipe level to prevent this reservoir from forming. Without proper maintenance, the functionality of the basin is questionable, as these catch basins are most commonly not cleaned annually as is needed to make them perform as designed. The trapping of debris serves no purpose because once filled they operate as if no basins were present, but continue to allow a shallow area of water retention for the breeding of mosquito. Moreover, even if cleaned and maintained, the water reservoir remains filled, accommodating the breeding of mosquitoes.\n\nStorm drains are separate and distinct from sanitary sewer systems. The separation of storm sewers from sanitary sewers helps prevent sewage treatment plants becoming overwhelmed by infiltration/inflow during a rainstorm, which could discharge untreated sewage into the environment.\n\nMany storm drainage systems drain untreated storm water into rivers or streams. Many local governments conduct public awareness campaigns about this, lest people dump waste into the storm drain system. In the city of Cleveland, Ohio, for example, all new catch basins installed have inscriptions on them not to dump any waste, and usually include a fish imprint as well. Trout Unlimited Canada recommends that a yellow fish symbol be painted next to existing storm drains.\n\nCities that installed their sewage collection systems before the 1930s typically used single piping systems to transport both urban runoff and sewage. This type of collection system is referred to as a \"combined sewer system\" (CSS). The cities' rationale when combined sewers were built was that it would be cheaper to build just a single system. In these systems a sudden large rainfall that exceeds sewage treatment capacity is allowed to overflow directly from storm drains into receiving waters via structures called \"combined sewer overflows\".\n\nStorm drains are typically at shallower depths than combined sewers; because, while storm drains are designed to accept surface runoff from streets, combined sewers were designed to also accept sewage flows from buildings with basements.\n\nNew York City, Washington DC, Seattle and other cities with combined systems have this problem due to a large influx of storm water after every heavy rain. Some cities have dealt with this by adding large storage tanks or ponds to hold the water until it can be treated. Chicago has a system of tunnels, collectively called the Deep Tunnel, underneath the city for storing its stormwater. Many areas require detention tanks or roof detention systems that temporarily hold runoff in heavy rains and restrict outlet flow to the public sewer. This lessens the risk of overwhelming the public sewer in heavy rain. An overflow outlet may also connect higher on the outlet side of the detention tank. This overflow prevents the detention tank from completely filling. By restricting the flow of water in this way and temporarily holding the water in a detention tank or by roof detention public sewers are less likely to overflow.\n\nBuilding codes and local government ordinances vary greatly on the handling of storm drain runoff. New developments might be required to construct their own storm drain processing capacity for returning the runoff to the water table and bioswales may be required in sensitive ecological areas to protect the watershed.\n\nIn the United States, cities, suburban communities and towns with over 10,000 population are required to obtain discharge permits for their storm sewer systems, under the Clean Water Act. The Environmental Protection Agency (EPA) issued stormwater regulations for large cities in 1990 and for other communities in 1999. The permits require local governments to operate stormwater management programs, covering both construction of new buildings and facilities, and maintenance of their existing municipal drainage networks. Many municipalities have revised their local ordinances covering management of runoff. State government facilities, such as roads and highways, are also subject to the stormwater management regulations. Many local municipalities have commercial and residential stormwater management ordinances that require builders to design and implement an approved system.\n\nSoutheastern Los Angeles County installed thousands of stainless steel, full-capture trash devices on their road drains in 2011.\n\nAn international subculture has grown up around the exploration of stormwater drains. Societies such as the Cave Clan regularly explore the drains underneath cities. This is commonly known as \"urban exploration\", but is also known as \"draining\" when in specific relation to storm drains.\n\nIn several large American cities, homeless people live in storm drains. At least 300 people live in the 200 miles of underground storm drains of Las Vegas, many of them making a living finding unclaimed winnings in the gambling machines. An organization called Shine a Light was founded in 2009 to help the drain residents after over 20 drowning deaths occurred in the preceding years. A man in San Diego was evicted from a storm drain after living there for nine months in 1986.\n\nArchaeological studies have revealed use of rather sophisticated stormwater runoff systems in ancient cultures. For example, in Minoan Crete approximately 4000 years before present, cities such as Phaistos were designed to have storm drains and channels to collect precipitation runoff. At Cretan Knossos, storm drains include stone-lined structures large enough for a person to crawl through. Other examples of early civilizations with elements of stormwater drain systems include early people of Mainland Orkney such as Gurness and the Brough of Birsay in Scotland.\n\n\n"}
{"id": "996889", "url": "https://en.wikipedia.org/wiki?curid=996889", "title": "Thyristor drive", "text": "Thyristor drive\n\nA thyristor drive is a motor drive circuit where AC supply current is regulated by a thyristor phase control to provide variable voltage to a DC motor.\n\nThyristor drives are very simple and were first introduced in the 1960s. They remained the predominant type of industrial motor controller until the end of the 1980s when the availability of low cost electronics led to their replacement by chopper drives for high performance systems and inverters for high reliability with AC motors. They are still employed in very high power applications, such as locomotives, where the high power capability of the thyristors and the simplicity of the design can make them a more attractive proposition than transistor based controllers.\n\nA derivative of the thyristor drive is the simple AC phase controller. This uses a single phase controlled triac to provide a variable voltage AC output for regulating a universal motor. This is the type of motor speed control most commonly used in domestic appliances, such as food mixers, and small AC powered tools, such as electric drills.\n"}
{"id": "1269245", "url": "https://en.wikipedia.org/wiki?curid=1269245", "title": "Traditional engineering", "text": "Traditional engineering\n\nTraditional engineering, also known as sequential engineering, is the process of marketing, engineering design, manufacturing, testing and production where each stage of the development process is carried out separately, and the next stage cannot start until the previous stage is finished. Therefore, the information flow is only in one direction, and it is not until the end of the chain that errors, changes and corrections can be relayed to the start of the sequence, causing estimated costs to be under predicted.\n\nThis can cause many problems; such as time consumption due to many modifications being made as each stage does not take into account the next. This method is hardly used today, as the concept of concurrent engineering is more efficient.\n\nTraditional engineering is also known as over the wall engineering as each stage blindly throws the development to the next stage over the wall.\n\nTraditional manufacturing has been driven by sales forecasts that companies need to produce and stockpile inventory to support. Lean manufacturing is based on the concept that production should be driven by the actual customer demands and requirements. Instead of pushing product to the marketplace, it is pulled through by the customers' actual needs.\n\n\n\n"}
{"id": "37732144", "url": "https://en.wikipedia.org/wiki?curid=37732144", "title": "Trichromy", "text": "Trichromy\n\nTrichromy is the colour theory by which any colour can be reproduced solely combining the three primary colours. It relies on human trichromacy.\n\nIt is also referred to the three colour process in photography. French histories of photography have claimed that Charles Cros and Ducos du Hauron simultaneously invented its application to photography around 1868, though English histories of photography have claimed that it was first suggested by J. C. Maxwell and defectively demonstrated with the help of by Thomas Sutton in 1861 (according to Maxwell himself, Sutton's widespread photochemistry wasn't sensitive enough to red and green light).\n"}
{"id": "3962212", "url": "https://en.wikipedia.org/wiki?curid=3962212", "title": "Wall's (ice cream)", "text": "Wall's (ice cream)\n\nWall's is an ice cream brand owned by the Anglo-Dutch food and personal care conglomerate Unilever. Originating as an independent food brand in the United Kingdom, Wall's is now part of the Heartbrand global frozen dessert subsidiary of Unilever, used in Australia, China, Hong Kong, Europe, Indonesia, India, Japan, Malaysia, Mauritius, New Zealand, Philippines, Qatar, Saudi Arabia, Pakistan, Singapore, Thailand, United Arab Emirates and Vietnam. Unilever also uses a merged brand called Kwality Wall's in South Asia.\n\nWall's was founded in 1786 by Richard Wall, when he opened a butcher's stall in St James’s Market, London. In the 1900s the business was led by Richard's grandson Thomas Wall II. Every year the company had to lay off staff in the summer as demand for its sausages, pies and meat fell, so in 1913 Thomas Wall II conceived the idea of making ice cream in the summer to avoid those lay-offs; the First World War meant that his idea was not implemented until 1922. Following his retirement in 1920, Thomas Wall II created his Trust for the “encouragement and assistance of educational work and social service”. Today, the Trust continues to assist in these areas by providing grants to individuals and organisations. \n\nBy 1922 the business had been jointly bought by Lever Brothers and Margarine Unie. Maxwell Holt was put in charge and he revived the idea of producing ice cream, with near instant success. Ice cream production commenced in 1922 at a factory in Acton, London. In 1959, Wall's doubled capacity by opening a purpose built ice cream factory in Gloucester, England.\n\nThere is a garage on the corner of Aultone Way and Angel Hill in Benhilton, Sutton, London, built in about 1913 and still in use today, which was originally used for the storing of the 'Stop Me and Buy One' bicycles of Thomas Wall's business.\n\nUnilever continues to use the brand for ice cream in the UK and it has become part of the company's international Heartbrand strategy, where it retains its name but shares a logo and most of the product lineup with the other Heartbrand companies. Whilst remaining (2006) the market leader in the UK for individual hand-held products such as Cornetto and Magnum, and value-added multi-portion products designed to be eaten at home, such as Viennetta, the Wall's brand faces severe competition from the major supermarket brands and to a lesser extent from Nestlé (absorbing the Rowntree's and Lyons Maid brands) and Mars spin-off ice cream products.\n\nIn 2013 Wall's expanded into the UK confectionery market following a licensing deal with Kinnerton Confectionery, leading to the introduction of ambient chocolate bar variations for the Magnum, Cornetto and Mini Milk ice cream brands.\n\n"}
{"id": "31284549", "url": "https://en.wikipedia.org/wiki?curid=31284549", "title": "Whole Building Design Guide", "text": "Whole Building Design Guide\n\nThe Whole Building Design Guide or WBDG is described by the Federal Energy Management Program as \"a complete internet resource to a wide range of building-related design guidance, criteria and technology\", and meets the requirements in guidance documents for Executive Order 13123. The WBDG is based on the premise that to create a successful high-performance building, one must apply an integrated design and team approach in all phases of a project, including planning, design, construction, operations and maintenance. The WBDG is managed by the National Institute of Building Sciences.\n\nThe WBDG was initially designed to serve U.S. Department of Defense (DOD) construction programs. A 2003 DOD memorandum named WBDG the “sole portal to design and construction criteria produced by the U.S. Army Corps of Engineers (USACE), Naval Facilities Engineering Command (NAVFAC), and U.S. Air Force.” Since then, WBDG has expanded to serve all building industry professionals. The majority of its 500,000 monthly users are from the private sector.\nThe WBDG draws information from the Construction Criteria Base and a privately owned database run by Information Handling Services.\n\nA significant amount of the Whole Building Design Guide content is organized by three categories: Design Guidance, Project Management, and Operations and Maintenance. It is structured to provide WBDG visitors first a broad understanding then increasingly specific information more targeted towards building industry professionals. The WBDG is the resource that federal agencies look to for policy and technical guidance on Federal High Performance and Sustainable Buildings In addition, the WBDG contains online tools, the original Construction Criteria Base, Building Information Modeling guides and libraries, a database of select case studies, federal mandates and other resources. The WBDG also provides over 70 online continuing education courses for architects and other building professionals, free of charge.\n\nDevelopment of the WBDG is a collaborative effort among federal agencies, private sector companies, non-profit organizations and educational institutions.\nThe WBDG web site maintained by the National Institute of Building Sciences through funding support from the DOD, the NAVFAC Engineering Innovation and Criteria Office, U.S. Army Corps of Engineers, the U.S. Air Force, the U.S. General Services Administration (GSA), the U.S. Department of Veterans Affairs, the National Aeronautics and Space Administration (NASA), and the U.S. Department of Energy (DOE), and the assistance of the Sustainable Buildings Industry Council (SBIC). A Board of Direction and an Advisory Committee consisting of representatives from over 25 participating federal agencies guide the development of the WBDG.\n\n"}
{"id": "755268", "url": "https://en.wikipedia.org/wiki?curid=755268", "title": "Working fluid", "text": "Working fluid\n\nA working fluid is a pressurized gas or liquid that actuates a machine or heat engine. Examples include steam in a steam engine, air in a hot air engine and hydraulic fluid in a hydraulic motor or hydraulic cylinder. More generally, in a thermodynamic system, the working fluid is a liquid or gas that absorbs or transmits energy.\n\nThe working fluid properties are essential for the full description of thermodynamic systems. Although working fluids have a very large number of physical properties which can be defined, the thermodynamic properties which are often required in engineering design and analysis are few. Pressure, temperature, enthalpy, entropy, specific volume and internal energy are the most common.\n\nIf at least two thermodynamic properties are known, the state of the working fluid can be defined. This is usually done on a property diagram which is simply a plot of one property versus another.\n\nWhen the working fluid passes through engineering components such as turbines and compressors, the point on a property diagram moves due to the possible changes of certain properties. In theory therefore it is possible to draw a line/curve which fully describes the thermodynamic properties of the fluid. In reality however this can only be done if the process is reversible. If not, the changes in property are represented as a dotted line on a property diagram. This issue does not really affect thermodynamic analysis since in most cases it is the end states of a process which are sought after.\n\nThe working fluid can be used to output useful work if used in a turbine. Also, in thermodynamic cycles energy may be input to the working fluid by means of a compressor. The mathematical formulation for this may be quite simple if we consider a cylinder in which a working fluid resides. A piston is used to input useful work to the fluid. From mechanics, the work done from state 1 to state 2 of the process is given by\n\nformula_1\n\nWhere \"ds\" is the incremental distance from one state to the next and \"F\" is the force applied. The negative sign is introduced since in this case a decrease in volume is being considered. The situation is shown in the figure which follows.\n\nThe force is given by the product of the pressure in the cylinder and its cross sectional area such that\nformula_2\nformula_3\nWhere \"A.ds = dV\" is the elemental change of cylinder volume. If from state 1 to 2 the volume increases then the working fluid actually does work on its surroundings and this is commonly denoted by a negative work. If the volume decreases the work is positive. By the definition given with the above integral the work done is represented by the area under a pressure - volume diagram. If we consider the case where we have a constant pressure process then the work is simply given by\nformula_4\nformula_5\n\nDepending on the application, various types of working fluids are used. In a thermodynamic cycle it may be the case that the working fluid changes state from gas to liquid or vice versa. Certain gases such as Helium can be treated as ideal gases. This is not generally the case for superheated steam and the ideal gas equation does not really hold. At much higher temperatures however it still yields relatively accurate results. The physical and chemical properties of the working fluid are extremely important when designing thermodynamic systems. For instance, in a refrigeration unit, the working fluid is called the refrigerant. Ammonia is a typical refrigerant and may be used as the primary working fluid. Compared with water (which can also be used as a refrigerant), ammonia makes use of relatively high pressures requiring more robust and expensive equipment.\n\nIn air standard cycles as in gas turbine cycles, the working fluid is air. In the open cycle gas turbine, air enters a compressor where its pressure is increased. The compressor therefore inputs work to the working fluid (positive work). The fluid is then transferred to a combustion chamber where this time heat energy is input by means of the burning of a fuel. The air then expands in a turbine thus doing work against the surroundings (negative work).\n\nDifferent working fluids have different properties and in choosing one in particular the designer must identify the major requirements. In refrigeration units, high latent heats are required to provide large refrigeration capacities.\n\nThe following table gives typical applications of working fluids and examples for each:\n\n\n"}
{"id": "1577411", "url": "https://en.wikipedia.org/wiki?curid=1577411", "title": "Ștefan Odobleja", "text": "Ștefan Odobleja\n\nŞtefan Odobleja (; 13 October 1902 – 4 September 1978) was a Romanian scientist considered to be one of the precursors of cybernetics. His major work, \"Psychologie consonantiste\" (first published in 1938 and 1939, in Paris) helped generated many of the major themes of cybernetics regarding cybernetics and systems thinking nine years before Norbert Wiener.\n\nŞtefan Odobleja was born into a family of peasants in 1902, in Valea Izvorului (now Ştefan Odobleja), Mehedinţi County, Romania. He attended the Faculty of Medicine in Bucharest and became a physician. He practiced medicine as a military doctor in Bucharest, Dej, Drobeta Turnu-Severin, Lugoj, Târgovişte and other Romanian cities. Endowed with an uncommon capacity of work and with an astonishing inventive spirit, Odobleja left an impressive work to the posterity. His completed works run to over 50,000 pages.\n\nIn 1936, Odobleja publishes “Phonoscopy and the clinical semiotics”. In 1937, he participates in the IXth International Congress of Military Medicine with a paper entitled “Demonstration de phonoscopie”, where he disseminates a prospectus in French, announcing the appearance of his future work “The Consonantist Psychology”.\n\nThe most important of his writings is \"Psychologie consonantiste\", in which Odobleja lays the theoretical foundations of the generalized cybernetics. The book, published in Paris by \"Librairie Maloine\" (vol. I in 1938 and vol. II in 1939), contains almost 900 pages and includes 300 figures in the text.\nThe author wrote at the time that \"this book is... a table of contents, an index or a dictionary of psychology, [for] a ... great Treatise of Psychology that should contain 20–30 volumes\".\n\nDue to the beginning of World War II, the publication went unnoticed. The first Romanian edition of this work did not appear until 1982 (the first edition was published in French).\n\nThe Romanian communist government had little understanding of the subject and even declared cybernetics a science of capitalist nature. Odobleja, being its main advocate, was put under surveillance and home arrest.\n\nŞtefan Odobleja died on September 4, 1978. His son put the epitaph “Father of Cybernetics” on his tomb stone, much to the Romanian government’s chagrin.\n\nEuropean recognition of his contribution to the foundations of cybernetics took place when his paper, \"Diversity and Unit in Cybernetics\" (presented at the Fourth Congress of Cybernetics and Systems in Amsterdam, August, 1978), was received with great acclaim.\n\nAs an appreciation for his work of mapping the unknown territory of the consonantist psychology, cybernetics and generalized cybernetics, Ştefan Odobleja was elected posthumously an honorary member of the Romanian Academy (1990).\n\nIn 1982 a group of scientists established the \"Cybernetics Academy \"Ştefan Odobleja\", a scientific forum registered in Lugano, Switzerland, financed by the Romanian billionaire Prof. Dr. Iosif Constantin Drăgan.\n\n\n"}
