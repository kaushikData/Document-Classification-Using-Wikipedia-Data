{"id": "29871283", "url": "https://en.wikipedia.org/wiki?curid=29871283", "title": "111 Eighth Avenue", "text": "111 Eighth Avenue\n\n111 Eighth Avenue, in New York City, is a full-block Art Deco multi-use building located between Eighth and Ninth Avenues, and 15th and 16th Streets in the Chelsea neighborhood of the Manhattan borough of New York City.\n\nAt , it is currently the city's fourth largest building in terms of floor area . It was the largest building until 1963 when the MetLife Building opened. The World Trade Center (which opened in 1970–71) and 55 Water Street , which opened in 1972, were also larger but the World Trade Center was destroyed in 2001. When the One World Trade Center opened in 2014, 111 became the city's fourth largest building.\n\nThe building, which has been owned by Google since 2010, is one of the largest technology-owned office buildings in the world. It is also larger than Apple Park, Apple's headquarters in Cupertino, California.\n\nThe building was designed by Lusby Simpson of Abbott, Merkt & Co. and completed in 1932. The building had a multipurpose design when it opened in 1932 with the first floor and basement designated as \"Union Inland Terminal #1\" which was to be used to transport goods by truck to and from railroad lines and/or shipping piers. The second floor was the Commerce section designed for exhibitions and the upper floors were designed for manufacturing. The Union Inland Terminal was built by the Port Authority to be a warehouse/union station to handle less than carload (LCL) shipments, consolidating the shipping functions of the Hudson River piers two blocks west of the building, the eight trunk railroads that operated a block west of the building and truck operations (an inland terminal by definition is a warehouse that is not immediately next to railroad lines/piers but is nearby and is used to relieve congestion at the transfer points).\n\nAt its peak in the 1930s the Port Authority said it was handling more than half of the LCL freight operations south of 59th Street in Manhattan with more than 8,000 tons of goods passing through it each month. On one day alone in 1937 it was reported that 650 trucks had used the facility. The railroads involved were the Pennsylvania Railroad; Lehigh Valley Railroad; Baltimore & Ohio; Erie Railroad; Delaware, Lackawanna & Western Railroad; Central Railroad of New Jersey; New York Central Railroad; New York, New Haven & Hartford. The building included four freight elevators that could carry fully loaded 20 ton trucks. Because of the warehouse mission of the building it was able to avoid some of the setback rules that greatly reduced the buildable space available for the skyscrapers that mark the Manhattan skyline. As a result, the 15-story building with its football field size floors has more available rentable square footage than the 102-story Empire State Building which has 2.2 million square feet. The construction occurred at a time of a massive projects built to deal with what at the time was street level freight railroad traffic on Manhattan's west side. Other projects in the neighborhood in the era included construction of the High Line and Starrett-Lehigh Building.\n\nThe initial plan had called for more inland freight operations to be erected in Manhattan. However the freight transportation changed and freight railroads played a dwindling importance in Manhattan. By 1947 its initial mission was pretty much abandoned and it became the Port Authority Building which remained the Port Authority's headquarters until the opening of the World Trade Center in 1970.\n\nThe massive building served a dwindling warehouse/backoffice outpost until 1998 when Taconic Investment Partners acquired it.\n\nTaconic began marketing it as a location to be used as a carrier hotel for the new booming internet business. This was coupled with the fashionable rise of the Chelsea neighborhood that surrounded it.\n\nIn 2010, Google, which had previously leased space in the building, contracted to purchase the entire building, in a deal reported to be worth around $1.9 billion.\n\nThe building's meet-me room in its carrier hotel was once the biggest in the city (the other big meet-me room is at 60 Hudson Street).\n\n111 8th Avenue is adjacent to trunk dark fiber lines stretching from Hudson Street and continuing up Ninth Avenue. That line at the time was owned by Lexent Metro Connect. There was speculation at the time of the acquisition that Google would use its strategic location to launch a Google Fiber operation in New York City. The Google Fiber plan never came to pass and Google has denied it has any plans to bring it to New York City anytime in the near future, although in 2013 it did begin offering free Wi-Fi to its Chelsea neighbors. The Lexent dark fiber line has been acquired by Lightower Fiber Networks.\n\nIn 2013 the first class of the newly created Cornell NYC Tech school began classes in the building, in space donated by Google. Classes continued in the building until the school moved to its new location on Roosevelt Island in 2017. Despite the massive size of the acquisition, Google has still found itself having to rent space elsewhere because it has been unable to break the leases with some of its tenants, including Nike, Deutsch Inc., and Bank of New York. After years of renting additional space across the street in the Chelsea Market, Google purchased that building in 2018.\n\n\n"}
{"id": "20695220", "url": "https://en.wikipedia.org/wiki?curid=20695220", "title": "AMOLED", "text": "AMOLED\n\nAMOLED (active-matrix organic light-emitting diode, ) is a display device technology used in smartwatches, mobile devices, laptops, and televisions. OLED describes a specific type of thin-film-display technology in which organic compounds form the electroluminescent material, and active matrix refers to the technology behind the addressing of pixels.\n\n, AMOLED technology was used in mobile phones, media players and digital cameras and continued to make progress toward low-power, low-cost and large-size (for example, 40-inch or 100-centimeter) applications.\n\nAn AMOLED display consists of an active matrix of OLED pixels generating light (luminescence) upon electrical activation that have been deposited or integrated onto a thin-film transistor (TFT) array, which functions as a series of switches to control the current flowing to each individual pixel.\n\nTypically, this continuous current flow is controlled by at least two TFTs at each pixel (to trigger the luminescence), with one TFT to start and stop the charging of a storage capacitor and the second to provide a voltage source at the level needed to create a constant current to the pixel, thereby eliminating the need for the very high currents required for passive-matrix OLED operation.\n\nTFT backplane technology is crucial in the fabrication of AMOLED displays. In AMOLEDs, the two primary TFT backplane technologies, polycrystalline silicon (poly-Si) and amorphous silicon (a-Si), are currently used offering the potential for directly fabricating the active-matrix backplanes at low temperatures (below 150 °C) onto flexible plastic substrates for producing flexible AMOLED displays.\n\nRed and green OLED films have longer lifespans compared to blue OLED films. This variation results in color shifts as a particular pixel fades faster than the other pixels. This is amplified with the effect of pixel burn-in, and that blue pixels are used very frequently.\n\nAMOLED displays are prone to screen burn-in, which leaves a permanent imprint of overused colors represented by overused images. This usually only happens if the screen is used at maximum brightness the majority of the time for a long time.\n\nManufacturers have developed in-cell touch panels, integrating the production of capacitive sensor arrays in the AMOLED module fabrication process. In-cell sensor AMOLED fabricators include AU Optronics and Samsung. Samsung has marketed its version of this technology as \"Super AMOLED\". Researchers at DuPont used computational fluid dynamics (CFD) software to optimize coating processes for a new solution-coated AMOLED display technology that is competitive in cost and performance with existing chemical vapor deposition (CVD) technology. Using custom modeling and analytic approaches, Samsung has developed short and long-range film-thickness control and uniformity that is commercially viable at large glass sizes.\n\nAMOLED displays provide higher refresh rates than passive-matrix, often reducing the response time to less than a millisecond, and they consume significantly less power. This advantage makes active-matrix OLEDs well-suited for portable electronics, where power consumption is critical to battery life.\n\nThe amount of power the display consumes varies significantly depending on the color and brightness shown. As an example, one commercial QVGA OLED display consumes 0.3 watts while showing white text on a black background, but more than 0.7 watts showing black text on a white background, while an LCD may consume only a constant 0.35 watts regardless of what is being shown on screen. Because the black pixels turn completely off, AMOLED also has contrast ratios that are significantly higher than LCD.\n\nAMOLED displays may be difficult to view in direct sunlight compared with LCDs because of their reduced maximum brightness. Samsung's \"Super AMOLED\" technology addresses this issue by reducing the size of gaps between layers of the screen. Additionally, PenTile technology is often used for a higher resolution display while requiring fewer subpixels than needed otherwise, sometimes resulting in a display less sharp and more grainy than a non-PenTile display with the same resolution.\n\nThe organic materials used in AMOLED displays are very prone to degradation over a relatively short period of time, resulting in color shifts as one color fades faster than another, image persistence, or burn-in.\n\nAs of 2010, demand for AMOLED screens was high and, due to supply shortages of the Samsung-produced displays, certain models of HTC smartphones were changed to use next-generation LCD displays from the Samsung-Sony joint-venture SLCD in the future.\n\nFlagship smartphones sold as of December 2011 used either Super AMOLED or IPS panel premium LCD. Super AMOLED displays, such as the one on the Galaxy Nexus and Samsung Galaxy S III have often been compared to IPS panel premium LCDs, found in the iPhone 4S, HTC One X, and Nexus 4. For example, according to ABI Research the AMOLED display found in the Motorola Moto X draws just 92 mA during bright conditions and 68 mA while dim. On the other hand, compared with the IPS, the yield rate of AMOLED is low; the cost is also higher.\n\n\"Super AMOLED\" is a marketing term created by device manufacturers for an AMOLED display with an integrated digitizer: the layer that detects touch is integrated into the screen, rather than overlaid on top of it. The display technology itself is not improved. According to Samsung, Super AMOLED reflects one-fifth as much sunlight as the first generation AMOLED. Super AMOLED is part of the Pentile matrix family, sometimes abbreviated as SAMOLED. For the Samsung Galaxy S III, which reverted to Super AMOLED instead of the pixelation-free conventional RGB (non-PenTile) Super AMOLED Plus of its predecessor Samsung Galaxy S II, the S III's larger screen size encourages users to hold the phone further from their face to obscure the PenTile effect.\n\nSuper AMOLED Advanced is a term marketed by Motorola to describe a brighter display than Super AMOLED screens, but also a higher resolution — qHD or 960×540 for Super AMOLED Advanced than WVGA or 800×480 for Super AMOLED and 25% more energy efficient. Super AMOLED Advanced features PenTile, which sharpens subpixels in between pixels to make a higher resolution display, but by doing this, some picture quality is lost. This display type is used on the Motorola Droid RAZR and HTC One S.\n\nSuper AMOLED Plus, first introduced with the Samsung Galaxy S II and Samsung Droid Charge smartphones, is a branding from Samsung where the PenTile RGBG pixel matrix (2 subpixels) used in Super AMOLED displays has been replaced with a traditional RGB RGB (3 subpixels) arrangement typically used in LCDs. This variant of AMOLED is brighter and therefore more energy efficient than Super AMOLED displays and produces a sharper, less grainy image because of the increased number of subpixels. In comparison to AMOLED and Super AMOLED displays, they are even more energy efficient and brighter. However, Samsung cited screen life and costs by not using Plus on the Galaxy S II's successor, the Samsung Galaxy S III.\n\nHD Super AMOLED is a branding from Samsung for an HD-resolution (above 1280×720) Super AMOLED display. The first device to use it was the Samsung Galaxy Note. The Galaxy Nexus and the Galaxy S III both implement the HD Super AMOLED with a PenTile RGBG-matrix (2 subpixels/pixel), while the Galaxy Note II uses an RBG matrix (3 subpixels/pixel) but not in the standard 3 stripe arrangement.\n\nA variant of the Samsung Galaxy S3 using Tizen OS 1 was benchmarked using a non-pentile HD Super AMOLED Plus screen in 2012.\n\nAs featured on the Samsung Galaxy S4, Samsung Galaxy S5, and Samsung Galaxy Note 3, this display has a 1920×1080 resolution that is better known as 1080p. It has a color gamut of up to 97% of the Adobe RGB color space.\n\nQuad HD Super AMOLED technology was first used by AU Optronics in April 2014. After AU Optronics released their phone which used a Quad HD Super AMOLED screen, other companies such as Samsung released phones utilizing the technology such as the Samsung Galaxy Note 4 and Samsung Galaxy Note 5 Broadband LTE-A and Samsung Galaxy S6 and S7.\n\nFuture displays exhibited from 2011 to 2013 by Samsung have shown flexible, 3D, unbreakable, transparent Super AMOLED Plus displays using very high resolutions and in varying sizes for phones. These unreleased prototypes use a polymer as a substrate removing the need for glass cover, a metal backing, and touch matrix, combining them into one integrated layer.\n\nSo far, Samsung plans on branding the newer displays as Youm, or y-octa\n\nAlso planned for the future are 3D stereoscopic displays that use eye tracking (via stereoscopic front-facing cameras) to provide full resolution 3D visuals.\n\nBelow is a mapping table of marketing terms versus resolutions and sub-pixel types. Note how the pixel density relates to choices of sub-pixel type.\n\nCommercial devices using AMOLED include:\n"}
{"id": "26061344", "url": "https://en.wikipedia.org/wiki?curid=26061344", "title": "Aircruise", "text": "Aircruise\n\nAircruise is a concept hydrogen airship envisioned as the combination of cruise ship and luxury hotel, designed by the UK company Seymourpowell. Its design director is Nick Talbot. It has attracted the attention of Samsung Construction and Trading, for whom a concept video was produced. It was later revealed that the concept was a publicity stunt by Seymourpowell, and nothing like this concept could ever work.\n\nThe Aircruise would be a solar and hydrogen fuel cell-powered airship. According to its design specifications, it would be tall containing of air and would carry a payload of . It was \"designed\" to carry about 100 passengers with a flight crew of 6, 2 of whom are flight engineers, and another 14 supporting staff to look after passengers, for a total of 20 crew.\n\nIt would have glass floors for passengers to view the land and sea beneath.\n\nIts land speed would depend on the wind, but it would supposedly be capable of traveling at , in calm conditions, sufficient to travel from London to New York in 37 hours and from Los Angeles to Hong Kong or Shanghai in 4 days.\n\nIts maximum altitude was , and it was \"specified\" that it could fly at an altitude of about 300 feet for sightseeing.\n\nThe Aircruise concept received generally favourable criticism, much to the embarrassment of reporters when the hoax was revealed. For example, \"The Daily Telegraph\" stated that it could herald a new era of luxury travel. Coverage in \"The Scotsman\" welcomed it as attractive for its environmental friendliness.\n\nOrdinary cruise ships release 3 times as much greenhouse gas as a Boeing 747 per passenger and 36 times as much as the Eurostar. The Aircruise was thus claimed to be an eco friendly alternative, with its zero emission design.\n"}
{"id": "24791227", "url": "https://en.wikipedia.org/wiki?curid=24791227", "title": "Alberta Regional Network", "text": "Alberta Regional Network\n\nThe Alberta Regional Network also known as the ARN is an electronic networking organization in Alberta that links Alberta Credit Unions with ATB Financial. \n\nCustomers of ARN members can pay bills, inquire accounts, and deposit money with any other member institution without having to pay fees. \n"}
{"id": "52403868", "url": "https://en.wikipedia.org/wiki?curid=52403868", "title": "Arnold Ferdinand Arnold", "text": "Arnold Ferdinand Arnold\n\nArnold Ferdinand Arnold (February 6, 1921 - January 20, 2012) was an author, game designer and cyberneticist, known more for the fame of his relatives and wives in later life. His first and only legal wife, Eve Arnold, was known for photography. His second partner, who he never married, was writer Gail E. Haley. Arnold's two brothers-in-law were Theodor Gaster and Peter Drucker.\n\nArnold followed his eldest sister to the United States where he gained work as a writer and cartoonist. He was drafted into the U.S. military in 1941, and after training in South Carolina, was sent to France as a member of the 101st Infantry division. Badly wounded after his jeep ran over a German landmine, he returned to New York where he settled down to married life with Eve.\n\nBy the 1950s, Arnold was well established in the New York literary world. He taught at the New School, had a one-man show at MOMA, and published his first book with acquaintance, Ian Ballantine. The success of the book, \"How To Play With Your Child\", which sold over 100,000 initial copies, established Arnold as an author, and allowed the family to buy a house in Long Island Sound.\n\nArnold was also a successful and well known advertising and commercial designer, and created the famous Parker Brothers swirl logo, first used in 1964. He created and designed many innovative educational and teaching games for leading game designers through the 1960s. He also designed classical record covers for EPIC Records during the 1950s.\n\nIt was during this time that he also began to help his wife, Eve, establish herself as a photographer. They had one son. In 1961, they sold their house in Long Island, and Eve moved with their son to England. By 1965, however, Arnold had not followed the family. Instead he met Gail E. Haley, then married to Joseph Haley and fell in love with her. She divorced her husband, but Eve refused to divorce Arnold.\n\nArnold and Gail had two children. Arnold had by this time established himself as a national columnist with The Chicago Tribune with a weekly column on childrearing called \"Parents and Their Children.\"\n\nIn 1973, Arnold, Gail and their two children moved to England. Arnold was increasingly distraught over the direction of U.S. politics and publicly spoke out and wrote against Richard Nixon, which resulted in his being placed on Nixon's \"enemies list.\"\n\nDuring the 1980s and 90's, Arnold published several books, but never again had a financially successful career. He moved back to Petersfield in 1998, where his health rapidly declined. He died in 2012, from complications of sepsis and pneumonia.\n"}
{"id": "17737696", "url": "https://en.wikipedia.org/wiki?curid=17737696", "title": "Aspex", "text": "Aspex\n\nAspex Corporation, founded in 1992, is a supplier of electron microscopy tools to researchers, developers and manufacturers working on Process control through automated Scanning Electron Microscope and Energy-dispersive X-ray spectroscopy.\n\nASPEX specializes in the manufacturing integrated Energy-dispersive X-ray spectroscopy and electron microscope. The company develops automated algorithms for routine production monitoring and control. Aspex's microanalysis solutions are seen in a wide range of production environments, including critical cleanliness, microcontamination analysis, product purity, contamination diagnostics, predictive maintenance, and other statistical process control initiatives. Aspex instruments are typically more industrialized then standard SEM equipment. You will typically find them installed in non-traditional locations, like steel mills and automotive assembly lines. The most common usage revolves around automated feature analysis for particle characterization.\n\n\n\nAspex Corporation maintains research and development and sales operations in Delmont, Pennsylvania.\n\nThe origins of Aspex Corporation date back to 1992 (formally a division of RJ Lee Group, Inc.) where developers recognized the limitations and costs associated with traditional scanning electron microscopy (SEM). As a result, the company developed and introduced the world's first \"personalized\" SEM, or PSEM. The timing of the introduction coincided with a number of important trends, including increased emphasis on materials characterization, the outsourcing of R&D applications to service laboratories, and the \"quality revolution\" of the 1990s. Since then, Aspex has continued to develop various industrialized SEMs for quality and process control purposes.\n\nAs of January 9, 2012, FEI Company purchased ASPEX from their privately held venture capitalists. After one year of operating under the name \"ASPEX, an FEI Company\", the stand-alone entity of ASPEX was absorbed by FEI and the electron microscopes produced in the Delmont, Pennsylvania facility are now grouped with the FEI offerings - under the newly created \"ASPEX Product Group\".\n\nAlthough ASPEX is not operating as a stand-alone business entity, the three product lines (Express, Explorer and, Extreme) are still available for purchase with their automated software. This enables the user to engage the SEM (with EDX, if equipped) to automatically scan an area of interest about a sample and collect particulate information. Such information includes: Morphology, Thumbnail image, Chemistry (if EDX is equipped) \n\n"}
{"id": "2459800", "url": "https://en.wikipedia.org/wiki?curid=2459800", "title": "Atmospheric Reentry Demonstrator", "text": "Atmospheric Reentry Demonstrator\n\nThe Advanced Reentry Demonstrator (ARD) was a European Space Agency (ESA) suborbital reentry vehicle. It was developed and operated for experimental purposes, specifically to validate the multiple reentry technologies integrated upon it and the vehicle's overall design, as well as to gain greater insight into the various phenomenon encountered during reentry.\n\nThe ARD only performed a single spaceflight. On 21 October 1998, the vehicle was launched upon the third flight of the Ariane 5 expendable launch system. Reaching a recorded altitude of 830 km, the ARD was performed a guided reentry back to Earth before splashing down relatively close to its intended target point in the Pacific Ocean after one hour and 41 minutes of flight. Following its recovery and subsequent analysis, the vehicle was found to have performed well, the nose cone and heat shield thermal protection having remaining in an ideal state and having remained completely airtight and perfectly intact.\n\nThe ARD held the distinction of being the very first guided sub-orbital reentry vehicle to be manufactured, launched and recovered by Europe. One of the core purposes of the mission was the gathering of knowledge that could be subsequently used during the development of future re-entry vehicles and precise landing capabilities. In the aftermath of the programme, the ESA decided to embark on a follow-up reentry demonstrator, known as the Intermediate eXperimental Vehicle (IVX). The first IXV vehicle underwent its first successful test flight during February 2015. The ARD and IVX demonstrators are intended to serve as developmental stepping stones towards a series of production-standard spaceplanes.\n\nFrom the 1980s onwards, there was growing international interest in the development of reusable spacecraft; at this time, only the superpowers of the era, the Soviet Union and the United States, had developed this capability. European nations such as Britain and France embarked on their own national programmes to produce spaceplanes, such as HOTOL and Hermes, while attempting to attract the backing of the multinational European Space Agency (ESA). While these programmes ultimately did not garner enough support to continue development, there was still demand within a number of the ESA's member states to pursue the development of reusable space vehicles. Accordingly, shortly after the abandonment of the Hermes programme, it was decided to conduct a technology demonstrator programme with the aim of producing a vehicle which would support the development of subsequent reusable spacecraft. The ESA later referred to this programme, which became known as the Atmospheric Reentry Demonstrator (ARD), as being: \"major step towards developing and operating space transportation vehicles that can return to Earth... For the first time, Europe will fly a complete space mission – launching a vehicle into space and recovering it safely.\"\n\nThe ARD was developed and operated as a cooperative civilian space programme under the oversight of the ESA; it fell within the agency's Manned Space Transportation Program (MSTP) framework. Under this framework, the programme was pursued with a pair of expressed principal objectives. First, the ESA was keen to perform a demonstration of the ability of the European space industry to design and produce low-cost reentry vehicles, as well as its ability to handle the critical mission phases involved in their operation, such as sub-orbital flight, reentry and vehicle recovery. In addition, the ARD was equipped with a comprehensive suite of sensors and recording equipment so that detailed measurements were obtained during testing; it was recognised that exploration of various phenomena across the successive phases of flight would be of high value. The data gained would be subsequently catalogued and harnessed during further programmes, especially future reentry vehicles and reusable launch systems.\n\nThe prime contractor selected to perform the ARD's development and construction was French aerospace company Aérospatiale (which later merged into the multinational EADS – SPACE Transportation group). During 1995 and 1996, multiple development studies exploring concepts for the shape of such a vehicle were conducted; ultimately, it was decided to adopt a configuration that resembled the classical manned Apollo capsule which had been previously operated by NASA. The use of an existing shape was a deliberate measure to avoid a length exploration of the craft's aerodynamic properties; both the dimensions and mass of the craft were also defined by the capabilities of the Ariane 5 expendable launch system used to deploy the vehicle.\n\nIt has been claimed that even early on, the programme schedule was relatively tight and funding was limited. According to the ESA, the restrictive financing of the programme was an intentional effort, to prove that such a vehicle could be demonstrated with a smaller budget than previous efforts had been.\n\nThe ARD is an unmanned 3-axis stabilised automated capsule which served as experimental reentry vehicle primarily for technology-proving and data-gathering purposes. In terms of its shape, the vehicle bares an external resemblance to a 70 per cent-scale version of the American Apollo capsule, and considered by the ESA to be a 50 per cent-scale vehicle of a prospective potentially operational transportation vehicle; as such, it is 2.8 meters in diameter and weighs 2.8 tons at atmospheric interface point. The ARD possesses an air- and water-tight pressurised structure primarily composed of an aluminium alloy, which is protected by a layer of Norcoat 62250 FI cork composite tiles across the exterior of nosecone and by an arrangement of aleastrasil silicon dioxide-phenol formaldehyde resin tiles over the heat shield. The vehicle itself can be divided into three distinct sections: the frontshield section, the rear-cone section and the backcover section.\n\nThe ARD possesses manoeuvrability capabilities during re-entry; a favourable lift-to-drag ratio is achieved via an off-set center of gravity. The guidance law is akin to that of Apollo and the Space Shuttle, being based on a drag-velocity profile control and bank angles manoeuvres in order to conform with heating, load factor, rebound, and other required conditions; according to the ESA, this provided acceptable final guidance accuracy (within 5 km) with limited real-time calculation complexity. In operation, the guidance system becomes active once the aerodynamic forces become efficient and as long as the reaction control system remains efficient. Instead of using flight control surfaces, non-linear control is instead ensured by an assortment of seven hydrazine thrusters which, according to the manufacturer, were derived from the Ariane 5 expendable launch system. These rocket thrusters, each typically generating 400-N of thrust, were arranged in a blow-down configuration and positioned so that three units provide pitch control, two for roll and two for yaw.\n\nDuring its reentry into the atmosphere, the ARD’s heat shield is exposed to temperatures reaching as high as 2000 °C and a heat flux peaking at 1000 kW/m2, resulting from the ionisation of the atmosphere, which is in turn caused by the vehicle travelling at hypersonic speeds, in excess of 27,000 km/h during parts of its reentry descent. While the conical area of the vehicle may reach 1000 °C, with a heat flux of 90–125 kW/m2, the interior temperature will not rise above 40 °C. The thermal protection measures used were a combination of pre-existing materials that Aerospatiale had already developed under French military programmes along with multiple new-generation materials, the latter of which had been principally included for testing purposes. During reentry, the ARD's head shield loses only 0.5 mm of its thickness, keeping its aerodynamic shape relatively constant, which in turn simplifies the flight control algorithms.\n\nThe vehicle is equipped with a Descent Recovery System (DRS), deployed prior to splashdown in order to limit the impact loads and to ensure its flotation for up to 36 hours. This system involving the deployment of multiple parachutes, stored within the internal space of the tip of the nose cone; in total, one flat ribbon pilot chute, one conical ribbon drogue chute with a single reefing stage, and three slotted ribbon main parachutes with two reefing stages are typically deployed. For buoyancy purposes, a pair of inflatable balloons are also present in the DRS, helping to keep the vehicle upright. To aid in its recovery, the ARD is furnished with both a satellite-based search and rescue radio beacon and flashing light.\n\nThe internal space of the ARD and was packed with the most advanced technologies to test and qualify new technologies and flight control capabilities for atmospheric reentry and landing. The avionics of the vehicle were primarily sourced from existing equipment used upon the Ariane 5 launcher. The guidance and navigation systems used a computerised inertial navigation system which, via a databus, would be automatically corrected by GPS during the ballistic phase of flight. However, the ARD was designed to be tolerant to instances of GPS failure; this is achieved via a series of control loop algorithms that verify the GPS-derived data to be within a pre-established ‘credibility window’, defined by the inertial navigation readings. During the vehicle's sole mission, it continuously recorded and transmitted to the ground in excess of 200 critical parameters that were used to analyse the ARD's flight performance as well as the behaviour of the equipment on board.\n\nThe ARD only performed a single spaceflight. On 21 October 1998, the ARD was launched upon the third flight of the Ariane 5 expendable launch system. It was released shortly after separation of the launcher's cryogenic main stage (at an altitude of about 216 km) 12 minutes after lift-off from the Guiana Space Centre, Europe's spaceport in Kourou, French Guiana. The ARD attained a recorded altitude of 830 km, after which a guided reentry into the atmosphere was conducted. It splashed down to within 4.9 km of its target point in the Pacific Ocean between the Marquesas Islands and Hawaii after one hour and 41 minutes of flight.\n\nThe ARD was recovered roughly five hours following splash down. Following recovery, the vehicle was transported back to Europe and subject to detailed technical analysis in order to acquire more information on its performance. Engineers analysing data from its sub-orbital flight reported that all the capsule's systems had performed well and according to expectations; analysis of the craft's real-time telemetry broadcast during the flight had also reported that all electrical equipment and propulsion systems functioned nominally. The onboard telemetry systems and reception stations had all performed well, and the onboard GPS receiver worked satisfactorily during the entire flight except, as expected, during black-out in reentry.\n\nFollowing post-mission analysis of the ARD's performance, it was announced that all of the demonstration and system requirements of the programme had been successfully achieved. The test flight itself was described as having been \"nearly nominal\", particularly the trajectory and flight control aspects; additionally, many of the onboard systems, such as the navigation (primary and backup), propulsion, thermal protection, communication, and DRS were found to have performed either as predicted or to have been outside these predictions by only a small margin. During reentry, the heat shield temperature reached a recorded peak temperature of 900 °C; nevertheless, both the vehicle's cone and heat shield thermal protection were found in a perfect state following its retrieval.\n\nIssues highlighted during analysis included the role of design uncertainties having led to difficulties in observing some physical phenomena such as real gas effects, addressing aerothermal environment characterization was also hindered due to the premature failure of some thermocouples. Overall, the flight was stated to have brought a great amount of high quality aerodynamic information back which, amongst other benefits, served to confirm and enhance the capabilities of ground-based prediction tools. Since its retrieval and the conclusion of post-mission examination, the sole ARD vehicle itself has been preserved and has become a publicly-accessible exhibit at the European Space Research and Technology Centre in Noordwijk, Netherlands.\n\n\n"}
{"id": "20182719", "url": "https://en.wikipedia.org/wiki?curid=20182719", "title": "Cartoning machine", "text": "Cartoning machine\n\nA cartoning machine or cartoner, is a packaging machine that forms cartons: erect, close, folded, side seamed and sealed cartons.\n\nPackaging machines which form a carton board blank into a carton filled with a product or bag of products or number of products say into single carton, after the filling, the machine engages its tabs / slots to apply adhesive and close both the ends of carton completely sealing the carton.\n\nCartoning machines can be divided into two types:\n\nA cartoning machine which picks a single piece from stack of folded carton and erects it, fills with a product or bag of products or number of products horizontally through an open end and closes by tucking the end flaps of the carton or applying glue or adhesive. The product might be pushed in the carton either through the mechanical sleeve or by pressurized air. For many applications however, the products are inserted into the carton manually. This type of Cartoning machine is widely used for packaging foodstuffs, confectionery, medicine, cosmetics, sundry goods, etc.\n\nA cartoning machine which erects a folded carton, fills with a product or number of products vertically through an open end and closes by either tucking the end flaps of the carton or applying glue or adhesive, is called an end load cartoning machine. Cartoning machines are widely used for packaging bottled foodstuffs, confectionery, medicine, cosmetics, etc., and can vary based on the scale of business.\n\n\n"}
{"id": "9925110", "url": "https://en.wikipedia.org/wiki?curid=9925110", "title": "Command Post of the Future", "text": "Command Post of the Future\n\nThe United States Army's Command Post of the Future (CPOF) is a C2 software system that allows commanders to maintain topsight over the battlefield; collaborate with superiors, peers and subordinates over live data; and communicate their intent.\n\nOriginally a DARPA technology demonstration, in 2006 CPOF became an Army Program of Record. It is managed by the Product Manager Tactical Mission Command at Aberdeen Proving Ground, Maryland, and integrated with the Army's Maneuver Control System and other products. The prime contractor on the CPOF program is General Dynamics C4 Systems, which purchased the original developer of the software (MAYA Viz Ltd) in 2005.\n\nCPOF began as a DARPA investigation to improve mission command using networked information visualization systems, with the goal of doubling the speed and quality of command decisions. The system was developed in a research setting by Global Infotek, Inc.; ISX Corporation (now part of Lockheed Martin Advanced Technology Laboratories); Oculus Info, Inc. (now called Uncharted Software Inc.); SYS Technologies, Inc.; and MAYA Viz (now part of General Dynamics C4 Systems) with the active participation of military personnel as subject matter experts.\n\nCPOF is one of several examples of collaborative software, but intended specifically for use in a mission command. A shared workspace is the main interface, in which every interface element in CPOF is a shared piece of data in a networked repository. Shared visual elements in CPOF include iconic representations of hard data, such as units, events, and tasks; visualization frameworks such as maps or schedule charts on which those icons appear; and brush-marks, ink-strokes, highlighting, notes and other annotation.\n\nAll visual elements in CPOF are interactive via drag-and-drop gestures. Users can drag data-elements and annotation from any visualization framework into any other (i.e., from a chart to a table), which reveal different data-attributes in context depending on the visualization used. Most data-elements can be grouped and nested via drag-and-drop to form associations that remain with the data in all of its views. Drag-and-drop composition on live visualizations is CPOF's primary mechanism for editing data values, such as locations on a map or tasks on a schedule (for example, moving an event-icon on a map changes the lat/lon values of that event in the shared repository; moving a task icon on a schedule changes its time-based values in the shared repository). The results of editing gestures are conveyed in real-time to all observers and users of a visualization; when one user moves an event on a map, for example, that event-icon moves on all maps and shared views, such that all users see its new location immediately. Data inputs from warfighters are conveyed to all collaborators as the \"natural\" result of a drop-gesture in-situ, requiring no explicit publishing mechanism.\n\nCPOF is also used as a live-data alternative to PowerPoint briefings. During a CPOF briefing, commanders can drill into any data element in a high-level view to see details on demand, and view outliers or other elements of interest in different visual contexts without switching applications. Annotations and editing-gestures made during briefings become part of the shared repository. The commander's topsight is based on ground-truth at the moment of the briefing; the commander can then communicate intent on live data.\n\nCPOF users at any level can assemble workspaces out of smaller tool-and-appliance primitives, allowing members of a collaborating group to organize their workflows according to their needs, without affecting or disrupting the views of other users. CPOF's Tool-and-appliance primitives are designed to let users create quick, throw-away mini-applications to meet their needs in-situ, supporting on-the-fly uses of the software that no developer or designer could have anticipated.\n\nThe CPOF software is based on the CoMotion platform, a proprietary commercial framework for building collaborative information visualization systems and domain-independent \"decision communities\". CoMotion's design principles originated as a research program at Carnegie Mellon University led by Steven Roth, and was subsequently developed at MAYA Viz Ltd and General Dynamics C4 Systems.\n\nCPOF uses a proprietary navigational style database based on U-forms to store, represent, and operate upon a wide variety of types of data. CPOF can receive real-time or near-real-time data from a variety of standard sources—such as GCCS-A, C2PC, and ABCS—and display them using MIL-STD-2525B symbols on maps and charts. Plans, schedules, notes, briefings, and other battle-related information can be composed and shared between warfighters. All maps, charts, pasteboards, and other work products can be marked up with permanent and/or fading ink, and annotated with text or \"stickies\" to provide further context. A VOIP solution is included, although it can integrate with a pre-existing voice solution.\n\nFault tolerance for low bandwidth, high latency, and/or error-prone TCP/IP networks is supported by CPOF's multi-tiered client-server architecture. It can thus be deployed on systems from a two-hop geosynchronous satellite link to a radio network such as JNN while remaining collaborative. The software is largely Java-based, but is only currently deployed on a Microsoft Windows platform.\n\nCPOF was first deployed operationally in a handful of locations in Baghdad, Iraq by the 1st Cavalry Division of the US Army in 2004, and was subsequently deployed throughout Iraq and Afghanistan and used by coalition forces. Variants of CPOF have participated in United States Joint Forces Command's Urban Resolve 2015, the United States Air Force's Joint Expeditionary Force Experiment 06 and 08, and has been in use by the Marines in Combat Operation Centers since 2007.\n\nCPOF became an official US Army program of record in 2006.\n\n\n2. Jacob Mowry, Lead Trainer\n\n"}
{"id": "5213766", "url": "https://en.wikipedia.org/wiki?curid=5213766", "title": "Correction paper", "text": "Correction paper\n\nCorrection paper, or correction film, its plastic based equivalent, is a tab of plastic with one side coated with white correction material. It is used to correct typing errors made when using a typewriter. When inserted between the paper and the ribbon, the impression of the typebar presses the shape of the character into the film, which prints the white correction material onto the paper, hiding the erroneous character and preparing the document for the correct character.\n\n"}
{"id": "3012612", "url": "https://en.wikipedia.org/wiki?curid=3012612", "title": "Delay-locked loop", "text": "Delay-locked loop\n\nIn electronics, a delay-locked loop (DLL) is a digital circuit similar to a phase-locked loop (PLL), with the main difference being the absence of an internal voltage-controlled oscillator, replaced by a delay line.\n\nA DLL can be used to change the phase of a clock signal (a signal with a periodic waveform), usually to enhance the \"clock rise\"-to-\"data output valid\" timing characteristics of integrated circuits (such as DRAM devices). DLLs can also be used for clock recovery (CDR). From the outside, a DLL can be seen as a negative-delay gate placed in the clock path of a digital circuit.\n\nThe main component of a DLL is a delay chain composed of many delay gates connected output-to-input. The input of the chain (and thus of the DLL) is connected to the clock that is to be negatively delayed. A multiplexer is connected to each stage of the delay chain; the selector of this multiplexer is automatically updated by a control circuit to produce the negative delay effect. The output of the DLL is the resulting, negatively delayed clock signal.\n\nAnother way to view the difference between a DLL and a PLL is that a DLL uses a variable phase (=delay) block where a PLL uses a variable frequency block.\n\nA DLL compares the phase of its last output with the input clock to generate an error signal which is then integrated and fed back as the control to all of the delay elements.\nThe integration allows the error to go to zero while keeping the control signal, and thus the delays, where they need to be for phase lock. Since the control signal directly impacts the phase this is all that is required.\n\nA PLL compares the phase of its oscillator with the incoming signal to generate an error signal which is then integrated to create a control signal for the voltage-controlled oscillator. The control signal impacts the frequency of the oscillator, and phase is the integral of frequency, so a second integration is unavoidably performed by the oscillator itself.\n\nIn the Control Systems jargon, the DLL is a loop one step lower in order and in type with respect to the PLL, because it lacks the 1/s factor in the controlled block: the delay line has a transfer function phase-out/phase-in that is just a constant, the VCO transfer function is instead G/s. In the comparison made in the previous sentences (that correspond to the figure where the integrator, and not the flat gain, is used), the DLL is a loop of 1st order and type 1 and the PLL of 2nd order and type 2. Without the integration of the error signal, the DLL would be 0th order and type 0 and the PLL 1st order and type 1.\n\nThe number of elements in the delay chain must be even, or else the duty cycle of the clock at the intermediate nodes of the chain might become irregular.\n\nIf 2N +1 was the -odd- number of stages, a 50% duty-cycle would become at times N/(2N+1), at times (N+1)/(2N+1), following the jittering of the error signal around the value corresponding to perfect lock.\n\nCalling 2N the number of stages of the DLL chain, it is easy to see that the figure above would change from a DLL to a PLL, locked to the same phase and frequency, if the following modifications were made:\nThe resulting chain becomes a ring oscillator with a period equal to the delay of the previous chain, and the loop locks to the same reference clock with the same level of error signal. \n\nThe loop order and type are both incremented by one.\nIt may be further remarked that, in the case where the integrator instead of the flat gain is chosen, the PLL that can be obtained is unstable. \n\nThe phase shift can be specified either in absolute terms (in delay chain gate units), or as a proportion of the clock period, or both.\n\n\nThe Delay Lock Loop has been derived by J.J. Spilker, JR. and D.T. Magill, \"The delay-lock discriminator--an optimum tracking device,\" Proc. IRE, vol.49, pp. 1403–1416, September 1961.\n"}
{"id": "27628921", "url": "https://en.wikipedia.org/wiki?curid=27628921", "title": "Eighteen Nation Committee on Disarmament", "text": "Eighteen Nation Committee on Disarmament\n\nThe Eighteen Nation Committee on Disarmament (ENCD) was sponsored by the United Nations in 1961. The ENCD considered disarmament, confidence-building measures and nuclear test controls. Between 1965 and 1968, the ENCD negotiated the Treaty on the Non-Proliferation of Nuclear Weapons.\n\nThe United Nations (UN) General Assembly accepted the decision of the major powers to create the ENCD through resolution 1722 (XVI) on December 21, 1961. The ENCD began work on March 14, 1962 in Geneva, Switzerland and met regularly until August 26, 1969. On that date the ENCD was reconstituted as the Conference of the Committee on Disarmament (CCD). The August 26 meeting of the ENCD was its 431st since its inception. Soon after the ENCD began work the Soviet Union submitted a draft treaty for consideration. The USSR Draft Treaty on General and Complete Disarmament under Strict International Control was submitted to the ENCD on March 15, 1962. The Soviet draft treaty was an 18-point plan for disarmament in three stages which included nuclear disarmament and the creation of a UN special disarmament organization. The United States quickly countered with its own proposals on April 18, 1962.\n\nThe ENCD included the original members of the Ten Nation Committee on Disarmament (TNCD) as well as eight additional member nations. The ENCD actually only included the participation of seventeen nations, as France did not participate in an official capacity. However, they were involved in an unofficial role in consultations with the other Western representatives.\n\nOriginal members of TNCD: (Western Bloc) - Canada, France, Great Britain, Italy, United States. (Eastern Bloc) - Bulgaria, Czechoslovakia, Poland, Romania, Soviet Union.\n\nNations added to ENCD: Brazil, Burma, Ethiopia, India, Mexico, Nigeria, Sweden, United Arab Republic (UAR).\n\nThe ENCD (1962-69) was one of several predecessors to the current UN disarmament organization, the Conference on Disarmament (CD). The ENCD followed the short-lived Ten Nation Committee on Disarmament (1960), and was succeeded by the CCD (1969-78) until the CD was formed in 1979.\n\nAfter the events of the Cuban Missile Crisis in 1962 leaders in both Washington, D.C. and Moscow found that communicating with each other was plagued with certain delay. In the aftermath of the crisis the ENCD presented a working paper titled \"Measures to Reduce the Risk of War Through Accident, Miscalculation, or Failure of Communication\". In the paper, the United States proposed a direct link of communication between Washington and Moscow. Though the Soviets had previously rejected such a link outside the framework of a larger disarmament agreement in April 1963 both sides announced a willingness to commit to such an agreement. The agreement, usually known as the \"Hot Line Agreement\" was signed and entered into force on June 20, 1963. Although the ENCD ratified the Hot Line Agreement the negotiations that actually led to its implementation occurred outside the confines of the committee.\n"}
{"id": "10677607", "url": "https://en.wikipedia.org/wiki?curid=10677607", "title": "Elpida Memory", "text": "Elpida Memory\n\nElpida Memory was founded in 1999 as a merger of NEC's and Hitachi's DRAM operation and began development operations for DRAM products in 2000.\n\nIn 2001, the company began construction of its 300mm wafer fabrication plant. Later that year, it began sales operations in domestic markets.\n\nIn 2003, the company took over Mitsubishi Electric Corporation's DRAM operations and employed Mitsubishi development engineers.\n\nIn 2004, Elpida Memory went public and was listed on the Tokyo Stock Exchange.\n\nIn 2006, the company established Akita Elpida to take on the development of advanced back-end technology processes.\n\nIn March 2006, Elpida reported consolidated sales of 241,500,000,000 Japanese yen. It employed 3196 people.\n\nIn 2002, armed with the Sherman Antitrust Act, the United States Department of Justice began a probe into the activities of dynamic random access memory (DRAM) manufacturers. US computer makers, including Dell and Gateway, claimed that inflated DRAM pricing was causing lost profits and hindering their effectiveness in the marketplace. To date, five manufacturers have pleaded guilty to their involvement in an international price-fixing conspiracy including Hynix, Infineon, Micron Technology, Samsung, and Elpida. Micron Technology was not fined for its involvement due to co-operation with investigators.\n\nThe company received 140 billion yen in financial aid and loans from the Japanese government and banks during the financial crisis in 2009.\n\nOn April 3, 2010, Elpida Memory sold ¥18.5billion worth of shares to Kingston Technology \n\nOn April 22, 2010, Elpida announced it had developed the world's first four-gigabit DDR3 SDRAM. Based on a 40 nm process, this DRAM was said to use about thirty percent less power compared to two 40 nm process two-gigabit DDR3 SDRAMs. It was to operate at both standard DDR3 1.5 V and 1.35 V to further reduce power consumption.\n\nIn July 2011, Elpida announced that it planned to raise $987 million by selling shares and bonds. In August 2011, Elpida claimed to be the first memory maker to begin sampling 25 nm DRAMs.\n\nOn February 27, 2012, Elpida filed for bankruptcy. \nWith liabilities of 448 billion yen (US$5.5 billion), the company's bankruptcy was Japan's largest since Japan Airlines bankrupted in January 2010. The company suffered from both strong yen and a sharp drop of DRAM prices as a result of stagnant demand of personal computers and disruption of computer production caused by flooding in Thailand. DRAM prices plunged to a record low in 2011 as the price of the benchmark DDR3 2-gigabit DRAM declined 85%. Elpida was the third largest DRAM maker, held 18 percent of the market by revenue in 2011.\n\nOn March 28, 2012, Elpida was delisted from the Tokyo Stock Exchange. At the time, Elpida was one of the suppliers of SDRAM components for the A6 processor in the Apple iPhone 5.\n\nIn February 2013, Tokyo court and Elpida creditors approved an acquisition by Micron Technology.\n\nThe company became a fully owned subsidiary of Micron Technology on July 31, 2013.\n\nEffective February 28, 2014, Elpida changed its name to Micron Memory Japan and Elpida Akita changed its name to Micron Akita, Inc.\n\n\n"}
{"id": "29720839", "url": "https://en.wikipedia.org/wiki?curid=29720839", "title": "Futon dryer", "text": "Futon dryer\n\nA is an electric device that warms and dries a Japanese futon by forcing warm air through it.\n\nA futon dryer works by drawing in air with an electric fan and heating it, and then forcing it into a large cloth pouch. The pouch is usually inserted between the futon mattress and comforter, and hot air is forced through it for about an hour to dry the futon.\n\nFuton dryers run on a timer, and when the timer reaches zero, cold air is blown before the unit shuts off. A futon drier uses about 600 watts of electricity, about the same as a hair drier.\n\nA futon must be kept dry because human sweat and body heat create a warm, moist environment which is amenable to mites. Traditionally futons are kept dry by hanging them out on a balcony in the sun to dry in the open air, but some people are too busy to do this since they must be hung during daylight hours and brought in before sunset, when the temperature drops and moisture condenses inside the futon. Finding time to hang a futon may also be difficult during the rainy season in Japan, since there are few sunny days and there are frequent and sudden periods of rain so a futon dryer can fulfill the function of keeping the futon dry and keeping the level of mites living in the futon down without needing to hang it out.\n\nMany dryers also have attachments for drying shoes or boots, or air pouches which can be folded to dry clothes in place of a clothes dryer. The dryer can be run for under an hour to warm the futon without completely drying it, or over an hour to reduce the population of mites.\n"}
{"id": "14447758", "url": "https://en.wikipedia.org/wiki?curid=14447758", "title": "Generic Bootstrapping Architecture", "text": "Generic Bootstrapping Architecture\n\nGeneric Bootstrapping Architecture (GBA) is a technology that enables the authentication of a user. This authentication is possible if the user owns a valid identity on an HLR (Home Location Register) or on an HSS (Home Subscriber Server).\n\nGBA is standardized at the 3GPP (http://www.3gpp.org/ftp/Specs/html-info/33220.htm). The user authentication is instantiated by a shared secret, one in the smartcard, for example a SIM card inside the mobile phone and the other is on the HLR/HSS.\n\nGBA authenticates by making a network component challenge the smartcard and verify that the answer is the one predicted by the HLR/HSS.\n\nInstead of asking the service provider to trust the BSF and relying on it for every authentication request, the BSF establishes a shared secret between the simcard card and the service provider. This shared secret is limited in time and for a specific domain.\n\nThis solution has some strong points of certificate and shared secrets without having some of their weaknesses: \n- There is no need for user enrollment phase nor secure deployment of keys, making this solution a very low cost one when compared to PKI.\n\n- Another advantage is the ease with which the authentication method may be integrated into terminals and service providers, as it is based on HTTP's well known \"Digest access authentication\". Every Web server already implement HTTP digest authentication and the effort to implement GBA on top of digest authentication is minimal. For example, it could be implemented on SimpleSAMLPhP http://rnd.feide.no/simplesamlphp with 500 PHP lines of code and only a few tens of lines of code are Service Provider specific making it really easy to port it to another Web site.\n\n- On device side is needed: \n\nActually, contents in this section are from external literature.\n\nThere are two ways to use GAA (Generic Authentication Architecture). \n\nIn the shared secret cases, the customer and the operator are first mutually authenticated through 3G and Authentication Key (AKA) and they agree on session keys which can then be used between the client and services that the customer wants to use. \nThis is called \"bootstrapping\".\nAfter that, the services can retrieve the session keys from the operator, and they can be used in some application specific protocol between the client and services.\n\nFigure above shows the network GAA entities and interfaces between them. Optional entities are drawn with lines\nnetwork and borders dotted the scoreboard. The User Equipment (UE) is, for example, the user's mobile phone. The UE and\nBootstrapping Server Function (BSF) mutually authenticate themselves during the Ub (number [2] above) interface, using the Digest access authentication AKA protocol. The UE also communicates with the Network Application Functions (NAF), which are the implementation servers, over the Ua [4] interface, which can use any specific application protocol necessary.\n\nBSF retrieves data from the subscriber from the Home Subscriber Server (HSS) during the Zh [3] interface, which uses the\nDiameter Base Protocol. If there are several HSS in the network, BSF must first know which one to use. This can be done by either setting up a pre-defined HSS to BSF, or by querying the Subscriber Locator Function (SLF).\nNAFs recover the key session of BSF during the Zn [5] interface, which also uses the diameter at the base Protocol. If\nNAF is not in the home network, it must use a Zn-proxy to contact BSF .\n\n\nSadly, despite many advantages and potential uses of GBA, its implementation in handsets has been limited since GBA standardization in 2006. Most notably, GBA was implemented in Symbian-based handsets.\n"}
{"id": "13793754", "url": "https://en.wikipedia.org/wiki?curid=13793754", "title": "Geophysical MASINT", "text": "Geophysical MASINT\n\nGeophysical MASINT is a branch of Measurement and Signature Intelligence (MASINT) that involves phenomena transmitted through the earth (ground, water, atmosphere) and manmade structures including emitted or reflected sounds, pressure waves, vibrations, and magnetic field or ionosphere disturbances.\n\nAccording to the United States Department of Defense, MASINT is technically derived intelligence (excluding traditional imagery IMINT and signals intelligence SIGINT) that – when collected, processed, and analyzed by dedicated MASINT systems – results in intelligence that detects, tracks, identifies, or describes the signatures (distinctive characteristics) of fixed or dynamic target sources. MASINT was recognized as a formal intelligence discipline in 1986. Another way to describe MASINT is a \"non-literal\" discipline. It feeds on a target's unintended emissive by-products, the \"trails\" - the spectral, chemical or RF that an object leaves behind. These trails form distinct signatures, which can be exploited as reliable discriminators to characterize specific events or disclose hidden targets.\"\n\nAs with many branches of MASINT, specific techniques may overlap with the six major conceptual disciplines of MASINT defined by the Center for MASINT Studies and Research, which divides MASINT into Electro-optical, Nuclear, Geophysical, Radar, Materials, and Radiofrequency disciplines.\n\nGeophysical sensors have a long history in conventional military and commercial applications, from weather prediction for sailing, to fish finding for commercial fisheries, to nuclear test ban verification. New challenges, however, keep emerging.\n\nFor first-world military forces opposing other conventional militaries, there is an assumption that if a target can be located, it can be destroyed. As a result, concealment and deception have taken on new criticality. \"Stealth\" low-observability aircraft have gotten much attention, and new surface ship designs feature observability reduction. Operating in the confusing littoral environment produces a great deal of concealing interference.\n\nOf course, submariners feel they invented low observability, and others are simply learning from them. They know that going deep or at least ultraquiet, and hiding among natural features, makes them very hard to detect.\n\nTwo families of military applications, among many, represent new challenges against which geophysical MASINT can be tried. Also, see Unattended Ground Sensors.\n\nOne of the easiest ways for nations to protect weapons of mass destruction, command posts, and other critical structures is to bury them deeply, perhaps enlarging natural caves or disused mines. Deep burial is not only a means of protection against physical attack, as even without the use of nuclear weapons, there are deeply penetrating precision guided bombs that can attack them. Deep burial, with appropriate concealment during construction, is a way to avoid the opponent's knowing the buried facility's position well enough to direct precision guided weapons against it.\n\nFinding deeply buried structures, therefore, is a critical military requirement. The usual first step in finding a deep structure is IMINT, especially using hyperspectral IMINT sensors to help eliminate concealment. \"Hyperspectral images can help reveal information not obtainable through other forms of imagery intelligence such as the moisture content of soil. This data can also help distinguish camouflage netting from natural foliage.\" Still, a facility dug under a busy city would be extremely hard to find during construction. When the opponent knows that it is suspected that a deeply buried facility exists, there can be a variety of decoys and lures, such as buried heat sources to confuse infrared sensors, or simply digging holes and covering them, with nothing inside.\n\nMASINT using acoustic, seismic, and magnetic sensors would appear to have promise, but these sensors must be fairly close to the target. Magnetic Anomaly Detection (MAD) is used in antisubmarine water, for final localization before attack. The existence of the submarine is usually established through passive listening and refined with directional passive sensors and active sonar.\n\nOnce these sensors (as well as HUMINT and other sources) have failed, there is promise for surveying large areas and deeply concealed facilities using gravitimetric sensors. Gravity sensors are a new field, but military requirements are making it important while the technology to do it is becoming possible.\n\nEspecially in today's \"green water\" and \"brown water\" naval applications, navies are looking at MASINT solutions to meet new challenges of operating in littoral areas of operations. This symposium found it useful to look at five technology areas, which are interesting to contrast to the generally accepted categories of MASINT: acoustics and geology and geodesy/sediments/transport, nonacoustical detection (biology/optics/chemistry), physical oceanography, coastal meteorology, and electromagnetic detection.\n\nAlthough it is unlikely there will ever be another World War II-style opposed landing on a fortified beach, another aspect of the littoral is being able to react to opportunities for amphibious warfare. Detecting shallow-water and beach mines remains a challenge, since mine warfare is a deadly \"poor man's weapon.\"\n\nWhile initial landings from an offshore force would be from helicopters or tiltrotor aircraft, with air cushion vehicles bringing ashore larger equipment, traditional landing craft, portable causeways, or other equipment will eventually be needed to bring heavy equipment across a beach. Shallow depth and natural underwater obstacles can block beach access to these craft and equipment, as can shallow-water mines. Synthetic Aperture Radar (SAR), airborne laser detection and ranging (LIDAR) and use of bioluminescence to detect wake trails around underwater obstacles all may help solve this challenge.\n\nMoving onto and across the beach has its own challenges. Remotely operated vehicles may be able to map landing routes, and they, as well as LIDAR and multispectral imaging, may be able to detect shallow water. Once on the beach, the soil has to support heavy equipment. Techniques here include estimating soil type from multispectral imaging, or from an airdropped penetrometer that actually measures the loadbearing capacity of the surface.\n\nThe science and art of weather prediction used the ideas of measurement and signatures to predict phenomena, long before there were any electronic sensors. Masters of sailing ships might have no more sophisticated instrument than a wetted finger raised to the wind, and the flapping of sails.\n\nWeather information, in the normal course of military operations, has a major effect on tactics. High winds and low pressures can change artillery trajectories. High and low temperatures cause both people and equipment to require special protection. Aspects of weather, however, also can be measured and compared with signatures, to confirm or reject the findings of other sensors.\n\nThe state of the art is to fuse meteorological, oceanographic, and acoustic data in a variety of display modes. Temperature, salinity and sound speed can be displayed horizontally, vertically, or in three-dimensional perspective.\n\nWhile early sailors had no sensors beyond their five senses, the modern meteorologist has a wide range of geophysical and electro-optical measuring devices, operating on platforms from the bottom of the sea to deep space. Prediction based on these measurements are based on signatures of past weather events, a deep understanding of theory, and computational models.\n\nWeather predictions can give significant negative intelligence, when the signature of some combat system is such that it can operate only under certain weather conditions. Weather has long been an extremely critical part of modern military operations, as when the decision to land at Normandy on June 6, rather than June 5, 1944 depended on Dwight D. Eisenhower's trust in his staff weather advisor, Group Captain James Martin Stagg. It is rarely understood that something as fast as a ballistic missile reentry vehicle, or as \"smart\" as a precision guided munition, can still be affected by winds in the target area.\n\nAs part of Unattended Ground Sensors. The Remote Miniature Weather Station (RMWS), from System Innovations, is an air-droppable version with a lightweight, expendable and modular system with two components: a meteorological (MET) sensor and a ceilometer (cloud ceiling height) with limited MET. The basic MET system is surface-based and measures wind speed and direction, horizontal visibility, surface atmospheric pressure, air temperature and relative humidity. The ceilometer sensor determines cloud height and discrete cloud layers. The system provides near-real-time data capable of 24-hour operation for 60 days. The RMWS can also go in with US Air Force Special Operations combat weathermen\n\nThe man-portable version, brought in by combat weathermen, has an additional function, as remote miniature ceilometer. Designed to measure multiple layer cloud ceiling heights and then send that data via satellite communications link to an operator display, the system uses a Neodinum YAG (NdYAG), 4 megawatt non-eye safe laser. According to one weatherman, \"We have to watch that one,” he said. “Leaving it out there basically we’re worried about civilian populace going out there and playing with it—firing the laser and there goes somebody’s eye. There are two different units [to RMWS]. One has the laser and one doesn’t. The basic difference is the one with the laser is going to give you cloud height.\"\n\nHydrographic MASINT is subtly different from weather, in that it considers factors such as water temperature and salinity, biologic activities, and other factors that have a major effect on sensors and weapons used in shallow water. ASW equipment, especially acoustic performance depends on the season the specific coastal site. Water column conditions, such as temperature, salinity, and turbidity are more variable in shallow than deep water. Water depth will influence bottom bounce conditions, as will the material of the bottom. Seasonal water column conditions (particularly summer versus winter) are inherently more variable in shallow water than in deep water.\n\nWhile much attention is given to shallow waters of the littoral, other areas have unique hydrographic characteristics.\n\nA submarine tactical development activity observed, \"Fresh water eddies exist in many areas of the world. As we have experienced recently in the Gulf of Mexico using the Tactical Oceanographic Monitoring System (TOMS), there exist very distinct surface ducts that causes the Submarine Fleet Mission Program Library (SFMPL) sonar prediction to be unreliable. Accurate bathythermic information is paramount and a precursor for accurate sonar predictions.”\n\nCritical to the prediction of sound, needed by active and passive MASINT systems operating in water is knowing the temperature and salinity at specific depths. Antisubmarine aircraft, ships, and submarines can release independent sensors that measure the water temperature at various depths. The water temperature is critically important in acoustic detections, as changes in water temperature at thermoclines can act as a \"barrier\" or \"layer\" to acoustic propagation. To hunt a submarine, which is aware of water temperature, the hunter must drop acoustic sensors below the thermocline.\n\nWater conductivity is used as a surrogate marker for salinity. The current and most recently developed software, however, does not give information on suspended material in the water or bottom characteristics, both considered critical in shallow-water operations.\n\nThe US Navy does this by dropping expendable probes, which transmit to a recorder, of 1978-1980 vintage, the AN/BQH-7 for submarines and the AN/BQH-71 for surface ships. While the redesign of the late seventies did introduce digital logic, the devices kept hard-to-maintain analog recorders, and maintainability became critical by 1995. A project was begun to extend with COTS components, to result in the AN/BQH-7/7A EC-3. In 1994-5, the maintainability of the in-service units became critical.\n\nVariables in selecting the appropriate probe include:\n\nLarge schools of fish contain enough entrapped air to conceal the sea floor, or manmade underwater vehicles and structures. Fishfinders, developed for commercial and recreational fishing, are specialized sonars that can identify acoustic reflections between the surface and the bottom. Variations on commercial equipment are apt to be needed, especially in littoral areas rich in marine life.\n\nA variety of sensors can be used to characterise the sea bottom into, for example, mud, sand, and gravel. Active acoustic sensors are the most obvious, but there is potential information from gravitimetric sensors, electro-optical and radar sensors for making inferences from the water surface, etc.\n\nRelatively simple sonars such as echo sounders can be promoted to seafloor classification systems via add-on modules, converting echo parameters into sediment type. Different algorithms exist, but they are all based on changes in the energy or shape of the reflected sounder pings.\n\nSide-scan sonars can be used to derive maps of the topography of an area by moving the sonar across it just above the bottom. Multibeam hull-mounted sonars are not as precise as a sensor near the bottom, but both can give reasonable three-dimensional visualization.\n\nAnother approach comes from greater signal processing of existing military sensors. The US Naval Research Laboratory demonstrated both seafloor characterization, as well as subsurface characteristics of the seafloor. Sensors used, in different demonstrations, included normal incidence beams from the AM/UQN-4 surface ship depthfinder, and AN/BQN-17 submarine fathometer; backscatter from the Kongsberg EM-121 commercial multibeam sonar; AN/UQN-4 fathometers on mine countermeasures (MCM) ships, and the AN/AQS-20 mine-hunting system. These produced the \"Bottom and Subsurface Characterization\" graphic.\n\nOne of the improvements in the Fuchs 2 reconnaissance vehicle is adding onboard weather instrumentations, including data such as wind direction and speed;, air and ground temperature; barometric pressure and humidity.\n\nThis includes the collection of passive or active emitted or reflected sounds, pressure waves or vibrations in the atmosphere (ACOUSTINT) or in the water (ACINT) or conducted through the ground Going well back into the Middle Ages, military engineers would listen to the ground for sounds of telltale digging under fortifications.\n\nIn modern times, acoustic sensors were first used in the air, as with artillery ranging in World War I. Passive hydrophones were used by the World War I Allies against German submarines; the UC-3, was sunk with the aid of hydrophone on 23 April 1916. Since submerged submarines cannot use radar, passive and active acoustic systems are their primary sensors. Especially for the passive sensors, the submarine acoustic sensor operators must have extensive libraries of acoustic signatures, to identify sources of sound.\n\nIn shallow water, there are sufficient challenges to conventional acoustic sensors that additional MASINT sensors may be required. Two major confounding factors are:\n\nWhile now primarily of historical interest, one of the first applications of acoustic and optical MASINT was locating enemy artillery by the sound of their firing and flashes respectively during World War I. Effective sound ranging was pioneered by the British Army under the leadership of the Nobel Lauriate William Bragg. Flash spotting developed in parallel in the British, French and German armies. The combination of sound ranging (i.e., acoustic MASINT) and flash ranging (i.e., before modern optoelectronics) gave information unprecedented for the time, in both accuracy and timeliness. Enemy gun positions were located within 25 to 100 yards, with the information coming in three minutes or less.\n\nIn the \"Sound Ranging\" graphic, the manned Listening (or Advanced) Post, is sited a few 'sound seconds (or about 2000 yards) forward of the line of the unattended microphones, it sends an electrical signal to the recording station to switch on the recording apparatus. The positions of the microphones are precisely known. The differences in sound time of arrival, taken from the recordings, were then used to plot the source of the sound by one of several techniques. See http://nigelef.tripod.com/p_artyint-cb.htm#SoundRanging\n\nWhere sound ranging is a time-of-arrival technique not dissimilar to that of modern multistatic sensors, flash spotting used optical instruments to take bearings on the flash from accurately surveyed observation posts. The location of the gun was determined by plotting the bearings reported to the same gun flashes. See http://nigelef.tripod.com/p_artyint-cb.htm#FieldSurveyCoy Flash ranging, today, would be called electro-optical MASINT.\n\nArtillery sound and flash ranging remained in use through World War II and in its latest forms until the present day, although flash spotting generally ceased in the 1950s due to the widespread adoption of flashless propellants and the increasing range of artillery. Mobile counterbattery radars able to detect guns, itself a MASINT radar sensor, became available in the late 1970s, although countermortar radars appeared in World War II. These techniques paralleled radio direction finding in SIGINT that started in World War I, using graphical bearing plotting and now, with the precision time synchronization from GPS, is often time-of-arrival.\n\nArtillery positions now are located primarily with Unmanned Air Systems and IMINT or counterartillery radar, such as the widely used Swedish ArtHuR. SIGINT also may give clues to positions, both with COMINT for firing orders, and ELINT for such things as weather radar. Still, there is renewed interest in both acoustic and electro-optical systems to complement counterartillery radar.\n\nAcoustic sensors have come a long way since World War I. Typically, the acoustic sensor is part of a combined system, in which it cues radar or electro-optical sensors of greater precision, but narrower field of view.\n\nThe UK's hostile artillery locating system (HALO) has been in service with the British Army since the 1990s. HALO is not as precise as radar, but especially complements the directional radars. It passively detects artillery cannon, mortars and tank guns, with 360 degree coverage and can monitor over 2,000 square kilometers. HALO has worked in urban areas, the mountains of the Balkans, and the deserts of Iraq.\n\nThe system consists of three or more unmanned sensor positions, each with four microphones and local processing, these deduce the bearing to a gun, mortar, etc. These bearings are automatically communicated to a central processor that combines them to triangulate the source of the sound. It can compute location data on up to 8 rounds per second, and display the data to the system operator. HALO may be used in conjunction with COBRA and ArtHur counter battery radars, which are not omnidirectional, to focus on the correct sector.\n\nAnother acoustic system is the Unattended Transient Acoustic MASINT Sensor (UTAMS), developed by the U.S. Army Research Laboratory, which detects mortar and rocket launches and impacts. UTAMS remains the primary cueing sensor for the Persistent Threat Detection System (PTDS). ARL mounted aerostats with UTAMS, developing the system in a little over two months. After receiving a direct request from Iraq, ARL merged components from several programs to enable the rapid fielding of this capability. \n\nUTAMS has three to five acoustic arrays, each with four microphones, a processor, radio link, power source, and a laptop control computer. UTAMS, which was first operational in Iraq, first tested in November 2004 at a Special Forces Operating Base (SFOB) in Iraq. UTAMS was used in conjunction with AN/TPQ-36 and AN/TPQ-37 counter-artillery radar. While UTAMS was intended principally for detecting indirect artillery fire, Special Forces and their fire support officer learned it could pinpoint improvised explosive device (IED) explosions and small arms/rocket-propelled grenade (RPG) fires. It detected Points of Origin (POO) up to 10 kilometers from the sensor.\n\nAnalyzing the UTAMS and radar logs revealed several patterns. The opposing force was firing 60 mm mortars during observed dining hours, presumably since that gave the largest groupings of personnel and the best chance of producing heavy casualties. That would have been obvious from the impact history alone, but these MASINT sensors established a pattern of the enemy firing locations.\n\nThis allowed the US forces to move mortars into range of the firing positions, give coordinates to cannon when the mortars were otherwise committed, and to use attack helicopters as a backup to both. The opponents changed to night fires, which, again, were countered with mortar, artillery, and helicopter fires. They then moved into an urban area where US artillery was not allowed to fire, but a combination of PSYOPS leaflet drops and deliberate near misses convinced the locals not to give sanctuary to the mortar crews.\n\nOriginally for a Marine requirement in Afghanistan, UTAMS was combined with electro-optical MASINT to produce the Rocket Launch Spotter (RLS) system useful against both rockets and mortars.\n\nIn the Rocket Launch Spotter (RLS) application, each array consists of four microphones and processing equipment. Analyzing the time delays between an acoustic wavefront’s interaction with each microphone in the array UTAMS provides an azimuth of origin. The azimuth from each tower is reported to the UTAMS processor at the control station, and a POO is triangulated and displayed. The UTAMS subsystem can also detect and locate the point of impact (POI), but, due to the difference between the speeds of sound and light, it may take UTAMS as long as 30 seconds to determine the POO for a rocket launch 13 km away. In this application, the electro-optical component of RLS will detect the rocket POO earlier, while UTAMS may do better with the mortar prediction.\n\nModern hydrophones convert sound to electrical energy, which then can undergo additional signal processing, or that can be transmitted immediately to a receiving station. They may be directional or omnidirectional.\n\nNavies use a variety of acoustic systems, especially passive, in antisubmarine warfare, both tactical and strategic. For tactical use, passive hydrophones, both on ships and airdropped sonobuoys, are used extensively in antisubmarine warfare. They can detect targets far further away than with active sonar, but generally will not have the precision location of active sonar, approximating it with a technique called Target Motion Analysis (TMA). Passive sonar has the advantage of not revealing the position of the sensor.\n\nThe Integrated Undersea Surveillance System (IUSS) consists of multiple subsystems in SOSUS, Fixed Distributed System (FDS), and the Advanced Deployable System (ADS or SURTASS). Reducing the emphasis on Cold War blue-water operations put SOSUS, with more flexible \"tuna boat\" sensing vessels called SURTASS being the primary blue-water long-range sensors\nSURTASS used longer, more sensitive towed passive acoustic arrays than could be deployed from maneuvering vessels, such as submarines and destroyers. \n\nSURTASS is now being complemented by Low Frequency Active (LFA) sonar; see the sonar section.\n\nPassive sonobuoys, such as the AN/SSQ-53F, can be directional or omnidirectional and can be set to sink to a specific depth. These would be dropped from helicopters and maritime patrol aircraft such as the P-3.\n\nThe US installed massive Fixed Surveillance System (FSS, also known as SOSUS) hydrophone arrays on the ocean floor, to track Soviet and other submarines.\n\nPurely from the standpoint of detection, towed hydrophone arrays offer a long baseline and exceptional measurement capability. Towed arrays, however, are not always feasible, because when deployed, their performance can suffer, or they can suffer outright damage, from fast speeds or radical turns. A state-of-the-art British towed array, with both passive and active capabilities, is Sonar 2087 made by Thales Underwater Systems.\n\nSteerable sonar arrays on the hull or bow usually have a passive as well as active mode, as do variable-depth sonars\n\nSurface ships may have warning receivers to detect hostile sonar.\n\nModern submarines have multiple passive hydrophone systems, such as a steerable array in a bow dome, fixed sensors along the sides of the submarines, and towed arrays. They also have specialized acoustic receivers, analogous to radar warning receivers, to alert the crew to the use of active sonar against their submarine.\n\nUS submarines made extensive clandestine patrols to measure the signatures of Soviet submarines and surface vessels. This acoustic MASINT mission included both routine patrols of attack submarines, and submarines sent to capture the signature of a specific vessel. US antisubmarine technicians on air, surface, and subsurface platforms had extensive libraries of vessel acoustic signatures.\n\nPassive acoustic sensors can detect aircraft flying low over the sea.\n\nVietnam-era acoustic MASINT sensors included \"Acoubuoy (36 inches long, 26 pounds) floated down by camouflaged parachute and caught in the trees, where it hung to listen. The Spikebuoy (66 inches long, 40 pounds) planted itself in the ground like a lawn dart. Only the antenna, which looked like the stalks of weeds, was left showing above ground.\"\nThis was part of Operation Igloo White.\n\nPart of the AN/GSQ-187 Improved Remote Battlefield Sensor System (I-REMBASS) is a passive acoustic sensor, which, with other MASINT sensors, detects vehicles and personnel on a battlefield. Passive acoustic sensors provide additional measurements that can be compared with signatures, and used to complement other sensors. I-REMBASS control will integrate, in approximately 2008, with the .\n\nFor example, a ground search radar may not be able to differentiate between a tank and a truck moving at the same speed. Adding acoustic information, however, may quickly distinguish between them.\n\nCombatant vessels, of course, made extensive use of active sonar, which is yet another acoustic MASINT sensor. Besides the obvious application in antisubmarine warfare, specialized active acoustic systems have roles in:\n\nVarious synthetic aperture sonars have been built in the laboratory and some have entered use in mine-hunting and search systems. An explanation of their operation is given in synthetic aperture sonar.\n\nThe water surface and bottom are reflecting and scattering boundaries. Large schools of fish, with air in their swim bladder balance apparatus, can also have a significant effect on acoustic propagation.\n\nFor many purposes, but not all naval tactical applications, the sea-air surface can be thought of as a perfect reflector. \"The effects of the seafloor and the sea surface on acoustic systems in shallow water are highly complex, making range predictions difficult. Multi-path degradation affects overall figure of merit and active classification. As a result, false target identifications are frequent.\"\n\nThe acoustic impedance mismatch between water and the bottom is generally much less than at the surface and is more complex. It depends on the bottom material types and depth of the layers. Theories have been developed for predicting the sound propagation in the bottom in this case, for example by Biot and by Buckingham.\n\nFor high frequency sonars (above about 1 kHz) or when the sea is rough, some of the incident sound is scattered, and this is taken into account by assigning a reflection coefficient whose magnitude is less than one.\n\nRather than measuring surface effects directly from a ship, radar MASINT, in aircraft or satellites, may give better measurements. These measurements would then be transmitted to the vessel's acoustic signal processor.\n\nA surface covered with ice, of course, is tremendously difference than even storm-driven water. Purely from a collision avoidance and acoustic propagation, a submarine needs to know how close it is to the bottom of ice. Less obvious is the need to know the three-dimensional structure of the ice, because submarines may need to break through it to launch missiles, to raise4 electronic masts, or to surface the boat. Three-dimensional ice information also can tell the submarine captain whether antisubmarine warfare aircraft can detect or attack the boat.\n\nThe state of the art is providing the submarine with a three-dimensional visualization of the ice above: the lowest part (ice keel) and the ice canopy. While sound will propagate differently in ice than liquid water, the ice still needs to be considered as a volume, to understand the nature of reverberations within it.\n\nA typical basic depth measuring device is the US AN/UQN-4A. Both the water surface and bottom are reflecting and scattering boundaries. For many purposes, but not all naval tactical applications, the sea-air surface can be thought of as a perfect reflector. In reality, there are complex interactions of water surface activity, seafloor characteristics, water temperature and salinity, and other factors that make \"...range predictions difficult. Multi-path degradation affects overall figure of merit and active classification. As a result, false target identifications are frequent.\"\n\nThis device, however, does not give information on the characteristics of the bottom. In many respects, commercial fishing and marine scientists have equipment that is perceived as needed for shallow water operation.\n\nA further complication is the presence of wind generated bubbles or fish close to the sea surface.\n. The bubbles can also form plumes that absorb some of the incident and scattered sound, and scatter some of the sound themselves.\n\nThis problem is distinct from biologic interference caused by acoustic energy generated by marine life, such as the squeaks of porpoises and other cetaceans, and measured by acoustic receivers. The signatures of biologic sound generators need to be differentiated from more deadly denizens of the depths. Classifying biologics is a very good example of an acoustic MASINT process.\n\nModern surface combatants with an ASW mission will have a variety of active systems, with a hull- or bow-mounted array, protected from water by a rubber dome; a \"variable-depth\" dipping sonar on a cable, and, especially on smaller vessels, a fixed acoustic generator and receiver.\n\nSome, but not all, vessels carry passive towed arrays, or combined active-passive arrays. These depend on target noise, which, in the combined littoral environment of ultraquiet submarines in the presence of much ambient noise. Vessels that have deployed towed arrays cannot make radical course maneuvers. Especially when active capabilities are included, the array can be treated as a bistatic or multistatic sensor, and act as a synthetic aperture sonar (SAS)\n\nFor ships that cooperate with aircraft, they will need a data link to sonobuoys and a sonobuoy signal processor, unless the aircraft has extensive processing capability and can send information that can be accepted directly by tactical computers and displays.\n\nSignal processors not only analyze the signals, but constantly track propagation conditions. The former is usually considered part of a particular sonar, but the US Navy has a separate propagation predictor called the AN/UYQ-25B(V) Sonar \"in situ\" Mode Assessment System (SIMAS)\nEcho Tracker Classifiers (ETC) are adjuncts, with a clear MASINT flavor, to existing surface ship sonars\nETC is an application of synthetic aperture sonar (SAS). SAS is already used for minehunting, but could help existing surface combatants, as well as future vessels and unmanned surface vehicles (USV), detect threats, such as very silent air-independent propulsion non-nuclear submarines, outside torpedo range. Torpedo range, especially in shallow water, is considered anything greater than 10 nmi.\n\nConventional active sonar may be more effective than towed arrays, but the small size of modern littoral submarines makes them difficult threats. Highly variable bottom paths, biologics, and other factors complicate sonar detection. If the target is slow-moving or waiting on the bottom, they have little or no Doppler effect, which current sonars use to recognize threats.\n\nContinual active tracking measurement of all acoustically detected objects, with recognition of signatures as deviations from ambient noise, still gives a high false alarm rate (FAR) with conventional sonar. SAS processing, however, improves the resolution, especially of azimuth measurements, by assembling the data from multiple pings into a synthetic beam that gives the effect of a far larger receiver.\n\nMASINT-oriented SAS measures shape characteristics and eliminates acoustically detected objects that do not conform to the signature of threats. Shape recognition is only one of the parts of the signature, which include course and Doppler when available.\n\nActive sonobuoys, containing a sonar transmitter and receiver, can be dropped from fixed-wing maritime patrol aircraft (e.g., P-3, Nimrod, Chinese Y-8, Russian and Indian Bear ASW variants), antisubmarine helicopters, and carrier-based antisubmarine aircraft (e.g., S-3). While there have been some efforts to use other aircraft simply as carriers of sonobuoys, the general assumption is that the sonobuoy-carrying aircraft can issue commands to the sonobuoys and receive, and to some extent process, their signals.\n\nThe Directional Hydrophone Command Activated Sonobuoy system (DICASS) both generate sound and listen for it. A typical modern active sonobuoy, such as the AN/SSQ 963D, generates multiple acoustic frequencies \n. Other active sonobuoys, such as the AN/SSQ 110B, generate small explosions as acoustic energy sources.\n\nAntisubmarine helicopters can carry a \"dipping\" sonar head at the end of a cable, which the helicopter can raise from or lower into the water. The helicopter would typically dip the sonar when trying to localize a target submarine, usually in cooperation with other ASW platforms or with sonobuoys. Typically, the helicopter would raise the head after dropping an ASW weapon, to avoid damaging the sensitive receiver. Not all variants of the same basic helicopter, even assigned to ASW, carry dipping sonar; some may trade the weight of the sonar for more sonobuoy or weapon capacity.\n\nThe EH101 helicopter, used by a number of nations, has a variety of dipping sonars. The (British) Royal Navy version has Ferranti/Thomson-CSF (now Thales) sonar, while the Italian version uses the HELRAS. Russian Ka-25 helicopters carry dipping sonar, as does the US LAMPS, US MH-60R helicopter, which carries the Thales AQS-22 dipping sonar. The older SH-60F helicopter carries the AQS-13F dipping sonar.\n\nNewer Low-Frequency Active (LFA) systems are controversial, as their very high sound pressures may be hazardous to whales and other marine life \nA decision has been made to employ LFA on SURTASS vessels, after an environmental impact statement that indicated, if LFA is used with decreased power levels in certain high-risk areas for marine life, it would be safe when employed from a moving ship. The ship motion, and the variability of the LFA signal, would limit the exposure to individual sea animals. LFA operates in the low-frequency (LF) acoustic band of 100–500 Hz. It has an active component, the LFA proper, and the passive SURTASS hydrophone array. \"The active component of the system, LFA, is a set of 18 LF acoustic transmitting source elements (called projectors) suspended by cable from underneath an oceanographic surveillance vessel, such as the Research Vessel (R/V) Cory Chouest, USNS Impeccable (T-AGOS 23), and the Victorious class (TAGOS 19 class).\n\n\"The source level of an individual projector is 215 dB. These projectors produce the active sonar signal or “ping.” A \"ping,\" or transmission, can last between 6 and 100 seconds. The time between transmissions is typically 6 to 15 minutes with an average transmission of 60 seconds. Average duty cycle (ratio of sound “on” time to total time) is less than 20 percent. The typical duty cycle, based on historical LFA operational parameters (2003 to 2007), is normally 7.5 to 10 percent.\"\n\nThis signal \"...is not a continuous tone, but rather a transmission of waveforms that vary in frequency and duration. The duration of each continuous frequency sound transmission is normally 10 seconds or less. The signals are loud at the source, but levels diminish rapidly over the first kilometer.\"\n\nThe primary tactical active sonar of a submarine is usually in the bow, covered with a protective dome. Submarines for blue-water operations used active systems such as the AN/SQS-26 and AN/SQS-53 have been developed but were generally designed for convergence zone and single bottom bounce environments.\n\nSubmarines that operate in the Arctic also have specialized sonar for under-ice operation; think of an upside-down fathometer.\n\nSubmarines also may have minehunting sonar. Using measurements to differentiate between biologic signatures and signatures of objects that will permanently sink the submarine is as critical a MASINT application as could be imagined.\n\nSonars optimized to detect objects of the size and shapes of mines can be carried by submarines, remotely operated vehicles, surface vessels (often on a boom or cable) and specialized helicopters.\n\nThe classic emphasis on minesweeping, and detonating the mine released from its tether using gunfire, has been replaced with the AN/SLQ-48(V)2 mine neutralization system (MNS)AN/SLQ-48 - (remotely operated) Mine Neutralization Vehicle. This works well for rendering save mines in deep water, by placing explosive charges on the mine and/or its tether. The AN/SLQ-48 is not well suited to the neutralization of shallow-water mines. The vehicle tends to be underpowered and may leave on the bottom a mine that looks like a mine to any subsequent sonar search and an explosive charge subject to later detonation under proper impact conditions.\n\nThere is mine-hunting sonar, as well as (electro-optical) television on the ROV, and AN/SQQ-32 minehunting sonar on the ship.\n\nAn assortment of time-synchronized sensors can characterize conventional or nuclear explosions. One pilot study, the Active Radio Interferometer for Explosion Surveillance (ARIES). This technique implements an operational system for monitoring ionospheric pressure waves resulting from surface or atmospheric nuclear or chemical explosives. Explosions produce pressure waves that can be detected by measuring phase variations between signals generated by ground stations along two different paths to a satellite. This is a very modernized version, on a larger scale, of World War I sound ranging.\n\nAs can many sensors, ARIES can be used for additional purposes. Collaborations are being pursued with the Space Forecast Center to use ARIES data for total electron content measures on a global scale, and with the meteorology/global environment community to monitor global climate change (via tropospheric water vapor content measurements), and by the general ionospheric physics community to study travelling ionospheric disturbances.\n\nSensors relatively close to a nuclear event, or a high-explosive test simulating a nuclear event, can detect, using acoustic methods, the pressure produced by the blast. These include infrasound microbarographs (acoustic pressure sensors) that detect very low-frequency sound waves in the atmosphere produced by natural and man-made events.\n\nClosely related to the microbarographs, but detecting pressure waves in water, are hydro-acoustic sensors, both underwater microphones and specialized seismic sensors that detect the motion of islands.\n\nUS Army Field Manual 2-0 defines seismic intelligence as \"The passive collection and measurement of seismic waves or vibrations in the earth surface.\" One strategic application of seismic intelligence makes use of the science of seismology to locate and characterize nuclear testing, especially underground testing. Seismic sensors also can characterize large conventional explosions that are used in testing the high-explosive components of nuclear weapons. Seismic intelligence also can help locate such things as large underground construction projects.\n\nSince many areas of the world have a great deal of natural seismic activity, seismic MASINT is one of the emphatic arguments that there must be a long-term commitment to measuring, even during peacetime, so that the signatures of natural behavior is known before it is necessary to search for variations from signatures.\n\nFor nuclear test detection, seismic intelligence is limited by the \"threshold principle\" coined in 1960 by George Kistiakowsky, which recognized that while detection technology would continue to improve, there would be a threshold below which small explosions could not be detected.\n\nThe most common sensor in the Vietnam-era \"McNamara Line\" of remote sensors was the ADSID (Air-Delivered Seismic Intrusion Detector) sensed earth motion to detect people and vehicles. It resembled the Spikebuoy, except it was smaller and lighter (31 inches long, 25 pounds).\nThe challenge for the seismic sensors (and for the analysts) was not so much in detecting the people and the trucks as it was in separating out the false alarms generated by wind, thunder, rain, earth tremors, and animals—especially frogs.\"\n\nThis subdiscipline is also called piezoelectric MASINT after the sensor most often used to sense vibration, but vibration detectors need not be piezoelectric. Note that some discussions treat seismic and vibration sensors as a subset of acoustic MASINT. Other possible detectors could be moving coil or surface acoustic wave.\n. Vibration, as a form of geophysical energy to be sensed, has similarities to acoustic and seismic MASINT, but also has distinct differences that make it useful, especially in unattended ground sensors (UGS). In the UGS application, one advantage of a piezoelectric sensor is that it generates electricity when triggered, rather than consuming electricity, an important consideration for remote sensors whose lifetime may be determined by their battery capacity.\n\nWhile acoustic signals at sea travel through water, on land, they can be assumed to come through the air. Vibration, however, is conducted through a solid medium on land. It has a higher frequency than is typical of seismic conducted signals.\n\nA typical detector, the Thales MA2772 vibration is a piezoelectric cable, shallowly buried below the ground surface, and extended for 750 meters. Two variants are available, a high-sensitivity version for personnel detection, and lower-sensitivity version to detect vehicles. Using two or more sensors will determine the direction of travel, from the sequence in which the sensors trigger.\n\nIn addition to being buried, piezoelectric vibration detectors, in a cable form factor, also are used as part of high-security fencing. They can be embedded in walls or other structures that need protection.\n\nA magnetometer is a scientific instrument used to measure the strength and/or direction of the magnetic field in the vicinity of the instrument. The measurements they make can be compared to signatures of vehicles on land, submarines underwater, and atmospheric radio propagation conditions. They come in two basic types: \n\nEarth's magnetism varies from place to place and differences in the Earth's magnetic field (the magnetosphere) can be caused by two things:\n\nMetal detectors use electromagnetic induction to detect metal. They can also determine the changes in existing magnetic fields caused by metallic objects.\n\nOne of the first means for detecting submerged submarines, first installed by the Royal Navy in 1914, was the effect of their passage over an anti-submarine indicator loop on the bottom of a body of water. A metal object passing over it, such as a submarine, will, even if degaussed, have enough magnetic properties to induce a current in the loop's cable. . In this case, the motion of the metal submarine across the indicating coil acts as an oscillator, producing electric current.\n\nA magnetic anomaly detector (MAD) is an instrument used to detect minute variations in the Earth's magnetic field. The term refers specifically to magnetometers used either by military forces to detect submarines (a mass of ferromagnetic material creates a detectable disturbance in the magnetic field)Magnetic anomaly detectors were first employed to detect submarines during World War II. MAD gear was used by both Japanese and U.S. anti-submarine forces, either towed by ship or mounted in aircraft to detect shallow submerged enemy submarines. After the war, the U.S. Navy continued to develop MAD gear as a parallel development with sonar detection technologies.\n\nTo reduce interference from electrical equipment or metal in the fuselage of the aircraft, the MAD sensor is placed at the end of a boom or a towed aerodynamic device. Even so, the submarine must be very near the aircraft's position and close to the sea surface for detection of the change or anomaly. The detection range is normally related to the distance between the sensor and the submarine. The size of the submarine and its hull composition determine the detection range. MAD devices are usually mounted on aircraft \n\nThere is some misunderstanding of the mechanism of detection of submarines in water using the MAD boom system. Magnetic moment displacement is ostensibly the main disturbance, yet submarines are detectable even when oriented parallel to the Earth's magnetic field, despite construction with non-ferromagnetic hulls.\n\nFor example, the Soviet-Russian Alfa class submarine, was constructed out of titanium. This light, strong material, as well as a unique nuclear power system, allowed the submarine to break speed and depth records for operational boats. It was thought that nonferrous titanium would defeat magnetic ASW sensors, but this was not the case. to give dramatic submerged performance and protection from detection by MAD sensors, is still detectable.\n\nSince titanium structures are detectable, MAD sensors do not directly detect deviations in the Earth's magnetic field. Instead, they may be described as long-range electric and electromagnetic field detector arrays of great sensitivity.\n\nAn electric field is set up in conductors experiencing a variation in physical environmental conditions, providing that they are contiguous and possess sufficient mass. Particularly in submarine hulls, there is a measurable temperature difference between the bottom and top of the hull producing a related salinity difference, as salinity is affected by temperature of water. The difference in salinity creates an electric potential across the hull. An electric current then flows through the hull, between the laminae of sea-water separated by depth and temperature. The resulting dynamic electric field produces an electromagnetic field of its own, and thus even a titanium hull will be detectable on a MAD scope, as will a surface ship for the same reason.\n\nThe Remotely Emplaced Battlefield Surveillance System (REMBASS) is a US Army program for detecting the presence, speed, and direction of a ferrous object, such as a tank. Coupled with acoustic sensors that recognize the sound signature of a tank, it could offer high accuracy. It also collects weather information.\n\nThe Army's AN/GSQ-187 Improved Remote Battlefield Sensor System (I-REMBASS) includes both magnetic-only and combined passive infrared/magnetic intrusion detectors. The DT-561/GSQ hand emplaced MAG \"sensor detects vehicles (tracked or wheeled) and personnel carrying ferrous metal. It also provides information on which to base a count of objects passing through its detection zone and reports their direction of travel relative to its location. The monitor uses two different (MAG and IR) sensors and their identification codes to determine direction of travel.\n\nMagnetic sensors, much more sophisticated than the early inductive loops, can trigger the explosion of mines or torpedoes. Early in World War II, the US tried to put magnetic torpedo exploder far beyond the limits of the technology of the time, and had to disable it, and then work on also-unreliable contact fuzing, to make torpedoes more than blunt objects than banged into hulls.\n\nSince water is incompressible, an explosion under the keel of a vessel is far more destructive than one at the air-water interface. Torpedo and mine designers want to place the explosions in that vulnerable spot, and countermeasures designers want to hide the magnetic signature of a vessel. Signature is especially relevant here, as mines may be made selective for warships, merchant vessel unlikely to be hardened against underwater explosions, or submarines.\n\nA basic countermeasure, started in World War II, was degaussing, but it is impossible to remove all magnetic properties.\n\nLandmines often contain enough ferrous metal to be detectable with appropriate magnetic sensors. Sophisticated mines, however, may also sense a metal-detection oscillator, and, under preprogrammed conditions, detonate to deter demining personnel.\n\nNot all landmines have enough metal to activate a magnetic detector. While, unfortunately, the greatest number of unmapped minefields are in parts of the world that cannot afford high technology, a variety of MASINT sensors could help demining. These would include ground-mapping radar, thermal and multispectral imaging, and perhaps synthetic aperture radar to detect disturbed soil.\n\nGravity is a function of mass. While the average value of Earth's surface gravity is approximately 9.8 meters per second squared, given sufficiently sensitive instrumentation, it is possible to detect local variations in gravity from the different densities of natural materials: the value of gravity will be greater on top of a granite monolith than over a sand beach. Again with sufficiently sensitive instrumentation, it should be possible to detect gravitational differences between solid rock, and rock excavated for a hidden facility.\n\nStreland 2003 points out that the instrumentation indeed must be sensitive: variations of the force of gravity on the earth’s surface are on the order of 10 of the average value. A practical gravitimetric detector of buried facilities would need to be able to measure \"less than one one millionth of the force that caused the apple to fall on Sir Isaac Newton’s head.\" To be practical, it would be necessary for the sensor to be able to be used while in motion, measuring the change in gravity between locations. This change over distance is called the \"gravity gradient\", which can be measured with a gravity gradiometer.\n\nDeveloping an operationally useful gravity gradiometer is a major technical challenge. One type, the SQUID Superconducting Quantum Interference Device gradiometer, may have adequate sensitivity, but it needs extreme cryogenic cooling, even if in space, a logistic nightmare. Another technique, far more operationally practical but lacking the necessary sensitivity, is the Gravity Recovery and Climate Experiment (GRACE) technique, currently using radar to measure the distance between pairs of satellites, whose orbits will change based on gravity. Substituting lasers for radar will make GRACE more sensitive, but probably not sensitive enough.\n\nA more promising technique, although still in the laboratory, is quantum gradiometry, which is an extension of atomic clock techniques, much like those in GPS. Off-the-shelf atomic clocks measure changes in atomic waves over time rather than the spatial changes measured in a quantum gravity gradiometer. One advantage of using GRACE in satellites is that measurements can be made from a number of points over time, with a resulting improvement as seen in synthetic aperture radar and sonar. Still, finding deeply buried structures of human scale is a tougher problem than the initial goals of finding mineral deposits and ocean currents.\n\nTo make this operationally feasible, there would have to be a launcher to put fairly heavy satellites into polar orbits, and as many earth stations as possible to reduce the need for large on-board storage of the large amounts of data the sensors will produce. Finally, there needs to be a way to convert the measurements into a form that can be compared against available signatures in geodetic data bases. Those data bases would need significant improvement, from measured data, to become sufficiently precise that a buried facility signature would stand out.\n"}
{"id": "11543687", "url": "https://en.wikipedia.org/wiki?curid=11543687", "title": "Grain cradle", "text": "Grain cradle\n\nA grain cradle or \"cradle\", is a modification to a standard scythe to keep the cut grain stems aligned. The cradle scythe has an additional arrangement of fingers attached to the snaith (snath or snathe) to catch the cut grain so that it can be cleanly laid down in a row with the grain heads aligned for collection and efficient threshing.\n\nAs the cultivation of grain developed, the seasonal harvest became a major agricultural event. Grain could be pulled or, later, cut with a sickle and tied into sheaves to be threshed. The scythe improved on the sickle by giving the mower a more ergonomic stance and permitting a larger blade. However, keeping the grain stems aligned in the windrow required great skill and where these skills were less available the addition of a cradle helped to manage the grain heads, reducing the sheaver's work-load and improving efficiency at threshing. Lesser skilled mowers could harvest significantly more grain by using the cradle. Although the grain cradle was in previous use in parts of Europe it was not generally used because skilled labour was traditionally available. Between 1800 to 1840 the cradle was widely adopted in the expanding grain growing area of Midwestern United States, undergoing some refinement there and resulting in the American-pattern cradle. Fifty American patents were issued between 1823 and 1930, the first in 1823 in western New York state and the last in 1924 in West Virginia peaking between 1875 and 1900..\nHay does not require aligning and the scythe is more efficient without a cradle, so it was removed for haymaking.\n\nThe cradle was commonly used throughout the 1800s and into the beginning of the 20th century, in part because many of the smaller farms were not designed for mechanical reaping and in part because there were still a great number of smaller farms where the mechanical reaper was not economical. However, by the end of the 19th century the cradle had been generally replaced by the mechanical reaper, a horse-drawn (or tractor-drawn) machine patented by Cyrus McCormick in 1834, and later by other mechanical methods of harvesting such as the combine harvester.\n"}
{"id": "14517537", "url": "https://en.wikipedia.org/wiki?curid=14517537", "title": "Hand sack", "text": "Hand sack\n\nA hand sack is any sack or similar object, typically made of cloth and filled with sand or a similar material, that is tossed on the back of the hand in a variety of games.\n\nHand sack originates from the lighter game, a game often played on college campuses. Lighter game founded in the greater Burlington Vermont area in the early 90's which spread to the music festival scene and on to college campuses nationwide.\n\n\n"}
{"id": "34494017", "url": "https://en.wikipedia.org/wiki?curid=34494017", "title": "Hydraulic fracturing proppants", "text": "Hydraulic fracturing proppants\n\nA proppant is a solid material, typically sand, treated sand or man-made ceramic materials, designed to keep an induced hydraulic fracture open, during or following a fracturing treatment. It is added to a \"fracking fluid\" which may vary in composition depending on the type of fracturing used, and can be gel, foam or slickwater–based. In addition, there may be unconventional fracking fluids. Fluids make tradeoffs in such material properties as viscosity, where more viscous fluids can carry more concentrated proppant; the energy or pressure demands to maintain a certain flux pump rate (flow velocity) that will conduct the proppant appropriately; pH, various rheological factors, among others. In addition, fluids may be used in low-volume well stimulation of high-permeability sandstone wells (20k to 80k gallons per well) to the high-volume operations such as shale gas and tight gas that use millions of gallons of water per well.\n\nConventional wisdom has often vacillated about the relative superiority of gel, foam and slickwater fluids with respect to each other, which is in turn related to proppant choice. For example, Zuber, Kuskraa and Sawyer (1988) found that gel-based fluids seemed to achieve the best results for coalbed methane operations, but as of 2012, slickwater treatments are more popular.\n\nOther than proppant, slickwater fracturing fluids are mostly water, generally 99% or more by volume, but gel-based fluids can see polymers and surfactants comprising as much as 7 vol% , ignoring other additives. Other common additives include hydrochloric acid (low pH can etch certain rocks, dissolving limestone for instance), friction reducers, guar gum, biocides, emulsion breakers, emulsifiers, 2-butoxyethanol, and radioactive tracer isotopes.\n\nProppants used should be permeable or permittive to gas under high pressures; the interstitial space between particles should be sufficiently large, yet have the mechanical strength to withstand closure stresses to hold fractures open after the fracturing pressure is withdrawn. Large mesh proppants have greater permeability than small mesh proppants at low closure stresses, but will mechanically fail (i.e. get crushed) and produce very fine particulates (\"fines\") at high closure stresses such that smaller-mesh proppants overtake large-mesh proppants in permeability after a certain threshold stress.\n\nThough sand is a common proppant, untreated sand is prone to significant fines generation; fines generation is often measured in wt% of initial feed. A commercial newsletter from Momentive cites untreated sand fines production to be 23.9% compared with 8.2% for lightweight ceramic and 0.5% for their product. One way to maintain an ideal mesh size (i.e. permeability) while having sufficient strength is to choose proppants of sufficient strength; sand might be coated with resin,to form CRCS (Curable Resin Coated Sand) or PRCS (Pre-Cured Resin Coated Sands). In certain situations a different proppant material might be chosen altogether—popular alternatives include ceramics and sintered bauxite.\n\nIncreased strength often comes at a cost of increased density, which in turn demands higher flow rates, viscosities or pressures during fracturing, which translates to increased fracturing costs, both environmentally and economically. Lightweight proppants conversely are designed to be lighter than sand (~2.5 g/cm) and thus allow pumping at lower pressures or fluid velocities. Light proppants are less likely to settle. Porous materials can break the strength-density trend, or even afford greater gas permeability. Proppant geometry is also important; certain shapes or forms amplify stress on proppant particles making them especially vulnerable to crushing (a sharp discontinuity can classically allow infinite stresses in linear elastic materials).\n\nProppant mesh size also affects fracture length: proppants can be \"bridged out\" if the fracture width decreases to less than twice the size of the diameter of the proppant. As proppants are deposited in a fracture, proppants can resist further fluid flow or the flow of other proppants, inhibiting further growth of the fracture. In addition, closure stresses (once external fluid pressure is released) may cause proppants to reorganise or \"squeeze out\" proppants, even if no fines are generated, resulting in smaller effective width of the fracture and decreased permeability. Some companies try to cause weak bonding at rest between proppant particles in order to prevent such reorganisation. The modelling of fluid dynamics and rheology of fracturing fluid and its carried proppants is a subject of active research by the industry.\n\nThough good proppant choice positively impacts output rate and overall ultimate recovery of a well, commercial proppants are also constrained by cost. Transport costs from supplier to site form a significant component of the cost of proppants.\n\nOther than proppant, slickwater fracturing fluids are mostly water, generally 99% or more by volume, but gel-based fluids can see polymers and surfactants comprising as much as 7 vol% , ignoring other additives. Other common additives include hydrochloric acid (low pH can etch certain rocks, dissolving limestone for instance), friction reducers, guar gum, biocides, emulsion breakers, emulsifiers, and 2-Butoxyethanol.\n\nRadioactive tracer isotopes are sometimes included in the hydrofracturing fluid to determine the injection profile and location of fractures created by hydraulic fracturing. Patents describe in detail how several tracers are typically used in the same well. Wells are hydraulically fractured in different stages. Tracers with different half-lives are used for each stage. Their half-lives range from 40.2 hours (lanthanum-140) to 5.27 years (cobalt-60). Amounts per injection of radionuclide are listed in The US Nuclear Regulatory Commission (NRC) guidelines. The NRC guidelines also list a wide range or radioactive materials in solid, liquid and gaseous forms that are used as field flood or enhanced oil and gas recovery study applications tracers used in single and multiple wells.\n\nIn the US, except for diesel-based additive fracturing fluids, noted by the American Environmental Protection Agency to have a higher proportion of volatile organic compounds and carcinogenic BTEX, use of fracturing fluids in hydraulic fracturing operations was explicitly excluded from regulation under the American Clean Water Act in 2005, a legislative move that has since attracted controversy for being the product of special interests lobbying.\n\n"}
{"id": "4642616", "url": "https://en.wikipedia.org/wiki?curid=4642616", "title": "Lotion", "text": "Lotion\n\nA lotion is a low-viscosity topical preparation intended for application to unbroken skin. By contrast, creams and gels have higher viscosity.\n\nLotions are applied to external skin with bare hands, a brush, a clean cloth, cotton wool, or gauze. While lotion may be used as a medicine delivery system, many lotions, especially hand lotions and body lotions are meant instead to simply smooth, moisturize and soften the skin. These may be used in anti-aging lotions, which can also be classified as a cosmetic in many cases, and may contain fragrances. The Food and Drug Administration voiced concern about lotions not classified as drugs that advertise anti-aging or anti-wrinkle properties.\n\nDermatologists can prescribe lotions to treat or prevent skin diseases. It is not unusual for the same drug ingredient to be formulated into a lotion, cream and ointment. Creams are the most convenient of the three but are inappropriate for application to regions of hairy skin such as the scalp, while a lotion is less viscous and may be readily applied to these areas (many medicated shampoos are in fact lotions). Historically, lotions also had an advantage in that they may be spread thinly compared to a cream or ointment and may economically cover a large area of skin, but product research has steadily eroded this distinction. Non-comedogenic lotions are recommended for use on acne prone skin.\n\nLotions can be used for the delivery to the skin of medications such as:\n\nSince health care workers must wash their hands frequently to prevent disease transmission, hospital grade lotion is recommended to prevent skin dermatitis caused by frequent exposure to cleaning agents in the soap. A 2006 study found that application of hospital grade lotion after hand washing significantly reduced skin roughness and dryness.\n\nCare must be taken not to use consumer lotions in a hospital environment, as the perfumes and allergens may be a danger to those who are immunodeficient.\n\nMost lotions are oil-in-water emulsions using a substance such as cetearyl alcohol to keep the emulsion together, but water-in-oil lotions are also formulated. The key components of a skin care lotion, cream or gel emulsion (that is mixtures of oil and water) are the and oily phases, an emulgent to prevent separation of these two phases, and, if used, the drug substance or substances. A wide variety of other ingredients such as fragrances, glycerol, petroleum jelly, dyes, preservatives, proteins and stabilizing agents are commonly added to lotions.\n\nSince thickness and consistency are key factors in lotions and creams, it is important to understand the manufacturing process that determines viscosity.\n\nManufacturing lotions and creams can be completed in two cycles:\n\nA typical oil-in-water manufacturing process might go like this:\n\nCareful note should be taken in choosing the right mixing equipment for lotion manufacturing to avoid agglomerates and long processing times. It can make all the difference in manufacturing time and costs. Conventional agitators can present a number of problems including agglomerates and longer processing times. On the other hand, high shear in-line mixers can produce quality lotions and creams without many of the complications encountered with conventional mixers. Sonolation is also a process that is growing in popularity.\n\nDepending on their composition, lotions can be comedogenic, meaning that they can result in the increased formation of comedones. Sufferers of acne, or those who are predisposed to forming comedones, should look for formulations that are designed to be noncomedogenic.\n\nAll topical products, including lotions, can result in the percutaneous absorption of their contents. Though this has limited use as a route of drug administration, it more commonly results in unintended, and often undesirable, consequences. For example, medicated lotions such as Diprolene are often used with the intention of exerting only local effects, but absorption of the drug through the skin can occur to a small degree, resulting in systemic side effects such as hyperglycemia and glycosuria. Absorption is increased when lotions are applied and then covered with an occlusive layer, when they are applied to large areas of the body, or when they are applied to damaged or broken skin.\n\nA 2015 study funded by the California Breast Cancer Research Program found that parabens, a common ingredient in cosmetic lotions, stimulate breast cancer cell proliferation.\n\nThere is currently no regulation over use of the term \"hypoallergenic\", and even pediatric skin products with the label were found to still contain allergens. Those with eczema are especially vulnerable to an allergic reaction with lotion, as their compromised skin barrier allows preservatives to bind with and activate immune cells.\n\nThe American Academy of Allergy, Asthma, and Immunology warns that natural lotion containing ingredients commonly found in food (such as goats milk, cow's milk, coconut milk, or oil) may introduce new allergies, and an allergic reaction when those foods are later consumed.\n\nLotions are mainly intended to help the skin, but can also harm your skin. Christina Marino, who practices at Johns Hopkins Community Physicians, has conducted much research on this aspect. Moisturizers contain ingredients that are either occlusive or humectant. Occlusive agents are used to help block the loss of water from the skin. Humectant agents are used to attract water to the skin. Significant water exposure to the skin can cause the loss of soluble natural factors. Persistent moisturization to the skin from exposure to water may contribute to an allergic reaction or irritant contact dermatitis, and can result in penetration of foreign objects. Changes in the skin's normal ecological environment, in or on the skin, can support the overgrowth of pathological organisms. Lotions contain 65-85% of water. Water acts as an agent to disperse the active and inactive ingredients in the lotion. A high water content also serves as a way for the absorption of some components and evaporation of the moisturizer. Water acts as a temporary hydration agent.\n\n\n"}
{"id": "11571153", "url": "https://en.wikipedia.org/wiki?curid=11571153", "title": "Low-level windshear alert system", "text": "Low-level windshear alert system\n\nA low-level windshear alert system (LLWAS) measures average surface wind speed and direction using a network of remote sensor stations, situated near runways and along approach or departure corridors at an airport. Wind shear is the generic term for wind differences over an operationally short distance (in relation to flight) which encompass meteorological phenomena including gust fronts, microbursts, vertical shear, and derechos.\n\nLLWAS compares results over its operating area to determine whether calm, steady winds, wind shifts (in relation to runways), wind gusts, divergent winds, sustained divergent winds (indicative of shear), or strong and sustained divergent winds (indicative of microbursts) are observed. A LLWAS master station polls each remote station every system cycle (nominally every ten seconds) and provides prevailing airport wind averages, runway specific winds, gusts, may set new wind shear alerts or microburst alerts and reset countdown timers of elapsed time since the last alert. By airline rules, pilots must avoid microbursts if warnings are issued by an automated wind shear detection system, and must wait until a safe time interval passes, to assure departure or landing conditions are safe for the performance of the airframe. Pilots may decide whether to land (or conduct a missed approach) after wind shear alerts are issued. LLWAS wind shear alerts are defined as wind speed gain or loss of between 20 and 30 knots aligned with the active runway direction. \"Low level\" refers to altitudes of or less above ground level (AGL). Arriving aircraft on descent, generally within six nautical miles of touchdown will fly within this low level, maintaining a glide slope and may lack recovery altitude sufficient to avoid a stall or flight-into-terrain if caught unaware by a microburst. LLWAS microburst alerts are issued for greater than 30 knot loss of airspeed at the runway or within three nautical miles of approach or two nautical miles of departure. Microbursts in excess of 110 knots have been observed. \n\nEach LLWAS equipped airport may have as few as six or as many as thirty-two remote stations. Each remote station uses a tall pole with anemometer and radio-telecommunication equipment mounted on a lowerable ring. Remote station wind measurements are transmitted to a master station at the Air Traffic Control Tower (ATCT), which polls the remote stations, runs wind shear and gust front algorithms, and generates warnings when windshear or microburst conditions are detected. Current observations and warnings are displayed for approach controllers in the terminal radar approach control facility (TRACON) and for local and ground controllers in the air traffic control tower.\n\nAir traffic controller (ATC) users at local, ground and departure positions in the ATCT relay the LLWAS runway specific alerts to pilots via voice radio communication. Recent wind shear alerts may also feature in radio broadcasts by the automated terminal information system (ATIS). LLWAS wind shear and microburst alerts assist pilots during busy times on final approach and on departure, often when heavy traffic, low ceilings, obstructions to vision, and moderate to heavy precipitation add to the difficulty in determining in just a few seconds whether mounting wind and weather hazards should be risked or avoided.\n\nThe original LLWAS system (LLWAS I) was developed by the Federal Aviation Administration (FAA) in 1976 in response to the 1975 Eastern Air Lines Flight 66 windshear accident in New York. LLWAS I used a center field anemometer along with five pole mounted anemometers sited around the periphery of a single runway. It was installed at 110 FAA towered airports between 1977 and 1987. Windshear was detected using a simple vector difference algorithm, triggering an alarm when the magnitude of the difference vector between the center field anemometer and any of the five remotes exceeded 15 knots. The LLWAS II deployment included software and hardware upgrades to the existing LLWAS I to improve the windshear detection and reduce false alarms. Between 1988 and 1991, all of the LLWAS I systems were upgraded to be LLWAS II compliant. Windshear deployment studies conducted from 1989 through 1994 determined at which LLWAS-II sites weather exposure justified upgrade to a weather radar (Terminal Doppler Weather Radar (TDWR) or Weather Systems Processor (WSP)) an LLWAS Network Expansion (LLWAS-NE) or LLWAS-Relocate/Sustain (LLWAS-RS) upgrade, singly or in combination. By 2005 all LLWAS-II had been decommissioned for one of these replacement wind shear detection systems or for two in combination. \n\nThe LLWAS-NE added the ability to cover more than a single runway, using up to 32 remote stations to provide runway specific alerts for parallel and crossing runways at ten large airports in combination with TDWR. The LLWAS-RS further upgrades service at 40 remaining LLWAS-2 operating sites (not justified for a radar solution) to employ LLWAS-NE algorithms and extend service life by 20 years, in part by adding ultrasonic anemometers with no moving parts. The LLWAS-RS program began in response to the National Transportation Safety Board (NTSB) investigation of the USAir Flight 1016 accident at Charlotte, North Carolina, in 1994. From that accident, a determination was made that LLWAS-II must regain and retain its original capability, often degraded by tree growth and airport construction such as hangars that obstruct or deflect wind near LLWAS remote station sensors.\n\n\n"}
{"id": "957368", "url": "https://en.wikipedia.org/wiki?curid=957368", "title": "MicroMV", "text": "MicroMV\n\nMicroMV is a proprietary videotape format introduced in October 2001 by Sony. It is the smallest videotape format — 70% smaller than MiniDV or about the size of two US quarter coins; it is also smaller than a Digital8 or DV cassette. It was the first helical scan tape system using MR read head introduced to the market. Each cassette can hold up to 60 minutes of video. \n\nThe MicroMV format does not use the \"DV25\" codec used by the highly popular DV & MiniDV videocassette formats. Instead, it uses 12 Mbit/s MPEG-2 compression, like that used for DVDs and HDV. Footage recorded on MicroMV format initially could not be directly edited with mainstream DV editing software such as Adobe Premiere or Apple Final Cut Pro; instead Sony supplied its own video editing software MovieShaker (for Windows PCs only). Later versions of Ulead Video Studio and several freeware applications however could capture and edit from Sony MicroMV Camcorders.\n\nMicroMV has not been a successful format. Sony was the only electronics manufacturer to sell MicroMV cameras. In 2006, Sony stopped offering new MicroMV camcorder models. In November 2015, Sony announced that shipment of MicroMV cassettes would be discontinued in March 2016.\n\n\n"}
{"id": "56855798", "url": "https://en.wikipedia.org/wiki?curid=56855798", "title": "Ministry of Energy (Zambia)", "text": "Ministry of Energy (Zambia)\n\nThe Ministry of Energy is a ministry in Zambia. It is headed by the Minister of Energy.\n\nIn 2012 the Ministry of Water and Energy was merged with the Ministry of Mines to form the Ministry of Mines Energy and Water Development. The merger was reversed in 2015, with the Ministry of Energy and Water Development coming into being. In 2016 Water Development was transferred to the new Ministry of Water Development, Sanitation and Environmental Protection.\n"}
{"id": "19395056", "url": "https://en.wikipedia.org/wiki?curid=19395056", "title": "National States Geographic Information Council", "text": "National States Geographic Information Council\n\nThe National States Geographic Information Council, known as NSGIC, is an organization in the United States of America of the states, the District of Columbia, and the territories that works to improve the use and sharing of geospatial data and GIS tools. The purpose of the organization is \"to encourage effective and efficient government through the coordinated development of geographic information and technologies to ensure that information may be integrated at all levels of government.\"\n\nNSGIC members include state GIS coordinators and senior state GIS managers, representatives of federal agencies, local and county governments, the private sector, the academic sector, and other professional organizations. Among the NSGIC membership are experts, recognized nationally and internationally, in GIS, in IT policy, and in data creation and data management.\n\nNSGIC works to foster the creation of \"intelligent maps and databases that enable public and private decision makers to make better informed and timelier decisions in a wide array of governmental areas.\"\n\nAmong the major focus areas of NSGIC are:\n\nNSGIC members gather in person twice each year; once in early spring for a Mid-Year Meeting and once in the early fall for an Annual Conference. Mid-Year meetings are held in Annapolis, Maryland and include meetings with federal leaders on Capitol Hill. NSGIC Annual Conferences have been held since 1991 and rotate among the states. \n\nThe NSGIC Board of Directors meets monthly by conference call. Several standing committees and special workgroups meet by conference call monthly, or as needed.\n\nThe membership is kept informed via a NSGIC e-mail list, a NSGIC web site, and a NSGIC Blog.\n"}
{"id": "46768483", "url": "https://en.wikipedia.org/wiki?curid=46768483", "title": "Neutral build", "text": "Neutral build\n\nIn software development, a neutral build is a software build that reflects the current state of the source code checked into the source code version control system by the developers, and done in a neutral environment (an environment not used for development).\n\nA nightly build is a neutral build that takes place automatically. These typically take place when no one is likely to be working in the office so that there are no changes to the source code during the build. The results of the build are inspected by the arriving programmers, who generally place a priority on ensuring the recent changes to the source code have not broken the build process or functionality of the software. Nightly builds also ensure that the build tools have not broken due to system updates, and are therefore often run whether any source code has changed or not.\n\nIn contrast, continuous integration environments automatically rebuild the project whenever changes are checked in – often several times a day – and provide more immediate feedback; however, they do not necessarily include nightly builds. As a result, compiler and tool updates may break the ability to compile older projects easily without warning. Nonetheless, CI techniques are considered the more modern approach. CI jobs are often run on isolated virtual machines, and typically include automated testing as well.\n\nWhen someone says a developer \"broke the build\", they are effectively saying that a developer checked in code which might very well have compiled (and hopefully also run properly) in their account, but does not compile (and therefore, cannot be run) in anyone else's account. This is typically due to additional developer-specific changes that were either not checked in, or (in the case of environment variables, etc.) were modifications to systems not under revision control. One of the most common cases is remembering to check in all \"modified\" files, but forgetting to add \"newly created\" files to the repository. If the other developers check out the new code without being aware of the problem, their work may grind to a halt while they wait for the problem to be fixed (or try to fix it themselves, which can be even more problematic, if multiple developers attempt to fix the issue at the same time). This naturally can result in a significant loss of productivity.\n\nNeutral builds are important for software development processes running at high loads with short schedules (see extreme programming, startup). Not having them means that any build that needs to be created for the software quality assurance department will use code that may be in the middle of major modifications, and which is therefore best left out of a build intended for independent validation – particularly a build being evaluated for possible release.\n\nSome obstacles to a reliable neutral build process are:\n\n\nThe following list gives some examples of software that has publicly available nightly and/or neutral builds.\n\n"}
{"id": "44893163", "url": "https://en.wikipedia.org/wiki?curid=44893163", "title": "New Formalism (architecture)", "text": "New Formalism (architecture)\n\nNew Formalism is an architectural style that emerged in the United States during the mid 1950s and flowered in the 1960s. Buildings designed in that style exhibited many Classical elements including \"strict symmetrical elevations\" building proportion and scale, Classical columns, highly stylized entablatures and colonnades. The style was used primarily for high-profile cultural, institutional and civic buildings. They were \"typically constructed using rich materials such as marble, granite or man-made composites and also incorporated certain qualities of concrete that allowed for the creation of distinctive forms such as umbrella shells, waffle slabs and folded plates\".\nEdward Durrell Stone's New Delhi American Embassy (1954), which blended the architecture of the east with modern western concepts, is considered to be the symbolic start of New Formalism architecture.\n\n\"Common features of the New Formalism style, include:\n\n\n"}
{"id": "44856539", "url": "https://en.wikipedia.org/wiki?curid=44856539", "title": "Non linear piezoelectric effects in polar semiconductors", "text": "Non linear piezoelectric effects in polar semiconductors\n\nNon linear piezoelectric effects in polar semiconductors are the manifestation that the strain induced piezoelectric polarization depends not just on the product of the first order piezoelectric coefficients times the strain tensor components but also on the product of the second order (or higher) piezoelectric coefficients times products of the strain tensor components. The idea was put forward for zincblende GaAs and InAs semiconductors since 2006, and then extended to all commonly used wurtzite and zincblende semiconductors. Given the difficulty of finding direct experimental evidence for the existence of these effects, there are different schools of thought on how one can calculate reliably all the piezoelectric coefficients.\nOn the other hand, there is widespread agreement on the fact that non linear effects are rather large and comparable to the linear terms (first order). Indirect experimental evidence of the existence of these effects has been reported in the literature in relation to GaN and InN semiconductor optoelectronic devices.\n\nNon linear piezoelectric effects in polar semiconductors were first reported in 2006 by G.Bester et al. and by M.A. Migliorato et al., in relation to zincblende GaAs and InAs. Different methods were used in the seminal papers and while the influence of second (and third) order piezoelectric coefficients was generally recognized as being comparable to first order, fully ab initio and what is currently known as Harrison's model, appeared to predict slightly different results, particularly for the magnitude of the first order coefficients.\n\nWhile first order piezoelectric coefficients are of the form e, the second and third order coefficients are in the form of a higher rank tensor, expressed as e and e. The piezoelectric polarization would then be expressed in terms of products of the piezoelectric coefficients and strain components, products of two strain components, and products of three strain components for the first, second and third order approximation respectively.\n\nSince 2006 many more articles were published on the subject. Non linear piezoelectric coefficients are now available for many different semiconductor materials and crystal structures:\n\n\nParticularly for III-N semiconductors, the influence of non linear piezoelectricity was discussed in the context of light-emitting diodes:\n\n"}
{"id": "13481218", "url": "https://en.wikipedia.org/wiki?curid=13481218", "title": "Open platform", "text": "Open platform\n\nIn computing, an open platform describes a software system which is based on open standards, such as published and fully documented external application programming interfaces (API) that allow using the software to function in other ways than the original programmer intended, without requiring modification of the source code. Using these interfaces, a third party could integrate with the platform to add functionality. The opposite is a closed platform.\n\nAn open platform does not mean it is open source, however most open platforms have multiple implementations of APIs. For example, Common Gateway Interface (CGI) is implemented by open source web servers as well as Microsoft Internet Information Server (IIS). An open platform can consist of software components or modules that are either proprietary or open source or both. It can also exist as a part of closed platform, such as CGI, which is an open platform, while many servers that implement CGI also have other proprietary parts that are not part of the open platform.\n\nAn open platform implies that the vendor allows, and perhaps supports, the ability to do this. Using an open platform a developer could add features or functionality that the platform vendor had not completed or had not conceived of. An open platform allows the developer to change existing functionality, as the specifications are publicly available open standards.\n\nA service-oriented architecture allows applications, running as services, to be accessed in a distributed computing environment, such as between multiple systems or across the Internet. A major focus of Web services is to make functional building blocks accessible over standard Internet protocols that are independent from platforms and programming languages. An open SOA platform would allow anyone to access and interact with these building blocks.\n\nA 2008 paper from the Harvard Business School differentiated a platform's openness in four aspects and gave example platforms.\n"}
{"id": "49389930", "url": "https://en.wikipedia.org/wiki?curid=49389930", "title": "Phocas Software", "text": "Phocas Software\n\nPhocas Software is a business intelligence software company headquartered in Coventry, England. As of 2015, it services more than 1,000 customers.\n\nPhocas was founded in Oxford, England, in 1999. Led by co-founders Paul Magee, Sean Morley, Chris Howard, Harold Roffey and Myles Glashier, software development commenced that same year in Collaroy, Australia. The company officially launched in the UK in 2001, and Phocas Pty Ltd began operating in Australia in 2003.\n\nIn 2006, Phocas Inc. opened a New York City office in Manhattan.\n\nIn 2014, Phocas opened an office in Orlando, Florida and in 2015 expanded US operations opening a Reno, Nevada office. It also made news when it opened an office and R&D centre in the regional NSW town of Orange.\n\nIn 2015, Phocas formed a strategic partnership with MAM Software to expand its reach into the automotive aftermarket, supply and distribution industries.\n\nThe latest version of Phocas Software's data analytics software integrates with popular enterprise resource planning (ERP) systems.\n\nPhocas is a governed data discovery application with an HTML5+Javascript presentation layer, .NET Framework business logic layer and Microsoft SQL Server database layer.\n\nThe web application is deployed on premise or in the cloud on Microsoft IIS7. It has been optimised for the type of device whether it be a desktop, tablet, or phone.\n\nThe company claims a user base of 1,000 customers, with a 97% client retention rate. It reported $20 million revenue in 2014.\n\n"}
{"id": "20151959", "url": "https://en.wikipedia.org/wiki?curid=20151959", "title": "Photographic lens design", "text": "Photographic lens design\n\nThe design of photographic lenses for use in still or cine cameras is intended to produce a lens that yields the most acceptable rendition of the subject being photographed within a range of constraints that include cost, weight and materials. For many other optical devices such as telescopes, microscopes and theodolites where the visual image is observed but often not recorded the design can often be significantly simpler than is the case in a camera where every image is captured on film or image sensor and can be subject to detailed scrutiny at a later stage. Photographic lenses also include those used in enlargers and projectors.\n\nFrom the perspective of the photographer, the ability of a lens to capture sufficient light so that the camera can operate over a wide range of lighting conditions is important. Designing a lens that reproduces colour accurately is also important as is the production of an evenly lit and sharp image over the whole of the film or sensor plane.\n\nFor the lens designer, achieving these objectives will also involve ensuring that internal flare, optical aberrations and weight are all reduced to the minimum whilst zoom, focus and aperture functions all operate smoothly and predictably.\n\nHowever, because photographic films and electronic sensors have a finite and measurable resolution, photographic lenses are not always designed for maximum possible resolution since the recording medium would not be able to record the level of detail that the lens could resolve. For this, and many other reasons, camera lenses are unsuited for use as projector or enlarger lenses.\n\nThe design of a fixed focal length lens (also known as \"prime lenses\") presents fewer challenges than the design of a zoom lens. A high-quality prime lens whose focal length is about equal to the diameter of the film frame or sensor may be constructed from as few as four separate lens elements, often as pairs on either side of the aperture diaphragm. Good examples include the Zeiss Tessar or the Leitz Elmar.\n\nTo be useful in photography any lens must be able to fit the camera for which it is intended and this will physically limit the size where the bayonet mounting or screw mounting is to be located.\n\nPhotography is a highly competitive commercial business and both weight and cost constrain the production of lenses.\n\nRefractive materials such as glass have physical limitations which limit the performance of lenses. In particular the range of refractive indices available in commercial glasses span a very narrow range. Since it is the refractive index that determines how much the rays of light are bent at each interface and since it is the differences in refractive indices in paired plus and minus lenses that constrains the ability to minimise chromatic aberrations, having only a narrow spectrum of indices is a major design constraint.\n\nExcept for the most simple and inexpensive lenses, each complete lens is made up from a number of separate lens elements arranged along a common axis. The use of many lens elements serves to minimise aberrations and to provide a sharp image free from visible imperfections. To do this requires lens elements of different compositions and different shapes. To minimise chromatic aberrations, e. g., in which different wavelengths of light are refracted to different degrees, requires, at a minimum, a doublet of lens elements with a positive element having a high Abbe number matched with a negative element of lower Abbe number. With this design one can achieve a good degree of convergence of different wavelengths in the visible spectrum. Most lens designs do not attempt to bring infrared wavelengths to the same common focus and it is therefore necessary to manually alter the focus when photographing in infrared light. Other kinds of aberrations like coma or astigmatism can also be minimized by combining different lens elements. Complex photographic lenses can consist of more than 15 lens elements.\n\nMost lens elements are made with curved surfaces with a spherical profile. That is, the curved shape would fit on the surface of a sphere. This is partly to do with the history of lens making but also because grinding and manufacturing of spherical surface lenses is relatively simple and cheap. However, spherical surfaces also give rise to lens aberrations and can lead to complicated lens designs of great size. Higher-quality lenses with fewer elements and lower size can be achieved by using aspheric lenses in which the curved surfaces are not spherical, giving more degrees of freedom to correct aberrations.\n\nThe majority of photographic lenses have the lens elements made from glass although the use of high-quality plastics is becoming more common in high-quality lenses and has been common in inexpensive cameras for some time. The design of photographic lenses is very demanding as designers push the limits of existing materials to make more versatile, better-quality, and lighter lenses. As a consequence many exotic glasses have been used in modern lens manufacturing. Caesium and lanthanum glass lenses are now in use because of their high refractive index and very low dispersion properties. It is also likely that a number of other transition element glasses are in use but manufacturers often prefer to keep their material specification secret to retain a commercial or performance edge over their rivals.\n\nUntil recent years focusing of a camera lens to achieve a sharp image on the film plane was achieved by means of a very shallow helical thread in the lens mount through which the lens could be rotated moving it closer or further from the film plane. This arrangement, whilst simple to design and construct, has some limitations not least the rotation of the greater part of the lens assembly including the front element. This could be problematical if devices such as polarising filters were in use that require maintaining an accurate vertical orientation irrespective of focus distance.\n\nLater developments adopted designs in which internal elements were moved to achieve focus without affecting the outer barrel of the lens or the orientation of the front element.\n\nMany modern cameras now use automatic focusing mechanisms which use ultrasonic motors to move internal elements in the lens to achieve optimum focus.\n\nThe aperture control, usually a multi-leaf diaphragm, is critical to the performance of a lens. The role of the aperture is to control the amount of light passing through the lens to the film or sensor plane. An aperture placed outside of the lens, as in the case of some Victorian cameras, risks vignetting of the image in which the corners of the image are darker than the centre. A diaphragm too close to the image plane risks the diaphragm itself being recorded as a circular shape or at the very least causing diffraction patterns at small apertures. In most lens designs the aperture is positioned about midway between the front surface of the objective and the image plane. In some zoom lenses it is placed some distance away from the ideal location in order to accommodate the movement of floating lens elements needed to perform the zoom function.\n\nMost modern lenses for 35mm format rarely provide a stop smaller than f/22 because of the diffraction effects caused by light passing through a very small aperture. As diffraction is based on aperture width in absolute terms rather than the f-stop ratio, lenses for very small formats common in compact cameras rarely go above f/11 (1/1.8\") or f/8 (1/2.5\"), while lenses for medium- and large-format provide f/64 or f/128.\n\nVery-large-aperture lenses designed to be useful in very low light conditions with apertures ranging from f/1.2 to f/0.9 are generally restricted to lenses of standard focal length because of the size and weight problems that would be encountered in telephoto lenses and the difficulty of building a very wide aperture wide angle lens with the refractive materials currently available. Very-large-aperture lenses are commonly made for other types of optical instruments such as microscopes but in such cases the diameter of the lens is very small and weight is not an issue.\n\nMany very early cameras had diaphragms external to the lens often consisting of a rotating circular plate with a number of holes of increasing size drilled through the plate. Rotating the plate would bring an appropriate sized hole in front of the lens. All modern lenses use a multi-leaf diaphragm so that at the central intersection of the leaves a more or less circular aperture is formed. Either a manual ring, or an electronic motor controls the angle of the diaphragm leaves and thus the size of the opening.\n\nThe placement of the diaphragm within the lens structure is constrained by the need to achieve even illumination over the whole film plane at all apertures and the requirement to not interfere with the movement of any movable lens element. Typically the diaphragm is situated at about the level of the optical centre of the lens.\n\nA shutter controls the length of time light is allowed to pass through the lens onto the film plane. For any given light intensity, the more sensitive the film or detector or the wider the aperture the shorter the exposure time need to be to maintain the optimal exposure.\nIn the earliest camera exposures were controlled by moving a rotating plate from in front of the lens and then replacing it. Such a mechanism only works effectively for exposures of several seconds or more and carries a considerable risk of inducing camera shake. By the end of the 19th century spring tensioned shutter mechanisms were in use operated by a lever or by a cable release. Some simple shutters continued to be placed in front of the lens but most were incorporated within the lens mount itself.\nSuch lenses with integral shutter mechanisms developed in the current Compur shutter as used in many non-reflex cameras such as Linhof. These shutters have a number of metal leaves that spring open and then close after a pre-determined interval. The material and design constraints limit the shortest speed to about 0.002 second. Although such shutters cannot yield as short an exposure time as focal-plane shutter they are able to offer flash synchronisation at all speeds.\n\nIncorporating a commercial made Compur type shutter required lens designers to accommodate the width of the shutter mechanism in the lens mount and provide for the means of triggering the shutter on the lens barrel or transferring this to the camera body by a series of levers as in the Minolta twin-lens cameras.\n\nThe need to accommodate the shutter mechanism within the lens barrel limited the design of wide-angle lenses and it was not until the widespread use of focal-plane shutters that extreme wide-angle lenses were developed.\n\nThe type of lens being designed is significant in setting the key parameters.\n\n\n\nLenses used in photographic enlargers are required to focus light passing through a relatively small film area on a larger area of photographic paper or film. Requirements for such lenses include\n\nThe design of the lens is required to work effectively with light passing from near focus to far focus - exactly the reverse of a camera lens. This demands that internal light baffling within the lens is designed differently and that the individual lens elements are designed to maximize performance for this change of direction of incident light.\n\nProjector lenses share many of the design constraints as enlarger lenses but with some critical differences. Projector lenses are always used at full aperture and must produce an acceptably illuminated and acceptably sharp image at full aperture.\n\nHowever, because projected images are almost always viewed at some distance, lack of very fine focus and slight unevenness of illumination is often acceptable. Projector lenses have to be very tolerant of prolonged high temperatures from the projector lamp and frequently have a focal length much longer than the taking lens. This allows the lens to be positioned at a greater distance from the illuminated film and allows an acceptable sized image with the projector some distance from the screen. It also permits the lens to be mounted in a relatively coarsely threaded focusing mount so that the projectionist can quickly correct any focusing errors.\n\nThe lenses of the very earliest cameras were simple meniscus or simple bi convex lenses. It was not until 1840 that Chevalier in France introduced the achromatic lens formed by cementing a crown glass bi-convex lens to a flint glass plano-concave lens.\nBy 1841 Voigtländer using the design of Joseph Petzval manufactured the first commercially successful two element lens.\n\nCarl Zeiss was an entrepreneur who needed a competent designer to take his firm beyond just another optical workshop. In 1866, the service of Dr Ernst Abbe was enlisted. From then on novel products appeared in rapid succession which brought the Zeiss company to the forefront of optical technology.\n\nAbbe was instrumental in the development of the famous Jena optical glass. When he was trying to eliminate astigmatism from microscopes, he realised that the range of optical glasses available was insufficient. After some calculations, he realised that performance of optical instruments would dramatically improve, if optical glasses of appropriate properties were available. His challenge to glass manufacturers was finally answered by Dr Otto Schott, who established the famous glassworks at Jena from which new types of optical glass began to appear from 1888, and employed by Zeiss and other makers.\n\nThe new Jena optical glass also opened up the possibility of increased performance of photographic lenses. The first use of Jena glass in a photographic lens was by Voigtländer, but as the lens was an old design its performance was not greatly improved. Subsequently, the new glasses would demonstrate their value in correcting astigmatism, and in the production of achromatic and apochromatic lenses. Abbé started the design of a photographic lens of symmetrical design with five elements, but went no further.\n\nZeiss' innovative photographic lens design was due to Dr Paul Rudolph. In 1890, Rudolph designed an asymmetrical lens with a cemented group at each side of the diaphragm, and appropriately named \"Anastigmat\". This lens was made in three series: Series III, IV and V, with maximum apertures of f/7.2, f/12.5, and f/18 respectively. In 1891, Series I, II and IIIa appeared with respective maximum apertures of f/4.5, f/6.3, and f/9 and in 1893 came Series IIa of f/8 maximum aperture. These lenses are now better known by the trademark \"Protar\" which was first used in 1900.\n\nAt the time, single combination lenses, which occupy one side of the diaphragm only, were still popular. Rudolph designed one with three cemented elements in 1893, with the option of fitting two of them together in a lens barrel as a compound lens, but it was found to be the same as the Dagor by C.P. Goerz, designed by Emil von Höegh. Rudolph then came up with a single combination with four cemented elements, which can be considered as having all the elements of the Protar stuck together in one piece. Marketed in 1894, it was called the Protarlinse Series VII, the most highly corrected single combination lens with maximum apertures between f/11 and f/12.5, depending on its focal length.\n\nBut the important thing about this Protarlinse is that two of these lens units can be mounted in the same lens barrel to form a compound lens of even greater performance and larger aperture, between f/6.3 and f/7.7. In this configuration it was called the Double Protar Series VIIa. An immense range of focal lengths can thus be obtained by the various combination of Protarlinse units.\n\nRudolph also investigated the Double-Gauss concept of a symmetrical design with thin positive meniscii enclosing negative elements. The result was the Planar Series Ia of 1896, with maximum apertures up to f/3.5, one of the fastest lenses of its time. Whilst it was very sharp, it suffered from coma which limited its popularity. However, further developments of this configuration made it the design of choice for high-speed lenses of standard coverage.\n\nProbably inspired by the Stigmatic lenses designed by Hugh Aldis for Dallmeyer of London, Rudolph designed a new asymmetrical lens with four thin elements, the Unar Series Ib, with apertures up to f/4.5. Due to its high speed it was used extensively on hand cameras.\n\nThe most important Zeiss lens by Rudolph was the Tessar, first sold in 1902 in its Series IIb f/6.3 form. It can be said as a combination of the front half of the Unar with the rear half of the Protar. This proved to be a most valuable and flexible design, with tremendous development potential. Its maximum aperture was increased to f/4.7 in 1917, and reached f/2.7 in 1930. It is probable that every lens manufacturer has produced lenses of the Tessar configuration.\n\nRudolph left Zeiss after the First World War, but many other competent designers such as Merté, Wandersleb, etc. kept the firm at the leading edge of photographic lens innovations. One of the most significant designer was the ex-Ernemann man Dr Ludwig Bertele, famed for his Ernostar high-speed lens.\n\nWith the advent of the Contax by Zeiss-Ikon, the first serious challenge to the Leica in the field of professional 35 mm cameras, both Zeiss-Ikon and Carl Zeiss decided to beat the Leica in every possible way. Bertele's Sonnar series of lenses designed for the Contax were the match in every respect for the Leica for at least two decades. Other lenses for the Contax included the Biotar, Biogon, Orthometar, and various Tessars and Triotars.\n\nThe last important Zeiss innovation before the Second World War was the technique of applying anti-reflective coating to lens surfaces invented by Olexander Smakula in 1935. A lens so treated was marked with a red \"T\", short for \"Transparent\". The technique of applying multiple layers of coating was also described in the original patent writings in 1935.\n\nAfter the partitioning of Germany, a new Carl Zeiss optical company was established in Oberkochen, while the original Zeiss firm in Jena continued to operate. At first both firms produced very similar lines of products, and extensively cooperated in product-sharing, but they drifted apart as time progressed. Jena's new direction was to concentrate on developing lenses for the 35 mm single-lens reflex camera, and many achievements were made, especially in ultra-wide angle designs. In addition to that, Oberkochen also worked on designing lenses for large format cameras, interchangeable front element lenses such as for the 35 mm single-lens reflex Contaflex, and other types of cameras.\n\nSince the beginning of Zeiss as a photographic lens manufacturer, it has had a licensing programme which allows other manufacturers to produce its lenses. Over the years its licensees included Voigtländer, Bausch & Lomb, Ross, Koristka, Krauss, Kodak. etc. In the 1970s, the western operation of Zeiss-Ikon got together with Yashica to produce the new Contax cameras, and many of the Zeiss lenses for this camera, among others, were produced by Yashica's optical arm, Tomioka. Yashica's owner Kyocera ended camera production in 2006. Yashica lenses were then made by Cosina, who also manufactured most of the new Zeiss designs for the new Zeiss Ikon coupled rangefinder camera. Another licensee active today is Sony who uses the Zeiss name on lenses on its video and digital still cameras.\n\n"}
{"id": "52336246", "url": "https://en.wikipedia.org/wiki?curid=52336246", "title": "Pyze", "text": "Pyze\n\nPyze is an American company that provides a growth intelligence platform for mobile and web app developers, headquartered in Redwood City, California.\n\nPyze was founded in 2013 by John Chisholm, Dickey Singh and Prabhjot Singh. They developed a behavioral analytics and marketing platform to help app publishers monetize their apps and retain users. The company exited stealth mode in March 2016 and announced a $1.7 million seed round. The company launched its free platform in March 2016 and its Hypergrowth Tier in August 2016.\n\nPyze offers a Software as a Service (SaaS) platform to help mobile and web app publishers retain and grow users for iOS, Android, Web and Unity platform. The services include Visual Intelligence, Contextual Mobile Marketing, Intelligence Explorer, Personalization Intelligence, Growth Automation, Loyalty & Attrition Management and App Dimensions.\n\n"}
{"id": "56747271", "url": "https://en.wikipedia.org/wiki?curid=56747271", "title": "Rafail Krichevskii", "text": "Rafail Krichevskii\n\nRafail Evseevich Krichevskii (Рафаил Евсеевич Кричевский, sometimes transliterated as \"Krichevskiy\" or \"Krichevsky\", born 1936, Kharkov) is a Russian mathematician and information theorist, now living in the United States.\n\nHe graduated from Moscow State University in 1958. He became a doctor of physical and mathematical sciences and professor, specializing in the field of mathematical cybernetics and information theory. From 1961 to 1996 he worked at the Sobolev Institute of Mathematics. He supervised 7 doctoral (Russian candidate degree) students and 2 higher doctoral (habilitation) students. He is the author of about 80 scientific papers.\n\nIn 1986 Krichevskii was an Invited Speaker with talk \"Retrieval and data compression complexity\" at the ICM in Berkeley, California.\n\n\n"}
{"id": "21779320", "url": "https://en.wikipedia.org/wiki?curid=21779320", "title": "Rainscreen", "text": "Rainscreen\n\nA rainscreen is an exterior wall detail where the siding (wall cladding) stands off from the moisture-resistant surface of an air barrier applied to the sheathing (sheeting) to create a capillary break and to allow drainage and evaporation. The \"rain screen\" is the siding itself but the term rainscreen implies a system of building. Ideally the rain screen prevents the wall air/moisture barrier on sheathing from getting wet. In some cases a rainscreen wall is called a \"pressure-equalized rainscreen\" wall where the ventilation openings are large enough for the air pressure to nearly equalize on both sides of the rain screen, but this name has been criticized as being redundant and is only useful to scientists and engineers.\n\nA \"screen\" in general terms is a barrier. The rainscreen in a wall is sometimes defined as the first layer of material on the wall, the siding itself. Also, rainscreen is defined as the entire system of the siding, \"drainage plane\" and a \"moisture/air barrier\". A veneer that does not stand off from the wall sheathing to create a cavity is not a rainscreen. However, a masonry veneer can be a rainscreen wall if it is ventilated.\n\nMany terms have been applied to rain screen walls including basic, open, conventional, pressure-equalized, pressure-moderated rainscreen systems or assemblies. These terms have caused confusion as to what a rain screen is but all reflect the \"rainscreen principle\" of a primary and secondary line of defense. One technical difference is between a plane, a gap of or less and a channel, a gap of more than .\n\nIn general terms a rainscreen wall may be called a \"cavity\" or \"drained wall\". The two other basic types of exterior walls in terms of water resistance are \"barrier walls\" which rely on the one exterior surface to prevent ingress and \"mass walls\" which allow but absorb some leakage.\n\nIn the early 1960s research was conducted in Norway on rain penetration of windows and walls, and Øivind Birkeland published a treatise referring to a \"rain barrier\". In 1963 the Canadian National Research Counsel published a pamphlet titled \"Rain Penetration and its Control\" using the term \"open rain screen\".\n\nRainscreen cladding is a kind of double-wall construction that utilizes a surface to help keep the rain out, as well as an inner layer to offer thermal insulation, prevent excessive air leakage and carry wind loading. The surface breathes just like a skin as the inner layer reduces energy losses.\n\nFor water to enter a wall first the water must get onto the wall and the wall must have openings. Water can then enter the wall by capillary action, gravity, momentum, and air pressure (wind). The rainscreen system provides for two lines of defense against the water intrusion into the walls: The rainscreen and a means to dissipate leakage often referred to as a channel.\nIn a rainscreen the air gap allows the circulation of air on the moisture barrier. (These may or may not serve as a vapour barrier, which can be installed on the interior or exterior side of the insulation depending on the climate). This helps direct water away from the main exterior wall which in many climates is insulated. Keeping the insulation dry helps prevent problems such as mold formation and water leakage. The vapour-permeable air/weather barrier prevents water molecules from entering the insulated cavity but allows the passage of vapour, thus reducing the trapping of moisture within the main wall assembly.\n\nThe air gap (or cavity) can be created in several ways. One method is to use furring (battens, strapping) fastened vertically to the wall. Ventilation openings are made at the bottom and top of the wall so air can naturally rise through the cavity. Wall penetrations including windows and doors require special care to maintain the ventilation. In the pressure-equalized system the ventilation openings must be large enough to allow air-flow to equalize the pressure on both sides of the cladding. A ratio of 10:1 cladding leakage area to ventilation area has been suggested.\n\nA water/air resistant membrane is placed between the furring and the sheathing to prevent rain water from entering the wall structure. The membrane directs water away and toward special drip edge flashings which protect other parts of the building.\n\nInsulation may be provided beneath the membrane. The thickness of insulation is determined by building code requirements as well as performance requirements set out by the architect.\n\nThe system is a form of double-wall construction that uses an outer layer to keep out the rain and an inner layer to provide thermal insulation, prevent excessive air leakage and carry wind loading. The outer layer breathes like a skin while the inner layer reduces energy losses. The structural frame of the building is kept absolutely dry, as water never reaches it or the thermal insulation. Evaporation and drainage in the cavity removes water that penetrates between panel joints. Water droplets are not driven through the panel joints or openings because the rainscreen principle means that wind pressure acting on the outer face of the panel is equalized in the cavity. Therefore, there is no significant pressure differential to drive the rain through joints. During extreme weather, a minimal amount of water may penetrate the outer cladding. This, however, will run as droplets down the back of the cladding sheets and be dissipated through evaporation and drainage.\n\nA rainscreen drainage plane is a separation between the veneer and the weather resistant barrier of a rainscreen. It provides predictable, unobstructed path drainage for liquid moisture to drain from a high point of the wall (where it enters) to a low point of the wall (where it exits) the wall detail. The drainage plane must move the water out of the wall system quickly to prevent absorption and consequential rot, mold, and structural degradation.\n\nA drainage plane \nis designed to shed bulk rainwater and/or condensation downward and outward in a manner that will prevent uncontrolled water penetration into the conditioned spaces of a building or structure. In a barrier wall system, the exterior cladding also serves as the principal drainage plane and primary line of defense against bulk rainwater penetration. In cavity wall construction, however, the principal drainage plane and primary line of defense against bulk rainwater penetration is located inside the wall cavity, generally on the inboard side of the air space (either directly applied to the outboard surface of the exterior sheathing layer or, in the case of insulated cavity walls, on the outboard surface of the rigid or otherwise moisture-impervious insulation layer).\nAir pressure difference is one of the dominant forces for driving a rainwater into wall systems. A rainscreen drainage plane that works as a predictable pressure equalization plane creates a separation (an air chamber) between the backside of a rainscreen and the exterior surface of the weather-resistant barrier that is installed on the exterior sheeting of the structural back up wall. This separation allows air contaminated with water vapor from all points in that wall system to exit the interior of the wall system. Moisture laden air that is allowed to pressurize will attempt to move to a lower pressure area that may be deeper into the interior of a wall detail.\n\n\nOnce moisture has penetrated deep into a wall system through the weather resistant barrier and into the exterior sheathing, the wall is deep wet. The air flow that exists in most wall systems is a slight draft that will not dry this condition out in a timely manner. The result is a compromised wall system with rot, rust, and mold potential. The structural integrity of the wall is at stake, as is the health of the occupants. The longer the wall remains wet, the greater the risk. 50% percent of homes suffer from mold problems. Billions of dollars are spent annually on litigation involving mold and rot problems stemming from entrapped moisture; this has created an entire industry centered around construction litigation. Such litigation has caused insurance premiums for contractors to increase significantly and has made it difficult for contractors involved in moisture related lawsuits to obtain insurance at all. An effective rainscreen drainage plane system mitigates this risk.\n\n Dampness levels in construction are measured in wood moisture equivalent (WME) percentages and is calculated as follows: \n\nA normal range is 8–13% WME, with fungal growth beginning at the 16% threshold. A 20% WME is enough to promote wood rot. It logically follows that the more time a part of a wall system exceeds one of these thresholds the greater chance of damage from fungal growth or rot.\n\n\n * Synapse Construction (a Seattle Siding and Window Contractor) article for open joint rainscreen cladding in Journal of Light Construction \n"}
{"id": "32692970", "url": "https://en.wikipedia.org/wiki?curid=32692970", "title": "Recon Instruments", "text": "Recon Instruments\n\nRecon Instruments was a Canadian technology company that produced smartglasses and wearable displays marketed by the company as \"heads-up displays\" for sports. (However, none of Recon's products contained a transparent display element delivering actual see-through capability and can thus be considered heads-up displays in the true meaning of the term.) Recon's products delivered live activity metrics, GPS maps, and notifications directly to the user's eye. Recon's first heads-up display offering was released commercially in October 2010, roughly a year and a half before Google introduced Google Glass.\n\nRecon received investments from companies including Motorola Solutions and Intel. It also partnered with enterprise software vendors in order to make its latest smart eyewear device, the Jet, suitable for industrial applications.\n\nOn June 17, 2015, Recon was acquired by Intel. Recon then described itself as \"an Intel company.\"\n\nIn June 2017, Intel announced that all remaining Recon Instruments products were going to be discontinued by the end of the year . According to a Bloomberg report in October 2017, Intel had in fact completely closed its Recon Instruments division already in early summer 2017.\n\nThe technology behind Recon Instruments' products was born in September 2006 from an integrated MBA project. That project was undertaken by co-founders Dan Eisenhardt, Hamid Abdollahi, Fraser Hall, and Darcy Hughes at the University of British Columbia, Robert H. Lee Sauder School of Business.\n\nRecon Instruments incorporated in January 2008, operating from small office and lab spaces rented from the University of British Columbia. In April 2010, the company moved to its current headquarters in the Yaletown area of downtown Vancouver. As of March 2015, Recon is still led by co-founders Dan Eisenhardt and Hamid Abdollahi.\n\nRecon's co-founders originally looked into developing a HUD product for swimmers. Eisenhardt, a competitive swimmer himself, believed a HUD would be a valuable replacement for the clock at the side of the pool. Eisenhardt and his fellow founders developed the idea while studying at the University of British Columbia. However, a patent already existed for swimming goggles with a heads-up display. Because of that patent and the challenges presented by the technology's small form factor and intended operating conditions, the team eventually chose to focus on a winter sports product. The co-founders subsequently turned this school project into their first retail product, which was distributed globally in October 2010.\n\nRecon has received investments from both venture-capital firms and other technology companies.\n\nIn January 2012, Recon received $10 million in Series A funding from Vanedge Capital and Kopin Corporation. Vanedge Capital is a Canadian venture capital firm that specializes in \"interactive entertainment and digital media businesses.\" Kopin Corporation is a U.S. firm known for microdisplays aimed at mobile electronics.\n\nIn September 2013, Intel Capital, the venture capital arm of Intel, announced that it had invested in Recon. Details of the deal were not disclosed. However, the announcement described wearables as \"an area of significant focus\" for Intel Capital, and it said the investment would allow Recon to \"accelerate product development, marketing and global sales, as well as gain access to Intel Capital's expertise in manufacturing, operations and technology.\"\n\nIn April 2014, Motorola Solutions announced an investment in Recon. Motorola Solutions describes itself as a provider of communications equipment for \"government and enterprise customers.\" The terms of the deal were not made public. In July 2014, Motorola Solutions demonstrated a Recon product as a piece of kit for law enforcement personnel.\n\nOn June 17, 2015, Recon was acquired by Intel. The value of the deal was initially reported to be as high as C$175M. However, this sum was not confirmed by Recon Instrument's Dan Eisenhardt, and was later generally considered inaccurate. After the acquisition, Recon stayed in Vancouver and planned to make use of Intel's technological resources in order to \"develop smart device platforms for a broader set of customers and market segments.\"\n\nIn June 2017, it became known that Intel intended to discontinue all remaining Recon Instruments products, i.e., Recon Jet and Recon Jet Pro.\nAround the same time, Recon Instruments ceased all activities on both social media and its own website.\nAccording to a Bloomberg report in October 2017, Intel had in fact completely closed its Recon Instruments division already in early summer 2017.\n\nRecon's first products were smart goggles and what the company marketed (incorrectly) as \"heads-up displays\" aimed at the winter sports market. More recently, the company broadened its focus with the Jet, a smart eyewear device designed for activities like cycling and running.\n\nAll of Recon Instrument's products were essentially head-worn, self contained mobile devices equipped with GPS and environmental sensors. A near-eye display was provided in the form of a single non-translucent (solid) micro display situated below and to the side of one eye. This required the wearer to glance down and to the side in order to read the screen contents. Recon's head-worn displays were therefore Peripheral Head-Mounted Displays rather than Head-up displays in the common meaning of the term; much less were they able to deliver an Augmented Reality experience due to their lack of see-through capabilities.\n\nRecon's first commercial product, the Transcend, was released in October 2010. It was designed for winter sports and featured a small LCD screen embedded into a snow goggle frame by eyewear maker Zeal Optics, which is now a subsidiary of Maui Jim, Inc. The Transcend displayed data like GPS maps, temperature, speed, and altitude, and it allowed users to share that data. In 2011, the Transcend earned the Consumer Electronics Show's Best of Innovations award for Personal Electronics.\n\nRecon's MOD and MOD Live heads-up displays were released in November 2011. Unlike the Transcend, the MOD and MOD Live were sold separately from snow goggles. Users could fit them into specially designed \"Recon-Ready\" goggles from eyewear makers including Uvex, Alpina, and Briko. Oakley also integrated the MOD Live into a specially designed snow goggle frame and marketed the resulting product as the Airwave.\n\nBoth the MOD and MOD Live offered functionality similar to the Transcend's, but the MOD Live introduced the ability to connect to smartphones via Bluetooth. When connected to a user's smartphone, the MOD Live could display caller ID and SMS notifications.\n\nIntroduced in November 2013, the Snow2 is Recon's latest standalone heads-up display. It features a faster processor than the MOD and MOD Live along with improved display brightness and contrast, longer battery life, 802.11a/b/g/n Wi-Fi connectivity, and Made for iPhone (MFi) certification.\n\nLike the MOD Live, the Snow2 can connect to smartphones in order to display call and SMS notifications. It also lets users connect to Facebook and track their friends using the GPS-enabled maps feature.\n\nThe Snow2 heads-up display is designed to fit inside compatible eyewear from Oakley, Smith, Scott, Uvex, Alpina, Briko, and Zeal. Oakley has integrated the Snow2 into a snow goggle frame and markets the resulting product as the Airwave 1.5. Despite running an Android-based operating system, the Airwave 1.5 is sold by Apple through both Apple retail stores and the online Apple Store.\n\nUnlike the Snow2, the Jet combines a heads-up display with a Recon-designed sunglass frame and polarized lenses.\n\nThe Jet is aimed at activities like cycling and running rather than winter sports. Recon has also partnered with enterprise software firms SAP and APX Labs with the aim of making Jet suitable for industrial applications in fields like manufacturing and oil-and-gas extraction. Motorola Solutions, one of Recon's investors, has also demonstrated the Jet as law-enforcement equipment, as well.\n\nBuilt into the Jet are GPS connectivity as well as sensors to track metrics like speed, pace, distance, and elevation gain. Users can also connect third-party sensors via ANT+ and smartphones via Bluetooth. Like the MOD Live and Snow2, the Jet can display call and SMS notifications from user's smartphones.\n\nThe Jet is powered by a 1 GHz processor with dual ARM Cortex-A9 cores. Its processor, display, and camera sit on the right side of the frame, while the battery sits on the left, evening out weight distribution. The battery is designed to be interchangeable, as well.\n\nRecon devices run ReconOS, an operating system based on Android.\n\nReconOS has a custom user interface designed for small displays. It shows live activity metrics and lets users share those metrics to social media. ReconOS also features GPS maps that display the locations of nearby friends and rotate depending on the user's head orientation. When a Recon device is paired with a smartphone, ReconOS can display call and SMS notifications, and it allows users to control the phone's music playback.\n\nReconOS runs third-party applications, as well. Developers can write ReconOS apps using the Recon SDK.\n\nThe Recon Engage website allows users to browse, display, and share activity metrics recorded with a Recon device. Users can also tag friends, share photos, download software updates and third-party applications for their Recon device, and see their activities mapped in an embedded Google Maps pane.\n\nAvailable for iOS and Android, the Recon Engage mobile app lets users view and share their activity metrics, and it also allows compatible Recon devices to connect to smartphones. Connecting a Recon device to a smartphone enables features like friend tracking, call and SMS notification display, and music playback controls.\n\nThe Recon Uplink desktop application lets users register their Recon device, update the device's software, and sync data from the device to an Engage account. When used with Jet, the Uplink application can download photos from the device to the user's computer.\n\nAimed at developers, the Recon SDK includes the tools, documentation, and samples necessary to write third-party applications for Recon's Jet and Snow2 devices. The Recon SDK API augments the Android API with extensions specific to Recon device hardware. Developers do not need to register or to pay a fee to access the Recon SDK.\n\nBy visiting the App Center on Recon's Engage website, users can download third-party apps for Recon's Jet and Snow2 products. Among the apps on offer are Refuel, a \"smart nutrition\" app that tells users when to eat and rehydrate during activities, and MyGoproRemote2, which makes it possible to control GoPro cameras using a Jet or Snow2.\n\nThe flagship product of Recon Instruments, Recon Jet, launched in 2015 to mixed reviews, with Engadget calling the goggles \"expensive fitness glasses with potential to be better\". Reviewers praised Recon Instruments for bringing the first fitness-oriented head-worn displays to market. Frequently voiced criticisms were the high price point, insufficient battery life, wearer distraction and limited field of view by the non-see-through (solid) micro display, unsatisfactory GPS lag and accuracy, complex user interface, and general software problems.\n\n"}
{"id": "5745677", "url": "https://en.wikipedia.org/wiki?curid=5745677", "title": "Risk management information systems", "text": "Risk management information systems\n\nA risk management information system (RMIS) is an information system that assists in consolidating property values, claims, policy, and exposure information and providing the tracking and management reporting capabilities to enable the user to monitor and control the overall cost of risk management.\n\nThe management of risk data and information is key to the success of any risk management effort regardless of an organization's size or industry sector. Risk management information systems/services (RMIS) are used to support expert advice and cost-effective information management solutions around key processes such as:\n\nTypically, RMIS facilitates the consolidation of information related to insurance, such as claims from multiple sources, property values, policy information, and exposure information, into one system. Often, RMIS applies primarily to “casualty” claims/loss data systems. Such casualty coverages include auto liability, auto physical damage, workers' compensation, general liability and products liability.\n\nRMIS products are designed to provide their insured organizations and their brokers with basic policy and claim information via electronic access, and most recently, via the Internet. This information is essential for managing individual claims, identifying trends, marketing an insurance program, loss forecasting, actuarial studies and internal loss data communication within a client organization. They may also provide the tracking and management reporting capabilities to enable one to monitor and control overall cost of risk in an efficient and cost-effective manner.\n\nIn the context of the acronym RMIS, the word “risk” pertains to an insured or self-insured organization. This is important because prior to the advent of RMIS, insurance company loss information reporting typically organized loss data around insurance policy numbers. The historical focus on insurance policies detracted from a clear, coherent and consolidated picture of a single customer's loss experience. The advent of the first PC and UNIX based standalone RMIS was in 1982, by Mark Dorn, under the trade name RISKMASTER. This began a breakthrough step in the insurance industry's evolution toward persistent and focused understanding of their end-customer needs. Typically, the best solution for an organization depends on whether it is enhancing an existing RMIS system, ensuring the highest level of data quality, or designing and implementing a new system while maintaining a focus on state-of-the-art technology.\n\nMost major insurance companies (carriers), broker/agents, and third party administrators (TPAs)offer/provide at least one external RMIS product to their insureds (clients) and any brokers involved in the insurance program. Most commonly, RMIS products allow individual claim detail look-up, basic trend report production, policy summaries and ad hoc queries. The resulting information can then be shared throughout the client's organization, usually for insurance program cost allocation, loss prevention and effective claim management at the local level. More advanced products allow multiple claim data sources to be consolidated into one “Master RMIS,” which is essential for most large client organizations with complex insurance programs.\n\nThe primary users of RMIS are risk/insurance departments of insured organizations and any insurance broker involved. It is much less common for the insured's safety department and vehicle operations department to have access to RMIS despite similar interest in the data. In fact, safety and vehicle operations of larger organizations typically maintain their own separate database systems of “accidents/incidents,” many of which will correlate to RMIS claim data.\n\nInsurance companies normally use a different version of externally provided RMIS for internal use, such as by underwriting and loss control personnel. Occasionally, there could be timing or other differences that could cause data discrepancies between the internal system and externally provided RMIS.\n\nInsurance brokers have a similar need for access to their insured client's claim data. Brokers are normally added as an additional user to the RMIS product provided to their clients by the insurance carrier and TPAs. The information available from RMIS is critical to the broker for interfacing effectively with their counterparts in the insurance carrier and TPAs. Additionally, effectively presented RMIS information that shows trends and analysis is essential to successfully marketing their clients' insurance programs.\n\nInsurance carrier and a TPA claim adjusters traditionally use claims management systems to collect and manage claim information and to administer claims. Some client organizations, however, may choose to manage certain types of claims or those within a loss retention layer and thus use this type of system as well.\n\nTypically, the claims management system provides the primary data to RMIS products. RMIS products in turn provide an externally accessed view into the client's claims data. RMIS products are commonly available directly from larger insurance carriers and TPAs, but the most advanced systems are often offered by independent RMIS vendors. Independent RMIS vendor systems are most desirable when a client organization needs to consolidate claims data from multiple current insurance programs and/or past programs with current program information.\n\nAlong with insurance carriers, broker/agents and TPAs that offer their own proprietary systems, there are a variety of direct RMIS technology companies who sell to direct insureds and even the carriers, broker/agents and TPAs themselves.\n\nMajor differences among RMIS vendors include:\n\nRMIS system compatibility varies among carriers, broker/agents and TPAs. However, quality independent RMIS vendors by design can take almost any claim data source and convert or map the data to their particular system's file structure. A few major insurance carriers offer similar consolidation services, i.e., combining the insured client's current claim data with another carrier's or TPA's data for the same insured client. The other data sources can be for current separate insurance programs or from expired insurance programs. Usually, this type of consolidation service is performed to accommodate their major policyholder organizations. Major TPAs, however, more commonly offer such data consolidation services.\n\nThe cost of a typical independent RMIS product varies from $60,000 to $150,000 for the first year, and ongoing annual charges are slightly less. Insurance company RMIS product lines typically average around $50,000 for the first user, but they often offer less expensive light-weight versions for claim look-up only. More costly full-featured products are sometimes available with more advanced reporting systems. The products are usually priced on a per-user basis on a sliding scale for a larger number of users. Insured clients' brokers are given access at no cost or occasionally for a flat annual fee for multiple insured clients with a particular broker.\n\nTPAs commonly include one or two RMIS access IDs within their claims management pricing to encourage both the client's broker and the client to use their claim look-up product. Normally, beyond the first two access IDs, the pricing follows the same per-user range of the insurance companies. The cost drivers of RMIS include:\n\nHigher cost systems do not always correlate to better performance in terms of both usefulness and speed. While most carrier and TPA RMIS systems are similarly priced, the independent RMIS vendors' price range varies significantly, as previously mentioned.\n\n\n"}
{"id": "59148834", "url": "https://en.wikipedia.org/wiki?curid=59148834", "title": "Robert Mottar", "text": "Robert Mottar\n\nRobert Mottar (October 29, 1919, Springfield, Illinois—November 1967, New York City) was an American industrial and magazine photographer, active 1950s-1960s. \n\nRobert Mottar was born in Springfield, Illinois on October 29, 1919 to Louise Ann Mottar (Lindrew) and University of Illinois Pharmacy graduate (1910) Samuel Mayo Mottar (b.1889), a salesman for Squibb in Chicago and trustee of the University of Illinois. He had a sister Bonnie Louise Mottar born August 16, 1915. His mother remarried when Robert was about 10 or 11. He lived in Los Angeles and Baltimore in the 1940s, and in Oregeval, Paris and New York during the 1950s.\n\nRobert Mottar, often credited as 'Robert M. Mottar', began his career as a staff photographer for The Baltimore Sun in the 1940s, and was invited to conduct a workshop in 1949 at the first Missouri Photo Workshop, before graduating to freelancer for business magazines including \"Fortune.\" He was also commissioned for stories on celebrities for mass-circulation magazines such as \"LOOK\" and \"LIFE\". Mottar produced photographs of major building projects, technology and industry and made portraits of major players. Known for accepting dangerous and challenging architecture commissions, in 1959, he documented the building of the Chase Manhattan Bank, at one point assembling all of the construction workers (at a labour down-time cost of $10,000) for a multi-storey vertical panorama in which the grid structure of the steel and reinforced concrete facade of the skyscraper forms dozens of frames, each containing twenty or so cheering workers. \n\nHis near-silhouette against blank sky of a dogman standing on a girder being lifted by a crane featured in the world-touring \"The Family of Man\",  cropped to a tight vertical and mounted floor-to-ceiling to cover an entire structural column in the exhibition space at the Museum of Modern Art showing January 24–May 8, 1955. \"The Family of Man\" was seen by 9 million visitors worldwide and is now on show in perpetuity at Clervaux Castle in Luxembourg. He was represented in two additional exhibitions at the Museum of Modern Art; \"MoMA; Photographs from the Museum Collection\", November 26, 1958–January 18, 1959; and \"70 Photographers Look at New York\", November 27, 1957–April 15, 1958.\n\nMottar's portraits preserve the appearance of many American academics and intellectuals, and some from Europe. \n\nIn the collection of Johns Hopkins University are Mottar’s photographs of inventors and entrepreneurs, especially in the television industry, where he also photographed academics, campus views and student life, production of the \"JHU Science Review\" television program, and on one occasion, scenes of sixth-grade students using a mock-up television studio. \n\nHis portraits made at Johns Hopkins include John Allen Austin, Harold Ingle, Arthur Oncken Lovejoy, Leo Spitzer, Alex Quiroga, Ferdinand J. Hamburger, James William Perry Jr., Ben Wolfe, Alphonse Chapanis, Henry Carrington Lancaster, Ernst Cloys Laurence Hall Fowler, Robert Lowell and Lowell Jacob Reed, Sheldon Keith Spalding (1957), Lyn D. Poole (1957), William Bennett Kouwenhoven (1950), Frank Vigor Morley (1953), William Moore Passano (1956), Edward Russel Hawkins (1950), Alexander Graham Christie (1950), Leo Orville Forkey (1950), George Friederic Wislicenus (1950), Francis Henry Horn (1950), Robert Freedman (1956), Franco Dino Rasetti (1953), Eben Francis Perkins III (1956), Theodore McKeldin (1956), Audrey Smid (1956), Sidney Painter (1954), John Charles Hubbard (1952), Ronald Taylor  Abercrombie (1952), Ralph Knieriem Witt (1951), Robert Fenwick (1951), John Lehman (1951), and Lowell Jacob Reed on his farm in New Hampshire (1953).\n\nIn 1952 Mottar made portraits of alumni emeritus professors at Princeton University in their academic environments that were published in \"The Princeton Alumni Weekly\" and exhibited in in 1953 in the Princetoniana Room of the University Library: Henry Norris Russell, Astronomy; Charles Rufus Morey, Art and Archaeology; Robert Russell Wicks, Dean of the University Chapel, Arthur Maurice Greene, Dean of Engineering; William Starr Myers, Politics; Charles Grosvenor Osgood, Belles Lettres; Frank Jewett Mather Jr., Art and Archaeology; Gordon Hall Gerould, Belles Lettres; Edward Samuel Corwin, Jurisprudence; Thomas Jefferson Wertenbaker, American History; George Wicker Elederkin, Art and Archaeology; Gilbert Chinard, French Literature; George Harrison Shull, Botany and Genetics.\nDuring the 1950s, Mottar accompanied University of Arkansas folklorist Mary Celestia Parler undertaking the Folklore Research Project (1949-1965). The photographs feature Mary Celestia Parler and others active in collecting folklore, as well as the subjects of Ozark folklore studies.\n\nOverseas Mottar worked between Oregeval, Paris and New York where he was represented by Scope Associates. There, Mottar photographed Andre Gustave Parodi at the Swiss embassy in Rome, Russell Baker in London (1953), Mark R. Lazarus in Oxford (1953), Ulrich Ernst Von Gienanth in Eisenberg (1954), John Richard Cary in Munich (1954), Francis Torrance Wiliamson in the US embassy, Rome (1954), Ludwig Edelstein in Oxford (1954), Tom Edward Davis and Thomas Southcliffe Ashton at the London School of Economics (1953), Thomas Watkins McElhiney in Berlin (1954), Arthur Kurtz Myers in Geneva (1953), Frank Vigor Morley in London (1953).\n\nMottar was a member of the American Society of Magazine Photographers (ASMP).\n\nIn March 1947, Mottar married Mary Carlile Boyd in Reno, Nevada and they lived in Westport. They had three children, Mary Mottar (1947–1955), Jill Mottar, born 1948, and Peter Mottar, born 1951. The marriage ended in divorce in 1956 with Mary winning full custody of the couple's two children.\n\n\n"}
{"id": "4468184", "url": "https://en.wikipedia.org/wiki?curid=4468184", "title": "Rolling code", "text": "Rolling code\n\nA rolling code (or sometimes called a hopping code) is used in keyless entry systems to prevent replay attacks, where an eavesdropper records the transmission and replays it at a later time to cause the receiver to 'unlock'. Such systems are typical in garage door openers and keyless car entry systems.\n\n\nA rolling code transmitter is useful in a security system for providing secure encrypted radio frequency (RF) transmission comprising an interleaved trinary bit fixed code and rolling code. A receiver demodulates the encrypted RF transmission and recovers the fixed code and rolling code. Upon comparison of the fixed and rolling codes with stored codes and determining that the signal has emanated from an authorized transmitter, a signal is generated to actuate an electric motor to open or close a movable component.\n\nRemote controls send signals in code. When the sending code is the same as the code that is expected by the receiver, then the receiver will actuate the relay, unlock the door, or open the barrier. Remote controls with a fixed code always send the same fixed code. Remote controls with a rolling code (or hopping code) always send out a different code from the one previously sent.\n\nThe Microchip HCS301 was once the most widely used system on garage and gate remote control and receivers. The chip uses the KeeLoq algorithm. The HCS301 KeeLoq system transmits 66 data bits.\n\nA rolling code transmitted by radio signal that can be intercepted can be vulnerable to falsification. In 2015, it was reported that Samy Kamkar had built an inexpensive electronic device about the size of a wallet that could be concealed on or near a locked vehicle to capture a single keyless entry code to be used at a later time to unlock the vehicle. The device transmits a jamming signal to block the vehicle's reception of rolling code signals from the owner's fob, while recording these signals from both of his two attempts needed to unlock the vehicle. The recorded first code is forwarded to the vehicle only when the owner makes the second attempt, while the recorded second code is retained for future use. Kamkar stated that this vulnerability had been widely known for years to be present in many vehicle types, but was previously undemonstrated. A demonstration was done during DEF CON 23.\n\n"}
{"id": "4640464", "url": "https://en.wikipedia.org/wiki?curid=4640464", "title": "Satchel", "text": "Satchel\n\nA satchel is a bag, often with a strap. The strap is often worn so that it diagonally crosses the body, with the bag hanging on the opposite hip, rather than hanging directly down from the shoulder. They are traditionally used for carrying books. The back of a satchel extends to form a flap that folds over to cover the top and fastens in the front. Unlike a briefcase, a satchel is soft-sided.\n\nThe satchel has been a typical accessory of English students for centuries, as attested in Shakespeare's famous monologue, \"All the world's a stage.\" The traditional Oxford and Cambridge style satchel features a simple pouch with a front flap. Variations include designs with a single or double pocket on the front and sometimes a handle on the top of the bag. The classic school bag satchel often had two straps, so that it could be worn like a backpack, with the design having the straps coming in a V from the centre of the back of the bag, rather than separate straps on each side. This style is sometimes called a satchel backpack.\n\nThe use of school bag satchels is common in the United Kingdom, Australia, Western Europe and Japan.\nIn Japan the term for a school bag satchel is \"randoseru\". The Unicode for the school satchel Emoji is U+1F392.\n\nMuch of the popularity of the satchel as a fashion accessory in the United Kingdom, the United States of America and Canada since 2008 is driven by the Cambridge Satchel Company, whose product was on a Guardian gift guide in 2009, and was described as a cross-body bag in a 2010 article.\n\n"}
{"id": "1515653", "url": "https://en.wikipedia.org/wiki?curid=1515653", "title": "Satellite navigation", "text": "Satellite navigation\n\nA satellite navigation or satnav system is a system that uses satellites to provide autonomous geo-spatial positioning. It allows small electronic receivers to determine their location (longitude, latitude, and altitude/elevation) to high precision (within a few metres) using time signals transmitted along a line of sight by radio from satellites. The system can be used for providing position, navigation or for tracking the position of something fitted with a receiver (satellite tracking). The signals also allow the electronic receiver to calculate the current local time to high precision, which allows time synchronisation. Satnav systems operate independently of any telephonic or internet reception, though these technologies can enhance the usefulness of the positioning information generated.\n\nA satellite navigation system with global coverage may be termed a global navigation satellite system (GNSS). , the United States' Global Positioning System (GPS) and Russia's GLONASS are fully operational GNSSs, with China's BeiDou Navigation Satellite System (BDS) and the European Union's Galileo scheduled to be fully operational by 2020. India, France and Japan are in the process of developing regional navigation and augmentation systems as well.\n\nGlobal coverage for each system is generally achieved by a satellite constellation of 18–30 medium Earth orbit (MEO) satellites spread between several orbital planes. The actual systems vary, but use orbital inclinations of >50° and orbital periods of roughly twelve hours (at an altitude of about ).\n\nSatellite navigation systems that provide enhanced accuracy and integrity monitoring usable for civil navigation are classified as follows:\n\n\nGround based radio navigation has long been practiced. The DECCA, LORAN, GEE and Omega systems used terrestrial longwave radio transmitters which broadcast a radio pulse from a known \"master\" location, followed by a pulse repeated from a number of \"slave\" stations. The delay between the reception of the master signal and the slave signals allowed the receiver to deduce the distance to each of the slaves, providing a fix.\n\nThe first satellite navigation system was Transit, a system deployed by the US military in the 1960s. Transit's operation was based on the Doppler effect: the satellites travelled on well-known paths and broadcast their signals on a well-known radio frequency. The received frequency will differ slightly from the broadcast frequency because of the movement of the satellite with respect to the receiver. By monitoring this frequency shift over a short time interval, the receiver can determine its location to one side or the other of the satellite, and several such measurements combined with a precise knowledge of the satellite's orbit can fix a particular position. Satellite orbital position errors are induced by variations in the gravity field and radar refraction, among others. These were resolved by a team led by Harold L Jury of Pan Am Aerospace Division in Florida from 1970-1973. Using real-time data assimilation and recursive estimation, the systematic and residual errors were narrowed down to a manageable level to permit accurate navigation. \n\nPart of an orbiting satellite's broadcast included its precise orbital data. In order to ensure accuracy, the US Naval Observatory (USNO) continuously observed the precise orbits of these satellites. As a satellite's orbit deviated, the USNO would send the updated information to the satellite. Subsequent broadcasts from an updated satellite would contain its most recent ephemeris.\n\nModern systems are more direct. The satellite broadcasts a signal that contains orbital data (from which the position of the satellite can be calculated) and the precise time the signal was transmitted. The orbital ephemeris is transmitted in a data message that is superimposed on a code that serves as a timing reference. The satellite uses an atomic clock to maintain synchronization of all the satellites in the constellation. The receiver compares the time of broadcast encoded in the transmission of three (at sea level) or four different satellites, thereby measuring the time-of-flight to each satellite. Several such measurements can be made at the same time to different satellites, allowing a continual fix to be generated in real time using an adapted version of trilateration: see GNSS positioning calculation for details.\n\nEach distance measurement, regardless of the system being used, places the receiver on a spherical shell at the measured distance from the broadcaster. By taking several such measurements and then looking for a point where they meet, a fix is generated. However, in the case of fast-moving receivers, the position of the signal moves as signals are received from several satellites. In addition, the radio signals slow slightly as they pass through the ionosphere, and this slowing varies with the receiver's angle to the satellite, because that changes the distance through the ionosphere. The basic computation thus attempts to find the shortest directed line tangent to four oblate spherical shells centred on four satellites. Satellite navigation receivers reduce errors by using combinations of signals from multiple satellites and multiple correlators, and then using techniques such as Kalman filtering to combine the noisy, partial, and constantly changing data into a single estimate for position, time, and velocity.\n\nThe original motivation for satellite navigation was for military applications. Satellite navigation allows precision in the delivery of weapons to targets, greatly increasing their lethality whilst reducing inadvertent casualties from mis-directed weapons. (See Guided bomb). Satellite navigation also allows forces to be directed and to locate themselves more easily, reducing the fog of war.\n\nThe ability to supply satellite navigation signals is also the ability to deny their availability. The operator of a satellite navigation system potentially has the ability to degrade or eliminate satellite navigation services over any territory it desires.\n\nThe United States' Global Positioning System (GPS) consists of up to 32 medium Earth orbit satellites in six different orbital planes, with the exact number of satellites varying as older satellites are retired and replaced. Operational since 1978 and globally available since 1994, GPS is the world's most utilized satellite navigation system.\n\nThe formerly Soviet, and now Russian, \"Global'naya Navigatsionnaya Sputnikovaya Sistema\", (GLObal NAvigation Satellite System or GLONASS), is a space-based satellite navigation system that provides a civilian radionavigation-satellite service and is also used by the Russian Aerospace Defence Forces. GLONASS has full global coverage with 24 satellites.\n\nThe European Union and European Space Agency agreed in March 2002 to introduce their own alternative to GPS, called the Galileo positioning system. Galileo became operational on 15 December 2016 (global Early Operational Capability (EOC)) At an estimated cost of €3 billion, the system of 30 MEO satellites was originally scheduled to be operational in 2010. The original year to become operational was 2014. The first experimental satellite was launched on 28 December 2005. Galileo is expected to be compatible with the modernized GPS system. The receivers will be able to combine the signals from both Galileo and GPS satellites to greatly increase the accuracy. Galileo is expected to be in full service in 2020 and at a substantially higher cost.\nThe main modulation used in Galileo Open Service signal is the Composite Binary Offset Carrier (CBOC) modulation.\n\nChina has indicated their plan to complete the entire second generation Beidou Navigation Satellite System (BDS or BeiDou-2, formerly known as COMPASS), by expanding current regional (Asia-Pacific) service into global coverage by 2020. The BeiDou-2 system is proposed to consist of 30 MEO satellites and five geostationary satellites. A 16-satellite regional version (covering Asia and Pacific area) was completed by December 2012.\n\nChinese regional (Asia-Pacific, 16 satellites) network to be expanded into the whole BeiDou-2 global system which consists of all 35 satellites by 2020.\n\nThe NAVIC or NAVigation with Indian Constellation is an autonomous regional satellite navigation system developed by Indian Space Research Organisation (ISRO) which would be under the total control of Indian government. The government approved the project in May 2006, with the intention of the system completed and implemented on 28 April 2016. It will consist of a constellation of 7 navigational satellites. 3 of the satellites will be placed in the Geostationary orbit (GEO) and the remaining 4 in the Geosynchronous orbit(GSO) to have a larger signal footprint and lower number of satellites to map the region. It is intended to provide an all-weather absolute position accuracy of better than 7.6 meters throughout India and within a region extending approximately 1,500 km around it. A goal of complete Indian control has been stated, with the space segment, ground segment and user receivers all being built in India. All seven satellites, IRNSS-1A, IRNSS-1B, IRNSS-1C, IRNSS-1D, IRNSS-1E, IRNSS-1F, and IRNSS-1G, of the proposed constellation were precisely launched on 1 July 2013, 4 April 2014, 16 October 2014, 28 March 2015, 20 January 2016, 10 March 2016 and 28 April 2016 respectively from Satish Dhawan Space Centre. The system is expected to be fully operational by August 2016.\n\nThe Quasi-Zenith Satellite System (QZSS) is a proposed four-satellite regional time transfer system and enhancement for GPS covering Japan and the Asia-Oceania regions. QZSS services are available on a trial basis as of January 12, 2018, and are scheduled to be launched in November 2018. The first satellite was launched in September 2010.\n\nSources: \n\nGNSS augmentation is a method of improving a navigation system's attributes, such as accuracy, reliability, and availability, through the integration of external information into the calculation process, for example, the Wide Area Augmentation System, the European Geostationary Navigation Overlay Service, the Multi-functional Satellite Augmentation System, Differential GPS, GPS Aided GEO Augmented Navigation (GAGAN) and inertial navigation systems.\n\nDoppler Orbitography and Radio-positioning Integrated by Satellite (DORIS) is a French precision navigation system. Unlike other GNSS systems, it is based on static emitting stations around the world, the receivers being on satellites, in order to precisely determine their orbital position. The system may be used also for mobile receivers on land with more limited usage and coverage. Used with traditional GNSS systems, it pushes the accuracy of positions to centimetric precision (and to millimetric precision for altimetric application and also allows monitoring very tiny seasonal changes of Earth rotation and deformations), in order to build a much more precise geodesic reference system.\n\nThe two current operational low Earth orbit satellite phone networks are able to track transceiver units with accuracy of a few kilometers using doppler shift calculations from the satellite. The coordinates are sent back to the transceiver unit where they can be read using AT commands or a graphical user interface. This can also be used by the gateway to enforce restrictions on geographically bound calling plans.\n\n\n\n\n"}
{"id": "58383744", "url": "https://en.wikipedia.org/wiki?curid=58383744", "title": "Separation principle in stochastic control", "text": "Separation principle in stochastic control\n\nThe separation principle is one of the fundamental principles of stochastic control theory, which states that the problems of optimal control and state estimation can be decoupled under certain conditions. In its most basic formulation it deals with a linear stochastic system\nwith a state process formula_2, an output process formula_3 and a control formula_4, where formula_5 is a vector-valued Wiener process, formula_6 is a zero-mean Gaussian random vector independent of formula_5, formula_8, and formula_9, formula_10, formula_11, formula_12, formula_13 are matrix-valued functions which generally are taken to be continuous of bounded variation. Moreover, formula_14 is nonsingular on some interval formula_15. The problem is to design an output feedback law formula_16 which maps the observed process formula_3 to the control input formula_4 in a nonanticipatory manner so as to minimize the functional\nwhere formula_20 denotes expected value, prime (formula_21) denotes transpose. and formula_22 and formula_23 are continuous matrix functions of bounded variation, formula_24 is positive semi-definite and formula_25 is positive definite for all formula_26. Under suitable conditions, which need to be properly stated, formula_27 can be chosen in the form\nwhere formula_29 is the linear least-squares estimate of the state vector formula_30 obtained from the Kalman filter\nwhere formula_32 is the gain of the optimal linear-quadratic regulator obtained by taking formula_33 and formula_6 deterministic, and where formula_35 is the Kalman gain. There is also a non-Gaussian version of this problem (to be discussed below) where the Wiener process formula_5 is replaced by a more general square-integrable martingale with possible jumps.. In this case, the Kalman filter needs to be replace by a nonlinear filter providing an estimate of the (strict sense) conditional mean\nwhere\nis the \"filtration\" generated by the output process; i.e., the family of increasing sigma fields representing the data as it is produced.\n\nIn the early literature on the separation principle it was common to allow as admissible controls formula_4 all processes that are \"adapted\" to the filtration formula_40 in the class of deterministically well-posed control laws that minimizes formula_41, and it is given by\nwhere formula_32 is the deterministic control gain and formula_44 is given by the linear (distributed) filter\nwhere formula_46 is the innovation process\nand the gain formula_2 is as defined in page 120 in Lindquist.\n"}
{"id": "26478177", "url": "https://en.wikipedia.org/wiki?curid=26478177", "title": "Sex dice", "text": "Sex dice\n\nSex dice is a dice game intended to heighten the sexual atmosphere and promote foreplay. Instead of numbers, each face on the dice contains the name of a body part; the body part that faces up when the die is rolled must then be given sexual attention. \"The Daily Princetonian\" suggests rolling sex dice to \"break the ice and extend [one's] foreplay.\" The \"University Daily Kansan\" advises a roll of the sex dice for those who are not particularly limber (and therefore cannot try \"new and inventive position[s]\") as a means to \"bring variety to [one's] bedroom romps.\"\n\nA commercially available version, also called Foreplay Dice, consists of two dice, one with body parts and the other with activities; a roll of the dice will determine which action is to be applied to which body part. According to \"SPIN\" magazine, the game is especially popular among American teenagers.\n"}
{"id": "5248591", "url": "https://en.wikipedia.org/wiki?curid=5248591", "title": "Shower gel", "text": "Shower gel\n\nShower gel (also shower cream or body wash) is a specialized liquid product used for cleaning the body during showers. Not to be confused with liquid soaps, shower gels, in fact, do not contain saponified oil. Instead, it uses synthetic detergents derived from either petroleum or plant sources.\n\nBody washes and shower gels holds a lower pH value than the traditional soap, which is also known to feel less drying to the skin. In certain cases, sodium stearate is added to the chemical combination to create a solid version of the shower gel.\n\nShower gel is a derivative invention of the liquid soap, which first appeared in the 1800s. In 1865, William Shepphard patented the formula behind the liquid soap, but the product gained eventual popularity with the rise of Palmolive soap in 1898, by B.J. Johnson.\n\nModern chemistry later enabled the creation of the shower gel, which specialized in cleaning the entire body during baths and showers.\n\nShower gels are known to consist of the same basic ingredients as soap - water, betaines, and sodium laureth sulfate, or SLS. But the main difference between the two products lie in its surfactants - compounds known to lower the surface tension between substances, which helps in the emulsification and the washing away of oily dirt. The surfactants of shower gels do not come from saponification, that is by reacting a type of oil or fat with lye. Instead, it uses synthetic detergents for surfactants derived from either plant-based sources or petroleum. This gives the product a lower pH value than soap and might also feel less drying to the skin. Some people have likened the effect to feeling less \"squeaky clean\", however. \n\nSurfactants can make up as much as 50 percent of the shower gel content, with the remaining proportion being made up of a combination of water and ingredients to thicken, preserve, emulsify, add fragrance, and color. Multiple surfactants are often used to achieve desired product qualities. A primary surfactant can provide good foaming ability and cleaning effectiveness, while a secondary surfactant can add qualities of mildness to prevent irritation or over-drying of the skin. To prevent shower gel ingredients from separating, emulsifiers such as diethanolamine are added. Conditioning agents may also be added to moisturize the skin during and after product use. They are also available in different colours and scents. Ingredients, like scent in the form of essential oils or fragrance oils and colorant in the form of water soluble dyes are common in shower gels. \n\nMicrobeads were commonly used in shower gels until recently. Microbeads are tiny spheres of plastic that were added to a variety of cosmetic products for their exfoliating qualities.  They are too small to filter out of water systems and end up in waterways and oceans, potentially passing toxins to animal life and humans. Following the legislative actions of other countries, the United States passed the Microbead-Free Waters Act in 2015, which bans microbeads in the U.S. incrementally starting in 2017, with full implementation set for 2019. \n\nShower gels for men may contain the ingredient menthol, which gives a cooling and stimulating sensation on the skin, and some men's shower gels are also designed specifically for use on hair and body. Shower gels contain milder surfactant bases than shampoos, and some also contain gentle conditioning agents in the formula. This means that shower gels can also double as an effective and perfectly acceptable substitute to shampoo, even if they are not labelled as a hair and body wash. Washing hair with shower gel should give approximately the same result as using a moisturising shampoo.\n\nLike shampoo and bubble bath products, many are marketed directly towards children. These often feature scents intended to appeal to children, such as fruit scents, or cookies or cotton candy scents. Many bottles feature popular characters from children's television or movies. As with men's body wash, they often are specifically designed to be used also as a shampoo and conditioner. They also often contain gentle ingredients designed for young skin.\n\n"}
{"id": "7490319", "url": "https://en.wikipedia.org/wiki?curid=7490319", "title": "Silicon photomultiplier", "text": "Silicon photomultiplier\n\nSilicon photomultipliers, often called \"SiPM\" in the literature, are solid-state single-photon-sensitive devices based on Single-photon avalanche diode (SPAD) implemented on common silicon substrate. The dimension of each single SPAD can vary from 10 to 100 micrometres, and their density can be up to 10000 per square millimeter. Every SPAD in SiPM operates in Geiger mode and is coupled with the others by a metal or polysilicon quenching resistor. Although the device works in digital/switching mode, most of SiPM are an analog device because all the microcells are read in parallel, making it possible to generate signals within a dynamic range from a single photon to 1000 photons for a device with just a square-millimeter area. More advanced readout schemes are utilized for the lidar applications. The supply voltage (\"V\") depends on APD technology used and typically varies between 20 V and 100 V, thus being from 15 to 75 times lower than the voltage required for a traditional photomultiplier tubes (PMTs) operation.\n\nTypical specifications for a SiPM:\n\nSiPM for medical imaging are attractive candidates for the replacement of the conventional PMT in PET and SPECT imaging, since they provide high gain with low voltage and fast response, they are very compact and compatible with magnetic resonance setups. Nevertheless, there are still several challenges, for example, SiPM requires optimization for larger matrices, signal amplification and digitization.\n\n"}
{"id": "15750290", "url": "https://en.wikipedia.org/wiki?curid=15750290", "title": "Sitronics", "text": "Sitronics\n\nSitronics () is a microelectronics company based in Moscow, Russia and controlled by Sistema holding, that also manufactures mobile phones. Its main assets are the electronics fabs, research and development facilities NIIME and Mikron in Zelenograd, and other facilities elsewhere in Eastern Europe.\n\nDespite losses in 2007, the company is planning to remain a part of Sistema and to expand its product portfolio, bidding for large telecommunications contracts such as a $1 billion deal to build a network in Saudi Arabia.\n\nSitronics was founded in 1997 in Zelenograd and was initially named \"Scientific Center\". In January 2005 its headquarters was moved to Moscow and in November 2005 the company got its current name. On February 7, 2007 the company placed an IPO on the London Stock Exchange. Through a series of acquisitions it has become a leading Russian microelectronics company with revenues exceeding $1 billion in the first 9 months of 2007. The company delisted from the London Stock Exchange in 2012.\n\nThe company will produce nano-SIM cards on orders from one of Russia’s big three mobile phone operators to be used in new Apple iPhones. The first batch will comprise about 50,000 cards that will be used in iPhone 5 gadgets.\n\nSitronics also makes an array of mobile phones.\n\n\n"}
{"id": "37506594", "url": "https://en.wikipedia.org/wiki?curid=37506594", "title": "Snapchat", "text": "Snapchat\n\nSnapchat is a multimedia messaging app used globally, created by Evan Spiegel, Bobby Murphy, and Reggie Brown, former students at Stanford University, and developed by Snap Inc., originally Snapchat Inc.\n\nOne of the principal features of Snapchat is that pictures and messages are usually only available for a short time before they become inaccessible to users. The app has evolved from originally focusing on person-to-person photo sharing to presently featuring users' \"Stories\" of 24 hours of chronological content, along with \"Discover\", letting brands show ad-supported short-form content.\n\nSnapchat has become notable for representing a new, mobile-first direction for social media, and places significant emphasis on users interacting with virtual stickers and augmented reality objects. As of February 2018, Snapchat has 187 million daily active users.\n\nAccording to documents and deposition statements, Reggie Brown brought the idea for a disappearing pictures application to Evan Spiegel because Spiegel had prior business experience. Brown and Spiegel then pulled in Bobby Murphy, who had experience coding. The three worked closely together for several months and launched Snapchat as \"Picaboo\" on the iOS operating system on July 8, 2011. Reggie Brown was ousted from the company months after it was launched.\n\nThe app was relaunched as Snapchat in September 2011, and the team focused on usability and technical aspects, rather than branding efforts. One exception was the decision to keep a mascot designed by Brown, \"Ghostface Chillah\", named after Ghostface Killah of the hip-hop group Wu-Tang Clan.\n\nOn May 8, 2012, Reggie Brown sent an email to Evan Spiegel during their senior year at Stanford in which he offered to re-negotiate his equitable share regarding ownership of the company. Lawyers for Snapchat responded by insisting that he had never had any creative connection to the product. The attorneys also accused Brown of committing fraud against Spiegel and Murphy by falsely claiming to be a product inventor. On behalf of their clients, the law firm concluded that Reggie Brown had made no contributions of value or worth, and was therefore entitled to a share of nothing. In September 2014, Brown settled with Spiegel and Murphy for $157.5 million and was credited as one of the original authors of Snapchat.\n\nIn their first blog post, dated May 9, 2012, CEO Evan Spiegel described the company's mission: \"Snapchat isn’t about capturing the traditional Kodak moment. It’s about communicating with the full range of human emotion — not just what appears to be pretty or perfect.\" He presented Snapchat as the solution to stresses caused by the longevity of personal information on social media, evidenced by \"emergency detagging of Facebook photos before job interviews and photoshopping blemishes out of candid shots before they hit the internet\".\n\nAs of May 2012, 25 Snapchat images were being sent per second and, as of November 2012, users had shared over one billion photos on the Snapchat iOS app, with 20 million photos being shared per day. That same month, Spiegel cited problems with user base scalability as the reason why Snapchat was experiencing some difficulties delivering its images, known as \"snaps\", in real time. Snapchat was released as an Android app on October 29, 2012.\n\nIn June 2013, Snapchat version 5.0, dubbed \"Banquo\", was released for iOS. The updated version introduced several speed and design enhancements, including swipe navigation, double-tap to reply, an improved friend finder, and in-app profiles. The name is a reference to the ghostly hero from Shakespeare's Macbeth, a character in the play who is ultimately seen to be victorious over evil. Also in June 2013, Snapchat introduced Snapkidz for users under 13 years of age. Snapkidz was part of the original Snapchat application and was activated when the user provided a date of birth to verify his/her age. Snapkidz allowed children to take snaps and draw on them, but they could not send snaps to other users and could only save snaps locally on the device being used.\n\nAccording to Snapchat's published statistics, as of May 2015, the app's users were sending 2 billion videos per day, reaching 6 billion by November. By 2016, Snapchat had hit 10 billion daily video views. In May 2016, Snapchat raised $1.81 billion in equity offering, suggesting strong investor interest in the company. By May 31, 2016, the app had almost 10 million daily active users in the United Kingdom. In February 2017, Snapchat had 160 million daily active users, growing to 166 million in May.\nIn September 2016, Snapchat Inc. was renamed Snap Inc. to coincide with the introduction of the company's first hardware product, Spectacles— smartglasses with a built-in camera that can record 10 seconds of video at a time. On February 20, 2017, Spectacles became available for purchase online.\n\nSnapchat is primarily used for creating multimedia messages referred to as \"snaps\"; snaps can consist of a photo or a short video, and can be edited to include filters and effects, text captions, and drawings. Snaps can be directed privately to selected contacts, or to a semi-public \"Story\". The ability to send video snaps was added as a feature option in December 2012. By holding down on the photo button while inside the app, a video of up to ten seconds in length can be captured. After a single viewing, the video disappears by default. Spiegel explained that this process allowed the video data to be compressed into the size of a photo. On May 1, 2014, the ability to communicate via video chat was added. Direct messaging features were also included in the update, allowing users to send ephemeral text messages to friends and family while saving any needed information by clicking on it.\n\nPrivate message photo snaps can be viewed for a user-specified length of time (1 to 10 seconds as determined by the sender) before they become inaccessible. Users were previously required to hold down on the screen in order to view a snap; this behavior was removed in July 2015 The requirement to hold on the screen was intended to frustrate the ability to take screenshots of snaps; the Snapchat app does not prevent screenshots from being taken but can notify the sender if it detects that it has been saved. However, these notifications can be bypassed through either unauthorized modifications to the app or by obtaining the image through external means. One snap per day can be replayed for free. In September 2015, Snapchat introduced the option to purchase additional replays through in-app purchases. The ability to purchase extra replays was removed in April 2016.\n\nFriends can be added via usernames and phone contacts, using customizable \"Snapcodes\", or through the \"Add Nearby\" function, which scans for users near their location who are also in the Add Nearby menu. Spiegel explained that Snapchat is intended to counteract the trend of users being compelled to manage an idealized online identity of themselves, which he says has \"taken all of the fun out of communicating\".\n\nIn November 2014, Snapchat introduced \"Snapcash\", a feature that lets users send and receive money to each other through private messaging. The payments system is powered by Square.\n\nIn July 2016, Snapchat introduced a new, optional feature known as \"Memories\". Memories allows snaps and story posts to be saved into a private storage area, where they can be viewed alongside other photos stored on the device, as well as edited and published as snaps, story posts, or messages. When shared to a user's current story, the memory would have a white frame and timestamp to indicate its age. Content in the Memories storage area can be searched by date or using a local object recognition system. Snaps accessible within Memories can additionally be placed into a \"My Eyes Only\" area that is locked with a PIN. Snapchat has stated that the Memories feature was inspired by the practice of manually scrolling through photos on a phone to show them to others. In April 2017, the white border around old memories was removed. While originally intended to let viewers know the material was old, \"TechCrunch\" wrote that the indicator \"ended up annoying users who didn’t want their snaps altered, sometimes to the point where they would decide not to share the old content at all\".\n\nIn May 2017, an update made it possible to send snaps with unlimited viewing time, dropping the previous ten-second maximum duration, with the content disappearing after being deliberately closed by the recipient. New creative tools, namely the ability to draw with an emoji, videos that play in a loop, and an eraser that lets users remove objects in a photo with the app filling in the space with the background, were also released.\n\nIn June 2017, Snapchat started allowing users to add links to snaps, enabling them to direct viewers to specific websites; the feature was only available for brands previously. Additionally, the update added more creative tools: A \"Backdrop\" feature lets users cut out a specific object from their photo and apply colorful patterns to it in order to bring greater emphasis to that object, and \"Voice Filters\" enable users to remix the sounds of their voices in the snap. Voice Filters was previously available as part of the feature enabling augmented reality lenses, with the new update adding a dedicated speaker icon to remix the audio in any snap.\n\nSnaps can be personalized with various forms of visual effects and stickers. Geofilters are graphical overlays available if the user is within a certain geographical location, such as a city, event, or destination. A similar feature known as Geostickers was launched in 10 major cities in 2016. Bitmoji are stickers featuring personalized cartoon avatars, which can be used in snaps and messaging. Bitmoji characters can also be used as World Lenses.\n\nThe \"Lens\" feature, introduced in September 2015, allows users to add real-time effects into their snaps by using face detection technology. This is activated by long-pressing on a face within the viewfinder. In April 2017, Snapchat extended this feature into \"World Lenses\", which use augmented reality technology to integrate 3D rendered elements (such as objects and animated characters) into scenes; these elements are placed and anchored in 3D space.\n\nOn October 26, 2018 at TwitchCon, Snap launched the Snap Camera desktop application for macOS and Windows PCs, which enables use of Snapchat lenses in video chat and live streaming services such as Skype, Twitch, YouTube, and Zoom. Snapchat also launched integration with Twitch, including an in-stream widget for Snapcodes, the ability to offer lenses to stream viewers and as an incentive to channel subscribers. Several video game-themed lenses were also launched at this time, including ones themed around \"League of Legends\", \"Overwatch\", and \"PlayerUnknown's Battlegrounds\".\n\nIn October 2013, Snapchat introduced the \"My Story\" feature, which allows users to compile snaps into chronological storylines, accessible to all of their friends. By June 2014, photo and video snaps presented to friends in the Stories functionality had surpassed person-to-person private snaps as the most frequently-used function of the service, with over one billion viewed per day — double the daily views tallied in April 2014.\n\nIn June 2014, the story feature was expanded to incorporate \"Our Stories\", which was then changed to \"Live Stories\" about a year later. The feature allows users on-location at specific events (such as music festivals or sporting events) to contribute snaps to a curated story advertised to all users, showcasing a single event from multiple perspectives and viewpoints. These curated snaps provided by the app's contributors and selected for the \"Live\" section could also be more localized, but Snapchat eventually scaled back the more personal imaging streams in order to emphasize public events.\n\nAn \"Official Stories\" designation was added in November 2015 to denote the public stories of notable figures and celebrities, similar to Twitter's \"Verified account\" program.\n\nIn January 2015, Snapchat introduced \"Discover\", an area containing channels of ad-supported short-form content from major publishers, including BuzzFeed, CNN, ESPN, Mashable, \"People\", Vice and Snapchat itself among others. To address data usage concerns related to these functions, a \"Travel Mode\" option was added in August 2015. When activated, the feature prevents the automatic downloading of snaps until they are explicitly requested by the user.\n\nIn October 2016, the app was updated to replace its auto-advance functionality, which automatically moved users from one story to the next, with a \"Story Playlist\" feature, letting users select thumbnails of users in the list to play only selected stories.\n\nIn January 2017, Snapchat revamped its design, adding search functionality and a new global live \"Our Story\" feature, to which any user can contribute.\n\nIn May 2017, Snapchat introduced \"Custom Stories\", letting users collaboratively make stories combining their captures.\n\nIn June 2017, \"Snap Map\" was introduced, which allows users to optionally share their location with friends. A map display, accessible from the viewfinder, can be used to locate stories based on location data, supporting the use of Bitmoji as place markers. Entering a \"Ghost Mode\" hides the user from the map. The function is based on the app Zenly, which was acquired by Snap Inc. prior to its launch. The map data is supplied from OpenStreetMap and Mapbox, while satellite imagery comes from DigitalGlobe.\n\n\"The Wall Street Journal\" reported in May 2017 that Snap Inc., the company developing Snapchat, had signed deals with NBCUniversal, A&E Networks, BBC, ABC, Metro–Goldwyn–Mayer and other content producers to develop original shows for viewing through Snapchat's \"Stories\" format. According to the report, Snap hopes to have several new shows available on a daily basis, with each show lasting between three and five minutes, and the company has sent out detailed reports to its partners on how to produce content for Snapchat.\n\nIn contrast to other messaging apps, Spiegel described Snapchat's messaging functions as being \"conversational,\" rather than \"transactional,\" as they sought to replicate the conversations he engaged in with friends. Spiegel stated that he did not experience conversational interactions while using the products of competitors like iMessage.\n\nRather than a traditional online notification, a blue pulsing \"Here\" button is displayed within the sender's chat window if the recipient is currently viewing their own chat window. When this button is held down, a video chat function is immediately launched. By default, messages disappear after they are read, and a notification is sent to the recipient only when they start to type. Users can also use messages to reply to snaps that are part of a story. The video chat feature uses technology from AddLive—a real-time communications provider that Snapchat acquired prior to the feature's launch. In regards to the \"Here\" indicator, Spiegel explained that \"the accepted notion of an online indicator that every chat service has is really a negative indicator. It means 'my friend is available and doesn't want to talk to you,' versus this idea in Snapchat where 'my friend is here and is giving you their full attention.'\" Spiegel further claimed that the Here video function prevents the awkwardness that can arise from apps that use typing indicators because, with text communication, conversations lose their fluidity as each user tries to avoid typing at the same time.\n\nOn March 29, 2016, Snapchat launched a major revision of the messaging functionality known as \"Chat 2.0\", adding stickers, easier access to audio and video conferencing, the ability to leave audio or video \"notes\", and the ability to share recent camera photos. The implementation of these features are meant to allow users to easily shift between text, audio, and video chat as needed while retaining an equal level of functionality. In June 2018, Snapchat added the feature of deleting a sent message (including; audio, video, and text) before it is read. A feature introduced in August 2018, allows users to send Musical GIFs, TuneMojis.\n\nFrom its earliest days, Snapchat's main demographic has consisted of millennials. In 2014, researchers from the University of Washington and Seattle Pacific University designed a user survey to help understand how and why the application was being used. The researchers originally hypothesized that due to the ephemeral nature of Snapchat messages, its use would be predominantly for privacy-sensitive content including the much talked about potential use for sexual content and sexting. However, it appears that Snapchat is used for a variety of creative purposes that are not necessarily privacy-related at all. In the study, only 1.6% of respondents reported using Snapchat primarily for sexting, although 14.2% admitted to having sent sexual content via Snapchat at some point. These findings suggest that users do not seem to utilize Snapchat for sensitive content. Rather, the primary use for Snapchat was found to be for comedic content such as \"stupid faces\" with 59.8% of respondents reporting this use most commonly. The researchers also determined how Snapchat users do not use the application and what types of content they are not willing to send. They found that the majority of users are not willing to send content classified as sexting (74.8% of respondents), photos of documents (85.0% of respondents), messages containing legally questionable content (86.6% of respondents), or content considered mean or insulting (93.7% of respondents).\n\nThe study results also suggested that Snapchat's success is not due to its security properties, but because the users found the application to be fun. The researchers found that users seem to be well aware (79.4% of respondents) that recovering snaps is possible and a majority of users (52.8% of respondents) report that this does not affect their behavior and use of Snapchat. Many users (52.8% of respondents) were found to use an arbitrary timeout length on snaps regardless of the content type or recipient. The remaining respondents were found to adjust their snaps' timeout depending on the content or the recipient. Reasons for adjusting the time length of snaps included the level of trust and relationship with the recipient, the time needed to comprehend the snap, and avoiding screenshots.\n\nSnapchat has often been seen to represent a new direction in social media, with its users, particularly millennials, craving a more in-the-moment way of sharing and communicating via technology. With less emphasis on the accumulation of an ongoing status involving the presence of permanent material, Snapchat put focus on the ephemeral nature of fleeting encounters.\nBuilding on this distinction by launching as a mobile-first company, Snapchat, in the midst of the app revolution and the growing presence of cellular communication, didn't have to make the transition to mobile in the way other competing social media networks had to do. Evan Spiegel himself described Snapchat as primarily a camera company. Spiegel also dismissed past comparisons to other social media networks such as Facebook and Twitter when he was asked if the 2016 presidential race was going to be remembered as the Snapchat election, although major candidates did occasionally use the app to reach voters. Nevertheless, the growing mobile app moved to offer distinct publication, media, and news content within its Discover channel, as well as with its overall style of presentation. With Snapchat, a clear and identifiable line was drawn between brand content and user-based messaging and sharing, once again distinguishing the popular app from other social media networks, which typically have blended and blurred their different varieties of content.\n\nSnapchat's developing features embody a deliberate strategy of monetization.\n\nSnapchat announced its then-upcoming advertising efforts on October 17, 2014, when it acknowledged its need for a revenue stream. The company stated that it wanted to evaluate \"if we can deliver an experience that's fun and informative, the way ads used to be, before they got creepy and targeted.\" Snapchat's first paid advertisement, in the form of a 20-second movie trailer for the horror film \"Ouija\", was shown to users on October 19, 2014.\n\nIn January 2015, Snapchat began making a shift from focusing on growth to monetization. The company launched its \"Discover\" feature, which allowed for paid advertising by presenting short-form content from publishers. Its initial launch partners included CNN, Comedy Central, ESPN and Food Network, among others. In June 2015, Snapchat announced that it would allow advertisers to purchase sponsored geofilters for snaps; an early customer of the offering was McDonald's, who paid for a branded geofilter covering its restaurant locations in the United States. Snapchat made a push to earn ad revenue from its \"Live Stories\" feature in 2015, after initially launching the feature in 2014. Ad placements can be sold within a live story, or a story can be pitched by a sponsor. Live stories are estimated to reach an average of 20 million viewers in a 24-hour span. In September 2015, the service entered into a partnership with the National Football League to present live stories from selected games (including a Sunday game, and marquee games such as Monday Night Football and Thursday Night Football), with both parties contributing content and handling ad sales. The 2015 Internet Trends Report by Mary Meeker highlighted the significant growth of vertical video viewing. Vertical video ads like Snapchat's are watched in their entirety nine times more than landscape video ads.\n\nIn April 2016, NBC Olympics announced that it had reached a deal with Snapchat to allow stories from the 2016 Summer Olympics to be featured on Snapchat in the United States. The content would include a behind-the-scenes Discover channel curated by BuzzFeed (a company which NBCUniversal has funded), and stories featuring a combination of footage from NBC, athletes, and attendees. NBC will sell advertising and enter into revenue sharing agreements. This marked the first time NBC allowed Olympics footage to be featured on third-party property. In May 2016, as part of a campaign to promote \"\", 20th Century Fox paid for the entire array of lenses to be replaced by those based on characters from the \"X-Men\" series and films for a single day. In July 2016, it was reported that Snapchat had submitted a patent application for the process of using an object recognition system to deliver sponsored filters based on objects seen in a camera view. Later that year, in Sep 2016, Snapchat released its first hardware product, called the Spectacles. Evan Spiegel, CEO of Snap Inc., called it “a toy” but saw it as an upside to freeing his app from smartphone cameras.\n\nIn April 2017, \"Digiday\" reported that Snapchat would launch a self-service manager for advertising on the platform. The feature launched the following month, alongside news of a Snapchat Mobile Dashboard for tracking ad campaigns, set to roll out in June to select countries. Also in 2017, Snapchat introduced a \"Snap to Store\" advertising tool that lets companies using geostickers to track whether users buy their product or visit their store in a 7-day period after seeing the relevant geosticker. On November 13 2018, Snapchat announced the launch of the Snap Store, where they sell Bitmoji merchandise personalized by avatars from users and their friends. Items for sale include shirts, mugs, shower curtains, and phone cases.\n\nSnapchat is one of the top social networks that digital marketers are paying attention to. Fleetingness and temporality of Snapchat are its most attractive features in digital marketing. The feature is in conformity with real-time marketing and Snapchat makes good use of the real-time attention to this feature. It can help companies grasp the hot spots in promoting publicity so that it attracts the attention of the users. Unlike other social networks, the subject of Snapchat is video-based. Therefore, comparing to text, an interesting and creative video can leave a vivid memory in the audience. At the same time, \"Live Stories\" allows users to upload videos on specific activities and events. On one hand, multi-angle videos show more perspectives and meet the needs of the users. On the other hand, it appeals to more people to use Snapchat and join the events. In term of the marketing process, Snapchat plays the role of attracting attention, inducing interest and stimulating purchase desire for the customers. In other words, Snapchat plays an important role in digital marketing in combination with the AIDA (marketing) model and modern digital technology. This comprised some examples:\n\nSnapchat was hacked on December 31, 2013. Gibson Security, an Australian security firm, had disclosed an API security vulnerability to the company on August 27, 2013, and then made public the source code for the exploit on Christmas Day (Australian time; Christmas Eve in the US). On December 27, Snapchat announced that it had implemented mitigating features. Nonetheless, an anonymous group hacked them, saying that the mitigating features presented only \"minor obstacles\". The hackers revealed parts of approximately 4.6 million Snapchat usernames and phone numbers on a website named \"SnapchatDB.info\" and sent a statement to the popular technology blog \"TechCrunch\" saying that their objective had been to \"raise public awareness ... and ... put public pressure on Snapchat\" to fix the vulnerability. Snapchat apologized a week after the hack.\n\nIn 2014, Snapchat settled a complaint made by the Federal Trade Commission. The government agency alleged that the company had exaggerated to the public the degree to which mobile app images and photos could actually be made to disappear. Under the terms of the agreement, Snapchat was not fined, but the app service agreed to have its claims and policies monitored by an independent party for a period of 20 years. The FTC concluded that Snapchat was prohibited from \"misrepresenting the extent to which it maintains the privacy, security, or confidentiality of users' information.\"\n\nFollowing the agreement, Snapchat updated its privacy page to state that the company \"can't guarantee that messages will be deleted within a specific timeframe.\" Even after Snapchat deletes message data from their servers, that same data may remain in backup for a certain period of time. In a public blog post, the service warned that \"If you've ever tried to recover lost data after accidentally deleting a drive or maybe watched an episode of \"CSI\", you might know that with the right forensic tools, it's sometimes possible to retrieve data after it has been deleted.\"\n\nIn September 2015, an 18-year-old was using a Snapchat feature called \"Lens\" to record the speed she was driving her Mercedes C230 when she crashed into a Mitsubishi Outlander in Hampton, Georgia. The crash injured both drivers. The driver of the Outlander spent five weeks in intensive care while he was treated for severe traumatic brain injury. In April 2016, the Outlander driver sued both Snapchat and the user of Snapchat, alleging that Snapchat knew its application was being used in unlawful speed contests, yet did nothing to prevent such use so is negligent.\n\nA similar collision while driving at , occurred in Tampa, Florida in October 2016 that killed five people.\n\nIn 2016, Snapchat was sued by Canadian company for infringement on its geofiltering patent.\nThe company, Investel Capital Corp., sued Snapchat for patent infringement. They were seeking \"monetary compensation and an order that would prohibit California-based Snapchat from infringing on its patent in the future.\"\n\nAccording to former Snapchat employee Anthony Pompliano in a lawsuit filed against Snap Inc., Spiegel made a statement in 2015 that Snapchat is \"only for rich people\" and that he does not \"want to expand into poor countries like India and Spain\". The incident sparked a Twitter trend called \"#UninstallSnapchat\", in which Indian users uninstalled the app, and caused backlash against the company in terms of low \"one-star\" ratings for the app in the Google Play store and Apple's App Store. Snapchat's shares fell by 1.5%. In response to the allegation, Snapchat called Pompliano's claim \"ridiculous\", and elaborated that \"Obviously Snapchat is for everyone. It’s available worldwide to download for free\".\n\nThe June 2017 release of \"Snap Map\", a feature that broadcasts the user's location on a map, was met with concerns over privacy and safety. The feature, though an opt-in, delivers a message asking if the user would like to show their position on the map, but reportedly doesn't explain the ramifications of doing so, including that the app updates the user's position on the map each time the app is opened and not just when actively capturing snaps, potentially allowing for stalkers. The map can be zoomed in to feature detailed geographical information, such as street addresses. \"The Daily Telegraph\" reported that police forces had issued child safety warnings, while other media publications wrote that safety concerns were also raised for teenagers and adults unaware of the feature's actual behavior. In a statement to \"The Verge\", a Snapchat spokesperson said that \"The safety of our community is very important to us and we want to make sure that all Snapchatters, parents, and educators have accurate information about how the Snap Map works\".\nUsers have the ability to operate in \"Ghost Mode\", or select the friends that they wish to share their location with. Although there has been an increase in advertising on Snapchat, Snapchat has stated that they do not plan on running ads on Snap Map stories.\n\nIn November 2017, Snapchat announced a redesign, which proved controversial with many of its followers. CNBC's Ingrid Angulo listed some of the reasons why many disliked the update, citing that sending a snap and re-watching stories was more complicated, stories and incoming snaps were now listed on the same page, and that the Discover page now included featured and sponsored content.\n\nIn February 2018, Kylie Jenner sent a tweet criticizing the redesign of the Snapchat app. The tweet reportedly caused Snap Inc to lose more than $1.3 billion in market value.\n\nOver 1.2 million people signed a Change.org petition asking the company to remove the new app update.\n\nIn March 2018, a poll about Rihanna was posted stating, \"Would you rather punch Chris Brown or slap Rihanna?\" Rihanna tweeted that Snapchat was \"insensitive to domestic violence victims\" and urged fans to delete Snapchat.\n\n\n"}
{"id": "51702", "url": "https://en.wikipedia.org/wiki?curid=51702", "title": "Superscalar processor", "text": "Superscalar processor\n\nA superscalar processor is a CPU that implements a form of parallelism called instruction-level parallelism within a single processor. In contrast to a scalar processor that can execute at most one single instruction per clock cycle, a superscalar processor can execute more than one instruction during a clock cycle by simultaneously dispatching multiple instructions to different execution units on the processor. It therefore allows for more throughput (the number of instructions that can be executed in a unit of time) than would otherwise be possible at a given clock rate. Each execution unit is not a separate processor (or a core if the processor is a multi-core processor), but an execution resource within a single CPU such as an arithmetic logic unit.\n\nIn Flynn's taxonomy, a single-core superscalar processor is classified as an SISD processor (Single Instruction stream, Single Data stream), though many superscalar processors support short vector operations and so could be classified as SIMD (Single Instruction stream, Multiple Data streams). A multi-core superscalar processor is classified as an MIMD processor (Multiple Instruction streams, Multiple Data streams).\n\nWhile a superscalar CPU is typically also pipelined, superscalar and pipelining execution are considered different performance enhancement techniques. The former executes multiple instructions in parallel by using multiple execution units, whereas the latter executes multiple instructions in the same execution unit in parallel by dividing the execution unit into different phases.\n\nThe superscalar technique is traditionally associated with several identifying characteristics (within a given CPU):\n\nSeymour Cray's CDC 6600 from 1966 is often mentioned as the first superscalar design. The 1967 IBM System/360 Model 91 was another superscalar mainframe. The Motorola MC88100 (1988), the Intel i960CA (1989) and the AMD 29000-series 29050 (1990) microprocessors were the first commercial single-chip superscalar microprocessors. RISC microprocessors like these were the first to have superscalar execution, because RISC architectures frees transistors and die area which could be used to include multiple execution units (this was why RISC designs were faster than CISC designs through the 1980s and into the 1990s).\n\nExcept for CPUs used in low-power applications, embedded systems, and battery-powered devices, essentially all general-purpose CPUs developed since about 1998 are superscalar.\n\nThe P5 Pentium was the first superscalar x86 processor; the Nx586, P6 Pentium Pro and AMD K5 were among the first designs which decode x86-instructions asynchronously into dynamic microcode-like \"micro-op\" sequences prior to actual execution on a superscalar microarchitecture; this opened up for dynamic scheduling of buffered \"partial\" instructions and enabled more parallelism to be extracted compared to the more rigid methods used in the simpler P5 Pentium; it also simplified speculative execution and allowed higher clock frequencies compared to designs such as the advanced Cyrix 6x86.\n\nThe simplest processors are scalar processors. Each instruction executed by a scalar processor typically manipulates one or two data items at a time. By contrast, each instruction executed by a vector processor operates simultaneously on many data items. An analogy is the difference between scalar and vector arithmetic. A superscalar processor is a mixture of the two. Each instruction processes one data item, but there are multiple execution units within each CPU thus multiple instructions can be processing separate data items concurrently.\n\nSuperscalar CPU design emphasizes improving the instruction dispatcher accuracy, and allowing it to keep the multiple execution units in use at all times. This has become increasingly important as the number of units has increased. While early superscalar CPUs would have two ALUs and a single FPU, a modern design such as the PowerPC 970 includes four ALUs, two FPUs, and two SIMD units. If the dispatcher is ineffective at keeping all of these units fed with instructions, the performance of the system will be no better than that of a simpler, cheaper design.\n\nA superscalar processor usually sustains an execution rate in excess of one instruction per machine cycle. But merely processing multiple instructions concurrently does not make an architecture superscalar, since pipelined, multiprocessor or multi-core architectures also achieve that, but with different methods.\n\nIn a superscalar CPU the dispatcher reads instructions from memory and decides which ones can be run in parallel, dispatching each to one of the several execution units contained inside a single CPU. Therefore, a superscalar processor can be envisioned having multiple parallel pipelines, each of which is processing instructions simultaneously from a single instruction thread.\n\nAvailable performance improvement from superscalar techniques is limited by three key areas:\n\nExisting binary executable programs have varying degrees of intrinsic parallelism. In some cases instructions are not dependent on each other and can be executed simultaneously. In other cases they are inter-dependent: one instruction impacts either resources or results of the other. The instructions codice_1 can be run in parallel because none of the results depend on other calculations. However, the instructions codice_2 might not be runnable in parallel, depending on the order in which the instructions complete while they move through the units.\n\nWhen the number of simultaneously issued instructions increases, the cost of dependency checking increases extremely rapidly. This is exacerbated by the need to check dependencies at run time and at the CPU's clock rate. This cost includes additional logic gates required to implement the checks, and time delays through those gates. Research shows the gate cost in some cases may be formula_1 gates, and the delay cost formula_2, where formula_3 is the number of instructions in the processor's instruction set, and formula_4 is the number of simultaneously dispatched instructions.\n\nEven though the instruction stream may contain no inter-instruction dependencies, a superscalar CPU must nonetheless check for that possibility, since there is no assurance otherwise and failure to detect a dependency would produce incorrect results.\n\nNo matter how advanced the semiconductor process or how fast the switching speed, this places a practical limit on how many instructions can be simultaneously dispatched. While process advances will allow ever greater numbers of execution units (e.g., ALUs), the burden of checking instruction dependencies grows rapidly, as does the complexity of register renaming circuitry to mitigate some dependencies. Collectively the power consumption, complexity and gate delay costs limit the achievable superscalar speedup to roughly eight simultaneously dispatched instructions.\n\nHowever even given infinitely fast dependency checking logic on an otherwise conventional superscalar CPU, if the instruction stream itself has many dependencies, this would also limit the possible speedup. Thus the degree of intrinsic parallelism in the code stream forms a second limitation.\n\nCollectively, these limits drive investigation into alternative architectural changes such as very long instruction word (VLIW), explicitly parallel instruction computing (EPIC), simultaneous multithreading (SMT), and multi-core computing.\n\nWith VLIW, the burdensome task of dependency checking by hardware logic at run time is removed and delegated to the compiler. Explicitly parallel instruction computing (EPIC) is like VLIW, with extra cache prefetching instructions.\n\nSimultaneous multithreading, often abbreviated as SMT, is a technique for improving the overall efficiency of superscalar processors. SMT permits multiple independent threads of execution to better utilize the resources provided by modern processor architectures.\n\nSuperscalar processors differ from multi-core processors in that the several execution units are not entire processors. A single processor is composed of finer-grained execution units such as the ALU, integer multiplier, integer shifter, FPU, etc. There may be multiple versions of each execution unit to enable execution of many instructions in parallel. This differs from a multi-core processor that concurrently processes instructions from \"multiple\" threads, one thread per processing unit (called \"core\"). It also differs from a pipelined processor, where the multiple instructions can concurrently be in various stages of execution, assembly-line fashion.\n\nThe various alternative techniques are not mutually exclusive—they can be (and frequently are) combined in a single processor. Thus a multicore CPU is possible where each core is an independent processor containing multiple parallel pipelines, each pipeline being superscalar. Some processors also include vector capability.\n\n\n\n"}
{"id": "3588720", "url": "https://en.wikipedia.org/wiki?curid=3588720", "title": "Vegetable oil fuel", "text": "Vegetable oil fuel\n\nVegetable oil can be used as an alternative fuel in diesel engines and in heating oil burners. When vegetable oil is used directly as a fuel, in either modified or unmodified equipment, it is referred to as straight vegetable oil (SVO) or pure plant oil (PPO). Conventional diesel engines can be modified to help ensure that the viscosity of the vegetable oil is low enough to allow proper atomization of the fuel. This prevents incomplete combustion, which would damage the engine by causing a build-up of carbon. Straight vegetable oil can also be blended with conventional diesel or processed into biodiesel or bioliquids for use under a wider range of conditions.\n\nRudolf Diesel was the father of the engine which bears his name. His first attempts were to design an engine to run on coal dust, but later designed his engine to run on vegetable oil. The idea, he hoped, would make his engines more attractive to farmers having a source of fuel readily available. In a 1912 presentation to the British Institute of Mechanical Engineers, he cited a number of efforts in this area and remarked, \"The fact that fat oils from vegetable sources can be used may seem insignificant today, but such oils may perhaps become in course of time of the same importance as some natural mineral oils and the tar products are now.\"\n\nPeriodic petroleum shortages spurred research into vegetable oil as a diesel substitute during the 1930s and 1940s, and again in the 1970s and early 1980s when straight vegetable oil enjoyed its highest level of scientific interest. The 1970s also saw the formation of the first commercial enterprise to allow consumers to run straight vegetable oil in their automobiles, Elsbett of Germany. In the 1990s Bougainville conflict, islanders cut off from oil supplies due to a blockade used coconut oil to fuel their vehicles.\n\nMost diesel car engines are suitable for the use of straight vegetable oil (SVO), also commonly called pure plant oil (PPO), with certain modifications. Principally, the viscosity and surface tension of the SVO/PPO must be reduced by preheating it, typically by using waste heat from the engine or electricity, otherwise poor atomization, incomplete combustion and carbonization may result. One common solution is to add a heat exchanger and an additional fuel tank for the petrodiesel or biodiesel blend and to switch between this additional tank and the main tank of SVO/PPO. The engine is started on diesel, switched over to vegetable oil as soon as it is warmed up and switched back to diesel shortly before being switched off to ensure that no vegetable oil remains in the engine or fuel lines when it is started from cold again. In colder climates it is often necessary to heat the vegetable oil fuel lines and tank as it can become very viscous and even solidify.\n\nSingle tank conversions have been developed, largely in Germany, which have been used throughout Europe. These conversions are designed to provide reliable operation with rapeseed oil that meets the German rapeseed oil fuel standard DIN 51605. Modifications to the engines cold start regime assist combustion on start up and during the engine warm up phase. Suitably modified indirect injection (IDI) engines have proven to be operable with 100% PPO down to temperatures of . Direct injection (DI) engines generally have to be preheated with a block heater or diesel fired heater. The exception is the VW Tdi (Turbocharged Direct Injection) engine for which a number of German companies offer single tank conversions. For long term durability it has been found necessary to increase the oil change frequency and to pay increased attention to engine maintenance.\n\nMany cars powered by indirect injection engines supplied by in-line injection pumps, or mechanical Bosch injection pumps are capable of running on pure SVO/PPO in all but winter temperatures. Indirect injection Mercedes-Benz vehicles with in-line injection pumps and cars featuring the PSA XUD engine tend to perform reasonably, especially as the latter is normally equipped with a coolant heated fuel filter. Engine reliability would depend on the condition of the engine. Attention to maintenance of the engine, particularly of the fuel injectors, cooling system and glow plugs will help to provide longevity. Ideally the engine would be converted.\n\nThe relatively high kinematic viscosity of vegetable oils must be reduced to make them compatible with conventional compression-ignition engines and fuel systems. Cosolvent blending is a low-cost and easy-to-adapt technology that reduces viscosity by diluting the vegetable oil with a low-molecular-weight solvent. This blending, or \"cutting\", has been done with diesel fuel, kerosene, and gasoline, amongst others; however, opinions vary as to the efficacy of this. Noted problems include higher rates of wear and failure in fuel pumps and piston rings when using blends.\n\nWhen liquid fuels made from biomass are used for energy purposes other than transport, they are called bioliquids.\nWith often minimal modification, most residential furnaces and boilers that are designed to burn No. 2 heating oil can be made to burn either biodiesel or filtered, preheated waste vegetable oil (WVO). If cleaned at home by the consumer, WVO can result in considerable savings. Many restaurants will receive a minimal amount for their used cooking oil, and processing to biodiesel is fairly simple and inexpensive. Burning filtered WVO directly is somewhat more problematic, since it is much more viscous; nonetheless, its burning can be accomplished with suitable preheating. WVO can thus be an economical heating option for those with the necessary mechanical and experimental aptitude.\n\nA number of companies offer compressed ignition engine generators optimized to run on plant oils where the waste engine heat is recovered for heating.\n\nThe main form of SVO/PPO used in the UK is rapeseed oil (also known as canola oil, primarily in the United States and Canada) which has a freezing point of . However the use of sunflower oil, which gels at around , is currently being investigated as a means of improving cold weather starting. However, oils with lower gelling points tend to be less saturated (leading to a higher iodine number) and polymerize more easily in the presence of atmospheric oxygen.\n\nPolymerization also has been consequentially linked to catastrophic component failures such as injection pump shaft seizure and breakage, injector tip failure leading to various and/or combustion chamber components damaged. Most metallurgical problems such as corrosion and electrolysis are related to water based contamination or poor choices of plumbing (such as copper or Zinc) which can cause gelling- even with petroleum based fuels.\n\nSome Pacific island nations are using coconut oil as fuel to reduce their expenses and their dependence on imported fuels while helping stabilize the coconut oil market. Coconut oil is only usable where temperatures do not drop below , unless two-tank SVO/PPO kits or other tank-heating accessories, etc. are used. The same techniques developed to use, for example, canola and other oils in cold climates can be implemented to make coconut oil usable in temperatures lower than \n\nRecycled vegetable oil, also termed used vegetable oil (UVO), waste vegetable oil (WVO), used cooking oil (UCO), or yellow grease (in commodities exchange), is recovered from businesses and industry that use the oil for cooking.\n\n, the United States was producing in excess of 11 billion liters (2.9 billion U.S. gallons) of recycled vegetable oil annually, mainly from industrial deep fryers in potato processing plants, snack food factories and fast food restaurants. If all those 11 billion liters could be recycled and used to replace the energy equivalent amount of petroleum (an ideal case), almost 1% of US oil consumption could be offset. Utilizing recycled vegetable oil as a replacement for standard petroleum-derived fuels like gasoline would reduce the price of gasoline by preserving the supply of petroleum.\n\nVirgin vegetable oil, also termed pure plant oil or straight vegetable oil, is extracted from plants solely for use as fuel. In contrast to used vegetable oil, is not a byproduct of other industries, and thus its prospects for use as fuel are not limited by the capacities of other industries. Production of vegetable oils for use as fuels is theoretically limited only by the agricultural capacity of a given economy. However, doing so detracts from the supply of other uses of pure vegetable oil.\n\nTaxation on SVO/PPO as a road fuel varies from country to country. It is possible that the revenue departments in many countries are even unaware of its use or consider it too insignificant to legislate. Germany used to have 0% taxation, resulting in it being a leader in most developments of the fuel use.\n\n\n"}
{"id": "35605005", "url": "https://en.wikipedia.org/wiki?curid=35605005", "title": "Vertical innovation", "text": "Vertical innovation\n\nVertical innovation is an innovation on the quality of the goods; it consists to improve the quality of items by innovation permitting to those items to access to the highest quality available on the economy. In the literature of growth it is opposed to \"horizontal innovation\" which in turn refers to product innovation mean the introduction of new items in the economy. 'vertical integration' and 'open innovation'. The term vertical integration describes a style of business management control, and is typified by one firm engaged in different multiple parts of a production process. Open innovation is a term promoted by Henry Chesbrough, a professor and executive director at the Center for Open Innovation at the University of California, Berkeley. \"Open innovation is a paradigm that assumes that firms can and should use external ideas as well as internal ideas, and internal and external paths to market, as the firms look to advance their technology.\"\n\nThe company Nottingham Spirk coined the phrase \"vertical innovation\" in 2011, to describe the proprietary process developed over the forty years at the firm. The process is a development strategy to take new product and service concepts and turn them into tangible items within a compressed time. Nottingham Spirk serves clients as an open innovation partner and uses a vertically integrated model to execute on development; hence a 'vertical innovation' strategy.\n\nVertical innovation is both a process to encourage efficient problem solving and a corporate culture, where all development activities are handled by a cohesive and experienced team of experts. This group leverages the skills and talents of many individuals to move product development quickly forward. This process focuses on continuous improvement through each phase of a development program.\n\nVertical innovation ideally takes place at one location, providing many points of collaboration, continuity in communication and helps ensure confidentiality. Co-located skill groups have individuals participating in each of the typical development stages: Market Analysis, Customer Insights, Ideation and Design, Engineering and Prototyping, Packaging and Retail Experience, Sourcing and Production Support. This allows for continual improvement and optimization at many points along the development process, while providing shortened development timelines. The vertical innovation approach also encourages random connections between different skill sets to provide the best chances for new and innovative advancements.\n\nCorporations commonly rely on different companies, groups or separate individuals for different skill sets. They use separate internal groups or external organizations to provide the skills necessary for innovation development. For example:\n\nThis chain of hand-offs typically lengthens development timelines, hinders innovative ideas and dilutes responsibilities needed for project success.\n\nThe central idea behind open innovation is that in a world of widely distributed knowledge, companies cannot afford to rely entirely on their own research, but should instead buy or license skills, processes or inventions (i.e. patents) from other individuals or companies.\n\nThis format makes use of many different inputs of skills and data into a development program. Because these connections can occur from the inside or outside of a company, there is still a high need for analysis, coordination, testing and maintaining of confidentiality.\n\n"}
{"id": "53581639", "url": "https://en.wikipedia.org/wiki?curid=53581639", "title": "VugaPay", "text": "VugaPay\n\nVugaPay is a cross-platform payment service owned by Vuga Ltd, a Rwandan company. It allows businesses and users to transfer money across major payment systems, including credit card networks, mobile money and Bitcoin via an API, USSD interface, a mobile phone app or web interface. It processed over 5 million transactions in 2016. As of December 2016, VugaPay offers instant payments to and from 40 different mobile money networks/carriers in Rwanda, Kenya, Uganda, Niger, Malawi, Congo DR, Ghana,Tanzania and Zambia .\n\nVugaPay is a cross-platform payment service that lets businesses and users transfer money across major payment systems. It follows a similar business model to PayPal. VugaPay describes itself as a \"digital wallet\". Users sign up and create an account by providing basic information and bank account information using their mobile app, USSD for feature phones or on the VugaPay website and they can find others who have created an account. Friends and recipients of transactions can be found via phone number, VugaPay username, or email.\n\nUsers have a VugaPay balance that is used for their transactions. They can link their mobile money, bitcoin, PayPal bank accounts, debit cards, or credit cards to their VugaPay account. Paying with mobile money is free, but credit cards, bitcoin, PayPal and Bank accounts have a 3% fee for each transaction. If a user does not have enough funds on VugaPay itself when making a transaction, it will automatically withdraw the supplemental funds from the registered mobile money, bank account or card.\n\nVugaPay was founded by two Rwandan brothers, Patrick Muhire and Cedrick Muhoza. \nAfter a few years of watching M-pesa evolve from a small transfer platform into a full-blown payment solution, Patrick wondered what it might look like if it were redesigned from the ground up; purely for all countries.\nAccording to Patrick, the idea of VugaPay originated when Patrick wanted to transfer money from one mobile network operator to another. The process of sending money from one mobile network operator to another was a hassle, so they started working on a way to send money across all mobile network operators, interoperability for mobile money. Their original prototype sent money through MTN, TIGO, AIRTEL in Rwanda, but they eventually transitioned from Rwanda to the whole of East Africa, connecting telecoms in those regions.\n\nInitially VugaPay was based on mobile money. VugaPay interoperable transfers are considered a hack through mobile money carriers and can be conducted on any carrier with mobile money services. These transfers are instant, with a large number of carriers from Africa. The East African reports some Confidence trick exploit this on mobile money systems and other services.\nIn late 2016, Timothy C. Draper made his second-African investment in VugaPay. VugaPay later introduced an open source mobile money api for merchants who would accept mobile money as payment.\n\n"}
{"id": "4564818", "url": "https://en.wikipedia.org/wiki?curid=4564818", "title": "Waste-to-energy", "text": "Waste-to-energy\n\nThe first incinerator or \"Destructor\" was built in Nottingham UK in 1874 by Manlove, Alliott & Co. Ltd. to the design of Alfred Fryer.\n\nThe first US incinerator was built in 1885 on Governors Island in New York, New York.\n\nThe first waste incinerator in Denmark was built in 1903 in Frederiksberg\n\nThe first facility in the Czech Republic was built in 1905 in Brno.\n\nGasification and pyrolysis processes have been known and used for centuries and for coal as early as the 18th century... Development technologies for processing [residual solid mixed waste] has only become a focus of attention in recent years stimulated by the search for more efficient energy recovery. (2004) \n\nIncineration, the combustion of organic material such as waste with energy recovery, is the most common WtE implementation. All new WtE plants in OECD countries incinerating waste (residual MSW, commercial, industrial or RDF) must meet strict emission standards, including those on nitrogen oxides (NO), sulphur dioxide (SO), heavy metals and dioxins. Hence, modern incineration plants are vastly different from old types, some of which neither recovered energy nor materials. Modern incinerators reduce the volume of the original waste by 95-96 percent, depending upon composition and degree of recovery of materials such as metals from the ash for recycling.\n\nIncinerators may emit fine particulate, heavy metals, trace dioxin and acid gas, even though these emissions are relatively low from modern incinerators. Other concerns include proper management of residues: toxic fly ash, which must be handled in hazardous waste disposal installation as well as incinerator bottom ash (IBA), which must be reused properly.\n\nCritics argue that incinerators destroy valuable resources and they may reduce incentives for recycling. The question, however, is an open one, as European countries which recycle the most (up to 70%) also incinerate to avoid landfilling.\n\nIncinerators have electric efficiencies of 14-28%. In order to avoid losing the rest of the energy, it can be used for e.g. district heating (cogeneration). The total efficiencies of cogeneration incinerators are typically higher than 80% (based on the lower heating value of the waste).\n\nThe method of incineration to convert municipal solid waste (MSW) is a relatively old method of WtE generation. Incineration generally entails burning waste (residual MSW, commercial, industrial and RDF) to boil water which powers steam generators that generate electric energy and heat to be used in homes, businesses, institutions and industries. One problem associated is the potential for pollutants to enter the atmosphere with the flue gases from the boiler. These pollutants can be acidic and in the 1980s were reported to cause environmental degradation by turning rain into acid rain. Since then, the industry has removed this problem by the use of lime scrubbers and electro-static precipitators on smokestacks. By passing the smoke through the basic lime scrubbers, any acids that might be in the smoke are neutralized which prevents the acid from reaching the atmosphere and hurting the environment. Many other devices, such as fabric filters, reactors, and catalysts destroy or capture other regulated pollutants. According to the New York Times, modern incineration plants are so clean that \"many times more dioxin is now released from home fireplaces and backyard barbecues than from incineration. \" According to the German Environmental Ministry, \"because of stringent regulations, waste incineration plants are no longer significant in terms of emissions of dioxins, dust, and heavy metals\".\n\nThere are a number of other new and emerging technologies that are able to produce energy from waste and other fuels without direct combustion. Many of these technologies have the potential to produce more electric power from the same amount of fuel than would be possible by direct combustion. This is mainly due to the separation of corrosive components (ash) from the converted fuel, thereby allowing higher combustion temperatures in e.g. boilers, gas turbines, internal combustion engines, fuel cells. Some are able to efficiently convert the energy into liquid or gaseous fuels:\nThermal technologies: \nNon-thermal technologies: \n\nDuring the 2001–2007 period, the waste-to-energy capacity increased by about four million metric tons per year.\nJapan and China each built several plants based on direct smelting or on fluidized bed combustion of solid waste. \nIn China there are about 434 waste-to-energy plants in early 2016. \nJapan is the largest user in thermal treatment of municipal solid waste in the world, with 40 million tons.\nSome of the newest plants use stoker technology and others use the advanced oxygen enrichment technology. \nSeveral treatment plants exist worldwide using relatively novel processes such as direct smelting, the Ebara fluidization process and the Thermoselect JFE gasification and melting technology process. \nIn India its first energy bio-science center was developed to reduce the country’s green house gases and its dependency on fossil fuel. \nAs of June 2014, Indonesia had a total of 93.5 MW installed capacity of waste-to-energy, with a pipeline of projects in different preparation phases together amounting to another 373MW of capacity.\n\nBiofuel Energy Corporation of Denver, CO, opened two new biofuel plants in Wood River, Nebraska, and Fairmont, Minnesota, in July 2008. \nThese plants use distillation to make ethanol for use in motor vehicles and other engines. Both plants are currently reported to be working at over 90% capacity. \nFulcrum BioEnergy incorporated located in Pleasanton, California, is building a WtE plant near Reno, NV. The plant is scheduled to open in 2019 under the name of Sierra BioFuels plant. BioEnergy incorporated predicts that the plant will produce approximately 10.5 million gallons per year of ethanol from nearly 200,000 tons per year of MSW.\n\nWaste to energy technology includes fermentation, which can take biomass and create ethanol, using waste cellulosic or organic material. In the fermentation process, the sugar in the waste is changed to carbon dioxide and alcohol, in the same general process that is used to make wine. Normally fermentation occurs with no air present. \nEsterification can also be done using waste to energy technologies, and the result of this process is biodiesel. The cost effectiveness of esterification will depend on the feedstock being used, and all the other relevant factors such as transportation distance, amount of oil present in the feedstock, and others.\nGasification and pyrolysis by now can reach gross thermal conversion efficiencies (fuel to gas) up to 75%, however a complete combustion is superior in terms of fuel conversion efficiency. Some pyrolysis processes need an outside heat source which may be supplied by the gasification process, making the combined process self-sustaining.\n\nIn thermal WtE technologies, nearly all of the carbon content in the waste is emitted as carbon dioxide () to the atmosphere (when including final combustion of the products from pyrolysis and gasification; except when producing bio-char for fertilizer). Municipal solid waste (MSW) contain approximately the same mass fraction of carbon as itself (27%), so treatment of of MSW produce approximately of .\n\nIn the event that the waste was landfilled, of MSW would produce approximately methane via the anaerobic decomposition of the biodegradable part of the waste. This amount of methane has more than twice the global warming potential than the of , which would have been produced by combustion. In some countries, large amounts of landfill gas are collected, but still the global warming potential of the landfill gas emitted to atmosphere in e.g. the US in 1999 was approximately 32% higher than the amount of that would have been emitted by combustion.\n\nIn addition, nearly all biodegradable waste is biomass. That is, it has biological origin. This material has been formed by plants using atmospheric typically within the last growing season. If these plants are regrown the emitted from their combustion will be taken out from the atmosphere once more.\n\nSuch considerations are the main reason why several countries administrate WtE of the biomass part of waste as renewable energy. The rest—mainly plastics and other oil and gas derived products—is generally treated as non-renewables.\n\nMSW to a large extent is of biological origin (biogenic), e.g. paper, cardboard, wood, cloth, food scraps. Typically half of the energy content in MSW is from biogenic material. Consequently, this energy is often recognised as renewable energy according to the waste input.\n\nSeveral methods have been developed by the European CEN 343 working group to determine the biomass fraction of waste fuels, such as Refuse Derived Fuel/Solid Recovered Fuel. The initial two methods developed (CEN/TS 15440) were the manual sorting method and the selective dissolution method. A detailed systematic comparison of these two methods was published in 2010. Since each method suffered from limitations in properly characterizing the biomass fraction, two alternative methods have been developed.\n\nThe first method uses the principles of radiocarbon dating. A technical review (CEN/TR 15591:2007) outlining the carbon 14 method was published in 2007. A technical standard of the carbon dating method (CEN/TS 15747:2008) will be published in 2008. In the United States, there is already an equivalent carbon 14 method under the standard method ASTM D6866.\n\nThe second method (so-called balance method) employs existing data on materials composition and operating conditions of the WtE plant and calculates the most probable result based on a mathematical-statistical model. Currently the balance method is installed at three Austrian and eight Danish incinerators.\n\nA comparison between both methods carried out at three full-scale incinerators in Switzerland showed that both methods came to the same results.\n\nCarbon 14 dating can determine with precision the biomass fraction of waste, and also determine the biomass calorific value. Determining the calorific value is important for green certificate programs such as the Renewable Obligation Certificate program in the United Kingdom. These programs award certificates based on the energy produced from biomass. Several research papers, including the one commissioned by the Renewable Energy Association in the UK, have been published that demonstrate how the carbon 14 result can be used to calculate the biomass calorific value. The UK gas and electricity markets authority, Ofgem, released a statement in 2011 accepting the use of Carbon 14 as a way to determine the biomass energy content of waste feedstock under their administration of the Renewables Obligation. Their Fuel Measurement and Sampling (FMS) questionnaire describes the information they look for when considering such proposals.\n\nAccording to the International Solid Waste Association (ISWA) there are 431 WtE plants in Europe (2005) and 89 in the United States (2004). The following are some examples of WtE plants.\n\n\nA single plant is currently under construction. None are yet in commercial operation:\n\n\n\nBesides large plants, domestic waste-to-energy incinerators also exist. For example, the refuge de Sarenne has a domestic waste-to-energy plant. It is made by combining a wood-fired gasification boiler with a Stirling motor.\n\n\n"}
