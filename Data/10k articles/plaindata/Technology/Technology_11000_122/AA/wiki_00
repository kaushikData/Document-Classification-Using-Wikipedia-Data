{"id": "18293173", "url": "https://en.wikipedia.org/wiki?curid=18293173", "title": "441-line television system", "text": "441-line television system\n\n441 lines, or 383i if named using modern standard, is an early electronic television system. It was used with 50 interlaced frames per second in France and Germany, where it was an improvement over the previous 180-line system. In the United States it was used by RCA with 60 frames per second from 1938 to 1941.\n\nAfter trials in 375 lines during the Berlin Olympic Games of 1936, by 1937 Germany had introduced a 441 lines with 50 interlaced fields per second television system that replaced the previous 180 lines network relayed by a special Reichspost (National Post Office) cable network in the country's main cities (Berlin, Hamburg, Munich, Bayreuth, Nuremberg). The system's line frequency was 11025 Hz and the broadcast frequencies were 46.0 MHz for vision and 43.2 MHz for sound. Its image aspect ratio was close to 1.15:1.\n\nA project began in 1938 involving the National Post and several companies including Bosch, Blaupunkt, Loewe, Lorenz, and Telefunken that aimed to produce 10,000 units of the television system. However, due to the onset of the Second World War only about 50 devices were installed in military hospitals and various government departments. The transmitter's aerials in Berlin were destroyed during an Allied Forces' bombing in November 1943, but the station was also relayed by a special coaxial cables network to \"wide screen\" public \"TV-rooms\" () so it carried on this way until 1944. Sample programme on YouTube \n\nThe Einheitsempfänger is a German TV receiver created in 1939. It could only receive one channel, since its receiving frequency was preset at the factory. This allowed for lower prices and would have made difficult the reception of foreign channels (broadcasting in the same system), were any of them available.\n\nTo date, only a few surviving and functioning units are known:\n\n\nBy 1941 the \"Fernsehsender Paris\" station transmitted from the Eiffel Tower in Paris using the German 441 lines system and its main technical characteristics, while however keeping the previous French 455 lines frequencies 42 MHz - 46 MHz in use from 1938 to 1940, thus with a wider spectrum space than the station operating in Berlin. Television programs were mainly for wounded soldiers of the Wehrmacht occupation troops who recovered in the Greater Paris Area hospitals, but they also included French-language shows. Broadcasts were monitored in the United Kingdom during the Second World War to gather intelligence information from occupied France. Because the 819 lines standard had been adopted in 1948 for the national network, it was due to cease on January 1, 1958. However, after a long elections coverage night, most of the equipment was destroyed by fire on January 3, 1956. It was decided to indemnify the 3,000 owners of remaining 441 lines sets and to entitle them to reduced rates for their new 819 lines receivers. Since July 1952 the 441 lines transmitter was no longer broadcasting separate programs, but simply picked up the national network's picture through an 819-441 lines \"optical converter\" (a 441 lines camera, slightly out of focus, pointed at an 819 lines monitor equipped with an oval spotlight cathode ray tube).\n\nThe line frequency was 11025 Hz with vision broadcast at 46.0 MHz and sound at 42.0 MHz. Aerials were independent for audio and vision at the top of the Eiffel tower, both vertically polarized. No gain being obtained from these pre-war basic aerials, the effective radiated power was only the transmitter's peak one, i.e. 30 kW which enabled a good reception in a radius of 100 km around Paris. As displayed in J.M. Frost's WRTH (World Radio TV Handbook) editions at that time, the transmitter's frequencies (42-46 MHz) were listed as channel \"F1\" or channel \"S\" (or \"Special\" channel) in the European Broadcasting Union's official documents.\n\nReplacing pre-war tests in 343 lines, broadcasts using the 441 lines system began in Italy in 1939 with regular services from Rome using 2 kW power and Milan using 400 W power in the frequency band of 40-45 MHz. Broadcasts were discontinued on May 31, 1940.\n\nAs in France, all technical data – VHF frequencies excepted – were identical to those in use in Germany.\n\nField tests in Los Angeles on various line systems began in 1936, and the United States had adopted RCA's 441-line system by 1938. The following year the first pre-built TV receivers were sold on a very limited basis, mostly in New York City, the new system being publicly launched by NBC during the New York World's Fair in April 1939. Its manufacturers included RCA, General Electric, DuMont, and Andrea. Following a decision of the NTSC (National Television System Committee), the 525-line standard replaced the 441-line standard on July 1, 1941.\n\n"}
{"id": "13522081", "url": "https://en.wikipedia.org/wiki?curid=13522081", "title": "AACS LA", "text": "AACS LA\n\nAACS LA (Advanced Access Content System Licensing Administrator) is the body that develops and licenses the AACS copy-protection system used on the HD DVD and Blu-ray Disc high-definition optical disc formats.\n\n"}
{"id": "21106472", "url": "https://en.wikipedia.org/wiki?curid=21106472", "title": "AeroMobile", "text": "AeroMobile\n\nAeroMobile Communications Limited is a registered mobile network operator for the aviation industry and is based in the UK. It provides technology and services that allow the safe use of passengers' own mobile phones while inflight. A subsidiary of Panasonic Avionics Corporation its services are often installed alongside Panasonic's Wi-Fi network and can be installed either at the point of aircraft manufacture or retro-fitted across both Airbus and Boeing aircraft. Panasonic Avionic's Wi-Fi network and AeroMobile's mobile phone network are complimentary services and provide passengers with a choice of inflight connectivity options.\n\nSince launching the service in March 2008, over 40 million passengers have connected to the network and AeroMobile now has over 20 airline partners offering passengers inflight voice, SMS and data services on selected, connected flights. Airline partners include Emirates, Etihad, KLM, Lufthansa, SAS, Virgin Atlantic, Singapore Airlines, Cathay Pacific and Turkish Airlines.\n\nIn December 2015 AeroMobile launched the world's first 3.5G inflight mobile network with Air Berlin. The 3.5G network provides passengers with a substantial increase in data speeds, faster browsing and quicker posts.\n\n\n"}
{"id": "26425793", "url": "https://en.wikipedia.org/wiki?curid=26425793", "title": "Alternative payments", "text": "Alternative payments\n\nAlternative payments refers to payment methods that are used as an alternative to credit card payments. Most alternative payment methods address a domestic economy or have been specifically developed for electronic commerce and the payment systems are generally supported and operated by local banks. Each alternative payment method has its own unique application and settlement process, language and currency support, and is subject to domestic rules and regulations.\n\nThe most common alternative payment methods are debit cards, charge cards, prepaid cards, direct debit, bank transfers, phone and mobile payments, checks, money orders and cash payments.\n\nA debit card (also known as a bank card or check card) is a plastic card that provides an alternative payment method to cash when making purchases. A charge card is a plastic card that provides an alternative to cash when making purchases in which the issuer and the cardholder enter into an agreement that the debt incurred on the charge account will be paid in full and by due date. Debit and charge cards are used and accepted in many countries and can be used at a point of sale location or online.\n\nPrepaid or stored-value cards provide payment through a monetary value held on the actual card or on deposit in an account. One major difference between stored-value cards and prepaid cards is that prepaid cards are usually issued in the name of the individual account holders, while stored value cards are usually anonymous. In the United States, prepaid and stored-value cards typically can be processed on the credit card network, but this is not the case for all cards, especially those outside of the United States.\n\nA direct debit or direct withdrawal is an instruction that a bank account holder gives to his or her bank to collect an amount directly from another account. It is similar to a direct deposit but initiated by the beneficiary. Direct debit is available in several countries including the United Kingdom, Germany, Austria and the Netherlands. It was scheduled to be available across the whole Single European Payments Area by the end of 2010. In the United States, where checks are more popular than bank transfers, a similar service is available through the Automated Clearing House network.\n\nA bank transfer (also known as a wire transfer or credit transfer) is a method of transferring money from one person or institution (entity) to another. A wire transfer can be made from one bank account to another bank account or through a transfer of cash at a cash office. A bank wire transfer is often the most expedient method for transferring funds between bank accounts. The transfer messages are sent via a secure system (such as SWIFT or Fedwire) utilizing IBAN and BIC codes. Online bank transfer systems in Europe are popular alternative payment methods, where the bank transfer is authorized by the consumer who logs onto his bank website and authorizes the funds transfer for payment to a merchant.\n\nA giro transfer is a bank transfer payment, whereby order is given by the payer to his or her bank, which transfers funds into the payee's bank account; the receiving bank then notifies the payee. Giro is often used by post offices as well. The term is little used in the United States, although an ACH Transfer or direct deposit is the US electronic version of the giro transfer.\n\nOnline Banking ePayments (OBeP) are similar to giro transfers, but are designed specifically for use with online commerce. With OBeP, during the online checkout process, the merchant redirects the consumer to their financial institution’s online banking site where they login and authorize charges. After charges are authorized, the financial institution redirects the consumer back to the merchant site. With some services, like Trustly, the merchant can embed an iframe on their site so that the consumer doesn't have to leave the page to make a payment. All network communications are protected using industry standard encryption. Additionally, communications with the OBeP network take place on a virtual private network, not over the public Internet. OBeP systems protect consumer personal information by not requiring the disclosure of account numbers or other sensitive personal data to online merchants or other third parties.\n\nElectronic bill payment is a feature of online banking, similar in its effect to a bank transfer, allowing a depositor to send money from his demand account to a creditor or vendor such as a public utility or a department store to be credited against a specific account. The payment is optimally executed electronically in real-time, though some financial institutions or payment services will wait until the next business day to send out the payment. The bank can usually also generate and mail a paper check or banker's draft to a creditor who is not set up to receive electronic payments.\n\nWith phone payments, consumers are billed via their regular telephone number, whereby the charges are added to their phone bill. Premium-rate telephone numbers or 900 numbers are telephone numbers for telephone calls during which certain services are provided, and for which prices higher than normal are charged.\n\nMobile payments is a new and rapidly adopted alternative payment method – especially in Asia and Europe. Instead of paying with cash, check or credit cards, a consumer can use a mobile phone to pay for wide range of services and goods. The charges are then added to their phone bill.\n\nA check or cheque is a negotiable instrument instructing a financial institution to pay a specific amount of a specific currency from a specified demand account held in the maker/depositor's name with that institution. Both the maker and payee may be natural persons or legal entities.\n\nAn electronic check is often referred to as an ACH payment in USA.\n\nThe number of Alternative Payments has grown exponentially in the last few years due to the need for billing solutions on the Internet. Limited credit card penetration and customary local payment habits, combined with tight credit and security fears to use credit cards for online payments has increased the usage of Alternative Payments on a worldwide level.\n\nAlternative payments are offered by domestic banks and payment processors that offer merchants a variety of billing solutions. Most Alternative Payments have online applications and are integrated into electronic shopping carts used by online merchants.\n\nSeveral billing solutions have been devised specifically for web-based merchants to accept alternative payments online and to support and access distant markets. Alternative payments are used throughout North America, Europe and Asia, and have penetration levels of sixty percent or more in various countries. Language, currency and support, including trust and familiarity, often contribute to the success of a domestic alternative payment solution.\n\nDebit cards and charge cards are accepted worldwide as alternative payment and in some cases, debit cards are designed exclusively for use on the Internet, and there is no physical card only a virtual card. Certain systems also require the use of a PIN when a debit is used for online purchases.\n\nEuropean online direct debit solutions are particularly popular due to the lower use of credit cards in Europe as compared to other countries like the United States. Transactions can be approved in real-time and funds in 1 to 3 business days. Chargebacks remain a risk inherently when debiting a consumer’s bank account, however, using additional verification systems reduces the risk significantly and many payment processors maintain an extensive fraud database that mitigates the risks.\n\nUsing bank transfers to accept payments does not carry any inherent risk to the merchant, which makes it particularly attractive to both high and low risk merchants seeking to reduce chargebacks. The drawback to this approach from a merchant’s perspective are that re-billing cannot be made automatic and billing does not occur quickly, as their customers must manually transfer the funds.\n\nElectronic checks allow funds to be withdrawn directly from the consumer’s account. Recurring payments can be set up and the consumer’s personal information can be verified instantly. Merchants that opt to accept electronic checks enjoy convenient processing that reaches a large number of consumers that do not own credit cards or do not wish to use credit cards to make payments. Electronic checks are known to have long clearing times of up to five business days and carry an inherent risk of charge-backs. Checks that have been verified may come back after the clearing time as “insufficient funds”, meaning that the consumer does not have sufficient funds in their account to pay the balance of the transaction.\n\nPhone payments describe a system of allowing consumers to purchase products or services using their phone number. In most cases, the charge is verified via phone or SMS messaging before the transaction is approved. The resulting charge is then added to the customer’s phone bill.\n\nPhone billing is accepted in many countries and offers a flexible way for merchants to accept payment, especially online, where the risk of fraud is elevated. While convenient for the consumer, phone billing has several inherent issues for merchants. Payment processors that support phone billing typically charge a higher rate because the payments must go through an additional party, the phone provider, before reaching the merchant. The clearing time on funds is also exceptionally high because the funds are not collected until the consumer pays their phone bill.\n\nAlternative Payments have increasingly become more popular with merchants, as more options means more sales, and because nearly all Alternative Payments offer a variety of service specific features that addresses a global online marketplace. Geolocation software, automatic language translations, instant currency exchange and worldwide support are generally included to allow foreign buyers to make use of their domestic payment solution, while shopping outside of their country at a foreign based web merchant.\n\nUnlike traditional credit card transactions, many alternative payments often provide additional security features that protect the merchant from fraud and returned transactions, because the funds availability is verified and payment is made directly from a bank account. The banks guarantee the funds and because there are no chargebacks, merchants are often not required to provide collateral or keep a reserve. Furthermore, accounts are validated in real-time and fraud modules scrub transactions, similar to the approval process with credit cards.\n\nAlternative Payments have, in many areas, become the dominant form of online payment for consumers. Alternative payments gives consumers more options to pay and allows them to select payment methods that they are comfortable with. Language, domestic applications and familiarity with the payment method, coupled with the trust they place in their own bank, increases usage. Furthermore, consumers may simply elect to use alternative payment methods due to security concerns with credit card purchases. Many alternative payments often require additional security steps, such as a user name, password or personal identification number (PIN) to further protect the consumer.\n\n"}
{"id": "44473353", "url": "https://en.wikipedia.org/wiki?curid=44473353", "title": "American Powder Mills", "text": "American Powder Mills\n\nAmerican Powder Mills was a Massachusetts gunpowder manufacturing complex on the Assabet River. It expanded to include forty buildings along both sides of the river in the towns of Acton, Concord, Maynard, and Sudbury. Press mills, kernelling mills, glazing mills, and storehouses were dispersed over four-hundred acres to minimize damage during explosions. A narrow gauge railway transferred raw materials and products between the buildings.\n\nNathan Pratt purchased a mill pond dam on the Assabet River and converted the former sawmill to production of gunpowder in 1835. Pratt sold the mill to the American Powder Company in 1864. The mill manufactured of gunpowder per day through the American civil war. In 1872 American Powder Company joined the United States Gunpowder Trade Association, popularly known as the powder trust, with Austin Powder Company, DuPont, Hazard Powder Company, Laflin & Rand Powder Company, Miami Powder Company, and Oriental Powder Company. \n\nAmerican Powder Company reorganized as American Powder Mills in 1883 with business offices in Boston. The facility was sold to American Cyanamid in 1929, with gunpowder production continuing to 1940. In addition to black powder for blasting and other purposes, APM made smokeless gunpowder for shotgun-using hunters under the brand name \"Dead Shot.\" \n\nThe first explosion, in the first year the mills were operating, killed four men in 1836. Henry David Thoreau's journal records his observations of another explosion killing three men in 1853. Five men were killed in a series of explosions in 1898. While the plant was manufacturing gunpowder for the Russian Empire during World War I, a September 4, 1915 explosion was heard as far away as Lowell and Boston. The last three explosions in 1940 ended gunpowder production, and the property passed into W. R. Grace and Company ownership. The Assabet River dam at the original mill pond site is being used to generate hydroelectricity for municipal Concord. The body of water created by the dam goes by the name Ripple Pond, and is located in Acton and Maynard.\n"}
{"id": "2512475", "url": "https://en.wikipedia.org/wiki?curid=2512475", "title": "Application-oriented networking", "text": "Application-oriented networking\n\nApplication-oriented networking (AON) involves network devices designed to aid in computer-to-computer application integration. Application-oriented networks are sometimes called \"intelligent networks\" or \"content-based routing networks\" and they are generally network technology that can use the content of a network packet or message to take some sort of action. \n\nApplication-oriented networking was popularized by Cisco Systems in response to increasing use of XML messaging (combined with related standards such as XSLT, XPath and XQuery) to link miscellaneous applications, data sources and other computing assets. Most Application-Orientated Networks manipulate structured data based in a human-readable format like XML.\n\nMany of the operations required to mediate between applications, or to monitor their transactions, can be built into network devices that are optimized for the purpose.\n\nThe rules and policies for performing these operations, also expressed in XML, are specified separately and downloaded as required. Cisco has adopted the AON acronym as the name of a family of products that function in this way.\n\nDuring the rise of the internet many routing decisions were made at layer 4 i.e. based on the TCP/IP address and/or the port number. Application-oriented networks work at layer 7 of the OSI stack and because they can examine the content of the message they can make routing decisions based on many different criteria including such things as the value of the purchase order or the ship date.\n\n\n"}
{"id": "7127409", "url": "https://en.wikipedia.org/wiki?curid=7127409", "title": "BC Research", "text": "BC Research\n\nBC Research Inc. was based at a scientific research and development company located at the BC Research and Innovation Complex at the south end of the University of British Columbia campus. The facility closed in November 2007. The company specialized in consulting and applied research and development in the area of plant biotechnology and environment, health and safety, process and analysis, transportation and ship dynamics.\n\nThe company can be traced back to 1944 as it developed from the non-profit BC Research Council to a private company in 1993, founded by Dr. Hugh Wynne-Edwards, Ph.D, DSc., FRSC, a member of the Order of Canada, who served as the founding Chief Executive Officer and developed the facility into one of Canada's most recognized incubators in the fields of biotechnology, drug discovery and alternative fuel technologies. In 2000, BC Research was purchased by Immune Network Ltd and was sold to Cromedica (now PRA International) in July 2001 for a consideration of $8.3 million according to 2001 audited financials published on SEDAR. Its plant biotechnology team was mostly spun off in Silvagen Inc. which specialized in clonal reforestation and which became a part of CellFor. In 1999 Azure Dynamics, a hybrid commercial vehicle systems developer, was formed with some of the transportation team and left the facility in 2004 having gone public in 2001 as Azure Dynamics Corporation. Technologies, Inc., specializing in microwave-assisted natural product extraction, purification and isolation, was also spun off in 2001 as a joint venture with Environment Canada. The remaining laboratory and consulting business functions continued under the name Vizon SciTec until August 2006 when CANTEST Ltd. announced its acquisition from BC Research Inc. which continues as a privately held technology holding company. \n\nIn May 2007, the former Industrial Process Division of BC Research was acquired by Kemetco Research Inc. Kemetco provides contract research, process development and laboratory testing services to industry, primarily in mining, metallurgy and chemical processing. Since inception, Kemetco has grown to be one of the largest privately held contract R&D firms in Canada that services Environmental, Chemical and Mining companies. Kemetco operates in an state-of-the-art R&D facility in Richmond, British Columbia. \nFinally in 2010, BC Research Inc., BCRI, opened again for business in its new research facilities in Burnaby, B.C. Their board of directors is composed of Kemetco and NORAM Engineering and Constructors Ltd business and technology leaders. The Company continues to provide specialized consulting and applied research and development in an expanding number of different technologies and industries, including fluidized beds, storage of energy in batteries, fuel cells, electrochemical cells, corrosion testing and analysis, hydrogen, sulfur, chlorine, nitration, water treatment, and pulp and paper chemistry.\nAs of Q4 2016 BCRI is opening a newly constructed facility on Mitchell Island in Vancouver B.C. to expand their capabilities.\n\n"}
{"id": "337371", "url": "https://en.wikipedia.org/wiki?curid=337371", "title": "Bernoulli Box", "text": "Bernoulli Box\n\nThe Bernoulli Box (or simply Bernoulli, named after Daniel Bernoulli) is a high-capacity (for the time) removable floppy disk storage system that is Iomega's first widely known product. It was released in 1982.\n\nThe drive spins a PET film floppy disk at about 3000 rpm, 1 µm over a read-write head, using Bernoulli's principle to pull the flexible disk towards the head as long as the disk is spinning. In theory this makes the Bernoulli drive more reliable than a contemporary hard disk drive, since a head crash is impossible.\n\nThe original Bernoulli disks came in capacities of 5, 10, and 20 MB. They are roughly 21 cm by 27.5 cm, similar to the size of a sheet of A4 paper.\n\nThe most popular system was the Bernoulli Box II, whose disk cases are 13.6 cm wide, 14 cm long and 0.9 cm thick, somewhat resembling a 3½-inch standard floppy disk but in 5¼-inch form factor. Bernoulli Box II disks came in the following capacities: 20 MB, 35 MB, 44 MB, 65 MB, 90 MB (late 1980s), 105 MB, 150 MB, and in 1993, 230 MB. There are five types of drives, grouped by the maximum readable capacity: 20 MB, 44 MB, 90 MB, 150 MB, and 230 MB. The interface is usually SCSI. Drives were available as either internal units, which fit into standard 5¼-inch drive bays, or as external units with one or two drives in a self-contained case connected to the host computer via external SCSI connector. The disks have a physical switch similar to that on 3½-inch standard floppy disks to enable and disable write protection.\n\n\"PC Magazine\" in 1984 stated that the Bernoulli Box \"... combines the advantages of [standard] floppy- and hard-disk systems without their drawbacks.\" It reported no software-compatibility problems and cited the box's durable design. Bruce Webster of \"BYTE\" wrote favorably of the peripheral in February 1986, reporting that \"I have not had a single glitch or lost file\" in nine months of constant use.\n\nIomega's later removable-storage products such as the Zip floppy disk and Jaz and Rev removable hard disks do not use the Bernoulli technology.\n\n"}
{"id": "5727790", "url": "https://en.wikipedia.org/wiki?curid=5727790", "title": "Coastal zone color scanner", "text": "Coastal zone color scanner\n\nThe coastal zone color scanner (or CZCS) was a multi-channel scanning radiometer aboard the Nimbus 7 satellite, predominately designed for water remote sensing. Nimbus 7 was launched 24 October 1978, and CZCS became operational on 2 November 1978. It was only designed to operate for one year (as a proof-of-concept), but in fact remained in service until 22 June 1986. Its operation on board the Nimbus 7 was limited to alternate days as it shared its power with the passive microwave scanning multichannel microwave radiometer.\n\nCZCS measured reflected solar energy in six channels, at a resolution of 800 meters. These measurements were used to map chlorophyll concentration in water, sediment distribution, salinity, and the temperature of coastal waters and ocean currents. \nCZCS lay the foundations for subsequent satellite ocean color sensors, and formed a cornerstone for international efforts to understand the ocean's role in the carbon cycle.\n\nThe most significant product of the CZCS was its collection of so-called ocean color imagery. The \"color\" of the ocean in CZCS images comes from substances in the water, particularly phytoplankton (microscopic, free-floating photosynthetic organisms), as well as inorganic particulates.\n\nBecause ocean color data is related to the presence of phytoplankton and particulates, it can be used to calculate the concentrations of material in surface waters and the level of biological activity; as phytoplankton concentration increases, ocean color shifts from blue to green (note that most CZCS images are false colored, so that high levels of phytoplankton appear as red or orange). Satellite-based ocean color observations provide a global picture of life in the world's oceans, because phytoplankton is the basis for the vast majority of oceanic food chains. By recording images over a period of years, scientists also gained a better understanding of how the phytoplankton biomass changed over time; for instance, red tide blooms could be observed when they grew. Ocean color measurements are also of interest because phytoplankton removes carbon dioxide from the sea water during photosynthesis, and so forms an important part of the global carbon cycle.\n\nRaw data from the scanner were transmitted, at an average bit rate of 800 kbit/s, to the ground station, where they were saved on magnetic tape. The tapes were then sent to the Image Processing Division at Goddard Space Flight Center. The processed data were archived at Goddard, and available to scientists worldwide. The data were originally stored on 38,000 nine track magnetic tapes, and later migrated to optical disc.\nThe archive was one of the first instances of a system that provided a visual preview (\"browse\" ) of images, which assisted in ordering data. It became a model to be followed later by the Earth Observing System's Distributed Active Archive Centers.\n\nThe sea-viewing wide field-of-view sensor (SeaWiFS) was a follow-on to CZCS, launched in 1997. The MODIS/Aqua instrument currently provides ocean color data.\n\nThe CZCS instrument was manufactured by Ball Aerospace & Technologies Corp..\n\nReflected solar energy was measured in six channels to sense color caused by absorption due to chlorophyll, sediments, and gelbstoffe in coastal waters. The CZCS used a rotating plane mirror at a 45-degree angle to the optic axis of a Cassegrain telescope. The mirror scanned 360 degrees but only the 80 degrees of data centered on nadir were collected for ocean color measurements. The instrument viewed deep space and calibration sources during the remainder of the scan. The incoming radiation was collected by the telescope and divided into two streams by a dichroic beam splitter. One stream was transmitted to a field stop that was also the entrance aperture of a small polychromator. The radiance that entered the polychromator was separated and re-imaged in five wavelengths on five silicon detectors in the focal plane of the polychromator. The other stream was directed to a cooled mercury cadmium telluride detector in the thermal region (10.5–12.5 micrometer). A radiative cooler was used to cool the thermal detector. To avoid sun glint, the scanner mirror was tilted about the sensor pitch axis on command so that the line of sight of the sensor was moved in 2-degree increments up to 20 degrees with respect to the nadir. Spectral bands at 0.443 and 0.670 micrometers centered on the most intense absorption bands of chlorophyll, while the band at 0.550 micrometers centered on the \"hinge point,\" the wavelength of minimum absorption. Ratios of measured energies in these channels were shown to closely parallel surface chlorophyll concentrations. Data from the scanning radiometer were processed, with algorithms developed from the field experiment data, to produce maps of chlorophyll absorption. The temperatures of coastal waters and ocean currents were measured in a spectral band centered at 11.5 micrometers. Observations were made also in two other spectral bands, 0.520 micrometers for chlorophyll correlation and 0.750 micrometers for surface vegetation. The scan width was 1556 km centered on nadir and the ground resolution was 0.825 km at nadir.\n\n\n"}
{"id": "7011", "url": "https://en.wikipedia.org/wiki?curid=7011", "title": "Control engineering", "text": "Control engineering\n\nControl engineering or control systems engineering is an engineering discipline that applies automatic control theory to design systems with desired behaviors in control environments. The discipline of controls overlaps and is usually taught along with electrical engineering at many institutions around the world.\n\nThe practice uses sensors and detectors to measure the output performance of the process being controlled; these measurements are used to provide corrective feedback helping to achieve the desired performance. Systems designed to perform without requiring human input are called automatic control systems (such as cruise control for regulating the speed of a car). Multi-disciplinary in nature, control systems engineering activities focus on implementation of control systems mainly derived by mathematical modeling of a diverse range of systems.\n\nModern day control engineering is a relatively new field of study that gained significant attention during the 20th century with the advancement of technology. It can be broadly defined or classified as practical application of control theory. Control engineering has an essential role in a wide range of control systems, from simple household washing machines to high-performance F-16 fighter aircraft. It seeks to understand physical systems, using mathematical modelling, in terms of inputs, outputs and various components with different behaviors; use control systems design tools to develop controllers for those systems; and implement controllers in physical systems employing available technology. A system can be mechanical, electrical, fluid, chemical, financial or biological, and the mathematical modelling, analysis and controller design uses control theory in one or many of the time, frequency and complex-s domains, depending on the nature of the design problem.\n\nAutomatic control systems were first developed over two thousand years ago. The first feedback control device on record is thought to be the ancient Ktesibios's water clock in Alexandria, Egypt around the third century B.C.E. It kept time by regulating the water level in a vessel and, therefore, the water flow from that vessel. This certainly was a successful device as water clocks of similar design were still being made in Baghdad when the Mongols captured the city in 1258 A.D. A variety of automatic devices have been used over the centuries to accomplish useful tasks or simply to just entertain. The latter includes the automata, popular in Europe in the 17th and 18th centuries, featuring dancing figures that would repeat the same task over and over again; these automata are examples of open-loop control. Milestones among feedback, or \"closed-loop\" automatic control devices, include the temperature regulator of a furnace attributed to Drebbel, circa 1620, and the centrifugal flyball governor used for regulating the speed of steam engines by James Watt in 1788.\n\nIn his 1868 paper \"On Governors\", James Clerk Maxwell was able to explain instabilities exhibited by the flyball governor using differential equations to describe the control system. This demonstrated the importance and usefulness of mathematical models and methods in understanding complex phenomena, and it signalled the beginning of mathematical control and systems theory. Elements of control theory had appeared earlier but not as dramatically and convincingly as in Maxwell's analysis.\n\nControl theory made significant strides over the next century. New mathematical techniques, as well as advancements in electronic and computer technologies, made it possible to control significantly more complex dynamical systems than the original flyball governor could stabilize. New mathematical techniques included developments in optimal control in the 1950s and 1960s followed by progress in stochastic, robust, adaptive, nonlinear, and azid-based control methods in the 1970s and 1980s. Applications of control methodology have helped to make possible space travel and communication satellites, safer and more efficient aircraft, cleaner automobile engines, and cleaner and more efficient chemical processes.\n\nBefore it emerged as a unique discipline, control engineering was practiced as a part of mechanical engineering and control theory was studied as a part of electrical engineering since electrical circuits can often be easily described using control theory techniques. In the very first control relationships, a current output was represented by a voltage control input. However, not having adequate technology to implement electrical control systems, designers were left with the option of less efficient and slow responding mechanical systems. A very effective mechanical controller that is still widely used in some hydro plants is the governor. Later on, previous to modern power electronics, process control systems for industrial applications were devised by mechanical engineers using pneumatic and hydraulic control devices, many of which are still in use today.\n\nThere are two major divisions in control theory, namely, classical and modern, which have direct implications for the control engineering applications. The scope of classical control theory is limited to single-input and single-output (SISO) system design, except when analyzing for disturbance rejection using a second input. The system analysis is carried out in the time domain using differential equations, in the complex-s domain with the Laplace transform, or in the frequency domain by transforming from the complex-s domain. Many systems may be assumed to have a second order and single variable system response in the time domain. A controller designed using classical theory often requires on-site tuning due to incorrect design approximations. Yet, due to the easier physical implementation of classical controller designs as compared to systems designed using modern control theory, these controllers are preferred in most industrial applications. The most common controllers designed using classical control theory are PID controllers. A less common implementation may include either or both a Lead or Lag filter. The ultimate end goal is to meet requirements typically provided in the time-domain called the step response, or at times in the frequency domain called the open-loop response. The step response characteristics applied in a specification are typically percent overshoot, settling time, etc. The open-loop response characteristics applied in a specification are typically Gain and Phase margin and bandwidth. These characteristics may be evaluated through simulation including a dynamic model of the system under control coupled with the compensation model.\n\nIn contrast, modern control theory is carried out in the state space, and can deal with multiple-input and multiple-output (MIMO) systems. This overcomes the limitations of classical control theory in more sophisticated design problems, such as fighter aircraft control, with the limitation that no frequency domain analysis is possible. In modern design, a system is represented to the greatest advantage as a set of decoupled first order differential equations defined using state variables. Nonlinear, multivariable, adaptive and robust control theories come under this division. Matrix methods are significantly limited for MIMO systems where linear independence cannot be assured in the relationship between inputs and outputs. Being fairly new, modern control theory has many areas yet to be explored. Scholars like Rudolf E. Kalman and Aleksandr Lyapunov are well-known among the people who have shaped modern control theory.\n\nControl engineering is the engineering discipline that focuses on the modeling of a diverse range of dynamic systems (e.g. mechanical systems) and the design of controllers that will cause these systems to behave in the desired manner. Although such controllers need not be electrical, many are and hence control engineering is often viewed as a subfield of electrical engineering. However, the falling price of microprocessors is making the actual implementation of a control system essentially trivial. As a result, focus is shifting back to the mechanical and process engineering discipline, as intimate knowledge of the physical system being controlled is often desired.\n\nElectrical circuits, digital signal processors and microcontrollers can all be used to implement control systems. Control engineering has a wide range of applications from the flight and propulsion systems of commercial airliners to the cruise control present in many modern automobiles. \n\nIn most cases, control engineers utilize feedback when designing control systems. This is often accomplished using a PID controller system. For example, in an automobile with cruise control the vehicle's speed is continuously monitored and fed back to the system, which adjusts the motor's torque accordingly. Where there is regular feedback, control theory can be used to determine how the system responds to such feedback. In practically all such systems stability is important and control theory can help ensure stability is achieved.\n\nAlthough feedback is an important aspect of control engineering, control engineers may also work on the control of systems without feedback. This is known as open loop control. A classic example of open loop control is a washing machine that runs through a pre-determined cycle without the use of sensors.\n\nAt many universities around the world, control engineering courses are taught primarily in information engineering and electrical engineering, but some courses can be instructed in mechatronics engineering, mechanical engineering, and aerospace engineering. In others, control engineering is connected to computer science, as most control techniques today are implemented through computers, often as embedded systems (as in the automotive field). The field of control within chemical engineering is often known as process control. It deals primarily with the control of variables in a chemical process in a plant. It is taught as part of the undergraduate curriculum of any chemical engineering program and employs many of the same principles in control engineering. Other engineering disciplines also overlap with control engineering as it can be applied to any system for which a suitable model can be derived. However, specialised control engineering departments do exist, for example, the Department of Automatic Control and Systems Engineering at the University of Sheffield and the Department of Systems Engineering at the United States Naval Academy.\n\nControl engineering has diversified applications that include science, finance management, and even human behavior. Students of control engineering may start with a linear control system course dealing with the time and complex-s domain, which requires a thorough background in elementary mathematics and Laplace transform, called classical control theory. In linear control, the student does frequency and time domain analysis. Digital control and nonlinear control courses require Z transformation and algebra respectively, and could be said to complete a basic control education.\n\nOriginally, control engineering was all about continuous systems. Development of computer control tools posed a requirement of discrete control system engineering because the communications between the computer-based digital controller and the physical system are governed by a computer clock. The equivalent to Laplace transform in the discrete domain is the Z-transform. Today, many of the control systems are computer controlled and they consist of both digital and analog components.\n\nTherefore, at the design stage either digital components are mapped into the continuous domain and the design is carried out in the continuous domain, or analog components are mapped into discrete domain and design is carried out there. The first of these two methods is more commonly encountered in practice because many industrial systems have many continuous systems components, including mechanical, fluid, biological and analog electrical components, with a few digital controllers.\n\nSimilarly, the design technique has progressed from paper-and-ruler based manual design to computer-aided design and now to computer-automated design or CAutoD which has been made possible by evolutionary computation. CAutoD can be applied not just to tuning a predefined control scheme, but also to controller structure optimisation, system identification and invention of novel control systems, based purely upon a performance requirement, independent of any specific control scheme.\n\nResilient control systems extend the traditional focus of addressing only planned disturbances to frameworks and attempt to address multiple types of unexpected disturbance; in particular, adapting and transforming behaviors of the control system in response to malicious actors, abnormal failure modes, undesirable human action, etc.\n\n\n\n"}
{"id": "2600081", "url": "https://en.wikipedia.org/wiki?curid=2600081", "title": "Direct debit", "text": "Direct debit\n\nA direct debit or direct withdrawal is a financial transaction in which one person withdraws funds from another person's bank account. Formally, the person who directly draws the funds (\"the payee\") instructs his or her bank to collect (i.e., debit) an amount directly from another's (\"the payer's\") bank account designated by the payer and pay those funds into a bank account designated by the payee. Before the payer's banker will allow the transaction to take place, the payer must have advised the bank that he or she has authorized the payee to directly draw the funds. It is also called pre-authorized debit (PAD) or pre-authorized payment (PAP). After the authorities are set up, the direct debit transactions are usually processed electronically. Direct debits are typically used for recurring payments, such as credit card and utility bills, where the payment amounts vary from one payment to another. However, when the authorization is in place, the circumstances in which the funds are drawn as well as the dates and amounts are a matter of agreement between the payee and payer, with which the bankers are not concerned. In countries where setting up authorization is easy enough, direct debits can also be used for irregular payments, such as for mail order transactions or at a point of sale. The payer can cancel the authorization for a direct debit at any time, and the banker can decline to carry out a debit if the transaction would breach the terms of the bank account out of which payment is to be made, for example if it were to cause the account to overdraw. (Banking law does not authorize a bank to alter the payment amount.)\n\nA direct debit instruction differs from a direct deposit and standing order instruction, which are initiated by the payer. A standing order involves fixed payment amounts paid periodically, while a direct debit can be of any amount and can be casual or periodic. They also should not be confused with a continuous payment authority, where the payee collects money whenever it feels it is owed.\n\nDirect debits are available in a number of countries, including the United Kingdom, Brazil, Germany, Sweden, Spain, South Africa, Switzerland, and the Netherlands. Direct debits are made under each country's rules, and are usually restricted to domestic transactions in those countries. An exception in this respect is the Single Euro Payments Area which allows for Euro-denominated cross-border (and domestic) direct debits since November 2010. In the United States, direct debits are processed through the Automated Clearing House network.\n\nA direct debit instruction must in all cases be supported by some sort of authorization for the payee to collect funds from the payer's account. There are generally two methods to set up the authorization:\n\nOne method involves only the payer and the payee, with the payer authorizing the payee to collect amounts due on his or her account. However, the payer can instruct his or her bank to return any direct debit note without giving a reason. In that event, the payee has to pay all fees for the transaction and may eventually lose his or her ability to initiate direct debits if this occurs too often. However, it still requires all the account holders (not merely the payer) to watch statements and request returns if necessary, unless they have instructed their bank to block all direct debits.\n\nThe other method requires the payer to instruct his or her bank to honour direct debit notes from the payee. The payee is then notified that he or she is now authorised to initiate direct debit transfers from the payer. While this is more secure in theory, it can also mean for the payer that it is harder to return debit notes in the case of an error or dispute.\n\nIn Europe, the SEPA Direct Debit (SDD) and SEPA Credit Transfer payment schemes are available in each SEPA country for payments within the Eurozone. \n\nSEPA Direct Debit initially worked alongside national direct debit schemes until 1 August 2014. From that date, only SEPA Direct Debit was permitted for collecting euro-denominated payments in the EU. In October 2016, SDD and IBAN became the mandatory methods for euro transfers in all of the EU and EEA, but not mandatory for transfers in other currencies.\n\nDirect Debit is a payment method for recurring payments in the UK. It is the third most popular payment method in the UK, after cash and debit card, according to Payments UK. Bacs Payment Schemes Limited is the organisation with responsibility for the Direct Debit and Bacs Direct Credit schemes. \n\nDirect Debit was invented by Alastair Hanton while he was working at Unilever as a way of collecting payments more efficiently.\n\nDirect Debit accounts for the payment of 73% of household bills and almost nine out of ten British adults have at least one Direct Debit commitment. In fact, in 2015 nearly 3.9 billion Direct Debits were processed, representing a year-on-year increase of 239 million which surpasses the previous record for annual growth of 161 million, set in 2004. 4.07 billion Direct Debits were processed in 2016, an increase of 4.9% from 2015. Payments UK predicts the figure is expected to rise to 4.4 billion by 2026.\n\nTo set up payments by Direct Debit, the payer must complete a Direct Debit Instruction to the merchant. This instruction contains bank-approved wording that makes it clear the payer is setting up an ongoing authority for the merchant to debit their account. The interface for completing the Direct Debit Instruction is controlled by the merchant, who then sends the data from the form to the customer's bank, via Bacs.\n\nThe UK Direct Debit scheme rules allow for Direct Debit Instructions to be completed in several ways:\n\nAll UK payments collected by Direct Debit are covered by the Direct Debit Guarantee, which is provided by the UK's banks and building societies.\n\nUnder the guarantee a payer is entitled to a full and immediate refund in the event of an error in the payment of a Direct Debit from their account. Where an error has occurred, refunds are paid immediately by the payer's bank, who will then attempt to recover the money from the merchant's bank, who in turn will attempt to recover the money charged back from the merchant.\n\nUnder the Direct Debit scheme rules, merchants have very few grounds to challenge a charge-back generated under the Direct Debit Guarantee. Instead, they can pursue any payments which they believe have been incorrectly refunded to the payer directly through the small claims court.\n\nBefore a company or organisation can collect payment by Direct Debit they need to be approved by a bank, or use a commercial bureau to collect direct debit payments on their behalf. This approval process ensures the company will be able to operate within the direct debit scheme rules and maintain the integrity of the scheme.\n\nIf a large number of customers complain about direct debits set up by a particular service user then the service user may lose its ability to set up direct debits.\n\nAny direct debit instruction that has not been used to collect funds for over 13 months is automatically cancelled by the customer's bank (this is known as a \"dormancy period\"). This can cause problems when the mandate is used infrequently, for instance, taking a payment to settle the bill for a seldom-used credit card. If the credit card company has not collected a payment using the Direct Debit mandate for over 13 months the direct debit mandate may have been cancelled as dormant without the customer's knowledge, and the direct debit claim will fail.\n\nThe problem of direct debit fraud is extensive according to research by Liverpool Victoria Insurance which reveals that over 97,000 Britons have fallen victim to criminals setting up fraudulent direct debits from their accounts. An average of £540 goes missing before the customer notices. Direct debit payment fraud in 2010 accounted for around 10.6% of all identity fraud cases. The extent of direct debit scamming is set to grow to 41,000 cases a year by 2013, equating to a 57% rise.\n\nHowever, the problem is exacerbated by some of the banks themselves for failing to implement any controls which prevent companies or fraudsters taking monies from business and consumer accounts. The problem of cancelled and obsolete direct debits being wrongfully revived or re-implemented is estimated to cost UK consumers £385 million in 2010. For those customers who find out, it takes them on average four months to notice. Although no specific figures were collected it appears a substantial number of people lose considerable amounts of money annually because the obsolete direct debit is neither noticed nor recovered.\n\nOn 7 January 2008, Jeremy Clarkson found himself the subject of direct debit fraud after publishing his bank account and sort code details in his column in \"The Sun\" to make the point that public concern over the 2007 UK child benefit data scandal was unnecessary. He wrote, \"All you'll be able to do with them is put money into my account. Not take it out. Honestly, I've never known such a palaver about nothing\". Someone then used these details to set up a £500 direct debit to the charity Diabetes UK. In his next \"Sunday Times\" column, Clarkson wrote, \"I was wrong and I have been punished for my mistake.\"\n\nBusinesses and organisations collecting payments through Direct Debit are instructed by Bacs to take mandatory training. Whether businesses are collecting independently or through a bureau, their relevant staff need to understand the fundamentals of the payment method. Courses are available through Bacs or through accredited external training. There are only four recognised companies in the UK providing Bacs accredited training: Accountis (D+H), Bottomline Technologies, Clear Direct Debit and SmartDebit.\n\nIn Germany, banks generally have been providing direct debit (\"elektronisches Lastschriftverfahren (ELV)\", \"Lastschrift\", \"Bankeinzug\") using both methods since the advent of so-called \"Giro\" accounts in the 1950s.\n\nThe \"Einzugsermächtigung\" (\"direct debit authorisation\") just requires the customer to authorize the payee to make the collection. This can happen in written form, orally, by e-mail or through a web interface set up by the payee. Although organisations are generally required not to instruct their banks to make unauthorised collections, this is usually not verified by the banks involved. Customers can instruct their bank to return the debit note within at least six weeks.\n\nThis method is very popular within Germany as it allows quick and easy payments, and it is suited even for one-time payments. A customer might just give the authorisation at the same time she or he orders goods or services from an organisation. Compared to payments by credit cards, which allow similar usage, bank fees for successful collections are much lower. Often retailers such as supermarkets will process Girocards as direct debit (ELV) transactions after performing a real-time risk analysis when the card is swiped. This is possible because the bank account number and routing code of the giro account form part of every Girocard's card number (PAN), similar to early 1990s 19 digit Switch card numbers. The customer agrees to the direct debit by signing the back of the receipt, which normally contains a long contractual text that also allows the retailer to contact the customer's bank and get their address in the case the debit is returned (\"Rücklastschrift\"). Direct debits are practically free for the retailer, allowing them to save the Girocard interchange fees (approx. 0.3%) that would be associated with PIN based transactions, but incur a higher risk as the payment can be returned for any reason for up to 45 days. Therefore, they are used usually for returning customers that have already had successful Girocard (PIN-verified) transactions at the same store or are purchasing low-risk or small-ticket items.\n\nTo prevent abuse, account holders must watch their bank statements and ask their bank to return unauthorised (or wrong) debit notes. As fraudulent direct debit instructions are easily traced, abuse is rare. However, there can be issues when the amount billed and collected is incorrect or unexpectedly large. There have also been cases of fraudulent direct debit where the defrauders tried to collect very small individual sums from large numbers of accounts, in the hope that most account holders would be slow to raise an issue about such small sums, giving the defrauders enough time to withdraw the collected money and disappear.\n\nThe \"Abbuchungsauftrag\" (\"posting off\") requires the customer to instruct his or her bank to honour debit notes from the organisation. Direct debits made with this method are verified by the customer's bank and therefore can not be returned. As it is less convenient, it is rarely used, usually only in business-to-business relationships.\n\nIn the Netherlands, like in Germany, an account holder can authorize a company to collect direct debit payments, without notifying the bank. Doing so is very common, with as much as 45% of all banking transactions conducted via direct debit.\n\nA transaction can be ongoing, or one-time only. For both types collecting organizations must enter into a direct debit (\"automatische incasso\") contract with their bank. For each transaction the name and account number of the account holder must be provided. The collecting organisation can then collect from any account, provided there is enough money on the account and no block is set against direct debit from the collecting organisation.\n\nTransactions can be contested depending on the type of transaction, time since the transaction and the basis of dispute. Authorized transactions of the ongoing type can directly be recalled via the bank of the account holder within the 56 days (8 weeks) since the transaction, with the exception of transactions with regards to games of chance and perishables. Authorized one-time only transactions can be recalled via the bank within 5 days. Unauthorized transactions can be contested via the bank within a limited time period after the transaction.\n\nAnother security measure is a \"selective block\" whereby the customer can instruct the bank to disallow direct debits to a specified account number. Blanket blocks are also available.\n\nIn Poland, direct debit is operated by KIR (Krajowa Izba Rozliczeniowa) and participating banks as one of the functionalities of the Elixir clearing system. The payer has to authorize the payee by filling, signing and submitting a standardized paper form in two copies. One copy, after filling in payee details and a customer identification number, is sent by the payee to the payer's bank, which verifies the signature. From now on, the payee may debit the payer's account. Since 24 Oct 2012 it is also possible to submit such authorization through the payer's bank, often also online.\n\nThe payer can:\n\nIn case of cancelling a transaction, funds are immediately returned to the payer's account. Interest is also adjusted as if the transaction never happened.\n\nCollecting fees through direct debit is supported mostly by big players such as telco, insurance and utility companies and banks themselves but it is not widely used by consumers because of the bureaucracy involved in setting direct debits up.\n\nThe direct debit system in Ireland was previously operated by the Irish Payment Services Organisation (IPSO). Following an integration of Irish Banking Federation (IBF) with the Irish Payment Services Organisation (IPSO), all contents from www.ipso.ie are now hosted on the Banking and Payments Federation Ireland website www.bpfi.ie \nDirect debit instructions can be given in writing or by telephone. There are protections for the holder of the account being debited in the event of a dispute. The Irish payment system is in the process of migrating to SEPA credits and direct debits. The conversion deadline from the legacy credit transfer and direct debit systems to the Eurozone-wide SEPA system is 01.02.2014. SEPA regulations\n\nDirect debit is preferred payment method for recurring household payments in Denmark. The service, launched back in the early seventies, is called \"Betalingsservice\" and is used by about 96% per cent of the Danish households.\n\nDirect debit is a very common payment option in Japan. When signing up for a service, such as telephone, a customer is usually asked to enter their bank details on the service submission form, to set up for automatic payments, and the company they are signing up to will take care of the rest. Sometimes, but not always, the customer is offered the possibility to enter credit card details instead of bank account details, to have the money directly debited from credit card instead of bank account.\n\nIn Malaysia, the direct debit system is available via the product known as FPX – Financial Process Exchange. FPX supports online direct debit as well as batch direct debit. It opens new doors for e-commerce in Malaysia, in particular business to business (B2B) and business to commerce (B2C) payments.\n\nFPX allow customers to make payment at e-market places such as websites and online stores as well as for corporations to collect bulk payment from their customers.\n\nIt leverages on the Internet banking services of participating banks and provides fast, secure, reliable, real-time online payment processing. FPX provides a complete end-to-end business transaction, resourceful payment records, simplified reconciliation and reduced risks as fund movements are between established financial institutions.\n\nSupported by Bank Negara Malaysia and the local financial institutions, FPX is operated by FPX Payment Gateway Sdn Bhd, a subsidiary company of Malaysian Electronic Payment System (1997) Sdn Bhd (MEPS).\n\nIn Australia, direct debit is performed through the direct entry system also known as BECS (Bulk Electronic Clearing System) or CS2, managed by the Australian Payments Clearing Association. An account holder can authorise a company to collect direct debit payments, without notifying the bank, but direct debit is not available on all financial accounts.\n\nA common example of direct debit is authorising a credit card company to debit a bank account for the monthly balance.\n\nMany smaller companies do not have direct debit facilities themselves, and a third-party payment service must be used to interface between the biller and the customer's bank. For this a small charge (typically $1–2 per transaction, incorporated into the bill amount) is made by the payment service.\n\nDirect credit and debit instructions are only available to institutional customers. Direct credit instructions are used for payrolls and other large scale regular payments. Direct debit instructions are used by insurance companies, utilities and other large organisations which receive regular payments. Although governed by APCA rules, the actual exchanges of instructions occur through bilateral exchanges. There is no central \"clearinghouse\" for bulk direct entry payment instructions.\n\nIn the United States, direct debit usually means an Automated Clearing House (ACH) transfer from a bank account to a biller, initiated by the biller.\n\nIn South Africa direct debits, also known as debit orders, are performed through the ACB. An account holder can authorise a company to collect direct debit payments. The client signs a debit mandate form giving the requesting company permission to debit their account with a fixed or variable monthly value. This value can be recurring or once-off. This is an effective, safe and more cost effective alternative to receiving money in cash, by cheque or EFT (Electronic Funds Transfer).\n\nThere are three types of commonly used debit orders in South Africa: EFT (Electronic Funds Transfer), NAEDO (Non-authenticated Early Debit Order) and AEDO (Authenticated Early Debit Order).\n\nA new payment stream is being introduced by the South African Reserve Bank called AC or Authenticated Collections. This payment stream will be implemented from September 2016 (early 2017) and aims to replace the current NAEDO payment stream. The purpose of AC is to reduce customer disputes and abuse of the debit order system. The AC payment stream will be known to consumers as DebiCheck.\n\nIn Sweden, direct debit is available, called \"Autogiro\". The amount is withdrawn from the payer's account on the payee's request without specification. The approval is done once without time limit for each payer/payee combination, but can be withdrawn at any time. The request is normally sent through a form from the payer to the payee, but in the internet bank it is also possible to request it. There is a newer variant also, called \"E-faktura\", electronic bill. There the payer gets a specification in the computer (through the bank web page) looking like a bill, and is asked to approve every payment, even if it can be set up for automatic approval.\n\nIn Sweden, the problem of direct debit fraud is much smaller than in the UK, since there are more strict requirements on which companies that can use direct debit. Still there are claims that well-known companies have taken too much money through direct debit.\n\nIn Turkey, direct debits are widely used, for utility and credit card payments, as well as commercial transactions. However, whereas in other countries the payee instructs his/her bank to make a collection, in Turkey the payer needs to authorise his/her bank directly. The usage differs from standing orders, as payment amounts are not fixed and payments need not be periodical (i.e. payments can be of any amount and can be casual or periodic).\n\n\n"}
{"id": "25721743", "url": "https://en.wikipedia.org/wiki?curid=25721743", "title": "Drop table", "text": "Drop table\n\nA drop table or wheel drop is a device used in railway engineering during maintenance jobs that require the removal of locomotive or rolling stock wheelsets. The machine is built in a \"drop pit\" allowing a locomotive or rolling stock to be rolled onto it, avoiding the need for heavy cranes or jacks to lift the vehicle off the rails.\n\nThe vehicle is placed over the drop table, and the connections attaching the wheelset to the vehicle are unfastened. This allows the wheel set to 'float' independently of the locomotive. The wheelset is lowered into the drop pit on a short section of rail, and a dummy rail, normally a part of the drop table machinery, is then inserted in the gap over the lowered wheelset. This enables the vehicle to be moved clear of the drop table on its remaining wheels, so that the removed wheelset can then be lifted out of the drop pit for maintenance work to be performed on it.\n\n"}
{"id": "18661672", "url": "https://en.wikipedia.org/wiki?curid=18661672", "title": "FTDI", "text": "FTDI\n\nFuture Technology Devices International, commonly known by its acronym FTDI, is a Scottish privately held semiconductor device company, specializing in Universal Serial Bus (USB) technology.\n\nIt develops, manufactures, and supports devices and their related software drivers for converting RS-232 or TTL serial transmissions to USB signals, in order to allow support for legacy devices with modern computers.\n\nFTDI provides application-specific integrated circuit (ASIC) design services. They also provide consultancy services for product design, specifically in the realm of electronic devices.\n\nFTDI was founded on 13 March 1992 by its current CEO Fred Dart. The company is an indirect descendant of Computer Design Concepts Ltd, a former semiconductor technology startup, founded by Dart.\n\nFTDI's initial products were chipsets for personal computer motherboards, the primary customer of which was IBM, which used them in its AMBRA and PS/1 personal computers. It later expanded its product line to include interface translators, such as the MM232R and the USB-COM232-PLUS1, along with other devices for converting between USB and other communication protocols.\n\nCurrently, the headquarters for FTDI is located in Glasgow, Scotland, United Kingdom, while it also has offices in Singapore, Taipei, Taiwan, and Portland, Oregon. The company's manufacturing division is handled by subcontractors in the Asia Pacific region.\n\nOn 29 September 2014, FTDI released an updated version of their USB-to-Serial driver for Windows on their website. Users who manually downloaded the new drivers reported problems. After Windows drivers became available on 14 October (\"Patch Tuesday\") via Windows Update, it was reported by users of hardware enthusiast forums and websites that the drivers could soft brick counterfeit and software-compatible clones of the chips by changing their USB \"Product ID\" to \"0000\". The change prevents the chip from being recognized by drivers of any OS, effectively making them inoperable unless the product ID is changed back. The behaviour was supported by a notice in the drivers' end user license agreement, which warned that use of the drivers with non-genuine FTDI products would \"irretrievably damage\" them. Critics felt that FTDI's actions were unethical, considering that users may be unaware that their chips were counterfeit, or that Windows had automatically installed a driver meant to disable them. On 22 October 2014, an emergency patch was made to the FTDI drivers in the Linux kernel to recognize devices with the \"0000\" ID.\n\nOn 24 October 2014, in response to the criticism, FTDI withdrew the driver and admitted that the measure was intended to protect its intellectual property and encourage users to purchase genuine FTDI products. The company also stated that it was working to create an updated driver which would notify users of non-genuine FTDI products in a \"non-invasive\" manner.\n\nIn February 2016, it was reported that FTDI had published another driver on Windows Update with DRM components intended to block non-genuine products. This time, the driver will communicate with affected devices, but all transmitted and received data is replaced with the arbitrary, looped ASCII string \"NON GENUINE DEVICE FOUND!\", which could cause irregular interactions with devices.\n\nFTDI sells its products through major distributors around the world. FTDI also sells products through its ftdichip.com website.\n\n"}
{"id": "12568490", "url": "https://en.wikipedia.org/wiki?curid=12568490", "title": "Federal Ministry of the Environment, Nature Conservation and Nuclear Safety", "text": "Federal Ministry of the Environment, Nature Conservation and Nuclear Safety\n\nThe Federal Minister for the Environment, Nature Conservation, and Nuclear Safety, (), abbreviated BMU, is a cabinet-level ministry of the Federal Republic of Germany. It has branches in Bonn and Berlin.\n\nThe ministry was established on 6 June 1986 in response to the Chernobyl disaster. The then Federal Government wanted to combine environmental authority under a new minister in order to face new environmental challenges more effectively. Prior to this responsibilities for environmental issues were distributed among the ministries of the Interior, Agriculture and Health.\n\nThe ministry's primary functions include:\n\n\nThe ministry is led by the Minister for the Environment, Nature Conservation and Nuclear Safety. The current Minister is Svenja Schulze, appointed by Chancellor Angela Merkel. The minister is supported by two parliamentary state secretaries (members of the cabinet and federal government, \"deputy ministers\") and two career state secretaries (public servants) who manage the ministry's nine directorates:\n\n\nPolitical Party:\n\n"}
{"id": "21162612", "url": "https://en.wikipedia.org/wiki?curid=21162612", "title": "Fentin acetate", "text": "Fentin acetate\n\nFentin acetate is an organotin compound with the formula (CH)SnOCCH. It is a colourless solid that was previously used as a fungicide.\n"}
{"id": "11086565", "url": "https://en.wikipedia.org/wiki?curid=11086565", "title": "Flame programmer", "text": "Flame programmer\n\nA flame programmer is an electrical, electro-mechanical, or electronic device used to program the safe lighting of fuel burning equipment, as well as the safe shut-down of the flame when it is not needed.\n\nThese programmers are made with different time sequences to accommodate very small household furnaces to mammoth industrial steam boilers and many other fuel burning processes in industry.\n\nDuring pre-purge, the combustion air fan is started and the dampers are opened to allow fresh air into the combustion chamber and exhaust any other gases or residual air-fuel mixtures.\n\nIn the pilot trial, the pilot solenoid is opened and the ignition transformer is turned on. The fuel is lit to make sure the pilot flame stays lit. There is a flame detection device that will shut down the fuel valve if the flame fails or goes out \n\nWith the pilot still burning, the main burner valves are opened and the main flame is lit. The flame detection device here will also shut-down or close all the fuel valves if the flame is extinguished.\n\nThe pilot valve is shut and the main burner is left on and burning. The flame detector remains operational to kill the fuel valves if the flame should fail.\n\nWhen the flame is no longer needed, the fuel valves are shut and the combustion air fan is left running to clear the combustion chamber of all unburnt fuel and products of combustion. The combustion air fan is shut down after a period of time.\n"}
{"id": "3235923", "url": "https://en.wikipedia.org/wiki?curid=3235923", "title": "Fluorene", "text": "Fluorene\n\nFluorene , or 9\"H\"-fluorene, is a polycyclic aromatic hydrocarbon. It forms white crystals that exhibit a characteristic, aromatic odor similar to that of naphthalene. It is combustible. It has a violet fluorescence, hence its name. For commercial purposes it is obtained from coal tar. It is insoluble in water and soluble in many organic solvents.\n\nAlthough fluorene is obtained from coal tar, it can also be prepared by dehydrogenation of diphenylmethane. Alternatively, it can be prepared by the reduction of fluorenone with zinc. The fluorene molecule is nearly planar, although each of the two benzene rings is coplanar with the central carbon 9.\n\nThe C9-H sites of the fluorene ring are weakly acidic (pK = 22.6 in DMSO.) Deprotonation gives the stable fluorenyl \"anion\", nominally CH, which is aromatic and has an intense orange colour. The anion is a nucleophile, and most electrophiles react with it by adding to the 9-position. The purification of fluorene exploits its acidity and the low solubility of its sodium derivative in hydrocarbon solvents.\n\nBoth protons can be removed from C9. For example, 9,9-fluorenyldipotassium can be obtained by treating fluorene with potassium metal in boiling dioxane.\n\nFluorene and its derivatives can be deprotonated to give ligands akin to cyclopentadienide.\nFluorene is a precursor to other fluorene compounds; the parent species has few applications. Fluorene-9-carboxylic acid is a precursor to pharmaceuticals. Oxidation of fluorene gives fluorenone, which is nitrated to give commercially useful derivatives. 9-Fluorenylmethyl chloroformate (Fmoc chloride) is used to introduce the 9-fluorenylmethyl carbamate (Fmoc) protecting group on amines in peptide synthesis.\n\nPolyfluorene polymers (where carbon 7 of one unit is linked to carbon 2 of the next one, displacing two hydrogens) are electrically conductive and electroluminescent, and have been much investigated as a luminophore in organic light-emitting diodes.\n\nCicloprofen is a NSAID 2-arylpropionic acid made from fluorene.\n\nFluorene dyes are well developed. Most are prepared by condensation of the active methylene group with carbonyls. 2-Aminofluorene, 3,6-bis-(dimethylamino)fluorene, and 2,7-diiodofluorene are precursors to dyes. \n\n\n"}
{"id": "31624264", "url": "https://en.wikipedia.org/wiki?curid=31624264", "title": "Form and document creation", "text": "Form and document creation\n\nForm and Document Creation is one of the things that technical communicators do as part of creating deliverables for their companies or clients. Document design is: \"the field of theory and practice aimed at creating comprehensible, persuasive and usable functional documents\". These forms and documents can have many different purposes such as collecting or providing information.\n\nEva Brumberger, an instructor in the professional writing program at Virginia Tech, surveyed professional writers about the nature of their work in order to evaluate what student writers are taught before entering the work force. These professional writers confirmed that their role has developed past the verbal communication dominant in the literature presented to students and has developed into mostly visual communication.\n\nTechnical communicators must take data and convert it into information; this process is known as visualization, or visual communication. Because of the widespread use of digital media, modern technical communicators must also now think about visualization as it relates to digital forms and documents. Stuart K. Card, Jock D. Mackinlay, and Ben Shneiderman, editors of the book \"Readings in Information Visualization: Using Vision to Think\", define visualization as the: \"use of computer-supported, interactive, visual representations of data to amplify cognition\". Though many forms and documents will still have a paper copy for distribution, most forms and documents are now utilized online in some fashion; this is why there is such focus on the computer-supported representations for maximal cognition. Brumberger defines visual communication as: \"designing print, Web, and multimedia documents…creating visual displays of information/data, generating other visual material…and any other communication tasks which rely on visual language\".\n\nThere are many areas where professional writers utilize visualization. It is most useful in the following areas: complex documents, statistical and categorical data, personal services, and histories.\n\nVisual communication responsibilities include: designing visual content, determining when to use visual material, modifying existing material, and applying templates that already exist to material.\n\nVisual communication tasks include designing: presentations, print documents, page layouts, images, and data displays.\n\nHuman-centered design focuses on ensuring that the audience will comprehend the information being presented. It is: \"how a frustrated and confused subject...comprehends a critical message in a crowded and noisy environment\". The goal of human-centered design is \"to make information accessible\" and \"to give form to data\".\n\nLuke Wroblewski, senior director of Project Ideation and Design at Yahoo! Inc., and author of \"Web Form Design: Filling in the Blanks\", also has some human-centered design ideas for web forms and documents. He says: \"because people want what’s on the other side of a web form, their general tendency is to jump right in, start answering questions and hope to get it done quickly\". As a result, he recommends designing a clear path to completion for the form or document. He also mentions messaging without proper priority, like hard-to-find error messages, and unconnected primary actions that can similarly cloud the steps people need to take in order to get through a form. For a web form to have human-centered design, information must be structured \"in a logical pattern from start to finish\".\n\nWhen a technical communicator is creating a form or document, it is vital that they pay close attention to structure and organization as these are the means that allow visualization to work.\n\nIn order to design a form or document, the writer should understand and evaluate the different constraints in the rhetorical situation; this is called functional analysis. One of the biggest components in analyzing a form or document is to determine the communicative purpose of the form or document. Leo Lentz and Henk Pander Maat, at the University of Utrecht, break down communicative purpose into four elements:\n\nAfter analyzing the communicative purpose, the technical communicator can design a form or document that will match the requirements and meet expectations for it.\n\nOne aspect of form and document creation that technical communicators should pay close attention to is explicit structure. When the structure is explicit, the reader can interact with the form or document on a more effective level. The technical communicator’s \"primary means to make structure explicit is through headings and links\". The technical communicator must add these headings when they are drafting the form or document, because the structure will remain implicit until they are added.\n\nThe authors of \"Meet Your Type: A Field Guide to Love & Typography\" add hierarchy to the idea of making structure explicit. They say: \"effective hierarchy gets people to look where you want them to look, when you want them to look there. Without it the reader is left confused and frustrated. Emphasis can be stressed by size, weight, color, style, and placement\". Thus, emphasis from several different font decisions joins headings as a feature that makes structure explicit.\n\nAnother aspect to consider when designing a form or document is abstract structure. This is the idea that text has a graphical component. Text incorporates a graphical component not only because the words are: \"often accompanied by conventional graphics such as pictures or diagrams, but they themselves form graphical elements such as titles, headings, chapters, sections, captions, paragraphs, and bulleted lists\".\n\nWhen considering abstract structure in planning a form or document, a technical communicator must also look at what Richard Power, Nadjet Bouayad-Agha, and Donia Scott call document structure: \"the organization of a document into graphical constituents like sections, paragraphs, sentences, bulleted lists, and figures\". This document structure also goes hand-in-hand with the human-centered design aspect of visualization as pertaining to form and document creation. Technical communicators should look at their form or document to make sure that the abstract structure of the form or document is helping achieve the overall goal with the reader.\n\nThough it focuses on a visual and graphic effect, abstract structure also focuses on wording. The examples that follow are taken verbatim from Power, Bouayad-Agha, and Scott. They show a progression from a passage written by a technical communication novice (a), to an edit by a more experienced technical communicator (b), to an edit by a senior expert technical communicator (c). The successive changes are designed to make the structure and wording valid.\n\nOther considerations the technical communicator keeps in mind when creating a form or document include: number of pages, flush, capital letters, and bullets.\n\nIf at all possible, using one page for the document or form is best because the reader can: \"glance at the information without flipping pages or having to search for related sections, both results of poor planning that can distract and confuse the reader\".\n\nWhen typing a document, the technical communicator should make the text flush left because: \"it’s the easiest to read because we read it most often\". Keeping the text flush left instead of justifying it: \"[gives] the text a more harmonious appearance and makes it easier to read, since all wordspaces have the same width\". The reason justified text should be avoided is because of the: \"hideously [stretched] and [squished] words and spaces\".\n\nCapital letters should not be used to accentuate words on a form or document; it is too distracting and disrupts the look of the form or document. When capital letters are absolutely necessary, say in the case of an acronym or abbreviation, small caps should be used, with or without initial caps. When capital letters are not absolutely necessary, the technical communicator should evaluate the effectiveness of italicizing the word or phrase for emphasis.\n\nSome technical communicators use hyphens for listed items; however, a writer should use bullets or centered points instead.\n\nIn analyzing structure and organization for a form or document, it is beneficial for a technical communicator to determine if the form or document being created fits within a group of documents called the standard expository model (SE model). If the form or document being created is an SOE, then there are special strategies to writing the form or document.\n\nStandard expository models are: \"nonfiction print documents that (1) are primarily informational rather than expressive; (2) are intended to be read linearly (broadly speaking, from beginning to end); and (3) reveal structure explicitly with at least one heading level but very often more.\"\n\nIf a form being created fits within the SE model, there are three main strategies to be employed when creating it.\n\nGillian Harvey, a partner and senior designer at Plumbheavy Design Inc., a company that does graphic design and information design, has several recommendations for technical communicators regarding language and word choice.\n\nAfter evaluating the purpose and desired effect of a form or document, and creating a structure and wording that meets that purpose and effect, a technical communicator may think the majority of the job is complete. However, the typeface used for a form and document can greatly affect not only the reader, but the purpose and effect of that form or document.\n\nMost simply, \"a font is what you use, and a typeface is what you see\". \"The Typographer’s Glossary\" defines typeface as: \"An artistic interpretation, or design, of a collection of alphanumeric symbols\". Typeface includes \"letters, numerals, punctuation, various symbols, and more\". \"A typeface is usually grouped together in a family containing individual fonts for italic, bold, and other variations of the primary design\". A font, on the other hand, is: \"a collection of letters, numbers, punctuation, and other symbols used to set text (or related) matter\". To further explain, \"font refers to the physical embodiment…while typeface refers to the design\". In any event, the terms \"font\" and \"typeface\" are used interchangeably by some authors and designers.\n\nJo Mackiewicz, from the Composition and Linguistics Department of the University of Minnesota Duluth, has done extensive research into typeface and has published multiple articles on the topic. Mackiewicz says that students should: \"select typefaces that are appropriate for their technical documents\". What Mackiewicz means when she talks about an \"appropriate typeface\" is that it contributes to the desired \"overall rhetorical effect\" and conveys \"more specific effects…as intended\". In another article, Mackiewicz points out that \"typefaces substantially contribute to the visual, as opposed to the verbal, language of documents\". This is important, since it has already been established that professional technical communicators see their role as largely visual as compared to verbal.\n\nIn selecting an appropriate typeface, Mackiewicz focuses on what she calls \"typeface personality\". She researches other technical communicators’ works to come up with a definition of typeface personality as \"that aspect of typeface that imbues it ‘with the power to evoke in the perceiver certain emotional and cognitive responses’\" and \"the ability to convey different feelings and moods…strength, elegance, agitation, silliness, friendliness, scariness, and other moods\". Mackiewicz further explains that: \"increased attention to typeface personality is especially important now that students have access to thousands of typefaces, many of which can detract from or conflict with the seriousness, professionalism, and competency that students intend to convey\". The selection of typeface is also important in situations where more than one typeface is present in a form or document. Mackiewicz says: \"if more than one typeface is being used within a document, students should also carefully consider the extent to which the personalities of the typefaces they have selected are concordant\".\n\nOne way that Mackiewicz notes that technical communicators can determine a typeface’s personality is through looking at its history; she says: \"the personality a typeface conveys may stem in large part from the ways in which that typeface has been used in the past\". To show what she means, Mackiewicz notes that the typeface Fette Fraktur is rarely used today because it was used in Nazi propaganda from 1933 to 1945. Because of situations like the one involving Fette Fraktur, Mackiewicz points out: \"the ways in which a typeface has been used [in the past] can influence the overall affect (sic) of a student’s document and, consequently, it can send an unintended message\".\n\nAs in every stage of form and document design, technical communicators must be constantly aware of the impressions of design decision on the reader. Pamela W. Henderson, Joan L. Giese, and Joseph A. Cote, faculty in the Department of Marketing at Washington State University, point out that: \"it is important to determine the impact of the impressions created by typeface\". Their research also shows \"that individual differences [in typeface] can affect attentiveness to aesthetics\", or the pleasing effect of the form or document.\n\nThe \"Font Shop\" professionals also have a warning concerning typeface and its impression on readers. They recommend \"[avoiding] the embarrassment of typographic rejection by first determining the likes and dislikes of your target audience\".\n\nAdditionally, Jo Mackiewicz recommends technical communicators consider typefaces that are both legible and readable. Her research has shown that legible typefaces have \"the quality of being decipherable and recognizable\" and are important \"in situations where people are scanning pages, reading signs, or skimming through catalogs or lists\". One example of a legible typeface is Univers, while an illegible typeface example would be Snap ITC. On the other hand, typefaces that are readable have: \"the quality of giving ‘visual comfort,’ which is especially important in long stretches of text\". Mackiewicz uses Times New Roman as an example of an easily read typeface while Impact typeface is less so. Legibility and readability are important aspects of typeface to consider if the reader is going to be required to read and comprehend a large amount of text.\n\nOne aspect of typeface selection to consider is whether or not to use serif or sans serif typefaces. Serif typefaces are \"based on the carvings of the ancient Romans\" and \"feature small ‘feet’ at the end of the letterforms\". Jo Mackiewicz points out that: \"traditionally, serif typefaces have been used for the body text of technical (as well as other) documents because they seem to be more readable than sans serif typefaces\". On the other hand, sans serif typefaces \"were designed for the industrial age\" and are \"hard-working and modern, with no need for fancy serifs\". Sans serif typefaces \"are often used in ‘display’ elements like headings, diagrams, and tables\". Based on this information, technical communicators are advised to \"pair a serif and sans serif\" in their forms or documents.\n\nBecause of the prevalence of computers and other electronic media in the modern world, there are some special considerations for forms and documents that will be online. New typefaces are being developed specifically for forms and documents to be presented electronically. ClearType, developed by Microsoft in 1998 \"to improve the legibility of typefaces viewed on LCD displays\", encompasses seven of these new typefaces. They were \"designed for online reading of business documents, email, and web pages\". However, after a study of the online legibility of ClearType, Times New Roman, and Verdana typefaces, researchers concluded \"that it is not the technology alone that dictates legibility\", as some of the ClearType typefaces were more legible than other ClearType typefaces and one of the non-ClearType typefaces. A technical communicator, creating an online document, should carefully analyze the readability of the typeface selected for their form or document.\n\nIn order to see how the various typeface aspects work together for typeface selection, look at the Times New Roman typeface. Mackiewicz notes that its letterforms \"display complexity and perfection\". She also lists features of the Times New Roman typeface that make it professional in personality: \"moderate weight, moderate thick-to-thin transition, balanced straight-edged and rounded terminals, moderate x-height to cap-height ratio, uppercase \"J\" that sits on the baseline, horizontal crossbar on the \"e\" letterform, double-story \"a\" letterform, and double-story \"g\" letterform\". Reid Goldsborough, a syndicated newspaper columnist, provides the history of the Times New Roman typeface. Times New Roman \"was commissioned by the British newspaper \"The Times\" in 1931\", and in 2004, the U.S. State Department \"mandated that all U.S. diplomatic documents use Times New Roman instead of the previous Courier New\". In Jo Mackiewicz’s study of typefaces, \"one participant said that Times New Roman could be used in ‘any lengthy passages that need good readability\". Finally, in a study that evaluated Times New Roman against the newer ClearType typefaces, it was found that no participants confused Times New Roman letterforms with Times New Roman numbers, symbols, or other letterforms. Based on these observations, a technical communicator could determine that Times New Roman would be an effective typeface for a form or document if the purpose was professional, the document was being read in any format, and reader readability was required.\n\nDavid Sless, director and co-founder of the Communication Research Institute talks about what he calls: \"a crucial aspect of public communication: the demonstration, through testing and measurement, that an [organization’s] communication with its public does indeed work; that evidence can be produced to show that the information design is of the highest quality\". He is calling attention to the fact that documented evaluation is an important part of form and document design.\n\nEvaluation will show where a form or document needs to be improved, even when that form or document meets the overall needs for which it was created. For example, Michael Turton, a veteran designer of transactional documents and forms, was surprised to find that coworkers were having trouble with a form he designed that he knew was adequate. The form required employees to check boxes that measured 7mm by 6mm, but reported that these boxes needed to be bigger. Knowing that the boxes were adequate, Turton asked the employees to show him the problem; as it turned out, the employees were left-handed and when they were trying to mark a box, their writing hand was covering the writing on the form. So, through evaluation, Turton was able to discover the needs of his audience and created a left-handed form as well. This shows the importance of evaluation in form and document creation.\n\nSless points out several key factors to look for when evaluating a form or document. He recommends evaluation to determine if a form or document is: \"attractive, socially appropriate, physically appropriate, respectful [of the audience], credible, and containing information that is accessible and usable\".\n\nSless diagrams a circular pattern for the \"systematic process\" of forms and documents. There are seven stages to his diagram, and then number seven leads back into number one. The seven stages are: scoping, benchmarking, prototyping, testing, refining (returning to step four as many times as needed), implementing, and monitoring.\n\nHe says that evaluation must take place at three points in this \"systematic process\": during step two, during steps four and five, and during step seven. A form or document should be evaluated at the benchmarking stage to determine how a current design is working. It should be used in the testing and refining stages to evaluate changes being made. Finally, the form or document should be evaluated during the monitoring stage as it is in use to \"maintain its optimal performance\". Sless emphasizes the importance of testing at these designated times instead of evaluating as time and money permits.\n\nThere can be many individualized decisions and problems that can occur depending on the form or document being created, the target audience, and the organization producing the form or document. These problems can occur because of rules set by a technical communicator’s own company or an external governing body. Problems can also occur based on technical problems that the technical communicator could not prepare for. To highlight these potential problems, look at forms and documents in the medical field.\n\nIn a study done by professionals from multiple companies and universities, family medical practices administered surveys on paper forms and electronically. However, the team could not accurately judge the advantage of the electronic form because of problems with the firewall and \"institutional computer security issues\". The researchers report that: \"there does not appear to be an easy solution to these technical issues, especially in instances where the practice is part of a larger organization (e.g., university, hospital) that has strict requirements and procedures in place to limit transmission of information between the institution and external Internet Web sites\". This example shows where technology itself can interfere with the effectiveness of a form. Also, institutional requirements can limit the design being created by a technical writer.\n\nSometimes, there are external restrictions on a form or document that could affect the design process. Rita Tomlin, a freelance writer and instructor at San Diego State University, investigates the implications of the Food and Drug Administration (FDA) regulations on medical writing. Tomlin says, \"An essential task for the medical writer is to tailor data presentation and document content to the FDA’s expectations.\" The medical writer is therefore restricted by FDA expectations and not just organizational or personal expectations. Medical writers are expected to come to \"an understanding of the FDA’s complex expectations\" by \"careful reading and interpretation of the FDA regulations and guidance documents.\" This is an example of the type of work a form or document designer may have to do. There could be external research needed before form or document design or redesign can occur.\n\n"}
{"id": "52251277", "url": "https://en.wikipedia.org/wiki?curid=52251277", "title": "GTUIT", "text": "GTUIT\n\nGTUIT is a well site gas processing firm. The company's mobile equipment collects gas that would otherwise be flared and turns it into usable natural gas liquid and conditioned gas. In 2015, the company received an equity investment from Caterpillar Inc.'s Oil and Gas division and signed a marketing deal with the company in 2016. The company is headquartered in Billings, Montana and has a field office in Watford City, North Dakota.\n\nGTUIT was founded in August 2011 by the mechanical engineers, Brian Cebull and Jim Haider, and the chemical engineer, Mark Peterson after the three recognized the problems that flaring gas could cause. The company launched its first prototype, a device that separated and stored gases in 2012. GTUIT deployed its second generation of equipment in 2013. The company's first customer was Denbury Resources.\n\nIn April 2015, Caterpillar Oil and Gas made an equity investment in GTUIT, the funds raised by the investment allowed the company to expand manufacturing and seek new markets. That same year the company received ISO 9001:2008 certification for its manufacturing quality management systems, the company also received an award from the World Bank Global Gas Flaring Reduction Partnership for its work with Hess Corporation in the Bakken region.\n\nIn June 2016, the company signed a marketing agreement with Caterpillar Oil and Gas, the agreement allowed Caterpillar to sell and service GTUIT products.\n\nIn September 2016, the company was ranked 203 on \"Inc.'s\" list of 500 fastest growing companies in the US.\n\nGTUIT designs, markets and operates mobile well site gas processing equipment that can be set up in a day. There are three processing capacities for the systems, 250 MSCFD, 500 MSCFD and 1000 MSCFD. The systems are designed to capture gas that would otherwise be flared and remove and store valuable and harmful gasses from escaping into the atmosphere. The company uses a compression and refrigeration system that connects directly to the wellhead and can capture up to 3,856 tons of volatile organic compound emissions. The collected natural gas liquids can then be trucked to markets and sold or depending on the gas can be used as on-site fuel.\n"}
{"id": "8553629", "url": "https://en.wikipedia.org/wiki?curid=8553629", "title": "Harold Gatty", "text": "Harold Gatty\n\nHarold Charles Gatty (5 January 1903 – 30 August 1957) was an Australian navigator and aviation pioneer. Charles Lindbergh called Gatty the \"Prince of Navigators\". In 1931, Gatty served as navigator, along with pilot Wiley Post, on the flight which set the record for aerial circumnavigation of the world, flying a distance of 15,747 miles (24,903 km) in a Lockheed Vega named the \"Winnie Mae\", in 8 days, 15 hours and 51 minutes.\n\nGatty was born on 5 January 1903 in Campbell Town, Tasmania.\n\nHe began his career as a navigator in 1917, at age 14, when he was appointed a midshipman at the Royal Australian Naval College. After World War I, he became an apprentice on a steamship in the Australian merchant navy, where he learned constellations while standing night watch. He became an expert in celestial navigation and served on many ships, some sailing between Australia and California. After the Navy, he worked in Sydney Harbour provisioning vessels anchored there. In 1927 he relocated to California.\n\nGatty opened a navigation school, teaching marine navigation to yachtsmen. In 1928, his attention turned to air navigation.\n\nIn 1929, Gatty navigated a Lockheed Vega on a flight from Los Angeles to New York City for Nevada Airlines, in an effort to demonstrate the feasibility of coast-to-coast passenger service. The flight made four stops and took 19 hours and 53 minutes, which set the transcontinental airspeed record for a commercial airliner.\n\nIn 1930, Gatty prepared a coast-to-coast route and navigation charts for Anne Morrow Lindbergh, whom he had also taught as a student. Anne Lindbergh served as navigator with her husband Charles on a record-setting cross-country flight of 14 hours and 45 minutes.\n\nThe next year, Wiley Post asked Gatty to accompany him on an effort to break the world record for circumnavigating the earth, which was previously set at 21 days by the \"Graf Zeppelin\" airship. Gatty accepted, hoping to demonstrate the effectiveness of his navigation methods. The journey began on 23 June 1931 at Roosevelt Field in New York and followed a 15,000-mile course across Europe, Russia, and Siberia, due to the lack of suitable airfields nearer the equator. Post and Gatty crossed the Atlantic in a record time of 16 hours and 17 minutes and continued to Berlin, Moscow, and Khabarovsk, then crossed the Bering Sea, landing on the beach near Solomon, Alaska, then to Edmonton, Alberta, arriving finally back at Roosevelt Field after 8 days, 15 hours, and 51 minutes. The pair received a tickertape parade in New York City.\n\nAir navigation in Gatty's time used dead reckoning. When setting out for a destination the aircraft heading is taken with respect to a compass. Motion over the earth is determined by the wind triangle. Heading must therefore be compensated for wind speed as well as drift rate. \nIn 1931 Popular Mechanics published an article featuring Gatty's method for computing the wind drift experienced by an aircraft:\n\nThe article also lauds the artificial horizon and turn and bank indicator that Post and Gatty used, which was developed by Lawrence Sperry and manufactured by the Sperry Gyroscope Company.\n\nA year after the circumnavigation with Wiley Post, the US Congress passed a bill allowing civilians to receive the Distinguished Flying Cross. President Hoover presented the medals to Gatty and Post at the White House on August 18, 1932. Gatty was offered American citizenship and the newly created position of Senior Aerial Navigation Engineer for the US Army Air Corps. Gatty expressed his wish to remain associated with Australia and Congress passed a bill allowing foreign citizens to hold that post.\n\nIn 1934, Gatty formed the South Seas Commercial Company with Donald Douglas, with the plan to deliver air service to the islands of the South Pacific. However, the company was soon sold to Pan Am who brought Gatty into the company to organise flight routes in that region.\n\nDuring the Second World War, Gatty was given the honorary rank of group captain in the Royal Australian Air Force (RAAF) and worked for the US Army Air Forces (USAAF) in the South Pacific. He was later appointed director of Air Transport for the Allied forces, based in Australia, under General Douglas MacArthur. Gatty moved to Washington, D.C. in 1943 where he worked on a navigational supplement to a survival kit for Air Force personnel flying over the Pacific in the event they should become castaways.\n\nGatty produced \"The Raft Book: Lore of the Sea and Sky\" to fill the need:\n\nAfter World War II, Gatty relocated to Fiji with his Dutch-born second wife. Here he formed Fiji Airways which later became Air Pacific, subsequently the company name was changed in 2013 back to Fiji Airways.\n\nGatty suffered a stroke and died in 1957. He was buried in Fiji.\n\nIn the end, Gatty advocated \"natural navigation\" in his book \"Nature is your Guide\" which was published in 1958. He expanded on the ideas of \"The Raft Book\" and developed a narrative of Pacific settlement by Polynesian navigators following migration of seabirds. He attributed cultural significance to the use of the pelorus by ancient Polynesians.\n\n"}
{"id": "25571663", "url": "https://en.wikipedia.org/wiki?curid=25571663", "title": "High viscosity mixer", "text": "High viscosity mixer\n\nHigh viscosity mixers are mixers designed for mixing materials with laminar mixing processes because the ingredients have such high viscosities that a turbulent mixing phase cannot be obtained at all or cannot be obtained without a high amount of heat. The process can be used for high viscosity liquid to liquid mixing or for paste mixing combining liquid and solid ingredients. Some products that may require laminar mixing in a high viscosity mixer include putties, chewing gum, and soaps. The end product usually starts at several hundred thousand centipoise and can reach as high as several million centipoise. \n\nTypical mixers used for this purpose are of the Double Arm, Double Planetary or Planetary Disperser design. Models are built to include many features such as vacuum and jacketing to remove air and to control the temperature of the mixture. Capacities are available from 1/2 pint to several thousand gallons.\n\n"}
{"id": "24823037", "url": "https://en.wikipedia.org/wiki?curid=24823037", "title": "Hydride compressor", "text": "Hydride compressor\n\nA hydride compressor is a hydrogen compressor based on metal hydrides with absorption of hydrogen at low pressure and desorption of hydrogen at high pressure by raising the temperature with an external heat source like a heated waterbed or electric coil. \n\nAdvantages of the hydride compressor are the high volumetric density, no moving parts and reversible absorption/desorption, disadvantages are the high cost of the metal hydride and weight.\n\nThe first applications of metal hydrides were made by NASA to demonstrate long-term hydrogen storage for use in space propulsion. In the 1970s, automobiles, vans, and forklifts were demonstrated. The metal hydrides were used for hydrogen storage, separation, and refrigeration. An example of current use are hydrogen sorption cryocoolers and portable metal hydride compressors.\n\n"}
{"id": "55200177", "url": "https://en.wikipedia.org/wiki?curid=55200177", "title": "IPhone X", "text": "IPhone X\n\niPhone X (Roman numeral \"X\" pronounced \"ten\") was a smartphone designed, developed, and marketed by Apple Inc. It was the eleventh generation of the iPhone. It was announced on September 12, 2017, alongside the iPhone 8 and iPhone 8 Plus, at the Steve Jobs Theater in the Apple Park campus. The phone was released on November 3, 2017, marking the iPhone series' tenth anniversary.\n\nThe iPhone X is intended to showcase what Apple considers technology of the future, specifically adopting OLED screen technology for the first time in iPhone history, as well as using a glass and stainless-steel form factor, offering wireless charging, and removing the home button in favor of introducing a new bezel-less design, almost removing all the bezels in the smartphone and not having a \"chin\", unlike many Android phones. It also released a new type of password authentication called Face ID. Face ID is a new authentication method using advanced technologies to scan the user's face to unlock the device, as well as for the use of animated emojis called Animoji. The new, nearly bezel-less form factor marks a significant change to the iPhone user interaction, involving swipe-based gestures to navigate around the operating system rather than the typical home button used in every previous iteration of the iPhone lineup. At the time of its November 2017 launch, its price tag of US$999 also made it the most expensive iPhone ever, with even higher prices internationally due to additional local sales and import taxes.\n\nThe iPhone X received mixed reviews. Its display and build quality were universally praised, and the camera also scored positively on tests. The phone received particularly polarized reception due to the sensor housing \"notch\" at the top of the screen and the introduction of an all-new authentication method. The notch was heavily mocked by users on social media, although app developers responded either neutrally or positively to the changes it brought to the user experience in their apps and games. Reviewers had mixed reactions, with some condemning it and others acknowledging it as unusual in the first moments of use before getting accustomed to its presence. Face ID facial recognition was praised for its simple setup, but criticized for requiring direct eyes on the screen, though that option can be disabled within the system preferences.\n\nAlong with the iPhone 6s, its Plus variant, and the iPhone SE, the iPhone X was discontinued on September 12, 2018 following the announcement of the new iPhone XS, iPhone XS Max and iPhone XR devices. As a result, with a shelf life of just over 10 months, the iPhone X had the shortest ever tenure as the flagship device in the history of the iPhone.\n\nOn November 22, 2018 Apple resumed production of the iPhone X in some countries due to low sales of it successors, the iPhone X will not be available to buy on the Apple website and remains discontinued in the US, UK etc.\n\nThe technology behind the iPhone X was in development for five years, as far back as 2012. Rumors of a drastic iPhone redesign began circulating around the time of iPhone 7 announcement in the third quarter of 2016, and intensified when a HomePod firmware leak in July 2017 suggested that Apple would shortly release a phone with a nearly bezel-less design, lack of physical home button, facial recognition, and other new features. A near-final development version of the iOS 11 operating system was also leaked in September 2017, confirming the new design and features.\n\nOn August 31, 2017, Apple invited journalists to a September 12 press event, the first public event held at the Steve Jobs Theater on the company's new Apple Park campus in Cupertino, California. The iPhone X was unveiled during that keynote. Its US$999 starting price is the most expensive iPhone launch price. The price is even higher in international markets due to currency fluctuations, import fees and sales taxes.\n\nAn unlocked version of the phone was made available for purchase in the United States on December 5, 2017.\n\nIn April 2018, the Federal Communications Commission divulged images of an unreleased gold-colored iPhone X model. As opposed to the space gray and silver color options that the iPhone X ships with, it was divulged that there were initial plans to release a gold option for the device. However, it was put on hold due to production issues.\n\nApple did release a new revision iPhone X, the B model that fixed NFC issues for users in Japan, China, and some in America. \n\nThe iPhone X has a 5.8-inch OLED color-accurate screen that supports DCI-P3 wide color gamut, sRGB, and high dynamic range, and has a contrast ratio of 1,000,000:1. The Super Retina display has the True Tone technology found on the iPad Pro, which uses ambient light sensors to adapt the display's white balance to the surrounding ambient light. Although the iPhone X does not feature the same \"ProMotion\" technology used in the displays of the second-generation iPad Pro, where the display delivers a refresh rate of 120Hz, it does sample touch input at 120Hz. OLED screen technology has a known negative trend of \"burn-in\" effects, in which particular elements consistently on the screen for long periods of time leave a faint trace even after new images appear. Apple acknowledged that its OLED screens were not excluded from this issue, writing in a support document that \"This is also expected behavior\". Greg Joswiak, Apple's vice president of product marketing, told \"Tom's Guide\" that the OLED panels Apple used in the iPhone X had been engineered to avoid the \"oversaturation\" of colors that using OLED panels typically results in, having made color adjustments and \"subpixel\"-level refinements for crisp lines and round corners. For out-of-warranty servicing for damages not relating to manufacturing defects, screen repairs of iPhone X cost US$279, while other damage repairs cost US$549.\n\nThe iPhone X has two color options; silver and space gray. The front and back of the phone are composed of surgical-grade stainless steel to improve durability, and the device has a glass casing. The design is intended to be IP67 water and dust resistant.\n\nThe iPhone X contains Apple's A11 Bionic system-on-chip, also used in the iPhone 8 and 8 Plus, which is a six-core processor with two cores optimized for performance (25% faster than the A10 Fusion processor), along with four cores optimized for efficiency (70% faster than the previous generation). It also features the first Apple-designed graphics processing unit and a Neural Engine, which powers an artificial intelligence accelerator.\n\nFace ID replaces the Touch ID authentication system. The facial recognition sensor consists of two parts: a \"Romeo\" module that projects more than 30,000 infrared dots onto the user's face, and a \"Juliet\" module that reads the pattern. The pattern is sent to the Secure Enclave in the A11 Bionic chip to confirm a match with the phone owner's face. By default, the system will not work with eyes closed, in an effort to prevent unauthorized access but this requirement can be disabled in settings.\n\nThe iPhone X has two cameras on the rear. One is a 12-megapixel wide-angle camera with f/1.8 aperture, with support for face detection, high dynamic range and optical image stabilization. It is capable of capturing 4K video at 24, 30 or 60 frames per second, or 1080p video at 30, 60, 120 or 240 frames per second. A secondary, telephoto lens features 2× optical zoom and 10× digital zoom with an aperture of f/2.4 and optical image stabilization. A Portrait Mode is capable of producing photos with specific depth-of-field and lighting effects. It also has a quad-LED True Tone flash with 2× better light uniformity.\n\nOn the front of the phone, a 7-megapixel TrueDepth camera has an f/2.2 aperture, and features face detection and HDR. It can capture 1080p video at 30 frames per second, 720p video at 240 frames per second, and exclusively allows for the use of Animoji; animated emojis placed on top of the user's face that intelligently react to the user's facial expressions.\n\niPhone X also supports Qi-standard wireless charging. In tests conducted by \"MacRumors\", the iPhone X's charging speed varies significantly depending on what types of cables, powerbanks, adapters, or wireless chargers are used.\n\nDue to its different screen layout, iOS developers are required to update their apps to make full use of the additional screen real estate. Such changes include rounded corners, sensor \"notch\" at the top of the screen, and an indicator area at the bottom for accessing the home screen. Apple published a \"Human Interface Guidelines\" document to explain areas of focus, and discouraged developers from attempting to mask or call special attention to any of the new changes. Additionally, text within the app needs to be configured to properly reference Face ID rather than Touch ID where the authentication technology is used on iPhone X. In anticipation of the release of the phone, most major apps were quickly updated to support the new changes brought by iPhone X, though the required changes did cause delayed app updates for some major apps.\n\nThe traditional home button, found on all previous devices in the iPhone lineup, has been removed entirely, replaced by touch-based gestures. To wake up the device, users can tap the display or use the side button; to access the home screen, users must swipe up from the bottom of the display; and to access the multitasking window, users must swipe up similarly to the method of accessing the home screen, but stop while the finger is in the middle of the screen, causing an app carousel to appear.\n\niPhone X's rear camera received an overall rating of 97 from DxOMark, a camera testing company, short of the highest score of 99, awarded to Samsung's Galaxy S9+ smartphone. Google's Pixel 2 received a rating of 98. Consumer Reports, a non-profit, independent organization aiming to write impartial reviews of consumer products, ranked iPhone X below iPhone 8 and iPhone 8 Plus, as well as below Samsung's Galaxy S8, S8+ and Note 8, due to less durability and shorter battery life, although it praised the X's camera as \"the highest-rated smartphone camera\" it had ever tested.\n\nNilay Patel of \"The Verge\" praised the display, calling it \"polished and tight and clean\" and \"bright and colorful\". He criticized the repeated lack of a headphone jack, the device's fragility despite Apple's claims of durability, and the sensor notch, calling it \"ugly\". Patel highlighted the fact that apps required updates to fit the new screen, writing that not all popular apps had received updates by the time of the review, resulting in some apps with \"huge black borders\" resembling iPhone 8. He especially criticized the positioning of the sensor notch while holding the phone in landscape mode, causing the notch to go \"from being a somewhat forgettable element in the top status bar to a giant interruption on the side of the screen\". The cameras were given positive feedback for maintaining detail in low-light. Patel particularly praised Animoji, calling it \"probably the single best feature on the iPhone X\", writing that \"they just work, and they work incredibly well\". Finally, he wrote that Face ID was the whole foundation of iPhone X, and stated that it \"generally works great\", though acknowledging the occasional misstep, in which users must \"actively move the phone closer to your face to compensate\". He specifically criticized the limited range of Face ID, with authentication only working when holding the phone 25–50 centimeters away from the face.\n\nChris Velazco of \"Engadget\" also praised the display, writing that, in his experience, the sensor \"notch\" goes from being \"weird at first\" to not being noticeable due to action in videos usually happening in the center. The build quality was given particular acclaim, being called \"a beautifully made device\" with a construction that \"seamlessly\" connects the front and back glass with the stainless-steel frame. Velazco noted that the new gesture-based interaction takes time getting used to, particularly the Control Center being moved from the bottom to the top right of the display. The camera, processor performance, and battery life were also given positive thoughts.\n\nIn a heavily negative review, Dennis Green of \"Business Insider\" significantly criticized the impossible one-handed use of iPhone X, writing that the new gestures to use the phone, such as swiping from the top down to access notifications and the Control Center, did not work when using the phone with only one hand due to not being able to reach the top. His review sparked outrage among Twitter users, many of whom used condescending tones, which Green reasoned as \"I don't know whether the anger was directed toward me out of loyalty to Apple or to justify their own choice to spend $1,000 on a phone. It was obvious that much of the criticism came from people who had never used the phone\".\n\n\"Macworld\"s Roman Loyola praised the Face ID authentication system, writing that the setup process was \"easy\" and that its system integration was \"more seamless\" than the Touch ID fingerprint authentication of the past. That said, Loyola did note the \"half-second\" slower unlocking time than Touch ID as well as needing to look directly at the screen, making it impossible to unlock with the phone next to the user on a desk.\n\nFace ID has raised concerns regarding the possibility of law enforcement accessing an individual's phone by pointing the device at the user's face. United States Senator Al Franken asked Apple to provide more information on the security and privacy of Face ID a day after the announcement, with Apple responding by highlighting the recent publication of a security white paper and knowledge base detailing answers.\n\nInconsistent results have been shown when testing Face ID on identical twins, with some tests showing the system managing to separate the two, while other tests have failed.\n\nHowever, despite Apple's promise of increased security of Face ID compared to the Touch ID fingerprint authentication system, there have been multiple media reports indicating otherwise. \"The Verge\" noted that courts in the United States have granted different Fifth Amendment rights in the United States Constitution to biometric unlocking systems as opposed to keycodes. Keycodes are considered \"testimonial\" evidence based on the contents of users' thoughts, whereas fingerprints are considered physical evidence, with some suspects having been ordered to unlock their phones via fingerprint. Many attempts to break through Face ID with sophisticated masks have been attempted, though most have failed. A week after iPhone X was released, Vietnamese security firm Bkav announced in a blog post that it had successfully created a $150 mask that tricked Face ID, though \"WIRED\" noted that Bkav's technique was more of a \"proof-of-concept\" rather than active exploitation risk, with the technique requiring a detailed measurement or digital scan of the iPhone owner's face, putting the real risk of danger only to targets of espionage and world leaders.\n\nAdditionally, \"Reuters\" reported in early November 2017 that Apple would share certain facial data on users with third-party app developers for more precise selfie filters and for fictional game characters to mirror real-world user facial expressions. Although developers are required to seek customer permission, are not allowed to sell the data to others nor create profiles on users nor use the data for advertising, and are limited to a more \"rough map\" rather than full capabilities, they still get access to over 50 kinds of facial expressions. The American Civil Liberties Union (ACLU) and the Center for Democracy and Technology raised privacy questions about Apple's enforcement of the privacy restrictions connected to third-party access, with Apple maintaining that its App Store review processes were effective safeguards. The \"rough map\" of facial data third-parties can access is also not enough to unlock the device, according to \"Reuters\". However, the overall idea of letting developers access sensitive facial information was still not satisfactorily handled, according to Jay Stanley, a senior policy analyst with the ACLU, with Stanley telling \"Reuters\" that \"the privacy issues around of the use of very sophisticated facial recognition technology for unlocking the phone have been overblown. … The real privacy issues have to do with the access by third-party developers\".\n\nMuch of the debate about the iPhone X has revolved around the design of the sensor housing, dubbed \"notch\" by the media, at the top of the display. \"The Outline\" described it as \"a visually disgusting element\", and \"The Verge\" posted a report focusing on public criticism and people mocking Apple's \"odd design choice\", but not every reviewer was equally negative in their opinions. Third-party iOS developers interviewed by \"Ars Technica\" said that, despite the work of restructuring design elements in their apps, the notch did not cause any problems, with some even arguing that the notch was a good push to simplify their designs. Just two weeks after iPhone X's release, Apple approved a \"notch remover\" app through the App Store, that places black bars across the top of the home screen to make the notch visually disappear. The approval was done despite the company's user interface guidelines discouraging developers from specifically masking the design. It should be noted however that iPhone X was not the first device with a notch - both the Essential Phone and Sharp Aquos S2 were announced before it and had a display notch - but iPhone X arguably popularized it.\n\nIn November 2017, early adopters of the new phone reported that they were experiencing activation issues on certain cellular carriers, most notably AT&T. AT&T announced within hours that the issue had been fixed on their end, and a spokesperson for the Verizon carrier told the media none of its customers were affected despite some reports of problems.\n\nIn November 2017, iPhone X users reported on Reddit that the device's screen would become unresponsive after experiencing rapid temperature drops. Apple released the iOS 11.1.2 update on November 16, 2017, fixing the issue.\n\nForbes contributor Gordon Kelly reported in March 2018 that over 1,000 users experienced problems using camera flash in cold weather, with the problem still persisting as of iOS 11.3 beta 1.\n\nApple has been engaged in a legal battle with Qualcomm over allegedly anti-competitive practices and has been dual-sourcing cellular modem chips to reduce reliance on the semiconductor manufacturer. Starting with iPhone 7 in 2016, Apple has used about half Qualcomm modem chips and half Intel. Professional measurement tests performed by wireless signal testing firm \"Cellular Insights\" indicated that, as in the previous-gen iPhone 7, Qualcomm's chips outperform Intel's in LTE download speeds, up to 67% faster in very weak signal conditions, resulting in some sources recommending the purchase of an unlocked iPhone X or one bought through cellular carrier Verizon, in order to get the models featuring the faster Qualcomm modem. Additionally, \"CNET\" reported in September 2017 that the new iPhone models, including X, 8 and 8 Plus, do not have the ability to connect to the next-generation of wireless LTE data connection, despite 10 new Android devices, including flagships from main smartphone competitor Samsung, all having the capability to do so. While Apple's new smartphones have support for \"LTE Advanced\", with a theoretical peak speed of 500 megabits per second, the Android models have the ability to connect to \"Gigabit LTE\", allowing theoretical speeds up to 1 gigabit per second, doubling Apple's speed.\n\nAfter releasing the iPhone X in Japan and China, users who had Suica payment for transit and in China the Express Transit Cards had issues with NFC reading properly. The iPhone 8 did not see this similar problem and only affected the iPhone X models. April 2018, Apple released a revision to the X, Rev B that included a vastly improved NFC chip that solved the problem of NFC reader errors and double reads on transit gates or store readers on a regular basis: on average 1 out of 3 NFC attempts would error after initial reports. This affected users in America as well. \n\n\n"}
{"id": "26591046", "url": "https://en.wikipedia.org/wiki?curid=26591046", "title": "Improved Launch Control System", "text": "Improved Launch Control System\n\nThe Improved Launch Control System (ILCS) was a system used by the United States Air Force's Minuteman II ICBM force. ILCS was a method to transfer targeting information from a Minuteman Launch Control Center to an individual missile by communications lines. Prior to ILCS, new missile guidance would have to be physically loaded at the launch facility; the process usually took hours.\n\nILCS was operational at most Minuteman II wings (except the 44th Missile Wing, which was never upgraded) by the late 1970s. Minuteman III wings had a similar install, designated Command Data Buffer, providing the newer system the potential for remote retarging.\n\nILCS was phased out in mid-1990s by the retirement of the Minuteman II force, and the inactivation or reapportioning of units to Minuteman III. It was replaced by REACT.\n\n\n"}
{"id": "1124766", "url": "https://en.wikipedia.org/wiki?curid=1124766", "title": "International Medical Informatics Association", "text": "International Medical Informatics Association\n\nThe International Medical Informatics Association (IMIA) is an independent organization that plays a role in promoting and furthering the application of information science in modern society, particularly in the fields of healthcare, bioscience and medicine. It was established in 1967 as a technical committee of the International Federation for Information Processing (IFIP). It became an independent organization in 1987 and was established under Swiss law in 1989.\n\n\nInherent in this mission is to bring together, from a global perspective, scientists, researchers, vendors, consultants and suppliers in an environment of cooperation and sharing. The international membership network of national member societies, IMIA regions, corporate and academic institutional members, and working and special interest groups, constitute the \"IMIA family\".\n\nIMIA organizes various conferences and events around the world and is currently focusing on \"bridging the knowledge gap\" by facilitating and providing support to developing nations. Specific goals include supporting the ongoing development of the African Region.\n\nThe International Medical Informatics Association approved the endorsement of the IMIA Code of Ethics for Health Information Professionals at its General Assembly meeting on October 4, 2002 in Taipei. The code is the culmination of several years of a global collaborative effort led by IMIA's working Group on Data Protection in Health Information, Chaired by Professor Ab Baker.\n\nIn 2016, the General Assembly approved an updated version of the Code of Ethics, which was authored by Dr. Eike-Henner W. Kluge, Professor of the Department of Philosophy at the University of Victoria in Victoria, BC, Canada\n\nIMIA membership consists of National, Institutional and Affiliate Members and Honorary Fellows.\n\nNational Members represent individual countries. A member is a society, a group of societies, or an appropriate body, which is representative of the medical, and health informatics activities within that country. Where no representative societies exist, IMIA accommodates involvement through \"Corresponding\" members within developing countries.\n\nIMIA is governed by the General Assembly composed of representatives of member societies and a Board of Directors.\n\nDr. Christoph U. Lehmann, United States\n\nDr. Hyeoun-Ae Park, South Korea\n\nDr. Sabine Koch, Sweden\n\nDr. Petter Hurlen, Norway\n\nDr. Johanna Westbrook, Australia\n\nDr. Patrick Weber, Switzerland\n\nDr. Michio Kimura, Japan\n\nDr. Brigitte Séroussi, France\n\nDr. Elizabeth Borycki, Canada\n\nDr. Ying (Helen) Wu, China\n\nDr. Kyung-Hee Cho, South Korea\n\nDr. Anne Moen, Norway\n\nDr. Ghislain Kouematchoua Tchuitcheu, Cameroon\n\nDr. Amado Espinosa, Mexico\n\nDr. Andre Kushniruk, Canada\n\nDr. Antoine Geissbuhler, Switzerland\n\nDr. Hiroshi Takeda, Japan\n\nDr. Michio Kimura, Japan\n\n\n\nThe IMIA family includes a growing number of Working and Special Interest Groups, which consist of individuals who share common interests in a particular focal field. The groups hold Working Conferences on leading edge and timely health and medical informatics issues.\n\nIMIA Working Groups and Special Interest Groups include:\n\n\n\n"}
{"id": "37259051", "url": "https://en.wikipedia.org/wiki?curid=37259051", "title": "Invention of the integrated circuit", "text": "Invention of the integrated circuit\n\nThe idea of integrating electronic circuits into a single device was born when the German physicist and engineer developed and patented the first known integrated transistor amplifier in 1949 and the British radio engineer Geoffrey Dummer proposed to integrate a variety of standard electronic components in a monolithic semiconductor crystal in 1952. A year later, Harwick Johnson filed a patent for a prototype integrated circuit (IC).\n\nThese ideas could not be implemented by the industry in the early 1950s, but a breakthrough came in late 1958. Three people from three U.S. companies solved three fundamental problems that hindered the production of integrated circuits. Jack Kilby of Texas Instruments patented the principle of integration, created the first prototype ICs and commercialized them. Kurt Lehovec of Sprague Electric Company invented a way to electrically isolate components on a semiconductor crystal. Robert Noyce of Fairchild Semiconductor invented a way to connect the IC components (aluminium metallization) and proposed an improved version of insulation based on the planar technology by Jean Hoerni. On September 27, 1960, using the ideas of Noyce and Hoerni, a group of Jay Last's at Fairchild Semiconductor created the first operational semiconductor IC. Texas Instruments, which held the patent for Kilby's invention, started a patent war, which was settled in 1966 by the agreement on cross-licensing.\n\nThere is no consensus on who invented the IC. The American press of the 1960s named four people: Kilby, Lehovec, Noyce and Hoerni; in the 1970s the list was shortened to Kilby and Noyce, and then to Kilby, who was awarded the 2000 Nobel Prize in Physics \"for his part in the invention of the integrated circuit\". In the 2000s, historians Leslie Berlin, Bo Lojek and Arjun Saxena reinstated the idea of multiple IC inventors and revised the contribution of Kilby.\n\nDuring and immediately after World War II a phenomenon named \"the tyranny of numbers\" was noticed, that is, some computational devices reached a level of complexity at which the losses from failures and downtime exceeded the expected benefits. Each Boeing B-29 (put into service in 1944) carried 300–1000 vacuum tubes and tens of thousands of passive components. The number of vacuum tubes reached thousands in advanced computers and more than 17,000 in the ENIAC (1946). Each additional component reduced the reliability of a device and lengthened the troubleshooting time. Traditional electronics reached a deadlock and a further development of electronic devices required reducing the number of their components.\n\nThe invention of the transistor in 1947 led to the expectation of a new technological revolution. Fiction writers and journalists heralded the imminent appearance of \"intelligent machines\" and robotization of all aspects of life. Although transistors did reduce the size and power consumption, they could not solve the problem of reliability of complex electronic devices. On the contrary, dense packing of components in small devices hindered their repair. While the reliability of discrete components was brought to the theoretical limit in the 1950s, there was no improvement in the connections between the components.\n\nEarly developments of the integrated circuit go back to 1949, when the German engineer (Siemens AG) filed a patent for an integrated-circuit-like semiconductor amplifying device showing five transistors on a common substrate in a 3-stage amplifier arrangement with two transistors working \"upside-down\" as impedance converter. Jacobi disclosed small and cheap hearing aids as typical industrial applications of his patent. An immediate commercial use of his patent has not been reported.\n\nOn May 7, 1952, the British radio engineer Geoffrey Dummer formulated the idea of integration in a public speech in Washington:\n\nDummer later became famous as \"the prophet of integrated circuits\", but not as their inventor. In 1956 he produced an IC prototype by growth from the melt, but his work was deemed impractical by the UK Ministry of Defence, because of the high cost and inferior parameters of the IC compared to discrete devices.\n\nIn May 1952, Sidney Darlington filed a patent application in the United States for a structure with two or three transistors integrated onto a single chip in various configurations; in October 1952, Bernard Oliver filed a patent application for a method of manufacturing three electrically connected planar transistors on one semiconductor crystal. On May 21, 1953, Harwick Johnson filed a patent application for a method of forming various electronic components – transistors, resistors, lumped and distributed capacitances – on a single chip. Johnson described three ways of producing an integrated one-transistor oscillator. All of them used a narrow strip of a semiconductor with a bipolar transistor on one end and differed in the methods of producing the transistor. The strip acted as a series of resistors; the lumped capacitors were formed by fusion whereas inverse-biased p-n junctions acted as distributed capacitors. Johnson did not offer a technological procedure, and it is not known whether he produced an actual device. In 1959, a variant of his proposal was implemented and patented by Jack Kilby.\n\nThe leading US electronics companies (Bell Labs, IBM, RCA and General Electric) sought solution to \"the tyranny of numbers\" in the development of discrete components that implemented a given function with a minimum number of attached passive elements. During the vacuum tube era, this approach allowed to reduce the cost of a circuit at the expense of its operation frequency. For example, a memory cell of the 1940s consisted of two triodes and a dozen passive components and ran at frequencies up to 200 kHz. A MHz response could be achieved with two pentodes and six diodes per cell. This cell could be replaced by one thyratron with a load resistor and an input capacitor, but the operating frequency of such circuit did not exceed a few kHz.\n\nIn 1952, Jewell James Ebers from Bell Labs developed a prototype solid-state analog of thyratron – a four-layer transistor, or thyristor. William Shockley simplified its design to a two-terminal \"four-layer diode\" (Shockley diode) and attempted its industrial production. Shockley hoped that the new device will replace the polarized relay in telephone exchanges; however, the reliability of Shockley diodes was unacceptably low, and his company went into decline.\n\nAt the same time, works on thyristor circuits were carried at Bell Labs, IBM and RCA. Ian Munro Ross and David D'Azaro (Bell Labs) experimented with thyristor-based memory cells. Joe Logue and Rick Dill (IBM) were building counters using monojunction transistors. Torkel Wallmark and Harwick Johnson (RCA) used both the thyristors and field-effect transistors. The works of 1955–1958 that used germanium thyristors were fruitless. Only in the summer of 1959, after the inventions of Kilby, Lehovec and Hoerni became publicly known, D'Azaro reported an operational shift register based on silicon thyristors. In this register, one crystal containing four thyristors replaced eight transistors, 26 diodes and 27 resistors. The area of each thyristor ranged from 0.2 to 0.4 mm, with a thickness of about 0.1 mm. The circuit elements were isolated by etching deep grooves.\n\nFrom the point of view of supporters of functional electronics, semiconductor era, their approach was allowed to circumvent the fundamental problems of semiconductor technology. The failures of Shockley, Ross and Wallmark proved the fallacy of this approach: the mass production of functional devices was hindered by technological barriers.\n\nEarly transistors were made of germanium. By the mid-1950s it was replaced by silicon which could operate at higher temperatures. In 1954, Gordon Kidd Teal from Texas Instruments produced the first silicon transistor, which became commercial in 1955. Also in 1954, Fuller and Dittsenberger published a fundamental study of diffusion in silicon, and Shockley suggested using this technology to form p-n junctions with a given profile of the impurity concentration.\n\nIn early 1955, Carl Frosch from Bell Labs developed wet oxidation of silicon, and in the next two years, Frosch, Moll, Fuller and Holonyak brought it to the mass production. This accidental discovery revealed the second fundamental advantage of silicon over germanium: contrary to germanium oxides, \"wet\" silica is a physically strong and chemically inert electrical insulator.\n\nOn December 1, 1957, Jean Hoerni first proposed a planar technology of bipolar transistors. In this process, all the p-n junctions were covered by a protective layer, which should significantly improve reliability. However, in 1957, this proposal was considered technically impossible. The formation of the emitter of an n-p-n transistor required diffusion of phosphorus, and the work of Frosch suggested that SiO does not block such diffusion. In March 1959, Chi-Tang Sah, a former colleague of Hoerni, pointed Hoerni and Noyce to an error in the conclusions of Frosch. Frosch used a thin oxide layer, whereas the experiments of 1957–1958 showed that a thick layer of oxide can stop the phosphorus diffusion. Armed with this knowledge, by March 12, 1959 Hoerni made the first prototype of a planar transistor, and on May 1, 1959 filed a patent application for the invention of the planar process. In April 1960, Fairchild launched the planar transistor 2N1613, and by October 1960 completely abandoned the mesa transistor technology. By the mid-1960s, the planar process has become the main technology of producing transistors and monolithic integrated circuits.\n\nThe creation of the integrated circuit was hindered by three fundamental problems, which were formulated by Wallmark in 1958:\n\n\nIt happened so that three different companies held the key patents to each of these problems. Sprague Electric Company decided not to develop ICs, Texas Instruments limited itself to an incomplete set of technologies, and only Fairchild Semiconductor combined all the techniques required for a commercial production of monolithic ICs.\n\nIn May 1958, Jack Kilby, an experienced radio engineer and a veteran of World War II, started working at Texas Instruments. At first, he had no specific tasks and had to find himself a suitable topic in the general direction of \"miniaturization\". He had a chance of either finding a radically new research direction or blend into a multimillion-dollar project on the production of military circuits. In the summer of 1958, Kilby formulated three features of integration:\n\n\nOn August 28, 1958, Kilby assembled the first prototype of an IC using discrete components and received approval for implementing it on one chip. He had access to technologies that could form mesa transistors, mesa diodes and capacitors based on p-n junctions on a germanium (but not silicon) chip, and the bulk material of the chip could be used for resistors. The standard Texas Instruments chip for the production of 25 (5×5) mesa transistors was 10×10 mm in size. Kilby cut it into five-transistor 10×1.6 mm strips, but later used not more than two of them. On September 12, he presented the first IC prototype, which was a single-transistor oscillator with a distributed RC feedback, repeating the idea and the circuit in the 1953 patent by Johnson. On September 19, he made the second prototype, a two-transistor trigger. He described these ICs, referencing the Johnson's patent, in his .\n\nBetween February and May 1959 Kilby filed a series of applications: , , , and . According to Arjun Saxena, the application date for the key patent 3,138,743 is uncertain: while the patent and the book by Kilby set it to February 6, 1959, it could not be confirmed by the application archives of the federal patent office. He suggested that the initial application was filed on February 6 and lost, and the (preserved) resubmission was received by the patent office on 6 May 1959 – the same date as the applications for the patents 3,072,832 and 3,138,744. Texas Instruments introduced the inventions by Kilby to the public on March 6, 1959.\n\nNone of these patents solved the problem of isolation and interconnection – the components were separated by cutting grooves on the chip and connected by gold wires. Thus these ICs were of the hybrid rather than monolithic type. However, Kilby demonstrated that various circuit elements: active components, resistors, capacitors and even small inductances can be formed on one chip.\n\nIn autumn 1958, Texas Instruments introduced the yet non-patented idea of Kilby to military customers. While most divisions rejected it as unfit to the existing concepts, the US Air Force decided that this technology complies with their molecular electronics program, and ordered production of prototype ICs, which Kilby named \"functional electronic blocks\". Westinghouse added epitaxy to the Texas Instruments technology and received a separate order from the US military in January 1960.\n\nIn October 1961, Texas Instruments built for the Air Force a demonstration \"molecular computer\" with a 300-bit memory based on the #587 ICs of Kilby. Harvey Kreygon packed this computer into a volume of a little over 100 cm. In December 1961, the Air Force accepted the first analog device created within the molecular electronics program – a radio receiver. It uses costly ICs, which had less than 10–12 components and a high percentage of failed devices. This generated an opinion that ICs can only justify themselves for aerospace applications. However, the aerospace industry rejected those ICs for the low radiation hardness of their mesa transistors.\n\nIn April 1960, Texas Instruments announced multivibrator #502 as the world's first integrated circuit available on the market. The company assured that contrary to the competitors they actually sell their product, at a price of US$450 per unit or US$300 for quantities larger than 100 units. However, the sales began only in the summer of 1961, and the price was higher than announced. contained two transistors, four diodes, six resistors and two capacitors, and repeated the traditional discrete circuitry. The device contained two Si strips of 5 mm length inside a metal-ceramic housing. One strip contained input capacitors; the other accommodated mesa transistors and diodes, and its grooved body was used as six resistors. Gold wires acted as interconnections.\n\nIn late 1958, Kurt Lehovec, a scientist working at the Sprague Electric Company, attended a seminar at Princeton where Wallmark outlined his vision of the fundamental problems in microelectronics. On his way back to Massachusetts, Lehovec found a simple solution to the isolation problem which used the p-n junction:\n\nLehovec tested his idea using the technologies of making transistors that were available at Sprague. His device was a linear structure 2.2×0.5×0.1 mm in size, which was divided into isolated n-type cells (bases of the future transistors) by p-n junctions. Layers and transitions were formed by growth from the melt. The conductivity type was determined by the pulling speed of the crystal: an indium-rich p-type layer was formed at a slow speed, whereas an arsenic-rich n-type layer was produced at a high speed. The collectors and emitters of the transistors were created by welding indium beads. All electrical connections were made by hand, using gold wires.\n\nThe management of Sprague showed no interest to the invention by Lehovec. Nevertheless, on April 22, 1959 he filed a patent application at his own expense, and then left the United States for two years. Because of this disengagement, Gordon Moore concluded that Lehovec should not be considered as an inventor of the integrated circuit.\n\nOn January 14, 1959, Jean Hoerni introduced his latest version of the planar process to Robert Noyce and a patent attorney John Rallza at Fairchild Semiconductor. A memo of this event by Hoerni was the basis of a patent application for the invention of a planar process, filed in May 1959, and implemented in (the planar process) and (the planar transistor). On January 20, 1959, Fairchild managers met with Edward Keonjian, the developer of the onboard computer for the rocket \"Atlas\", to discuss the joint development of hybrid digital ICs for his computer. These events probably led Robert Noyce to return to the idea of integration.\n\nOn January 23, 1959, Noyce documented his vision of the planar integrated circuit, essentially re-inventing the ideas of Kilby and Lehovec on the base of the Hoerni's planar process. Noyce claimed in 1976 that in January 1959 he did not know about the work of Lehovec.\n\nAs an example, Noyce described an integrator that he discussed with Keonjian. Transistors, diodes and resistors of that hypothetical device were isolated from each other by p-n junctions, but in a different manner from the solution by Lehovec. Noyce considered the IC manufacturing process as follows. It should start with a chip of highly resistive intrinsic (undoped) silicon passivated with an oxide layer. The first photolithography step aims to open windows corresponding to the planned devices, and diffuse impurities to create low-resistance \"wells\" through the entire thickness of the chip. Then traditional planar devices are formed inside those wells. Contrary to the solution by Lehovec, this approach created two-dimensional structures and fit a potentially unlimited number of devices on a chip.\n\nAfter formulating his idea, Noyce shelved it for several months due to pressing company matters, and returned to it only by March 1959. It took him six months to prepare a patent application, which was then rejected by the US Patent Office because they already received the application by Lehovec. Noyce revised his application and in 1964 received and .\n\nIn early 1959, Noyce solved another important problem, the problem of interconnections that hindered mass-production of ICs. According to the colleagues from the traitorous eight his idea was self-evident: of course, the passivating oxide layer forms a natural barrier between the chip and the metallization layer. According to Turner Hasty, who worked with Kilby and Noyce, Noyce planned to make the microelectronic patents of Fairchild accessible to a wide range of companies, similar to Bell Labs which in 1951–1952 released their transistor technologies.\n\nNoyce submitted his application on July 30, 1959, and on April 25, 1961 received . According to the patent, the invention consisted of preserving the oxide layer, which separated the metallization layer from the chip (except for the contact window areas), and of depositing the metal layer so that it is firmly attached to the oxide. The deposition method was not yet known, and the proposals by Noyce included vacuum deposition of aluminium through a mask and deposition of a continuous layer, followed by photolithography and etching off the excess metal. According to Saxena, the patent by Noyce, with all its drawbacks, accurately reflects the fundamentals of the modern IC technologies.\n\nIn his patent, Kilby also mentions the use of metallization layer. However, Kilby favored thick coating layers of different metals (aluminium, copper or antimony-doped gold) and silicon monoxide instead of the dioxide. These ideas were not adopted in the production of ICs.\n\nIn August 1959, Noyce formed at Fairchild a group to develop integrated circuits. On May 26, 1960, this group, led by Jay Last, produced the first planar integrated circuit. This prototype was not monolithic – two pairs of its transistors were isolated by cutting a groove on the chip, according to the patent by Last. The initial production stages repeated the Hoerni's planar process. Then the 80-micron-thick crystal was glued, face down, to the glass substrate, and additional photolithography was carried on the back surface. Deep etching created a groove down to the front surface. Then the back surface was covered with an epoxy resin, and the chip was separated from the glass substrate.\n\nIn August 1960, Last started working on the second prototype, using the isolation by p-n junction proposed by Noyce. Robert Norman developed a trigger circuit on four transistors and five resistors, whereas Isy Haas and Lionel Kattner developed the process of boron diffusion to form the insulating regions. The first operational device was tested in September 27, 1960 – this was the first planar and monolithic integrated circuit.\n\nFairchild Semiconductor did not realize the importance of this work. Vice president of marketing believed that Last was wasting the company resources and that the project should be terminated. In January 1961, Last, Hoerni and their colleagues from the \"traitorous eight\" Kleiner and Roberts left Fairchild and headed Amelco. David Allison, Lionel Kattner and some other technologists left Fairchild to establish a direct competitor, the company Signetics.\n\nDespite the departure of their leading scientists and engineers, in March 1961 Fairchild announced their first commercial IC series, named \"Micrologic\", and then spent a year on creating a family of logic ICs. By that time ICs were already produced by their competitors. Texas Instruments abandoned the IC designs by Kilby and received a contract for a series of planar ICs for space satellites, and then for the LGM-30 Minuteman ballistic missiles. Whereas the ICs for the onboard computers of the Apollo spacecraft were designed by Fairchild, most of them were produced by Raytheon and Philco Ford. Each of these computers contained about 5,000 standard logic ICs, and during their manufacture, the price for an IC dropped from US$1,000 to US$20–30. In this way, NASA and the Pentagon prepared the ground for the non-military IC market.\n\nThe resistor-transistor logic of first ICs by Fairchild and Texas Instruments was vulnerable to electromagnetic interference, and therefore in 1964 both companies replaced it by the diode-transistor logic [91]. Signetics released the diode-transistor family Utilogic back in 1962, but fell behind Fairchild and Texas Instruments with the expansion of production. Fairchild was the leader in the number of ICs sold in 1961–1965, but Texas Instruments was ahead in the revenue: 32% of the IC market in 1964 compared to 18% of Fairchild.\n\nThe above logic ICs were built from standard components, with sizes and configurations defined by the technological process, and all the diodes and transistors on one IC were of the same type. The use of different transistor types was first proposed by Tom Long at Sylvania in 1961–1962. In late 1962, Sylvania launched the first family of transistor-transistor logic (TTL) ICs, which became a commercial success. Bob Widlar from Fairchild made a similar breakthrough in 1964–1965 in analog ICs (operational amplifiers).\n\nIn 1959–1961 years, when Texas Instruments and Westinghouse worked in parallel on aviation \"molecular electronics\", their competition had a friendly character. The situation changed in 1962 when Texas Instruments started to zealously pursue the real and imaginary infringers of their patents and received the nicknames \"The Dallas legal firm\" and \"semiconductor cowboys\". This example was followed by some other companies. Nevertheless, the IC industry continued to develop no matter the patent disputes.\n\n\nDuring the patent wars of the 1960s the press and professional community in the United States recognized that the number of the IC inventors could be rather large. The book \"Golden Age of Entrepreneurship\" named four people: Kilby, Lehovec, Noyce and Hoerni. Sorab Ghandhi in \"Theory and Practice of Microelectronics\" (1968) wrote that the patents of Lehovec and Hoerni were the high point of semiconductor technology of the 1950s and opened the way for the mass production of ICs.\n\nIn October 1966, Kilby and Noyce were awarded the Ballantine Medal from the Franklin Institute \"for their significant and essential contribution to the development of integrated circuits\". This event initiated the idea of two inventors. The nomination of Kilby was criticized by contemporaries who did not recognize his prototypes as \"real\" semiconductor ICs. Even more controversial was the nomination of Noyce: the engineering community was well aware of the role of the Moore, Hoerni and other key inventors, whereas Noyce at the time of his invention was CEO of Fairchild and did not participate directly in the creation of the first IC. Noyce himself admitted, \"I was trying to solve a production problem. I wasn't trying to make an integrated circuit\".\n\nAccording to Leslie Berlin, Noyce became the \"father of the integrated circuit\" because of the patent wars. Texas Instruments picked his name because of stood on the patent they challenged and thereby \"appointed\" him as a sole representative of all the development work at Fairchild. In turn, Fairchild mobilized all its resources to protect the company, and thus the priority of Noyce. While Kilby was personally involved in the public relation campaigns of Texas Instruments, Noyce kept away from publicity and was substituted by Gordon Moore.\n\nBy the mid-1970s, the two-inventor version became widely accepted, and the debates between Kilby and Lehovec in professional journals in 1976–1978 did not change the situation. Hoerni, Last and Lehovec were regarded as minor players; they did not represent large corporations and were not keen for public priority debates.\n\nIn scientific articles of the 1980s, the history of IC invention was often presented as follows\n\nIn 1984, the two-inventor version has been further supported by Thomas Reid in \"The Chip: How Two Americans Invented the Microchip and Launched a Revolution\". The book was reprinted up to 2008. Robert Wright of The New York Times criticized Reid for a lengthy description of the supporting characters involved in the invention, yet the contributions of Lehovec and Last were not mentioned, and Jean Hoerni appears in the book only as a theorist who consulted Noyce.\n\nPaul Ceruzzi in \"A History of Modern Computing\" (2003) also repeated the two-inventor story and stipulated that \"Their invention, dubbed at first \"Micrologic,\" then the \"Integrated Circuit\" by Fairchild, was simply another step along this path\" (of miniaturization demanded by the military programs of the 1950s). Referring to the prevailing in the literature opinion, he put forward the decision of Noyce to use the planar process of Hoerni, who paved the way for the mass production of ICs, but was not included in the list of IC inventors. Ceruzzi did not cover the invention of isolation of IC components.\n\nIn 2000, the Nobel Committee awarded the Nobel Prize in Physics to Kilby \"for his part in the invention of the integrated circuit\". Noyce died in 1990 and thus could not be nominated; when asked during his life about the prospects of the Nobel Prize he replied \"They don't give Nobel Prizes for engineering or real work\". Because of the confidentiality of the Nobel nomination procedure, it is not known whether other IC inventors had been considered. Saxena argued that the contribution of Kilby was pure engineering rather than basic science, and thus his nomination violated the will of Alfred Nobel.\n\nThe two-inventor version persisted through the 2010s. Its variation puts Kilby in front, and considers Noyce as an engineer who improved the Kilby's invention. Fred Kaplan in his popular book \"1959: The Year Everything Changed\" (2010) spends eight pages on the IC invention and assigns it to Kilby, mentioning Noyce only in a footnote and neglecting Hoerni and Last.\n\nIn the late 1990s and 2000s a series of books presented the IC invention beyond the simplified two-person story. In 1998, Michael Riordan and Lillian Hoddson described in detail the events leading to the invention of Kilby in their book \"Crystal Fire: The Birth of the Information Age\". However, they stopped on that invention. Leslie Berlin in her biography of Robert Noyce (2005) included the events unfolding at Fairchild and critically evaluated the contribution of Kilby. \"Connecting wire precluded production and Kilby could not know that.\n\nIn 2007, Bo Lojek opposed the two-inventor version; he described the contribution of Hoerni and Last and criticized Kilby.\n\nIn 2009, Saxena described the work of Dummer, Johnson, Stewart, Kilby, Noyce, Lehovec and Hoerni. He also played down the role of Kilby and Noyce.\n\n\n\n"}
{"id": "30556913", "url": "https://en.wikipedia.org/wiki?curid=30556913", "title": "IpTTY", "text": "IpTTY\n\nipTTY is a SIP soft phone that supports the conversion of Baudot Tones sent via IP to text. In other words, ipTTY facilitates a TTY/TDD conversation over IP without the need for modems or analog lines. ipTTY connects directly via IP to PBX and VoIP systems that support SIP. This type of communication is necessary for individuals with hearing impairments. Additionally, both the ADA (Americans with Disabilities Act) and Section 508 require organizations to support TTY for individuals if readily achievable. ipTTY, provided the PBX/VoIP system supports SIP, makes this requirement readily achievable.\n\nipTTY also supports RFC 4103 or what some commonly refer to as Text over IP.\n\n"}
{"id": "26954124", "url": "https://en.wikipedia.org/wiki?curid=26954124", "title": "Journal of Renewable and Sustainable Energy", "text": "Journal of Renewable and Sustainable Energy\n\nThe Journal of Renewable and Sustainable Energy is a free and rapid publishing peer-reviewed, online-only, open access, scientific journal published by the American Institute of Physics covering all areas of renewable and sustainable energy-related fields that apply to the physical science and engineering communities. Online submissions are posted daily and organized into bimonthly issues. The journal was established in 2009.The Co-Editors-in-Chief are P. Craig Taylor (Colorado School of Mines) and John A. Turner (National Renewable Energy Laboratory).\n\nThe Journal's 2017 impact factor was 1.337.\n"}
{"id": "21382561", "url": "https://en.wikipedia.org/wiki?curid=21382561", "title": "Knewton", "text": "Knewton\n\nKnewton is an adaptive learning company that has developed a platform to personalize educational content as well as has developed courseware for higher education concentrated in the fields of science, technology, engineering, and mathematics. The company was founded in 2008 by Jose Ferreira, a former executive at Kaplan, Inc. The Knewton platform allows schools, publishers, and developers to provide adaptive learning for any student. In 2011, Knewton announced a partnership with Pearson Education to enhance the company's digital content, including the MyLab and Mastering series. Additional partners announced include Houghton Mifflin Harcourt, Macmillan Education, Triumph Learning, and over a dozen others.\n\nKnewton's headquarters are at 440 Park Avenue South in Manhattan, New York City. The company also has an office in Tech City, London.\n\nKnewton is an adaptive learning technology provider that makes it possible for others to build adaptive learning applications. In 2016, the company also began developing courseware for higher education classes using content from educational companies and open educational resources. Knewton technology enables the company to perform \"sophisticated, real-time analysis of reams of student performance data.\" Knewton uses adaptive learning technology to identify each student's particular strengths and weaknesses. Concepts are tagged at very specific levels, which allows the platform to make custom recommendations based on students’ proficiency and needs. The company first launched with a GMAT preparation course, which has now been discontinued.\n\nIn 1995, researchers now working for Knewton proved that the small question pool available to the Graduate Record Examination (GRE) computer-adaptive test made it vulnerable to cheating.\n\nIn January 2011, Arizona State University began running developmental math and blended learning courses using Knewton's adaptive technology.\n\"The portion of students withdrawing from the courses fell from 13% to 6%, and pass rates rose from 66% to 75%\".\n\nIn its first round of funding, Knewton raised $2.5 million in investment capital from Accel Partners, Reid Hoffman, Ron Conway, and Josh Kopelman at First Round Capital. In April 2009 Knewton closed a $6 million round of funding led by Bessemer Venture Partners with returning investors, and in April 2010 Knewton closed a $12.5 million round of funding led by FirstMark Capital with returning investors. In October 2011 the company closed a $33 million series D round of funding led by the Founders Fund. In December 2013 the company closed a $51 million series E round of funding led by Atomico, joined by GSV Capital and returning investors. In February 2016, Knewton closed a series F $52 million round, its largest to date, led by Sofina and Atomico.\n\n"}
{"id": "12131374", "url": "https://en.wikipedia.org/wiki?curid=12131374", "title": "Litostroj", "text": "Litostroj\n\nLitostroj is a Slovenian heavy machinery manufacturer based in Ljubljana. Its products include mainly water turbines for hydroelectric powerplants. Most of production is exported. It provided around 8000 MW of installed power worldwide. Around 150 of them are pumped storage. It has also designed around 1200 large cranes. They also manufacture forging and pressing machines. \n\nIts name is derived from , meaning \"Foundry and Machine Factory\" in Slovene. Litostroj Street () in Ljubljana is named after the company.\n\n"}
{"id": "2264727", "url": "https://en.wikipedia.org/wiki?curid=2264727", "title": "Lowrance Electronics", "text": "Lowrance Electronics\n\nLowrance is a manufacturer of consumer sonar and GPS receivers, as well as digital mapping systems. Headquartered in Tulsa, Oklahoma, Lowrance employs approximately 1,000 people. The company is best known for its High Definition Systems (HDS) and add-on performance modules which include Broadband 4G Radar, StructureScan with SideScan and DownScan Imaging, Sonic Hub Audio, Sirius LWX-1 Weather, and NAIS Collision Avoidance.\n\nIn 2006, Simrad Yachting and Lowrence merged in a deal valued at $215 million, creating a new company named Navico. The Lowrance brand is wholly owned by Navico, Inc. A privately held, international corporation, Navico is currently the world’s largest marine electronics company, and is the parent company to leading marine electronics brands: Lowrance, Simrad Yachting and B&G. Navico has approximately 1,500 employees globally and distribution in more than 100 countries worldwide. www.navico.com\n\nLowrance, a designer and manufacturer of Sonar, GPS and Aviation instruments, got its start in Joplin, Missouri in 1957. Carl Lowrance, an avid fisherman with an inquisitive nature, shared his love of the sport with his two sons, Darrell and Arlen. Darrell and Arlen were among the first inland skin divers, and through this activity learned that most fish in any given body of water were generally found in schools and in specific areas.\n\nIt became obvious to the Lowrances that an instrument was needed to show fishermen where to concentrate their fishing efforts. They designed the world's first high frequency transistorized sonar for sport fishing and\nboating with several things in mind. It had to be portable, compact, lightweight, contain its own batteries and be relatively inexpensive. The advent of transistors into consumer electronics products was very important to the unit's concept. In 1957 commercial sonar units were selling for more than $2000. The first Lowrance unit sold for less than $150.\n\nA small manufacturer was selected to produce the first 2,000 units. Unfortunately, the manufacturer's performance was less than satisfactory. In July 1958, a second firm was selected, but again the quality and quantity specified by Lowrance was never achieved.\n\nIn 1959, the family decided the only way to produce the instrument they wanted-with excellent quality and in quantities to satisfy the demand-was to manufacture it themselves. Their first move was to replace, at no charge, over 700 of the previously produced products. \"We didn't have a lot of money,\" Carl said. \"We acquired an old warehouse and literally built a factory from scratch.\" In November 1959 the company introduced the first \"Little Green Box\" portable sonar unit. It became the most popular sonar in the world. More than a million were produced between 1959 and 1984.\n\nIn 1964 the operation moved to Tulsa, Oklahoma, and manufacturing started in a rented building in January 1965. A tremendous amount of work was done by the Lowrance's to promote, sell and advance the technology of their products. Boats grew larger and more powerful. Higher speeds meant new transducer designs. In 1965 Lowrance introduced the first sonars and transducers capable of high-speed performance.\n\nA new building was completed in 1971 and a large addition was added in 1974. As the company grew, it enhanced its reputation by introducing new products to meet the changing marketplace. Gimbal mount sonar units, especially the LFG-300, rapidly sold alongside and eventually replaced the Little Green Box. More and more effort went into research and development.\n\nIn 1974 the first graph recorder produced by the company, the LRG-600, combined a flasher and a paper chart in one box. The first straight-line graph recorder, the LRG-1510, came in 1977. Soon, many boats had more than one sonar unit on board.\n\nIn 1979, Lowrance introduced the LDD-1800, the world's first sonar unit guaranteed NOT to find fish. The LDD-1800 was one of the first digital depth sounders to be controlled by a tiny computer built inside. It would show only the bottom depth in large digital numbers and required no controls. This was Lowrance's first completely automatic depth sounder.\n\nLowrance introduced the System 2000 line of flashers in 1981. The X-15 was introduced in 1982. This computerized chart used a ten-key keypad instead of a range knob. For the first time fishermen could \"dial-up\" virtually any range. Another astonishing feature was its capability to show separate targets that were as little as one-inch apart. No other sonar unit of any kind had these features.\n\nIn 1985, Lowrance unveiled its first Liquid Crystal Graph (LCG), the X-3. Although it had only 83 vertical pixels (more than many of its competitors), it took the market by storm and was the beginning of a new era in Lowrance sonar. Also in 1985 the ultimate paper graph recorder was introduced-the legendary X-16. In 1987 the X-16 was select by a team of scientists and explorers to be the sonar used in Operation Deepscan, an expedition to explore Loch Ness in Scotland.\n\nIn 1988, Lowrance diversified into the navigation market and introduced its first Loran-C products, the LMS-200 and LMS-300. Boasting wide LCD screens, these dual-purpose units had both a Loran-C receiver and sonar capabilities. These were also the first units with menus, making it easy to select features and make adjustments.\n\nLowrance again increased its technological advantage in 1991 with the introduction of its first GPS receivers, incorporating Rockwell GPS technology. In 1995 Lowrance introduced the GlobalMap 2000, the world's first LCD product to offer integrated GPS, mapping and sonar capabilities.\n\nIn 1993, Lowrance expanded manufacturing operations in a facility located in Ensenada, Baja California, Mexico, where it became the main company manufacturing operations center. Additionally, new centers of competence were created in Ensenada, including Research and Development (Software, Hardware, Mechanical, Optics), Finance and Logistics among other key operations.\n\nIn 1996 Lowrance introduced the industry's first 12 channel hand held GPS sold for under $200. Also, in 1996, Lowrance introduced the industry's first hand held aviation GPS with full mapping capability. Concurrent with the introduction of the mapping products, was the introduction of the innovative Lowrance Inland Mapping System (IMS) cartridges, which offered detailed digitized mapping of most U.S. inland lakes and waterways. The IMS cartridge offering was expanded to include IMS SmartMap, IMS WorldMap and aviation cartridges.\n\nIn 1998 a new series of hand held GPS mapping products were introduced, shipped with Lowrance's breakthrough IMS MapCreate CD-ROM which includes all of Lowrance's IMS SmartMaps, IMS WorldMaps, rural roads, coastal navigation aids and wrecks and obstructions data enabling the upload of customizable maps into the units. In 2002 the Lowrance LCX-16ci SONAR/GPS combo was presented the prestigious \"Best of Show\" Award at ICAST 2000.\n\nDuring 2001, the Company announced an entire new family of SONAR and GPS-mapping products in totally new compact case designs. Lowrance announced over 15 new models for 2003...from entry-level fishfinders to mega-screen color combos.\n\nIn 2006, Lowrance was purchased by Simrad Yachting. The agreed price was $215 million. This merger went on to create Navico, now the largest leisure marine electronics manufacturer in the world.\n\nIn the fall of 2008, Lowrance announced a complete overhaul to its line GPS and sonar devices with the introduction of the Lowrance High Definition Systems (HDS). Available in four display sizes (5\", 7\", 8\" and 10\"), the multifuction range marked the first use of the renowned Navico Operating System (NOS), and greatly extended the functionality of the Lowrance line to include radar, AIS, realtime weather, integrated audio and advanced sonar imaging options.\n\nLowrance presently offers the most complete range of sonar and GPS products available from any one manufacturer for nearly every fishing, boating and outdoor need.\n\nLowrance products are protected by a two-year limited warranty, and supported for an additional three years by the Lowrance 5-Year Advantage Program, which allows customers to upgrade to the latest technology, at a discounted price, in the unlikely event that a product fails or needs repair.\n\nLowrance Link-5 VHF 05/01/12\nLowrance Partners with Phoenix Boats 04/27/12 \nLowrance LSS-2 StructureScanHD 03/15/12 \nLowrance Ultimate Upgrade Sales Event 02/22/12\nLowrance Nautic Insight HD and Insight PRO 02/16/12 \nLowrance Lake Insight HD and PRO 2012 02/16/12 \n\n\n"}
{"id": "23394289", "url": "https://en.wikipedia.org/wiki?curid=23394289", "title": "Malaysian National Computer Confederation", "text": "Malaysian National Computer Confederation\n\nThe Malaysian National Computer Confederation (MNCC) is an association for information and communications technology professionals. Its stated aims are \"dedicated to the development of IT Professionals and the creation of an Information Rich Society\".\n\nThe MNCC was formed in 1967, then known as the Malaysian Computer Society. It was renamed to the MNCC, in 1988, when the association was registered under the Companies Act.\n\nNotable activities include: operation of the various special interest groups (SIGs), annual conferences, seminars, as well as IT scholarships.\n\nMembers are required to comply with the code of professional conduct and practice .\n\nSpecial Interest Groups (SIGs) of the MNCC include wide and varied interests such as: the Open Document Format (ODFSIG), Open Source (OSSIG), Artificial Intelligence in Industry (AI3SIG), IP Telephony, Project Management and Storage.\n\nThe MNCC Transactions on ICT contains free journal articles and conference papers.\n\n\n"}
{"id": "1801703", "url": "https://en.wikipedia.org/wiki?curid=1801703", "title": "Microwave power meter", "text": "Microwave power meter\n\nA microwave power meter is an instrument which measures the electrical power at microwave frequencies typically in the range 100 MHz to 40 GHz.\n\nUsually a microwave power meter will consist of a measuring head which contains the actual power sensing element, connected via a cable to the meter proper, which displays the power reading. The head may be referred to as a power sensor or mount. Different power sensors can be used for different frequencies or power levels. Historically the means of operation in most power sensor and meter combinations was that the sensor would convert the microwave power into an analogue voltage which would be read by the meter and converted into a power reading. Several modern power sensor heads contain electronics to create a digital output and can be plugged via USB into a PC which acts as the power meter.\n\nMicrowave power meters have a wide bandwidth—they are not frequency-selective. To measure the power of a specific frequency component in the presence of other signals at different frequencies a spectrum analyzer or measuring receiver is needed.\n\nThere are a variety of different technologies which have been used as the power sensing element. Each has advantages and disadvantages.\n\nThermal sensors can generally be divided into two main categories, thermocouple power sensors and thermistor-based power sensors. Thermal sensors depend on the process of absorbing the RF and microwave signal energy, and sense the resulting heat rise. Therefore, they respond to true average power of the signal, whether it is pulsed, CW, AM/FM or any complex modulation. (Agilent 2008).\nThermocouple power sensors make up the majority of the thermal power sensors sold at present. They are generally reasonably linear and have a reasonably fast response time and dynamic range. The microwave power is absorbed in a load whose temperature rise is measured by the thermocouple. Thermocouple sensors often require a reference DC or microwave power source for calibration before measuring; this can be built into the power meter.\nThermistor-based power sensors such as the Agilent 8478B are generally only used in situations where their excellent linearity is important, as they are both much slower and have a smaller dynamic range than either thermocouple or diode-based sensors. Thermistor-based power sensors are still the sensor of choice for power transfer standards because of their DC power substitution capability (Agilent 2006). Other thermal sensing technologies include microwave calorimeters and bolometers,and quasi-optic pulsed microwave sensors.\n\nMany microwave power heads use one or more diode(s) to rectify the incident microwave power, and have extremely fast response. The diode would generally be used in its square-law region and hence give an output voltage proportional to the incident RF power. In order to extend their dynamic range beyond the square-law region, linearity correction circuits or multiple diode stacks are used. With advancement in comprehensive data compensation algorithm and diode stacks topology, diode sensors like the Agilent E9300A is able to respond properly to complex modulated signals over a wide dynamic range (Agilent, 2006). Like thermocouple sensors, they often require a reference source.\n\nOther technologies have been investigated or implemented for use as power sensors but are not widely used today; these include torque-vane, electron-beam, MEMS, Hall effect and atomic fountain based sensors.\n\nThe two main types of microwave power meters are:\n\nPower meters generally report the power in dBm (decibels relative to 1 milliwatt), dBW (decibels relative to 1 watt) or watts. Manufacturers of microwave power meters include: Aeroflex, Keysight, Anritsu, Bird Technologies, Boonton Electronics, Giga-tronics, Rohde and Schwarz, Tektronix and TEGAM Inc..\n\n\n"}
{"id": "3261944", "url": "https://en.wikipedia.org/wiki?curid=3261944", "title": "Military payment certificate", "text": "Military payment certificate\n\nMilitary payment certificates, or MPC, was a form of currency used to pay U.S. military personnel in certain foreign countries. It was used in one area or another from a few months after the end of World War II until a few months after the end of U.S. participation in the Vietnam War – from 1946 until 1973. MPC utilized layers of line lithography to create colorful banknotes that could be produced cheaply. Fifteen series of MPCs were created. However, only 13 series were issued. The remaining two were largely destroyed, although some examples remain. Among the 13 released series a total of 94 notes are recognized. \n\nMPCs evolved from Allied Military Currency as a response to the large amounts of US Dollars circulated by American servicemen in post-World War II Europe. The local citizens might not trust local currencies, as the future of their governments was unclear. Preferring a stable currency like U.S. dollars, local civilians often accepted payment in dollars for less than the accepted conversion rates. Dollars became more favorable to hold, inflating the local currencies and thwarting plans to stabilize local economies. Contributing to this problem was the fact that troops were being paid in dollars, which they could convert in unlimited amounts to the local currency with merchants at the floating (black market) conversion rate, which was much more favorable to the GIs than the government fixed conversion rate. From this conversion rate imbalance, a black market developed where the servicemen could profit from the more favorable exchange rate. \n\nTo reduce profiteering from currency arbitrage, the U.S. military devised the MPC program. MPCs were paper money denominated in amounts of 5 cents, 10 cents, 25 cents, 50 cents, 1 dollar, 5 dollars, 10 dollars, and starting in 1968 20 dollars. Unlike US currency, these certificates were issued under the authority of the Department of War (later Department of Defense) rather than the Department of the Treasury. Consequently they do not bear the US Treasury seal found on virtually every example of US currency. MPCs were fully convertible to U.S. dollars upon leaving a designated MPC zone, and convertible to local currencies when going on leave (but not vice versa). Because they were and are not obligations of the US Treasury, they are (unlike all US currency) no longer redeemable. It was illegal for unauthorized personnel to possess MPC, and that policy, in theory, eliminated U.S. dollars from local economies. Although actual greenbacks were not circulating, many local merchants accepted MPC on par with US dollars, since they could use them on the black market. This was especially evident during the Vietnam War when the MPC program was at its zenith. To prevent MPC from being used as a primary currency in the host country and destroying the local currency value and economy, MPC banknote styles were frequently changed to deter black marketers and reduce hoarding. A \"conversion day\" or \"C-day\" was the soldiers' only chance to trade in their old MPC for the new issue, after which the old MPC became worthless.\n\nC-days in Vietnam were always classified, never pre-announced. On a C-day, soldiers would be restricted to base, preventing them from helping Vietnamese civilians—especially local bars, brothels, bar girls and other black market people—from converting old MPC to the newer version. Since Vietnamese were not allowed to convert the currency, they frequently lost savings by holding old MPC that lost all value after the C-day was completed. People angry over their MPC loss would sometimes attack the nearest U.S. base the next night in retaliation.\n\nTo illustrate the Vietnam War MPC cycle, in mid-1970, a soldier could have a friend in the United States mail him a standard $100 bill, take if off-base and convert it to $180 in MPC, then change the MPC to South Vietnamese piastres at more than the legal rate. The soldier could then spend freely, paying in low-cost local currency, and end the day with a sizable profit. If the $100 bill found its way into the hands of a high-level Vietnamese government official, it could be taken out of the country and deposited in a bank account abroad, where U.S. currency could be more safely hoarded, and/or transferred into a North Vietnamese European exchange account.\nThirteen series of MPC were issued between 1946 and 1973, with varied designs often compared to Monopoly money due to their colors. After the official end of U.S. participation in the Vietnam War in early 1973, the only place where MPC remained in use was South Korea. In autumn of 1973, a surprise C-day was held there, retiring MPC and substituting greenbacks. MPC was never again issued, and the concept lay dormant until the late 1990s, when it was revived somewhat in the form of the Eagle Cash stored value card system, used by U.S. armed forces in Iraq.\n\n\n"}
{"id": "9459471", "url": "https://en.wikipedia.org/wiki?curid=9459471", "title": "Ministry of Science and Technology (South Korea)", "text": "Ministry of Science and Technology (South Korea)\n\nThe Ministry of Science and Technology (MoST) was ministry of the government of South Korea which coordinates science and technology activities in the country. In 2008, it was combined with another ministry and renamed the Ministry of Education, Science and Technology. However the current Korean government under Park Geun-hye has re-launched the ministry with the name under the Ministry of Science, ICT and Future Planning. \n"}
{"id": "17366018", "url": "https://en.wikipedia.org/wiki?curid=17366018", "title": "National conventions for writing telephone numbers", "text": "National conventions for writing telephone numbers\n\nThe national conventions for writing telephone numbers vary by country. While international standards exist in the form of the International Telecommunication Union sector ITU-T issued recommendation E.123, national telephone numbering plans define the format and length of telephone numbers assigned to telephones.\n\nThe presentation of telephone numbers in this article does not include any international dialing codes necessary to route calls via international circuits. In examples, a numeric digit is used only if the digit is the same in every number, and letters to illustrate groups. X is used as a wildcard character to represent any digit in lists of numbers.\n\nTwenty-four countries and territories share the North American Numbering Plan (NANP), with a single country code. The formatting convention for phone numbers is NPA-NXX-XXXX, where NPA is the three digit area code and NXX-XXXX is the seven digit subscriber number. The prefix NXX of the subscriber number is a code for the local central office, unique in the numbering plan area. The place holder \"N\" stands for the digits \"2\" to \"9\", as the subscriber number may not begin with the digits 0 and 1. It is a closed telephone numbering plan in which all subscriber telephone numbers are seven digits, in addition to a three digit area code.\n\nOriginally, local calls within an area code could be placed by dialing NXX-XXXX, omitting the area code, known as 7-digit dialing. Only calling a destination in a different area code required dialing the destination area code, known as ten-digit dialing, but due to the need for additional telephone numbers in many regions, seven digit dialing is becoming rare in the United States. With the rapid growth of telephony in the late 20th century, many metropolitan areas saw the introduction of additional area codes. With two or more area codes becoming available in the same vicinity, mandatory ten-digit dialing rules were instituted, requiring the area code to be dialed for all calls. The trunk code 1 may be optional for local calls in some areas.\n\nThe Canadian Government has stated on its Language Portal of Canada that numbers are to be written with a hyphen between each sequence, as follows: 1-NPA-NXX-XXXX or NPA-NXX-XXXX. 10-digit dialing is now required throughout most of Canada, including all of British Columbia, Alberta, Saskatchewan, Manitoba, Quebec, Nova Scotia, and Prince Edward Island, as well as most of Ontario. Areas not yet requiring 10-digit dialing are Ontario's 807 area code, Newfoundland, New Brunswick, the Yukon, Northwest Territories, and Nunavut, although 10-digit dialing may be accepted in some of these areas.\n\nIn the province of Québec, where French is the first language, the Office québécois de la langue française has established that phone numbers must be written with spaces first and then a hyphen for the last sequence, as follows: 1 NPA NXX-XXXX. Educational institutions of Quebec will mark improperly written phone numbers as orthographical mistakes in academic texts.\n\nMexican phone numbers are 10 digits long. Phone numbers can consist of a 2 digit (NN) area code plus an eight digit local number (ABCD XXXX) or a 3 digit area code (NNN) plus a seven digit local number (ABC XXXX).\n\nThe formatting convention of a phone number is\n\nIncluding the long distance access code\n\nIncluding the indicator of local cell phone call from a land line\n\nIncluding the indicator of long distance cell phone call from a land line\n\nBelgian telephone numbers consist of two major parts: Firstly '0', secondly the \"zone prefix\" (\"A\") which is 1 or 2 digits long for landlines and 3 digits long for mobile phones and thirdly the \"subscriber's number\" (\"B\").\n\nLand lines are always 9 digits long. They are prefixed by a zero, followed by the zone prefix. Depending on the length of the zone prefix, the subscriber's number consists of either 6 or 7 digits. Hence land line numbers are written either \"0AA BB BB BB\" or \"0A BBB BB BB\".\n\nMobile phone numbers always consist of 10 digits. The first digit of the \"zone prefix\" of a mobile number is always '4'. Then follows 2 digits indicating to which Mobile Operator's pool the number originally belonged when it was taken into usage. The fourth digit represents a \"sub-group\" of this pool and has no additional meaning other than increasing the amount of possible numbers. The subscriber's number consists of 6 digits. Hence, mobile phone numbers are written \"04AA BB BB BB\". Sometimes, the last 6 digits are written in two groups of 3 digits to increase readability: \"04AA BBB BBB\".\n\nNumbers are sometimes written with a slash in between the zone prefix and the subscriber's number. This is the case for both land lines and mobile phone numbers. Sometimes, dots are written between the blocks of the subscriber's number. Examples: \"0AA/BB BB BB\", \"0AA/BB.BB.BB\"; for mobile numbers: \"04AA/BB BB BB\", \"04AA/BB.BB.BB\" or \"04AA/BBB.BBB\".\n\nThe international country code prefix for Belgium is \"+32\". When dialing a number with the prefix, the 0 can be dropped, e.g.: \"+32 4AA BB BB BB\".\n\nDanish telephone numbers are eight digits long and are normally written\n\nDanish emergency and service numbers are three digits long and are written AAA. Danish short numbers used for text messaging services are four digits long and are written AAAA.\n\nFrench telephone numbers are 10 digits long written in groups of two separated by spaces, in the format 0A BB BB BB BB where 0 (the trunk prefix) was created in 1996 to be a carrier selection code, and A is the \"territorial area code\" included in the subscriber number A BB BB BB BB.\n\nThe A (territorial area code) can be 1 to 5 (for geographic numbers, depending of the area in the country, respectively: Paris/Suburbs, N-W, N-E, S-E, S-W), and it designates nationwide numbers when it is 6 or 7 (mobile numbers), 8 (special numbers), or 9 (phone over IP over xDSL/non-geographic numbers).\n\nThe numbering plan is a closed one, all digits must always be dialed.\n\nThe first two or three B can designate the area (old area code) for geographic numbers, or the operator to whom the number resource belongs.\n\nThere are also \"short numbers\" for emergencies (such as 112), that are written 1C or 1CC; and short numbers for special services, written 10 CC, 11C CCC, or 36 CC. 00 is the international access code.\n\nInternational format is +33 A BB BB BB BB where the leading trunk prefix 0 disappears (it must not be dialed from abroad). This format can be directly used in mobile phones.\n\nGerman telephone numbers have no fixed length for area code and subscriber number (an open numbering plan).\n\nThere are many ways to format a telephone number in Germany. The most prominent is DIN 5008 but the international format E.123 and Microsoft's canonical address format are also very common.\n\nTrunk access code is 0 and international call prefix code is 00.\n\nNumbers are often written in blocks of two. Example: +49 (A AA) B BB BB (Note the blocks go from right to left)\n\nThe very old format and E.123 local form are often used by older people but also for technical reasons.\n\nGreek telephone numbers are ten digits long, and usually written \"AAB BBBBBBB\" or \"AAAB BBBBBB\" where AAB or AAAB is the 2- or 3-digit national area code plus the first digit of the subscriber number, and BBBBBBB or BBBBBB are the remaining digits of the subscriber number. The entire number must always be dialed, even if calling within the same local area, therefore the national destination code is not separated from the subscriber number. According to international convention, numbers are sometimes written \"+30 AAB BBBBBBB\" or \"+30 AAAB BBBBBB\" to include the country calling code.\n\nIn Hungary the standard lengths for area codes is two, except for Budapest (the capital), which has the area code 1. Subscribers' numbers are six digits long in general, numbers in Budapest and cell phone numbers are seven digits long.\n\nPhone numbers in Iceland are seven digits long and generally written in the form \"XXX XXXX\" or \"XXX-XXXX\".\n\nPhone numbers in Ireland are part of an open numbering plan with varying number lengths. The area code system similar to some other northern European countries. Unlike the UK, Irish fixed line numbering is divided into a number of regions which are (except Dublin) further subdivided in a hierarchical structure, with the largest town often (but not always) taking 0A1.\n\nArea codes start with a trunk prefix \"0\" and extend for up to four digits but usually 3; followed by the local phone number of up to seven digits.\n\nIn industry jargon, these area codes and prefixes are referred to as NDCs (National Dialing Codes). This is the term used by ComReg and technical documents, as they include non-geographic codes. Historically, like the UK, the term STD code (Subscriber Trunk Dialling) was used.\nHowever, the terminology is archaic and is no longer universally understood and should not be used to avoid confusion.\n\nDublin uses the shorter (01) code which is not further subdivided. Other cities and major towns usually have codes ending in 1. Cork for example is 021, Galway 091, Limerick 061 etc.\n\nThe leading zero is always omitted when dialling from outside the country.\n\nLocal phone numbers are either 5, 6 or 7-digits long. In 7-digit numbering they are usually grouped as BBB BBBB, 6-digit numbers are grouped BBB BBB and 5-digit numbers are normally all grouped together BBBBB\n\nGrouping of numbers is not strictly adhered to, but usually fairly consistent. The area code should always be kept separated with a space or surrounded by brackets and not merged into the local number.\n\nThe use of hyphens is discouraged, particularly on websites as it can prevent mobile browsers identifying the number as a clickable link for easier dialling.\n\nFixed line numbers are normally presented as follows:\n\n01 BBB BBBB for a Dublin number (7 digit)\n\n021 BBB BBBB for a Cork number. (7 digit)\n\n064 BBB BBBB for a Killarney number (7 digit)\n\n061 BBB BBB for a Limerick number. (6 digit)\n\n098 BBBBB for a Westport number (5 digit)\n\n0404 BBBBB for a Wicklow number (5 digit)\n\nArea codes may also be surrounded by brackets, but this practice is falling out of use, as local dialing without the area code is optional on landlines and the area code must always be dialled on mobile phones.\n\nThe Irish telecommunication regulator, ComReg has been gradually rationalising area codes by merging and extending local numbering to 7-digits in the format 0AA BBB BBBB. This is being carried out only where necessary to avoid disruption. This means that varying fixed line number lengths will continue to exist in Ireland for the foreseeable future.\n\nMobile numbers are presented as follows:\n\n08A BBB BBBB\n\nBrackets should not be used around the mobile prefix as it is never optional and must always be dialled, even from another phone with the same prefix.\n\nSpecial rate numbers, such as free phone/toll free and premium rate are usually grouped:\n\nFreephone: 1800 BB BB BB or (spoken as one-eight-hundred)\nLocal rate: 1850 BB BB BB (eighteen-fifty)\n1550 BB BB BB (read as fifteen-fifty)\n\nHowever, for memorability, this is not consistently adhered to.\n\nAlphanumeric characters can also be used in presenting numbers for added memorability, but it is much less common than in North America\n\nFor example:\n18AA 99 TAXI\n\nNote: These special rate numbers are not reachable from outside Ireland and should never be presented as +353 1800 or +353 1550 etc. as this would result in a connection to an unrelated Dublin landline in the +353 1 BBB BBBB range.\n\nPhone numbers in Italy have variable length. There's no well established convention about how to group digits or which symbol to use, but this is hardly an issue since all the digits are always dialed.\n\nSince 10 October 1995 (Operation Decibel) all telephone numbers in the Netherlands are 10 digits long (including the trunk prefix '0'). The area code ('A') is commonly separated with a dash ('-') and sometimes a space from the subscriber's number ('B'). Alternatively, the area code (including the trunk prefix) can be enclosed in parentheses.\n\nThe length of the area code for landlines is either 2 or 3 digits, depending on the population density of the area. This leaves 7 or 6 digits for the subscriber's number, resulting in a format of either \"0AA-BBBBBBB\" or \"0AAA-BBBBBB\". Cellphone numbers are assigned the 1-digit area code 6, leaving 8 digits for the subscriber's number: \"06-CBBBBBBB\", where subscriber's number ('C') is neither 6 nor 7. Service numbers (area codes 800, 900, 906 and 909) have either 4 or 7 remaining digits, making them 8 or 11 digits in total: \"0AAA-BBBB\" or \"0AAA-BBBBBBB\". The area code 14 has no trunk prefix and is used for government numbers, currently only for municipalities. The remaining digits represent the area code of the municipality. Therefore, the length 14 numbers total either 5 or 6 digits: \"14 0AA\" or \"14 0AAA\"\n\nThe trunk prefix '0' is dropped when prefixed by the country code: +31 AA BBBBBBBB, +31 6 CBBBBBBB, etcetera. Note that there is not a trunk prefix for the 14 series so the international number becomes +31 14 0AAA.\n\nNorwegian telephone numbers are 8 digits long. A number to a fixed line is written in four groups of two separated by spaces, \"AA AA AA AA\". Cellphone numbers are written in three groups, \"AAA AA AAA\". This makes it easy to determine if the B-number is SMS capable. Mobile numbers start with 4 or 9.\n\nTelephone numbers in Poland are 9 digits long. For mobile phones, the preferred format is \"AAA-AAA-AAA\". For landline phones, the preferred format is \"AA-BBB-BB-BB\", where \"AA\" is area code. Occasionally, you can encounter numbers formatted as \"(AA) BBB-BB-BB\". Omitting area code is not permitted, because nowadays it is always required.\n\nStarting with 2002 phone numbers in Romania are 10 digits long, the first digit always being 0. The preferred format is \"AAAA-AAA-AAA\" for both mobile and landline phone numbers, except for landline phones in Bucharest where the preferred format is \"AAA-AAA-AA-AA\".\n\nRussia has an open numbering plan with 10-digit phone numbers. Trunk prefix is \"8\" (or \"8~CC\" when using alternative operators, where CC is 21–23, 52–55). International call prefix is \"8~10\" (or \"8~CC\" when using alternative operators, where CC is 26-29, 56-59). The country code is \"+7\".\n\nLength of geographical area codes (A) is usually 3 to 5 digits; length of non-geographical area codes is 3. The groups of digits in the local subscriber's number (B) are separated by dashes ('-'): \"BBB-BB-BB\", \"BB-BB-BB\", \"B-BB-BB\". The area code is included in parentheses, similarly to E.123 local notation: \"(AAA) BBB-BB-BB\", \"(AAAA) BB-BB-BB\", \"(AAAAA) B-BB-BB\".\n\nArea code dialing is optional in most geographical area codes, except Moscow (area codes 495, 498, 499); it is mandatory for non-geographical area codes. E.123 international and Microsoft formats are used for writing local phone numbers as well; international prefix and country code \"+7\" are replaced with trunk code \"8\" (or \"8~CC\") when dialing a mandatory area code.\n\nEven though trunk code is not needed for calls within the same geographical area, recent convention adds the default trunk code to the phone number notation: \"8 (AAAA) BB-BB-BB\". For mandatory area code dealing plans, notation \"8 AAAA BB-BB-BB\" is used. These formats are a mix of Microsoft format and E.123 local notation.\n\nMobile phones require full 10-digit number which starts with 3-digit non-geographical area codes 900-990. For international calls abroad or international roaming calls to Russia, E.123 international notation with an international call prefix '+' is the only allowed calling number format. For local calls both \"8\" and \"+7\" are accepted as a trunk code.\n\nSpanish telephone numbers are nine digits long, starting with '9' or '8' for fixed lines (excluding '90x' and '80x') or with '6' or '7' for mobile phones.\n\nThe first group in fixed lines always identifies the dialed province. That group might be of 2 or 3 digits; for example, 91 and 81 are for Madrid while 925 and 825 are for Toledo. The second group is always of 3 digits as it formerly identified the telephone exchange (it now identifies the telephone area).\n\nWhen the first group is 2 digits long (as in Madrid), the number is usually written in four groups of 2-3-2-2 digits (\"AB CCC DD DD\")\n\nWhen the first group is 3 digits long (as in Toledo), the number is usually written in 3 groups of 3 digits (\"ABB CCC DDD\") but the form 3-2-2-2 (\"ABB CC CD DD\") is not uncommon.\n\nMobile numbers are usually grouped by threes, \"ABB CCC CCC\", but the form 3-2-2-2 is also seen.\n\nSwiss telephone numbers are ten digits long, and usually written \"0AA BBB BB BB\" where 0AA is the national destination code and BBB BB BB is the subscriber number. The entire number must always be dialed, including the leading \"0\", even if calling within a local area, therefore the national destination code is not separated from the subscriber number. According to international convention, numbers are sometimes written \"+41 AA BBB BB BB\" to include the country calling code. Certain nationwide destination codes, such as for toll-free or premium-rate telephone numbers, are written \"0800 BBB BBB\" or \"0900 BBB BBB\". \"Short numbers\" are used for emergency services such as \"112\" that are written \"1CC\" or \"1CCC\".\n\nSwedish telephone numbers are between eight and ten digits long. They start with a two to four digit area code. A three digit code starting with 07 indicates that the number is for a mobile phone. All national numbers start with one leading 0, and international calls are specified by 00 or +. The numbers are written with the area code followed by a hyphen, and then two to three groups of digits separated by spaces.\n\nIn Turkey the format for telephone numbers is commonly seen as 0BBB AAA AA AA. While landline numbers having the prefix 02BB AAA AA AA, 03BB AAA AA AA, or 04BB AAA AA AA mobile numbers have the prefix 05BB AAA AA AA. Landline area codes are separated by cities and only one city, Istanbul, has two area codes: 216 for the Asian side, and 212 for the European side. Mobile numbers however are separated by carriers. There are three mobile carriers in Turkey: Vodafone TR, Turkcell and Turk Telekom. Turkcell has the prefix 053B AAA AA AA, Vodafone TR has the prefix 054B AAA AA AA, and Turk Telekom has the prefix 055B AAA AA AA.\n\nSince 9 November 2008, with the passing of the Number Carriability Regulation by ICTA, mobile numbers can be carried from one mobile carrier to the other, without having to change the prefix. This caused dialing 05BB to call another number on the same carrier to become mandatory. Calls to numbers which were carried to another operator are signaled by a unique sound upon dialing, to signify that the recipient is on another network and alert them against potentially unwanted interconnection charges. The same regulation passed on 10 September 2009 regarding landline numbers, without the requirement to dial the prefix among numbers with the same geographical area, sharing the same prefix.\n\nThe \"0\" on every prefix is an Area Code Exit code that must be dialed when a number with a different area code is being called. So when calling from outside of Turkey those 0s are not dialed. The dialing format when calling from outside Turkey is +90 BBB AAA AA AA and NOT +90 0BBB AAA AA AA. Unlike the North American system, the Country Exit Code isn't 011 but 00. So it is one \"0\" to exit area and one more \"0\" to exit the country.\n\nDialling codes, also known as \"area codes\" are optional for local callers (but should not be surrounded by parentheses), though there are trials running to make them mandatory, and are followed by the customer's telephone number.\n\nCodes with the form \"02x\" are followed by 8-digit local numbers and should be written as \"02x AAAA AAAA\". Area codes with the form \"011x\" or \"01x1\" are used for many of the major population centres in the UK, are always followed by 7-digit local numbers and should be written as \"01xx AAA BBBB\". Other area codes have the form \"01xxx\" with 5 or 6 figure local numbers written as \"01xxx AAAAA\" or \"01xxx AAAAAA\"; or have the form \"01xxxx\" with 4 or 5 figure local numbers written as \"01xx xx AAAA\" or \"01xx xx AAAAA\".\n\nGeographic numbers are also sometimes displayed in a format with a dash separating the code and number, this was formerly the recommended format for six major metropolitan areas in the UK, e.g. \"01x1-AAA BBBB\". They are sometimes also shown in a format with a space between the code and number, similar to the non-geographic format, e.g. \"01x1 AAA BBBB\".\n\nNumbers for mobile phones and pagers are formatted as \"07AAA BBBBBB\" and most other non-geographic numbers are 10 figures in length (excluding trunk digit '0') and formatted as \"0AAA BBB BBBB\". However, these numbers are sometimes written in other formats. 9 figure freephone numbers are \"0500 AAAAAA\" and \"0800 AAAAAA\" and there is one number of 8 figures length: \"0800 1111\" (Childline).\n\nDomestically, there are also a number of special service numbers such as 100 for the operator, 123 for the speaking clock and 155 for the international operator, as well as 118 AAA for various directory enquiry services, and 116 AAA for various helplines. For some services, the number you call will depend on which operator you use to connect the call. 112 and 999 work for calling the emergency services. These numbers cannot be called from abroad.\n\nWhen calling from abroad, the initial '0' trunk prefix is not required; it is, however, commonplace to represent telephone numbers with both the international code and the '0' trunk prefix - which is typically placed within parentheses - but this representation is inconsistent with the E.123 international standard.\n\nTelephone numbers in Pakistan have two parts. Area codes in Pakistan are from two to five digits long; the smaller the city, the longer the prefix. All the large cities have two-digit codes.\n\nSmaller towns have a six digit number. Large cities have seven-digit numbers. Azad Jammu and Kashmir has five digit numbers. On 1 July 2009, telephone numbers in Karachi and Lahore were changed from seven digits to eight digits. This was accomplished by adding the digit \"9\" to the beginning of any phone number that started with a \"9\" (government and semi-government connections), and adding the digit \"3\" to any phone numbers that did not start with the number \"9\".\n\nIt is common to write phone numbers as \"(0xx) yyyyyyy\", where \"xx\" is the area code. The \"0\" prefix is for trunk (long-distance) dialing from within the country. International callers should dial \"+92 xx yyyyyyyy\".\n\nAll mobile phone codes are four digits long and start with \"03xx\". All mobile numbers are seven digits long, and denote the mobile provider on a nationwide basis and not geographic location. Thus all Telenor numbers (for example) nationwide carry mobile code 0345 etc.\n\nUniversal access number\n\nEmergency Service Numbers\n\nPremium Rate services:\n\nToll free numbers (For callers within Pakistan):\n\nTelephone numbers in India are 10 digits long (excluding an initial zero which is required at times) and fall in at least four distinct categories:\n\nAll telephone numbers in Iran are 11 digits long (initial 0 plus ten numbers). The first two, three or four digits after the zero are the area code.<br>The possibilities are: (0xx) xxxx xxxx (for landlines), 09xx xxx xxxx (for mobiles) and 099xx xxx xxx (for MVNO).\n\nWhen making a call within the same landline area code, initial 0 plus the area code can be omitted.\n\nAn example for calling telephones in Tehran is as follows:\n\nAn example for mobile numbers is as follows:\n\nTelephone numbers in China are 10 or 11 digits long (excluding an initial zero which is required at times) and fall in at least four distinct categories:\n\nEvery number, except special service numbers, is an 8-digit number; they are grouped as XXXX YYYY. There are no area codes.\n\nThe traditional convention for phone numbers is (0AA) NXX-XXXX, where 0AA is the area code and NXX-XXXX is the subscriber number. This number format is very similar to the North American numbering plan, however, the country has a trunk code of 0 instead of 1, so international callers (using +81) do not have to dial the trunk code when calling to Japan. Telephone numbers were nine digits long in Tokyo and Osaka until the late 1990s, when a seventh digit was added to the subscriber number. Densely populated areas have shorter area codes, while rural areas have longer area codes, however, the last two digits of a five digit long area code (including the first zero) may also be the first two digits of the subscriber number. Area codes increase from north to south, except in areas such as the western Hokuriku region and the prefecture of Okinawa, where area codes increase from west to east or south to north.\n\nSome telephone numbers deviate from this rule:\n\nAll area codes including mobile start with a \"0\" (trunk prefix) for domestic calls. If you are dialing from another country the for Malaysia is \"60\" which may be confusing; do not dial an extra \"0\" before the rest of the digits. For fixed line and mobile phone numbers, a dash is written in between the area/mobile code and the subscriber number, with an optional space before the last four digits of the subscriber number. For example, a fixed line number in Kuala Lumpur is written as 03-XXXX YYYY or 03-XXXXYYYY, while a fixed line number in Kota Kinabalu is written as 088-XX YYYY or 088-XXYYYY. A typical mobile phone number is written as 01M-XXX YYYY or 01M-XXXYYYY. Toll-free and local charge numbers are written as 1-800-XX-YYYY and 1-300-XX-YYYY respectively, while premium rate numbers are written as 600-XX-YYYY.\n\nTelephone numbers in the Philippines are written as +63 (XXX) YYY ZZZZ for international callers. For domestic calls, the country code (+63) is omitted and a trunk prefix (0) is placed . For local calls, both the 0 and area code are omitted. Mobile numbers are written as +63 (XXX) YYY ZZZZ or 0 (XXX) YYY ZZZZ.\n\nIn Singapore, every phone number is written as +65-XXXX-YYYY or +65 XXXX YYYY.\n\nMobile phones starts with 8/9, landline phone numbers starts with 6 while VOIP numbers starts with 3.\n\nSubscriber numbers have 8 digits and there are no area codes.\n\nLandline numbers in Taiwan are written with the area code in parenthesis [with phone numbers totaling 9 digits]\nExample: (02) XXXX YYYY for phone numbers in Taipei area.\n\nMobile phones have 3 digit \"company code\" assigned to different mobile service carriers such as (09**) XXXXXX followed by a 6 digit phone number.\n\nSouth Korean phone numbers can be as short as 7 digits and as long as 11 digits, because, when making a local call (i.e. in the same city), there is no need to dial the area code. South Korean area codes are assigned based on city.\n\nLandline home numbers are usually written as: \"0XX-XXX-XXXX\" or \"(0XX) XXX-XXXX\" where 0XX indicates an area code. \"(0XX) XXX-XXX\" and \"0XX XXX XXXX\" (without hyphens) are comprehensible as well. The area code may be two digits long for some cities such as Seoul and Gwacheon (these two cities use the same area code) and three digits for other cities such as Incheon, Busan and most of the cities in Gyeonggi-do. The middle three-digit part is extended to four digits in many areas due to the increased number of telephone users.\n\nIn the international context, \"82 0XX-XXX-XXXX\" is commonly used as well. For international calls, \"0\" in the area code is often omitted, because it is not necessary to dial 0 from foreign countries. Therefore, it is better written as: \"82-(0)XX-XXX-XXXX\" or \"82-(0)XX-XXXX-XXXX\" The plus (+) sign is often added to the country code too (e.g., \"+82 0XX-XXX-XXXX\" or \"+82-0XX-XXXX-XXXX\").\n\nFor mobile numbers, \"01X-XXX-XXXX\" is commonly used. With the third generations of the mobile phone, most of the mobile numbers start with 010 but there still are a number of people who continue to use the second generation, with the numbers starting with 011, 016, 017, so on. As with the landline home numbers, the mobile numbers' middle three-digit part is extended to four digits (e.g., \"01X-XXXX-XXXX\") due to the increased number of mobile phone users.\n\nFor mobile numbers in the international context, \"82 (0)1X-XXX-XXXX\" or \"82-(0)1X-XXXX-XXXX\" are most often used. As with the home phone numbers, the plus (+) sign is often added to the country code (e.g., \"+82 01X-XXX-XXXX\" or \"+82-01X-XXXX-XXXX\")\n\nIf an area code starts with 070, the number does not belong to any particular area, and is a number given by an Internet telephone service. In this case, 070 is not usually put in the brackets, neither ( ) nor ).\n\nIn the business context, the numbers in the format of \"15XX-XXXX\" and \"16XX-XXXX\" without any area code are business representative agency or customer services. While the numbers starting with 080 (e.g., \"080-XXX-XXXX\") are also business-related numbers but are usually toll-free customer service centers. Also in this case, 15XX, 16XX or 070 are not put in the brackets, neither ( ) nor ).\n\nThere are national telephone services which have phone numbers in the format of \"1XX\" or \"1XXX\", without any area code. For example, 114 is for telephone yellow page, 119 is for fire/emergency number, 112 is for police station center, 131 is for weather forecast information, 1333 is for traffic information, and so on. The number 111 is for reporting spies, especially from North Korea. It used to be 113, so most of senior citizen still believe it is the number for reporting spies. These numbers do not need any brackets.\n\nIf there are multiple numbers used for one person/entity, the symbol \"~\" is usually used to avoid repetitions. For example, if one company has three phone numbers—031-111-1111, 031-111-1112 and 031-111-1113—then they are shortened as in \"031-111-1111~3\".\n\nIf the numbers are not consecutive, then the last digit is written together with commas. For example, if a company has three numbers—031-111-1111, 031-111-1115, 031-111-1119, then they are shortened as in \"031-111-1111, 5, 9\".\n\nMost Australian telephone numbers are 10 digits long, and are generally written \"0A BBBB BBBB\" or \"04XX XXX XXX\" for mobile telephone numbers, where 0A is the optional \"area code\" (2,3,7,8) and BBBB BBBB is the subscriber number. (http://www.acma.gov.au/Industry/Telco/Numbering/Numbering-Plan/phone-number-meanings-numbering-i-acma)\n\nWhen the number is to be seen by an international audience, it is written +61 A BBBB BBBB or +61 4XX XXX XXX. When written for a local audience, the optional area code is omitted. The area code is often written within parentheses (0A) BBBB BBBB. Mobile numbers should never have parentheses.\n\nTen-digit non-geographic numbers beginning with 1 are written 1X0Y BBB BBB, where X is 8 for toll free numbers, 3 for fixed-fee numbers and 9 for premium services. Six-digit non-geographic numbers are written 13 BB BB or 13B BBB; these are fixed-fee numbers. Seven-digit 180 BBBB numbers also exist. 'B's are sometimes written as letters.\n\nAlmost all New Zealand telephone numbers are seven digits long, with a single-digit access code and a single-digit area code for long-distance domestic calls. Traditionally, the number was given as \"(0A) BBB-BBBB\", with the two first digits (the STD code) often omitted for local calls. The brackets and the dash are also often omitted. Mobile numbers follow the same format, but with the area code being two digits, i.e. \"(02M) BBB-BBBB\". ( Some mobile numbers are longer: (021)02BBBBBB, (021)08BBBBBB, (020)40BBBBBB, (020) 41BBBBBB and (028) 25BBBBBB; and some are shorter: (021)3BBBBB, (021)4BBBBB, (021)5BBBBB, (021)6BBBBB, (021)7BBBBB, (021)8BBBBB and (021)9BBBBB)\n\nThere are also free-phone numbers (starting with 0800 or 0508) that are given in the format \"0800-AAA-AAA\". It is not uncommon for the 0800 and 0508 to be enclosed in brackets, although this is not strictly correct as the brackets denote optional parts of the number, and the 0800 and 0508 is required.\n\nFor international use, the prefix +64 is substituted for the leading zero, giving \"+64-A-BBB-BBBB\" for land-lines, and \"+64-MM-BBB-BBBB\" for mobile numbers.\n\nSome Central American countries write the country code for their own and other Central American countries in parentheses, instead of using a + sign, as recommended by E.123. For example, for a number in Costa Rica they would write (506) 2222-2222 instead of +506 2222-2222. On the other hand, Guatemala does have the custom of using the + sign. It is quite common for Central American businesses to write the whole phone number, including the country code in parentheses, on business cards, signs, stationery, etc.\n\nCosta Rican telephone numbers are 8 digits long, and are usually written in the format \"2NNN-NNNN\" (for landlines), \"8NNN-NNNN\" (for mobile telephone numbers from local telephone company ICE), \"6NNN-NNNN\" (for mobile telephone numbers from Movistar) and \"7NNN-NNNN\" (for mobile phone numbers from Claro). Toll-free numbers use the format \"800-NNN-NNNN\" and premium-rate telephone numbers are written \"90x-NNN-NNNN\" where \"x\" varies according to the type of service offered. There are also \"short numbers\" for emergencies such as \"911\".\n\nWhen Costa Rica switched from 7 to 8 digit numbers, it used a scheme similar to the 8 digit number scheme already in place in El Salvador at that time.\n\nEl Salvadoran telephone numbers are 8 digits long, usually written in the format \"2NNN-NNNN\" (for landline use) and \"7NNN-NNNN\" (for mobile telephone numbers). Premium-rate numbers start with a 9.\n\nGuatemalan telephone numbers are 8 digits long and written in the format \"2NNN-NNNN\" for landlines in Guatemala City, \"6NNN-NNNN\" for landlines for the rest of municipalities in the Guatemala Department, and \"7NNN-NNNN\" for landlines in Rural Guatemala / rest of country. Non-geographic numbers (mobile) are \"5NNN-NNNN\", \"4NNN-NNNN\", and \"3NNN-NNNN\". Within each area, there are different service providers. The following 3 digits indicate the service provider. However, their assignment is on a first-come first-served basis.\n\nAdditionally there are special numbers with the following conventions: 3 digit numbers for emergency systems, four digit numbers, 15NN for information and governmental institutions and 17NN for commercial and banking institutions with a high call influx, 6 digit numbers for Telephone carriers numbers and making operator assisted calls, collect calls. These calls are billed at different rates. 1-800: Toll-free calls redirected to out of country offices and 1-801: Local toll-free calls.\n\nHonduran telephone numbers have either 7 digits (for landlines), which are usually written \"NNN-NNNN\", or 8 digits (for mobile numbers), which are written \"NNNN-NNNN\". The fact that landline and mobile numbers are different lengths sometimes causes confusion.\n\nIn 2010, an additional digit (2) was added to the start of land line numbers, thus standardizing the length at 8 digits.\n\nArgentinian telephone numbers always consist of \"11 digits\", including the geographical area code.\n\nThe area code can have 3, 4 or 5 digits, the first being always 0 (indicative of long distance calls). Moreover, in 1999 the whole country (except Buenos Aires, and Greater Buenos Aires) was divided into two zones. Roughly and with exceptions, one includes most of the northern half of the country; and the other, most of the southern half, though the actual reason for this division is not geographical, but the fact that each zone is administered by a different company.\n\nSo, the second digit of area codes can be 1 (only in Buenos Aires and Greater Buenos Aires code \"011\") or else a 2 (for towns in the southern half of the country) or a 3 (for the northern half). For example, (011) for Buenos Aires, (0341) for Rosario, (02627) for San Rafael. And the subscriber's number will accordingly have 6, 7 or 8 digits, to complete the eleven digits.\n\nPhone numbers are mostly written as:\n\nThe area code is usually written between brackets.\n\nIn 1999, a general reform was introduced to telephone numbers, including the 1, 2 and 3 for area codes as explained above, and adding a 4 at the beginning of all subscriber's numbers. However, since the reform some local numbers starting with a 5 are beginning to appear. Moreover, a hyphen is usually placed to separate the last four digits. Code areas do not usually include one single city or town, but several neighbouring towns. So, the part before the hyphen (called a prefix) is usually indicative of either a town within the code area, or even of a part of a larger city, which is assigned several prefixes. As a matter of fact, each area code has only a limited number of prefixes assigned, and these are locally limited within the area.\n\nFor example, the (0342) area has numbers with a 456- prefix, mostly located in the centre of Santa Fe. It also has numbers with a 460- prefix, usually for phone lines in the north east of the city. And there are lines with a 474- prefix, located in Santo Tomé. But no 444- prefix exists within this area. As for the part after the hyphen, it may usually be any succession of four digits, though sometimes a prefix is shared by two or more small towns, and then, the first digit after the hyphen carries the distinction between towns.\n\nSometimes, a prefix is reserved for official numbers, that is for offices depending on the national, provincial or local state. In the (0342) area, this is 457-, and phones within this prefix communicate with each other, by simply dialing the four final digits, though from other phones the prefix must be dialled as well.\n\nMobile phones use the same area codes as landline telephones, but the number begins with a \"15\", added to a string of 6, 7 or 8 digits, just as described above. After the \"15\", the remainder of the number can start with a 3, a 4, a 5 or a 6. This \"15\" may be dropped when a call is made to a mobile phone in a different code area. And when sending text messages, the receiver's number is best dialled both without the \"15\" and \"with\" the long distance code, even if both sender and receiver share a code area, but without the initial \"0\".\n\nTo sum up, given the mobile phone (011) 154-123-4567, you will call it by dialing:\n\nAnd you will send messages to:\n\nTwo sorts of special numbers exist in Argentina. On the one hand, three-digit numbers are used for special services such as to call the police, fire brigade or emergency doctors, as well as to hear the official time. Also telephone companies have three-digit numbers to report a problem in the lines, or to ask for another subscriber's number, when a paper directory is not available.\n\nAdditionally, there are other longer numbers. These include (but are not limited to):\n\n0800 lines are used by companies, and it is the receiver and not the caller who pays for the call, so that potential clients are encouraged to contact business companies for free.\n\n0810 lines are paid by the caller, but the cost is for a local call, even if you are calling from a different area. The remaining is covered by the receiver.\n\nAnd 0600 numbers are special more expensive lines, that serve such purposes as TV games, fund collecting for charity companies, hot lines, etc. Basically a part of the extra money charged to the caller is sent to the owner of the line.\n\nOften the \"abcd\" or even \"(xx)xabcd\" part of the number is chosen, if available, to form a word that is representative of the company holding the number.\n\nBrazil is divided into 67 two-digit geographical area codes, all of which with eight-digit numbers, in the format \"AA NNNN-NNNN\", except for cell phones, which contain nine digits, usually in the format \"AA NNNNN-NNNN\".\n\nSee also: https://en.wikipedia.org/wiki/Telephone_numbers_in_Brazil\n\nPeru uses 2-digit area codes followed by 6-digit subscriber numbers outside of Lima. In Lima the area code is \"1\" and the subscriber number has 7 digits, divided XXX XXXX. The \"trunk 0\" is often used, especially for numbers outside Lima. For example, a phone number in Arequipa might be written (054) XX-XXXX.\n\nCellphone numbers used to have 8 digits with the first digit always being 9. In 2008 an additional digit was added to cellphone numbers, while land numbers remained the same. The previous convention for cell numbers in Lima was usually 9XXX XXXX, though 9-XXX XXXX was also used. With the new 9-digit number, the form 9XX XXX XXX is becoming increasingly common as opposed to 9 XXXX XXXX, 9X XXX XXXX or 9XXXX XXXX.\n\nOutside Lima cellphone numbers used to be 9 followed by six digits, i.e., 9 XXX XXX. The 2008 changes were somewhat more complicated. In four departments (similar to states), a 2 digit code now has to be entered before the 9. In the example of Arequipa, the code of 95 has to be entered before the 9, so the new numeration is 959 XXX XXX. The other codes are 94 for La Libertad (Trujillo), 96 for Piura and 97 for Lambayeque (Chiclayo). In the other 19 rural departments, the 9 is followed by the department's 2-digit area code then the 6-digit subscriber number. For example, the area code for Cusco is 84, so their new cellphone numeration is 984 XXX XXX. The effect is that all Peruvian cellphone numbers now have 9 digits; under the old system they had 8 digits in Lima and 7 everywhere else.\n\nSouth Africa uses 10 digit dialling which became required since 16 January 2007. The 10 digits used for local calls (AAA XXX XXXX or 0AA XXX XXXX) consists of a 3 digit area/type code (the first digit in the area/type code is a trunk prefix) and 7 digits. All area codes including mobile start with a \"0\" (trunk prefix) for domestic calls. When dialing from another country the international calling code for South Africa is \"+27\" with the rest of the digits excluding the trunk prefix (+27 AA XXX XXXX).\n\nThe International Telecommunication Union sector ITU-T issued recommendation E.123 entitled \"Notation for national and international telephone numbers, e-mail addresses and Web addresses.\"\n\n"}
{"id": "8452216", "url": "https://en.wikipedia.org/wiki?curid=8452216", "title": "PDF/UA", "text": "PDF/UA\n\nPDF/UA (PDF/Universal Accessibility) is the informal name for ISO 14289, the International Standard for accessible PDF technology. A technical specification intended for developers implementing PDF writing and processing software, PDF/UA provides definitive terms and requirements for accessibility in PDF documents and applications. For those equipped with appropriate software, conformance with PDF/UA ensures accessibility for people with disabilities who use assistive technology such as screen readers, screen magnifiers, joysticks and other technologies to navigate and read electronic content.\n\nOn February 18, 2015 the US Access Board announced its Proposed Rule for US federal policy on accessibility, commonly known as Section 508. The proposed rule identifies PDF/UA as equivalent to WCAG 2.0 for \"appropriate content\".\n\nPDF/UA is not a separate file-format but simply a way to use the familiar PDF format invented by Adobe Systems and now standardized as ISO 32000.\n\nIn general PDF/UA requires Tagged PDF (ISO 32000-1, 14.8), but adds a variety of qualitative requirements, especially regarding semantic correctness of the tags employed:\n\nPDF/UA complements WCAG 2.0, and should be used to make PDF files that also conform with WCAG 2.0.\n\nThe 2014 update to PDF/UA, published in December 2014, is the first fully accessible standard ISO has ever published; the PDF file distributed by ISO itself conforms to PDF/UA.\n\n\n\n\nThe formal name of PDF/UA is \"ISO 14289-1 Document management applications -- Electronic document file format enhancement for accessibility -- Part 1: Use of ISO 32000-1 (PDF/UA-1)\".\n\nWhile the PDF/UA specification is written for software developers, PDF/UA support is of interest to persons with disabilities who require or benefit from assistive technology when reading electronic content. With PDF/UA conforming files, readers and assistive technology, users are guaranteed – so far as the PDF format itself can provide – equal access to information.\n\nThe benefits of PDF/UA extend beyond people with disabilities. With support for PDF/UA, reader software will be able to reliably reflow text onto small screens, provide powerful navigation options, transform text appearance, improve search engine functionality, aid in the selection and copying of text, and more.\n\nThe PDF/UA project began in 2004 as an AIIM Standards Committee. The PDF/UA wiki, operated by AIIM, contains agendas, meeting-minutes and public-access documents pertaining to the development of PDF/UA.\n\nChaired since 2005 by Duff Johnson, in 2009 AIIM's PDF/UA Committee became the U.S. Committee for ISO/AWI (Accepted Work Item) 14289 with Cherie Ekholm of Microsoft as International Project Leader.\n\nThe US Committee continued as the main driver of the standard's development through the WD, CD, DIS and FDIS Phases of ISO standards development, with review, comment and balloting by the member bodies of ISO TC 171.\n\nISO 14289-1:2012 (PDF/UA) was published in July, 2012. A minor update (ISO 14289-1:2014) was published in December 2014. The document is available directly from the ISO's webstore.\n\nIn October 2012 the US Library of Congress added PDF/UA to their list of archivable file formats. The LoC's guidance states that PDF/UA is \"...a preferred format for page-oriented content by the Library of Congress.\"\n\nAdobe Systems, the largest implementer of interactive PDF software, has announced its intention to support PDF/UA.\n\nIn December 2014 ISO published ISO 14289-1:2014, a minor revision of the original 2012 document.\n\nIn March 2016 AIIM and ANSI published PDF/UA as an American National Standard:.\nAIIM's US Committee for PDF/UA has published the following documents in support of PDF/UA:\n\n\nWork on ISO 14289-2 (PDF/UA-2) is ongoing. PDF/UA-2 will be based on PDF 2.0.\n\nAIIM's US Committee for PDF/UA led development of the document from 2004 until 2010, when the draft became an ISO NWI. Both US and ISO Committees maintain their work on AIIM's PDF Standards wiki.\n\nParticipation by subject matter experts, software developers and other interested parties is invited and encouraged.\n\nThe PDF Association helps promote PDF/UA in a variety of ways.\n\nIn 2011 the PDF Association created the PDF/UA Competence Center as a way for vendors to develop educational resources and share experiences regarding implementation of PDF/UA. The Matterhorn Protocol (see below) is a product of the PDF/UA Competence Center.\n\nIn 2012 and 2013 the PDF Association helped raise money and awareness for NV Access's NVDA to further development of the world's first PDF/UA conforming screen reader.\n\nIn August 2013 the PDF Association's PDF/UA Competence Center published the Matterhorn Protocol, a set of 31 checkpoints and 136 failure conditions to help software developers make it easier for document authors to create fully accessible PDF files and forms.\n\nIn June 2014 the PDF Association's PDF/UA Competence Center published the first iteration of the PDF/UA Reference Suite, a set of reference-quality documents conforming to PDF/UA-1.<ref name=\"PDF/UA Reference Suite\"></ref>\n\nIn December 2015 the PDF Association's PDF/UA Competence Center unveiled the first public draft of its Tagged PDF Best Practice Guide. Publication is expected by the summer of 2016.\n\nThe PDF Association conducted numerous educational seminars on PDF/UA throughout Europe, North America and Australia in 2012 and 2013. A book, \"PDF/UA in a Nutshell\", was produced in both English and German. An informational brochure was also produced in both languages.\n\n"}
{"id": "34570458", "url": "https://en.wikipedia.org/wiki?curid=34570458", "title": "Pestalotiopsis microspora", "text": "Pestalotiopsis microspora\n\nPestalotiopsis microspora is a species of endophytic fungus capable of breaking down and digesting polyurethane. Originally identified in fallen foliage of common ivy (\"Hedera helix\") in Buenos Aires, it also causes leaf spot in Hypericum 'Hidcote' (\"Hypericum patulum\") shrubs in Japan.\n\nIts polyurethane degradation activity was discovered in two distinct P. microspora strains isolated from plant stems in the Yasuni National Forest within the Ecuadorian Amazon rainforest by a group of student researchers led by molecular biochemistry professor Scott Strobel as part of Yale's annual Rainforest Expedition and Laboratory. It's the first fungus species found to be able to subsist on polyurethane in anaerobic conditions. This makes the fungus a potential candidate for bioremediation projects involving large quantities of plastic.\n\n\"Pestalotiopsis microspora\" was originally described from Argentina in 1880 by mycologist Carlo Luigi Spegazzini, who named it \"Pestalotia microspora\".\n\nIn 1996 Julie C. Lee first isolated Torreyanic acid, a dimeric quinone, from \"P. microspora\", and noted that the species is likely the cause of the decline of Florida torreya (\"Torreya taxifolia\"), an endangered species that is related to the paclitaxel-producing \"Taxus brevifolia\".\n\n\n"}
{"id": "1256073", "url": "https://en.wikipedia.org/wiki?curid=1256073", "title": "Phasor", "text": "Phasor\n\nIn physics and engineering, a phasor (a portmanteau of phase vector), is a complex number representing a sinusoidal function whose amplitude (\"A\"), angular frequency (\"ω\"), and initial phase (\"θ\") are time-invariant. It is related to a more general concept called analytic representation, which decomposes a sinusoid into the product of a complex constant and a factor that encapsulates the frequency and time dependence. The complex constant, which encapsulates amplitude and phase dependence, is known as phasor, complex amplitude, and (in older texts) sinor or even complexor.\n\nA common situation in electrical networks is the existence of multiple sinusoids all with the same frequency, but different amplitudes and phases. The only difference in their analytic representations is the complex amplitude (phasor). A linear combination of such functions can be factored into the product of a linear combination of phasors (known as phasor arithmetic) and the time/frequency dependent factor that they all have in common.\n\nThe origin of the term phasor rightfully suggests that a (diagrammatic) calculus somewhat similar to that possible for vectors is possible for phasors as well. An important additional feature of the phasor transform is that differentiation and integration of sinusoidal signals (having constant amplitude, period and phase) corresponds to simple algebraic operations on the phasors; the phasor transform thus allows the analysis (calculation) of the AC steady state of RLC circuits by solving simple algebraic equations (albeit with complex coefficients) in the phasor domain instead of solving differential equations (with real coefficients) in the time domain. The originator of the phasor transform was Charles Proteus Steinmetz working at General Electric in the late 19th century.\n\nGlossing over some mathematical details, the phasor transform can also be seen as a particular case of the Laplace transform, which additionally can be used to (simultaneously) derive the transient response of an RLC circuit. However, the Laplace transform is mathematically more difficult to apply and the effort may be unjustified if only steady state analysis is required.\n\nEuler's formula indicates that sinusoids can be represented mathematically as the sum of two complex-valued functions:\n\nor as the real part of one of the functions:\n\nThe function formula_3 is called the \"analytic representation\" of formula_4. Figure 2 depicts it as a rotating vector in a complex plane. It is sometimes convenient to refer to the entire function as a \"phasor\", as we do in the next section. But the term \"phasor\" usually implies just the static vector formula_5. An even more compact representation of a \"phasor\" is the angle notation: formula_6. See also vector notation.\n\nMultiplication of the phasor  formula_7 by a complex constant,  formula_8 , produces another phasor. That means its only effect is to change the amplitude and phase of the underlying sinusoid:\n\nIn electronics, formula_8  would represent an impedance, which is independent of time. In particular it is \"not\" the shorthand notation for another phasor. Multiplying a phasor current by an impedance produces a phasor voltage. But the product of two phasors (or squaring a phasor) would represent the product of two sinusoids, which is a non-linear operation that produces new frequency components. Phasor notation can only represent systems with one frequency, such as a linear system stimulated by a sinusoid.\n\nThe time derivative or integral of a phasor produces another phasor. For example:\n\nTherefore, in phasor representation, the time derivative of a sinusoid becomes just multiplication by the constant formula_12.\n\nSimilarly, integrating a phasor corresponds to multiplication by formula_13. The time-dependent factor, formula_14, is unaffected.\n\nWhen we solve a linear differential equation with phasor arithmetic, we are merely factoring formula_14 out of all terms of the equation, and reinserting it into the answer. For example, consider the following differential equation for the voltage across the capacitor in an RC circuit:\n\nWhen the voltage source in this circuit is sinusoidal:\n\nwe may substitute formula_18\n\nwhere phasor formula_20, and phasor formula_21 is the unknown quantity to be determined.\n\nIn the phasor shorthand notation, the differential equation reduces to\n\nSince this must hold for all formula_23, specifically: formula_24, it follows that\n\nIt is also readily seen that\n\nSubstituting these into   and  , multiplying   by formula_27  and adding both equations gives\n\nSolving for the phasor capacitor voltage gives\n\nAs we have seen, the factor multiplying formula_32 represents differences of the amplitude and phase of formula_33  relative to formula_34  and formula_35.\n\nIn polar coordinate form, it is\n\nTherefore\n\nThe sum of multiple phasors produces another phasor. That is because the sum of sinusoids with the same frequency is also a sinusoid with that frequency:\nwhere\n\nor, via the law of cosines on the complex plane (or the trigonometric identity for angle differences):\nwhere formula_42.\n\nA key point is that \"A\" and \"θ\" do not depend on \"ω\" or \"t\", which is what makes phasor notation possible. The time and frequency dependence can be suppressed and re-inserted into the outcome as long as the only operations used in between are ones that produce another phasor. In angle notation, the operation shown above is written\n\nAnother way to view addition is that two vectors with coordinates and are added vectorially to produce a resultant vector with coordinates . (see animation)\nIn physics, this sort of addition occurs when sinusoids interfere with each other, constructively or destructively. The static vector concept provides useful insight into questions like this: \"What phase difference would be required between three identical sinusoids for perfect cancellation?\" In this case, simply imagine taking three vectors of equal length and placing them head to tail such that the last head matches up with the first tail. Clearly, the shape which satisfies these conditions is an equilateral triangle, so the angle between each phasor to the next is 120° ( radians), or one third of a wavelength . So the phase difference between each wave must also be 120°, as is the case in three-phase power\n\nIn other words, what this shows is that\n\nIn the example of three waves, the phase difference between the first and the last wave was 240 degrees, while for two waves destructive interference happens at 180 degrees. In the limit of many waves, the phasors must form a circle for destructive interference, so that the first phasor is nearly parallel with the last. This means that for many sources, destructive interference happens when the first and last wave differ by 360 degrees, a full wavelength formula_45. This is why in single slit diffraction, the minima occur when light from the far edge travels a full wavelength further than the light from the near edge.\n\nAs the single vector rotates in an anti-clockwise direction, its tip at point A will rotate one complete revolution of 360° or 2 radians representing one complete cycle. If the length of its moving tip is transferred at different angular intervals in time to a graph as shown above, a sinusoidal waveform would be drawn starting at the left with zero time. Each position along the horizontal axis indicates the time that has elapsed since zero time, \"t\" = 0. When the vector is horizontal the tip of the vector represents the angles at 0°, 180°, and at 360°.\n\nLikewise, when the tip of the vector is vertical it represents the positive peak value, ( +\"A\" ) at 90° or and the negative peak value, ( −\"A\" ) at 270° or . Then the time axis of the waveform represents the angle either in degrees or radians through which the phasor has moved. So we can say that a phasor represent a scaled voltage or current value of a rotating vector which is “frozen” at some point in time, ( \"t\" ) and in our example above, this is at an angle of 30°.\n\nSometimes when we are analysing alternating waveforms we may need to know the position of the phasor, representing the alternating quantity at some particular instant in time especially when we want to compare two different waveforms on the same axis. For example, voltage and current. We have assumed in the waveform above that the waveform starts at time \"t\" = 0 with a corresponding phase angle in either degrees or radians.\n\nBut if a second waveform starts to the left or to the right of this zero point, or if we want to represent in phasor notation the relationship between the two waveforms, then we will need to take into account this phase difference, of the waveform. Consider the diagram below from the previous Phase Difference tutorial.\n\nWith phasors, the techniques for solving DC circuits can be applied to solve AC circuits. A list of the basic laws is given below.\n\n\nGiven this we can apply the techniques of analysis of resistive circuits with phasors to analyze single frequency AC circuits containing resistors, capacitors, and inductors. Multiple frequency linear AC circuits and AC circuits with different waveforms can be analyzed to find voltages and currents by transforming all waveforms to sine wave components with magnitude and phase then analyzing each frequency separately, as allowed by the superposition theorem.\n\nIn analysis of three phase AC power systems, usually a set of phasors is defined as the three complex cube roots of unity, graphically represented as unit magnitudes at angles of 0, 120 and 240 degrees. By treating polyphase AC circuit quantities as phasors, balanced circuits can be simplified and unbalanced circuits can be treated as an algebraic combination of symmetrical circuits. This approach greatly simplifies the work required in electrical calculations of voltage drop, power flow, and short-circuit currents. In the context of power systems analysis, the phase angle is often given in degrees, and the magnitude in rms value rather than the peak amplitude of the sinusoid.\n\nThe technique of synchrophasors uses digital instruments to measure the phasors representing transmission system voltages at widespread points in a transmission network. Differences among the phasors indicate power flow and system stability.\n\nThe rotating frame picture using phasor can be a powerful tool to understand analog modulations such as amplitude modulation (and its variants \n) and frequency modulation.\n\nformula_46, where the term in brackets is viewed as a rotating vector in the complex plane.\n\nThe phasor has length formula_47, rotates anti-clockwise at a rate of formula_48 revolutions per second, and at time formula_49 makes an angle of formula_50 with respect to the positive real axis. \n\nThe waveform formula_51 can then be viewed as a projection of this vector onto the real axis.\n\n\n\n\n"}
{"id": "589149", "url": "https://en.wikipedia.org/wiki?curid=589149", "title": "Poire z", "text": "Poire z\n\npoire_z (pronounced \"pwar-zed\") was an electronic free improvisation music group formed in 1998. The group's members all have long careers in improvised music; critic Fred Grand of \"Avant\" calls poire_z a \"post-AMM supergroup.\"\n\nAt a music festival at Nantes, France in 1998, percussionist Gunter Muller and turntablist Erikm were scheduled to perform as a duo. The same festival featured a performance by duo Voice Crack. The four musicians made an unscheduled collaboration, and afterwards formed a semi-regular quartet.\n\nThey recorded several albums and made a number of live appearances before disbanding. poire_z often invited a fifth musician as a guest: Keith Rowe, Phil Minton, Christian Marclay, Otomo Yoshihide, Sachiko M and others performed or recorded with the core quartet.\n\nAs with much free improvisation, poire_z's music is generally focused more on a group sound, and given the musicians' unorthodox approach to their instruments, it's often difficult to discern who is generating any individual sound. Aside from glimpses of Muller's occasionally unprocessed percussion, their music is nearly all electronic blips, clicks and droning like a shortwave radio tuned in to an extraterrestrial broadcast; critic Ed Howard described the group's music as \"all whir and purr and throb, the soldered electronic exclamations of the Voice Crack duo spurting around the steady rhythmic drive of Muller and eRikm ... a dense cycling drone with harsh electronic interjections skipping over the top.\" One piece is described as \"delicate tones, crackles and light bubbling sounds, resting and intermingling with characteristic weightlessness, only occasionally being interrupted by glimpses of harsher textures.\"\n\nVoice Crack formally disbanded in about 2004, and poire_z have not recorded or performed since.\n\n\n"}
{"id": "1201641", "url": "https://en.wikipedia.org/wiki?curid=1201641", "title": "Pulse generator", "text": "Pulse generator\n\nA pulse generator is either an electronic circuit \"or\" a piece of electronic test equipment used to generate rectangular pulses. Pulse generators are used primarily for working with digital circuits, related function generators are used primarily for analog circuits.\n\nSimple bench pulse generators usually allow control of the pulse repetition rate (frequency), pulse width, delay with respect to an internal or external trigger and the high- and low-voltage levels of the pulses. More-sophisticated pulse generators may allow control over the rise time and fall time of the pulses. Pulse generators are available for generating output pulses having widths (duration) ranging from minutes down to under 1 picosecond.\nPulse generators are generally voltage sources, with true current pulse generators being available only from a few suppliers. \nPulse generators may use digital techniques, analog techniques, or a combination of both techniques to form the output pulses. For example, the pulse repetition rate and duration may be digitally controlled but the pulse amplitude and rise and fall times may be determined by analog circuitry in the output stage of the pulse generator. With correct adjustment, pulse generators can also produce a 50% duty cycle square wave. Pulse generators are generally single-channel providing one frequency, delay, width and output.\nLight pulse generators are the optical equivalent to electrical pulse generators with rep rate, delay, width and amplitude control. The output in this case is light typically from a LED or laser diode. One example is the Model 6040\n\nA new family of pulse generators can produce multiple-channels of independent widths and delays and independent outputs and polarities. Often called digital delay/pulse generators, the newest designs even offer differing repetition rates with each channel. These digital delay generators are useful in synchronizing, delaying, gating and triggering multiple devices usually with respect to one event. One is also able to multiplex the timing of several channels onto one channel in order to trigger or even gate the same device multiple times. \n\nA new class of pulse generator offers both multiple input trigger connections and multiple output connections. Multiple input triggers allows experimenters to synchronize both trigger events and data acquisition events using the same timing controller. \n\nIn general, generators for pulses with widths over a few microseconds employ digital counters for timing these pulses, while widths between approximately 1 nanosecond and several microseconds are typically generated by analog techniques such as RC (resistor-capacitor) networks or switched delay lines.\n\nPulse generators capable of generating pulses with widths under approximately 100 picoseconds are often termed as \"microwave pulsers\" and typically generate these ultra-short pulses using Step recovery diode (SRD) or Nonlinear Transmission Line (NLTL) methods (for example ). Step Recovery Diode pulse generators are inexpensive but typically require several volts of input drive level and have a moderately high level of random jitter (usually undesirable variation in the time at which successive pulses occur). \n\nNLTL-based pulse generators generally have lower jitter, but are more complex to manufacture and do not suit integration in low-cost monolithic ICs. A new class of microwave pulse generation architecture, the RACE (Rapid Automatic Cascode Exchange) pulse generation circuit , is implemented using low-cost monolithic IC technology and can produce pulses as short as 1 picosecond, and with repetition rates exceeding 30 billion pulses per second. These pulsers are typically used in military communications applications, and low-power microwave transceiver ICs. Such pulsers, if driven by a continuous frequency clock, will act as microwave comb generators, having output frequency components at integer multiples of the pulse repetition rate, and extending to well over 100 gigahertz (for example ).\n\nPulses can then be injected into a device that is under test and used as a stimulus or clock signal or analyzed as they progress through the device, confirming the proper operation of the device or \npinpointing a fault in the device. Pulse generators are also used to drive devices such as switches, lasers and optical components, modulators, intensifiers as well as resistive loads.The output of a pulse generator may also be used as the modulation signal for a signal generator. Non-electronic applications include those in material science, medical, physics and chemistry.\n\n"}
{"id": "31259060", "url": "https://en.wikipedia.org/wiki?curid=31259060", "title": "Radiation material science", "text": "Radiation material science\n\nRadiation materials science describes the interaction of radiation with matter: a broad subject covering many forms of irradiation and of matter.\n\nSome of the most profound effects of irradiation on materials occur in the core of nuclear power reactors where atoms comprising the structural components are displaced numerous times over the course of their engineering lifetimes. The consequences of radiation to core components includes changes in shape and volume by tens of percent, increases in hardness by factors of five or more, severe reduction in ductility and increased embrittlement, and susceptibility to environmentally induced cracking. For these structures to fulfill their purpose, a firm understanding of the effect of radiation on materials is required in order to account for irradiation effects in design, to mitigate its effect by changing operating conditions, or to serve as a guide for creating new, more radiation-tolerant materials that can better serve their purpose.\n\nThe types of radiation that can alter structural materials consist of neutrons, ions, electrons and gamma rays. All of these forms of radiation have the capability to displace atoms from their lattice sites, which is the fundamental process that drives the changes in structural metals.The inclusion of ions among the irradiating particles provides a tie-in to other fields and disciplines such as the use of accelerators for the transmutation of nuclear waste, or in the creation of new materials by ion implantation, ion beam mixing, plasma assisted ion implantation and ion beam assisted deposition.\n\nThe effect of irradiation on materials is rooted in the initial event in which an energetic projectile strikes a target. While the event is made up of several steps or processes, the primary result is the displacement of an atom from its lattice site. Irradiation displaces an atom from its site, leaving a vacant site behind (a vacancy) and the displaced atom eventually comes to rest in a location that is between lattice sites, becoming an interstitial atom. The vacancy-interstitial pair is central to radiation effects in crystalline solids and is known as a Frenkel pair (FP). The presence of the Frenkel pair and other consequences of irradiation damage determine the physical effects, and with the application of stress, the mechanical effects of irradiation by the occurring of interstitial, phenomena, such as swelling, growth, phase transition, segregation, etc., will be effected. In addition to the atomic displacement, an energetic charged particle moving in a lattice also gives energy to electrons in the system, via the electronic stopping power. This energy transfer can also for high-energy particles produce damage in non-metallic materials, as so called ion tracks.\n\nThe radiation damage event is defined as the transfer of energy from an incident projectile to the solid and the resulting distribution of target atoms after completion of the event. This event is composed of several distinct processes:\n\nThe result of a radiation damage event is, if the energy given to a lattice atom is above the threshold displacement energy, the creation of a collection of point defects (vacancies and interstitials) and clusters of these defects in the crystal lattice.\n\nThe essence of the quantification of radiation damage in solids is the number of displacements per unit volume per unit time formula_1 :\nwhere formula_3 is the atom number density, formula_4 and formula_5 are the maximum and minimum energies of the incoming particle, formula_6 is the energy dependent particle flux, formula_7 and formula_8 are the maximum and minimum energies transferred in a collision of a particle of energy formula_9 and a lattice atom, formula_10 is the cross section for the collision of a particle of energy formula_9 that results in a transfer of energy formula_12 to the struck atom, formula_13 is the number of displacements per primary knock-on atom.\n\nThe two key variables in this equation are formula_10 and formula_13. The term formula_10 describes the transfer of energy from the incoming particle to the first atom it encounters in the target, the primary knock-on atom (PKA); The second quantity formula_13 is the total number of displacements that the PKA goes on to make in the solid; Taken together, they describe the total number of displacements caused by an incoming particle of energy formula_9, and the above equation accounts for the energy distribution of the incoming particles. The result is the total number of displacements in the target from a flux of particles with a known energy distribution.\n\nIn radiation material Science the displacement damage in the alloy ( formula_19 = displacements per atom in the solid ) is a better representation of the effect of irradiation on materials properties than the fluence ( neutron fluence, formula_20 ).\n\nTo generate materials that fit the increasing demands of nuclear reactors to operate with higher efficiency or for longer lifetimes, materials must be designed with radiation resistance in mind. In particular, Generation IV nuclear reactors operate at higher temperatures and pressures compared to modern pressurized water reactors, which account for a vast amount of western reactors. This leads to increased vulnerability to normal mechanical failure in terms of creep resistance as well as radiation damaging events such as neutron-induced swelling and radiation induced segregation of phases. By accounting for radiation damage, reactor materials would be able to withstand longer operating lifetimes. This allows reactors to be decommissioned after longer periods of time, improving return on investment of reactors without compromising safety. This is of particular interest in developing commercial viability of advanced and theoretical nuclear reactors, and this goal can be accomplished through engineering resistance to these displacement events.\n\nFace centered cubic (FCC) metals such as austenitic steels and Ni-base alloys can benefit greatly from grain boundary engineering. Grain boundary engineering attempts to generate higher amounts of special grain boundaries, characterized by favorable orientations between grains. By increasing populations of low energy boundaries without increasing grain size, fracture mechanics of these FCC metals can be changed to improve mechanical properties given a similar displacements per atom (DPA) value versus non grain boundary engineered alloys. This method of treatment in particular yields better resistance to stress corrosion cracking and oxidation.\n\nBy using advanced methods of material selection, materials can be judged on criteria such as neutron-absorption cross sectional area. Selecting materials with minimum neutron-absorption can heavily minimize the number of DPA that occur over a reactor material's lifetime. This slows the radiation embrittlement process by preventing mobility of atoms in the first place, proactively selecting materials that do not interact with the nuclear radiation as frequently. This can have a huge impact on total damage especially when comparing the materials of modern advanced reactors of zirconium to stainless steel reactor cores, which can differ in absorption cross section by an order of magnitude from more optimal materials.\n\nExample values for thermal neutron cross section are shown in the table below.\nFor nickel-chromium and iron-chromium alloys, short range order can be designed on the nano-scale (<5 nm) that absorbs the interstitial and vacancy's generated by PKA events. This allows materials that mitigate the swelling that normally occurs in the presence of high DPAs and keep the overall volume percent change under the ten percent range. This occurs through generating a metastable phase that is in constant, dynamic equilibrium with surrounding material. This metastable phase is characterized by having an enthalpy of mixing that is effectively zero with respect to the main lattice. This allows phase transformation to absorb and disperse the point defects that typically accumulate in more rigid lattices. This extends the life of the alloy through making vacancy and interstitial creation less successful as constant neutron excitement in the form of displacement cascades transform the SRO phase, while the SRO reforms in the bulk solid solution.\n\n"}
{"id": "62437", "url": "https://en.wikipedia.org/wiki?curid=62437", "title": "SCADA", "text": "SCADA\n\nSupervisory control and data acquisition (SCADA) is a control system architecture that uses computers, networked data communications and graphical user interfaces for high-level process supervisory management, but uses other peripheral devices such as programmable logic controller (PLC) and discrete PID controllers to interface with the process plant or machinery. The operator interfaces that enable monitoring and the issuing of process commands, such as controller set point changes, are handled through the SCADA computer system. However, the real-time control logic or controller calculations are performed by networked modules that connect to the field sensors and actuators.\n\nThe SCADA concept was developed as a universal means of remote access to a variety of local control modules, which could be from different manufacturers allowing access through standard automation protocols. In practice, large SCADA systems have grown to become very similar to distributed control systems in function, but using multiple means of interfacing with the plant. They can control large-scale processes that can include multiple sites, and work over large distances as well as small distance. It is one of the most commonly-used types of industrial control systems, however there are concerns about SCADA systems being vulnerable to cyberwarfare/cyberterrorism attacks.\n\nThe key attribute of a SCADA system is its ability to perform a supervisory operation over a variety of other proprietary devices.\n\nThe accompanying diagram is a general model which shows functional manufacturing levels using computerised control.\n\nReferring to the diagram,\n\n\nLevel 1 contains the programmable logic controllers (PLCs) or remote terminal units (RTUs).\n\nLevel 2 contains the SCADA software and computing platform. The SCADA software exists only at this supervisory level as control actions are performed automatically by RTUs or PLCs. SCADA control functions are usually restricted to basic overriding or supervisory level intervention. For example, a PLC may control the flow of cooling water through part of an industrial process to a set point level, but the SCADA system software will allow operators to change the set points for the flow. The SCADA also enables alarm conditions, such as loss of flow or high temperature, to be displayed and recorded. A feedback control loop is directly controlled by the RTU or PLC, but the SCADA software monitors the overall performance of the loop.\n\nLevels 3 and 4 are not strictly process control in the traditional sense, but are where production control and scheduling takes place.\n\nData acquisition begins at the RTU or PLC level and includes instrumentation readings and equipment status reports that are communicated to level 2 SCADA as required. Data is then compiled and formatted in such a way that a control room operator using the HMI (Human Machine Interface) can make supervisory decisions to adjust or override normal RTU (PLC) controls. Data may also be fed to a historian, often built on a commodity database management system, to allow trending and other analytical auditing.\n\nSCADA systems typically use a \"tag database\", which contains data elements called \"tags\" or \"points\", which relate to specific instrumentation or actuators within the process system according to such as the Piping and instrumentation diagram. Data is accumulated against these unique process control equipment tag references.\n\nBoth large and small systems can be built using the SCADA concept. These systems can range from just tens to thousands of control loops, depending on the application. Example processes include industrial, infrastructure, and facility-based processes, as described below:\n\n\nHowever, SCADA systems may have security vulnerabilities, so the systems should be evaluated to identify risks and solutions implemented to mitigate those risks.\n\nA SCADA system usually consists of the following main elements:\n\nThis is the core of the SCADA system, gathering data on the process and sending control commands to the field connected devices. It refers to the computer and software responsible for communicating with the field connection controllers, which are RTUs and PLCs, and includes the HMI software running on operator workstations. In smaller SCADA systems, the supervisory computer may be composed of a single PC, in which case the HMI is a part of this computer. In larger SCADA systems, the master station may include several HMIs hosted on client computers, multiple servers for data acquisition, distributed software applications, and disaster recovery sites. To increase the integrity of the system the multiple servers will often be configured in a dual-redundant or hot-standby formation providing continuous control and monitoring in the event of a server malfunction or breakdown.\n\nRemote terminal units, also known as (RTUs), connect to sensors and actuators in the process, and are networked to the supervisory computer system. RTUs are \"intelligent I/O\" and often have embedded control capabilities such as ladder logic in order to accomplish boolean logic operations.\n\nAlso known as PLCs, these are connected to sensors and actuators in the process, and are networked to the supervisory system in the same way as RTUs. PLCs have more sophisticated embedded control capabilities than RTUs, and are programmed in one or more IEC 61131-3 programming languages. PLCs are often used in place of RTUs as field devices because they are more economical, versatile, flexible and configurable.\n\nThis connects the supervisory computer system to the RTUs and PLCs, and may use industry standard or manufacturer proprietary protocols.\nBoth RTU's and PLC's operate autonomously on the near-real time control of the process, using the last command given from the supervisory system. Failure of the communications network does not necessarily stop the plant process controls, and on resumption of communications, the operator can continue with monitoring and control. Some critical systems will have dual redundant data highways, often cabled via diverse routes.\n\nThe human-machine interface (HMI) is the operator window of the supervisory system. It presents plant information to the operating personnel graphically in the form of mimic diagrams, which are a schematic representation of the plant being controlled, and alarm and event logging pages. The HMI is linked to the SCADA supervisory computer to provide live data to drive the mimic diagrams, alarm displays and trending graphs. In many installations the HMI is the graphical user interface for the operator, collects all data from external devices, creates reports, performs alarming, sends notifications, etc.\n\nMimic diagrams consist of line graphics and schematic symbols to represent process elements, or may consist of digital photographs of the process equipment overlain with animated symbols.\n\nSupervisory operation of the plant is by means of the HMI, with operators issuing commands using mouse pointers, keyboards and touch screens. For example, a symbol of a pump can show the operator that the pump is running, and a flow meter symbol can show how much fluid it is pumping through the pipe. The operator can switch the pump off from the mimic by a mouse click or screen touch. The HMI will show the flow rate of the fluid in the pipe decrease in real time.\n\nThe HMI package for a SCADA system typically includes a drawing program that the operators or system maintenance personnel use to change the way these points are represented in the interface. These representations can be as simple as an on-screen traffic light, which represents the state of an actual traffic light in the field, or as complex as a multi-projector display representing the position of all of the elevators in a skyscraper or all of the trains on a railway.\n\nA \"historian\", is a software service within the HMI which accumulates time-stamped data, events, and alarms in a database which can be queried or used to populate graphic trends in the HMI. The historian is a client that requests data from a data acquisition server.\n\nAn important part of most SCADA implementations is alarm handling. The system monitors whether certain alarm conditions are satisfied, to determine when an alarm event has occurred. Once an alarm event has been detected, one or more actions are taken (such as the activation of one or more alarm indicators, and perhaps the generation of email or text messages so that management or remote SCADA operators are informed). In many cases, a SCADA operator may have to acknowledge the alarm event; this may deactivate some alarm indicators, whereas other indicators remain active until the alarm conditions are cleared.\n\nAlarm conditions can be explicit—for example, an alarm point is a digital status point that has either the value NORMAL or ALARM that is calculated by a formula based on the values in other analogue and digital points—or implicit: the SCADA system might automatically monitor whether the value in an analogue point lies outside high and low- limit values associated with that point.\n\nExamples of alarm indicators include a siren, a pop-up box on a screen, or a coloured or flashing area on a screen (that might act in a similar way to the \"fuel tank empty\" light in a car); in each case, the role of the alarm indicator is to draw the operator's attention to the part of the system 'in alarm' so that appropriate action can be taken.\n\n\"Smart\" RTUs, or standard PLCs, are capable of autonomously executing simple logic processes without involving the supervisory computer. They employ standardized control programming languages such as under, IEC 61131-3 (a suite of 5 programming languages including function block, ladder, structured text, sequence function charts and instruction list), is frequently used to create programs which run on these RTUs and PLCs. Unlike a procedural language such as the C programming language or FORTRAN, IEC 61131-3 has minimal training requirements by virtue of resembling historic physical control arrays. This allows SCADA system engineers to perform both the design and implementation of a program to be executed on an RTU or PLC.\n\nA programmable automation controller (PAC) is a compact controller that combines the features and capabilities of a PC-based control system with that of a typical PLC. PACs are deployed in SCADA systems to provide RTU and PLC functions. In many electrical substation SCADA applications, \"distributed RTUs\" use information processors or station computers to communicate with digital protective relays, PACs, and other devices for I/O, and communicate with the SCADA master in lieu of a traditional RTU.\n\nSince about 1998, virtually all major PLC manufacturers have offered integrated HMI/SCADA systems, many of them using open and non-proprietary communications protocols. Numerous specialized third-party HMI/SCADA packages, offering built-in compatibility with most major PLCs, have also entered the market, allowing mechanical engineers, electrical engineers and technicians to configure HMIs themselves, without the need for a custom-made program written by a software programmer.\nThe Remote Terminal Unit (RTU) connects to physical equipment. Typically, an RTU converts the electrical signals from the equipment to digital values such as the open/closed status from a switch or a valve, or measurements such as pressure, flow, voltage or current. By converting and sending these electrical signals out to equipment the RTU can control equipment, such as opening or closing a switch or a valve, or setting the speed of a pump.\n\nSCADA systems have traditionally used combinations of radio and direct wired connections, although SONET/SDH is also frequently used for large systems such as railways and power stations. The remote management or monitoring function of a SCADA system is often referred to as telemetry. Some users want SCADA data to travel over their pre-established corporate networks or to share the network with other applications. The legacy of the early low-bandwidth protocols remains, though.\n\nSCADA protocols are designed to be very compact. Many are designed to send information only when the master station polls the RTU. Typical legacy SCADA protocols include Modbus RTU, RP-570, Profibus and Conitel. These communication protocols, with the exception of Modbus (Modbus has been made open by Schneider Electric), are all SCADA-vendor specific but are widely adopted and used. Standard protocols are IEC 60870-5-101 or 104, IEC 61850 and DNP3. These communication protocols are standardized and recognized by all major SCADA vendors. Many of these protocols now contain extensions to operate over TCP/IP. Although the use of conventional networking specifications, such as TCP/IP, blurs the line between traditional and industrial networking, they each fulfill fundamentally differing requirements. Network simulation can be used in conjunction with SCADA simulators to perform various 'what-if' analyses.\n\nWith increasing security demands (such as North American Electric Reliability Corporation (NERC) and critical infrastructure protection (CIP) in the US), there is increasing use of satellite-based communication. This has the key advantages that the infrastructure can be self-contained (not using circuits from the public telephone system), can have built-in encryption, and can be engineered to the availability and reliability required by the SCADA system operator. Earlier experiences using consumer-grade VSAT were poor. Modern carrier-class systems provide the quality of service required for SCADA.\n\nRTUs and other automatic controller devices were developed before the advent of industry wide standards for interoperability. The result is that developers and their management created a multitude of control protocols. Among the larger vendors, there was also the incentive to create their own protocol to \"lock in\" their customer base. A list of automation protocols is compiled here.\n\nOLE for process control (OPC) can connect different hardware and software, allowing communication even between devices originally not intended to be part of an industrial network.\n\nStandardisation in the field of mySCADA protocols resulted into the vendor independent protocol called OPC UA (Unified Architecture). OPC UA is starting to be widely adopted among multiple SCADA vendors. \n\nSCADA systems have evolved through four generations as follows:\n\nEarly SCADA system computing was done by large minicomputers. Common network services did not exist at the time SCADA was developed. Thus SCADA systems were independent systems with no connectivity to other systems. The communication protocols used were strictly proprietary at that time. The first-generation SCADA system redundancy was achieved using a back-up mainframe system connected to all the Remote Terminal Unit sites and was used in the event of failure of the primary mainframe system. Some first generation SCADA systems were developed as \"turn key\" operations that ran on minicomputers such as the PDP-11 series made by the Digital Equipment Corporation..\n\nSCADA information and command processing was distributed across multiple stations which were connected through a LAN. Information was shared in near real time. Each station was responsible for a particular task, which reduced the cost as compared to First Generation SCADA. The network protocols used were still not standardized. Since these protocols were proprietary, very few people beyond the developers knew enough to determine how secure a SCADA installation was. Security of the SCADA installation was usually overlooked.\n\nSimilar to a distributed architecture, any complex SCADA can be reduced to the simplest components and connected through communication protocols. In the case of a networked design, the system may be spread across more than one LAN network called a process control network (PCN) and separated geographically. Several distributed architecture SCADAs running in parallel, with a single supervisor and historian, could be considered a network architecture. This allows for a more cost-effective solution in very large scale systems.\n\nWith the commercial availability of cloud computing, SCADA systems have increasingly adopted Internet of things technology to significantly improve interoperability, reduce infrastructure costs and increase ease of maintenance and integration. As a result, SCADA systems can now report state in near real-time and use the horizontal scale available in cloud environments to implement more complex control algorithms than are practically feasible to implement on traditional programmable logic controllers. Further, the use of open network protocols such as TLS inherent in the Internet of things technology, provides a more readily comprehensible and manageable security boundary than the heterogeneous mix of proprietary network protocols typical of many decentralized SCADA implementations.\n\nThis decentralization of data also requires a different approach to SCADA than traditional PLC based programs. When a SCADA system is used locally, the preferred methodology involves binding the graphics on the user interface to the data stored in specific PLC memory addresses. However, when the data comes from a disparate mix of sensors, controllers and databases (which may be local or at varied connected locations), the typical 1 to 1 mapping becomes problematic. A solution to this is data modeling, a concept derived from object oriented programming.\n\nIn a data model, a virtual representation of each device is constructed in the SCADA software. These virtual representations (“models”) can contain not just the address mapping of the device represented, but also any other pertinent information (web based info, database entries, media files, etc.) that may be used by other facets of the SCADA/IoT implementation. As the increased complexity of the Internet of things renders traditional SCADA increasingly “house-bound,” and as communication protocols evolve to favor platform-independent, service-oriented architecture (such as OPC UA), it is likely that more SCADA software developers will implement some form of data modeling.\n\nSCADA systems that tie together decentralized facilities such as power, oil, gas pipelines, water distribution and wastewater collection systems were designed to be open, robust, and easily operated and repaired, but not necessarily secure. The move from proprietary technologies to more standardized and open solutions together with the increased number of connections between SCADA systems, office networks and the Internet has made them more vulnerable to types of network attacks that are relatively common in computer security. For example, United States Computer Emergency Readiness Team (US-CERT) released a vulnerability advisory warning that unauthenticated users could download sensitive configuration information including password hashes from an Inductive Automation Ignition system utilizing a standard attack type leveraging access to the Tomcat Embedded Web server. Security researcher Jerry Brown submitted a similar advisory regarding a buffer overflow vulnerability in a Wonderware InBatchClient ActiveX control. Both vendors made updates available prior to public vulnerability release. Mitigation recommendations were standard patching practices and requiring VPN access for secure connectivity. Consequently, the security of some SCADA-based systems has come into question as they are seen as potentially vulnerable to cyber attacks.\n\nIn particular, security researchers are concerned about\n\nSCADA systems are used to control and monitor physical processes, examples of which are transmission of electricity, transportation of gas and oil in pipelines, water distribution, traffic lights, and other systems used as the basis of modern society. The security of these SCADA systems is important because compromise or destruction of these systems would impact multiple areas of society far removed from the original compromise. For example, a blackout caused by a compromised electrical SCADA system would cause financial losses to all the customers that received electricity from that source. How security will affect legacy SCADA and new deployments remains to be seen.\n\nThere are many threat vectors to a modern SCADA system. One is the threat of unauthorized access to the control software, whether it is human access or changes induced intentionally or accidentally by virus infections and other software threats residing on the control host machine. Another is the threat of packet access to the network segments hosting SCADA devices. In many cases, the control protocol lacks any form of cryptographic security, allowing an attacker to control a SCADA device by sending commands over a network. In many cases SCADA users have assumed that having a VPN offered sufficient protection, unaware that security can be trivially bypassed with physical access to SCADA-related network jacks and switches. Industrial control vendors suggest approaching SCADA security like Information Security with a defense in depth strategy that leverages common IT practices.\n\nThe reliable function of SCADA systems in our modern infrastructure may be crucial to public health and safety. As such, attacks on these systems may directly or indirectly threaten public health and safety. Such an attack has already occurred, carried out on Maroochy Shire Council's sewage control system in Queensland, Australia. Shortly after a contractor installed a SCADA system in January 2000, system components began to function erratically. Pumps did not run when needed and alarms were not reported. More critically, sewage flooded a nearby park and contaminated an open surface-water drainage ditch and flowed 500 meters to a tidal canal. The SCADA system was directing sewage valves to open when the design protocol should have kept them closed. Initially this was believed to be a system bug. Monitoring of the system logs revealed the malfunctions were the result of cyber attacks. Investigators reported 46 separate instances of malicious outside interference before the culprit was identified. The attacks were made by a disgruntled ex-employee of the company that had installed the SCADA system. The ex-employee was hoping to be hired by the utility full-time to maintain the system.\n\nIn April 2008, the Commission to Assess the Threat to the United States from Electromagnetic Pulse (EMP) Attack issued a Critical Infrastructures Report which discussed the extreme vulnerability of SCADA systems to an electromagnetic pulse (EMP) event. After testing and analysis, the Commission concluded: \"SCADA systems are vulnerable to an EMP event. The large numbers and widespread reliance on such systems by all of the Nation’s critical infrastructures represent a systemic threat to their continued operation following an EMP event. Additionally, the necessity to reboot, repair, or replace large numbers of geographically widely dispersed systems will considerably impede the Nation’s recovery from such an assault.\"\n\nMany vendors of SCADA and control products have begun to address the risks posed by unauthorized access by developing lines of specialized industrial firewall and VPN solutions for TCP/IP-based SCADA networks as well as external SCADA monitoring and recording equipment.\nThe International Society of Automation (ISA) started formalizing SCADA security requirements in 2007 with a working group, WG4. WG4 \"deals specifically with unique technical requirements, measurements, and other features required to evaluate and assure security resilience and performance of industrial automation and control systems devices\".\n\nThe increased interest in SCADA vulnerabilities has resulted in vulnerability researchers discovering vulnerabilities in commercial SCADA software and more general offensive SCADA techniques presented to the general security community. In electric and gas utility SCADA systems, the vulnerability of the large installed base of wired and wireless serial communications links is addressed in some cases by applying bump-in-the-wire devices that employ authentication and Advanced Encryption Standard encryption rather than replacing all existing nodes.\n\nIn June 2010, anti-virus security company VirusBlokAda reported the first detection of malware that attacks SCADA systems (Siemens' WinCC/PCS 7 systems) running on Windows operating systems. The malware is called Stuxnet and uses four zero-day attacks to install a rootkit which in turn logs into the SCADA's database and steals design and control files. The malware is also capable of changing the control system and hiding those changes. The malware was found on 14 systems, the majority of which were located in Iran.\n\nIn October 2013 \"National Geographic\" released a docudrama titled \"American Blackout\" which dealt with an imagined large-scale cyber attack on SCADA and the United States' electrical grid.\n\n\n"}
{"id": "254938", "url": "https://en.wikipedia.org/wiki?curid=254938", "title": "Scissors", "text": "Scissors\n\nScissors are hand-operated shearing tools. A pair of scissors consists of a pair of metal blades pivoted so that the sharpened edges slide against each other when the handles (bows) opposite to the pivot are closed. Scissors are used for cutting various thin materials, such as paper, cardboard, metal foil, cloth, rope, and wire. A large variety of scissors and shears all exist for specialized purposes. Hair-cutting shears and kitchen shears are functionally equivalent to scissors, but the larger implements tend to be called shears. Hair-cutting shears have specific blade angles ideal for cutting hair. Using the incorrect scissors to cut hair will result in increased damage or split ends, or both, by breaking the hair. Kitchen shears, also known as kitchen scissors, are intended for cutting and trimming foods such as meats.\n\nModern scissors are often designed ergonomically with composite thermoplastic and rubber handles which enable the user to exert either a power grip or a precision grip.\n\nThe noun \"scissors\" is treated as a plural noun, and therefore takes a plural verb (e.g., \"these scissors are\"). Alternatively, the tool is referred to by the singular phrase \"a pair of scissors\". The word \"shears\" is used to describe similar instruments that are larger in size and for heavier cutting.\n\nThe earliest known scissors appeared in Mesopotamia 3,000 to 4,000 years ago. These were of the 'spring scissor' type comprising two bronze blades connected at the handles by a thin, flexible strip of curved bronze which served to hold the blades in alignment, to allow them to be squeezed together, and to pull them apart when released.\n\nSpring scissors continued to be used in Europe until the 16th century. However, pivoted scissors of bronze or iron, in which the blades were pivoted at a point between the tips and the handles, the direct ancestor of modern scissors, were invented by the Romans around 100 AD. They entered common use in not only ancient Rome, but also China, Japan, and Korea, and the idea is still used in almost all modern scissors.\nDuring the Middle Ages and Renaissance, spring scissors were made by heating a bar of iron or steel, then flattening and shaping its ends into blades on an anvil. The center of the bar was heated, bent to form the spring, then cooled and reheated to make it flexible.\n\nThe Hangzhou Zhang Xiaoquan Company in Hangzhou, China, has been manufacturing scissors since 1663.\n\nWilliam Whiteley & Sons (Sheffield) Ltd. was producing scissors by 1760, although it is believed the business began trading even earlier. The first trade-mark, 332, was granted in 1791. The company is still manufacturing scissors today, and is the oldest company in the West to do so.\n\nPivoted scissors were not manufactured in large numbers until 1761, when Robert Hinchliffe of Sheffield produced the first pair of modern-day scissors made of hardened and polished cast steel. His major challenge was to form the bows; first he made them solid, then drilled a hole, and then filed away metal to make this large enough to admit the user's fingers. This process was laborious, and apparently Hinchliffe improved upon it in order to increase production. Hinchliffe lived in Cheney Square (now the site of Sheffield Town Hall), and set up a sign identifying himself as a \"fine scissor manufacturer\". He achieved strong sales in London and elsewhere.\n\nDuring the 19th century, scissors were hand-forged with elaborately decorated handles. They were made by hammering steel on indented surfaces known as 'bosses' to form the blades. The rings in the handles, known as bows, were made by punching a hole in the steel and enlarging it with the pointed end of an anvil.\n\nIn 1649, in Swedish-ruled Finland, an ironworks was founded in the village of Fiskars between Helsinki and Turku. In 1830, a new owner started the first cutlery works in Finland, making, among other items, scissors with the Fiskars trademark.\n\nA pair of scissors consists of two pivoted blades. In lower-quality scissors, the cutting edges are not particularly sharp; it is primarily the shearing action between the two blades that cuts the material. In high-quality scissors, the blades can be both extremely sharp, and tension sprung – to increase the cutting and shearing tension only at the exact point where the blades meet. The hand movement (pushing with the thumb, pulling with the fingers) can add to this tension. An ideal example is in high-quality tailor's scissors or shears, which need to be able to perfectly cut (and not simply tear apart) delicate cloths such as chiffon and silk.\n\nChildren's scissors are usually not particularly sharp, and the tips of the blades are often blunted or 'rounded' for safety.\n\nMechanically, scissors are a first-class double-lever with the pivot acting as the fulcrum. For cutting thick or heavy material, the mechanical advantage of a lever can be exploited by placing the material to be cut as close to the fulcrum as possible. For example, if the applied force (at the handles) is twice as far away from the fulcrum as the cutting location (i.e., the point of contact between the blades), the force at the cutting location is twice that of the applied force at the handles. Scissors cut material by applying at the cutting location a local shear stress which exceeds the material's shear strength.\n\nSome scissors have an appendage, called a finger brace or finger tang, below the index finger hole for the middle finger to rest on to provide for better control and more power in precision cutting. A finger tang can be found on many quality scissors (including inexpensive ones) and especially on scissors for cutting hair (see hair scissors pictured below). In hair cutting, some claim the ring finger is inserted where some place their index finger, and the little finger rests on the finger tang.\n\nFor people who do not have the use of their hands, there are specially designed foot-operated scissors. Some quadriplegics can use a motorized mouth-operated style of scissor.\n\nMost scissors are best-suited for use with the right hand, but \"left-handed\" scissors are designed for use with the left hand. Because scissors have overlapping blades, they are not symmetric. This asymmetry is true regardless of the orientation and shape of the handles: the blade that is on top always forms the same diagonal regardless of orientation. Human hands are also asymmetric, and when closing, the thumb and fingers do not close vertically, but have a lateral component to the motion. Specifically, the thumb pushes out from the palm and the fingers pull inwards. For right-handed scissors held in the right hand, the thumb blade is closer to the user's body, so that the natural tendency of the right hand is to force the cutting blades together. Conversely, if right-handed scissors are held in the left hand, the natural tendency of the left hand would be to force the cutting blades laterally apart. Furthermore, with right-handed scissors held by the right hand, the shearing edge is visible, but when they are used with the left hand, the cutting edge of the scissors is behind the top blade, and one cannot see what is being cut.\n\nSome scissors are marketed as ambidextrous. These have symmetric handles so there is no distinction between the thumb and finger handles, and have very strong pivots so that the blades simply rotate and do not have any lateral give. However, most \"ambidextrous\" scissors are in fact still right-handed in that the upper blade is on the right, and hence is on the outside when held in the right hand. Even if they cut successfully, the blade orientation will block the view of the cutting line for a left-handed person. True ambidextrous scissors are possible if the blades are double-edged and one handle is swung all the way around (to almost 360 degrees) so that the back of the blades become the new cutting edges. has been awarded for true ambidextrous scissors.\n\nAmong specialized scissors and shears used for different purposes are:\n\nDue to their ubiquity across cultures and classes, scissors have numerous representations across world culture.\n\nNumerous forms of art worldwide enlist scissors as a tool/material with which to accomplish the art. For cases where scissors appear in or are represented by the final art product, see .\n\n\nThe game Rock-paper-scissors involves two or more players making shapes with their hands to determine the outcome of the game. One of the three shapes, 'scissors', is made by extending the index and middle fingers to mimic the shape of most scissors.\n\nIn the horror game series, Clock Tower, there is a character called Scissorman, who has a variety of identities as the main antagonist throughout the series. Scissorman is a demonic serial killer with a giant pair of scissors and will kill anyone without showing even a sign of mercy or remorse.\n\nAugusten Burroughs' 2002 memoir \"Running with Scissors\" spent eight weeks on the \"New York Times\" best seller list. The book was later adapted into a film.\n\n\nThe term 'scissor kick' may be found in several sports, including:\n\nScissors have a widespread place in cultural superstitions. In many cases, the specifics of the superstition may be specific to a given country, region, tribe, religion or even situation.\n\nScissors have been used in the sciences for various purposes, including descriptions of animals or natural features.\n\nAnimals named after scissors include:\n\n"}
{"id": "36539671", "url": "https://en.wikipedia.org/wiki?curid=36539671", "title": "Tam O'Shaughnessy", "text": "Tam O'Shaughnessy\n\nTam Elizabeth O'Shaughnessy (born January 27, 1952) is an American children's science writer and former professional tennis player who co-founded, with Sally Ride, the science education company Sally Ride Science. The company was relaunched as a nonprofit entity, Sally Ride Science at UC San Diego, on October 1, 2015. O’Shaughnessy serves as executive director.\n\nO'Shaughnessy was born in San Andreas, California and attended Troy High School in Fullerton, California, where she was active in tennis. As a junior player, she was coached by Billie Jean King.\n\nO’Shaughnessy went on to play on the women's professional tennis circuit from 1971 to 1974. She competed in the U.S. National Championships (now known as the U.S. Open) in 1966, 1970, and 1972. Her entry into the 1966 U.S. National Championships at the age of 14 came about by serendipity.\n\nO’Shaughnessy was being coached by Dr. Robert Walter Johnson, a physician who played a key role in the tennis careers of Althea Gibson and Arthur Ashe. Johnson was an official of the American Tennis Association (ATA), an organization that promotes tennis for African Americans but welcomes players of all backgrounds. During the summer of 1966, O’Shaughnessy, who is not African American, competed in ATA tournaments in addition to U.S. Tennis Association junior events. O’Shaughnessy won the ATA national 18-and-under championship and so was automatically entered in the U.S. National Championship draw.\n\nO’Shaughnessy also competed in the 1972 Wimbledon Championships. During her tennis career, she was ranked as high as No. 52 in the world in women's singles by the Women's Tennis Association and as high as No. 3 in the U.S. in women's doubles (with Ann Lebedeff) by the USTA. O’Shaughnessy won national hard-court doubles titles in the junior division (with Ann Lebedeff) and in the women's division (with Pam Austin).\n\nAfter retiring from tennis, O’Shaughnessy was the founding publisher of the Women's Tennis Association newsletter for several years before going to college to study biology.\n\nO’Shaughnessy earned B.S. and M.S. degrees in biology from Georgia State University and a Ph.D. in school psychology from the University of California, Riverside. She was assistant professor of school psychology at Georgia State University from 1998 through 2001, and then associate professor of school psychology at San Diego State University from 2002 until 2007. O’Shaughnessy's research on preventive interventions for children with reading difficulties was continuously funded by the U.S. Department of Education starting in graduate school. She retired early to devote her time and energy to Sally Ride Science, and was named associate professor emeritus at San Diego State University.\n\nO’Shaughnessy has extensive experience cultivating girls’ and boys’ interest in reading, math, and science. Besides being a former science teacher, she is an award-winning writer of science books for children. O’Shaughnessy has written 12 children's science books, including six with Sally Ride, the first American woman in space. Ride and O’Shaughnessy's clear and eloquent writing style earned them many accolades, including the American Institute of Physics Children's Science Writing Award in 1995 for their second book, \"The Third Planet: Exploring the Earth From Space\". In October 2015, O’Shaughnessy published a children's biography of Ride, \"Sally Ride: A Photobiography of America’s Pioneering Woman in Space\". The book combines reminiscences from Ride's family and friends with dozens of photos, including many never-before-published family and personal photos.\n\nAs a scientist and educator, O’Shaughnessy became deeply concerned about the underrepresentation of women in science and technical professions. Research shows that young girls like science and have the same aptitude for it as boys, but in adolescence, girls tend to drift away from science, in part because of subtle stereotypes and lack of role models. In 2001, Ride, O’Shaughnessy, and three like-minded friends—Karen Flammer, Terry McEntee, and Alann Lopes—founded Sally Ride Science with the goal of narrowing the gender gap in science.\nFrom 2001 to 2015, O’Shaughnessy served as the company's chief creative officer, overseeing all content—books, websites, and teacher training curricula. She guided the creation of the \"Cool Careers in STEM\" and \"Key Concepts in Science\" programs, which combine professional development for teachers with student books and teacher guides. O’Shaughnessy also served as chief operating officer of Sally Ride Science from 2009 through 2013, chairman of the board of directors from 2013 to 2015, and chief executive officer from 2014 to 2015.\n\nSally Ride Science was acquired by the University of California, San Diego, in October 2015. O’Shaughnessy is executive director of the resulting nonprofit entity, Sally Ride Science at UC San Diego, which will create new educational programs and make use of existing Sally Ride Science programs.\n\nO’Shaughnessy was the romantic partner of astronaut Sally Ride, the first American woman in space, from 1985 until Ride's death in 2012. O’Shaughnessy and Ride also were business partners in Sally Ride Science, and they wrote children's science books together.\n\n\n"}
{"id": "26419860", "url": "https://en.wikipedia.org/wiki?curid=26419860", "title": "Test 219", "text": "Test 219\n\nTest 219 was a nuclear test conducted by the Soviet Union in the atmosphere via ICBM. The test was performed on December 24, 1962 over the Novaya Zemlya test range. It was a thermonuclear fusion bomb with a yield of about 24.2 megatons and a destruction radius of about six miles, making it the second largest thermonuclear explosion in history behind Tsar Bomba.\n"}
{"id": "7944529", "url": "https://en.wikipedia.org/wiki?curid=7944529", "title": "Threshing board", "text": "Threshing board\n\nA threshing board is an obsolete farm implement used to separate cereals from their straw; that is, to thresh. It is a thick board, made with a variety of slats, with a shape between rectangular and trapezoidal, with the frontal part somewhat narrower and curved upward (like a sled or sledge) and whose bottom is covered with lithic flakes or razor-like metal blades.\n\nOne form, once common in the Mediterranean Sea area, was \"about three to four feet wide and six feet deep (these dimensions often vary, however), consisting of two or three wooden planks assembled to one another, of more than four inches wide, in which several hard and cutting flints crammed into the bottom part pull along over the grains. In the rear part there is a large ring nailed, that is used to tie the rope that pulls it and to which two horses are usually harnessed; and a person, sitting on the threshing board, drives it in circles over the cereal that is spread on the threshing floor. Should the person need more weight, he need only put some big stones over it.\"\n\nThe dimensions of threshing boards varied. In Spain, they could be up to approximately two metres in length and a metre and a half wide. There were also smaller threshing boards, as little about a metre-and-a-half long and a metre wide. The thickness of the slats of the threshing board is some five or six cm. Nonetheless, since threshing boards are nowadays custom made, made to order or made smaller as an adornment or souvenir, they may range from miniatures up to the sizes previously described.\n\nThe threshing board has been traditionally pulled by mules or by oxen over the grains spread on the threshing floor. As it was moved in circles over the harvest that was spread, the stone chips or blades cut the straw and the ear of wheat (which remained between the threshing board and the pebbles on the ground), thus separating the seed without damaging it. The threshed grain was then gathered and set to be cleaned by some means of winnowing.\n\nUntil the arrival of combine harvesters, which reap, thresh and clean grain in a single process, the traditional methods of threshing cereals and some legumes were those described by Pliny the Elder in his \"Natural History\", with three variants: \"The cereals are threshed in some places with the threshing board on the threshing floor; in others they are trampled by a train of horses, and in others they are beaten with flails\".\n\nIn this manner, Pliny refers to the three traditional methods of threshing grain:\n\nThe threshing board is a historical form of threshing that can still be seen in some regions that practise a marginal agriculture. It is also somewhat preserved as an occasional folkloric and ceremonial practice, to commemorate traditional local customs.\n\nFor threshing with the threshing board, first one brings the baled stalks to the threshing floor. Some are stacked, waiting their turn, and others are untied and placed in a circle forming a pile of grain that is heated by the sun. Then, the farmers drag the threshing board over the stalks, first going several times around in circles, and then in figure-eights, and stirring the grain with a wooden pitchfork. Sometimes, this work was done with another kind of threshing implement: a \"Plostellum punicum\" (Latin; literally \"Punic cart\") or threshing cart, fitted with a group of rollers, each with metallic transverse razors. In this first stage, the straw is detached from the ear; much chaff and dirty dust remains, mixed with the edible grain. Every time that the work of dragging the threshing board is repeated, the grain is stirred again, moving more straw to the edge of the threshing floor. If too much grain is spread on the ground, it has to be raked and swept in order to make a round mound and, if possible, to remove as much straw as possible.\n\nAfter turning the grain and straw upside down and leaving it to dry during a lunch break, the farmers carry out a second round of threshing in order to separate the last of the grain from the straw. Then, they use pitchforks, rakes and brooms to create a mound. A pair of oxen or mules pulls the threshing board by means of a chain or a strap fixed to a hook nailed in the front plank; donkeys are not used, because unlike mules and oxen they often defecate on the crops. The driver rides on the threshing board, both guiding the draft animals and increasing the weight of the threshing board. If the driver's weight is not enough, large stones are put on the board. In recent times, the animals are sometimes replaced with a tractor; because the driver no longer sits on the board, the weight of stones becomes more important. Children enjoy riding on the threshing board for fun, and the farmers allow it because their weight is useful, as long as the children are not too boisterous. During this process, if the stalks are excessively squashed, two large metal arcs are affixed to the back of the threshing board; these turn-up and give volume to the straws, behind the threshing. \n\nAfter threshing is finished, to avoid mixing the dirty remnants with the clean, new stalks, the threshing floor must be cleaned first with a rake to move the heavier material, and then with several brooms (in the narrow sense: they are typically made from \"broom shrub\"—\"Cytisus scoparius\"—and are stronger than domestic brooms). Also the straw was accumulated carefully and stored, because it was a good fodder for livestock. The entire process of threshing generates a thin dust that soaks in through the respiratory system and sticks to the throat.\n\nDuring the sweeping, the husks and the chaff are separated to one part of the threshing-floor, while the grain, still not entirely clean, was winnowed, either by traditional of winnowing with sieves, or by a mechanical winnowing machine.\n\nTraditional threshing implements (including the threshing board) were gradually abandoned and replaced by modern combine harvesters. This change, of course, occurred in some areas before others. For instance, in Spain, it happened in the 1950s and 1960s.\nUntil that time, threshing boards were made in certain particular towns and villages with specialised craftsmen. Whereas the woodworking involved is simple, even rough, the flintknapping and the inlaying of flakes into the bottom of board need specialised skills that were passed from father to son. The most famous Spanish town for this work was, doubtlessly, Cantalejo (Segovia), where the craftsmen who made threshing boards were known as \"briqueros\".\n\nPatricia C. Anderson (of \"Centre d’Etudes Préhistoire, Antiquité et Moyen Age del CNRS\"), discovered archaeological remains that demonstrate the existence of threshing boards at least 8,000 years old in the Near East and Balkans. The artefacts are lithic flakes and, above all obsidian or flint blades, recognizable through the type of microscopic wear that it has. Her work was completed by Jacques Chabot (of the \"Centre interuniversitaire d'études sur les lettres, les arts et les traditions\", CELAT), who has studied Mitanni (northern Mesopotamia and Armenia). Both count among their specialties the study of microwear analysis, through which it is possible to take a particular piece of flint or obsidian (to take the most common examples) and determine the tasks for which it was used.\nConcretely, the cutting of cereals leaves a very characteristic \"glossy\" pattern of wear, owing to the presence of microscopic mineral particles (\"phytoliths\") in the stalks of the plants. Therefore, scholars using controlled experimental replication studies and functional analysis with a scanning electron microscope are able to identify stone artefacts that were used as sickles or the teeth of threshing boards. The edge damage on the pieces used in threshing boards is distinct because, besides the \"glossy\" abrasion characteristic of cutting cereals, they have micro-scars from chipping, as a result of the blows of the threshing board against the rock surface of the threshing floor.\n\nThe most productive archaeological site is Aratashen, Armenia: a village occupied between 5000 and 3000 BC (Neolithic and Copper Age). The archaeological excavations have provided thousands of pieces from the knapping of obsidian (suggesting that Aratashen was a centre of production and trade of artefacts of that highly regarded stone); the rest of the archaeological record consists mainly of fragments of common pottery, ground stones, and other agricultural tools. Analysing a sample of 200 lithic flakes and blades, selected from the best-preserved pieces, it is possible to differentiate between those used in sickles and those used in threshing boards. The lithic blades of obsidian were knapped using highly developed and standardized methods, such as the use of a \"pectoral crutch\" with a copper point. Beginning at the headwaters of the river Euphrates, where this site is located, the craftsmen and peddlers sold their wares throughout the Middle East.\n\nThe threshing boards must have been important in the protohistory of Mesopotamia, since they already appear in some of the oldest written documents discovered: specifically, several sandstone tablets from the early town of Kish (Iraq), engraved with cuneiform pictograms, which could be the world's oldest surviving written record, dating to the middle of the 4th millennium BC (Early Uruk period). One of these tablets, preserved in the Ashmolean Museum of Oxford University, appears to have pictures of threshing boards on both faces, next to some numeric symbols and other pictograms. These presumed threshing boards (which might instead be sledges) have a shape similar to threshing carts that were used until recently in parts of the Middle East where non-industrial agriculture survived. Descriptions also appear in numerous cuneiform clay tablets as early as the third millennium BC.\n\nThere are another representation, in this case without writing, in central Turkey. It is an impression of a cylinder seal from the archaeological site of Arslantepe-Malatya, which appeared near of the named «Temple B». The archaeological layers were dated to 3374 BC using dendrochronology. The stamp shows a figure seated on a threshing board, with a clear image of the lithic flakes inlaid in the bottom of the board. The main figure is sitting (possibly on a throne) under a dossal. In front is a driver or oxherd, and there are peasants with pitchforks nearby. According to M. A. Frangipane, that the seal may illustrate a religious scene:\n\nIt closely resembles another scene, painted on the walls of the same site (a ceremonial procession of a person of high rank, painted in an archaic lineal style in the colours red and black), although the current condition of the wall obscures the exact nature of the vehicle in which he is seated, it is indeed possible to see that it is pulled by a pair of oxen. Professor Sherratt interprets both scenes as presenting manifestations of civil or religious power. In that era, the threshing board was a sophisticated and expensive implement, built by specialized artisans, using pieces of flint or obsidian; in the case of Lower Mesopotamia, these were imported from far away: in the alluvial plateau of Sumer, as in all south of Mesopotamia, it was impossible to find stone, not even a pebble.\n\nFurthermore, threshing required a threshing floor composed of natural rock, cobbles or, at least, very hard and compressed ground, located in a sunny place situated in a rise in the ground with a constant dry wind, and with a flat base that would not allow puddles to form when it rained. So, a threshing floor was not available to just anybody. It was expensive, as we can see from the biblical citations in the following section. Also, it required draft animals, expensive and difficult to drive (because this was not a matter of having them walk in a straight line). All this meant that a threshing implement required a large amount of harvested grain to pay off the expenditure. Thus, the rise of the threshing board turns on a distinctive, sophisticated system of powerful elites.\n\nThe discovery of a ceremonial sledge (perhaps a threshing board) with gold ornaments in the Tomb of Pu-Abi, one of the \"royal tombs\" of Ur, dated from the 3rd millennium BC, makes clear the underlying problem of distinguishing in the ancient representations between a true threshing board and a sledge (that is, an unwheeled vehicle for hauling freight). Although we know that the threshing board appears no later than the 4th millennium BC (as we can see in \"Atarashen\" and \"Arslantepe-Malatya\"), and we also know that the wheel was invented in Mesopotamia in the middle of that same millennium, still, the utilisation and spread of the wheel was not instantaneous. The sledges survived at least until the invention of the articulated axle, nearly 2000 BC. During this time, some vehicles were hybrids: sledges with wheels that could be dismantled to overcome obstacles by carrying it on shoulders or, simply, dragging it. Consequently—except in the case of Arslantepe, where the lithic flakes are clear visible—we cannot determine whether these representations are threshing boards or sledges for freight or for rites.\n\nJames Frazer compiled numerous ceremonies of harvest and thresh, that centered on a \"Cereal Spirit\". From the Ancient Egyptian era to pre-industrial period, this spirit seems to have reside d in the first threshed sheaf or, sometimes, in the last one.\n\nThe first biblical mention of the threshing floor is in . As such, it was not a shed, building, or any place covered with a roof and surrounded by walls, but a circular piece of ground from fifty to a hundred feet in diameter, in the open air, on elevated ground, and made smooth, hardy, and clean. Here the grain was threshed and winnowed.\n\nIn the Bible are found four modes of threshing grain (some of which modes are expounded in ):\nAll four methods are discussed at length in the Talmud.\nTwo apparently coincident descriptive narratives are given in and with regard to King David's purchase of the threshing floor on Mount Moriah (as well as Mount Moriah itself). In it, the Lord's directive to Gad, King David's prophet, was to instruct David to \"rear an altar unto the Lord in the threshingfloor of Araunah the Jebusite\" ( and ) during the end of the three-day plague that was then ensuing upon David's people of Israel. In the 1Chronicles account, the purchase of the entire hilltop of Mount Moriah is the subject, which is why the purchase price (verse 25) is different from the 2 Samuel account.\n\nThis selection is highly significant for several reasons:\n\nThe selected place, Mount Moriah, was owned by Araunah (or, \"Ornan\") the Jebusite, a rich villager who owned and operated the threshing floor on Mount Moriah. Araunah was awed by the visit of the king and offered his oxen to sacrifice and the threshing boards and other implements as wood for the sacrifice. Even so, David rejected any gift to come from a pagan and instead affirmed his intent to purchase first the oxen and threshing floor, then, later, the whole of Mount Moriah that contained it. Had David accepted Araunah's offer, it would have been Araunah's sacrifice instead of David's.\n\nThe references to the purchase are ostensively contradictory about the purchase price; it is 50 shekels of silver according to , and 600 shekels of gold according to . However, this contradiction is only on the surface. The 50 shekels was the initial price to purchase the two oxen and the threshing floor, but the later price of 600 shekels was paid to purchase the property of Mount Moriah \"in addition\" to the oxen and threshing floor \"contained within\" and \"earlier purchased\".\n\nGenerally, all threshing floors were located on hilltops, not to associate with the various gods, but to expose them to prevailing winds that assist in the process of winnowing. The wind adds to the typology of the threshing floor, as the wind is a type of the Holy Ghost (see ).\n\nThe last biblical mention of threshing floors is in and . The term \"floor\" is the Greek \"halon\" which means threshing floor, and this fits the import of the two verses. The typology of the threshing floor on Mount Moriah, Solomon's Temple, and these two verses are significant.\n\nAs for the word \"thresh,\" the last biblical mention is in , again showing the typology of separating the chaff from the wheat.\n\nDuring the early history of Greece and Rome the threshing board was not used. Only after the development of commerce (with occurred in the 5th and 4th centuries in Greece and 2nd and 1st centuries in Rome) and the subsequent transmission of information from the near east that it became widely used. According to V.V. Struve, who cites, in part, verses of The Iliad, the Greeks of the 8th century BC threshed cereals by trampling them with oxen:\nCarthage, which colonized the southeastern Iberian Peninsula in the 3rd century BC, had advanced agricultural technology, greatly superior to the Roman techniques of the time. Their methods astonished travellers such as Agathocles and Regulus, and were an inspiration for the writings of Varro and Pliny. One well-known Carthaginian agronomist, \"Mago\", wrote a treatise that was translated into Latin by order of the Roman Senate. The ancient Romans describe Tunisia, today mainly desert, as a fertile landscape of olive groves and wheat fields. In Hispania, the Carthaginians are known to have introduced several new crops (mainly fruit trees) and some machines like the threshing board, either the version with stone-chips (\"tribulum\" in Latin) or the version with rollers (\"threshing cart\", named in their honour \"plostellum punicum\" by the Romans).\n\nIn Rome, the threshing board had only economic significance, without the religious symbolism it took on in ancient Israel. The treatises of agriculture written by Roman experts as Cato, Varro, Columella and Pliny the Elder (quoted above), touch the topic of threshing. In chronological order:\n\n\n\nWe will speak of the Middle Ages in a broad sense, without entering into great detail and focusing, essentially, on Western Europe because it is difficult to find any trustworthy documents about threshing boards in that era. The barbarian invasions of Europe had detrimental effects upon agriculture as well as upon the general economy, leading to the loss of many of the more advanced techniques, among them the threshing board, which was completely alien to Germanic tradition. The eastern Mediterranean areas, on the other hand, continued the use of the threshing board, passing into the Muslim culture, where it took deep root.\n\nIn the Iberian Peninsula, in the Visigothic kingdom and the Christian zone during the \"Reconquista\", the threshing board was little known (although awareness of it never quite disappeared). The degradation extended not only to the economy, but also to the very sources that we have to study the period: scholars are confronted with a documentary void that is difficult to get around. It is certain that in Islamic Al-Andalus, the threshing board continued to be very popular, which led to the Christians recuperating the tradition as they advanced in the \"Reconquista.\" This act coincides with a generalized recovery in all of Europe. Economic prosperity began to return at the start of the 11th century; the experts speak of the increased area of tilled lands; the increased use of draft animals (first oxen, thanks to the frontal yoke, and later horses, thanks to harness collar); of the increase in metal tools and improved metalworking; of the appearance of the moldboard plough, often with wheels; and the increase in watermills. Livestock became a sign of progress: peasants, less dependent and more prosperous, became able to buy draft animals, and even plows. The peasants who had their own plow and one or two draft animals were a small elite, pampered by the feudal lord, who acquired a distinct status,\n\nthat of \"yeoman farmers\", quite distinct from \"farm-hand labourer\" whose only tool was their own arms. The existence of draft animals does not imply the diffusion of the threshing board in Western Europe, where the flail continued to be the preferred threshing implement. On the other hand, in Spain, the weight of Eastern tradition made the difference: Professor Julio Caro Baroja admits that in Spain the threshing board appears cited or represented in works of art. Concretely, he mentions some Romanesque reliefs in Beleña (Salamanca) and Campisábalos (Guadalajara), both from the 12th century. One may add a document written in 1265, in which a woman named Doña Mayor (widow of one Don Arnal, an ecclesiastical tax collector, and thus a person of good position), leaves to the diocese chapter of Salamanca her inheritance of \"Valcuevo\", a farm in the municipality of Valverdón, Salamanca:\nAnd I, Doña Mayor, leave on my death these two yokings of country state above referred to the Cathedral Chapter, well preserved with 76 bushels of wheat, 38 bushels of rye, and 38 bushels of barley to seed each yoking and with ploughshares, and with plows, and with ploughbeams, and with threshing boards and with all the accoutrements that a well-laid-out property should have.\n\nThese documents, at least testify for the presence of threshing boards, which, undoubtedly was continuous from then until recently in the Mediterranean basin. The rest is mere generalized speculation, given that traditional historiography centers on features more like of Western Europe. In any case, none of the consulted authors describe the threshing board as playing a relevant part in the progress of medieval agriculture. We must, then, join with the despair of French historian Georges Duby, in his complaint:\nThrough all that has been said, we can see how interesting it would be to measure the influence of technical progress on agricultural output. Nevertheless, we must renounce this hope. Before the end of the 12th century, the methods of seignorial administration were all quite primitive; they granted little importance to writing and even less to numbers. The documents are more deceptive than those of the Carolingian era.\"\n\nNowadays, numerous elements of traditional agriculture are being lost, and because of this various entities have been working to conserve or recover this cultural capital. Among these is an international interdisciplinary project called E.A.R.T.H., Early Agricultural Remnants and Technical Heritage. Participating countries include Bulgaria, Canada, France, Russia, Scotland, Spain, and the United States, in alphabetical order. Investigations center on broad archeological, documentary and ethnological aspects, related to diverse elements of traditional agriculture, threshing boards among them, in diverse countries, historical periods and societies.\n\nCantalejo is located at the confluence of the Duraton River and the Cega River. Modern Cantalejo arose in the 11th century, although there are architectural remains that are much older, and forms part of Segovia province in Spain. It was apparently prosperous at the beginning but, lost its liberty in the 17th Century and became a jurisdictional lordship. There is no clear record of when the specialized craft of making threshing boards was introduced into Cantalejo, but all who have written on the topic indicate that it was probably during the 16th or 17th century.\n\nThe production of threshing boards appears to coincide with the arrival of foreign artisans, although this is speculation based on the fact that the artisans' form of speech includes aspects of many foreign languages, especially French. The makers of threshing boards and plows were called \"«Briqueros»\", a word of French origin that refers to the making of tinderboxes and matchlocks for guns, which in France were called \"briquets\", literally in archaic French: \"«Petite pièce d’acier don't on se sert pour tirer du feu d’un caillou.»\" (today, synonym of briquette or flammable matter, but also refers to a lighter) for the early firearms (Flintlocks, arquebuses, muskets...). It is therefore plausible that foreign gunsmiths introducing firearms in the Iberian Peninsula created an important colony in Cantalejo, although it is difficult to determine their origin\n\nIn any case, with time Cantalejo opted for peaceful and productive crafts such as the making of diverse agricultural implements, including threshing boards. By the 1950s, Cantalejo had reached 400 workshops producing more than 30,000 threshing boards each year; this suggests that more than half the population was dedicated to this occupation. The threshing boards were then distributed throughout the entire Meseta Central of Spain.\n\nThis article concentrates on threshing boards made only with flint chips, although there were threshing boards made with metal teeth or according to other, less common designs.\n\nAt the end of summer, or in the autumn, work began with the selection of black pines, which they cut and carefully smoothed with a device called a \"tronzador\" until the trunks were formed into cylinders nearly two meters in length; these log sections were called \"tozas\". They also prepared long, straight planks to serve as transverse headpieces. They carried the log sections to the sawmill, where they cut slats as wide as the log permitted (no less than 20 centimeters), of some five centimeters in thickness and with a curved shape (just like a ski) on the end that would eventually be in front. The planks were dried in the sun for several months, turned over every so often. The village during this period took on a peculiar appearance, as many buildings had their façades covered with drying planks. Afterwards, these were piled in \"castillos\", crossing some slats with others in order to make the pile more stable.\n\nOnce the slat was in the right conditions, the next process was chiseling: using a hammer and chisel to prepare the slots (\"ujeros\") for the chips of flint (\"chinas\" or lithic flakes). The chiseling was done with the slats front-on, guided by pencil-marks so the workman wouldn't err. Before beginning, the workman made sure that the slat had not warped since being cut, as that would make it unusable.\n\nThe next pass took place in the mechanical presses; the Spanish language term for these was \"cárceles\", \"jails\". The three, four, or five planks had to be perfectly joined: spread with glue and pressed, using small reinforcement jigs, called \"tasillos\" (wooden cylinders glued and nailed with a mallet at the edges of a slat), and wedges. When the slats were well-aligned and fixed in place, the \"cabezales\" (headers), or crosspieces, were nailed in place with big nails known in Spain as \"puntas de París\" (although, at least in the 19th and 20th centuries, they came from Bilbao).\n\nOnce the basic structure of the threshing board is ready, it must be smoothed. This is first done by \"working\" it with an adze lengthwise, along the grain. Then the final finishing is done with various specialized carpenter's planes, on both top and bottom; first going across in a transverse direction, and later lengthwish.\n\nThe final phase of the work consisted of covering the junctions of the slats on the top side, which is done with thin strips at the front, tacked on with a board called the \"front piece\", and on the rest using long thin little boards (\"tapajuntas\" or stopgaps). On the front header they attached a strong hook for the \"barzón\", or iron ring with a strap or a long rod for tying on the drafthorses or oxen.\n\nTo create the lithic flakes used to cover threshing boards, the \"briqueros\" in Cantalejo used a manufacturing technique similar to prehistoric methods of making tools, except that they used metal hammers rather than percussors made of stone, wood, or antler.\n\nThe raw materials preferred by these artisans was a whitish flint imported from the province of Guadalajara. When they had to repair threshing boards at home, if they did not have any other raw materials, they would use rounded river pebbles, made of homogeneous, high-grade quartzite, which they selected during their travels. The flint from Guadalajara was extracted from quarries in large blocks, which were split by hand with hammers of various sizes until the stone reached a size small enough to be comfortably held in the hand.\n\nKnapping: Once manageable chunks of flint were obtained, knapping to obtain lithic flakes was performed using a very light hammer (called a \"pickaxe\") with a narrow handle and a pointed head. Knapping was considered \"men's work.\" To work quartzite pebbles, they used a hammer with a head that was rounded and slightly wider. During the process of removing stone flakes, they sometimes resorted to a mormal hammer to crack the stone and achieve perussion plains inaccessible with the pickaxe alone.\n\nThe \"briquero\" held the stone core in the left hand, protected with a piece of leather and with the palm upward, and struck rapid blows using a pick held in the right hand. The stone flakes would fall into the palm of the right hand, on top of the leather protector, which allowed the worker to evaluate them during fractions of a second: if they were acceptable, the \"briquero\" allowed them to fall into a tin; if not, he threw them into a reject pile. This pile was also where the \"briquero\" threw used-up stone cores —that is, stone blocks incapable of producing more chips; pieces of stone broken by accident; cortical flint flakes, useless fragments, and debris.\n\nThe working of pebbles was similar to the working of flint, except that with pebbles only the outside layer was chipped off. Thus, the pebbles were essentially \"peeled\" and discarded (unlike flint stones, whose interiors were worked until they were used up); using only the cortical flakes.\n\nCovering the board with stone flakes was mainly the work of women called \"enchifleras\". The task is monotonous and repetitive. Up to three thousand lithic flakes may be pounded into a large threshing board. In addition, it is necessary to sort the flakes: small ones in the front, medium-sized in the middle, and the largest on the sides and in the back. It is necessary to pound in each flake without damaging its sharp edge, although it was impossible to avoid leaving at least some small mark (a \"spontaneous retouch\" in technical terms). The tool used was a light hammer with a cylindrical head and flat or concave ends. The flakes are inserted into the cracks at their thickest part (technically, the percussion zone, that is, the proximal heel of the flake as it is struck off.)\n\nThreshing boards from Cantalejo captured nearly all the sales in Castile and León, Madrid, Castile-La Mancha, Aragon and Valencia. They sometimes also reached Andalusia y Cantabria. At first, artesans from Cantalejo travelled with large carts loaded with selected threshing boards, winnowing bellows, grain measures (of different traditional dry units: \"celemín\" is a wooden case with 4,6 L, \"cuartilla\" has 14 L, and \"fanega\" equivalent to 55.5 L...) and other implements for threshing or winnowing, which they peddled from town to town. They also carried flint chips, and tools and supplies for repairing damaged threshing boards and farming implements. In later times, they traveled by train to pre-arranged stations, and then in small trucks. They typically brought their entire families along; combined with their strange manner of speaking and unusual occupation, this gave the \"briqueros\" an air of mystery. They began selling threshing boards as soon as the threshing boards were complete, beginning in April and lasting until August. The \"briqueros\" would then return to their home town (the \"Vilorio\") to celebrate the festivals of the Assumption of Mary (August 15) and Saint Roch (August 16) with their families.\n\n\"Gacería\" is a slang or argot used by makers and vendors of threshing boards in Cantalejo and some other parts of Spain. It is not a technical vocabulary, but rather a code made up of a small group of words that allows the speakers to communicate freely in the presence of strangers without others understanding the content of the conversation.\n\n\"Gacería\" was purely verbal, colloquial, and associated with the selling of threshing boards; as a result, it largely disappeared with the mechanization of agriculture. Nevertheless, a number of studies have attempted to record its varied aspects. There are many doubts regarding the origin of the words that make up the vocabulary of \"Gacería\", including the word \"Gacería\" itself, which may derive from the Basque word \"gazo,\" which means \"ugly\" or \"good-for-nothing\". The most commonly accepted opinion is that most of the words derive from French, with additions from other languages including Latin, Basque, Arabic, German, and even Caló. What is certain is that the makers and vendors of threshing boards took words from any area they visited regularly, creating a linguistic mishmash.\n\nIn general, the term \"threshing board\" is used to refer to all the different variants of this primitive implement. Technically, we should distinguish at least the two main types of threshing boards: the \"threshing sledge,\" which is the subject of this article, and the \"threshing cart.\"\n\nThe \"threshing sledge\" is the most common type. As its name indicates, it is dragged over the ripe grain, and it threshes using cutting pieces made of stone or metal. This is what is referred to in Hebrew as \"morag\" (מורג) and in Arabic as \"mowrej.\" Strictly speaking, the threshing boards of the Middle East have characteristics that render them easily distinguishable from those found in Europe. On the Iberian peninsula, cutting blades found on the bottom part of the threshing board are arranged on end and in rows roughly parallel to the direction of threshing. In contrast, the \"mogag\" and \"mowrej\" found to the Middle East have circular holes (made with a special short, wide drill) into which are pressed small round, semicircular stones with sharp ridges. (See a detail photo of a Middle-Eastern threshing board).\n\nAs mentioned previously, not all threshing boards are equipped with stones: some have metal knife blades embedded along the full length of the threshing board, and others have smaller blades encrusted here and there. Threshing sledges with metal knives usually have a few small wheels (four to six, depending on the size) with eccentric axes. These wheels protect the blades. They also make the threshing board wobble, causing parts of the board to rise and fall at random in an oscillating motion that improves the effectiveness of the threshing.\n\n</div>\n\nA second model, which the classic sources refer to as \"plostellum punicum\" (literally, the \"Punic cart\"), ought to be called \"threshing cart\". Although the Carthaginians, heirs of the Phoenicians, brought this model to the Western Mediterranean, this implement was known at least since the second millennium BC, appearing in the Babylonian texts with a name that we can transliterate as \"gīš-bad\". Both variants continued to be used well into the 20th century in Europe, and continue to be used in the regions where agriculture has not been mechanized and industrialized. Museums and collectors in Spain retain some threshing carts, which were once highly prized in areas such as the province of Zamora, where they were used to thresh garbanzos.\n\n\nThis article draws heavily on the corresponding article in the , accessed in the version of 20 November 2006.\n\n"}
{"id": "436166", "url": "https://en.wikipedia.org/wiki?curid=436166", "title": "Variable-frequency oscillator", "text": "Variable-frequency oscillator\n\nA variable frequency oscillator (VFO) in electronics is an oscillator whose frequency can be tuned (i.e., varied) over some range. It is a necessary component in any tunable radio receiver or transmitter that works by the superheterodyne principle, and controls the frequency to which the apparatus is tuned.\n\nIn a simple superheterodyne receiver, the incoming radio frequency signal (at frequency formula_1) from the antenna is \"mixed\" with the VFO output signal tuned to formula_2, producing an intermediate frequency (IF) signal that can be processed downstream to extract the modulated information. Depending on the receiver design, the IF signal frequency is chosen to be either the sum of the two frequencies at the mixer inputs (up-conversion), formula_3 or more commonly, the difference frequency (down-conversion), formula_4.\n\nIn addition to the desired \"IF\" signal and its unwanted image (the mixing product of opposite sign above), the mixer output will also contain the two original frequencies, formula_1 and formula_2 and various harmonic combinations of the input signals. These undesired signals are rejected by the IF filter. If a double balanced mixer is employed, the input signals appearing at the mixer outputs are greatly attenuated, reducing the required complexity of the IF filter.\n\nThe advantage of using a VFO as a heterodyning oscillator is that only a small portion of the radio receiver (the sections before the mixer such as the preamplifier) need to have a wide bandwidth. The rest of the receiver can be finely tuned to the IF frequency.\n\nIn a direct-conversion receiver, the VFO is tuned to the same frequency as the incoming radio frequency and formula_7 Hz. Demodulation takes place at baseband using low-pass filters and amplifiers.\n\nIn a radio frequency (RF) transmitter, VFOs are often used to tune the frequency of the output signal, often indirectly through a heterodyning process similar to that described above. Other uses include chirp generators for radar systems where the VFO is swept rapidly through a range of frequencies, timing signal generation for oscilloscopes and time domain reflectometers, and variable frequency audio generators used in musical instruments and audio test equipment.\n\nThere are two main types of VFO in use: analog and digital.\n\nAn analog VFO is an electronic oscillator where the value of at least one of the passive components is adjustable under user control so as to alter its output frequency.\nThe passive component whose value is adjustable is usually a capacitor, but could be a variable inductor.\n\nThe variable capacitor is a mechanical device in which the separation of a series of interleaved metal plates is physically altered to vary its capacitance. Adjustment of this capacitor is sometimes facilitated by a mechanical step-down gearbox to achieve fine tuning.\n\nA reversed-biased semiconductor diode exhibits capacitance. Since the width of its non-conducting depletion region depends on the magnitude of the reverse bias voltage, this voltage can be used to control the junction capacitance. The varactor bias voltage may be generated in a number of ways and there may need to be no significant moving parts in the final design.\nVaractors have a number of disadvantages including temperature drift and aging, electronic noise, low Q factor and non-linearity.\n\nModern radio receivers and transmitters usually use some form of digital frequency synthesis to generate their VFO signal.\nThe advantages include smaller designs, lack of moving parts, the higher stability of set frequency reference oscillators, and the ease with which preset frequencies can be stored and manipulated in the digital computer that is usually embedded in the design in any case.\n\nIt is also possible for the radio to become extremely frequency-agile in that the control computer could alter the radio's tuned frequency many tens, thousands or even millions of times a second.\nThis capability allows communications receivers effectively to monitor many channels at once, perhaps using digital selective calling (DSC) techniques to decide when to open an audio output channel and alert users to incoming communications.\nPre-programmed frequency agility also forms the basis of some military radio encryption and stealth techniques.\nExtreme frequency agility lies at the heart of spread spectrum techniques that have gained mainstream acceptance in computer wireless networking such as Wi-Fi.\n\nThere are disadvantages to digital synthesis such as the inability of a digital synthesiser to tune smoothly through all frequencies, but with the channelisation of many radio bands, this can also be seen as an advantage in that it prevents radios from operating in between two recognised channels.\n\nDigital frequency synthesis relies on stable crystal controlled reference frequency sources. Crystal controlled oscillators are more stable than inductively and capacitively controlled oscillators. Their disadvantage is that changing frequency (more than a small amount) requires changing the crystal, but frequency synthesizer techniques have made this unnecessary in modern designs.\n\nThe electronic and digital techniques involved in this include:\n\nThe quality metrics for a VFO include frequency stability, phase noise and spectral purity. All of these factors tend to be inversely proportional to the tuning circuit's Q factor. Since in general the tuning range is also inversely proportional to Q, these performance factors generally degrade as the VFO's frequency range is increased.\n\nStability is the measure of how far a VFO's output frequency drifts with time and temperature. To mitigate this problem, VFOs are generally \"phase locked\" to a stable reference oscillator. PLLs use negative feedback to correct for the frequency drift of the VFO allowing for both wide tuning range and good frequency stability.\n\nIdeally, for the same control input to the VFO, the oscillator should generate exactly the same frequency. A change in the calibration of the VFO can change receiver tuning calibration; periodic re-alignment of a receiver may be needed. VFO's used as part of a phase-locked loop frequency synthesizer have less stringent requirements since the system is as stable as the crystal-controlled reference frequency.\n\nA plot of a VFO's amplitude vs. frequency may show several peaks, probably harmonically related. Each of these peaks can potentially mix with some other incoming signal and produce a \"spurious\" response. These \"spurii\" (sometimes spelled \"spuriae\") can result in increased noise or two signals detected where there should only be one. Additional components can be added to a VFO to suppress high-frequency parasitic oscillations, should these be present.\n\nIn a transmitter, these spurious signals are generated along with the one desired signal. Filtering may be required to ensure the transmitted signal meets regulations for bandwidth and spurious emissions.\n\nWhen examined with very sensitive equipment, the pure sine-wave peak in a VFO's frequency graph will most likely turn out not to be sitting on a flat noise-floor. Slight random 'jitters' in the signal's timing will mean that the peak is sitting on 'skirts' of phase noise at frequencies either side of the desired one.\n\nThese are also troublesome in crowded bands. They allow through unwanted signals that are fairly close to the expected one, but because of the random quality of these phase-noise 'skirts', the signals are usually unintelligible, appearing just as extra noise in the received signal. The effect is that what should be a clean signal in a crowded band can appear to be a very noisy signal, because of the effects of strong signals nearby.\n\nThe effect of VFO phase noise on a transmitter is that random noise is actually transmitted either side of the required signal. Again, this must be avoided for legal reasons in many cases.\n\nDigital or digitally controlled oscillators typically rely on constant single frequency references, which can be made to a higher standard than semiconductor and LC circuit-based alternatives. Most commonly a quartz crystal based oscillator is used, although in high accuracy applications such as TDMA cellular networks atomic clocks such as the Rubidium standard are as of 2018 also common.\n\nBecause of the stability of the reference used, digital oscillators themselves tend to be more stable and more repeatable in the long term. This in part explains their huge popularity in low-cost and computer-controlled VFOs. In the shorter term the imperfections introduced by digital frequency division and multiplication (jitter), and the susceptibility of the common quartz standard to acoustic shocks, temperature variation, aging, and even radiation, limit the applicability of a naïve digital oscillator.\n\nThis is why higher end VFO's like RF transmitters locked to atomic time, tend to combine multiple different references, and in complex ways. Some references like rubidium or cesium clocks provide higher long term stability, while others like hydrogen masers yield lower short term phase noise. Then lower frequency (and so lower cost) oscillators phase locked to a digitally divided version of the master clock deliver the eventual VFO output, smoothing out the noise induced by the division algorithms. Such an arrangement can then give all of the longer term stability and repeatability of an exact reference, the benefits of exact digital frequency selection, and the short term stability, imparted even onto an arbitrary frequency analogue waveform—the best of all worlds.\n\n"}
{"id": "343505", "url": "https://en.wikipedia.org/wiki?curid=343505", "title": "Wear leveling", "text": "Wear leveling\n\nWear leveling (also written as wear levelling) is a technique for prolonging the service life of some kinds of erasable computer storage media, such as flash memory, which is used in solid-state drives (SSDs) and USB flash drives, and phase change memory. There are several wear leveling mechanisms that provide varying levels of longevity enhancement in such memory systems.\n\nThe term \"preemptive wear leveling\" (PWL) has been used by Western Digital to describe their preservation technique used on hard disk drives (HDDs) designed for storing audio and video data. However, HDDs generally are not wear-leveled devices in the context of this article.\n\nEEPROM and flash memory media have individually erasable segments, each of which can be put through a limited number of erase cycles before becoming unreliable. This is usually around 3,000/5,000 cycles but many flash devices have one block with a specially extended life of 100,000+ cycles that can be used by the Flash memory controller to track wear and movement of data across segments. Erasable optical media such as CD-RW and DVD-RW are rated at up to 1,000 cycles (100,000 cycles for DVD-RAM media).\n\nWear leveling attempts to work around these limitations by arranging data so that erasures and re-writes are distributed evenly across the medium. In this way, no single erase block prematurely fails due to a high concentration of write cycles. In flash memory, a single block on the chip is designed for longer life than the others so that the memory controller can store operational data with less chance of its corruption.\n\nConventional file systems such as FAT, UFS, HFS, ext2, and NTFS were originally designed for magnetic disks and as such rewrite many of their data structures (such as their directories) repeatedly to the same area. When these systems are used on flash memory media, this becomes a problem. The problem is aggravated by the fact that some file systems track last-access times, which can lead to file metadata being constantly rewritten in-place.\n\nThere are three basic types of wear leveling mechanisms used in flash memory storage devices:\n\nA flash memory storage system with \"no wear leveling\" will not last very long if data is written to the flash. Without wear leveling, the underlying flash controller must permanently assign the logical addresses from the operating system (OS) to the physical addresses of the flash memory. This means that every write to a previously written block must first be read, erased, modified, and re-written to the same location. This approach is very time-consuming and frequently written locations will wear out quickly, while other locations will not be used at all. Once a few blocks reach their end of life, such a device becomes inoperable.\n\nThe first type of wear leveling is called \"dynamic wear leveling\" and it uses a map to link logical block addresses (LBAs) from the OS to the physical flash memory. Each time the OS writes replacement data, the map is updated so the original physical block is marked as \"invalid\" data, and a new block is linked to that map entry. Each time a block of data is re-written to the flash memory, it is written to a new location. However, flash memory blocks that never get replacement data would sustain no additional wear, thus the name comes only from the dynamic data being recycled. Such a device may last longer than one with no wear leveling, but there are blocks still remaining as active even though the device is no longer operable.\n\nThe other type of wear leveling is called \"static wear leveling\" which also uses a map to link the LBA to physical memory addresses. Static wear leveling works the same as dynamic wear leveling except the static blocks that do not change are periodically moved so that these low usage cells are able to be used by other data. This rotational effect enables an SSD to continue to operate until most of the blocks are near their end of life. This is also sometimes referred to as \"global wear leveling\", as the entire image is leveled.\n\nThe following table compares static and dynamic wear leveling:\n\nThere are several techniques for extending the media life:\n\n\nOn some specialist Secure Digital cards, techniques are implemented in hardware by a built-in microcontroller. On such devices, wear leveling is transparent, and most conventional file systems can be used on them as-is.\n\nWear leveling can also be implemented in software by special-purpose file systems such as JFFS2 and YAFFS on flash media or UDF on optical media. All three are log-structured file systems in that they treat their media as circular logs and write to them in sequential passes. File systems which implement copy-on-write strategies, such as ZFS, also implement a form of wear leveling.\n\n\n"}
{"id": "33123", "url": "https://en.wikipedia.org/wiki?curid=33123", "title": "Wireless Valley", "text": "Wireless Valley\n\nWireless Valley is a term that was coined by Professor Ted Rappaport in 1990 when he was a faculty member at Virginia Tech, and was used to describe the Roanoke/Blacksburg, Virginia region and the potential of research to create spin-out companies. In 1990 he and his students founded TSR Technologies, a company that made software-defined cellular and paging intercept and drive test equipment that was sold to Allen Telecom in 1993, and in 1995 Wireless Valley Communications, a company that pioneered the creation of computer-aided wireless network prediction and management software that was sold to Motorola in late 2005. This term was later used as nickname to describe several regional clusters of companies in the information technology sector, in analogy to California's Silicon Valley:\n\n"}
