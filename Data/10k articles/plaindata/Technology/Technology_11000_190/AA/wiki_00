{"id": "27168879", "url": "https://en.wikipedia.org/wiki?curid=27168879", "title": "4K resolution", "text": "4K resolution\n\n4K resolution, also called 4K, refers to a horizontal display resolution of approximately 4,000 pixels. Digital television and digital cinematography commonly use several different 4K resolutions. In television and consumer media, 38402160 (4K UHD) is the dominant 4K standard. In the movie projection industry, 40962160 (DCI 4K) is the dominant 4K standard.\n\nThe 4K television market share increased as prices fell dramatically during 2014 and 2015. By 2020, more than half of U.S. households are expected to have 4K-capable TVs, a much faster adoption rate than that of Full HD (1080p).\n\nThe term \"4K\" is generic and refers to any resolution with a horizontal pixel count of approximately 4,000. Several different 4K resolutions have been standardized by various organizations.\n\nIn 2005, Digital Cinema Initiatives (DCI), a prominent standards organization in the cinema industry, published the Digital Cinema System Specification. This specification establishes standardized 2K and 4K container formats for digital cinema production, with resolutions of 20481080 and 40962160 respectively. The resolution of the video content inside follows the SMPTE 428-1 standard, which establishes the following resolutions for a 4K distribution:\n2K distributions can have a frame rate of either 24 or 48FPS, while 4K distributions must have a frame rate of 24FPS.\nSome articles claim that the terms \"2K\" and \"4K\" were coined by DCI and refer exclusively to the 2K and 4K formats defined in the DCI standard. However, usage of these terms in the cinema industry predates the publication of the DCI standard, and they are generally understood as casual terms for any resolution approximately 2000 or 4000 pixels in width, rather than names for specific resolutions.\n\nIn 2007, the Society of Motion Picture and Television Engineers published SMPTE ST 2036-1, which defines parameters for two UHDTV systems called UHDTV1 and UHDTV2. The standard defines the following characteristics for these systems:\n\nIn 2012, the International Telecommunication Union, Radiocommunication Sector published Recommendation ITU-R BT.2020, also known as the Ultra High Definition Television (UHDTV) standard. This standard adopts the same image parameters defined in SMPTE ST 2036-1.\n\nAlthough the UHDTV standard does not define any official names for the formats it defines, ITU typically uses the terms \"4K\", \"4K UHD\", or \"4K UHDTV\" to refer to the 38402160 system in public announcements and press releases (\"8K\" for the 76804320 system). In some of ITU's other standards documents, the terms \"UHDTV1\" and \"UHDTV2\" are used as shorthand.\n\nIn October 2012, the Consumer Electronics Association (CEA) announced their definition of the term Ultra High-Definition (or Ultra HD) for use with marketing consumer display devices. CEA defines an \"Ultra HD\" product as a TV, monitor, or projector with the following characteristics:\n\nThe CEA definition does allow manufacturers to use other terms—such as \"4K\"—alongside the Ultra HD logo. Since the resolution in CEA's definition is only a minimum requirement, displays with higher resolutions such as 40962160 or 51202880 also qualify as \"Ultra HD\" displays, provided they meet the other requirements.\n\nSome 4K resolutions, like 38402160 are often casually referred to as \"2160p\". This name follows from the previous naming convention used by HDTV and SDTV formats, which refer to a format by the number of pixels/lines along the vertical axis (such as \"1080p\" for 19201080 progressive scan, or \"480i\" for the 480-line interlaced SDTV formats) rather than the horizontal pixel count (≈4000 or \"4K\" for 38402160).\n\nThe term \"2160p\" could be applied to any format with a height of 2160 pixels, but it is most commonly used in reference to the 4K UHDTV resolution of 38402160 due to its association with the well-known 720p and 1080p HDTV formats. Although 38402160 is both a 4K resolution and a 2160p resolution, these terms cannot always be used interchangeably since not all 4K resolutions are 2160 pixels tall, and not all 2160p resolutions are ≈4000 pixels wide. However, some companies have begun using the term \"4K\" to describe devices with support for a 2160p resolution, even if it is not close to 4000 pixels wide. For example, many \"4K\" dash cams only support a resolution of 28802160 (4∶3); although this is a 2160p resolution, it is not a 4K resolution. Samsung also released a 51202160 (64∶27) TV, but marketed it as a \"4K\" TV despite its 5K-class resolution.\n\nYouTube and the television industry have adopted 38402160 as their 4K standard. , 4K content from major broadcasters remains limited. On April 11, 2013, Bulb TV created by Canadian serial entrepreneur Evan Kosiner became the first broadcaster to provide a 4K linear channel and VOD content to cable and satellite companies in North America. The channel is licensed by the Canadian Radio-Television and Telecommunications Commission to provide educational content. However, 4K content is becoming more widely available online including on YouTube, Netflix, and Amazon. By 2013, some UHDTV models were available to general consumers in the range of US$600. , prices on smaller computer and television panels had dropped below US$400.\n\nIn 2014, the Digital Video Broadcasting Project released a new set of standards intended to guide the implementation of high resolution content in broadcast television. Dubbed DVB-UHDTV, it establishes two standards, known as UHD-1 (for 4K content) and UHD-2 (for 8K content). These standards use resolutions of 38402160 and 76804320 respectively, with framerates of up to 60Hz, color depth up to 10bpc (30bit/px), and HEVC encoding for transmission. DVB is currently focusing on the implementation of the UHD-1 standard.\n\nDVB finalized UHD-1 Phase 2 in 2016, with the introduction of service by broadcasters expected in 2017. UHD-1 Phase 2 adds features such as high dynamic range (using HLG and PQ at 10 or 12 bits), wide color gamut (BT. 2020/2100 colorimetry), and high frame rate (up to 120Hz).\n\nYouTube, since 2010, and Vimeo allow a maximum upload resolution of 4096 × 3072 pixels (12.6 megapixels, aspect ratio 4:3). Vimeo's 4K content is currently limited to mostly nature documentaries and tech coverage.\n\nHigh Efficiency Video Coding (H.265) should allow streaming 4K content with a bandwidth of 20 to 30 Mbit/s.\n\nIn January 2014, Naughty America launched the first adult video service streaming in 4K.\n\nThe first commercially available 4K camera for cinematographic purposes was the Dalsa Origin, released in 2003. 4K technology was developed by several research groups in universities around the world, such as University of California, San Diego, CALIT2, Keio University and others that realized several demonstrations in venues such as IGrid in 2004 and CineGrid. YouTube began supporting 4K for video uploads in 2010 as a result of leading manufacturers producing 4K cameras. Users could view 4K video by selecting \"Original\" from the quality settings until December 2013, when the 2160p option appeared in the quality menu. In November 2013, YouTube began to use the VP9 video compression standard, saying that it was more suitable for 4K than High Efficiency Video Coding (HEVC). Google, which owns YouTube, developed VP9.\n\nTheaters began projecting movies at 4K resolution in 2011. Sony was offering 4K projectors as early as 2004. The first 4K home theater projector was released by Sony in 2012.\n\nSony is one of the leading studios promoting UHDTV content, offering a little over 70 movie and television titles via digital download to a specialized player that stores and decodes the video. The large files (~40GB), distributed through consumer broadband connections, raise concerns about data caps.\n\nIn 2014, Netflix began streaming \"House of Cards\", \"Breaking Bad\", and \"some nature documentaries\" at 4K to compatible televisions with an HEVC decoder. Most 4K televisions sold in 2013 did not natively support HEVC, with most major manufacturers announcing support in 2014. Amazon Studios began shooting their full-length original series and new pilots with 4K resolution in 2014. They are now currently available though Amazon Video.\n\nIn March 2016 the first players and discs for Ultra HD Blu-ray—a physical optical disc format supporting 4K resolution and HDR at 60 frames per second—were released.\n\nIn 2016, Sony and Microsoft released the PlayStation 4 Pro and Xbox One S, respectively, both of which are video game consoles that support 4K streaming, although in most cases the resolution is upscaled to 4K; the Xbox One S also features an Ultra HD Blu-ray disc drive. On November 7, 2017 Microsoft released the Xbox One X, which is capable of native 4K streaming and gaming.\n\nThough experiencing rapid price drops beginning in 2013 for viewing devices, the home cinema digital video projector market saw little expansion, with only a few manufacturers (only Sony ) offering limited 4K-capable lineups, with native 4K projectors commanding five-figure price tags well into 2015 before finally breaking the US$10,000 barrier. Critics state that at normal direct-view panel size and viewing distances, the extra pixels of 4K are redundant at the ability of normal human vision. Projection home cinemas, on the other hand, employ much larger screen sizes without necessarily increasing viewing distance to scale. JVC has used a technique known as \"e-shift\" to extrapolate extra pixels from 1080p sources to display 4K on screens through upscaling or from native 4K sources at a much lower price than native 4K projectors. This technology of non-native 4K entered its fourth generation for 2016. JVC used this same technology to provide 8K flight simulation for Boeing that met the limits of 20/25 visual acuity.\n\nPixel shifting, as described here, was pioneered in the consumer space by JVC, and later in the commercial space by Epson. That said, it isn't the same thing as \"true\" 4K. More recently, some DLP projectors claim 4K UHD (which the JVCs and Epsons do not claim).\n\nAs noted above, DCI 4K is 40962160, while 4K UHD is 38402160, producing a slight difference in aspect ratio rather than a significant difference in resolution. Traditional displays, such as LCD or OLED, are 3840 pixels across the screen, with each pixel being 1/3840th of the screen width. They do not overlap—if they did, they would suffer reduced detail. The diameter of each pixel is basically 1/3840th of the screen width or 1/2160th of the screen height - either gives the same size pixel. That 38402160 works out to 8.3 megapixels, the official resolution of 4K UHD (and therefore Blu-ray UHD discs).\n\nThe 4K UHD standard doesn't specify how large the pixels are, so a 4K UHD projector (Optoma, BenQ, Dell, et al.) counts because these projectors have a 2718×1528 pixel structure. Those projectors process the true 4K of data and project it with overlapping pixels, which is what pixel shifting is. Unfortunately, each of those pixels is far larger: each one has 50% more area than true 4K. Pixel shifting projectors project a pixel, shift it up to the right, by a half diameter, and project it again, with modified data, but that second pixel overlaps the first.\n\nIn other words, pixel shifting cannot produce adjacent vertical lines of RGBRGB or other colors where each line is one pixel (1/3840th of the screen) wide. Adjacent red and green pixels would end up looking like yellow, with a fringe on one side of red, on the other of green - except that the next line of pixels overlapps as well, changing the color of that fringe. 4K UHD or 1080p pixel shifting cannot reveal the fine detail of a true 4K projector such as those Sony ships (business, education and home markets). Also, JVC has one true 4K projector priced at $35,000 (as of mid-2017).\n\nSo while 4K UHD sounds like it has a pixel structures with 1/4 the area of 1080p, that does not happen with pixel shifting. Only a true 4K projector offers that level of resolution. That's why \"true\" 4K projectors cost so much more than 4K UHD projectors with otherwise similar feature sets. They produce smaller pixels, finer resolution, no compromising of detail or color from overlapping pixels. By comparison, the slight difference in aspect ratio between DCI and 3840×2160 pixel displays without overlap is insignificant relative to the amount of detail.\n\nIn November 2014, United States satellite provider DirecTV became the first pay TV provider to offer access to 4K content, although limited to selected video-on-demand films. In August 2015, British sports network BT Sport launched a 4K feed, with its first broadcast being the 2015 FA Community Shield football match. Two production units were used, producing the traditional broadcast in high-definition, and a separate 4K broadcast. As the network did not want to mix 4K footage with upconverted HD footage, this telecast did not feature traditional studio segments at pre-game or half-time, but those hosted from the stadium by the match commentators using a 4K camera. BT envisioned that if viewers wanted to watch studio analysis, they would switch to the HD broadcast and then back for the game. Footage was compressed using H.264 encoders and transmitted to BT Tower, where it was then transmitted back to BT Sport studios and decompressed for distribution, via 4K-compatible BT TV set-top boxes on an eligible BT Infinity internet plan with at least a 25 Mbit/s connection.\n\nIn late 2015 and January 2016, three Canadian television providers including Quebec-based Videotron, Ontario-based Rogers Cable, and Bell Fibe TV, announced that they would begin to offer 4K compatible set-top boxes that can stream 4K content to subscribers over gigabit internet service. On October 5, 2015, alongside the announcement of its 4K set-top box and gigabit internet, Canadian media conglomerate Rogers Communications announced that it planned to produce 101 sports telecasts in 4K in 2016 via its Sportsnet division, including all Toronto Blue Jays home games, and \"marquee\" National Hockey League games beginning in January 2016. Bell Media announced via its TSN division a slate of 4K telecasts to begin on January 20, 2016, including selected Toronto Raptors games and regional NHL games.\n\nOn January 14, 2016, in cooperation with BT Sport, Sportsnet broadcast the first ever NBA game produced in 4K a Toronto Raptors/Orlando Magic game at O2 Arena in London, England. On January 20, also during a Raptors game, TSN presented the first live 4K telecast produced in North America. Three days later, Sportsnet presented the first NHL game in 4K.\n\nDome Productions, a joint venture of Bell Media and Rogers Media (the respective owners of TSN and Sportsnet), constructed a \"side-by-side\" 4K mobile production unit shared by Sportsnet and TSN's first 4K telecasts; it was designed to operate alongside a separate HD truck and utilize cameras capable of output in both formats. For the opening game of the 2016 Toronto Blue Jays season, Dome constructed \"Trillium\" a production truck integrating both 4K and 1080i high-definition units. Bell Media's CTV also broadcast the 2016 Juno Awards in 4K as the first awards show presented in the format.\n\nIn February 2016, Univision trialed 4K by producing a closed circuit telecast of a football friendly between the national teams of Mexico and Senegal from Miami in the format. The broadcast was streamed privately to several special viewing locations. Univision aimed to develop a 4K streaming app to publicly televise the final of Copa América Centenario in 4K. In March 2016, DirecTV and CBS Sports announced that they would produce the \"Amen Corner\" supplemental coverage from the Masters golf tournament in 4K.\n\nAfter having trialed the technology in limited matches at the 2013 FIFA Confederations Cup, and the 2014 FIFA World Cup (via private tests and public viewings in the host city of Rio de Janeiro), the 2018 FIFA World Cup was the first FIFA World Cup in which all matches were produced in 4K. Host Broadcasting Services stated that at least 75% of the broadcast cut on each match would come from 4K cameras (covering the majority of main angles), with instant replays and some camera angles being upconverted from 1080p sources. These broadcasts were made available from selected rightsholders, such as the BBC in the United Kingdom, and selected television providers in the United States.\n\nThe resolution of 38402160 is the dominant 4K resolution used in the consumer media and display industries. This is the resolution of the UHDTV1 format defined in SMPTE ST 2036-1, as well as the 4K UHDTV format defined by ITU-R in Rec. 2020, and is also the minimum resolution for CEA's definition of \"Ultra HD\" displays and projectors. The resolution of 38402160 was also chosen by the DVB Project for their 4K broadcasting standard, UHD-1.\n\nThis resolution has an aspect ratio of 16∶9, with 8,294,400 total pixels. It is exactly double the horizontal and vertical resolution of 1080p (19201080) for a total of 4 times as many pixels, and triple the horizontal and vertical resolution of 720p (1280720) for a total of 9 times as many pixels. It is sometimes referred to as \"2160p\", based on the naming patterns established by the previous 720p and 1080p HDTV standards.\n\nTelevisions capable of displaying UHD resolutions are seen by consumer electronics companies as the next trigger for an upgrade cycle after a lack of consumer interest in 3D television.\n\nThis resolution is used mainly in digital cinema production, and has a total of 8,847,360 pixels with an aspect ratio 256∶135 (≈19∶10). It was standardized as the resolution of the 4K container format defined by Digital Cinema Initiatives in the Digital Cinema System Specification, and is the native resolution of all DCI-compliant 4K digital projectors and monitors. The DCI specification allows several different resolutions for the content inside the container, depending on the desired aspect ratio. The allowed resolutions are defined in SMPTE 428-1: \n\n\nThe DCI 4K standard has twice the horizontal and vertical resolution of DCI 2K (20481080), with four times as many pixels overall.\n\nDigital movies made in 4K may be produced, scanned, or stored in a number of other resolutions depending on what storage aspect ratio is used. In the digital cinema production chain, a resolution of 4096 × 3112 is often used for acquiring \"open gate\" or anamorphic input material, a resolution based on the historical resolution of scanned Super 35mm film.\n\nVarious other non-standardized 4K resolutions have seen use in displays, including:\n\nThe main advantage of recording video at the 4K standard is that fine spatial detail is resolved well. If the final video quality is reduced to 2K from a 4K recording, more detail is apparent than would have been achieved from a 2K recording. Increased fineness and contrast is then possible with output to DVD and Blu-ray. Some cinematographers record at 4K with the Super 35 film format to offset any resolution loss that may occur during video processing.\n\nWith Axiom devices there is some open source hardware available that uses a 4K image sensor.\n\n\n<section begin=ExternalLinks/>\n\n\n\n<section end=ExternalLinks /><section begin=ExternalLinksLinkTemplates/>\n\n<section end=ExternalLinksLinkTemplates />\n"}
{"id": "2973824", "url": "https://en.wikipedia.org/wiki?curid=2973824", "title": "AN/APG-65 radar family", "text": "AN/APG-65 radar family\n\nThe AN/APG-65 and AN/APG-73 are designations for a family of all-weather multimode airborne radar systems designed by Hughes Aircraft (now Raytheon) for the F/A-18 Hornet, and used on a variety of fighter aircraft types. The APG-79 is an upgraded AESA version.\n\nThese I band (8 to 12 GHz) pulse-Doppler radar systems are designed for both air-to-air and air-to-surface missions. For air-to-air operations they incorporate a variety of search, track and track-while-scan modes to give the pilot a complete look-down/shoot-down capability. Air-to-surface modes include Doppler beam sharpened sector and patch mapping, medium range synthetic aperture radar, fixed and moving ground target track and sea surface search. In the F/A-18, the radar is installed in a slide-out nose rack to facilitate maintenance.\n\nThe APG-65 was developed in the late 1970s and has been operational since 1983. The radar includes a velocity search (to provide maximum detection range capability against nose aspect targets), range-while-search (to detect all-aspect targets), track-while-scan (which, when combined with an autonomous missile such as AIM-120, gives the aircraft a fire-and-forget capability), single target track, gun director and raid assessment (which enables the operator to expand the region centred on a single tracked target, permitting radar separation of closely spaced targets) operating modes.\n\nAlthough no longer in production, the APG-65 remains in service in F/A-18 Hornet strike fighters of the U.S. Navy and Marine Corps, and the air forces of Canada, Australia, Kuwait, and Spain. It has also been adapted to upgrade the German and Greek F-4 Phantom aircraft, and the AV-8B Harrier II Plus for the U.S. Marine Corps and the Spanish and Italian Navies.\n\nThe APG-73 is a late 1980s \"upgrade of the APG-65 that provides higher throughputs, greater memory capacity, improved reliability, and easier maintenance\". To reduce production costs, many of the upgraded radar's modules are common with the APG-70 (F-15E Strike Eagle) radar; its software engineers chose the JOVIAL programming language so that they could borrow and adapt existing software written for the APG-70. When fitted with a motion-sensing subsystem and stretch waveform generator and special test equipment, the APG-73 can generate high resolution ground maps and make use of 'advanced' image correlation algorithms to enhance weapon designation accuracy.\n\nSince 1992 the APG-73 has been operational in U.S. Navy and Marine Corps F/A-18C and D aircraft; early models of the U.S. Navy F/A-18E/F Super Hornet; and in the air forces of Finland, Switzerland, Malaysia, Canada, and Australia. A total of 932 APG-73 systems were delivered, with the final delivery in 2006.\n"}
{"id": "45458135", "url": "https://en.wikipedia.org/wiki?curid=45458135", "title": "Amelia Greenhall", "text": "Amelia Greenhall\n\nAmelia Cousins Greenhall is an American feminist tech blogger. She cofounded feminist tech blog and publication \"Model View Culture\" with Shanley Kane. Greenhall is co-founder and Executive Director of Double Union, a feminist women-only hackerspace in San Francisco, with Valerie Aurora, and is a Quantified Self enthusiast. Greenhall is the publisher and co-founder of \"Open Review Quarterly\", a literary journal on modern culture (founded in September 2010).\n\nPrior to co-founding \"Model View Culture\" in November 2013, Greenhall was a user experience designer, user interface designer and data scientist in Seattle. She left \"Model View Culture\" in May 2014.\n\nBorn in Hawaii and raised in Arizona, Greenhall is a 2009 studio art and electrical engineering graduate of Vanderbilt University in Tennessee. She went on to earn a master's degree in public health at the University of Washington.\n\n"}
{"id": "38386981", "url": "https://en.wikipedia.org/wiki?curid=38386981", "title": "Apikoğlu", "text": "Apikoğlu\n\nApikoğlu Brothers is a family owned and operated meat packing organization founded in Turkey. The company is renowned for its production of sujuk and pastirma. Apikoğlu is the first company to mass-produce meat products in Turkey and serve the demand of its entire national market.\n\nApikoğlu was founded in Kayseri in 1910 by ethnic Armenian Krikor Apikoğlu (1878–1945). Apikoğlu moved its base of operations to Istanbul in 1920, opening its production facility in a neighborhood of the Maltepe suburb called İdealtepe. The company's headquarters occupied the lower level of the family's home, limiting their domicile to the upper levels.\n\nAs business expanded, the company eventually outgrew the Maltepe facility, subsequently moving its manufacturing operations to Alibeyköy. The factory in Alibeyköy was managed by brothers, Agop and Hayk Apikoğlu. This is when the company began doing business as (dba) Apikoğlu Brothers. Apikoğlu Brothers was soon selling its products throughout Turkey.\n\nEfforts to meet the increasing national demand spawned an innovation Apikoğlu Brothers was able to incorporate from the 1960s. Their patented system made it possible to produce marketable halal meats directly from the slaughterhouse. The technique had the cow dangling upside down so that when the cow was slaughtered, the blood could easily flow out of its body. The technique had never been practiced in the manufacturing of foodstuffs and the precedent quickly became standard practice throughout Turkey, and eventually the entire meat packing industry. In order to supply the increasing needs of national demand, Apikoğlu Brothers increased its workforce to 200 people in 1987. They also purchased a modern facility in Tuzla. As of 2013, Apikoğlu Brothers are continuing their manufacturing operations from the Tuzla facilities, with no indications that Tuzla operations will discontinue at any time in the near future.\n\nIn the 2009 book: \"Markaların Öyküsü\", (English translation: \"The History of Brands\") Apikoğlu is described as one of \"the 304 most famous brands in the world\". This designation preceded the Apikoğlu Brothers celebration of 100 continuous years in operation on its 2010 anniversary.\n\nThe Apikoglu Brothers were mentioned in Nobel Prize winning author Orhan Pamuk's book Istanbul: Memories of a City.\n\nApikoğlu Brothers received the Altın Marka Award for two consecutive years (2005–06).\n\n\n"}
{"id": "5077847", "url": "https://en.wikipedia.org/wiki?curid=5077847", "title": "Applied Digital Data Systems", "text": "Applied Digital Data Systems\n\nApplied Digital Data Systems (ADDS) was a supplier of video display computer terminals, founded in 1969 by Leeam Lowin and William J. Catacosinos. Lowin simultaneously founded Solid State Data Sciences (SSDS). SSDS was one of the first developers of the MOS/LSI integrated circuits that were key to ADDS's product line.\n\nIt became a subsidiary of NCR Corporation in 1980, which sold the \"Mentor 2000\" professional computer in the United States in 1986.\n\nThe \"Mentor 2000\" ran at 5 MHz using a Zilog processor, 640 KB RAM, and included one 60MB hard disk. It used the Pick operating system and database management system. It was able to manage 16 or 32 video terminals at once.\n\nADDS (along with NCR) was later part of AT&T,\nthen independent briefly before being acquired by SunRiver Data Systems.\n\nHowever, their version of the Pick operating system was acquired by Pick Systems Inc, now called TigerLogic. That version is now called mvBase. MvBase was sold to Rocket Software in 2013.\n\n\n"}
{"id": "479719", "url": "https://en.wikipedia.org/wiki?curid=479719", "title": "Applied Materials", "text": "Applied Materials\n\nApplied Materials, Inc. is an American corporation that supplies equipment, services and software to enable the manufacture of semiconductor (integrated circuit) chips for electronics, flat panel displays for computers, smartphones and televisions, and solar products. The company also supplies equipment to produce coatings for flexible electronics, packaging and other applications. The company is headquartered in Santa Clara, California, in Silicon Valley.\n\nFounded in 1967 by Michael A. McNeilly and others, Applied Materials went public in 1972. In subsequent years, the company diversified, until James C. Morgan became CEO in 1976 and returned the company's focus to its core business of semiconductor manufacturing equipment. By 1978, sales increased by 17%.\n\nIn 1984, Applied Materials became the first U.S. semiconductor equipment manufacturer to open its own technology center in Japan and the first semiconductor equipment company to operate a service center in China. In 1987, Applied introduced a CVD machine called the Precision 5000, which differed from existing machines by incorporating diverse processes into a single machine that had multiple process chambers.\n\nIn 1992, the corporation settled a lawsuit with three former employees for an estimated $600,000. The suit complained that the employees were driven out of the company after complaining about the courses Applied Scholastics had been hired to teach there.\n\nIn 1993, the Applied Materials' Precision 5000 was inducted into the Smithsonian Institution's permanent collection of Information Age technology.\n\nIn November 1996, Applied Materials acquired two Israeli companies for an aggregate amount of $285 million. Opal Technologies and Orbot Instruments for $175 million and $110 million in cash, respectively. Orbot produces systems for inspecting patterned silicon wafers for yield enhancement during the semiconductor manufacturing process, as well as systems for inspecting masks used during the patterning process. Opal develops and manufactures high-speed metrology systems used by semiconductor manufacturers to verify critical dimensions during the production of integrated circuits.\n\nIn 2000, Etec Systems, Inc. was purchased.\n\nOn June 27, 2001, Applied acquired Israeli company Oramir Semiconductor Equipment Ltd., a supplier of laser cleaning technologies for semiconductor wafers, in a purchase business combination for $21 million in cash.\n\nIn January 2008, Applied Materials purchased an Italian company Baccini, a designer of tools used in manufacturing solar cells.\n\nIn 2009, Applied Materials opened its Solar Technology Center—the world's largest commercial solar energy research and development facility in Xi'an, China.\n\nApplied Materials' acquisition of Semitool Inc. was completed in December 2009.\n\nApplied Materials announced its acquisition of Varian Semiconductor in May 2011.\n\nApplied Materials announced its merger with Tokyo Electron on September 24, 2013. If approved by government regulators, the combined company, to be called Eteris, would be the world's largest supplier of semiconductor processing equipment, with a total market value of more than $30 billion.\n\nBut on April 27, 2015, Applied Materials announced that its merger with Tokyo Electron has been scrapped due to fears of dominating the semiconductor equipment industry.\n\nApplied Materials is named among FORTUNE World's Most Admired Companies in 2018.\n\nFor the fiscal year 2018, Applied Materials reported earnings of US$3.313 billion, with an annual revenue of US$17.253 billion, a 33.3% increase over the previous fiscal cycle. Applied Materials market capitalization was valued at over US$36.6 billion in November 2018.\nApplied is organized into three major business sectors: Semiconductor Systems, Applied Global Services, and Display and Adjacent Markets. Applied Materials also operates a venture investing arm called Applied Ventures.\n\nThe company develops and manufactures equipment used in the wafer fabrication steps of creating a semiconductor device, including atomic layer deposition (ALD), chemical vapor deposition (CVD), physical vapor deposition (PVD), rapid thermal processing (RTP), chemical mechanical polishing (CMP), etch, ion implantation and wafer inspection. The company acquired Semitool for this group in late 2009.\n\nThe Applied Global Services (AGS) group offers equipment installation support and warranty extended support, as well as maintenance support. AGS also offers new and refurbished equipment, as well as upgrades and enhancements for installed base equipment. This sector also includes automation software for manufacturing environments.\n\nAGS combined an existing business unit with the display business of Applied Films Corporation, acquired in mid-2006.\n\nThe manufacturing process for TFT LCDs (thin film transistor liquid crystal displays), commonly employed in computer monitors and televisions, is similar to that employed for integrated circuits. In cleanroom environments both TFT-LCD and integrated circuit production use photolithography, chemical and physical vapor deposition, and testing.\n\nIn 2006, the company acquired Applied Films, a glass coating and web coating business. Also in 2006, Applied announced it was entering the solar manufacturing equipment business. The solar, glass and web businesses were organized into the company's Energy and Environmental Solutions (EES) sector.\n\nIn 2007, Applied announced the Applied SunFab thin film photovoltaic module production line, with single or tandem junction capability. SunFab applies silicon thin film layers to glass substrate that then produce electricity when exposed to sunlight. In 2009, the company's SunFab line was certified by the International Electrotechnical Commission (IEC). In 2010, Applied announced that it was abandoning the thin film market and closing down their SunFab division. Also in 2007, the company acquired privately held, Switzerland-based HCT Shaping Systems SA, a specialist in wafer sawing tools for both solar and semiconductor wafer manufacture, paying approximately $475 million.\n\nIn 2008, Applied acquired privately held, Italy-based Baccini SpA for $330M, company that worked in the metallization steps of solar cell manufacturing. The company was listed at the top of VLSI Research's list of supplier of photovoltaic manufacturing equipment for 2008, with sales of $797M.\n\nSince July 2016 this sector is no longer reported separately. Remaining solar business activities have been included in \"Corporate and Others\".\n\nApplied operates in many locations globally, including in Europe, Japan, North America (principally the United States), Israel, China, Italy, India, Korea, Southeast Asia and Taiwan. Applied moved into its Bowers Avenue headquarters in Santa Clara, CA, in 1974.\n\n\n"}
{"id": "49498955", "url": "https://en.wikipedia.org/wiki?curid=49498955", "title": "Ather Energy", "text": "Ather Energy\n\nAther Energy is an Indian electric vehicle company founded by Tarun Mehta and Swapnil Jain in 2013. It manufactures two electric scooter models, 340 and 450. It has also established electric vehicle charging infrastructure AtherGrid.\n\nAther Energy was founded in 2013 by Tarun Mehta and Swapnil Jain. In early 2014, it received Rs 45 lakh from the Technology Development Board under Department of Science and Technology, IIT Madras and Srini V Srinivasan, IIT alumnus and founder of Aerospike. In December 2014, Flipkart founders, Sachin Bansal and Binny Bansal invested $1 million as the seed capital. Sachin Bansal and Binny Bansal expressed a positive sentiment towards the company and showed inclination towards energy-efficient vehicles. In May 2015, it received US$ 12 million (Rs.75.33 crore) from Tiger Global for investments in development, testing, production and the launch of the vehicle. On 23 February 2016, the company unveiled its smart scooter S340 at a technology conference Surge in Bangalore. Hero MotoCorp invested US$ 30.5 million (Rs. 205 crore) as Series B round of funding in October 2016 and gained 32.31% stake in the company. It again invested US$ 19 million (Rs. 130 crore) in 2018.\n\nThe company has announced an online-only purchase model for selling the product with doorstep service. It set up its manufacturing unit in Whitefield, Bangalore which commenced production in 2018 with capacity of 600 vehicles per week. The company has disclosed the price of the scooter to be ₹109,750 for the base 340 model and ₹124,750 for the more powerful 450 version (roughly $1,600 to $1,850 USD). It is also establishing AtherGrid, an electric vehicle charging infrastructure in Bengaluru. The company has setup over 20 charging points across the city, as of July 2018\n\nThe Ather 450 is the more powerful version of the two scooters' sold by Ather. The scooter is constructed using an all aluminium frame, comes with a 5.4kW (7.2 BHP) Brushless DC electric motor, and a 2.4 kWh Lithium ion battery pack. The scooter can accelerate to 40 km/h in 3.9 seconds, attain a top speed of 80 km/h, and can travel 75 km on one charge in city riding conditions(107 km in the Indian Drive Cycle). \n\nThe scooter features a 7-inch touchscreen dashboard, and comes with features like on-board navigation, diagnostics, all-LED lighting, auto-cancelling indicators, smartphone integration, and cloud connectivity to send and receive data from Ather's servers. \n\nAther has started setting up its own charging network dubbed as AtherGrid, in Bengaluru. These DC-fast-charging stations use Ather's proprietary charging method and connector to charge the scooters at a rate of 1km/min. The charging points are also equipped with a 3-pin wall socket to supply AC power to other electric vehicles that do not use Ather's connector. Other vehicle can connect to the charging point and start charging using the AtherGrid App for iOS and Android.\n\nAther has plans to setup around 60 points in Bengaluru, and setup more AtherGrid in other cities as it expands. Ather also sets up a home charging point at customer's homes which will charge the Ather 450 to 80% in 2 hours 40 minutes and to 100% in 4 hours 18 minutes.\n\nUnlike most auto-manufactures in India, Ather Energy owns and operates its Experience Centers, the company claims that this helps in maintaining end-to-end customer experience. The experience centers dubbed, AtherSpace, are aimed at increasing product understanding, providing customer education, and giving a first-hand experience of the product to consumers. Test rides are also provided at the experience centers and can be pre-booked on the company's website. \n\nExperience centers have components of the scooter displayed with useful information provided alongside each of them. There is also a fully built naked scooter on display to show the chassis of the scooter.\n\nThe first experience center opened to consumers on June 8, 2018 at 100-ft road Indiranagar, Bengaluru.\n\nSince the launch, the Ather 450 has gained positive reviews from auto-journalists, based on early testing. The response by consumers has also been mostly positive on various social media platforms. Most reviews claiming the scooter to be the best electric scooter in the country by a fair margin. Ather has provided close to 2,500 test rides at AtherSpace, and has already sold out the Ather 450 until early 2019.\n\nAther 450 deliveries began on 11th, September, 2018. A set of ten vehicles was handed over to the customers at a special delivery event held at Ather's vehicle factory. \n"}
{"id": "42098869", "url": "https://en.wikipedia.org/wiki?curid=42098869", "title": "Aviation in the Digital Age", "text": "Aviation in the Digital Age\n\nThe Information Age is generally understood to have arrived with the Internet as it was developed through the 1970s and rolled out throughout the 1980s, and continues evolving to this day. So too the adoption of digital techniques in aviation also arrived progressively at around the same time and also continues today.\n\nThe use of digital computers in aircraft design was developed by large aerospace companies throughout the 1970s and included technique such as CAD, CAM, structural component stress analysis using FEA and for aerodynamic modelling. Composite materials lend themselves better than metal to fluid \"organic\" aerodynamic shapes of high efficiency, and the advent of sophisticated computer-aided design and modelling has led to an expansion in the use of these materials and forms.\n\nDigital systems also appeared in the aircraft themselves and grew steadily in sophistication. The first FADEC (Full Authority Digital Engine Control) trials took place in 1968, with the first operational system entering service in 1985. The first operational fully authoritative fly-by-wire system was developed for the General Dynamics F-16 Fighting Falcon and its introduction in 1978 heralded a revolution in taking over the task of ensuring stability in flight from the traditional aerodynamic stabilizers. This use of \"relaxed static stability\" allowed aircraft to be made more manoeuvrable and to be given an artificial \"feel\" to aid pilots in their main task. Meanwhile, the \"glass cockpit\" was replacing the traditional analogue electro-mechanical instrumentation with graphical digital displays which could display any information selected. Early glass cockpits provided less critical flight information in the form of the EFIS system, with fully glass systems appearing from 1988.\n\nThe Cold War era ended shortly after the arrival of digital technologies, bringing a marked decrease of military aviation among the major powers. More recently the rise of the Indian and Chinese economies has spurred development of military aircraft in these countries.\n\nThe first operational fully authoritative fly-by-wire system was developed for the General Dynamics F-16 Fighting Falcon and its introduction in 1978 heralded a revolution in taking over the task of ensuring stability in flight from the traditional aerodynamic stabilizers. This use of \"relaxed static stability\" allowed aircraft to be made more manoeuvrable and to be given an artificial \"feel\" to aid pilots in their main task.\n\nComposite materials lend themselves better than metal to fluid \"organic\" aerodynamic shapes of high efficiency, and the advent of sophisticated computer-aided design and modelling has led to an expansion in the use of these materials and forms.\n\nThis period has seen an upsurge in the use of electrical power systems for light aircraft and UAVs. Enabling technologies include the widespread availability and affordability of new high-performance battery technologies, high-strength rare-earth magnets in electric motors, falling costs of solar cells and sophisticated computerised control and management systems.\n\nMeanwhile, conventional aero engines, both piston- and turbine-based, have continued the process of refinement, becoming steadily more reliable and fuel-efficient, while at the same time less polluting.\n\nDigital systems also appeared in the aircraft themselves and grew steadily in sophistication. Early digital systems were self-contained with limited functionality. The first FADEC (Full Authority Digital Engine Control) trials took place in 1968, with the first operational system entering service in 1985.\n\nIntegrated data systems require a digital data bus. The MIL-STD-1553 bus was defined in 1973. This enabled the first operational fully authoritative fly-by-wire system to be developed for the General Dynamics F-16 Fighting Falcon. The introduction of this aircraft in 1978 heralded a revolution in taking over the task of ensuring stability in flight from the traditional aerodynamic stabilizers. This use of \"relaxed static stability\" allowed aircraft to be made more manoeuvrable and to be given an artificial \"feel\" to aid pilots in their main task. Meanwhile, the \"glass cockpit\" was replacing the traditional analogue electro-mechanical instrumentation with graphical digital displays which could display any information selected. Early glass cockpits provided less critical flight information in the form of the EFIS system, with fully glass systems appearing from 1988.\n\nPrior to the Digital Age, unmanned aerial vehicles (UAV) or drones were of limited use, having either limited guidance capability or a vulnerable radio-control link back to a remote pilot.\n\nThe development of lightweight and low-cost sensors such as digital cameras together with mobile computing technologies has allowed UAVs to become more sophisticated and to undertake autonomous flight decisions. UAVs are being increasingly used in both civil and military roles.\n\nUAVs are an attractive attack weapon because they combine the flexibility and firepower of a manned aircraft with the expendability of a missile. They have come to the fore through their use for air-to-ground surgical strikes in Afghanistan. However such use is controversial due to the risk of causing civilian deaths by mistake.\n\nIn the 21st century, civilian UAVs such as the quadcopter are increasingly being used for recreational purposes and for aerial observation via a digital camera.\n\nA micro-UAV is small enough for several to be carried at once, and these are finding applications in military reconnaissance and scientific research.\n\nDuring this period, civil aviation continued to expand. Airliners and engines grew larger and more fuel-efficient, while digital systems progressively took over the flight control and other avionics. Modern jet airliners have glass cockpits, full-authority digital engine and fly-by-wire computerised flight controls and, most recently, Mobile Internet communications connectivity.\n\nMajor disruptions to air travel in the 21st century included the closing of U.S. airspace due to the September 11 attacks, and the closing of most of European airspace after the 2010 eruption of Eyjafjallajökull.\n\nUltralight and microlight aircraft have grown in popularity, along with other sporting activities such as paragliding.\n\nIn 1986 Dick Rutan and Jeana Yeager flew the Rutan Voyager around the world non-stop and with no aerial refuelling. \n\nIn 1999 Bertrand Piccard became the first person to circle the earth in a balloon.\n\nThe use of digital fly-by-wire systems and relaxed static stability gave military aircraft increased manoeuvrability without sacrificing safety or flyability. Advanced tactical manoeuvres such as Pugachev's Cobra became possible.\n\nDigital technology allowed missile guidance systems to shrink in size and to compute and correct their flight path en route. The use of onboard maps, video processing and terrain comparison (TERCOM) software gave cruise missiles unprecedented accuracy.\n\nDuring the postwar period, radar detection was a constant threat to the attacker. Attack aircraft developed the tactic of flying at low level, \"under the radar\" where they were hidden by hills and other obstacles from the radar stations. The advent of low-level radar chains, as a defence against cruise missiles, made this tactic increasingly difficult. At the same time, advances in electromagnetic radiation-absorbent materials (RAM) and electromagnetic modelling techniques offered the opportunity to develop \"stealthy\" aircraft which would be invisible to the defending radar. The first stealthy attack aircraft, the Lockheed F-117 Nighthawk entered service in 1983. Today, stealth is a requirement for any advanced attack aircraft.\n\nThe \"U.S. Centennial of Flight Commission\" was established in 1999 to encourage the broadest national and international participation in the celebration of 100 years of powered flight. It publicized and encouraged a number of programs, projects and events intended to educate people about the history of aviation.\n\nThe widespread use of digital techniques throughout design and manufacture has led to a revolution in aircraft design. Now, a designer can create an aircraft, model its aerodynamic and mechanical characteristics, design the production components and have them manufactured on the shop floor, all within a single end-to-end digital domain.\n\nThe increasing use of fibre composite materials has also led to ever-larger autoclaves for applying and curing the resin which binds the structural fibres in place. Novel test and inspection techniques have also had to be developed, as the failure modes and symptoms of composite components tend to be very different from those made of metal. For example, layers of fibre can delaminate within a multi-layer component, weakening it with no outward visible sign of cracking. Where a metal skin tends to conduct the current from a lightning strike in all directions and to shield sensitive components, carbon fibre tends to conduct along the fibres and to allow more of the energy into the interior, requiring more careful design to protect critical flight components from lightning EMP.\n\nThe increasing sophistication of avionics systems has led to longer development times. In particular the use of digital flight systems such as fly-by-wire has led to an ever-increasing sophistication and complexity of the control software, which can take many years to develop and validate. During this period, any change to the aircraft's physical design may require revision and revalidation of the associated software.\n\nAs computers became more sophisticated in the 2000s, they began to take over routine aspects of the air traffic controller's task. Up until then all air traffic in nearby airspace was tracked and displayed, with the air traffic controller responsible for monitoring its position and assessing any need for action. Modern computerised systems are capable of monitoring the flight paths of many more aircraft at a given time, allowing the controller to manage more aircraft and to focus on the decision-making and follow-up processes.\n\n"}
{"id": "5136643", "url": "https://en.wikipedia.org/wiki?curid=5136643", "title": "Blunger", "text": "Blunger\n\nA blunger is a machine commonly used in the pottery industry for mixing slip, a mixture of clay and water. A blunger usually consists of a round or octagonal tank with a mixer. Clay is added to the water-filled blunger and then mixed into a slurry, which is also called slip. The electrical charge of the clay particles is neutralized by the addition of a deflocculant, which assists in keeping the particles in suspension. This slip can be then sieved as it is emptied from the blunger.\n\nIn the potbanks around Stoke-on-Trent the blunger was fed by the \"sliphouse blunger charger\", who was often assisted by the \"sliphouse blunger charger's mate\". Different types and amounts of clay and marl had to be shovelled in manually to be mixed with water.\n\nBlungers are often used in the kaolin mining industry to mix certain grades of kaolin clay with water.\n"}
{"id": "163269", "url": "https://en.wikipedia.org/wiki?curid=163269", "title": "Claude Chappe", "text": "Claude Chappe\n\nClaude Chappe (25 December 1763 – 23 January 1805) was a French inventor who in 1792 demonstrated a practical semaphore system that eventually spanned all of France. This was the first practical telecommunications system of the industrial age, making Chappe the first telecom mogul with his \"mechanical internet.\"\n\nChappe was born in Brûlon, Sarthe, France, the grandson of a French baron. He was raised for church service, but lost his sinecure during the French Revolution. He was educated at the \"Lycée Pierre Corneille\" in Rouen.\n\nHis uncle was the astronomer Jean-Baptiste Chappe d'Auteroche famed for his observations of the Transit of Venus in 1761 and again in 1769. The first book Claude read in his youth was \nhis uncle's journal of the 1761 trip, \"Voyage en Siberie\". His brother, Abraham, wrote \"Reading this book greatly inspired him, and gave him a taste for the physical sciences. From this point on, all his studies, and even his pastimes, were focused on that subject.\" Because of his astronomer uncle, Claude may also have become familiar with the properties of telescopes.\n\nHe and his four unemployed brothers decided to develop a practical system of semaphore relay stations, a task proposed in antiquity, yet never realized.\n\nClaude's brother, Ignace Chappe (1760–1829) was a member of the Legislative Assembly during the French Revolution. With his help, the Assembly supported a proposal to build a relay line from Paris to Lille (fifteen stations, about 120 miles), to carry dispatches from the war.\n\nThe Chappe brothers determined by experiment that the angles of a rod were easier to see than the presence or absence of panels. Their final design had two arms connected by a cross-arm. Each arm had seven positions, and the cross-arm had four more permitting a 196-combination code. The arms were from three to thirty feet long, black, and counterweighted, moved by only two handles. Lamps mounted on the arms proved unsatisfactory for night use. The relay towers were placed from 12 to 25 km (10 to 20 miles) apart. Each tower had a telescope pointing both up and down the relay line. Chappe first called his invention the \"tachygraph\", which means fast writer but the army started the word telegraph meaning far writer coined by the statesman André François Miot de Mélito.\n\nIn 1792, the first messages were successfully sent between Paris and Lille. In 1794 the semaphore line informed Parisians of the capture of Condé-sur-l'Escaut from the Austrians less than an hour after it occurred. Other lines were built, including a line from Paris to Toulon. The system was widely copied by other European states, and was used by Napoleon to coordinate his empire and army.\n\nIn 1805, Claude Chappe killed himself. He was said to be depressed by illness, and claims by rivals that he had plagiarized from military semaphore systems.\n\nIn 1824 Ignace Chappe attempted to increase interest in using the semaphore line for commercial messages, such as commodity prices; however, the business community resisted.\n\nIn 1846, the government of France committed to a new system of electric telegraph lines. Many contemporaries warned of the ease of sabotage and interruption of service by cutting a wire. With the emergence of the electric telegraph, slowly the Chappe telegraph ended in 1852.\n\nThe Chappe semaphore figures prominently in Alexandre Dumas' \"The Count of Monte Cristo\". The Count bribes an underpaid \noperator to transmit a false message.\n\nA bronze sculpture of Claude Chappe was erected at the crossing of Rue du Bac and Boulevard Raspail, in Paris. It was removed and melted down during the Nazi occupation of Paris, in 1941 or 1942.\n\n\n"}
{"id": "13580135", "url": "https://en.wikipedia.org/wiki?curid=13580135", "title": "Conditional short-circuit current", "text": "Conditional short-circuit current\n\nConditional short-circuit current is defined as the value of the alternating current (a.c) component of a prospective current, which a switch without integral short-circuit protection, but protected by a suitable short circuit protective device (SCPD) in series, can withstand for the operating time of the current under specified test conditions. It may be understood to be the RMS value of the maximum permissible current over a specified time interval (t,t) and operating conditions.\nThe IEC definition is critiqued to be open to interpretation. \n\nformula_1\n"}
{"id": "23265726", "url": "https://en.wikipedia.org/wiki?curid=23265726", "title": "Corrinne Yu", "text": "Corrinne Yu\n\nCorrinne Yu is an American game programmer.\n\nYu attended California State Polytechnic University, Pomona to study electrical engineering before beginning her career as a professional programmer.\n\nYu's early career was as a programmer for the \"King's Quest\" series for the Apple II although she had her own 3D engine projects that she sold to various companies. She programmed for QuickDraw 3D, an early rasterisation API. She worked on the game \"Zombie\", and created the video game engine used in \"Spec Ops\". In November 1997, she was employed by video game developer Ion Storm. She worked on the 2001 video game \"Anachronox\" and served as Director of Technology at the studio. While at Ion she was responsible for the \"Quake 2\" code base used in their games and any games based on that engine. In November 1998, she left Ion Storm and later became the Lead Technology Programmer at 3D Realms. Yu worked as an engine programmer at Gearbox Software, creator of \"Brothers in Arms\" and \"Borderlands\". Yu worked to heavily modify the Epic Unreal Engine 3 with an emphasis on lighting, shadows and physics. Yu was a founding member of Microsoft's Direct 3D Advisory Board. She participated in CUDA and GPU simulation at NVidia.\n\nIn 2008, Microsoft Studios hired Yu as the Principal Engine Architect for an internal studio, 343 Industries. 343 Industries was established in 2007 to oversee the \"Halo\" franchise following Bungie's separation from Microsoft. Yu programmed lighting, facial animation, and developed new technology for the 2012 video game \"Halo 4\". While coding on Halo team, Yu researched new lighting techniques, and invented new dynamic radiosity algorithms. Microsoft applied a software patent for Yu's Halo lighting work.\n\nIn November 2013, Yu joined video game developer Naughty Dog, a subsidiary of Sony Computer Entertainment, to work as a graphic programmer on PlayStation 4 projects. In November 2014, she left Naughty Dog and joined Amazon.com to work on their Amazon Prime Air program. In March 2018, she left Amazon and joined General Motors as a VP of Engineering.\n\nBesides working as a game programmer, Yu programmed on the Space Shuttle program at Rockwell International California. She designed and conducted accelerator experiments at LINAC in California and the accelerator at Brookhaven National Laboratory. Her nuclear physics research won her a national award from the U.S. Department of Energy. In 2009, Corrinne Yu won Best in Engineering internationally at GDC (Game Developers Conference) WiG nominated and judged by a panel of her industry peers for the last 2 years in a row, for her work in programming. In 2010, Yu was identified by Kotaku as one of the 10 most influential women in games in the last decade. She is the only director of technology, and the only engine programmer, on this list.\n\nYu is married to Kenneth Scott, Senior Art Director at 343 Industries. Together they have a daughter.\n\nYu is driven by her interest in how complex pieces can be made to fit together, and compared every day to playing a game of \"Minecraft\", only more flexible and with greater real world applicability.\n\n\n"}
{"id": "8410", "url": "https://en.wikipedia.org/wiki?curid=8410", "title": "Decibel", "text": "Decibel\n\nThe decibel (symbol: dB) is a unit of measurement used to express the ratio of one value of a physical property to another on a logarithmic scale. It can be used to express a change in value (e.g., +1 dB or −1 dB) or an absolute value. In the latter case, it expresses the ratio of a value to a reference value; when used in this way, the decibel symbol should be appended with a suffix that indicates the reference value, or some other property. For example, if the reference value is 1 volt, then the suffix is \"V\" (e.g., \"20 dBV\"), and if the reference value is one milliwatt, then the suffix is \"m\" (e.g., \"20 dBm\").\n\nThere are two different scales used when expressing a ratio in decibels depending on the nature of the quantities: power and field (root-power). When expressing power quantities, the number of decibels is ten times the logarithm to base 10 of the ratio of two power quantities. That is, a change in \"power\" by a factor of 10 corresponds to a 10 dB change in level. When expressing field (root-power) quantities, a change in \"amplitude\" by a factor of 10 corresponds to a 20 dB change in level. The extra factor of two is due to the logarithm of the quadratic relationship between power and amplitude. The decibel scales differ so that direct comparisons can be made between related power and field quantities when they are expressed in decibels.\n\nThe definition of the decibel is based on the measurement of power in telephony of the early 20th century in the Bell System in the United States. One decibel is one tenth (deci-) of one bel, named in honor of Alexander Graham Bell; however, the bel is seldom used. Today, the decibel is used for a wide variety of measurements in science and engineering, most prominently in acoustics, electronics, and control theory. In electronics, the gains of amplifiers, attenuation of signals, and signal-to-noise ratios are often expressed in decibels.\n\nIn the International System of Quantities, the decibel is defined as a unit of measurement for quantities of type level or level difference, which are defined as the logarithm of the ratio of power- or field-type quantities.\n\nThe decibel originates from methods used to quantify signal loss in telegraph and telephone circuits. The unit for loss was originally \"Miles of Standard Cable\" (MSC). 1 MSC corresponded to the loss of power over a 1 mile (approximately 1.6 km) length of standard telephone cable at a frequency of 5000 radians per second (795.8 Hz), and matched closely the smallest attenuation detectable to the average listener. The standard telephone cable implied was \"a cable having uniformly distributed resistance of 88 Ohms per loop-mile and uniformly distributed shunt capacitance of 0.054 microfarads per mile\" (approximately corresponding to 19 gauge wire).\n\nIn 1924, Bell Telephone Laboratories received favorable response to a new unit definition among members of the International Advisory Committee on Long Distance Telephony in Europe and replaced the MSC with the \"Transmission Unit\" (TU). 1 TU was defined such that the number of TUs was ten times the base-10 logarithm of the ratio of measured power to a reference power.\nThe definition was conveniently chosen such that 1 TU approximated 1 MSC; specifically, 1 MSC was 1.056 TU. In 1928, the Bell system renamed the TU into the decibel, being one tenth of a newly defined unit for the base-10 logarithm of the power ratio. It was named the \"bel\", in honor of the telecommunications pioneer Alexander Graham Bell.\nThe bel is seldom used, as the decibel was the proposed working unit.\n\nThe naming and early definition of the decibel is described in the NBS Standard's Yearbook of 1931:\n\nIn 1954, C. W. Horton argued that the use of the decibel as a unit for quantities other than transmission loss led to confusion, and suggested the name 'logit' for \"standard magnitudes which combine by addition\".\n\nIn April 2003, the International Committee for Weights and Measures (CIPM) considered a recommendation for the inclusion of the decibel in the International System of Units (SI), but decided against the proposal. However, the decibel is recognized by other international bodies such as the International Electrotechnical Commission (IEC) and International Organization for Standardization (ISO). The IEC permits the use of the decibel with field quantities as well as power and this recommendation is followed by many national standards bodies, such as NIST, which justifies the use of the decibel for voltage ratios. The term \"field quantity\" is deprecated by ISO 80000-1, which favors root-power. In spite of their widespread use, suffixes (such as in dBA or dBV) are not recognized by the IEC or ISO.\n\nISO 80000-3 describes definitions for quantities and units of space and time. The decibel for use in acoustics is defined in ISO 80000-8. The major difference from the article below is that for acoustics the decibel has no absolute value.\n\nThe ISO Standard 80000-3:2006 defines the following quantities. The decibel (dB) is one-tenth of a bel: . The bel (B) is  ln(10) nepers: . The neper is the change in the level of a field quantity when the field quantity changes by a factor of \"e\", that is , thereby relating all of the units as nondimensional natural log of field-quantity ratios, . Finally, the level of a quantity is the logarithm of the ratio of the value of that quantity to a reference value of the same kind of quantity.\n\nTherefore, the bel represents the logarithm of a ratio between two power quantities of 10:1, or the logarithm of a ratio between two field quantities of :1.\n\nTwo signals whose levels differ by one decibel have a power ratio of 10, which is approximately 1.25893, and an amplitude (field quantity) ratio of 10 (1.12202).\n\nThe bel is rarely used either without a prefix or with SI unit prefixes other than \"deci\"; it is preferred, for example, to use \"hundredths of a decibel\" rather than \"millibels\". Thus, five one-thousandths of a bel would normally be written '0.05 dB', and not '5 mB'.\n\nThe method of expressing a ratio as a level in decibels depends on whether the measured property is a \"power quantity\" or a \"field quantity\"; see Field, power, and root-power quantities for details.\n\nWhen referring to measurements of \"power\" quantities, a ratio can be expressed as a level in decibels by evaluating ten times the base-10 logarithm of the ratio of the measured quantity to reference value. Thus, the ratio of \"P\" (measured power) to \"P\" (reference power) is represented by \"L\", that ratio expressed in decibels, which is calculated using the formula:\n\nThe base-10 logarithm of the ratio of the two power quantities is the number of bels. The number of decibels is ten times the number of bels (equivalently, a decibel is one-tenth of a bel). \"P\" and \"P\" must measure the same type of quantity, and have the same units before calculating the ratio. If in the above equation, then \"L\" = 0. If \"P\" is greater than \"P\" then \"L\" is positive; if \"P\" is less than \"P\" then \"L\" is negative.\n\nRearranging the above equation gives the following formula for \"P\" in terms of \"P\" and \"L\":\n\nWhen referring to measurements of field quantities, it is usual to consider the ratio of the squares of \"F\" (measured field) and \"F\" (reference field). This is because in most applications power is proportional to the square of field, and it is desirable for the two decibel formulations to give the same result in such typical cases. Thus, the following definition is used:\n\nThe formula may be rearranged to give\n\nSimilarly, in electrical circuits, dissipated power is typically proportional to the square of voltage or current when the impedance is held constant. Taking voltage as an example, this leads to the equation:\nwhere \"V\" is the voltage being measured, \"V\" is a specified reference voltage, and \"G\" is the power gain expressed in decibels. A similar formula holds for current.\n\nThe term \"root-power quantity\" is introduced by ISO Standard 80000-1:2009 as a substitute of \"field quantity\". The term \"field quantity\" is deprecated by that standard.\n\nSince logarithm differences measured in these units are used to represent power ratios and field ratios, the values of the ratios represented by each unit are also included in the table.\n\nAll of these examples yield dimensionless answers in dB because they are relative ratios expressed in decibels. The unit dBW is often used to denote a ratio for which the reference is 1 W, and similarly dBm for a reference point.\n, illustrating the consequence from the definitions above that \"G\" has the same value, 30, regardless of whether it is obtained from powers or from amplitudes, provided that in the specific system being considered power ratios are equal to amplitude ratios squared.\n\nA change in power ratio by a factor of 10 corresponds to a change in level of . A change in power ratio by a factor of 2 or is approximately a change of 3 dB. More precisely, the change is ±3.0103 dB, but this is almost universally rounded to \"3 dB\" in technical writing. This implies an increase in voltage by a factor of . Likewise, a doubling or halving of the voltage, and quadrupling or quartering of the power, is commonly described as \"6 dB\" rather than ±6.0206 dB.\n\nShould it be necessary to make the distinction, the number of decibels is written with additional significant figures. 3.00 dB is a power ratio of 10, or 1.9953, about 0.24% different from exactly 2, and a voltage ratio of 1.4125, 0.12% different from exactly . Similarly, an increase of 6.00 dB is the power ratio is , about 0.5% different from 4.\n\nThe decibel is useful for representing large ratios and for simplifying representation of multiplied effects such as attenuation from multiple sources along a signal chain. Its application in systems with additive effects is less intuitive.\n\nThe logarithmic scale nature of the decibel means that a very large range of ratios can be represented by a convenient number, in a similar manner to scientific notation. This allows one to clearly visualize huge changes of some quantity. See Bode plot and semi-log plot. For example, 120 dB SPL may be clearer than \"a trillion times more intense than the threshold of hearing\".\n\nLevel values in decibels can be added instead of multiplying the underlying power values, which means that the overall gain of a multi-component system, such as a series of amplifier stages, can be calculated by summing the gains in decibels of the individual components, rather than multiply the amplification factors; that is, log(\"A\" × \"B\" × \"C\") = log(\"A\") + log(\"B\") + log(\"C\"). Practically, this means that, armed only with the knowledge that 1 dB is approximately 26% power gain, 3 dB is approximately 2× power gain, and 10 dB is 10× power gain, it is possible to determine the power ratio of a system from the gain in dB with only simple addition and multiplication. For example:\nHowever, according to its critics, the decibel creates confusion, obscures reasoning, is more related to the era of slide rules than to modern digital processing, and is cumbersome and difficult to interpret.\n\nAccording to Mitschke, \"The advantage of using a logarithmic measure is that in a transmission chain, there are many elements concatenated, and each has its own gain or attenuation. To obtain the total, addition of decibel values is much more convenient than multiplication of the individual factors.\" However, for the same reason that humans excel at additive operation over multiplication, decibels are awkward in inherently additive operations: \"if two machines each individually produce a [sound pressure] level of, say, 90 dB at a certain point, then when both are operating together we should expect the combined sound pressure level to increase to 93 dB, but certainly not to 180 dB!\" \"suppose that the noise from a machine is measured (including the contribution of background noise) and found to be 87 dBA but when the machine is switched off the background noise alone is measured as 83 dBA. ... the machine noise [level (alone)] may be obtained by 'subtracting' the 83 dBA background noise from the combined level of 87 dBA; i.e., 84.8 dBA.\" \"in order to find a representative value of the sound level in a room a number of measurements are taken at different positions within the room, and an average value is calculated. (...) Compare the logarithmic and arithmetic averages of ... 70 dB and 90 dB: logarithmic average = 87 dB; arithmetic average = 80 dB.\"\n\nAddition on a logarithmic scale is called logarithmic addition, and can be defined by taking exponentials to convert to a linear scale, adding there, and then taking logarithms to return. For example, where operations on decibels are logarithmic addition/subtraction and logarithmic multiplication/division, while operations on the linear scale are the usual operations:\nNote that the logarithmic mean is obtained from the logarithmic sum by subtracting formula_12, since logarithmic division is linear subtraction.\nQuantities in decibels are not necessarily additive, thus being \"of unacceptable form for use in dimensional analysis\".\n\nThe human perception of the intensity of sound and light approximates the logarithm of intensity rather than a linear relationship (Weber–Fechner law), making the dB scale a useful measure.\n\nThe decibel is commonly used in acoustics as a unit of sound pressure level. The reference pressure for sound in air is set at the typical threshold of perception of an average human and there are common comparisons used to illustrate different levels of sound pressure. Sound pressure is a field quantity, therefore the field version of the unit definition is used:\nwhere \"p\" is the root mean square of the measured sound pressure in pascals and \"p\" is the standard reference sound pressure of 20 micropascals in air or 1 micropascal in water.\n\nUse of the decibel in underwater acoustics leads to confusion, in part because of this difference in reference value.\n\nThe human ear has a large dynamic range in sound reception. The ratio of the sound intensity that causes permanent damage during short exposure to that of the quietest sound that the ear can hear is greater than or equal to 1 trillion (10). Such large measurement ranges are conveniently expressed in logarithmic scale: the base-10 logarithm of 10 is 12, which is expressed as a sound pressure level of 120 dB re 20 μPa.\n\nSince the human ear is not equally sensitive to all sound frequencies, noise levels at maximum human sensitivity, somewhere between 2 and 4 kHz, are factored more heavily into some measurements using frequency weighting. (See also Stevens' power law.)\n\nThe main instrument used for measuring sound levels in the environment and in the workplace is the Sound Level Meter. Most sound level meters provide readings in A, C, and Z-weighted decibels and must meet international standards such as IEC 61672 - 2013. \n\nAccording to Hickling, \"Decibels are a useless affectation, which is impeding the development of noise control as an engineering discipline\".\nIn electronics, the decibel is often used to express power or amplitude ratios (gains), in preference to arithmetic ratios or percentages. One advantage is that the total decibel gain of a series of components (such as amplifiers and attenuators) can be calculated simply by summing the decibel gains of the individual components. Similarly, in telecommunications, decibels denote signal gain or loss from a transmitter to a receiver through some medium (free space, waveguide, coaxial cable, fiber optics, etc.) using a link budget.\n\nThe decibel unit can also be combined with a suffix to create an absolute unit of electric power. For example, it can be combined with \"m\" for \"milliwatt\" to produce the \"dBm\". A power level of 0 dBm corresponds to one milliwatt, and 1 dBm is one decibel greater (about 1.259 mW).\n\nIn professional audio specifications, a popular unit is the dBu. This is relative to the root mean square voltage which delivers 1 mW (0 dBm) into a 600-ohm resistor, or ≈ 0.775 V. When used in a 600-ohm circuit (historically, the standard reference impedance in telephone circuits), dBu and dBm are identical.\n\nIn an optical link, if a known amount of optical power, in dBm (referenced to 1 mW), is launched into a fiber, and the losses, in dB (decibels), of each component (e.g., connectors, splices, and lengths of fiber) are known, the overall link loss may be quickly calculated by addition and subtraction of decibel quantities.\n\nIn spectrometry and optics, the blocking unit used to measure optical density is equivalent to −1 B.\n\nIn connection with video and digital image sensors, decibels generally represent ratios of video voltages or digitized light intensities, using 20 log of the ratio, even when the represented optical power is directly proportional to the voltage, not to its square, as in a CCD imager where response voltage is linear in intensity.\nThus, a camera signal-to-noise ratio or dynamic range quoted as 40 dB represents a power ratio of 100:1 between signal power and noise power, not 10,000:1.\nSometimes the 20 log ratio definition is applied to electron counts or photon counts directly, which are proportional to intensity without the need to consider whether the voltage response is linear.\n\nHowever, as mentioned above, the 10 log intensity convention prevails more generally in physical optics, including fiber optics, so the terminology can become murky between the conventions of digital photographic technology and physics. Most commonly, quantities called \"dynamic range\" or \"signal-to-noise\" (of the camera) would be specified in 20 log dB, but in related contexts (e.g. attenuation, gain, intensifier SNR, or rejection ratio) the term should be interpreted cautiously, as confusion of the two units can result in very large misunderstandings of the value.\n\nPhotographers typically use an alternative base-2 log unit, the stop, to describe light intensity ratios or dynamic range.\n\nSuffixes are commonly attached to the basic dB unit in order to indicate the reference value by which the ratio is calculated. For example, dBm indicates power measurement relative to 1 milliwatt.\n\nIn cases where the unit value of the reference is stated, the decibel value is known as \"absolute\". If the unit value of the reference is not explicitly stated, as in the dB gain of an amplifier, then the decibel value is considered relative.\n\nThe SI does not permit attaching qualifiers to units, whether as suffix or prefix, other than standard SI prefixes. Therefore, even though the decibel is accepted for use alongside SI units, the practice of attaching a suffix to the basic dB unit, forming compound units such as dBm, dBu, dBA, etc., is not. The proper way, according to the IEC 60027-3, is either as \"L\" (re \"x\") or as \"L\", where \"x\" is the quantity symbol and \"x\" is the value of the reference quantity, e.g., \"L\" (re 1 μV/m) = \"L\" for the electric field strength \"E\" relative to 1 μV/m reference value.\n\nOutside of documents adhering to SI units, the practice is very common as illustrated by the following examples. There is no general rule, with various discipline-specific practices. Sometimes the suffix is a unit symbol (\"W\",\"K\",\"m\"), sometimes it is a transliteration of a unit symbol (\"uV\" instead of μV for microvolt), sometimes it is an acronym for the unit's name (\"sm\" for square meter, \"m\" for milliwatt), other times it is a mnemonic for the type of quantity being calculated (\"i\" for antenna gain with respect to an isotropic antenna, \"λ\" for anything normalized by the EM wavelength), or otherwise a general attribute or identifier about the nature of the quantity (\"A\" for A-weighted sound pressure level). The suffix is often connected with a dash (dB-Hz), with a space (dB HL), with no intervening character (dBm), or enclosed in parentheses, dB(sm).\n\nSince the decibel is defined with respect to power, not amplitude, conversions of voltage ratios to decibels must square the amplitude, or use the factor of 20 instead of 10, as discussed above.\n\n\n\n\n\n\nProbably the most common usage of \"decibels\" in reference to sound level is dB SPL, sound pressure level referenced to the nominal threshold of human hearing: The measures of pressure (a field quantity) use the factor of 20, and the measures of power (e.g. dB SIL and dB SWL) use the factor of 10.\n\nSee also dBV and dBu above.\n\n\n\n\n\n\n\n\n\n\n\nNp or cNp\n\nAttenuation constants, in fields such as optical fiber communication and radio propagation path loss, are often expressed as a fraction or ratio to distance of transmission. \"dB/m\" means decibels per meter, \"dB/mi\" is decibels per mile, for example. These quantities are to be manipulated obeying the rules of dimensional analysis, e.g., a 100-meter run with a 3.5 dB/km fiber yields a loss of 0.35 dB = 3.5 dB/km × 0.1 km.\n\n\n\n"}
{"id": "34335120", "url": "https://en.wikipedia.org/wiki?curid=34335120", "title": "DevLab (research alliance)", "text": "DevLab (research alliance)\n\nDevLab (Development laboratory) is a research center headquartered in Eindhoven, The Netherlands. It is an alliance of thirteen Small and Medium Enterprises. In close co-operation with universities, with a network of professors and lectors, research projects are carried out by graduation students, Ph.D students, and employees of the member SMEs. With this concept DevLab is also partner in larger consortia, together with industry, universities and other research institutes.\n\nThe Development Club is one of the clusters of the Federation of Technology Branches in the Netherlands, FHI, within the branch industrial electronics. It is a network of approximately 45 technology companies working on product development in the field of electronics, mechatronics, embedded software and industrial design. The Development Club is the source of the idea to set up a co-operation where scientific research will be carried out, by and for the member companies. This eventually led to the foundation in 2004 of DevLab, Development Laboratories.\n\nThe DevLab research agenda currently contains 4 focus areas:\n\nThese areas are captured into a number of projects DevLab is leading or participating in.\n\n"}
{"id": "28014568", "url": "https://en.wikipedia.org/wiki?curid=28014568", "title": "Double-spending", "text": "Double-spending\n\nDouble-spending is a potential flaw in a digital cash scheme in which the same single digital token can be spent more than once. Unlike physical cash, a digital token consists of a digital file that can be duplicated or falsified. As with counterfeit money, such double-spending leads to inflation by creating a new amount of copied currency that did not previously exist. This devalues the currency relative to other monetary units or goods and diminishes user trust as well as the circulation and retention of the currency. Fundamental cryptographic techniques to prevent double-spending, while preserving anonymity in a transaction, are blind signatures and, particularly in offline systems, secret splitting.\n\nA \"double spending attack\", is a potential attack against cryptocurrencies that has happened to several cryptocurrencies, e.g. due to the 51% attack. While it hasn't happened against many of the largest cryptocurrencies, such as Bitcoin (with even the capability arising for it in 2014), it has happened to one of its forks, Bitcoin Gold, then 26th largest cryptocurrency.\n\nThe prevention of double-spending attack has taken two general forms: centralized and decentralized.\n\nThis is usually implemented using an online central trusted third party that can verify whether a token has been spent. This normally represents a single point of failure from both availability and trust viewpoints.\n\nBy 2007, a number of distributed systems for the prevention of double-spending had been proposed.\n\nThe cryptocurrency bitcoin implemented a solution in early 2009. It uses a cryptographic protocol called a proof-of-work system to avoid the need for a trusted third party to validate transactions. Instead, transactions are recorded in a public ledger called a blockchain. A transaction is considered valid when it is included in the blockchain that contains the greatest amount of computational work. This makes double-spending more difficult as the size of the overall network grows. Other cryptocurrencies also have similar features.\n\nDecentralized currencies that rely on blockchain are vulnerable to the , in which a malicious actor can rewrite the ledger if they control enough of the computational work being done. For example, one could theoretically spend cryptocurrency then erase the transaction so it appears it never happened. In May 2018, this double-spending technique was used against Bitcoin Gold, the 26th largest cryptocurrency, to defraud cryptocurrency exchanges of millions of dollars. In response, exchanges repeatedly raised the threshold needed to confirm a transaction, but the attacker had enough computing power to exceed those thresholds and continued double-spending for three days.\n"}
{"id": "19967008", "url": "https://en.wikipedia.org/wiki?curid=19967008", "title": "Dynamic combustion chamber", "text": "Dynamic combustion chamber\n\nThe Dynamic Combustion Chamber is an engine that allows for the combustion of gases in a vacuum or under pressure, eliminating the production of air pollutants. Combined with a clean renewable source of electricity, the Dynamic Combustion Chamber would serve as a battery to store and discharge electricity when needed. This would fill in the gaps that sources of electricity such as wind and solar have during different time periods, and replace peaker plants or fossil fuel based power plants which supply electricity to the grid when there is a shortage. \n\nHydrogen and Oxygen gas are fed into a vacuum inside a chamber. The hydrogen is combusted with oxygen\nto form water and heat. A heat exchanger is inside the chamber to remove the heat from the chamber.\nThe heat exchanger has water inside which is boiled to make steam. The steam can be used to create\nelectricity through a turbine and generator, or it can be used as a source of heat. The water created in the chamber can be recycled for creating steam or for electrolysis.\n\nSOG R&D Corporation, the company which holds the patents on the Dynamic Combustion Chamber, has contracted the Hydrogen Production and Utilization Laboratory at University of California, Davis to perform the testing. A lab bench scale model was built and verified that it produced steam and also that there were no pollutants formed during the process. \n\nTypically combustion based engines use the air as their source of oxygen. The problem with using\nair is that it contains other gases than oxygen. Gases such as nitrogen react with oxygen at high\ntemperatures to form pollutants. By moving the combustion reaction inside a vacuum and using pure\noxygen, there are no pollutants formed during the reaction. \n\nRequiring pure oxygen limits the portability and usability of the Dynamic Combustion Chamber. A vehicle would need to carry a tank of hydrogen and oxygen. Competing technologies such as Fuel Cells can use hydrogen and air, however using air instead of pure oxygen makes fuel cells less efficient. Considering the need for pure oxygen as well as hydrogen, hydrolysis or\nsplitting water molecules into hydrogen and oxygen, seems to be the most practical source of reactants. \n\nTo generate electricity a turbine with a generator is necessary. A fuel cell can generate electricity directly, removing extra moving parts. Since fuel cells do not have any moving parts they do not lose energy to friction. Fuel cells generate heat as well as electricity which reduces their efficiency.\n\nJust like other engines the Dynamic Combustion Chamber needs a source of fuel. There has been some controversy about whether hydrogen technology actually reduces pollution or just moves it to a more central source such as a power plant. By using clean renewable sources such as wind or solar, hydrogen can be produced without any pollution. This hydrogen can be transported or consumed later when needed. This adds to the portability and usability of an electricity source. Commonly in the electrical grid there are base load power plants and peaker plants.Using hydrogen as an energy storage medium would reduce emissions as peaker plants are particularly inefficient.\n\nhttp://sograndd.com/news.html\n\nFitzgerald, Michael (6/25/06). \"Invention has huge potential\"\nhttp://www.recordnet.com/apps/pbcs.dll/article?AID=/20060625/OPED0301/606250357/-1/OPED03\n"}
{"id": "15041503", "url": "https://en.wikipedia.org/wiki?curid=15041503", "title": "Dynamit Nobel", "text": "Dynamit Nobel\n\nDynamit Nobel AG is a German chemical and weapons company whose headquarters is in Troisdorf, Germany. It was founded in 1865 by Alfred Nobel.\n\nAfter the death of his younger brother Emil in an 1864 nitroglycerin explosion at the family's armaments factory in Heleneborg, Stockholm, Nobel founded Nitroglycerin AB in Vinterviken, Stockholm. A year later, having found some German business partners, he launched the Alfred Nobel & Company in Germany, building an isolated factory in the Krümmel hills of Geesthacht near Hamburg. This business exported a liquid combination of nitroglycerin and gunpowder known as \"Blasting Oil\", but it was extremely unstable and difficult to transport, as shown in numerous catastrophes. The buildings of the Krümmel factory itself were destroyed in 1866 and again in 1870.\n\nIn April 1866, the company shipped three unmarked crates of nitroglycerin to California for the Central Pacific Railroad, who wished to experiment with its blasting capability to speed the construction of a tunnel through the Sierra Nevada for the First Transcontinental Railroad. One of the crates exploded, destroying a Wells Fargo office in San Francisco and killing fifteen people, leading to a complete ban on the transport of liquid nitroglycerin in California.\n\nLiquid nitroglycerin was widely banned elsewhere as well, and this finally led to Alfred Nobel & Company's development of dynamite in 1867, made by mixing the nitroglycerin with the diatomaceous earth (\"kieselguhr\") found in the Krümmel hills. Competitors tried to mix nitroglycerin with other inert absorbents in many different combinations to get around Nobel's tightly controlled patents.\n\n\nDynamit Nobel AG originates from the company Alfred Nobel & Co., founded on 21 June 1865 in Hamburg by the Swedish chemist and industrialist Alfred Nobel. At the beginning, the company was manufacturing nitroglycerin explosives in the dynamite factory of Krümmel located in Geesthacht, near Hamburg. This factory was the first one to be located outside of Sweden.\n\nNobel wanted to produce nitroglycerine on several sites in Europe because the transportation of explosives was very dangerous due to its well-known sensitivity to shocks.\n\nBecause of the danger associated with handling nitroglycerine, Nobel started to develop a more secure explosive commonly known as dynamite. During the experimental stage, a very severe explosion occurred in 1866 within the Krümmel factory which was nearly destroyed. He was successful in mixing nitroglycerine with kieselgur which made it less sensitive to shocks. In October 1867, Nobel filed a patent for this new explosive in Sweden, the United States of America and in the United Kingdom (the patent was not filed in Germany before 1877). The new explosive was marketed under the name of security powder. In 1874, in order to ensure a better supply of the main buyers, the mines of the region of Ruhr, the company took over the Schlebusch factory in Manfort (since 1930 a district of Leverkusen; Nobel was involved in its construction since 1872 and supervised the production on a temporary basis.\nIn 1876, Nobel’s company became a limited liability company and was renamed Dynamit AG, i.e. DAG). The company started manufacturing defense equipment, and soon afterwards became the biggest manufacturer of powder and ammunitions of the German Reich.\n\nIn 1884, similar to other European countries, the biggest German powder manufacturers agreed to form a cartel (here known as « Interessensgemeinschaft ») named Deutsche Union which was led by Dynamit Nobel for five years. All the largest manufacturers of powder of the German Reich abided by this union which prevented them from competing against each other due to their cooperation and agreement upon the export prices. In 1886, they approached the English powder cartel, the Nobel Dynamite Trust Co and managed to establish the General cartel made up of both the German and English powder factories.\n\nDue to the high demand for defense equipments for the First World War, the manufacturers of powder made very high profits, which were reinforced due to their cartel organisation. Furthermore, during this period, the states strongly encouraged the development and production of weapons.\nIn 1886, Dynamit Nobel opened a branch in Troisdorf and as from 1905 this factory also manufactured a plastic material based on nitrocellulose (an explosive product): known as celluloid.\n\nUntil the start of the First World War, Dynamit Nobel grew by acquiring smaller competitor companies to become the biggest explosive manufacturer in Europe. During the war, it employed prisoners of war in its factory (namely Russian prisoners in the factory of Dömitz).\n\nWithout descendants, Alfred Nobel, decided that after his death his fortune would be used to create the Nobel foundation. This was done in 1900. Each year this foundation awards the Nobel Prize. The fortune of Nobel which was converted into shares that finance the Nobel foundation.\n\nAfter the end of the war, parts of the factory’s facilities were dismantled and after the enforcement of the Versailles treaty, companies were forbidden to manufacture defense equipment. As from then, the company manufactured mostly explosives destined to mines, detonators, ignition systems as well as ammunitions for hunting and sports. The end of the production of the highly profitable defense equipment caused the company to suffer from heavy financial losses. This caused the company to close down some factories while reducing the production capacities in others. In 1923, the company launched the manufacture of plastic parts made of nitrocellulose. Indeed, Dynamit Nobel aimed at reducing its dependence to the defense equipment in order to give more importance to the manufacture of chemical products.\n\nIn 1925, the Lindener Zündhütchen- und Thoonwaarenfabrik of Empelde was taken over by the Chemische Werke Lothringen GmbH, which itself belonged to BASF. Production was stopped in 1928 and did not restart until the rearmament which occurred in 1938. During the 1920s, the company collaborated closely with the Siegener Dynamitfabrik AG and the Rheinisch-Westfälischen Sprengstoff-AG which belonged to I.G. Farben since 1931.\n\nIn 1926, as a result of the fusion with Köln-Rottweil AG which belonged to \"I.G. Farben\", Dynamit Nobel was taken over by I.G. Farben. As from then, it formed a cartel with Westfalit AG (the forerunner of the WASAG, which was also taken over by I.G. Farben in 1945), which had a monopolistic position on the German powder market. In 1930, the \"Rheinische Spritzguß-Werk GmbH\" (today the Dynamit Nobel Kunststoff GmbH) was founded in Cologne. After the national-socialists took lead of the German government, and wished to develop a strong German defense industry, the Wehrmacht explicitly requested bigger ammunition manufacturing capacities. In order to do so, the \"WASAG\" and Dynamit Nobel founded the Deutsche Sprengchemie GmbH in 1934. New factories manufacturing explosives and ammunitions were built on government lands and received help from a company which had been nationalised, the Verwertungsgesellschaft für Mountain-Industrie mbH.\n\nAfterwards, the Deutsche Sprengchemie GmbH became the only subsidiary of WASAG. Dynamit Nobel had the same activities in Gesellschaft zur Verwertung chemischer Erzeugnisse mbH (i.e. Verwertchemie). The latter ran more than 30 factories, namely in Liebenau, Empelde, and Stadtallendorf. At that time, Stadtallendorf was the largest place of ammunition production in Europe. During World War Two, more than people coming from camps managed by the SS were forced to work there. In 1938, a new factory manufacturing nitrocellulose was built in Aschau am Inn. After the war it became the property of WASAG due to the decartelisation of I.G. Farben.\n\nAfter World War Two, Dynamit Nobel began manufacturing plastic equipment and ammunition in West Germany but was not able to keep the factories located in the areas occupied by the Soviets. These factories were partly dismembered. From 1953, Dynamit Nobel tried to develop intermediary organic products in order not to rely completely on the plastic equipment. After deciding the rearmament of Bundeswehr, the manufacture of ammunition was restarted in 1957, at first in the factory of Liebenau by the Gesellschaft zur Verwertung chemischer Erzeugnisse mbH which had survived the war.\nAt the beginning of the 1960s, once again, the company became the leader of the military and civil powder market in Germany, namely due to the takeover of the manufacturer of ammunitions Gustav Genschow & Co. AG de Karlsruhe in 1963. At the same time, Dynamit Nobel took an increasing position in the mines market. Since 1958, around 3 million antitank mines, models DM-11 were manufactured in Liebenau under licence of the Swedish company LIAB. Moreover, in collaboration with Bölkow and Dornier, Dynamit Nobel participated to the research projects of the Ministry of nuclear energy (today known as the Federal Ministry of education and research) for the possible supply of Germany with missiles.\nDuring the end of the 1950s, Friedrich Karl Flick, who was a board member of the supervisory board before the war, started to monopolise the company to the detriment of the minority shareholders, sometimes by brutal means. Supported by the Bremen speculator Hermann Krages and partly due to the collusive trading of actions with the Feldmühle AG of which Flick was also a shareholder, he managed to obtain the majority of the shares of the company and became the president of the board of directors. As from then, Flick who already possessed 82% of the share capital, made use of the controversial \"Umwandlungssteuergesetzes\" (fiscal law regarding commercial companies) in order to squeeze out the minority shareholders of the company, in exchange of a compensation (this tool is comparable to the one used today in order to exclude minority shareholders out of a company). After several groups of shareholders protested against this law put into place under the third Reich, the Federal Constitutional Tribunal gave a judgement in favour of Flick.\n\nIn 1959, Dynamit AG changed its name to Dynamit Nobel AG, in remembrance of its famous founder. As from 1962, due to the pressure which occurred during the conference, the company which belonged to Flick started to negotiate the compensation to be given to forced Jewish employees who worked in the factory of Troisdorf in 1944 and 1955. After an agreement was made, Friedrich Flick personally blocked the payment of five million Deutsche Mark and no sum of money was released until his death in 1972. In January 1970, Flick made a final statement and declared that he He affirmed that a payment would contradict his previous statements of innocence at the Flick trial and he was unwilling to make a payment to be considered as an admission of guilt. He also affirmed that the Swiss Dieter Bührle (Oerlikon-Bührle) was also a shareholder of Dynamit Nobel, being the owner of 18% of the shares.\n\nAfter Bundeswehr was fully equipped with antitank mines during the late 1960s, the factory of Liebenau was sold in 1977 to the Dutch ammunitions manufacturer Eurometaal, owned by Dynamit Nobel (33% of shares). As from then, the big mines projects were put into place in Troisdorf and in Burbach-Würgendorf.\nIn 1986, the Flick group was bought by Deutsche Bank for an approximate amount of 5.36 billions of DM. The latter restructured the group and sold parts of it while putting the rest of the group on the stock market. Deutsche Bank finally agreed to compensate the forced workers of \"Dynamit Nobel AG\" in respect of the conditions established in the 1960s. During the restructuring of Dynamit Nobel AG, a joint-venture was made with two of the companies of the Flick Group, namely Feldmühle AG and Buderus AG and was renamed Feldmühle Nobel AG. In 1986, the new group was introduced on the stock market. In 1988, the grand children of Friedrich Flick (Friedrich Christian Flick and his brother Gert Rudolf Flick) failed to gain control over the Feldmühle Nobel AG. Indeed, in 1992, Metallgesellschaft AG (today known as the GEA Group) took over the company to fraction it again. Shares of Dynamit Nobel AG and Buderus remained the property of Metallgesellschaft, while the pulp and paper division (formerly known as Feldmühle AG) was sold under the name of Feldmühle Nobel AG to the Swedish company Stora (Stora Enso since 1998).\nAs early as 1988, Gesellschaft zur Verwertung chemischer Erzeugnisse mbH, which had been managed as an affiliate, and Dynamit Nobel put into force an agreement regarding the consolidation and profit transfer In 1990, the subsidiary was merged with another subsidiary, Dynamit Nobel Explosivstoff- und Systemtechnik GmbH.\n\nAt the beginning of the 1990s, the company was present in the basic and intermediary chemical products, synthetic fibres, specific chemical products (such as silicon wafers) and the processing of plastic materials (in particular PVC). About one quarter of the turnover of the company originated from the traditional sector of explosives, as well as the ammunition technology which is closely linked to the equipment projects of the Bundeswehr.\nIn 1992, the company took over \"Cerasiv GmbH\" and Chemetall GmbH, and in 1994 \"Sachtleben Chemie GmbH\" and Chemson GmbH were also taken over. In 1996, the company acquired \"CeramTec AG\" which belonged to Hoechst and merged with Cerasiv GmbH under the name of CeramTec Innovative Ceramic Engineering AG.\nIn 1997, Dynamit Nobel took over Phoenix Kunststoff GmbH in order to reinforce its position in the plastic equipments market. In 1999, Dynamit Nobel and the chemical company Solvadis were united within the MG chemical group. The scope of chemical activities within Chemetall was optimized by the acquisition of C\"yprus Foote\" (1998) and \"Brent\" (1999), followed by the sale of two subsidiaries namely, \"Chemson GmbH\" (1999) et \"Coventya GmbH\" (2000).\nIn 2001, the industrial activities of \"Dynamit Nobel Explosivstoff und Systemtechnik GmbH\" was taken over by Orica. In 2002, the Swiss group RUAG technology took over \"Dynamit Nobel Ammotec GmbH\", which had been separated from \"Dynamit Nobel Explosivstoff und Systemtechnik GmbH\". In this company, the production was focussed on the manufacture of ammunitions of small calibre.\nIn 2004, \"MG technologies AG\" sold its chemical activities to emphasise the manufacture of equipment. Therefore, Dynamit Nobel AG was dismantled and taken over by several companies. The American company \"Rockwood Specialties Group Inc\", the largest buyer, acquired \"Sachtleben Chemie GmbH\", \"Chemetall GmbH\", \"CeramTec Innovative Ceramic Engineering AG\" and \"DNSC GmbH\" as of 31 July 2004 for 2.25 billions of euros through its Luxemburg subsidiary Knight Lux 1 S.A.R.L.. Part of DNSC GmbH remains in Leverkusen and is known as \"Dynamit Nobel GmbH ES.\"\n\"Rockwood\" is a chemical holding company that the financial investor company Kohlberg Kravis Roberts & Co had acquired. D\"ynamit Nobel Kunststoff GmbH\" was taken over in 2004 by the Swedish company \"Plastal Holding AB\" for 915 million euros.\n\nThe technical armament activity was reduced under the name of Dynamit Nobel Defence GmbH, with the registered office located in Würgendorf (Burbach). The company was specialised in the manufacture of small calibres for the army, official authorities, hunters and shooting sports and was taken over by the Swiss group \"RUAG\" in 2002 and merged with its ammunitions branch. Therefore, the former companies \"Dynamit Nobel Marken RWS\", \"Rottweil\" and \"Geco\" were named \"RUAG Ammotec GmbH\" (Fürth).\nThe dismantlement of the Nobel group was made in close collaboration with the employees representatives, who were highly involved during the sales negotiations. Indisputably the employees’ committee would have preferred that the chemical activity remains within \"MG technologies AG\", but an agreement was reached as Rockwood Inc. certified that the long term interests of the company would be maintained and that all the German employees would keep their jobs.\n\nAs from 1958, Gesellschaft zur Verwertung chemischer Erzeugnisse mbH/Verwertchemie, a subsidiary of \"Dynamit Nobel\" manufactured antitank mines of type DM-11 in Liebenau, under licence of the Swedish company \"LIAB\".\nThe AT-2 antitank mines were created by Dynamit Nobel and approximately 1.3 millions were manufactured. The Bundeswehr ordered of them for the LARS (a system of light artillery with missiles) which was into place until 2000, about were manufactured for Minenwurfsystem Skorpion (a vehicle which installed mines) and for M270 (a multiple rocket launcher).\nBetween 1981 and 1986, the Bundeswehr invested 564.7 millions of DM in mines projects, . Besides the AT-2 antitank mines, Dynamit Nobel developed an antipersonnel mine AP-2, an anti-material mine, a signal mine and a shallow water mine.\nHK G11, a new assault rifle using an ammunition without casing was developed in collaboration with the weapons manufacturer \"Heckler & Koch\" between 1968 and 1990, whereas \"Dynamit Nobel\" developed the ammunitions without casing. The project was completed but the Bundeswehr declined it due to financial considerations.\n\nDynamit Nobel marketed the Swedish antitank mine FFV 028SN of the company \"FFV\" and took charge of the transformation of 125 000 antipersonnel mines, model DM-31 which had been manufactured between 1962 and 1967 by Industriewerke Karlsruhe (today KUKA) which belonged to the Quandt group at the time, in order to enable them to conform to the antitank mines authorised by the Ottawa convention. However, the detonator was not sufficiently modified, and it could therefore be used against people, while it should only have been used against tanks.\nIn respect of an agreement made in 1989, Dynamit Nobel was manufacturing blank cartridge bullets and figure targets in Würgendorf. Dynamit Nobel also agreed to develop the Panzerfaust 3, in order to progressively deliver it to the Bundeswehr and other armies as the priority antitank defence mechanism in the infantry. In 2010, several different types are manufactured for the Bundeswehr.\n\nUntil the 1970s, Dynamit Nobel polymerised the monomer vinyl chloride into polyvinyl chloride (PVC) in the factory of Troisdorf. At this time, about 130 to 140 employees were regularly in touch with it. In total, about 3600 persons have worked within this division since the launching of the production in Troisdorf in the 1940s.\n\nInfringing the health and safety regulations in force at that time, the employees of Dynamit Nobel were exposed for years, with little protection, to this harmful substance which later turned out to be carcinogenic. Therefore, they were heavily contaminated by vinyl chloride gas or by cleaning up the autoclaves. At this time, most of the other manufacturers of PVC had already put into place systems of production, which were less dangerous for the health. The same had not been done at Dynamit Nobel because of financial considerations. Moreover, regular controls were not made, others were partially manipulated or the results were kept secret. Also, the company made an important contribution to the region, regularly obtained extensions of time to apply the regulations. Contamination due to vinyl chloride was so severe that for years in the company, the employees complained of damage relating to the liver, anemia, finger circulation disorder resulting in acro-osteolysis (necrosis of the first phalanxes), as well as headaches and dizziness. Cancers also resulted from the exposure.\n\nAfter the announcement of the first thirteen severe diseases during spring 1972, work inspectors from Bonn ordered Dynamit Nobel to take the appropriate measures in order to improve the health and safety conditions of work. But the company took a long time to put them into place.\n\nAfterwards, the 40 sick employees gathered together as Interessengemeinschaft der VC-Geschädigten in order to file a complaint for violation of duty against the Land of North-Rhine-Westphalia and asked for damage compensation, just like in the Contergan trial. The local committee of the DKP in Troisdorf filed a complaint for injury and involuntary homicides against the board of directors of Dynamit Nobel AG. Both complaints remained unsuccessful. After more details about the scandal were made public, employees and inhabitants of Troisdorf organised a series of demonstrations. In 1975, the company’s board of directors decided to shut down PVC polymerisation workshop to escape the expensive costs related to modernization and security of the plant.\n\nSince the first announcement about the existing risks, the company has tried its best to keep things quiet. In that respect, it has put a lot of pressure on journalists and editors. During the following years, some employees were contaminated and died due to their disease without the company ever compensating them.\n\n"}
{"id": "36740300", "url": "https://en.wikipedia.org/wiki?curid=36740300", "title": "Faiveley Transport", "text": "Faiveley Transport\n\nFaiveley Transport (), formerly Faiveley, is an international manufacturer and supplier of equipment for the railway industry founded in 1919. It introduced the single-arm pantograph in 1955. The company has subsidiaries in more than 24 countries. The majority of Faiveley Transport's outstanding stock is owned by Wabtec Corporation, which acquired majority stock ownership from the Faiveley family in 2016.\n\nIn 1919, Louis Faiveley founded in Saint Ouen, France, the \"Établissments Louis Faiveley\", a small assembly shop centered on electromechanical parts. It soon grew and became one of the French railway system's leading suppliers. It introduced in 1923 its first pantograph. In 1930, it also ventured in the manufacture of door systems for trains. By the 30s, it was already one of France's leading companies in all its fields of activity. In 1935, the company became a Societé Anonyme, although the shares' majority stayed in hands of the Faiveley family.\n\nAfter the Second World War, the company quickly recovered. In 1946, it introduced electric heating systems. In 1955, it helped set a new high-speed train record, as a Faiveley-equipped train exceeded 331 kilometres per hour. That year, Faiveley also introduced the first single-arm pantograph. This innovation helped the company to ensure its position as world leader in railway pantograph systems.\n\nIn 1961, the company created a research and development division with the aim of adapting electronic applications to the railroad industry, included automatic door systems. It also begun to equipe the new rubber-wheeled Paris Metro cars. In 1965, the company started to produce automatic doors for buildings, creating in 1968 a subsidiary specifically for this area: Faiveley Automatismes.\n\nThe company continued its expansion. In 1966, it opened a subsidiary in Spain. In 1976, one in Brazil (Equipfer). In 1979, was created the Italian branch. Attempts to enter into the American and Canadian markets, however, were not as successful. Only in the late 90s it could settle in those countries.\n\nFrance remained as Faiveley's core market. During the 70s, the company introduced new corail coaches for the SNCF and provided equipment for a new generation of subway trains, the MF77. In 1972, Faiveley presented its first very-high-speed pantograph. Soon after, it introduced its first electric automated road system.\n\nIn 1984, Faiveley purchased Saint-Gobain subsidiary Air-Industrie's transport division, giving it operations in passenger train air conditioning systems. That year, it acquired from the Matra's subsidiary Interlec its tachometry activities. Together with the company's other transportation-related activities, these subsidiary operations were gathered into a newly created subsidiary: Faiveley Transport.\n\nThe company was one of the suppliers of the new SNCF's TGV trains. In 1990, its pantographs were key to achieve a new 513.3 kilometres per hour record by TGV Atlantique.\n\nIn 1991, the company moved its headquarters to Saint-Pierre-des-Corps.\n\nWith competition becoming more intense in its field of activity, Faiveley decided it was time to expand their operations.\n\nIn 1992, the company acquired plastics product maker Grand-Perret. In 1993, Faiveley moved to concentrate its activities around its historic core of railroad and transportation equipment and its brand new plastics division, selling its Faiveley Automatismes subsidiary. It also made an aggressive move to increase its business in the Japanese market, one of the most important at the time, forming a joint venture with Nabco called Nabco-Faiveley Ltd which became one of leading providers of Japan's railroad. Later, Faiveley partnered also with Mitsui. That year Faivaley created a British subsidiary, to take a part of the business in the recently privatised rail system.\n\nIn 1994, Faiveley was listed on the Paris Bourse. In 1995, it acquired VPI-Verchère Plastiques Industriels, a thermo-injected plastic company. The following year, Faiveley added the operations of Rhône Moulage and Sepal Ltd, two companies also centered in plastics. By the end of the decade, that material had risen to nearly 20 percent of Faiveley's total sales.\n\nIn 1995. Faiveley purchased the German railroad air conditioning company Hagenuk Fahrzeugklima from its parent company Siemens. The purchase was a key step into the German market as well as the Asian through is Chinese subsidiary Shanghai Hagenuk Refrigerating Machine, but also brought losses to the company during the following years. This led to a reorganisation that included staff reductions in Germany and France and changes in the Board of Directors.\n\nIn December 2002, Faiveley purchased a 75 percent stake in the Czech pantograph and electro-mechanical equipment supplier Lekov. In 2004, it acquired the train brakes manufacturer Sab Wabco and the air conditioning manufacturer Neu Systèmes. In early 2007, it purchased the electronic systems and rolling stock manufacturer ESPAS group. Faiveley CX pantographs were fitted in the V150 TGV's record-breaking attempt of 2007, which set a new world railway speed record of 574.8 kilometres per hour.\n\nIn April 2008, the company acquired from Carbone Lorraine its sintered brake material manufacturing and design department. In July of that year, it purchased the American freight wagon components' manufacturer Ellcon-National.\n\nIn September 2009 Faiveley SA and its subsidiary Faiveley Transport merged into a sole company, called Faiveley Transport SA.\n\nIn March 2011, the company purchased an 80 percent stake in the rolling stock heating, ventilation and air conditioning equipment manufacturer Urs Dolder AG and the remaining stake of Lekov.\n\nOn 3 February 2012, Faiveley Transport completed the purchase of Graham-White, an American designer and manufacturer of compressed air drying and brake systems for rail transport.\n\nIn February 2013, the company won a trial against Wabtec for the acts of unfair competition and secrets violation.\n\nOn 30 November 2016, the 51% stake of the company controlled by the Faiveley family was purchased by Wabtec, giving Wabtec controlling interest in Faiveley Transport.\n\nFaiveley Transport offers a wide range of products related to the train equipment, such as cabin heating, ventilation and air conditioning (HVAC); HVAC system room, air distribution ducts, exhaust, urs dolder heaters; pantographs and high voltage switches, energy meters, auxiliary power converters, master controllers and driver awareness system. The company also provides access and information systems, such as platform screen doors and automatic platform gates, portal platform, door systems, passenger information systems and CCTV vigilance. In addition, it provides checks and security products, including couplers, odometry/tachometry systems and event recorders, brake control units, oil-free air generator BURAN, Nowe sanding, axle mounted disc, magnetictrack brake, disc brakes controllers, air generation and air treatment, and pantograph compressor. The company provides renovation, maintenance, installation and consultancy services, including torque and engineering maintenance and spare parts and logistics. It serves tram, metro, high speed locomotives, and regional market segments trains.\n\nIn December 2016, Wabtec Corporation acquired majority stock ownership of Faiveley Transport from the Faiveley family, who had owned about 51% of the outstanding shares, for €1.6 billion and 6.3 million shares of Wabtec's common stock. After tender offers for the remaining outstanding shares, Wabtec owned 98.53% of Faiveley stock, with 97.66% of the voting rights. Wabtec plans to complete the acquisition with a mandatory squeeze-out of the shares which were not tendered.\n\n"}
{"id": "3731512", "url": "https://en.wikipedia.org/wiki?curid=3731512", "title": "Fan heater", "text": "Fan heater\n\nA fan heater, also called a blow heater, is a heater that works by using a fan to pass air over a heat source (e.g. a heating element). This heats up the air, which then leaves the heater, warming up the surrounding room. They can heat an enclosed space such as a room faster than a heater without fan, but, like any fan, create audible noise.\n\nElectric fan heaters can be less expensive to buy than other heaters due to simple construction. The fan carries heat away from the device, which can be made smaller without overheating. The relatively small amount of electricity used to operate the fan is partly converted to additional heat, so that efficiency is not a problem. All heaters without external ventilation are nearly 100% efficient, meaning that almost all energy input goes into the room as heat. However, if the efficiency of generating the electricity is taken into account, the overall efficiency decreases significantly.\n\nElectric fan heaters are more expensive to run than fuel powered heaters due to the cost of electricity. This makes them best suited to occasional use rather than as regularly used heat sources.\n\nExternally vented non-electrical (combustion powered) fan heaters lose some heat to the outdoors, and are thus less efficient. These are used where it is necessary not to release the fumes of combustion into the heated area.\n\nMost modern fan heaters have a power setting to determine power output. Some also have a thermostat which switches off heating when the desired ambient temperature is reached. They do not maintain perfect room temperature control, since\n\nWhile the fans in fan heaters are electrically powered, various heat sources may be used:\n\nElectric fan heaters are unsealed appliances with live electric parts inside, so they are not safe to use in wet environments because of the risk of electrocution if moisture provides a conductive path to electrically-live parts. Electric fan heaters usually have a thermal fuse close to the heating element(s) to protect against overheating in the event of fan failure or air intakes becoming blocked, and a tip-over switch to shut the heater off when the fan outlet is not in the required orientation. Metal-cased heaters perform better in the case of possible fire-causing faults than plastic-cased ones, since the case will stay intact and is not flammable, but the metal case presents a higher risk of electrocution if a heater malfunctions. \n\nPortable fuel-powered fan heaters release all the fumes of combustion into the room, creating a risk of poisoning by carbon monoxide and carbon dioxide. Most installed fuel fan heaters in the first world use a heat exchanger and external ventilation, avoiding that risk by venting the combustion gases to the outdoors.\n\nThe picture immediately to the right (the top on the mobile site) shows most of the component parts of a typical plug-in electric fan heater.\n\nThe next picture shows the two overheat cutouts. The bimetal cutout (left) operates if the device overheats because the intake is blocked or the fan fails, and resets automatically or manually depending on specification, once the heater cools after the operational fault is corrected. The thermal fuse (right) is a failsafe backup device that will blow and disconnect the heating element permanently should the bimetal cutout fail to operate (e.g. due to its contacts welding together) and in so doing prevent extreme overheating which could result in a fire.\n\nIndustrial fan heaters use high-output finned heating elements in front of a fan to provide a larger airflow and higher kilowatt rating than many smaller residential fan heaters.\nIndustrial fan heaters can be used in warehouses, shipping containers, clean rooms, shops and other general purpose heating applications.\nThey can also be used as dryers or dehumidifiers with modified attachments or mountings. Portable industrial fan heaters tend to range from around 1.5 kW up to about 45 kW with either axial or centrifugal fans and various staged controls and over-temperature safety limit controls.\n\n"}
{"id": "23519805", "url": "https://en.wikipedia.org/wiki?curid=23519805", "title": "Food, Inc. (book)", "text": "Food, Inc. (book)\n\nFood, Inc.: How Industrial Food Is Making Us Sicker, Fatter, and Poorer—And What You Can Do About It is a 2009 companion book to the documentary film of the same name about the industrialization of food production and about the negative results to human health and to the natural environment. Edited by Karl Weber, the book is co-published by Participant Media and PublicAffairs Books.\n\n\n"}
{"id": "12143913", "url": "https://en.wikipedia.org/wiki?curid=12143913", "title": "Fruit preserves", "text": "Fruit preserves\n\nFruit preserves are preparations of fruits, vegetables and sugar, often stored in glass jam jars.\n\nMany varieties of fruit preserves are made globally, including sweet fruit preserves, such as those made from strawberry or apricot, and savory preserves, such as those made from tomatoes or squash. The ingredients used and how they are prepared determine the type of preserves; jams, jellies, and marmalades are all examples of different styles of fruit preserves that vary based upon the fruit used. In English, the word, in plural form, \"preserves\" is used to describe all types of jams and jellies.\n\nThe term 'preserves' is usually interchangeable with 'jams'. Some cookbooks define preserves as cooked and gelled whole fruit (or vegetable), which includes a significant portion of the fruit. In the English speaking world, the two terms are more strictly differentiated and, when this is not the case, the more usual generic term is 'jam'.\n\nThe singular \"preserve\" or \"conserve\" is used as a collective noun for high fruit content jam, often for marketing purposes. Additionally, the name of the type of fruit preserves will also vary depending on the regional variant of English being used.\n\nA chutney is a relish of Indian origin made of fruit, spices and herbs. Although originally intended to be eaten soon after production, modern chutneys are often made to be sold, so require preservatives – often sugar and vinegar – to ensure they have a suitable shelf life. Mango chutney, for example, is mangoes reduced with sugar.\n\nWhile confit, the past participle of the French verb \"confire\", \"to preserve\", is most often applied to preservation of meats, it is also used for fruits or vegetables seasoned and cooked with honey or sugar till jam-like. Savory confits, such as ones made with garlic or fennel, may call for a savory oil, such as virgin olive oil, as the preserving agent.\n\nKonfyt (Afrikaans: \"jam\" or \"fruit preserve\") is a type of jam eaten in Southern Africa. It is made by boiling selected fruit or fruits (such as strawberries, apricots, oranges, lemons, water melons, berries, peaches, prickly pears or others) and sugar, and optionally adding a small quantity of ginger to enhance the flavour. The origins of the jam is obscure but it is theorized that it came from the French. The word is also based on the French term \"confiture\" via the Dutch \"confijt\" (meaning candied fruit).\n\nA conserve, or whole fruit jam, is a preserve made of fruit stewed in sugar. Traditional whole fruit preserves are particularly popular in Eastern Europe (Russia, Ukraine, Belarus) where they are called varenye, the Baltic region where they're known by a native name in each of the countries (, , , ), as well as in many regions of Western, Central and Southern Asia, where they are referred to as murabba.\n\nOften the making of conserves can be trickier than making a standard jam; it requires cooking or sometimes steeping in the hot sugar mixture for just enough time to allow the flavour to be extracted from the fruit, and sugar to penetrate the fruit; and not cooking too long such that the fruit will break down and liquify. This process can also be achieved by spreading the dry sugar over raw fruit in layers, and leaving for several hours to steep into the fruit, then just heating the resulting mixture only to bring to the setting point. As a result of this minimal cooking, some fruits are not particularly suitable for making into conserves, because they require cooking for longer periods to avoid issues such as tough skins. Currants and gooseberries, and a number of plums are among these fruits.\n\nBecause of this shorter cooking period, not as much pectin will be released from the fruit, and as such, conserves (particularly home-made conserves) will sometimes be slightly softer set than some jams.\n\nAn alternative definition holds that conserves are preserves made from a mixture of fruits or vegetables. Conserves may also include dried fruit or nuts.\n\nFruit butter, in this context, refers to a process where the whole fruit is forced through a sieve or blended after the heating process.\n\nFruit curd is a dessert topping and spread usually made with lemon, lime, orange, or raspberry. The basic ingredients are beaten egg yolks, sugar, fruit juice and zest which are gently cooked together until thick and then allowed to cool, forming a soft, smooth, intensely flavored spread. Some recipes also include egg whites or butter.\n\nAlthough the FDA has \"Requirements for Specific Standardized Fruit Butters, Jellies, Preserves, and Related Products\", there is no specification of the meaning of the term Fruit spread. Although some assert it refers to a jam or preserve with no added sugar, there are many \"fruit spreads\" by leading manufacturers that do contain added sugar. This can be easily verified by searching the listings under \"fruit spread\" on common web sites, such as those of Amazon or Walmart, or to look at the ingredient list and nutritional information on specific fruit spread products.\n\nJam typically contains both the juice and flesh of a fruit or vegetable, although one cookbook defines it as a cooked and jelled puree. The term \"jam\" refers to a product made of whole fruit cut into pieces or crushed, then heated with water and sugar to activate its pectin before being put into containers:\nPectin is mainly D-galacturonic acid connected by α (1–4) glycosidic linkages. The side chains of pectin may contain small amounts of other sugars such as L-fructose, D-glucose, D-mannose, and D-xylose. In jams, pectin is what thickens the final product via cross-linking of the large polymer chains.\n\n\"Freezer jam\" is uncooked (or cooked less than 5 minutes), then stored frozen. It is popular in parts of North America for its very fresh taste.\n\nRecipes without added pectin use the natural pectin in the fruit to set. Tart apples, sour blackberries, cranberries, currants, gooseberries, Concord grapes, soft plums, and quinces work well in recipes without added pectin.\n\nOther fruits, such as apricots, blueberries, cherries, peaches, pineapple, raspberries, rhubarb, and strawberries are low in pectin. In order to set, or gel, they must be combined with one of the higher pectin fruits or used with commercially produced or homemade pectin. Use of added pectin decreases cooking time.\n\nIn Canada, fruit jam is categorized into two types: fruit jam and fruit jam with pectin. Both types contain fruit, fruit pulp or canned fruit and are boiled with water and a sweetening ingredient. Both must have 66% water-soluble solids. Fruit jam and fruit jam with pectin may contain a class II preservative, a pH adjusting agent or an antifoaming agent. Both types cannot contain apple or rhubarb fruit.\n\nThough both types of jam are very similar, there are some differences in fruit percent, added pectin and added acidity. Fruit jam must have at least 45% fruit and may contain added pectin to compensate for the natural pectin level found in the fruit. Fruit jam with pectin need only contain 27% fruit and is allowed to contain added acidity to compensate for the natural acidity of the fruit.\n\nIn the U.S., jelly (from the French \"gelée\") refers exclusively to a clear or translucent fruit spread made from sweetened fruit (or vegetable) juice—thus differing from jam by excluding the fruit's flesh—and is set by using its naturally occurring pectin, whereas outside North America jelly more often refers to a gelatin-based dessert, though the term is also used to refer to clear jams such as blackcurrant and apple.\nIn the United Kingdom, redcurrant jelly is a condiment often served with lamb, game meat including venison, turkey and goose in a festive or Sunday roast. It is a clear jam, set with pectin from the fruit, and is made in the same way, by adding the redcurrants to sugar, boiling, and straining.\n\nPectin is essential to the formation of jelly because it acts as a gelling agent, meaning when the pectin chains combine, they create a network that results in a gel. The strength and effectiveness of the side chains and the bonds they form depend on the pH of the pectin, the optimal pH is between 2.8–3.2.\n\nAdditional pectin may be added where the original fruit does not supply enough, for example with grapes.\nJelly can be made from sweet, savory or hot ingredients. It is made by a process similar to that used for making jam, with the additional step of filtering out the fruit pulp after the initial heating. A muslin or stockinette \"jelly bag\" is traditionally used as a filter, suspended by string over a bowl to allow the straining to occur gently under gravity. It is important not to attempt to force the straining process, for example by squeezing the mass of fruit in the muslin, or the clarity of the resulting jelly will be compromised. Jelly can come in a variety of flavors such as grape jelly, strawberry jelly, hot chile pepper, and others. It is typically eaten with a variety of foods. This includes jelly with toast, or a peanut butter and jelly sandwich.\n\nMarmalade is a fruit preserve made from the juice and peel of citrus fruits boiled with sugar and water. It can be produced from lemons, limes, grapefruits, mandarins, sweet oranges, bergamots and other citrus fruits, or any combination thereof. Marmalade is generally distinguished from jam by its fruit peel.\n\nThe benchmark citrus fruit for marmalade production in Britain is the Spanish Seville orange, \"Citrus aurantium\" var. \"aurantium\", prized for its high pectin content, which gives a good set. The peel has a distinctive bitter taste which it imparts to the preserve. In America, marmalade is sweet.\n\nIn general, jam is produced by taking mashed or chopped fruit or vegetable pulp and boiling it with sugar and water. The proportion of sugar and fruit varies according to the type of fruit and its ripeness, but a rough starting point is equal weights of each. When the mixture reaches a temperature of 104 °C (219 °F), the acid and the pectin in the fruit react with the sugar, and the jam will set on cooling. However, most cooks work by trial and error, bringing the mixture to a \"fast rolling boil\", watching to see if the seething mass changes texture, and dropping small samples on a plate to see if they run or set.\n\nCommercially produced jams are usually produced using one of two methods. The first is the open pan method, which is essentially a larger scale version of the method a home jam maker would use. This gives a traditional flavor, with some caramelization of the sugars. The second commercial process involves the use of a vacuum vessel, where the jam is placed under a vacuum, which has the effect of reducing its boiling temperature to anywhere between 65 and 80 °C depending on the recipe and the end result desired. The lower boiling temperature enables the water to be driven off as it would be when using the traditional open pan method, but with the added benefit of retaining more of the volatile flavor compounds from the fruit, preventing caramelization of the sugars, and of course reducing the overall energy required to make the product. However, once the desired amount of water has been driven off, the jam still needs to be heated briefly to to kill off any micro-organisms that may be present; the vacuum pan method does not kill them all.\n\nDuring commercial filling it is common to use a flame to sterilize the rim and lid of jars to destroy any yeasts and molds which may cause spoilage during storage. Steam is commonly injected immediately prior to lidding to create a vacuum, which both helps prevent spoilage and pulls down tamper-evident safety button when used.\n\nGlass or plastic jars are an efficient method of storing and preserving jam. Though sugar can keep for exceedingly long times, containing it in a jar is far more useful than older methods. Other methods of packaging jam, especially for industrially produced products, include cans and plastic packets, especially used in the food service industry for individual servings. Fruit preserves typically are of low water activity and can be stored at room temperature after opening, if used within a short period of time.\n\nThe U.S. Food and Drug Administration (FDA) published standards of identity in 21 CFR 150, and treats jam and preserves as synonymous, but distinguishes jelly from jams and preserves. All of these are cooked and pectin-gelled fruit products, but jellies are based entirely on fruit juice or other liquids, while jams and preserves are gelled fruit that may include the seeds and pulp. The United States Department of Agriculture offers grading service based on these standards.\n\nUnder the Processed Products Regulations (C.R.C., c. 291), jams, jellies, citrus marmalade and preserves are defined. Each must contain a minimum percentage of the named fruit and a minimum percentage of water-soluble solids. Jams \"shall be the product made by boiling fruit, fruit pulp or canned fruit to a suitable consistency with water and a sweetening ingredient\", jellies \"shall be the product made by boiling fruit juice or concentrated fruit juice that is free from seeds and pulp with water and a sweetening ingredient until it acquires a gelatinous consistency.\"\n\nIn the European Union, the jam directive (Council Directive 79/693/EEC, 24 July 1979) set minimum standards for the amount of \"fruit\" in jam, but the definition of fruit was expanded to take account of several unusual kinds of jam made in the EU. For this purpose, \"fruit\" is considered to include fruits that are not usually treated in a culinary sense as fruits, such as tomatoes, cucumbers, and pumpkins; fruits that are not normally made into jams; and vegetables that are sometimes made into jams, such as rhubarb (the edible part of the stalks), carrots, and sweet potatoes. This definition continues to apply in the new directive, Council Directive 2001/113/EC of 20 December 2001 relating to fruit jams, jellies and marmalades and sweetened chestnut purée intended for human consumption.\n\nExtra jam is subject to somewhat stricter rules that set higher standards for the minimum fruit content (45% instead of 35% as a general rule, but lower for some fruits such as redcurrants and blackcurrants), as well specifying as the use of unconcentrated fruit pulp, and forbidding the mixture of certain fruits and vegetables with others.\n\nExtra jelly similarly specifies that the quantity of fruit juice or aqueous extracts used to make 1,000 grams of finished product must not be less than that laid down for the manufacture of extra jam.\n\n\n\n"}
{"id": "26457881", "url": "https://en.wikipedia.org/wiki?curid=26457881", "title": "Greenhouse gas monitoring", "text": "Greenhouse gas monitoring\n\nGreenhouse gas monitoring is the direct measurement of greenhouse gas emissions and levels. There are several different methods of measuring carbon dioxide concentrations in the atmosphere, including infrared analyzing and manometry. Methane and nitrous oxide are measured by other instruments. Greenhouse gases are measured by satellite monitoring such as the Orbiting Carbon Observatory and networks of ground stations such as the Integrated Carbon Observation System.\n\nManometry is a key measurement tool for atmospheric carbon dioxide by first measuring the volume, temperature, and pressure of a particular amount of dry air. The air sample is dried by passing it through multiple dry ice traps and then collecting it in a five liter vessel. The temperature is taken via a thermometer and pressure is calculated using manometry. Then, liquid nitrogen is added, causing the carbon dioxide to condense and become measurable by volume. The ideal gas law is accurate to 0.3% in these pressure conditions.\n\nInfrared analyzers were used at Mauna Loa Observatory and at Scripps Institution of Oceanography between 1958 and 2006. IR analyzers operate by pumping an unknown sample of dry air through a 40 cm long cell. A reference cell contains dry carbon dioxide-free air. A glowing nichrome filament radiates broadband IR radiation which splits into two beams and passes through the gas cells. Carbon dioxide absorbs some of the radiation, allowing more radiation that passes through the reference cell to reach the detector than radiation passing through the sample cell. Data is collected on a strip chart recorder. The concentration of carbon dioxide in the sample is quantified by calibrating with a standard gas of known carbon dioxide content.\n\nTitrimetry is another method of measuring atmospheric carbon dioxide that was first used by a Scandinavian group at 15 different ground stations. They began passing a 100.0 mL air sample through a solution of barium hydroxide containing cresolphthalein indicator.\n\nRange-resolved infrared differential absorption lidar (DIAL) is a means of measuring methane emissions from various sources, including active and closed landfill sites. The DIAL takes vertical scans above methane sources and then spatially separates the scans to accurately measure the methane emissions from individual sources. Measuring methane emissions is a crucial aspect of climate change research, as methane is among the most impactful gaseous hydrocarbon species. \n\nNitrous oxide is one of the most prominent anthropogenic ozone-depleting gases in the atmosphere. It is released into the atmosphere primarily through natural sources such as soil and rock, as well as anthropogenic process like farming. Atmospheric nitrous oxide is also created in the atmosphere as a product of a reaction between nitrogen and electronically excited ozone in the lower thermosphere.\n\nThe Atmospheric Chemistry Experiment‐Fourier Transform Spectrometer (ACE-FTS) is a tool used for measuring nitrous oxide concentrations in the upper to lower troposphere. This instrument, which is attached to the Canadian satellite SCISAT, has shown that nitrous oxide is present throughout the entire atmosphere during all seasons, primarily due to energetic particle precipitation. Measurements taken by the instrument show that different reactions create nitrous oxide in the lower thermosphere than in the mid to upper mesosphere. The ACE-FTS is a crucial resource in predicting future ozone depletion in the upper stratosphere by comparing the different ways in which nitrous oxide is released into the atmosphere. \n\nThe Orbiting Carbon Observatory (OCO) was first launched in February of 2009 but was lost due to launch failure. The Satellite was launched again in 2014, this time called the Orbiting Carbon Observatory-2, with an estimated lifespan of about two years. The apparatus uses spectrometers to take 24 carbon dioxide concentration measurements per second of Earth's atmosphere. The measurements taken by OCO-2 can be used for global atmospheric models and will allow scientists to locate carbon sources when its data is paired with wind patterns. The Orbiting Carbon Observatory-3 is ready to be launched in 2018 and will stands alone on the International Space Station (ISS).\n\nSatellite observations provides accurate readings of carbon dioxide and methane gas concentrations for short-term and long-term purposes in order to detect changes over time. The goals of this satellite, released in January of 2009, is to monitor both carbon dioxide and methane gas in the atmosphere, and to identify their sources. GOSat is a project of three main entities: the Japan Aerospace Exploration Agency (JAXA), the Ministry of the Environment (MOE), and the National Institute for Environmental Studies (NIES).\n\nThe Integrated Carbon Observation System was established in October 2015 in Helsinki, Finland as a European Research Infrastructure Consortium (ERIC). The main task of ICOS is to establish an Integrated Carbon Observation System Research Infrastructure (ICOS RI) that facilitates research on greenhouse gas emissions, sinks, and their causes. The ICOS ERIC strives to link its own research with other greenhouse gas emissions research to produce coherent data products and to promote education and innovation.\n\n"}
{"id": "10347776", "url": "https://en.wikipedia.org/wiki?curid=10347776", "title": "HDHomeRun", "text": "HDHomeRun\n\nHDHomeRun is a network-attached digital television tuner box, produced by the company SiliconDust USA, Inc..\n\nUnlike standard set-top box (or set-top unit) appliances, HDHomeRun does not have a video output that connects directly to the user's television. Instead it receives a live TV signal and then streams the decoded video over a local area network to an existing smart phone, tablet computer, smart tv, computer, or game console. This allows it to stream content to multiple viewing locations.\n\nThere are currently a number of HDHomeRun models on the market: \nAll models are designed to receive unencrypted digital broadcast or cable television and stream it over a network for use by any PC on the network. HDHomeRun normally receives an IP address via DHCP but will also work via an auto IP address if no DHCP server is available.\n\nThe HDHomeRun Windows driver presents the tuners as standard BDA tuners, enabling BDA-compliant applications to work with the HDHomeRun. The HDHomeRun can also be controlled via a command-line application which is available for Windows, Mac OS X, Linux, FreeBSD, and other POSIX-compliant operating systems. The control library is open source and is available under the LGPL for use in custom applications.\n\nSelect retail packaged HDHomeRun units are distributed with Arcsoft TotalMedia Theatre.\n\n\n\n\n\nThe HDHomeRun can be controlled and viewed from a wide variety of DVR/PVR software. Microsoft provides Windows Media Center for Windows XP through 8, but discontinued the product in Windows 10. Apple macOS 10 runs EyeTV 3. Linux runs Myth TV.\n\nNewer models of HDHomeRun are DLNA device compatible.\n\nIntroduced Fall 2011, the HDHomeRun Prime adds CableCARD support to allow viewers to view and record all the digital cable channels they subscribe to without using a cable supplied set-top-box. This includes premium cable channels like HBO, Showtime, and Starz. It integrates easily with Windows Media Center (WMC), included with Windows 7 and available with Windows 8, and turns your PC into an HD DVR. With 3 tuners, the Prime lets you record two programs and watch another live all at the same time. Once set up on your local network, other DLNA home video devices like most current HDTV's and Blu-ray players, Xbox 360 and PS3 consoles, and WMC Extenders Version 2 all have access to live TV and recorded cable TV.\n\n\nPlease note that WMC was included with Windows 7 but is an additional $100 for Windows 8/8.1 and an additional $10 for Windows 8/8.1 Pro. It is not available from Microsoft on Windows 10 but members at The Green Button are developing a way to use a modified version of WMC with Windows 10.\n\nThe HDHomeRun DVR is a DVR software designed for installation on a network-attached storage device. It is intended to be used with a HDHomeRun tuner and is expected to overcome digital rights management complications.\n\nLaunched in 2018, HDHomeRun Premium TV is a virtual MVPD service that works with HDHomeRun Prime and HDHomeRun DVR service. A unique feature of this service over most other MVPDs is the ability to record the channel streams to a local hard drive for time-shifted viewing.\n\n\n"}
{"id": "19989240", "url": "https://en.wikipedia.org/wiki?curid=19989240", "title": "Half-keyboard", "text": "Half-keyboard\n\nA half-keyboard is a specially designed and programmed keyboard used in limited space situations or when the typist needs a hand free to answer the phone, hold documents, etc.\n\nThe \"Half-QWERTY\" keyboard, invented by Edgar Matias, consists of only the left-hand half of a normal QWERTY keyboard, but when the space bar is held down, it switches to the right half of the keyboard, allowing a person to type with only one hand.\n\nIt is said to be quick to learn, because our bodies can easily replicate one motion on one side to the other side, and almost as fast as a normal keyboard.\n\n"}
{"id": "52171258", "url": "https://en.wikipedia.org/wiki?curid=52171258", "title": "ISO 22715", "text": "ISO 22715\n\nThe ISO 22715 standard provides guidelines for manufacturers in the best practices for cosmetic packaging and labelling of all cosmetic products. This standard applies to products that fall under the category of cosmetics that are sold or given away as free samples. ISO 22715 was initially published in April 2006.\n\nThe intent of Standard ISO 22715 is to specify how cosmetic products should be packaged and labeled to maintain a certain level of standards within the cosmetic industry. This standard applies to cosmetic products whether the product is sold or given away. It is one of 26 published standards that are devoted to the cosmetic industry sector.\n\nISO 22715 does not regulate what products are to be considered cosmetic. This determination is left to the national regulations of those countries that follow the ISO 22715 and use it as a guide to best practices for packaging and labeling cosmetic products. Often, these national regulations are stricter than those provided in the standard are.\n\nISO 22715 supports the need for consumers to know what is in the cosmetic products they purchase, how those products should be used and who has manufactured the products. To accomplish this, 22715 specifies that a product’s packaging should show certain information such as the ingredients used in the product listed in descending order according to the percentage of each ingredient in the product. A list of coloring agents used in the product then follows the list of ingredients.\n\nFor safety purposes, ISO recommends including an explanation of what the product function is on the package, along with instructions for its use. Any precautionary or warning statements should also be printed on the package to caution consumers in the use of said product. Additional information that should appear on the packaging of cosmetic products includes:\n\n\nThe International Organization for Standardization (ISO) was formed in 1947. It is a non-governmental organization (NGO) that is based in Geneva, Switzerland. The ISO has 162 members and the organization represents the interest of international standards of 196 countries. This represents almost 97 percent of the world’s population.\n\nThe ISO’s purpose is to create standards that are used to help form public policies and business objectives that benefit people throughout the world. Since May 2016, there has been over 21,500 ISO standards published. The ISO develops new standards when industry sectors and their stakeholders determine that a need exists. The standards are developed under the oversight of the ISO through cooperative efforts between other NGOs, consumer organizations, representatives from government agencies, academics and testing laboratories.\n\nISO 22715 is not legally binding, but it is the common denominator used for developing national regulations that address the labeling and packaging of cosmetic products. Regulators in individual countries often look to ISO standards as the benchmark for best practices for the different industry sectors in which they apply. Many regulators require businesses and manufacturers to comply with applicable ISO standards, in addition to local regulations.\n\nCurrently, there are 26 published ISO standards devoted to the cosmetic sector, including ISO 22715. These standards are overseen by the ISO’s cosmetic product technical committee that was established in 1998. This committee is composed of standardization bodies from major markets, such as leading ASEAN countries, most European counties and the United States via ANSI. Thirty-nine countries participate in the creation of standards for cosmetic products, with 27 observing countries within the committee.\n\nTo better distribute and update standards as needed, the ISO maintains the copyright on its standards. Most standards are reviewed and updated every five to seven years to remain relevant to the latest technologies within industry sectors.\n\nOften, local regulations may exceed the requirements specified in ISO 22715. In the United States, the Food & Drug Administration (FDA) regulates the cosmetic industry with standards that are provided by the American National Standards Institute (ANSI), in which the FDA is member.\n\nIn the European Union, cosmetic manufacturers must abide by Regulation (EC) No. 1223/2009 Article 19 for the labeling of cosmetic products.\n\n"}
{"id": "57918208", "url": "https://en.wikipedia.org/wiki?curid=57918208", "title": "Imperial Buttery", "text": "Imperial Buttery\n\nImperial Buttery or Yuchashanfang () was a division of the Imperial Household Department in charge of cooking ordinary meals for the Qing court.\n"}
{"id": "25256459", "url": "https://en.wikipedia.org/wiki?curid=25256459", "title": "In situ capping of subaqueous waste", "text": "In situ capping of subaqueous waste\n\nIn-Situ Capping (ISC) of Subaqueous Waste is a non-removal remediation technique for contaminated sediment that involves leaving the waste in place and isolating it from the environment by placing a layer of soil and/or material over the contaminated waste as to prevent further spread of the contaminant. In-situ capping provides a viable way to remediate an area that is contaminated. It is an option when pump and treat becomes too expensive and the area surrounding the site is a low energy system. The design of the cap and the characterization of the surrounding areas are of equal importance and drive the feasibility of the entire project. Numerous successful cases exist and more will exist in the future as the technology expands and grows more popular. In-situ capping uses techniques developed in chemistry, biology, geotechnical engineering, environmental engineering, and environmental geotechnical engineering.\nContaminants located in sediments still pose a risk to the environment and human health. Some of the direct effects on aquatic life that can be associated with contaminated sediment include “the development of cancerous tumors in fish exposed to polycyclic aromatic hydrocarbons in sediments.\" These high-risk sediments need to be remediated. There are usually only four options for remediation:\n\n\nThe cap can be made up of many different things, including but not limited to sand, gravel, geotextiles, and multiple layers of these options.\n\nThere are many ways that a contaminant inside sediment can become introduced to the environment. These ways include but are not limited to advection, diffusion, benthic organisms mixing and reworking of the upper layer of the contaminated sediment, and sediment re-suspension by different subaquatic forces. In-situ capping (ISC) can fix all of these adverse effects with three primary functions:\n\n\nA fourth, although not necessary, function of an in-situ cap should be the \"encouragement of habitat values.\" This should not be made a primary goal except in extreme circumstances. This can be achieved by altering the superficial characteristics of the cap to \"encourage desirable species or discourage undesirable species.\"\n\nThe obvious advantage of using in-situ capping is that the waste will not be disturbed, and it prevents further contamination of the surrounding area from movement of the contaminant by removal. Sadly, the long-term effects of ISC have not been studied since it is an emerging technology.\n\nIn-situ capping has been effective in numerous locations. For example, in several places in the interior of Japan “in-situ capping of nutrient-laden sediments with sand” has worked very well in preserving water quality by reducing “the release of nutrients (nitrogen and phosphorus)” and oxygen depletion by bottom sediments.\n\nIt is very important to evaluate the site and goals of a specific project to determine if ISC is the right technique to use. First, it is important to find out if ISC will satisfy all of the desired remedial objectives. To determine if ISC will satisfy the remedial objectives it is important to look at the three primary functions previously listed for ISC. For the first function it is important to realize that “the ability of an ISC to isolate aquatic organisms from the sediment contaminants is dependent upon” the deposition of new sediment contaminants being deposited on the cap. If contaminated sediment is deposited back on top of the cap then a cap was built to separate to contaminated layers. Thus, “ISC should only be considered if source control has been implemented.\" Stabilization of the contaminated sediment could be a design function if the goal of remediation is to prevent negative environmental impacts due to “resuspension, transport and redeposition” of the contaminated sediments to other remote areas. Furthermore, if a remedial objective is desired, then the purpose of the ISC could be to isolate the contaminated soil from the surrounding environment, thus controlling the environment of the contaminated soil and causing possible degradation of the contaminate.\n\nOn site evaluation to see if ISC is a good remediation technique is based on several criteria: the surrounding physical environment, current and long-term hydrodynamic conditions, the geotechnical and geological conditions, hydrogeological conditions, on-site sediment characterization, and current and long-term waterway uses.\n\nMany of the physical properties of the surrounding area where the cap would be placed are important. Some things to consider when constructing a cap would be “waterway dimensions, water depths, tidal patterns, ice formations, aquatic vegetation, bridge crossings and proximity of lands or marine structures”. It is best if the area surrounding the ISC is flat for ease of installation.\n\nThe hydrodynamic conditions are of equal importance. It is best if in-situ capping projects are performed in low-energy waterways such as harbors, low flow streams, or estuaries. High energy and high flow environments can affect the long-term stability of the cap and cause plausible erosion over time. Currents are also important. Currents vary along a water column and placement of the ISC can be negatively affected by changing currents. It is important to take into consideration the long-term impacts of episodic events such as tidal flow on bottom current velocities. Modeling must be done to determine if placement of the in-situ cap will alter existing hydrodynamic conditions.\n\nA study of the geotechnical and geological conditions must be made before the placement of the in-situ cap because of potential settling underneath the cap. If settling is predicted to be significant, the cap design may have to be designed thicker than originally projected to allow the settling to not alter the integrity of the cap.\n\nHydrogeological conditions are important to consider before placement. It is important to locate areas of discharge, which are areas where the groundwater flow path has an upward component. This discharge can cause the in-situ cap to become displaced or cause containments to be transported to the surface water, thus causing decreased effectiveness of the in-situ cap.\n\nTypical sediment characterization is needed before construction and design of the ISC can be implemented. These tests on the sediments include: “visual classification, natural water content/solids concentrations, plasticity indices (Atterberg limits), total organic carbon (TOC) content, grain size distribution, specific gravity, and Unified Soil Classification System (USCS)”.\n\nIt is important to realize what the current waterway uses are and how they may be affected with the placement of an in-situ cap. Some waterway uses that may be affected by the construction of an in-situ cap include but are not limited to “navigation, flood control, recreation, water supply, storm water or effluent discharge, waterfront development, and utility crossing.\" Since the construction of an in-situ cap may limit some of these activities due to the importance that the caps integrity be maintained over an extended period of time, any use that may cause displacement of the cap should be limited. Furthermore, the construction of an in-situ cap will cause a drop in water depth thus limiting the size of ships that may cross the area. These limitations on the waterway may also have social and economic impacts that must be considered.\n\nIt is important to know all of the regulatory standards in place for the desired location of the ISC. All ISC must comply with the requirements in the Resource Conservation and Recovery Act (RCRA) and the Toxic Substances Control Act (TSCA), although the ability of in-situ capping to meet those standards in the long term has not been successfully researched and studied enough due to lack of data.\n\nCap design, which includes the composition and dimensions of the components, is probably the most important aspect of in-situ capping. The cap designs “must be compatible with available construction and placement techniques” along with meeting the three previously mentioned criteria above. The cap designs usually are over small areas with small volumes of contaminants. The cap is usually constructed with many layers of granular media, armor stone, and geotextiles. Presently, laboratory tests and models of the various processes involved (advection, diffusion, bioturbation, consolidation, erosion), limited field experience, and monitoring data drive cap design. Since data and field experience is limited a conservative approach is used when designing an in-situ cap. This approach uses the idea that the many different components are additive and no cap component provides a dual function, although a component may provide a dual function in actual practice.\n\nThe six general steps for in-situ cap design, provided by Palermo et al. are listed below:\n\n\nIdentifying the materials should be assessed at the beginning of the project because they typically represent the largest cost to the project. Thus, if the materials needed cost too much the project may not be feasible at all.\nGranular materials are used in most cases. These can include but are not limited to “quarry sand, naturally occurring sediments or soil materials”. Studies have shown that fine-grained materials and sandy materials can be effective in the construction of an in-situ cap. Furthermore, fine grain materials have been shown to act as better chemical barriers than sand caps. Thus a fine grain material is a better capping component than factory-washed sand. It is important to have control the amount of organic material within the cap because the benthic organisms have shown interest in burrowing within any unconsolidated fine grained sediments containing organic matter. Increased levels of organic matter in sands have shown an increase in the retardation of hydrophobic organic contaminants through the cap and encourage degradation of contaminant. Thus a careful balance of organics is necessary.\n\nGeomembranes can serve numerous purposes in a cap design, including “provide a bioturbation barrier; stabilize the cap; reduce contaminant flux; prevent mixing of cap materials with underlying sediments; promote uniform consolidation, and; reduce erosion of capping materials”. Geomembranes have been used for stabilization in two projects along with granular media for the ISC constructed at Sheboygan River and in Eitrheim Bay, Norway. Although geomembranes seem to have great benefits, the problem of uplift and ballooning has arisen and not much research has gone into assessing what causes the lift of the geomembranes off of the surface. Further research is needed to determine the overall effectiveness of geosynthetics for chemical isolation.\n\nArmoring stone, which is any stone that is used to \"shield\" the rest of the in-situ cap, can be used for resistance to erosion and should be considered in cap design. The long-term ability of the cap to perform depends primarily on its ability to withstand external forces, mostly hydraulic forces. There are three basic approaches that may be used to have long-term cap stability:\n\n\nBioturbation is defined as the disturbance and mixing of sediments by benthic organisms. Many aquatic organisms live on or in the bottom sediments and can greatly increase the “migration of sediment contaminants through the direct movement of sediment particles, increasing the surface area of sediments exposed to the water column, and as a food for epibenthic or pelagic organism grazing on the benthos.\" The depth of bioturbation in marine environments is greater than that in fresh water environments. To prevent and reduce the impact of bioturbation on the cap, the cap should be designed with a sacrificial layer, typically only a few centimeters thick (5–10 cm). This layer will be assumed to be completely mixed with the environment and should prevent benthic organism from descending further into the in-situ cap. The thickness of the sacrificial layer should be based on a study of the local organisms and their behavior in the surrounding sediment near the area of the cap construction, since some benthic organisms have been known to burrow at depths of 1m or more. The presence of armor stone has been known to limit the colonization by deep burrowing benthic creatures. Another method of preventing benthic organisms from destroying the integrity of the cap design is to pick a granular media that the local benthic organism find unattractive and are not known to readily colonize on that surface, thus limiting the chance a benthic organism will grown on the cap.\n\nThe consolidation of the in-situ cap must be considered, provided that the selected material for the cap “is fine-grained granular material.\" The consolidation of the underlying material should be taken into account due to “advection of pore water upward into the cap during consolidation.\"\n\nErosion should be carefully considered. To determine the level of protection against erosion it is important to look at “the potential severity of the environmental impacts associated with cap erosion and potential dispersion of the sediment contaminants in an extreme event” (such as a 100-year event). An under-designed in-situ cap could be compromised by erosion resulting in the release of contaminants. An over-designed cap would result in extremely high costs.\n\nSince the construction of the cap will directly affect the ability of the in-situ cap to perform it is important to plan carefully. It is important to note that \"many contaminated sediment sites exhibit exceedingly soft sediments that can be easily disturbed, may be dislocated or destabilized by uneven placement, and may have insufficient load bearing capacity to support some cap materials.\" \n\nThere are two basic was to construct an in-situ cap:\n\nFredette et al. outlines five steps for the development of a physical/biological monitoring program for ISC projects:\n\n\nThus it is important a monitoring program be put into place at the onset of construction. A short-term monitoring program should be used to monitor the in-situ cap during construction and immediately following construction. This monitoring program should include frequent testing so real-time data is provided to allow quick adjustments to the overall cap design. A long-term monitoring program should be established to provide data about the overall effectiveness of the cap design and to make sure the cap is meeting all of its required regulations and that the cap is not excessively eroded. This long-term monitoring need only be assessed on a yearly to bi-yearly basis unless a problem is discovered; then more frequent testing will be required.\n\nDuring monitoring, it is important to schedule routine maintenance. This may include placement of material equal to the predicted amount of material removed due to erosion.\n\nAlthough ISC is a relatively new remediation procedure several groups have used it with great success.\n\nIn Massena, New York, at the General Motors Superfund site, PCB-contaminated soils were dredged repeatedly but some areas still had high levels of contaminant (>10ppm). These areas were capped, an approximate area of , with a three-layer ISC composed of 6 inches of sand, 6 inches of gravel and 6 inches of armor stone.\n\nIn Manistique River, Michigan, PCB-contaminated sediments were capped with a 40mm thick plastic liner over an area of with varying depths of up to 15 ft.\n\nIn Sheboygan River, Wisconsin, PCB-contaminated sediments were capped with a sand layer and armor stone layer. This was done in shallow regions were direct placement was possible.\n\nIn Cold Spring, New York, in the Hudson River, sediment was contaminated with cadmium and nickel from a battery manufacturing facility. A Geosynthetic clay liner (GCL) and a 12-inch covering of sandy loam was planted on top of the contaminated area.\n\nIn Elkton, Maryland, contaminated sediment was discovered with excess amounts of volatile organic components and dense non-aqueous phase liquids, resulting is severe discharge. The cap system constructed over the contaminated waste involved a geotextile working mat, a GCL, a scrim-reinforced polypropylene liner, a geotextile cushion, and a gabion mat.\n\nThere are four major areas of research that currently need to be assessed:\n\n\n"}
{"id": "20804272", "url": "https://en.wikipedia.org/wiki?curid=20804272", "title": "Kayayei", "text": "Kayayei\n\nKayayei or Kaya Yei is a Ghanaian term that refers to a female porter or bearer. Many of these women have migrated from a rural community to any of Ghana's urban cities in search of work. They generally carry their burdens on their heads.\n\nThe term \"kayayei\" (singular, \"kaya yoo\") is a compound formed from two languages spoken in Ghana. \"Kaya\" means \"load, luggage, goods or burden\" in the Hausa language, and \"yei\" means \"women or females\" in the Ga language. People in Kumasi refer to the porters as \"paa o paa\".\n\n\"Kaya\" have always been manual labourers. They transport goods to and from markets, particularly agricultural goods. Typically, \"Kaya\" carry their loads in a large pan placed on their heads, using a moistened coil of cloth as a buffer, see Head-carrying.\n\n\"Kaya Yei\" still toil away in markets in Ghana today, often in poor conditions and with minimal income. Occasionally, \"Kaya\" are brought into private homes to perform domestic tasks, where they may earn slightly more. The \"Kaya\" are often transient, and often without basic sanitation. Basic hygiene & nutrition conditions are also poor. In larger cities such as Accra & Kumasi, \"Kaya\" are often migrants from remote regions who have come to the cities in search for better employment prospects.\n\nIn May 2016, the Minister for Gender, Children and Social Protection Nana Oye Lithur ensured that over 1,000 \"kayayei\" from Agbogbloshie and Mallam Atta markets in Accra were registered onto the National Health Insurance Scheme, to help provide them access to basic healthcare service. In the 2017 annual budget, the Minister of Finance Ken Ofori-Atta exempted \"kayayei\" from paying market tolls to their various assemblies .\n\n"}
{"id": "43082530", "url": "https://en.wikipedia.org/wiki?curid=43082530", "title": "Ministry of Energy, Science &amp; Technology and Public Utilities", "text": "Ministry of Energy, Science &amp; Technology and Public Utilities\n\nThe Ministry of Energy, Science & Technology, and Public Utilities (Belize) was founded in 2012. The Ministry is currently divided into the Department of Geology and Petroleum, the Energy Unit and the Science and Technology Unit. The Ministry is represented by Senator Joy Grant and CEO Dr Colin Young, and has an office in Belmopan.\n\nThe Geology and Petroleum Department was established in 1984 as part of the Ministry of Natural Resources. In 2012 the department moved to the new Ministry of Energy, Science & Technology and Public Utilities. The department is responsible for governance of the petroleum industry in Belize. The department's mission statement is to \"To accelerate the development of Belize’s petroleum resources through the creation of a vibrant petroleum industry, with the assistance of international investors, cognizant of environmental costs, thereby improving the welfare of Belizeans into the 21st century.\"\n\nThe Energy Unit was established in 2012 and has responsibility for governance of the energy sector in Belize. The Unit's mission statement is to \"To plan, promote and effectively manage the production, delivery and use of energy through Energy Efficiency, Renewable Energy, and Cleaner Production interventions for the sustainable development of Belize.\"\nKey activities performed by the Energy Unit include data collection for the purpose of planning Belize's future energy supplies and calculating greenhouse gas emissions, public awareness on topics such as energy efficiency, as well as regulation and market reforms that promote a sustainable future for Belize.\n\nThe Science and Technology Unit is responsible for the promotion of Science and Technology in Belize. The Unit plays a key role in Belize's efforts to achieve in the millennium development goals. The Unit conducts a number of activities that promote engagement with Science and Technology in Belize, including through the ICT roadshow.\n\n"}
{"id": "5620714", "url": "https://en.wikipedia.org/wiki?curid=5620714", "title": "Mockup", "text": "Mockup\n\nIn manufacturing and design, a mockup, or mock-up, is a scale or full-size model of a design or device, used for teaching, demonstration, design evaluation, promotion, and other purposes. A mockup is a \"prototype\" if it provides at least part of the functionality of a system and enables testing of a design. Mock-ups are used by designers mainly to acquire feedback from users. Mock-ups address the idea captured in a popular engineering one-liner: You can fix it now on the drafting board with an eraser or you can fix it later on the construction site with a sledge hammer.\n\nMockups are used as design tools virtually everywhere a new product is designed.\n\nMockups are used in the automotive device industry as part of the product development process, where dimensions, overall impression, and shapes are tested in a wind tunnel experiment. They can also be used to test consumer reaction.\n\nMockups, wireframes and prototypes are not so cleanly distinguished in software and systems engineering, where mockups are a way of designing user interfaces on paper or in computer images. A software mockup will thus look like the real thing, but will not do useful work beyond what the user sees. A software prototype, on the other hand, will look and work just like the real thing. In many cases it is best to design or prototype the user interface before source code is written or hardware is built, to avoid having to go back and make expensive changes.\n\nEarly layouts of a World Wide Web site or pages are often called \"mockups\". A large selection of proprietary or open-source software tools are available for this purpose.\n\nMockups are part of the military acquisition process. Mockups are often used to test human factors and aerodynamics, for example. In this context, mockups include wire-frame models. They can also be used for public display and demonstration purposes prior to the development of a prototype, as with the case of the Lockheed Martin F-35 Lightning II mock-up aircraft.\n\nMockups are used in the consumer goods industry as part of the product development process, where dimensions, human factors, overall impression, and commercial art are tested in marketing research.\n\nMockups are commonly required by designers, architects, and end users for custom furniture and cabinetry. The intention is often to produce a full-sized replica, using inexpensive materials in order to verify a design. Mockups are often used to determine the proportions of the piece, relating to various dimensions of the piece itself, or to fit the piece into a specific space or room. The ability to see how the design of the piece relates to the rest of the space is also an important factor in determining size and design.\n\nWhen designing a functional piece of furniture, such as a desk or table, mockups can be used to test whether they suit typical human shapes and sizes. Designs that fail to consider these issues may not be practical to use. Mockups can also be used to test color, finish, and design details which cannot be visualized from the initial drawings and sketches. Mockups used for this purpose can be on a reduced scale.\n\nThe cost of making mockups is often more than repaid by the savings made by avoiding going into production with a design which needs improvement.\n\nThe most common use of mockups in software development is to create user interfaces that show the end user what the software will look like without having to build the software or the underlying functionality. Software UI mockups can range from very simple hand drawn screen layouts, through realistic bitmaps, to semi functional user interfaces developed in a software development tool.\n\nMockups are often used to create unit tests - there they are usually called mock objects. The main reasons to create such mockups is to be able to test one part of a software system (a unit) without having to use dependent modules. The function of these dependencies is then \"faked\" using mock objects.\n\nThis is especially important if the functions that are simulated like this are difficult to obtain (for example because it involves complex computation) or if the result is non-deterministic, such as the readout of a sensor.\n\nA common style of software design is Service-oriented architecture (SOA), where many components communicate via protocols such as HTTP. Service virtualization and API mocks and simulators are examples of implementations of mockups or so called over-the-wire test doubles in software systems that are modelling dependent components or microservices in SOA environments.\n\nMockup software can also be used for micro level evaluation, for example to check a single function, and derive results from the tests to enhance the products power and usability on the whole.\n\nAt the beginning of a project's construction, architects will often direct contractors to provide material mockups for review. These allow the design team to review material and color selections, and make modifications before product orders are placed. Architectural mockups can also be used for performance testing (such as water penetration at window installations, for example) and help inform the subcontractors how details are to be installed.\n\n"}
{"id": "1762926", "url": "https://en.wikipedia.org/wiki?curid=1762926", "title": "Molecular machine", "text": "Molecular machine\n\nA molecular machine, nanite, or nanomachine, refers to any discrete number of molecular components that produce quasi-mechanical movements (output) in response to specific stimuli (input). In biology, macromolecular machines frequently perform tasks essential for life such as DNA replication and ATP synthesis. The expression is often more generally applied to molecules that simply mimic functions that occur at the macroscopic level. The term is also common in nanotechnology where a number of highly complex molecular machines have been proposed that are aimed at the goal of constructing a molecular assembler.\n\nFor the last several decades, chemists and physicists alike have attempted, with varying degrees of success, to miniaturize machines found in the macroscopic world. Molecular machines research is currently at the forefront with the 2016 Nobel Prize in Chemistry being awarded to Jean-Pierre Sauvage, Sir J. Fraser Stoddart and Bernard L. Feringa for the design and synthesis of molecular machines.\n\nMolecular machines can be divided into two broad categories; artificial and biological. In general, artificial molecular machines (AMMs) refer to molecules that are artificially designed and synthesized whereas biological molecular machines can commonly be found in nature.\n\nA wide variety of artificial molecular machines (AMMs) have been synthesized by chemists which are\nrather simple and small compared to biological molecular machines.\nThe first AMM, a molecular shuttle, was synthesized by Sir J. Fraser Stoddart. <ref name=\"10.1021/ja00013a096\"></ref>\nA molecular shuttle is a rotaxane molecule where a ring is mechanically interlocked onto an axle with two bulky stoppers. The ring can move between two binding sites with various stimuli such as light, pH, solvents, and ions. As the authors of this 1991 JACS paper noted: “Insofar as it becomes possible to control the movement of one molecular component with respect to the other in a [2]rotaxane, the technology for building molecular machines will emerge.”, mechanically interlocked molecular architectures spearheaded AMM design and synthesis as they provide directed molecular motion. Today a wide variety of AMMs exists as listed below.\n\nMolecular motors are molecules that are capable of rotary motion around a single or double bond.\nSingle bond rotary motors are generally fueled by chemical reactions whereas double bond rotary motors are generally fueled by light. The rotation speed of the motor can also be tuned by careful molecular design. Carbon nanotube nanomotors have also been produced. \n\nA molecular propeller is a molecule that can propel fluids when rotated, due to its special shape that is designed in analogy to macroscopic propellers. It has several molecular-scale blades attached at a certain pitch angle around the circumference of a nanoscale shaft. Also see molecular gyroscope.\n\nA molecular switch is a molecule that can be reversibly shifted between two or more stable states. <ref name=\"10.1021/cr9900228\"></ref> The molecules may be shifted between the states in response to changes in pH, light, temperature, an electric current, microenvironment, or the presence of a ligand. <ref name=\"10.1021/cr9900228\"></ref> \n\nA molecular shuttle is a molecule capable of shuttling molecules or ions from one location to another. <ref name=\"10.1038/369133a0\"></ref> A common molecular shuttle consists of a rotaxane where the macrocycle can move between two sites or stations along the dumbbell backbone. <ref name=\"10.1038/369133a0\"></ref> <ref name=\"10.1021/ja00013a096\"></ref> \n\nNanocars are single molecule vehicles that resemble macroscopic automobiles and are important for understanding how to control molecular diffusion on surfaces. The first nanocars were synthesized by James M. Tour in 2005. They had an H shaped chassis and 4 molecular wheels (fullerenes) attached to the four corners. In 2011, Ben Feringa and co-workers synthesized the first motorized nanocar which had molecular motors attached to the chassis as rotating wheels. The authors were able to demonstrate directional motion of the nanocar on a copper surface by providing energy from a scanning tunneling microscope tip. Later in 2017, worlds first ever Nanocar race took place in France.\n\nA molecular balance is a molecule that can interconvert between two and more conformational or configurational states in response to the dynamic of multiple intra- and intermolecular driving forces, such as hydrogen bonding, solvophobic/hydrophobic effects, π interactions, and steric and dispersion interactions. \n\nMolecular tweezers are host molecules capable of holding items between their two arms. The open cavity of the molecular tweezers binds items using non-covalent bonding including hydrogen bonding, metal coordination, hydrophobic forces, van der Waals forces, π interactions, or electrostatic effects. Examples of molecular tweezers have been reported that are constructed from DNA and are considered DNA machines. \n\nA molecular sensor is a molecule that interacts with an analyte to produce a detectable change. Molecular sensors combine molecular recognition with some form of reporter, so the presence of the item can be observed.\nA molecular logic gate is a molecule that performs a logical operation on one or more logic inputs and produces a single logic output. Unlike a molecular sensor, the molecular logic gate will only output when a particular combination of inputs are present.\n\nA molecular assembler is a molecular machine able to guide chemical reactions by positioning reactive molecules with precision.\n\nA molecular hinge is a molecule that can be selectively switched from one configuration to another in a reversible fashion. Such configurations must have distinguishable geometries, for instance, \"Cis\" or \"Trans\" isomers of a V-shape molecule. Azo compounds perform Cis–trans isomerism upon receiving UV-Vis light.\n\nThe most complex macromolecular machines are found within cells, often in the form of multi-protein complexes. Some biological machines are motor proteins, such as myosin, which is responsible for muscle contraction, kinesin, which moves cargo inside cells away from the nucleus along microtubules, and dynein, which moves cargo inside cells towards the nucleus and produces the axonemal beating of motile cilia and flagella. \"[I]n effect, the [motile cilium] is a nanomachine composed of perhaps over 600 proteins in molecular complexes, many of which also function independently as nanomachines...Flexible linkers allow the mobile protein domains connected by them to recruit their binding partners and induce long-range allostery via . \" Other biological machines are responsible for energy production, for example ATP synthase which harnesses energy from proton gradients across membranes to drive a turbine-like motion used to synthesise ATP, the energy currency of a cell. Still other machines are responsible for gene expression, including DNA polymerases for replicating DNA, RNA polymerases for producing mRNA, the spliceosome for removing introns, and the ribosome for synthesising proteins. These machines and their nanoscale dynamics are far more complex than any molecular machines that have yet been artificially constructed.\n\nThese biological machines might have applications in nanomedicine. For example, they could be used to identify and destroy cancer cells. Molecular nanotechnology is a speculative subfield of nanotechnology regarding the possibility of engineering molecular assemblers, biological machines which could re-order matter at a molecular or atomic scale. Nanomedicine would make use of these nanorobots, introduced into the body, to repair or detect damages and infections. Molecular nanotechnology is highly theoretical, seeking to anticipate what inventions nanotechnology might yield and to propose an agenda for future inquiry. The proposed elements of molecular nanotechnology, such as molecular assemblers and nanorobots are far beyond current capabilities.\n\nThe construction of more complex molecular machines is an active area of theoretical and experimental research. A number of molecules, such as molecular propellers, have been designed, although experimental studies of these molecules are inhibited by the lack of methods to construct these molecules. In this context, theoretical modeling can be extremely useful to understand the self-assembly/disassembly processes of rotaxanes, important for the construction of light-powered molecular machines. This molecular-level knowledge may foster the realization of ever more complex, versatile, and effective molecular machines for the areas of nanotechnology, including molecular assemblers.\n\nAlthough currently not feasible, some potential applications of molecular machines are transport at the molecular level, manipulation of nanostructures and chemical systems, high density solid-state informational processing and molecular prosthetics. Many fundamental challenges need to be overcome before molecular machines can be used practically such as autonomous operation, complexity of machines, stability in the synthesis of the machines and the working conditions.\n\nIn 2018, an international team of researchers, led by researchers from the University of Osaka, Japan, created a molecular machine in which elements of a mechanical ratchet were used. The main advantage of this machine is that it provides movement in only one direction. In addition, some features of the structure of the molecular machine provide the best balance between the produced motion and chemical reactivity of the molecules that make up it, that is a problem in itself.\n"}
{"id": "383472", "url": "https://en.wikipedia.org/wiki?curid=383472", "title": "Moodle", "text": "Moodle\n\nMoodle is a free and open-source learning management system (LMS) written in PHP and distributed under the GNU General Public License. Developed on pedagogical principles, Moodle is used for blended learning, distance education, flipped classroom and other e-learning projects in schools, universities, workplaces and other sectors. \n\nWith customizable management features, it is used to create private websites with online courses for educators and trainers to achieve learning goals. Moodle (acronym for \"modular object-oriented dynamic learning environment\") allows for extending and tailoring learning environments using community sourced plugins.\n\nMoodle was originally developed by Martin Dougiamas to help educators create online courses with a focus on interaction and collaborative construction of content, and it is in continual evolution. The first version of Moodle was released on 20 August 2002. Nowadays the Moodle Project is led and coordinated by Moodle HQ, an Australian company of 50 developers which is financially supported by a network of eighty-four Moodle Partner service companies worldwide. Moodle's development has also been assisted by the work of open-source programmers.\n\nMoodle as a learning platform can enhance existing learning environments. As an E-learning tool, Moodle has a wide range of standard and innovative features such as a calendar and a Gradebook. Moodle is a leading virtual learning environment and can be used in many types of environments such as education, training and development and in business settings. \n\nPlugins are a flexible tool set, allowing Moodle users to extend the features of the site. There are hundreds of plugins for Moodle, extending the features of Moodle's core functionality. Each plugin is maintained in the Moodle plugins directory. As of June 2017, there are 1,342 plugins available for Moodle with over 405,200 recent downloads.\n\nGraphical themes for Moodle can be installed to change the look and functionality of a Moodle site or of an individual course.\n\nMany Moodle themes, based on responsive web design, allow Moodle to be used on mobile devices. Also, a Moodle mobile app is available in Google Play, App Store (iOS), and the Windows Phone Store.\n\nMoodle has adopted the following e-learning standards:\n\nUsers can download and install Moodle on a Web server, such as Apache HTTP Server, and a number of database management systems, such as PostgreSQL, are supported. Pre-built combinations of Moodle with a Web server and database are available for Microsoft Windows and Macintosh. Other automated installation approaches exist, such as installing a Debian package, deploying a ready-to-use TurnKey Moodle appliance, using the Bitnami installer, or using a \"one-click install\" service such as Installatron.\n\nCertified Moodle Partners provide other Moodle services, including hosting, training, customization and content development. This network of providers support development of the Moodle project through royalties.\n\nMoodle runs without modification on Unix, Linux, FreeBSD, Windows, OS X, NetWare and any other systems that support PHP and a database, including webhost providers.\n\nMoodle also has import features for use with other specific systems, such as importing quizzes or entire courses from Blackboard or WebCT. , Moodle will not import Blackboard courses, apparently due to changes in php code-releases.\n\nIn March 2012 Blackboard acquired two companies based on Moodle's software including Baltimore-based Moodlerooms Inc. and NetSpot of Adelaide, Australia. In August 2015, Blackboard acquired Colombia-based Nivel7, possibly the largest Moodle services provider in Latin America. The Red Hat site, Opensource.com, reports that Moodle will always be an open-source project, with clear delineation between Blackboard and Moodle.\n\nFor many schools, colleges and universities, SIMS integration is key to ensure a seamless data flow from the Management Information System through to the Learning Platform and back again. Many Learning Platforms do not enable write-back functionality, however this is often a key requirement from schools needing to write data onto their MIS and back again.\n\nMartin Dougiamas, who has graduate degrees in computer science and education, wrote the first version of Moodle. Dougiamas started a Ph.D. to examine \"the use of open source software to support a social constructionist epistemology of teaching and learning within Internet-based communities of reflective inquiry.\" Although how exactly social constructionism makes Moodle different from other eLearning platforms is difficult to show, it has been cited as an important factor by Moodle adopters. Other Moodle adopters, such as the Open University in the UK, have pointed out that Learning Management Systems can equally be seen as \"relatively pedagogy-neutral\".\n\nThe stated philosophy of Moodle includes a constructivist and social constructionist approach to education, emphasizing that learners (and not just teachers) can contribute to the educational experience. Using these pedagogical principles, Moodle provides an environment for learning communities.\n\nThe acronym \"Moodle\" stands for \"modular object-oriented dynamic learning environment.\" (In the early years the \"m\" stood for \"Martin's\", named after Martin Dougiamas, the original developer). As well as being an acronym, the name was chosen because of the dictionary definition of Moodle and to correspond to an available domain name.\n\nMoodle has continued to evolve since 1999 (since 2001 with the current architecture). It has been translated into over 100 different languages and is accessible in many countries worldwide. Institutions can add as many Moodle servers as needed without having to pay license fees. The Open University of the UK currently uses a Moodle installation for their 200,000 users while the UK government uses a Moodle installation for their Civil Service Learning platform serving half a million employees.\n\nSee official release page\n\nA MoodleMoot is a conference for Moodle community members, held to learn about Moodle, share experiences of the learning platform, discuss research in related educational technologies and contribute ideas to future Moodle development. Held around the world, MoodleMoots are organised by universities or other large organisations using Moodle, Moodle Partners, Moodle associations or Moodle HQ.\n\nIn the higher education market in the United States, Moodle was for a time the second largest provider with 23% market share in 2013 (following Blackboard (41%), Desire2Learn (11%), and Instructure). However, by 2017 Moodle had dropped to the third largest provider, due in part to increased adoption of Instructure's Canvas platform. In March 2016 Blackboard became an official Moodle partner.\n\n"}
{"id": "15058577", "url": "https://en.wikipedia.org/wiki?curid=15058577", "title": "Norman Feather", "text": "Norman Feather\n\nNorman Feather FRS FRSE PRSE (16 November 1904, Pecket Well, Yorkshire – 14 August 1978, Christie Hospital, Manchester), was an English nuclear physicist. Feather and Egon Bretscher were working at the Cavendish Laboratory, Cambridge in 1940, when they proposed that the 239 isotope of element 94 (plutonium) would be better able to sustain a nuclear chain reaction. This research, a breakthrough, was part of the Tube Alloys project, the secret British project during World War II to develop nuclear weapons.\n\nFeather was the author of a series of noted introductory texts on the history, fundamental concepts, and meaning of physics.\n\nHe was born the son of Samson Feather, headmaster of Pecket Well, where he was born. The family moved during his infancy when his father became headmaster of Holme Primary School in Yorkshire, and his wife Lucy Clayton.\n\nHe was educated at Bridlington Grammar School and Trinity College, Cambridge, graduating BA in 1925 before taking a year in the University of London gaining a BSc in 1926. He was a Fellow of Trinity College from 1929 to 1933 then Fellow and Lecturer in Natural Sciences there from 1936 to 1945. Feather received his doctorate (PhD) at Cambridge in 1931 under James Chadwick and Ernest Rutherford. His research employed a Wilson cloud chamber and focused on the problem of the long-range alpha particles.\n\nIn 1932 Feather married Kathleen Grace Burke (d.1975).\n\nFeather conducted some of the earliest investigations of the neutron following its discovery by James Chadwick at Cambridge in 1932. Indeed, Feather assisted Chadwick with his initial investigations leading to the discovery. The radioactive polonium source Chadwick used to discover the neutron was derived from old radon tubes that had been acquired by Feather from Kelly Hospital in Baltimore during a year-long visit to Johns Hopkins University in 1929. Feather obtained the first evidence that neutrons can produce nuclear disintegrations. The year 1932 would later be referred to as the\n\"annus mirabilis\" for nuclear physics in the Cavendish Laboratory.\n\nIn 1940 Feather and Egon Bretscher at the Cavendish Laboratory, made a breakthrough in nuclear research for the Tube Alloys project. They proposed that the 239 isotope of element 94 could be produced from the common isotope of uranium-238 by neutron capture. Like U-235, this new element should be able to sustain a nuclear chain reaction. A slow neutron reactor fueled with uranium would, in theory, produce substantial amounts of plutonium-239 as a by-product, since U-238 absorbs slow neutrons to form the new isotope U-239. This nuclide rapidly emits an electron, decaying into an element with a mass of 239 and an atomic number of 93. This nuclide then emits another electron to become a new element still of mass 239, but with an atomic number 94 and a much greater half-life.\n\nBretscher and Feather showed theoretically feasible grounds that element 94 would be readily 'fissionable' by both slow and fast neutrons, and had the added advantage of being chemically different from uranium and therefore could easily be separated from it. This was confirmed independently in 1940 by Edwin M. McMillan and Philip Abelson at the Berkeley Radiation Laboratory. Nicholas Kemmer of the Cambridge team proposed the names Neptunium for the new element 93 and Plutonium for 94 by analogy with the outer planets Neptune and Pluto beyond Uranus (uranium being element 92). The Americans fortuitously suggested the same names. The production and identification of the first sample of plutonium in 1941 is generally credited to Glenn Seaborg, who used a cyclotron rather than a reactor.\n\nFeather was Professor of Natural Philosophy at the University of Edinburgh from 1945 to 1975, then Emeritus Professor. He was active in nuclear physics research throughout his career, preferring small-scale, modest experiments, rather than the large experiments that became common after the war. Feather was noted for his active service to the University of Edinburgh and the city of Edinburgh.\n\nFeather was appointed a Fellow of the Royal Society (FRS) in 1945. From 1946 he was also a Fellow of the Royal Society of Edinburgh (his proposers including C. T. R. Wilson) and was President of that Society from 1967 to 1970. Feather won the Makdougall Brisbane Prize of the Royal Society of Edinburgh for 1968-70.\n\nFeather died on 14 August 1978 at Christie Hospital in Manchester.\n\n\n"}
{"id": "40622", "url": "https://en.wikipedia.org/wiki?curid=40622", "title": "Ohmmeter", "text": "Ohmmeter\n\nAn ohmmeter is an electrical instrument that measures electrical resistance, the opposition to an electric current. Micro-ohmmeters (microhmmeter or microohmmeter) make low resistance measurements. Megohmmeters (also a trademarked device Megger) measure large values of resistance. The unit of measurement for resistance is ohms (Ω).\n\nThe first ohmmeters were based on a type of meter movement known as a 'ratiometer'. These were similar to the galvanometer type movement encountered in later instruments, but instead of hairsprings to supply a restoring force they used conducting 'ligaments'. These provided no net rotational force to the movement. Also, the movement was wound with two coils. One was connected via a series resistor to the battery supply. The second was connected to the same battery supply via a second resistor and the resistor under test. The indication on the meter was proportional to the ratio of the currents through the two coils. This ratio was determined by the magnitude of the resistor under test. The advantages of this arrangement were twofold. First, the indication of the resistance was completely independent of the battery voltage (as long as it actually produced some voltage) and no zero adjustment was required. Second, although the resistance scale was non linear, the scale remained correct over the full deflection range. By interchanging the two coils a second range was provided. This scale was reversed compared to the first. A feature of this type of instrument was that it would continue to indicate a random resistance value once the test leads were disconnected (the action of which disconnected the battery from the movement). Ohmmeters of this type only ever measured resistance as they could not easily be incorporated into a multimeter design. Insulation testers that relied on a hand cranked generator operated on the same principle. This ensured that the indication was wholly independent of the voltage actually produced.\n\nSubsequent designs of ohmmeter provided a small battery to apply a voltage to a resistance via a galvanometer to measure the current through the resistance (battery, galvanometer and resistance all connected in series). The scale of the galvanometer was marked in ohms, because the fixed voltage from the battery assured that as resistance is increased, the current through the meter (and hence deflection) would decrease. Ohmmeters form circuits by themselves, therefore they cannot be used within an assembled circuit. This design is much simpler and cheaper than the former design, and was simple to integrate into a multimeter design and consequently was by far the most common form of analogue ohmmeter. This type of ohmmeter suffers from two inherent disadvantages. First, the meter needs to be zeroed by shorting the measurement points together and performing an adjustment for zero ohms indication prior to each measurement. This is because as the battery voltage decreases with age, the series resistance in the meter needs to be reduced to maintain the zero indication at full deflection. Second, and consequent on the first, the actual deflection for any given resistor under test changes as the internal resistance is altered. It remains correct at the centre of the scale only, which is why such ohmmeter designs always quote the accuracy \"at centre scale only\".\n\nA more accurate type of ohmmeter has an electronic circuit that passes a constant current (I) through the resistance, and another circuit that measures the voltage (V) across the resistance. These measurements are then digitized with an analog digital converter (adc) after which a microcontroller or microprocessor make the division of the current and voltage according to Ohm's Law and then decode these to a display to offer the user a reading of the resistance value they're measuring at that instant. Since these type of meters already measure current,voltage and resistance all at once, these type of circuits are often used in digital multimeters.\n\nFor high-precision measurements of very small resistances, the above types of meter are inadequate. This is partly because the change in deflection itself is small when the resistance measured is too small in proportion to the intrinsic resistance of the ohmmeter (which can be dealt with through current division), but mostly because the meter's reading is the sum of the resistance of the measuring leads, the contact resistances and the resistance being measured. To reduce this effect, a precision ohmmeter has four terminals, called Kelvin contacts. Two terminals carry the current from and to the meter, while the other two allow the meter to measure the voltage across the resistor. In this arrangement, the power source is connected in series with the resistance to be measured through the external pair of terminals, while the second pair connects in parallel with the galvanometer which measures the voltage drop. With this type of meter, any voltage drop due to the resistance of the first pair of leads and their contact resistances is ignored by the meter. This four terminal measurement technique is called Kelvin sensing, after William Thomson, Lord Kelvin, who invented the Kelvin bridge in 1861 to measure very low resistances. The Four-terminal sensing method can also be utilized to conduct accurate measurements of low resistances.\n\n\n"}
{"id": "21376719", "url": "https://en.wikipedia.org/wiki?curid=21376719", "title": "Oxygen plant", "text": "Oxygen plant\n\nOxygen plants are industrial systems designed to generate oxygen. They typically use air as a feedstock and separate it from other components of air using pressure swing adsorption or membrane separation techniques. Such plants are distinct from cryogenic separation plants which separate and capture all the components of air.\n\nOxygen finds broad application in various technological processes and in almost all industry branches. The primary oxygen application is associated with its capability of sustaining burning process, and the powerful oxidant properties.\n\nDue to that, oxygen has become widely used in the metal processing, welding, cutting and brazing processes. In the chemical and petrochemical industries, as well as in the oil and gas sector oxygen is used in commercial volumes as an oxidizer in chemical reactions.\n\nThe use of oxygen in gas-flame operations, such as metal welding, cutting and brazing is one of the most significant and common applications of this gas. Oxygen allows generating high-temperature flame in welding torches thus ensuring high quality and speed of work performance.\n\nOxygen is heavily used in the metal industry where it helps to increase burning temperature by the production of ferrous and non-ferrous metals and significantly improve the overall process efficiency. Another important oxygen application in the metal industry is connected with its use for carbon fixation with the yield of carbon dioxide.\n\nIn the chemical and petrochemical industries, oxygen is widely used for oxidation of raw chemicals for recovery of nitric acid, ethylene oxide, propylene oxide, vinyl chloride and other important chemical compounds.\n\nIn the oil and gas industry, oxygen finds application as a means for viscosity improvement and enhancement of oil-and-gas flow properties. Oxygen is also used for boosting production capacity of oil cracking plants, efficiency of high-octane components processing, as well as for the reduction of sulfuric deposits in refineries.\n\nThe use of oxygen in the fish farming helps increase the survival and fertility ratios and reduce the incubation period. Along with fish culture, oxygen is applied for shrimps, crabs and mussels rearing.\n\nIn glass furnaces oxygen is effectively used for burning temperature increase and burning processes improvement.\n\nThe use of oxygen in incinerators allows significantly increased flame temperatures and eventually ensures enhanced cost efficiency and incinerator production capacity.\n\nGas separation by adsorption systems is based on the fixation of various gas mixture components by a solid substance called adsorbent. Physically, this phenomenon is brought about by the forces of gas and adsorbent molecules interaction.\nThe current methods of gaseous oxygen production from air with the use of adsorption technology are nearly brought to perfection. The operation of a modern oxygen adsorption plant is based on the dependence of gas intake by adsorbent on the temperature and partial pressure of a gas component.\n\nThe gas adsorption and adsorbent regeneration processes may therefore be regulated by varying of the pressure and temperature parameters.\n\nThe oxygen plant flow process is arranged in such a way that highly absorbable gas mixture components are taken in by adsorbent, while low absorbable and non-absorbable components go through the plant. Today, there exist three methods of arranging the adsorption-based air separation process with the use of swing technologies: pressure (PSA), vacuum (VSA) and mixed (VPSA) ones. In the pressure swing adsorption flow processes, oxygen is recovered under above-atmospheric pressure and regeneration is achieved under atmospheric pressure. In vacuum swing adsorption flow processes, oxygen is recovered under atmospheric pressure, and regeneration is achieved under negative pressure. The mixed systems operation combines pressure variations from positive to negative.\n\nThe adsorption oxygen plants produce 5 to 5,000 Nm/h of oxygen with a purity of 93-95%. These systems, designated for indoor operation, are set to effectively produce gaseous oxygen from atmospheric air.\n\nAn unquestionable advantage of adsorption-based oxygen plants is the low cost of oxygen produced in the cases where there are no rigid requirements to the product oxygen purity.\n\nStructurally, the adsorption oxygen plant consists of several adsorbers, the compressor unit, pre-purifier unit, valve system and the plant control system.\n\nA simple adsorber is a column filled with layers of specially selected adsorbents – granular substances preferentially adsorbing highly adsorbable components of a gas mixture.\n\nWhere gaseous oxygen purity is required at the level of 90-95% with the capacity of up to 5,000 Nm per hour, adsorption oxygen plants are the optimal choice. This oxygen purity may also be obtained through the use of systems based on the cryogenic technology; however, cryogenic plants are more cumbersome and complex in operation.\n\nSome companies produce high-efficiency systems for oxygen production from atmospheric air with the help of membrane technology.\n\nThe basis of gas media separation with the use of membrane systems is the difference in velocity with which various gas mixture components permeate membrane substance. The driving force behind the gas separation process is the difference in partial pressures on different membrane sides.\n\nA modern gas separation membrane used by GRASYS is no longer a flat plate, but is formed by hollow fibers. Membrane consists of a porous polymer fiber with the gas separation layer applied to its external surface. Structurally, a hollow fiber membrane is configured as a cylindrical cartridge representing a spool with specifically reeled polymer fiber.\nDue to the membrane material high permeability for oxygen in contrast to nitrogen, the design of membrane oxygen complexes requires a special approach. Basically, there are two membrane-based oxygen production technologies: compressor and vacuum ones.\n\nIn the case of compressor technology, air is supplied into the fiber space under excess pressure, oxygen exits the membrane under slight excess pressure, and where necessary, is pressurized by booster compressor to the required pressure level. By the use of vacuum technology, a vacuum pump is used for the achievement of partial pressures difference.\n\nDesigned for indoor operation, membrane oxygen plants allow efficient air enrichment with oxygen up to the concentration of 30-45%. The complexes are rated to 5 to 5,000 nm3/hr of oxygenated air.\n\nIn the membrane oxygen plant, gas separation is achieved in the gas separation module composed of hollow-fiber membranes and representing the plant critical and high-technology unit. Apart from the gas separation unit, other important technical components are the booster compressor or vacuum pump, pre-purifier unit, and the plant control system.\n\nThe adoption of membrane systems for air enrichment purposes promises multiple oxygen savings where the oxygen concentration of 30-45% is sufficient to cover process needs. In addition to customer saving on the product oxygen cost, there is a collateral economic effect based on extremely low operating costs.\n\nWith the incorporation of the membrane technology, oxygen plants have outstanding technical characteristics. Membrane oxygen plants are highly reliable due to the absence of moving parts in the gas separation module.\n\nThe systems are very simple in operation – control of all operating parameters is carried out automatically. Because of the plants high automation degree, no constant manned attendance is required during its operation.\n\nMembrane oxygen plants are finding increasingly broad application in various industries all over the world. With moderate requirements to oxygen purity in product - up to 30-45%, membrane systems generally prove more economically sound than adsorption and cryogenic systems. Besides, membrane plants are much simpler in operation and more reliable.\n\n\n\n"}
{"id": "68351", "url": "https://en.wikipedia.org/wiki?curid=68351", "title": "Pattern", "text": "Pattern\n\nA pattern is a regularity in the world, man-made design, or abstract ideas. As such, the elements of a pattern repeat in a predictable manner. A geometric pattern is a kind of pattern formed of geometric shapes and typically repeated like a wallpaper design.\n\nAny of the senses may directly observe patterns. Conversely, abstract patterns in science, mathematics, or language may be observable only by analysis. Direct observation in practice means seeing visual patterns, which are widespread in nature and in art. Visual patterns in nature are often chaotic, never exactly repeating, and often involve fractals. Natural patterns include spirals, meanders, waves, foams, tilings, cracks, and those created by symmetries of rotation and reflection. Patterns have an underlying mathematical structure; indeed, mathematics can be seen as the search for regularities, and the output of any function is a mathematical pattern. Similarly in the sciences, theories explain and predict regularities in the world.\n\nIn art and architecture, decorations or visual motifs may be combined and repeated to form patterns designed to have a chosen effect on the viewer. In computer science, a software design pattern is a known solution to a class of problems in programming. In fashion, the pattern is a template used to create any number of similar garments.\n\nNature provides examples of many kinds of pattern, including symmetries, trees and other structures with a fractal dimension, spirals, meanders, waves, foams, tilings, cracks and stripes.\n\nSymmetry is widespread in living things. Animals that move usually have bilateral or mirror symmetry as this favours movement. Plants often have radial or rotational symmetry, as do many flowers, as well as animals which are largely static as adults, such as sea anemones. Fivefold symmetry is found in the echinoderms, including starfish, sea urchins, and sea lilies.\n\nAmong non-living things, snowflakes have striking sixfold symmetry: each flake is unique, its structure recording the varying conditions during its crystallisation similarly on each of its six arms. Crystals have a highly specific set of possible crystal symmetries; they can be cubic or octahedral, but cannot have fivefold symmetry (unlike quasicrystals).\n\nSpiral patterns are found in the body plans of animals including molluscs such as the nautilus, and in the phyllotaxis of many plants, both of leaves spiralling around stems, and in the multiple spirals found in flowerheads such as the sunflower and fruit structures like the pineapple.\nChaos theory predicts that while the laws of physics are deterministic, events and patterns in nature never exactly repeat because extremely small differences in starting conditions can lead to widely differing outcomes. Many natural patterns are shaped by this apparent randomness, including vortex streets and other effects of turbulent flow such as meanders in rivers.\nWaves are disturbances that carry energy as they move. Mechanical waves propagate through a medium – air or water, making it oscillate as they pass by. Wind waves are surface waves that create the chaotic patterns of the sea. As they pass over sand, such waves create patterns of ripples; similarly, as the wind passes over sand, it creates patterns of dunes.\n\nFoams obey Plateau's laws, which require films to be smooth and continuous, and to have a constant average curvature. Foam and bubble patterns occur widely in nature, for example in radiolarians, sponge spicules, and the skeletons of silicoflagellates and sea urchins.\nCracks form in materials to relieve stress: with 120 degree joints in elastic materials, but at 90 degrees in inelastic materials. Thus the pattern of cracks indicates whether the material is elastic or not. Cracking patterns are widespread in nature, for example in rocks, mud, tree bark and the glazes of old paintings and ceramics.\n\nAlan Turing, and later the mathematical biologist James D. Murray and other scientists, described a mechanism that spontaneously creates spotted or striped patterns, for example in the skin of mammals or the plumage of birds: a reaction-diffusion system involving two counter-acting chemical mechanisms, one that activates and one that inhibits a development, such as of dark pigment in the skin. These spatiotemporal patterns slowly drift, the animals' appearance changing imperceptibly as Turing predicted.\n\nIn visual art, pattern consists in regularity which in some way \"organizes surfaces or structures in a consistent, regular manner.\" At its simplest, a pattern in art may be a geometric or other repeating shape in a painting, drawing, tapestry, ceramic tiling or carpet, but a pattern need not necessarily repeat exactly as long as it provides some form or organizing \"skeleton\" in the artwork. In mathematics, a tessellation is the tiling of a plane using one or more geometric shapes (which mathematicians call tiles), with no overlaps and no gaps.\n\nIn architecture, motifs are repeated in various ways to form patterns. Most simply, structures such as windows can be repeated horizontally and vertically (see leading picture). Architects can use and repeat decorative and structural elements such as columns, pediments, and lintels. Repetitions need not be identical; for example, temples in South India have a roughly pyramidal form, where elements of the pattern repeat in a fractal-like way at different sizes.\n\nMathematics is sometimes called the \"Science of Pattern\", in the sense of rules that can be applied wherever needed. For example, any sequence of numbers that may be modeled by a mathematical function can be considered a pattern. Mathematics can be taught as a collection of patterns.\n\nSome mathematical rule-patterns can be visualised, and among these are those that explain patterns in nature including the mathematics of symmetry, waves, meanders, and fractals. Fractals are mathematical patterns that are scale invariant. This means that the shape of the pattern does not depend on how closely you look at it. Self-similarity is found in fractals. Examples of natural fractals are coast lines and tree shapes, which repeat their shape regardless of what magnification you view at. While self-similar patterns can appear indefinitely complex, the rules needed to describe or produce their formation can be simple (e.g. Lindenmayer systems describing tree shapes).\n\nIn pattern theory, devised by Ulf Grenander, mathematicians attempt to describe the world in terms of patterns. The goal is to lay out the world in a more computationally friendly manner.\n\nIn the broadest sense, any regularity that can be explained by a scientific theory is a pattern. As in mathematics, science can be taught as a set of patterns.\n\nIn computer science, a software design pattern, in the sense of a template, is a general solution to a problem in programming. A design pattern provides a reusable architectural outline that may speed the development of many computer programs.\n\nIn fashion, the pattern is a template, a technical two-dimensional tool used to create any number of identical garments. It can be considered as a means of translating from the drawing to the real garment.\n\n\n\n\n"}
{"id": "18589212", "url": "https://en.wikipedia.org/wiki?curid=18589212", "title": "Reverse osmosis", "text": "Reverse osmosis\n\nReverse osmosis (RO) is a water purification technology that uses a semipermeable membrane to remove ions, molecules and larger particles from drinking water. In reverse osmosis, an applied pressure is used to overcome osmotic pressure, a colligative property, that is driven by chemical potential differences of the solvent, a thermodynamic parameter. Reverse osmosis can remove many types of dissolved and suspended species from water, including bacteria, and is used in both industrial processes and the production of potable water. The result is that the solute is retained on the pressurized side of the membrane and the pure solvent is allowed to pass to the other side. To be \"selective\", this membrane should not allow large molecules or ions through the pores (holes), but should allow smaller components of the solution (such as solvent molecules) to pass freely.\n\nIn the normal osmosis process, the solvent naturally moves from an area of low solute concentration (high water potential), through a membrane, to an area of high solute concentration (low water potential). The driving force for the movement of the solvent is the reduction in the free energy of the system when the difference in solvent concentration on either side of a membrane is reduced, generating osmotic pressure due to the solvent moving into the more concentrated solution. Applying an external pressure to reverse the natural flow of pure solvent, thus, is reverse osmosis. The process is similar to other membrane technology applications. However, key differences are found between reverse osmosis and filtration. The predominant removal mechanism in membrane filtration is straining, or size exclusion, so the process can theoretically achieve perfect efficiency regardless of parameters such as the solution's pressure and concentration. Reverse osmosis also involves diffusion, making the process dependent on pressure, flow rate, and other conditions. Reverse osmosis is most commonly known for its use in drinking water purification from seawater, removing the salt and other effluent materials from the water molecules.\n\nA process of osmosis through semipermeable membranes was first observed in 1748 by Jean-Antoine Nollet. For the following 200 years, osmosis was only a phenomenon observed in the laboratory. In 1950, the University of California at Los Angeles first investigated desalination of seawater using semipermeable membranes. Researchers from both University of California at Los Angeles and the University of Florida successfully produced fresh water from seawater in the mid-1950s, but the flux was too low to be commercially viable until the discovery at University of California at Los Angeles by Sidney Loeb and Srinivasa Sourirajan at the National Research Council of Canada, Ottawa, of techniques for making asymmetric membranes characterized by an effectively thin \"skin\" layer supported atop a highly porous and much thicker substrate region of the membrane. John Cadotte, of FilmTec Corporation, discovered that membranes with particularly high flux and low salt passage could be made by interfacial polymerization of \"m\"-phenylene diamine and trimesoyl chloride. Cadotte's patent on this process was the subject of litigation and has since expired. Almost all commercial reverse osmosis membrane is now made by this method. By the end of 2001, about 15,200 desalination plants were in operation or in the planning stages, worldwide.\n\nIn 1977 Cape Coral, Florida became the first municipality in the United States to use the RO process on a large scale with an initial operating capacity of 11.35 million liters (3 million US gal) per day. By 1985, due to the rapid growth in population of Cape Coral, the city had the largest low pressure reverse osmosis plant in the world, capable of producing 56.8 million liters (15 million US gal) per day (MGD).\n\nand reverse osmosis is used in water purification\nFormally, reverse osmosis is the process of forcing a solvent from a region of high solute concentration through a semipermeable membrane to a region of low solute concentration by applying a pressure in excess of the osmotic pressure. The largest and most important application of reverse osmosis is the separation of pure water from seawater and brackish waters; seawater or brackish water is pressurized against one surface of the membrane, causing transport of salt-depleted water across the membrane and emergence of potable drinking water from the low-pressure side.\n\nThe membranes used for reverse osmosis have a dense layer in the polymer matrix—either the skin of an asymmetric membrane or an interfacially polymerized layer within a thin-film-composite membrane—where the separation occurs. In most cases, the membrane is designed to allow only water to pass through this dense layer while preventing the passage of solutes (such as salt ions). This process requires that a high pressure be exerted on the high concentration side of the membrane, usually 2–17 bar (30–250 psi) for fresh and brackish water, and 40–82 bar (600–1200 psi) for seawater, which has around 27 bar (390 psi) natural osmotic pressure that must be overcome. This process is best known for its use in desalination (removing the salt and other minerals from sea water to produce fresh water), but since the early 1970s, it has also been used to purify fresh water for medical, industrial, and domestic applications.\n\nAround the world, household drinking water purification systems, including a reverse osmosis step, are commonly used for improving water for drinking and cooking.\n\nSuch systems typically include a number of steps:\nThe latest developments in the sphere include nano materials and membranes.\n\nIn some systems, the carbon prefilter is omitted, and a cellulose triacetate membrane is used. CTA (cellulose triacetate) is a paper by-product membrane bonded to a synthetic layer and is made to allow contact with chlorine in the water. These require a small amount of chlorine in the water source to prevent bacteria from forming on it. The typical rejection rate for CTA membranes is 85–95%.\n\nThe cellulose triacetate membrane is prone to rotting unless protected by chlorinated water, while the thin film composite membrane is prone to breaking down under the influence of chlorine. A thin film composite (TFC) membrane is made of synthetic material, and requires chlorine to be removed before the water enters the membrane. To protect the TFC membrane elements from chlorine damage, carbon filters are used as pre-treatment in all residential reverse osmosis systems. TFC membranes have a higher rejection rate of 95–98% and a longer life than CTA membranes.\n\nPortable reverse osmosis water processors are sold for personal water purification in various locations. To work effectively, the water feeding to these units should be under some pressure (280 kPa (40 psi) or greater is the norm). Portable reverse osmosis water processors can be used by people who live in rural areas without clean water, far away from the city's water pipes. Rural people filter river or ocean water themselves, as the device is easy to use (saline water may need special membranes). Some travelers on long boating, fishing, or island camping trips, or in countries where the local water supply is polluted or substandard, use reverse osmosis water processors coupled with one or more ultraviolet sterilizers.\n\nIn the production of bottled mineral water, the water passes through a reverse osmosis water processor to remove pollutants and microorganisms. In European countries, though, such processing of natural mineral water (as defined by a European directive) is not allowed under European law. In practice, a fraction of the living bacteria can and do pass through reverse osmosis membranes through minor imperfections, or bypass the membrane entirely through tiny leaks in surrounding seals. Thus, complete reverse osmosis systems may include additional water treatment stages that use ultraviolet light or ozone to prevent microbiological contamination.\n\nMembrane pore sizes can vary from 0.1 to 5,000 nm depending on filter type. Particle filtration removes particles of 1 µm or larger. Microfiltration removes particles of 50 nm or larger. Ultrafiltration removes particles of roughly 3 nm or larger. Nanofiltration removes particles of 1 nm or larger. Reverse osmosis is in the final category of membrane filtration, hyperfiltration, and removes particles larger than 0.1 nm.\n\nA reverse osmosis water purification unit (ROWPU) is a portable, self-contained water treatment plant. Designed for military use, it can provide potable water from nearly any water source. There are many models in use by the United States armed forces and the Canadian Forces. Some models are containerized, some are trailers, and some are vehicles unto themselves.\n\nEach branch of the United States armed forces has their own series of reverse osmosis water purification unit models, but they are all similar. The water is pumped from its raw source into the reverse osmosis water purification unit module, where it is treated with a polymer to initiate coagulation. Next, it is run through a multi-media filter where it undergoes primary treatment by removing turbidity. It is then pumped through a cartridge filter which is usually spiral-wound cotton. This process clarifies the water of any particles larger than 5 µm and eliminates almost all turbidity.\n\nThe clarified water is then fed through a high-pressure piston pump into a series of vessels where it is subject to reverse osmosis. The product water is free of 90.00–99.98% of the raw water's total dissolved solids and by military standards, should have no more than 1000–1500 parts per million by measure of electrical conductivity. It is then disinfected with chlorine and stored for later use.\n\nWithin the United States Marine Corps, the reverse osmosis water purification unit has been replaced by both the Lightweight Water Purification System and Tactical Water Purification Systems. The Lightweight Water Purification Systems can be transported by Humvee and filter per hour. The Tactical Water Purification Systems can be carried on a Medium Tactical Vehicle Replacement truck, and can filter per hour.\n\nRain water collected from storm drains is purified with reverse osmosis water processors and used for landscape irrigation and industrial cooling in Los Angeles and other cities, as a solution to the problem of water shortages.\n\nIn industry, reverse osmosis removes minerals from boiler water at power plants. The water is distilled multiple times. It must be as pure as possible so it does not leave deposits on the machinery or cause corrosion. The deposits inside or outside the boiler tubes may result in underperformance of the boiler, bringing down its efficiency and resulting in poor steam production, hence poor power production at the turbine.\n\nIt is also used to clean effluent and brackish groundwater. The effluent in larger volumes (more than 500 m/day) should be treated in an effluent treatment plant first, and then the clear effluent is subjected to reverse osmosis system. Treatment cost is reduced significantly and membrane life of the reverse osmosis system is increased.\n\nThe process of reverse osmosis can be used for the production of deionized water.\n\nReverse osmosis process for water purification does not require thermal energy. Flow-through reverse osmosis systems can be regulated by high-pressure pumps. The recovery of purified water depends upon various factors, including membrane sizes, membrane pore size, temperature, operating pressure, and membrane surface area.\n\nIn 2002, Singapore announced that a process named NEWater would be a significant part of its future water plans. It involves using reverse osmosis to treat domestic wastewater before discharging the NEWater back into the reservoirs.\n\nIn addition to desalination, reverse osmosis is a more economical operation for concentrating food liquids (such as fruit juices) than conventional heat-treatment processes. Research has been done on concentration of orange juice and tomato juice. Its advantages include a lower operating cost and the ability to avoid heat-treatment processes, which makes it suitable for heat-sensitive substances such as the protein and enzymes found in most food products.\n\nReverse osmosis is extensively used in the dairy industry for the production of whey protein powders and for the concentration of milk to reduce shipping costs. In whey applications, the whey (liquid remaining after cheese manufacture) is concentrated with reverse osmosis from 6% total solids to 10–20% total solids before ultrafiltration processing. The ultrafiltration retentate can then be used to make various whey powders, including whey protein isolate. Additionally, the ultrafiltration permeate, which contains lactose, is concentrated by reverse osmosis from 5% total solids to 18–22% total solids to reduce crystallization and drying costs of the lactose powder.\n\nAlthough use of the process was once avoided in the wine industry, it is now widely understood and used. An estimated 60 reverse osmosis machines were in use in Bordeaux, France, in 2002. Known users include many of the elite classed growths (Kramer) such as Château Léoville-Las Cases in Bordeaux.\n\nIn 1946, some maple syrup producers started using reverse osmosis to remove water from sap before the sap is boiled down to syrup. The use of reverse osmosis allows about 75–90% of the water to be removed from the sap, reducing energy consumption and exposure of the syrup to high temperatures. Microbial contamination and degradation of the membranes must be monitored.\n\nFor small-scale hydrogen production, reverse osmosis is sometimes used to prevent formation of minerals on the surface of electrodes.\n\nMany reef aquarium keepers use reverse osmosis systems for their artificial mixture of seawater. Ordinary tap water can contain excessive chlorine, chloramines, copper, nitrates, nitrites, phosphates, silicates, or many other chemicals detrimental to the sensitive organisms in a reef environment. Contaminants such as nitrogen compounds and phosphates can lead to excessive and unwanted algae growth. An effective combination of both reverse osmosis and deionization is the most popular among reef aquarium keepers, and is preferred above other water purification processes due to the low cost of ownership and minimal operating costs. Where chlorine and chloramines are found in the water, carbon filtration is needed before the membrane, as the common residential membrane used by reef keepers does not cope with these compounds.\n\nAn increasingly popular method of cleaning windows is the so-called \"water-fed pole\" system. Instead of washing the windows with detergent in the conventional way, they are scrubbed with highly purified water, typically containing less than 10 ppm dissolved solids, using a brush on the end of a long pole which is wielded from ground level. Reverse osmosis is commonly used to purify the water.\n\nTreatment with reverse osmosis is limited, resulting in low recoveries on high concentration (measured with electrical conductivity) and fouling of the RO membranes. Reverse osmosis applicability is limited by conductivity, organics, and scaling inorganic elements such as CaSO4, Si, Fe and Ba. Low organic scaling can use two different technologies, one is using spiral wound membrane type of module, and for high organic scaling, high conductivity and higher pressure (up to 90 bars) disc tube modules with reverse osmosis membranes can be used. Disc tube modules were redesigned for landfill leachate purification, that is usually contaminated with high levels of organic material. Due to the cross-flow with high velocity it is given a flow booster pump, that is recirculating the flow over the same membrane surface between 1,5 and 3 times before it is released as a concentrate. High velocity is also good against membrane scaling and allows successful membrane cleaning.\n\nAreas that have either no or limited surface water or groundwater may choose to desalinate. Reverse osmosis is an increasingly common method of desalination, because of its relatively low energy consumption.\n\nIn recent years, energy consumption has dropped to around 3 kWh/m, with the development of more efficient energy recovery devices and improved membrane materials. According to the International Desalination Association, for 2011, reverse osmosis was used in 66% of installed desalination capacity (0.0445 of 0.0674 km³/day), and nearly all new plants. Other plants mainly use thermal distillation methods: multiple-effect distillation and multi-stage flash.\n\nSea water reverse osmosis (SWRO) desalination, a membrane process, has been commercially used since the early 1970s. Its first practical use was demonstrated by Sidney Loeb from University of California at Los Angeles in Coalinga, California, and Srinivasa Sourirajan of National Research council, Canada. Because no heating or phase changes are needed, energy requirements are low, around 3 kWh/m, in comparison to other processes of desalination, but are still much higher than those required for other forms of water supply, including reverse osmosis treatment of wastewater, at 0.1 to 1 kWh/m. Up to 50% of the seawater input can be recovered as fresh water, though lower recoveries may reduce membrane fouling and energy consumption.\n\nBrackish water reverse osmosis refers to desalination of water with a lower salt content than sea water, usually from river estuaries or saline wells. The process is substantially the same as sea water reverse osmosis, but requires lower pressures and therefore less energy. Up to 80% of the feed water input can be recovered as fresh water, depending on feed salinity.\n\nThe Ashkelon sea water reverse osmosis desalination plant in Israel is the largest in the world. The project was developed as a build-operate-transfer by a consortium of three international companies: Veolia water, IDE Technologies, and Elran.\n\nThe typical single-pass sea water reverse osmosis system consists of:\n\nPretreatment is important when working with reverse osmosis and nanofiltration membranes due to the nature of their spiral-wound design. The material is engineered in such a fashion as to allow only one-way flow through the system. As such, the spiral-wound design does not allow for backpulsing with water or air agitation to scour its surface and remove solids. Since accumulated material cannot be removed from the membrane surface systems, they are highly susceptible to fouling (loss of production capacity). Therefore, pretreatment is a necessity for any reverse osmosis or nanofiltration system. Pretreatment in sea water reverse osmosis systems has four major components:\n\nThe high pressure pump supplies the pressure needed to push water through the membrane, even as the membrane rejects the passage of salt through it. Typical pressures for brackish water range from 1.6 to 2.6 MPa (225 to 376 psi). In the case of seawater, they range from 5.5 to 8 MPa (800 to 1,180 psi). This requires a large amount of energy. Where energy recovery is used, part of the high pressure pump's work is done by the energy recovery device, reducing the system energy inputs.\n\nThe membrane assembly consists of a pressure vessel with a membrane that allows feedwater to be pressed against it. The membrane must be strong enough to withstand whatever pressure is applied against it. Reverse osmosis membranes are made in a variety of configurations, with the two most common configurations being spiral-wound and hollow-fiber.\n\nOnly a part of the saline feed water pumped into the membrane assembly passes through the membrane with the salt removed. The remaining \"concentrate\" flow passes along the saline side of the membrane to flush away the concentrated salt solution. The percentage of desalinated water produced versus the saline water feed flow is known as the \"recovery ratio\". This varies with the salinity of the feed water and the system design parameters: typically 20% for small seawater systems, 40% – 50% for larger seawater systems, and 80% – 85% for brackish water. The concentrate flow is at typically only 3 bar / 50 psi less than the feed pressure, and thus still carries much of the high pressure pump input energy.\n\nThe desalinated water purity is a function of the feed water salinity, membrane selection and recovery ratio. To achieve higher purity a second pass can be added which generally requires re-pumping. Purity expressed as total dissolved solids typically varies from 100 to 400 parts per million (ppm or mg/litre)on a seawater feed. A level of 500 ppm is generally accepted as the upper limit for drinking water, while the US Food and Drug Administration classifies mineral water as water containing at least 250 ppm.\n\nEnergy recovery can reduce energy consumption by 50% or more. Much of the high pressure pump input energy can be recovered from the concentrate flow, and the increasing efficiency of energy recovery devices has greatly reduced the energy needs of reverse osmosis desalination. Devices used, in order of invention, are:\n\n\nThe desalinated water is \"stabilized\" to protect downstream pipelines and storage, usually by adding lime or caustic soda to prevent corrosion of concrete-lined surfaces. Liming material is used to adjust pH between 6.8 and 8.1 to meet the potable water specifications, primarily for effective disinfection and for corrosion control. Remineralisation may be needed to replace minerals removed from the water by desalination. Although this process has proved to be costly and not very convenient if it is intended to meet mineral demand by humans and plants. The very same mineral demand that freshwater sources provided previously. For instance water from Israel's national water carrier typically contains dissolved magnesium levels of 20 to 25 mg/liter, while water from the Ashkelon plant has no magnesium. After farmers used this water, magnesium deficiency symptoms appeared in crops, including tomatoes, basil, and flowers, and had to be remedied by fertilization. Current Israeli drinking water standards set a minimum calcium level of 20 mg/liter. The postdesalination treatment in the Ashkelon plant uses sulfuric acid to dissolve calcite (limestone), resulting in calcium\nconcentration of 40 to 46 mg/liter. This is still lower than the 45 to 60 mg/liter found in typical Israeli freshwaters.\n\nPost-treatment consists of preparing the water for distribution after filtration. Reverse osmosis is an effective barrier to pathogens, but post-treatment provides secondary protection against compromised membranes and downstream problems. Disinfection by means of ultra violet (UV) lamps (sometimes called germicidal or bactericidal) may be employed to sterilize pathogens which bypassed the reverse osmosis process. Chlorination or chloramination (chlorine and ammonia) protects against pathogens which may have lodged in the distribution system downstream, such as from new construction, backwash, compromised pipes, etc.\n\nHousehold reverse osmosis units use a lot of water because they have low back pressure. As a result, they recover only 5 to 15% of the water entering the system. The remainder is discharged as waste water. Because waste water carries with it the rejected contaminants, methods to recover this water are not practical for household systems. Wastewater is typically connected to the house drains and will add to the load on the household septic system. A reverse osmosis unit delivering 19 L of treated water per day may discharge between 75–340 L of waste water per day. This is having disastrous consequence for mega cities like Delhi where large-scale use of household R.O. devices has increased the total water demand of the already water parched National Capital Territory of India.\n\nLarge-scale industrial/municipal systems recover typically 75% to 80% of the feed water, or as high as 90%, because they can generate the high pressure needed for higher recovery reverse osmosis filtration. On the other hand, as recovery of wastewater increases in commercial operations, effective contaminant removal rates tend to become reduced, as evidenced by product water total dissolved solids levels.\n\nDue to its fine membrane construction, reverse osmosis not only removes harmful contaminants present in the water, but it also may remove many of the desirable minerals from the water. A number of peer-reviewed studies have looked at the long-term health effects of drinking demineralized water.\n\nDepending upon the desired product, either the solvent or solute stream of reverse osmosis will be waste. For food concentration applications, the concentrated solute stream is the product and the solvent stream is waste. For water treatment applications, the solvent stream is purified water and the solute stream is concentrated waste. The solvent waste stream from food processing may be used as reclaimed water, but there may be fewer options for disposal of a concentrated waste solute stream. Ships may use marine dumping and coastal desalination plants typically use marine outfalls. Landlocked reverse osmosis plants may require evaporation ponds or injection wells to avoid polluting groundwater or surface runoff.\n\nSince the 1970s, prefiltration of high-fouling waters with another larger-pore membrane, with less hydraulic energy requirement, has been evaluated and sometimes used. However, this means that the water passes through two membranes and is often repressurized, which requires more energy to be put into the system, and thus increases the cost.\n\nOther recent developmental work has focused on integrating reverse osmosis with electrodialysis to improve recovery of valuable deionized products, or to minimize the volume of concentrate requiring discharge or disposal.\n\nIn the production of drinking water, the latest developments include nanoscale and graphene membranes.\n\nThe world's largest RO desalination plant was built in Sorek, Israel, in 2013. It has an output of 624,000 m a day. It is also the cheapest and will sell water to the authorities for US$0.58/m.\n"}
{"id": "30862811", "url": "https://en.wikipedia.org/wiki?curid=30862811", "title": "Rotating biological contactor", "text": "Rotating biological contactor\n\nA rotating biological contactor or RBC is a biological treatment process used in the treatment of wastewater following primary treatment. The primary treatment process means protection by removal of grit and sand and coarse material through a screening process, followed by a removal process of sediment by settling. The RBC process involves allowing the wastewater to come in contact with a biological medium in order to remove pollutants in the wastewater before discharge of the treated wastewater to the environment, usually a body of water (river, lake or ocean). A rotating biological contactor is a type of secondary (Biological) treatment process. It consists of a series of closely spaced, parallel discs mounted on a rotating shaft which is supported just above the surface of the waste water. Microorganisms grow on the surface of the discs where biological degradation of the wastewater pollutants takes place.\n\nThe rotating packs of disks (known as the media) are contained in a tank or trough and rotate at between 2 and 5 revolutions per minute. Commonly used plastics for the media are polyethylene, PVC and expanded polystyrene. The shaft is aligned with the flow of wastewater so that the discs rotate at right angles to the flow, with several packs usually combined to make up a treatment train. About 40% of the disc area is immersed in the wastewater.\n\nBiological growth is attached to the surface of the disc and forms a slime layer. The discs contact the wastewater with the atmospheric air for oxidation as it rotates. The rotation helps to slough off excess solids. The disc system can be staged in series to obtain nearly any detention time or degree of removal required. Since the systems are staged, the culture of the later stages can be acclimated to the slowly degraded materials.\n\nThe discs consist of plastic sheets ranging from 2 to 4 m in diameter and are up to 10 mm thick. Several modules may be arranged in parallel and/or in series to meet the flow and treatment requirements. The discs are submerged in waste water to about 40% of their diameter. Approximately 95% of the surface area is thus alternately submerged in waste water and then exposed to the atmosphere above the liquid. Carbonaceous substrate is removed in the initial stage of RBC. Carbon conversion may be completed in the first stage of a series of modules, with nitrification being completed after the 5th stage. Most design of RBC systems will include a minimum of 4 or 5 modules in series to obtain nitrification of waste water.\n\nBiofilms, which are biological growths that become attached to the discs, assimilate the organic materials in the wastewater. Aeration is provided by the rotating action, which exposes the media to the air after contacting them with the wastewater, facilitating the degradation of the pollutants being removed. The degree of wastewater treatment is related to the amount of media surface area and the quality and volume of the inflowing wastewater.\n\nThe first RBC was installed in West Germany in 1959, later it was introduced in the United States and Canada. In the United States, rotating biological contactors are used for industries producing wastewaters high in biochemical oxygen demand (BOD) (e.g., petroleum industry and dairy industry).\n\nA properly designed RBC produced a very high quality final effluent. However both the organic and hydraulic loading had to be addressed. \n\nIn the 1980's problems were encountered in the USA prompting the Environmental Agency to commission a number of reports.\n\nThese reports identified a number of issues and criticized the RBC process. One author suggested that since manufacturers were aware of the problem, the problems would be resolved and suggested that design engineers should specify a long life.\n\nHowever, this was only possible when the manufacturers became aware of the design problems and the stress to ensure a long life and since failures still occurred it was unlikely any design stresses were known.\n\nSevern Trent Water Ltd, a large UK Water Company based in the Midlands, employed RBCs as the preferred process for their small works which amount to over 700 sites Consequently, long life was essential to compliance.\n\nThis issue was successfully addressed by Eric Findlay C Eng when he was employed by Severn Trent Water Ltd in the UK following a period of failure of a number of plants. As a result, the issue of short life failure became fully understood in the early 1990s when the correct process and hydraulic issues had been identified to produce a high quality nitrified effluent. \nThere are several other papers which address the whole issue of RBCs.\nFindlay also developed a system for repairing defective RBCs enabling shaft and frame life to be extended up to 30 years based on the Cranfield designed frame. Where additional capacity was required intermediate frames are used maximising minimising the need for duplication\n\nSecondary clarifiers following RBCs are identical in design to conventional humus tanks, as used downstream of trickling filters. Sludge is generally removed daily, or pumped automatically to the primary settlement tank for co-settlement. Regular sludge removal reduces the risk of anaerobic conditions from developing within the sludge, with subsequent sludge flotation due to the release of gases.\n\n\n"}
{"id": "176571", "url": "https://en.wikipedia.org/wiki?curid=176571", "title": "Small appliance", "text": "Small appliance\n\nA small appliance, small domestic appliance, or small electrics are portable or semi-portable machines, generally used on table-tops, counter-tops, or other platforms, to accomplish a household task. Examples include microwave ovens, toasters, humidifiers, and coffeemakers. They contrast with major appliances (British \"white goods\"), such as the refrigerator and washing machine, which cannot be easily moved and are generally placed on the floor. Small appliances also contrast with consumer electronics (British \"brown goods\") which are for leisure and entertainment rather than purely practical tasks.\n\nSome small appliances perform the same or similar function as their larger counterparts. For example, a toaster oven is a small appliance that performs a similar function as an oven. Small appliances often have a home version and a commercial version, for example waffle irons, food processors, and blenders. The commercial, or industrial, version is designed to be used nearly continuously in a restaurant or other similar setting. Commercial appliances are typically connected to a more powerful electrical outlet, are larger and stronger, have more user-serviceable parts, and cost significantly more.\n\n\nSmall appliances can be very inexpensive, such as an electric can opener, hot pot, toaster, or coffee maker which may cost only a few U.S. dollars, or very expensive, such as an elaborate espresso maker, which may cost several thousand U.S. dollars. Most homes in developed economies contain several cheaper home appliances, with perhaps a few more expensive appliances, such as a high-end microwave oven or mixer.\n\nMany small appliances are powered by electricity. The appliance may use a permanently attached cord which is plugged into a wall outlet or a detachable cord. The appliance may have a cord storage feature. A few hand-held appliances use batteries, which may be disposable or rechargeable. Some appliances consist of an electrical motor upon which is mounted various attachments so as to constitute several individual appliances, such as a blender, a food processor, or a juicer. Many stand mixers, while functioning primarily as a mixer, have attachments which can perform additional functions.\n\nA few gasoline and gas-powered appliances exist for use in situations where electricity is not expected to be available, but these are typically larger and not as portable as most small appliances. Items that perform the same function as small appliances but are hand powered are generally referred to as tools or gadgets, for example a hand cranked egg beater, a grater, a mandoline, or a hand-powered meat grinder.\n\nSmall appliances which are defective or improperly used or maintained may cause house fires and other property damage, or may harbor bacteria if not properly cleaned. It is important that users read the instructions carefully and that appliances that use a grounded cord be attached to a grounded outlet. Because of the risk of fire, some appliances have a short detachable cord that is connected to the appliance magnetically. If the appliance is moved further than the cord length from the wall, the cord will detach from the appliance.\n\nDesignations and regulations of \"small appliances\" vary by country and are not simply determined by physical sizes. For instance, United States Environmental Protection Agency regulations mandate that small appliances must meet two standards:\n\n"}
{"id": "417603", "url": "https://en.wikipedia.org/wiki?curid=417603", "title": "Spinet desk", "text": "Spinet desk\n\nA spinet desk is an antique desk with an exterior shape similar to a writing table, But slightly higher and is fitted with a single drawer under the whole length of the flat top surface. The spinet desk is so named because when closed it resembles a spinet, a musical instrument of the harpsichord family.\n\nThis single drawer, however, is a dummy. It is a hinged panel which is meant to be folded in, at the same time as half of the hinged top surface is folded back on to the top of the other half, revealing an inner desktop surface of normal height, with small drawers and pigeonholes in the back. In certain spinet desks the inner desktop surface can be drawn out a few inches, adding working space. \n\nThe image of the front of the spinet desk shows it in a closed position while the image of the side shows it in a partly open position, just before the hinged mobile part of the top is placed on the fixed part of the top.\n\nBy this capacity of hiding or revealing the main working area the spinet desk could be said to be a smaller, less obtrusive cousin of the rolltop desk and the cylinder desk. Like them, and unlike the secretary desk or the fall front desk, it can be closed up without disturbing too much the paperwork and various documents and implements left on the main desktop surface.\n\n\n"}
{"id": "27593472", "url": "https://en.wikipedia.org/wiki?curid=27593472", "title": "Thermalite", "text": "Thermalite\n\nThermalite a specific type of fuse used in pyrotechnic applications. The product was designed to be used in cross matching safety fuses of the Bickford type. As safety fuse is designed to neither give nor take fire through the heavy fuse jacket, ignition may be achieved by punching a hole perpendicular to and through a safety fuse powder core, threading a piece of Thermalite or similar igniter cord through the hole, then gently squeezing the safety fuse with pliers or similar to bring the powder core into contact with the igniter cord. The Thermalite could be ignited by a match, or more certainly by a purpose made igniter, similar to a wire sparkler.\n\nAn expedient formerly used to ignite bickford style safety fuses was to split the end of a safety fuse, place a match head into the split and tie the split back together, holding the match head against the powder core. This technique was slower, cumbersome and more failure prone than piercing and cross matching. Also, a single length of igniter cord could pass through and serially ignite multiple pieces of safety fuse attached to various charges in more complex, multiple charge blasting schemes. This technique and the several burn rate types of igniter cord manufactured by ICI could be used to implement quite complex ignition sequences.\n\nThis fuse is used in high-power model rocketry as a means of simultaneously igniting multiple \"clustered\" rocket motors. A single flashbulb or flash pan is used to ignite pieces of Thermalite leading to each motor.\n\nThermalite comes in three burn rates identifiable by the colour of the fuse wrapping: \n\nThermalite igniter cords and connectors were manufactured in Quebec by ICA Canada Inc., however in November 1995 they ceased manufacturing igniter cord. Sources for thermalite are increasingly hard to come by and purchasing it by mail will usually require permits and licenses. As a result, those who want to use thermalite fuses will sometimes make their own.\n\n"}
{"id": "46193", "url": "https://en.wikipedia.org/wiki?curid=46193", "title": "Threshing machine", "text": "Threshing machine\n\nA threshing machine or thresher is a piece of farm equipment that threshes grain, that is, it removes the seeds from the stalks and husks. It does so by beating the plant to make the seeds fall out. \n\nBefore such machines were developed, threshing was done by hand with flails: such hand threshing was very laborious and time-consuming, taking about one-quarter of agricultural labour by the 18th century. Mechanization of this process removed a substantial amount of drudgery from farm labour. The first threshing machine was invented circa 1786 by the Scottish engineer Andrew Meikle, and the subsequent adoption of such machines was one of the earlier examples of the mechanization of agriculture. During the 19th century, threshers and mechanical reapers and reaper-binders gradually became widespread and made grain production much less laborious.\n\nSeparate reaper-binders and threshers have largely been replaced by machines that combine all of their functions, that is combine harvesters or combines. However, the simpler machines remain important as appropriate technology in low-capital farming contexts, both in developing countries and in developed countries on small farms that strive for especially high levels of self-sufficiency. For example, pedal-powered threshers are a low-cost option, and some Amish sects use horse-drawn binders and old-style threshers.\n\nAs the verb \"thresh\" is cognate with the verb \"thrash\" (and synonymous in the grain-beating sense), the names thrashing machine and thrasher are (less common) alternate forms.\n\nThe Swing Riots in the UK were partly a result of the threshing machine. Following years of war, high taxes and low wages, farm labourers finally revolted in 1830. These farm labourers had faced unemployment for a number of years due to the widespread introduction of the threshing machine and the policy of enclosing fields. No longer were thousands of men needed to tend the crops, a few would suffice. With fewer jobs, lower wages and no prospects of things improving for these workers the threshing machine was the final straw, the machine was to place them on the brink of starvation. The Swing Rioters smashed threshing machines and threatened farmers who had them.\n\nThe riots were dealt with very harshly. Nine of the rioters were hanged and a further 450 were transported to Australia.\n\nEarly threshing machines were hand-fed and horse-powered. They were small by today's standards and were about the size of an upright piano. Later machines were steam-powered, driven by a portable engine or traction engine. Isaiah Jennings, a skilled inventor, created a small thresher that doesn't harm the straw in the process. In 1834, John Avery and Hiram Abial Pitts devised significant improvements to a machine that automatically threshes and separates grain from chaff, freeing farmers from a slow and laborious process. Avery and Pitts were granted United States patent #542 on December 29, 1837.\n\nJohn Ridley, an Australian inventor, also developed a threshing machine in South Australia in 1843.\n\nThe 1881 \"Household Cyclopedia\" said of Meikle's machine:\n\nSteam-powered machines used belts connected to a traction engine; often both engine and thresher belonged to a contractor who toured the farms of a district. Steam remained a viable commercial option until the early post-WWII years.\n\nThreshing is just one step of the process in getting cereals to the grinding mill and customer.\nThe wheat needs to be grown, cut, stooked (shocked, bundled), hauled, threshed, de-chaffed, straw baled, and then the grain hauled to a grain elevator. For many years each of these steps was an individual process, requiring teams of workers and many machines. In the steep hill wheat country of Palouse in the Northwest of the United States, steep ground meant moving machinery around was problematic and prone to rolling. To reduce the amount of work on the sidehills, the idea arose of combining the wheat binder and thresher into one machine, known as a combine harvester. About 1910, horse pulled combines appeared and became a success. Later, gas and diesel engines appeared with other refinements and specifications.\n\nModern day combine harvesters (or simply combines) operate on the same principles and use the same components as the original threshing machines built in the 19th century. Combines also perform the reaping operation at the same time. The name \"combine\" is derived from the fact that the two steps are combined in a single machine. Also, most modern combines are self-powered (usually by a diesel engine) and self-propelled, although tractor powered, pull type combines models were offered by John Deere and Case International into the 1990s.\n\nToday, as in the 19th century, the threshing begins with a cylinder and concave. The cylinder has sharp serrated bars, and rotates at high speed (about 500 RPM), so that the bars beat against the grain. The concave is curved to match the curve of the cylinder, and serves to hold the grain as it is beaten. The beating releases the grain from the straw and chaff.\n\nWhilst the majority of the grain falls through the concave, the straw is carried by a set of \"walkers\" to the rear of the machine, allowing any grain and chaff still in the straw to fall below. Below the straw walkers, a fan blows a stream of air across the grain, removing dust and fines and blowing them away.\n\nThe grain, either coming through the concave or the walkers, meets a set of sieves mounted on an assembly called a shoe, which is shaken mechanically. The top sieve has larger openings, and serves to remove large pieces of chaff from the grain. The lower sieve separates clean grain, which falls through, from incompletely threshed pieces. The incompletely threshed grain is returned to the cylinder by means of a system of conveyors, where the process repeats.\n\nSome threshing machines were equipped with a bagger, which invariably held two bags, one being filled, and the other being replaced with an empty. A worker called a \"sewer\" removed and replaced the bags, and sewed full bags shut with a needle and thread. Other threshing machines would discharge grain from a conveyor, for bagging by hand. Combines are equipped with a grain tank, which accumulates grain for deposit in a truck or wagon.\n\nA large amount of chaff and straw would accumulate around a threshing machine, and several innovations, such as the air chaffer, were developed to deal with this. Combines generally chop and disperse straw as they move through the field, though the chopping is disabled when the straw is to be baled, and chaff collectors are sometimes used to prevent the dispersal of weed seed throughout a field.\n\nThe corn sheller was almost identical in design, with slight modifications to deal with the larger kernel size and presence of cobs. Modern-day combines can be adjusted to work with any grain crop, and many unusual seed crops.\n\nBoth the older and modern machines require a good deal of effort to operate. The concave clearance, cylinder speed, fan velocity, sieve sizes, and feeding rate must be adjusted for crop conditions.\n\nFrom the early 20th century, petrol or diesel-powered threshing machines, designed especially to thresh rice, the most important crop in Asia, have been developed along different lines to the modern combine.\n\nEven after the combine was invented and became popular, a new compact-size thresher called a \"harvester\", with wheels, still remains in use and at present it is available from a Japanese agricultural manufacturer. The compact-size machine is very convenient to handle in small terrace fields in mountain areas where a large machine, such as combine, is not usable.\n\nPeople there use this harvester with a modern compact binder.\n\nA number of older threshing machines have survived into preservation. They are often to be seen in operation at live steam festivals and traction engine rallies such as the Great Dorset Steam Fair in England, and the Western Minnesota Steam Threshers Reunion in northwest Minnesota.\n\nIrish songwriter John Duggan immortalised the threshing machine in a song \"The Old Thrashing Mill\". The song has been recorded by Foster and Allen and Brendan Shine.\n\nOn the Alan Lomax collection Songs of Seduction (Rounder Select, 2000), there's a bawdy Irish folk song called \"The Thrashing Machine\" sung by tinker Annie O'Neil, as recorded in the early 20th Century.\n\nIn his film score for \"Of Mice and Men\" (1939) and consequently in his collection \"Music for the Movies\" (1942), American composer Aaron Copland titled a section of the score \"Threshing Machines,\" to suit a scene in the Lewis Milestone film where Curley is threatening Slim over giving May a puppy, when many of the itinerant worker men are standing around or working on threshers.\n\nIn the song Thrasher from the album Rust Never Sleeps, Neil Young compares the modern threshing machine's technique of separating wheat from wheat stalks to the natural forces of time that separate close friends from one another.\n\nThreshing machines appear in Twenty One Pilots' music video for the song House of Gold.\n\n\n"}
{"id": "27257243", "url": "https://en.wikipedia.org/wiki?curid=27257243", "title": "Time-slot interchange", "text": "Time-slot interchange\n\nA time-slot interchange (TSI) switch is a network switch that stores data in RAM in one sequence, and reads it out in a different sequence. It uses RAM, a small routing memory and a counter. Like any switch, it has input and output ports. The RAM stores the packets or other data that arrive via its input terminal.\n\nIn a pure time-slot interchange switch, there is only one physical input, and one physical output. Each physical connection is an opportunity for a switching fabric to fail. The limited number of connections of this switch is therefore valuable in a large switching fabric, because it makes this type of switching very reliable. The disadvantage of this type of switch is that it introduces delay into the signals. \n\nWhen a packet (or byte, on telephone switches) comes to the input, the switch stores the data in RAM in one sequence, and reads it out in a different sequence. Switch designs vary, but typically, a repeating counter is incremented with an internal clock. It typically wraps-around to zero. The RAM location chosen for the incoming data is taken from a small memory indexed by the counter. It is usually a location for the desired output time-slot. The current value of the counter also selects the RAM data to forward in the current output time slot. Then the counter is incremented to the next value. The switch repeats the algorithm, eventually sending data from any input time-slot to any output time-slot.\n\nTo minimize connections, and therefore improve reliability, the data to reprogram the switch is usually programmed via a single wire that threads through the entire group of integrated circuits in a printed circuit board. The software typically compares the data shifted-in with the data shifted-out, to verify that the ICs remain correctly connected. The switching data entered into the ICs is double-buffered. That is, a new switch set-up is shifted-in, and then a single pulse applies the new configuration instantly to all the connected ICs.\n\nIn a time-slot interchange (TSI) switch, two memory accesses are required for each connection (one to read and one to store). Let T be the time to access the memory. Therefore, for a connection, 2T time will be taken to access the memory. If there are n connections and t is the operation time for n lines, then\nt=2nT which givesn=t/2T\n\nt and n normally come from a higher-level system design of the switching fabric. Hence the technology yielding T determines n for a given t. T also limits t for a given n. Real switching fabrics have real requirements for n and t, and therefore since T must be an actual number set by a possible technology, real switches cannot be arbitrarily large n or small t.\n\nIn higher-speed switches, the limit from T can be halved by using a more expensive, less reliable two-port RAM. In these designs, the read and write usually occur at the same time. The switch must still arbitrate when there is an attempt to read and write a RAM slot at the same time. This is normally done by avoiding the case in the control software, by rearranging the connections in the switching fabric. (E.g. see Nonblocking minimal spanning switch)\n\nIn packet-switching networks, a time-slot interchange switch is often combined with two space-division switches to implement small network switches.\n\nIn telephone switches, time-slot interchange switches usually form the outer layer of the switching fabric at a central office's switch. They take data from time-multiplexed T-1 or E-1 lines that serve neighborhoods. The T-1 or E-1 lines serve the subscriber line interface cards (SLICs) in local neighborhoods. The SLICs serve as the outer space-division switches of a modern wired telephone system.\n"}
{"id": "31434843", "url": "https://en.wikipedia.org/wiki?curid=31434843", "title": "Timeline of mathematical innovation in South and West Asia", "text": "Timeline of mathematical innovation in South and West Asia\n\nSouth and West Asia consists of a wide region extending from the present-day country of Turkey in the west to Bangladesh and India in the east.\n\n\n\n"}
{"id": "27567694", "url": "https://en.wikipedia.org/wiki?curid=27567694", "title": "Workmanship", "text": "Workmanship\n\nWorkmanship is a human attribute relating to knowledge and skill at performing a task. The type of work may include the creation of handcrafts, art, writing, machinery and other products. Workmanship is also a quality imparted to a product.\n\nWorkmanship is considered to have been a valued human attribute even in prehistoric times. In the opinion of the economist and sociologist Thorstein Veblen, the sense of workmanship is probably the single most important attribute governing the material well being of a people, with only the parental instinct coming a close second. There have however been periods in history where workmanship was looked down on; for example in Classical Greece and Ancient Rome, where it had become associated with slavery. This was not always the case - back in the archaic period, Greeks had valued workmanship, celebrating it in Homeric hymns. In the western world, a return to a more positive attitude towards work emerged with the rise of Christianity. In Europe, Veblen considers that the social value of workmanship reached its peak with the \"Era of handicraft\". The era began as workmanship flourished with the relative peace and security of property rights that Europe had achieved by the Late middle ages. The era ended as machine driven processes began to displace the need for workmanship after the Industrial revolution. Workmanship was such a central concept during the handicraft era, that according to Veblen, even key theological questions about God's intentions for humanity were re-framed from \"What has God ordained?\" to \"What has God wrought?\". The high value placed on workmanship could sometimes be an oppressive force for certain individuals - for example, one explanation for the origin of the English phrase sent to Coventry is that it was born from the practice where London guild members expelled due to poor workmanship were forced to move to Coventry, which used to be a guild free town. But workmanship was still widely appreciated by the common people themselves. For example, when workers accustomed to practicing high standards of workmanship were first recruited to work on production lines in factories, it would be common for them to walk out, as the new roles were relatively monotonous, giving them little scope to use their skills. After Henry Ford introduced the first Assembly line in 1913, he could need to recruit about ten men to find one willing to stay in the job. Over time, and with Ford offering high rates of pay, the aversion of labor to the new ways of working was reduced.\n\nWorkmanship began to receive considerable attention from scholars once its place in society came under threat by the rise of industrialization. The Arts and Crafts movement arose in the late 19th and early 20th century, as workmanship began to be displaced by developments like greater emphases on process, machine work, and the separation of design and planning skills from the actual execution of work. Scholars involved in founding the movement, like William Morris, John Ruskin and Charles Eliot Norton, argued that the opportunity to engage in workmanship used to be a great source of fulfillment for the working class. From a historical perspective however, the arts and crafts movement has been seen as a palliative, which unintentionally reduced resistance to the displacement of workmanship.\n\nIn a book written on the nature of workmanship, David Pye writes that the displacement of workmanship has continued into the late 20th century. He writes that since World War II especially, there has been \"an enormous intensification of interest in design\", at the expense of workmanship. The trend started in the 19th century has continued, with Industrial processes increasingly designed to minimize the skill needed for workers to produce quality products. 21st century scholars such as Matthew Crawford have argued that office and other white collar work is now being displaced by similar technological developments to the ones that caused large numbers of manual workers to be made redundant from the late 19th to early 20th century. Even when the jobs remain, the cognitive aspects of the jobs taken away from the workers, due to knowledge being centralized. He calls for a revaluing of workmanship, saying that certain manual roles like mechanics, plumbers and carpenters have been resistant to further automation, and are among the most likely to continue offering the worker the chance for independent thought. Writers like Alain de Botton and Jane McGonigal have argued that the world of work needs to be reformed to make it more fulfilling and less stressful. In particular, workers need to be able to make a deeply felt imaginative connection between their own efforts and the end product. McGonigal argues that computer games can be a source of ideas for doing this; she says the primary reason for World of Warcraft being so popular is the sense of \"blissful productivity\" that its players enjoy.\n\nWorkmanship and Craftsmanship are sometimes considered synonyms, but many draw a distinction between the two terms, or at least consider craftsmanship to mean \"workmanship of the better sort\". Among those who do consider workmanship and craftsmanship to be different, the word \"workmanlike\" is sometimes even used as a pejorative, to suggest for example that while an author might understand the basics of their craft, they lack flair. David Pye has written that no one can definitively state where workmanship ends and craftsmanship begins.\n- an extract from a Homeric hymn celebrating craftsmanship.\n\nDuring the Middle Ages, smiths and especially armor smiths developed unique symbols of workmanship to distinguish the quality of their work. These became some of the most unusual signs of workmanship, comparable to the \"mon\" family crests of Japan.\n\nIt has often been held in older economic writings that people are always adverse to labor and can only be motivated to work by threats or tangible rewards such as money. While Christianity has generally been positive about workmanship, certain Bible passages such as Genesis 3:17 (\"...Cursed is the ground because of you; through painful toil you will eat food from it all the days of your life.\") have contributed to the view that labor is a necessary evil, part of the punishment for original sin, but work existed before original sin and the fall of man in Genesis 2:15 (\"Yahweh God took the man and put him in the garden of Eden to work it and keep it.\") The view that work is a punishment takes Genesis 3:17 out of context. God's curse wasn't work, but that work would inherently be harder. Veblen and others agree with this view, saying that work can be inherently joyful and satisfying in its own right. Veblen acknowledges that humans have an innate tendency towards idleness, but asserts that they also have a countervailing tendency to value work for its own sake, as is demonstrated by the vast amount of work that is undertaken without obvious external pressure. As evidence for the widely shared instinct towards workmanship, Veblen also notes the near universal tendency for humans to approve of others' good work. Psychologist Pernille Rasmussen has written that the tendency to value work can become so strong that it stops being a positive source of motivation, contributing instead to some people losing balance and becoming workaholics.\n\nThe reliability of electronic devices is greatly affected by the quality of the workmanship. Therefore, the electronics manufacturing industry has developed several voluntary consensus standards to provide guidance on how products should be designed, built, inspected, and tested.\n\nNotes\n\nCitations\n\nSources\n\n"}
{"id": "15818261", "url": "https://en.wikipedia.org/wiki?curid=15818261", "title": "Zeolitic imidazolate framework", "text": "Zeolitic imidazolate framework\n\nZeolitic imidazolate frameworks (ZIFs) are a class of metal-organic frameworks that are topologically isomorphic with zeolites. ZIFs are composed of tetrahedrally-coordinated transition metal ions (e.g. Fe, Co, Cu, Zn) connected by imidazolate linkers. Since the metal-imidazole-metal angle is similar to the 145° Si-O-Si angle in zeolites, ZIFs have zeolite-like topologies. As of 2010, 105 ZIF topologies have been reported in the literature. Due to their robust porosity, resistance to thermal changes, and chemical stability, ZIF’s are being investigated for applications such as carbon capture.\n\nZIFs are prepared by solvothermal or hydrothermal techniques. Crystals slowly grow from a heated solution of a hydrated metal salt, an ImH (imidazole with acidic proton), a solvent, and base. Functionalized ImH linkers allow for control of ZIF structure. This process is ideal for generating monocrystalline materials for single-crystal X-ray diffraction. A wide range of solvents, bases, and conditions have been explored, with an eye towards improving crystal functionality, morphology, and dispersity. Prototypically, an amide solvent such as N,N-dimethylformamide (DMF) is used. The heat applied decomposes the amide solvent to generate amines, which in turn generate the imidazolate from the imidazole species. Methanol, ethanol, isopropanol, and water have also been explored as alternative solvents for ZIF formation but require bases such as pyridine, TEA, sodium formate, and NaOH. Polymers such as poly(ethylene oxide)–poly(propylene oxide)–poly(ethylene oxide), polyvinylpyrrolidone, and poly-(diallyldimethylammonium chloride) have been found to act as crystal dispersants, imparting particle-size and morphology control.\n\nDue to their promising material properties, significant interest lies in economical large-scale production methods. Sonochemical synthesis, which allows nucleation reactions to proceed rapidly through acoustic generation of localized heat and pressure, has been explored as a way to shorten synthesis times. As with the case of zeolites, microwave-assisted synthesis has also been of interest for the rapid synthesis of ZIFs. Both methods have been shown to reduce reaction times from days to hours, or from hours to minutes. Solvent-free methods, such as ball-milling or chemical vapor deposition, have also been described to produce high-quality ZIF-8. Chemical vapor deposition is of particular promise due to the high degree of uniformity and aspect ratio control it can offer, and its ability to be integrated into traditional lithographic workflows for functional thin films (e.g. microelectronics). Environmentally-friendly synthesis based on supercritical carbon dioxide (scCO) have been also reported as a feasible procedure for the preparation of ZIF-8 at an industrial scale. Working under stoichiometric conditions, ZIF-8 could be obtained in 10 hours and does not require the use of ligand excess, additives, organic solvents or cleaning steps.\n\nZIF’s exhibit some properties relevant to carbon capture, but commercial technology is based on amine solvents.\n\nOne method to separate carbon dioxide exploits differences in its permeability. Because of the tunability of the pores, zeolites have been used to separate carbon dioxide. The pore size ranges from 3-12 Angstroms. Because the size of a carbon dioxide molecule is approximately 5.4 Angstroms, zeolites with a pore size of 4-5 Angstroms can be a great fit for carbon capture. However, there are factors other than just pore size that need to be considered when determining how effective zeolites will be at carbon capture. The first is basicity, which can be created by doing an alkali metal cation exchange. The second is the Si/Al ratio which impacts the cation exchange capacity. To get a higher adsorption capacity, there must be a lower Si/Al ratio in order to increase the cation exchange capacity.\n\nZif’s 68, 69, 70, 78, 81, 82, 95, and 100 have been found to have very high uptake capacity, meaning that they can store a lot of carbon dioxide even if they don’t have extremely high affinities. Of those, 68, 69, and 70 show high affinities for carbon dioxide, evidenced by their adsorption isotherms, which show steep uptakes at low pressures. One liter of ZIF can hold 83 liters of . This could also be useful for pressure-swing adsorption.\n\nMuch ZIF research focuses on the separation of hydrogen and carbon dioxide because a well-studied ZIF, ZIF-8, has a very high separation factor for hydrogen and carbon dioxide mixtures. It is also very good for the separation of hydrocarbon mixtures, like the following: \nIn addition to gas separations, ZIF’s have the potential to separate components of biofuels, specifically, water and ethanol. Of all of the ZIF’s that have been tested, ZIF-8 shows high selectivity. ZIF’s have also shown potential in separating other alcohols, like propanol and butanol, from water. Typically, water and ethanol (or other alcohols) are separated using distillation, however ZIF’s offer a potential lower-energy separation option.\n\nZIF’s also have great potential as heterogeneous catalysts; ZIF-8 has been shown to act as good catalysts for the transesterification of vegetable oils, the Friedel-Crafts acylation reaction between benzoyl chloride and anisole, and for the formation of carbonates. ZIF-8 nanoparticles can also be used to enhance the performance in the Knoevenagel condensation reaction between benzaldehyde and malononitrile. ZIF’s have also been shown to work well in oxidation and epoxidation reactions; ZIF-9 has been shown to catalyze the aerobic oxidation of tetralin and the oxidation of many other small molecules. It can also catalyze reactions to produce hydrogen at room temperature, specifically the dehydrogenation of dimethylamine borane and NaBH hydrolysis. \n\nThe table below gives a more comprehensive list of ZIF’s that can act as catalysts for different organic reactions.\nZIF’s are also good candidates for chemical sensors because of their tunable adsorbance properties. ZIF-8 exhibits sensitivity when exposed to the vapor of ethanol and water mixtures, and this response is dependent on the concentration of ethanol in the mixture. Additionally, ZIF’s are attractive materials for matrices for biosensors, like electrochemical biosensors, for in-vivo electrochemical measurements. They also have potential applications as luminescent probes for the detection of metal ions and small molecules. ZIF-8 luminescence is highly sensitive to Cu2+, and Cd2+ ions as well as acetone. ZIF nanoparticles can also sense fluorescently tagged single stranded pieces of DNA.\n\nBecause ZIF’s are porous, chemically stable, thermally stable, and tunable, they are potentially a platform for drug delivery and controlled drug release. ZIF-8 is very stable in water and aqueous sodium hydroxide solutions but decompose quickly in acidic solutions, indicating a pH sensitivity that could aid in the development of ZIF-based drug-release platforms.\n\nWhile ZIFs are a subset of the MOF hybrids that combine organic and metal frameworks to create hybrid microporous and crystalline structures, they are much more restricted in their structure. Similar to MOFs, most ZIF properties are largely dependent on the properties of the metal clusters, ligands, and synthesis conditions in which they were created.\n\nMost ZIF alterations up to this point have involved changing the linkers - bridging O2- anions and imizazolate-based ligands - or combining two types of linkers to change bond angles or pore size due to limitations in synthesizing methods and production. A large portion of changing linkers included adding functional groups with various polarities and symmetries to the imidazolate ligands to alter the ZIFs carbon dioxide adsorption ability without changing the transitional-metal cations. Compare this to MOFs, which have a much larger degree of variety in the types of their building units.\n\nDespite these similarities with MOFs, ZIFs have significant properties that distinguish these structures as uniquely to be applied to carbon capture processes. Because ZIFs tend to resemble the crystalline framework of zeolites, their thermal and chemical stability are higher than those of other MOFs, allowing them to work at a wider range in temperatures, making them suitable to chemical processes.\n\nPerhaps the most important difference is the ZIFs hydrophobic properties and water stability. A main issue with zeolites and MOFs, to a certain extent, was their adsorption of water along with CO2.[7] Water vapor is often found in carbon-rich exhaust gases, and MOFs would absorb the water, lowering the amount of CO2 required to reach saturation. MOFs are also less stable in moist and oxygen rich environments due to metal-oxygen bonds performing hydrolysis. ZIFs, however, have nearly identical performances in dry and humid conditions, showing much higher CO2 selectivity over water, allowing the adsorbent to store more carbon before saturation is reached.\n\nEven in comparison with other materials, the ZIFs most attractive quality is still its hydrophobic properties. When compared to ZIFs in dry conditions, activated carbon was nearly identical with its uptake capacity. However, once the conditions were changed to wet, the activated carbon’s uptake was halved. When this saturation and regeneration tests were run at these conditions, ZIFs also showed minimal to no structural degradation, a good indication of the adsorbent’s re-usability.\n\nHowever, ZIFs tend to be expensive to synthesize. MOFs require synthesis methods with long reaction periods, high pressures, and high temperatures, which aren’t methods that are easy to scale-up. ZIFs do tend to be more affordable than commercially available non-ZIF MOFs.\n\nWhen combined with polymer-sorbent materials, research determined that hybrid polymer-ZIF sorbent membranes no longer following the upper bound of the Robeson plot, which is a plot of selectivity as a function of permeation for membrane gas separation.\n\n\n"}
