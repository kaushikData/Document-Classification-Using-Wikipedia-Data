{"id": "17669313", "url": "https://en.wikipedia.org/wiki?curid=17669313", "title": "Absolute Category Rating", "text": "Absolute Category Rating\n\nAbsolute Category Rating (ACR) is a test method used in quality tests.\n\nThe levels of the scale are, sorted by quality in decreasing order:\n\nIn this method, a single test condition (generally an image or a video sequence) is presented to the viewers once only. They should then give a quality rating on an ACR scale. Test conditions should be presented in a random order per test person.\n\nThe ACR scale is evaluated based on numbers that are assigned to the individual items, where \"Excellent\" equals to 5 and \"Bad\" equals to 1. The average numeric score over all experiment participants, for each test condition that was shown, is called the Mean Opinion Score.\n\nThe ACR scale is also used for telephony voice quality to give a Mean Opinion Score.\n\n\n"}
{"id": "26755147", "url": "https://en.wikipedia.org/wiki?curid=26755147", "title": "Aromachologist", "text": "Aromachologist\n\nAn aromachologist is a person who practices aromachology, which is a method of using smells or essential oils to create, either in isolation or through blending in formulations, essential oils that have behavioral, physical and emotional benefits. Smell is the least studied of the senses, but aromachology is being used increasingly in healthcare and building science, and also in the world of sports and in practical matters such as real estate sales.\n\nWhile all aromachologists have a refined sense of smell, some employ other senses including sight, sound and hearing. These are synesthetes and history documents famous synesthetes such as the British painter David Hockney, the Russian artist Wassily Kandinsky, the Hungarian composer Franz Liszt and the English poet William Blake.\n\nAn aromachologist is a person who studies the effects of fragrance on human psychology and behavior and works with essential oils for their positive effects on behavior and feeling. An aromachologist is a practitioner of aromachology, which is a term coined in 1982 by the Olfactory Research Fund, now known as the Sense of Smell Institute, a division of the Fragrance Foundation, which has funded numerous medical, university and individual studies on the effects of scents on sleep and performance. Aromachology differs from aromatherapy.\n\nAn aromachologist is a formulator who works with essential oils for their aromatic and physical effects and is an expert in the way essential oils can be blended and articulated together to create “behavioral fragrances” to establish the positive effects of aromas on human behavior including feelings and emotions.\n\nThe aims of aromachology are to “study the interrelationship of psychology and the latest in fragrance technology and to transmit through odor a variety of specific feelings (such as relaxation, exhilaration, sensuality, happiness and achievement) directly to the brain.\n\nWhen odors activate the olfactory pathways that lead to the limbic portion of the brain they trigger the release of neurotransmitters that affect the brain and mental state of the individual in a variety of ways. Further, stimuli transmitted to the limbic system cannot be consciously blocked and all olfactory stimuli therefore influence our emotions.\n\nSmell as a sense is the last frontier of neuroscience and has not been studied in as much depth as vision and hearing. The brain is able to process small differences in smell and the sense of smell may last longer in the aging process than sight and hearing. The olfactory bulb is that portion of the brain which processes smells information and its oscillations alter dynamically according to the tasks involved.\n\nThere are some people who process smells differently, hearing them as sounds. Canadian aromachologist, Nadine Artemis, author and formulator for Living Libations, is one such synestheste.\n\nWhen Artemis picks up a scent, she also sees colors. Therefore, when she is creating in aromachology, she is not only combining a palette of smells, she is also mixing a palette of colors to blend the purest, organic essential oils into an array of beneficial products. For example, when Artemis sniffs tainted or synthetic fragrances, she sees them as murky gray or muddy brown. Discovering this extraordinary sense of smell at a young age, Artemis sees different distillations of rose, one of the world’s most expensive essential oils, as hues and tones of pink and red. Calming chamomile comes across in hues varying from oceanic teals to deep royal blues.\n\nIt is believed that we all possess a small degree of synesthesia according to Dr Jamie Ward — quality of intermixing images, sounds and other sensations. The Russian painter Wassily Kandinsky is believed to have employed four senses of touch, smell, color and sound.\n\nThere are many instances of literature that may have portrayed synesthesia. For example, the famous poet William Blake (1757–1827) was probably a synesthete. In his poem “Wild Flower Song”, he writes: \n\nIn the book \"Perfume\" by Patrick Süskind, the main character, Grenouille, is born with a hyper-acute sense of smell but lacks his own bodily scent. He goes on a life quest, including many murders, to recreate the scent of innocence he sniffed on a beautiful girl.\n\nStudies have been conducted to show that those parts of the brain which govern alertness and concentration can be influenced positively or negatively by the olfactory substances used. Jasmine in a testing room enhanced the problem-solving cognitive skills of participants and also led to them demonstrating more interest and motivation for the task at hand. A combination of eucalyptus, peppermint oil and ethanol has been shown to improve cognitive performance, and after a monotonous stressful task experimental subjects were shown to demonstrate greater motivation after being exposed to a blend of peppermint, bergamot, sandalwood and lavender.\n\nPleasant aromas cause people to linger longer, a boon to retail stores, museums, spas and casinos. Pleasant smells have been shown to improve productivity, and improve physical performance, with athletes running faster, doing more pushups, and experiencing shorter recovery time after an extensive workout when the room was scented with either peppermint or lemon.\n\nBy blending specific smells, an aromachologist can create a more restful environment and improve health conditions. A study in 1987 showed that the smells found in nutmeg oil, maize extract, neroli oil, valerian oil, myristici, soelemcin and elemicin reduce stress in humans as well as reducing stress-related high blood pressure. The Mind Lab, an independent consultancy in the UK, studies the odor of a building as part of research on the brain’s responses to stimuli. Real estate brokers have been recommending to their clients to have smells of freshly baked cookies or the aroma of coffee in the house when it is being presented to potential buyers to create a sense of home. By bottling and releasing appropriate smells to evoke comfort, safety and joy, an owner may be able to accelerate the sale of a house.\n\nWorker productivity can be enhanced by improving the quality of air in a building, not just by removing the negative pollutants, but also by introducing through ventilation or air conditioning systems olfactory stimulation]s to get a mix of ventilated air and odor.\n\nIt is necessary to ensure that the dosage is such that the odor is not excessive and should be kept just above the detection level. Also, these olfactory substances are very different from perfume and should instead replicate the smell of natural outdoor air.\n\nA skilled aromachologist can concoct combinations of oils to reduce road rage, reduce fatigue and improve concentration while driving. Peppermint decreases anxiety and fatigue while driving, and in combination with cinnamon it reduces the level of frustration encountered in traffic and also heightens alertness.\n\nJasmine is used as a sleep aid and the scent of vanilla is useful for those who want to cut the craving for sweets after lunch.\n\n"}
{"id": "246802", "url": "https://en.wikipedia.org/wiki?curid=246802", "title": "Audio power", "text": "Audio power\n\nAudio power is the electrical power transferred from an audio amplifier to a loudspeaker, measured in watts. The electrical power delivered to the loudspeaker, together with its efficiency, determines the sound power generated (with the rest of the electrical power being converted to heat).\n\nAmplifiers are limited in the electrical energy they can output, while loudspeakers are limited in the electrical energy they can convert to sound energy without being damaged or distorting the audio signal. These limits, or power ratings, are important to consumers finding compatible products and comparing competitors.\n\nIn audio electronics, there are several methods of measuring power output (for such things as amplifiers) and power handling capacity (for such things as loudspeakers).\n\nAmplifier output power is limited by voltage, current, and temperature:\nAs an amplifier's power output strongly influences its price, there is an incentive for manufacturers to exaggerate output power specs to increase sales. Without regulations, imaginative approaches to advertising power ratings became so common that in 1975 the US Federal Trade Commission intervened in the market and required all amplifier manufacturers to use an engineering measurement (continuous average power) in addition to any other value they might cite.\n\nFor loudspeakers, there is also a thermal and a mechanical aspect to maximum power handling.\nThere are no similar loudspeaker power handling regulations in the US; the problem is much harder as many loudspeaker systems have very different power handling capacities at different frequencies (e.g., tweeters which handle high frequency signals are physically small and easily damaged, while woofers which handle low frequency signals are larger and more robust).\n\nSince the instantaneous power of an AC waveform varies over time, AC power, which includes audio power, is measured as an average over time. It is based on this formula:\n\nFor a purely resistive load, a simpler equation can be used, based on the root mean square (RMS) values of the voltage and current waveforms:\n\nIn the case of a steady sinusoidal tone (not music) into a purely resistive load, this can be calculated from the peak amplitude of the voltage waveform (which is easier to measure with an oscilloscope) and the load's resistance:\n\nThough a speaker is \"not\" purely resistive, these equations are often used to approximate power measurements for such a system. Approximations may be used as reference on a specification sheet of a product.\n\nAn amplifier under test can drive a sinusoidal signal with a peak amplitude of 6 V (driven by a 12 V battery). When connected to an 8 ohm loudspeaker this would deliver:\n\nThus the output of an inexpensive car audio amplifier is limited by the voltage of the alternator. In most actual car systems, the amplifiers are connected in a bridge-tied load configuration, and speaker impedances are no higher than 4 Ω. High-power car amplifiers use a DC-to-DC converter to generate a higher supply voltage.\n\n\"Continuous\" \"average sine wave power\" ratings are a staple of performance specifications for audio amplifiers and, sometimes, loudspeakers.\n\nAs described above, the term \"average power\" refers to the average value of the instantaneous power waveform over time. As this is typically derived from the root mean square (RMS) of the sine wave voltage, it is often referred to as \"RMS power\" or \"watts RMS\", but this is incorrect: it is \"not\" the RMS value of the power waveform (which would be a larger, but meaningless, number). (The erroneous term \"watts RMS\" is actually used in CE regulations.) This is also referred to as the nominal value, there being a regulatory requirement to use it.\n\n\"Continuous\" (as opposed to \"momentary\") implies that the device can function at this power level for long periods of time; that heat can be removed at the same rate it is generated, without temperature building up to the point of damage.\n\nOn May 3, 1974, the Federal Trade Commission (FTC) instated its Amplifier Rule to combat the unrealistic power claims made by many hi-fi amplifier manufacturers. This rule prescribes continuous power measurements performed with sine wave signals for advertising and specifications of amplifiers sold in the US. (See more in the section Standards at the end of this article). This rule was amended in 1998 to cover self-powered speakers such as are commonly used with personal computers (see examples below).\n\nTypically, an amplifier's power specifications are calculated by measuring its RMS output voltage, with a continuous sine wave signal, at the onset of clipping—defined arbitrarily as a stated percentage of total harmonic distortion (THD), usually 1%, into specified load resistances. Typical loads used are 8 and 4 ohms per channel; many amplifiers used in professional audio are also specified at 2 ohms. Considerably more power can be delivered if distortion is allowed to increase; some manufacturers quote maximum power at a higher distortion, like 10%, making their equipment appear more powerful than if measured at an acceptable distortion level.\n\nContinuous power measurements do not actually describe the highly varied signals found in audio equipment (which could vary from high crest factor instrument recordings down to 0 dB crest factor square waves) but are widely regarded as a reasonable way of describing an amplifier's maximum output capability. For audio equipment, this is nearly always the nominal frequency range of human hearing, 20 Hz to 20 kHz.\n\nIn loudspeakers, thermal capacities of the voice coils and magnet structures largely determine continuous power handling ratings. However, at the lower end of a loudspeaker's usable frequency range, its power handling might necessarily be derated because of mechanical excursion limits. For example, a subwoofer rated at 100 watts may be able to handle 100 watts of power at 80 hertz, but at 25 hertz it might not be able to handle nearly as much power since such frequencies would, for some drivers in some enclosures, force the driver beyond its mechanical limits much before reaching 100 watts from the amplifier.\n\n\"Peak power\" refers to the maximum of the instantaneous power waveform, which, for a sine wave, always get a factor of formula_5 the average power. For other waveforms, the relationship between peak power and average power is the peak-to-average power ratio (PAPR).\n\nThe peak power of an amplifier is determined by the voltage rails and the maximum amount of current its electronic components can handle for an instant without damage. This characterizes the ability of equipment to handle quickly changing power levels, as many audio signals have a highly dynamic nature.\n\nIt always produces a higher value than the average power figure, however, and so has been tempting to use in advertising without context, making it look as though the amp has twice the power of competitors.\n\nTotal system power is a term often used in audio electronics to rate the power of an audio system. Total system power refers to the total power consumption of the unit, rather than the power handling of the speakers or the power output of the amplifier. This can be viewed as a somewhat deceptive marketing ploy, as the total power consumption of the unit will of course be greater than any of its other power ratings, except for, perhaps, the peak power of the amplifier, which is essentially an exaggerated value anyway. Shelf stereos and surround sound receivers are often rated using total system power.\n\nOne way to use total system power to get a more accurate estimate of power is to consider the amplifier class which would give an educated guess of the power output by considering the efficiency of the class. For example, class AB amplifiers can vary widely from 25% to 75% efficiency while Class D amps are much higher at 80% to 95%. An exceptionally efficient Class D amp, the ROHM BD5421efs, operates at 90% efficiency.\n\nIn some cases, an audio device may be measured by the \"total system power\" of all its loudspeakers by adding all their peak power ratings. Many home theater in a box systems are rated this way. Often low-end home theater systems' power ratings are taken at a high level of harmonic distortion as well; as high as 10%, which would be noticeable.\n\n\"PMPO\", which stands for \"Peak Music Power Output\" or \"Peak momentary performance output\", is a much more dubious figure of merit, of interest more to advertising copy-writers than to consumers. The term PMPO has never been defined in any standard, but it is often taken to be the sum of some sort of peak power for each amplifier in a system. Different manufacturers use different definitions, so that the ratio of PMPO to continuous power output varies widely; it is not possible to convert from one to the other. Most amplifiers can sustain their PMPO for only a very short time, if at all; loudspeakers are not designed to withstand their stated PMPO for anything but a momentary peak without serious damage.\n\nPerceived \"loudness\" varies approximately logarithmically with acoustical output power. The change in perceived loudness as a function of change in acoustical power is dependent on the reference power level. It is both useful and technically accurate to express perceived loudness in the logarithmic decibel (dB) scale that is independent of the reference power, with a somewhat straight-line relationship between 10 dB changes and doublings of perceived loudness.\n\nThe approximately logarithmic relationship between power and perceived loudness is an important factor in audio system design. Both amplifier power and speaker sensitivity affect the maximum realizable loudness. Standard measurement practice of speaker sensitivity is driving 1 watt electrical power to the source, with the receiver 1 meter away from the source, and measuring the resulting acoustical power in dB relative to the threshold of hearing (defined as 0 dB). Sensitivity is typically measured either suspended in an anechoic chamber in 'free space' (for full range speakers), or with the source and receiver outside on the ground in 'half space' (for a subwoofer).\n\nWhile a doubling/halving of perceived loudness corresponds to approximately 10 dB increase/decrease in speaker sensitivity, it also corresponds to approximately 10X multiplication/division of acoustical power. Even a relatively modest 3 dB increase/decrease in sensitivity corresponds to a doubling/halving of acoustical power. When measuring in 'half space', the boundary of the ground plane cuts the available space that the sound radiates into in half and doubles the acoustical power at the receiver, for a corresponding 3 dB increase in measured sensitivity, so it is important to know the test conditions. ±3 dB change in measured sensitivity also corresponds to a similar doubling/halving of electrical power required to generate a given perceived loudness, so even deceptively 'minor' differences in sensitivity can result in large changes in amplifier power requirement. This is important because power amplifiers become increasingly impractical with increasing amplifier power output.\n\nMany high quality domestic speakers have a sensitivity between ~84 dB and ~94 dB, but professional speakers can have a sensitivity between ~90 dB and ~100 dB. An '84 dB' source would require a 400-watt amplifier to produce the same acoustical power (perceived loudness) as a '90 dB' source being driven by a 100-watt amplifier, or a '100 dB' source being driven by a 10 watt amplifier. A good measure of the 'power' of a system is therefore a plot of maximum loudness before clipping of the amplifier and loudspeaker combined, in dB SPL, at the listening position intended, over the audible frequency spectrum. The human ear is less sensitive to low frequencies, as indicated by Equal-loudness contours, so a well-designed system should be capable of generating relatively higher sound levels below 100 Hz before clipping.\n\nLike perceived loudness, speaker sensitivity also varies with frequency and power. The sensitivity is measured at 1 watt to minimize nonlinear effects such as power compression and harmonic distortion, and averaged over the usable bandwidth. The bandwidth is often specified between the measured '+/-3 dB' cutoff frequencies where the relative loudness becomes attenuated from the peak loudness by at least 6 dB. Some speaker manufacturers use '+3 dB/-6 dB' instead, to take into account the real-world in-room response of a speaker at frequency extremes where the floor/wall/ceiling boundaries may increase the perceived loudness.\n\nSpeaker sensitivity is measured and rated on the assumption of a fixed amplifier output voltage because audio amplifiers tend to behave like voltage sources. Sensitivity can be a misleading metric due to differences in speaker impedance between differently designed speakers. A speaker with a higher impedance may have lower measured sensitivity and thus appear to be less efficient than a speaker with a lower impedance even though their efficiencies are actually similar. Speaker efficiency is a metric that only measures the actual percentage of electrical power that the speaker converts to acoustic power and is sometimes a more appropriate metric to use when investigating ways to achieve a given acoustic power from a speaker.\n\nAdding an identical and mutually coupled speaker driver (much less than a wavelength away from each other) and splitting the electrical power equally between the two drivers increases their combined efficiency by a maximum of 3 dB, similar to increasing the size of a single driver until the diaphragm area doubles. Multiple drivers can be more practical to increase efficiency than larger drivers since frequency response is generally proportional to driver size.\n\nSystem designers take advantage of this efficiency boost by using mutually coupled drivers in a speaker cabinet, and by using mutually coupled speaker cabinets in a venue. Each doubling of total driver area in the array of drivers brings ~3 dB increase in efficiency until the limit where the total distance between any two drivers of the array exceeds ~1/4 wavelength.\n\nPower handling capability is also doubled when the number of drivers doubles, for a maximum realizable increase of ~6 dB in total acoustic output per doubling of mutually coupled drivers when the total amplifier power is also doubled. Mutual coupling efficiency gains become difficult to realize with multiple drivers at higher frequencies because the total size of a single driver including its diaphragm, basket, waveguide or horn may already exceed one wavelength.\n\nSources that are much smaller than a wavelength behave like point sources that radiate omnidirectionally in free space, whereas sources larger than a wavelength act as their own 'ground plane' and beam the sound forward. This beaming tends to make high frequency dispersion problematic in larger venues, so a designer may have to cover the listening area with multiple sources aimed in various directions or placed in various locations.\n\nLikewise, speaker proximity much less than 1/4 wavelength to one or more boundaries such as floor/walls/ceiling can increase the effective sensitivity by changing free space into half space, quarter space, or eighth space. When the distance to boundaries is > 1/4 wavelength, delayed reflections can increase the perceived loudness but can also induce ambient effects such as comb filtering and reverberation that can make the frequency response uneven across a venue or make the sound diffuse and harsh, especially with smaller venues and hard reflective surfaces.\n\nSound absorbing structures, sound diffusing structures, and digital signal processing may be employed to compensate for boundary effects within the designated listening area.\n\nThe term \"Music Power\" has been used in relation to both amplifiers and loudspeakers with some validity. When live music is recorded without amplitude compression or limiting, the resulting signal contains brief peaks of much higher amplitude (20 dB or more) than the mean, and since power is proportional to the square of signal voltage their reproduction would require an amplifier capable of providing brief peaks of power around a hundred times greater than the average level. Thus, the ideal 100-watt audio system would need to be capable of handling brief peaks of 10,000 watts in order to avoid clipping (see Programme levels). Most loudspeakers are in fact capable of withstanding peaks of several times their continuous rating (though not a hundred times) since thermal inertia prevents the voice coils from burning out on short bursts. It is therefore, acceptable, and desirable, to drive a loudspeaker from a power amplifier with a higher continuous rating several times the steady power that the speaker can withstand, but only if care is taken not to overheat it; this is difficult, especially on modern recordings which tend to be heavily compressed and so can be played at high levels without the obvious distortion that would result from an uncompressed recording when the amplifier started clipping.\n\nAn amplifier can be designed with an audio output circuitry capable of generating a certain power level, but with a power supply unable to supply sufficient power for more than a very short time, and with heat sinking that will overheat dangerously if full output power is maintained for long. This makes good technical and commercial sense, as the amplifier can handle music with a relatively low mean power, but with brief peaks; a high 'music power' output can be advertised (and delivered), and money saved on the power supply and heat sink. Program sources that are significantly compressed are more likely to cause trouble, as the mean power can be much higher for the same peak power. Circuitry which protects the amplifier and power supply can prevent equipment damage in the case of sustained high power operation.\n\nMore sophisticated equipment usually used in a professional context has advanced circuitry which can handle high peak power levels without delivering more average power to the speakers than they and the amplifier can handle safely.\n\nCharles \"Chuck\" McGregor, while serving as senior technologist for Eastern Acoustic Works, wrote a guideline for professional audio purchasers wishing to select properly sized amplifiers for their loudspeakers. Chuck McGregor recommended a rule of thumb in which the amplifier's maximum power output rating was twice the loudspeaker's continuous (so-called \"RMS\") rating, give or take 20%. In his example, a loudspeaker with a continuous power rating of 250 watts would be well-matched by an amplifier with a maximum power output within the range of 400 to 625 watts.\n\nJBL, which tests and labels their loudspeakers according to the IEC 268-5 standard (called IEC 60268-5 more recently) has a more nuanced set of recommendations, depending on the usage profile of the system, which more fundamentally involves the (worst case) crest factor of the signal used to drive the loudspeakers:\n\nActive speakers comprise two or three speakers per channel, each fitted with its own amplifier, and preceded by an electronic crossover filter to separate the low-level audio signal into the frequency bands to be handled by each speaker. This approach enables complex active filters to be used on the low level signal, without the need to use passive crossovers of high power handling capability but limited rolloff and with large and expensive inductors and capacitors. An additional advantage is that peak power handling is greater if the signal has simultaneous peaks in two different frequency bands. A single amplifier has to handle the peak power when both signal voltages are at their crest; as power is proportional to the square of voltage, the peak power when both signals are at the same peak voltage is proportional to the square of the sum of the voltages. If separate amplifiers are used, each must handle the square of the peak voltage in its own band. For example, if bass and midrange each has a signal corresponding to 10 W of output, a single amplifier capable of handling a 40 W peak would be needed, but a bass and a treble amplifier each capable of handling 10 W would be sufficient. This is relevant when peaks of comparable amplitude occur in different frequency bands, as with wideband percussion and high-amplitude bass notes.\n\nFor most audio applications more power is needed at low frequencies. This requires a high-power amplifier for low frequencies (e.g., 200 watts for 20–200 Hz band), lower power amplifier for the midrange (e.g., 50 watts for 200 to 1000 Hz), and even less the high end (e.g. 5 watts for 1000–20000 Hz). Proper design of a bi/tri amplifier system requires a study of driver (speaker) frequency response and sensitivities to determine optimal crossover frequencies and power amplifier powers.\n\nPeak momentary power output and peak music power output are two different measurements with different specifications and should not be used interchangeably. Manufacturers who use different words such as pulse or performance may be reflecting their own non-standard system of measurement, with an unknown meaning. The Federal Trade Commission is putting an end to this with Federal Trade Commission (FTC) Rule 46 CFR 432 (1974), affecting Power Output Claims for Amplifiers Utilized in Home Entertainment Products.\n\nIn response to a Federal Trade Commission order, the Consumer Electronics Association has established a clear and concise measure of audio power for consumer electronics. They have posted an FTC approved product marking template on their web site and the full standard is available for a fee.\nMany believe this will resolve much of the ambiguity and confusion in amplifier ratings.\nThere will be ratings for speaker and powered speaker system too. This specification only applies to audio amplifiers. An EU counterpart is expected and all equipment sold in the US and Europe will be identically tested and rated.\n\nThis regulation did not cover automobile entertainment systems, which consequently still suffer from power ratings confusion. However, a new Approved American National Standard ANSI/CEA-2006-B which includes testing & measurement methods for mobile audio amplifiers is being slowly phased into the market by many manufacturers.\n\nDIN (Deutsches Institut für Normung, German Institute for Standardization) describes in DIN 45xxx several standards for measuring audio power. The DIN-standards (DIN-norms) are in common use in Europe.\n\nIEC 60268-2 defines power amplifier specifications including power output.\n\n\n"}
{"id": "1815897", "url": "https://en.wikipedia.org/wiki?curid=1815897", "title": "Aviation Industry Computer-Based Training Committee", "text": "Aviation Industry Computer-Based Training Committee\n\nThe Aviation Industry Computer-Based Training Committee (AICC) was an international association of technology-based training professionals that existed from 1988 to 2014. The AICC developed guidelines for aviation industry in the development, delivery, and evaluation of CBT, WBT, and related training technologies. \n\nAICC specifications were usually designed to be general purpose (not necessarily Aviation Specific) so that learning technology vendors can spread their costs across multiple markets and thus provide products (needed by the Aviation Industry) at a lower cost. This strategy resulted in AICC specifications having broad acceptance and relevance to non-aviation and aviation users alike.\n\nThe AICC was formed in 1988 by Aircraft manufacturers (Boeing, Airbus, and McDonnell Douglas) to address Airline concerns about non-standard computing (cost) issues arising from the proliferation of new multimedia training materials emerging at that time.\n\nIn 1989, the AICC published computing platform recommendations for CBT, training media. A PC-platform was established as the primary delivery platform for CBT media. \n\nIn 1992, the AICC produced a digital audio interoperability specification for DOS based platforms. This specification allowed multiple CBT vendors to use a single audio card. AICC audio drivers were produced for Elan, SoundBlaster(tm), WICAT, and other audio cards. A large number of older legacy CBT applications still use this specification today.\n\nIn 1993, the AICC produced what is widely regarded as the first runtime interoperability specification for Learning Management Systems (LMS) a.k.a. CMI Systems. This AICC specification (CMI001 - AICC/CMI Guidelines For Interoperability) was originally designed for CD-ROM/LAN (local file-based) operation and was updated in January 1998 to add a web-based interface called HACP (HTTP-based AICC/CMI Protocol). In September 1999, the CMI001 specification was updated to add a JavaScript API runtime interface. The runtime environment data model and API used in the SCORM specification is a derivative of this work.\n\nThe AICC HACP standard for CMI is widely used by Learning Management Systems and other systems to call content and assessments. Although it is pre-XML, it is very robust and unambiguous and many consider it to be more secure and reliable than alternatives such as SCORM, especially for content or assessments hosted on web servers not collocated with the calling system.\n\nAn emerging standard is the AICC PENS standard, which lets content creating tools send a manifest to an LMS easily. (See CMI010 - Package Exchange Notification Services). The September 2006 AICC meeting included a Plugfest where vendors demonstrated PENS interoperability. \n\nIn November 2010, the AICC announced that it would begin work on a replacement of its existing CMI specification. This effort was later given the name \"CMI-5\". A SOAP-Based specification for CMI-5 was drafted in May 2012 but was never formally released.\n\nIn October 2012, the AICC announced that it had adopted the Experience API (xAPI) specification (a.k.a. \"Tin Can\") for its CMI-5 effort starting a significant re-design that is currently in progress. \n\nThe AICC coordinated its efforts with other learning technology specifications organizations engaged in similar work such as IMS Global, OKI, ADL, IEEE/LTSC, LETSI, and ISO/SC36.\n\nIn December 2014, the AICC formally announced that it had dissolved citing declining participation. \nThe AICC transferred the CMI-5 effort and its document archive to the ADL. \n\nSee AICC Document Archive - Hosted by ADL\n\ncmi5 is still currently under development and is now administered by the ADL. (See external Links below for project status)\n\n"}
{"id": "43530417", "url": "https://en.wikipedia.org/wiki?curid=43530417", "title": "Badge tether", "text": "Badge tether\n\nA badge tether or badge reel is a spring-loaded reeled tether that resembles a button badge in appearance or attachment. It is used to avoid damage to or the loss of small important objects kept on-person that need to be accessed frequently or quickly, such as a ski pass, identification card or badge, name badge, keys, a phone or other handheld device, or a penknife or other small tool.\n\nBadge tethers consist of a thin cord, dimensions on the order of a millimetre diameter by a metre long with one end wound round a spring-loaded reel contained within a small badge-like body that has a clip for a belt, belt loop, pocket, the edge of the clothing itself, or an attachment specifically for such a tether. The other end of the cord has a clip, loop, splitring, strap, or other fastener.\n"}
{"id": "31313402", "url": "https://en.wikipedia.org/wiki?curid=31313402", "title": "Ballistic conduction in single-walled carbon nanotubes", "text": "Ballistic conduction in single-walled carbon nanotubes\n\nSingle-walled carbon nanotubes have the ability to conduct electricity. This conduction can be ballistic, diffusive, or based on scattering. When ballistic in nature conductance can be treated as if the electrons experience no scattering.\n\nConduction in single-walled carbon nanotubes is quantized due to their one-dimensionality and the number of allowed electronic states is limited, if compared to bulk graphite. The nanotubes behave consequently as quantum wires and charge carriers are transmitted through discrete conduction channels. This conduction mechanism can be either ballistic or diffusive in nature, or based on tunneling. When ballistically conducted, the electrons travel through the nanotubes channel without experiencing scattering due to impurities, local defects or lattice vibrations. As a result, the electrons encounter no resistance and no energy dissipation occurs in the conduction channel.\nIn order to estimate the current in the carbon nanotube channel, the Landauer formula can be applied, which considers a one-dimensional channel, connected to two contacts – source and drain.\n\nAssuming no scattering and ideal (transparent) contacts, the conductance of the one-dimensional system is given by G = GNT, where T is the probability that an electron will be transmitted along the channel, N is the number of the channels available for transport, and G is the conductance quantum 2e/h = (12.9kΩ). Perfect contacts, with reflection R = 0, and no back scattering along the channel result in transmission probability T = 1 and the conductance of the system becomes G = (2e/h) N. Thus each channel contributes 2G to the total conductance.\nFor metallic armchair nanotubes, there are two subbands, which cross the Fermi level, and for semiconducting nanotubes – bands which don’t cross the Fermi level. Thus there are two conducting channels and each band accommodates two electrons of opposite spin. Thus the value of the conductance is G = 2G = (6.45 kΩ).\n\nIn a non-ideal system, T in the Landauer formula is replaced by the sum of the transmission probabilities for each conduction channel. When the value of the conductance for the above example approaches the ideal value of 2G, the conduction along the channel is said to be ballistic. This happens when the scattering length in the nanotube is much greater than the distance between the contacts.\nIf a carbon nanotube is a ballistic conductor, but the contacts are nontransparent, the transmission probability, T, is reduced by back-scattering in the contacts. If the contacts are perfect, the reduced T is due to back-scattering along the nanotube only.\nWhen the resistance measured at the contacts is high, one can infer the presence of Coulomb blockade and Luttinger liquid behavior for different temperatures. Low contact resistance is a prerequisite for investigating conduction phenomena in CNTs in the high transmission regime.\n\nWhen the size of the CNT device scales with the electron coherence length, important in the ballistic conduction regime in CNTs becomes the interference pattern arising when measuring the differential conductance formula_1 as a function of the gate voltage. This pattern is due to the quantum interference of multiply reflected electrons in the CNT channel. Effectively, this corresponds to a Fabry-Perot resonator, where the nanotube acts as a coherent waveguide and the resonant cavity is formed between the two CNT-electrode interfaces. Phase coherent transport, electron interference, and localized states have been observed in the form of fluctuations in the conductance as a function of the Fermi energy.\n\nPhase coherent electrons give rise to the observed interference effect at low temperatures. Coherence then corresponds to a decrease in the occupation numbers of phonon modes and a decreased rate of inelastic scattering. Correspondingly, increased conduction is reported for low temperatures.\n\nCNT FETs exhibit four regimes of charge transport:\nOhmic contacts require no scattering as the charge carriers are transported through the channel, i.e. the length of the CNT should be much smaller than the mean free path (L« l). The opposite is valid for diffusive transport.\nIn semiconducting CNTs at room temperature and for low energies, the mean free path is determined by the electron scattering from acoustic phonons, which results in l ≈ 0.5μm. In order to satisfy the conditions for ballistic transport, one has to take care of the channel length and the properties of the contacts, while the geometry of the device could be any top-gated doped CNT FET.\n\nBallistic transport in a CNT FET takes place when the length of the conducting channel is much smaller than the mean free path of the charge carrier, l.\n\nOhmic i.e. transparent contacts are most favorable for an optimized current flow in a FET.\nIn order to derive the current-voltage (I-V) characteristics for a ballistic CNT FET, one can start with Planck's postulate, which relates the energy of the i-th state to its frequency:<br>\nformula_2\n\nThe total current for a many-state system is then the sum over the energy of each state multiplied by the occupation probability function, in this case the Fermi–Dirac statistics:\n\nformula_3\n\nFor a system with dense states, the discrete sum can be approximated by an integral:\n\nformula_4\n\nIn CNT FETs, the charge carriers move either left (negative velocity) or right (positive velocity) and the resulting net current is called drain current. The source potential controls the right-moving, and the drain potential - the left moving carriers and if the source potential is set to zero, the Fermi energy at the drain subsequently decreases to yield positive drain voltage. The total drain current is computed as a sum of all contributing subbands in the semiconductor CNT, but given the low voltages used with nanoscale electronics, higher subbands can be effectively ignored and the drain current is given only by the contribution of the first subband:\n\nformula_5\nwhere formula_6<br> and formula_7 is the quantum resistance.\n\nThe expression for formula_8 gives the ballistic current dependence on the voltage in a CNT FET with ideal contacts.\n\nIdeally, ballistic transport in CNT FETs requires no scattering from optical or acoustic phonons, however the analytical model yields only partial agreement with experimental data. Thus, one needs to consider a mechanism, which would improve the agreement and recalibrate the definition of ballistic conduction in CNTs. Partially ballistic transport is modeled to involve optical phonon scattering. Scattering of electrons by optical phonons in carbon nanotube channels has two requirements:\n\nCNT FETs with Schottky contacts are easier to fabricate than those with ohmic contacts. In these transistors, the gate voltage controls the thickness of the barrier, and the drain voltage can lower the barrier height at the drain electrode. Quantum tunneling of the electrons through the barrier should also be taken into account here. In order to understand the charge conduction in Schottky barrier CNT FETs, we need to study the band schemes under different bias conditions (Fig 2):\nThus, the Schottky barrier CNT FET is effectively an ambipolar transistor, since the ON electron current is opposed by an OFF hole current, which flows at values smaller than the critical gate voltage value.\n\nFrom the band diagrams, one can deduce the formula_9 characteristics of the Schottky CNT FETs. Starting at the OFF state, there is hole current, which gradually decreases as the gate voltage is increased until it is opposed with equal strength by the electron current coming from the source. Above the critical gate voltage in the ON state, the electron current prevails and reaches a maximum at formula_10 and the formula_11 curve will roughly have a V-shape.\n"}
{"id": "46390755", "url": "https://en.wikipedia.org/wiki?curid=46390755", "title": "Barry Clemson", "text": "Barry Clemson\n\nBarry Allen Clemson (born 1941) is an American cybernetician. His work experience includes custom manufacturing, community development (in the US and India), software development, university research and teaching, starting a construction company, consulting, and writing a novel. His academic career was in educational administration (University of Maryland and University of Maine) and engineering management (Old Dominion University)\n\nDuring his first academic post, at the University of Maryland, Clemson was elected President of the American Society for Cybernetics (ASC). At that time there was a split within the American cybernetics community, with some who had been members of the ASC having joined a new group called the American Cybernetics Association (ACA). Working with Larry Heilprin and Klaus Krippendorff, the three of them were able to merge the two groups and arrange for an election in which Stuart Umpleby was elected as president. Umpleby was able to rejuvenate the ASC and put it onto a trajectory which continues to this day.\n\nIn 1948, shortly before his seventh birthday, Clemson's family, with siblings aged six, four and six months, moved from the Lancaster, Pennsylvania area to a wilderness homestead two miles north of Anchor Point, Alaska. Anchor Point consisted of a couple of families and a tiny trading post. The nearest road was 20 miles away in Homer and Homer could be reached only by boat or plane. They spent the first summer in a tent and the following two years in a log cabin with a dirt floor. Water came from a spring a hundred yards away (during the summer) and from melted snow in winter. Food was largely hunted (mostly moose) or from the garden. After guiding the dog sled home from school, chores included splitting wood. Wilderness Alaska nurtured a love of nature and required both self-sufficiency and lots of responsibility at a very early age.\n\nClemson spent almost a year (June 1964 to April 1965) in Mississippi working on voter registration with the Student Nonviolent Coordinating Committee (SNCC). This project was transformative for most of the volunteers who participated, and it definitely was for Clemson. Colleagues like Fannie Lou Hamer vividly and dramatically lived out the fundamental truth that freedom is a state of mind that has nothing to do with one's external circumstances. One always has choices, one can always decide what he or she will or will not do.\n\nDuring Clemson's work on voter registration his car was shot, a motorcycle cop tried to run over him, he was attacked by two men, another man threatened to kill him with a rifle, and he three times managed to (briefly) integrate Alabama jails. Several times he expected to die but was never even injured. These experiences are recounted in his \"Mississippi Freedom Summer: An autobiographical fragment\"\nFrom 1969 to 1972 Clemson was a Fellow of the Ecumenical Institute, now called the Institute of Cultural Affairs, at their base in Chicago. The Ecumenical Institute had two main thrusts: community development and renewal of local congregations. The Institute was truly ecumenical, with members from many different Christian denominations and also Jews and Hindus. The Institute understood that fundamental change in societal institutions required a deep spiritual component. The Fellows of the Ecumenical Institute lived as a religious order, under vows of poverty, chastity, and obedience.\n\nBy the time Clemson finished a bachelor's degree in science (1965), he was convinced that large organizations were crucial in the struggle for a more just society. A masters in political science and a PhD in organization theory followed. During the masters program, Clemson read a paper by the management cybernetician, Stafford Beer, and was fascinated. Cybernetics presented laws and principles that applied to all complex systems, including human, biological, and physical systems. This paper started a journey into complex systems that continues today and is the reason he considers himself a management cybernetician.\n\nClemson spent ten years running a small construction company after several other, more “intellectual” careers. This ten years of “wandering in the desert” culminated with the realization that all of his varied experiences had equipped him to write a specific kind of novel, a novel that would vividly make the case for the practicality of strategic nonviolence, especially as articulated by Dr. Gene Sharp. The first novel, Denmark Rising, tells the story of how Denmark used strategic nonviolence to resist the Nazi occupation during World War II. The novel garnered rave reviews as a vivid, compelling story that convincingly shows the Danes holding their own against Hitler.\n\nClemson watched his wife, Reverend Dr. Mary Clemson, heal people by praying for them. Mostly out of curiosity, Clemson went through a week of training in healing prayer with Francis MacNutt. Dr. Francis MacNutt has more than 30 years experience with healing prayer, has written half a dozen influential books on the subject, conducts healing prayer training seminars all over the world, and is the head of Christian Healing Ministries. At Mary's insistence he then prayed for a woman who for many years had been unable to lift her arm above shoulder height. The woman immediately regained the full range of motion and six years later still has full range of motion. Their church now has a major emphasis on healing prayer and dramatic healings occur on a regular basis. As a scientist, Clemson says he has given up trying to understand the efficacy of healing prayer except that the faith of the sick person doesn't seem to have much to do with whether or not they are healed.\n\nIn Clemson's opinion, he has published only two important works:\n\nLong-term viability requires dealing with a host of difficult, inter-related crises including but not limited to over-population, ecosystem degradation and species extinction, global warming / climate change, resource shortages, gross inequality and lack of justice, and wars. If civilization is to survive we need systemic / cybernetic thinking and policies.\n\nThere is little hope of achieving long-term viability of humankind without effective governance. Government by tyranny obviously doesn't work (and never can work because it lacks requisite variety ). Unfortunately, the democratic governments currently in existence are also largely failing. For a number of years, Clemson has been involved with systems scientists and cyberneticians from around the world who are grappling with the issue of how we might achieve more effective and more truly democratic forms of governance.\n\nClemson, along with his wife and a few others, ran a “children’s church” for the youth in a large public housing project. The children's church lasted seven years, caring for roughly fifty children during most of that time. Many of these children had no responsible adults in their lives. The church moved out of the housing project a few years ago and shifted its focus from a “children’s church” to an emphasis on healing prayer in one of the poorer neighborhoods of our city. Clemson has served as the administrator for Triumph Community Church since its inception.\n\nClemson is the editor for the on-line, peer-reviewed journal for Systems Thinking World, an online community of roughly 20,000 people interested in systems thinking and cybernetics.\n\nAikido, the “way of harmony” or the “way of love” depends upon moving in harmony with the attacker. Succeeding in “moving in harmony with the attacker” will throw that person across the room with essentially no effort. It is the most cybernetic of the martial arts and Clemson has loved it since he began training almost 20 years ago.\n\nAs of 2014, there are several novels in various stages of development and a series of online courses on management cybernetics are being prepared.\n\nClemson is married to Reverend Dr. Mary Clemson. Their combined family includes five children, nine grandchildren, and seven great-grandchildren (as of 2014). His parents, now in their nineties, are again living in Alaska where Clemson grew up.\n\nClemson insists that the following quotes tell you where his center is.\n\n\"It's not how much you do, its how much love you put into it … Do small things with great love.\" -- Mother Teresa --\n\n\"The entire cosmos waits to see what vision we commit to.\" -- Matthew Fox—The Ontological Covenant, from Thomas Berry, might serve as the foundation for a cybernetic ethic. The following quotes provide a sense of the Ontological Covenant:\n\n"}
{"id": "26100904", "url": "https://en.wikipedia.org/wiki?curid=26100904", "title": "Blocks to Robots", "text": "Blocks to Robots\n\nBlocks to Robots: Learning with Technology in the Early Childhood Classroom (2008) is an educational guide book by Marina Umaschi Bers that introduces the idea of learning with technology in the early childhood classroom. \nResearch shows that attitudes about science, math, and technology start to form during early education. This book shows how to successfully use technology in the classroom, using a constructivist approach to teaching and learning. Bers focusses on robotic manipulatives that allow children to explore complex concepts in a concrete and fun way (Bers,2008).\n\n\n"}
{"id": "2867523", "url": "https://en.wikipedia.org/wiki?curid=2867523", "title": "Calculator watch", "text": "Calculator watch\n\nA calculator watch is a digital watch with a built in calculator, usually including buttons on the watch face. Calculator watches were first introduced in the 1970s and continue to be produced, despite falling from their peak popularity during the 1980s. The most notable brand is the Casio Databank series, though watches made by Timex were also popular.\n\nMost calculator watches contain only a few number of functions such as +, -, x, / and percents. However, there are several models with additional functions: scientific, including transcendent and trigonometry, in models Casio CFX-200, CFX-400, Citizen 49-9421, and also financial functions (in the Casio CBA-10).\n\nUsually calculator watch operates with eight digits number, however, calculator watch can operate with 6 digits (for example, Casio C-801) or 10 digits (Casio CBA-10) numbers.\n\nCalculator watches first appeared in the mid 1970s introduced by Pulsar (1975, then a brand of the Hamilton Watch Company) and Hewlett Packard.\n\nSeveral watch manufacturers have made calculator watches over the years, such as Pulsar and Timex, but the Japanese electronics company Casio produced the largest variety of models. In the mid-1980s, Casio created the Databank calculator watch, which not only performed calculator functions, but also stored appointments, names, addresses, and phone numbers.\n\nMass-produced calculator watches appeared in the early 1980s, with the most being produced in the middle of the decade,\n\nThe future of the calculator watch as a practical and useful electronic device has been impacted by the introduction of PDAs, mobile phones, and other powerful multi-functional compact computing devices. As a result, many calculator watches are used for aesthetic purposes.\n\n\n"}
{"id": "40837", "url": "https://en.wikipedia.org/wiki?curid=40837", "title": "Call-second", "text": "Call-second\n\nIn telecommunication, a call-second is a unit used to measure communications traffic density. \n\n\"Note 1:\" A call-second is equivalent to 1 call with a duration of 1 second. \n\n\"Note 2:\" One user making two 75-second calls is equivalent to two users each making one 75-second call. Each case produces 150 call-seconds of traffic. \n\n\"Note 3:\" The acronym CCS (Centum Call Seconds) is often used to describe 100 call-seconds.\n\n\"Note 4:\" 3600 call-seconds = 36 CCS = 1 call-hour. \n\n\"Note 5:\" 3600 call-seconds per hour = 36 CCS per hour = 1 call-hour per hour = 1 erlang = 1 traffic unit.\n\nIn a communication network, a trunk (link) can carry numerous concurrent calls by means of multiplexing. Hence a particular number of CCS can be carried in infinitely many ways as calls are established and cleared over time. For example 3600 could be one call for an hour, or 2 (possibly concurrent) calls for half an hour each. CCS gives a measure of the average number of concurrent calls (i.e. Erlangs) over a time period of one hour.\n\nHence, in a one-hour period:\n"}
{"id": "30858999", "url": "https://en.wikipedia.org/wiki?curid=30858999", "title": "Camalot", "text": "Camalot\n\nCamalot, or cam, is a brand of spring-loaded camming devices, manufactured by Black Diamond Equipment, Ltd. \n\nThe device is used in cracks to secure ropes while rock climbing.\n\nThere are two types of Camalots, those with four lobes (original Camalots and modern C4 models) and those with three lobes (Camalot C3s). The original Camalots and the C4s utilize a patented dual-axle cam system, resulting in a slightly higher expansion range than for a similarly sized single axle unit, and also resulting in increased strength, allowing placement as a passive stopper. C4s range from the small #0.3, to the rarely seen massive #6.\n\nBlack Diamond has also announced the Camalot X4, which have a flexible cable stem similar to Aliens or Metolius Master cams. The X4 were released in Spring 2013.\n\nC3s are single axle units designed for smaller, narrower placements, and range from a tiny #000 to #2. There is some overlap between the C4 #0.3 and the C3 1 and 2. As with most microcams, the C3 #000 is rated for aid climbing only.\n\nCamalot lobes were designed to have a logarithmic spiral shape, resulting in a constant angle between the cam and the rock at each contact point; this constant angle is designed to always provide the necessary friction to hold a cam in equilibrium.\n\n"}
{"id": "58446702", "url": "https://en.wikipedia.org/wiki?curid=58446702", "title": "Canoe pack", "text": "Canoe pack\n\nA canoe pack, also known as a portage pack, is a specialized type of backpack used primarily where travel is largely by water punctuated by portages where the gear needs to be carried over land. \n\nWhen worn, a canoe pack must ride below the level of the shoulders in order to accommodate the wearing also carrying a canoe. Their shallow stature typically has a lower center of gravity than a normal hiking backpack, making storage in a canoe more stable.\n\nA typical pack weight while portaging was during the North American fur trade era. In order to support the heavy load of the pack(s), canoe packs are sometimes used in conjunction with a \"tumpline\" or \"portage collar,\" a strap attached to the pack and placed over the top of the head. Portage packs lack many features of long distance hiking backpacks, and so are generally not used for such.\n\n"}
{"id": "30922946", "url": "https://en.wikipedia.org/wiki?curid=30922946", "title": "Cel-Fi", "text": "Cel-Fi\n\nCel-Fi is a device, similar to a cellular repeater, designed to eliminate in-building dead zones and improve indoor mobile phone reception.\n\nCreated by Nextivity, Inc., the Cel-Fi systems are designed with smart antenna technology to seek out the best available signal to maximize signal gain to phone users. The Cel-Fi consists of two wireless devices, a window unit and coverage unit, that work jointly to increase 3G and 4G mobile broadband connectivity throughout the building. The window unit receives the signal and relays it to the receiver, which converts it and amplifies it.\n\nThese units were trialled for a period of time in both the US and UK by T-Mobile, and approved in UK by T-Mobile and Orange in 2011 and O2 in 2012.\nCel-Fi systems are comparable to cellular repeaters and femtocell technology. In October 2012 the Cel-Fi system was the first and only signal boosting device to be approved for use in Australia.\n\nFully compliant under the February 2013 Safe Harbor 2 issued by the FCC: On February 20th 2013, the FCC released a Report & Order, thus establishing two Safe Harbors and defining the use of “network safe” consumer boosters on licensed spectrum. The Safe Harbors represent a compromise solution between Technology Manufacturers and Wireless Operators. It is widely considered a landmark decision which was many years in the making.\n\nCel-Fi has an LTE/4G/3G product. It was launched in September 2014.\n\n\n"}
{"id": "4277314", "url": "https://en.wikipedia.org/wiki?curid=4277314", "title": "Cellular repeater", "text": "Cellular repeater\n\nA cellular repeater (also known as cell phone signal booster or amplifier) is a type of bi-directional amplifier used to improve cell phone reception. A cellular repeater system commonly consists of a donor antenna that receives and transmits signal from nearby cell towers, coaxial cables, a signal amplifier, and an indoor rebroadcast antenna.\n\nA \"donor antenna\" is typically installed by a window or on the roof a building and used to communicate back to a nearby cell tower. A donor antenna can be any of several types, but is usually directional or omnidirectional. An omnidirectional antenna (which broadcast in all directions) is typically used for a repeater system that amplify coverage for all cellular carriers. A directional antenna is used when a particular tower or carrier needs to be isolated for improvement. The use of a highly directional antenna can help improve the donor's signal-to-noise ratio, thus improving the quality of signal redistributed inside a building.\n\nSome cellular repeater systems can also include an omnidirectional antenna for rebroadcasting the signal indoors. Depending on attenuation from obstacles, the advantage of using an omnidirectional antenna is that the signal will be equally distributed in all directions.\n\nCelullar repeater systems include a signal amplifier. Standard GSM channel selective repeaters (operated by telecommunication operators for coverage of large areas and big buildings) have output power around 2 W, high power repeaters have output power around 10 W. The power gain is calculated by the following equation:\nA repeater needs to secure sufficient isolation between the donor and the service antenna. When the isolation is lower than actual gain plus a margin (of typically 5–15 dB), the repeater may go into in loop oscillation. This oscillation can cause interference to the cellular network.\n\nThe isolation may be improved by antenna type selection in a macro environment, which involves adjusting the angle between the donor and service antennas (ideally 180°), space separation (typically the vertical distance in the case of the tower installation between donor and service antenna is several meters), insertion into an attenuating environment (e.g. installing a metal mesh between donor and service antennas), and/or reduction of reflections (no near obstacles in front of the donor antenna such as trees or buildings).\n\nIsolation can be also improved by integrated feature called ICE (interference cancellation equipment) offered in some products (e.g., NodeG, RFWindow). Activation of this feature has a negative impact on internal delay (higher delay => approximately +5 μs up to standard rep. delay) and consequently a shorter radius from donor site. Amplification and filtering introduce a delay (typically between 5 and 15 μs), depending on the type of repeater and features used. Additional distance also adds propagation delay.\n\nIn many rural areas the housing density is too low to make construction of a new base station commercially viable. Installing a home cellular repeater may remedy this. In flat rural areas the signal is unlikely to suffer from multipath interference. \n\nCertain construction materials can attenuate cell phone signal strength. Older buildings, such as churches, often block celullar signals. Any building that has a significant thickness of concrete, or a large amount of metal used in its construction, will attenuate the signal. Concrete floors are often poured onto a metal pan, which completely blocks most radio signals. Some solid foam insulation and some fiberglass insulation used in roofs or exterior walls have foil backing, which can reduce transmittance. Energy efficient windows and metal window screens are also very effective at blocking radio signals. Some materials have peaks in their absorption spectra, which decrease signal strength.\n\nLarge buildings, such as warehouses, hospitals, and factories, often lack cellular reception. Low signal strength also tends to occur in underground areas (such as basements, and in shops and restaurants located towards the centre of shopping malls). In these cases, an external antenna is usually used.\n\nEven in urban areas (which usually have strong cellular signals throughout), there may be dead zones caused by destructive interference of waves. These usually have an area of a few blocks and will usually only affect one of the two frequency ranges used by cell phones. This happens because different wavelengths of the different frequencies interfere destructively at different points. Directional antennas can be helpful at overcoming this issue since they may be used to select a single path from several (see Multipath interference for more details).\n\nThe longer wavelengths have the advantage of diffracting more, and so line of sight is not as necessary to obtain a good signal. Because the frequencies that cell phones use are too high to reflect off the ionosphere as shortwave radio waves do, cell phone waves cannot travel via the ionosphere. (See Diffraction and Attenuation for more details).\n\nRepeaters are available for all of the GSM frequency bands. Some repeaters will handle different types of networks (such as multi-mode GSM and UMTS). Repeater systems are available for certain Satellite phone systems, allowing these to be used indoors without a clear line of sight to the satellite.\n\nIt used to be legal to use the low power devices available for home and small scale use in commercial areas (offices, shops, bars, etc.).\n\nOn February 20, 2013, the FCC released a Report & Order, thus establishing two Safe Harbors and defining the use of \"network safe\" consumer boosters on licensed spectrum. The Safe Harbors represent a compromise solution between Technology Manufacturers and Wireless Operators. Only a few companies have a product compatible with the new FCC regulations.\n\nThe FCC has defined two types of repeaters:\n\n\nThese new rules by the FCC were implemented on March 1, 2014. Here are the rules.\n\nIn May 2011, Ofcom stated the following:\n\nInstallation or use of repeater devices (as with any radio equipment) is a criminal offence unless two conditions are satisfied:\n\nUnder WT Act 2006 section 1.15, the wireless act also allows an exemption if the device does not \"involve undue interference with wireless telegraphy\". This is expected to follow the US-style regulations where a mobile repeater must have protection built in against interference.\n\nOfcom stated that \"Repeater devices transmit or re-transmit in the cellular frequency bands. Only the mobile network operators are licensed to use equipment that transmits in these bands. Installation or use of repeater devices by anyone without a licence is a criminal offence under Section 8 of the WT Act 2006.\" Repeaters operating in rural and less densely populated areas do not pose a quantifiable problem.\n"}
{"id": "48621044", "url": "https://en.wikipedia.org/wiki?curid=48621044", "title": "Construction History Society", "text": "Construction History Society\n\nThe Construction History Society (not to be confused with \"The Construction History Society of America\") is a learned society that promotes the international study of the history of construction. Though based in Britain, it is interested in the history of construction of all countries and particularly how those histories inter-relate. A key aim is the preservation of the primary records of construction companies and individuals. . The society publishes a peer-reviewed journal - Construction History - twice a year; and a magazine - The Construction Historian - as and when it can. It hosts an annual conference at Cambridge University and supports the triennial conferences if its sister organisations worldwide.\n\nThe society has approximately 350 members, drawn from all parts of the world, and is managed by a Board of Trustees, all of whom give their time freely. The society is a Registered a Charity and its registered address is the Department of Architecture and Art History at Cambridge University.\n\nThe society was formed as the Construction History Group in 1983, but later changed its name to the Construction History Society. It has sponsored three international conferences on the history of construction at Queens' College, Cambridge University, and supported by the Cambridge School of Architecture, in 2014, 2015, and 2016. The society is a registered charity.\n\nThe chairman is Dr. James W.P. Campbell of the University of Cambridge.\n\nThe society has published a peer-reviewed journal, Construction History, since 1985 which is abstracted and indexed in the Arts & Humanities Citation Index, SCOPUS and the Web of Science.\n"}
{"id": "1916359", "url": "https://en.wikipedia.org/wiki?curid=1916359", "title": "Dust collection system", "text": "Dust collection system\n\nA dust collection system is an air quality improvement system used in industrial, commercial, and home production shops to improve breathable air quality and safety by removing particulate matter from the air and environment. Dust collection systems work on the basic formula of \"capture\", \"convey\" and \"collect\".\n\nFirst, the dust must be \"captured\". This is accomplished with devices such as capture hoods to catch dust at its source of origin. Many times, the machine producing the dust will have a port to which a duct can be directly attached.\n\nSecond, the dust must be \"conveyed\". This is done via a ducting system, properly sized and manifolded to maintain a consistent minimum air velocity required to keep the dust in suspension for conveyance to the collection device. A duct of the wrong size can lead to material settling in the duct system and clogging it.\n\nFinally, the dust is \"collected\". This is done via a variety of means, depending on the application and the dust being handled. It can be as simple as a basic pass-through filter, a cyclonic separator, or an impingement baffle. It can also be as complex as an electrostatic precipitator, a multistage baghouse, or a chemically treated wet scrubber or stripping tower.\n\nSmaller dust collection systems use a single-stage vacuum unit to create suction and perform air filtration, where the waste material is drawn into an impeller and deposited into a container such as a bag, barrel, or canister. Air is recirculated into the shop after passing through a filter to trap smaller particulate.\n\nLarger systems utilize a two-stage system, which separates larger particles from fine dust using a pre-collection device, such as a cyclone or baffled canister, before drawing the air through the impeller. Air from these units can then be exhausted outdoors or filtered and recirculated back into the work space.\n\nDust collection systems are often part of a larger air quality management program that also includes large airborne particle filtration units mounted to the ceiling of shop spaces and mask systems to be worn by workers. Air filtration units are designed to process large volumes of air to remove fine particles (2 to 10 micrometres) suspended in the air. Masks are available in a variety of forms, from simple cotton face masks to elaborate respirators with tanked air — the need for which is determined by the environment in which the worker is operating.\n\nIn industry, round or rectangular ducts are used to prevent buildup of dust in processing equipment.\n\nProper dust collection and air filtration is important in any work space. Repeated exposure to wood dust can cause chronic bronchitis, emphysema, \"flu-like\" symptoms, and cancer. Wood dust also frequently contains chemicals and fungi, which can become airborne and lodge deeply in the lungs, causing illness and damage.\n\n"}
{"id": "41096", "url": "https://en.wikipedia.org/wiki?curid=41096", "title": "Electromagnetic interference control", "text": "Electromagnetic interference control\n\nIn telecommunication, electromagnetic interference control (EMI) is the control of radiated and conducted energy such that emissions that are unnecessary for system, subsystem, or equipment operation are reduced, minimized, or eliminated. \n\n\"Note:\" Electromagnetic radiated and conducted emissions are controlled regardless of their origin within the system, subsystem, or equipment. Successful EMI control with effective susceptibility control leads to electromagnetic compatibility.\n\n"}
{"id": "20876374", "url": "https://en.wikipedia.org/wiki?curid=20876374", "title": "Explosives engineering", "text": "Explosives engineering\n\nExplosives engineering is the field of science and engineering which is related to examining the behavior and usage of explosive materials.\n\nSome of the topics that explosives engineers study, research, and work on include:\n\n\n\n"}
{"id": "10604784", "url": "https://en.wikipedia.org/wiki?curid=10604784", "title": "Fido explosives detector", "text": "Fido explosives detector\n\nThe FIDO Explosives Detector is created by ICx Technologies, Inc. and is based on a proprietary technology developed by MIT called amplifying fluorescent polymer (AFP). The AFP technology was invented by 2007 Lemelson-MIT Prize winner Professor Timothy M. Swager and adapted for use in the FIDO explosives detector to detect trace levels of explosive materials. The product is so named because its level of detection is comparable to highly trained explosives detection dogs, the gold standard in explosives detection technology.\n\nThe overall system of the Fido Explosive Detector included a component called the Fido XT Explosive Detector developed at the U.S. Army Research Laboratory. This sensor can be used for detecting traces certain vapors, including those of explosives. It could also be used for screening people, vehicles, or buildings.\n\nThis lightweight system is based on a sensitive amplifying fluorescent polymer. It is used for detecting explosives, in various methods including direct detection or tethered sensing. Tethered sensing is when the system is mounted on a robotic platform, such as unmanned ground, aerial, or on underwater autonomous vehicles.. \n\nDr. Stephen Lee is credited with inventing the FIDO Explosives Detector while working at the Army Research Laboratory. \n\nFIDO is designed for operation in either handheld, desktop or robot-mounted configurations, and has recently been integrated on to the iRobot Packbot and Foster-Miller Talon as an explosives detection payload for EOD applications in both Iraq and Afghanistan. FIDO has won the U.S. Army Greatest Invention Award twice since the year 2005.\n\n"}
{"id": "22333127", "url": "https://en.wikipedia.org/wiki?curid=22333127", "title": "Front Panel Data Port", "text": "Front Panel Data Port\n\nThe front panel data port (FPDP) is a bus that provides high speed data transfer between two or more VMEbus boards at up to 160 MB/sec with low latency. The FPDP bus uses a 32-bit parallel synchronous bus wired with an 80-conductor ribbon cable.\n\nThe following interface functions are supported:\n\nThe connector, denoted by the FPDP specification, is a KEL P/N 8825E-080-175.\n\n\nThe following types of data frames are supported:\n\nFPDP interfaces work with up to a cable length of 1 meter when used in multi-drop configuration. They work up to 2 meter when using STROBE signal during point-to-point configuration. They work up to 5 meter when used with PSTROBE differential signal during point-to-point configuration.\n\n"}
{"id": "337864", "url": "https://en.wikipedia.org/wiki?curid=337864", "title": "Fuzzball router", "text": "Fuzzball router\n\nFuzzball routers were the first modern routers on the Internet. They were DEC LSI-11 computers loaded with the Fuzzball software written by David L. Mills (of the University of Delaware). The name \"Fuzzball\" was the colloquialism for Mills' routing software. Six Fuzzball routers provided the routing backbone of the first 56 kbit/s NSFnet, allowing the testing of many of the Internet's first protocols. It allowed the development of the first TCP/IP routing protocols, and the Network Time Protocol. They were the first routers to implement key refinements to TCP/IP like variable-length subnet masks.\n\n"}
{"id": "8564378", "url": "https://en.wikipedia.org/wiki?curid=8564378", "title": "Genesi", "text": "Genesi\n\nGenesi is an international group of technology and consulting companies in the United States, Mexico and Germany. It is most widely known for designing and manufacturing ARM architecture and Power Architecture computing devices. The Genesi Group consists of Genesi USA Inc., Genesi Americas LLC, Genesi Europe UG, Red Efika, bPlan GmbH and the affiliated non-profit organization Power2People.\n\nGenesi is an official Linaro partner and its software development team has been instrumental in moving Linux on the ARM architecture towards a wider adoption of the hard-float application binary interface, which is incompatible with most existing applications but provides enormous performance gains for many use cases.\n\nThe main products of Genesi are ARM-based computers that were designed to be inexpensive, quiet and highly energy efficient, and a custom Open Firmware compliant firmware. All products can run a multitude of operating systems.\n\n\n\nGenesi designed and maintains PowerDeveloper, an online platform for Genesi products and ARM products from other manufacturers. Via the PowerDeveloper Projects programs, hundreds of systems have been provided to the PowerDeveloper community so far, thereby supporting open source development in many countries. Linux distributions that directly benefited from the programs include but are not limited to Crux, Debian, Raspbian, Fedora, Gentoo, openSuSE and Ubuntu.\n\nGenesi once funded the development of the MorphOS operating system but shifted its focus towards Linux in 2004. However, Genesi remains the main supporter of the operating system and continues to actively support its user and developer communities via the MorphZone social platform, which features discussion forums, a digital library, a software repository and a bounty system.\n\n"}
{"id": "49253056", "url": "https://en.wikipedia.org/wiki?curid=49253056", "title": "Global Technopreneurship Challenge", "text": "Global Technopreneurship Challenge\n\nThe Global Technopreneurship Challenge is an annual international competition organised by The Technopreneurship Institute through which global challenges are addressed, including the 14 Grand Challenges for Engineering for the 21st century and The Global Goals. The challenge is akin to the olympics of technopreneurship where participants from all over the world participate to address the selected challenges through technology and entrepreneurship. Each year, cities from all over the world bid to host the year's Global Technopreneurship Challenge.\n\nThe Global Technopreneurship Challenge is run in two stages, the preliminary stage and the finals. The preliminary stage is carried out online via a massive open online course on Open Learning where participants form teams and participate in online classes and activities. During this stage the participants acquire the knowledge necessary to conceive, design, implement and operate solutions that are technologically feasible, economically viable, desirable and environmentally sustainable. The teams with the best solutions will then be invited to the finals where they will be given a chance to pitch to academics, and industrial experts as well as investors.\n\nThe first challenge was held in Kuwait in 2015.\n\nThe Kuwait Global Technopreneurship Challenge 2015 was held from July to November. It was organised by Kuwait University in collaboration with Taylor's School of Engineering and The Technopreneurship Institute. \n\nThe Kuwait Global Technopreneurship Challenge aimed to address three of the 14 grand challenges, making solar energy economical, advance personalise learning and advance health informatics. The preliminary stage of the competition was held from July to September 2015 a total of 65 teams from 151 countries worked on providing solutions to address these grand challenges. 17 teams of finalist were chosen by a panel of international judges to attend the finals in Kuwait. The finals were held from the 14th to 16 November in conjunction with the 40th anniversary of the College of Petroleum and Engineering, Kuwait University.\n\nThe winners of the challenge were: Champion: Legacy, 1st Runner Up: E-Innovation, 2nd Runner Up: Diabetes Lifecycle Management System, and Judges Choice: The Concept Crew.\n\nInnovation partner - General Electric\n\nSponsors - KFAS, Zain, The National Fund, Boubyan Bank, BP\n\nSupported by - Global Engineering Deans Council, Global Challenges Alliance, Asia School of Business, Open Learning.\n\n"}
{"id": "2581386", "url": "https://en.wikipedia.org/wiki?curid=2581386", "title": "HITEC City", "text": "HITEC City\n\nThe Hyderabad Information Technology and Engineering Consultancy City, abbreviated as HITEC City, is an Information Technology, Engineering, Health informatics, and Bioinformatics campus in the suburbs of Hyderabad, Ranga Reddy district, India. Located 15km² from the centre of Hyderabad, HITEC City lies within the Cyberabad Development Area, now part of the Hyderabad metropolitan region.\n\nHITEC City was launched in 1995 as a joint venture between Larsen & Toubro (L&T) and the Andhra Pradesh government.\nThe project outside the city limits when launched, included hubs in the suburbs of Madhapur, Gachibowli, Kondapur, Manikonda, Nanakramguda, Pocharam, Uppal, Shamirpet, and Shamshabad. The of land was contributed by the government.\nThe Project is being developed through its Special Purpose Vehicle, L&T Hitech City Limited, a joint venture company of L&T Infocity Limited and the then Andhra Pradesh Industrial Infrastructure Corporation (APIIC). HITEC City has emerged as the symbolic heart of cosmopolitan Hyderabad.\n\nThe Cyberabad Development Area (CDA) was established in 2001 to promote \"the effective development of the areas around the HITEC City and surroundings\". The CDA Master Plan was published that year by the Cyberabad Development Authority. The 52 km² zone, neighbouring the residential and commercial areas of Jubilee Hills and Banjara Hills, lies mostly within Serilingampally, 15km from the centre of Hyderabad. The Cyberabad Development Authority was dissolved in 2008, its functions merged into the Hyderabad Metropolitan Development Authority.\n\n"}
{"id": "10848603", "url": "https://en.wikipedia.org/wiki?curid=10848603", "title": "IEEE Circuits and Systems Society", "text": "IEEE Circuits and Systems Society\n\nFrom the IEEE CAS web site, the field of interest of the society is defined to be\n\nThe first meeting of the IRE Professional Group on Circuit Theory was on March 20 of 1951. After the IRE and the AIEE merged, the IRE Professional Group on Circuit Theory became the IEEE Professional Technical Group on Circuit Theory on March 25 of 1963. In 1966 the group changed its name to the Group on Circuit Theory, and in 1973 became the IEEE Circuits and Systems Society.\n\nThe Society organizes many conferences every year and operates local chapters around the world. It coordinates the operation of several councils, task forces, and technical committees.\n\nThe Circuits and Systems Society oversees the publication of eleven periodical magazines and scholarly journals:\n\nThe Society organizes, sponsors, and co-sponsors many conferences every year (64 conferences in 2006). A list of them can be retrieved by the link: .\n\nInternational Symposium on Circuits and Systems—Biomedical Circuit and Systems Conference --\nDesign Automation Conference\n\n"}
{"id": "2696160", "url": "https://en.wikipedia.org/wiki?curid=2696160", "title": "Index of electrical engineering articles", "text": "Index of electrical engineering articles\n\nThis is an alphabetical list of articles pertaining specifically to electrical and electronics engineering. For a thematic list, please see List of electrical engineering topics. For a broad overview of engineering, see List of engineering topics. For biographies, see List of engineers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLyapunov, Alexander –\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "21110412", "url": "https://en.wikipedia.org/wiki?curid=21110412", "title": "Joachim Radkau", "text": "Joachim Radkau\n\nJoachim Radkau (born October 4, 1943) is a German historian.\n\nRadkau was born in Oberlübbe, now Hille, Landkreis Minden. Son of a Protestant priest, he studied history in Münster, Berlin (Freie Universität) and Hamburg from 1963 to 1968. He was influenced e.g. by Fritz Fischer. His doctorate 1970 treated the role of German immigrants 1933-45 on Franklin D. Roosevelt. From 1971 on he started to teach at Bielefeld University.\n\n1972 till 1974 Radkau wrote together with George W. F. Hallgarten a synopsis of German industry and politics. His views about the role of Hermann Josef Abs in German negotiations with Israel led to legal claim of Deutsche Bank.\n\nRadkau received the venia legendi with a study about Rise and Crises of the German nuclear industry 1980. History of technology and Environmental history are favorite topics of Radkau. Besides forestry and the role of environmental protection in the German history, including the Third Reich, Radkau researched as well the relation between Nervosität (Anxiety) and technical development in the German Empire, adding biographical studies about Thomas Mann and Max Weber.\n\n\n\n"}
{"id": "652136", "url": "https://en.wikipedia.org/wiki?curid=652136", "title": "John R. Levine", "text": "John R. Levine\n\nJohn R. Levine is an Internet author and consultant specializing in email infrastructure, spam filtering, and software patents. He chaired the Anti-Spam Research Group (ASRG) of the Internet Research Task Force (IRTF), is president of CAUCE (the Coalition Against Unsolicited Commercial Email),is a member of the ICANN (Internet Corporation For Assigned Names and Numbers) Stability and Security Advisory Committee, and runs Taughannock Networks. He has co-authored many books, including \"The Internet For Dummies\" (with Carol Baroudi and Margaret Levine Young), \"UNIX For Dummies\" (with Margaret Levine Young), \"Fighting Spam for Dummies\" (with Margaret Levine Young, Ray Everett-Church), and \"flex & bison\". He was also the mayor of the village of Trumansburg, New York, United States from March 2004 until March 2007.\n\nLevine graduated from Yale University in 1975 and earned his Ph.D. in Computer Science from Yale in 1984 with a thesis about the design and implementation of small databases. His roommate at Yale was economist Paul Krugman. Levine was a co-founder and board member of Segue Software and Senior Programmer at Javelin Software. He was a member of the R.E.S.I.S.T.O.R.S., one of the first computer clubs in the United States. Levine is the only moderator of the comp.compilers usenet group for 32 years.\n\n"}
{"id": "14440931", "url": "https://en.wikipedia.org/wiki?curid=14440931", "title": "Mammotome", "text": "Mammotome\n\nA Mammotome device is a vacuum assisted breast biopsy device that uses image guidance such as x-ray, ultrasound and/or MRI to perform breast biopsies. A biopsy using a Mammotome device can be done on an outpatient basis with a local anesthetic, removes only a small amount of healthy tissue, and doesn’t require sutures (stitches) because the incision is very small. Mammotome is a registered trademark of Devicor Medical Products, Inc.\n\nA definitive diagnosis of the breast abnormality can usually, but not always, be made. Most women who have had both a surgical excisional biopsy and a biopsy using a Mammotome device state that the biopsy using a Mammotome device is far less stressful physically and emotionally. Cancer that is first diagnosed by a biopsy using a Mammotome device can usually be treated with one operation instead of two.\n\nDuring the biopsy, most patients experience only mild discomfort. Rarely patients may experience significant bleeding or pain during the biopsy. Following the biopsy, bruising is common. Discomfort, tenderness, and bleeding at the biopsy site are all usually mild, but occasionally they are significant. Post-biopsy breast infection and other risks and complications rarely occur. Complications, from this biopsy or any other type of breast biopsy, can delay subsequent breast surgery. The metal marker sometimes attaches to breast tissue too far from the biopsy site to be useful as a future reference. Rarely patients may have an allergic reaction to the local anesthetic.\n\nLesions accompanied by diffuse calcium deposits scattered throughout the breast are difficult to target by stereotactic biopsy. Those lesions near the chest wall also are hard to evaluate by this method. If the mammogram shows only a vague change in tissue density but no definite mass or nodule, the x-ray-guided method may not be successful. Occasionally, even after a successful biopsy, the tissue diagnosis remains uncertain and a surgical biopsy will be necessary, especially when atypical or precancerous cells are found on core biopsy.\n\n"}
{"id": "2078819", "url": "https://en.wikipedia.org/wiki?curid=2078819", "title": "Multipoint control unit", "text": "Multipoint control unit\n\nA multipoint control unit (MCU) is a device commonly used to bridge videoconferencing connections. The multipoint control unit is an endpoint on the LAN that provides the capability for three or more terminals and gateways to participate in a multipoint conference. The MCU consists of a mandatory multipoint controller (MC) and optional multipoint processors (MPs). \n\n\n"}
{"id": "42455979", "url": "https://en.wikipedia.org/wiki?curid=42455979", "title": "Network literacy", "text": "Network literacy\n\nNetwork literacy is an emerging digital literacy that deals with computer network knowledge and skills. It is linked to computer literacy and information literacy.\n\nNetwork literacy relates to the basic knowledge and skills required for citizens to participate in the networked society. Networking is become ubiquitous in the 21st Century and an understanding of network systems (such as the Internet) and network devices (such as smartphones) is vital for full participation in many modern societies.\n\nThe knowledge and skills normally associated with network literacy include:\n\n\nNetwork literacy may also embrace knowledge of social networks and personal learning networks. Behavioural protocols (\"netiquette\") are normally included.\n\nThe Internet of Things (IoT) will significantly extend the scope and reach of computer networks, and put greater focus on network literacy.\n\nAn important aspect of network literacy is the personal, economic, political, cultural and societal impact of the growth of networks on modern societies. Revelations by Edward Snowden illustrated the potential conflict between state security and personal freedom created by the growth of networking.\n\n"}
{"id": "3387689", "url": "https://en.wikipedia.org/wiki?curid=3387689", "title": "Neyer d-optimal test", "text": "Neyer d-optimal test\n\nThe Neyer d-optimal test is a sensitivity test. It can be used to answer questions such as \"How far can a carton of eggs fall, on average, before one breaks?\" If these egg cartons are very expensive, the person running the test would like to minimize the number of cartons dropped, to keep the experiment cheaper and to perform it faster. The Neyer test allows the experimenter to choose the experiment that gives the most information. In this case, given the history of egg cartons which have already been dropped, and whether those cartons broke or not, the Neyer test says \"you will learn the most if you drop the next egg carton from a height of 32.123 meters.\"\n\nThe Neyer test is useful in any situation when you wish to determine the average amount of a given stimulus needed in order to trigger a response. Examples:\n\n\nThe Neyer-d optimal test was described by Barry T. Neyer in 1994. This method has replaced the earlier Bruceton analysis or \"Up and Down Test\" that was devised by Dixon and Mood in 1948 to allow computation with pencil and paper. Samples are tested at various stimulus levels, and the results (response or no response) noted. The Neyer Test guides the experimenter to pick test levels that provide the maximum amount of information. Unlike previous methods that have been developed, this method requires the use of a computer program to calculate the test levels.\n\nAlthough not directly related to the test method, the likelihood ratio analysis method is often used to analyze the results of tests conducted with the Neyer D-Optimal test. The combined test and analysis methods are commonly known as the Neyer Test.\n\nDror and Steinberg (2008) suggest another experimental design method which is more efficient than Neyer's, by enabling the usage of a D-optimal design criterion from the outset of the experiment. Furthermore, their method is extended to deal with situations which are not handled by previous algorithms, including extension from fully sequential designs (updating the plan after each observation) to group-sequential designs (any partition of the experiment to blocks of numerous observations), from a binary response (\"success\" or \"failure\") to any generalized linear model, and from the univariate case to the treatment of multiple predictors (such as designing an experiment to test a response in a medical treatment where the experimenters changes doses of two different drugs).\n\n"}
{"id": "22618579", "url": "https://en.wikipedia.org/wiki?curid=22618579", "title": "Omni-ID", "text": "Omni-ID\n\nOmni-ID is a vendor of passive UHF Radio-frequency identification (RFID) tags. Founded in 2007 as Omni-ID, Ltd., its products are a range of RFID tags designed to operate in all environments, including on metal and liquids.\n\nThe company is based on a research unit that was formed within QinetiQ, an international defence technology company that was spun off from the former UK government agency Defence Evaluation and Research Agency (DERA) when it was split up in June 2001.\n\nOne of the problems the research unit studied was the difficulty reading radio frequency identification (RFID) tags placed close to metals and liquids. On August 22, 2007, Omni-ID was granted UK patent number GB2429878 for an “electromagnetic radiation decoupler. With a working RFID tag that overcame those difficulties, it was decided to commercialize the design by forming a new company, Omni-ID, Ltd., which was launched in March 2007. Initial funding for Omni-ID was provided by Cody Gate Ventures LLP a technology venture fund created in 2007 by QinetiQ and Coller Capital. An additional $15 million in Series C funding was provided to Omni-ID by Cody Gate Ventures in February 2009.\n\nThe core principle of the new passive RFID tag design marketed by Omni-ID was that it contained a complex and proprietary arrangement of metal layers within the tag which reflected and boosted the signal broadcast from an RFID reader. This allowed the tag to be read accurately even when placed on metal, on a liquid container, or even when immersed. This was confirmed in independent testing in November, 2008.\n\nThe company offers five different tag designs: Ultra, Max, Flex, Prox, and Curv. Each has a different read range, based on the trade-off between tag size and performance.\n\nThe Ultra tag has the longest read range in the Omni-ID product family; up to 135 ft. with a stationary reader.\n\nThe Max tag has a long read range and is designed for tracking conveyances and outdoor assets. The Max HD is a long-range global tag for use in all geographic regions.\n\nThe Flex tag is a thin, low-profile tag that offers a medium read range.\n\nThe Prox tag is a small tag with a read range of 4m has the shortest read range and is often used for tracking high value IT assets.\n\nThe Curv tag is a small, flexible tag for use on cylindrical assets.\n\nIn October 2008, the company announced a new product, OmniTether, designed to attach RFID tags to items that lacked sufficient surface area for normal attachment methods.\n\nAt the same time, the company introduced their Service Bureau product, intended to simplify the RFID tag commissioning process for its customers. Omni-ID prints a barcode on the outer label, along with human-readable information, and commissions tags prior to shipping so they arrive at the customer's facility with inventory and associated EPC coding on each tag.\n\nIn November 2008, the company announced Omni-ID On Demand for their Prox RFID tag. The two-part product allows customers to print, encode and deploy Omni-ID tags as needed. Omni-ID has applied for a patent on the product.\n\nIn September 2009, the Ultra tag was introduced, a long range passive tag. Benchmark tests by Martin Bjerre, RFID Global Solution, Inc., demonstrated a read range of 135 ft. with a stationary RFID reader, a record for a passive tag.\n\nRFID tag technology is used in supply chain management to improve inventory visibility and asset management tracking. It is also used in transponders that time races, vehicle access control, biometric passports, and in libraries.\n\nIn April, 2008, Omni-ID was a finalist and the second runner-up at the RFID Journal Awards.\n\nOmni-ID was awarded the 2009 Asia Pacific Frost & Sullivan Technology Innovation Award for UHF RFID Tags.\n\n"}
{"id": "32407015", "url": "https://en.wikipedia.org/wiki?curid=32407015", "title": "Open Source Ecology", "text": "Open Source Ecology\n\nOpen Source Ecology (OSE) is a network of farmers, engineers, architects and supporters, whose main goal is the eventual manufacturing of the Global Village Construction Set (GVCS). As described by Open Source Ecology \"the GVCS is an open technological platform that allows for the easy fabrication of the 50 different Industrial Machines that it takes to build a small civilization with modern comforts.\" Groups in Oberlin, Ohio, Pennsylvania, New York and California are developing blueprints, and building prototypes in order to pass them on to Missouri. The devices are built and tested on the \"Factor e Farm\" in rural Missouri. Recently, 3D-Print reports OSE has been experimenting with RepRap 3-D printers as suggested by academics for sustainable development.\n\nMarcin Jakubowski Ph.D. founded the group in 2003. In the final year of his doctoral thesis at the University of Wisconsin, he had the feeling that his career field was too closed off from the world's problems, and he wanted to go a different way. After graduation, he devoted himself entirely to OSE.\n\nOSE made it to the world stage in 2011 when Jakubowski presented his Global Village Construction Set TED Talk. Shortly after, the GVCS won Make magazine's Green Project Contest. The Internet blogs Gizmodo and Grist produced detailed features on OSE. Jakubowski has since become a Shuttleworth Foundation Fellow (2012) and TED Senior Fellow (2012).\n\nIn December 2013, Marcin married Catarina Mota. She co-chaired the Open Hardware Summit 2012, served on the board of directors of the Open Source Hardware Association, taught as an adjunct faculty member at ITP-NYU, and was a fellow of the National Science and Technology Foundation of Portugal.\n\nCatarina finished her PhD dissertation on the social impact of open and collaborative practices for the development of physical goods and technologies. She was a visiting scholar at ITP-NYU, Research Chair at the Open Source Hardware Association, TED Fellow, and member of NYC Resistor.\n\nOpen Source Ecology is also developing in Europe as OSE Europe.\n\nIn 2016, OSE and the Open Building Institute joined forces to make affordable, ecological housing widely accessible. The initiative has prototyped the Seed Eco-Home - a 1400 square foot home with the help of 50 people in a 5 day period - demonstrating that OSE's Extreme Manufacturing techniques can be apply to rapid swarm builds of large structures. Materials for the Seed Eco-Home cost around US$30,000. Further, OBI has prototyped the Aquaponic Greenhouse - which was also built in 5 days with 50 people: \n\nImage: not letting me upload my own image at https://www.openbuildinginstitute.org/portfolio/aquaponic-greenhouse-workshop-2/ with caption \"Open source vertical growing towers with bok choi, lettuce, and other greens. Tilapia ponds under the towers provide nutrients as water circulates from the ponds to the towers. The ponds are 4 feet deep, making for a vertical biological zone 12 feet high to maximize productivity. The goal for the greenhouse is to experiment with the feasibility of producing a full diet for one person from only 800 square feet of space.\"\nThe Factor e Farm is the main headquarters, where the machines are prototyped and tested. The farm itself also serves as a prototype. Utilizing the Open Source Ecology principles, Marcin and Catarina have built four prototype modules which comprise their home. An added greenhouse demonstrates how a family can grow their own fresh vegetables and fish. Outside there is also a large garden including fruit trees..\n\nAs of 2014, twelve of the fifty machines have been designed, blueprinted, and prototyped, with four of those reaching the documentation stage. On October 2011 a Kickstarter fundraising campaign collected 63,573 USD for project expenses and the construction of a training facility. The project has been funded by the Shuttleworth Foundation and is a semifinalist in the Focus Forward Film Festival.\n\nThe Global Village Construction Set (GVCS) comprises 50 industrial machines:\n\nDuring October, 2011 the first successful duplication of a Global Village Construction Set product by a third-party group was completed. Jason Smith along with James Slade and his organization Creation Flame developed a functioning open source CEB press. A group in Baltimore, Maryland, and a group in Dallas, Texas have also begun production of GVCS machines.\n\n"}
{"id": "38650474", "url": "https://en.wikipedia.org/wiki?curid=38650474", "title": "Poll Everywhere", "text": "Poll Everywhere\n\nPoll Everywhere is a privately held company headquartered in San Francisco, California. The company, founded in April 2007 is an online service for classroom response and audience response systems. Poll Everywhere's product allows audiences and classrooms in over 100 countries to use mobile phones, thereby \"plotting the obsolescence\" of proprietary hardware response devices otherwise known as clickers.\n\nThe company raised $20,000 in venture funding from Y Combinator in 2008.\n\nJeff Vyduna, Brad Gessler, and Sean Eby were coworkers at Deloitte Consulting charged with giving internal presentations. One day, to keep the audience awake, they decided to pull a text message \"out of a hat\". Code was started in April 2007, and the online site launched in September 2007. During that time, Vyduna matriculated at the Sloan School of Management at MIT. On May 14, 2008, the company placed as a semi finalist in the MIT $100K Entrepreneurship Challenge. Vyduna subsequently took a leave of absence when Poll Everywhere was accepted into Y Combinator in mid-2008; In 2010 the company moved to San Francisco.\n\nIn September 2012, the company announced it had acquired competitors ClassMetric and TextTheMob\n\nThe company is managed by two of its cofounders: Jeff Vyduna, Chief Executive Officer; and Brad Gessler, Chief Technology Officer. The company earns revenue via monthly and annual subscriptions to its software as a service website. Bespoke development and onsite technical support are also provided to large enterprise customers and events.\n\n\n\n"}
{"id": "30537640", "url": "https://en.wikipedia.org/wiki?curid=30537640", "title": "Pro Scientia Medal", "text": "Pro Scientia Medal\n\nThe Pro Scientia Medal was established in 1988 by NSSC, National Scientific Student Council (in Hungarian OTDT, Országos Tudományos Diákköri Tanács), Budapest. Every two years 45-50 young Hungarian artist, researcher and/or scientist gets it, who achieved remarkable results with their scientific work while at university (primarily at NSSC conferences).\n\nThe prizes are awarded in 16 sub-sections, therefore the winners are from almost all discipline fields. There is a medal in the artistic and junior (for middle school students) field as well. Currently (as of April 2010) 519 people possess medals.\n\nThe winners are grouped in the Society of Pro Scientia Medalists (SPSM), in Hungarian as Pro Scientia Aranyérmesek Társasága (PSAT).\n\n"}
{"id": "11904093", "url": "https://en.wikipedia.org/wiki?curid=11904093", "title": "Ramsbottom carbon residue", "text": "Ramsbottom carbon residue\n\nRamsbottom carbon residue, which abbreviation is RCR, is well known in the petroleum industry as a method to calculate the carbon residue of a fuel. The carbon residue value is considered by some to give an approximate indication of the combustibility and deposit forming tendencies of the fuel.\n\nThe Ramsbottom test is used to measure carbon residues of an oil. In brief, the carbon residue of a fuel is the tendency to form carbon deposits under high temperature conditions in an inert atmosphere. This is an important value for the crude oil refinery, and usually one of the measurements in a crude oil assay. Carbon residue is an important measurement for the feed to the refinery process fluid catalytic cracking and delayed coking.\n\nThere are three methods to calculate this carbon residue. It may be expressed as Ramsbottom carbon residue (RCR), Conradson carbon residue (CCR) or micro carbon residue (MCR). Numerically, the CCR value is the same as that of MCR.\nSometimes the carbon residue value can be listed as residual carbon content, RCC, which is normally the same as MCR/CCR.\n\nFor the test, 4 grams of the sample are put into a weighed glass bulb. The sample in the bulb is heated in a bath at 553°C for 20 minutes. After cooling the bulb is weighed again and the difference noted.\n\n"}
{"id": "53695751", "url": "https://en.wikipedia.org/wiki?curid=53695751", "title": "Ruud-Jan Kokke", "text": "Ruud-Jan Kokke\n\nRuud-Jan Kokke (Velp, 1956) is a Dutch designer who started his career in the mid-eighties and became known for his furniture, inventive objects, interiors and designs for public space. He has received numerous nominations and awards. He is married to the visual artist and jewellery designer Petra Hartman.\n\nRuud-Jan Kokke studied at the Sociale Academie (College for Social Studies) and at the Academy of Fine Arts in Arnhem. He is considered a designer in the tradition of Gerrit Rietveld developing his first chairs in his workshop. In 1986 he started his own label, Ruud-Jan Kokke Product & Design. Since then his furniture has been produced by companies such as Metaform, Leolux, Spectrum Design Eindhoven, Kembo, Auping and Ahrend. His first interior designs date from the early nineties. In close cooperation with his partner, Petra Hartman, he has also designed various school interiors.\n\nThe Kokke chair designed in 1984, was first produced in series by Metaform in 1988 and can now be found in the permanent collection of the Stedelijk Museum Amsterdam. It is made of flexible slats and has a transparent and strong structure. The notorious TC museum stool from 1990 was made for Museum Boijmans Van Beuningen in Rotterdam and is named after Trees Coenders, who was looking for a lightweight stool that was easy to carry and also easy to stack. It is now serving in several other museums and is collected by the MoMa New York under the title “Wander” Stacking Stool. Four years later, in 1992, Kokke designed the Kokkestok, a walking stick which is easy to use because of the long curl, the stiff end and the rubber strip on the side. Several other furniture designs followed, often in commission, such as the chairs for the restaurant of the Province House in Groningen in 1991, the Next bed for Auping (1996) and the Ahrend Run sofa (2009). In 2013 he developed Zami, an ergonomically designed stool, in close cooperation with the orthopaedic surgeon Piet van Loon.\n\nIn the mid-nineties interior and public space became increasingly important. Often this concerned the upgrading of squares and parks or the renovation of existing buildings, such as the City Hall Arnhem (2007/2008) and several local Rabobank buildings. With the new millennium Kokke started a series of interior designs for schools, such as Mozaïek College Arnhem (2001/2002), Het Stedelijk Zutphen (2010) and Canisius College Nijmegen (2016), and sports facilities, such as Olympiaplein Amsterdam (2008).\n\n\n\n\n\n\n"}
{"id": "27012220", "url": "https://en.wikipedia.org/wiki?curid=27012220", "title": "Shock tube detonator", "text": "Shock tube detonator\n\nA shock tube detonator is a non-electric explosive fuze or initiator in the form of small-diameter hollow plastic tubing used to transport an initiating signal to an explosive charge by means of a percussive wave traveling the length of the tube. It was invented by Per Anders Persson of Nitro Nobel AB, patented, and sold by them under the registered trademark Nonel, containing a small quantity of high explosive, but safer and more reliable than detonating cord with the same quantity of explosive. Another early product contained an enclosed combusting, non-detonating fiber.\n\nThe most common product is 3 mm outer diameter and 1 mm inner diameter, with a tiny dusting of HMX/aluminum explosive powder on the tubing's inner surface, which detonates down the tube at a speed greater than 6500 feet per second but does not burst the tube. Being non-electrical and non-metallic, shock tubes are less sensitive to static electricity and radio frequency energy and thus have replaced many uses of electric detonators and are safer to handle and store than detonating cord. A version containing an explosive gas mixture has the additional advantage of being entirely inert until the tubing is charged with the gas.\n\nOne manufacturer estimates that over 2 billion feet of shock tube are used each year worldwide, in commercial blasting, military demolition, theatrical special effects, automobile airbags, aircraft escape systems, IED initiation and professional fireworks.\n"}
{"id": "52381524", "url": "https://en.wikipedia.org/wiki?curid=52381524", "title": "Siae Microelettronica", "text": "Siae Microelettronica\n\nSIAE MICROELETTRONICA is an Italian multinational corporation and a global supplier of telecom network equipments. It provides wireless backhaul and fronthaul solutions that comprise microwave and millimeter wave radio systems, along with fiber optics transmission systems provided by its subsidiary SM Optics.\n\nCompany products are deployed in more than 90 countries worldwide. The company is headquartered in Milan, Italy, with 26 regional offices around the globe.\n\nEdoardo Mascetti, after graduating in 1949 in Electrical Engineering at the Polytechnic University of Milan and working as electronic designer for Siemens, founded his own company and named it SIAE, acronym for Società Italiana Apparecchiature Elettroniche The company manufactured measurement systems such as electro-mechanical testers, analog oscilloscopes, telephone system analyzers and signal generators. SIAE's sales volume in 1955 was 6.224.000 Italian lire and doubled by the end of 1957.\n\nA 431A-model oscilloscope by SIAE was also part of the synthesizer in the Studio di fonologia musicale di Radio Milano(RAI) until its dismissal on 1983 and is currently on permanent display with the original study equipment at the Musical Instrument museum hosted at the Castello Sforzesco, Milan.\nA few years after founding SIAE, Edoardo Mascetti co-founded in 1958 Microelettronica S.p.A., a company whose business was the design of telecommunication equipment for radio and landline systems and which was initially located in a basement in Milan. In 1963, the two complementary companies were merged into SIAE MICROELETTRONICA S.p.A. and the headquarters was moved to the nearby town of Cologno Monzese, where a larger area was available to accommodate the new offices and manufacturing plant.\n\nThe new company counted less than 50 employees and focused its business on telecommunication systems, which were rising thanks to the capillary diffusion of telephone systems in Italy. Analog multiplexing systems for telephone providers constituted the company's principal product; nonetheless, in 1963, the company began an active collaboration with ENEL, italian provider of electric power, in order to create a supervision system for the national distribution network, whose successful outcome later fostered similar activities in northern Europe and specifically in Norway.\n\nBy the mid '60s, the company began actively promoting its products by advertising in technical journals as natural consequence of a growing business, thanks to its first large-scale commercialized radio transceiver: the 3-channel 3-B3 and later the RT450 (1966), capable of aggregating 48 channels into UHF band. The RT450 equipment was also certified by under the commercial name H450 as fallback link for its high-capacity solutions. Power-line communication systems were also manufactured by the company in those years along with the first fixed and mobile communication terminals in VHF band (1972) for vehicular communications (precurring modern mobile phones) and anti burglar alarm systems.\n\nBy 1973, a whole new internal division was created for the design of television broadcasting equipments (repeaters and transmitters) whose main customer was the national television company RAI. The first television products were based on thermo-ionic tubes though the improvement of solid state technologies soon replaced vacuum tubes. Similar improvements in printed board manufacturing made microstrip circuits a viable solution for increasingly-high microwave frequencies and in 1978 the RT12 radio equipment boasted the first direct-conversion 2.3 GHz synthesized modulator and could aggregate 120 telephone channels. In these years, the company manufactured the historical link to connect the Milan and Rome branches of the Corriere della Sera newspaper.\n\nThe employees were about a hundred though the company still remained a family-run business lead by the founder and a tight board of managers through the '70s. Computer-aided design of electronic circuits was approached and exploited to improve yield and reduce the design time especially for the critical high-frequency sections.\n\nThe early '80s witnessed two intertwined aspects which boosted the activities: the digital revolution reached commercial radio links and the RT20 thus leveraged 4-QAM modulation in order to provide low-capacity links while the increased globalization opened up international markets and the company business expanded to Norway, Great Britain and gradually to Europe. A quality control system was soon implemented to meet and certify the improved production standards.\n\nIn partnership with then , the Company developed the multi channel radio network covering the national extents, named RIAM, for the national electric company, Enel. The radio equipments were governed by a microprocessor, in the wake of the widespread usage of these new components which offered a huge range of new possibilities for coordinating radio components when compared to traditional dedicated circuits.\n\nWith the increased demand for traffic, higher frequencies were needed and in the second half of the 80's the company commercialized its 18 GHz radio transceiver with a capacity up to 2Mbit/s, based on specifications of Enel. A 13 GHz equipment with 4Mbit/s was instead first provided to the Mercury operator in the UK.\n\nThin film manufacturing techniques for printed circuit boards were adopted during the 80s by the company for its microwave products beyond 10 GHz with ad-hoc equipments and production lines (yellow room) and soon upgraded to chip-and-wire technologies in white rooms (clean room). Documents show the RT28 transceiver remained in service until 2008 in the Italian Aosta Valley.\n\nIn 1986 the Company introduced the \"split-mount\" configuration, where an indoor unit (IDU) is connected to an outdoor unit ODU. The IDU provides the network interfaces and carries out the baseband tasks while communicates with the ODU, usually by means of an intermediate frequency, which is tasked of radio frequency up/down conversion and is connected to the antenna. The split-mount configuration is still currently in use in several modern radio products and the boost in sales at that time was recognized in the 1988 edition of Major Companies of Europe.\n\nIn the early '90s the diffusion of data services and SDH required higher capacities to be transported and in 1992 the Company provided its HS13 SDH equipment to the SIP telephone company (later become Telecom Italia) entering thus the market of high-capacity microwave radio links and introducing enhanced techniques to counteract non-idealities in the communication channel and the transceiver hardware.\n\nSimultaneously, subsidiaries were opened in Latin America and Far East. To meet international standards for control and supervision of the increasingly complex networks of radio links, the Company developed its own Network management system providing a unified interface for all its products. One of the first versions of monitoring software was still in use as of 2012 for a network.\n\nIn the meantime, the production of television transmitters and repeaters was halted in order to focus the activities towards the core business of backhauling radio links. The increased volume of chip and wire components fostered the introduction of improved Cleanrooms, upgraded to the 10'000 class. The Company further evolved internally to expand its business and offer the services surrounding the mere provision of hardware, such as network planning, assistance, installation and contributed to the activities of international standardization bodies.\n\nIn the late 90s the Company commercialized a family of products whose design was dedicated to the booming market of GSM cellular mobile communications. In 1999, the offer was expanded to support high-capacity SDH traffic and a number of multiplexing equipments were developed as well to address the increased complexity and demanded flexibility of network configurations and interfaces.\n\nThe Twenty-first century was inaugurated by the first direct collaboration and commercial relationships with China which in 2014 lead to a dedicated subsidiary, Siae Telecommunications Shenzhen Limited. The first european subsidiary opened in 2002 in Paris while the first non-european branch initiated activities in 2006 in Bangkok. The production and assembly processes began transitioning from manual to automatic SMT placement equipment in the early 2000s to cope with a volume of 15'000 radios/year while the yearly sales in 2002 increased by 25.8%.\n\nIn partnership with Cisco, the Company developed algorithms and dedicated implementations for adaptive bandwidth and adaptive modulation schemes, which allow to react to impaired communication conditions (such as rain) gracefully reducing the link throughput instead of temporarily suspending the communication (out of service). Such solutions, extensively relied upon for network planning, are currently standard in most modern point-to-point radio links for backhauling.\n\nIn the wake of the global migration from circuit switching towards IP packet networks, the Company developed Full IP equipments in split-mount and full-outdoor versions and also dual native radios, supporting both TDM and Ethernet modes, whose popularity positively affected company's sales volume. To meet the ever-increasing demand for higher throughput without need for additional bandwidth, which is licensed at a price in most dedicated bands, frequency reuse dominated subsequent industrial developments and the adoption of dual-polarization techniques was commercially proposed in 2007, when the company and Vodafone (Omnitel by that time) presented the paper \"2xSTM1 frequency reuse system with XPIC\" at the European Conference on Fixed Wireless Network Technologies 2007 in Paris.\n\nThanks to Full IP and XPIC equipment sales, the yearly production of radios hit 70'000 in 2011 while the sales income topped 180 million Euro and by the end of 2012 the number of people employed by the company exceeded 1000 over the 25 world branches, about 700 of which located in the Cologno Monzese headquarters.\n\nThe juridical literature reports of a litigation initiated in 2006 by NEC Corporation against SIAE Microelettronica over alleged infringement of several former's patents in the courts of Milan and Munich. The case was closed in 2010 with a bilateral agreement to withdraw all infringement, nullity and opposition actions pending worldwide.\n\nAlthough not directly involved in antenna manufacturing, the Company and Polytechnic University of Milan patented in 2011 a dielectric antenna for mm-wave communications based on a novel design aimed at reducing the overall size, whose application targeted small form factor radio equipments. The visual impact of transceivers began in those years to earn importance for the deployment of urban links, where the existing environment should be affected as little as possible by access stations and backhauling transceivers with their antennas.\n\nIn 2013 the company entered the millimeter-wave market with its full-outdoor transceivers in E-Band. In 2014, the product portfolio included also V band radios for small cell backhaul and NLOS solutions for urban communications which may benefit from exploiting reflections on buildings in order to increase coverage. A new network monitoring software was also released in 2014 to offer enhanced features to network operators for managing and evaluating aggregated performances, which are gaining more and more relevance as key figures in the assessment of traffic bottlenecks and overall behavior of a complex network.\n\nIn 2014 the labs for the management system for terrestrial networks and two families of equipment for fiber optic telecommunications—OMSN (Optical Multi-Service Node) and TSS (Transport Service Switch)— were transferred from Alcatel Lucent (now Nokia) to a new dedicated company, SM Optics, a subsidiary of SIAE Microelettronica.\n\nFrom 2014 to 2016, through its entirely owned company \"Twist-off\" in Padua (Italy), Siae Microelettronica was active in researching applications of Orbital Angular Momentum of light (OAM) communications applied to long-distance links. Field tests of the principles were also published and filed under an industrial patent based on the twisted parabola. As a means to increase capacity and frequency reuse orbital angular momentum multiplexing was also compared to traditional MIMO spatial multiplexing techniques in terms of antenna size/spacing/occupation and achievable performance, which resulted in a critical assessment showing that both have equal performances. Near field properties of OAM beams at microwaves have also been investigated resulting in valid theoretical and practical demonstrations to lay the ground for short-range secure communications with application to contactless payments and transactions, filed under a dedicated patent.\n\nThe company continues its long standing cooperation strategy with research institutions and in 2015 it joined the Laurea Magistrale Plus program, promoted by University of Pavia for Master's degree achieved with an extended and close participation in existing companies' activities.\n\nIn 2016 the company presented, at the Mobile World Congress, Layer-3 VPN services over microwave radio links using SM-OS, based on Software-defined networking (SDN).\n\nIn 2016, an internal team of researchers coauthored the \"Receiver\", \"Modem\" and \"Antenna\" chapters in a comprehensive book describing electronic design of transceiver frontends for backhauling.\n\n"}
{"id": "319515", "url": "https://en.wikipedia.org/wiki?curid=319515", "title": "Silicon controlled rectifier", "text": "Silicon controlled rectifier\n\nA silicon controlled rectifier or semiconductor controlled rectifier is a four-layer solid-state current-controlling device. The principle of four-layer p–n–p–n switching was developed by Moll, Tanenbaum, Goldey and Holonyak of Bell Laboratories in 1956. The practical demonstration of silicon controlled switching and detailed theoretical behavior of a device in agreement with the experimental results was presented by Dr Ian M. Mackintosh of Bell Laboratories in January 1958.\nThe name \"silicon controlled rectifier\" is General Electric's trade name for a type of thyristor. The SCR was developed by a team of power engineers led by Gordon Hall and commercialized by Frank W. \"Bill\" Gutzwiller in 1957.\n\nSome sources define silicon-controlled rectifiers and thyristors as synonymous, other sources define silicon-controlled rectifiers as a proper subset of the set of thyristors, those being devices with at least four layers of alternating n- and p-type material. According to Bill Gutzwiller, the terms \"SCR\" and \"controlled rectifier\" were earlier, and \"thyristor\" was applied later, as usage of the device spread internationally. \n\nSCRs are unidirectional devices (i.e. can conduct current only in one direction) as opposed to TRIACs, which are bidirectional (i.e. current can flow through them in either direction). SCRs can be triggered normally only by currents going into the gate as opposed to TRIACs, which can be triggered normally by either a positive or a negative current applied to its gate electrode.\n\nThe silicon control rectifier (SCR) consists of four layers of semiconductors, which form NPNP or PNPN structures, having three P-N junctions labeled J1, J2 and J3, and three terminals. The anode terminal of an SCR is connected to the p-type material of a PNPN structure, and the cathode terminal is connected to the n-type layer, while the gate of the SCR is connected to the p-type material nearest to the cathode.\n\nAn SCR consists of four layers of alternating p- and n-type semiconductor materials.\nSilicon is used as the intrinsic semiconductor, to which the proper dopants are added.\nThe junctions are either diffused or alloyed (alloy is a mixed semiconductor or a mixed metal).\nThe planar construction is used for low-power SCRs (and all the junctions are diffused).\nThe mesa-type construction is used for high-power SCRs.\nIn this case, junction J2 is obtained by the diffusion method, and then the outer two layers are alloyed to it, since the PNPN pellet is required to handle large currents.\nIt is properly braced with tungsten or molybdenum plates to provide greater mechanical strength.\nOne of these plates is hard-soldered to a copper stud, which is threaded for attachment of a heat sink.\nThe doping of PNPN depends on the application of SCR, since its characteristics are similar to those of the thyristor.\nToday, the term \"thyristor\" applies to the larger family of multilayer devices that exhibit bistable state-change behaviour, that is, switching either on or off.\n\nThe operation of an SCR and other thyristors can be understood in terms of a pair of tightly coupled bipolar junction transistors, arranged to cause the self-latching action:\n\nThere are three modes of operation for an SCR depending upon the biasing given to it:\n\nIn this mode of operation, the anode (+) is given a positive voltage while the cathode (−) is given a negative voltage, keeping the gate at zero (0) potential i.e. disconnected. In this case junction J1 and J3 are forward-biased, while J2 is reverse-biased, due to which only a small leakage current exists from the anode to the cathode until the applied voltage reaches its breakover value, at which J2 undergoes avalanche breakdown, and at this breakover voltage it starts conducting, but below breakover voltage it offers very high resistance to the current and is said to be in the off state.\n\nAn SCR can be brought from blocking mode to conduction mode in two ways: Either by increasing the voltage between anode and cathode beyond the breakover voltage, or by applying a positive pulse at the gate. Once the SCR starts conducting, no more gate voltage is required to maintain it in the ON state.\n\nThere are two ways to turn it off:\n\nWhen a negative voltage is applied to the anode and a positive voltage to the cathode, the SCR is in reverse blocking mode, making J1 and J3 reverse biased and J2 forward biased. The device behaves as two reverse-biassed diodes connected in series. A small leakage current flows. This is the reverse blocking mode. If the reverse voltage is increased, then at critical breakdown level, called the reverse breakdown voltage (V), an avalanche occurs at J1 and J3 and the reverse current increases rapidly. \nSCRs are available with reverse blocking capability, which adds to the forward voltage drop because of the need to have a long, low-doped P1 region. (If one cannot determine which region is P1, a labeled diagram of layers and junctions can help.) Usually, the reverse blocking voltage rating and forward blocking voltage rating are the same. The typical application for a reverse blocking SCR is in current-source inverters.\n\nAn SCR incapable of blocking reverse voltage is known as an asymmetrical SCR, abbreviated ASCR. It typically has a reverse breakdown rating in the tens of volts. ASCRs are used where either a reverse conducting diode is applied in parallel (for example, in voltage-source inverters) or where reverse voltage would never occur (for example, in switching power supplies or DC traction choppers).\n\nAsymmetrical SCRs can be fabricated with a reverse conducting diode in the same package. These are known as RCTs, for reverse conducting thyristors.\n\n\nForward-voltage triggering occurs when the anode–cathode forward voltage is increased with the gate circuit opened. This is known as avalanche breakdown, during which junction J2 will break down. At sufficient voltages, the thyristor changes to its on state with low voltage drop and large forward current. In this case, J1 and J3 are already forward-biased.\n\nSCRs are mainly used in devices where the control of high power, possibly coupled with high voltage, is demanded. Their operation makes them suitable for use in medium- to high-voltage AC power control applications, such as lamp dimming, power regulators and motor control.\n\nSCRs and similar devices are used for rectification of high-power AC in high-voltage dc power transmission.\nThey are also used in the control of welding machines, mainly GTAW (gas tungsten arc welding) processes similar. It is used as switch in various devices.\n\nA silicon-controlled switch (SCS) behaves nearly the same way as an SCR; but there are a few differences: Unlike an SCR, an SCS switches off when a positive voltage/input current is applied to another anode gate lead. Unlike an SCR, an SCS can also be triggered into conduction when a negative voltage/output current is applied to that same lead. \nSCSs are useful in practically all circuits that need a switch that turns on/off through two distinct control pulses. This includes power-switching circuits, logic circuits, lamp drivers, counters, etc.\n\nA TRIAC resembles an SCR in that both act as electrically controlled switches. Unlike an SCR, a TRIAC can pass current in either direction. Thus, TRIACs are particularly useful for AC applications. TRIACs have three leads: a gate lead and two conducting leads, referred to as MT1 and MT2. If no current/voltage is applied to the gate lead, the TRIAC switches off. On the other hand, if the trigger voltage is applied to the gate lead, the TRIAC switches on.\n\nTRIACs are suitable for light-dimming circuits, phase-control circuits, AC power-switching circuits, AC motor control circuits, etc.\n\n\n\n"}
{"id": "3342347", "url": "https://en.wikipedia.org/wiki?curid=3342347", "title": "Softcoding", "text": "Softcoding\n\nSoftcoding is a computer coding term that refers to obtaining a value or function from some external resource, such as a preprocessor macro, external constant, configuration file, command line argument or database table. It is the opposite of hardcoding, which refers to coding values and functions in the source code.\n\nAvoiding hard-coding of commonly altered values is good programming practice. Users of the software should be able to customize it to their needs, within reason, without having to edit the program's source code. Similarly, careful programmers avoid magic numbers in their code, to improve its readability, and assist maintenance. These practices are generally not referred to as 'softcoding'.\n\nThe term is generally used where softcoding becomes an anti-pattern. Abstracting too many values and features can introduce more complexity and maintenance issues than would be experienced with changing the code when required. Softcoding, in this sense, was featured in an article on The Daily WTF.\n\nAt the extreme end, soft-coded programs develop their own poorly designed and implemented scripting languages, and configuration files that require advanced programming skills to edit. This can lead to the production of utilities to assist in configuring the original program, and these utilities often end up being 'softcoded' themselves.\n\nThe boundary between proper configurability and problematic soft-coding changes with the style and nature of a program. Closed-source programs must be very configurable, as the end user does not have access to the source to make any changes. In-house software and software with limited distribution can be less configurable, as distributing altered copies is simpler. Custom-built web applications are often best with limited configurability, as altering the scripts is seldom any harder than altering a configuration file. \n\nTo avoid 'softcoding', consider the value to the end user of any additional flexibility you provide, and compare it with the increased complexity and related ongoing maintenance costs the added configurability involves.\n\nSeveral legitimate design patterns exist for achieving the flexibility that softcoding attempts to provide. An application requiring more flexibility than is appropriate for a configuration file may benefit from the incorporation of a scripting language. In many cases, the appropriate design is a domain-specific language integrated into an established scripting language. Another approach is to move most of an application's functionality into a library, providing an API for writing related applications quickly.\n\nIn feature design, softcoding has other meanings.\n\n"}
{"id": "8333073", "url": "https://en.wikipedia.org/wiki?curid=8333073", "title": "Sound reduction index", "text": "Sound reduction index\n\nThe sound reduction index is used to measure the level of sound insulation provided by a structure such as a wall, window, door, or ventilator. It is defined in the series of international standards ISO 16283 (parts 1-3) and the older ISO 140 (parts 1-14), or the regional or national variants on these standards. In the United States, the sound transmission class rating is generally used instead. The basic method for both the actual measurements and the mathematical calculations behind both standards is similar, however they diverge to a significant degree in the detail, and in the numerical results produced.\n\nStandardized methods exist for measuring the sound insulation produced by various structures in both laboratory and field (actual functional buildings and building sites) environments. A number of indexes are defined which each offer various benefits for different situations.\n\nThe most basic index is the Weighted Difference level D. This index is defined by measuring in decibels (dB), the noise level produced on each side of a building element under test (e.g. a wall) when noise is produced in a room on one side (or outdoors) and measured both in the room where the noise is produced and in the room on the other side of the element under test.\n\nThis measurement may be carried out by measuring the levels in octave bands, or in 1/3 octave bands. (the latter is normally used for most applications). The minimum requirements of the standards require for the frequency range from 100 Hz to 3.15 kHz to be measured (16 octave bands). In some situations measurements may be carried out in the bands down to 50 Hz and/or up to 10 kHz.\n\nThe measured levels in each 1/3 octave band (or octave band) from the source room (or area) (S) are then compared to the measured levels in the receiving room (R), and the difference is taken (S-R). this produces a measured difference level 'D' for each frequency band in the measured spectrum.\n\nTo produce a single integer number the measured spectrum is plotted on a graph, and compared against a reference curve (defined in ISO 717-1 for airborne sound insulation, and 717-2 for impact sound insulation). The reference curve is moved in 1 dB steps until the total of the unfavorable deviations (measured points on the graph below the reference graph) is as close to 32 as possible but not greater than 32.\n\nThe value of the reference curve at 500 Hz is taken as the Weighted Difference Level, D\nThis is considered to be approximately equal to the A-weighted level difference which would be observed if normal speech was used as the test signal.\n\nThe Sound Reduction Index is expressed in decibels (dB). It is the weighted sound reduction index for a partition or single component only. This is a laboratory-only measurement, which uses knowledge of the relative sizes of the rooms in the test suite, and the reverberation time in the receiving room, and the known level of noise which can pass between the rooms in the suite by other routes (flanking) plus the size of the test sample to produce a very accurate and repeatable measurement of the performance of the sampled material or construction.\n\nThis is a field measurement which attempts to measure the sound reduction index of a material on a real completed construction (e.g. a wall between two offices, houses or cinema auditoriums). It is unable to isolate or allow for the result of alternate sound transmission routes and therefore will generally produce a lower result than the laboratory measured value.\n\nThe calculation method used to produce the Sound Reduction Index takes into account the relative size of the tested rooms, and the size of the tested panel, and is therefore (theoretically) independent of these features, therefore a 1×1 panel of plasterboard (drywall) should have the same R as a 10×10 panel.\n\nThis is an index which is measured in field conditions, between \"real\" rooms. It is a measurement which deliberately includes effects due to flanking routes and differences in the relative size of the rooms. It attempts, however, to normalize the measured difference level to the level which would be present when the rooms are furnished by measuring the quantity of acoustic absorption in the receiving room and correcting the difference level to the level which would be expected if there was 10m Sabine absorption in the receiving room. Detailed, accurate knowledge of the dimensions of the receiving room are required.\n\nSimilar to the normalized level difference, this index corrects the measured difference to a standardized reverberation time. For dwellings, the standard reverberation time used is 0.5 seconds, for other larger spaces longer reverberation times will be used. 0.5 seconds is often cited as approximately average for a medium-sized, carpeted and furnished living room. Due to not requiring detailed and accurate knowledge of the dimensions of the test rooms, this index is easier to obtain, and arguably of slightly more relevance.\n\nOnce the difference level or sound reduction index is obtained, the weighted value may be obtained from the corrected spectrum as described above from the reference curve.\n\n"}
{"id": "23730636", "url": "https://en.wikipedia.org/wiki?curid=23730636", "title": "Spy-Bi-Wire", "text": "Spy-Bi-Wire\n\nSpy-Bi-Wire is a serialised JTAG protocol developed by Texas Instruments for their MSP430 micro controllers.\n\nIn this protocol only two connections are used instead of the usual four pins for the general JTAG interface. The two connections are a bidirectional data output, and a clock. The clocking signal is split into a period of three clock pulses, for each clock pulse the TDI, TDO and TMS signals are passed on the micro controller via the bidirectional data output.\n\n"}
{"id": "10842334", "url": "https://en.wikipedia.org/wiki?curid=10842334", "title": "Television Electronic Disc", "text": "Television Electronic Disc\n\nTelevision Electronic Disc (TeD) is a discontinued video recording format, released in 1975 by Telefunken and Teldec. The format used flexible foil discs, which spun at 1,500 rpm on a cushion of air. TeD never gained wide acceptance, and could not compete against the emerging videocassette systems of the time.\n\nInitially known as, \"The Video Disc\" or the Teldec Television Disc, TeD (Television Electronic Disc) was first announced at a press conference in Berlin on June 24, 1970. It was developed by a team from two German companies: AEG-Telefunken and Teldec. Program information was stored in the form of ridges in the surface of a thin, flexible foil disc, which was claimed to be sufficiently robust to withstand being played 1,000 times. The main technological breakthrough was the vertical recording method that reduced the track pitch to 0.007 mm, and increased the rotation speed to 1,500 rpm, making it possible to record 130–150 grooves per millimeter, compared with the typical 10–13 grooves on an audio disc. This increased the available bandwidth from around 15 kHz to 3 MHz. The tracks were read by a pressure pick-up, which translated the surface of the ridges, via a piezo-electric crystal, into an electrical signal.\nTracking was controlled not by the pick-up resting within the walls of a groove, but by a mechanical coupling on which the pick-up mounting is supported. There was no turntable. The rotation of the disc at 1,500 rpm created a thin cushion of air between the disc and a fixed plate. Vertical movement of the disc was kept to within 0.05mm. Eight-inch discs could store five minutes of programming, twelve-inch discs about 7½ minutes.\n\nBy autumn 1972, the name of the system had just been changed to TeD (for Television Disc) and the playing time of the eight-inch disc had been doubled to 10 minutes. TeD players were finally introduced on the West German market on 17 March 1975. Six labels offered programs: Decca, Teldec Intertel, Telefunken, Ufa/ATB, Ullstein AV and Videophon. Within the first three months 6,000 players had been shipped to 2,500 dealers, and 50,000 discs were in the shops. Sanyo Electric Co., Ltd. of Japan, which had been conducting similar research itself (but was by now also developing the V-Cord videocassette recorder), was granted a licence to produce a version that would play out in the NTSC television format and by the end of 1976 had devised a long-awaited auto-changer that took 12 10-minute discs. Also in Japan, General Corporation took a manufacturing licence in July 1976 with an expectation of coming to market in April 1977. A software consortium, Nippon Video Systems, was formed around the same time. By the end of 1978 Telefunken itself had adopted VHS.\n\n"}
{"id": "31436006", "url": "https://en.wikipedia.org/wiki?curid=31436006", "title": "The Novel Construction Initiative", "text": "The Novel Construction Initiative\n\nThe Novel Construction Initiative (NCI), established in 2003, is an independent voluntary initiative established through the International Pipe Line & Offshore Contractors Association. The mission of the initiative is to bring industry professionals together to improve the processes surrounding the laying of pipe, namely innovation in pipeline methods & technology, with the goal to improve efficiency, reduce environmental impact, and mitigate risk of accidents and injuries to those engaged in the construction process.\n\nThe initiative was proposed in 2001 and formally adopted in 2004. The first Novel Construction steering session took place in Windsor with 60 participants from oil and gas companies, contractors, technical engineers and manufacturers. They met to establish new innovations and solutions to the industry’s current problems, and set the project priority list for the Novel Construction Initiative.\n\nSince the first meeting the Novel Construction Initiative has evolved, and now includes seven working groups (Planning & Design/Monitoring & Control/Pipeline Earthworks/Lowering and Laying/Pipe Facing, Lining Up & Welding (FLUW)/ External Corrosion Protection System (ECPS)/Risk & Contracts (R&C))\n\nIn 2009 the combined efforts of the different working groups were brought together in the Initiative’s first publication: \"Onshore Pipelines: The Road to Success\".\n\nThe goal of the Road to Success document was to review in depth all the phases of an onshore pipeline project, analyse the elements that lead to successful projects and draw from the significant project experience of the participant companies. A wide range of companies and individuals from oil & gas investors & owners, design companies, construction contractors, suppliers and specialised subcontractors collaborated in a series of technical meetings and discussions over three years to ultimately produce a document intended for wide use in the pipeline industry. More than 100 persons from 45 companies participated in the preparation of the final document that was finally released in September 2009.\nThe document is available to all interested parties, and has become a reference point for experienced engineers, clients, contractors and people starting out in the pipeline industry.\n"}
{"id": "57046145", "url": "https://en.wikipedia.org/wiki?curid=57046145", "title": "Tokken", "text": "Tokken\n\nTokken is a payment system and mobile app most known for being a legal and secure option for businesses transactions within the cannabis industry because of its compliance with bank requirements. The startup company was created by Lamine Zarrad, a former regulator at the Office of the Comptroller of the Currency.\n"}
{"id": "52106718", "url": "https://en.wikipedia.org/wiki?curid=52106718", "title": "Wescom Resources Group", "text": "Wescom Resources Group\n\nWescom Resources Group (WRG) is a wholly owned subsidiary of Wescom Credit Union that operates as a CUSO headquartered in Pasadena, California, providing technology products and services to credit unions across the United States.\n\nWescom Resources Group was formed in 2002 as a CUSO of Wescom Credit Union in response to the growing needs of credit unions seeking similar technology products that Wescom Credit Union had developed for its membership. Initially, the CUSO's products and services included Online Banking, Bill Pay, Automated Lending, and New Member Application. Each of these leveraged Wescom's use of integration techniques to the Episys host financial system from Symitar, now a division of Jack Henry, Inc. In addition, the CUSO offered a hosted Service Bureau platform for some of its initial clients.\n\nThe first services Wescom Resources Group (WRG) offered to credit unions in the Southern California area was ATM Deposit Processing, with Kinecta FCU being WRG's first ATM Deposit Processing client in October 2002. \n\nIn 2003, John Best joined WRG to lead numerous development initiatives related to the home electronic banking application, and by 2009 led the WRG development team to completing its first Mobile Banking app. Wescom was among the first credit unions to deploy a native smart phone app on the then nascent Android operating system. John formed the WRG Mobile Advisory Group consisting of a small number of prominent credit unions to foster additional collaboration between WRG and its clients. In 2014, John Best left WRG in pursuit of other business opportunities within the credit union industry and remains an active technology advocate and speaker in the industry.\n\nTim Dolan, after serving nine years as WRG’s president, including twelve years as a member of Wescom’s executive team, announced his retirement earlier this year. As of March 2017 Kevin Sarber, who served as WRG’s original president for five years after its founding in 2002, has been appointed for a second term following Tim Dolan’s retirement with the current Board of Directors being Rob Guilford, Darren Williams, and Jane Wood.\n\nWRG's developing technology and customer products earn approximately $12 million annually in license and service revenue. By 2013, WRG served approximately 260 credit unions with a variety of products and services including digital channel products, as well as a number of ancillary products which have since been discontinued – including Automated Lending, Remote Deposit, CRM, and a number of internal system programming and information processing utilities.\n\nIn 2014, the CUSO's management shifted its focus to a smaller subset of products and invested more heavily in digital online channels. This resulted in a reduction to approximately 130 active customer relationships (as of July 2016), and an increase in revenue to approximately $12 million per year. Operating in the financial technology industry, also known as \"FinTech\", WRG competes with a large number of other entities, both publicly and privately held companies of various sizes. In 2010, WRG formed a new partnership with Corelation, a new financial core to further develop product integration techniques in the credit union industry. Since the launch of this partnership, WRG has grown its Service Bureau platform, based on Corelation's Keystone host processing system.\n\nWescom Resources Group's flagship product, Symmetry eBanking, is an Online Banking, Mobile Banking and Bill Pay user interface that integrates with a credit union's host system to provide account access and transaction support. The two host systems supported are Episys, by Symitar, and Keystone, by Corelation. In addition to host system integration, the WRG platform Symmetry integrates a number of ancillary service providers in check imaging, deposit capture, bill pay services, eStatements, lending, and credit cards.\n\nThe majority of WRG products start as services offered through Wescom Credit Union to its members. Once operational inside Wescom, those products are then made available to other credit unions through WRG. Although WRG creates and sells products that support different operational areas at the credit union, in recent years the CUSO has focused most of its resources on digital channels that include online banking, mobile banking, and their ancillary services.\n\nWRG also provides Service Bureau data processing to approximately 30 credit unions under the name Unitri. In addition to hosting the credit union's core processing system (either Episys or Keystone), Unitri supports related areas including Interactive Voice Response, item capture, document capture, document management, IT managed services, and other products. Tellergy, WRG's most recent product, is an interactive teller station application that resides on a digital display device from Verifone.\n\n"}
{"id": "9280953", "url": "https://en.wikipedia.org/wiki?curid=9280953", "title": "White Train", "text": "White Train\n\nDuring the height of the Cold War, the White Train worked out of the Pantex plant in the Texas panhandle. This train moved nuclear weapons from the plant where they were constructed to numerous sites to support the US nuclear program. From 1951 to 1987 The Department of Energy’s Office of Secure Transportation (OST) used the train to move the weapons by rail. The train became the focus for peace and anti-nuclear weapon activism in the west. While the train's color was changed numerous times to avoid notice, it continued to be referred to by its original color. After the last attempt to prosecute protesters who blocked the passage of the train failed, the Department of Energy began instead to move nuclear weapons by truck without public notice.\n\n"}
{"id": "12265111", "url": "https://en.wikipedia.org/wiki?curid=12265111", "title": "Zara Spook", "text": "Zara Spook\n\nZara Spook 9260 (1939–present) is a topwater type fishing lure. The lure is cast out and retrieved in a \"walk the dog\" fashion (side to side or zigzag motion). It is supposed to mimic an injured fish and comes in many different fashions but the most prominent is the minnow type. Any fish that takes its prey from the water's surface would be considered a targeted species with this lure.\n\nThe Zara Spook was first developed by the Heddon company as a wooden lure named the Zaragossa 6500 series lures. In 1939 The plastic version was introduced and following the naming of other plastic lures Heddon added spook to the name to classify it as a plastic lure. The Zara Spook 9260 Series lures contained different types of hardware throughout their production. Heddon two piece hardware were the first hardware on the 1939 catalogued Zara Spook. Later lures through the 1940s and 1950s had surface hardware. Later models contained bell hardware and moulded hangers through the 1970s. In the mid-1990s, a 4-inch long model with three hooks was introduced, and a smaller 2-inch model has also been developed for light tackle fishing. The Zara spook and spin off models are one of a few lures still marketed under the Heddon brand name by EBSCO.\n\n"}
