{"id": "26325690", "url": "https://en.wikipedia.org/wiki?curid=26325690", "title": "Access stratum", "text": "Access stratum\n\nThe access stratum (AS) is a functional layer in the UMTS and LTE wireless telecom protocol stacks between radio network and user equipment. \nWhile the definition of the access stratum is very different between UMTS and LTE, in both cases the access stratum is responsible for transporting data over the wireless connection and managing radio resources. The radio network is also called access network.\n\n"}
{"id": "989207", "url": "https://en.wikipedia.org/wiki?curid=989207", "title": "Anti-tank mine", "text": "Anti-tank mine\n\nAn anti-tank mine (abbreviated to \"AT mine\") is a type of land mine designed to damage or destroy vehicles including tanks and armored fighting vehicles.\n\nCompared to anti-personnel mines, anti-tank mines typically have a much larger explosive charge, and a fuze designed only to be triggered by vehicles or, in some cases, tampering with the mine.\n\nWhile obviously the anti-tank mine as such did not pre-date the deployment of tanks in 1916, essentially identical devices were used earlier against locomotives. For example, during the U.S. Civil War, Confederate forces created pressure-activated anti-railroad mines which destroyed at least two trains.\n\nThe first anti-tank mines were improvised during the First World War as a countermeasure against the first tanks introduced by the British towards the end of the war. Initially they were nothing more than a buried high-explosive shell or mortar bomb with its fuze upright. Later, purpose-built mines were developed, including the Flachmine 17, which was simply a wooden box packed with explosives and triggered either remotely or by a pressure fuze. By the end of the war, the Germans had developed row mining techniques, and mines accounted for 15% of U.S. tank casualties during the Battle of Saint-Mihiel, Third Battle of the Aisne, Battle of Selle and Meuse-Argonne Offensive.\n\nThe Soviet Union began developing mines in the early 1920s, and in 1924 produced its first anti-tank mine, the EZ mine. The mine, which was developed by Yegorov and Zelinskiy, had a 1 kg charge, which was enough to break the tracks of contemporary tanks. Meanwhile, in Germany, defeat spurred the development of anti-tank mines, with the first truly modern mine, the Tellermine 29, entering service in 1929. It was a disc-shaped device approximately 30 cm across filled with about 5 kg of high explosives. A second mine, the Tellermine 35 was developed in 1935. Anti-tank mines were used by both sides during the Spanish Civil War. Notably, Republican forces lifted mines placed by Nationalist forces and used them against the Nationalists. This spurred the development of anti-handling devices for anti-tank mines.\n\nThe Winter War between the Soviet Union and Finland also saw widespread use of anti-tank mines.\n\nThe German Tellermine was a purpose-built anti-tank mine developed during the period between the first and second world wars, the first model being introduced in 1929. Some variants were of a rectangular shape, but in all cases the outer casing served only as container for the explosives and fuze, without being used to destructive effect (e.g. shrapnel). Tellermine was the prototypical anti-tank mine, with many elements of its design emulated in the Pignone P-1, NR 25, and M6 mine (among others). Because of its rather high operating pressure, a vehicle would need to pass directly over top of the mine to set it off. But since the tracks represent only about 20% of a tanks width, the pressure fuse had a limited area of effect.\n\nAs one source has it: \"Since they were pressure-detonated, these early anti-tank mines typically did most of their damage to a tank's treads, leaving its crew unharmed and its guns still operational but immobilised and vulnerable to aircraft and enemy anti-tank weapons ... During World War II they (the Wehrmacht) began using a mine with a tilt-rod fuze, a thin rod standing approximately two feet up from the center of the charge and nearly impossible to see after the mine had been buried. As a tank passed over the mine, the rod was pushed forward, causing the charge to detonate directly beneath it. The blast often killed the crew and sometimes exploded onboard ammunition. Now that tank crews were directly at risk, they were less likely to plow through a minefield.\"\n\nAlthough other measures such as satchel charges, sticky bombs and bombs designed to magnetically adhere to tanks were developed, they do not fall within the category of land mines as they are not buried and detonated remotely or by pressure. The Hawkins mine was a British anti-tank device that could be employed as a mine laid on the road surface for a tank to run over setting off a crush fuze or thrown at the tank in which case a timer fuze was used.\n\nShaped charge devices like the Hohl-Sprung mine 4672 were also developed by Germany later in the war, although these did not see widespread use. The most advanced German anti-tank mine of the war was their minimal metal Topfmine.\n\nIn contrast to the \"dinner plate\" mines such as the German Tellermine were \"bar mines\" such as the German Riegel mine 43 and Italian B-2 mine. These were long mines designed to increase the probability of a vehicle triggering it, the B2 consisted of multiple small shaped-charge explosive charges along its length designed to ensure a mobility kill against enemy vehicles by destroying their tracks. This form of mine was the inspiration for the British L9 bar mine.\n\nSeveral advances have been made in the development of modern anti-tank mines, including:\n\nMore modern anti-tank mines are usually more advanced than simple containers full of explosives detonated by remote or the vehicles pressure. The biggest advances were made in the following areas:\n\nMost modern mine bodies or casings are made of plastic material to avoid easy detection. They feature combinations of pressure or magnetically activated detonators to ensure that they are only triggered by vehicles.\n\nThere are several systems for dispersing mines to quickly cover wide areas, as opposed to a soldier laying each one individually. These system can take the form of cluster bombs or be artillery fired. Cluster bombs contain several mines each, which could be a mixture of anti-personnel mines. When the cluster bomb reaches a preset altitude it disperses the mines over a wide area. Some anti-tank mines are designed to be fired by artillery, and arm themselves once they impact the target area.\n\nOff-route mines are designed to be effective when detonated next to a vehicle instead of underneath the vehicle. They are useful in cases where the ground or surface is not suitable for burying or concealing a mine. They normally employ a Misznay–Schardin shaped charge to fire a penetrating slug through the target armour. This self forging projectile principle has been used for some French and Soviet off route mines and has earned infamy as an improvised explosive devices (IED) technique in Israel and especially Iraq.\n\nDue to the critical standoff necessary for penetration and the development of standoff neutralization technologies, shaped charge off-route mines using the Munroe effect are more rarely encountered, though the British/French/German ARGES mine with a tandem warhead is an example of one of the more successful.\n\nThe term \"off-route mine\" refers to purpose designed and manufactured anti-tank mines. Explosively Formed Projectiles (EFPs) are one type of IED that was used in Iraq, but most \"home made\" IEDs are not employed in this manner.\n\nThe most effective countermeasure deployed against mine fields is mine clearing, using either explosive methods or mechanical methods. Explosive methods, such as the Giant Viper and the SADF Plofadder 160 AT, involve laying explosives across a minefield, either by propelling the charges across the field with rockets, or by dropping them from aircraft, and then detonating the explosive, clearing a path. Mechanical methods include plowing and pressure-forced detonation. In plowing, a specially designed plow attached to the front end of a heavily armored tank is used to push aside the earth and any mines embedded in it, clearing a path as wide as the pushing tank. In pressure-forced detonation, a heavily armored tank pushes a heavy spherical or cylindrical solid metal roller ahead of it, causing mines to detonate.\n\nThere are also several ways of making vehicles resistant to the effects of a mine detonation to reduce the chance of crew injury. In case of a mine's blast effect, this can be done by absorbing the blast energy, deflecting it away from the vehicle hull or increasing the distance between the crew and the points where wheels touch the ground–where any detonations are likely to centre.\n\nA simple, and highly effective, technique to protect the occupants of a wheeled vehicle is to fill the tires with water. This will have the effect of absorbing and deflecting the mine's blast energy. Steel plates between the cabin and the wheels can absorb the energy and their effectiveness is enhanced if they can be angled to deflect it away from the cabin. Increasing the distance between the wheels and passenger cabin, as is done on the South African Casspir personnel carrier, is an effective technique, although there are mobility and ease of driving problems with such a vehicle. A mine resistant vehicle can use a wedge-shaped passenger cabin, with the thin edge of the wedge downwards, to divert blast energy away from occupants. Improvised measures such as sandbags in the vehicle floor or bulletproof vests placed on the floor may offer a small measure of protection against tiny mines.\n\nSteel plates on the floor and sides and armoured glass will protect the occupants from fragments. Mounting seats from the sides or roof of the vehicle, rather than the floor, will help protect occupants from shocks transmitted through the structure of the vehicle and a four-point seat harness will minimise the chance of injury if the vehicle is flung onto its side or its roof–a mine may throw a vehicle 5 – 10 m from the detonation point.\n\nAnti-tank mines have played an important role in most wars fought since they were first used.\nAnti-tank mines played a major role on the Eastern front, where they were used in huge quantities by Soviet troops. The most common AT mines included the TM-41, TM-44, TMSB, YAM-5, and AKS. In the Battle of Kursk, combat engineers laid a staggering 503,663 AT mines, for a density of 1500 mines per kilometer. This was four times greater than what was seen in the Battle of Moscow.\n\nFurthermore, mobile detachments were tasked with laying more mines directly in the path of advancing enemy tanks. According to one source: \"... Each artillery battalion and, in some cases, each artillery battery, had a mobile reserve of 5 to 8 combat engineers equipped with 4 to 5 mines each. Their function was to mine unguarded tank approaches after the direction of the enemy attack had been definitely ascertained. These mines proved highly effective in stopping and even in destroying many enemy tanks.\"\n\nThe Wehrmacht also relied heavily on anti-tank mines to defend the Atlantic Wall, having planted six million mines of all types in Northern France alone. Mines were usually laid in staggered rows about 500 yards (460 meters) deep. Along with the anti-personnel types, there were various model of Tellermines, Topfmines, and Riegel mines. On the Western front, anti-tank mines were responsible for 20-22% of Allied tank losses. Since the majority of these mines were equipped with pressure fuzes (rather than tilt-rods), tanks were more often crippled than destroyed outright.\n\nDuring the Vietnam War, both 'regular' NVA and Viet Cong forces used AT mines. These were of Soviet, Chinese or local manufacture. Anti-tank mines were also used extensively in Cambodia and along the Thai border, planted by Pol Pot's Maoist guerrillas and the Vietnamese army, which invaded Cambodia in 1979 to topple the Khmer Rouge. Millions of these mines remain in the area, despite clearing efforts. It is estimated that they cause hundreds of deaths annually.\n\nConflict in southern Africa since the 1960s have often involved Soviet, United States or South African supported irregular armies or fighters engaged in guerrilla warfare. What makes these conflicts significant to the study of anti-tank mines is that they featured the widespread use of these mines in situations other than conventional warfare (or static minefields) and also saw the development of effective mine resistant vehicles. As a result, both Angola and Mozambique are littered with such devices to this day (as with Cambodia).\n\nIn the Angolan Civil War or South African Border War that covered vast sparsely populated area of southern Angola and northern Namibia, it was easy for small groups to infiltrate and lay their mines on roads before escaping again often undetected. The anti-tank mines were most often placed on public roads used by civilian and military vehicles and had a great psychological effect.\n\nMines were often laid in complex arrangements. One tactic was to lay multiple mines on top of each other to increase the blast effect. Another common tactic was to link together several mines placed within a few metres of each other, so that all would detonate when any one was triggered.\n\nIt was because of this threat that some of the first successful mine protected vehicles were developed by South African military and police forces. Chief amongst these were the Buffel and Casspir armoured personnel carriers and Ratel armoured fighting vehicle. They employed v-shaped hulls that deflected the blast force away from occupants. In most cases occupants survived anti-tank mine detonations with only minor injuries. The vehicles themselves could often be repaired by replacing the wheels or some drive train components that were designed to be modular and replaceable for exactly this reason.\n\nMost countries involved in Middle Eastern peace keeping missions deploy modern developments of these vehicles like the RG-31 (Canada, United Arab Emirates, United States)\nand RG-32 (Sweden).\n\n\n\n\n"}
{"id": "37995111", "url": "https://en.wikipedia.org/wiki?curid=37995111", "title": "Argentine–Chilean naval arms race", "text": "Argentine–Chilean naval arms race\n\nIn the late nineteenth and early twentieth centuries, the South American nations of Argentina and Chile engaged in an expensive naval arms race to ensure the other would not gain supremacy in the Southern Cone.\n\nAlthough the Argentine and Chilean navies possessed insignificant naval forces in the 1860s, with zero and five warships, respectively, Argentina's concern over a strong Brazilian Navy and the Chilean war against Spain caused them to add capable warships to their fleets in the 1870s. During this time, diplomatic relations between Argentina and Chile soured due to conflicting boundary claims, particularly in Patagonia. By the beginning of the 1880s, after the War of the Pacific, the Chilean government possessed possibly the strongest navy in the Americas. They planned to add to it with an 1887 appropriation for one battleship, two protected cruisers, and two torpedo gunboats. Argentina responded a year later with an order for two battleships of its own. The naval arms race unfolded over the next several years, with each country buying and ordering vessels that were slightly better than the previous ship, but the Argentines eventually pulled ahead with the acquisition of multiple \"Garibaldi\"-class cruisers.\n\nThe race ended in 1902 with the British-arbitrated Pacts of May, which contained a binding naval-limiting agreement. Both governments sold or canceled the ships they had ordered, and three major warships were mostly disarmed to balance the fleets. The pacts proved to be the answer to the Argentine and Chilean disputes, as the countries enjoyed a period of warm relations. This did not last, though, as the Brazilian government's attempt to rebuild its own naval forces sparked another naval arms race, involving all three countries' orders for revolutionary new dreadnoughts, powerful battleships whose capabilities far outstripped older vessels in the world's navies.\n\nConflicting Argentine and Chilean claims to Patagonia, the southernmost region in South America, had been causing tension between the two countries since the 1840s. Both countries were incapable of enforcing these claims with a seaborne force, though: in 1860, the Chileans had only five small vessels, while the Argentine Navy had no seagoing ships. These attitudes quickly changed when the circumstances warranted; when Chile joined Peru against Spain in the Chincha Islands War, the Spaniards blockaded and bombarded Valparaíso, leading the Chilean government to strengthen the navy. The Argentine government, under President Domingo Sarmiento, decided to build a navy in the 1870s to counter Brazilian naval acquisitions. Two large monitors, \"Los Andes\" and \"El Plata\", were ordered from Laird Brothers, a British company, along with two gunboats. They were delivered in 1874 and 1875.\n\nThe Patagonian tensions heightened in 1872 and 1878, when Chilean warships seized merchant ships which had been licensed to operate in the disputed area by the Argentine government. An Argentine warship did the same to a Chilean-licensed American ship in 1877. This action nearly led to war in November 1878, when the Argentines dispatched a squadron of warships to the Santa Cruz River. The Chilean Navy responded in kind, and war was only avoided by a hastily signed treaty.\n\nBoth countries were incapable of enforcing these claims with a seaborne force in the next few years, as the Argentines were occupied with internal military operations against the indigenous population (1870–84), and the Chileans involved in the War of the Pacific (\"Guerra del Pacífico\", 1879–83) against Bolivia and Peru. Still, several warships were ordered by both nations: the Argentines commissioned a central battery ironclad, , and a protected cruiser, , in 1880 and 1885, respectively. For their part, the Chileans ordered a protected cruiser, , to bolster its fleet, which was centered on two central battery ironclads, and . With these ships, the Chilean Navy emerged from the War of the Pacific as the preeminent navy in the Americas, surpassing even the navy of the United States, which had fallen into steep decline after the American Civil War. The Chilean government utilized this advantage when it deployed \"Esmeralda\" to Panama in 1885 to block the U.S. from attempting to annex the region.\n\nThe Chilean government moved first to begin the naval arms race when it ordered a modern ironclad battleship, , two protected cruisers, and two torpedo boats from France and the United Kingdom. Bought with a £3,129,500 appropriation in the 1887 budget, the ships would have upset the balance of naval power in Latin America—while the Argentines had more vessels, the Chileans had larger warships with far more experienced crewmen. This purchase was made worse, from the Argentine perspective, by a large order for rifles, field guns, sabers, and carbines, enough to arm an 80,000-strong army. The Argentine government responded with two battleships— and , though they were individually smaller than their Chilean counterpart—and two protected cruisers, one purchased on the stocks in 1890 () and a new-build of the same design in 1891 (). The purchases were funded largely through export-related windfall, Chile through nitrates and Argentina through grain and cattle.\n\nThe Chilean Civil War (1891), rather than calming the naval ambitions of Chile, escalated them. In that conflict, the Chilean Navy played a significant role on the congressional side against the president and the army. The resulting victory of the congressional side and subsequent presidency of Admiral Jorge Montt led to a large increase in prestige and consequent funding for the navy. Argentine naval units assisted failed revolts in Argentina, but the continuing acrimony with and naval acquisitions of Chile meant this had little effect.\n\nThe Chilean government purchased a protected cruiser, , on the stocks in 1892, while the Argentines purchased one, , being built for the British in late 1893. The Chileans sold their oldest protected cruiser, \"Esmeralda\", in late 1894 to finance the order of an armored cruiser. This materialized in May 1895 with a new , along with four torpedo boats; a Brazilian protected cruiser, , was purchased while under construction in August 1895. The Argentines purchased an Italian armored cruiser, , on the stocks on 14 July 1895.\n\nIn April 1896, Chile ordered another armored cruiser, , and six torpedo boats. Naval historian Robert Scheina states that Argentina replied in the same month with , a near-sister ship to \"Garibaldi\" which was under construction in Italy. However, he notes that the small time lapse between the orders makes it difficult or impossible to know if this, the opposite, or either are true. As historian Jonathan Grant writes, the Argentines may have moved first to secure a definite, if momentarily tenuous, advantage over the Chilean Navy. In May 1898, the Chilean government found that the Argentines were planning on acquiring one, then two, \"Garibaldi\"-class cruisers ( and ). With tensions extremely high and war seemingly imminent, the two countries agreed to submit their boundary disputes to the British. They also signed pacts which led to the resolution of the Puna de Atacama dispute. As the former arbitration took much time, leaving that particular boundary dispute unresolved, the naval arms race quickly picked back up.\n\nThe Argentines ordered two additional armored cruisers that were similar but more powerful than the previous four. To counter them, the Chilean government ordered two new battleships, \"Constitución\" and \"Libertad\", using part of its gold reserve to pay for them. These battleships' fast speed would make them suitable for opposing the new Argentine armored cruisers. The Chileans also purchased the protected cruiser , which had been built on speculation, on the stocks in late 1901. The Argentines responded in May 1901 with an inquiry, possibly a full order, to Ansaldo for a new battleship design. This would mount a main battery and be capable of steaming at .\n\nThe increased tensions and near state of war between Argentina and Chile caused the British to push for a resolution, lest their economic interests in the region, which included the export of British goods and the import of Latin American raw materials, be disrupted. Talks were held in the Chilean capital, Santiago, between the British ambassador to Chile, the Argentine ambassador to Chile, and the Chilean foreign minister and President Germán Riesco. This led to the three Pacts of May on 28 May 1902, which ended the dispute. The third limited the naval armaments of both countries. Argentina and Chile were barred from acquiring any further warships for five years, unless they gave the other eighteen months of advance notice. The warships under construction were sold to the United Kingdom, with Chile's battleships becoming the , and Japan, with Argentina's final two armored cruisers becoming the . The two planned Argentine battleships were either never ordered or canceled, and \"Garibaldi\" and \"Pueyrredón\", along with Chile's \"Capitán Prat\", were disarmed with the exception of their main batteries, as the Argentine Navy had no crane capable of removing the armored cruiser's gun turrets.\n\nThe Argentine–Chilean naval arms race was extremely expensive for both countries. The Argentine government was able to purchase £4,534,800 worth of ships between 1890 and 1898 with large foreign loans, which were given to them despite the country's role in the Baring crisis of 1890. The government's total foreign debt reached 421 million gold pesos by 1896. As for Chile, it was forced to take out a £2 million pound loan in order to purchase Krupp weaponry, and this combined with its other loans led the banking industry to suspend loans to Chile until the diplomatic crisis with Argentina was solved. Both the Argentine President Julio Argentino Roca and American ambassador to Argentina William Paine Lord ascribed the ending of the arms race to the diminished credit of Argentina and Chile.\n\nBy all measures, the Pacts of May were an unmitigated success. Both Argentina and Chile enjoyed a period of lessened tensions, leaving the near state of war they were in, and the pacts ended their expensive naval buildups. However, the third major country in South America, Brazil, brought this to a crashing halt in 1904, when its congress passed a large naval construction plan. This culminated in 1907 with a Brazilian order for three \"dreadnoughts\", a new form of warship whose advanced armament and propulsion capabilities far outstripped older vessels in the world's navies. Two would be laid down immediately, with a third to follow. The Argentine and Chilean governments quickly moved to cancel the remaining months of the naval-limiting Pacts of May, and both eventually responded with orders for their own dreadnoughts.\n\n\n"}
{"id": "25657496", "url": "https://en.wikipedia.org/wiki?curid=25657496", "title": "Beef Products", "text": "Beef Products\n\nBeef Products Inc. (BPI) is an American meat processing company based in Dakota Dunes, South Dakota. Prior to high media visibility of its products, it was a major supplier to fast food chains, groceries and school lunch programs. It had three additional plants, which closed in 2012.\n\nBeef Products Inc. was established in 1981 by its current CEO Eldon Roth.\n\nIn 2007, after the USDA reviewed BPI's processing technique, the company was exempted from routine testing of hamburger meat.\n\nIn December 2009, \"The New York Times\" reported that as early as 2003, school lunch officials and other customers had complained that the product tasted and smelled like ammonia, after which the company devised a plan to make a less alkaline version. The USDA determined that at least some of BPI's product was no longer receiving \"the full lethality treatment.\" NYT reported that BPI's products had tested positive for E. coli three times and salmonella 48 times since 2005. This prompted the USDA to revoke the exemption and conduct a review of the company's practices.\n\nIn July 2011, after widespread coverage of an unrelated linked to sprouts, Beef Products Inc. began voluntarily testing its beef products for six additional strains of E. coli contamination because the FDA had not taken any formal actions for increased safety actions. The testing began at one of its plants, with a planned expansion to the rest of its U.S. plants when the test kit manufacturer could increase its production to meet the demand.\n\nIn 2012, after a series of ABC News reports, concern amongst the public led McDonald's, Burger King, Taco Bell, Wal-Mart, Safeway, and several other grocery stores to abandon the product. Company officials suspended production at three of its four plants. The United States Department of Agriculture issued a statement supporting the product's safety, and the company launched a public relations offensive with the help of governors Rick Perry, Terry Branstad, and Sam Brownback, who joined ABC News on a tour of the remaining plant.\n\nBeef Products Inc. closed its facilities in Amarillo, Texas; Garden City, Kansas; and Waterloo, Iowa on 25 May 2012.\n\nOn 13 September 2012, the company announced it would be suing ABC News for $1.2 billion in a defamation lawsuit.\n\nBPI was a major supplier to McDonald's and Burger King, as well as restaurants and grocery stores, and its products were reportedly used in 75% of the United States' hamburger patties in 2008. The School Lunch Program, another large buyer of Beef Product's goods, used about 5.5 million pounds in 2009.\n\nBeef Products Inc. is the creator of a product called \"lean finely textured beef,\" also known as \"pink slime.\" The latter term was first used in 2002 by a Food Safety Inspection Service worker.\n\nIn 2002, it patented a process that turns materials that had previously gone for pet food or oil into products for human consumption. In this process, beef trimmings are warmed, put through a centrifuge to remove fat, then treated with ammonia to increase pH and kill bacteria.\n\nThe product is found as a lean meat source which has been added to ground beef, constituting up to 25 percent of the final product. This process is approved by the U.S. Department of Agriculture and Food and Drug Administration.\n\nCarol Tucker Foreman, director of the Food Safety Institute for the Consumer Federation of America, and Nancy Donley, president of the industry-funded group Safe Tables Our Priority, are strong backers of this technology-based approach to food safety. Journalists, however, have questioned the safety of meat treated with the process.\n"}
{"id": "5874930", "url": "https://en.wikipedia.org/wiki?curid=5874930", "title": "Blast radius", "text": "Blast radius\n\nA blast radius is the distance from the source that will be affected when an explosion occurs. A blast radius is often associated with bombs, mines, explosive projectiles (propelled grenades), and other weapons with an explosive charge.\n\nFor instance, a 2000 pound Mk-84 bomb has a blast radius of 400 yards (365 metres).\n\nOf late, the term is being used for impact analysis in technology parlance. For example, when a particular IT service fails, the users, customers, other dependent services that are affected, fall into Blast Radius.\n"}
{"id": "12220389", "url": "https://en.wikipedia.org/wiki?curid=12220389", "title": "CHP Directive", "text": "CHP Directive\n\nThis refers to the Directive on the promotion of cogeneration based on a useful heat demand in the internal energy market and amending Directive 92/42/EEC, officially 2004/8/EC and popularly better known as the 'Combined Heat and Power (CHP) Directive'.\n\nIt is a European Union directive for promoting the use of cogeneration in order to increase the energy efficiency and improve the security of supply of energy. This is intended to be achieved by creating a framework for the promotion and development of high efficiency cogeneration.\n\nThe directive entered into force in February 2004 and member states have been obliged to begin its implementation since 2006. (however due to delays resulting out of the comitology process, member states had to adopt the first obligations of the directive by 6 August 2007.)\n\nIt is intended that the directive will have a significant impact on the legislation and the diffusion of CHP/cogeneration and district heating within the member states of the European Union.\n\nIn summary, the Member States are obliged to produce reports covering their analysis of the state of CHP in their own countries, to promote CHP and show what is being done to promote it, to report on and remove barriers, and to track progress of high-efficiency cogeneration within the energy market.\n\nThe directive comes under and is administered by The European Commission’s Directorate-General for Energy\n\nEU27 – Member States’ national support schemes available by categories\nRES = Renewable energy sources\nNG = Natural gas\nCertificates: W = White certificates, R = Red certificates, Y = Yellow certificates, G = Green certificates\nNotes:\n\n\n"}
{"id": "13652080", "url": "https://en.wikipedia.org/wiki?curid=13652080", "title": "Carrot harvester", "text": "Carrot harvester\n\nA carrot harvester is an agricultural machine for harvesting carrots. Carrot harvesters are either top lifters or share lifters and may be tractor mounted, trailed behind a tractor or self-propelled. The machine typically harvests between one and six rows of carrots at once.\n\nThe two types of harvesters differ in how they get the carrots from the ground.\n\nTop lifters use rubber belts to grab the green tops of the carrot plant and pull them from the soil. A share pushes under the carrot root and loosens the plant.\n\nThe belt takes the carrots, with tops, in to the machine where the tops are cut off and sent along a waste path and dropped back on to the field.\n\nA share lifter uses a share to get the carrots out of the ground from underneath. The machine must be preceded by a topper to cut the green tops off the carrot plants. The carrots travel along a longer web to separate out the soil.\n\nThe carrot roots travel along one or more webs to remove most of the soil attached to the carrot. The carrots are collected either in a storage tank on the machine or in a trailer pulled alongside the machine by another tractor.\n"}
{"id": "33987543", "url": "https://en.wikipedia.org/wiki?curid=33987543", "title": "Clevo", "text": "Clevo\n\nClevo is an OEM/ODM computer manufacturer which produces laptop computers exclusively. They manufacture and sell complete laptops under their own brand; they also sell barebone laptops chassis (barebooks) to value-added resellers who build customized laptops for individual customers.\n\nClevo was founded in 1983. In 1987, the company established its laptop computer business, with production starting in 1990. The company was first listed on the Taiwan Stock Exchange in 1997. In the year of 1999, Clevo merged with their subsidiary, Kapok, to increase efficiency. On August 2002, Clevo had built a new factory in Kunshan, China.\n\nClevo has been ranked as fourth among Taiwanese exporters, marketing its products in over 50 countries. The company has also founded several service centers in Canada, Germany, Great Britain, China, Taiwan, South Korea and the United States. These centers serve various businesses, ranging in size from small to multinational, with a variety of product selections in either small or large quantities. Clevo focuses its business on designing, developing, manufacturing, and distributing electronic equipment and laptops.\n\n"}
{"id": "47000835", "url": "https://en.wikipedia.org/wiki?curid=47000835", "title": "Computer Engineers Association of Spain", "text": "Computer Engineers Association of Spain\n\nThe Computer Engineers Association of Spain (ATI, \"Asociación de Técnicos de Informática\" in Spanish) is a non-profit association of professionals and students from the sector of Information and Communications Technology (ICT). It is based in Spain, where it is established through a number of Chapters. Founded in 1967, it is the most veteran association in the ICT profession in Spain, with the main headquarters in Barcelona and headquarters in Madrid also.\n\nATI publishes in Spanish the magazine , the oldest magazine in Spain about computing, and also . From 2000 to 2011 publishes also the e-magazine in English UPGRADE: The European Journal for the Informatics Professional, commissioned by CEPIS (Council for European Professional Informatic Societies).\n\nATI has sixteen working groups covering different areas of the ICT sector and takes part, either as organizer or collaborator, in several events in this field.\n\nATI is the Spanish representative in International Federation for Information Processing (IFIP), and represents Spanish computer professionals too in CEPIS, an organization from which ATI is a founding member. ATI also has a collaboration agreement with Association for Computing Machinery (ACM).\n\nIn Spain, ATI has established collaboration agreements with \"Ada Spain\", \"ASTIC\", \"Hispalinux\", \"AI2\" and \"RITSI (Asociación Nacional de Estudiantes de Ingenierías e Ingenierías Técnicas en Informática)\".\n\nATI expresses its view about different matters (Libre Software, university degrees on Computer Science, Private copying levy, etc.) through communications, press releases and editorial pages at \"Novática\" magazine.\n\n"}
{"id": "28202859", "url": "https://en.wikipedia.org/wiki?curid=28202859", "title": "Convenient number", "text": "Convenient number\n\nThe concept of convenient numbers is related to that of preferred numbers. A structure is defined to build a set of numbers that are convenient for use by humans in counting or measuring.\n\nThe National Bureau of Standards (NBS) (which was later renamed to the National Institute of Standards and Technology (NIST)) defined a set of convenient numbers during the 1970s when it was developing procedures for metrication in the United States. The NBS technical note describes that system of convenient metric values as the 1-2-5 series in reverse, with assigned preferences for those numbers which are multiples of 5, 2, and 1 (plus their powers of 10), excluding linear dimensions above 100 mm (because such measurements are defined by another set of rules), from which the Schedule of Convenient Numbers Between 10 and 100 below is reproduced.\n\nThe NBS technical note also states that \"Basically, integers are more convenient than expressions which include decimal parts [decimal fractions]. Furthermore, where measuring devices are used, values which represent numbered subdivisions on such instruments are more useful than values which have to be interpolated. For example, where a tape or a scale is graduated in intervals of 5, any value that represents a multiple of 5 is more \"convenient\" to measure or verify than one which is not. In addition, where operations involve the subdivision of quantities into two or more equal parts, any number that is highly divisible has an explicit advantage.\"\n\nNotes:\nThe Technical Note also states, \"In the practical application of a \"convenient numbers approach\" to the selection of suitable metric values, it is desirable to start with the highest possible preference and then to gradually refine the difference until an acceptable and convenient metric value has been found.\"\n\n"}
{"id": "17886240", "url": "https://en.wikipedia.org/wiki?curid=17886240", "title": "Crawl space vent", "text": "Crawl space vent\n\nA crawl space vent is a penetration in the wall of a building which allows air to circulate from the crawl space beneath the building to the exterior.\n\nDry rot and other conditions detrimental to buildings (particularly wood and timber structures) can develop in enclosed spaces. Providing adequate ventilation is thought to reduce the occurrence of these problems. Crawl space vents are openings in the wall which allow air movement. Such vents are usually fitted with metal grating, mesh, or louvers which can block the movement of rodents and vermin but generally not insects such as termites and carpenter ants. One common rule is to provide vents in cross sectional area equal to 1/150 of the floor area served.\n\nModern crawl space thinking has reconsidered the usage of crawl space vents in the home. While crawl space vents do allow outside air to ventilate into the home, the ability of that air to dry out the crawl space is debatable. In areas with humid summers, during the summer months, the air vented into a crawl space will be humid, and as it enters the crawl space, which has been cooled naturally by the earth, the relative humidity of the air will rise. In those cases, crawl space vents can even increase the humidity level of a crawl space and lead to condensation on cool surfaces within, such as metal and wood. In the winter, crawl space vents should be shut off entirely, to keep out the cold winter air which can cool hot water pipes, furnaces, and water heaters stored within. During rainy weather, crawl space vents bring wet air into the crawl space, which will not dry the space effectively.\n\nCrawl space vents also contribute to the \"stack effect\", a phenomenon by which warm air rises in the home and exits from the upper levels. This creates a vacuum in the lower levels and can pull crawl space air upwards into the home. By sealing off all openings in the crawl space, including the vents, unconditioned outside air is kept out of the home.\n"}
{"id": "549779", "url": "https://en.wikipedia.org/wiki?curid=549779", "title": "Cunife", "text": "Cunife\n\nCunife is an alloy of copper (Cu), nickel (Ni), iron (Fe), and in some cases cobalt (Co). The alloy has the same linear coefficient of expansion as certain types of glass, and thus makes an ideal material for the lead out wires in light bulbs and thermionic valves. Fernico exhibits a similar property. It is a magnetic alloy and can be used for making magnets.\n\nCunife has a magnetic coercivity of several hundred oersteds. Unlike most high coercivity magnetic materials which are hard and brittle and need to be cast into shape, cunife can be drawn into thin wires. Wires as thin as five thou can be produced this way.\n\n\nAt a certain point, Fender Musical Instruments Corporation used Cunife magnets in their Wide-Range humbucking pickups, however they discontinued use, due to Cunife being hard to source.\n\n"}
{"id": "37488980", "url": "https://en.wikipedia.org/wiki?curid=37488980", "title": "Cybernetics and Systems", "text": "Cybernetics and Systems\n\nCybernetics and Systems is a peer-reviewed scientific journal of cybernetics and systems science, including artificial intelligence, computer science, cybernetics, human computer intelligence, information and communication technology, machine learning, and robotics. The journal was established in 1971 as \"Journal of Cybernetics\" and obtained its current title in 1980. It is published by Taylor & Francis in cooperation with the Austrian Society for Cybernetic Studies and the editor-in-chief is Robert Trappl.\n\nCybernetics and Systems is abstracted and indexed in:\n\nAccording to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 1.434, ranking it 12th out of 22 journals in the category \"Computer Science, Cybernetics\"\n"}
{"id": "13303011", "url": "https://en.wikipedia.org/wiki?curid=13303011", "title": "Daintree Networks", "text": "Daintree Networks\n\nDaintree Networks, Inc. is a building automation company that provides wireless control systems for commercial and industrial buildings. Commercial building control and lighting control systems can reduce energy consumption, cost, and carbon footprint, and comply with \"green\" building regulations.\n\nDaintree's ControlScope wireless control includes switches, sensors, LED drivers, programmable thermostas, and plug load controllers. Wireless communication is achieved either by wirelessly adapting traditional wired devices (such as sensors), or by building wireless communications capabilities directly into devices.\n\nDaintree has produced a design verification and operational support tool, the Sensor Network Analyzer (SNA), which supports wireless embedded technologies including IEEE 802.15.4, ZigBee, ZigBee RF4CE, 6LoWPAN, JenNet (from Jennic Limited), SimpliciTI (from Texas Instruments), and Synkro (from Freescale Semiconductor).\n\nFounded in 2003, Daintree is headquartered in Los Altos, California with an R&D lab in Melbourne, Australia.\n\nDaintree was founded in 2003 by Bill Wood, who had previously worked as a General Manager for Agilent Technologies, and prior to that at Hewlett-Packard.\n\nDaintree managers have previously held roles within wireless standards bodies, including chair of several working groups within the ZigBee Alliance.\n\nIn 2003, when many wireless technologies were new, Daintree provided design verification and operational support tools for wireless embedded developers. In 2007 the company began developing and delivering wireless systems for specific purposes; by 2009 it had narrowed its focus to lighting and building control.\n\nOn April 21, 2016, Current, an energy management startup within GE, acquired Daintree Networks for $77 million to combine its open-standard wireless network with GE's open source platform Predix to offer a new energy management system to businesses.\n\n\nZigBee is a specification for a suite of high-level communication protocols using small, low-power digital radios based on the IEEE 802.15.4 standard for wireless personal area networks (WPANs). ZigBee is targeted at RF applications that require a low data rate, long battery life, and secure networking.\n\nDaintree is an active member of the ZigBee Alliance, and their Sensor Network Analyzer is used by the ZigBee Alliance for product certification.\n\n"}
{"id": "206242", "url": "https://en.wikipedia.org/wiki?curid=206242", "title": "Differential (mechanical device)", "text": "Differential (mechanical device)\n\nA differential is a gear train with three shafts that has the property that the rotational speed of one shaft is the average of the speeds of the others, or a fixed multiple of that average.\n\nIn automobiles and other wheeled vehicles, the differential allows the outer drive wheel to rotate faster than the inner drive wheel during a turn. This is necessary when the vehicle turns, making the wheel that is travelling around the outside of the turning curve roll farther and faster than the other. The average of the rotational speed of the two driving wheels equals the input rotational speed of the drive shaft. An increase in the speed of one wheel is balanced by a decrease in the speed of the other.\n\nWhen used in this way, a differential couples the longitudinal input propellor shaft to the pinion, which in turn drives the transverse ring gear of the differential. This also usually works as reduction gearing. On rear wheel drive vehicles the differential may connect to half-shafts inside an axle housing, or drive shafts that connect to the rear driving wheels. Front wheel drive vehicles tend to have the engine crankshaft and the gearbox shafts transverse, and with the pinion on the end of the main-shaft of the gearbox and the differential enclosed in the same housing as the gearbox. There are individual drive-shafts to each wheel. A differential consists of one input, the drive shaft, and two outputs which are the two drive wheels, however the rotation of the drive wheels are coupled to each other by their connection to the roadway. Under normal conditions, with small tyre slip, the ratio of the speeds of the two driving wheels is defined by the ratio of the radii of the paths around which the two wheels are rolling, which in turn is determined by the track-width of the vehicle (the distance between the driving wheels) and the radius of the turn.\n\nNon-automotive uses of differentials include performing analog arithmetic. Two of the differential's three shafts are made to rotate through angles that represent (are proportional to) two numbers, and the angle of the third shaft's rotation represents the sum or difference of the two input numbers. The earliest known use of a differential gear is in the Antikythera mechanism, circa 80 BCE, which used a differential gear to control a small sphere representing the moon from the difference between the sun and moon position pointers. The ball was painted black and white in hemispheres, and graphically showed the phase of the moon at a particular point in time. An equation clock that used a differential for addition was made in 1720. In the 20th Century, large assemblies of many differentials were used as analog computers, calculating, for example, the direction in which a gun should be aimed. However, the development of electronic digital computers has made these uses of differentials obsolete. Military uses may still exist, for example, for a hypothetical computer designed to survive an electromagnetic pulse. Practically all the differentials that are now made are used in automobiles and similar vehicles.\n\nThere are many claims to the invention of the differential gear, but it is possible that it was known, at least in some places, in ancient times. Some historical milestones of the differential include:\n\n\nAn epicyclic differential can use epicyclic gearing to split and apportion torque asymmetrically between the front and rear axles. An epicyclic differential is at the heart of the Toyota Prius automotive drive train, where it interconnects the engine, motor-generators, and the drive wheels (which have a second differential for splitting torque as usual). It has the advantage of being relatively compact along the length of its axis (that is, the sun gear shaft).\n\nEpicyclic gears are also called planetary gears because the axes of the planet gears revolve around the common axis of the sun and ring gears that they mesh with and roll between. In the image, the yellow shaft carries the sun gear which is almost hidden. The blue gears are called planet gears and the pink gear is the ring gear or annulus.\n\nRing gears are also used in starter motors.\n\nA spur-gear differential has two equal-sized spur gears, one for each half-shaft, with a space between them. Instead of the Bevel gear, also known as a miter gear, assembly (the \"spider\") at the centre of the differential, there is a rotating carrier on the same axis as the two shafts. Torque from a prime mover or transmission, such as the drive shaft of a car, rotates this carrier.\n\nMounted in this carrier are one or more pairs of identical pinions, generally longer than their diameters, and typically smaller than the spur gears on the individual half-shafts. Each pinion pair rotates freely on pins supported by the carrier. Furthermore, the pinion pairs are displaced axially, such that they mesh only for the part of their length between the two spur gears, and rotate in opposite directions. The remaining length of a given pinion meshes with the nearer spur gear on its axle. Therefore, each pinion couples that spur gear to the other pinion, and in turn, the other spur gear, so that when the drive shaft rotates the carrier, its relationship to the gears for the individual wheel axles is the same as that in a bevel-gear differential.\n\nA spur gear differential is constructed from two identical coaxial epicyclic gear trains assembled with a single carrier such that their planet gears are engaged. This forms a planetary gear train with a fixed carrier train ratio \"R = -1\".\n\nIn this case, the fundamental formula for the planetary gear train yields,\nor\n\nThus, the angular velocity of the carrier of a spur gear differential is the average of the angular velocities of the sun and annular gears.\n\nIn discussing the spur gear differential, the use of the term \"annular gear\" is a convenient way to distinguish the sun gears of the two epicyclic gear trains. The second sun gear serves the same purpose as the annular gear of a simple planetary gear train, but clearly does not have the internal gear mate that is typical of an annular gear.\n\nChinese south-pointing chariots may also have been very early applications of differentials. The chariot had a pointer which constantly pointed to the south, no matter how the chariot turned as it travelled. It could therefore be used as a type of compass. It is widely thought that a differential mechanism responded to any difference between the speeds of rotation of the two wheels of the chariot, and turned the pointer appropriately. However, the mechanism was not precise enough, and, after a few miles of travel, the dial could have very well been pointing in the complete opposite direction.\n\nThe earliest definitely verified use of a differential was in a clock made by Joseph Williamson in 1720. It employed a differential to add the equation of time to local mean time, as determined by the clock mechanism, to produce solar time, which would have been the same as the reading of a sundial. During the 18th Century, sundials were considered to show the \"correct\" time, so an ordinary clock would frequently have to be readjusted, even if it worked perfectly, because of seasonal variations in the equation of time. Williamson's and other equation clocks showed sundial time without needing readjustment. Nowadays, we consider clocks to be \"correct\" and sundials usually incorrect, so many sundials carry instructions about how to use their readings to obtain clock time.\n\nIn the first half of the twentieth century, mechanical analog computers, called differential analyzers, were constructed that used differential gear trains to perform addition and subtraction. The U.S. Navy Mk.1 gun fire control computer used about 160 differentials of the bevel-gear type.\n\nA differential gear train can be used to allow a difference between two input axles. Mills often used such gears to apply torque in the required axis. Differentials are also used in this way in watchmaking to link two separate regulating systems with the aim of averaging out errors. Greubel Forsey use a differential to link two double tourbillon systems in their Quadruple Differential Tourbillon.\n\nA vehicle with two drive wheels has the problem that when it turns a corner the drive wheels must rotate at different speeds to maintain traction. The automotive differential is designed to drive a pair of wheels while allowing them to rotate at different speeds. In vehicles without a differential, such as karts, both driving wheels are forced to rotate at the same speed, usually on a common axle driven by a simple chain-drive mechanism.\n\nWhen cornering, the inner wheel travels a shorter distance than the outer wheel, so without a differential either the inner wheel rotates too quickly or the outer wheel rotates too slowly, which results in difficult and unpredictable handling, damage to tires and roads, and strain on (or possible failure of) the drivetrain.\n\nIn rear-wheel drive automobiles the central drive shaft (or prop shaft) engages the differential through a hypoid gear (ring and pinion). The ring gear is mounted on the carrier of the planetary chain that forms the differential. This hypoid gear is a bevel gear that changes the direction of the drive rotation.\n\nThe following description of a differential applies to a traditional rear-wheel-drive car or truck with an open or limited slip differential combined with a reduction gearset using bevel gears (these are not strictly necessary – see spur-gear differential):\n\nThus, for example, if the car is making a turn to the right, the main ring gear may make 10 full rotations. During that time, the left wheel will make more rotations because it has farther to travel, and the right wheel will make fewer rotations as it has less distance to travel. The sun gears (which drive the axle half-shafts) will rotate at different speeds relative to the ring gear (one faster, one slower) by, say, 2 full turns each (4 full turns relative to each other), resulting in the left wheel making 12 rotations, and the right wheel making 8 rotations.\n\nThe rotation of the ring gear is always the average of the rotations of the side sun gears. This is why if the driven roadwheels are lifted clear of the ground with the engine off, and the drive shaft is held (say, leaving the transmission in gear preventing the ring gear from turning inside the differential), manually rotating one driven roadwheel causes the opposite roadwheel to rotate in the opposite direction by the same amount.\n\nWhen the vehicle is traveling in a straight line there will be no differential movement of the planetary system of gears other than the minute movements necessary to compensate for slight differences in wheel diameter, undulations in the road which make for a longer or shorter wheel path, etc.\n\nOne undesirable side effect of an open differential is that it can limit traction under less than ideal conditions. The amount of traction required to propel the vehicle at any given moment depends on the load at that instant—how heavy the vehicle is, how much drag and friction there is, the gradient of the road, the vehicle's momentum, and so on.\n\nThe torque applied to each driving wheel is the result of the engine, transmission, and drive axle applying a twisting force against the resistance of the traction at that roadwheel. In lower gears, and thus at lower speeds, and unless the load is exceptionally high, the drivetrain can supply as much torque as necessary, so the limiting factor becomes the traction under each wheel. It is therefore convenient to define traction as the amount of force that can be transmitted between the tire and the road surface before the wheel starts to slip. If the torque applied to one of the drive wheels exceeds the threshold of traction, then that wheel will spin, and thus provide torque only at the other driven wheel equal to the sliding friction at the slipping wheel. The reduced net traction may still be enough to propel the vehicle slowly.\n\nAn open (non-locking or otherwise traction-aided) differential always supplies close to equal torque to each side. To illustrate how this can limit torque applied to the driving wheels, imagine a simple rear-wheel drive vehicle, with one rear roadwheel on asphalt with good grip, and the other on a patch of slippery ice. It takes very little torque to spin the side on slippery ice, and because a differential splits torque equally to each side, the torque that is applied to the side that is on asphalt is limited to this amount.\n\nBased on the load, gradient, etc., the vehicle requires a certain amount of torque applied to the drive wheels to move forward. Since an open differential limits total torque applied to both drive wheels to the amount used by the lower traction wheel multiplied by 2, when one wheel is on a slippery surface, the total torque applied to the driving wheels may be lower than the minimum torque required for vehicle propulsion.\n\nA proposed alternate way to distribute power to the wheels, is to use the concept of a \"gearless differential\", about which a review has been reported by Provatidis, but the various configurations seem to correspond either to the \"sliding pins and cams\" type, such as the ZF B-70 available on early Volkswagens, or are a variation of the ball differential.\n\nMany newer vehicles feature traction control, which partially mitigates the poor traction characteristics of an open differential by using the anti-lock braking system to limit or stop the slippage of the low traction wheel, increasing the torque that can be applied to the opposite wheel. While not as effective as a traction-aided differential, it is better than a simple mechanical open differential with no traction assistance.\n\nA relatively new technology is the electronically controlled 'active differential'. An electronic control unit (ECU) uses inputs from multiple sensors, including yaw rate, steering input angle, and lateral acceleration—and adjusts the distribution of torque to compensate for undesirable handling behaviours such as understeer.\n\nFully integrated active differentials are used on the Ferrari F430, Mitsubishi Lancer Evolution, and on the rear wheels in the Acura RL. A version manufactured by ZF is also being offered on the B8 chassis Audi S4 and Audi A4. The Volkswagen Golf GTI Mk7 in Performance trim also has an electronically controlled front-axle transverse differential lock, also known as VAQ.\n\n\n"}
{"id": "12226371", "url": "https://en.wikipedia.org/wiki?curid=12226371", "title": "Dry-ice blasting", "text": "Dry-ice blasting\n\nDry ice blasting is a form of carbon dioxide cleaning, where dry ice, the solid form of carbon dioxide, is accelerated in a pressurized air stream and directed at a surface in order to clean it.\n\nThe method is similar to other forms of media blasting such as sand blasting, plastic bead blasting, or sodablasting in that it cleans surfaces using a media accelerated in a pressurized air stream, but dry ice blasting uses dry ice as the blasting medium. Dry ice blasting is nonabrasive, non-conductive, nonflammable, and non-toxic.\n\nDry ice blasting is an environmentally responsible cleaning method. Dry ice is made of reclaimed carbon dioxide that is produced from other industrial processes, does not add additional greenhouse gases to the atmosphere, and is an approved media by the EPA, FDA and USDA. It also reduces or eliminates employee exposure to the use of chemical cleaning agents.\n\nCompared to other media blasting methods, dry ice blasting does not create secondary waste or chemical residues as dry ice sublimates, or converts back to a gaseous state, when it hits the surface that is being cleaned. Dry ice blasting does not require clean-up of a blasting medium. The waste products, which includes just the dislodged media, can be swept up, vacuumed or washed away depending on the containment.\n\nDry ice blasting involves propelling pellets at extremely high speeds. The actual dry ice pellets are quite soft, and much less dense than other media used in blast-cleaning (i.e. sand or plastic pellets). Upon impact, the pellet sublimates almost immediately, transferring minimal kinetic energy to the surface on impact and producing minimal abrasion. The sublimation process absorbs a large volume of heat from the surface, producing shear stresses due to thermal shock. This is assumed to improve cleaning as the top layer of dirt or contaminant is expected to transfer more heat than the underlying substrate and flake off more easily. The efficiency and effectiveness of this process depends on the thermal conductivity of the substrate and contaminant. The rapid change in state from solid to gas also causes microscopic shock waves, which are also thought to assist in removing the contaminant.\n\nThe dry ice used can be in solid pellet form or shaved from a larger block of ice. The shaved ice block produces a less dense ice medium and is more delicate than the solid pellet system. In addition, pellets may be made by either compressing dry ice snow, or using tanks of liquid CO2 to form solid pellets. Dry ice made with compressed snow breaks apart more easily and is not as aggressive for cleaning.\n\nDry ice blasting technology can trace its roots to conventional abrasive blasting. The differences between an abrasive-blasting machine and a dry ice blasting machine are in how they handle the blast media. Unlike sand or other media, dry ice is generally used at its sublimation temperature. Other differences include systems for preventing the ice from forming snowball-like jams, and different materials to allow operation at very low temperatures.\n\nThere are two methods of dry ice blasting, two-hose and single hose. The single hose system is more aggressive for cleaning, since the particles are accelerated to faster speeds.\n\nTwo-hose dry ice blasting was developed before the single-hose system. The two-hose dry ice blasting approach is very similar to a suction-feed abrasive blast system. Compressed air is delivered in one hose, and ice pellets are sucked out of a second hose by the venturi effect. Compared to a single-hose system, the two-hose system delivers ice particles less forcefully (approximately 5% for a given air supply). For a given amount of compressed air, two-hose systems can have less vertical distance between the machine and applicator. For most systems available today this limit is well in excess of 7.5 m (25 feet). Two-hose systems are generally cheaper to produce due to a simpler delivery system. These systems are rarely seen today as they are less efficient in most applications. Their principal advantage is in allowing finer particles of ice to be delivered to the applicator as the late combination of warm air with cold ice results in less sublimation in the hose. These systems allow for more delicate surfaces to be cleaned such as in semiconductor.\n\nThe first dry ice blasting machine to be commercialized was a single-hose system. It was developed by Cold Jet, LLC in 1986, and uses a single hose to deliver air blasts and dry ice. Single-hose dry ice blasters share many of the advantages of single-hose abrasive-blast systems. To avoid the potential dangers of a pressurized hopper, single-hose dry ice blasters make use of a quickly cycling airlock. The single-hose system can use a longer hose than its double-hose counterpart without a significant drop in pressure when the ice leaves the hose. The additional power comes at the cost of increased complexity. Single-hose systems are used where more aggressive cleaning is an advantage. This allows heavier build-up to be cleaned and allows moderate buildup to be cleaned faster.\n\nDry ice blasting is utilized in many different types of industries. The unique properties of dry ice make it an ideal cleaning solution in many commercial and manufacturing settings.\n\nDry ice blasting can clean numerous objects with differing, complex geometries at once, which is why cleaning plastic and rubber molds is a main application for the technology. Dry ice replaces traditional cleaning methods that rely on manual scrubbing and the use of chemical cleaning agents. Dry ice blasting cleans the molds in-place at operating temperature, which eliminates the need to shut production down for cleaning.\n\nDry ice blasting can be used to clean food processing equipment and the Food Standards Agency documented the process to effectively decontaminate surfaces of \"Salmonella enteritidis\", \"E. coli\", and \"Listeria monocytogenes\" such that these microorganisms are not detectable using conventional microbiological methods. It may also be used to clean some equipment without disassembly and without producing fire or electrical hazards. The EPA recommends dry ice blasting as an alternative to many types of solvent-based cleaning.\n\nThe cleaning process is used for disaster remediation including mold, smoke, fire, and water damage.\n\nDue to the nonabrasive nature of dry ice and the absence of secondary waste from the cleaning process, dry ice blasting is used in conservation and historical preservation projects. The cleaning process was used in the conservation of the USS Monitor and the Philadelphia Museum of Art.\n\nDue to the blast media sublimating without residue, dry ice blasting finds use in the semiconductor, aerospace, and medical device manufacturing industries.\n\nThe cleaning process is also used in other manufacturing settings, such as cleaning production equipment on automated weld lines, cleaning composite tooling, cleaning industrial printing presses, cleaning molds and equipment used in foundries, and to clean equipment and tooling in onshore and offshore environments in the oil and gas industry.\n\nDry ice blasting is also used to deburr and deflash parts and surface preparation prior to painting.\n\nCarbon dioxide is increasingly toxic starting at concentrations above 1%, and can also displace oxygen resulting in asphyxia if equipment is not used in a ventilated area. In addition, because carbon dioxide is heavier than air, exhaust vents are required to be at or near ground level to efficiently remove the gas. At normal pressure dry ice is and must be handled with insulated gloves. Eye and ear protection are required to safely use dry ice cleaning equipment.\n\nIt is believed the US Navy, in 1945, were the first to experiment with dry ice blasting. They were interested in using the technology for various degreasing applications.\n\nIn 1959, Unilever filed a patent for using dry ice blasting (or water-ice blasting, or some combination of the two) as a method of removing meat from bone.\n\nIn 1971, Chemotronics International Inc. filed a patent for using dry ice blasting for the purposes of deburring and deflashing.\n\nA patent for dry ice blasting was filed by Lockheed Martin in 1974.\n\nThe first patents regarding development and design of modern-day single-hose dry ice blasting technology were awarded to David Moore of Cold Jet, LLC in 1986, 1988 ( and ).\n\n\n"}
{"id": "11151417", "url": "https://en.wikipedia.org/wiki?curid=11151417", "title": "Eastern Trough Area Project", "text": "Eastern Trough Area Project\n\nThe Eastern Trough Area Project, commonly known as ETAP, is a network of nine smaller oil and gas fields in the Central North Sea covering an area up to 35 km in diameter. There are a total of nine different fields, six operated by BP and another three operated by Shell, and together, they are a rich mix of geology, chemistry, technology and equity arrangements.\n\nThe ETAP complex was sanctioned for development in 1995 with first hydrocarbons produced in 1998. The original development included Marnock, Mungo, Monan and Machar from BP and Heron, Egret, Skua from Shell. In 2002, BP brought Mirren and Madoes on stream. With these nine fields, the total reserves of ETAP are approximately of oil, of natural gas condensate and of natural gas.\n\nA single central processing facility (CPF) sits over the Marnock field and serves as a hub for all production and operations of the asset including all processing and export and a base for expedition to the Mungo NUI. The CPF consists of separate platforms for operations and accommodation linked by two 60 m bridges. The Processing, drilling and Riser platform (PdR), contains the process plant and the export lines, a riser area to receive production fluids from the other ETAP fields and the wellheads of Marnock. The Quarters and Utilities platform (QU) provides accommodation for up to 117 personnel operating this platform or travelling onwards to the Mungo NUI. This partitioning of accommodation and operations into two platforms, adds an extra element of safety, a particular concern for the designers coming only a few years after the Cullen report on the Piper Alpha disaster.\n\nLiquids are exported to Kinneil at Grangemouth through the Forties pipeline system. Gas is exported by the Central Area Transmission System to Teesside.\n\nApart from Mungo, which has surface wellheads on a NUI, all other fields use subsea tie-backs.\n\nA tenth field, Fidditch, is currently under development by BP. (which has now been put on hold due to the global economic downturn)\n\nThe Marnock field is located in UKCS block 22/24 and is named after Saint Marnock. It is a high pressure, high temperature gas condensate field with initial reservoir pressure of 9000psi. Estimated recoverable reserves are 600 billion scf and of condensate. Marnock produces directly to surface wellheads on the CPF. It is operated by BP in partnership with Shell, Esso and AGIP. The holdings in the Marnock field are as follows: BP = 73%, Esso = 13.5%, Shell = 13.5%.\n\nThe Mungo field is located in UKCS block 23/16 and is named after Saint Mungo. It is an oilfield with a natural gas cap. Water and gas injection are used to manage the reservoir, which necessitated a small normally unmanned installation be built to support these facilities. The NUI is tied back to the CPF. The field is operated by BP in partnership with Nippon Oil, Murphy Oil and Total S.A..\n\nThe holdings in Mungo are: BP = 82.35%, Zennor = 12.65%, JX Nippon = 5%\n\nThe Monan Field is located in UKCS block 22/20 and is named after Saint Monan. It is a small turbidite oil and gas field produced under natural depletion using subsea manifolds. Its production fluids are fed into the pipelines connecting Mungo to the CPF. The field is operated by BP in partnership with Nippon Oil, Murphy Oil and Total S.A..\n\nThe holdings in Monan are BP = 83.25%, Zennor = 12.65%, JX Nippon = 5% \n\nThe Machar is located in UKCS block 23/26 named after Saint Machar. It is an oil field in a chalk reservoir located on top of a large salt diapir. Originally, the half dozen wells produced under natural depletion but modifications are being made to include the capacity for gas lift. The field is solely a BP possession.\n\nThese two were later additions to the ETAP complex. The Mirren field is located in UKCS block 22/25 and is named after Saint Mirren. It is an oil field with a gas cap in the Paleocene structure. The Madoes field is located in UKCS block 22/23 and is named after Saint Madoes. It is a light oil field located in the Eocene rock. Both are subsea tiebacks to the CPF, with the capacity for gas lift in the future to aid production. They are both operated by BP with Nippon Oil, Shell, Esso and AGIP.\n\nThe holdings in the Mirren field are as follows: BP = 44.7%, ESSO = 21%, JX Nippon = 13.3%, Shell = 21%.\nThe holdings in the Madoes field are as follows: ARCO = 31.7%, BP = 6.5%, Esso = 25%, JX Nippon = 12%, Shell = 25% \n\nThese fields are high temperature, high pressure oil producing wells. Heron is in UKCS block 22/30a and has a Triassic reservoir. Skua is an extension of the Marnock Field. They are subsea tiebacks to the CPF. All three fields are operated by Shell in partnership with Esso.\n\nOn 18 February 2009, a Super Puma Helicopter ditched in the sea whilst approaching one of the ETAP installations. All 18 passengers and crew were rescued. Bernard Looney, a President of BP’s North Sea business based in Aberdeen, credited their Project Jigsaw with the safe, quick and efficient recovery of the 16 passengers and 2 crew. Project Jigsaw uses locator beacons on all helicopters, standby vessels and fast rescue craft, connected to a computerised system located in Aberdeen. This way locations of all rescue craft and their response time are always known to staff in the BP control centre. In addition all staff are supplied with wristwatch personal locator beacons (WWPLB) that automatically activate when immersed in water.\n\n\n"}
{"id": "42167253", "url": "https://en.wikipedia.org/wiki?curid=42167253", "title": "Ecomechatronics", "text": "Ecomechatronics\n\nEcomechatronics is an engineering approach to developing and applying mechatronical technology in order to reduce the ecological impact and total cost of ownership of machines. It builds upon the integrative approach of mechatronics, but not with the aim of only improving the functionality of a machine. Mechatronics is the multidisciplinary field of science and engineering that merges mechanics, electronics, control theory, and computer science to improve and optimize product design and manufacturing. In ecomechatronics, additionally, functionality should go hand in hand with an efficient use and limited impact on resources. Machine improvements are targeted in 3 key areas: energy efficiency, performance and user comfort (noise & vibrations).\n\nAmong policy makers and manufacturing industries there is a growing awareness of the scarcity of resources and the need for sustainable development. This results in new regulations with respect to the design of machines (e.g. European Ecodesign Directive 2009/125/EC) and to a paradigm shift in the global machines market: \"instead of maximum profit from minimum capital, maximum added value must be generated from minimal resources\". Manufacturing industries increasingly require high performance machines that use resources (energy, consumables) economically in a human-centered production. Machine building companies and original equipment manufacturers are thus urged to respond to this market demand with a new generation of high performance machines with higher energy efficiency and user comfort.\nA reduction of the energy consumption lowers energy costs and reduces environmental impact. Typically more than 80% of the total-life-cycle impact of a machine is attributed to its energy consumption during the use phase. Therefore, improving a machine's energy efficiency is the most effective way of reducing its environmental impact. \nPerformance quantifies how well a machine executes its function and is typically related to productivity, precision and availability. User comfort is related to the exposure of operators and the environment to noise & vibrations due to machine operation.\n\nSince energy efficiency, performance and noise & vibrations are coupled in a machine they need to be addressed in an integrated way in the design phase. Example of the interrelation between the 3 key areas: with increasing machine speed typically the machine’s productivity increases, but energy consumption will increase as well and machine vibrations may grow such that machine accuracy (e.g. positioning accuracy) and availability (due to downtime and maintenance) decrease. Ecomechatronical design deals with the trade-off between these key areas.\n\nEcomechatronics impacts the way mechatronical systems and machines are being designed and implemented. Therefore, the transformation to a new generation of machines concerns knowledge institutes, original equipment manufacturers, CAE software suppliers, machine builders and industrial machine owners. The fact that about 80% of the environmental impact of a machine is determined by its design puts emphasis on making the right technological design choices. A model-based, multidisciplinary design approach is required in order to address the energy efficiency, performance and user comfort of a machine in an integrated way.\n\nThe key enabling technologies can be categorized in machine components, machine design methods & tools, and machine control. A few examples are listed below per category.\n\nMachine components\n\nDesign methods & tools\n\nMachine control\n\nSome examples of ecomechatronical system applications are:\n\n\n"}
{"id": "28902374", "url": "https://en.wikipedia.org/wiki?curid=28902374", "title": "Edenspiekermann", "text": "Edenspiekermann\n\nEdenspiekermann is an agency for strategy, design and communication with offices in Amsterdam, Berlin, San Francisco, Los Angeles and Singapore. The agency is the result of a merger in 2009 between Eden design & communication Amsterdam and SpiekermannPartners Berlin, founded by German typographer Erik Spiekermann.\n\nEdenspiekermann is the result of several mergers and collaborations between design and architectural firms over the past decades.\n\nIn 1956 and Jan Vonk started a collaboration. Premsela worked at De Bijenkorf where he remained Head of Publicity and Shop Windows until 1963.\n\nIn 1969, graphic designers Jan Brinkman and Niko Spelbrink started their collaboration. Guus Ros joined the duo in 1971 and from then on, they continued their cooperation as Brinkman, Ros en Spelbrink. Edo Smitshuijzen joined in 1975 when Niko Spelbrink went to London for one year. The BRS acronym wasn't introduced until 1980 when John Stegmeijer joined the company as 5th partner.\n\nIn 1986, Premsela Vonk merged with BRS.\n\nIn 1989 BRS Premsela Vonk formed the European Designers Network E.D.E.N. EESV, together with Metadesign (Berlin), King & Miranda Design (Milan) and Eleven Danes (Copenhagen).\n\nIn 1999, BRS Premsela Vonk merged with Linea, specialised in information design and forms design, and DC3, specialised in interaction design.\n\nErik Spiekermann left Metadesign in 2001 to run his own projects, forming the United Designers Network in 2002-3 with American designer, Susanna Dulkinys. Changing the name to SpiekermannPartners in 2007.\n\nIn January 2009, SpiekermannPartners and Eden Design & Communication merged to form Edenspiekermann. Between 2009 and 2017, Edenspiekermann opened new offices in Stuttgart, San Francisco, Los Angeles, and Singapore.\n\nFurther reading\n\nwww.edenspiekermann.com\n"}
{"id": "8707643", "url": "https://en.wikipedia.org/wiki?curid=8707643", "title": "Electronic circuit", "text": "Electronic circuit\n\nAn electronic circuit is composed of individual electronic components, such as resistors, transistors, capacitors, inductors and diodes, connected by conductive wires or traces through which electric current can flow. To be referred to as \"electronic\", rather than \"electrical\", generally at least one active component must be present. The combination of components and wires allows various simple and complex operations to be performed: signals can be amplified, computations can be performed, and data can be moved from one place to another. \n\nCircuits can be constructed of discrete components connected by individual pieces of wire, but today it is much more common to create interconnections by photolithographic techniques on a laminated substrate (a printed circuit board or PCB) and solder the components to these interconnections to create a finished circuit. In an integrated circuit or IC, the components and interconnections are formed on the same substrate, typically a semiconductor such as silicon or (less commonly) gallium arsenide.\n\nAn electronic circuit can usually be categorized as an analog circuit, a digital circuit, or a mixed-signal circuit (a combination of analog circuits and digital circuits).\n\nBreadboards, perfboards, and stripboards are common for testing new designs. They allow the designer to make quick changes to the circuit during development.\n\nAnalog electronic circuits are those in which current or voltage may vary continuously with time to correspond to the information being represented. Analog circuitry is constructed from two fundamental building blocks: series and parallel circuits.\n\nIn a series circuit, the same current passes through a series of components. A string of Christmas lights is a good example of a series circuit: if one goes out, they all do.\n\nIn a parallel circuit, all the components are connected to the same voltage, and the current divides between the various components according to their resistance.\n\nThe basic components of analog circuits are wires, resistors, capacitors, inductors, diodes, and transistors. (In 2012 it was demonstrated that memristors can be added to the list of available components.) Analog circuits are very commonly represented in schematic diagrams, in which wires are shown as lines, and each component has a unique symbol. Analog circuit analysis employs Kirchhoff's circuit laws: all the currents at a node (a place where wires meet), and the voltage around a closed loop of wires is 0. Wires are usually treated as ideal zero-voltage interconnections; any resistance or reactance is captured by explicitly adding a parasitic element, such as a discrete resistor or inductor. Active components such as transistors are often treated as controlled current or voltage sources: for example, a field-effect transistor can be modeled as a current source from the source to the drain, with the current controlled by the gate-source voltage.\n\nAn alternative model is to take independent power sources and induction as basic electronic units; this allows modeling frequency dependent negative resistors, gyrators, negative impedance converters, and dependent sources as secondary electronic components.\n\nWhen the circuit size is comparable to a wavelength of the relevant signal frequency, a more sophisticated approach must be used, the distributed element model. Wires are treated as transmission lines, with nominally constant characteristic impedance, and the impedances at the start and end determine transmitted and reflected waves on the line. Circuits designed according to this approach are distributed element circuits. Such considerations typically become important for circuit boards at frequencies above a GHz; integrated circuits are smaller and can be treated as lumped elements for frequencies less than 10GHz or so.\n\nIn digital electronic circuits, electric signals take on discrete values, to represent logical and numeric values. These values represent the information that is being processed. In the vast majority of cases, binary encoding is used: one voltage (typically the more positive value) represents a binary '1' and another voltage (usually a value near the ground potential, 0 V) represents a binary '0'. Digital circuits make extensive use of transistors, interconnected to create logic gates that provide the functions of Boolean logic: AND, NAND, OR, NOR, XOR and all possible combinations thereof. Transistors interconnected so as to provide positive feedback are used as latches and flip flops, circuits that have two or more metastable states, and remain in one of these states until changed by an external input. Digital circuits therefore can provide both logic and memory, enabling them to perform arbitrary computational functions. (Memory based on flip-flops is known as static random-access memory (SRAM). Memory based on the storage of charge in a capacitor, dynamic random-access memory (DRAM) is also widely used.) \n\nThe design process for digital circuits is fundamentally different from the process for analog circuits. Each logic gate regenerates the binary signal, so the designer need not account for distortion, gain control, offset voltages, and other concerns faced in an analog design. As a consequence, extremely complex digital circuits, with billions of logic elements integrated on a single silicon chip, can be fabricated at low cost. Such digital integrated circuits are ubiquitous in modern electronic devices, such as calculators, mobile phone handsets, and computers. As digital circuits become more complex, issues of time delay, logic races, power dissipation, non-ideal switching, on-chip and inter-chip loading, and leakage currents, become limitations to the density, speed and performance. \n\nDigital circuitry is used to create general purpose computing chips, such as microprocessors, and custom-designed logic circuits, known as application-specific integrated circuit (ASICs). Field-programmable gate arrays (FPGAs), chips with logic circuitry whose configuration can be modified after fabrication, are also widely used in prototyping and development. \n\nMixed-signal or hybrid circuits contain elements of both analog and digital circuits. Examples include comparators, timers, phase-locked loops, analog-to-digital converters, and digital-to-analog converters. Most modern radio and communications circuitry uses mixed signal circuits. For example, in a receiver, analog circuitry is used to amplify and frequency-convert signals so that they reach a suitable state to be converted into digital values, after which further signal processing can be performed in the digital domain.\n\n\n"}
{"id": "82272", "url": "https://en.wikipedia.org/wiki?curid=82272", "title": "Electronic warfare", "text": "Electronic warfare\n\nElectronic warfare (EW) is any action involving the use of the electromagnetic spectrum (EM spectrum) or directed energy to control the spectrum, attack an enemy, or impede enemy assaults. The purpose of electronic warfare is to deny the opponent the advantage of, and ensure friendly unimpeded access to, the EM spectrum. EW can be applied from air, sea, land, and/or space by manned and unmanned systems, and can target humans, communication, radar, or other assets (military and civilian).\n\nMilitary operations are executed in an information environment increasingly complicated by the electromagnetic spectrum. The electromagnetic spectrum portion of the information environment is referred to as the Electromagnetic Environment (EME). The recognized need for military forces to have unimpeded access to and use of the electromagnetic environment creates vulnerabilities and opportunities for electronic warfare in support of military operations.\n\nWithin the information operations construct, EW is an element of information warfare; more specifically, it is an element of offensive and defensive counterinformation.\n\nNATO has a different and arguably more encompassing and comprehensive approach to EW. A military committee conceptual document from 2007 (\"MCM_0142 Nov 2007 Military Committee Transformation Concept for Future NATO Electronic Warfare\") recognised the EME as an operational maneuver space and warfighting environment/domain. In NATO, EW is considered to be warfare in the EME. NATO has adopted simplified language which parallels those used in the other warfighting environments like maritime, land and air/space. For example, Electronic Attack is offensive use of EM energy, Electronic Defence (ED) and Electronic Surveillance (ES). The use of the traditional NATO EW terms, Electronic Countermeasures (ECM), Electronic Protective Measures (EPM) and Electronic Support Measures (ESM) has been retained as they contribute to and support Electronic Attack (EA), Electronic Defense (ED) and Electronic Support (ES). Besides EW, other EM operations include Intelligence, Surveillance, Target Acquisition, and Reconnaissance (ISTAR) and Signals Intelligence (SIGINT). Subsequently, NATO has issued EW Policy and Doctrine and is addressing the other NATO defense lines of development.\n\nPrimary EW activities have been developed over time to exploit the opportunities and vulnerabilities that are inherent in the physics of EM energy. Activities used in EW include: electro-optical, infrared and radio frequency countermeasures; EM compatibility and deception; Radio jamming, Radar jamming and deception and Electronic counter-countermeasures (or anti-jamming); electronic masking, probing, reconnaissance, and intelligence; electronic security; EW reprogramming; emission control; spectrum management; and wartime reserve modes.\n\nElectronic warfare consists of three major subdivisions: Electronic Attack (EA), Electronic Protection (EP), and Electronic Warfare Support (ES). Each of these is discussed below.\n\nElectronic Attack (EA) (previously known as Electronic Counter Measures (ECM)) involves the offensive use of EM energy, directed energy, or anti-radiation weapons to attack personnel, facilities, or equipment with the intent of degrading, neutralizing, or destroying enemy combat capability including human life. In the case of EM energy, this action is most commonly referred to as jamming and can be performed on communications systems or radar systems. In the case of anti-radiation weapons, many times this includes missles or bombs that can home in on a specific signal (radio or radar) and follow that path directly to impact, thus destroying the system broadcasting.\n\nElectronic Protection (EP) (previously known as Electronic Protective Measures (EPM) or Electronic Counter-CounterMeasures (ECCM)) involves actions taken to protect friendly forces (personnel, facilities, and equipment) from any effects of friendly or enemy use of the electromagnetic spectrum that degrade, neutralize, or destroy friendly combat capability (EA). So, EP brings with it the ability to defeat EA. Not to confuse the issue, but \"jamming\" is not part of EP, it is the target of EP. Jamming is an EA capability (see above).\n\nFlares are often used to distract infrared homing missles to miss their target. The use of flare rejection logic in the guidance (seeker head) of an infrared homing missile to counter an adversary's use of flares is an example of EP. While defensive EA actions (jamming) and EP (defeating jamming) both protect personnel, facilities, capabilities, and equipment, EP protects from the \"effects\" of EA (friendly and/or adversary). Other examples of EP include spread spectrum technologies, use of restricted frequency lists, emissions control (EMCON), and low observability (stealth) technology.\n\nElectronic Warfare Self Protection (EWSP) is a suite of countermeasure systems fitted primarily to aircraft for the purpose of protecting the host from weapons fire and can include among others: Directional Infrared Counter Measures (DIRCM - protection against IR missiles), Flare systems (protection against IR missiles) and other forms of Infrared countermeasures (protection against IR missiles), Chaff (protection against radar guided missiles), and DRFM decoy systems (protection against radar targeted anti-aircraft weapons).\n\nAn Electronic Warfare Tactics Range (EWTR) is a practice range which provides for the training of personnel in electronic warfare. There are two examples of such ranges in Europe; one at RAF Spadeadam in the northwest county of Cumbria, England and the Multinational Aircrew Electronic Warfare Tactics Facility Polygone range on the border between Germany and France. EWTRs are equipped with ground-based equipment to simulate electronic warfare threats that aircrew might encounter on missions. Other EW training and tactics ranges are available for ground and naval forces as well.\n\nAntifragile EW is a step beyond standard EP, occurring when a communications link being jammed actually increases in capability as a result of a jamming attack, although this is only possible under certain circumstances such as reactive forms of jamming.\n\nElectronic Warfare Support (ES) is a subdivision of EW involving actions taken by an operational commander or operator to detect, intercept, identify, locate, and/or localize sources of intended and unintended radiated electromagnetic (EM) energy. This is often referred to as simply reconnaissance, although today, more common terms are Intelligence, Surveillance and Reconnaissance (ISR) or Intelligence, Surveillance, Target Acquisition, and Reconnaissance (ISTAR). The purpose is to provide immediate recognition, prioritization, and targeting of threats to battlefield commanders.\n\nSignals Intelligence (SIGINT), a discipline overlapping with ES, is the related process of analyzing and identifying intercepted transmissions from sources such as radio communication, mobile phones, radar or microwave communication. SIGINT is broken into three categories: Electronic Intelligence (ELINT), Communications Intelligence (COMINT), and Foreign Instrumentation Signals Intelligence FISINT. Analysis parameters measured in signals of these categories can include frequency, bandwidth, modulation, and polarization. \n\nThe distinction between SIGINT and ES is determined by the controller of the collection assets, the information provided, and the intended purpose of the information. Electronic warfare support is conducted by assets under the operational control of a commander to provide tactical information, specifically threat prioritization, recognition, location, targeting, and avoidance. However, the same assets and resources that are tasked with ES can simultaneously collect information that meets the collection requirements for more strategic intelligence.\n\nThe history of Electronic Warfare goes back to at least the beginning of the 20th century. The earliest documented consideration of EW was during the Russo-Japanese War of 1904-1905. The Japanese auxiliary cruiser, Shinano Maru, had located the Russian Baltic Fleet in Tsushima Strait, and was communicating the fleet's location by \"wireless\" to the Imperial Japanese Fleet HQ. The captain of the Russian warship, Orel, requested permission to disrupt the Japanese communications link by attempting to transmit a stronger radio signal over the Shinano Mura's signal hoping to distort the Japanese signal at the receiving end. Russian Admiral Zinovy Rozhestvensky refused the advice and denied the Orel permission to electronically jam the enemy, which in those circumstances might have proved invaluable. The intelligence the Japanese gained ultimately led to the decisive Battle of Tsushima. The battle was humiliating for Russia. The Russian navy lost all their battleships and most of its cruisers and destroyers. These staggering losses effectively ended the Russo-Japanese War in Japan's favor. 4,380 Russians were killed and 5,917 were captured, including two admirals, with a further 1,862 interned.\n\nDuring World War II, the Allies and Axis Powers both extensively used EW, or what Winston Churchill referred to as the \"Battle of the Beams\". Navigational radars had gained in use to vector bombers to their targets and back to their home base. The first application of EW in WWII was to defeat those navigational radars. Chaff was also introduced during WWII to confuse and defeat tracking radar systems.\n\nAs time progressed and battlefield communication and radar technology improved, so did electronic warfare. Electronic warefare played a major role in many military operations during the Vietnam War. Aircraft on bombing runs and air-to-air missions often relied on EW to survive the battle, although many were defeated by Vietnamese ECCM.\n\nAs another example, in 2007, an Israeli attack on a suspected Syrian nuclear site during Operation Outside the Box (or Operation Orchard) used electronic warfare systems to disrupt Syrian air defenses while Israeli jets crossed much of Syria, bombed their targets, and returned to Israel undeterred. The target of the flight of 10 F-15 aircraft was a suspected nuclear reactor under construction near the Euphrates River modeled after a North Korean reactor and supposedly financed with Iranian assistance. Some reports say Israeli EW systems deactivated all of Syria’s air defense systems for the entire period of the raid, infiltrating the country, bombing their target and escaping.\n\nIn December 2010, the Russian army received their first land-based Army operated multifunctional electronic warfare system known as Borisoglebsk 2 developed by Sozvezdie. Development of the system started in 2004 and evaluation testing successfully completed in December 2010. The Borisoglebsk-2 brings four different types of jamming stations into a single system with a single control console helping the operator make battlefield decisions within seconds. The Borisoglebsk-2 system is mounted on nine MT-LB armored vehicles and is intended to suppress mobile satellite communications and satellite-based navigation signals. This EW system is developed to conduct electronic reconnaissance and suppression of radio-frequency sources. Newspaper, Svenska Dagbladet, claimed its initial usage caused concern within NATO. A Russian blog described Borisoglebsk-2 thusly: \n\n\nOther Electronic Warfare Systems:\n\nHistoric:\n\nU.S. specific:\n\n\n"}
{"id": "51322796", "url": "https://en.wikipedia.org/wiki?curid=51322796", "title": "Environmentally Concerned Citizens of South Central Michigan", "text": "Environmentally Concerned Citizens of South Central Michigan\n\nEnvironmentally Concerned Citizens of South Central Michigan (ECCSCM) is an association of citizens in Michigan, who are concerned about the consequences of concentrated animal feeding operations (CAFOs). The ECCSCM is located in the Western Lake Erie Watershed.\n\nThe association's aim is to educate the public on the health risks and the environmental damage (such as air and water pollution) of CAFOs. They also promote sustainable alternatives (such as eating local food, pasture-based meat, eggs and dairy).\nECCSCM is concerned about the area's drinking water, the risk of liquid manure systems, which might drain to field tiles, which drain to streams and then finally flow to Lake Erie. ECCSCM claims that CAFOs in this area are violating Michigan's Water Quality Standards. The organization also monitors and reports illegal activities of CAFOs.\n\nLynn Henning, who won the Goldman Environmental Prize for her fight against CAFOs, had a leading role in the formation of the ECCSCM.\n"}
{"id": "47111983", "url": "https://en.wikipedia.org/wiki?curid=47111983", "title": "Ernesto Priani", "text": "Ernesto Priani\n\nErnesto Priani (9 May 1962 Mexico City) is a philosopher, professor, digital humanist, digital editor.\n\nHe is currently a professor at the Faculty of Philosophy and Letters of the National Autonomous University of Mexico, vice president of Digital Humanists Network, and founder of Club Wikipedia.\nDivides his philosophical work between the history of medieval and Renaissance thought and ethics.\n\nHe was born in Mexico City. He received undergraduate, master's and doctorate in philosophy at the Faculty of Philosophy and Letters of the National Autonomous University of Mexico, obtaining this last degree with honors in 1998.\n\n\n\n\n"}
{"id": "1663956", "url": "https://en.wikipedia.org/wiki?curid=1663956", "title": "Exit criteria", "text": "Exit criteria\n\nExit criteria are the criteria or requirements which must be met to complete a specific task or process as used in some fields of business or science, such as software engineering.\n\nThe term exit criteria is often used in research and development, but it could be applicable to any field where Business process reengineering is (or could be) applied. The benefits of business process re-engineering — including the use of terms such as this one — could include: understanding goals clearly; using language (and data) carefully when talking about (or measuring) methods for getting things done; and taking a scientific approach towards evaluating and improving the methods that are used.\n\nFor example, for Fagan inspection, the low-level document must comply with specific exit criteria (as specified in the high-level document) before the development process can be taken to the next phase.\n\nIn telecommunications, when testing new software or hardware for release, a set of test specifications are created to test this new product to ensure that it meets minimum acceptable operational specifications. This test specification will state the minimum criteria necessary for the testing process to be considered complete and the product is ready for release IE: Exit the testing phase of the program.\n"}
{"id": "54683524", "url": "https://en.wikipedia.org/wiki?curid=54683524", "title": "Federal Ministry of Petroleum Resources (Nigeria)", "text": "Federal Ministry of Petroleum Resources (Nigeria)\n\nThe Federal Ministry of Petroleum Resoruces is a part of the Federal Ministries of Nigeria that directs petroleum resources and its activities in Nigeria. It is located at Block D, NNPC Towers, Herbert Macaulay way, CDB, Abuja.\n\nIt was established in 1970. President Muhammadu Buhari appointed Dr. Emmanuel Ibe Kachikwu as the Minister of State and Petroleum Resources in November 2015.\n\nIt functions include;\n\nPetroleum Technology Development Fund=== Parastatals ===\n"}
{"id": "52501", "url": "https://en.wikipedia.org/wiki?curid=52501", "title": "Fork", "text": "Fork\n\nIn cutlery or kitchenware, a fork (from the Latin \"furca\" (\"pitchfork\")) is a utensil, now usually made of metal, whose long handle terminates in a head that branches into several narrow and often slightly curved tines with which one can spear foods either to hold them to cut with a knife or to lift them to the mouth. \n\nBone forks have been found in archaeological sites of the Bronze Age Qijia culture (2400–1900 BC), the Shang dynasty (c. 1600–c. 1050 BC), as well as later Chinese dynasties. A stone carving from an Eastern Han tomb (in Ta-kua-liang, Suide County, Shaanxi) depicts three hanging two-pronged forks in a dining scene. Conversely, similar forks has also been depicted on top of a stove in a scene at another Eastern Han tomb (in Suide County, Shaanxi). In Ancient Egypt, large forks were used as cooking utensils. \n\nIn the Roman Empire, bronze and silver forks were used, many surviving examples of which are displayed in museums around Europe. Use varied according to local customs, social class, and the type of food, but in earlier periods forks were mostly used as cooking and serving utensils. \n\nAlthough its origin may go back to Ancient Greece, the personal table fork was most likely invented in the Eastern Roman (\"Byzantine\") Empire, where they were in common use by the 4th century. Records show that by the 9th century in some elite circles of Persia a similar utensil known as a \"barjyn\" was in limited use. By the 10th century, the table fork was in common use throughout the Middle East.\n\nThe introduction of the fork to Western Europe, according to theologian and cardinal Peter Damian, was by Theophano Sklereina, wife of Holy Roman Emperor Otto II, who at an Imperial banquet in 972 nonchalantly produced one, astonishing her Western guests. By the 11th century, the table fork had become increasingly prevalent in the Italian peninsula before other European regions because of historical ties with Byzantium and, as pasta became a greater part of the Italian diet, continued to gain popularity, displacing the long wooden spike formerly used since the forks three spikes proved better suited to gathering the noodles. By the 14th century the table fork had become commonplace in Italy, and by 1600 was almost universal among the merchant and upper classes. It was proper for a guest to arrive with his own fork and spoon enclosed in a box called a \"cadena\"; this usage was introduced to the French court with Catherine de' Medici's entourage.\nAlthough in Portugal forks were first used around 1450 by Infanta Beatrice, Duchess of Viseu, King Manuel I of Portugal's mother, only by the 16th century, when they had become part of Italian etiquette, did forks enter into common use in Southern Europe, gaining some currency in Spain, and gradually spreading to France. The rest of Europe did not adopt the fork until the 18th century.\n\nThe fork's adoption in northern Europe was slower. Its use was first described in English by Thomas Coryat in a volume of writings on his Italian travels (1611), but for many years it was viewed as an unmanly Italian affectation. Some writers of the Roman Catholic Church expressly disapproved of its use, St. Peter Damian seeing it as \"excessive delicacy\": It was not until the 18th century that the fork became commonly used in Great Britain, although some sources say that forks were common in France, England and Sweden already by the early 17th century.\n\nThe fork did not become popular in North America until near the time of the American Revolution. The curved fork used in most parts of the world today was developed in Germany in the mid 18th century while the standard four-tine design became current in the early 19th century. The fork was important in Germany because they believed that eating with the fingers was rude and disrespectful. The fork led to family dinners and sit-down meals, which are important features of German culture.\n\n\n\n"}
{"id": "683342", "url": "https://en.wikipedia.org/wiki?curid=683342", "title": "Fuse (electrical)", "text": "Fuse (electrical)\n\nIn electronics and electrical engineering, a fuse is an electrical safety device that operates to provide overcurrent protection of an electrical circuit. Its essential component is a metal wire or strip that melts when too much current flows through it, thereby interrupting the current. It is a sacrificial device; once a fuse has operated it is an open circuit, and it must be replaced or rewired, depending on type. \n\nFuses have been used as essential safety devices from the early days of electrical engineering. Today there are thousands of different fuse designs which have specific current and voltage ratings, breaking capacity and response times, depending on the application. The time and current operating characteristics of fuses are chosen to provide adequate protection without needless interruption. Wiring regulations usually define a maximum fuse current rating for particular circuits. Short circuits, overloading, mismatched loads, or device failure are the prime reasons for fuse operation. \n\nA fuse is an automatic means of removing power from a faulty system; often abbreviated to ADS (Automatic Disconnection of Supply). Circuit breakers can be used as an alternative to fuses, but have significantly different characteristics.\n\nBreguet recommended the use of reduced-section conductors to protect telegraph stations from lightning strikes; by melting, the smaller wires would protect apparatus and wiring inside the building. A variety of wire or foil fusible elements were in use to protect telegraph cables and lighting installations as early as 1864.\n\nA fuse was patented by Thomas Edison in 1890 as part of his electric distribution system.\n\nA fuse consists of a metal strip or wire fuse element, of small cross-section compared to the circuit conductors, mounted between a pair of electrical terminals, and (usually) enclosed by a non-combustible housing. The fuse is arranged in series to carry all the current passing through the protected circuit. The resistance of the element generates heat due to the current flow. The size and construction of the element is (empirically) determined so that the heat produced for a normal current does not cause the element to attain a high temperature. If too high a current flows, the element rises to a higher temperature and either directly melts, or else melts a soldered joint within the fuse, opening the circuit.\n\nThe fuse element is made of zinc, copper, silver, aluminum, or alloys to provide stable and predictable characteristics. The fuse ideally would carry its rated current indefinitely, and melt quickly on a small excess. The element must not be damaged by minor harmless surges of current, and must not oxidize or change its behavior after possibly years of service.\n\nThe fuse elements may be shaped to increase heating effect. In large fuses, current may be divided between multiple strips of metal. A dual-element fuse may contain a metal strip that melts instantly on a short-circuit, and also contain a low-melting solder joint that responds to long-term overload of low values compared to a short-circuit. Fuse elements may be supported by steel or nichrome wires, so that no strain is placed on the element, but a spring may be included to increase the speed of parting of the element fragments.\n\nThe fuse element may be surrounded by air, or by materials intended to speed the quenching of the arc. Silica sand or non-conducting liquids may be used.\n\nA maximum current that the fuse can continuously conduct without interrupting the circuit.\nThe speed at which a fuse blows depends on how much current flows through it and the material of which the fuse is made. The operating time is not a fixed interval, but decreases as the current increases. Fuses have different characteristics of operating time compared to current. A standard fuse may require twice its rated current to open in one second, a fast-blow fuse may require twice its rated current to blow in 0.1 seconds, and a slow-blow fuse may require twice its rated current for tens of seconds to blow.\n\nCartridge fuses may list the following speeds\n\nFuse selection depends on the load's characteristics. Semiconductor devices may use a fast or \"ultrafast\" fuse as semiconductor devices heat rapidly when excess current flows. The fastest blowing fuses are designed for the most sensitive electrical equipment, where even a short exposure to an overload current could be very damaging. Normal fast-blow fuses are the most general purpose fuses. A \"time-delay fuse\" (also known as an \"anti-surge\" or \"slow-blow\" fuse) is designed to allow a current which is above the rated value of the fuse to flow for a short period of time without the fuse blowing. These types of fuse are used on equipment such as motors, which can draw larger than normal currents for up to several seconds while coming up to speed.\n\nManufacturers can provide a plot of current vs time, often plotted on logarithmic scales, to characterize the device and to allow comparison with the characteristics of protective devices upstream and downstream of the fuse.\n\nThe It rating is related to the amount of energy let through by the fuse element when it clears the electrical fault. This term is normally used in short circuit conditions and the values are used to perform co-ordination studies in electrical networks. It parameters are provided by charts in manufacturer data sheets for each fuse family. For coordination of fuse operation with upstream or downstream devices, both melting It and clearing It are specified. The melting It is proportional to the amount of energy required to begin melting the fuse element. The clearing It is proportional to the total energy let through by the fuse when clearing a fault. The energy is mainly dependent on current and time for fuses as well as the available fault level and system voltage. Since the It rating of the fuse is proportional to the energy it lets through, it is a measure of the thermal damage from the heat and magnetic forces that will be produced by a fault.\n\nThe breaking capacity is the maximum current that can safely be interrupted by the fuse. This should be higher than the prospective short-circuit current. Miniature fuses may have an interrupting rating only 10 times their rated current. Some fuses are designated High Rupture Capacity (HRC) and are usually filled with sand or a similar material. Fuses for small, low-voltage, usually residential, wiring systems are commonly rated, in North American practice, to interrupt 10,000 amperes. Fuses for commercial or industrial power systems must have higher interrupting ratings, with some low-voltage current-limiting high interrupting fuses rated for 300,000 amperes. Fuses for high-voltage equipment, up to 115,000 volts, are rated by the total apparent power (megavolt-amperes, MVA) of the fault level on the circuit.\n\nThe voltage rating of the fuse must be equal to or, greater than, what would become the open-circuit voltage. For example, a glass tube fuse rated at 32 volts would not reliably interrupt current from a voltage source of 120 or 230V. If a 32V fuse attempts to interrupt the 120 or 230 V source, an arc may result. Plasma inside the glass tube may continue to conduct current until the current diminishes to the point where the plasma becomes a non-conducting gas. Rated voltage should be higher than the maximum voltage source it would have to disconnect. Connecting fuses in series does not increase the rated voltage of the combination, nor of any one fuse.\n\nMedium-voltage fuses rated for a few thousand volts are never used on low voltage circuits, because of their cost and because they cannot properly clear the circuit when operating at very low voltages.\n\nThe manufacturer may specify the voltage drop across the fuse at rated current. There is a direct relationship between a fuse's cold resistance and its voltage drop value. Once current is applied, resistance and voltage drop of a fuse will constantly grow with the rise of its operating temperature until the fuse finally reaches thermal equilibrium. The voltage drop should be taken into account, particularly when using a fuse in low-voltage applications. Voltage drop often is not significant in more traditional wire type fuses, but can be significant in other technologies such as resettable (PPTC) type fuses.\n\nAmbient temperature will change a fuse's operational parameters. A fuse rated for 1 A at 25 °C may conduct up to 10% or 20% more current at −40 °C and may open at 80% of its rated value at 100 °C. Operating values will vary with each fuse family and are provided in manufacturer data sheets.\n\nMost fuses are marked on the body or end caps with markings that indicate their ratings. Surface-mount technology \"chip type\" fuses feature few or no markings, making identification very difficult.\n\nSimilar appearing fuses may have significantly different properties, identified by their markings. Fuse markings will generally convey the following information, either explicitly as text, or else implicit with the approval agency marking for a particular type:\n\nFuses come in a vast array of sizes and styles to serve in many applications, manufactured in standardised package layouts to make them easily interchangeable. Fuse bodies may be made of ceramic, glass, plastic, fiberglass, molded mica laminates, or molded compressed fibre depending on application and voltage class.\n\nCartridge (ferrule) fuses have a cylindrical body terminated with metal end caps. Some cartridge fuses are manufactured with end caps of different sizes to prevent accidental insertion of the wrong fuse rating in a holder, giving them a bottle shape.\n\nFuses for low voltage power circuits may have bolted blade or tag terminals which are secured by screws to a fuseholder. Some blade-type terminals are held by spring clips. Blade type fuses often require the use of a special purpose extractor tool to remove them from the fuse holder.\n\nRenewable fuses have replaceable fuse elements, allowing the fuse body and terminals to be reused if not damaged after a fuse operation.\n\nFuses designed for soldering to a printed circuit board have radial or axial wire leads. Surface mount fuses have solder pads instead of leads.\n\nHigh-voltage fuses of the expulsion type have fiber or glass-reinforced plastic tubes and an open end, and can have the fuse element replaced.\n\n\"Semi-enclosed fuses\" are fuse wire carriers in which the fusible wire itself can be replaced. The exact fusing current is not as well controlled as an enclosed fuse, and it is extremely important to use the correct diameter and material when replacing the fuse wire, and for these reasons these fuses are slowly falling from favour.\n\nCurrent ratings of fuse wire (from Table 53A of BS 7671: 1992)\n\nThese are still used in consumer units in some parts of the world, but are becoming less common.\nWhile glass fuses have the advantage of a fuse element visible for inspection purposes, they have a low breaking capacity (interrupting rating), which generally restricts them to applications of 15 A or less at 250 V. Ceramic fuses have the advantage of a higher breaking capacity, facilitating their use in circuits with higher current and voltage. Filling a fuse body with sand provides additional cooling of the arc and increases the breaking capacity of the fuse. Medium-voltage fuses may have liquid-filled envelopes to assist in the extinguishing of the arc. Some types of distribution switchgear use fuse links immersed in the oil that fills the equipment.\n\nFuse packages may include a rejection feature such as a pin, slot, or tab, which prevents interchange of otherwise similar appearing fuses. For example, fuse holders for North American class RK fuses have a pin that prevents installation of similar-appearing class H fuses, which have a much lower breaking capacity and a solid blade terminal that lacks the slot of the RK type.\n\nFuses can be built with different sized enclosures to prevent interchange of different ratings of fuse. For example, \"bottle style\" fuses distinguish between ratings with different cap diameters. Automotive glass fuses were made in different lengths, to prevent high-rated fuses being installed in a circuit intended for a lower rating.\n\nGlass cartridge and plug fuses allow direct inspection of the fusible element. Other fuses have other indication methods including:\n\nSome fuses allow a special purpose micro switch or relay unit to be fixed to the fuse body. When the fuse element blows, the indicating pin extends to activate the micro switch or relay, which, in turn, triggers an event.\n\nSome fuses for medium-voltage applications use two or three separate barrels and two or three fuse elements in parallel.\n\nThe International Electrotechnical Commission publishes standard 60269 for low-voltage power fuses. The standard is in four volumes, which describe general requirements, fuses for industrial and commercial applications, fuses for residential applications, and fuses to protect semiconductor devices. The IEC standard unifies several national standards, thereby improving the interchangeability of fuses in international trade. All fuses of different technologies tested to meet IEC standards will have similar time-current characteristics, which simplifies design and maintenance.\n\nIn the United States and Canada, low-voltage fuses to 1 kV AC rating are made in accordance with Underwriters Laboratories standard UL 248 or the harmonized Canadian Standards Association standard C22.2 No. 248. This standard applies to fuses rated 1 kV or less, AC or DC, and with breaking capacity up to 200 kA. These fuses are intended for installations following Canadian Electrical Code, Part I (CEC), or the National Electrical Code, NFPA 70 (NEC).\n\nThe standard ampere ratings for fuses (and circuit breakers) in USA/Canada are considered 15, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100, 110, 125, 150, 175, 200, 225, 250, 300, 350, 400, 450, 500, 600, 700, 800, 1000, 1200, 1600, 2000, 2500, 3000, 4000, 5000, and 6000 amperes. Additional standard ampere ratings for fuses are 1, 3, 6, 10, and 601.\n\nUL 248 currently has 19 \"parts\". UL 248-1 sets the general requirements for fuses, while the latter parts are dedicated to specific fuses sizes (ex: 248-8 for Class J, 248-10 for Class L), or for categories of fuses with unique properties (ex: 248-13 for semiconductor fuses, 248-19 for photovoltaic fuses). The general requirements (248-1) apply except as modified by the supplemental part (240-x). For example, UL 248-19 allows photovoltaic fuses to be rated up to 1500 volts, DC, versus 1000 volts under the general requirements.\n\nIEC and UL nomenclature varies slightly. IEC standards refer to a \"fuse\" as the assembly of a fusible link and a fuse holder. In North American standards, the \"fuse\" is the replaceable portion of the assembly, and a \"fuse link\" would be a bare metal element for installation in a fuse.\n\nLow-voltage high rupture capacity fuses , HRC fuses for short (NH fuses in German), are also known under the names knife fuse, Blade fuse or (in connection with house connection boxes) armoured fuse. They are used in the area of main distribution boards in low-voltage networks. Their features include a significantly larger construction volume compared to screw-type fuses and solid contact blades at both ends. They can therefore carry and separate larger currents. Conventional designs as high-performance fuses permit safe disconnection of residual currents up to 120 kA (rated breaking capacity), whereby the rated current can be up to 1.6 kA (rated current).\n\nHRC fuses are widely used in industrial installations and are also used in the public power grid, e.g. in transformer stations, main distribution boards, or in building junction boxes and as meter fuses.\n\nHRC fuses sizes 000 and 00 (here 63A and 125A rated current)\nHRC fuses sizes 1, 2 and 3 (here 250A, 400A and 630A rated current)\nTAB 2007 (Technical Connection Conditions of Energy Network Operators) requires one disconnecting device per meter in the pre-metering area of customer installations.\n\nA disconnecting device is a device for disconnecting the customer system from the distribution network, which can also be operated by the customer (electrotechnical layman).\nThis requirement is met, for example, by selective miniature circuit breakers or neozed load break switches, but not by HRC fuses. HRC fuses are therefore only used as meter fuses in new systems if another disconnecting device that can be operated by laypersons (e.g. in the form of a meter fuse with a Neozed load break switch) is available.\n\nHRC fuses also have an indicator that indicates a blown fuse. Depending on the application, it is designed as a hinged detector mounted at the front (top) or as a central indicator, which is visible from the front when the fuse is inserted. Almost all manufacturers also offer HRC fuses with two detectors (combination detectors).\n\nHRC fuses are available with different tripping characteristics. These are described above in the Operating Classes section.\n\nHRC fuses are manufactured in different sizes for different rated current ranges. Size 0 is no longer permitted in new installations.\n\nHRC fuse links are equipped with handle straps for handling, which can be either live or de-energised (insulated). A fuse handle is required to insert the fuse links into or remove them from a fuse base with one pole.\n\nWhen live, HRC fuse links may only be replaced by a qualified electrician with suitable protective equipment. The protective equipment must include at least one push-on handle with permanently attached leather cuff, a helmet with face protection or a flame-retardant hood and closed, flame-retardant work clothing. When pulling or plugging HRC fuses over 63 A, arc-tested work clothing is recommended by the employers' liability insurance associations. An insulating mat and insulating gloves may be required. Improper pulling of a HRC fuse insert under load can result in an arcing fault which, without protective equipment, can result in serious to fatal injuries.\n\nSo-called HRC fuse switch disconnectors facilitate fuse replacement. They have a hinged cover which accommodates the handle flaps and replaces the fuse handle.\n\nHRC fuse switch disconnectors should be operated quickly, as there is a risk of material damage or personal injury due to an arc fault when switching under heavy loads or in the event of a short circuit.\n\nHRC fuse switch disconnectors are also available in these designs:\n\n\nHRC cutting knives of sizes 1 and 2\nThere are various special fuses for semiconductor protection, motor protection or other special requirements; also disconnect knives as pure switching elements.\n\n\"Automotive fuses\" are used to protect the wiring and electrical equipment for vehicles. There are several different types of automotive fuses and their usage is dependent upon the specific application, voltage, and current demands of the electrical circuit. Automotive fuses can be mounted in fuse blocks, inline fuse holders, or fuse clips. Some automotive fuses are occasionally used in non-automotive electrical applications. Standards for automotive fuses are published by SAE International (formerly known as the Society of Automotive Engineers).\n\nAutomotive fuses can be classified into four distinct categories:\n\nMost automotive fuses rated at 32 volts are used on circuits rated 24 volts DC and below. Some vehicles use a dual 12/42 V DC electrical system that will require a fuse rated at 58 V DC.\n\nFuses are used on power systems up to 115,000 volts AC. High-voltage fuses are used to protect instrument transformers used for electricity metering, or for small power transformers where the expense of a circuit breaker is not warranted. A circuit breaker at 115 kV may cost up to five times as much as a set of power fuses, so the resulting saving can be tens of thousands of dollars.\n\nIn medium-voltage distribution systems, a power fuse may be used to protect a transformer serving 1–3 houses. Pole-mounted distribution transformers are nearly always protected by a fusible cutout, which can have the fuse element replaced using live-line maintenance tools.\n\nMedium-voltage fuses are also used to protect motors, capacitor banks and transformers and may be mounted in metal enclosed switchgear, or (rarely in new designs) on open switchboards.\n\nLarge power fuses use fusible elements made of silver, copper or tin to provide stable and predictable performance. High voltage \"expulsion fuses\" surround the fusible link with gas-evolving substances, such as boric acid. When the fuse blows, heat from the arc causes the boric acid to evolve large volumes of gases. The associated high pressure (often greater than 100 atmospheres) and cooling gases rapidly quench the resulting arc. The hot gases are then explosively expelled out of the end(s) of the fuse. Such fuses can only be used outdoors.\n\nThese type of fuses may have an impact pin to operate a switch mechanism, so that all three phases are interrupted if any one fuse blows.\n\n\"High-power fuse\" means that these fuses can interrupt several kiloamperes. Some manufacturers have tested their fuses for up to 63 kA short-circuit current.\n\nFuses have the advantages of often being less costly and simpler than a circuit breaker for similar ratings. The blown fuse must be replaced with a new device which is less convenient than simply resetting a breaker and therefore likely to discourage people from ignoring faults. On the other hand, replacing a fuse without isolating the circuit first (most building wiring designs do not provide individual isolation switches for each fuse) can be dangerous in itself, particularly if the fault is a short circuit.\n\nHigh rupturing capacity fuses can be rated to safely interrupt up to 300,000 amperes at 600  V AC. Special current-limiting fuses are applied ahead of some molded-case breakers to protect the breakers in low-voltage power circuits with high short-circuit levels.\n\n\"Current-limiting\" fuses operate so quickly that they limit the total \"let-through\" energy that passes into the circuit, helping to protect downstream equipment from damage. These fuses open in less than one cycle of the AC power frequency; circuit breakers cannot match this speed.\n\nSome types of circuit breakers must be maintained on a regular basis to ensure their mechanical operation during an interruption. This is not the case with fuses, which rely on melting processes where no mechanical operation is required for the fuse to operate under fault conditions.\n\nIn a multi-phase power circuit, if only one fuse opens, the remaining phases will have higher than normal currents, and unbalanced voltages, with possible damage to motors. Fuses only sense overcurrent, or to a degree, over-temperature, and cannot usually be used independently with protective relaying to provide more advanced protective functions, for example, ground fault detection.\n\nSome manufacturers of medium-voltage distribution fuses combine the overcurrent protection characteristics of the fusible element with the flexibility of relay protection by adding a pyrotechnic device to the fuse operated by external protective relays.\n\nIn the UK, older electrical consumer units (also called fuse boxes) are fitted either with semi-enclosed (rewirable) fuses or cartridge fuses (Fuse wire is commonly supplied to consumers as short lengths of 5 A-, 15 A- and 30 A-rated wire wound on a piece of cardboard.) Modern consumer units usually contain miniature circuit breakers (MCBs) instead of fuses, though cartridge fuses are sometimes still used, as in some applications MCBs are prone to nuisance tripping.\n\nRenewable fuses (rewirable or cartridge) allow user replacement, but this can be hazardous as it is easy to put a higher-rated or double fuse element (link or wire) into the holder (\"overfusing\"), or simply fitting it with copper wire or even a totally different type of conducting object (coins, hairpins, paper clips, nails, etc.) to the existing carrier. One form of fuse box abuse was to put a penny in the socket, which defeated overcurrent protection and resulted in a dangerous condition. Such tampering will not be visible without full inspection of the fuse. Fuse wire was never used in North America for this reason, although renewable fuses continue to be made for distribution boards.\n\nThe \"Wylex standard\" consumer unit was very popular in the United Kingdom until the wiring regulations started demanding residual-current devices (RCDs) for sockets that could feasibly supply equipment outside the equipotential zone. The design does not allow for fitting of RCDs or RCBOs. Some Wylex standard models were made with an RCD instead of the main switch, but (for consumer units supplying the entire installation) this is no longer compliant with the wiring regulations as alarm systems should \"not\" be RCD-protected. There are two styles of fuse base that can be screwed into these units: one designed for rewirable fusewire carriers and one designed for cartridge fuse carriers. Over the years MCBs have been made for both styles of base. In both cases, higher rated carriers had wider pins, so a carrier couldn't be changed for a higher rated one without also changing the base. Cartridge fuse carriers are also now available for DIN-rail enclosures.\n\nIn North America, fuses were used in buildings wired before 1960. These \"Edison base\" fuses would screw into a fuse socket similar to Edison-base incandescent lamps. Ratings were 5, 10, 15, 20, 25, and 30 amperes. To prevent installation of fuses with an excessive current rating, later fuse boxes included rejection features in the fuse-holder socket, commonly known as \"Rejection Base (Type S fuses)\" which have smaller diameters that vary depending on the rating of the fuse. This means that fuses can only be replaced by the preset (Type S) fuse rating. This is a North American, tri-national standard (UL 4248-11; CAN/CSA-C22.2 NO. 4248.11-07 (R2012); and, NMX-J-009/4248/11-ANCE). Existing Edison fuse boards can easily be converted to only accept Rejection Base (Type S) fuses, by screwing-in a tamper-proof adapter. This adapter screws into the existing Edison fuse holder, and has a smaller diameter threaded hole to accept the designated Type S rated fuse.\n\nSome companies manufacture resettable miniature thermal circuit breakers, which screw into a fuse socket. Some installations use these Edison-base circuit breakers. However, any such breaker sold today does have one flaw. It may be installed in a circuit-breaker box with a door. If so, if the door is closed, the door may hold down the breaker's reset button. While in this state, the breaker is effectively useless: it does not provide any overcurrent protection.\n\nIn the 1950s, fuses in new residential or industrial construction for branch circuit protection were superseded by low voltage circuit breakers.\n\nWhere several fuses are connected in series at the various levels of a power distribution system, it is desirable to blow (clear) only the fuse (or other overcurrent device) electrically closest to the fault. This process is called \"coordination\" or \"discrimination\" and may require the time-current characteristics of two fuses to be plotted on a common current basis. Fuses are selected so that the minor, branch, fuse disconnects its circuit well before the supplying, major, fuse starts to melt. In this way, only the faulty circuit is interrupted with minimal disturbance to other circuits fed by a common supplying fuse.\n\nWhere the fuses in a system are of similar types, simple rule-of-thumb ratios between ratings of the fuse closest to the load and the next fuse towards the source can be used.\n\nSo-called self-resetting fuses use a thermoplastic conductive element known as a Polymeric Positive Temperature Coefficient (or PPTC) thermistor that impedes the circuit during an overcurrent condition (by increasing device resistance). The PPTC thermistor is self-resetting in that when current is removed, the device will cool and revert to low resistance. These devices are often used in aerospace/nuclear applications where replacement is difficult, or on a computer motherboard so that a shorted mouse or keyboard does not cause motherboard damage.\n\nA \"thermal fuse\" is often found in consumer equipment such as coffee makers, hair dryers or transformers powering small consumer electronics devices. They contain a fusible, temperature-sensitive composition which holds a spring contact mechanism normally closed. When the surrounding temperature gets too high, the composition melts and allows the spring contact mechanism to break the circuit. The device can be used to prevent a fire in a hair dryer for example, by cutting off the power supply to the heater elements when the air flow is interrupted (e.g., the blower motor stops or the air intake becomes accidentally blocked). Thermal fuses are a 'one shot', non-resettable device which must be replaced once they have been activated (blown).\n\nA cable limiter is similar to a fuse but is intended only for protection of low voltage power cables. It is used, for example, in networks where multiple cables may be used in parallel. It is not intended to provide overload protection, but instead protects a cable that is exposed to a short circuit. The characteristics of the limiter are matched to the size of cable so that the limiter clears a fault before the cable insulation is damaged. \n\nThe Unicode character for the fuse's schematic symbol, found in the Miscellaneous Technical block, is (⏛).\n\n\n A fuse is a safety device\n\n"}
{"id": "15999718", "url": "https://en.wikipedia.org/wiki?curid=15999718", "title": "Gee Broadcast", "text": "Gee Broadcast\n\nGee Broadcast Systems Ltd was founded in the UK in early 1987 by Keith and Sarah Gee. The company was initially set up to provide a design and installation service for broadcast Television systems but expanded to equipment sales and distribution, including videographics and \"engineering\" products. Later the 'Geevs' (Gee Video Server) family of video servers was developed and led to growth particularly in exports. Exports of Geevs have reached over 50 countries and formed the largest part of the Gee Broadcast business.\n\nIn 2004, Gee Broadcast acquired Lightworks in order to provide a tapeless production system with multichannel servers and editing systems.\n\nIn 2007, Gee Broadcast Systems with Lightworks had a team of engineers based in Basingstoke, Hampshire, UK with of office and workshop space, flexibly configured to allow the manufacturing and testing of large and small Geevs and Lightworks systems, together with a range of distributed products. \n\nIn 2009, Gee Broadcast Systems Ltd went into administration, and was dissolved in November 2010. Prior to this, certain assets of the company were acquired by EditShare.\n\nGeevs Servers can be built to meet the requirements of many different applications. Geevs can provide system design and consultancy or work with other system integrators for specific requirements.\n\nLightworks Editors are used in many different areas including feature films, TV drama, soaps, commercials and sports. Lightworks is known for its console and user interface which together provide editing. \nGee Broadcast Tapeless Production Systems were configured to match specific customers needs and can include Geevs servers and Lightworks Editors.\n"}
{"id": "12662643", "url": "https://en.wikipedia.org/wiki?curid=12662643", "title": "Hardware compatibility list", "text": "Hardware compatibility list\n\nA hardware compatibility list (HCL) is a list of computer hardware (typically including many types of peripheral devices) that is compatible with a particular operating system or device management software. In today's world, there is a vast amount of computer hardware in circulation, and many operating systems too. A hardware compatibility list is a database of hardware models and their compatibility with a certain operating system.\n\nHCLs can be centrally controlled (one person or team keeps the list of hardware maintained) or user-driven (users submit reviews on hardware they have used).\n\nThere are many HCLs. Usually, each operating system will have an official HCL on its website.\n\n"}
{"id": "12499773", "url": "https://en.wikipedia.org/wiki?curid=12499773", "title": "Holloware", "text": "Holloware\n\nHolloware (hollowware, or hollow-ware ) is metal tableware such as sugar bowls, creamers, coffee pots, teapots, soup tureens, hot food covers, water jugs, platters, butter pat plates, and other items that accompany dishware on a table. It does not include cutlery or other metal utensils. Holloware is constructed for durability. It differs from some other silverplated items, with thicker walls and more layers of silverplate.\n\nDining car holloware is a type of railroad collectible (railroadiana). The relative value of pieces depends on their scarcity, age and condition, and the popularity of the trains on which the items were used.\n\nHolloware is the traditional gift in the UK and the modern gift in the US for the 16th wedding anniversary. Holloware is the traditional gift for jubilee or wedding in Russia.\n\n"}
{"id": "25861478", "url": "https://en.wikipedia.org/wiki?curid=25861478", "title": "Inadine", "text": "Inadine\n\nInadine is a brand of non-adherent surgical dressing containing the disinfectant povidone-iodine (PVP-I). It was produced by Johnson & Johnson - Today by Systagenix a KCI company. This dressing is typically used on open wounds that may become infected, before or after surgery or even used on existing infected wounds. Before use, patients will be asked about any known susceptibility to iodine hypersensitivity.\n\nIt is a thin dressing that comes sandwiched between backing papers. . It is applied directly to a recently cleaned wound or even over necrotic tissue with no underlying treatment.\n\nThe dressing itself does not have adherent properties apart from the ability to absorb some moisture or blood, and is typically held in place using gauze and then bandage material.\n\nThe dressing is a topical wound dressing containing an ointment of 10% povidone-iodine or PVP-I. The dressing contains polyethylene glycol (PEG) and purified water as inactive components.\n\nThe povidone polymer provides a sustained release of iodine. The polyethylene glycol provides a moist environment allowing the iodine to reach the bacteria in the wound. PVP-I is the broadest spectrum antiseptic for human use and has no reported cases of acquired resistance and is therefore a very effective bactericide.\n\nThe dressing may be quite difficult to apply to areas such as fingers as it has a tendency not to adhere easily by itself by design, and will require incisions and shaping. Once applied, it will conform to the shape correctly when dressed over. It is very effective at preventing bacterial infection.\n\nAfter a few days the dressing has a tendency to dry out, and must be changed often. Left on too long, it can engage with wound tissue and wound secretions when drier, and be more difficult to remove. A few drops of saline will normally free this up as with most dressings.\n\n"}
{"id": "6082894", "url": "https://en.wikipedia.org/wiki?curid=6082894", "title": "Kyeyang Electric", "text": "Kyeyang Electric\n\nKeyang Electric Co, Ltd. () is a South Korean machinery, electric, engineering and automotive company headquartered in Seoul. It was established in 1977, and manufactures machine, tool and auto parts products, manual and auto pipe cleaner machinery. It has factories in Ansan and Cheonan. The CEO is Lee Hyeong-Ho (이형호).\n\n\n\n"}
{"id": "40564617", "url": "https://en.wikipedia.org/wiki?curid=40564617", "title": "List of Thai inventions and discoveries", "text": "List of Thai inventions and discoveries\n\nThis is a list of Thai inventions and discoveries.\n\n\n\n\n\n\n\n"}
{"id": "41157074", "url": "https://en.wikipedia.org/wiki?curid=41157074", "title": "Marques Brownlee", "text": "Marques Brownlee\n\nMarques K. Brownlee (born December 3, 1993), also known professionally as MKBHD, is an American YouTuber, best known for his technology-focused videos. The channel, whose name is a concatenation of MKB (Brownlee's initials) and HD (for high-definition), has over 7 million subscribers and over 1 billion total video views. In August 2013, Vic Gundotra, former Senior Vice President, Social for Google, called Brownlee \"the best technology reviewer on the planet right now\". During one of the 2016 Democratic presidential primary debates that was cosponsored by YouTube, Brownlee asked the candidates, by video, whether tech companies and the government can find a middle ground over encryption while considering rights to privacy and national security.\nMKBHD joined YouTube on March 21, 2008. He first started uploading technology videos in January 2009, while still in high school, about new products or reviews of products he already owned. He says his first videos were screencasting where he would post an image and just talk over it, then responded to requests from viewers of what they would like to watch. His first several hundred videos were primarily hardware tutorials and freeware. He was later approached by companies to demonstrate their paid software and hardware, but only reviews products that would be of interest to his audience of technology enthusiasts. He is noted for responding to viewers' feedback about what questions they have, and features they want explained, as well as demonstrating a \"deep knowledge of the products he discusses, and his understanding of what his audience wants to learn\". His reviews are often timely, coming out the same day or soon after for anticipated products. He is arguably best known today for his content about smartphones. As of March 2018, his channel has gained over 5.8 million subscribers, making MKBHD one of the most subscribed-to \"tech-based\" YouTube channels. On average, Brownlee uploads one video per week. The channel receives an average of over 6,000 subscribers every day as of September 2018. Brownlee uploaded his 1000th video on March 29, 2018.\n\nBrownlee's reviews have been promoted by other review sites as well. Engadget promoted the site in January 2012 when they featured his tour of the then-new cloud storage service called Insync. In November 2013, one of Brownlee's most viral videos was posted based on the LG G Flex. He performed various scratch tests to portray the self-healing ability of the device. The video hit a million views on the first day. , the video has over 7 million views. In December 2013, Brownlee did an interview with Motorola CEO Dennis Woodside. In May 2014, Brownlee did the first over-the-air interview with Evan Blass, also known by his previous name, evleaks.\n\nBrownlee's video review and scratch test of a rumored 4.7 inch micromax 7 front glass component, uploaded July 7, 2014, gained immediate popularity, being featured on sites such as \"The Verge\", \"Forbes Tech\", \"HuffPost Tech\", \"CNET\", and \"Time Magazine\". The video appeared on NBC news, and in newspapers across the world. , the video has gained over 9.1 million views on YouTube and has had over 60,000 ratings. Brownlee also has a similar video regarding a dummy model of the iPhone 6, uploaded a couple of months earlier, which (as of May 2017) has since gained over 7 million views on YouTube.\n\nIn December 2015, Brownlee did an interview with Kobe Bryant, a professional NBA basketball player, and uploaded the results, titled \"Talking Tech with Kobe Bryant!\" in which he talks about Tech interests of Kobe and the most recent Kobe designed Nike shoes, the \"Kobe 11\".\n\nIn October 2016, he interviewed Apple's senior vice president of Software Engineering Craig Federighi during the release of their latest MacBook Pro 2016.\n\nIn December 2014, Brownlee started his Smartphone Award Series, where he picks the best phones in certain categories from the past year. In 2017, Brownlee created physical awards featured in the video, some of which were later requested and sent to the companies whose phones won them. \n\n† This category was called \"Craziest Design\" in 2014\n\nBrownlee grew up in Maplewood, New Jersey. He attended Columbia High School and studied at the Howe School at Stevens Institute of Technology, where he majored in business and information technology. Brownlee graduated from college in May 2015 and became a full-time YouTuber. His videos were produced at his apartment until he moved out in Spring 2016, and he now works out of a studio in Kearny, New Jersey, using RED video equipment. Being a tech enthusiast, he also owns a Tesla Model S P100D which he occasionally features in his channel. Other than producing content, he plays golf and is a professional ultimate frisbee player for the Philadelphia Phoenix (2017) and Garden State Ultimate (2015–present). Brownlee previously played for the now-folded New Jersey Hammerheads, a team belonging to the American Ultimate Disc League, and the New York Rumble, which was in the now-defunct league Major League Ultimate (MLU).\n\n"}
{"id": "43062570", "url": "https://en.wikipedia.org/wiki?curid=43062570", "title": "Mesotherm", "text": "Mesotherm\n\nA mesotherm (from Greek μέσος \"mesos\" \"intermediate\" and \"thermē\" \"heat\") is a type of animal with a thermoregulatory strategy intermediate to cold-blooded ectotherms and warm-blooded endotherms.\n\nMesotherms have two basic characteristics: \n\nThe first trait distinguishes mesotherms from ectotherms, the second from endotherms. For instance, endotherms, when cold, will generally resort to shivering or metabolizing brown fat to maintain a constant body temperature, leading to higher metabolic rates. A mesotherm, however, will experience lower body temperatures and lower metabolic rates as ambient temperature drop. In addition, mesotherm body temperatures tend to rise as body size increases (a phenomenon known as gigantothermy), unlike endotherms. This reflects the lower surface area to volume ratio in large animals, which reduces rates of heat loss.\n\nWhile extant mesotherms are relatively rare, good examples include tuna, lamnid sharks (e.g., the Great White Shark), the leatherback sea turtle and the various monotremes.\n\nHistorically, the same term was used by Alphonse Pyramus de Candolle to describe plants that require a moderate degree of heat for successful growth. In his scheme, a mesotherm plant grew in regions where the warmest month had a mean temperature greater than and the coldest month had a mean temperature of at least .\n\nThe thermoregulatory status of dinosaurs has long been debated, and is still an active area of research. The term 'mesothermy' was originally coined to advocate for an intermediate status of non-avian dinosaur thermoregulation, between endotherms and ectotherms. A more technical definition was provided by Grady \"et al\", who argued for dinosaur mesothermy on the basis of their intermediate growth rates, and the empirical relationship between growth, metabolism and thermoregulation in extant vertebrates.\n\nThis viewpoint was challenged by D'Emic, who argued that because growth rates are sensitive to seasonal variation in resources, dinosaur maximum growth rates were underestimated by Grady \"et al\". Adjusting dinosaur rates upwards by a factor of two, D'Emic found dinosaurs to grow similarly to mammals, and thus were likely endothermic. However, sensitivity to seasonal variation in resources should be true for all vertebrates. If all vertebrate taxa were similarly adjusted, the relative differences in rates does not change. Dinosaurs remain intermediate growers and good candidates for mesothermy.\n\nNonetheless, the dinosaur mesothermy hypothesis requires further support to be confirmed. Fossil oxygen isotopes, which can reveal an organism's body temperature, should be particularly informative. Recently, a study of theropod and sauropod isotopes offered some support for dinosaur mesothermy. Feathered theropods are probably the best candidates for dinosaur endothermy, yet the examined theropods had relatively low body temperatures . Large sauropods had higher body temperatures , which may be reflective of mesothermic gigantothermy. Future isotopic analysis of small, juvenile dinosaurs will better resolve this question.\n\n"}
{"id": "869886", "url": "https://en.wikipedia.org/wiki?curid=869886", "title": "Nanosys", "text": "Nanosys\n\nNanosys is a nanotechnology company located in Milpitas, California, founded in 2001.\nNanosys designs products for displays based on quantum dots.\n\nNanosys Quantum Dot Enhancement Film, or QDEF, is an optical film component for LED driven LCDs. Each sheet of QDEF contains trillions of tiny Quantum Dot Phosphors. QDEF enables LED-backlit LCDs to be brighter and more colorful by providing a high quality, tri-color white light from a standard blue LED light source. Larger than a water molecule, but smaller than a virus, these tiny phosphors convert blue light from a standard Gallium Nitride (GaN) LED into different wavelengths based upon their size. Larger dots emit longer wavelengths (red), while smaller dots emit shorter wavelengths (green). Blending together a mix of dot colors allows Nanosys to precisely engineer a new spectrum of light to customer specifications.\n\nThe Quantum Dots are tuned to create better color by changing their size during fabrication to emit light at just the right wavelengths. Traditional light emitting materials such as crystal phosphors have a broad fixed spectrum. Quantum dots can convert light to nearly any color in the visible spectrum, giving display designers the ability to tune and match the spectrum more accurately to color filters while improving energy efficiency.\nDesigned to replace the functionality of a diffuser sheet while actively converting color, QDEF can be added to an LCD's film stack with little change in overall thickness or manufacturing process (Fig 1).\n\nQDEF was announced on May 17, 2011 at the Society for Information Display (SID) Display Week tradeshow. It has been adopted in products such as the Amazon Kindle Fire HDX 7 (2013) and the ASUS Zenbook NX-500 (2014).\n\nAt the Consumer Electronics Show 2015 it became known that Nanosys has licensed Samsung Electronics as well as 3M to manufacture QDEF products. QDEFs from 3M are used by top US TV brand Vizio (R-Series) as well as Chinese LED-backlit LCD TV manufacturers Hisense (\"ULED TV\") and TCL (\"QLED TV\") TV sets.\nAnnounced just after CES in January 2010 as part of a commercial agreement with Korean consumer electronics manufacturer and LG subsidiary LG Innotek The quantum rail is a glass capillary optical component containing red and green quantum dots that is inserted between the LEDs and the lightguide panel (LGP) of an LED LCD in manufacturing to improve color gamut.\n\n\n\n\n\n\n\nNanosys was founded by Larry Bock, Charles Lieber and Paul Alivisatos. They were subsequently joined by Steve Empedocles, Wally Parce and Calvin Chow.\n\nMajor funders of the company include Venrock Associates, Samsung, Arch Venture Partners, Intel, El Dorado Ventures, Polaris Venture Partners, Prospect Ventures, Harris & Harris Group, Lux Capital, Kodak, and Wasatch Advisors.\n\nNanosys has developed a significant quantum dot patent portfolio with over 320 issued and pending patents worldwide. These patents cover the fundamentals of quantum dot construction as well as component and manufacturing designs. This portfolio is the result of collaborations between Nanosys and universities such as Massachusetts Institute of Technology (MIT), Lawrence Berkeley National Laboratory and Hebrew University, as well as industry collaborations with companies including Philips-Lumileds and Life Technologies.\n\n"}
{"id": "14804597", "url": "https://en.wikipedia.org/wiki?curid=14804597", "title": "National Fenestration Rating Council", "text": "National Fenestration Rating Council\n\nThe National Fenestration Rating Council (NFRC) is a United States 501(c)3 non-profit organization which sponsors an energy efficiency certification and labeling program for windows, doors, and skylights.\n\nNFRC labels provide performance ratings for such products in five categories: U-value, Solar Heat Gain Coefficient, Visible Transmittance, Air Leakage, and Condensation Resistance. This allows architects, builders, code officials, contractors, home owners, and specifiers to compare the energy efficiency among products, and determine whether a product meets code.\n"}
{"id": "29462241", "url": "https://en.wikipedia.org/wiki?curid=29462241", "title": "Open Data Center Alliance", "text": "Open Data Center Alliance\n\nopendatacenteralliance.org appears to have been closed down.\n\nThe Open Data Center Alliance is an independent organization created in Oct. 2010 with the assistance of Intel to coordinate the development of standards for cloud computing. Approximately 100 companies, which account for more than $50bn of IT spending, have joined the Alliance, including BMW, Royal Dutch Shell and Marriott Hotels. \"The Alliance's Cloud 2015 vision is aimed at creating a federated cloud where common standards will be laid down for those in the hardware and software arena.\"\n\nThe organization sees a growing need for solutions developed in an open, industry-standard and multi-vendor fashion, and has thus created a usage model roadmap featuring 19 prioritized usage models. The usage models provide detailed requirements for data center and cloud solutions, and will include detailed technical documentation discussing the requirements for technology deployments. \n\nTo further its roadmap development, the steering committee established five initial technical workgroups in the areas of infrastructure, management, regulation & ecosystem, security and services.\n\nThe organization delivered a 0.50 usage model roadmap to Open Data Center Alliance technical workgroups in Oct. 2010, and delivered a full 1.0 roadmap for public use in June 2011.\n\nThe steering committee consists of BMW, Capgemini, China Life, China Unicom Group, Deutsche Bank, JPMorgan Chase, Lockheed Martin, Marriott International, Inc., National Australia Bank, Royal Dutch Shell, Terremark and UBS. Other members include AT&T, CERN, eBay, Logica, Motorola Mobility Inc. and Nokia.\n\n\"The demands on the IT organisations are coming at such an alarming rate that there are many, many different solutions being developed today that maybe don't work with each other. We need one voice, one road map, so that companies are able to say to manufacturers here is a clear vision of what they should be developing their product to do.\" says Marvin Wheeler, of Terremark, chairman of the Alliance.\n\n\"While it's unclear how successful this alliance will be, it is at least shedding the spotlight on cloud interoperability, a big emerging issue,\" said Larry Dignan of ZDNet.\n\n"}
{"id": "3687551", "url": "https://en.wikipedia.org/wiki?curid=3687551", "title": "Payment card", "text": "Payment card\n\nPayment cards are part of a payment system issued by financial institutions, such as a bank, to a customer that enables its owner (the cardholder) to access the funds in the customer's designated bank accounts, or through a credit account and make payments by electronic funds transfer and access automated teller machines (ATMs). Such cards are known by a variety of names including bank cards, ATM cards, MAC (money access cards), client cards, key cards or cash cards.\n\nThere are a number of types of payment cards, the most common being credit cards and debit cards. Most commonly, a payment card is electronically linked to an account or accounts belonging to the cardholder. These accounts may be deposit accounts or loan or credit accounts, and the card is a means of authenticating the cardholder. However, stored-value cards store money on the card itself and are not necessarily linked to an account at a financial institution.\n\nIt can also be a smart card that contains a unique card number and some security information such as an expiration date or CVVC (CVV) or with a magnetic strip on the back enabling various machines to read and access information. Depending on the issuing bank and the preferences of the client, this may allow the card to be used as an ATM card, enabling transactions at automatic teller machines; or as a debit card, linked to the client's bank account and able to be used for making purchases at the point of sale; or as a credit card attached to a revolving credit line supplied by the bank.\n\nMost payment cards, such as debit and credit cards can also function as ATM cards, although ATM-only cards are also available. Charge and proprietary cards cannot be used as ATM cards. The use of a credit card to withdraw cash at an ATM is treated differently to a POS transaction, usually attracting interest charges from the date of the cash withdrawal. Interbank networks allow the use of ATM cards at ATMs of private operators and financial institutions other than those of the institution that issued the cards.\n\nAll ATM machines, at a minimum, will permit cash withdrawals of customers of the machine's owner (if a bank-operated machine) and for cards that are affiliated with any ATM network the machine is also affiliated. They will report the amount of the withdrawal and any fees charged by the machine on the receipt. Most banks and credit unions will permit routine account-related banking transactions at the bank's own ATM, including deposits, checking the balance of an account, and transferring money between accounts. Some may provide additional services, such as selling postage stamps.\n\nFor other types of transactions through telephone or online banking, this may be performed with an ATM card without in-person authentication. This includes account balance inquiries, electronic bill payments, or in some cases, online purchases (see Interac Online). However now days, you cannot buy stuff online or in real life if you are a minor with an ATM card.\n\nATM cards can also be used on improvised ATMs such as \"mini ATMs\", merchants' card terminals that deliver ATM features without any cash drawer. These terminals can also be used as cashless scrip ATMs by cashing the receipts they issue at the merchant's point of sale.\n\nHistorically, bank cards have also served the purpose of a cheque guarantee card, a now almost defunct system to guarantee cheques at point of sale.\n\nThe first bank cards were ATM cards issued by Barclays in London in 1967 and by Chemical Bank in Long Island, New York, in 1969. \n\nIn 1972, Lloyds Bank issued the first bank card to feature an information-encoding magnetic strip, using a personal identification number (PIN) for security.\n\nIn some banking networks, the two functions of ATM cards and debit cards are combined into a single card, simply called a \"debit card\" or also commonly a \"bank card\". These are able to perform banking tasks at ATMs and also make point-of-sale transactions, with both features using a PIN.\n\nCanada's Interac and Europe's Maestro are examples of networks that link bank accounts with point-of-sale equipment.\n\nSome debit card networks also started their lives as ATM card networks before evolving into full-fledged debit card networks, example of these networks are: Development Bank of Singapore (DBS)'s Network for Electronic Transfers (NETS) and Bank Central Asia (BCA)'s Debit BCA, both of them were later on adopted by other banks (with Prima Debit being the Prima interbank network version of Debit BCA).\n\nPayment cards are usually plastic cards, and rounded corners with a radius of 2.88–3.48 mm, in accordance with ISO/IEC 7810#ID-1 standard. \n\nThey usually also have a unique embossed card number conforming with the ISO/IEC 7812 numbering standard, the cardholder’s name and the card expiry date, in addition to other security features. \n\nPayment cards have features in common, as well as distinguish features. Types of payment cards can be distinguished on the basis of the features of each type of card, including:\n\nA credit card is linked to a line of credit (usually called a credit limit) created by the issuer of the credit card for the cardholder on which the cardholder can draw (i.e. borrow), either for payment to a merchant for a purchase or as a cash advance to the cardholder. Most credit cards are issued by or through local banks or credit unions, but some non-bank financial institutions also offer cards directly to the public.\n\nThe cardholder can choose either to repay the full outstanding balance by the payment due date or to repay a smaller amount, not less than the \"minimum amount\", by that date. In the former case, interest is typically not charged; while in the latter case, the cardholder will be charged with interest. The rate of interest and method of calculating the charge vary between credit cards, even for different types of card issued by the same company. Many credit cards can also be used to take cash advances through ATMs, which also attract interest charges, usually calculated from the date of cash withdrawal. Some merchants charge a fee for purchases by credit card, as they will be charged a fee by the card issuer.\n\nWith a debit card (also known as a \"bank card\", \"check card\" or some other description) when a cardholder makes a purchase, funds are withdrawn directly either from the cardholder's bank account, or from the remaining balance on the card, instead of the holder repaying the money at a later date. In some cases, the \"cards\" are designed exclusively for use on the Internet, and so there is no physical card.\n\nThe use of debit cards has become widespread in many countries and has overtaken use of cheques, and in some instances cash transactions, by volume. Like credit cards, debit cards are used widely for telephone and internet purchases.\n\nDebit cards can also allow instant withdrawal of cash, acting as the ATM card, and as a cheque guarantee card. Merchants can also offer \"cashback\"/\"cashout\" facilities to customers, where a customer can withdraw cash along with their purchase. Merchants usually do not charge a fee for purchases by debit card.\n\nWith charge cards, the cardholder is required to pay the full balance shown on the statement, which is usually issued monthly, by the payment due date. It is a form of short-term loan to cover the cardholder's purchases, from the date of the purchase and the payment due date, which may typically be up to 55 days. Interest is usually not charged on charge cards and there is usually no limit on the total amount that may be charged. If payment is not made in full, this may result in a late payment fee, the possible restriction of future transactions, and perhaps the cancellation of the card.\n\nAn ATM card (known under a number of names) is any card that can be used in automated teller machines (ATMs) for transactions such as deposits, cash withdrawals, obtaining account information, and other types of transactions, often through interbank networks. Cards may be issued solely to access ATMs, and most debit or credit cards may also be used at ATMs, but charge and proprietary cards cannot.\n\nThe use of a credit card to withdraw cash at an ATM is treated differently to an POS transaction, usually attracting interest charges from the date of the cash withdrawal. The use of a debit card usually does not attract interest. Third party ATM owners may charge a fee for the use of their ATM.\n\nWith a stored-value card, a monetary value is stored on the card, and not in an externally recorded account. This differs from prepaid cards where money is on deposit with the issuer similar to a debit card. One major difference between stored value cards and prepaid debit cards is that prepaid debit cards are usually issued in the name of individual account holders, while stored-value cards are usually anonymous.\n\nThe term \"stored-value card\" means that the funds and or data are physically stored on the card. With prepaid cards the data is maintained on computers controlled by the card issuer. The value stored on the card can be accessed using a magnetic stripe embedded in the card, on which the card number is encoded; using radio-frequency identification (RFID); or by entering a code number, printed on the card, into a telephone or other numeric keypad.\n\nA fleet card is used as a payment card, most commonly for gasoline, diesel and other fuels at gas stations. Fleet cards can also be used to pay for vehicle maintenance and expenses, at the discretion of the fleet owner or manager. The use of a fleet card reduces the need to carry cash, thus increasing the security for fleet drivers. The elimination of cash also helps to prevent fraudulent transactions at the fleet owner's or manager's expense.\n\nFleet cards provide convenient and comprehensive reporting, enabling fleet owners/managers to receive real time reports and set purchase controls with their cards, helping to keep them informed of all business related expenses. They may also reduce administrative work or otherwise be essential in arranging fuel taxation refunds.\n\nOther types of payment cards include:\n\nA number of International Organization for Standardization standards, ISO/IEC 7810, ISO/IEC 7811, ISO/IEC 7812, ISO/IEC 7813, ISO 8583, and ISO/IEC 4909, define the physical properties of payment cards, including size, flexibility, location of the magstripe, magnetic characteristics, and data formats. They also provide the standards for financial cards, including the allocation of card number ranges to different card issuing institutions.\n\nOriginally charge account identification was paper-based. In 1959 American Express was the first charge card operator to issue embossed plastic cards which enabled cards to be manually imprinted for processing, making processing faster and reducing transcription errors. Other credit card issuers followed suit. The information typically embossed are the bank card number, card expiry date and cardholder's name. Though the imprinting method has been predominantly superseded by the magnetic stripe and then by the integrated chip, cards continue to be embossed in case a transaction needs to be processed manually. Under manual processing, cardholder verification was by the cardholder signing the payment voucher after which the merchant would check the signature against the cardholder's signature on the back of the card. Cards conform to the ISO/IEC 7810 ID-1 standard, ISO/IEC 7811 on embossing, and the ISO/IEC 7812 card numbering standard.\n\nMagnetic stripes started to be rolled out on debit cards in the 1970s with the introduction of ATMs. The magnetic stripe stores card data which can be read by physical contact and swiping past a reading head. The magnetic stripe contains all the information appearing on the card face, but allows for faster processing at point-of-sale than the then manual alternative as well as subsequently by the transaction processing company. When the magnetic stripe is being used, the cardholder will have been issued with a PIN, which is used for cardholder identification at the point-of-sale, and a signature is no longer required. The magnetic stripe is in the process of being augmented by the integrated chip.\n\nA smart card, chip card, or integrated circuit card (ICC), is any pocket-sized card with embedded integrated circuits which can process data. This implies that it can receive input which is processed — by way of the ICC applications — and delivered as an output. There are two broad categories of ICCs. Memory cards contain only non-volatile memory storage components, and perhaps some specific security logic. Microprocessor cards contain volatile memory and microprocessor components. The card is made of plastic, generally PVC, but sometimes ABS. The card may embed a hologram to avoid counterfeiting. Using smart cards is also a form of strong security authentication for single sign-on within large companies and organizations.\n\nEMV is the standard adopted by all major issuers of smart payment cards.\n\nProximity card (or prox card) is a generic name for contactless integrated circuit devices used for security access or payment systems. It can refer to the older 125 kHz devices or the newer 13.56 MHz contactless RFID cards, most commonly known as contactless smartcards.\n\nModern proximity cards are covered by the ISO/IEC 14443 (proximity card) standard. There is also a related ISO/IEC 15693 (vicinity card) standard. Proximity cards are powered by resonant energy transfer and have a range of 0–3 inches in most instances. The user will usually be able to leave the card inside a wallet or purse. The price of the cards is also low, usually US$2–$5, allowing them to be used in applications such as identification cards, keycards, payment cards and public transit fare cards.\n\nRe-programmable/dynamic magnetic stripe cards are standard sized transaction cards that include a battery, a processor, and a means (inductive coupling or otherwise)of sending a variable signal to a magnetic stripe reader. Re-programmable stripe cards are often more secure than standard magnetic stripe cards and can transmit information for multiple cardholder accounts.\n\nDue to increased illegal copies of cards with a magnetic stripe, the European Payments Council established a Card Fraud Prevention Task Force in 2003 that spawned a commitment to migrate all ATMs and POS applications to use a chip-and-PIN solution until the end of 2010. The \"SEPA for Cards\" has completely removed the magnetic stripe requirement from the former Maestro debit cards.\n\n"}
{"id": "2521217", "url": "https://en.wikipedia.org/wiki?curid=2521217", "title": "Picnic basket", "text": "Picnic basket\n\nA picnic basket or picnic hamper is a basket intended to hold food and tableware for a picnic meal. Picnic baskets are standard equipment at many picnics.\n\nWhile the basic concept of a picnic basket is quite simple, some picnic baskets sold by gourmet stores are quite large and elaborate, with insulated compartments to hold hot and cold foods, and dishware including wine glasses and porcelain plates which are secured in pockets along with items of cutlery, condiments, corkscrew etc. Many modern picnic baskets also have a special storage section to hold a bottle of wine, the traditional beverage brought along to intimate picnics. Some modern picnic baskets come with detachable blankets (to sit on) as well.\n"}
{"id": "41485856", "url": "https://en.wikipedia.org/wiki?curid=41485856", "title": "Pudding cloth", "text": "Pudding cloth\n\nA pudding cloth is a culinary utensil similar to a cheesecloth or muslin. It is a reusable alternative to cooking in skins made of animal intestines that became popular in England in the seventeenth century for boiling a wide range of puddings.\n\nPrior to the 19th century, the English Christmas pudding was boiled in a pudding cloth. Clootie pudding, a traditional Scottish dessert, is boiled in a pudding cloth. The traditional way to cook jam roly poly is using a pudding cloth.\n\nPease pudding was first made possible at the beginning of the 17th century with the advent of the pudding cloth.\n"}
{"id": "2622542", "url": "https://en.wikipedia.org/wiki?curid=2622542", "title": "Radisys", "text": "Radisys\n\nRadisys Corporation is a publicly traded company located in Hillsboro, Oregon, United States that makes technology used by telecommunications companies in mobile networks. Founded in 1987 in Oregon by former employees of Intel, the company went public in 1995. The company's products are used in mobile network applications such as small cell radio access networks, wireless core network elements, deep packet inspection and policy management equipment; conferencing, and media services including voice, video and data. In 2015, Radisys first-quarter revenues totaled $48.7 million, and employed 700 people. Brian Bronson is the company's chief executive officer.\n\nOn 30 June 2018, Indian conglomerate Reliance Industries acquired Radisys for $74 million.\n\nRadiSys was founded in 1987 as Radix Microsystems in Beaverton, Oregon, by former Intel engineers Dave Budde and Glen Myers. The first investors were employees who put up $50,000 each, with Tektronix later investing additional funds into the company. Originally located in space leased from Sequent Computer Systems, by 1994 the company had grown to annual sales of $20 million. The company's products were computers used in end products such as automated teller machines to paint mixers. On October 20, 1995, the company became a publicly traded company when it held an initial public offering (IPO). The IPO raised $19.6 million for RadiSys after selling 2.7 million shares at $12 per share.\n\nIn 1996, the company moved its headquarters to a new campus in Hillsboro, and at that time sales reached $80 million and the company had a profit of $9.6 million that year with 175 employees. Company co-founder Dave Budde left the company in 1997, with company revenues at $81 million annually at that time. The company grew in part by acquisitions such as Sonitech International in 1997, part of IBM's Open Computing Platform unit and Texas Micro in 1999, all of S-Link in 2001, and Microware also in 2001. RadiSys also moved some production to China in order to take advantage of the lower manufacturing costs.\n\nIn 2002, the company had grown to annual revenues of $200 million, and posted a profit in the fourth quarter for the first time in several quarters. That year Scott Grout was named as chief executive officer of the company and C. Scott Gibson became the chairman of the board, both replacing Glen Myers who co-founded the company. The company sold off its signaling gateway line in 2003.\n\nThey raised $97 million through selling convertible senior notes in November 2003. In 2004, the company stopped granting stock options to employees and transitioned to giving restricted shares for some compensation. RadiSys grew to annual revenues of $320 million by 2005. The company continued to grow through acquisitions such as a $105 million deal that added Convedia Corp. in 2006. RadiSys continued buying assets when it purchased part of Intel's communications business for about $30 million in 2007. After five-straight quarterly losses, the company posted a profit of $481,000 in their 2009 fourth quarter.\n\nIn May 2011, the company announced they were buying Continuous Computing for $105 million in stock and cash. Once the transaction was completed in July 2011, Continuous' CEO Mike Dagenais became the CEO of RadiSys. Dagenais left the company in October 2012 with former CFO Brian Bronson taking over as CEO.\n\nThe company's headquarters are located in the Dawson Creek Industrial Park adjacent to the headquarters of FEI Company in Hillsboro, Oregon, within the Portland metropolitan area. Radisys in 2015 had approximately 700 employees located in offices in the United States (Oregon), India ([Bangalore]), China (Shenzhen), and Canada (Burnaby, BC). Overall, the company only builds about 15 percent of their products, with the remainder outsourced to other companies.\n\nRadisys supports two markets: communications networking and commercial systems. The latter makes products for use in the testing, medical imaging, defense, and industrial automation fields. For example, end-products that Radisys' is a supplier to original equipment manufacturers include items such as MRI scanners, ultrasound equipment, logic analyzers, and items used in semiconductor manufacturing. Communications networking equipment includes those for wireless communications, switches, distribution of video, and internet protocol based networking equipment.\n\nThe company has engineering groups, working on computer architecture, systems integration, embedded operating systems such as OS-9, application-specific integrated circuit (ASIC) design, and middleware. Its platforms and building blocks are based on custom form factors as well as industry standards such as AdvancedTCA, COM Express, CompactPCI, and PCI. In 2009, Radisys' biggest customers were Philips Healthcare, Agilent, Fujitsu, Danaher Corporation, and Nokia Siemens Network (NSN). NSN was the largest single customer, totaling over 43% of revenues.\n\n\n"}
{"id": "565362", "url": "https://en.wikipedia.org/wiki?curid=565362", "title": "Recife Antigo", "text": "Recife Antigo\n\nRecife Antigo (Old Recife) is the historical section of central Recife, Brazil. It is located on the Island of Recife, near the Recife harbor. This historic area has been recently recovered and now holds several clubs, bars and a high-tech center called \"Porto Digital\".\n\nRecife Antigo consists of the initial Portuguese settlement in the 16th century around the port. Sugar cane production from Pernambuco was delivered to Portugal through Recife's port. While Recife had port functions, Olinda was the capital. In 1630, the Dutch invaded Pernambuco, set Olinda partially on fire and Recife became the seat of the Dutch government. Count John Maurice of Nassau-Siegen became Governor-General of the Dutch colony and built a new town on a neighboring island. This city was named Mauritsstadt and the Palacio do Campo das Princesas, seat of the State of Pernambuco government, is built on its ruins. \n\nThe Dutch were forced out in 1654 of a Recife with good infrastructure, for they had built canals and improved the port and the defenses of it. A flourishing Jewish community lived in Recife under them and they had to leave it because of the Portuguese Inquisition. Thus, a group of 24 Portuguese Jews who had previously migrated from Portugal to the Netherlands because of antisemitism, headed farther North with the Dutch, where New Amsterdam --present-day Manhattan-- was founded. The first Synagogue built in the Americas, the Kahal Zur Israel Synagogue, is located in Recife Antigo, on Rua do Bom Jesus, formerly Rua dos Judeus, or Street of the Jews. The Portuguese synagogue was founded in lower Manhattan and it is located on Central Park West in Manhattan nowadays under the name Portuguese & Spanish synagogue. \n"}
{"id": "1137250", "url": "https://en.wikipedia.org/wiki?curid=1137250", "title": "Shape coding", "text": "Shape coding\n\nShape coding is a method of design of a control that allows the control's function to be signified by the shape of the control. It was used successfully by Alphonse Chapanis on airplane controls to improve aviation safety.\n\n"}
{"id": "860000", "url": "https://en.wikipedia.org/wiki?curid=860000", "title": "Smokeless powder", "text": "Smokeless powder\n\nSmokeless powder is the name given to a number of propellants used in firearms and artillery that produce negligible smoke when fired, unlike the gunpowder or black powder they replaced. The term is unique to the United States and is generally not used in other English-speaking countries, which initially used proprietary names such as \"Ballistite\" and \"Cordite\" but gradually shifted to \"propellant\" as the generic term.\n\nThe basis of the term smokeless is that the combustion products are mainly gaseous, compared to around 55% solid products (mostly potassium carbonate, potassium sulfate, and potassium sulfide) for black powder. Despite its name, smokeless powder is not completely free of smoke; while there may be little noticeable smoke from small-arms ammunition, smoke from artillery fire can be substantial. This article focuses on nitrocellulose formulations, but the term smokeless powder was also used to describe various picrate mixtures with nitrate, chlorate, or dichromate oxidizers during the late 19th century, before the advantages of nitrocellulose became evident.\n\nSince the 14th century gunpowder was not actually a physical \"powder\", and smokeless powder can be produced only as a pelletized or extruded granular material. Smokeless powder allowed the development of modern semi- and fully automatic firearms and lighter breeches and barrels for artillery. Burnt gunpowder leaves a thick, heavy fouling that is hygroscopic and causes rusting of the barrel. The fouling left by smokeless powder exhibits none of these properties (though some primer compounds can leave hygroscopic salts that have a similar effect; non-corrosive primer compounds were introduced in the 1920s). This makes an autoloading firearm with many moving parts feasible (which would otherwise jam or seize under heavy black powder fouling).\n\nSmokeless powders are classified as, typically, division 1.3 explosives under the UN \"Recommendations on the transportation of Dangerous goods – Model Regulations\", regional regulations (such as ADR) and national regulations (such the United States' ATF). However, they are used as solid propellants; in normal use, they undergo deflagration rather than detonation.\n\nBefore the widespread introduction of smokeless powder the use of gunpowder or black powder caused many problems on the battlefield. Military commanders since the Napoleonic Wars reported difficulty with giving orders on a battlefield obscured by the smoke of firing. Verbal commands could not be heard above the noise of the guns, and visual signals could not be seen through the thick smoke from the gunpowder used by the guns. Unless there was a strong wind, after a few shots, soldiers using gunpowder ammunition would have their view obscured by a huge cloud of smoke. Snipers or other concealed shooters were given away by a cloud of smoke over the firing position. Gunpowder is a low explosive that does not detonate but rather deflagrates (burns quickly at \"subsonic\" velocity). Gunpowder produces lower pressures and is about three times less powerful when compared to smokeless powder. Gunpowder is also corrosive, making cleaning mandatory after every use. Likewise, gunpowder's tendency to produce severe fouling caused actions to jam and often made reloading difficult.\n\nNitroglycerine was synthesized by the Italian chemist Ascanio Sobrero in 1847. It was subsequently developed and manufactured by Alfred Nobel as an industrial explosive, but even then it was unsuitable as a propellant: despite its energetic and smokeless qualities, it detonates instead of deflagrating smoothly, making it more liable to shatter a gun, rather than propel a projectile out of it. Nitroglycerine is also highly sensitive, making it unfit to be carried in battlefield conditions.\n\nA major step forward was the invention of guncotton, a nitrocellulose-based material, by German chemist Christian Friedrich Schönbein in 1846. He promoted its use as a blasting explosive and sold manufacturing rights to the Austrian Empire. Guncotton was more powerful than gunpowder, but at the same time was once again somewhat more unstable. John Taylor obtained an English patent for guncotton; and John Hall & Sons began manufacture in Faversham.\n\nEnglish interest languished after an explosion destroyed the Faversham factory in 1847. Austrian Baron Wilhelm Lenk von Wolfsberg built two guncotton plants producing artillery propellent, but it too was dangerous under field conditions, and guns that could fire thousands of rounds using gunpowder would reach the end of their service life after only a few hundred shots with the more powerful guncotton. Small arms could not withstand the pressures generated by guncotton.\n\nAfter one of the Austrian factories blew up in 1862, Thomas Prentice & Company began manufacturing guncotton in Stowmarket in 1863; and British War Office chemist Sir Frederick Abel began thorough research at Waltham Abbey Royal Gunpowder Mills leading to a manufacturing process that eliminated the impurities in nitrocellulose making it safer to produce and a stable product safer to handle. Abel patented this process in 1865 when the second Austrian guncotton factory exploded. After the Stowmarket factory exploded in 1871, Waltham Abbey began production of guncotton for torpedo and mine warheads.\n\nIn 1863, Prussian artillery captain Johann F. E. Schultze patented a small-arms propellent of nitrated hardwood impregnated with saltpeter or barium nitrate. Prentice received an 1866 patent for a sporting powder of nitrated paper manufactured at Stowmarket, but ballistic uniformity suffered as the paper absorbed atmospheric moisture. In 1871, Frederick Volkmann received an Austrian patent for a colloided version of Schultze powder called \"Collodin\", which he manufactured near Vienna for use in sporting firearms. Austrian patents were not published at the time, and the Austrian Empire considered the operation a violation of the government monopoly on explosives manufacture and closed the Volkmann factory in 1875.\n\nIn 1882, the Explosives Company at Stowmarket patented an improved formulation of nitrated cotton gelatinised by ether-alcohol with nitrates of potassium and barium. These propellants were suitable for shotguns but not rifles, because rifling results in resistance to a smooth expansion of the gas, which is reduced in smoothbore shotguns.\n\nIn 1884, Paul Vieille invented a smokeless powder called Poudre B (short for \"poudre blanche\"—white powder, as distinguished from black powder) made from 68.2% insoluble nitrocellulose, 29.8% soluble nitrocellulose gelatinized with ether and 2% paraffin. This was adopted for the Lebel rifle. It was passed through rollers to form paper thin sheets, which were cut into flakes of the desired size. The resulting propellant, today known as \"pyrocellulose\", contains somewhat less nitrogen than guncotton and is less volatile. A particularly good feature of the propellant is that it will not detonate unless it is compressed, making it very safe to handle under normal conditions.\n\nVieille's powder revolutionized the effectiveness of small guns because it gave off almost no smoke and was three times more powerful than black powder. Higher muzzle velocity meant a flatter trajectory and less wind drift and bullet drop, making shots practicable. Since less powder was needed to propel a bullet, the cartridge could be made smaller and lighter. This allowed troops to carry more ammunition for the same weight. Also, it would burn even when wet. Black powder ammunition had to be kept dry and was almost always stored and transported in watertight cartridges.\n\nOther European countries swiftly followed and started using their own versions of Poudre B, the first being Germany and Austria, which introduced new weapons in 1888. Subsequently, Poudre B was modified several times with various compounds being added and removed. Krupp began adding diphenylamine as a stabilizer in 1888.\n\nMeanwhile, in 1887, Alfred Nobel obtained an English patent for a smokeless gunpowder he called Ballistite. In this propellant the fibrous structure of cotton (nitro-cellulose) was destroyed by a nitro-glycerine solution instead of a solvent. In England in 1889, a similar powder was patented by Hiram Maxim, and in the United States in 1890 by Hudson Maxim. Ballistite was patented in the United States in 1891.\n\nThe Germans adopted ballistite for naval use in 1898, calling it WPC/98. The Italians adopted it as \"filite\", in cord instead of flake form, but realising its drawbacks changed to a formulation with nitroglycerine they called \"solenite\". In 1891 the Russians tasked the chemist Mendeleev with finding a suitable propellant. He created nitrocellulose gelatinised by ether-alcohol, which produced more nitrogen and more uniform colloidal structure than the French use of nitro-cottons in Poudre B. He called it pyrocollodion.\n\nBritain conducted trials on all the various types of propellant brought to their attention, but were dissatisfied with them all and sought something superior to all existing types. In 1889, Sir Frederick Abel, James Dewar and Dr W Kellner patented (Nos 5614 and 11,664 in the names of Abel and Dewar) a new formulation that was manufactured at the Royal Gunpowder Factory at Waltham Abbey. It entered British service in 1891 as Cordite Mark 1. Its main composition was 58% Nitro-glycerine, 37% Guncotton and 3% mineral jelly. A modified version, Cordite MD, entered service in 1901, with the guncotton percentage increased to 65% and nitro-glycerine reduced to 30%. This change reduced the combustion temperature and hence erosion and barrel wear. Cordite's advantages over gunpowder were reduced maximum pressure in the chamber (hence lighter breeches, etc.) but longer high pressure. Cordite could be made in any desired shape or size. The creation of cordite led to a lengthy court battle between Nobel, Maxim, and another inventor over alleged British patent infringement.\n\nThe Anglo-American Explosives Company began manufacturing its shotgun powder in Oakland, New Jersey in 1890. DuPont began producing guncotton at Carneys Point Township, New Jersey in 1891. Charles E. Munroe of the Naval Torpedo Station in Newport, Rhode Island patented a formulation of guncotton colloided with nitrobenzene, called \"Indurite\", in 1891. Several United States firms began producing smokeless powder when Winchester Repeating Arms Company started loading sporting cartridges with Explosives Company powder in 1893. California Powder Works began producing a mixture of nitroglycerine and nitrocellulose with ammonium picrate as \"Peyton Powder\", Leonard Smokeless Powder Company began producing nitroglycerine-nitrocellulose \"Ruby\" powders, Laflin & Rand negotiated a license to produce \"Ballistite\", and DuPont started producing smokeless shotgun powder. The United States Army evaluated 25 varieties of smokeless powder and selected \"Ruby\" and \"Peyton Powders\" as the most suitable for use in the Krag-Jørgensen service rifle. \"Ruby\" was preferred, because tin-plating was required to protect brass cartridge cases from picric acid in the \"Peyton Powder\". Rather than paying the required royalties for \"Ballistite\", Laflin & Rand financed Leonard's reorganization as the American Smokeless Powder Company. United States Army Lieutenant Whistler assisted American Smokeless Powder Company factory superintendent Aspinwall in formulating an improved powder named W.A. for their efforts. W.A. smokeless powder was the standard for United States military service rifles from 1897 until 1908.\n\nIn 1897, United States Navy Lieutenant John Bernadou patented a nitrocellulose powder colloided with ether-alcohol. The Navy licensed or sold patents for this formulation to DuPont and the California Powder Works while retaining manufacturing rights for the Naval Powder Factory, Indian Head, Maryland constructed in 1900. The United States Army adopted the Navy single-base formulation in 1908 and began manufacture at Picatinny Arsenal. By that time Laflin & Rand had taken over the American Powder Company to protect their investment, and Laflin & Rand had been purchased by DuPont in 1902. Upon securing a 99-year lease of the Explosives Company in 1903, DuPont enjoyed use of all significant smokeless powder patents in the United States, and was able to optimize production of smokeless powder. When government anti-trust action forced divestiture in 1912, DuPont retained the nitrocellulose smokeless powder formulations used by the United States military and released the double-base formulations used in sporting ammunition to the reorganized Hercules Powder Company. These newer and more powerful propellants were more stable and thus safer to handle than Poudre B.\n\nCurrently, propellants using nitrocellulose (detonation velocity ) (typically an ether-alcohol colloid of nitrocellulose) as the sole explosive propellant ingredient are described as single-base powder.\n\nPropellants mixtures containing nitrocellulose and nitroglycerin (detonation velocity ) as explosive propellant ingredients are known as double-base powder.\n\nDuring the 1930s triple-base propellant containing nitrocellulose, nitroglycerin, and a substantial quantity of nitroguanidine (detonation velocity ) as explosive propellant ingredients was developed. These propellant mixtures have reduced flash and flame temperature without sacrificing chamber pressure compared to single and double base propellants, albeit at the cost of more smoke. In practice, triple base propellants are reserved mainly for large caliber ammunition such as used in (naval) artillery and tank guns. During World War II it had some use by British artillery. After that war it became the standard propellant in all British large caliber ammunition designs except small-arms. Most western nations, except the United States, followed a similar path.\n\nIn the late 20th century new propellant formulations started to appear. These are based on nitroguanidine and high explosives of the RDX type (detonation velocity ).\n\nDetonation velocities are of limited value in assessing the reaction rates of nitrocellulose propellants formulated to avoid detonation. Although the slower reaction is often described as burning because of similar gaseous end products at elevated temperatures, the decomposition differs from combustion in an oxygen atmosphere. Conversion of nitrocellulose propellants to high-pressure gas proceeds from the exposed surface to the interior of each solid particle in accordance with Piobert's law. Studies of solid single- and double-base propellant reactions suggest reaction rate is controlled by heat transfer through the temperature gradient across a series of zones or phases as the reaction proceeds from the surface into the solid. The deepest portion of the solid experiencing heat transfer melts and begins phase transition from solid to gas in a \"foam zone\". The gaseous propellant decomposes into simpler molecules in a surrounding \"fizz zone\". Energy is released in a luminous outer \"flame zone\" where the simpler gas molecules react to form conventional combustion products like steam and carbon monoxide. The \"foam zone\" acts as an insulator slowing the rate of heat transfer from the \"flame zone\" into the unreacted solid. Reaction rates vary with pressure; because the foam allows less effective heat transfer at low pressure, with greater heat transfer as higher pressures compress the gas volume of that foam. Propellants designed for a minimum heat transfer pressure may fail to sustain the \"flame zone\" at lower pressures.\n\nNitrocellulose deteriorates with time, yielding acidic byproducts. Those byproducts catalyze the further deterioration, increasing its rate. The released heat, in case of bulk storage of the powder, or too large blocks of solid propellant, can cause self-ignition of the material. Single-base nitrocellulose propellants are hygroscopic and most susceptible to degradation; double-base and triple-base propellants tend to deteriorate more slowly. To neutralize the decomposition products, which could otherwise cause corrosion of metals of the cartridges and gun barrels, calcium carbonate is added to some formulations.\n\nTo prevent buildup of the deterioration products, stabilizers are added. Diphenylamine is one of the most common stabilizers used. Nitrated analogs of diphenylamine formed in the process of stabilizing decomposing powder are sometimes used as stabilizers themselves. The stabilizers are added in the amount of 0.5–2% of the total amount of the formulation; higher amounts tend to degrade its ballistic properties. The amount of the stabilizer is depleted with time. Propellants in storage should be periodically tested for the amount of stabilizer remaining, as its depletion may lead to auto-ignition of the propellant.\n\nSmokeless powder may be corned into small spherical balls or extruded into cylinders or strips with many cross-sectional shapes (strips with various rectangular proportions, single or multi-hole cylinders, slotted cylinders) using solvents such as ether. These extrusions can be cut into short ('flakes') or long pieces ('cords' many inches long). Cannon powder has the largest pieces.\n\nThe properties of the propellant are greatly influenced by the size and shape of its pieces. The specific surface area of the propellant influences the speed of burning, and the size and shape of the particles determine the specific surface area. By manipulation of the shape it is possible to influence the burning rate and hence the rate at which pressure builds during combustion. Smokeless powder burns only on the surfaces of the pieces. Larger pieces burn more slowly, and the burn rate is further controlled by flame-deterrent coatings that retard burning slightly. The intent is to regulate the burn rate so that a more or less constant pressure is exerted on the propelled projectile as long as it is in the barrel so as to obtain the highest velocity. The perforations stabilize the burn rate because as the outside burns inward (thus shrinking the burning surface area) the inside is burning outward (thus increasing the burning surface area, but faster, so as to fill up the increasing volume of barrel presented by the departing projectile). Fast-burning pistol powders are made by extruding shapes with more area such as flakes or by flattening the spherical granules. Drying is usually performed under a vacuum. The solvents are condensed and recycled. The granules are also coated with graphite to prevent static electricity sparks from causing undesired ignitions.\n\nFaster-burning propellants generate higher temperatures and higher pressures, however they also increase wear on gun barrels.\n\nThe propellant formulations may contain various energetic and auxiliary components:\n\nThe United States Navy manufactured single-base tubular powder for naval artillery at Indian Head, Maryland, beginning in 1900. Similar procedures were used for United States Army production at Picatinny Arsenal beginning in 1907 and for manufacture of smaller grained Improved Military Rifle (IMR) powders after 1914. Short-fiber cotton linter was boiled in a solution of sodium hydroxide to remove vegetable waxes, and then dried before conversion to nitrocellulose by mixing with concentrated nitric and sulfuric acids. Nitrocellulose still resembles fibrous cotton at this point in the manufacturing process, and was typically identified as pyrocellulose because it would spontaneously ignite in air until unreacted acid was removed. The term guncotton was also used; although some references identify guncotton as a more extensively nitrated and refined product used in torpedo and mine warheads prior to use of TNT.\n\nUnreacted acid was removed from pyrocellulose pulp by a multistage draining and water washing process similar to that used in paper mills during production of chemical woodpulp. Pressurized alcohol removed remaining water from drained pyrocellulose prior to mixing with ether and diphenylamine. The mixture was then fed through a press extruding a long tubular cord form to be cut into grains of the desired length.\n\nAlcohol and ether were then evaporated from \"green\" powder grains to a remaining solvent concentration between 3 percent for rifle powders and 7 percent for large artillery powder grains. Burning rate is inversely proportional to solvent concentration. Grains were coated with electrically conductive graphite to minimize generation of static electricity during subsequent blending. \"Lots\" containing more than ten tonnes of powder grains were mixed through a tower arrangement of blending hoppers to minimize ballistic differences. Each blended lot was then subjected to testing to determine the correct loading charge for the desired performance.\n\nMilitary quantities of old smokeless powder were sometimes reworked into new lots of propellants. Through the 1920s Fred Olsen worked at Picatinny Arsenal experimenting with ways to salvage tons of single-base cannon powder manufactured for World War I. Olsen was employed by Western Cartridge Company in 1929 and developed a process for manufacturing spherical smokeless powder by 1933. Reworked powder or washed pyrocellulose can be dissolved in ethyl acetate containing small quantities of desired stabilizers and other additives. The resultant syrup, combined with water and surfactants, can be heated and agitated in a pressurized container until the syrup forms an emulsion of small spherical globules of the desired size. Ethyl acetate distills off as pressure is slowly reduced to leave small spheres of nitrocellulose and additives. The spheres can be subsequently modified by adding nitroglycerine to increase energy, flattening between rollers to a uniform minimum dimension, coating with phthalate deterrents to retard ignition, and/or glazing with graphite to improve flow characteristics during blending.\n\nModern smokeless powder is produced in the United States by St. Marks Powder, Inc. owned by General Dynamics.\n\nMuzzle flash is the light emitted in the vicinity of the muzzle by the hot propellant gases and the chemical reactions that follow as the gases mix with the surrounding air. Before projectiles exit a slight pre-flash may occur from gases leaking past the projectiles. Following muzzle exit the heat of gases is usually sufficient to emit visible radiation – the primary flash. The gases expand but as they pass through the Mach disc they are re-compressed to produce an intermediate flash. Hot combustible gases (e.g. hydrogen and carbon-monoxide) may follow when they mix with oxygen in the surrounding air to produce the secondary flash, the brightest. The secondary flash does not usually occur with small-arms.\n\nNitrocellulose contains insufficient oxygen to completely oxidize its carbon and hydrogen. The oxygen deficit is increased by addition of graphite and organic stabilizers. Products of combustion within the gun barrel include flammable gasses like hydrogen and carbon monoxide. At high temperature, these flammable gasses will ignite when turbulently mixed with atmospheric oxygen beyond the muzzle of the gun. During night engagements the flash produced by ignition can reveal the location of the gun to enemy forces and cause temporary night-blindness among the gun crew by photo-bleaching visual purple.\n\nFlash suppressors are commonly used on small arms to reduce the flash signature, but this approach is not practical for artillery. Artillery muzzle flash up to from the muzzle has been observed, and can be reflected off clouds and be visible for distances up to . For artillery the most effective method is a propellant that produces a large proportion of inert nitrogen at relatively low temperatures that dilutes the combustible gases. Triple based propellants are used for this because of the nitrogen in the nitroguanidine.\n\nBefore the use of triple based propellants the usual method of flash reduction was to add inorganic salts like potassium chloride so their specific heat capacity might reduce the temperature of combustion gasses and their finely divided particulate smoke might block visible wavelengths of radiant energy of combustion.\n\n\n\n"}
{"id": "6079682", "url": "https://en.wikipedia.org/wiki?curid=6079682", "title": "Susceptor", "text": "Susceptor\n\nA susceptor is a material used for its ability to absorb electromagnetic energy and convert it to heat (which is sometimes designed to be re-emitted as infrared thermal radiation). This energy is typically radiofrequency or microwave radiation used in industrial heating processes, and also occasionally in microwave cooking. The name is derived from \"susceptance\", an electrical property of materials that measures their tendency to convert electromagnetic energy to heat.\n\nIn microwave cooking, susceptors are built into paper packaging of certain foods, where they absorb microwaves which penetrate the packaging. This process raises the susceptor patch temperature to levels where it may then heat food by conduction or by infrared radiation.\n\n\nSusceptors are usually made of metallised film, ceramics or metals (such as aluminium flakes).\n\nThe susceptor (which may be located on examination from its gray or blue-gray color, which is different from paper) is the reason products meant to be browned via susceptor-generated thermal radiation carry instructions to microwave the food while still inside its packaging.\n\nSusceptors meant to heat goods by direct conduction in places where less browning will occur may be seen in the gray lining of packaging directly holding food and in good contact with it. A typical example of the latter is the paper susceptor-lined dish directly holding a microwaveable pot pie or casserole.\n\nSusceptors built into packaging create high temperatures in a microwave oven. This is useful for crisping and browning foods, as well as concentrating heat on the oil in a microwave popcorn bag (which is solid at room temperature) in order to melt it rapidly.\n\nAmong the first microwave susceptors marketed were those from the mid-1980s in a product called McCain Micro Chips by McCain Foods. It consisted of a susceptor sheet which cooked French fries in a microwave oven. These are currently used in several types of packaging for heating and cooking products in microwave ovens. Care in package design and use is needed for proper food safety.\n\nA \"crisping sleeve\" is a device made of paperboard and affixed with a susceptor used both as a rigid container to support the food items within and to focus heat on the foodstuff. They are generally intended for a single use. Hot Pockets are a form of the crisping sleeve, while the actual cuisine is a stuffed bread type of food.\n\n\n\n"}
{"id": "2455122", "url": "https://en.wikipedia.org/wiki?curid=2455122", "title": "TATB", "text": "TATB\n\nTATB, triaminotrinitrobenzene or 2,4,6-triamino-1,3,5- trinitrobenzene is an aromatic explosive, based on the basic six-carbon benzene ring structure with three nitro functional groups (NO) and three amine (NH) groups attached, alternating around the ring.\n\nTATB is a powerful explosive (somewhat less powerful than RDX, but more than TNT), but it is extremely insensitive to shock, vibration, fire, or impact. Because it is so difficult to detonate by accident, even under severe conditions, it has become preferred for applications where extreme safety is required, such as the explosives used in nuclear weapons, where accidental detonation during an airplane crash or rocket misfiring would present extreme dangers. All British \nnuclear warheads, except those where weight is a factor, are believed to use TATB-based explosives for main explosive charges. According to David Albright, South Africa's nuclear weapons used TATB to increase their safety.\n\nTATB is normally used as the explosive ingredient in plastic bonded explosive compositions, such as PBX-9502, LX-17-0, and PBX-9503 (with 15% HMX). These formulations are described as Insensitive High Explosives or IHE in nuclear weapons literature.\n\nThough it could theoretically be mixed with other explosive compounds in castable mixtures or other use forms, the applications for such forms would be unclear since they would largely undo the insensitivity of pure TATB.\n\nTATB's chemical structure is somewhat similar to the powerful experimental insensitive high explosive FOX-7.\n\nAt a pressed density of 1.80, TATB has a velocity of detonation of 7,350 meters per second.\n\nTATB has a crystal density of 1.93 grams/cm, though most use forms have no higher density than 1.80. TATB melts at 350 °C. The chemical formula for TATB is C(NO)(NH).\n\nTATB is a bright yellow color.\n\nTATB has been found to remain stable at temperatures at least as high as 250 °C for prolonged periods of time.\n\nTATB is produced by nitration of 1,3,5-trichlorobenzene to 1,3,5-trichloro-2,4,6-trinitrobenzene, then the chlorine atoms are substituted with amine groups.\n\nHowever, it is likely that the production of TATB will be switched over to a process involving the nitration and transamination of phloroglucinol, since this process is milder, cheaper, and reduces the amount of ammonium chloride salt produced in waste effluents (greener).\n\nStill another process has been found for the production of TATB from materials that are surplus to military use. 1,1,1-trimethylhydrazinium iodide (TMHI) is formed from the rocket fuel unsymmetrical dimethylhydrazine (UDMH) and methyl iodide, and acts as a vicarious nucleophilic substitution (VNS) amination reagent. When Picramide, which is easily produced from Explosive D, is reacted with TMHI it is aminated to TATB. Thus, materials that would have had to have been destroyed when no longer needed are converted into a high value explosive.\n\n\n"}
{"id": "805752", "url": "https://en.wikipedia.org/wiki?curid=805752", "title": "Tritium radioluminescence", "text": "Tritium radioluminescence\n\nTritium radioluminescence is the use of gaseous tritium, a radioactive isotope of hydrogen, to create visible light. Tritium emits electrons through beta decay and, when they interact with a phosphor material, light is emitted through the process of phosphorescence. The overall process of using a radioactive material to excite a phosphor and ultimately generate light is called radioluminescence. As tritium illumination requires no electrical energy, it has found wide use in applications such as emergency exit signs, illumination of wristwatches, and portable yet very reliable sources of low intensity light which won't degrade human night vision. Gun sights for night use and small lights (which need to be more reliable than battery powered lights yet not interfere with night vision or be bright enough to easily give away one's location) used mostly by military personnel fall under the latter application.\n\nTritium lighting is made using glass tubes with a phosphor layer in them and tritium gas inside the tube. Such a tube is known as a \"gaseous tritium light source\" (GTLS), or \"beta light\" (since the tritium undergoes beta decay).\n\nThe tritium in a gaseous tritium light source undergoes beta decay, releasing electrons that cause the phosphor layer to phosphoresce. \n\nDuring manufacture, a length of borosilicate glass tube that has had the inside surface coated with a phosphor-containing material is filled with radioactive tritium. The tube is then sealed at the desired length using a carbon dioxide laser. Borosilicate is preferred for its strength and resistance to breakage. In the tube, the tritium gives off a steady stream of electrons due to beta decay. These particles excite the phosphor, causing it to emit a low, steady glow. Tritium is not the only material that can be used for self-powered lighting. Radium was used to make self-luminous paint from the early years of the 20th century until approximately 1970. Promethium briefly replaced radium as a radiation source. Tritium is the only radiation source used in radioluminescent light sources today.\n\nVarious preparations of the phosphor compound can be used to produce different colors of light. Some of the colors that have been manufactured in addition to the common phosphors are green, red, blue, yellow, purple, orange, and white.\n\nThe GTLSs used in watches give off a small amount of light: not enough to be seen in daylight, but enough to be visible in the dark from a distance of several meters. The average such GTLS has a useful life of 10–20 years. As the tritium component of the lighting is often more expensive than the rest of the watch itself, manufacturers try to use as little as possible. Being an unstable isotope with a half-life of 12.32 years, the rate of beta emissions decreases by half in that period. Additionally, phosphor degradation will cause the brightness of a tritium tube to drop by more than half in that period. The more tritium that is initially placed in the tube, the brighter it is to begin with, and the longer its useful life. Tritium exit signs usually come in three brightness levels guaranteed for 10-, 15-, or 20-year useful life expectancies. The difference between the signs is how much tritium the manufacturer installs.\n\nThe light produced by GTLSs varies in colour and size. Green is usually the brightest color and white the least bright. Sizes range from tiny tubes small enough to fit on the hand of a watch to ones the size of a pencil. Large tubes (5mm diameter and up to 100mm long) are usually only found in green, and can surprisingly be not as bright as the standard 22.5mm x 3mm sized tritium; this smaller size is usually the brightest and is used mainly in keychains available commercially.\n\nThese light sources are most often seen as \"permanent\" illumination for the hands of wristwatches intended for diving, nighttime, or combat use. They are also used in glowing novelty keychains and in self-illuminated exit signs. They are favored by the military for applications where a power source may not be available, such as for instrument dials in aircraft, compasses, and sights for weapons.\n\nTritium lights or \"beta lights\" were formerly used in fishing lures. Some flashlights have slots for tritium vials so that the flashlight can be easily located in the dark.\n\nTritium is used to illuminate the iron sights of some small arms. The reticle on the SA80's optical SUSAT sight as well as the LPS 4x6° TIP2 telescopic sight of a PSL rifle, contains a small amount of tritium for the same effect as an example of tritium use on a rifle sight. The electrons emitted by the radioactive decay of the tritium cause phosphor to glow, thus providing a long-lasting (several years) and non-battery-powered firearms sight that is visible in dim lighting conditions. The tritium glow is not noticeable in bright conditions such as during daylight, however. As a result, some manufacturers have started to integrate fiber optic sights with tritium vials to provide bright, high-contrast firearms sights in both bright and dim conditions.\n\nWhile these devices contain a radioactive substance, it is currently believed that self-powered lighting does not pose a significant health concern. A 2007 report by the UK government's Health Protection Agency Advisory Group on Ionizing Radiation declared the health risks of tritium exposure to be double than previously set by the International Commission on Radiological Protection, but encapsulated tritium lighting devices, typically taking the form of a luminous glass tube embedded in a thick block of clear plastic, prevent the user from being exposed to the tritium at all unless the device is broken apart.\n\nTritium presents no external beta radiation threat when encapsulated in non-hydrogen-permeable containers due to its low penetration depth, which is insufficient to penetrate intact human skin. However, GTLS devices do emit low levels of X-rays due to bremsstrahlung. The primary danger from tritium arises if it is inhaled, ingested, injected, or absorbed into the body. This results in the absorption of the emitted radiation in a relatively small region of the body, again due to the low penetration depth. The biological half-life of tritium—the time it takes for half of an ingested dose to be expelled from the body—is low, at only 12 days. Tritium excretion can be accelerated further by increasing water intake to 3–4 liters/day.\n\nDirect, short-term exposure to small amounts of tritium is mostly harmless. If a tritium tube breaks, one should leave the area and allow the gas to diffuse into the air. Tritium exists naturally in the environment, but in very small quantities.\n\nBecause tritium is used in boosted fission weapons and thermonuclear weapons (though in quantities several thousand times larger than that in a keychain), consumer and safety devices containing tritium for use in the United States are subject to certain possession, resale, disposal, and use restrictions. In the US, devices such as self-luminous exit signs, gauges, wristwatches, etc. that contain small amounts of tritium are under the jurisdiction of the Nuclear Regulatory Commission, and are subject to possession, distribution, and import and export regulations found in 10 CFR Parts, 30, 32, and 110. They are also subject to regulations for possession, use, and disposal in certain states. Luminous products containing more tritium than needed for a wristwatch are not widely available at retail outlets in the United States.\n\nThey are readily sold and used in the UK and US. They are regulated in England and Wales by environmental health departments of local councils. Tritium lighting is legal in most of Asia and Australia.\n\n\n"}
{"id": "11484653", "url": "https://en.wikipedia.org/wiki?curid=11484653", "title": "Videcom international", "text": "Videcom international\n\nVidecom International Limited is a United Kingdom travel technology company based in Henley-on-Thames. It designs, develops and provides modern computer reservations systems to airlines and the travel industry, specializing in the hosting and distribution of airline sales.\n\nThe system is connected to the Global Distribution Systems of Sabre, Amadeus, Galileo, Worldspan and Abacus, which travel agents use to make airline bookings, and is also connected to other airline systems for interline bookings. The IATA airline designator for Videcom is U1.\n\nFounded in 1972, the company originally manufactured computer terminals for uses throughout the aviation and travel sectors, including airline reservation centers, airport operations and travel agency systems. Over 450,000 computer terminals were manufactured between 1972 and 2002 and refurbishments are still supported today, with many units still in use globally at airlines and airports. The company diversified into airline software development in 1987.\n\nIn 1976, Videcom International along with British Airways, British Caledonian and CCL, launched Travicom, the world's first multi-access reservations system. It was wholly based on Videcom technology. They formed a network to thousands of travel agents in the UK providing distribution for 49 subscribing international airlines, including British Airways, British Caledonian, TWA, Pan American World Airways, Qantas, Singapore Airlines, Air France, Lufthansa, SAS, Air Canada, KLM, Alitalia, Cathay Pacific, JAL) and some African airlines. It allowed agents and airlines to communicate via a common distribution language and network, handling 97% of UK airline business trade bookings by 1987.\n\nThe system went on to be replicated by Videcom in other areas of the world including the Middle East (DMARS), New Zealand, Kuwait (KMARS), Ireland, Caribbean, United States and Hong Kong. The Travicom multi access system was eventually replaced by Galileo in the UK and in 1988, Travel Automations Services Ltd (trading as Travicom) changed its trading name to Galileo UK and agents using Travicom were migrated to Galileo.\n\nSince the late 1980s to the current day, Videcom has continued to develop products mostly related to airlines and airports, including terminal emulator software, airport check-in systems, Common Use Terminal Equipment (CUTE), Aircraft Weight and Balance systems, Unit Load Device management, and a modern Airline Reservations System, including an integrated Departure Control System.\n\nIn 2004, Videcom sold some of their standalone products, such as standalone DCS, Weight and Balance and the Common Use System, to Ultra Electronics.\n\nThe new product suite, which the company has provided to approximately 35 airlines since 2001, includes VRS, the Airline Reservations Systems and an integrated Departure Control System used by regional and international airlines.\n\nInventory Hosting\n\nInternet Booking Engine\n\nTicketing\n\nGlobal Distribution Systems\nConnected to:\n\nInterline Sales\n\nReporting\n\n"}
{"id": "42411343", "url": "https://en.wikipedia.org/wiki?curid=42411343", "title": "Yüksel Tohumculuk", "text": "Yüksel Tohumculuk\n\nYüksel Tohumculuk (Yuksel Seeds) is a Turkish plant breeding company, based in Antalya. It is the largest developer, marketer and producer of vegetable seeds in Turkey.\n\nYuksel Seeds operates five research and production stations and owns 1,200 hectares of agricultural land and 650 acres of qualified high-tech greenhouses with the presence of advanced R&D activities. The company has a wide chain of distributors worldwide and exports its products to over 40 countries.\n\nThe company is run as a family business by seven brothers, the oldest of which, Mehmet, is a founder, general manager and the main breeder of the company. He is also a member of the Board of Antalyaspor, a local football club. Mehmet Yüksel was twice selected the successful entrepreneur of the year (in 2007 and in 2011) by Antalya Industrialists' and Businessmen's Association (ANSIAD).\n\nYuksel Seeds is one of the sponsors of Antalyaspor, a local football club.\n\n\n"}
