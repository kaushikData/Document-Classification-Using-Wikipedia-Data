{"id": "39116911", "url": "https://en.wikipedia.org/wiki?curid=39116911", "title": "ATSC 3.0", "text": "ATSC 3.0\n\nATSC 3.0 is a major version of the ATSC standards for television broadcasting created by the Advanced Television Systems Committee (ATSC). ATSC 3.0 comprises around 20 standards covering different aspects of the system and in total will have over 1,000 pages of documentation.\n\nThe standards are designed to offer support for newer technologies, including HEVC for video channels of up to 2160p 4K resolution at 120 frames per second, wide color gamut, high dynamic range, Dolby AC-4 and MPEG-H 3D Audio, datacasting capabilities, and more robust mobile television support. The capabilities have also been foreseen as a way to enable targeted advertising and finer public alerting.\n\nThe first major deployments of ATSC 3.0 occurred in South Korea, with the country's major television networks launching terrestrial ATSC 3.0 services in May 2017 in preparation for the 2018 Winter Olympics. In November 2017, the United States' Federal Communications Commission approved regulations allowing broadcast stations to voluntarily offer ATSC 3.0 services (Next Gen TV); however, they must be offered alongside a standard ATSC digital signal, and there will not be a mandatory transition as was done with the transition from analog NTSC to ATSC.\n\nATSC 3.0 uses a bootstrap signal which allows a receiver to discover and identify the signals that are being transmitted. The bootstrap signal has a fixed configuration that can allow for new signal types to be used in the future.\n\nATSC 3.0 uses a physical layer that is based on orthogonal frequency-division multiplexing (OFDM) modulation with low-density parity-check code (LDPC) FEC codes. With a 6 MHz channel the bit rate can vary from 1 Mbit/s to 57 Mbit/s depending on the parameters that are used. ATSC 3.0 can have up to 4 physical layer pipes (PLP) in a channel with different robustness levels used for each PLP. An example of how PLP can be used would be a channel that delivers HD video over a robust PLP and enhances the video to UHD with Scalable Video Coding over a higher bitrate PLP.\n\nATSC 3.0 supports Dolby AC-4 and MPEG-H 3D Audio.\n\nATSC 3.0 supports three video formats: Legacy SD Video, Interlaced HD Video, and Progressive Video. Legacy SD Video and Interlaced HD Video support frame rates up to 60 fps and can only use the Rec. 709 color space. Legacy SD Video and Interlaced HD Video are included for compatibility with existing content and can't use HDR, HFR, or WCG.\n\nLegacy SD Video supports resolutions up to 720×480 and supports High Efficiency Video Coding (HEVC) Main 10 profile at Level 3.1 Main Tier.\n\nInterlaced HD Video supports 1080-line interlaced video with 1920 or 1440 pixels per line, and supports HEVC Main 10 profile at Level 4.1 Main Tier.\n\nProgressive Video supports resolutions up to 3840×2160 progressive scan and supports HEVC Main 10 profile at Level 5.2 Main Tier. Progressive Video supports frame rates up to 120 fps and the Rec. 2020 color space. Progressive Video supports HDR using Hybrid Log-Gamma (HLG) and Perceptual Quantizer (PQ).\n\nATSC 3.0 supports digital watermarking of the audio signal and video signal.\n\nA U.S. consortium known as AWARN has advocated for the use of ATSC 3.0 features, including datacasting, and automatically waking up devices, in order to provide an emergency alert system with support for embedded rich media and finer geotargeting. These features are defined within the \"Advanced Emergency Alerting\" portions of the ATSC 3.0 standards.\n\nOn March 26, 2013, the Advanced Television Systems Committee announced a call for proposals for the ATSC 3.0 physical layer which states that the plan is for the system to support video with a resolution of 3840×2160 at 60 fps (4K UHDTV).\n\nIn February 2014, a channel-sharing trial began between Los Angeles television stations KLCS (a public television station that is a PBS member) and KJLA, a commercial ethnic broadcaster owned-and-operated by LATV, with support from the CTIA and approval of the Federal Communications Commission. The test involved multiplexing multiple HD and SD subchannels together, experimenting with both current MPEG-2 / H.262 and MPEG-4 AVC / H.264 video codecs. Ultimately, it has been decided that H.264 would not be considered for ATSC-3.0, but rather the newer MPEG-H HEVC / H.265 codec would be used instead, with OFDM instead of 8VSB for modulation, allowing for 28 Mbit/s to 36 Mbit/s or more of bandwidth on a single 6-MHz channel.\n\nIn May 2015, and continuing on for six months afterward, the temporary digital transition transmitter and antenna of Cleveland, Ohio's Fox affiliate, WJW, will be used by the National Association of Broadcasters to test the \"Futurecast\" ATSC 3.0 standard advanced by LG Corporation and GatesAir. In September 2015 further tests in the Baltimore and Washington, DC area were announced by Sinclair Broadcast Group's Baltimore station, WBFF, which is also a Fox affiliate. The Futurecast system had previously been tested in October 2014 during off-air hours through Madison, Wisconsin ABC affiliate WKOW. Unlike ATSC 1.0/2.0's Distributed Transmission System's pseudo-single-frequency network operations, WI9XXT's two transmitters operate as a true Single Frequency Network.\n\nFurther tests began in on January 6, 2016 of ATSC 3.0 with High Dynamic Range (using the Scalable HEVC video codec with HE-AAC audio) from Las Vegas independent station, KHMP-LD on UHF 18. It would later be joined in these tests by Sinclair's CW affiliate, KVCW simulcasting on a temporary test frequency (UHF 45).\n\nOn January 20, 2016, a working group in South Korea led by LG Electronics and others performed the first \"end-to-end\" broadcast of 4K resolution programming via an ATSC 3.0 signal, using an IP transmission from the Seoul Broadcasting System's Mok-dong studio to feed a transmitter on Gwanak Mountain. The broadcaster's technical director stated that the successful test \"highlights the potential for Korea’s launch of terrestrial UHD TV commercial services using ATSC 3.0 in February 2017.\" Following the test broadcast, South Korean broadcasters announced that they planned to launch ATSC 3.0 services in February 2017.\n\nOn March 28, 2016, the Bootstrap component of ATSC 3.0 (System Discovery and Signalling) was upgraded from candidate standard to finalized standard.\n\nOn June 29, 2016, NBC affiliate WRAL-TV in Raleigh, North Carolina, a station known for its pioneering roles in testing the original ATSC standards, launched an experimental ATSC 3.0 channel carrying the station's programming in 1080p, as well as a 4K demo loop. WRAL-EX has also carried 4K coverage of the 2016 Summer Olympics and 2018 Winter Olympics in an experimental manner.\n\nOn July 27, 2016, South Korea's Ministry of Science, ICT and Future Planning officially endorsed ATSC 3.0 as the country's broadcasting standard for ultra-high-definition television. On January 6, 2017, LG Electronics announced that their 2017 4K TVs sold in South Korea would include ATSC 3.0 tuners. \n\nOn May 31, 2017, SBS, MBC, and KBS officially launched their full-time ATSC 3.0 services in major South Korean markets such as Seoul and Incheon. The launch had been delayed from February 2017 due to issues obtaining the required equipment. \n\nThe transition made South Korea the first country in the world to deploy a terrestrial UHD format, and enabled 4K broadcasts of the 2018 Winter Olympics in Pyeongchang County.\n\nOn February 2, 2017, the Federal Communications Commission (FCC) issued a notice of proposed rulemaking (NPRM) that would allow for the deployment of ATSC 3.0 in the United States. The Notice of Proposed Rulemaking seeks comments on issues such as carriage obligations, interference, public interest obligations, simulcasting, and a tuner mandate. Gary Shapiro of the Consumer Technology Association (CTA) has stated that a TV tuner mandate is not necessary and that it should be market-driven and voluntary. On February 24, 2017, the FCC voted unanimously to approve two portions of the NPRM, opening the door for manufacturers to begin producing ATSC 3.0 hardware.\n\nOn November 14, 2017, the Pearl consortium (comprising a number of major broadcasting conglomerates, including Cox Media, Graham Media Group, Hearst Television, Meredith Corporation, Nexstar Media Group, Scripps Media, and Tegna Inc.) announced that it would use Phoenix, Arizona as a test market for an ATSC 3.0 transition in 2018. Two days later, the FCC voted 3-2 in favor of an order authorizing voluntary deployments of Next Gen TV (ATSC 3.0); stations that choose to deploy ATSC 3.0 services must continue to maintain an ATSC-compatible signal that is \"substantially similar\" in programming to their ATSC 3.0 signal (besides programming that leverages ATSC 3.0 features, and advertising), and covers the station's entire community of license (the FCC stated that it would expedite approval for transitions if the loss in over-the-air coverage post-transition is 5% or less). This clause will remain in effect for at least five years; permission from the FCC must be obtained before a full-power station can shut down its ATSC signal, but low-power stations are exempt from the simulcasting requirement and are allowed to flash-cut to ATSC 3.0 if they choose.\n\nATSC 1.0 signals will still be subject to mandatory carriage rules for television providers during the five-year simulcasting mandate; the FCC stated that voluntary carriage of 3.0 signals by television providers would be left to the marketplace. The order does require stations to provide sufficient on-air notice about transitions to ATSC 3.0 services. The FCC will not allocate a second channel to each broadcaster to enable a gradual consumer transition. Instead, it has been suggested that multiple broadcasters in each market cooperate by locating multiple degraded ATSC 1.0 services on a single transmitter. At the same time, the broadcasters would share the remaining transmitters for ATSC 3.0 transmissions. After sufficient consumer adoption, ATSC 1.0 transmissions would be abandoned, allowing stations to return to operation on their owned transmitters. It is unclear how the complications of this approach would be overcome, especially in light of spectrum reallocation in heavily populated markets. The FCC published its final rules on ATSC 3.0 to the Federal Register on February 2, 2018, and they formally take effect 30 days afterward.\n\nAs the transition is voluntary, the FCC will not require ATSC 3.0 tuners to be included in new televisions, and there will not be a subsidy program for the distribution of ATSC 3.0-compatible equipment.\n\nAs part of the ATSC 3.0 trials by Pearl, Univision's KFPH-CD in Phoenix was converted to an ATSC 3.0 station on April 9, 2018, which will be shared by Univision and several other broadcasters. Univision and Sinclair Broadcast Group are also planning a trial in Dallas, which will utilize spectrum vacated by KSTR-DT and KTXD-TV to test ATSC 3.0 transmission using a Single-frequency network.\n\nConsumer advocates have noted the opportunity in which ATSC 3.0 can allow advertisers to run targeted advertising. The targeted ads would allow advertisers to track more directly viewer ratings rather than indirectly by companies such as Nielsen ratings. The FCC is expected to defer the decision on targeted ads to be in accordance with Federal Trade Commission's guidelines on privacy.\n\nA consortium of U.S. television providers criticized the domestic plans for the transition, citing the \"voluntary\" transition, inconsistencies in commitments to simulcasting arrangements for compatibility, potential downgrades in service for ATSC 1.0 viewers, as well as how these signals will factor into retransmission consent negotiations.\n\n\n"}
{"id": "53990998", "url": "https://en.wikipedia.org/wiki?curid=53990998", "title": "Abigail Gawthern", "text": "Abigail Gawthern\n\nAbigail Gawthern (7 July 1757 – 7 January 1822) was a British diarist and lead manufacturer.\n\nGawthern was probably born in Nottingham. Her parents were Ann and Thomas Frost. She was named \"Abigail Anna\" after her father's mother. This was opportune as the Frosts were to inherit a substantial bequest from her grandmother's brother Thomas Secker. Secker had been the Archbishop of Canterbury. After Secker died his will was disputed by Thomas Frost and he managed to persuade the court that £11,000 intended for charity should be added to an existing legitimate bequest to his family. \n\nFrom an early age Gawthern kept a diary.\n\nIn 1791 Gawthern, husband died. She took control of the family's white lead business and its money and property. She continued to live in their substantial house at 26 Low Pavement in Nottingham. When her parents died in 1801 she received another large inheritance. The business continued and her son briefly led the business until it folded in 1808 due to more efficient methods of production being developed. \n\nGawthern is remembered because in the early 1800s she to copied her diaries into a single volume that in time would document Nottingham's history for the period 1751–1810. The diaries show how the professional classes lived at that time. In her own street was the home of the \"Ladies Assembly\" which was run by ladies but which was open to both genders but not the working class.\n\nGawthern died at her home in Low Pavement in 1822. A son, Francis, and a daughter, Anna, survived her.\n"}
{"id": "31464187", "url": "https://en.wikipedia.org/wiki?curid=31464187", "title": "Adrianov compass", "text": "Adrianov compass\n\nThe Adrianov compass () is a military compass designed by Russian Imperial Army topographist Vladimir Adrianov in 1907. Wrist-worn versions of the compass were then adopted and widely used by the Red and Soviet Army.\n"}
{"id": "8635864", "url": "https://en.wikipedia.org/wiki?curid=8635864", "title": "Amorphous computing", "text": "Amorphous computing\n\nAmorphous computing refers to computational systems that use very large numbers of identical, parallel processors each having limited computational ability and local interactions. The term Amorphous Computing was coined at MIT in 1996 in a paper entitled \"Amorphous Computing Manifesto\" by Abelson, Knight, Sussman, et al.\n\nExamples of naturally occurring amorphous computations can be found in many fields, such as: developmental biology (the development of multicellular organisms from a single cell), molecular biology (the organization of sub-cellular compartments and intra-cell signaling), neural networks, and chemical engineering (non-equilibrium systems) to name a few. The study of amorphous computation is \"hardware agnostic\"—it is not concerned with the physical substrate (biological, electronic, nanotech, etc.) but rather with the characterization of amorphous algorithms as abstractions with the goal of both understanding existing natural examples and engineering novel systems.\n\nAmorphous computers tend to have many of the following properties:\n\n\n\n"}
{"id": "51050410", "url": "https://en.wikipedia.org/wiki?curid=51050410", "title": "Brainscape", "text": "Brainscape\n\nBrainscape is a web and mobile education platform that allows students to study adaptive flashcards. The website and mobile application allow students, teachers, and corporate trainers to create (or upload) electronic flashcards, and to find flashcards created by other users and publishers around the world. Flashcards are all stored in the cloud and can be shared with groups of other learners.\n\nBrainscape's most distinguishing characteristic is its flashcard repetition algorithm, called Confidence-Based Repetition (CBR). CBR is a personalized form of spaced repetition, in which the learner rates his/her confidence in each flashcard, on a scale of 1-5, which subsequently determines how frequently to repeat the flashcard. Lower-confidence items are repeated more frequently until the user upgrades his/her confidence rating, thereby creating an optimized study stream. Brainscape has published a white paper which cites academic studies proving the viability of the cognitive science research that it applies in its technology.\n\nThe idea for Brainscape arose when its founder, Andrew Cohen, was attempting to study Spanish and French while living in Panama and Martinique from 2005 to 2007. When Rosetta Stone and other educational resources were not working efficiently enough for him, Cohen created a Microsoft Excel program that would quiz him on individual vocabulary words and verb conjugations, then repeat those concepts within an interval of time that felt appropriate to his pace of learning.\n\nCohen later followed this passion by pursuing a master's degree in Education Technology from Columbia University, where he focused his graduate research on the concept of CBR and built a more complete prototype using the Java programming language. In 2010, he partnered with Andy Lutz, the ex-VP of Product from The Princeton Review, and he began seeking venture capital and was able to raise over $3 million, in three tranches, by 2015. Brainscape has since expanded to several million registered users among students of all ages, especially in graduate and medical programs.\n\nBrainscape provides services including electronic flashcards, various study modes, collaborative editing tools, and teacher dashboards, via its website and iPhone, iPad and Android applications.\n"}
{"id": "26477771", "url": "https://en.wikipedia.org/wiki?curid=26477771", "title": "Capillary action through synthetic mesh", "text": "Capillary action through synthetic mesh\n\nCapillary action through synthetic mesh is the result of the intermolecular attraction between moisture and semi-synthetic polymers, causing a current of thermionic energy through a specific pathway within a mesh material. The combination of the adhesive forces and the surface tension that arises from cohesion produces the characteristic upward curve in a fluid, such as water. Capillarity is the result of cohesion of water molecules and adhesion of those molecules to the solid material forming the void. As the edges of the material are brought closer together, such as in a very narrow path, the interaction causes the liquid to be drawn away from the original source. The more narrow the pathway, the greater the rise of the liquid. Greater surface tension and increased ratio of adhesion to cohesion also result in greater rise. Synthetic materials using conductive polymer as found in polypyrrole to reduce liquid density to a manageable state.\n\nThe force with which water is held by capillary action varies with the quantity of water being held. As part of a demonstration conducted by Bright Idea and Webb development: Water entering a natural void, such as a pore within a synthetic mesh material, forms a film on the surface of the material surrounding the pore. The adhesion of the water molecules nearest the solid material is greatest. As water is added to the pore, the thickness of the film increases, the capillary force is reduced in magnitude, and water molecules on the outer portion of the film may begin to flow away from its source. As more water enters the pore the capillary force is reduced to zero when the pore is saturated, unless a hydrophilic body is introduced. The movement of moisture through the mesh is controlled by this capillary action.\n\n"}
{"id": "6352441", "url": "https://en.wikipedia.org/wiki?curid=6352441", "title": "Certolizumab pegol", "text": "Certolizumab pegol\n\nCertolizumab pegol (CDP870, tradename Cimzia) is a biologic medication for the treatment of Crohn's disease, rheumatoid arthritis, psoriatic arthritis and ankylosing spondylitis. It is a fragment of a monoclonal antibody specific to tumor necrosis factor alpha (TNF-α) and is manufactured by UCB.\n\n\n\n\nCertolizumab pegol is a monoclonal antibody directed against tumor necrosis factor alpha. More precisely, it is a PEGylated Fab fragment of a humanized TNF inhibitor monoclonal antibody.\n\n\n\n\nSignificant side effects occur in 2% of people who take the medication.\n\n"}
{"id": "1001110", "url": "https://en.wikipedia.org/wiki?curid=1001110", "title": "Charpai", "text": "Charpai\n\nCharpai, Charpaya or Charpoy or Manji (Hindi : चारपाई, Urdu, Saraiki, Punjabi; \"char\" \"four\" + \"paya\" \"footed\") is a traditional woven bed used in the Indian subcontinent. It is also known as Khaat, Khatia, or Manji. \n\nCharpai is well known due to its natural qualities mostly it is used in warm areas; its net is made out of cotton, natural fibres and date leaves. It's a very simple design that's very easy to construct. There are many interpretations of the traditional design and over the years crafts people have innovated on the weave patterns and materials used. \n\nThe construction is simple, it has a frame of four strong vertical posts connected by four horizontal members. This simple construction of the Charpai also makes it self leveling. Weaving of the Charpai is done in many ways, e.g diagonal cross weave with one end left for adjustments as linear members. This helps in controlling the sagging of the bed as it ages with use.\n\nIbn Battuta describes them as having \"four conical legs with four crosspieces of wood on which braids of silk or cotton are woven. When one lies down on it, there is no need for anything to make it pliable, for it is pliable of itself.\"\n"}
{"id": "41899683", "url": "https://en.wikipedia.org/wiki?curid=41899683", "title": "Choice in eCommerce", "text": "Choice in eCommerce\n\nChoice in eCommerce - Initiative for Choice and Innovation in Online-Trade - is an initiative of online retailers throughout Europe that works for unrestricted trade and innovation in Europe. Spokesman for the initiative is Oliver Prothmann, founder of the Multi-Channel tool chartixx.\n\nChoice in eCommerce was founded on May 8, 2013 by several online retailers in Berlin, Germany. The cause was, in the view of the initiative, sales bans and online restrictions by individual manufacturers. The dealers felt cut off from their main sales channel and thus deprived them the opportunity to use online platforms like Amazon, eBay or Rakuten in a competitive market for the benefit of their customers. Sales of all products and services traded online in Europe in 2012 counted 311.6 billion Euros. Through online trading in Europe is estimated that up to two million jobs were created.\nThe initiative later received support from other industry stakeholders including German BVOH (Bundesverband Onlinehandel) and CCIA (Computer and Communications Industry Association).\n\nIn the summer of 2013 Choice started a petition calling for free and fair trade. On December 17, 2013 Oliver Prothmann handed the petition containing 14,341 signatures of online retailers from across Europe to Olli Rehn, Vice-President of the European Commission. The petition calls for manufacturers and brand owners to refrain from trade restrictions or prohibitions for online retailers.\n"}
{"id": "26666150", "url": "https://en.wikipedia.org/wiki?curid=26666150", "title": "Committee for Nuclear Responsibility", "text": "Committee for Nuclear Responsibility\n\nThe Committee for Nuclear Responsibility was formed as a \"political and educational organization to disseminate anti-nuclear views and information to the public\". The goals of the organization were a moratorium on nuclear power and the commercialization of alternative energy sources. \n\nJohn Gofman founded the Committee for Nuclear Responsibility in 1971, as a small non-profit, public interest association with four Nobel Laureates on its Board. These Nobel scientists were Linus Pauling, Harold Urey, George Wald and James D. Watson. Other scientists who were involved included Paul Ehrlich, John Edsall, and Richard E. Bellman. The Board of Directors included Lewis Mumford, Ramsey Clark, Ian MacHarg, and Richard Max McCarthy. Actor Jack Lemmon endorsed the goals of the Committee for Nuclear Responsibility. \n\nGofman was Director of the Committee for Nuclear Responsibility for many years and his independent research yielded higher risk estimates from low-level radiation than the estimates presented by various government agencies. His books carefully show how his analyses proceed from raw data to final conclusions, with no hidden steps.\n\n"}
{"id": "18470361", "url": "https://en.wikipedia.org/wiki?curid=18470361", "title": "Concurrent estimation", "text": "Concurrent estimation\n\nIn discrete event simulation concurrent estimation is a technique used to estimate the effect of alternate parameter settings on a discrete event system. For example from observation of a (computer simulated) telecommunications system with a specified buffer size formula_1, one estimates what the performance would be if the buffer size had been set to the alternate values formula_2. Effectively the technique generates (during a single simulation run) formula_3 alternative histories for the system state variables, which have the same probability of occurring as the main simulated state path; this results in a computational saving as compared to running formula_3 additional simulations, one for each alternative parameter value.\n\nThe technique was developed by Cassandras, Strickland and Panayiotou.\n\n"}
{"id": "1803615", "url": "https://en.wikipedia.org/wiki?curid=1803615", "title": "Crown Jewel Defense", "text": "Crown Jewel Defense\n\nIn business, when a company is threatened with takeover, the crown jewel defense is a strategy in which the target company sells off its most attractive assets to a friendly third party or spin off the valuable assets in a separate entity. Consequently, the unfriendly bidder is less attracted to the company assets. Other effects include dilution of holdings of the acquirer, making the takeover uneconomical to third parties, and adverse influence of current share prices. \n\n"}
{"id": "232249", "url": "https://en.wikipedia.org/wiki?curid=232249", "title": "Crystal radio", "text": "Crystal radio\n\nA crystal radio receiver, also called a crystal set, is a simple radio receiver, popular in the early days of radio. It uses only the power of the received radio signal to produce sound, needing no external power. It is named for its most important component, a crystal detector, originally made from a piece of crystalline mineral such as galena. This component is now called a diode.\n\nCrystal radios are the simplest type of radio receiver and can be made with a few inexpensive parts, such as a wire for an antenna, a coil of wire, a capacitor, a crystal detector, and earphones. Crystal radios are passive receivers, while other radios use an amplifier powered by current from a battery or wall outlet to make the radio signal louder. Thus, crystal sets produce rather weak sound and must be listened to with sensitive earphones, and can only receive stations within a limited range.\n\nThe rectifying property of a contact between a mineral and a metal was discovered in 1874 by Karl Ferdinand Braun. Crystals were first used as a detector of radio waves in 1894 by Jagadish Chandra Bose, in his microwave optics experiments. They were first used as a demodulator for radio communication reception in 1902 by G. W. Pickard. Crystal radios were the first widely used type of radio receiver, and the main type used during the wireless telegraphy era. Sold and homemade by the millions, the inexpensive and reliable crystal radio was a major driving force in the introduction of radio to the public, contributing to the development of radio as an entertainment medium with the beginning of radio broadcasting around 1920.\n\nAround 1920, crystal sets were superseded by the first amplifying receivers, which used vacuum tubes. Crystal sets became obsolete for commercial use but continued to be built by hobbyists, youth groups, and the Boy Scouts mainly as a way of learning about the technology of radio. They are still sold as educational devices, and there are groups of enthusiasts devoted to their construction.\n\nCrystal radios receive amplitude modulated (AM) signals, and can be designed to receive almost any radio frequency band, but most receive the AM broadcast band. A few receive shortwave bands, but strong signals are required. The first crystal sets received wireless telegraphy signals broadcast by spark-gap transmitters at frequencies as low as 20 kHz.\n\nCrystal radio was invented by a long, partly obscure chain of discoveries in the late 19th century that gradually evolved into more and more practical radio receivers in the early 20th century. The earliest practical use of crystal radio was to receive Morse code radio signals transmitted from spark-gap transmitters by early amateur radio experimenters. As electronics evolved, the ability to send voice signals by radio caused a technological explosion around 1920 that evolved into today's radio broadcasting industry.\n\nEarly radio telegraphy used spark gap and arc transmitters as well as high-frequency alternators running at radio frequencies. The coherer was the first means of detecting a radio signal. These, however, lacked the sensitivity to detect weak signals.\n\nIn the early 20th century, various researchers discovered that certain metallic minerals, such as galena, could be used to detect radio signals.\n\nIndian physicist Jagadish Chandra Bose was first to use a crystal as a radio wave detector, using galena detectors to receive microwaves starting around 1894. In 1901, Bose filed for a U.S. patent for \"A Device for Detecting Electrical Disturbances\" that mentioned the use of a galena crystal; this was granted in 1904, #755840. The device depended on the large variation of a semiconductor's conductance with temperature; today we would call his invention a bolometer. Bose's patent is frequently, but erroneously, cited as a type of rectifying detector. On August 30, 1906, Greenleaf Whittier Pickard filed a patent for a silicon crystal detector, which was granted on November 20, 1906.\n\nA crystal detector includes a crystal, usually a thin wire or metal probe that contacts the crystal, and the stand or enclosure that holds those components in place. The most common crystal used is a small piece of galena; pyrite was also often used, as it was a more easily adjusted and stable mineral, and quite sufficient for urban signal strengths. Several other minerals also performed well as detectors. Another benefit of crystals was that they could demodulate amplitude modulated signals. This device brought radiotelephones and voice broadcast to a public audience. Crystal sets represented an inexpensive and technologically simple method of receiving these signals at a time when the embryonic radio broadcasting industry was beginning to grow.\n\nIn 1922 the (then named) US Bureau of Standards released a publication entitled \"Construction and Operation of a Simple Homemade Radio Receiving Outfit\". This article showed how almost any family having a member who was handy with simple tools could make a radio and tune into weather, crop prices, time, news and the opera. This design was significant in bringing radio to the general public. NBS followed that with a more selective two-circuit version, \"Construction and Operation of a Two-Circuit Radio Receiving Equipment With Crystal Detector\", which was published the same year and is still frequently built by enthusiasts today.\n\nIn the beginning of the 20th century, radio had little commercial use, and radio experimentation was a hobby for many people. Some historians consider the autumn of 1920 to be the beginning of commercial radio broadcasting for entertainment purposes. Pittsburgh station KDKA, owned by Westinghouse, received its license from the United States Department of Commerce just in time to broadcast the Harding-Cox presidential election returns. In addition to reporting on special events, broadcasts to farmers of crop price reports were an important public service in the early days of radio.\n\nIn 1921, factory-made radios were very expensive. Since less-affluent families could not afford to own one, newspapers and magazines carried articles on how to build a crystal radio with common household items. To minimize the cost, many of the plans suggested winding the tuning coil on empty pasteboard containers such as oatmeal boxes, which became a common foundation for homemade radios.\n\nIn early 1920s Russia, Oleg Losev was experimenting with applying voltage biases to various kinds of crystals for manufacture of radio detectors. The result was astonishing: with a zincite (zinc oxide) crystal he gained amplification. This was negative resistance phenomenon, decades before the development of the tunnel diode. After the first experiments, Losev built regenerative and superheterodyne receivers, and even transmitters.\n\nA crystodyne could be produced in primitive conditions; it can be made in a rural forge, unlike vacuum tubes and modern semiconductor devices. However, this discovery was not supported by authorities and soon forgotten; no device was produced in mass quantity beyond a few examples for research.\n\nIn addition to mineral crystals, the oxide coatings of many metal surfaces act as semiconductors (detectors) capable of rectification. Crystal radios have been improvised using detectors made from rusty nails, corroded pennies, and many other common objects.\n\nWhen Allied troops were halted near Anzio, Italy during the spring of 1944, powered personal radio receivers were strictly prohibited as the Germans had equipment that could detect the local oscillator signal of superheterodyne receivers. Crystal sets lack power driven local oscillators, hence they could not be detected. Some resourceful soldiers constructed \"crystal\" sets from discarded materials to listen to news and music. One type used a blue steel razor blade and a pencil lead for a detector. The lead point touching the semiconducting oxide coating (magnetite) on the blade formed a crude point-contact diode. By carefully adjusting the pencil lead on the surface of the blade, they could find spots capable of rectification. The sets were dubbed \"foxhole radios\" by the popular press, and they became part of the folklore of World War II.\n\nIn some German-occupied countries during WW2 there were widespread confiscations of radio sets from the civilian population. This led determined listeners to build their own clandestine receivers which often amounted to little more than a basic crystal set. Anyone doing so risked imprisonment or even death if caught, and in most of Europe the signals from the BBC (or other allied stations) were not strong enough to be received on such a set.\n\nWhile it never regained the popularity and general use that it enjoyed at its beginnings, the crystal radio circuit is still used. The Boy Scouts have kept the construction of a radio set in their program since the 1920s. A large number of prefabricated novelty items and simple kits could be found through the 1950s and 1960s, and many children with an interest in electronics built one.\n\nBuilding crystal radios was a craze in the 1920s, and again in the 1950s. Recently, hobbyists have started designing and building examples of the early instruments. Much effort goes into the visual appearance of these sets as well as their performance. Annual crystal radio 'DX' contests (long distance reception) and building contests allow these set owners to compete with each other and form a community of interest in the subject.\n\nA crystal radio can be thought of as a radio receiver reduced to its essentials. It consists of at least these components:\n\nAs a crystal radio has no power supply, the sound power produced by the earphone comes solely from the transmitter of the radio station being received, via the radio waves captured by the antenna. The power available to a receiving antenna decreases with the square of its distance from the radio transmitter. Even for a powerful commercial broadcasting station, if it is more than a few miles from the receiver the power received by the antenna is very small, typically measured in microwatts or nanowatts. In modern crystal sets, signals as weak as 50 picowatts at the antenna can be heard. Crystal radios can receive such weak signals without using amplification only due to the great sensitivity of human hearing, which can detect sounds with an intensity of only 10 W/cm. Therefore, crystal receivers have to be designed to convert the energy from the radio waves into sound waves as efficiently as possible. Even so, they are usually only able to receive stations within distances of about 25 miles for AM broadcast stations, although the radiotelegraphy signals used during the wireless telegraphy era could be received at hundreds of miles, and crystal receivers were even used for transoceanic communication during that period.\n\nCommercial passive receiver development was abandoned with the advent of reliable vacuum tubes around 1920, and subsequent crystal radio research was primarily done by radio amateurs and hobbyists. Many different circuits have been used. The following sections discuss the parts of a crystal radio in greater detail.\n\nThe antenna converts the energy in the electromagnetic radio waves to an alternating electric current in the antenna, which is connected to the tuning coil. Since in a crystal radio all the power comes from the antenna, it is important that the antenna collect as much power from the radio wave as possible. The larger an antenna, the more power it can intercept. Antennas of the type commonly used with crystal sets are most effective when their length is close to a multiple of a quarter-wavelength of the radio waves they are receiving. Since the length of the waves used with crystal radios is very long (AM broadcast band waves are 182-566 m or 597–1857 ft. long) the antenna is made as long as possible, from a long wire, in contrast to the whip antennas or ferrite loopstick antennas used in modern radios.\n\nSerious crystal radio hobbyists use \"inverted L\" and \"T\" type antennas, consisting of hundreds of feet of wire suspended as high as possible between buildings or trees, with a feed wire attached in the center or at one end leading down to the receiver. However more often random lengths of wire dangling out windows are used. A popular practice in early days (particularly among apartment dwellers) was to use existing large metal objects, such as bedsprings, fire escapes, and barbed wire fences as antennas.\n\nThe wire antennas used with crystal receivers are monopole antennas which develop their output voltage with respect to ground. The receiver thus requires a connection to ground (the earth) as a return circuit for the current. The ground wire was attached to a radiator, water pipe, or a metal stake driven into the ground. In early days if an adequate ground connection could not be made a counterpoise was sometimes used. A good ground is more important for crystal sets than it is for powered receivers, as crystal sets are designed to have a low input impedance needed to transfer power efficiently from the antenna. A low resistance ground connection (preferably below 25 Ω) is necessary because any resistance in the ground reduces available power from the antenna. In contrast, modern receivers are voltage-driven devices, with high input impedance, hence little current flows in the antenna/ground circuit. Also, mains powered receivers are grounded adequately through their power cords, which are in turn attached to the earth by way of a well established ground.\n\nThe tuned circuit, consisting of a coil and a capacitor connected together, acts as a resonator, similar to a tuning fork. Electric charge, induced in the antenna by the radio waves, flows rapidly back and forth between the plates of the capacitor through the coil. The circuit has a high impedance at the desired radio signal's frequency, but a low impedance at all other frequencies. Hence, signals at undesired frequencies pass through the tuned circuit to ground, while the desired frequency is instead passed on to the detector (diode) and stimulates the earpiece and is heard. The frequency of the station received is the resonant frequency \"f\" of the tuned circuit, determined by the capacitance \"C\" of the capacitor and the inductance \"L\" of the coil:\n\nThe circuit can be adjusted to different frequencies by varying the inductance (L), the capacitance (C), or both. In the lowest-cost sets, the inductor was made variable via a spring contact pressing against the windings that could slide along the coil, thereby introducing a larger or smaller number of turns of the coil into the circuit. Thus the inductance could be varied, \"tuning\" the circuit to the frequencies of different radio stations. Alternatively, a variable capacitor is used to tune the circuit. Some modern crystal sets use a ferrite core tuning coil, in which a ferrite magnetic core is moved into and out of the coil, thereby varying the inductance by changing the magnetic permeability (this eliminated the less reliable mechanical contact).\n\nThe antenna is an integral part of the tuned circuit and its reactance contributes to determining the circuit's resonant frequency. Antennas usually act as a capacitance, as antennas shorter than a quarter-wavelength have capacitive reactance. Many early crystal sets did not have a tuning capacitor, and relied instead on the capacitance inherent in the wire antenna (in addition to significant parasitic capacitance in the coil) to form the tuned circuit with the coil.\n\nThe earliest crystal receivers did not have a tuned circuit at all, and just consisted of a crystal detector connected between the antenna and ground, with an earphone across it. Since this circuit lacked any frequency-selective elements besides the broad resonance of the antenna, it had little ability to reject unwanted stations, so all stations within a wide band of frequencies were heard in the earphone (in practice the most powerful usually drowns out the others). It was used in the earliest days of radio, when only one or two stations were within a crystal set's limited range.\n\nAn important principle used in crystal radio design to transfer maximum power to the earphone is impedance matching. The maximum power is transferred from one part of a circuit to another when the impedance of one circuit is the complex conjugate of that of the other; this implies that the two circuits should have equal resistance. However, in crystal sets, the impedance of the antenna-ground system (around 10-200 ohms) is usually lower than the impedance of the receiver's tuned circuit (thousands of ohms at resonance), and also varies depending on the quality of the ground attachment, length of the antenna, and the frequency to which the receiver is tuned.\n\nTherefore, in improved receiver circuits, in order to match the antenna impedance to the receiver's impedance, the antenna was connected across only a portion of the tuning coil's turns. This made the tuning coil act as an impedance matching transformer (in an autotransformer connection) in addition to providing the tuning function. The antenna's low resistance was increased (transformed) by a factor equal to the square of the turns ratio (the ratio of the number of turns the antenna was connected to, to the total number of turns of the coil), to match the resistance across the tuned circuit. In the \"two-slider\" circuit, popular during the wireless era, both the antenna and the detector circuit were attached to the coil with sliding contacts, allowing (interactive) adjustment of both the resonant frequency and the turns ratio. Alternatively a multiposition switch was used to select taps on the coil. These controls were adjusted until the station sounded loudest in the earphone.\n\nOne of the drawbacks of crystal sets is that they are vulnerable to interference from stations near in frequency to the desired station. Often two or more stations are heard simultaneously. This is because the simple tuned circuit does not reject nearby signals well; it allows a wide band of frequencies to pass through, that is, it has a large bandwidth (low Q factor) compared to modern receivers, giving the receiver low selectivity.\n\nThe crystal detector worsened the problem, because it has relatively low resistance, thus it \"loaded\" the tuned circuit, drawing significant current and thus damping the oscillations, reducing its Q factor so it allowed through a broader band of frequencies. In many circuits, the selectivity was improved by connecting the detector and earphone circuit to a tap across only a fraction of the coil's turns. This reduced the impedance loading of the tuned circuit, as well as improving the impedance match with the detector.\n\nIn more sophisticated crystal receivers, the tuning coil is replaced with an adjustable air core antenna coupling transformer which improves the selectivity by a technique called \"loose coupling\". This \nconsists of two magnetically coupled coils of wire, one (the \"primary\") attached to the antenna and ground and the other (the \"secondary\") attached to the rest of the circuit. The current from the antenna creates an alternating magnetic field in the primary coil, which induced a current in the secondary coil which was then rectified and powered the earphone. Each of the coils functions as a tuned circuit; the primary coil resonated with the capacitance of the antenna (or sometimes another capacitor), and the secondary coil resonated with the tuning capacitor. Both the primary and secondary were tuned to the frequency of the station. The two circuits interacted to form a resonant transformer.\n\nReducing the \"coupling\" between the coils, by physically separating them so that less of the magnetic field of one intersects the other, reduces the mutual inductance, narrows the bandwidth, and results in much sharper, more selective tuning than that produced by a single tuned circuit. However, the looser coupling also reduced the power of the signal passed to the second circuit. The transformer was made with adjustable coupling, to allow the listener to experiment with various settings to gain the best reception.\n\nOne design common in early days, called a \"loose coupler\", consisted of a smaller secondary coil inside a larger primary coil. The smaller coil was mounted on a rack so it could be slid linearly in or out of the larger coil. If radio interference was encountered, the smaller coil would be slid further out of the larger, loosening the coupling, narrowing the bandwidth, and thereby rejecting the interfering signal.\n\nThe antenna coupling transformer also functioned as an impedance matching transformer, that allowed a better match of the antenna impedance to the rest of the circuit. One or both of the coils usually had several taps which could be selected with a switch, allowing adjustment of the number of turns of that transformer and hence the \"turns ratio\".\n\nCoupling transformers were difficult to adjust, because the three adjustments, the tuning of the primary circuit, the tuning of the secondary circuit, and the coupling of the coils, were all interactive, and changing one affected the others.\n\nThe crystal detector demodulates the radio frequency signal, extracting the modulation (the audio signal which represents the sound waves) from the radio frequency carrier wave. In early receivers, a type of crystal detector often used was a \"cat whisker detector\". The point of contact between the wire and the crystal acted as a semiconductor diode. The cat whisker detector constituted a crude Schottky diode that allowed current to flow better in one direction than in the opposite direction. Modern crystal sets use modern semiconductor diodes. The crystal functions as an envelope detector, rectifying the alternating current radio signal to a pulsing direct current, the peaks of which trace out the audio signal, so it can be converted to sound by the earphone, which is connected to the detector.\nThe rectified current from the detector has radio frequency pulses from the carrier frequency in it, which are blocked by the high inductive reactance and do not pass well through the coils of early date earphones. Hence, a small capacitor called a bypass capacitor is often placed across the earphone terminals; its low reactance at radio frequency bypasses these pulses around the earphone to ground. In some sets the earphone cord had enough capacitance that this component could be omitted.\n\nOnly certain sites on the crystal surface functioned as rectifying junctions, and the device was very sensitive to the pressure of the crystal-wire contact, which could be disrupted by the slightest vibration. Therefore, a usable contact point had to be found by trial and error before each use. The operator dragged the wire across the crystal surface until a radio station or \"static\" sounds were heard in the earphones. Alternatively, some radios \"(circuit, right)\" used a battery-powered buzzer attached to the input circuit to adjust the detector. The spark at the buzzer's electrical contacts served as a weak source of static, so when the detector began working, the buzzing could be heard in the earphones. The buzzer was then turned off, and the radio tuned to the desired station.\n\nGalena (lead sulfide) was the most common crystal used, but various other types of crystals were also used, the most common being iron pyrite (fool's gold, FeS), silicon, molybdenite (MoS), silicon carbide (carborundum, SiC), and a zincite-bornite (ZnO-CuFeS) crystal-to-crystal junction trade-named \"Perikon\". Crystal radios have also been improvised from a variety of common objects, such as blue steel razor blades and lead pencils, rusty needles, and pennies In these, a semiconducting layer of oxide or sulfide on the metal surface is usually responsible for the rectifying action.\n\nIn modern sets, a semiconductor diode is used for the detector, which is much more reliable than a crystal detector and requires no adjustments. Germanium diodes (or sometimes Schottky diodes) are used instead of silicon diodes, because their lower forward voltage drop (roughly 0.3V compared to 0.6V) makes them more sensitive.\n\nAll semiconductor detectors function rather inefficiently in crystal receivers, because the low voltage input to the detector is too low to result in much difference between forward better conduction direction, and the reverse weaker conduction. To improve the sensitivity of some of the early crystal detectors, such as silicon carbide, a small forward bias voltage was applied across the detector by a battery and potentiometer. The bias moves the diode's operating point higher on the detection curve producing more signal voltage at the expense of less signal current (higher impedance). There is a limit to the benefit that this produces, depending on the other impedances of the radio. This improved sensitivity was caused by moving the DC operating point to a more desirable voltage-current operating point (impedance) on the junction's I-V curve. The battery did not power the radio, but only provided the biasing voltage which required little power.\n\nThe requirements for earphones used in crystal sets are different from earphones used with modern audio equipment. They have to be efficient at converting the electrical signal energy to sound waves, while most modern earphones sacrifice efficiency in order to gain high fidelity reproduction of the sound. In early homebuilt sets, the earphones were the most costly component.\n\nThe early earphones used with wireless-era crystal sets had moving iron drivers that worked in a way similar to the horn loudspeakers of the period. Each earpiece contained a permanent magnet about which was a coil of wire which formed a second electromagnet. Both magnetic poles were close to a steel diaphram of the speaker. When the audio signal from the radio was passed through the electromagnet's windings, current was caused to flow in the coil which created a varying magnetic field that augmented or diminished that due to the permanent magnet. This varied the force of attraction on the diaphragm, causing it to vibrate. The vibrations of the diaphragm push and pull on the air in front of it, creating sound waves. Standard headphones used in telephone work had a low impedance, often 75 Ω, and required more current than a crystal radio could supply. Therefore, the type used with crystal set radios (and other sensitive equipment) was wound with more turns of finer wire giving it a high impedance of 2000-8000 Ω.\n\nModern crystal sets use piezoelectric crystal earpieces, which are much more sensitive and also smaller. They consist of a piezoelectric crystal with electrodes attached to each side, glued to a light diaphragm. When the audio signal from the radio set is applied to the electrodes, it causes the crystal to vibrate, vibrating the diaphragm. Crystal earphones are designed as ear buds that plug directly into the ear canal of the wearer, coupling the sound more efficiently to the eardrum. Their resistance is much higher (typically megohms) so they do not greatly \"load\" the tuned circuit, allowing increased selectivity of the receiver. The piezoelectric earphone's higher resistance, in parallel with its capacitance of around 9 pF, creates a filter that allows the passage of low frequencies, but blocks the higher frequencies. In that case a bypass capacitor is not needed (although in practice a small one of around 0.68 to 1 nF is often used to help improve quality), but instead a 10-100 kΩ resistor must be added in parallel with the earphone's input.\n\nAlthough the low power produced by crystal radios is typically insufficient to drive a loudspeaker, some homemade 1960s sets have used one, with an audio transformer to match the low impedance of the speaker to the circuit. Similarly, modern low-impedance (8 Ω) earphones cannot be used unmodified in crystal sets because the receiver does not produce enough current to drive them. They are sometimes used by adding an audio transformer to match their impedance with the higher impedance of the driving antenna circuit.\n\nA crystal radio tuned to a strong local transmitter can be used as a power source for a second amplified receiver of a distant station that cannot be heard without amplification.\n\nThere is a long history of unsuccessful attempts and unverified claims to recover the power in the carrier of the received signal itself. Traditional crystal sets use half-wave rectifiers. As AM signals have a modulation factor of only 30% by voltage at peaks, no more than 9% of received signal power (formula_2) is actual audio information, and 91% is just rectified DC voltage. Given that the audio signal is unlikely to be at peak all the time, the ratio of energy is, in practice, even greater. Considerable effort was made to convert this DC voltage into sound energy. Some earlier attempts include a one-transistor amplifier in 1966. Sometimes efforts to recover this power are confused with other efforts to produce a more efficient detection. This history continues now with designs as elaborate as \"inverted two-wave switching power unit\".\n\n\n\n"}
{"id": "141496", "url": "https://en.wikipedia.org/wiki?curid=141496", "title": "Depth charge", "text": "Depth charge\n\nA depth charge is an anti-submarine warfare weapon. It is intended to destroy a submarine by being dropped into the water nearby and detonating, subjecting the target to a powerful and destructive hydraulic shock. Most depth charges use high explosive charges and a fuze set to detonate the charge, typically at a specific depth. Depth charges can be dropped by ships, patrol aircraft, and helicopters.\n\nDepth charges were developed during World War I, and were one of the first effective methods of attacking a submarine underwater. They were widely used in World War I and World War II. They remained part of the anti-submarine arsenals of many navies during the Cold War. Depth charges have now largely been replaced by anti-submarine homing torpedoes.\n\nA depth charge fitted with a nuclear warhead is known as a \"nuclear depth bomb\". These were designed to be dropped from a patrol plane or deployed by an anti-submarine missile from a surface ship, or another submarine, located a safe distance away. All nuclear anti-submarine weapons were withdrawn from service by the United States, the United Kingdom, France, Russia, and China in or around 1990. They were replaced by conventional weapons whose accuracy and range had improved greatly as ASW technology improved.\n\nThe first attempt to fire charges against submerged targets was with aircraft bombs attached to lanyards which triggered them. A similar idea was a guncotton charge in a lanyarded can. Two of these lashed together became known as the \"depth charge Type A\". Problems with the lanyards tangling and failing to function led to the development of a chemical pellet trigger as the \"Type B\". These were effective at a distance of around .\n\nA 1913 Royal Navy Torpedo School report described a device intended for countermining, a \"dropping mine\". At Admiral John Jellicoe's request, the standard Mark II mine was fitted with a hydrostatic pistol (developed in 1914 by Thomas Firth and Sons of Sheffield) preset for firing, to be launched from a stern platform. Weighing , and effective at , the \"cruiser mine\" was a potential hazard to the dropping ship. The design work was carried out by Herbert Taylor at the RN Torpedo and Mine School, HMS \"Vernon\". The first effective depth charge, the Type D, became available in January 1916. It was a barrel-like casing containing a high explosive (usually TNT, but amatol was also used when TNT became scarce). There were initially two sizes—Type D, with a charge for fast ships, and Type D* with a charge for ships too slow to leave the danger area before the more powerful charge detonated.\n\nA hydrostatic pistol actuated by water pressure at a pre-selected depth detonated the charge. Initial depth settings were . Because production could not keep up with demand, anti-submarine vessels initially carried only two depth charges, to be released from a chute at the stern of the ship. The first success was the sinking of \"U-68\" off Kerry, Ireland, on 22 March 1916, by the Q-ship \"Farnborough.\" Germany became aware of the depth charge following unsuccessful attacks on \"U-67\" on 15 April 1916, and \"U-69\" on 20 April 1916. The only other submarines sunk by depth charge during 1916 were \"UC-19\" and \"UB-29\".\n\nNumbers of depth charges carried per ship increased to four in June 1917, to six in August, and 30-50 by 1918. The weight of charges and racks caused ship instability unless heavy guns and torpedo tubes were removed to compensate. Improved pistols allowed greater depth settings in 50-foot (15-meter) increments, from . Even slower ships could safely use the Type D at below and at or more, so the relatively ineffective Type D* was withdrawn. Monthly use of depth charges increased from 100 to 300 per month during 1917 to an average of 1745 per month during the last six months of World War I. The Type D could be detonated as deep as by that date. By the war's end, 74,441 depth charges had been issued by the RN, and 16,451 fired, scoring 38 kills in all, and aiding in 140 more.\n\nThe United States requested full working drawings of the device in March 1917. Having received them, Commander Fullinwider of the U.S. Bureau of Naval Ordnance and U.S. Navy engineer Minkler made some modifications and then patented it in the U.S. It has been argued that this was done to avoid paying the original inventor.\n\nThe Royal Navy Type D depth charge was designated the \"Mark VII\" in 1939. Initial sinking speed was with a terminal velocity of at a depth of if rolled off the stern, or upon water contact from a depth charge thrower. Cast iron weights of were attached to the Mark VII at the end of 1940 to increase sinking velocity to . New hydrostatic pistols increased the maximum detonation depth to . The Mark VII's amatol charge was estimated to be capable of splitting a submarine pressure hull at a distance of , and forcing the submarine to surface at twice that. The change of explosive to Torpex (or Minol) at the end of 1942 was estimated to increase those distances to .\n\nThe British Mark X depth charge weighed and was launched from torpedo tubes of older destroyers to achieve a sinking velocity of . The launching ship needed to clear the area at 11 knots to avoid damage, and the charge was seldom used. Only 32 were actually fired, and they were known to be troublesome.\n\nThe teardrop-shaped United States Mark 9 depth charge entered service in the spring of 1943. The charge was of Torpex with a sinking speed of and depth settings of up to . Later versions increased depth to and sinking speed to with increased weight and improved streamlining.\n\nAlthough the explosions of the standard United States Mark 4 and Mark 7 depth charge used in World War II were nerve-wracking to the target, an U-boat’s undamaged pressure hull would not rupture unless the charge detonated closer than about . Placing the weapon within this range was entirely a matter of chance and quite unlikely as the target maneuvered evasively during the attack. Most U-boats sunk by depth charges were destroyed by damage accumulated from a long barrage rather than by a single charge. Many survived hundreds of depth charges over a period of many hours; \"U-427\" survived 678 depth charges fired against it in April 1945.\n\nThe first delivery mechanism was to simply roll the \"ashcans\" off racks at the stern of the moving attacking vessel. Originally depth charges were simply placed at the top of a ramp and allowed to roll. Improved racks, which could hold several depth charges and release them remotely with a trigger, were developed towards the end of the First World War. These racks remained in use throughout World War II, because they were simple and easy to reload.\n\nSome Royal Navy trawlers used for anti-submarine work during 1917 and 1918 had a thrower on the forecastle for a single depth charge, but there do not seem to be any records of it being used in action. Specialized depth charge throwers were developed to generate a wider dispersal pattern when used in conjunction with rack-deployed charges. The first of these was developed from a British Army trench mortar, 1277 were issued, 174 installed in auxiliaries during 1917 and 1918. The bombs they launched were too light to be truly effective; only one U-boat is known to have been sunk by them.\n\nThornycroft created an improved version able to throw a charge . The first was fitted in July 1917 and became operational in August. In all, 351 torpedo boat destroyers and 100 other craft were equipped. Projectors called \"Y-guns\" (in reference to their basic shape), developed by the U.S. Navy's Bureau of Ordnance from the Thornycroft thrower, became available in 1918. Mounted on the centerline of the ship with the arms of the \"Y\" pointing outboard, two depth charges were cradled on shuttles inserted into each arm. An explosive propellant charge was detonated in the vertical column of the Y-gun to propel a depth charge about over each side of the ship. The main disadvantage of the Y-gun was that it had to be mounted on the centerline of a ship's deck, which could otherwise be occupied by superstructure, masts, or guns. The first were built by New London Ship and Engine Company beginning on 24 November 1917.\n\nThe K-gun, standardized in 1942, replaced the Y-gun as the primary depth charge projector. The K-guns fired one depth charge at a time and could be mounted on the periphery of a ship's deck, thus freeing valuable centerline space. Four to eight K-guns were typically mounted per ship. The K-guns were often used together with stern racks to create patterns of six to ten charges. In all cases, the attacking ship needed to be moving above a certain speed or it would be damaged by the force of its own weapons.\n\nDepth charges could also be dropped from an attacking aircraft against submarines. At the start of World War II, Britain's aerial anti-submarine weapon was the anti-submarine bomb. This weapon was too light and ultimately a failure. To remedy the failure of this weapon, the Royal Navy's Mark VII depth charge was modified for aerial use by the addition of a streamlined nose fairing and stabilising fins on the tail.\n\nThe first to deploy depth charges from airplanes in actual combat were the Finns. Experiencing the same problems as the RAF with insufficient charges on anti-submarine bombs, Captain Birger Ek of Finnish Air Force squadron LeLv 6 contacted one of his navy friends and suggested testing the aerial use of standard Finnish Navy depth charges. The tests proved successful, and the Tupolev SB bombers of LeLv 6 were modified in early 1942 to carry depth charges. The success of the anti-submarine missions reached RAF Coastal Command, which promptly began modifying depth charges for aerial use.\n\nLater depth charges would be developed specifically for aerial use. Such weapons still have utility today and are in limited use, particularly for shallow-water situations where a homing torpedo may not be suitable. Depth charges are especially useful for \"flushing the prey\" in the event of a diesel submarine lying on the bottom or otherwise hiding, with all machinery shut down. \n\nThe effective use of depth charges required the combined resources and skills of many individuals during an attack. Sonar, helm, depth charge crews and the movement of other ships had to be carefully coordinated. Aircraft depth charge tactics depended on the aircraft using its speed to rapidly appear from over the horizon and surprising the submarine on the surface (where it spent most of its time) during the day or night (using radar to detect the target and a Leigh light to illuminate just prior to the attack), then quickly attacking once it had been located, as the submarine would normally crash dive to escape attack.\n\nAs the Battle of the Atlantic wore on, British and Commonwealth forces became particularly adept at depth charge tactics, and formed some of the first destroyer hunter-killer groups to actively seek out and destroy German U-boats.\n\nSurface ships usually used ASDIC (sonar) to detect submerged submarines. However, to deliver its depth charges a ship had to pass over the contact to drop them over the stern; sonar contact would be lost just before attack, rendering the hunter blind at the crucial moment. This gave a skillful submarine commander an opportunity to take evasive action. In 1942 the forward-throwing \"hedgehog\" mortar, which fired a spread salvo of bombs with contact fuzes at a \"stand-off\" distance while still in sonar contact, was introduced and proved to be effective.\n\nIn the Pacific Theater of World War II, Japanese depth charge attacks initially proved fairly unsuccessful against U.S. and British submarines. Unless caught in shallow water, a submarine would just dive below the Japanese depth charge attack. The Japanese were unaware that the submarines could dive so deep. The old United States S-class submarines (1918–1925) had a test depth of ; the more modern fleet-boat Salmon-class submarines (1937) had a test depth of ; the Gato-class submarines (1940) were , and Balao-class submarines (1943) were .\n\nIn June 1943, the deficiencies of Japanese depth-charge tactics were inadvertently revealed in a press conference held by U.S. Congressman Andrew J. May, a member of the House Military Affairs Committee, who had visited the Pacific theater and received many intelligence and operational briefings. May mentioned the highly sensitive fact that American submarines had a high survival rate in combat with Japanese destroyers because Japanese depth charges were fuzed to explode at too shallow a depth.\n\nVarious press associations reported the depth issue over their wires and many newspapers (including one in Honolulu, Hawaii) published it. Soon, Japanese forces were setting their depth charges to explode at a more effective average depth of , to the detriment of American submariners. Vice Admiral Charles A. Lockwood, commander of the U.S. submarine fleet in the Pacific, later estimated that May's revelation cost the United States Navy as many as ten submarines and 800 seamen killed in action. The leak became known as The May Incident.\n\nFor the reasons expressed above, the depth charge was generally replaced as an anti-submarine weapon. Initially, this was by ahead-throwing weapons such as the British-developed Hedgehog and later squid. These weapons threw a pattern of warheads ahead of the attacking vessel to bracket a submerged contact. The Hedgehog was contact fuzed, while the squid fired a pattern of three large (200 kg) depth charges with clockwork detonators. Later developments included the Mark 24 \"Fido\" acoustic homing torpedo (and later such weapons), and the SUBROC, which was armed with a nuclear depth charge. The USSR, United States and United Kingdom developed anti-submarine weapons using nuclear warheads, sometimes referred to as \"nuclear depth bombs\". As of present, the Royal Navy retains a depth charge labelled as Mk11 Mod 3, which can be deployed from its Wildcat helicopters and Merlin Mk2 helicopters. \n\nDuring the Cold War when it was necessary to inform submarines of the other side that they had been detected but without actually launching an attack, low-power \"signalling depth charges\" (also called \"practice depth charges\") were sometimes used, powerful enough to be detected when no other means of communication was possible, but not destructive.\n\nThe high explosive in a depth charge undergoes a rapid chemical reaction at an approximate rate of 8,000 meters per second (25,000 ft/s). The gaseous products of that reaction momentarily occupy the volume previously occupied by the solid explosive, but at very high pressure. This pressure is the source of the damage and is proportional to the explosive density and the square of the detonation velocity. A depth charge gas bubble expands to reach the pressure of the surrounding water.\n\nThis gas expansion propagates a shock wave. The density difference of the expanding gas bubble from the surrounding water causes the bubble to rise toward the surface. Unless the explosion is shallow enough to vent the gas bubble to the atmosphere during its initial expansion, the momentum of water moving away from the gas bubble will create a gaseous void of lower pressure than the surrounding water. Surrounding water pressure then collapses the gas bubble with inward momentum causing excess pressure within the gas bubble. Re-expansion of the gas bubble then propagates another potentially damaging shock wave. Cyclical expansion and contraction continues until the gas bubble vents to the atmosphere.\n\nConsequently, explosions where the depth charge is detonated at a shallow depth and the gas bubble vents into the atmosphere very soon after the detonation are quite ineffective, even though they are more dramatic and therefore preferred in movies. A sign of an effective detonation depth is that the surface just slightly rises and only after a while vents into a water burst.\n\nVery large depth charges, including nuclear weapons, may be detonated at sufficient depth to create multiple damaging shock waves. Such depth charges can also cause damage at longer distances, if reflected shock waves from the ocean floor or surface converge to amplify radial shock waves. Submarines or surface ships may be damaged if operating in the convergence zones of their own depth charge detonations.\n\nThe damage that an underwater explosion inflicts on a submarine comes from a primary and a secondary shock wave. The primary shock wave is the initial shock wave from the depth charge, and will cause damage to personnel and equipment inside the submarine if detonated close enough. The secondary shock wave is a result from the cyclical expansion and contraction of the gas bubble and will bend the submarine back and forth and cause catastrophic hull breach, in a way that can be best described as bending a plastic ruler back and forth until it snaps. Up to sixteen cycles of the secondary shock wave have been recorded in tests. The effect of the secondary shock wave can be reinforced if another depth charge detonates on the other side of the hull in a close proximity in time of the first detonation, which is why depth charges normally are launched in pairs with different pre-set detonation depths.\n\nThe killing radius of a depth charge depends on the depth of detonation, the proximity of detonation to the submarine, the payload of the depth charge and the size and strength of the submarine hull. A depth charge of approximately 100 kg of TNT (400 MJ) would normally have a killing radius (hull breach) of only against a conventional 1000-ton submarine, while the disablement radius (where the submarine is not sunk but put out of commission) would be approximately . A larger payload increases the radius only relatively little because the effect of an underwater explosion decreases as the cube of the distance to the target.\n\n\n\n"}
{"id": "10107406", "url": "https://en.wikipedia.org/wiki?curid=10107406", "title": "Double inverted pendulum", "text": "Double inverted pendulum\n\nA double inverted pendulum is the combination of the inverted pendulum and the double pendulum. The double inverted pendulum is unstable, meaning that it will fall down unless it is controlled in some way. The two main methods of controlling a double inverted pendulum are moving the base, as with the inverted pendulum, or by applying a torque at the pivot point between the two pendulums.\n\n\n"}
{"id": "610881", "url": "https://en.wikipedia.org/wiki?curid=610881", "title": "Dry well", "text": "Dry well\n\nA dry well or drywell is an underground structure that disposes of unwanted water, most commonly surface runoff and stormwater, and in some cases greywater. It is a covered, porous-walled chamber that allows water to slowly soak into the ground (that is, percolate), dissipating into the groundwater. Such structures are often called a soakaway in the United Kingdom, a soakwell in Australia, and a soak pit in India. \n\nA sump in a basement can be built in drywell form, allowing the sump pump to cycle less frequently (handling only occasional peak demand). A French drain can resemble a horizontal dry well that is not covered. A larger open pit or artificial swale that receives storm water and dissipates it into the ground is called an infiltration basin or recharge basin. In places where the amount of water to be dispersed is not as large, a rain garden can be used instead. \n\nA dry well is a passive structure. Water flows through it under the influence of gravity. A dry well receives water from one or more entry pipes or channels at its top and discharges the same water through a number of small exit openings distributed over a larger surface area in the side(s) and bottom of the dry well. When a dry well is above the water table, most of its internal volume will contain air. Such a dry well can accept an initial inrush of water very quickly, until the air is displaced. After that, the dry well can only accept water as fast as it can dissipate water. Some dry wells deliberately incorporate a large storage capacity, so that they can accept a large amount of water very quickly and then dissipate it gradually over time, a method that is compatible with the intermittent nature of rainfall. A dry well maintains the connection between its inflow and outflow openings by resisting collapse and resisting clogging.\n\nSimple dry wells consist of a pit filled with gravel, riprap, rubble, or other debris. Such pits resist collapse, but do not have much storage capacity because their interior volume is mostly filled by stone. A more advanced dry well defines a large interior storage volume by a concrete or plastic chamber with perforated sides and bottom. These dry wells are usually buried completely, so that they do not take up any land area. The dry wells for a parking lot's storm drains are usually buried below the same parking lot.\n\nA covered pit that disposes of the water component of sewage by the same principle as a dry well is called a cesspool. A septic drain field operates on the same slow-drain/large-area principle as an infiltration basin. \n\n\n"}
{"id": "24747714", "url": "https://en.wikipedia.org/wiki?curid=24747714", "title": "Dynamic Business Modeling", "text": "Dynamic Business Modeling\n\nDynamic Business Modeling (\"DBM\") describes the ability to automate business models within an open framework. The independent analyst firm Gartner has recently called Dynamic Business Modeling \"critical for BSS solutions to succeed\".\n\nDynamic Business Modeling is based on principles wherein the business logic of an application is managed independently from the application servers that automate the services and processes defined in the business logic. Business modeling and integration (which itself is defined as part of the business model) are defined in a business logic layer, allowing underlying application servers to be business logic agnostic and therefore need no business driven customization. DBM applied correctly should reduce both the cost and risk in the initial implementation and its future evolution of systems.\n\nPrevious generations of IT systems (from 1990 to approximately 2001) were designed to address specific business models and regulatory practices and no value was given to logic–infrastructure segregation. These systems provided value by automating predefined business models (commonly referred to as \"off-the-shelf\"). As a result, they implicitly drove business strategy where DBM states that they should be driven by it. By being \"predefined\" they do not:\n\n\nDynamic Business modeling is suited for open automation of strategy-driven business models. By removing the need for customization of core application servers it is postulated as more cost efficient, rapidly deployed and evolveable. Dynamic Business Modeling was initially described (though applied much earlier in practice) by Doug Zone at MetraTech Corp. in reference to the billing segment of the enterprise software market. \"Service Oriented Applications\" (also known as \"service based applications\") coined by IBM describe potential methodologies to achieve DBM.\n\nDynamic Business Modeling is defined as the automation of Enterprise Business Models based on the principle that the model's underlying business processes and business services need to be dynamically and openly definable and re-definable.\n\nDynamic Business Modeling is defined as the enabler of a strategic advantage achieved by focused differentiation in any aspect of business (from marketing to finance to operations). This differentiation is achieved through how business is conducted: openly and dynamically defining the business model. Capital investment – human, physical and intellectual – must be aimed at allowing the definition of the business model to be dynamic.\n\nDynamic Business Modeling recognises that businesses dynamically evolve, re-inventing their (business) models to achieve strategic advantage. DBM posits that the role of enterprise software (CRM, billing, ERP) is to dynamically automate and advance the business processes and services that lie behind these Business models.\n\nThe term was first used to describe the architecture of MetraNet, a charging, billing, settlement and customer care from MetraTech Corp.\n\n\n\n\nhttp://ralyx.inria.fr/2008/Raweb/triskell/triskell.pdf\n\n"}
{"id": "52190884", "url": "https://en.wikipedia.org/wiki?curid=52190884", "title": "EKWB", "text": "EKWB\n\nEKWB (Edvard König Water Blocks), better known as EK Water Blocks, is a Slovenian company founded in 1999 that manufactures computer water cooling, extreme cooling and some air cooling components for CPUs, GPU, RAM and SSDs. Targeted at custom PC building enthusiasts, as well as professions alike, they offer a complete range of water cooling products from blocks to fittings and tubes. EKWB sells products to a number of international distributors, and via their own web store. EKWB target the international market, and use YouTubers to advertise their products through sponsorship or review samples for reviews. \n\nEKWB was founded in 1999 by Edvard König, who wanted better thermal and acoustic performance for his personal computer. The company released new versions of their products, resulting in a 40% cooling performance increase from 2006 to 2011. As of 2011 EKWB was one of the three largest personal computer water cooling companies. EKWB works with the overclocking community through forums to better understand the needs of its customers.\n\nEKWB specialises in cooling supplies. Its product line includes: Supremacy CPU water blocks, GPU water blocks, radiators, computer fans, AIO (All-in-one) liquid cooling sets and several accessories.\n\nCurrently EKWB sell a number of CPU water cooling blocks, primarily EK-Annihilator designed for socket LGA 3647, EK-Supremacy sTR4 designed for Socket SP3r2, EK-Supremacy EVO, an evolution of a EK-Supremacy water block, EK-Supremacy MX, a reduced cost version of the Supremacy EVO, and EK-Velocity, their new flagship CPU waterblock, launched 1st October 2018. \n\nThe EK-Supremacy EVO, EK-Supremacy MX and EK-Velocity have mounting kits available for Intel's Socket T, B, H, H2, H3, H4, R, R3 and R4. (N.B. Socket R2 is also compatible but was only used for certain server processors) and AMD's Socket 754, 939, 940, AM2(+), AM3(+), AM4, FM1 and FM2(+). The EK-Annihilator is required for Intels server LGA 3647 and the EK-Supremacy sTR is required for AMD's EPYC and Threadripper CPU's mounting holes, respectfully. \n\nEKWB also sells a range of Monoblocks, a CPU block that is much larger and also cools the motherboard's VRM, ensuring it does not overheat from high load. These monoblocks are based off the EK-Supremacy EVO.\n\nBlocks are offered in a number of finishes, with interchange top parts. Blocks are made from copper (or in the Fluid Gaming case aluminium) and either left as bare copper, nickel plated or gold plated. The block's top, as they call it, are made from either nickel plated brass, Acetal (usually black, though white is an option) or plexi tops (both clear and colour tinted).\n\nEKWB produces both universal GPU water blocks, that only cool the GPU's silicon die itself, and full cover water blocks that cool the GPU, VRAM and VRMs of a graphics card. EKWB makes a number of versions for so called AIB parner cards such as Sapphire, ASUS and EVGA, custom PCB versions of the standard video card, which have different PCB layouts. Full cover blocks are not compatible with different versions, even with the same GPU core, eg. a block for a Nvidia GeForce GTX 1080 Founders Edition will not an ASUS ROG Strix version of the GeForce GTX 1080.\n\nEKWB Universal GPU blocks, marketed as EK-VGA Supremacy for single graphics card setup, and a larger version called EK-Thermosphere, designed for use with more than one graphics card. These blocks only cool the GPU core, and so leave the VRAM and VRMs on a card to be cooled by air, usually. Should a user fail to ensure these vital parts of the card are also cooled, either by placing a fan to direct air over them, or by the use of stick on heat sinks, it's possible for the card to shutdown, or die even, from overheating. \n\nJust like the CPU blocks, they offer the blocks in different style offerings in both bare copper and nickel plating, and cover plates made from Acetal or Plexi. EK-VGA Supremacy can also be found with a nickel plated brass top option. \n\nEKWB offer a range of blocks for RAM DIMM modules, marketed as EK-RAM Monarch. Due to the tight spaces involved in RAM DIMM layouts on modern PCs, the water blocks do not sit directly on the RAM modules, but rather connect to special heat spreaders attached to the DIMM's, they then screw into the RAM block which sits on top and is thermally connected via TIM. They sell these blocks in two and four DIMM versions. They are available as both bare copper and nickel plated copper, as well as in acetal and plexi versions. The heat spreaders are offered in aluminium with black or nickel finishes. Copper versions are also available for the extreme cooling version. \n\nEKWB have their own range of fans especially designed for use with PC water cooling radiators. These fans are marketed as being high static pressure and low noise, both factors to be considered for airflow through a radiator. These fans are available in a number of colour combinations, rotational speed and in sizes of 120mm and 140mm, the two main PC cooling standards for enthusiasts. \n\nEKWB offer radiators in a number of sizes and thicknesses, marketed as CoolStream. \n\nThey offer their 120mm radiator options as single (120mmx120mm), dual (240mmx120mm), triple (360mmx120mm) and quad (480mmx120mm) fan places in thicknesses of 28mm (SE line) 38mm (PE line) and 60mm (XE line). They offer 140mm radiator options as single (140mmc140mm), dual (280mmx140mm), triple (420mmx140mm) and quad (560mmx140mm) fan places in thicknesses of 28mm (SE Line) and 45mm (CE line). Recently, following the revival of 180mm fans, they also started offering 180mm size rads, in single (180mmx180mm), dual (360mmx180mm) and triple (540mmx180mm) in a thickness of 35mm (WE line).\n\nThe radiators are made with copper tubes, brass end tanks soldered to the tubes and copper fins in an aluminium housing.\n\nEKWB offer their own range of AIO's which use their own parts and are ready to use, or a DIY kit, featuring all the parts you need to build a custom loop out of the box. These ready packaged kits feature all the same parts as you'd get if you buy the parts separately, but all arranged neatly. Marketed as being for beginners to take all the guess work out of getting into custom watercooling. \n"}
{"id": "26287714", "url": "https://en.wikipedia.org/wiki?curid=26287714", "title": "G.I. pocket stove", "text": "G.I. pocket stove\n\nThe G.I. pocket stove is a World War II–era portable pressurized-burner liquid-fuel stove made by the Coleman Company of Wichita, Kansas.\n\nDuring World War II, the U.S. government tasked Coleman to develop a compact stove for military use. The stove had to be lightweight, no larger than a quart-sized thermos bottle, burn any kind of fuel, and operate in weather from −60 to +125 °Fahrenheit. Within 60 days, Coleman came up with what became the G.I. Pocket Stove. Designated the Model 520 Coleman Military Burner, the stove first saw service in November 1942 when 5,000 of the stoves accompanied U.S. forces during the invasion of North Africa. Over 1 million of the stoves were produced for war use, where it won high praise in the field: Ernie Pyle ranked it \"just behind the ” in its usefulness.\n\nBy the end of the war, Coleman began production of a civilian version of the Model 520, designated the Model 530, and advertised as the \"G.I. pocket stove.\" The Model 530 was promoted by Coleman as the \"perfect pal for hunting, fishing and camping trips\" that would \"slip easily into a hunting coat pocket, glove compartment of a car, or corner of [a] picnic hamper.\" The single-burner G.I. Pocket Stove was only manufactured between 1946 and 1949; Coleman did not manufacture another single-burner, non-military backpacking stove until 1972. Larger single-burner stoves continued in production, starting with the 500 Sportster.\n\nThe G.I. pocket stove is 8½ inches high and 4½ inches in diameter, and weighs about three pounds. It was designed to burn either leaded or unleaded automobile gasoline (sometimes referred to as “white gasoline” or pure gasoline, without lead or additives). It can hold a pint of fuel, burn for over 3 hours on a full tank, and generate over per hour. Six small hinged metal pieces on the top fold outward for use as pot supports, and fold inward for storage. The stove comes with a two-piece telescoping aluminum case, which can be used as cook pots, a steel wrench that also serves as a handle for the cooking pots, a small metal disc or top plate which is placed on the burner grate to help disperse the flame, and a fuel funnel. An integrated hand-operated cleaning needle is used to remove soot or other impurities that can clog the burner tip.\n\nThe civilian version differs only slightly from its military cousin: the Model 530 G.I. pocket stove has a nickel-plated brass fuel tank, while the model 520 military version was painted olive drab. The military Model 520 also has three small folding legs at the base, which are omitted on the Model 530, and the civilian version has four vertical supports for the upper frame assembly (supporting the cooking grate) while the military version has three.\n\nThe fuel tank must first be pressurized by using the small hand-pump on the side of the stove. After pumping, the control valve is opened just slightly, allowing a mix of fuel (drawn from the bottom of the tank) and pressurized air (drawn from the top of the tank) to reach the burner head. There, the mixture is ignited using a match or lighter. Once the flame burns steadily for 2 to 3 minutes and the burner head is sufficiently heated up, the control valve is opened as far as possible. This cuts off the air from the tank, changing the mix to pure fuel. The heat of the burner head is then sufficient to vaporize the pure fuel prior to combustion. The size of the flame depends on the amount of pressure in the tank, which must be re-pressurized periodically using the hand pump.\n\n\n"}
{"id": "3570616", "url": "https://en.wikipedia.org/wiki?curid=3570616", "title": "GPS for the visually impaired", "text": "GPS for the visually impaired\n\nSince the Global Positioning System (GPS) was introduced in the late 1980s there have been many attempts to integrate it into a navigation-assistance system for blind and visually impaired people.\n\nCorsair is a GPS for pedestrians. It allows you to discover places around you and take you there. A new way of guidance has been developed by using the smartphone's vibration feature to indicate the direction to follow. This solution is particularly useful for people with visual impairments.\n\nCydalion is a navigation aid for people with visual impairments for Tango-enabled devices. Cydalion detects objects (including their height), offers custom sounds, and has a personalized user interface.\n\nLazarillo is based on Google Maps, OpenStreetMap and Foursquare alongside they own databases and with this information, Lazarillo collects the necessary data about the surroundings of the user to support the following features:\n\n\nWas designed in France to compensate for the limitations of traditional GPS and smartphone applications for the blind and visually impaired .\nThe fruit of 8 years of research in collaboration with the CNRS, ANGEO is the only device capable of discretely, reliably guiding you when crossing areas where GPS satellites are masked.\n\nWhen Apple introduced the iPhone 3GS in 2009, it was the first ever touch screen device accessible to the blind. iOS device usage has steadily increased among the blind and visually impaired population and numerous GPS apps targeting this user group have been developed since.\n\nAriadne GPS, developed by Luca Giovanni Ciaffoni, was released in June 2011 and was one of the first GPS apps specifically designed for blind and visually impaired users. It is based on Google map data and has the following features:\n\nBlindSquare is developed by MIPsoft and was first released in May 2012. It uses data from Foursquare and OpenStreetMap and offers a large feature set covering the needs of blind and visually impaired travelers. It is based on Foursquare, Open Street Map, and Apple Maps data and supports the following features:\n\niMove has been developed by EveryWare Technologies and was first released in January 2013. It is unique, because it lets users record sound clips and associate them with saved locations. iMove offers the following features:\n\nMyWay Classic was first released in January 2012 and is developed by the Swis Federation of the Blind. It has evolved into an app with a large set of features covering the needs of blind and visually impaired travelers. It uses Open Street Map data and includes the following features:\n\nSeeing Assistant move is developed by Transition Technologies S.A. and was first released in March 2013. It is the only GPS app designed for blind and visually impaired people that lets the user operate the app through predefined speech commands. It is based on Open Street Map and supports the following features:\n\nSendero Seeing Eye GPS is developed by the Sendero Group in collaboration with several organizations for the blind (Seeing Eye, RNIB, Guide Dogs NSW ACT) and was first released in July 2013. The Seeing Eye GPS is a fully accessible turn-by-turn GPS iPhone app developed by Sendero Group. It has all the normal navigation features plus features unique to blind users, such as simple menu structure, automatic announcements of intersections and points of interest, and routes for both pedestrian and vehicle with heads-up announcements for approaching turns.\nIt uses Foursquare and Google Places for points of interest and Google Maps for street info.\n\nSeeing Eye is not available globally and is offered under various names:\nThe Sendero apps include the following features:\n\nViaOpta Nav is developed by Novartis Pharmaceuticals Corporation and was first released in August 2014. It is available for both IOS and Android devices. It is the only GPS app targeting blind and visually impaired users that offers the possibility to search for accessibility information for example information about intersections, tactile paving, and audible traffic signals. Although Open Street Map supports respective categories, this information is not very widely available yet in the map data itself.\n\nViaOpta Nav uses Apple Maps (on iOS devices) and Google Maps (on Android devices) for address retrieval, and Open Street Map for route calculation, intersection information, and public points of interest. ViaOpta Nav supports the following main features:\n\nThe Loadstone project is developing an open source software for satellite navigation for blind and visually impaired users. The software is free and runs currently on many different Nokia devices with the S60 platform under all versions of the Symbian operating system. A GPS receiver must be connected to the cell phone by Bluetooth. Many blind people around the world are using Nokia cell phones because there are two screen reader products for the S60 Symbian platform; Talks from Nuance Communications and Mobile Speak from the Spanish company Code Factory. This makes these devices accessible by output of synthetic speech and also allow the use of third party software, such as Loadstone GPS.\n\nThe Loadstone developers, who are blind, are from Vancouver, Glasgow, and Amsterdam. Many users from around the world have contributed improvement proposals as they know exactly what functionality helps to increase their pedestrian mobility. Monty Lilburn and Shawn Kirkpatrick started the project in 2004. After the first development successes, they made it public in May 2006. Since then, other volunteers have found their way to this project of global self-help. The program is under the GNU General Public License (GPL), and was financed entirely by the private developers and by donations of users. This product provides blind people with more independence from the trading policy and prices of the few global vendors of accessible satellite navigation solutions.\n\nIn large rural regions and developing or newly industrializing countries, nearly no exact map data is available in common map databases. As such, the Loadstone software provides users an option to create and store their own waypoints for navigation and share them with others. The Loadstone community is working on importing coordinates from free sources, such as the OpenStreetMap project. In addition they are searching for a sponsor of licenses for commercial map data, such as is offered by the company Tele Atlas. The other major supplier is Navteq, which belongs to Nokia.\n\nLodestone is the name of a natural magnetic iron that was used throughout history in the manufacturing of compasses. Sighted owners of S60 devices can use Loadstone for leisure-time activities geocaching.\n\nLoroDux was a project by Fachhochschule Hannover. Like in Loadstone the user is led by direction and distance information. The text on the screen is read out by a screenreader. Vibration-Only navigation is possible. Data can be imported from the OpenStreetMap project. The development is discontinued because the team prefers to use Java on Android for the future. LoroDux LoroDux\n\nMobile Geo is Code Factory’s GPS navigation software for Windows Mobile-based Smartphones, Pocket PC phones and personal digital assistants (PDAs). Powered by GPS and mapping technology from the Sendero Group, Mobile Geo is the first solution specifically designed to serve as a navigation aid for people with a visual impairment which works with a wide range of mainstream mobile devices. Though it is a separately licensed product, Mobile Geo is seamlessly integrated with Code Factory’s popular screen readers – Mobile Speak for Pocket PCs and Mobile Speak for Windows Mobile Smartphones.\n\nThe Victor Trekker, designed and manufactured by HumanWare (previously known as VisuAide), was launched on March 2003. It is a personal digital assistant (PDA) application operating on a Dell Axim 50/51 or later replaced by HP IPAQ 2490B Pocket PC, adapted for the blind and visually impaired with talking menus, talking maps, and GPS information. Fully portable (weight 600g), it offered features enabling a blind person to determine position, create routes and receive information on navigating to a destination. It also provided search functions for an exhaustive database of point of interests, such as restaurants, hotels, etc.\n\nThe PDA's touch screen is made accessible by a tactile keypad with buttons that is held in place with an elastic strap.\n\nIt is fully upgradeable, so it can expand to accommodate new hardware platforms and more detailed geographic information.\n\nTrekker and Maestro, which is the first off-the-shelf accessible PDA based on Windows Mobile Pocket PC, are integrated and available since May 2005.\n\nThe Trekker is no longer sold by Humanware; the successor \"Trekker Breeze\" is a standalone unit. The software has fewer features than the original Trekker.\n\nThe Trekker Breeze is standalone hardware. Routes need to be recorded before they can be used. POIs are supported.\n\nThe BrailleNote GPS device is developed by Sendero Group, LLC, and Pulse Data International, now called HumanWare, in 2002. It is like a combination of a personal digital assistant, Map-quest software and a mechanical voice.\n\nWith a receiver about the size of a small cell phone, the BrailleNote GPS utilizes the GPS network to pinpoint a traveler’s position on earth and nearby points of interest. The BrailleNote receives radio signals from satellites to chart the location of users and direct them to their destination with spoken information from the speech synthesizer. The system uses satellites to triangulate the carrier’s position, much like a ship finding its location at sea.\n\nUsers can record points of interest such as local restaurants or any other location into the PDA’s database. Afterward, they can use keyboard commands on the unit’s keyboard to direct themselves to a specific point of interest.\n\nThe French company Kapsys offers a navigation system without a display, that works with speech input and output, called Kapten.\n\nIt was originally developed for cyclists but soon became a favourite in blind communities because of its low price compared to other accessible navigation solutions. Later Versions took feedback about accessibility into account.\n\nThe Trinetra project aims to develop cost-effective, independence-enhancing technologies to benefit blind people. One such system addresses accessibility concerns of blind people using public transportation systems. Using GPS receivers and staggered Infrared sensors, information is relayed to a centralized fleet management server via a cellular modem. Blind people, using common text-to-speech enabled cell phones can query estimated time of arrival, locality, and current bus capacity using a web browser.\n\nTrinetra, spearheaded by Professor Priya Narasimhan, is an ongoing project at the Electrical and Computer Engineering department of Carnegie Mellon University. Additional research topics include item-level UPC and RFID identification while grocery shopping and indoor navigation in retail settings.\n\nMoBIC means \"Mobility of Blind and Elderly people Interacting with Computers\", which was carried out from 1994 to 1996 supported by the Commission of the European Union. It was developing a route planning system which is designed to allow a blind person access to information from many sources such as bus and train timetables as well as electronic maps of the locality. The planning system helps blind people to study and plan their routes in advance, indoors.\n\nWith the addition of devices to give the precise current position and orientation of the blind pedestrian, the system could then be used outdoors. The outdoor positioning system is based on signals and satellites which give the longitude and latitude to within a metre; the computer converts this data to a position on an electronic map of locality. The output from the system is in the form of spoken messages.\n\nDrishti is a wireless pedestrian navigation system. It integrates several technologies including wearable computers, voice recognition and synthesis, wireless networks, Geographic information system (GIS) and GPS. It augments contextual information to the visually impaired and computed optimized routes based on user preference, temporal constraints (e.g. traffic congestion), and dynamic obstacles (e.g. ongoing ground work, road blockade for special events).\n\nThe system constantly guides the blind user to navigate based on static and dynamic data. Environmental conditions and landmark information queries from a spatial database along their route are provided on the fly through detailed explanatory voice cues. The system also provides capability for the user to add intelligence, as perceived by the blind user, to the central server hosting the spatial database.\n\nIn 1985, Jack Loomis, a Professor of Psychology at the University of California, Santa Barbara, came up with the idea of a GPS-based navigation system for the visually impaired. A short unpublished paper (Loomis, 1985) outlined the concept and detailed some ideas for implementation, including the idea of a virtual sound interface. Loomis directed the project for over 20 years, in collaboration with Reginald Golledge (1937–2009), Professor of Geography at UCSB, and Roberta Klatzky, Professor of Psychology (now at Carnegie Mellon University). Their combination of development and applied research was supported by three multi-year grants from the National Eye Institute (NEI) and another multi-year consortium grant from the National Institute on Disability and Rehabilitation Research (NIDRR), headed by Michael May of Sendero Group. In 1993, the UCSB group first publicly demonstrated the Personal Guidance System (PGS) using a bulky prototype carried in a backpack. Since then, they created several versions of the PGS, one of which was carried in a small pack worn at the waist. Their project mostly focused on the user interface and the resulting research has defined the legacy of the project. As indicated earlier in this entry, several wearable systems are now commercially available. These systems provide verbal guidance and environmental information via speech and Braille displays. But just as drivers and pilots want pictorial information from their navigation systems, survey research by the UCSB group has shown that visually impaired people often want direct perceptual information about the environment. Most of their R&D has dealt with several types of “spatial display”, with researchers Jim Marston and Nicholas Giudice contributing to the recent efforts. The first is a virtual acoustic display, which provides auditory information to the user via earphones (as proposed in the 1985 concept paper). With this display, the user hears important environmental locations, such as turn points along the route and points of interest. The labels of these locations are converted to synthetic speech and then displayed using auditory direction and distance cues, such that the spoken labels appear in the auditory space of the user. A second type of display, which the group calls a “haptic pointer interface”, was inspired by the hand-held receiver used in the Talking Signs© system of remote signage. The user holds a small wand, to which are attached an electronic compass and a small loudspeaker or vibrator. When the hand is pointing toward some location represented in the computer database, the user hears a tone or feels a vibration. Supplementary verbal information can be provided by synthetic speech. The user moves toward the desired location by aligning the body with the hand while maintaining the \"on-course\" auditory or vibratory signal. Other variants of the pointer interface involve putting the compass on the body or head and turning the body or head until the on-course signal is perceived. Six published route-guidance studies indicate that spatial displays provide effective route guidance, entail less cognitive load than speech interfaces, and are generally preferred by visually impaired users.\n\nProf. W. Balachandran is the pioneer and the head of GPS research group at Brunel University. He and his research team are pursuing research on navigation system for blind and visually impaired people. The system is based on the integration of state of the art current technologies, including high-accuracy GPS positioning, GIS, electronic compass and wireless digital video transmission (remote vision) facility with an accuracy of 3~4m. It provides an automated guidance using the information from daily updated digital map datasets e.g. roadworks. If required the remote guidance of visually impaired pedestrians by a sighted human guide using the information from the digital map and from the remote video image provides flexibility.\n\nThe difficulties encountered include the availability of up to date information and what information to offer including the navigation protocol. Levels of functionality have been created to tailor the information to the user’s requirements.\n\nNOPPA navigation and guidance system was designed to offer public transport passenger and route information using GPS technology for the visually impaired. This was a three-year (2002~2004) project in VTT Industrial Systems in Finland. The system provides an unbroken trip chain for a pedestrian using buses, commuter trains and trams in three neighbor cities’ area. It is based on an information server concept, which has user-centered and task oriented approach for solving information needs of special needs groups.\n\nIn the system, the Information Server is an interpreter between the user and Internet information systems. It collects, filters and integrates information from different sources and delivers results to the user. The server handles speech recognition and functions requiring either heavy calculations or data transfer. The data transfer between the server and the client is minimized. The user terminal holds speech synthesis and most of route guidance.\n\nNOPPA can currently offer basic route planning and navigation services in Finland. In practice, map data can have outdated information or inaccuracies, positioning can be unavailable or inaccurate, or wireless data transmission is not always available.\n\nNAVIG is a multidisciplinary project, with fundamental and applied aspects. The main objective is to increase the autonomy of blind people in their navigation capabilities. Reaching a destination while avoiding obstacles is one of the most difficult issue that blind individuals have to face. Achieving autonomous navigation will be pursued indoor and outdoor, in known and unknown environments. The project consortium is composed by two research centers in computer sciences specialized in human-machine interaction (IRIT) for handicapped people and in auditory perception, spatial cognition, sound design and augmented reality (LIMSI). Another research center is specialized in human and computer vision (CERCO), and two industrial partners are active in artificial vision (Spikenet Technology) and in pedestrian geolocalisation (Navocap). The last member of the consortium is an educational research center for the visually impaired (CESDV – IJA, Institute of Blind Youth).\n\nTANIA is a project founded at the University of Stuttgart, Germany. The hardware is based on GPS and RFID. It allows navigation for blind and deafblind persons with step accuracy. It only works where special maps have been created for the system.\n\nWayfinder Access was a GPS solution from the Swedish company Wayfinder Systems AB. This application for Symbian phones was designed especially to work with screen readers, such as Mobile Speak from Code Factory or TALKS from Nuance Communications and offers text-to-speech technology. It is able to take the special needs of the blind and visually impaired into consideration. Symbian screen reader software offers more than just the reading of the application’s screens, but also supports Braille devices.\n\nHighlights of Wayfinder Access include, but are not limited to:\nThe Wayfinder Access Service was shut down in 2011 after the company was taken over by Vodafone.\n\n\n"}
{"id": "7920461", "url": "https://en.wikipedia.org/wiki?curid=7920461", "title": "Gas cluster ion beam", "text": "Gas cluster ion beam\n\nGas Cluster Ion Beams (GCIB) is a technology for nano-scale modification of surfaces. It can smooth a wide variety of surface material types to within an angstrom of roughness without subsurface damage. It is also used to chemically alter surfaces through infusion or deposition.\n\nUsing GCIB a surface is bombarded by a beam of high-energy, nanoscale cluster ions. The clusters are formed when a high pressure gas (approximately 10 atmospheres pressure) expands into a vacuum (1e-5 atmospheres). The gas expands adiabatically and cools then condenses into clusters. The clusters are nano-sized bits of crystalline matter with unique properties that are intermediate between the realms of atomic physics and those of solid state physics. The expansion takes place inside of a nozzle that shapes the gas flow and facilitates the formation of a jet of clusters. The jet of clusters passes through differential pumping apertures into a region of high vacuum (1e-8 atmospheres) where the clusters are ionized by collisions with energetic electrons. The ionized clusters are accelerated electrostatically to very high velocities, and they are focused into a tight beam.\n\nThe GCIB beam is then used to treat a surface — typically the treated substrate is mechanically scanned in the beam to allow uniform irradiation of the surface. Argon is a commonly used gas in GCIB treatments because it is chemically inert and inexpensive. Argon forms clusters readily, the atoms in the cluster are bound together with Van der Waals forces. Typical parameters for a high-energy, Argon GCIB are : acceleration voltage 30 kV, average cluster size 10,400 atoms, average cluster charge +3.2, average cluster energy 64 keV, average cluster velocity 6.5 km/s, with a total electrical current of 200 µA or more. When an Argon cluster with these parameters strikes a surface, a shallow crater is formed with a diameter of approximately 20 nm and a depth of 10 nm. When imaged using Atomic Force Microscopy (AFM) the craters have an appearance much like craters on planetary bodies . A typical GCIB surface treatment allows every point on the surface to be struck by many cluster ions, resulting in smoothing of surface irregularities.\n\nLower energy GCIB treatments can be used to further smooth the surface, and GCIB can be used to produce an atomic level smoothness on both planar and nonplanar surfaces. Almost any gas can be used for GCIB, and there are many more uses for chemically reactive clusters such as for doping semiconductors (using BH gas), cleaning and etching (using NF gas), and for depositing chemical layers.\n\nIn industry, GCIB has been used for the manufacture of semiconductor devices, optical thin films, trimming SAW and FBAR filter devices , fixed disk memory systems and for other uses. GCIB smoothing of high voltage electrodes has been shown to reduce field electron emission, and GCIB treated RF cavities are being studied for use in future high energy particle accelerators .\n\nSmall argon cluster GCIB sources are increasingly used for analytical depth-profiling by secondary ion mass spectrometry (SIMS) and x-ray photoelectron spectroscopy (XPS). Argon clusters greatly reduce the damage introduced to the specimen during depth-profiling, making it practical to do so for many organic and polymeric materials for the first time. This has greatly extended the range of materials to which XPS (for example) can be applied .\n\nA related technique, with a limited range of applications, using high-velocity carbon Fullerenes to treat surfaces, has been studied (reference?).\n\nAccelerated Neutral Atoms Beams (ANAB) is a recent variation on GCIB . With ANAB the high velocity clusters are heated and evaporated by collisions with thermal energy gas molecules and the charged cluster remnants are deflected out of the beam leaving an intense focused beam of individual fast neutral monomers/atoms. The monomers are evaporated from the clusters with low thermal energies and they retain the center of mass velocity of the cluster and hence do not move out of the beam before colliding with the surface. When used to treat a surface, an ANAB beam has nearly the same total energy and velocity of the original GCIB beam but the smoothing effect on the surface is much different as the dispersed impacts of the individual fast atoms is more gentle than that of the clusters. With ANAB there is even less subsurface damage than with GCIB. The lack of electrical charge eliminates space-charge defocusing of the beam and static charge buildup on surfaces which is very useful for applications such as semiconductor device manufacturing .\n\n\n"}
{"id": "36619052", "url": "https://en.wikipedia.org/wiki?curid=36619052", "title": "Hana Micron", "text": "Hana Micron\n\nHana Micron (KOSDAQ 067310) is a semiconductor company specializing in assembly and product packaging as well as test and module manufacturing services. Hana Micron was founded in 2001 and its headquarters is located in Asan City, South Korea. As of 2011, Hana Micron has over 1300 employees and reported over $260 million in sales.\n\nHana Micron has other offices around the world including offices in the United States, Brazil, and China. Hana Micron's manufacturing factories are located in South Korea and Brazil.\n\nIn 2007, Hana Silicon was created as a subsidiary of Hana Micron. Hana Silicon provides consumable parts for the semiconductor etching process which is the silicon based cathode ring essential for manufacturing semiconductor products.\n\nIn 2008, Hana Micron America along with Hana Innosys were formed as a subsidiary of Hana Micron to concentrate on the SI (System Integration) business. Hana Innosys has developed a system integration solution for animal traceability by using RFID technology. In addition to the animal traceability solution, Hana Innosys has implemented a system integration solution for GPS fleet tracking systems.\n\n"}
{"id": "34220202", "url": "https://en.wikipedia.org/wiki?curid=34220202", "title": "IEC 60068", "text": "IEC 60068\n\nIEC 60068 is an international standard for the environmental testing of electrotechnical products that is published by the International Electrotechnical Commission.\n\nIEC 60068 has three parts:\n\n"}
{"id": "18306801", "url": "https://en.wikipedia.org/wiki?curid=18306801", "title": "Instruments used in gastroenterology", "text": "Instruments used in gastroenterology\n\nThis is a list of instruments used specially in Gastroenterology.\n"}
{"id": "37325303", "url": "https://en.wikipedia.org/wiki?curid=37325303", "title": "International Hearing Society", "text": "International Hearing Society\n\nThe International Hearing Society is a professional membership organization established in 1951 that represents hearing healthcare providers. Based in Livonia, Michigan, the IHS provides services worldwide, assisting consumers in locating qualified hearing aid specialists for testing hearing problems, selecting and fitting hearing devices, and providing support and repairs. It also operates a national \"Hearing Aid Helpline\" that provides informational resources on hearing loss and helps locate hearing aid specialists.\n"}
{"id": "24883628", "url": "https://en.wikipedia.org/wiki?curid=24883628", "title": "Jennifer S. Light", "text": "Jennifer S. Light\n\nJennifer S. Light is Professor of Science, Technology and Society at the Massachusetts Institute of Technology. Light's research investigates the work of technical experts in the political process, with special interest in these figures' influences on US urban history. Light serves on the editorial boards of the Journal of Communication and the IEEE Annals of the History of Computing.\n\nAn essay by Light from 1999, \"When Computers Were Women\", discusses an aspect of the history of computers — specifically that women were not credited for their work on the ENIAC computer, which was America's first electronic computer to automate ballistics computations during World War II. The women built the machine which replaced them, yet their contributions to it were kept out of history.\n\n\n\n\n"}
{"id": "4546010", "url": "https://en.wikipedia.org/wiki?curid=4546010", "title": "Jennifer Seberry", "text": "Jennifer Seberry\n\nJennifer Roma Seberry (born 13 February 1944 in Sydney) is an Australian cryptographer, mathematician, and computer scientist, currently a professor at the University of Wollongong, Australia. She was formerly the head of the Department of Computer Science and director of the Centre for Computer Security Research at the university.\n\nSeberry attended Parramatta High School and got her BSc at University of N.S.W., 1966; MSc at La Trobe University, 1969; PhD at La Trobe University, 1971 (Computational Mathematics); B.Ec. with two years completed at University of Sydney.\n\nSeberry was the first person to teach cryptology at an Australian University (University of Sydney). She was also the first woman Professor of Computer Science in Australia. She was the first woman Reader in Combinatorial Mathematics in Australia.\n\nSeberry was a founding member of the University of Sydney's Research Foundation for Information Technology Information Security Group in 1987. The group grew into the Australian Information Security Association, an Australian representative industry body with over 1000 paid members and branches in most capitals.\n\nSeberry was one of the founders of the Asiacrypt international conference in 1990 (then called Auscrypt).\n\nSeberry has contributed to the knowledge and use of Hadamard\nmatrices and bent functions for network security. She has published numerous papers on mathematics, cryptography, and computer and network security. She led the team that produced the LOKI and LOKI97 block ciphers and the HAVAL cryptographic hash functions. Seberry is also a co-author of the Py stream cipher, which was a candidate for the eSTREAM stream cipher project.\n\n"}
{"id": "4495354", "url": "https://en.wikipedia.org/wiki?curid=4495354", "title": "LTX", "text": "LTX\n\nXcerra Corporation (formerly LTX-Credence Corporation) is a semiconductor Automatic Test Equipment (ATE) vendor, founded in 1976 and headquartered in Norwood, MA (Greater Boston area).\n\nThe focus of the company is the design and development of ATE for the semiconductor marketplace, but it distinguished itself in the early days as a provider of functional and parametric testers for discrete component RF products. Today, LTX offers test platforms capable of testing mixed signal (analog & digital) devices.\n\nLTX was founded by Graham Miller, Roger Blethen, et al. All of the founders left nearby competitor, Teradyne. Although never verified, corporate lore holds that the name LTX was an abbreviation for the clarion call of its founders: \"Leave Teradyne by Christmas (Xmas)\" or possibly \"Left Teradyne at Christmas.\" Others believe that LTX stands for \"Linear Test eXcellence\", \"Linear Test eXperts\", or is simply an abbreviation of the word \"Electronics\".\n\nOn June 22, 2008 LTX signed a merger agreement with one of its principal competitors: Credence Systems Corporation. LTX CEO and President David Tacelli became CEO of merged company.\n\nOn August 29, 2008 LTX and Credence Systems Corporation completed a merger to form LTX-Credence Corporation.\n\nXcerra Corporation was formed in 2014 following the LTX-Credence acquisition of Everett Charles Technologies (ECT) and Multitest from Dover Corporation in December 2013. Xcerra Corporation is the parent company of four powerful brands that have been supplying innovative products and services to the semiconductor and electronics manufacturing industry for more than 30 years. Xcerra’s four brands are atg-Luther & Maelzer, Everett Charles Technologies, LTX-Credence, and Multitest.\n\nUnic Capital Management, an affiliate of Chinese private equity fund Sino IC Capital, announced plans to purchase Xcerra in April 2017 for approximately $580 million. However, in February 2018 Xcerra terminated the deal due to difficulties securing approval from the Committee on Foreign Investment in the U.S., which oversees deals with the potential for national-security concerns.\n\n"}
{"id": "598272", "url": "https://en.wikipedia.org/wiki?curid=598272", "title": "Leblanc process", "text": "Leblanc process\n\nThe Leblanc process was an early industrial process for the production of \"soda ash\" (sodium carbonate) used throughout the 19th century, named after its inventor, Nicolas Leblanc. It involved two stages: production of sodium sulfate from sodium chloride, followed by reaction of the sodium sulfate with coal and calcium carbonate to produce sodium carbonate. The process gradually became obsolete after the development of the Solvay process.\n\nSoda ash (sodium carbonate) and potash (potassium carbonate), collectively termed \"alkali\", are vital chemicals in the glass, textile, soap, and paper industries. The traditional source of alkali in western Europe had been potash obtained from wood ashes. However, by the 13th century, deforestation had rendered this means of production uneconomical, and alkali had to be imported. Potash was imported from North America, Scandinavia, and Russia, where large forests still stood. Soda ash was imported from Spain and the Canary Islands, where it was produced from the ashes of glasswort plants (called barilla ashes in Spain), or imported from Syria. The soda ash from glasswort plant ashes was mainly a mixture of sodium carbonate and potassium carbonate. In addition in Egypt, naturally occurring sodium carbonate, the mineral natron, was mined from dry lakebeds. In Britain, the only local source of alkali was from kelp, which washed ashore in Scotland and Ireland.\n\nIn 1783, King Louis XVI of France and the French Academy of Sciences offered a prize of 2400 livres for a method to produce alkali from sea salt (sodium chloride). In 1791, Nicolas Leblanc, physician to Louis Philip II, Duke of Orléans, patented a solution. That same year he built the first Leblanc plant for the Duke at Saint-Denis, and this began to produce 320 tons of soda per year. He was denied his prize money because of the French Revolution.\n\n\"For more recent history, see industrial history below.\"\n\nThe Leblanc process was a batch process in which sodium chloride was subjected to a series of treatments, eventually producing sodium carbonate. In the first step, the sodium chloride was heated with sulfuric acid to produce sodium sulfate (called the \"salt cake\") and hydrogen chloride gas according to the chemical equation\n\nThis chemical reaction had been discovered in 1772 by the Swedish chemist Carl Wilhelm Scheele. Leblanc's contribution was the second step, in which the salt cake was mixed with crushed limestone (calcium carbonate) and coal and fired. This reaction happens in two parts. First the coal (carbon) was oxidized to carbon dioxide, reducing the sulfate to sulfide. Second, the calcium and sodium swap their ligands to leave the thermodynamically favorable combination of sodium carbonate and calcium sulfide. This mixture is called \"black ash\".\n\nThe soda ash was then separated from the black ash by washing it with water. The wash water was then evaporated to yield solid sodium carbonate. This extraction process was termed lixiviation.\n\nThe sodium chloride is initially mixed with concentrated sulfuric acid and the mixture exposed to low heat. The hydrogen chloride gas bubbles off and was discarded to atmosphere before gas absorption towers were introduced. This continues until all that is left is a fused mass. This mass still contains enough chloride to contaminate the later stages of the process. The mass is then exposed to direct flame, which evaporates nearly all of the remaining chloride.\n\nThe coal used in the next step must be low in nitrogen to avoid the formation of cyanide. The calcium carbonate, in the form of limestone or chalk, should be low in magnesia and silica. The weight ratio of the charge is 2:2:1 of salt cake, calcium carbonate, and carbon respectively. It is fired in a reverberatory furnace at about 1000 °C.\n\nThe black-ash product of firing must be lixiviated right away to prevent oxidation of sulfides back to sulfate. In the lixiviation process, the black-ash is completely covered in water, again to prevent oxidation. To optimize the leaching of soluble material, the lixiviation is done in cascaded stages. That is, pure water is used on the black-ash that has already been through prior stages. The liquor from that stage is used to leach an earlier stage of the black-ash, and so on.\n\nThe final liquor is treated by blowing carbon dioxide through it. This precipitates dissolved calcium and other impurities. It also volatilizes the sulfide, which is carried off as HS gas. Any residual sulfide can be subsequently precipitated by adding zinc hydroxide. The liquor is separated from the precipitate and evaporated using waste heat from the reverberatory furnace. The resulting ash is then redissolved into concentrated solution in hot water. Solids that fail to dissolve are separated. The solution is then cooled to recrystallize nearly pure sodium carbonate decahydrate.\n\nLeblanc established the first Leblanc process plant in 1791 in St. Denis. However, French Revolutionaries seized the plant, along with the rest of Louis Philip's estate, in 1794, and publicized Leblanc's trade secrets. Napoleon I returned the plant to Leblanc in 1801, but lacking the funds to repair it and compete against other soda works that had been established in the meantime, Leblanc committed suicide in 1806.\n\nBy the early 19th century, French soda ash producers were making 10,000 - 15,000 tons annually. However, it was in Britain that the Leblanc process became most widely practiced. The first British soda works using the Leblanc process was built by the Losh family of iron founders at the Losh, Wilson and Bell works in Walker on the River Tyne in 1816, but steep British tariffs on salt production hindered the economics of the Leblanc process and kept such operations on a small scale until 1824. Following the repeal of the salt tariff, the British soda industry grew dramatically, and the chemical works established by James Muspratt in Liverpool and Flint, and by Charles Tennant near Glasgow became some of the largest in the world. By 1852, annual soda production had reached 140,000 tons in Britain and 45,000 tons in France. By the 1870s, the British soda output of 200,000 tons annually exceeded that of all other nations in the world combined.\n\nThe Leblanc process plants were quite damaging to the local environment. The process of generating salt cake from salt and sulfuric acid released hydrochloric acid gas, and because this acid was industrially useless in the early 19th century, it was simply vented into the atmosphere. Also, an insoluble, smelly solid waste was produced. For every 8 tons of soda ash, the process produced 5.5 tons of hydrogen chloride and 7 tons of calcium sulfide waste. This solid waste (known as galligu) had no economic value, and was piled in heaps and spread on fields near the soda works, where it weathered to release hydrogen sulfide, the toxic gas responsible for the odor of rotten eggs.\n\nBecause of their noxious emissions, Leblanc soda works became targets of lawsuits and legislation. An 1839 suit against soda works alleged, \"the gas from these manufactories is of such a deleterious nature as to blight everything within its influence, and is alike baneful to health and property. The herbage of the fields in their vicinity is scorched, the gardens neither yield fruit nor vegetables; many flourishing trees have lately become rotten naked sticks. Cattle and poultry droop and pine away. It tarnishes the furniture in our houses, and when we are exposed to it, which is of frequent occurrence, we are afflicted with coughs and pains in the head ... all of which we attribute to the Alkali works.\"\n\nIn 1863, the British Parliament passed the first of several Alkali Acts, the first modern air pollution legislation. This act allowed that no more than 5% of the hydrochloric acid produced by alkali plants could be vented to the atmosphere. To comply with the legislation, soda works passed the escaping hydrogen chloride gas up through a tower packed with charcoal, where it was absorbed by water flowing in the other direction. The chemical works usually dumped the resulting hydrochloric acid solution into nearby bodies of water, killing fish and other aquatic life.\n\nThe Leblanc process also meant very unpleasant working conditions for the operators. It originally required careful operation and frequent operator interventions (some involving heavy manual labour) into processes giving off hot noxious chemicals. This improved somewhat later as processes were more heavily mechanised to improve economics and uniformity of product.\n\nBy the 1880s, methods for converting the hydrochloric acid to chlorine gas for the manufacture of bleaching powder and for reclaiming the sulfur in the calcium sulfide waste had been discovered, but the Leblanc process remained more wasteful and more polluting than the Solvay process. The same is true when it is compared with the later electrolytical processes which eventually replaced it for chlorine production.\n\nIn 1861, the Belgian chemist Ernest Solvay developed a more direct process for producing soda ash from salt and limestone through the use of ammonia. The only waste product of this Solvay process was calcium chloride, and so it was both more economical and less polluting than the Leblanc method. From the late 1870s, Solvay-based soda works on the European continent provided stiff competition in their home markets to the Leblanc-based British soda industry. Additionally the Brunner Mond Solvay plant which opened in 1874 at Winnington near Northwich provided fierce competition nationally. Leblanc producers were unable to compete with Solvay soda ash, and their soda ash production was effectively an adjunct to their still profitable production of chlorine, bleaching powder etc. (The unwanted by-products had become the profitable products). The development of electrolytic methods of chlorine production removed that source of profits as well, and there followed a decline moderated only by \"gentlemen's' agreements\" with Solvay producers. By 1900, 90% of the world's soda production was through the Solvay method, or on the North American continent, through the mining of trona, discovered in 1938, which caused the closure of the last North American Solvay plant in 1986. The last Leblanc-based soda ash plant closed in the early 1920s. However, because of solubility of the bicarbonate, the Solvay process does not work for the manufacture of potassium carbonate, and the Leblanc process continued in limited use for its manufacture until much later.\n\nThere is a strong case for arguing that Leblanc process waste is the most endangered habitat in the UK, since the waste weathers down to calcium carbonate and produces a haven for plants that thrive in lime-rich soils, known as calcicoles. Only four such sites have survived the new millennium; three are protected as local nature reserves of which the largest, at Nob End near Bolton, is an SSSI and Local Nature Reserve - largely for its sparse orchid-calcicole flora, most unusual in an area with acid soils. This alkaline island contains within it an acid island, where acid boiler slag was deposited, which now shows up as a zone dominated by heather, \"Calluna vulgaris\".\n\n"}
{"id": "508912", "url": "https://en.wikipedia.org/wiki?curid=508912", "title": "Leica Geosystems", "text": "Leica Geosystems\n\nLeica Geosystems (formerly known as Wild Heerbrugg or just Wild) based in eastern Switzerland produces products and systems for surveying and geographical measurement (geomatics). Its products employ a variety of technologies including GPS satellite navigation and laser rangefinders to enable users to model existing structures, terrains in computer based systems to high accuracies, often under 1 cm.\n\nThe brand Leica, better known for its cameras, was formed on 2 April 1990 after the merger of several companies. In 2005, the company was acquired by Hexagon AB. Leica was listed on the Swiss stock exchange until the 7 August 2006 which saw the cancellation of all publicly held registered shares in Leica Geosystems Holdings Ltd and delisting of all listed registered shares of a par value of CHF 50.\n\nLeica Geosystems Geospatial Imaging is a Strategic Member in the Open Geospatial Consortium since 2008. Strategic Membership is the highest level of membership in the OGC. Strategic Members provide significant resources to support OGC objectives in the form of funding for program initiatives and staff resources inserted into the OGC process.\n\nHeinrich Wild (1877–1951) of Glarus, Switzerland, a leading designer of geodetic and astronomical instruments, began his career as an apprentice surveyor. In 1908, having invented a military rangefinder and convinced Zeiss to manufacture it, Wild moved to the city of Jena and became head of the new Zeiss branch responsible for surveying instruments, GEO.\n\nWild returned to Switzerland after the First World War. In April 1921, with Colonel Jacob Schmidheiny of Balgach and geologist Dr. Robert Helbling of Flums and with the help of Swiss financiers, he founded the \"Heinrich Wild, Werkstätte für Feinmechanik und Optik\" in Heerbrugg in the Alpine Rhine Valley. In the early 1930s, having recognized that he was not cut out to be a factory manager, Wild moved to Zurich, severed his connections with the firm in Heerbrugg, and designed instruments for Kern & Co in Aarau. His old firm became Wild Heerbrugg in 1937.\n\nIt merged with the optical firm of Ernst Leitz of Wetzlar in 1987, acquired a majority interest in Kern in 1988, was renamed Wild Leitz AG in 1989, and became part of the Leica holding company in 1990. Starting in 1996, the company was divided gradually into smaller units. Thus in 1996, Leica Camera AG was founded, followed in October 1997 by Leica Geosystems AG, and on April 1, 1998 by Leica Microsystems AG. The Swedish company Hexagon AB purchased Leica Geosystems in late 2005.\n\nToday these three companies are all independent public companies. Leica Geosystems produces - in continuation of Wild Heerbrugg - the geodetic instruments and is the global market leader in this section. The main customers of Leica Geosystems are surveyors, manufacturers and GIS companies.\n\nIn 2000 Leica Geosystems purchased Cyra Technologies, maker of the Cyrax laser 3D scanner line. In April 2001, Leica Geosystems acquired ERDAS, Inc and LH Systems. ERDAS is a remote sensing software firm, while LH Systems was a photogrammetric software firm. These two companies were merged into software company, ERDAS, Inc. ERDAS purchased Acquis, ER Mapper and IONIC in 2007. This expanded further ERDAS in the remote sensing market, let them enter the enterprise topological GIS market, and expanded their footprint in the image web serving and image enterprise market.\n\nIn October 2010, Intergraph was acquired by Hexagon AB. As part of the Hexagon acquisition, Hexagon moved ERDAS from under Leica Geosystems to Intergraph, and Z/I Imaging airborne imaging sensors from under Intergraph to Leica Geosystems.\n\nIn September 2011, Leica Geosystems Mining announced an exclusive industry partnership with Locata Corporation to provide the mining industry with the world’s only high precision radio positioning system that is not reliant on GPS.\n"}
{"id": "3728321", "url": "https://en.wikipedia.org/wiki?curid=3728321", "title": "List of government space agencies", "text": "List of government space agencies\n\nThis is a list of government agencies engaged in activities related to outer space and space exploration. (There is also a List of private spaceflight companies.)\n\nAs of 2018, 72 different government space agencies are in existence; 14 of those have launch capability. Six government space agencies—the China National Space Administration (CNSA), the European Space Agency (ESA), the Indian Space Research Organization (ISRO), the Japan Aerospace Exploration Agency (JAXA), the National Aeronautics and Space Administration (NASA), and the Russian Federal Space Agency (RFSA or Roscosmos)—have full launch capabilities; these include the ability to launch and recover multiple satellites, deploy cryogenic rocket engines and operate extraterrestrial probes. \n\nThe name given is the English version, with the native language version below. The acronym given is the most common acronym: this can either be the acronym of the English version (e.g. JAXA), or the acronym in the native language. Where there are multiple acronyms in common use, the English one is given first.\n\nThe date of the founding of the space agency is the date of first operations where applicable. If the space agency is no longer running, then the date when it was terminated (i.e. the last day of operations) is given. A link to the Agency's primary website is also given.\n\n\nThe annual budgets listed are the official budgets for national space agencies available in public domain. The budgets are not normalized to the expenses of space research in different countries, i.e. higher budget does not necessarily mean more activity or better performance in space exploration. Note also that budget could be used for different projects: e.g. GPS is maintained from the US defence budget, whereas ESA's money is used for developing the European Galileo positioning system. The data for authoritarian countries are unreliable. For European contributors to ESA, the national budgets shown include also their contributions to ESA.\n\n\n"}
{"id": "38729218", "url": "https://en.wikipedia.org/wiki?curid=38729218", "title": "List of lost silent films (1925–1929)", "text": "List of lost silent films (1925–1929)\n\nThis is a list of lost silent films that were released from 1925 to 1929.\n"}
{"id": "19601877", "url": "https://en.wikipedia.org/wiki?curid=19601877", "title": "Loading arm", "text": "Loading arm\n\nA loading arm permits the transfer of liquid or liquefied gas from one tank to another through an articulated pipe system consisting of rigid piping and swivel joints to obtain flexibility.\n\nTransfer to or from a truck transported tank or rail transported tank requires a Top Loading Arm or a Bottom Loading Arm. Transfer to or from a ship or barge requires a marine loading arm.\n\n\nBoth types of loading arms are typically made of 3 pipes – respectively called inner arm, outer arm and drop pipe.\nThe size can be from 2” to 6”. These 3 pipes are connected by swivel joints. Swivel joints are required to provide the flexibility needed. The loading arm unfolds to get the required working envelope to load or unload the tanker, and the reverse is to retract or get a minimal space for parking or storage.\nBoth types of loading arms may be mounted on a column or via a plate to an existing wall.\nBalancing is needed due to the weight of the steel piping. Balancing of the arm is made by counterweight or by a spring balance cylinder.\n\nTop loading arms are used to load or unload road or rail tankers.\nLoading or unloading is done through the manhole on the top of the tanker.\nDepending on the nature of the product (not dangerous, without any toxic vapors…), the connection may be “open”, this means that the manhole is not sealed.\nA “Semi-Closed” connection can be made by a cone on the manhole of the tanker.\nA “Closed” connection is required for all dangerous and toxic products. This connection is made by a flange to the top of the tanker.\nA cone can be equipped with a vapour hose to prevent the vapors from being released to the atmosphere.\nNumerous accessories are available to make a loading arm more ergonomic and more effective: press down system, pneumatic up/down system, top level control, safety break valve, purge, drain…etc.\n\nThis kind of arm is reserved for the unloading of road/rail tanker from the bottom, at the rear or side.\nThe location of the connection has an influence on the length of the pipes. As an example, a rear connection requires a longer length of arms than for a side connection.\nThe connection of this kind of loading arm can be made by flange, thread or by quick connection coupler such as a Dry Disconnect.\nThere are also a numerous accessories which can be fitted to the Bottom loading arm to make it more ergonomic and more effective as described for the Top loading arm.\n\nTo load or unload a ship, a hose or loading arm is needed to follow the movement of the ship due to the dynamics of changing draft, changing tide, wind and other factors.\nA Marine Loading Arm offers a significant improvement over a hose in the transfer of liquids between vessel and shore because It provides an easier and more ergonomic operation, gives longer service life and permits Emergency Release Action without any spillage of product and without any pollution.\n\nLike the Top and Bottom loading arm, the Marine Loading Arm is a system consisting of rigid piping and swivel joints to obtain flexibility. The OCIMF (Oil Companies International Marine Forum) and ASME have established guidelines for matters of strength calculations, working envelope and accessories. However the design of the loading arm is not included in this guideline and left to the individual manufacturer.\n\nThe product line reposes on a fixed stand post or riser and is connected by an inner and an outer arm – both arms are movable parts. Balancing is needed due to the weight of the steel piping. Balancing of the Marine Loading Arm is done by a rotating counterweight, which is connected to the inner arm and the outer arm via a rigid pantograph.\nSmall loading arms may be manually operated. The bigger ones are hydraulically operated.\n\nThe product line can be self supported or mounted on an independent support frame.\nSince the swivels are strong enough to absorb the weights of the product line, the liquid in the arms and the forces of the wind, the marine loading arm can be built as a self-supporting structure.\nNowadays marine loading arms can also be built on an independent support frame as is required for LNG and highly corrosive liquids.\n\nRecently the design of the marine loading arms has been significantly improved by the symmetric design. This new concept offers many advantages such as the equal distribution of forces on the swivel joints under all conditions, less force within the structure, reduces the weight of the loading arm, and permits larger working envelopes without requiring significantly stronger construction.\nSince the symmetric design has proven its effectiveness and its reliability. The symmetric design is also applicable to marine loading arms requiring an independent support frame.\n\n"}
{"id": "22752311", "url": "https://en.wikipedia.org/wiki?curid=22752311", "title": "Lobster pick", "text": "Lobster pick\n\nA lobster pick or lobster fork is a long, narrow food utensil used to extract meat from joints, legs, claws, and other small parts of a lobster. Lobster picks are usually made of stainless steel and weigh as much as an average teaspoon. They have a long, textured cylindrical handle, ending in a crescent-shaped moderately sharp pick, or else a small two-tined fork. The other end may have a spoon for scooping out meat from inside the lobster. The lobster pick can also be used with other seafood, such as crab and crawfish.\n"}
{"id": "14655528", "url": "https://en.wikipedia.org/wiki?curid=14655528", "title": "Machine Check Architecture", "text": "Machine Check Architecture\n\nIn computing, Machine Check Architecture (MCA) is an Intel mechanism in which the CPU reports hardware errors to the operating system.\n\nIntel's Pentium 4, Intel Xeon, P6 family processors as well as the Itanium architecture implement a machine check architecture that provides a mechanism for detecting and reporting hardware (machine) errors, such as: system bus errors, ECC errors, parity errors, cache errors, and translation lookaside buffer errors. It consists of a set of model-specific registers (MSRs) that are used to set up machine checking and additional banks of MSRs used for recording errors that are detected.\n\n\n"}
{"id": "25872275", "url": "https://en.wikipedia.org/wiki?curid=25872275", "title": "Markstein number", "text": "Markstein number\n\nIn combustion engineering and explosion studies, the Markstein number characterizes the effect of local heat release of a propagating flame on variations in the surface topology along the flame and the associated local flame front curvature. The dimensionless Markstein number is defined as:\n\nwhere formula_2 is the Markstein length, and formula_3 is the characteristic laminar flame thickness. The larger the Markstein length, the greater the effect of curvature on localised burning velocity. It is named after George H. Markstein (1911—2011), who showed that thermal diffusion stabilized the curved flame front and proposed a relation between the critical wavelength for stability of the flame front, called the Markstein length, and the thermal thickness of the flame. Phenomenological Markstein numbers with respect to the combustion products are obtained by means of the comparison between the measurements of the flame radii as a function of time and the results of the analytical integration of the linear relation between the flame speed and either flame stretch rate or flame curvature. The burning velocity is obtained at zero stretch, and the effect of the flame stretch acting upon it is expressed by a Markstein length. Because both flame curvature and aerodynamic strain contribute to the flame stretch rate, there is a Markstein number associated with each of these components.\n\nThe Markstein number with respect to the unburnt gas mixture for a one step reaction in the limit of large activation energy asymptotics was derived by Paul Clavin and Forman A. Williams in 1982. The Markstein number then is\n\nwhere \n\nand the Markstein number with respect to the burnt gas mixture is derived by Clavin (1985)\n\nIn general, Markstein number for the curvature effects formula_9 and strain effects formula_10 are not same in real flames. In that case, one defines a second Markstein number as\n\n"}
{"id": "13340571", "url": "https://en.wikipedia.org/wiki?curid=13340571", "title": "Michael Meeks (software developer)", "text": "Michael Meeks (software developer)\n\nMichael Meeks is a British software developer. He is primarily known for his work on GNOME, OpenOffice.org and now LibreOffice. He has been a major contributor to the GNOME project for a long time working on its infrastructure and associated applications, particularly CORBA, Bonobo, Nautilus and GNOME accessibility. He was hired as a Ximian developer by Nat Friedman and Miguel de Icaza in mid-2000 , continuing at Novell, SuSE and then Collabora.\n\nMeeks is a free software hacker who has contributed a lot of time to decreasing program load time. He created the direct binding, hashvals, and dynsort implementations for GNU Binutils and glibc. Most of this work was focused at making OpenOffice.org and now its fork LibreOffice start faster, and was later subsumed into the \"-hash-style=gnu\" linking optimization. His work on iogrind also allows applications to be profiled and optimized to first-time (or 'cold') start far more rapidly .\n\nHe supports LibreOffice and Evolution as the free software solutions for document editing and groupware.\n\nPreviously he worked for Quantel gaining expertise in real time AV editing and playback achieved with high performance focused hardware/software solutions.\n\nMeeks is a Christian, which he says made him think about the moral aspects of his own illegal use of non-free software and converted him finally to free software.\n\n"}
{"id": "22763312", "url": "https://en.wikipedia.org/wiki?curid=22763312", "title": "Micromirror device", "text": "Micromirror device\n\nMicromirror devices are devices based on microscopically small mirrors. The mirrors are Microelectromechanical systems (MEMS), which means that their states are controlled by applying a voltage between the two electrodes around the mirror arrays. Digital micromirror devices are used in video projectors and optics and micromirror devices for light deflection and control.\n\n\"→ See main article digital micromirror device\"\n\nDigital Micromirror Devices (DMD) were invented by Texas Instruments in 1987 and are the core of the DLP technology used for video projection. The mirrors are arranged in a matrix and have two states, \"on\" or \"off\" (digital). In the on state, light from the projector bulb is reflected into the lens making the pixel appear bright on the screen. In the off state, the light is directed elsewhere (usually onto a heatsink), making the pixel appear dark. Colours could be produced by various technologies like different light sources or gratings.\n\nThe mirrors could not only be switched between two states, their rotation is in fact continuous. This could be used for controlling the intensity and direction of incident light. One future application is controlling the light in buildings, based on micromirrors between the two panes of Insulated glazing. The power and direction of the incident light is determined by the mirrors state, which itself is controlled electrostatically.\n\nA MEMS scanning micromirror consists of a silicon device with a millimeter-scale mirror at the center. The mirror is typically connected to flexures that allow it to oscillate on a single axis or biaxially, to project or capture light.\n"}
{"id": "47239453", "url": "https://en.wikipedia.org/wiki?curid=47239453", "title": "Mounted corkscrew", "text": "Mounted corkscrew\n\nA mounted or bar corkscrew is a device screwed or clamped to a wall or counter top, used to draw corks from beer, wine or other bottles.\n\nCorks have been used to seal jars and bottles for over 400 years. (Modern, machine made bottles with threaded tops for screw caps date from the 1920s.) Early glass bottles were cumbersome (and possibly dangerous, being hand-blown) to hold, and the simple “T” corkscrew required strength to use.\nThe first mounted corkscrews, made some time in the mid 1800s, were very simple, consisting of a frame incorporating a bracket that held the bottle down and a metal hook that pulled almost any “T” handled corkscrew up. The helix or worm was turned into the cork, then the bottle was held with the protruding corkscrew in the frame, with the lever hooked onto it. Pulling the lever extracted the cork. The mechanical advantage incorporated into these frames consisted of various types of rack and pinion, gear or lever mechanisms.\n\nTurning the worm into the cork and then turning the cork off the worm was a time-consuming process. Eventually, the popularity of bottled beer stopped with corks would necessitate the invention of a faster, simpler method of removing the corks.\n\nThe first mounted mechanical corkscrews are known as “coffee grinder” or “crank and pump” types. Introduced in the late 1800s, this invention combined the corkscrew and mechanical advantage in one device. In most examples the worm was attached to a stem, with a crank, inserted through a frame with a lever. In most examples the worm was cranked into the cork, and then a lever was pressed or pulled to extract the cork. Then, the cork was held by hand, and the crank operated in reverse, turning it off the worm. For the most part, this style of corkscrew was confined to the home consumer market. John Bloeser of St. Louis, Missouri patented a floor-stand model of the crank and pump in 1886.\n\nUntil many years after the invention in 1892 of crown corks, beer bottles were stopped with corks. The increasing popularity of bottled beer created a need for a faster corkscrew. This led to the invention of a variety of “automatic” corkscrew mechanisms patented in the late 1800s and early 1900s in the United States, England and several European countries.\n\nIn 1865, Henry John Sanders of England patented a machine that would hold the bottle, turn the worm into the cork, remove the cork and release it “by one up-and-down motion of a lever or treadle, or by turning a handle or cam.” No example of this invention is known to exist. The first automatic mounted corkscrew still in existence (two are known) was patented in 1870 by P.F. Lindstrom of Stockholm, Sweden. Weighing over thirty pounds, with 144 different components, it was not commercially viable.\n\nMounted corkscrews were originally screwed to a wall (wall mount), clamped to a counter (clamp mount), or screwed to a counter top (top mount). In recent years, some have been adapted to operate from floor stands.\n\nThe challenge for inventors of automatic corkscrews was to devise a method of turning the worm in both directions, while preventing it from turning during the cork extraction phase of the operation. There are only two practical mechanisms that have been manufactured to cause the worm to turn in an automatic bar corkscrew.\n\nA stem is fastened to the worm and causes the stem to turn by attaching a gear, rack or stem carrier to the threading of the stem and pushing it through a stem-nut. This is the method used by Sanders in his 1865 patent.\n\nThe worm is pushed or pulled through a worm nut, causing it to rotate. The first patent for a worm nut was awarded to an American inventor M. Redlinger in 1893.\n\nA vast number of utility and design patents were registered for mechanical and automatic mounted and bar corkscrews. These were manufactured in many countries, but mostly in the United States. Arcade Manufacturing Co. of Freeport, Illinois held forty-five different patents, from R. Gilchrist’s “Lightning” and #384,839 (both 1888) to C. Morgan’s “Pix” (1913). Next came Edwin Walker’s Erie Specialty Co. with sixteen different patents, including three crank and pump variations.\n\nAmerican, German and French bar corkscrews were made of cast iron, often plated with nickel. The British favoured brass, as did the Spanish.\n\n\nBecause the bar corkscrews were mounted on counters in full view of customers, they offered a point of sale advertisement for breweries, such as Anheuser-Busch, and cigar makers who affixed private label advertising plates to them.\n\nLike the marketing departments of the auto makers who followed them, the bar corkscrew sales gurus came up with model names to appeal to customers. Some gave an indication of function (Extractee, Pullmee, Yankee), some of size (Midget, Little Giant, Jumbo) some of strength (Champion, Hero, Invincible, Titan), some of how slickly they worked (Quick & Easy, Simple, Express, Safety, Perfect, Presto, Little Quicker, Schnell, Swift) and some of class (Ritz, Luxury, L’Élégant). Some were named in a spirit of celebration (New Century, Jubilee, Triumph, Victor).\n\n"}
{"id": "48795986", "url": "https://en.wikipedia.org/wiki?curid=48795986", "title": "OpenAI", "text": "OpenAI\n\nOpenAI is a non-profit artificial intelligence (AI) research company that aims to promote and develop friendly AI in such a way as to benefit humanity as a whole. Founded in late 2015, the San Francisco-based organization aims to “freely collaborate” with other institutions and researchers by making its patents and research open to the public. The founders (notably Elon Musk and Sam Altman) are motivated in part by concerns about existential risk from artificial general intelligence.\n\nIn October 2015, Musk, Altman and other investors announced the formation of the organization, pledging over US$1 billion to the venture.\n\nOn April 27, 2016, OpenAI released a public beta of “OpenAI Gym”, its platform for reinforcement learning research. \n\nOn December 5, 2016, OpenAI released Universe, a software platform for measuring and training an AI's general intelligence across the world's supply of games, websites and other applications. On February 21, 2018, Musk resigned his board seat, citing “a potential future conflict (of interest)” with Tesla AI development for self driving cars, but remained a donor.\n\nAs of 2018, OpenAI is headquartered in San Francisco's Mission District, sharing an office building with Neuralink, another company co-founded by Musk.\n\n\nOther backers of the project include:\n\nCompanies:\n\nThe group started in early January 2016 with nine researchers. According to \"Wired\", Brockman met with Yoshua Bengio, one of the “founding fathers” of the deep learning movement, and drew up a list of the “best researchers in the field”. Microsoft's Peter Lee stated that the cost of a top AI researcher exceeds the cost of a top NFL quarterback prospect. While OpenAI pays corporate-level (rather than nonprofit-level) salaries, it doesn't currently pay AI researchers salaries comparable to those of Facebook or Google. Nevertheless, Sutskever stated that he was willing to leave Google for OpenAI “partly of because of the very strong group of people and, to a very large extent, because of its mission.” Brockman stated that “the best thing that I could imagine doing was moving humanity closer to building real AI in a safe way.” OpenAI researcher Wojciech Zaremba stated that he turned down “borderline crazy” offers of two to three times his market value to join OpenAI instead.\n\nSome scientists, such as Stephen Hawking and Stuart Russell, believed that if advanced AI someday gains the ability to re-design itself at an ever-increasing rate, an unstoppable “intelligence explosion” could lead to human extinction. Musk characterizes AI as humanity's \"biggest existential threat.\" OpenAI's founders structured it as a non-profit so that they could focus its research on creating a positive long-term human impact.\n\nOpenAI states that “it's hard to fathom how much human-level AI could benefit society,” and that it's equally difficult to comprehend “how much it could damage society if built or used incorrectly”. Research on safety cannot safely be postponed: “because of AI's surprising history, it's hard to predict when human-level AI might come within reach.” OpenAI states that AI “should be an extension of individual human wills and, in the spirit of liberty, as broadly and evenly distributed as possible...”, and which sentiment has been expressed elsewhere in reference to a potentially enormous class of AI-enabled products: \"Are we really willing to let our society be infiltrated by autonomous software and hardware agents whose details of operation are known only to a select few? Of course not.\" Co-chair Sam Altman expects the decades-long project to surpass human intelligence.\n\nVishal Sikka, former CEO of Infosys, stated that an “openness” where the endeavor would “produce results generally in the greater interest of humanity” was a fundamental requirement for his support, and that OpenAI “aligns very nicely with our long-held values” and their “endeavor to do purposeful work”. Cade Metz of \"Wired\" suggests that corporations such as Amazon may be motivated by a desire to use open-source software and data to level the playing field against corporations such as Google and Facebook that own enormous supplies of proprietary data. Altman states that Y Combinator companies will share their data with OpenAI.\n\nMusk posed the question: “what is the best thing we can do to ensure the future is good? We could sit on the sidelines or we can encourage regulatory oversight, or we could participate with the right structure with people who care deeply about developing AI in a way that is safe and is beneficial to humanity.” Musk acknowledged that “there is always some risk that in actually trying to advance (friendly) AI we may create the thing we are concerned about”; nonetheless, the best defense is “to empower as many people as possible to have AI. If everyone has AI powers, then there's not any one person or a small set of individuals who can have AI superpower.”\n\nMusk and Altman's counter-intuitive strategy of trying to reduce the risk that AI will cause overall harm, by giving AI to everyone, is controversial among those who are concerned with existential risk from artificial intelligence. Philosopher Nick Bostrom is skeptical of Musk's approach: “If you have a button that could do bad things to the world, you don't want to give it to everyone.” During a 2016 conversation about the technological singularity, Altman said that “we don’t plan to release all of our source code” and mentioned a plan to “allow wide swaths of the world to elect representatives to a new governance board”. Greg Brockman stated that “Our goal right now... is to do the best thing there is to do. It’s a little vague.”\n\nGym aims to provide an easy-to-setup general-intelligence benchmark with a wide variety of different environments - somewhat akin to, but broader than, the ImageNet Large Scale Visual Recognition Challenge used in supervised learning research - and that hopes to standardize the way in which environments are defined in AI research publications, so that published research becomes more easily reproducible. The project claims to provide the user with a simple interface. As of June2017, the gym can only be used with Python. As of September 2017, the gym documentation site was not maintained, and active work focused instead on its GitHub page.\n\nIn “RoboSumo”, virtual humanoid “metalearning” robots initially lack knowledge of how to even walk, and given the goals of learning to move around, and pushing the opposing agent out of the ring. Through this adversarial learning process, the agents learn how to adapt to changing conditions; when an agent is then removed from this virtual environment and placed in a new virtual environment with high winds, the agent braces to remain upright, suggesting it had learned how to balance in a generalized way. OpenAI's Igor Mordatch argues for that competition between agents can create an intelligence “arms race” that can increase an agent's ability to function, even outside the context of the competition.\n\nIn 2018, OpenAI launched the Debate Game, which teaches machines to debate toy problems in front of a human judge. The purpose is to research whether such an approach may assist in auditing AI decisions and in developing explainable AI.\n\nOpenAI Five is the name of a team of five OpenAI-curated bots that are used in the competitive five-on-five video game \"Dota 2\", who learn to play against human players at a high skill level entirely through trial-and-error algorithms. Before becoming a team of five, the first public demonstration occurred at The International 2017, the annual premiere championship tournament for the game, where Dendi, a professional Ukrainian player of the game, lost against a bot in a live 1v1 matchup. After the match, CTO Greg Brockman explained that the bot had learned by playing against itself for two weeks of real time, and that the learning software was a step in the direction of creating software that can handle complex tasks “like being a surgeon”. The system uses a form of reinforcement learning, as the bots learn over time by playing against themselves hundreds of times a day for months, and are rewarded for actions such as killing an enemy and destroying towers. By June 2018, the ability of the bots expanded to play together as a full team of five and were able to defeat teams of amateur and semi-professional players. At The International 2018, OpenAI Five played in two games against professional players. Although the bots lost both games, OpenAI considered it a successful venture, stating that playing against some of the best players in \"Dota 2\" allowed them to analyze and adjust their algorithms for future games.\n\nDactyl uses machine learning to train a robot Shadow Hand from scratch, using the same reinforcement learning algorithm code that OpenAI Five uses. The robot hand is trained entirely in physically inaccurate simulation.\n\n\n"}
{"id": "49550476", "url": "https://en.wikipedia.org/wiki?curid=49550476", "title": "People Pattern", "text": "People Pattern\n\nPeople Pattern is a digital marketing company based in Austin, Texas. The company's text analytics software applications extract facts, relationships and sentiment from unstructured data, which comprise approximately 85% of the information companies store electronically.\n\nCorporations that use or have used People Pattern software include McDonald's, Charles Schwab, Campbell's Soup, Discover, Wal-Mart, Nintendo, The University of Texas, and Cisco Systems.\n\nPeople Pattern was founded in 2013. An early investor in People Pattern was Mohr Davidow Ventures.\n\n"}
{"id": "28215876", "url": "https://en.wikipedia.org/wiki?curid=28215876", "title": "Post hole digger", "text": "Post hole digger\n\nA post hole digger is a tool used to dig narrow holes to install posts, such as for fences and signs.\n\nThere are different kinds of post hole diggers. A post hole pincer (pictured) is jabbed into the ground in the open position until the blades are buried. At that point the handles are pulled apart to close the tool and grab the chunk of soil loosened. They are then pulled out of the ground with the chunk of soil. The process is repeated until the hole is deep enough, or until the hole is so deep and narrow that the handles can no longer be pulled apart fully. This is one of the weaknesses of this kind of post hole digger.\n\nThere are also auger type post hole diggers. These are generally easier to use, and they can be used to a much greater depth, as the hole can be dug as deep as the entire length of the shaft. More importantly, they form a much neater hole, with a well-defined circumference. However, the very fact that the person doing the digging is guaranteed a perfect hole with an auger post hole digger is also its greatest weakness: any given auger style post hole digger can only dig one size hole—a hole with the same diameter as the auger's screw.\n"}
{"id": "2341454", "url": "https://en.wikipedia.org/wiki?curid=2341454", "title": "Pulp mill", "text": "Pulp mill\n\nA pulp mill is a manufacturing facility that converts wood chips or other plant fibre source into a thick fibre board which can be shipped to a paper mill for further processing. Pulp can be manufactured using mechanical, semi-chemical or fully chemical methods (kraft and sulfite processes). The finished product may be either bleached or non-bleached, depending on the customer requirements.\n\nWood and other plant materials used to make pulp contain three main components (apart from water): cellulose fibres (desired for papermaking), lignin (a three-dimensional polymer that binds the cellulose fibres together) and hemicelluloses, (shorter branched carbohydrate polymers). The aim of pulping is to break down the bulk structure of the fibre source, be it chips, stems or other plant parts, into the constituent fibres.\n\nChemical pulping achieves this by degrading the lignin and hemicellulose into small, water-soluble molecules which can be washed away from the cellulose fibres without depolymerizing the cellulose fibres (chemically depolymerizing the cellulose weakens the fibres). The various mechanical pulping methods, such as groundwood (GW) and refiner mechanical (RMP) pulping, physically tear the cellulose fibres one from another. Much of the lignin remains adhering to the fibres. Strength is impaired because the fibres may be cut. Related hybrid pulping methods use a combination of chemical and thermal treatment to begin an abbreviated chemical pulping process, followed immediately by a mechanical treatment to separate the fibres. These hybrid methods include thermomechanical pulping (TMP) and chemithermomechanical pulping (CTMP). The chemical and thermal treatments reduce the amount of energy subsequently required by the mechanical treatment, and also reduce the amount of strength loss suffered by the fibres.\n\nMuch of the information about the technology in following subsections is from the book by C.J. Biermann. The chemistry of the various pulping processes can be found in Sjöström's book.\n\nThe most common fibre source for pulp mills is pulpwood. Other common sources are bagasse and fibre crops. The first step in all mills using wood (trees) as the fibre source is to remove the bark. Bark contains relatively few usable fibres and darkens the pulp. The removed bark is burned, along with other unusable plant material, to generate steam to run the mill. Almost all wood is then chipped before being processed further in order to free the fibres.\n\nRemoval of the bark is done in a \"barker\" (or \"debarker\"). The bark adhesion is about 3–5 kg/cm in the growing season (summer) and 2-3 times higher in the dormant season (winter). The bark of frozen logs is even more difficult to remove.\n\nIn chemical pulp mills, the bark introduces unwanted contaminants such as calcium, silica and aluminium that cause scaling and give an extra loading for the chemical recovery system. Birch bark contains betulin, a terpenoid that easily creates deposits in a pulp mill.\n\nThe earliest mills used sandstone grinding rollers to break up small wood logs called \"bolts\", but the use of natural stone ended in the 1940s with the introduction of manufactured stones with embedded silicon carbide or aluminum oxide. The pulp made by this process is known as \"stone groundwood\" pulp (SGW). If the wood is ground in a pressurized, sealed grinder the pulp is classified as \"pressure groundwood\" (PGW) pulp. Most modern mills use chips rather than logs and ridged metal discs called refiner plates instead of grindstones. If the chips are just ground up with the plates, the pulp is called \"refiner mechanical\" pulp (RMP), if the chips are steamed while being refined the pulp is called \"thermomechanical\" pulp (TMP). Steam treatment significantly reduces the total energy needed to make the pulp and decreases the damage (cutting) to fibers. Mechanical pulp mills use large amounts of energy, mostly electricity to power motors which turn the grinders. A rough estimate of the electrical energy needed is 10,000 megajoules (MJ) per tonne of pulp (2,750 kWh per tonne)\n\nChemical pulping processes such as the kraft (or sulphate) process and the sulfite process remove much of the hemicelluloses and lignin. The kraft process does less damage to the cellulose fibres than the sulfite process, thereby producing stronger fibres, but the sulfite process makes pulp that is easier to bleach. The chemical pulping processes use a combination of high temperature and alkaline (kraft) or acidic (sulfite) chemicals to break the chemical bonds of the lignin.\n\nThe material fed into the digester must be small enough to allow the pulping liquor to penetrate the pieces completely. In the case of wood, the logs are chipped and the chips screened so that what is fed to the digester is a uniform size. The oversize chips are rechipped or used as fuel, sawdust is burned. The screened chips or cut plant material (bamboo, kenaf, etc.) goes to the digester where it is mixed with an aqueous solution of the pulping chemicals, then heated with steam. In the kraft process the pulping chemicals are sodium hydroxide and sodium sulphide and the solution is known as white liquor. In the sulfite process the pulping chemical is a mixture of metal (sodium, magnesium, potassium or calcium) and ammonium sulfite or sulphite.\nAfter several hours in the digester, the chips or cut plant material breaks down into a thick porridge-like consistency and is \"blown\" or squeezed from the outlet of the digester through an airlock. The sudden change in pressure results in a rapid expansion of the fibres, separating the fibres even more. The resulting fibre suspension in water solution is called \"brown stock\".\n\nBrown stock washers, using countercurrent flow, remove the spent cooking chemicals and degraded lignin and hemicellulose. The extracted liquid, known as black liquor in the kraft process, and red or brown liquor in the sulfite processes, is concentrated, burned and the sodium and sulphur compounds recycled in the recovery process. Lignosulphonates are a useful byproduct recovered from the spent liquor in the sulfite process. The clean pulp (stock) can be bleached in the bleach plant or left unbleached, depending on the end use. The stock is sprayed onto the pulp machine wire, water drains off, more water is removed by pressing the sheet of fibres, and the sheet is then dried. At this point the sheets of pulp are several millimetres thick and have a coarse surface: it is not yet paper. The dried pulp is cut, stacked, bailed and shipped to another facility for whatever further process is needed.\n\nBleached kraft pulp and bleached sulfite pulp are used to make high quality, white printing paper. One of the most visible uses for unbleached kraft pulp is to make brown paper shopping bags and wrapping paper where strength is particularly important. A special grade of bleached sulfite pulp, known as dissolving pulp, is used to make cellulose derivatives such as methylcellulose which are used in a wide range of everyday products from laxatives to baked goods to wallpaper paste.\n\nSome mills pretreat wood chips or other plant material like straw with sodium carbonate, sodium hydroxide, sodium sulfite, and other chemical prior to refining with equipment similar to a mechanical mill. The conditions of the chemical treatment are much less vigorous (lower temperature, shorter time, less extreme pH) than in a chemical pulping process, since the goal is to make the fibers easier to refine, not to remove lignin as in a fully chemical process. Pulps made using these hybrid processes are known as chemi-thermomechanical pulps (CTMP). Sometimes a CTMP mill is located on the same site as a kraft mill so that the effluent from the CTMP mill can be treated in the kraft recovery process to regenerate the inorganic pulping chemicals.\n\nThe pulp process involves many production stages, usually coupled with intermediate storage tanks. As each stage has a different reliability and bottlenecks may vary from day to day, scheduling a pulp mill needs to take into account these bottlenecks and the probability of a disturbance or breakdown. Each stage also may have different decision variables, such as steam / water / chemical input, etc. Finally, scheduling needs to consider fuel optimisation and CO emissions, because part of the energy requirements may be met from fossil-fuel boilers. The overall aim is to maximise production at minimum cost.\n\nStainless steels are used extensively in the Pulp and Paper industry for two primary reasons, to avoid iron contamination of the product and their corrosion resistance to the various chemicals used in the paper making process.\n\nA wide range stainless steels are used throughout the pulp making process. For example, duplex stainless steels are being used in digesters to convert wood chips into wood pulp and 6% Mo superaustenitic stainless steels are used in the bleach plant.\n\n"}
{"id": "5183658", "url": "https://en.wikipedia.org/wiki?curid=5183658", "title": "SMS Magdeburg", "text": "SMS Magdeburg\n\nSMS \"Magdeburg\" (\"His Majesty's Ship \"Magdeburg\") was a lead ship of the of light cruisers in the German \"Kaiserliche Marine\" (Imperial Navy). Her class included three other ships: , , and . \"Magdeburg\" was built at the AG Weser shipyard in Bremen from 1910 to August 1912, when she was commissioned into the High Seas Fleet. The ship was armed with a main battery of twelve 10.5 cm SK L/45 guns and had a top speed of . \"Magdeburg\" was used as a torpedo test ship after her commissioning until the outbreak of World War I in August 1914, when she was brought to active service and deployed to the Baltic.\n\nIn the Baltic, \"Magdeburg\" fired the first shots of the war against the Russians on 2 August, when she shelled the port of Libau. She participated in a series of bombardments of Russian positions until late August. On the 26th, she participated in a sweep of the entrance to the Gulf of Finland; while steaming off the Estonian coast, she ran aground off the island of Odensholm and could not be freed. A pair of Russian cruisers appeared and seized the ship. Fifteen crew members were killed in the brief engagement. They recovered three intact German code books, one of which they passed to the British. The ability to decrypt German wireless signals provided the British with the ability to ambush German units on several occasions during the war, including the Battle of Jutland. The Russians partially scrapped \"Magdeburg\" while she remained grounded before completely destroying the wreck.\n\n\"Magdeburg\" was ordered under the contract name \"Ersatz\" \" and was laid down at the AG Weser shipyard in Bremen in 1910 and launched on 13 May 1911, after which fitting-out work commenced. She was commissioned into the High Seas Fleet on 20 August 1912. The ship was long overall and had a beam of and a draft of forward. She displaced at full combat load. Her propulsion system consisted of two sets of AEG-Vulcan steam turbines driving two propellers. They were designed to give , but reached in service. These were powered by sixteen coal-fired Marine-type water-tube boilers, although they were later altered to use fuel oil that was sprayed on the coal to increase its burn rate. These gave the ship a top speed of . \"Magdeburg\" carried of coal, and an additional of oil that gave her a range of approximately at . She had a crew of 18 officers and 336 enlisted men.\n\nThe ship was armed with twelve 10.5 cm SK L/45 guns in single pedestal mounts. Two were placed side by side forward on the forecastle, eight were located amidships, four on either side, and two were side by side aft. The guns had a maximum elevation of 30 degrees, which allowed them to engage targets out to . They were supplied with 1,800 rounds of ammunition, for 150 shells per gun. She was also equipped with a pair of torpedo tubes with five torpedoes submerged in the hull on the broadside. She could also carry 120 mines. The ship was protected by a waterline armored belt that was thick amidships. The conning tower had thick sides, and the deck was covered with up to 60 mm thick armor plate.\n\nAfter her commissioning, \"Magdeburg\" was used as a torpedo test ship. Following the outbreak of World War I in August 1914, she was assigned to the Baltic Sea, under the command of Rear Admiral Robert Mischke. \"Magdeburg\" fired the first shots of the war with Russia on 2 August when she shelled the Russian port of Libau while laid a minefield outside the harbor. The Russians had in fact already left Libau, which was seized by the German Army. The minefield laid by \"Augsburg\" was poorly marked and hindered German operations more than Russian efforts. \"Magdeburg\" and the rest of the Baltic forces then conducted a series of bombardments of Russian positions, including one ten days later, on 12 August, where \"Magdeburg\" shelled the Dagerort lighthouse. On 17 August, \"Magdeburg\", \"Augsburg\", three destroyers, and the minelayer encountered a pair of powerful Russian armored cruisers, and . The Russian commander, under the mistaken assumption that the German armored cruisers and were present, did not attack and both forces withdrew.\nPrince Heinrich, the overall commander of the Baltic naval forces, replaced Mischke with Rear Admiral Behring. Behring ordered another operation for 26 August to sweep for Russian reconnaissance forces in the entrance to the Gulf of Finland. Early that morning, \"Magdeburg\" ran aground off the lighthouse at Odensholm on the Estonian coast. Her escorting destroyer, , attempted to pull her free but was unable to do so and began taking off part of \"Magdeburg\"s crew. While the evacuation was going on, the Russian cruisers and appeared and shelled the stranded cruiser. The Germans destroyed the forward section of the ship, but could not complete her destruction before the Russians reached the ship. Fifteen crew members from \"Magdeburg\" were killed in the attack. The German code books were also not destroyed; the Russians were able to recover three of the books along with the current encryption key. They passed one copy to the British Royal Navy via a pair of Russian couriers on 13 October. The Russian Navy partially scrapped the ship in situ and eventually destroyed the wreck.\n\nThe capture of the code books proved to provide a significant advantage for the Royal Navy. The Admiralty had recently created a deciphering department known as Room 40 to process intercepted German wireless signals. With the code books and cipher key, the British were able to track the movements of most German warships; this information could be passed on to the Admiral John Jellicoe, the commander of the Grand Fleet. This allowed the British to ambush parts of or the entire German fleet on several occasions, most successfully at the Battles of Dogger Bank in January 1915 and Jutland in May 1916.\n\n"}
{"id": "26068545", "url": "https://en.wikipedia.org/wiki?curid=26068545", "title": "Selection and amplification binding assay", "text": "Selection and amplification binding assay\n\nSelection and amplification binding assay (SAAB) is a molecular biology technique typically used to find the DNA binding site for proteins. It was developed by T. Keith Blackwell and Harold M. Weintraub in 1990.\n\nSAAB experimental procedure consists of several steps, depending upon the knowledge available about the binding site. A typical SAAB consists of the following steps:\n\n\nQuox1 is a homeobox gene involved in the regulation of patterns of development (morphogenesis) in animals, fungi and plants and was originally isolated from cDNA library of five week quail embryo. It is the only gene in the hox family that has been found to express in both prosencephalon and mesencephalon involved in the differentiation of the central and peripheral nerve cells. The optimal DNA binding site for Quox1 or its mammalian homologs was identified by SAAB in 2004. The amplified Quox1 DNA fragment obtained from PCR amplification from a human embryo cDNA librarywas digested with EcoRV and XhoI and cloned into the SmaI and XhoI restriction site of the expression vector pGEMEXxBal. The recombinant plasmids were transformed into competent Escherichia coli strain BL21 and Quox1 fusion proteins were isolated by chromatographic techniques.\n\nThe radio labeled probe was incubated with 25 pmol of purified Quox1 homeodomain fusion protein in binding buffer for EMSA. The protein bound DNA was detected by autoradiography, and the bands representing protein–DNA complexes were excised from the gel and the eluted DNA were amplified by PCR using primers complementary to the 20 bp nonrandom flanking sequences. After 5 set of the same procedure,the purified DNA was cloned into pMD 18T and sequenced. Finally the sequence was identified as the consensus binding sequence for Quox1 homeodomain.\n\nBy combining the power of random-sequence selection with pooled sequencing, the SAAB imprint assay makes possible simultaneous screening of a large number of binding site mutants. SAAB also allows the identification of sites with high relative binding affinity since the competition is inherent in the protocol. It can also identify site positions that are neutral or specific bases that can interfere with binding, such as a T at - 4 in the E47 half-site. We can apply the technique to less affinity binding sequence also, provided to keep high concentration of binding protein at each step of binding. It is also possible to identify the binding site even if both the protein and sequence is not known.\n"}
{"id": "51596610", "url": "https://en.wikipedia.org/wiki?curid=51596610", "title": "Shale band", "text": "Shale band\n\nIn the economics of petroleum extraction, the term shale band refers to the range of global oil prices that can support current levels of United States shale oil extraction via hydraulic fracturing techniques.\n\nThe term was defined on May 8, 2015 by Olivier Jakob, the director of Petromatrix, a Swiss-based consultancy company that publishes a daily note on the oil markets. Petromatrix described the shale band as a price range between $45 and $65 per barrel for West Texas Intermediate (WTI) crude oil. Below $45 per barrel, production of US shale oil would fall sharply, and above $65 production would surge. According to Olivier Jakob, the price of crude oil would, therefore, remain within that range as long as the US remained the swing producer of crude oil in an environment where OPEC is producing at capacity. The concept of the shale band was subsequently verified in the market action of 2015 and the Wall Street Journal characterized the shale band as one of the key terms and phrases from 2015. The term is now widely used in by the financial press and market analysts to describe the price action of oil during over the year of 2015 and 2016.\n\nThe price reference used by Petromatrix for the shale band is based on the CME WTI futures contract for the month of December of the next calendar year, a contract also known as the Red December contract. Due to cost reductions taken by US shale oil producers, Olivier Jakob revised the shale band in August 2016 lower by $5 per barrel to a range between $40–$60, on the basis of the WTI December contract for the next calendar year.\n"}
{"id": "38924078", "url": "https://en.wikipedia.org/wiki?curid=38924078", "title": "Sheet explosive", "text": "Sheet explosive\n\nSheet explosives are materials formed by combining an explosive with a \"rubberizer\"—a flexible binding agent. The resulting compound is cast into a flat sheet which is typically pliable and deformable over a wide range of temperature. Typical products are generally shock-insensitive secondary explosives, requiring a blasting cap or other detonator.\n\nDetonation velocities are frequently very high, which can improve the detonation synchronicity across the area of a tertiary charge with a low detonation velocity. This property makes them suitable for use in detonation trains which require precise timing and homogeneous delivery of force across a complex surface (but see also shaped charge for an orthogonal technique).\n\nIn an explosively-pumped magnetic flux compression generator, explosives are used to accelerate the plates of a large capacitor at each other, while the capacitor has a charge. The result is a colossal spike in amperage that can be used, in a typical application, to fire a railgun for kinetic effects or a transient electromagnetic munition for electronic warfare applications. For maximum amperage, the plates must remain parallel as they accelerate towards each other; high detonation velocity is required. In typical designs, however, the capacitor is not made from two flat plates, but from two concentric cylinders. Therefore, sheet explosives simplify construction. The reference cited for this section demonstrates a railgun design using Detasheet C as the sheet explosive.\n\n\nRocket engines have been created out of a large number of sequentially-fired \"stages\" of sheet explosive discs. (This approach is a rough conventional-explosive analogue to the Project Orion nuclear rocket, although with many more explosions per second.) Note that this differs from a pulsed detonation rocket because the fuel consists solely of pre-positioned solid explosives.\n\nThe primary advantages of the technique are reliability, stable long-term storage, and the complete absence of any moving parts or pumps for fuel delivery—the design is merely a set of alternating disks of explosives and delayed primers.\n\n"}
{"id": "2354771", "url": "https://en.wikipedia.org/wiki?curid=2354771", "title": "Spurtle", "text": "Spurtle\n\nThe spurtle (or \"spurtel\", \"spurtil\", \"spirtle\" or \"spartle\") is a wooden Scottish kitchen tool, dating from the fifteenth century, that is used to stir porridge, soups, stews and broths.\n\nThe rod-like shape means that porridge can be stirred without congealing and forming lumps, unlike a spoon that would have a dragging effect during stirring, and the low surface area reduces the chances of porridge sticking to the instrument.\n\nSpurtles are made from wood, including beech, cherry wood, and maple. They come in a range of sizes. Traditional spurtles have thistles at the top, while modern ones often have a smooth taper.\n\nThe custom is that a spurtle should be used to stir in a clockwise direction with the right hand.\n\nEarly spurtles were flat, wooden or metal instruments, with long handles and flat blades. The spatula-like utensils, known as 'Couthie Spurtles,' can be used for flipping oatcakes or making pancakes - but not for making porridge.\n\nThe World Porridge Making Championship awards a \"Golden Spurtle\" as its main prize.\n"}
{"id": "30695471", "url": "https://en.wikipedia.org/wiki?curid=30695471", "title": "THUMS Islands", "text": "THUMS Islands\n\nThe THUMS Islands or Astronaut Islands are a set of four artificial islands in San Pedro Bay off the coast of Long Beach, California. They were built in 1965 to tap into the East Wilmington Oil Field. The landscaping and sound walls were designed to camouflage the operation and reduce noise, and they are the only decorated oil islands in the United States.\n\nAfter a 1964 court case that gave the state of California mineral rights to the area, the islands were built at an estimated cost of $22 million in 1965, the islands were operated by THUMS, a consortium named after the parent companies who bid for the island contract: Texaco, Humble (now Exxon), Union Oil, Mobil, and Shell. The rim of the islands are made of 640,000 tons of boulders from Catalina Island, and the islands were then filled with 3.2 million cubic yards of dredged material from the bay. The islands contain significant landscaping, a waterfall, and tall structures concealing the drilling rigs, including one known as The Condo and mistaken for \"a ritzy hotel\" by those on land. The structures are lit by colored lights at night. The aesthetic mitigation cost $10 million at time of construction, and was overseen by theme park architect Joseph Linesch. They were described by a Los Angeles Times writer as \"part Disney, part Jetsons, part Swiss Family Robinson\".\n\nIn 1975, the state of California and the city of Long Beach sued the THUMS oil companies for artificially depressing oil prices. A federal jury cleared Exxon of all charges, and the other four oil companies settled out of court for \"hundreds of millions of dollars\".\nThe islands and operation were purchased by Occidental Petroleum in 2000. In 2014, Occidental Petroleum spun off all California assets into California Resources Corporation (CRC), who still owns and operates THUMS Islands as of 2018.\n\nSince 1967, they have been known as the Astronaut Islands, having been renamed in honor of four American astronauts who lost their lives in the service of NASA. Island Freeman (Theodore C. \"Ted\" Freeman) is named for the first astronaut to perish in active duty (piloting a T-38 Talon jet trainer). Island Grissom (Virgil I. \"Gus\" Grissom), Island White (Ed White), and Island Chaffee (Roger B. Chaffee) are named after the Apollo 1 astronauts (launch pad accident). \n\nA peak of were produced in 1969. of oil were pumped from THUMS by 1974, the 500 millionth barrel of crude oil was pumped in 1980. By 1992, the pumping volume was of oil per day through the water injection method of oil recovery, producing low-grade crude oil. The 900 millionth barrel of oil was pumped in April 2002, and the one billionth in 2011. The pumped oil contained 20% water in 1965, and by 1994 it was 92% water.\n\nIsland Grissom is the closest to land, containing waterfalls and more sculptured screens than the other islands. The screens and decorations also serve to deter the public from swimming to the islands (trespassers are subject to arrest).\n\nIsland Freeman is the largest of the islands, at .\n\n"}
{"id": "42758904", "url": "https://en.wikipedia.org/wiki?curid=42758904", "title": "United States Aeronautical Reserve", "text": "United States Aeronautical Reserve\n\nThe United States Aeronautical Reserve (U.S.A.R.) was an early aviation organization created by Harvard University’s Aero Club on September 8, 1910. The founder was John H. Ryan, and the General Secretary Richard R. Sinclair. The earliest aviators and others to enroll near the founding date were: “Glen H. Curtiss, Wilbur Wright, Harry S. Harkness, Augustus Post, Clifford B. Harmon, Allan R. Ryan, Herbert L. Saterlee, ex-governor Curtis Guild, Jr., Edwin Gould, Charles K. Hamilton, Horace F. Karnay, John G. Stratton, George M. Cox, Gen. Nelson A. Miles, Commodore John H. Hubbard, Charles F. Willard, Charles J. Glidden, Walter Brookins, Ralph J. Stone, William Hilliard, Cromwell Dixon, Samuel F. Perkins, Capt. Thomas F. Baldwin, Greeley S. Curtiss, General W. A. Bancroft, and Adams D. Clafton.”, Recruiting stations were at Harvard University, in Boston, Massachusetts; Mineola, Long Island; and Belmont Park, Long Island (Belmont Park is most known for its racetrack but it also has quite an early aviation history that became U.S.A.R.'s earliest members such as Glenn Curtiss and Wilber Wright of the Wright Bros).\n\nSome of the United States Aeronautical Reserve's General Board were Clifford Harmon, Chief of Staff; John Barry Ryan, Commodore; Herbert I. Satterlee, NY; John Barry Ryan, NY; Wilbur Wright, Dayton, Ohio; Glenn Curtiss, Hammondsport, NY; Cortland Field Bishop, NY; Hon. John F. Fitzgerald, Boston, MA; Charles H. Allen (Treasurer), NY and Richard R. Sinclair (Assistant Treasurer) NY. The United States Aeronautical Reserve military contacts were the Army’s Brigadier General James Allen, Aeronautical Division, U.S. Signal Corps, Chief Signal Officer; and, Captain W. Irving Chambers of the Navy; and Major General Leonard Wood, Chief of Staff and U.S.A.R. member.\n\n\"With offices not far from those of the Aero Club of America in New York City, the U.S.A.R. by November 1910 claimed no less than 3,200 members, including President Taft [U.S. President William Howard Taft].\"\n\nThe United States Aeronautical Reserve was officially recognized by the “War and Navy Departments,” and was “organized along strictly military lines, with a view of advancing the science as a means of supplementing the national defense . . . And they are anxious that the U.S.A.R. shall not be confused with other aero clubs in New York and other cities, which appear to be striving for existence along lines made famous by certain characteristics peculiar to the female inhabitants of Kilkenny.” \n\nIn 1910, The United States Aeronautical Reserve founder John H. Ryan also started the Commodore John Barry International Target Practice Cup through the Aeronautical Society and offered a $10,000.00 prize for a winning “bomb throwing” contest from an airplane, and the bronze trophy statue was of “Commodore Barry who was the first Commodore in the American Navy.” \"The Washington Post\" reported in a social article in October 1910 that Harmon and Grahame-White split the prize.,Ryan's plan in 1910 to create an airplane landing strip on the roof of the U.S.A.R.'s main headquarters in Manhattan's 53rd Fifth Avenue address in New York City was covered by the media. Ryan figured that by combining several rooftops, he would create a landing strip of approximately 250 feet long by 17 feet wide.\n\nIn 1910, The United States Aeronautical Reserve’s General Board produced its official monthly publication, \"The Air-Scout\", that later merged into \"Town & Country\" magazine.\n\nThe Air-Scout was an upscale glossy magazine, approximately 14 inches long and 17 inches wide, filled with U.S. aviation and foreign news. It also contained social pages (such as with socialite aeroplane supporters: Mr. and Mrs. Cornelius Vanderbilt, Mrs. Harry Payne Whitney, Miss Vivien Gould, Mrs. August Belmont, Mr. Allan A. Ryan, Colonel John Jacob Astor, Mrs. Mortimer Schiff, Mrs. Charles Gibson, Miss Lilla B. Gilbert, Miss Hannah Randolph . . .); a woman's aviator page (Baroness Raymond de La Roche of France was said to be the first woman to obtain a pilot license and operate an airplane) in several issues; wireless technology news; airship news, airplane contests, military aviation news including where the U.S.A.R. may be needed; and more. There were plenty of photos from war correspondents and other professional photographers and agencies. Many of the feature writers were U.S.A.R. members including Harry M. Horton credited with \"creating the earliest longest distance wireless apparatus that was first used on an airplane in flight, military aviators and similar.\" There were many advertisements in the publication.\n\nIn 1911, the First International :Industrial Airplane Show was held in conjunction with the 11th U.S. International Auto Show at Manhattan’s :Grand Central Palace, in New York City. The aviation show was the invent of the Aero Club of New York, and the event had the largest Palace attendance ever recorded back then., The United States Aeronautical Reserve had an exhibition booth with interesting airplane displays and a demonstration on January 5, 1911 of early wireless communication technology utilizing the \"Wilcox aeroplane equipped with Horton [Harry M. Horton] wireless apparatus\" used to communicate from the airplane to the land-based news media and to test distance with steamships out at sea., The Aeronautical Society and the United States Aeronautical Reserve had their full-size airplane displays in the second gallery of the :Grand Central Palace among other full-size airplanes. Charles W. Chappelle, a member of the United States Aeronautical Reserve, exhibited a full-size airplane which won him a medal for being the only African-American to invent and display an airplane.\n\nBoth the \"Boston Daily Globe\" and the United States Aeronautical Reserve's (U.S.A.R.'s) \"The Air-Scout\" covered Grahame-White landing an airplane near the War Office in Washington, D.C. in October 1910. It was a distance and speed demonstration display, with the U.S.A.R. requesting Grahame-White to perform the test in front of hundreds of military personnel that stood outside and watched as he successfully landed his airplane in a narrow street within a few minutes from a satisfactory distance.\n\nAccording to the \"Boston Daily Globe\", \". . . and within 10 minutes, had landed lightly on the narrow roadway between the White House and the war department, at the feet of General Leonard Wood and within a few yards of the window of President Taft's office.\" The \"Boston Daily Globe\" mentioned General Nelson A. Miles stating, \"I am convinced that one aeroplane would annihilate an entire fleet by dropping bombs upon the deck, or the more vital spot--their engine rooms by way of the funnels . . .\", and Major General Leonard Wood, commander of the army spoke on how the escalation of airplane technology and the wanted airplane capabilities would be \"fulfilled\" in the future.\n\nAlthough the U.S.A.R. had much bigger plans for many of their airplanes to be used by the U.S. military, the U.S. military did utilize at least one of their airplanes in a peacekeeping effort with two of the U.S.A.R. members, according to \"The Air-Scout's\" March 1911 issue:\n“On February 16 [1911], the General Staff of the United States Army accepted the service of Mr. Collier’s biplane offered by the U.S.A.R. On the same day, Major General Leonard Wood publicly announced that the craft would be ordered to the Mexican frontier. On the next day, for the first time in the history of man, an aeroplane was ordered to the scene of the battle, with instructions to patrol the Mexican border in order to preserve neutrality laws. Lieutenant Foulios, a trained United States Army aviator officer, stationed at Fort Sam Houston near San Antonio, Texas, was commanded to report for service on board the airplane. Phillip O. Parmalee, one of the Wright aviators, a lieutenant of the U.S.A.R., native of Michigan, volunteered his services to the government through the reserves which were accepted. He was also commanded to proceed to Texas.” Photos of this were published in \"The Air-Scout\".\n\n"}
