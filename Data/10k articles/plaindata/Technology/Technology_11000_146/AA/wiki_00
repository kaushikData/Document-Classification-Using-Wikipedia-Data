{"id": "884099", "url": "https://en.wikipedia.org/wiki?curid=884099", "title": "Advanced Camera for Surveys", "text": "Advanced Camera for Surveys\n\nThe Advanced Camera for Surveys (ACS) is a third-generation axial instrument aboard the Hubble Space Telescope (HST). The initial design and scientific capabilities of ACS were defined by a team based at Johns Hopkins University. ACS was assembled and tested extensively at Ball Aerospace & Technologies Corp. and the Goddard Space Flight Center and underwent a final flight-ready verification at the Kennedy Space Center before integration in the cargo bay of the Columbia orbiter. It was launched on March 1, 2002 as part of Servicing Mission 3B (STS-109) and installed in HST on March 7, replacing the Faint Object Camera (FOC), the last original instrument. ACS cost 86 million USD at that time.\n\nACS is a highly versatile instrument that became the primary imaging instrument aboard HST. It offered several important advantages over other HST instruments: three independent, high-resolution channels covering the ultraviolet to the near-infrared regions of the spectrum, a large detector area and quantum efficiency, resulting in an increase in HST's discovery efficiency by a factor of ten, a rich complement of filters, and coronagraphic, polarimetric, and grism capabilities. The observations undertaken with ACS provided astronomers with a view of the Universe with uniquely high sensitivity, as exemplified by the Hubble Ultra Deep Field, and encompass a wide range of astronomical phenomena, from comets and planets in the Solar System to the most distant quasars known.\n\nOn 25 June 2006 ACS went out of action due to electronic failure. It was powered up successfully after switching to its redundant (Side-2) set of electronics. The instrument sub-systems, including the CCD detectors, all seemed to be working well and after some engineering tests, ACS resumed science operations on July 4, 2006. On 23 September 2006, the ACS again failed, though by 9 October the problem had been diagnosed and resolved. On January 27, 2007 the ACS failed due to a short circuit in its backup power supply. The instrument's Solar Blind Channel (SBC) was returned to operation on 19 February 2007 using the side-1 electronics. The Wide Field Channel (WFC) was returned to service by STS-125 in May 2009. The High Resolution Channel (HRC), however, remains offline.\n\nACS includes three independent channels (one now disabled), each optimized for specific scientific tasks:\n\nThe WFC is the most utilized channel of ACS. Its detector consists of two butted 2048x4096, 15 µm/pixel charge-coupled devices (CCDs) for a total of 16 megapixels manufactured by Scientific Imaging Technologies (SITe). The WFC plate scale is 0.05″ per pixel and it has an effective field-of-view of 202″×202″. The spectral range of the WFC detector is 350–1100 nm.\n\nAn example of a use of this channel was SWEEPS, which found 16 candidate exoplanets in the Galactic core.\n\nThe HRC, which has been permanently disabled since 2007 due to an electrical fault, provided ultra-sharp views over a smaller field-of-view. The channel used two light suppression options for imaging faint objects around bright stars, improving the contrast of targets close to bright sources by tenfold. The first was a commandable coronagraphic mask that included two occulting spots, one of diameter 1.8\" at the center of the field and the other of diameter 3.0\" nearer to a corner. The first spot was the most popular of the two, for example, for imaging circumstellar disks around nearby bright stars or the host galaxies of luminous quasars. The second was the so-called Fastie Finger, 0.8\" in width and 5\" in length, located at the entrance of the HRC dewar window. The HRC detector was a 1024×1024 SITe CCD which had a smaller field-of-view (26\"×29\") than the WFC but twice the spatial sampling (0.025\" per pixel). This detector was also significantly more sensitive than the WFC at near-ultraviolet wavelengths (<350 nm).\n\nThe Multi Anode Microchannel Array (MAMA) of the SBC is a low-background photon-counting device optimized for the ultraviolet in the wavelength range of 115–170 nm. It consists of a photocathode, a microchannel plate, and an anode array. Its spatial sampling is 0.030\" per pixel and its field-of-view is 25\"×25\". The ACS SBC is in fact a flight spare from the Space Telescope Imaging Spectrograph (STIS).\n\nACS possesses a set of 38 filters and dispersers distributed among three wheels. Two of these wheels are shared by the HRC and WFC light paths while the third is dedicated to the SBC. The HRC and WFC elements consist of eleven broad-band filters, one medium-band filter, five narrow-band filters, three visible and three ultraviolet polarizers, one prism for the HRC, and one grism (580–1100 nm). Four of the filters have bandpasses in the near-ultraviolet and so can be used with the HRC only. The primary broad-band filters are equivalent to the \"u\", \"g\", \"r\", \"i\", and \"z\" filters of the ground-based Sloan Digital Sky Survey (SDSS). Five linear ramp filters divided into three individual segments each provide continuous imaging capability from 380 nm to 1070 nm and so ensure adequate sampling of emission lines over a large range in redshift. Only the middle segment is accessible to the HRC. The SBC wheel is populated with one medium-band filter (Lyα), five long-pass filters, and two objective prisms.\n\n\n\n"}
{"id": "324807", "url": "https://en.wikipedia.org/wiki?curid=324807", "title": "Amatol", "text": "Amatol\n\nAmatol is a highly explosive material made from a mixture of TNT and ammonium nitrate. The British name originates from the words ammonium and toluene (an ingredient of TNT). Similar mixtures (1 part dinitronaphthalene and 7 parts ammonium nitrate) were known as Schneiderite in France. Amatol was used extensively during World War I and World War II, typically as an explosive in military weapons such as aircraft bombs, shells, depth charges, and naval mines. It was eventually replaced with alternative explosives such as composition B, torpex, and tritonal.\n\nAmatol exploits synergy between TNT and ammonium nitrate. TNT has higher explosive velocity and brisance, but is deficient in oxygen. Oxygen deficiency causes black smoke residue from a pure TNT explosion. The oxygen surplus of ammonium nitrate increases the energy release of TNT during detonation. Depending on the ratio of ingredients used, amatol leaves a residue of white or grey smoke after detonation. Amatol has a lower explosive velocity and correspondingly lower brisance than TNT but is cheaper because of the lower cost of ammonium nitrate. \n\nAmatol allowed supplies of TNT to be expanded considerably, with little reduction in the destructive power of the final product, so long as the amount of TNT in the mixture did not fall below 60%. Mixtures containing as little as 20% TNT were for less demanding uses.\n\nTNT is 50% deficient in oxygen. Amatol is oxygen balanced and is therefore more effective than pure TNT when exploding underground or underwater. RDX also has a negative oxygen balance. \n\nRelatively unsophisticated cannery equipment can be adapted to amatol production. TNT is gently heated with steam or hot water until it melts, acquiring the physical characteristics of a syrup. Then the correct weight ratio of powdered ammonium nitrate is added and mixed in. Whilst this mixture is still in a molten state, it is poured into empty bomb casings and allowed to cool and solidify. The lowest grades of amatol could not be produced by casting molten TNT. Instead, flaked TNT was thoroughly mixed with powdered ammonium nitrate and then compressed or extruded.\n\nThe colour of amatol ranges from off-white to slightly yellow or pinkish brown, depending on the mixture used, and remains soft for long periods of storage. It is also hygroscopic, which complicates long-term storage. To prevent moisture problems, amatol charges were coated with a thin layer of pure molten TNT or alternatively bitumen. Long-term storage was rare during wars because munitions charged with amatol were generally used soon after manufacture.\n\nAmatol should not be stored in containers made from copper or brass, as it can form unstable compounds sensitive to vibration. Pressed, it is relatively insensitive but may be detonated by severe impact, whereas when cast, it is extremely insensitive. Primary explosives such as mercury fulminate were often used as a detonator, in combination with an explosive booster charge such as tetryl.\n\nThe explosive charges hidden in HMS \"Campbeltown\" during the St. Nazaire Raid of 1942 contained amatol. The British X class midget submarines which planted explosive charges beneath the German battleship \"Tirpitz\" in September 1943 carried two \"saddle charges\" containing four tons of amatol. Warheads for the German V-1 flying bomb and V-2 rockets also contained amatol.\n\nA derivative of amatol is amatex, consisting of 51% ammonium nitrate, 40% TNT, and 9% RDX.\n\nAmatol is rare today, except in legacy munitions or unexploded ordnance. A form of amatol exists under a different name — ammonite. Ammonite is a civilian explosive, generally comprising a 20/80 mixture of TNT and ammonium nitrate. Typically, it is used for quarrying or mining purposes. It is a popular civil engineering explosive in Eastern Europe and China. \n\nBecause the proportion of TNT is significantly lower than in its military counterpart, ammonite has much less destructive power; given ammonite's use, this is not a problem. In general, a 30 kilogram charge of ammonite is roughly equivalent to 20 kilograms of TNT.\n\nAmatol was the name given to a munitions factory and planned community built by the United States government in Mullica Township, New Jersey during World War I. After the war, the town was dismantled. The Atlantic City Speedway was built on part of the Amatol site in 1926.\n\n"}
{"id": "586357", "url": "https://en.wikipedia.org/wiki?curid=586357", "title": "Artificial general intelligence", "text": "Artificial general intelligence\n\nArtificial general intelligence (AGI) is the intelligence of a machine that could successfully perform any intellectual task that a human being can. It is a primary goal of some artificial intelligence research and a common topic in science fiction and future studies. Artificial general intelligence is also referred to as \"strong AI\", \"full AI\" or as the ability of a machine to perform \"general intelligent action\". Academic sources reserve \"strong AI\" to refer to machines capable of experiencing consciousness.\n\nSome references emphasize a distinction between strong AI and \"applied AI\" (also called \"narrow AI\" or \"weak AI\"): the use of software to study or accomplish specific problem solving or reasoning tasks. Weak AI, in contrast to strong AI, does not attempt to perform the full range of human cognitive abilities.\n\nAs of 2017, over forty organizations worldwide are doing active research on AGI.\n\nVarious criteria for intelligence have been proposed (most famously the Turing test) but to date, there is no definition that satisfies everyone. However, there \"is\" wide agreement among artificial intelligence researchers that intelligence is required to do the following:\n\nOther important capabilities include the ability to sense (e.g. see) and the ability to act (e.g. move and manipulate objects) in the world where intelligent behaviour is to be observed. This would include an ability to detect and respond to hazard. Many interdisciplinary approaches to intelligence (e.g. cognitive science, computational intelligence and decision making) tend to emphasise the need to consider additional traits such as imagination (taken as the ability to form mental images and concepts that were not programmed in) and autonomy.\nComputer based systems that exhibit many of these capabilities do exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent), but not yet at human levels.\n\n\nThe most difficult problems for computers are informally known as \"AI-complete\" or \"AI-hard\", implying that solving them is equivalent to the general aptitude of human intelligence, or strong AI, beyond the capabilities of a purpose-specific algorithm.\n\nAI-complete problems are hypothesised to include general computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real world problem.\n\nAI-complete problems cannot be solved with current computer technology alone, and also require human computation. This property can be useful to test for the presence of humans, as with CAPTCHAs, and for computer security to repel brute-force attacks.\n\nModern AI research began in the mid 1950s. The first generation of AI researchers was convinced that artificial general intelligence was possible and that it would exist in just a few decades. As AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do.\" Their predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's character HAL 9000, who accurately embodied what AI researchers believed they could create by the year 2001. Of note is the fact that AI pioneer Marvin Minsky was a consultant on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time; Crevier quotes him as having said on the subject in 1967, \"Within a generation ... the problem of creating 'artificial intelligence' will substantially be solved,\" although Minsky states that he was misquoted.\n\nHowever, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of AGI and put researchers under increasing pressure to produce useful \"applied AI\". As the 1980s began, Japan's Fifth Generation Computer Project revived interest in AGI, setting out a ten-year timeline that included AGI goals like \"carry on a casual conversation\". In response to this and the success of expert systems, both industry and government pumped money back into the field. However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled. For the second time in 20 years, AI researchers who had predicted the imminent achievement of AGI had been shown to be fundamentally mistaken. By the 1990s, AI researchers had gained a reputation for making vain promises. They became reluctant to make predictions at all and to avoid any mention of \"human level\" artificial intelligence for fear of being labeled \"wild-eyed dreamer[s].\"\n\nIn the 1990s and early 21st century, mainstream AI has achieved far greater commercial success and academic respectability by focusing on specific sub-problems where they can produce verifiable results and commercial applications, such as artificial neural networks, computer vision or data mining. These \"applied AI\" systems are now used extensively throughout the technology industry, and research in this vein is very heavily funded in both academia and industry.\n\nMost mainstream AI researchers hope that strong AI can be developed by combining the programs that solve various sub-problems using an integrated agent architecture, cognitive architecture or subsumption architecture. Hans Moravec wrote in 1988: \"I am confident that this bottom-up route to artificial intelligence will one day meet the traditional top-down route more than half way, ready to provide the real world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphorical golden spike is driven uniting the two efforts.\"\n\nHowever, even this fundamental philosophy has been disputed; for example, Stevan Harnad of Princeton concluded his 1990 paper on the Symbol Grounding Hypothesis by stating: \"The expectation has often been voiced that \"top-down\" (symbolic) approaches to modeling cognition will somehow meet \"bottom-up\" (sensory) approaches somewhere in between. If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols: from the ground up. A free-floating symbolic level like the software level of a computer will never be reached by this route (or vice versa) – nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings (thereby merely reducing ourselves to the functional equivalent of a programmable computer).\"\n\nArtificial general intelligence (AGI) describes research that aims to create machines capable of general intelligent action. The term was introduced by Mark Gubrud in 1997 in a discussion of the implications of fully automated military production and operations. The research objective is much older, for example Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project are regarded as within the scope of AGI. AGI research activity in 2006 was described by Pei Wang and Ben Goertzel as \"producing publications and preliminary results\". As yet, most AI researchers have devoted little attention to AGI, with some claiming that intelligence is too complex to be completely replicated in the near term. However, a small number of computer scientists are active in AGI research, and many of this group are contributing to a series of AGI conferences. The research is extremely diverse and often pioneering in nature. In the introduction to his book, Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century, but the consensus in the AGI research community seems to be that the timeline discussed by Ray Kurzweil in \"The Singularity is Near\" (i.e. between 2015 and 2045) is plausible. Most mainstream AI researchers doubt that progress will be this rapid. Organizations explicitly pursuing AGI include the Swiss AI lab IDSIA, Nnaisense, Vicarious, Maluuba, the OpenCog Foundation, Adaptive AI, LIDA, and Numenta and the associated Redwood Neuroscience Institute. In addition, organizations such as the Machine Intelligence Research Institute and OpenAI have been founded to influence the development path of AGI. Finally, projects such as the Human Brain Project have the goal of building a functioning simulation of the human brain. A 2017 survey of AGI categorized forty-five known \"active R&D projects\" that explicitly or implicitly (through published research) research AGI, with the largest three being DeepMind, the Human Brain Project, and OpenAI.\n\nA popular approach discussed to achieving general intelligent action is whole brain emulation. A low-level brain model is built by scanning and mapping a biological brain in detail and copying its state into a computer system or another computational device. The computer runs a simulation model so faithful to the original that it will behave in essentially the same way as the original brain, or for all practical purposes, indistinguishably. Whole brain emulation is discussed in computational neuroscience and neuroinformatics, in the context of brain simulation for medical research purposes. It is discussed in artificial intelligence research as an approach to strong AI. Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book \"The Singularity Is Near\" predicts that a map of sufficient quality will become available on a similar timescale to the required computing power.\n\n For low-level brain simulation, an extremely powerful computer would be required. The human brain has a huge number of synapses. Each of the 10 (one hundred billion) neurons has on average 7,000 synaptic connections to other neurons. It has been estimated that the brain of a three-year-old child has about 10 synapses (1 quadrillion). This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 10 to 5×10 synapses (100 to 500 trillion). An estimate of the brain's processing power, based on a simple switch model for neuron activity, is around 10 (100 trillion) synaptic updates per second (SUPS). In 1997 Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 10 computations per second (cps). (For comparison, if a \"computation\" was equivalent to one \"floating point operation\" – a measure used to rate current supercomputers – then 10 \"computations\" would be equivalent to 10 petaFLOPS, achieved in 2011). He used this figure to predict the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued.\n\nThe artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently only understood in the broadest of outlines. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition the estimates do not account for glial cells, which are at least as numerous as neurons, and which may outnumber neurons by as much as 10:1, and are now known to play a role in cognitive processes.\n\nThere are some research projects that are investigating brain simulation using more sophisticated neural models, implemented on conventional computing architectures. The Artificial Intelligence System project implemented non-real time simulations of a \"brain\" (with 10 neurons) in 2005. It took 50 days on a cluster of 27 processors to simulate 1 second of a model. The Blue Brain project used one of the fastest supercomputer architectures in the world, IBM's Blue Gene platform, to create a real time simulation of a single rat neocortical column consisting of approximately 10,000 neurons and 10 synapses in 2006. A longer term goal is to build a detailed, functional simulation of the physiological processes in the human brain: \"It is not impossible to build a human brain and we can do it in 10 years,\" Henry Markram, director of the Blue Brain Project said in 2009 at the TED conference in Oxford. There have also been controversial claims to have simulated a cat brain. Neuro-silicon interfaces have been proposed as an alternative implementation strategy that may scale better.\n\nHans Moravec addressed the above arguments (\"brains are more complicated\", \"neurons have to be modeled in more detail\") in his 1997 paper \"When will computer hardware match the human brain?\". He measured the ability of existing software to simulate the functionality of neural tissue, specifically the retina. His results do not depend on the number of glial cells, nor on what kinds of processing neurons perform where.\n\nA fundamental criticism of the simulated brain approach derives from embodied cognition where human embodiment is taken as an essential aspect of human intelligence. Many researchers believe that embodiment is necessary to ground meaning. If this view is correct, any fully functional brain model will need to encompass more than just the neurons (i.e., a robotic body). Goertzel proposes virtual embodiment (like Second Life), but it is not yet known whether this would be sufficient.\n\nDesktop computers using microprocessors capable of more than 10 cps (Kurzweil's non-standard unit \"computations per second\", see above) have been available since 2005. According to the brain power estimates used by Kurzweil (and Moravec), this computer should be capable of supporting a simulation of a bee brain, but despite some interest no such simulation exists . There are at least three reasons for this:\n\nIn addition, the scale of the human brain is not currently well-constrained. One estimate puts the human brain at about 100 billion neurons and 100 trillion synapses. Another estimate is 86 billion neurons of which 16.3 billion are in the cerebral cortex and 69 billion in the cerebellum. Glial cell synapses are currently unquantified but are known to be extremely numerous.\n\nAlthough the role of consciousness in strong AI/AGI is debatable, many AGI researchers regard research that investigates possibilities for implementing consciousness as vital. In an early effort Igor Aleksander argued that the principles for creating a conscious machine already existed but that it would take forty years to train such a machine to understand language.\n\nIn 1980, philosopher John Searle coined the term \"strong AI\" as part of his Chinese room argument. He wanted to distinguish between two different hypotheses about artificial intelligence:\nThe first one is called \"the \"strong\" AI hypothesis\" and the second is \"the \"weak\" AI hypothesis\" because the first one makes the \"stronger\" statement: it assumes something special has happened to the machine that goes beyond all its abilities that we can test. Searle referred to the \"strong AI hypothesis\" as \"strong AI\". This usage is also common in academic AI research and textbooks.\n\nThe weak AI hypothesis is equivalent to the hypothesis that artificial general intelligence is possible. According to Russell and Norvig, \"Most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\"\n\nIn contrast to Searle, Kurzweil uses the term \"strong AI\" to describe any artificial intelligence system that acts like it has a mind, regardless of whether a philosopher would be able to determine if it \"actually\" has a mind or not.\n\nSince the launch of AI research in 1956, the growth of this field has slowed down over time and has stalled the aims of creating machines skilled with intelligent action at the human level. A possible explanation for this delay is that computers lack a sufficient scope of memory or processing power. In addition, the level of complexity that connects to the process of AI research may also limit the progress of AI research.\n\nWhile most AI researchers believe that strong AI can be achieved in the future, there are some individuals like Hubert Dreyfus and Roger Penrose that deny the possibility of achieving AI. John McCarthy was one of various computer scientists who believe human-level AI will be accomplished, but a date cannot accurately be predicted.\n\nConceptual limitations are another possible reason for the slowness in AI research. AI researchers may need to modify the conceptual framework of their discipline in order to provide a stronger base and contribution to the quest of achieving strong AI. As William Clocksin wrote in 2003: \"the framework starts from Weizenbaum’s observation that intelligence manifests itself only relative to specific social and cultural contexts\".\n\nFurthermore, AI researchers have been able to create computers that can perform jobs that are complicated for people to do, but conversely they have struggled to develop a computer that is capable of carrying out tasks that are simple for humans to do . A problem that is described by David Gelernter is that some people assume that thinking and reasoning are equivalent. However, the idea of whether thoughts and the creator of those thoughts are isolated individually has intrigued AI researchers.\n\nThe problems that have been encountered in AI research over the past decades have further impeded the progress of AI. The failed predictions that have been promised by AI researchers and the lack of a complete understanding of human behaviors have helped diminish the primary idea of human-level AI. Although the progress of AI research has brought both improvement and disappointment, most investigators have established optimism about potentially achieving the goal of AI in the 21st century.\n\nOther possible reasons have been proposed for the lengthy research in the progress of strong AI. The intricacy of scientific problems and the need to fully understand the human brain through psychology and neurophysiology have limited many researchers from emulating the function of the human brain into a computer hardware. Many researchers tend to underestimate any doubt that is involved with future predictions of AI, but without taking those issues seriously can people then overlook solutions to problematic questions.\n\nClocksin says that a conceptual limitation that may impede the progress of AI research is that people may be using the wrong techniques for computer programs and implementation of equipment. When AI researchers first began to aim for the goal of artificial intelligence, a main interest was human reasoning. Researchers hoped to establish computational models of human knowledge through reasoning and to find out how to design a computer with a specific cognitive task.\n\nThe practice of abstraction, which people tend to redefine when working with a particular context in research, provides researchers with a concentration on just a few concepts. The most productive use of abstraction in AI research comes from planning and problem solving. Although the aim is to increase the speed of a computation, the role of abstraction has posed questions about the involvement of abstraction operators.\n\nA possible reason for the slowness in AI relates to the acknowledgement by many AI researchers that heuristics is a section that contains a significant breach between computer performance and human performance. The specific functions that are programmed to a computer may be able to account for many of the requirements that allow it to match human intelligence. These explanations are not necessarily guaranteed to be the fundamental causes for the delay in achieving strong AI, but they are widely agreed by numerous researchers.\n\nThere have been many AI researchers that debate over the idea whether machines should be created with emotions. There are no emotions in typical models of AI and some researchers say programming emotions into machines allows them to have a mind of their own. Emotion sums up the experiences of humans because it allows them to remember those experiences. David Gelernter writes, \"No computer will be creative unless it can simulate all the nuances of human emotion.\" This concern about emotion has posed problems for AI researchers and it connects to the concept of strong AI as its research progresses into the future.\n\nThere are other aspects of the human mind besides intelligence that are relevant to the concept of strong AI which play a major role in science fiction and the ethics of artificial intelligence:\nThese traits have a moral dimension, because a machine with this form of strong AI may have legal rights, analogous to the rights of non-human animals. Also, Bill Joy, among others, argues a machine with these traits may be a threat to human life or dignity. It remains to be shown whether any of these traits are necessary for strong AI. The role of consciousness is not clear, and currently there is no agreed test for its presence. If a machine is built with a device that simulates the neural correlates of consciousness, would it automatically have self-awareness? It is also possible that some of these properties, such as sentience, naturally emerge from a fully intelligent machine, or that it becomes natural to \"ascribe\" these properties to machines once they begin to act in a way that is clearly intelligent. For example, intelligent action may be sufficient for sentience, rather than the other way around.\n\nIn science fiction, AGI is associated with traits such as consciousness, sentience, sapience, and self-awareness observed in living beings. However, according to philosopher John Searle, it is an open question whether general intelligence is sufficient for consciousness. \"Strong AI\" (as defined above by Ray Kurzweil) should not be confused with Searle's \"'strong AI hypothesis\". The strong AI hypothesis is the claim that a computer which behaves as intelligently as a person must also necessarily have a mind and consciousness. AGI refers only to the amount of intelligence that the machine displays, with or without a mind.\n\nOpinions vary both on \"whether\" and \"when\" artificial general intelligence will arrive. At one extreme, AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do\". However, this prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\". Writing in The Guardian, roboticist Alan Winfield claimed the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight. AI experts' views on the feasibility of AGI wax and wane, and may have seen a resurgence in the 2010s. Four polls conducted in 2012 and 2013 suggested that the median guess among experts for when they'd be 50% confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081. It is also interesting to note 16.5% of the experts answered with \"never\" when asked the same question but with a 90% confidence instead.\n\nThe creation of artificial general intelligence may have repercussions so great and so complex that it may not be possible to forecast what will come afterwards. Thus the event in the hypothetical future of achieving strong AI is called the technological singularity, because theoretically one cannot see past it. But this has not stopped philosophers and researchers from guessing what the smart computers or robots of the future may do, including forming a utopia by being our friends or overwhelming us in an AI takeover. The latter potentiality is particularly disturbing as it poses an existential risk for mankind.\n\nSmart computers or robots would be able to design and produce improved versions of themselves. A growing population of intelligent robots could conceivably out-compete inferior humans in job markets, in business, in science, in politics (pursuing robot rights), and technologically, sociologically (by acting as one), and militarily.\n\nIf research into strong AI produced sufficiently intelligent software, it would be able to reprogram and improve itself – a feature called \"recursive self-improvement\". It would then be even better at improving itself, and would probably continue doing so in a rapidly increasing cycle, leading to an intelligence explosion and the emergence of superintelligence. Such an intelligence would not have the limitations of human intellect, and might be able to invent or discover almost anything.\n\nHyper-intelligent software might not necessarily decide to support the continued existence of mankind, and might be extremely difficult to stop. This topic has also recently begun to be discussed in academic publications as a real source of risks to civilization, humans, and planet Earth.\n\nOne proposal to deal with this is to make sure that the first generally intelligent AI is a friendly AI that would then endeavor to ensure that subsequently developed AIs were also nice to us. But friendly AI is harder to create than plain AGI, and therefore it is likely, in a race between the two, that non-friendly AI would be developed first. Also, there is no guarantee that friendly AI would remain friendly, or that its progeny would also all be good.\n\n"}
{"id": "4024051", "url": "https://en.wikipedia.org/wiki?curid=4024051", "title": "Beltel", "text": "Beltel\n\nBeltel was the name and trademark used by the South African Department of Posts and Telecommunications (later Telkom) for its Videotex system between the mid eighties and 1999.\n\nThe system used telephone lines and modems connected to personal computers or to dumb terminals which had built in modems and no processing functionality. Telkom's dedicated terminal was named the Minitel.\n\nThe system incorporated a billing system which enabled information providers and service providers to receive payment for information and services provided to users. The billing system was capable of handling very small transactions, and referred to as Micro-billing. Today all of the functionality of the Beltel system, and more, is delivered by handheld mobile phones or cellphones and PDA's which utilize mobile payment.\n\nBooklets/Magazines:\nWhat is BELTEL?\nBELTEL user manual. (Setting up, Facilities, Passwords, Logging off, Mail, Commands) \nVideotex SA Beltel. (Various business services for its 14,000 users) (In 1990)\nBELTEL as a business tool\nThe BELTEL post box.\nTelematics: (News, views, reviews and adverts) (Seasonal)\n\nBeltel provided many online services such as:\nDebates, competitions, prizes.\nBanking:\nAllied Bank, \nFirst National Bank,\nNedbank,\nStandard Bank,\nTrust bank,\nUnited Bank,\nVolkskas Bank\n\nCredit Checking:\nNational Credit Bureau\n\nMessage Handling:\nBeltel e-Mail,\nInterlink,\nTelkom 400\n\nAgriculture\nAgritel Fresh Produce,\nAgritel Meat Service, \nBoere-Data,\nMielieraad\n\nDirectories:\nINNOBEL,\nElectronic Yellow Pages\n\nEntertainment:\nBel-Base,\nElmdene SA,\nPlayworld,\nTimes Media\n\nChatlines:\nFROGG,\nIntercom\n\nNews/Weather:\nAgritel,\nBel-Base,\nElmdene SA,\nTimes Media\n\nClassifieds:\nBel-Base,\nCDS Classifieds,\nJunk Mail\n"}
{"id": "4202769", "url": "https://en.wikipedia.org/wiki?curid=4202769", "title": "Benefield Anechoic Facility", "text": "Benefield Anechoic Facility\n\nBenefield Anechoic Facility (BAF) is an anechoic chamber located at the southwest side of the Edwards Air Force Base main base. It is currently the world's largest anechoic chamber. The BAF supports installed systems testing for avionics test programs requiring a large, shielded chamber with radio frequency (RF) absorption capability that simulates free space.\n\nThe facility is named after Rockwell test pilot and flight commander Tommie Douglas \"Doug\" Benefield who was killed in a crash 22 miles northeast of Edwards Air Force Base in the desert east of Boron on August 29, 1984 during a USAF B-1 Lancer flight test.\n\nThe BAF is a ground test facility to investigate and evaluate anomalies associated with EW systems, avionics, tactical missiles and their host platforms. Tactical-sized, single or multiple, or large vehicles can be operated in a controlled electromagnetic (EM) environment with emitters on and sensors stimulated while RF signals are recorded and analyzed. The largest platforms tested at the BAF have been the B-52 and C-17 aircraft. The BAF supports testing of other types of systems such as spacecraft, tanks, satellites, air defense systems, drones and armored vehicles.\n\nThe BAF equipment generates RF signals with a wide variety of characteristics, simulating red/blue/gray (unfriendly/friendly/unknown) surface-based, sea-based, and airborne systems. With the combination of signals and control functions available, a wide variety of test conditions can be emulated. Many conditions that are not available on outdoor ranges can be easily generated from the aspect of signal density, pulse density and number of simultaneous types.\n\nThrough the use of environmental monitoring systems, an independent agency captures, records, and verifies RF generated signals. These systems have the capabilities for real-time and post-test RF signal parameter measurement, instrument display recording, data analysis and test coordination, as well as providing the data for signal verification.\n\nSome aircraft tested at the BAF include:\n\nIn 2003, BMW tested levels of electromagnetic interference on then-upcoming 2004 models of the 530i, 545i and debut model, 645i.\n\n"}
{"id": "18037596", "url": "https://en.wikipedia.org/wiki?curid=18037596", "title": "Broadband Forum", "text": "Broadband Forum\n\nThe Broadband Forum is a non-profit industry consortium dedicated to developing broadband network specifications. Members include telecommunications networking and service provider companies, broadband device and equipment vendors, consultants and independent testing labs (ITLs). Service provider members are primarily wire-line service providers (non-mobile) telephone companies.\n\nThe DSL Forum was founded in 1994 with about 200 member companies in different divisions of the telecommunication and information technology sector. It is used as a platform for companies that operate in the broadband market. Its initial main purpose was the establishment of new standards around Digital Subscriber Line communication products such as provisioning. This cooperation has brought different standardizations for ADSL, SHDSL, VDSL, ADSL2+ and VDSL2. \n\nThe group was established in 1994 as the ADSL Forum, but became the DSL Forum in 1999, named after the digital subscriber line (DSL) family of technology.\n\nAmong its early design documents, the Forum created TR-001 (1996) system reference model, which together with later TR-012 (1999) core network architecture, recommended PPP over an ATM transport layer as the best practice for a DSL ISP. This was subsequently refined in TR-025 and TR-059. \n\nStarting in 2004, the Forum expanded its work into other last mile technologies including optical fiber. On 17 June 2008 it changed its name to \"Broadband Forum\". DSL-related specifications, while still a key part of the forum's work, are no longer its only work. For instance, the forum produced work specific to passive optical networking (PON). Its Auto-Configuration Server specification TR-069, originally published in 2004, was adapted for use with set-top box and Network Attached Storage units.\n\nThe Forum's TR-101 specification (2006) documents migration toward an Ethernet-based DSL aggregation model (Ethernet DSLAMs).\n\nIn May 2009, IP/MPLS Forum merged with the Broadband Forum. It had promoted the frame relay and Multiprotocol Label Switching technologies.\nTechnical work of IP/MPLS Forum continued in a newly created \"IP/MPLS and Core\" Working Group of the Broadband Forum. The historical specifications from the IP/MPLS Forum's predecessors, ATM Forum, Frame Relay Forum, MFA Forum, and MPLS Forum, are archived on the Broadband Forum's website, under IP/MPLS Forum specifications.\n\nBroadband Forum issued Femto Access Point Service Data Model TR-196 during April 2009 and version 2 released during November 2011.\n\nBroadband Forum specified in TR-348 for Hybrid Access Networks an architecture that enables network operators to efficiently combine XDSL and LTE.\n\n\n"}
{"id": "59159271", "url": "https://en.wikipedia.org/wiki?curid=59159271", "title": "Cellular agriculture society", "text": "Cellular agriculture society\n\nCellular Agricultural Society (or CAS) is an international 501c3 nonprofit organization created to research, fund and advance cellular agriculture.\n\nCellular Agriculture is the emerging science of producing animal products from cells instead of from live animals. \n\nCellular Agriculture, or Cell-Ag, is actively developing foodstuffs and other animal products that include meat, milk, and eggs, also leather, silk and even rhinoceros horn from animal cells.\n\nCellular agriculture uses biotechnology to produce animal products currently harvested from living tissue . \n\nCellular agriculture sciences are evolving based on two techniques:\n\n\nThe process is controversial as certain government ( France, Australia and most recently the state of Missouri) entities are in conflict over what can officially be termed \"meat\".\n"}
{"id": "3243253", "url": "https://en.wikipedia.org/wiki?curid=3243253", "title": "Comb generator", "text": "Comb generator\n\nA comb generator is a signal generator that produces multiple harmonics of its input signal. The appearance of the output at the spectrum analyzer screen, resembling teeth of a comb, gave the device its name.\n\nComb generators find wide range of uses in microwave technology. E.g., synchronous signals in wide frequency bandwidth can be produced by a comb generator. The most common use is in broadband frequency synthesizers, where the high frequency signals act as stable references correlated to the lower energy references; the outputs can be used directly, or to synchronize phase-locked loop oscillators. It may be also used to generate a complete set of substitution channels for testing, each of which carries the same baseband audio and video signal.\n\nComb generators are also used in RFI testing of consumer electronics, where their output is used as a simulated RF emissions, as it is a stable broadband noise source with repeatable output. It is also used during compliance testing to various government requirements for products such as medical devices (FDA), military electronics (MIL-STD-461), commercial avionics (Federal Aviation Administration), digital electronics (Federal Communications Commission), in the USA.\n\nAn optical comb generator can be used as generators of terahertz radiation. Internally, it is a resonant electro-optic modulator, with the capability of generating hundreds of sidebands with total span of at least 3 terahertz (limited by the optical dispersion of the lithium niobate crystal) and frequency spacing of 17 GHz. Other construction can be based on erbium-doped fiber laser or Ti-sapphire laser often in combination with carrier envelope offset control.\n\n\n"}
{"id": "22431918", "url": "https://en.wikipedia.org/wiki?curid=22431918", "title": "Crunchbase", "text": "Crunchbase\n\nCrunchbase is a platform for finding business information about private and public companies.\n\nCrunchbase information includes investments and funding information, founding members and individuals in leadership positions, mergers and acquisitions, news, and industry trends. Originally built to track startups, the Crunchbase website contains information on public and private companies globally.\n\nCrunchbase sources their data in four ways: the venture program, machine learning, an in-house data team, and the Crunchbase community. Members of the public can submit information to the Crunchbase database. These submissions are subject to registration, social validation, and are often reviewed by a moderator before being accepted for publication. \n\nCrunchbase was originally founded in 2007 by entrepreneur Michael Arrington, as a place to track the startups that parent company TechCrunch featured in articles. From 2007 to September 2015, TechCrunch maintained control of the Crunchbase database.\n\nIn September 2010, AOL acquired TechCrunch and subsequently Crunchbase as one of TechCrunch's portfolio companies. In November 2013, AOL was in dispute with start-up Pro Populi over the company’s use of the entire Crunchbase dataset in apps that Pro Populi developed. One of these apps, known as People+ Pro Populi, was represented by the Electronic Frontier Foundation.\n\nIn 2014, Crunchbase added incubators, venture capital partners, and a new leaderboard feature to the startup database. \n\nIn 2015, Crunchbase separated from AOL/Verizon/TechCrunch to become a private entity. In September 2015, in conjunction with the spin out, Crunchbase announced $6.5 million in funding raised from Emergence Capital. This was shortly followed with a follow-up round of $2 million in November 2015.\n\nIn 2016, the company rebranded from CrunchBase to Crunchbase and launched their first product: Crunchbase Pro.\n\nIn April 2017, Crunchbase announced an $18 million Series B from Mayfield Fund. At the same time, Crunchbase launched two new products – Crunchbase Enterprise and Crunchbase for Applications. \n\nIn 2018, Crunchbase launched Crunchbase Marketplace.\n\n\n\n\nCrunchbase has more than 560,000 active community contributors on the platform. Over 5 million users access the Crunchbase website each month.\n"}
{"id": "7982399", "url": "https://en.wikipedia.org/wiki?curid=7982399", "title": "Cryogenic current comparator", "text": "Cryogenic current comparator\n\nThe cryogenic current comparator (CCC) is used in the electrical precision measurements to compare electric currents with highest accuracy. This device exceeds the accuracy of other current comparators around several orders of magnitude and is used in electrical metrology for highly precise comparative measurements of electric resistances or for the amplification and measurement of extremely small electric currents.\n\nThe CCC principle goes back on \"Harvey\" and is based substantially on the properties of superconductors. CCCs make use of macroscopic quantum effects that occur in superconducting materials or circuits underneath their critical temperature of typically a few kelvins. The term “Cryogenic Current Comparator” stems from \"κρυος\" (Gr. \"frost\", \"ice\") and \"comparare\" (Lat. \"compare\"). The two quantum effects used in a CCC are the ideal diamagnetism of the superconductor, caused by the Meissner effect, and the macroscopic quantum interference of currents in a superconducting quantum sensor.\n\nFor the comparison of two currents these are fed through two wires which are led through a superconducting tube. The Meissner effect induces a screening current on the inner surface of the tube, flowing opposite to and being exactly as large as the sum of the currents inside the tube. Thus, this shielding current exactly cancels the magnetic field inside the tube produced by the currents in the wires. The screening current flows back across the outer surface of the tube, giving rise to a magnetic field in the room outside of the tube. This field is detected by a highly sensitive magnetometer, acting as a null detector. The signal of this null detector thus is a measure for the equality of the currents; in particular it is zero if the two currents are of exactly equal magnitude. The important and crucial point characterizing the CCC is the fact that the magnitude of the screening current and its distribution on the surface of the superconducting screen are independent of the position and the path of the wires inside the tube.\n\nTypical for a CCC is the use of a SQUID magnetometer as null detector for the magnetic field (SQUID = Superconducting Quantum Interference Device). These are capable of detecting extremely small changes of the magnetic field corresponding to fractions of the magnetic flux quantum = \"h\"/2\"e\" ≈ 2×10 V·s (\"h\" is Planck's constant and \"e\" the elementary charge). The function principle of a SQUID is based on macroscopic quantum interferences of electric currents, arising in superconducting circuits (loops) with tunnel junctions.\n\nResistance bridges based on CCCs are used for the comparison of electrical resistances, in particular if highest-precision measurements are required, as there is the traceability of the resistance unit to the quantum Hall effect (QHE). In this way, measurements connecting standard resistors ranging within 1 ohm up to 10 kΩ to a QHE resistor of 12.9 kΩ are performed at several national institutes of metrology as, for instance, the National Institute of Standards and Technology (NIST, USA) or the Physikalisch-Technische Bundesanstalt (PTB, D). Here, electrical resistance comparisons using CCCs are accomplished with relative measurement uncertainties of only about 10.\n\n"}
{"id": "41136428", "url": "https://en.wikipedia.org/wiki?curid=41136428", "title": "Curved space diamond structure", "text": "Curved space diamond structure\n\nThe curved space diamond structure is a patented modular building system representative of a diamond crystal enlarged 8 billion times. These playground climbing sculptures were installed in dozens of parks and playgrounds in California, and also throughout the U.S. and Japan in the mid 1970s and early 1980s. Initially installed as playground climbing sculptures, they have also been exhibited as architectural design pieces, referred to as \"usable artworks\", and sold as art at auction.\n\nThe climbing sculptures were designed by Peter Jon Pearce who also designed the Biosphere 2 complex in Arizona, worked for Charles and Ray Eames, and was an assistant to Buckminster Fuller. In addition to playground installations, Curved Spaced Diamond Structures were installed at the 1975 Aspen Design Conference, The Brooklyn Children's Museum, Epcot theme park in Orlando, Florida, and The Hakone Open Air Museum in Japan. The sculptures are constructed of Lexan, a transparent polycarbonate plastic, having a lifespan of at least 20 years. The material used in their construction is tough enough to use as bulletproofing and the joining systems can withstand tensile loads of up to 1000 psi. In 1975, a Curved Space structure was constructed in Aspen Colorado, at the \"Aspen Design Conference. Because of an overwhelming number of requests from the community, it was left standing until the end of summer, long after the conference was over. Three \"Curved Space Labyrinths\" installed in The Brooklyn Children's Museum in 1976, helped to teach children about the geometry of crystal structures. The Curved Space installation at the Hakone Open Air Museum in Japan was originally installed in 1978, and is frequently referred to as “The Soap Bubble Castle”. Marking the 40th anniversary of the museum, there has been an extensive remodel and the Curved Space playground exhibit has been replaced with a new and updated version. Since 1978, the sounds of happy children have always been heard coming from the exhibit at this museum. Originally installed at a playground in the suburbs of Chicago, one of Pearce's sculptures was available for purchase at the Wright Auction House, which specializes in modern and contemporary art, and according to Art & Antiques Magazine \"The structure has subtle richness\".\n\nIn the summer of 2013, a module of the Curved Space Diamond Structure, was on display at The Schindler House, in West Hollywood and at the Yale School of Architecture in New Haven, Connecticut, as part of an exhibition titled \"Everything Loose Will Land\", which highlighted Art and Architecture in 1970's Los Angeles. This exhibit, of the Getty Museum's \"Pacific Standard Time Presents\" series: \"Modern Architecture in L.A.\", was meant to focus our gaze on things we might see in everyday life outside of a museum setting.\n\n\n"}
{"id": "39068", "url": "https://en.wikipedia.org/wiki?curid=39068", "title": "Digital electronics", "text": "Digital electronics\n\nDigital electronics or digital (electronic) circuits are electronics that operate on digital signals. In contrast, analog circuits manipulate analog signals whose performance is more subject to manufacturing tolerance, signal attenuation and noise. Digital techniques are helpful because it is a lot easier to get an electronic device to switch into one of a number of known states than to accurately reproduce a continuous range of values.\n\nDigital electronic circuits are usually made from large assemblies of logic gates (often printed on integrated circuits), simple electronic representations of Boolean logic functions.\n\nThe binary number system was refined by Gottfried Wilhelm Leibniz (published in 1705) and he also established that by using the binary system, the principles of arithmetic and logic could be joined. Digital logic as we know it was the brain-child of George Boole in the mid 19th century. In an 1886 letter, Charles Sanders Peirce described how logical operations could be carried out by electrical switching circuits. Eventually, vacuum tubes replaced relays for logic operations. Lee De Forest's modification, in 1907, of the Fleming valve can be used as an AND gate. Ludwig Wittgenstein introduced a version of the 16-row truth table as proposition 5.101 of \"Tractatus Logico-Philosophicus\" (1921). Walther Bothe, inventor of the coincidence circuit, shared the 1954 Nobel Prize in physics, for the first modern electronic AND gate in 1924.\n\nMechanical analog computers started appearing in the first century and were later used in the medieval era for astronomical calculations. In World War II, mechanical analog computers were used for specialized military applications such as calculating torpedo aiming. During this time the first electronic digital computers were developed. Originally they were the size of a large room, consuming as much power as several hundred modern personal computers (PCs).\n\nThe Z3 was an electromechanical computer designed by Konrad Zuse. Finished in 1941, it was the world's first working programmable, fully automatic digital computer. Its operation was facilitated by the invention of the vacuum tube in 1904 by John Ambrose Fleming.\n\nAt the same time that digital calculation replaced analog, purely electronic circuit elements soon replaced their mechanical and electromechanical equivalents. The bipolar junction transistor was invented in 1947. From 1955 onwards, transistors replaced vacuum tubes in computer designs, giving rise to the \"second generation\" of computers. Compared to vacuum tubes, transistors have many advantages: they are smaller, and require less power than vacuum tubes, so give off less heat. Silicon junction transistors were much more reliable than vacuum tubes and had longer, indefinite, service life. Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space.\n\nAt the University of Manchester, a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of vacuum tubes. Their first transistorised computer and the first in the world, was operational by 1953, and a second version was completed there in April 1955.\n\nWhile working at Texas Instruments in July 1958, Jack Kilby recorded his initial ideas concerning the integrated circuit then successfully demonstrated the first working integrated on 12 September 1958. This new technique allowed for quick, low-cost fabrication of complex circuits by having a set of electronic circuits on one small plate (\"chip\") of semiconductor material, normally silicon.\n\nIn the early days of integrated circuits, each chip was limited to only a few transistors, and the low degree of integration meant the design process was relatively simple. Manufacturing yields were also quite low by today's standards. As the technology progressed, millions, then billions of transistors could be placed on one chip, and good designs required thorough planning, giving rise to new design methods.\n\nAn advantage of digital circuits when compared to analog circuits is that signals represented digitally can be transmitted without degradation caused by noise. For example, a continuous audio signal transmitted as a sequence of 1s and 0s, can be reconstructed without error, provided the noise picked up in transmission is not enough to prevent identification of the 1s and 0s.\n\nIn a digital system, a more precise representation of a signal can be obtained by using more binary digits to represent it. While this requires more digital circuits to process the signals, each digit is handled by the same kind of hardware, resulting in an easily scalable system. In an analog system, additional resolution requires fundamental improvements in the linearity and noise characteristics of each step of the signal chain.\n\nWith computer-controlled digital systems, new functions to be added through software revision and no hardware changes. Often this can be done outside of the factory by updating the product's software. So, the product's design errors can be corrected after the product is in a customer's hands.\n\nInformation storage can be easier in digital systems than in analog ones. The noise immunity of digital systems permits data to be stored and retrieved without degradation. In an analog system, noise from aging and wear degrade the information stored. In a digital system, as long as the total noise is below a certain level, the information can be recovered perfectly. Even when more significant noise is present, the use of redundancy permits the recovery of the original data provided too many errors do not occur.\n\nIn some cases, digital circuits use more energy than analog circuits to accomplish the same tasks, thus producing more heat which increases the complexity of the circuits such as the inclusion of heat sinks. In portable or battery-powered systems this can limit use of digital systems. For example, battery-powered cellular telephones often use a low-power analog front-end to amplify and tune in the radio signals from the base station. However, a base station has grid power and can use power-hungry, but very flexible software radios. Such base stations can be easily reprogrammed to process the signals used in new cellular standards.\n\nDigital circuits are sometimes more expensive, especially in small quantities.\n\nMost useful digital systems must translate from continuous analog signals to discrete digital signals. This causes quantization errors. Quantization error can be reduced if the system stores enough digital data to represent the signal to the desired degree of fidelity. The Nyquist-Shannon sampling theorem provides an important guideline as to how much digital data is needed to accurately portray a given analog signal.\n\nIn some systems, if a single piece of digital data is lost or misinterpreted, the meaning of large blocks of related data can completely change. Because of the cliff effect, it can be difficult for users to tell if a particular system is right on the edge of failure, or if it can tolerate much more noise before failing.\n\nDigital fragility can be reduced by designing a digital system for robustness. For example, a parity bit or other error management method can be inserted into the signal path. These schemes help the system detect errors, and then either correct the errors, or at least ask for a new copy of the data. In a state-machine, the state transition logic can be designed to catch unused states and trigger a reset sequence or other error recovery routine.\n\nDigital memory and transmission systems can use techniques such as error detection and correction to use additional data to correct any errors in transmission and storage.\n\nOn the other hand, some techniques used in digital systems make those systems more vulnerable to single-bit errors. These techniques are acceptable when the underlying bits are reliable enough that such errors are highly unlikely.\n\nA single-bit error in audio data stored directly as linear pulse code modulation (such as on a CD-ROM) causes, at worst, a single click. Instead, many people use audio compression to save storage space and download time, even though a single-bit error may corrupt the entire song.\n\nA digital circuit is typically constructed from small electronic circuits called logic gates that can be used to create combinational logic. Each logic gate is designed to perform a function of boolean logic when acting on logic signals. A logic gate is generally created from one or more electrically controlled switches, usually transistors but thermionic valves have seen historic use. The output of a logic gate can, in turn, control or feed into more logic gates.\n\nIntegrated circuits consist of multiple transistors on one silicon chip, and are the least expensive way to make large number of interconnected logic gates. Integrated circuits are usually designed by engineers using electronic design automation software (see below for more information) to perform some type of function.\n\nIntegrated circuits are usually interconnected on a printed circuit board which is a board which holds electrical components, and connects them together with copper traces.\n\nEach logic symbol is represented by a different shape. The actual set of shapes was introduced in 1984 under IEEE/ANSI standard 91-1984. \"The logic symbol given under this standard are being increasingly used now and have even started appearing in the literature published by manufacturers of digital integrated circuits.\"\n\nAnother form of digital circuit is constructed from lookup tables, (many sold as \"programmable logic devices\", though other kinds of PLDs exist). Lookup tables can perform the same functions as machines based on logic gates, but can be easily reprogrammed without changing the wiring. This means that a designer can often repair design errors without changing the arrangement of wires. Therefore, in small volume products, programmable logic devices are often the preferred solution. They are usually designed by engineers using electronic design automation software.\n\nWhen the volumes are medium to large, and the logic can be slow, or involves complex algorithms or sequences, often a small microcontroller is programmed to make an embedded system. These are usually programmed by software engineers.\n\nWhen only one digital circuit is needed, and its design is totally customized, as for a factory production line controller, the conventional solution is a programmable logic controller, or PLC. These are usually programmed by electricians, using ladder logic.\n\nEngineers use many methods to minimize logic functions, in order to reduce the circuit's complexity. When the complexity is less, the circuit also has fewer errors and less electronics, and is therefore less expensive.\n\nThe most widely used simplification is a minimization algorithm like the Espresso heuristic logic minimizer within a CAD system, although historically, binary decision diagrams, an automated Quine–McCluskey algorithm, truth tables, Karnaugh maps, and Boolean algebra have been used.\n\nRepresentations are crucial to an engineer's design of digital circuits. Some analysis methods only work with particular representations.\n\nThe classical way to represent a digital circuit is with an equivalent set of logic gates. Another way, often with the least electronics, is to construct an equivalent system of electronic switches (usually transistors). One of the easiest ways is to simply have a memory containing a truth table. The inputs are fed into the address of the memory, and the data outputs of the memory become the outputs.\n\nFor automated analysis, these representations have digital file formats that can be processed by computer programs. Most digital engineers are very careful to select computer programs (\"tools\") with compatible file formats.\n\nTo choose representations, engineers consider types of digital systems. Most digital systems divide into \"combinational systems\" and \"sequential systems.\" A combinational system always presents the same output when given the same inputs. It is basically a representation of a set of logic functions, as already discussed.\n\nA sequential system is a combinational system with some of the outputs fed back as inputs. This makes the digital machine perform a \"sequence\" of operations. The simplest sequential system is probably a flip flop, a mechanism that represents a binary digit or \"bit\".\n\nSequential systems are often designed as state machines. In this way, engineers can design a system's gross behavior, and even test it in a simulation, without considering all the details of the logic functions.\n\nSequential systems divide into two further subcategories. \"Synchronous\" sequential systems change state all at once, when a \"clock\" signal changes state. \"Asynchronous\" sequential systems propagate changes whenever inputs change. Synchronous sequential systems are made of well-characterized asynchronous circuits such as flip-flops, that change only when the clock changes, and which have carefully designed timing margins.\n\nThe usual way to implement a synchronous sequential state machine is to divide it into a piece of combinational logic and a set of flip flops called a \"state register.\" Each time a clock signal ticks, the state register captures the feedback generated from the previous state of the combinational logic, and feeds it back as an unchanging input to the combinational part of the state machine. The fastest rate of the clock is set by the most time-consuming logic calculation in the combinational logic.\n\nThe state register is just a representation of a binary number. If the states in the state machine are numbered (easy to arrange), the logic function is some combinational logic that produces the number of the next state.\n\nAs of 2014, most digital logic is synchronous because it is easier to create and verify a synchronous design. However, asynchronous logic is thought can be superior because its speed is not constrained by an arbitrary clock; instead, it runs at the maximum speed of its logic gates. Building an asynchronous system using faster parts makes the circuit faster.\n\nNevertherless, most systems need circuits that allow external unsynchronized signals to enter synchronous logic circuits. These are inherently asynchronous in their design and must be analyzed as such. Examples of widely used asynchronous circuits include synchronizer flip-flops, switch debouncers and arbiters.\n\nAsynchronous logic components can be hard to design because all possible states, in all possible timings must be considered. The usual method is to construct a table of the minimum and maximum time that each such state can exist, and then adjust the circuit to minimize the number of such states. Then the designer must force the circuit to periodically wait for all of its parts to enter a compatible state (this is called \"self-resynchronization\"). Without such careful design, it is easy to accidentally produce asynchronous logic that is \"unstable,\" that is, real electronics will have unpredictable results because of the cumulative delays caused by small variations in the values of the electronic components.\n\nMany digital systems are data flow machines. These are usually designed using synchronous register transfer logic, using hardware description languages such as VHDL or Verilog.\n\nIn register transfer logic, binary numbers are stored in groups of flip flops called registers. The outputs of each register are a bundle of wires called a \"bus\" that carries that number to other calculations. A calculation is simply a piece of combinational logic. Each calculation also has an output bus, and these may be connected to the inputs of several registers. Sometimes a register will have a multiplexer on its input, so that it can store a number from any one of several buses. Alternatively, the outputs of several items may be connected to a bus through buffers that can turn off the output of all of the devices except one. A sequential state machine controls when each register accepts new data from its input.\n\nAsynchronous register-transfer systems (such as computers) have a general solution. In the 1980s, some researchers discovered that almost all synchronous register-transfer machines could be converted to asynchronous designs by using first-in-first-out synchronization logic. In this scheme, the digital machine is characterized as a set of data flows. In each step of the flow, an asynchronous \"synchronization circuit\" determines when the outputs of that step are valid, and presents a signal that says, \"grab the data\" to the stages that use that stage's inputs. It turns out that just a few relatively simple synchronization circuits are needed.\n\nThe most general-purpose register-transfer logic machine is a computer. This is basically an automatic binary abacus. The control unit of a computer is usually designed as a microprogram run by a microsequencer. A microprogram is much like a player-piano roll. Each table entry or \"word\" of the microprogram commands the state of every bit that controls the computer. The sequencer then counts, and the count addresses the memory or combinational logic machine that contains the microprogram. The bits from the microprogram control the arithmetic logic unit, memory and other parts of the computer, including the microsequencer itself. A \"specialized computer\" is usually a conventional computer with special-purpose control logic or microprogram.\n\nIn this way, the complex task of designing the controls of a computer is reduced to a simpler task of programming a collection of much simpler logic machines.\n\nAlmost all computers are synchronous. However, true asynchronous computers have also been designed. One example is the Aspida DLX core. Another was offered by ARM Holdings. Speed advantages have not materialized, because modern computer designs already run at the speed of their slowest component, usually memory. These do use somewhat less power because a clock distribution network is not needed. An unexpected advantage is that asynchronous computers do not produce spectrally-pure radio noise, so they are used in some mobile-phone base-station controllers. They may be more secure in cryptographic applications because their electrical and radio emissions can be more difficult to decode.\n\nComputer architecture is a specialized engineering activity that tries to arrange the registers, calculation logic, buses and other parts of the computer in the best way for some purpose. Computer architects have applied large amounts of ingenuity to computer design to reduce the cost and increase the speed and immunity to programming errors of computers. An increasingly common goal is to reduce the power used in a battery-powered computer system, such as a cell-phone. Many computer architects serve an extended apprenticeship as microprogrammers.\n\nDigital circuits are made from analog components. The design must assure that the analog nature of the components doesn't dominate the desired digital behavior. Digital systems must manage noise and timing margins, parasitic inductances and capacitances, and filter power connections.\n\nBad designs have intermittent problems such as \"glitches\", vanishingly fast pulses that may trigger some logic but not others, \"runt pulses\" that do not reach valid \"threshold\" voltages, or unexpected (\"undecoded\") combinations of logic states.\n\nAdditionally, where clocked digital systems interface to analog systems or systems that are driven from a different clock, the digital system can be subject to metastability where a change to the input violates the set-up time for a digital input latch. This situation will self-resolve, but will take a random time, and while it persists can result in invalid signals being propagated within the digital system for a short time.\n\nSince digital circuits are made from analog components, digital circuits calculate more slowly than low-precision analog circuits that use a similar amount of space and power. However, the digital circuit will calculate more repeatably, because of its high noise immunity. On the other hand, in the high-precision domain (for example, where 14 or more bits of precision are needed), analog circuits require much more power and area than digital equivalents.\n\nTo save costly engineering effort, much of the effort of designing large logic machines has been automated. The computer programs are called \"electronic design automation tools\" or just \"EDA.\"\n\nSimple truth table-style descriptions of logic are often optimized with EDA that automatically produces reduced systems of logic gates or smaller lookup tables that still produce the desired outputs. The most common example of this kind of software is the Espresso heuristic logic minimizer.\n\nMost practical algorithms for optimizing large logic systems use algebraic manipulations or binary decision diagrams, and there are promising experiments with genetic algorithms and annealing optimizations.\n\nTo automate costly engineering processes, some EDA can take state tables that describe state machines and automatically produce a truth table or a function table for the combinational logic of a state machine. The state table is a piece of text that lists each state, together with the conditions controlling the transitions between them and the belonging output signals.\n\nIt is common for the function tables of such computer-generated state-machines to be optimized with logic-minimization software such as Minilog.\n\nOften, real logic systems are designed as a series of sub-projects, which are combined using a \"tool flow.\" The tool flow is usually a \"script,\" a simplified computer language that can invoke the software design tools in the right order.\n\nTool flows for large logic systems such as microprocessors can be thousands of commands long, and combine the work of hundreds of engineers.\n\nWriting and debugging tool flows is an established engineering specialty in companies that produce digital designs. The tool flow usually terminates in a detailed computer file or set of files that describe how to physically construct the logic. Often it consists of instructions to draw the transistors and wires on an integrated circuit or a printed circuit board.\n\nParts of tool flows are \"debugged\" by verifying the outputs of simulated logic against expected inputs. The test tools take computer files with sets of inputs and outputs, and highlight discrepancies between the simulated behavior and the expected behavior.\n\nOnce the input data is believed correct, the design itself must still be verified for correctness. Some tool flows verify designs by first producing a design, and then scanning the design to produce compatible input data for the tool flow. If the scanned data matches the input data, then the tool flow has probably not introduced errors.\n\nThe functional verification data are usually called \"test vectors\". The functional test vectors may be preserved and used in the factory to test that newly constructed logic works correctly. However, functional test patterns don't discover common fabrication faults. Production tests are often designed by software tools called \"test pattern generators\". These generate test vectors by examining the structure of the logic and systematically generating tests for particular faults. This way the fault coverage can closely approach 100%, provided the design is properly made testable (see next section).\n\nOnce a design exists, and is verified and testable, it often needs to be processed to be manufacturable as well. Modern integrated circuits have features smaller than the wavelength of the light used to expose the photoresist. Manufacturability software adds interference patterns to the exposure masks to eliminate open-circuits, and enhance the masks' contrast.\n\nThere are several reasons for testing a logic circuit. When the circuit is first developed, it is necessary to verify that the design circuit meets the required functional and timing specifications. When multiple copies of a correctly designed circuit are being manufactured, it is essential to test each copy to ensure that the manufacturing process has not introduced any flaws.\n\nA large logic machine (say, with more than a hundred logical variables) can have an astronomical number of possible states. Obviously, in the factory, testing every state is impractical if testing each state takes a microsecond, and there are more states than the number of microseconds since the universe began. This ridiculous-sounding case is typical.\n\nLarge logic machines are almost always designed as assemblies of smaller logic machines. To save time, the smaller sub-machines are isolated by permanently installed \"design for test\" circuitry, and are tested independently.\n\nOne common test scheme known as \"scan design\" moves test bits serially (one after another) from external test equipment through one or more serial shift registers known as \"scan chains\". Serial scans have only one or two wires to carry the data, and minimize the physical size and expense of the infrequently used test logic.\n\nAfter all the test data bits are in place, the design is reconfigured to be in \"normal mode\" and one or more clock pulses are applied, to test for faults (e.g. stuck-at low or stuck-at high) and capture the test result into flip-flops and/or latches in the scan shift register(s). Finally, the result of the test is shifted out to the block boundary and compared against the predicted \"good machine\" result.\n\nIn a board-test environment, serial to parallel testing has been formalized with a standard called \"JTAG\" (named after the \"Joint Test Action Group\" that made it).\n\nAnother common testing scheme provides a test mode that forces some part of the logic machine to enter a \"test cycle.\" The test cycle usually exercises large independent parts of the machine.\n\nSeveral numbers determine the practicality of a system of digital logic: cost, reliability, fanout and speed. Engineers explored numerous electronic devices to get a favourable combination of these personalities.\n\nThe cost of a logic gate is crucial, primarily because very many gates are needed to build a computer or other advanced digital system and because the more gates can be used, the more able and/or respondent the machine can become. Since the bulk of a digital computer is simply an interconnected network of logic gates, the overall cost of building a computer correlates strongly with the price per logic gate. In the 1930s, the earliest digital logic systems were constructed from telephone relays because these were inexpensive and relatively reliable. After that, electrical engineers always used the cheapest available electronic switches that could still fulfill the requirements.\n\nThe earliest integrated circuits were a happy accident. They were constructed not to save money, but to save weight, and permit the Apollo Guidance Computer to control an inertial guidance system for a spacecraft. The first integrated circuit logic gates cost nearly $50 (in 1960 dollars, when an engineer earned $10,000/year). To everyone's surprise, by the time the circuits were mass-produced, they had become the least-expensive method of constructing digital logic. Improvements in this technology have driven all subsequent improvements in cost.\n\nWith the rise of integrated circuits, reducing the absolute number of chips used represented another way to save costs. The goal of a designer is not just to make the simplest circuit, but to keep the component count down. Sometimes this results in more complicated designs with respect to the underlying digital logic but nevertheless reduces the number of components, board size, and even power consumption. A major motive for reducing component count on printed circuit boards is to reduce the manufacturing defect rate and increase reliability, as every soldered connection is a potentially bad one, so the defect and failure rates tend to increase along with the total number of component pins.\n\nFor example, in some logic families, NAND gates are the simplest digital gate to build. All other logical operations can be implemented by NAND gates. If a circuit already required a single NAND gate, and a single chip normally carried four NAND gates, then the remaining gates could be used to implement other logical operations like logical and. This could eliminate the need for a separate chip containing those different types of gates.\n\nThe \"reliability\" of a logic gate describes its mean time between failure (MTBF). Digital machines often have millions of logic gates. Also, most digital machines are \"optimized\" to reduce their cost. The result is that often, the failure of a single logic gate will cause a digital machine to stop working. It is possible to design machines to be more reliable by using redundant logic which will not malfunction as a result of the failure of any single gate (or even any two, three, or four gates), but this necessarily entails using more components, which raises the financial cost and also usually increases the weight of the machine and may increase the power it consumes.\n\nDigital machines first became useful when the MTBF for a switch got above a few hundred hours. Even so, many of these machines had complex, well-rehearsed repair procedures, and would be nonfunctional for hours because a tube burned-out, or a moth got stuck in a relay. Modern transistorized integrated circuit logic gates have MTBFs greater than 82 billion hours (8.2 · 10 hours), and need them because they have so many logic gates.\n\nFanout describes how many logic inputs can be controlled by a single logic output without exceeding the electrical current ratings of the gate outputs. The minimum practical fanout is about five. Modern electronic logic gates using CMOS transistors for switches have fanouts near fifty, and can sometimes go much higher.\n\nThe \"switching speed\" describes how many times per second an inverter (an electronic representation of a \"logical not\" function) can change from true to false and back. Faster logic can accomplish more operations in less time. Digital logic first became useful when switching speeds got above 50 Hz, because that was faster than a team of humans operating mechanical calculators. Modern electronic digital logic routinely switches at 5 GHz (5 · 10 Hz), and some laboratory systems switch at more than 1 THz (1 · 10 Hz).\n\nDesign started with relays. Relay logic was relatively inexpensive and reliable, but slow. Occasionally a mechanical failure would occur. Fanouts were typically about 10, limited by the resistance of the coils and arcing on the contacts from high voltages.\n\nLater, vacuum tubes were used. These were very fast, but generated heat, and were unreliable because the filaments would burn out. Fanouts were typically 5...7, limited by the heating from the tubes' current. In the 1950s, special \"computer tubes\" were developed with filaments that omitted volatile elements like silicon. These ran for hundreds of thousands of hours.\n\nThe first semiconductor logic family was resistor–transistor logic. This was a thousand times more reliable than tubes, ran cooler, and used less power, but had a very low fan-in of 3. Diode–transistor logic improved the fanout up to about 7, and reduced the power. Some DTL designs used two power-supplies with alternating layers of NPN and PNP transistors to increase the fanout.\n\nTransistor–transistor logic (TTL) was a great improvement over these. In early devices, fanout improved to 10, and later variations reliably achieved 20. TTL was also fast, with some variations achieving switching times as low as 20 ns. TTL is still used in some designs.\n\nEmitter coupled logic is very fast but uses a lot of power. It was extensively used for high-performance computers made up of many medium-scale components (such as the Illiac IV).\n\nBy far, the most common digital integrated circuits built today use CMOS logic, which is fast, offers high circuit density and low-power per gate. This is used even in large, fast computers, such as the IBM System z.\n\nIn 2009, researchers discovered that memristors can implement a boolean state storage (similar to a flip flop, implication and logical inversion), providing a complete logic family with very small amounts of space and power, using familiar CMOS semiconductor processes.\n\nThe discovery of superconductivity has enabled the development of rapid single flux quantum (RSFQ) circuit technology, which uses Josephson junctions instead of transistors. Most recently, attempts are being made to construct purely optical computing systems capable of processing digital information using nonlinear optical elements.\n\n\n"}
{"id": "7275487", "url": "https://en.wikipedia.org/wiki?curid=7275487", "title": "Discrete manufacturing", "text": "Discrete manufacturing\n\nDiscrete manufacturing is the production of distinct items. Automobiles, furniture, toys, smartphones, and airplanes are the examples of discrete manufacturing products. The resulting products are easily identifiable and differ greatly from process manufacturing where the products are undifferentiated, for example oil, natural gas and salt.\n\nDiscrete manufacturing is often characterized by individual or separate unit production. Units can be produced in low volume with very high complexity or high volumes of low complexity. Low volume/high complexity production results in the need for an flexible manufacturing system that can improve quality and time-to-market speed while cutting costs. High volume/low complexity production puts high premiums on inventory controls, lead times and reducing or limiting materials costs and waste.\nIndustry Profile - Discrete Manufacturing includes makers of consumer electronics, computer and accessories, appliances, and other household items, as well as \"big ticket” consumer and commercial goods like cars and airplanes. Discrete Manufacturing companies make physical products that go directly to businesses and consumers, and assemblies that are used by other manufacturers. \nThe processes deployed in discrete manufacturing are not continuous in nature. Each process can be individually started or stopped and can be run at varying production rates. The final product may be produced out of single or multiple inputs. Producing a steel structure will need only one type of raw material - steel. Producing a mobile phone requires many different inputs, The plastic case, LCD display, the mainboard, PVC keypad, sockets, cables are made from different materials, at different places. This is different from Process manufacturing like production of paper or petroleum refining, where the end product is obtained by a continuous process or a set of continuous processes.\n\nProduction capacity of the factory as a whole in discrete manufacturing is impossible to calculate. It is the question of common sense that how can one calculate the production capacity of its multiple characterize different products because the production time and machine setups of the parts produced are different from each other.\n\n"}
{"id": "16870100", "url": "https://en.wikipedia.org/wiki?curid=16870100", "title": "Eight-Foot High Speed Tunnel (Hampton, Virginia)", "text": "Eight-Foot High Speed Tunnel (Hampton, Virginia)\n\nThe Eight-Foot High Speed Tunnel, also known as Eight-Foot Transonic Tunnel, was a wind tunnel located in Building 641 of NASA's Langley Research Center in Hampton, Virginia. It was a National Historic Landmark.\n\nThe tunnel was completed in 1936 at a cost of $36,266,000. Because of its high speed and Bernoulli's principle, the pressure in the test section is much lower than that in the rest of the tunnel. This required a structure that could withstand an inward force due to the pressure difference. Instead of steel construction, it was built from reinforced concrete with walls up to 1 ft (0.3 m) thick. This resulted in an \"igloo-like\" structure at the test section. The wind tunnel was designed as a single-return tunnel capable of moving air at speeds up to a Mach number up to 0.75. It was powered by an electric motor. It was repowered to to give Mach number 1 capability in 1945. In 1947, the speed was increased to a Mach number of 1.2 with the installation of a contoured nozzle. In 1950, a slotted-throat test section was installed, and it was repowered to . \n\nBecause it was the first continuous-flow high-speed tunnel, this tunnel was a landmark in wind tunnel design. This meant it could operate almost indefinitely to produce a high-speed airstream approaching the speed of sound. And it was large enough to accommodate large-scale models and even full-scale aircraft sections. \nIn 1950, the tunnel was the first in the world to be modified to incorporate a slotted throat design. This revolutionary design gave researchers their first accurate data on airframe performance in the transonic range. The tunnel was deactivated in 1956, when a new tunnel was built near it.\n\nThe wind tunnel was used for critical tests that validated the area rule for the design of supersonic aircraft. This said that the fuselage of the aircraft should narrow at the wings and expand at their trailing edges. This resulted in \"wasp-waisted\" aircraft. \n\nThe tunnel was taken out of service in 1956.\n\nIt was declared a National Historic Landmark in 1985. In 2011, Building 641, which housed the tunnel, was demolished. The landmark designation was withdrawn in 2014, and it was removed from the National Register of Historic Places.\n\nThere are additional photographs of the wind tunnel in the Historic American Engineering Record collection.\n\n\n"}
{"id": "9476", "url": "https://en.wikipedia.org/wiki?curid=9476", "title": "Electron", "text": "Electron\n\nThe electron is a subatomic particle, symbol or , whose electric charge is negative one elementary charge. Electrons belong to the first generation of the lepton particle family, and are generally thought to be elementary particles because they have no known components or substructure. The electron has a mass that is approximately 1/1836 that of the proton. Quantum mechanical properties of the electron include an intrinsic angular momentum (spin) of a half-integer value, expressed in units of the reduced Planck constant, \"ħ\". As it is a fermion, no two electrons can occupy the same quantum state, in accordance with the Pauli exclusion principle. Like all elementary particles, electrons exhibit properties of both particles and waves: they can collide with other particles and can be diffracted like light. The wave properties of electrons are easier to observe with experiments than those of other particles like neutrons and protons because electrons have a lower mass and hence a longer de Broglie wavelength for a given energy.\n\nElectrons play an essential role in numerous physical phenomena, such as electricity, magnetism, chemistry and thermal conductivity, and they also participate in gravitational, electromagnetic and weak interactions. Since an electron has charge, it has a surrounding electric field, and if that electron is moving relative to an observer, it will generate a magnetic field. Electromagnetic fields produced from other sources will affect the motion of an electron according to the Lorentz force law. Electrons radiate or absorb energy in the form of photons when they are accelerated. Laboratory instruments are capable of trapping individual electrons as well as electron plasma by the use of electromagnetic fields. Special telescopes can detect electron plasma in outer space. Electrons are involved in many applications such as electronics, welding, cathode ray tubes, electron microscopes, radiation therapy, lasers, gaseous ionization detectors and particle accelerators.\n\nInteractions involving electrons with other subatomic particles are of interest in fields such as chemistry and nuclear physics. The Coulomb force interaction between the positive protons within atomic nuclei and the negative electrons without, allows the composition of the two known as atoms. Ionization or differences in the proportions of negative electrons versus positive nuclei changes the binding energy of an atomic system. The exchange or sharing of the electrons between two or more atoms is the main cause of chemical bonding. In 1838, British natural philosopher Richard Laming first hypothesized the concept of an indivisible quantity of electric charge to explain the chemical properties of atoms. Irish physicist George Johnstone Stoney named this charge 'electron' in 1891, and J. J. Thomson and his team of British physicists identified it as a particle in 1897. Electrons can also participate in nuclear reactions, such as nucleosynthesis in stars, where they are known as beta particles. Electrons can be created through beta decay of radioactive isotopes and in high-energy collisions, for instance when cosmic rays enter the atmosphere. The antiparticle of the electron is called the positron; it is identical to the electron except that it carries electrical and other charges of the opposite sign. When an electron collides with a positron, both particles can be annihilated, producing gamma ray photons.\n\nThe ancient Greeks noticed that amber attracted small objects when rubbed with fur. Along with lightning, this phenomenon is one of humanity's earliest recorded experiences with electricity. In his 1600 treatise , the English scientist William Gilbert coined the New Latin term , to refer to this property of attracting small objects after being rubbed. Both \"electric\" and \"electricity\" are derived from the Latin ' (also the root of the alloy of the same name), which came from the Greek word for amber, (').\n\nIn the early 1700s, Francis Hauksbee and French chemist Charles François du Fay independently discovered what they believed were two kinds of frictional electricity—one generated from rubbing glass, the other from rubbing resin. From this, du Fay theorized that electricity consists of two electrical fluids, \"vitreous\" and \"resinous\", that are separated by friction, and that neutralize each other when combined. American scientist Ebenezer Kinnersley later also independently reached the same conclusion. A decade later Benjamin Franklin proposed that electricity was not from different types of electrical fluid, but a single electrical fluid showing an excess (+) or deficit (-). He gave them the modern charge nomenclature of positive and negative respectively. Franklin thought of the charge carrier as being positive, but he did not correctly identify which situation was a surplus of the charge carrier, and which situation was a deficit.\n\nBetween 1838 and 1851, British natural philosopher Richard Laming developed the idea that an atom is composed of a core of matter surrounded by subatomic particles that had unit electric charges. Beginning in 1846, German physicist William Weber theorized that electricity was composed of positively and negatively charged fluids, and their interaction was governed by the inverse square law. After studying the phenomenon of electrolysis in 1874, Irish physicist George Johnstone Stoney suggested that there existed a \"single definite quantity of electricity\", the charge of a monovalent ion. He was able to estimate the value of this elementary charge \"e\" by means of Faraday's laws of electrolysis. However, Stoney believed these charges were permanently attached to atoms and could not be removed. In 1881, German physicist Hermann von Helmholtz argued that both positive and negative charges were divided into elementary parts, each of which \"behaves like atoms of electricity\".\n\nStoney initially coined the term \"electrolion\" in 1881. Ten years later, he switched to \"electron\" to describe these elementary charges, writing in 1894: \"... an estimate was made of the actual amount of this most remarkable fundamental unit of electricity, for which I have since ventured to suggest the name \"electron\"\". A 1906 proposal to change to \"electrion\" failed because Hendrik Lorentz preferred to keep \"electron\". The word \"electron\" is a combination of the words \"electric\" and \"ion\". The suffix -\"on\" which is now used to designate other subatomic particles, such as a proton or neutron, is in turn derived from electron.\n\nThe German physicist Johann Wilhelm Hittorf studied electrical conductivity in rarefied gases: in 1869, he discovered a glow emitted from the cathode that increased in size with decrease in gas pressure. In 1876, the German physicist Eugen Goldstein showed that the rays from this glow cast a shadow, and he dubbed the rays cathode rays. During the 1870s, the English chemist and physicist Sir William Crookes developed the first cathode ray tube to have a high vacuum inside. He then showed that the luminescence rays appearing within the tube carried energy and moved from the cathode to the anode. Furthermore, by applying a magnetic field, he was able to deflect the rays, thereby demonstrating that the beam behaved as though it were negatively charged. In 1879, he proposed that these properties could be explained by what he termed 'radiant matter'. He suggested that this was a fourth state of matter, consisting of negatively charged molecules that were being projected with high velocity from the cathode.\n\nThe German-born British physicist Arthur Schuster expanded upon Crookes' experiments by placing metal plates parallel to the cathode rays and applying an electric potential between the plates. The field deflected the rays toward the positively charged plate, providing further evidence that the rays carried negative charge. By measuring the amount of deflection for a given level of current, in 1890 Schuster was able to estimate the charge-to-mass ratio of the ray components. However, this produced a value that was more than a thousand times greater than what was expected, so little credence was given to his calculations at the time.\n\nIn 1892 Hendrik Lorentz suggested that the mass of these particles (electrons) could be a consequence of their electric charge.\nWhile studying naturally fluorescing minerals in 1896, the French physicist Henri Becquerel discovered that they emitted radiation without any exposure to an external energy source. These radioactive materials became the subject of much interest by scientists, including the New Zealand physicist Ernest Rutherford who discovered they emitted particles. He designated these particles alpha and beta, on the basis of their ability to penetrate matter. In 1900, Becquerel showed that the beta rays emitted by radium could be deflected by an electric field, and that their mass-to-charge ratio was the same as for cathode rays. This evidence strengthened the view that electrons existed as components of atoms.\n\nIn 1897, the British physicist J. J. Thomson, with his colleagues John S. Townsend and H. A. Wilson, performed experiments indicating that cathode rays really were unique particles, rather than waves, atoms or molecules as was believed earlier. Thomson made good estimates of both the charge \"e\" and the mass \"m\", finding that cathode ray particles, which he called \"corpuscles,\" had perhaps one thousandth of the mass of the least massive ion known: hydrogen. He showed that their charge-to-mass ratio, \"e\"/\"m\", was independent of cathode material. He further showed that the negatively charged particles produced by radioactive materials, by heated materials and by illuminated materials were universal. The name electron was again proposed for these particles by the Irish physicist George Johnstone Stoney, and the name has since gained universal acceptance.\n\nThe electron's charge was more carefully measured by the American physicists Robert Millikan and Harvey Fletcher in their oil-drop experiment of 1909, the results of which were published in 1911. This experiment used an electric field to prevent a charged droplet of oil from falling as a result of gravity. This device could measure the electric charge from as few as 1–150 ions with an error margin of less than 0.3%. Comparable experiments had been done earlier by Thomson's team, using clouds of charged water droplets generated by electrolysis, and in 1911 by Abram Ioffe, who independently obtained the same result as Millikan using charged microparticles of metals, then published his results in 1913. However, oil drops were more stable than water drops because of their slower evaporation rate, and thus more suited to precise experimentation over longer periods of time.\n\nAround the beginning of the twentieth century, it was found that under certain conditions a fast-moving charged particle caused a condensation of supersaturated water vapor along its path. In 1911, Charles Wilson used this principle to devise his cloud chamber so he could photograph the tracks of charged particles, such as fast-moving electrons.\n\nBy 1914, experiments by physicists Ernest Rutherford, Henry Moseley, James Franck and Gustav Hertz had largely established the structure of an atom as a dense nucleus of positive charge surrounded by lower-mass electrons. In 1913, Danish physicist Niels Bohr postulated that electrons resided in quantized energy states, with their energies determined by the angular momentum of the electron's orbit about the nucleus. The electrons could move between those states, or orbits, by the emission or absorption of photons of specific frequencies. By means of these quantized orbits, he accurately explained the spectral lines of the hydrogen atom. However, Bohr's model failed to account for the relative intensities of the spectral lines and it was unsuccessful in explaining the spectra of more complex atoms.\n\nChemical bonds between atoms were explained by Gilbert Newton Lewis, who in 1916 proposed that a covalent bond between two atoms is maintained by a pair of electrons shared between them. Later, in 1927, Walter Heitler and Fritz London gave the full explanation of the electron-pair formation and chemical bonding in terms of quantum mechanics. In 1919, the American chemist Irving Langmuir elaborated on the Lewis' static model of the atom and suggested that all electrons were distributed in successive \"concentric (nearly) spherical shells, all of equal thickness\". In turn, he divided the shells into a number of cells each of which contained one pair of electrons. With this model Langmuir was able to qualitatively explain the chemical properties of all elements in the periodic table, which were known to largely repeat themselves according to the periodic law.\n\nIn 1924, Austrian physicist Wolfgang Pauli observed that the shell-like structure of the atom could be explained by a set of four parameters that defined every quantum energy state, as long as each state was occupied by no more than a single electron. This prohibition against more than one electron occupying the same quantum energy state became known as the Pauli exclusion principle. The physical mechanism to explain the fourth parameter, which had two distinct possible values, was provided by the Dutch physicists Samuel Goudsmit and George Uhlenbeck. In 1925, they suggested that an electron, in addition to the angular momentum of its orbit, possesses an intrinsic angular momentum and magnetic dipole moment. This is analogous to the rotation of the Earth on its axis as it orbits the Sun. The intrinsic angular momentum became known as spin, and explained the previously mysterious splitting of spectral lines observed with a high-resolution spectrograph; this phenomenon is known as fine structure splitting.\n\nIn his 1924 dissertation \"\" (Research on Quantum Theory), French physicist Louis de Broglie hypothesized that all matter can be represented as a de Broglie wave in the manner of light. That is, under the appropriate conditions, electrons and other matter would show properties of either particles or waves. The corpuscular properties of a particle are demonstrated when it is shown to have a localized position in space along its trajectory at any given moment. The wave-like nature of light is displayed, for example, when a beam of light is passed through parallel slits thereby creating interference patterns. In 1927 George Paget Thomson, discovered the interference effect was produced when a beam of electrons was passed through thin metal foils and by American physicists Clinton Davisson and Lester Germer by the reflection of electrons from a crystal of nickel.\nDe Broglie's prediction of a wave nature for electrons led Erwin Schrödinger to postulate a wave equation for electrons moving under the influence of the nucleus in the atom. In 1926, this equation, the Schrödinger equation, successfully described how electron waves propagated. Rather than yielding a solution that determined the location of an electron over time, this wave equation also could be used to predict the probability of finding an electron near a position, especially a position near where the electron was bound in space, for which the electron wave equations did not change in time. This approach led to a second formulation of quantum mechanics (the first by Heisenberg in 1925), and solutions of Schrödinger's equation, like Heisenberg's, provided derivations of the energy states of an electron in a hydrogen atom that were equivalent to those that had been derived first by Bohr in 1913, and that were known to reproduce the hydrogen spectrum. Once spin and the interaction between multiple electrons were describable, quantum mechanics made it possible to predict the configuration of electrons in atoms with atomic numbers greater than hydrogen.\n\nIn 1928, building on Wolfgang Pauli's work, Paul Dirac produced a model of the electron – the Dirac equation, consistent with relativity theory, by applying relativistic and symmetry considerations to the hamiltonian formulation of the quantum mechanics of the electro-magnetic field. In order to resolve some problems within his relativistic equation, Dirac developed in 1930 a model of the vacuum as an infinite sea of particles with negative energy, later dubbed the Dirac sea. This led him to predict the existence of a positron, the antimatter counterpart of the electron. This particle was discovered in 1932 by Carl Anderson, who proposed calling standard electrons \"negatons\" and using \"electron\" as a generic term to describe both the positively and negatively charged variants.\n\nIn 1947 Willis Lamb, working in collaboration with graduate student Robert Retherford, found that certain quantum states of the hydrogen atom, which should have the same energy, were shifted in relation to each other; the difference came to be called the Lamb shift. About the same time, Polykarp Kusch, working with Henry M. Foley, discovered the magnetic moment of the electron is slightly larger than predicted by Dirac's theory. This small difference was later called anomalous magnetic dipole moment of the electron. This difference was later explained by the theory of quantum electrodynamics, developed by Sin-Itiro Tomonaga, Julian Schwinger and\nRichard Feynman in the late 1940s.\n\nWith the development of the particle accelerator during the first half of the twentieth century, physicists began to delve deeper into the properties of subatomic particles. The first successful attempt to accelerate electrons using electromagnetic induction was made in 1942 by Donald Kerst. His initial betatron reached energies of 2.3 MeV, while subsequent betatrons achieved 300 MeV. In 1947, synchrotron radiation was discovered with a 70 MeV electron synchrotron at General Electric. This radiation was caused by the acceleration of electrons through a magnetic field as they moved near the speed of light.\n\nWith a beam energy of 1.5 GeV, the first high-energy\nparticle collider was ADONE, which began operations in 1968. This device accelerated electrons and positrons in opposite directions, effectively doubling the energy of their collision when compared to striking a static target with an electron. The Large Electron–Positron Collider (LEP) at CERN, which was operational from 1989 to 2000, achieved collision energies of 209 GeV and made important measurements for the Standard Model of particle physics.\n\nIndividual electrons can now be easily confined in ultra small (, ) CMOS transistors operated at cryogenic temperature over a range of −269 °C (4 K) to about −258 °C (15 K). The electron wavefunction spreads in a semiconductor lattice and negligibly interacts with the valence band electrons, so it can be treated in the single particle formalism, by replacing its mass with the effective mass tensor.\n\nIn the Standard Model of particle physics, electrons belong to the group of subatomic particles called leptons, which are believed to be fundamental or elementary particles. Electrons have the lowest mass of any charged lepton (or electrically charged particle of any type) and belong to the first-generation of fundamental particles. The second and third generation contain charged leptons, the muon and the tau, which are identical to the electron in charge, spin and interactions, but are more massive. Leptons differ from the other basic constituent of matter, the quarks, by their lack of strong interaction. All members of the lepton group are fermions, because they all have half-odd integer spin; the electron has spin .\n\nThe invariant mass of an electron is approximately  kilograms, or  atomic mass units. On the basis of Einstein's principle of mass–energy equivalence, this mass corresponds to a rest energy of 0.511 MeV. The ratio between the mass of a proton and that of an electron is about 1836. Astronomical measurements show that the proton-to-electron mass ratio has held the same value, as is predicted by the Standard Model, for at least half the age of the universe.\n\nElectrons have an electric charge of coulombs, which is used as a standard unit of charge for subatomic particles, and is also called the elementary charge. This elementary charge has a relative standard uncertainty of . Within the limits of experimental accuracy, the electron charge is identical to the charge of a proton, but with the opposite sign. As the symbol \"e\" is used for the elementary charge, the electron is commonly symbolized by , where the minus sign indicates the negative charge. The positron is symbolized by because it has the same properties as the electron but with a positive rather than negative charge.\n\nThe electron has an intrinsic angular momentum or spin of . This property is usually stated by referring to the electron as a spin- particle. For such particles the spin magnitude is  \"ħ\". while the result of the measurement of a projection of the spin on any axis can only be ±. In addition to spin, the electron has an intrinsic magnetic moment along its spin axis. It is approximately equal to one Bohr magneton,=\\frac{e\\hbar}{2m_{\\mathrm{e}}}.</math>|group=note}} which is a physical constant equal to . The orientation of the spin with respect to the momentum of the electron defines the property of elementary particles known as helicity.\n\nThe electron has no known substructure and it is assumed to be a point particle with a point charge and no spatial extent.\n\nThe issue of the radius of the electron is a challenging problem of the modern theoretical physics. The admission of the hypothesis of a finite radius of the electron is incompatible to the premises of the theory of relativity. On the other hand, a point-like electron (zero radius) generates serious mathematical difficulties due to the self-energy of the electron tending to infinity. Observation of a single electron in a Penning trap suggests the upper limit of the particle's radius to be 10 meters. \nThe upper bound of the electron radius of 10 meters can be derived using the uncertainty relation in energy. There \"is\" also a physical constant called the \"classical electron radius\", with the much larger value of , greater than the radius of the proton. However, the terminology comes from a simplistic calculation that ignores the effects of quantum mechanics; in reality, the so-called classical electron radius has little to do with the true fundamental structure of the electron.\n\nThere are elementary particles that spontaneously decay into less massive particles. An example is the muon, with a mean lifetime of  seconds, which decays into an electron, a muon neutrino and an electron antineutrino. The electron, on the other hand, is thought to be stable on theoretical grounds: the electron is the least massive particle with non-zero electric charge, so its decay would violate charge conservation. The experimental lower bound for the electron's mean lifetime is years, at a 90% confidence level.\n\nAs with all particles, electrons can act as waves. This is called the wave–particle duality and can be demonstrated using the double-slit experiment.\n\nThe wave-like nature of the electron allows it to pass through two parallel slits simultaneously, rather than just one slit as would be the case for a classical particle. In quantum mechanics, the wave-like property of one particle can be described mathematically as a complex-valued function, the wave function, commonly denoted by the Greek letter psi (\"ψ\"). When the absolute value of this function is squared, it gives the probability that a particle will be observed near a location—a probability density.\nElectrons are identical particles because they cannot be distinguished from each other by their intrinsic physical properties. In quantum mechanics, this means that a pair of interacting electrons must be able to swap positions without an observable change to the state of the system. The wave function of fermions, including electrons, is antisymmetric, meaning that it changes sign when two electrons are swapped; that is, , where the variables \"r\" and \"r\" correspond to the first and second electrons, respectively. Since the absolute value is not changed by a sign swap, this corresponds to equal probabilities. Bosons, such as the photon, have symmetric wave functions instead.\n\nIn the case of antisymmetry, solutions of the wave equation for interacting electrons result in a zero probability that each pair will occupy the same location or state. This is responsible for the Pauli exclusion principle, which precludes any two electrons from occupying the same quantum state. This principle explains many of the properties of electrons. For example, it causes groups of bound electrons to occupy different orbitals in an atom, rather than all overlapping each other in the same orbit.\n\nIn a simplified picture, every photon spends some time as a combination of a virtual electron plus its antiparticle, the virtual positron, which rapidly annihilate each other shortly thereafter. The combination of the energy variation needed to create these particles, and the time during which they exist, fall under the threshold of detectability expressed by the Heisenberg uncertainty relation, Δ\"E\" · Δ\"t\" ≥ \"ħ\". In effect, the energy needed to create these virtual particles, Δ\"E\", can be \"borrowed\" from the vacuum for a period of time, Δ\"t\", so that their product is no more than the reduced Planck constant, . Thus, for a virtual electron, Δ\"t\" is at most .\nWhile an electron–positron virtual pair is in existence, the coulomb force from the ambient electric field surrounding an electron causes a created positron to be attracted to the original electron, while a created electron experiences a repulsion. This causes what is called vacuum polarization. In effect, the vacuum behaves like a medium having a dielectric permittivity more than unity. Thus the effective charge of an electron is actually smaller than its true value, and the charge decreases with increasing distance from the electron. This polarization was confirmed experimentally in 1997 using the Japanese TRISTAN particle accelerator. Virtual particles cause a comparable shielding effect for the mass of the electron.\n\nThe interaction with virtual particles also explains the small (about 0.1%) deviation of the intrinsic magnetic moment of the electron from the Bohr magneton (the anomalous magnetic moment). The extraordinarily precise agreement of this predicted difference with the experimentally determined value is viewed as one of the great achievements of quantum electrodynamics.\n\nThe apparent paradox in classical physics of a point particle electron having intrinsic angular momentum and magnetic moment can be explained by the formation of virtual photons in the electric field generated by the electron. These photons cause the electron to shift about in a jittery fashion (known as zitterbewegung), which results in a net circular motion with precession. This motion produces both the spin and the magnetic moment of the electron. In atoms, this creation of virtual photons explains the Lamb shift observed in spectral lines.\n\nAn electron generates an electric field that exerts an attractive force on a particle with a positive charge, such as the proton, and a repulsive force on a particle with a negative charge. The strength of this force in nonrelativistic approximation is determined by Coulomb's inverse square law. When an electron is in motion, it generates a magnetic field. The Ampère-Maxwell law relates the magnetic field to the mass motion of electrons (the current) with respect to an observer. This property of induction supplies the magnetic field that drives an electric motor. The electromagnetic field of an arbitrary moving charged particle is expressed by the Liénard–Wiechert potentials, which are valid even when the particle's speed is close to that of light (relativistic).\nWhen an electron is moving through a magnetic field, it is subject to the Lorentz force that acts perpendicularly to the plane defined by the magnetic field and the electron velocity. This centripetal force causes the electron to follow a helical trajectory through the field at a radius called the gyroradius. The acceleration from this curving motion induces the electron to radiate energy in the form of synchrotron radiation. The energy emission in turn causes a recoil of the electron, known as the Abraham–Lorentz–Dirac Force, which creates a friction that slows the electron. This force is caused by a back-reaction of the electron's own field upon itself.\nPhotons mediate electromagnetic interactions between particles in quantum electrodynamics. An isolated electron at a constant velocity cannot emit or absorb a real photon; doing so would violate conservation of energy and momentum. Instead, virtual photons can transfer momentum between two charged particles. This exchange of virtual photons, for example, generates the Coulomb force. Energy emission can occur when a moving electron is deflected by a charged particle, such as a proton. The acceleration of the electron results in the emission of Bremsstrahlung radiation.\n\nAn inelastic collision between a photon (light) and a solitary (free) electron is called Compton scattering. This collision results in a transfer of momentum and energy between the particles, which modifies the wavelength of the photon by an amount called the Compton shift. The maximum magnitude of this wavelength shift is \"h\"/\"m\"\"c\", which is known as the Compton wavelength. For an electron, it has a value of . When the wavelength of the light is long (for instance, the wavelength of the visible light is 0.4–0.7 μm) the wavelength shift becomes negligible. Such interaction between the light and free electrons is called Thomson scattering or linear Thomson scattering.\n\nThe relative strength of the electromagnetic interaction between two charged particles, such as an electron and a proton, is given by the fine-structure constant. This value is a dimensionless quantity formed by the ratio of two energies: the electrostatic energy of attraction (or repulsion) at a separation of one Compton wavelength, and the rest energy of the charge. It is given by \"α\" ≈ , which is approximately equal to .\n\nWhen electrons and positrons collide, they annihilate each other, giving rise to two or more gamma ray photons. If the electron and positron have negligible momentum, a positronium atom can form before annihilation results in two or three gamma ray photons totalling 1.022 MeV. On the other hand, a high-energy photon can transform into an electron and a positron by a process called pair production, but only in the presence of a nearby charged particle, such as a nucleus.\n\nIn the theory of electroweak interaction, the left-handed component of electron's wavefunction forms a weak isospin doublet with the electron neutrino. This means that during weak interactions, electron neutrinos behave like electrons. Either member of this doublet can undergo a charged current interaction by emitting or absorbing a and be converted into the other member. Charge is conserved during this reaction because the W boson also carries a charge, canceling out any net change during the transmutation. Charged current interactions are responsible for the phenomenon of beta decay in a radioactive atom. Both the electron and electron neutrino can undergo a neutral current interaction via a exchange, and this is responsible for neutrino-electron elastic scattering.\nAn electron can be \"bound\" to the nucleus of an atom by the attractive Coulomb force. A system of one or more electrons bound to a nucleus is called an atom. If the number of electrons is different from the nucleus' electrical charge, such an atom is called an ion. The wave-like behavior of a bound electron is described by a function called an atomic orbital. Each orbital has its own set of quantum numbers such as energy, angular momentum and projection of angular momentum, and only a discrete set of these orbitals exist around the nucleus. According to the Pauli exclusion principle each orbital can be occupied by up to two electrons, which must differ in their spin quantum number.\n\nElectrons can transfer between different orbitals by the emission or absorption of photons with an energy that matches the difference in potential. Other methods of orbital transfer include collisions with particles, such as electrons, and the Auger effect. To escape the atom, the energy of the electron must be increased above its binding energy to the atom. This occurs, for example, with the photoelectric effect, where an incident photon exceeding the atom's ionization energy is absorbed by the electron.\n\nThe orbital angular momentum of electrons is quantized. Because the electron is charged, it produces an orbital magnetic moment that is proportional to the angular momentum. The net magnetic moment of an atom is equal to the vector sum of orbital and spin magnetic moments of all electrons and the nucleus. The magnetic moment of the nucleus is negligible compared with that of the electrons. The magnetic moments of the electrons that occupy the same orbital (so called, paired electrons) cancel each other out.\n\nThe chemical bond between atoms occurs as a result of electromagnetic interactions, as described by the laws of quantum mechanics. The strongest bonds are formed by the sharing or transfer of electrons between atoms, allowing the formation of molecules. Within a molecule, electrons move under the influence of several nuclei, and occupy molecular orbitals; much as they can occupy atomic orbitals in isolated atoms. A fundamental factor in these molecular structures is the existence of electron pairs. These are electrons with opposed spins, allowing them to occupy the same molecular orbital without violating the Pauli exclusion principle (much like in atoms). Different molecular orbitals have different spatial distribution of the electron density. For instance, in bonded pairs (i.e. in the pairs that actually bind atoms together) electrons can be found with the maximal probability in a relatively small volume between the nuclei. By contrast, in non-bonded pairs electrons are distributed in a large volume around nuclei.\n\nIf a body has more or fewer electrons than are required to balance the positive charge of the nuclei, then that object has a net electric charge. When there is an excess of electrons, the object is said to be negatively charged. When there are fewer electrons than the number of protons in nuclei, the object is said to be positively charged. When the number of electrons and the number of protons are equal, their charges cancel each other and the object is said to be electrically neutral. A macroscopic body can develop an electric charge through rubbing, by the triboelectric effect.\n\nIndependent electrons moving in vacuum are termed \"free\" electrons. Electrons in metals also behave as if they were free. In reality the particles that are commonly termed electrons in metals and other solids are quasi-electrons—quasiparticles, which have the same electrical charge, spin, and magnetic moment as real electrons but might have a different mass. When free electrons—both in vacuum and metals—move, they produce a net flow of charge called an electric current, which generates a magnetic field. Likewise a current can be created by a changing magnetic field. These interactions are described mathematically by Maxwell's equations.\n\nAt a given temperature, each material has an electrical conductivity that determines the value of electric current when an electric potential is applied. Examples of good conductors include metals such as copper and gold, whereas glass and Teflon are poor conductors. In any dielectric material, the electrons remain bound to their respective atoms and the material behaves as an insulator. Most semiconductors have a variable level of conductivity that lies between the extremes of conduction and insulation. On the other hand, metals have an electronic band structure containing partially filled electronic bands. The presence of such bands allows electrons in metals to behave as if they were free or delocalized electrons. These electrons are not associated with specific atoms, so when an electric field is applied, they are free to move like a gas (called Fermi gas) through the material much like free electrons.\n\nBecause of collisions between electrons and atoms, the drift velocity of electrons in a conductor is on the order of millimeters per second. However, the speed at which a change of current at one point in the material causes changes in currents in other parts of the material, the velocity of propagation, is typically about 75% of light speed. This occurs because electrical signals propagate as a wave, with the velocity dependent on the dielectric constant of the material.\n\nMetals make relatively good conductors of heat, primarily because the delocalized electrons are free to transport thermal energy between atoms. However, unlike electrical conductivity, the thermal conductivity of a metal is nearly independent of temperature. This is expressed mathematically by the Wiedemann–Franz law, which states that the ratio of thermal conductivity to the electrical conductivity is proportional to the temperature. The thermal disorder in the metallic lattice increases the electrical resistivity of the material, producing a temperature dependence for electric current.\n\nWhen cooled below a point called the critical temperature, materials can undergo a phase transition in which they lose all resistivity to electric current, in a process known as superconductivity. In BCS theory, this behavior is modeled by pairs of electrons entering a quantum state known as a Bose–Einstein condensate. These Cooper pairs have their motion coupled to nearby matter via lattice vibrations called phonons, thereby avoiding the collisions with atoms that normally create electrical resistance. (Cooper pairs have a radius of roughly 100 nm, so they can overlap each other.) However, the mechanism by which higher temperature superconductors operate remains uncertain.\n\nElectrons inside conducting solids, which are quasi-particles themselves, when tightly confined at temperatures close to absolute zero, behave as though they had split into three other quasiparticles: spinons, orbitons and holons. The former carries spin and magnetic moment, the next carries its orbital location while the latter electrical charge.\n\nAccording to Einstein's theory of special relativity, as an electron's speed approaches the speed of light, from an observer's point of view its relativistic mass increases, thereby making it more and more difficult to accelerate it from within the observer's frame of reference. The speed of an electron can approach, but never reach, the speed of light in a vacuum, \"c\". However, when relativistic electrons—that is, electrons moving at a speed close to \"c\"—are injected into a dielectric medium such as water, where the local speed of light is significantly less than \"c\", the electrons temporarily travel faster than light in the medium. As they interact with the medium, they generate a faint light called Cherenkov radiation.\nThe effects of special relativity are based on a quantity known as the Lorentz factor, defined as formula_1 where \"v\" is the speed of the particle. The kinetic energy \"K\" of an electron moving with velocity \"v\" is:\nwhere \"m\" is the mass of electron. For example, the Stanford linear accelerator can accelerate an electron to roughly 51 GeV.\nSince an electron behaves as a wave, at a given velocity it has a characteristic de Broglie wavelength. This is given by \"λ\" = \"h\"/\"p\" where \"h\" is the Planck constant and \"p\" is the momentum. For the 51 GeV electron above, the wavelength is about , small enough to explore structures well below the size of an atomic nucleus.\n\nThe Big Bang theory is the most widely accepted scientific theory to explain the early stages in the evolution of the Universe. For the first millisecond of the Big Bang, the temperatures were over 10 billion kelvins and photons had mean energies over a million electronvolts. These photons were sufficiently energetic that they could react with each other to form pairs of electrons and positrons. Likewise, positron-electron pairs annihilated each other and emitted energetic photons:\nAn equilibrium between electrons, positrons and photons was maintained during this phase of the evolution of the Universe. After 15 seconds had passed, however, the temperature of the universe dropped below the threshold where electron-positron formation could occur. Most of the surviving electrons and positrons annihilated each other, releasing gamma radiation that briefly reheated the universe.\n\nFor reasons that remain uncertain, during the annihilation process there was an excess in the number of particles over antiparticles. Hence, about one electron for every billion electron-positron pairs survived. This excess matched the excess of protons over antiprotons, in a condition known as baryon asymmetry, resulting in a net charge of zero for the universe. The surviving protons and neutrons began to participate in reactions with each other—in the process known as nucleosynthesis, forming isotopes of hydrogen and helium, with trace amounts of lithium. This process peaked after about five minutes. Any leftover neutrons underwent negative beta decay with a half-life of about a thousand seconds, releasing a proton and electron in the process,\nFor about the next –, the excess electrons remained too energetic to bind with atomic nuclei. What followed is a period known as recombination, when neutral atoms were formed and the expanding universe became transparent to radiation.\nRoughly one million years after the big bang, the first generation of stars began to form. Within a star, stellar nucleosynthesis results in the production of positrons from the fusion of atomic nuclei. These antimatter particles immediately annihilate with electrons, releasing gamma rays. The net result is a steady reduction in the number of electrons, and a matching increase in the number of neutrons. However, the process of stellar evolution can result in the synthesis of radioactive isotopes. Selected isotopes can subsequently undergo negative beta decay, emitting an electron and antineutrino from the nucleus. An example is the cobalt-60 (Co) isotope, which decays to form nickel-60 ().\n\nAt the end of its lifetime, a star with more than about 20 solar masses can undergo gravitational collapse to form a black hole. According to classical physics, these massive stellar objects exert a gravitational attraction that is strong enough to prevent anything, even electromagnetic radiation, from escaping past the Schwarzschild radius. However, quantum mechanical effects are believed to potentially allow the emission of Hawking radiation at this distance. Electrons (and positrons) are thought to be created at the event horizon of these stellar remnants.\n\nWhen a pair of virtual particles (such as an electron and positron) is created in the vicinity of the event horizon, random spatial positioning might result in one of them to appear on the exterior; this process is called quantum tunnelling. The gravitational potential of the black hole can then supply the energy that transforms this virtual particle into a real particle, allowing it to radiate away into space. In exchange, the other member of the pair is given negative energy, which results in a net loss of mass-energy by the black hole. The rate of Hawking radiation increases with decreasing mass, eventually causing the black hole to evaporate away until, finally, it explodes.\nCosmic rays are particles traveling through space with high energies. Energy events as high as have been recorded. When these particles collide with nucleons in the Earth's atmosphere, a shower of particles is generated, including pions. More than half of the cosmic radiation observed from the Earth's surface consists of muons. The particle called a muon is a lepton produced in the upper atmosphere by the decay of a pion.\nA muon, in turn, can decay to form an electron or positron.\n\nRemote observation of electrons requires detection of their radiated energy. For example, in high-energy environments such as the corona of a star, free electrons form a plasma that radiates energy due to Bremsstrahlung radiation. Electron gas can undergo plasma oscillation, which is waves caused by synchronized variations in electron density, and these produce energy emissions that can be detected by using radio telescopes.\n\nThe frequency of a photon is proportional to its energy. As a bound electron transitions between different energy levels of an atom, it absorbs or emits photons at characteristic frequencies. For instance, when atoms are irradiated by a source with a broad spectrum, distinct absorption lines appear in the spectrum of transmitted radiation. Each element or molecule displays a characteristic set of spectral lines, such as the hydrogen spectral series. Spectroscopic measurements of the strength and width of these lines allow the composition and physical properties of a substance to be determined.\n\nIn laboratory conditions, the interactions of individual electrons can be observed by means of particle detectors, which allow measurement of specific properties such as energy, spin and charge. The development of the Paul trap and Penning trap allows charged particles to be contained within a small region for long durations. This enables precise measurements of the particle properties. For example, in one instance a Penning trap was used to contain a single electron for a period of 10 months. The magnetic moment of the electron was measured to a precision of eleven digits, which, in 1980, was a greater accuracy than for any other physical constant.\n\nThe first video images of an electron's energy distribution were captured by a team at Lund University in Sweden, February 2008. The scientists used extremely short flashes of light, called attosecond pulses, which allowed an electron's motion to be observed for the first time.\n\nThe distribution of the electrons in solid materials can be visualized by angle-resolved photoemission spectroscopy (ARPES). This technique employs the photoelectric effect to measure the reciprocal space—a mathematical representation of periodic structures that is used to infer the original structure. ARPES can be used to determine the direction, speed and scattering of electrons within the material.\n\nElectron beams are used in welding. They allow energy densities up to across a narrow focus diameter of and usually require no filler material. This welding technique must be performed in a vacuum to prevent the electrons from interacting with the gas before reaching their target, and it can be used to join conductive materials that would otherwise be considered unsuitable for welding.\n\nElectron-beam lithography (EBL) is a method of etching semiconductors at resolutions smaller than a micrometer. This technique is limited by high costs, slow performance, the need to operate the beam in the vacuum and the tendency of the electrons to scatter in solids. The last problem limits the resolution to about 10 nm. For this reason, EBL is primarily used for the production of small numbers of specialized integrated circuits.\n\nElectron beam processing is used to irradiate materials in order to change their physical properties or sterilize medical and food products. Electron beams fluidise or quasi-melt glasses without significant increase of temperature on intensive irradiation: e.g. intensive electron radiation causes a many orders of magnitude decrease of viscosity and stepwise decrease of its activation energy.\n\nLinear particle accelerators generate electron beams for treatment of superficial tumors in radiation therapy. Electron therapy can treat such skin lesions as basal-cell carcinomas because an electron beam only penetrates to a limited depth before being absorbed, typically up to 5 cm for electron energies in the range 5–20 MeV. An electron beam can be used to supplement the treatment of areas that have been irradiated by X-rays.\n\nParticle accelerators use electric fields to propel electrons and their antiparticles to high energies. These particles emit synchrotron radiation as they pass through magnetic fields. The dependency of the intensity of this radiation upon spin polarizes the electron beam—a process known as the Sokolov–Ternov effect. Polarized electron beams can be useful for various experiments. Synchrotron radiation can also cool the electron beams to reduce the momentum spread of the particles. Electron and positron beams are collided upon the particles' accelerating to the required energies; particle detectors observe the resulting energy emissions, which particle physics studies .\n\nLow-energy electron diffraction (LEED) is a method of bombarding a crystalline material with a collimated beam of electrons and then observing the resulting diffraction patterns to determine the structure of the material. The required energy of the electrons is typically in the range 20–200 eV. The reflection high-energy electron diffraction (RHEED) technique uses the reflection of a beam of electrons fired at various low angles to characterize the surface of crystalline materials. The beam energy is typically in the range 8–20 keV and the angle of incidence is 1–4°.\n\nThe electron microscope directs a focused beam of electrons at a specimen. Some electrons change their properties, such as movement direction, angle, and relative phase and energy as the beam interacts with the material. Microscopists can record these changes in the electron beam to produce atomically resolved images of the material. In blue light, conventional optical microscopes have a diffraction-limited resolution of about 200 nm. By comparison, electron microscopes are limited by the de Broglie wavelength of the electron. This wavelength, for example, is equal to 0.0037 nm for electrons accelerated across a 100,000-volt potential. The Transmission Electron Aberration-Corrected Microscope is capable of sub-0.05 nm resolution, which is more than enough to resolve individual atoms. This capability makes the electron microscope a useful laboratory instrument for high resolution imaging. However, electron microscopes are expensive instruments that are costly to maintain.\n\nTwo main types of electron microscopes exist: transmission and scanning. Transmission electron microscopes function like overhead projectors, with a beam of electrons passing through a slice of material then being projected by lenses on a photographic slide or a charge-coupled device. Scanning electron microscopes rasteri a finely focused electron beam, as in a TV set, across the studied sample to produce the image. Magnifications range from 100× to 1,000,000× or higher for both microscope types. The scanning tunneling microscope uses quantum tunneling of electrons from a sharp metal tip into the studied material and can produce atomically resolved images of its surface.\n\nIn the free-electron laser (FEL), a relativistic electron beam passes through a pair of undulators that contain arrays of dipole magnets whose fields point in alternating directions. The electrons emit synchrotron radiation that coherently interacts with the same electrons to strongly amplify the radiation field at the resonance frequency. FEL can emit a coherent high-brilliance electromagnetic radiation with a wide range of frequencies, from microwaves to soft X-rays. These devices are used in manufacturing, communication, and in medical applications, such as soft tissue surgery.\n\nElectrons are important in cathode ray tubes, which have been extensively used as display devices in laboratory instruments, computer monitors and television sets. In a photomultiplier tube, every photon striking the photocathode initiates an avalanche of electrons that produces a detectable current pulse. Vacuum tubes use the flow of electrons to manipulate electrical signals, and they played a critical role in the development of electronics technology. However, they have been largely supplanted by solid-state devices such as the transistor.\n\n"}
{"id": "29397981", "url": "https://en.wikipedia.org/wiki?curid=29397981", "title": "Gas appliance", "text": "Gas appliance\n\nA gas appliance is any appliance that uses natural gas, propane, oil, coal, etc as its power source rather than electricity. commonly used for space heating, water heating, cooking, and the like.\n\n\n\n"}
{"id": "54044", "url": "https://en.wikipedia.org/wiki?curid=54044", "title": "Gothic architecture", "text": "Gothic architecture\n\nGothic architecture is a style that flourished in Europe during the High and Late Middle Ages. It evolved from Romanesque architecture and was succeeded by Renaissance architecture. Originating in 12th-century France, it was widely used, especially for cathedrals and churches, until the 16th century.\n\nIts most prominent features included the use of the rib vault and the flying buttress, which allowed the weight of the roof to be counterbalanced by buttresses outside the building, giving greater height and more space for windows. Another important feature was the extensive use of stained glass, and the rose window, to bring light and color to the interior. Another feature was the use of realistic statuary on the exterior, particularly over the portals, to illustrate biblical stories for the largely illiterate parishioners. These technologies had all existed in Romanesque architecture, but they were used in more innovative ways and more extensively in Gothic architecture to make buildings taller, lighter and stronger.\n\nThe first notable example is generally considered to be the Abbey of Saint-Denis, near Paris, whose choir and facade were reconstructed with Gothic features. The choir was completed in 1144. The style also appeared in some civic architecture in northern Europe, notably in town halls and university buildings. A Gothic revival began in mid-18th-century England, spread through 19th century Europe and continued, largely for ecclesiastical and university structures, into the 20th century.\n\nGothic architecture was known during the period as (\"French/Frankish work\"), The term \"Gothic architecture\" originated in the 16th century, and was originally very negative, suggesting barbaric. Giorgio Vasari used the term \"barbarous German style\" in his 1550 \"Lives of the Artists\" to describe what is now considered the Gothic style, and in the introduction to the \"Lives\" he attributed various architectural features to \"the Goths\" whom he held responsible for destroying the ancient buildings after they conquered Rome, and erecting new ones in this style.\n\nThe Gothic style originated in the Ile-de-France region of northern France in the first half of the 12th century. A new dynasty of French Kings, the Capetians, had subdued the feudal lords, and had become the most powerful rulers in France, with their capital in Paris. They allied themselves with the bishops of the major cities of northern France, and reduced the power of the feudal abbots and monasteries. Their rise coincided with an enormous growth of the population and prosperity of the cities of northern France. The Capetian Kings and their bishops wished to build new cathedrals as monuments of their power, wealth, and religious faith.\n\nThe church which served as the primary model for the style was the Abbey of St-Denis, which underwent reconstruction by the Abbot Suger, first in the choir and then the facade (1140–44), Suger was a close ally and biographer of the French King, Louis VII, who was a fervent Catholic and builder, and the founder of the University of Paris. Suger remodeled the ambulatory of the Abbey, removed the enclosures that separated the chapels, and replaced the existing structure with imposing pillars and rib vaults. This created created higher and wider bays, into which he installed larger windows, which filled the end of the church with light. Soon afterwards he rebuilt the facade, adding three deep portals, each with a tympanum, an arch filled with sculpture illustrating biblical stories. The new facade was flanked by two towers. He also installed a small circular rose window over the central portal. This design became the prototype for a series of new French cathedrals.\n\nSens Cathedral (begun between 1135 and 1140) was the first Cathedral to be built in the new style (St. Denis was an abbey, not a Cathedral). Other versions of the new style soon appeared in Noyon Cathedral (begun 1150); Laon Cathedral (begun 1165); and the most famous of all, Notre-Dame de Paris, where construction had begun in 1160.\n\nThe Gothic style was also adapted by some French monastic orders, notably the Cistercian order under Saint Bernard of Clairvaux It was used in an austere form without ornament at the new Cistercian Abbey of Fontenay (1139–1147) and the church of Clairvaux Abbey, whose site is now occupied by a French prison.\n\nThe new style was also copied outside the Kingdom of France in the Duchy of Normandy. Early examples of Norman Gothic included Coutances Cathedral (1210–1274); Bayeux Cathedral (rebuilt from Romanesque style in 12th century), Le Mans Cathedral (rebuilt from Romanesque 12th century) and Rouen Cathedral. Through the rule of the Angevin dynasty, the new style was introduced to England and spread from there to Low Countries, Germany, Spain, northern Italy and Sicily. The Gothic style did not immediately replace the Romanesque everywhere in Europe. The Late Romanesque continued to flourish in the Holy Roman Empire under the Hohenstaufens and Rhineland.\n\nFrom the end of the 12th century until the middle of the 13th century, the gothic style spread from the Île-de-France to appear in other cities of northern France. New structures in the style included Chartres Cathedral (begun 1200); Bourges Cathedral (1195 to 1230), Reims Cathedral (1211–1275), and Amiens Cathedral (begun 1250); At Chartres, the use of the flying buttresses allowed the elimination of the tribune level, which allowed much higher arcades and nave, and larger windows. The early type of rib vault used of Saint Denis and Notre Dame, with six parts, was modified to four parts, making it simpler and stronger. Amiens and Chartres were among the first to use the flying buttress; the buttresses were strengthened by an additional arch and with a supporting arcade, allowing even higher and walls and more windows. At Reims, the buttresses were given greater weight and strength by the addition of heavy stone pinnacles on top. These were often decorated with statues of angels, and became an important decorative element of the High Gothic style. Another practical and decorative element, the gargoyle, appeared; it was an ornamental rain spout which channeled the water from the roof away from the building. At Amiens, the windows of the nave were made larger, and an additional row of clear glass windows (the claire-voie) flooded the interior with light. The new structural technologies allowed the enlargement of the transepts and of the choirs at the east end of the cathedrals, creating the space for a ring of well-lit chapels. The transept of Notre-Dame was rebuilt with the new technology, and two spectacular rose windows added.\n\nThe next period was termed \"Rayonnant\" (\"Radiant\"), describing the tendency toward the use of more and more stained glass and less masonry in the design of the structure, until the walls seemed entirely made of glass. The most celebrated example was the chapel of Sainte-Chapelle, the chapel attached to the royal residence on the Palais de la Cité. An elaborate system of exterior columns and arches reduced the walls of the upper chapel to a thin framework for the enormous windows. The weight of each of the masonry gables above the archivolt of the windows also helped the walls to resist the thrust and to distribute the weight.\n\nAnother landmark of the Rayonnant Gothic are the two rose windows on the north and south of the transept of Notre Dame Cathedral. Whereas earlier rose windows, like those of Amiens Cathedral, were framed by stone and occupied only a portion of the wall, these two windows, with a delicate lace-like framework, occupied the entire space between the pillars.\n\nThe Flamboyant Gothic style appeared in the second half of the 14th century. Its characteristic features were more exuberant decoration, as the nobles and wealthy citizens of mostly northern French cities competed to build more and more elaborate churches and cathedrals. It took its name from the sinuous, flame-like designs which ornamented windows. Other new features included the \"arc en accolade\", a window decorated with an arch, stone pinnacles and floral sculpture. It also featured an increase in the number of \"nervures\", or ribs, that supported and decorated each vault of the ceiling, both for greater support and decorative effect. Notable examples of Flamboyant Gothic include the western facade of Rouen Cathedral and Sainte-Chapelle de Vincennes in Paris, both built in the 1370s; and the Choir of Mont Saint Michel Abbey (1448 c.). Subsequently, the style spread to Northern Europe. Notable example of Flamboyant Gothic in the Baltic region includes the Brick Gothic Church of St. Anne in Vilnius (1490s).\n\nThe plan of the Gothic cathedral was based on the model of the ancient Roman basilica, which was a combined public market and courthouse; which was also the basis of the plan of the Romanesque cathedral. The cathedral is in the form of a Latin cross. The entrance is traditionally on the west end, has three portals decorated with sculpture, usually a rose window, and is flanked by two towers. The long nave, where the congregation worshiped, occupies the west end. This is usually divided from the nave by rows of pillars, which support the roof, flanked by one or two aisles, called \"collaérals\". There are usually small chapels on the two sides, placed between the buttresses, which provide additional support to the walls.\n\nThe cathedral usually has a transept, a crossing, roughly in the middle, which sometimes projects outwards some distance, and in other cases, such as Notre-Dame, is minimal. The \"croisée\" or crossing of the transept, is the center of the church, and is surrounded by particularly massive pillars, which sometimes support a lantern tower, which brings light into the center of the cathedral. The north and south facades of the transept often feature rose windows, as at Notre Dame de Paris.\n\nTo the east of the transept is the choir, where the altar is located, where ceremonies take place, and where only the clergy was allowed. This space grew greatly in the 12th century, as ceremonies became more elaborate. Behind the choir is single or double a walkway called the ambulatory. At the eastern end of the church is the apse usually in the form of a half-circle, and the chevet. There is usually a chapel here dedicated to the Virgin Mary, which can be very large. Around chevet there are usually several other smaller chapels.\n\nThe earlier Gothic cathedrals had four levels, from the floor to the roof. On the ground floor there were two rows grand arcades with large pillars, which received the weight of the vaults of the ceiling. Above these were the tribunes, a section of arched openings, giving more support. Above these was the \"triforium\", a section of small arches. On the top level, just below the vaults, were the upper windows, the main source of light for the Cathedral.The lower walls were supported by massive \"contreforts\" or buttresses placed directly up against them, with pinnacles on top which provided additional weight.\n\nLater, with the development of the flying buttress, the supports moved further away from the walls, and the walls were built much higher. Gradually the tribunes and the triforium disappeared, and the walls above the arcades were occupied almost entirely with stained glass.\n\nThe eastern arm shows considerable diversity. In England it is generally long and may have two distinct sections, both choir and presbytery. It is often square ended or has a projecting \"Lady Chapel\", dedicated to the Virgin Mary. In France the eastern end is often polygonal and surrounded by a walkway called an ambulatory and sometimes a ring of chapels called a \"chevet\". While German churches are often similar to those of France, in Italy, the eastern projection beyond the transept is usually just a shallow apsidal chapel containing the sanctuary, as at Florence Cathedral.\n\nAnother characteristic feature of the Gothic style, domestic and ecclesiastical alike, is the division of interior space into individual cells according to the building's ribbing and vaults, regardless of whether or not the structure actually has a vaulted ceiling. This system of cells of varying size and shape juxtaposed in various patterns was again totally unique to antiquity and the Early Middle Ages and scholars, Frankl included, have emphasised the mathematical and geometric nature of this design. Frankl in particular thought of this layout as \"creation by division\" rather than the Romanesque's \"creation by addition.\" Others, namely Viollet-le-Duc, Wilhelm Pinder, and August Schmarsow, instead proposed the term \"articulated architecture.\" The opposite theory as suggested by Henri Focillon and Jean Bony is of \"spacial unification\", or of the creation of an interior that is made for sensory overload via the interaction of many elements and perspectives. Interior and exterior partitions, often extensively studied, have been found to at times contain features, such as thoroughfares at window height, that make the illusion of thickness. Additionally, the piers separating the isles eventually stopped being part of the walls but rather independent objects that jut out from the actual aisle wall itself.\n\nBoth the pointed arch and the rib vault had been used in romanesque architecture, but Gothic builders refined them and used them to much greater effect. They made the structures lighter and stronger, and thus allowed the great heights and expanses of stained glass found in Gothic cathedrals.\n\nIn Romanesque architecture, the rounded arches of the barrel vaults that supported the roof pressed directly down on the walls with crushing weight. This required massive columns, thick walls and small windows, and naturally limited the height of the building. The pointed or broken arch, introduced during the Romanesque period, was stronger, lighter, and carried the thrust outwards, rather than directly downwards.\n\nThe rib vault took advantage of the strength of the pointed arch. The vault was supported by thin ribs or arches of stone, which reached downwards and outwards to cluster around supporting pillars along the inside of the walls. The earlier rib vaults, used at Notre Dame, Noyon, and Laon, were divided by the ribs into six compartments, and could only cross a limited space. In later cathedral construction, the design was improved, and the rib vaults had only four compartments, and could cover a wider span; a single vault could cross the nave, and fewer pillars were needed. The four-part vault was used at Amiens, Reims, and the other later cathedrals, and eventually at cathedrals across Europe.\n\nIn the later period of the Gothic style, the rib vaults lost their elegant simplicity, and were loaded with additional ribs, sculptural designs, and sometimes pendants and other purely decorative elements.\n\nAnother important feature of Gothic architecture was the flying buttress, designed to support the walls by means of arches connected to counter-supports outside the walls. Flying buttresses had existed in simple forms since Roman times, but the Gothic builders raised their use to a fine art, balancing the thrust from the roof inside against the counter-thrust of the buttresses. The earliest Gothic cathedrals, including Saint-Denis and Notre-Dame in its beginning stages, did not have flying buttresses. Their walls were supported by heavy stone abutments placed directly against the walls, The roof was supported by the ribs of the vaults, which were bundled with the columns below.\n\nIn the later 12th and early 13th century, the buttresses became more sophisticated. New arches carried the thrust of the weight entirely outside the walls, where it was met by the counter-thrust of stone columns, with pinnacles placed on top for decoration and for additional weight. Thanks to this system of external buttresses, the walls could be higher and thinner, and could support larger stained glass windows. The buttresses themselves became part of the decoration; the pinnacles became more and more ornate, becoming more and more elaborate, as at Beauvais Cathedral and Reims Cathedral. The arches had an additional practical purpose; they contained lead channels which carried rain water off the roof; it was expelled from the mouths of stone gargoyles placed in rows on the buttresses.\n\nIn the late Gothic periods the buttresses became extremely ornate, with a large amount of non-functional decoration in the form of pinnacles, curving arches, counter-curves, statuary and ornamental pendants.\n\nAn important characteristic of Gothic church architecture is its height, both absolute and in proportion to its width, the verticality suggesting an aspiration to Heaven. The increasing height of cathedrals over the Gothic period was accompanied by an increasing proportion of the wall devoted to windows, until, by the late Gothic, the interiors became like cages of glass. This was made possible by the development of the flying buttress, which transferred the thrust of the weight of the roof to the supports outside the walls. As a result, the walls gradually became thinner and higher, and masonry was replaced with glass. The four-part elevation of the naves of early Cathedrals such as Notre-Dame (arcade, tribune, triforium, claire-voie) was transformed in the choir of Beauvais Cathedral to very tall arcades, a thin triforium, and soaring windows up to the roof.\n\nBeauvais Cathedral reached the limit of what was possible with Gothic technology. A portion of the choir collapsed in 1284, causing alarm in all of the cities with very tall cathedrals. Panels of experts were created in Sienna and Chartres to study the stability of those structures. Only the transept and choir of Beauvais were completed, and in the 21st century the transept walls were reinforced with cross-beams. No cathedral built since exceeded the height of the choir of Beauvais.\n\nA section of the main body of a Gothic church usually shows the nave as considerably taller than it is wide. In England the proportion is sometimes greater than 2:1, while the greatest proportional difference achieved is at Cologne Cathedral with a ratio of 3.6:1. The highest internal vault is at Beauvais Cathedral at .\n\nOne of the most prominent features of Gothic architecture was the use of stained glass window, which steadily grew in height and size and filled cathedrals with light and color. Historians including Viollet-le-Duc, Focillon, Aubert, and Max Dvořák contended that this is one of the most universal features of the Gothic style.\n\nReligious teachings in the Middle Ages, particularly the writings of Religious Pseudo-Dionysius, a 6th-century mystic whose book, \"The Celestial Hierarchy\", was popular among monks in France, taught that all light was divine. When the Abbot Suger ordered the reconstruction of the Basilica of Saint Denis, he instructed that the windows in the choir admit as much light as possible.\n\nMany earlier Romanesque churches had stained glass windows, and many had round windows, called \"oculi\", but these windows were necessarily small, due to the thickness of the walls. The primary interior decorations of Romanesque cathedrals were painted murals. In the Gothic period, the improvements in rib vaults and flying buttresses allowed Cathedral walls to be higher, thinner and stronger, and windows were consequently considerably larger, The windows of churches in the late Gothic period, such as Sainte Chapelle in Paris, filled the entire wall between the ribs of stone. Enormous windows were also an important element of York Minster and Gloucester Cathedral.\n\nThe main threat to cathedral windows was the wind; frames had to be extremely strong. The early windows were fit into openings cut into the stone. The small pieces of colored glass were joined together with pieces of lead, and then their surfaces were painted with faces and other details. and then the windows were mounted in the stone frames. Thin vertical and horizontal bars of iron, called \"vergettes\" or \"barlotierres\", were placed inside the window to reinforce the glass.\n\nThe stories told in the glass were usually episodes from the Bible, but they also sometimes illustrated the professions of the guilds which had funded the windows, such as the drapers, stonemasons or the barrel-makers.\n\nMuch of the stained glass in Gothic cathedrals today dates from later restorations, but a few cathedrals, notably Chartres Cathedral and Bourges Cathedral, still have many of their original windows.\n\nEarly Gothic Cathedrals traditionally have their main entrance at the western end of the church, opposite the choir. Based on the model of the Basilica of Saint Denis and Notre-Dame de Paris, there are usually three doorways with pointed arches, richly filled with sculpture. The tympanum, or arch, over each doorway is filled with realistic statues illustrating biblical stories, and the columns between the doors are often also crowded with statuary. Following the example of Amiens, the tympanum over the central portal traditionally depicted the Last Judgement, the right portal showed the coronation of the Virgin Mary, and the left portal showed the lives of saints who were important in the diocese.\n\nThe iconography of the sculptural decoration on the facade was not left to the artists. An edict of the Second Council of Nicaea in 787 had set the rules: \"The composition of religious images is not to be left to the inspiration of artists; it is derived from the principles put in place by the Catholic Church and religious tradition. Only the art belongs to the artist; the composition belongs to the Fathers.\"\n\nThe portals and interiors were much more colorful than they are today. Each sculpture on the tympanum and in the interior was painted by the \"peintre imagier\", or image painter, following a system of colors codified in the 12th century; yellow, called \"gold\", symbolized intelligence, grandeur and virtue; white, called \"argent\", symbolized purity, wisdom, and correctness; black, or \"sable\", meant sadness, but also will; green, or \"sinopole\", represented hope, liberty and joy; red or \"guelues\" meant charity or victory; blue, or \"azure\" symbolized the sky, faithfulness and perseverance; and violet, or \"pourpre\", was the color of royalty and sovereignty.\n\nThe plan for the Basilica of Saint-Denis called for two towers of equal height on the west facade, and this general plan was copied at Notre-Dame and most of the early cathedrals. The towers of Notre-Dame de Paris, 69 meters (226 ft) tall, were intended to be seen throughout the city; they were the tallest towers in Paris until the completion of the Eiffel Tower in 1889. An informal but vigorous competition began in northern France for the tallest Cathedral towers.\n\nTo make the churches taller and more prominent, and visible from a distance, heir builders often added a \"flèche\", a spire usually made of wood and covered with lead, to the top of each tower, or, as in Notre-Dame de Paris, in the center of the transept. Later in the Gothic period, more massive towers were constructed over the transept, rivaling or exceeding in height the towers of the facade.\n\nThe towers were usually the last part of the Cathedral to be constructed. They were often built many years or decades after the rest of the building. Sometimes, by the time the towers were built, the plans had changed, or the money had run out. As a result, some Gothic cathedrals had just one tower, or two towers of different heights or styles. On the other hand, Laon Cathedral, begun just before Notre-Dame, boasted five towers; two on the facade, two on the transept, and a central lantern. An additional two were planned but not built. The Abbey of Saint-Étienne, Caen originally built in the Romanesque style, was rebuilt with nine Gothic towers in the 13th century.\n\nThe informal competition for the tallest church in Europe went on throughout the Gothic period, sometimes with disastrous results. Beauvais Cathedral had the tallest tower (153 meters or 502 feet), completed in 1569, for a brief time, until its tower collapsed in the wind in 1573. Lincoln Cathedral (159.7 meters or 524 feet) also had the record from 1311 until 1549 until its tower also collapsed. Today the tallest cathedral tower in France is Rouen Cathedral, and Cologne Cathedral (151.0 meters or 495 feet) is the tallest cathedral in Europe.\n\nThe Gothic Old St Paul's Cathedral (1087–1314) had been the tallest cathedral in England until it was destroyed by the Great Fire of London in 1666. Today the tallest combined Gothic tower and spire in the UK belongs to Salisbury Cathedral, (123 meters or 404 feet), built 1220–1258.\n\nIn Italy, the tower, if present, is almost always detached from the building, as at Florence Cathedral, and is often from an earlier structure. In France and Spain, two towers on the front is the norm. In England, Germany and Scandinavia this is often the arrangement, but an English cathedral may also be surmounted by an enormous tower at the crossing. Smaller churches usually have just one tower, but this may also be the case at larger buildings, such as Salisbury Cathedral or Ulm Minster in Ulm, Germany, completed in 1890 and possessing the tallest spire in the world, slightly exceeding that of Lincoln Cathedral, the tallest spire that was actually completed during the medieval period, at .\n\nThe exteriors and interiors of Gothic cathedrals, particularly in France, were lavishly ornamented with sculpture and decoration on religious themes, designed for the great majority of parishioners who could not read. They were described as \"Books for the poor.\" To add to the effect, all of the sculpture on the facades was originally painted and gilded.\n\nEach feature of the Cathedral had a symbolic meaning. The main portals at Notre Dame de Paris, for instance, represented the entrance to paradise, with the last judgement depicted on the tympanum over the doors, showing Christ surrounded by the apostles, and by the signs of the zodiac, representing the movements of the heavens. The columns below the tympanum are in the form of statues of saints, literally reprinting them as \"the pillars of the church.\" \n\nEach Saint had his own symbol; a winged lion stood for Saint Mark; an eagle with four wings meant Saint John the Apostle, and a winged bull symbolized Saint Luke. Sculpted angels had specific functions, sometimes as heralds, blowing trumpete, or holding up columns, as guardian angels; or holding crowns of thorns or crosses, as symbols of the crucifixion of Christ, or waving a container with incense, to illustrate theirfunction at the throne of God. Floral and vegetal decoration was also very common, representing the Garden of Eden; grapes represented the wines of Eucharist.\n\nThe tympanum over the central portal on the west facade of Notre Dame de Paris vividly illustrates the Last Judgement, with figures of sinners being led off to hell, and good Christians taken to heaven. The sculpture of the right portal shows the coronation of the Virgin Mary, and the left portal shows the lives of saints who were important to Parisians, particularly Saint Anne, the mother of the Virgin Mary.\n\nThe exteriors of cathedrals and other Gothic churches were also decorated with sculptures of a variety of fabulous and frightening grotesques or monsters. These included the chimera, a mythical hybrid creature which usually had the body of a lion and the head of a goat, and the Strix or stryge, a creature resembling an owl or bat, which was said to eat human flesh. The \"strix\" appeared in classical Roman literature; it was described by the Roman poet Ovid, who was widely read in the Middle Ages, as a large-headed bird with transfixed eyes, rapacious beak, and greyish white wings. They were part of the visual message for the illiterate worshipers, symbols of the evil and danger that threatened those who did not follow the teachings of the church.\n\nThe gargoyles, which were added to Notre Dame in about 1240, had a more practical purpose. They were the rain spouts of the cathedral, designed to divide the torrent of water which poured from the roof after rain, and to project it outwards as far as possible from the buttresses and the walls and windows so that it would not erode the mortar binding the stone. To produce many thin streams rather than a torrent of water, a large number of gargoyles were used, so they were also designed to be a decorative element of the architecture. The rainwater ran from the roof into lead gutters, then down channels on the flying buttresses, then along a channel cut in the back of the gargoyle and out of the mouth away from the cathedral.\n\nMany of the statues, particularly the grotesques, were removed from facade in the 17th and 18th century, or were destroyed during the French Revolution. They were replaced with figures in the Gothic style, designed by Eugene Viollet-le-Duc, during the 19th century restoration. Similar figures appear on the other Gothic Cathedrals of France.\n\nAnother common feature of Gothic cathedrals in France was a labyrinth or maze on the floor of the nave near the choir, which symbolized the difficult and often complicated journey of a Christian life before attaining paradise. Most labyrinths were removed by the 18th century, but a few, like the one at Amiens Cathedral, have been reconstructed, and the labyrinth at Chartres Cathedral still exists essentially in its original form.\n\nFrom the 12th century onwards, the Gothic style spread from Northern France to other regions of France and gradually to the rest of the Europe. It was often carried by the highly skilled craftsmen who had trained in the Ile-de-France and then carried their crafts to other cities. The style was adapted to local styles and materials.\n\nIn Normandy, the new naves were usually very long, sometimes more than one hundred meters, and, from the long Romanesque tradition, the walls were thicker than in northern France, and had shorter buttresses. The interiors were narrower than in the north, and were given a strong sense of verticality by long and narrow bays and lancet arches. Rose windows were rare, replaced on the exterior by a large bay in \"tiers point\". The facades had less sculptural decoration; decoration in the interior was largely in geometric forms.Norman Gothic also usually featured a profusion of towers, lanterns and spires; spires and spires sometimes were seventy meters high. Bayeux Cathedral, Rouen Cathedral, and Coutances Cathedral are notable examples of Norman Gothic.\n\nIn Burgundy, which had a long Romanesque style tradition, a lantern tower was often included, and cathedrals often had a narrow passage the length of the cathedral at the level of the stained glass windows. as in Auxerre Cathedral.\n\nIn the Southwest of France, the walls were thicker, with narrow openings, and doubled with arches. The flying buttress were rarely used, replaced by heavy abutments with chapels between.\n\nIn the South of France, the Gothic cathedrals were often built with brick and tile rather than stone. They generally had thick walls and narrow windows, and were braced by heavy abutments rather than flying buttresses. The form of the tower of Toulouse Cathedral was copied by several cathedrals in the south. They generally had a single nave or two or three of equal height. Some Gothic cathedrals in the Midi took unusual form; the Cathedral of Albi (1282–1480) was originally built as fortress, then converted to a cathedral. Albi Cathedral has another very distinctive feature; a colorful interior and painted ceiling.\n\nThe facade of Toulouse Cathedral is unusual; it is the combination of two unfinished cathedral buildings, begun in the 13th century and finally put together. Toulouse Cathedral has no flying buttresses; it is supported by massive \"contreforts\" the height of the building, with chapels between.\n\nThe Gothic style was imported very early into England, in part due to the close connection with the Duchy of Normandy, which until 1204 was still ruled by the Kings of England. The first period is generally called early English Gothic, and was dominant from about 1180 to 1275. The first part of major English cathedral to feature the new style was the choir of Canterbury Cathedral, begun about 1175. It was created by a French master builder, William of Sens. He added several original touches, including colored marble pavement, double columns in the arcades, and engaged slender \"colonettes\" which reached up to the vaults, borrowed from the design of Laon Cathedral. Westminster Abbey was rebuilt from 1245 to 1517. Salisbury Cathedral (1220–1320) is also a good example of early Gothic, with the exception of its tower and spire, which were added in 1320.\n\nThe second period of English Gothic is known as Decorated Gothic. It is customarily divided into two the \"Geometric\" style (1250–90) and the \"Curvilinear\" style (1290–1350), and it is similar to the French Rayonnant style, with an emphasis on curvilinear forms, particularly in the windows. This period saw detailed stone carving reach its peak, with elaborately carved windows and capitals, often with floral patterns, or with an accolade, a carved arch over a window decorated with pinnacles and a fleuron, or carved floral element.\n\nThe rib vaults of the Decorated Gothic became extremely ornate, with a profusion of ribs which were purely ornamental. The vaults were often decorated with hanging stone pendants. The columns also became more ornamental, as at Peterborough Cathedral, with ribs spreading upward.\n\nThe Perpendicular Gothic (c. 1380–1520) was final phase of English Gothic, lasting into the 16th century. As the name suggests, its emphasis was on clear horizontal and vertical lines, meeting at right angles. Columns extended upwards all the way to the roof, giving the interior the appearance of a cage of glass and stone, as in the nave of Gloucester Cathedral. The Tudor Arch appeared, wider and lower and often framed by moldings, which was used to create larger windows and to balance the strong vertical elements. The design of the rib vaults became even more complex, including the fan vault with pendants used in the Henry IV chapel at Westminster Abbey (1503–07).\n\nA distinctive characteristic of English cathedrals is their extreme length, and their internal emphasis upon the horizontal, which may be emphasised visually as much or more than the vertical lines. Each English cathedral (with the exception of Salisbury) has an extraordinary degree of stylistic diversity, when compared with most French, German and Italian cathedrals. It is not unusual for every part of the building to have been built in a different century and in a different style, with no attempt at creating a stylistic unity. Unlike French cathedrals, English cathedrals sprawl across their sites, with double transepts projecting strongly and \"Lady Chapels\" tacked on at a later date, such as at Westminster Abbey. In the west front, the doors are not as significant as in France, the usual congregational entrance being through a side porch. The West window is very large and never a rose, which are reserved for the transept gables. The west front may have two towers like a French Cathedral, or none. There is nearly always a tower at the crossing and it may be very large and surmounted by a spire. The distinctive English east end is square, but it may take a completely different form. Both internally and externally, the stonework is often richly decorated with carvings, particularly the capitals.\n\nBetween the 13th and 16th centuries, Gothic cathedrals were constructed in most of the major cities of northern Europe. For the most part, they followed the French model, but with variations depending upon local traditions and the materials available. The first Gothic churches in Germany were built from about 1230. They included \"Liebfrauenkirche\" ( ca. 1233–1283) in Trier, claimed to be the oldest Gothic church in Germany, and Freiburg Cathedral, which was built in three stages, the first beginning in 1120, though only the foundations of the original cathedral still exist. It is noted for its 116-metre tower, the only Gothic church tower in Germany that was completed in the Middle Ages (1330).\n\nPrague, in the region Bohemia within the Holy Roman Empire, was another flourishing center for Gothic architecture. Charles IV of Bohemia was both King of Bohemia and Holy Roman Empire, and he had monumental tastes. He began construction of Prague's St. Vitus Cathedral in the Gothic style in 1344, as well as a Gothic palace, Karlstein Castle in Central Bohemia, and Gothic buildings for the new University of Prague. The nave of Prague Cathedral featured the filet vault, a decorative type of vault in which the ribs criss-crossed in a mesh pattern, similar to the vaults of Bristol Cathedral and other English churches. His other Gothic projects included the lavishly-decorated Chapel of the Holy Cross inside Karlstein Castle (1357–1367), and the choir of Aachen Cathedral begun in 1355, which was built on the model of Sainte-Chapelle in Paris. Gothic architecture in Germany and the kingdoms of the Holy Roman Empire generally followed the French formula, but the towers were much taller and, if completed, were often surmounted by enormous openwork spires. The distinctive character of the interior of German Gothic cathedrals is their breadth and openness. German and Czech cathedrals, like the French, tend not to have strongly projecting transepts. There are also many hall churches (\"Hallenkirchen\") without clerestory windows. Cologne Cathedral is after Milan Cathedral the largest Gothic cathedral in the world. Construction began in 1248 and took, with interruptions, until 1880 to complete – a period of over 600 years. It is 144.5 metres long, 86.5 m wide and its two towers are 157 m tall.\n\nBrick Gothic (, ) is a specific style common in Northern Europe, especially in Northern Germany, Poland and in the regions around the Baltic Sea without natural rock resources. Prime examples of brick gothic include St. Mary's Church, Gdańsk (1379–1502), St. Mary's Basilica, Kraków (1290–1365) and Malbork Castle (13th century).\n\nSt. Stephen's Cathedral, Vienna (1339–1365) has the distinctive feature of a polychrome roof. Another regional variation is the Brabantine Gothic a style found in Belgium and the Netherlands. It is characterized by using light-colored sandstone or limestone, which allowed rich detailing but was prone to erosion. Features included columns with sculpted cabbage-like foliage, arched windows whose points came right up into the vaults. and, sometimes, a wooden ceiling. Examples include Grote Kerk, Haarlem, in Haarlem, the Netherlands, originally built as a Catholic Cathedral, now a Protestant church, and the Church of Our Blessed Lady of the Sablon in Brussels (15th century).\n\nStrikingly different variations of the Gothic style appeared in southern Europe, particularly in Spain and Portugal. Important examples of Spanish Gothic include Toledo Cathedral, León Cathedral, and Burgos Cathedral.\nThe distinctive characteristic of Gothic cathedrals of the Iberian Peninsula is their spatial complexity, with many areas of different shapes leading from each other. They are comparatively wide, and often have very tall arcades surmounted by low clerestories, giving a similar spacious appearance to the \"Hallenkirche\" of Germany, as at the Church of the Batalha Monastery in Portugal. Many of the cathedrals are completely surrounded by chapels. Like English cathedrals, each is often stylistically diverse. This expresses itself both in the addition of chapels and in the application of decorative details drawn from different sources. Among the influences on both decoration and form are Islamic architecture and, towards the end of the period, Renaissance details combined with the Gothic in a distinctive manner. The West front, as at Leon Cathedral, typically resembles a French west front, but wider in proportion to height and often with greater diversity of detail and a combination of intricate ornament with broad plain surfaces. At Burgos Cathedral there are spires of German style. The roofline often has pierced parapets with comparatively few pinnacles. There are often towers and domes of a great variety of shapes and structural invention rising above the roof.\n\nIn the territories under the Crown of Aragon (Aragon, Catalonia, Roussillon in France, the Balearic Islands, the Valencian Community, among others in the Italian islands), the Gothic style suppressed the transept and made the side-aisles almost as high as the main nave, creating wider spaces, and with few ornaments. There are two different Gothic styles in the Aragonese lands: Catalan Gothic and Valencian Gothic, which are different from those in the Kingdom of Castile and France.\n\nThe most important samples of Catalan Gothic style are the cathedrals of Girona, Barcelona, Perpignan and Palma (in Mallorca), the basilica of Santa Maria del Mar (in Barcelona), the Basílica del Pi (in Barcelona), and the church of Santa Maria de l'Alba in Manresa.\n\nThe most important examples of Valencian Gothic style in the old Kingdom of Valencia are the Valencia Cathedral, Llotja de la Seda (Unesco World Heritage site), Torres de Serranos, Torres de Quart, Monastery of Sant Jeroni de Cotalba, in Alfauir, Palace of the Borgias in Gandia, Monastery of Santa María de la Valldigna, Basilica of Santa Maria, in Alicante, Orihuela Cathedral, Castelló Cathedral and El Fadrí, Segorbe Cathedral, etc.\n\nItalian Gothic architecture went its own particular way, departing from the French model. It was influenced by other styles, notably the Byzantine style introduced in Ravenna. Major examples include Milan Cathedral, the Orvieto Cathedral, and particularly Florence Cathedral, before the addition of the Duomo in the Renaissance.\n\nThe Italian style was influenced by the materials available in the different regions; marble was available in great quantities in Tuscany, and was lavishly used in churches; it was scarce in Lombardy, and brick was used instead. But many of the architectural elements were used apparently mainly to be different from the French style.\n\nThe Cistercian monastic order introduced some of the first Gothic churches into Italy, in Fossanova Abbey (consecrated 1208) and the Casamari Abbey (1203–1217). They followed the basic plan of the Gothic Cistercian churches of Burgundy, particularly Cîteaux Abbey.\n\nThe Italian plan is usually regular and symmetrical, Italian cathedrals have few and widely spaced columns. The proportions are generally mathematically equilibrated, based on the square and the concept of \"armonìa\", and except in Venice where they loved flamboyant arches, the arches are almost always equilateral. Italian Gothic cathedrals often retained Romanesque features; the nave of Orvieto Cathedral had Romanesque arches and vaults.\n\nItalian Cathedrals also offered a variety of plans; Florence Cathedral (begun 1246) had a rectangular choir, based on the Cistercian model, but was designed to have three wings with polygonal chapels. Italian Gothic cathedrals were general not as tall as those in France; they rarely used flying buttresses, and generally had only two levels, an arcade and a claire-voie with small windows; but Bologna Cathedral (begun in 1388), rivaled Bourges Cathedral in France in height. The smallest notable Italian Gothic church is Santa Maria della Spina in Pisa (about 1330), which resembles a Gothic jewel box.\n\nA distinctive characteristic of Italian Gothic is the use of polychrome decoration, both externally as marble veneer on the brick façade and internally where the arches are often made of alternating black and white segments. The columns were sometimes painted red, and the walls were decorated with frescoes and the apse with mosaic. Italian cathedral façades are often polychrome and may include mosaics in the lunettes over the doors. The façades have projecting open porches and ocular or wheel windows rather than roses, and do not usually have a tower. The crossing is usually surmounted by a dome. There is often a free-standing tower and baptistery. The eastern end usually has an apse of comparatively low projection. The windows are not as large as in northern Europe and, although stained glass windows are often found, the favourite narrative medium for the interior is the fresco. The facade of Orvieto Cathedral, begun in 1310, is a striking example of mosaic decoration. Another innovation of Italian Gothic is the bronze doorway covered with sculpture; the most famous examples are the doors of the Baptistry of Florence, by Andrea Pisano (1330–1336).\n\nItalian Gothic cathedrals did not have the elaborate sculptural tympanums over the entrances of French cathedrals, but they had abundant realistic sculptural decoration. Some of the finest work was done by Nicola Pisano at the Baptistery of Pisa Cathedral (1259–60), and in Siena Cathedral; and by his son Giovanni Pisano on the west facade of Pisa Cathedral (1284–85).\n\nWhile cathedrals were the most prominent structures in the Gothic style, Gothic features were also built for many monasteries across Europe. Prominent examples were built by the Benedictines in England, France, and Normandy. They were the builders of the Abbey of St Denis, and Abbey of Saint-Remi in France. Later Benedictine projects (constructions and renovations) include Rouen's Abbey of Saint-Ouen,the Abbey La Chaise-Dieu, and the choir of Mont Saint-Michel in France.\n\nEnglish examples are Westminster Abbey, originally built as a Benedictine order monastic church; and the reconstruction of the Benedictine church at Canterbury. The Cistercians spread the style as far east and south as Poland and Hungary. Smaller orders such as the Carthusians and Premonstratensians also built some 200 churches, usually near cities. The Franciscans and Dominicans also carried out a transition to Gothic in the 13th and 14th centuries. The Teutonic Order, a military order, spread Gothic art into Pomerania, East Prussia, and the Baltic region.\n\nThe earliest example of Gothic architecture in Germany is Maulbronn Monastery, a Romanesque Cistercian abbey in southwest Germany whose narthex was built in the early 12th century by an anonymous architect.\n\nBatalha Monastery (1386–1517) is a Dominican monastery in Batalha, Portugal. The monastery was built in the Flamboyant Gothic style to thank the Virgin Mary for the Portuguese victory over the Kingdom of Castile in the Battle of Aljubarrota in 1385.\n\nThe Gothic style appeared in palaces in France, including the Papal Palace in Avignon and the Palais de la Cité in Paris, close to Notre-Dame de Paris, begun in 1119, which was the principal residence of the French Kings until 1417. Most of the Palais de la Cité is gone, but two of the original towers along the Seine, of the towers, the vaulted ceilings of the Hall of the Men-at-Arms (1302), (now in the Conciergerie; and the original chapel, Sainte-Chapelle, can still be seen.\n\nThe largest civic building built in the Gothic style in France was the Palais des Papes (Palace of the Popes) constructed between 1252 and 1364, when the Popes fled the political chaos and wars enveloping Rome. Given the complicated political situation, it combined the functions of a church, a seat of government and a fortress.\n\nIn the 15th century, following the late Gothic or flamboyant period, elements of Gothic decoration borrowed from cathedrals began to appear in the town halls of northern France, in Flanders and in the Netherlands. The Hôtel de Ville of Compiègne has an imposing gothic bell tower, featuring a spire surrounded by smaller towers, and its windows are decorated with ornate \"accolades\" or ornamental arches. Similarly flamboyant town halls were found in Arras, Douai, and Saint-Quentin, Aisne, and in modern Belgium, in Brussels and Ghent and Bruges.\n\nNotable Gothic civil architecture in Spain includes the Silk Exchange in Valencia, Spain (1482–1548), a major marketplace, which features a main hall with twisting columns beneath its vaulted ceiling. Another Spanish Gothic landmark is the Palace of the Kings of Navarre in Olite (1269–1512), which combining the features of a palace and a fortress.\n\nThe first universities in Europe were closely associated with the Catholic church, and in the late 15th century they adapted variations of the Gothic style for their architecture. The Gothic style was adapted from English monasteries for use in the first colleges of Oxford University, including Magdalen College. It was also used at the University of Salamanca in Spain. The use of the late Gothic style at Oxford and Cambridge University inspired the picturesque Gothic architecture in U.S. colleges in the 19th and 20th century.\n\nBy the late Middle Ages university towns had grown in wealth and importance as well, and this was reflected in the buildings of some of Europe's ancient universities. Particularly remarkable examples still standing nowadays include the Collegio di Spagna in the University of Bologna, built during the 14th and 15th centuries; the Collegium Carolinum of the Charles University in Prague in Bohemia; the Escuelas mayores of the University of Salamanca in Spain; the chapel of King's College, Cambridge; or the Collegium Maius of the Jagiellonian University in Kraków, Poland.\n\nIn the 13th century, the design of the \"chateau fort\", or castle, was modified, based on the Byzantine and Moslem castles the French knights had seen during the Crusades. The new kind of fortification was called Phillipienne, after Philippe Auguste, who had taken part in the Crusades. The new fortifications were more geometric, usually square, with a high main \"donjon\" or tower, in the center, which could be defended even if the walls of the castle were captured. The Donjon of the Chateau de Vincennes, begun by Philip VI of France, was a good example. It was 52 meters high, the tallest military tower in Europe.\n\nIn the Phillipienne castle other towers, usually round were placed at the corners and along the walls, close enough together to support each other. The walls had two levels of walkways on the inside, an upper parapet with openings (\"Crénaux\") from which soldiers could watch or fire arrows on besiegers below; narrow openings (\"Merlons\") through which they could be sheltered as they fired arrows; and floor openings (\"Mâchicoulis\"), from which they could drop rocks, burning oil or other objects on the besiegers. The upper walls also had protected protruding balconies, \"Échauguettes\" and \"Bretéches\", from which soldiers could see what was happening at the corners or on the ground below. In addition, the towers and walls were pierced with narrow vertical slits, called \"Meurtriéres\", through which archers could fire arrows. In later castles the slits took the form of crosses, so that archers could fire \"arbalètes\", or crossbows, in different directions.\n\nCastles were surrounded by deep moat, spanned by a single drawbridge. The entrance was also protected by a grill of iron which could be opened and closed. The walls at the bottom were often sloping, and protected with earthen barriers. One good surviving example is the Château de Dourdan in the Seine-et-Marne department, near Nemours.\n\nAfter the end of the Hundred Years War (1337–1453), with improvements in artillery, the castles lost most of their military importance. They remained as symbols of the rank of their noble occupants; the narrowing openings in the walls were often widened into the windows of bedchambers and ceremonial halls. The tower of the Chateau of Vincennes became a royal residence.\n\nBeginning in the 16th century, as Renaissance architecture from Italy began to appear in France and other countries in Europe, the dominance of Gothic architecture began to wane. Nonetheless, new Gothic buildings, particularly churches, continued to be built; new Gothic churches built in Paris in this period included Saint-Merri (1520–1552); and Saint-Germain-l'Auxerrois; The first signs of classicism in Paris churches, at St-Gervais-et-St-Protais, did not appear until 1540. The largest new church, Saint-Eustache (1532–1560), rivaled Notre-Dame in size, 105 meters long, 44 meters wide, and 35 meters high. As construction of this church continued, elements of Renaissance decoration, including the system of classical orders of columns, were added to the design, making it an early Gothic-Renaissance hybrid.\n\nThe Gothic style began to be described as outdated, ugly and even barbaric. The term \"Gothic\" was first used as a pejorative description. Giorgio Vasari used the term \"barbarous German style\" in his 1550 \"Lives of the Artists\" to describe what is now considered the Gothic style. In the introduction to the \"Lives\" he attributed various architectural features to \"the Goths\" whom he held responsible for destroying the ancient buildings after they conquered Rome, and erecting new ones in this style. In the 17th century, Molière also mocked the Gothic style in the 1669 poem \"La Gloire\": \"...the insipid taste of Gothic ornamentation, these odious monstrosities of an ignorant age, produced by the torrents of barbarism...\" The dominant styles in Europe became in turn Italian Renaissance architecture, Baroque architecture, and the grand classicism of the Louis XIV style.\n\nNonetheless, Gothic architecture, usually churches or university buildings, continued to be built. Ireland was an island of Gothic architecture in the 17th and 18th centuries, with the construction of Derry Cathedral (completed 1633), Sligo Cathedral (c. 1730), and Down Cathedral (1790–1818) are other notable examples. In the 17th and 18th century several important Gothic buildings were constructed at Oxford University and Cambridge University, including Tom Tower at Christ Church, Oxford, by Christopher Wren (1681–82) It also appeared, in a whimsical fashion, in Horace Walpole's Twickenham villa, Strawberry Hill (1749–1776) The two western towers of Westminster Abbey were constructed between 1722 and 1745 by Nicholas Hawksmoor, opening a new period of Gothic Revival.\n\nIn England, partly in response to a philosophy propounded by the Oxford Movement and others associated with the emerging revival of 'high church' or Anglo-Catholic ideas during the second quarter of the 19th century, neo-Gothic began to become promoted by influential establishment figures as the preferred style for ecclesiastical, civic and institutional architecture. The appeal of this Gothic revival (which after 1837, in Britain, is sometimes termed Victorian Gothic), gradually widened to encompass \"low church\" as well as \"high church\" clients. This period of more universal appeal, spanning 1855–1885, is known in Britain as High Victorian Gothic.\n\nThe Houses of Parliament in London by Sir Charles Barry with interiors by a major exponent of the early Gothic Revival, Augustus Welby Pugin, is an example of the Gothic revival style from its earlier period in the second quarter of the 19th century. Examples from the \"High Victorian Gothic\" period include George Gilbert Scott's design for the Albert Memorial in London, and William Butterfield's chapel at Keble College, Oxford. From the second half of the 19th century onwards, it became more common in Britain for neo-Gothic to be used in the design of non-ecclesiastical and non-governmental buildings types. Gothic details even began to appear in working-class housing schemes subsidised by philanthropy, though given the expense, less frequently than in the design of upper and middle-class housing.\n\nThe middle of the 19th century was a period marked by the restoration, and in some cases modification, of ancient monuments and the construction of neo-Gothic edifices such as the nave of Cologne Cathedral and the Sainte-Clotilde of Paris as speculation of medieval architecture turned to technical consideration. London's Palace of Westminster, St. Pancras railway station, New York's Trinity Church and St. Patrick's Cathedral are also famous examples of Gothic Revival buildings. Such style also reached the Far East in the period, for instance, the Anglican St. John's Cathedral which was located at the centre of Victoria City in Central, Hong Kong.\n\nWhile some credit for this new ideation can reasonably be assigned to German and English writers, namely Johannes Vetter, Franz Mertens, and Robert Willis respectively, this emerging style's champion was Eugène Viollet-le-Duc, whose lead was taken by archaeologists, historians, and architects like Jules Quicherat, Auguste Choisy, and Marcel Aubert. In the last years of the 19th century, a trend among study in art history emerged in Germany that a building, as defined by Henri Focillon was an interpretation of space. When applied to Gothic cathedrals, historians and architects used to the dimensions of 17th and 18th Baroque or Neoclassical structures, were astounded by the height and extreme length of the cathedrals compared to its proportionally modest width. Goethe, in the preceding century, was mesmerised by the space within a Gothic church and succeeding historians like Georg Dehio, Walter Ueberwasser, Paul Frankl, and Maria Velte sought to rediscover the methodology used in their construction by making measurements and drawings of the buildings, and reading and making conjectures from documents and treaties pertaining to their construction.\n\nFrance had an abundance of original Gothic architecture, and therefore Gothic restoration largely replaced revival; few new buildings appeared in the Gothic style.\n\nThe French Revolution caused enormous damage to the Gothic architecture of France. Notre-Dame de Paris and other cathedrals and churches were closed and stripped of much of their decoration. In the 1830s, under the new King, Louis-Philippe, the historic and artistic value of Cathedrals and other Gothic monuments was recognized. Interest in the Gothic style was also stimulated by the huge success of the novel \"Notre Dame de Paris\" (known in English as \"The Hunchback of Notre Dame\") by Victor Hugo, published in 1831. The writings of the romantic author François-René de Chateaubriand also played an important part; he praised the religious sentiments inspired by of the Gothic style, and declaring that the Gothic style was \"the true architecture of our country,\" as important in the history of France as the forests of ancient Gaul. A Commission of Historic Monuments was created in 1837, and major program of cataloging, protection and restoration of Gothic monuments was organized by the writer Prosper Merimée and conducted largely by the architect Eugène Viollet-le-Duc.\n\nViollet-le-Duc conducted the restoration of Notre Dame de Paris and many other churches, as well as the Gothic fortifications of Carcasonne and imaginatively finishing the castle of Château de Pierrefonds, which had been only a ruin. He did not always limit himself to strict restoration of what had existed; in 1838 he designed he built a new Gothic west facade for the Parish church of Saint-Ouen in Rouen. The architects Franz Christian Gau and then Théodore Ballu built The Basilica of Sainte-Clotilde (1846–57), the first entirely neo-Gothic church in Paris.\n\nGerman Gothic revival was born in 1785–86 in the Dessau-Wörlitz Garden Realm, an early landscape garden built by near Potsdam, in the garden of the country house of Leopold III, Duke of Anhalt-Dessau, designed by Friedrich Wilhelm von Erdmannsdorff. The \"Gothic House\" displayed his collection of medieval stained glass. It inspired other Gothic-style country houses near Berlin and Kassel. \n\nIn Germany, the early 19th century Gothic revival had close ties to the movement of Romanticism. Beginning in 1814, the writer and historian Joseph Gorres launched a movement for the completion Cologne cathedral, which had been halted in 1473. The work was begun in 1842, and the two towers were completed in 1880, six hundred years after the cathedral was begun. Similar projects were undertaken for Ulm Minster, Heilsbronn Abbey and the Isartor gate in Munich. The Baroque decoration of these monuments was removed, and they were returned to at least an approximation, sometimes romanticized, of their original Gothic appearance. \n\nThe most picturesque example of Gothic revival is Neuschwanstein Castle (1869–1876) in Bavaria built by Ludwig II of Bavaria. For inspiration, Ludwig visited other Gothic reconstruction projects, Warburg and Château de Pierrefonds, a castle which had been reconstructed by Eugene Viollet-le-Duc. The highly romantic Neuschwanstein Castle inspired, among other later buildings, the Sleeping Beauty Castle in Disneyland (1955).\n\nThough Russia had no tradition of Gothic architecture, The Empress Catherine the Great chose the style in Tsaritsyno Palace, a large palace complex she constructed south of Moscow from 1776 until her death in 1796. Later, Czar Nicholas I of Russia commissioned the German romantic writer and painter Karl Friedrich Schinkel to design a Gothic chapel for his park at Peterhof Palace near St. Petersburg. It was completed in 1834.\n\nThe Gothic revival in Italy focused on the completion of earlier Cathedrals, which had been largely left to crumble. The most striking example was Milan Cathedral, which received a new facade in 1806–1813 which almost completely covered the original; it was designed by Giuseppe Zanoia and Carlo Amati. In 1888 it received a Gothic crown on the front facade by Giuseppe Brentano. The famous spires and pinnacles of the building date to the 19th century.\n\nA small but notable example of Gothic revival in Italy is the Pedrocchi Café in Padua, Italy (1837), by Giuseppe Jappelli. It borrowed the Venetian Gothic style plus the pinnacles of Milan Cathedral. It became famous as the meeting place of Italian patriots rebelling against Austrian rule in 1848.\n\nOne of the earliest and most highly decorated Gothic Revival churches in North America is the Notre-Dame Basilica in Montreal. Built in 1784–1795, on the site of a 17th-century church. The vaults are colored deep blue and decorated with golden stars, and the rest of the sanctuary is decorated in blues, azures, reds, purples, silver, and gold. The stained glass windows depict scenes from the religious history of Montreal.\nThe English Gothic revival style was imported from England beginning in the 1830s, and was used primarily for Anglican and Catholic churches, and for university buildings. The best-known Gothic revival buildings in Canada are those Centre Bloc and the Victoria Tower of the Parliament of Canada on Parliament Hill in Ottawa, by Thomas Fuller (1859–1927). The architect William Thomas constructed St. Michael's Cathedral in Toronto (1845–48), and several other Gothic revival churches in the Province of Ontario.\n\nThe Gothic style was entirely contrary to the early Puritan architecture in the United States, which valued simplicity and a lack of ornament. It was also contrary to the Neoclassical style used for early U.S. government buildings. However, in the mid to late 19th century, large cathedrals and churches began to be built in the Gothic Revival style, modeled after European cathedrals. University buildings also adopted the style, using English colleges as a model. In the early 20th century, Gothic decorative elements appeared on the towers of the new skyscrapers, notably the Woolworth Building by Cass Gilbert in New York and the Chicago Tribune Building in Chicago, by John Mead Howells and Raymond Hood, symbolizing the role of these buildings as \"cathedrals of commerce.\"\n\nGothic architecture also inspired a popular style of residential architecture in the United States in the mid-19th century. It was known as Carpenter Gothic, and featured pointed lancet windows, a steep gable roof, and other simple Gothic elements, embellished with wooden ornament cut with a jigsaw or a scroll saw in Gothic designs. Sometimes the wooden trim was attached to a brick house. It became a popular style for wooden churches in rural communities. An example of Carpenter Gothic house, the American Gothic House, in Eldon, Iowa appears in the background of the celebrated painting \"American Gothic\" by Grant Wood, and gave the painting its name.\n\nIn the 19th century the Gothic style of church architecture was transported to Latin America by the Spanish and Portuguese, where it became a symbol of Christian values and traditions. It was also exported to Asia, largely for the churches of the European settlements there. The English Gothic style was a major influence in Australia, both for churches and civic buildings. Perth Town Hall (1867–70) was the only town hall in Australia (or perhaps anywhere) constructed by convicts.\n\nThe Basilica of the Twenty=Six Holy Martyrs (1879) was an enlargement of the first Christian church in Japan, which had been built in 1864. It survived the dropping of an atomic bomb on Nagasaki in August 1945. The plan was taken from a Belgian model, and the stained glass was imported from France.\n\nThe Church of the Holy Sacrament in Guadalajara, Mexico was begun in 1897, but construction was halted by the Mexican Revolution, and it was not completed until 1972.\n\nShanghai Cathedral in China (1906–1910) was originally built by the Jesuits as their primary church in the trading port of Shanghai. In 1966, at the opening of the Cultural Revolution, Red Guards from Beijing vandalized the cathedral, tearing down its spires and ceiling, and smashing its roughly 300 square meters of stained glass. For the next ten years the Cathedral served as a state-owned grain warehouse. It was reopened in 1978 and the spires were restored in the 1980s.\n\nIn India, the city of Mumbai (formerly Bombay) has a rich collection of neo-Gothic architecture. The style is found in the University buildings, the courthouse, and in the Chhatrapati Shivaji Terminus railway station (former Victoria Terminus), the main railway station.\n\nThe roots of the Gothic style lie in those towns that, since the 11th century, had been enjoying increased prosperity and growth, began to experience more and more freedom from traditional feudal authority. At the end of the 12th century, Europe was divided into a multitude of city states and kingdoms. The area encompassing modern Germany, southern Denmark, the Netherlands, Belgium, Luxembourg, Switzerland, Liechtenstein, Austria, Slovakia, Czech Republic and much of northern Italy (excluding Venice and Papal State) was nominally part of the Holy Roman Empire, but local rulers exercised considerable autonomy under the system of Feudalism. France, Denmark, Poland, Hungary, Portugal, Scotland, Castile, Aragon, Navarre, Sicily and Cyprus were independent kingdoms, as was the Angevin Empire, whose Plantagenet kings ruled England and large domains in what was to become modern France. Norway came under the influence of England, while the other Scandinavian countries and Poland were influenced by trading contacts with the Hanseatic League. Angevin kings brought the Gothic tradition from France to Southern Italy, while Lusignan kings introduced French Gothic architecture to Cyprus. Gothic art is sometimes viewed as the art of the era of feudalism but also as being connected to change in medieval social structure, as the Gothic style of architecture seemed to parallel the beginning of the decline of feudalism. Nevertheless, the influence of the established feudal elite can be seen in the Chateaux of French lords and in those churches sponsored by feudal lords.\n\nThroughout Europe at this time there was a rapid growth in trade and an associated growth in towns, and they would come to be predominate in Europe by the end of the 13th century. Germany and the Low Countries had large flourishing towns that grew in comparative peace, in trade and competition with each other or united for mutual weal, as in the Hanseatic League. Civic building was of great importance to these towns as a sign of wealth and pride. England and France remained largely feudal and produced grand domestic architecture for their kings, dukes and bishops, rather than grand town halls for their burghers. Viollet-le-Duc contended that the blossoming of the Gothic style came about as a result of growing freedoms in construction professions.\n\nThe primary use of the Gothic style is in religious structures, naturally leading it to an association with the Church and it is considered to be one of the most formal and coordinated forms of the physical church, thought of as being the physical residence of God on Earth. According to Hans Sedlmayr, it was \"even the considered the temporal image of Paradise, of the New Jerusalem.\" The horizontal and vertical scope of the Gothic church, filled with the light thought of as a symbol of the grace of God admitted into the structure via the style's iconic windows are among the very best examples of Christian architecture. Grodecki's \"Gothic Architecture\" also notes that the glass pieces of various colors that make up those windows have been compared to \"precious stones encrusting the walls of the New Jerusalem,\" and that \"the numerous towers and pinnacles evoke similar structures that appear in the visions of Saint John.\" Another idea, held by Georg Dehio and Erwin Panofsky, is that the designs of Gothic followed the current theological scholastic thought. The PBS show NOVA explored the influence of the Bible in the dimensions and design of some cathedrals.\n\nFrom the 10th to the 13th century, Romanesque architecture had become a pan-European style and manner of construction, affecting buildings in countries as far apart as Ireland and Croatia, and Sweden and Sicily. The same wide geographic area was then affected by the development of Gothic architecture, but the acceptance of the Gothic style and methods of construction differed from place to place, as did the expressions of Gothic taste. The proximity of some regions meant that modern country borders did not define divisions of style. On the other hand, some regions such as England and Spain produced defining characteristics rarely seen elsewhere, except where they have been carried by itinerant craftsmen, or the transfer of bishops. Many different factors like geographical/geological, economic, social, or political situations caused the regional differences in the great abbey churches and cathedrals of the Romanesque period that would often become even more apparent in the Gothic. For example, studies of the population statistics reveals disparities such as the multitude of churches, abbeys, and cathedrals in northern France while in more urbanised regions construction activity of a similar scale was reserved to a few important cities. Such an example comes from Roberto López, wherein the French city of Amiens was able to fund its architectural projects whereas Cologne could not because of the economic inequality of the two. This wealth, concentrated in rich monasteries and noble families, would eventually spread certain Italian, Catalan, and Hanseatic bankers. This would be amended when the economic hardships of the 13th century were no longer felt, allowing Normandy, Tuscany, Flanders, and the southern Rhineland to enter into competition with France.\n\nThe local availability of materials affected both construction and style. In France, limestone was readily available in several grades, the very fine white limestone of Caen being favoured for sculptural decoration. England had coarse limestone and red sandstone as well as dark green Purbeck marble which was often used for architectural features. In northern Germany, Netherlands, northern Poland, Denmark, and the Baltic countries local building stone was unavailable but there was a strong tradition of building in brick. The resultant style, Brick Gothic – called in Poland and in Germany and Scandinavia – is also associated with the Hanseatic League. In Italy, stone was used for fortifications, so brick was preferred for other buildings. Because of the extensive and varied deposits of marble, many buildings were faced in marble, or were left with undecorated façade so that this might be achieved at a later date. The availability of timber also influenced the style of architecture, with timber buildings prevailing in Scandinavia. Availability of timber affected methods of roof construction across Europe. It is thought that the magnificent hammerbeam roofs of England were devised as a direct response to the lack of long straight seasoned timber by the end of the Medieval period, when forests had been decimated not only for the construction of vast roofs but also for ship building.\n\nThe pointed arch, one of the defining attributes of Gothic, was earlier incorporated into Islamic architecture following the Islamic conquests of Roman Syria and the Sassanid Empire in the 7th century. The pointed arch and its precursors had been employed in Late Roman and Sassanian architecture; within the Roman context, evidenced in early church building in Syria and occasional secular structures, like the Roman Karamagara Bridge; in Sassanid architecture, in the parabolic and pointed arches employed in palace and sacred construction. Use of the pointed arch seems to have taken off dramatically after its incorporation into Islamic architecture. It begins to appear throughout the Islamic world in close succession after its adoption in the late Umayyad or early Abbasid period. Some examples are the Al-Ukhaidir Palace (775 AD), the Abbasid reconstruction of the Al-Aqsa mosque in 780 AD, the Ramlah Cisterns (789 AD), the Great Mosque of Samarra (851 AD), and the Mosque of Ibn Tulun (879 AD) in Cairo. It also appears in one of the early reconstructions of the Great Mosque of Kairouan in Tunisia, and the Mosque–Cathedral of Córdoba in 987 AD. David Talbot Rice points out that, \"The pointed arch had already been used in Syria, but in the mosque of Ibn Tulun we have one of the earliest examples of its use on an extensive scale, some centuries before it was exploited in the West by the Gothic architects.\"\n\nIncreasing military and cultural contacts with the Muslim world, including the Norman conquest of Islamic Sicily in 1090, the Crusades (beginning 1096), and the Islamic presence in Spain, may have influenced Medieval Europe's adoption of the pointed arch, although this hypothesis remains controversial. Certainly, in those parts of the Western Mediterranean subject to Islamic control or influence, rich regional variants arose, fusing Romanesque and later Gothic traditions with Islamic decorative forms, for example in Monreale and Cefalù Cathedrals, the Alcázar of Seville, and Teruel Cathedral.\n\nA number of scholars have cited the Armenian Cathedral of Ani, completed 1001 or 1010, as a possible influence on the Gothic, especially due to its use of pointed arches and cluster piers. However, other scholars such as Sirarpie Der Nersessian, who rejected this notion as she argued that the pointed arches did not serve the same function of supporting the vault. Lucy Der Manuelian contends that some Armenians (historically documented as being in Western Europe in the Middle Ages) could have brought the knowledge and technique employed at Ani to the west.\n\nThe view held by the majority of scholars however is that the pointed arch evolved naturally in Western Europe as a structural solution to a technical problem, with evidence for this being its use as a stylistic feature in Romanesque French and English churches.\n\n\n\n\n\n"}
{"id": "42873698", "url": "https://en.wikipedia.org/wiki?curid=42873698", "title": "Groundhog Technologies", "text": "Groundhog Technologies\n\nGroundhog Technologies is a privately held company founded in 2001 and is headquartered in Cambridge, Massachusetts, USA. As a spin-off of MIT Media Lab, it was a semi-finalist in MIT's $50k Entrepreneurship Competition in 2000 and was incorporated the following year. The company received the first round of financing from major Japanese corporations and their venture capital arms in November 2002: Marubeni, Yasuda Enterprise Development and Japan Asia Investment Co. It received second round of financing in 2004 and since then has become self-sustainable.\n\nThe company’s products are built on top of its Mobility Intelligence Platform, which analyzes the locations, Quality of Experience, context, and lifestyles of subscribers in mobile operator’s network. The intelligence about geolocation is then applied to improve subscribers’ experience and enable applications such as geomarketing and geotargeting. The Company has leveraged its platform to enable operators to address the advertising and data monetization opportunity both internally and in partnership with third party retailers, advertisers, and ad networks.\n\nGroundhog Technologies launched its Mobility Intelligence platform based on Chaos Theory and multi-dimensional modeling. The application of Chaos Theory gave rise to the company’s mathematical models of subscribers' mobility and usage behavior, which can be used for different applications such as by mobile operators to optimize networks according to the user demands.\nAccording to Chaos Theory, some seemly random or chaotic signals may be converted to analyze in phase space which can reveal the patterns behind it. The cases of most interest arise when the chaotic behavior shows patterns around an attractor in the phase space. Based on the attractor in the phase space, data can be utilized from different space, time, and individuals for modeling and indoor geolocation.\n\nIt is also found that the dimensional structure and characteristics of phase space can naturally neutralize the bias of positioning (based on techniques such as triangulation or trilateration) caused by reasons such as multipath. That is, although each input is biased in some way, the observation from different dimensions and angles are biased in different ways. Combining multi-dimensional input in the phase space, based on the Law of Large Numbers it can average out the bias with different samples through dimensions, time, and individuals.\n\n"}
{"id": "27613250", "url": "https://en.wikipedia.org/wiki?curid=27613250", "title": "Hailong Market", "text": "Hailong Market\n\nHailong Market is one of five major electronics markets in Zhongguancun, Beijing.\n\nHailong Market opened in December 1999.\n\n\n"}
{"id": "37575047", "url": "https://en.wikipedia.org/wiki?curid=37575047", "title": "High frame rate", "text": "High frame rate\n\nIn motion picture technology—either film or video—high frame rate (HFR) refers to higher frame rates than typical prior practice.\n\nThe frame rate for motion picture film cameras was typically 24 frames per second (fps) with multiple flashes on each frame during projection to prevent flicker. Analog television and video employed interlacing where only half of the image (known as a video field) was recorded and played back/refreshed at once but at twice the rate of what would be allowed for progressive video of the same bandwidth, resulting in smoother playback, as opposed to progressive video which is more similar to how celluloid works. The field rate of analog television and video systems was typically 50 or 60 fields per second. Usage of frame rates higher than 24 FPS for feature motion pictures and higher than 30 FPS for other applications are emerging trends in the 21st century.\n\nIn early cinema history, there was no standard frame rate established. Thomas Edison's early films were shot at 40 fps, while the Lumière Brothers used 16 fps. This had to do with a combination of the use of a hand crank rather than a motor, which created variable frame rates because of the inconsistency of the cranking of the film through the camera. After the introduction of synch sound recording, 24 fps became the industry standard frame rate for capture and projection of motion pictures. 24 fps was chosen because it was the minimum frame rate that would produce adequate sound quality. This was done because film was expensive, and using the lowest possible frame rate would use the least amount of film.\n\nA few film formats have experimented with frame rates higher than the 24 fps standard. The original 3-strip Cinerama features of the 1950s ran at 26 fps. The first two Todd-AO 70mm features, \"Oklahoma!\" (1955) and \"Around the World in 80 Days\" (1956) were shot and projected at 30 fps. Douglas Trumbull's 70mm Showscan film format operated at 60 fps.\n\nThe IMAX HD (high definition in this case meaning high definition film stock, as 70mm IMAX is the highest resolution motion picture image in the world) film \"Momentum\", presented at Seville Expo '92, was shot and projected at 48 fps. IMAX HD has also been used in film-based theme park attractions, including Disney's Soarin' Over California.\n\nThe proposed Maxivision 48 format ran 35mm film at 48 fps, but was never commercially deployed.\n\nDigital Cinema Initiatives has published a document outlining recommended practice for high frame rate digital cinema. This document outlines the frame rates and resolutions that can be used in high frame rate digital theatrical presentations with currently available equipment.\n\nPeter Jackson's \"The Hobbit\" film series, beginning with \"\" in December 2012, used a shooting and projection frame rate of 48 frames per second, becoming the first feature film with a wide release to do so. Its 2013 sequel, \"\" and 2014 sequel, \"\", followed suit. All films also have versions which are converted and projected at 24 fps.\n\nIn 2016, Ang Lee released \"Billy Lynn's Long Halftime Walk\". Unlike \"The Hobbit\" trilogy, which used 48 frames per second, the picture shot and projected selected scenes in 120 frames per second, which is five times faster than the 24 frames per second standard used in Hollywood.\n\nRocketJump's \"Video Game High School\" was the first web series to use HFR, and the first content shot and edited in a mixed frame rate. The series, which follows the lives of high school students in a world where gamers are revered as pro-athletes, adopted HFR in its second season, using the standard 24 frames per second for real world interactions, and 48 frames per second for \"in-game\" action sequences. Although the content is available on YouTube and Netflix, it can only be viewed in mixed frame rate using a special player on RocketJump's website.\n\nOther filmmakers who intend to use the high frame rate format include James Cameron in his \"Avatar\" sequels and Andy Serkis in his adaptation of George Orwell's \"Animal Farm\".\n\nFrame rates higher than 24 FPS are quite common in TV drama and in-game cinematics.\n\nSome media players are capable of showing HFR content and almost all computers and smart devices can handle this format as well. In recent years some televisions have the ability to take normal 24 fps videos and \"convert\" them to HFR content by interpolating the motion of the picture, effectively creating new computer generated frames between each two key frames and running them at higher refresh rate. Some computer programs allow for that as well but with higher precision and better quality as the computing power of the PC has grown.\n\nMotion interpolation may cause some artifacts, as a result of the computer \"guessing\" the frames wrong.\n\n"}
{"id": "28851000", "url": "https://en.wikipedia.org/wiki?curid=28851000", "title": "Housing industry", "text": "Housing industry\n\nThe housing industry is the development, construction, and sale of homes. Its interests are represented in the United States by the National Association of Home Builders (NAHB). In Australia the trade association representing the residential housing industry is the Housing Industry Association. It also refers to the housing market which means the supply and demand for houses, usually in a particular country or region.Housing market includes features as supply of housing, demand for housing, house prices, rented sector and government intervention in the Housing market.\n\n"}
{"id": "75654", "url": "https://en.wikipedia.org/wiki?curid=75654", "title": "Hyperthermia", "text": "Hyperthermia\n\nHyperthermia is elevated body temperature due to failed thermoregulation that occurs when a body produces or absorbs more heat than it dissipates. Extreme temperature elevation then becomes a medical emergency requiring immediate treatment to prevent disability or death.\nThe most common causes include heat stroke and adverse reactions to drugs. The former is an acute temperature elevation caused by exposure to excessive heat, or combination of heat and humidity, that overwhelms the heat-regulating mechanisms. The latter is a relatively rare side effect of many drugs, particularly those that affect the central nervous system. Malignant hyperthermia is a rare complication of some types of general anesthesia.\n\nHyperthermia differs from fever in that the body's temperature set point remains unchanged. The opposite is hypothermia, which occurs when the temperature drops below that required to maintain normal metabolism. The term is from Greek ὑπέρ, \"hyper\", meaning \"above\" or \"over\", and θέρμος, \"thermos\", meaning \"hot\".\n\nIn humans, hyperthermia is defined as a temperature greater than , depending on the reference used, that occurs without a change in the body's temperature set point.\n\nThe normal human body temperature can be as high as in the late afternoon. Hyperthermia requires an elevation from the temperature that would otherwise be expected. Such elevations range from mild to extreme; body temperatures above can be life-threatening.\n\nAn early stage of hyperthermia can be \"heat exhaustion\" (or \"heat prostration\" or \"heat stress\"), whose symptoms include heavy sweating, rapid breathing and a fast, weak pulse. If the condition progresses to heat stroke, then hot, dry skin is typical as blood vessels dilate in an attempt to increase heat loss. An inability to cool the body through perspiration may cause the skin to feel dry.\n\nOther signs and symptoms vary. Accompanying dehydration can produce nausea, vomiting, headaches, and low blood pressure and the latter can lead to fainting or dizziness, especially if the standing position is assumed quickly.\n\nIn severe heat stroke, there may be confused, hostile, or seemingly intoxicated behavior. Heart rate and respiration rate will increase (tachycardia and tachypnea) as blood pressure drops and the heart attempts to maintain adequate circulation. The decrease in blood pressure can then cause blood vessels to contract reflexively, resulting in a pale or bluish skin color in advanced cases. Young children, in particular, may have seizures. Eventually, organ failure, unconsciousness and death will result.\n\nHeat stroke occurs when thermoregulation is overwhelmed by a combination of excessive metabolic production of heat (exertion), excessive environmental heat, and insufficient or impaired heat loss, resulting in an abnormally high body temperature. In severe cases, temperatures can exceed . Heat stroke may be \"non-exertional\" (classic) or \"exertional\".\n\nSignificant physical exertion in hot conditions can generate heat beyond the ability to cool, because, in addition to the heat, humidity of the environment may reduce the efficiency of the body's normal cooling mechanisms. Human heat-loss mechanisms are limited primarily to sweating (which dissipates heat by evaporation, assuming sufficiently low humidity) and vasodilation of skin vessels (which dissipates heat by convection proportional to the temperature difference between the body and its surroundings, according to Newton's law of cooling). Other factors, such as insufficient water intake, consuming alcohol, or lack of air conditioning, can worsen the problem.\n\nThe increase in body temperature that results from a breakdown in thermoregulation affects the body biochemically. Enzymes involved in metabolic pathways within the body such as cellular respiration fail to work effectively at higher temperatures, and further increases can lead them to denature, reducing their ability to catalyse essential chemical reactions. This loss of enzymatic control affects the functioning of major organs with high energy demands such as the heart and brain.\n\nSituational heat stroke occurs in the absence of exertion. It mostly affects the young and elderly. In the elderly in particular, it can be precipitated by medications that reduce vasodilation and sweating, such as anticholinergic drugs, antihistamines, and diuretics. In this situation, the body's tolerance for high environmental temperature may be insufficient, even at rest.\n\nHeat waves are often followed by a rise in the death rate, and these 'classical hyperthermia' deaths typically involve the elderly and infirm. This is partly because thermoregulation involves cardiovascular, respiratory and renal systems which may be inadequate for the additional stress because of the existing burden of aging and disease, further compromised by medications. During the July 1995 heat wave in Chicago, there were at least 700 heat-related deaths. The strongest risk factors were being confined to bed, and living alone, while the risk was reduced for those with working air conditioners and those with access to transportation. Even then, reported deaths may be underestimates as diagnosis can be misclassified as stroke or heart attack.\n\nSome drugs cause excessive internal heat production. The rate of drug-induced hyperthermia is higher where use of these drugs is higher.\n\n\nThose working in industry, in the military, or as first responders may be required to wear personal protective equipment (PPE) against hazards such as chemical agents, gases, fire, small arms and even Improvised Explosive Devices (IEDs). PPE includes a range of hazmat suits, firefighting turnout gear, body armor and bomb suits, among others. Depending on design, the wearer may be encapsulated in a microclimate, due to an increase in thermal resistance and decrease in vapor permeability. As physical work is performed, the body’s natural thermoregulation (i.e., sweating) becomes ineffective. This is compounded by increased work rates, high ambient temperature and humidity levels, and direct exposure to the sun. The net effect is that desired protection from some environmental threats inadvertently increases the threat of heat stress.\n\nThe effect of PPE on hyperthermia has been noted in fighting the 2014 Ebola virus epidemic in Western Africa. Doctors and healthcare workers were only able to work 40 minutes at a stretch in their protective suits, fearing heat strokes.\n\nOther rare causes of hyperthermia include thyrotoxicosis and an adrenal gland tumor, called pheochromocytoma, both of which can cause increased heat production. Damage to the central nervous system, from brain hemorrhage, status epilepticus, and other kinds of injury to the hypothalamus can also cause hyperthermia.\n\nA fever occurs when the core temperature is set higher, through the action of the pre-optic region of the anterior hypothalamus. For example, in response to a bacterial or viral infection, certain white blood cells within the blood will release pyrogens which have a direct effect on the anterior hypothalamus, causing body temperature to rise, much like raising the temperature setting on a thermostat.\n\nIn contrast, hyperthermia occurs when the body temperature rises without a change in the heat control centers.\n\nSome of the gastrointestinal symptoms of acute exertional heat stroke, such as vomiting, diarrhea, and gastrointestinal bleeding, may be caused by barrier dysfunction and subsequent endotoxemia. Ultraendurance athletes have been found to have significantly increased plasma endotoxin levels. Endotoxin stimulates many inflammatory cytokines, which in turn may cause multiorgan dysfunction. Experimentally, monkeys treated with oral antibiotics prior to induction of heat stroke do not become endotoxemic.\n\nThere is scientific support for the concept of a temperature set point; that is, maintenance of an optimal temperature for the metabolic processes that life depends on. Nervous activity in the preoptic-anterior hypothalamus of the brain triggers heat losing (sweating, etc.) or heat generating (shivering and muscle contraction, etc.) activities through stimulation of the autonomic nervous system. The pre-optic anterior hypothalamus has been shown to contain warm sensitive, cool sensitive, and temperature insensitive neurons, to determine the body's temperature setpoint. As the temperature that these neurons are exposed to rises above 37 °C, the rate of electrical discharge of the warm-sensitive neurons increases progressively. Cold-sensitive neurons increase their rate of electrical discharge progressively below 37 °C.\n\nHyperthermia is generally diagnosed by the combination of unexpectedly high body temperature and a history that supports hyperthermia instead of a fever. Most commonly this means that the elevated temperature has occurred in a hot, humid environment (heat stroke) or in someone taking a drug for which hyperthermia is a known side effect (drug-induced hyperthermia). The presence of signs and symptoms related to hyperthermia syndromes, such as extrapyramidal symptoms characteristic of neuroleptic malignant syndrome, and the absence of signs and symptoms more commonly related to infection-related fevers, are also considered in making the diagnosis.\n\nIf fever-reducing drugs lower the body temperature, even if the temperature does not return entirely to normal, then hyperthermia is excluded.\n\nWhen ambient temperature is excessive, humans and many animals cool themselves below ambient by evaporative cooling of sweat (or other aqueous liquid; saliva in dogs, for example); this helps prevent potentially fatal hyperthermia. The effectiveness of evaporative cooling depends upon humidity. Wet-bulb temperature, which takes humidity into account, or more complex calculated quantities such as wet-bulb globe temperature (WBGT), which also takes solar radiation into account, give useful indications of the degree of heat stress and are used by several agencies as the basis for heat-stress prevention guidelines. (Wet-bulb temperature is essentially the lowest skin temperature attainable by evaporative cooling at a given ambient temperature and humidity.)\n\nA sustained wet-bulb temperature exceeding is likely to be fatal even to fit and healthy people unclothed in the shade next to a fan; at this temperature, environmental heat gain instead of loss occurs. , wet-bulb temperatures only very rarely exceeded anywhere, although significant global warming may change this.\n\nIn cases of heat stress caused by physical exertion, hot environments, or protective equipment, prevention or mitigation by frequent rest breaks, careful hydration, and monitoring body temperature should be attempted. However, in situations demanding one is exposed to a hot environment for a prolonged period or must wear protective equipment, a personal cooling system is required as a matter of health and safety. There is a variety of active or passive personal cooling systems; these can be categorized by their power sources and whether they are person- or vehicle-mounted.\n\nBecause of the broad variety of operating conditions, these devices must meet specific requirements concerning their rate and duration of cooling, their power source, and their adherence to health and safety regulations. Among other criteria are the user's need for physical mobility and autonomy. For example, active-liquid systems operate by chilling water and circulating it through a garment; the skin surface area is thereby cooled through conduction. This type of system has proven successful in certain military, law enforcement, and industrial applications. Bomb-disposal technicians wearing special suits to protect against improvised explosive devices (IEDs) use a small, ice-based chiller unit that is strapped to one leg; a liquid-circulating garment, usually a vest, is worn over the torso to maintain a safe core body temperature. By contrast, soldiers traveling in combat vehicles can face microclimate temperatures in excess of 65 °C and require a multiple-user, vehicle-powered cooling system with rapid connection capabilities. Requirements for hazmat teams, the medical community, and workers in heavy industry vary further.\n\nThe underlying cause must be removed. Mild hyperthemia caused by exertion on a hot day may be adequately treated through self-care measures, such as increased water consumption and resting in a cool place. Hyperthermia that results from drug exposure requires prompt cessation of that drug, and occasionally the use of other drugs as counter measures.\n\nAntipyretics (e.g., acetaminophen, aspirin, other nonsteroidal anti-inflammatory drugs) have no role in the treatment of heatstroke because antipyretics interrupt the change in the hypothalamic set point caused by pyrogens; they are not expected to work on a healthy hypothalamus that has been overloaded, as in the case of heatstroke. In this situation, antipyretics actually may be harmful in patients who develop hepatic, hematologic, and renal complications because they may aggravate bleeding tendencies.\n\nWhen body temperature is significantly elevated, mechanical cooling methods are used to remove heat and to restore the body's ability to regulate its own temperatures. Passive cooling techniques, such as resting in a cool, shady area and removing clothing can be applied immediately. Active cooling methods, such as sponging the head, neck, and trunk with cool water, remove heat from the body and thereby speed the body's return to normal temperatures. Drinking water and turning a fan or dehumidifying air conditioning unit on the affected person may improve the effectiveness of the body's evaporative cooling mechanisms (sweating).\n\nSitting in a bathtub of tepid or cool water (immersion method) can remove a significant amount of heat in a relatively short period of time. It was once thought that immersion in very cold water is counterproductive, as it causes vasoconstriction in the skin and thereby prevents heat from escaping the body core. However, a British analysis of various studies stated: \"this has never been proven experimentally. Indeed, a recent study using normal volunteers has shown that cooling rates were fastest when the coldest water was used.\" The analysis concluded that iced water immersion is the most-effective cooling technique for exertional heat stroke. No superior cooling method has been found for \"non-exertional heat stroke\". Thus, aggressive \"ice-water\" immersion remains the gold standard for \"life-threatening heat stroke\".\n\nWhen the body temperature reaches about , or if the affected person is unconscious or showing signs of confusion, hyperthermia is considered a medical emergency that requires treatment in a proper medical facility. In a hospital, more aggressive cooling measures are available, including intravenous hydration, gastric lavage with iced saline, and even hemodialysis to cool the blood.\n\nThe frequency of environmental hyperthermia can vary significantly from year to year depending on factors such as heat waves. Statistically, outdoor workers, including agricultural workers, are at increased risk of experiencing heat stress and the resulting negative health effects. Between 1992 and 2006 in the United States, 68 crop workers died from heat stroke, representing a rate 20 times that of US civilian workers overall.\n\nIn India, hundreds die every year from summer heat waves, including more than 2,500 in the year 2015. Later that same summer, the 2015 Pakistani heat wave killed about 2,000 people. An extreme 2003 European heat wave caused tens of thousands of deaths.\n\nHyperthermia can also be deliberately induced using drugs or medical devices and is being studied and applied in clinical routine as a treatment of some kinds of cancer.\n\n\n"}
{"id": "25898547", "url": "https://en.wikipedia.org/wiki?curid=25898547", "title": "Jay Westerdal", "text": "Jay Westerdal\n\nJay Westerdal (born 1978) is a domainer and entrepreneur, best known for his work creating DomainTools.com, a web service that looks up historical ownership of a website. The whois service was integrated into Google's onebox in May 2008. He later sold the company in 2008 for a reported $16–$18 million. He is a technology blogger. \nWesterdal started Name Intelligence/DomainTools in 2002 in his parent's garage. In May 2005, Jay started the domain conference \"Domain RoundTable\". He later sold DomainTools in 2008 to Thought Convergence, Inc. The following year, after being acquired, he left TCI.\n\nWesterdal's personal blog covers a wide range of topics, focusing mainly on technology, his mobile lifestyle, and search engine optimization from a personal perspective, in contrast to the DomainTools blog, where he wrote in an official capacity. He contributed to the EPP Protocol <nowiki>RFC 4930</nowiki>.\n"}
{"id": "32476781", "url": "https://en.wikipedia.org/wiki?curid=32476781", "title": "John Fritz Medal", "text": "John Fritz Medal\n\nThe John Fritz Medal has been awarded annually since 1902 by the American Association of Engineering Societies for \"outstanding scientific or industrial achievements\". The medal was created for the 80th birthday of John Fritz, who lived between 1822 and 1913. \n\nThe John Fritz Medal is described by some as the Nobel Prize for engineering.\"Lynn Beedle, `a world engineer, dies at 85.,\" News Article at \"lehigh.edu,\" October 30, 2003. Accessed 2017-09-13. This prestigious award is given annually for notable scientific or industrial achievements. It is granted to living people, but also posthumous. Since its initiation in 1902 it has been not awarded four years. \n\nThe John Fritz Medal board once consisted of sixteen representatives is four national societies in the fields of civil engineering, mining, metallurgical engineering, mechanical engineering and electrical engineering. \n\nAmong the most notorious winners are Alexander Graham Bell, George Westinghouse, Orville Wright, and Charles F. “Boss” Kettering. \n\n"}
{"id": "49298589", "url": "https://en.wikipedia.org/wiki?curid=49298589", "title": "Just10", "text": "Just10\n\nJust10 Incorporated (Just10), is an advertising-free private social network where users are limited to having just 10 friends, and all content, posts, and messages are strictly private, and are permanently deleted in 10 days. The company was started by privacy advocate and technology entrepreneur Frederick Ghahramani.\n\n"}
{"id": "5130190", "url": "https://en.wikipedia.org/wiki?curid=5130190", "title": "Karatmeter", "text": "Karatmeter\n\nThe Karatmeter is a scientific device bought and named by India-based Titan Company Limited (TATA Group) which uses X-rays to give an exact reading of the purity of gold. Due to its very high precision and fast result, X-ray analysis has been adopted by international agencies in India as part of the certification process used to hallmark gold. It is an accurate, non-destructive means of testing the purity of gold and other related elements. X-Ray method of Gold analysis gives the purity of gold upto 10-12 microns and hence it gives the analysis of coating only.\n\nUsing this technique, the precise percentage or karat (of karat) in a solid piece of jewellery can be determined in 30 seconds. It also accurately determines the element composition of all types of gold, white gold, platinum, silver, palladium, rhodium and related alloys.\n\nEnergy dispersive X-Ray fluorescence technology (ED-XRF) is a simple, most accurate and most economic analytical methods for the determination of the chemical composition of many types of materials. It is non-destructive and reliable, requires very little sample preparation and is suitable for solid, liquid and powdered samples. It can be used for a wide range of elements, from Chlorine (17) to Uranium (92), and provides detection limits at the sub-ppm level.\n\nThere are many models of Gold Purity Testing machines available - from portable (light weight) to industrial grade machines.\n\nApart from X-ray spectrometer technique, other older traditional methods are using the TouchStone and Acid to test the gold purity. But TouchStone and Acid are destructive testing - a tiny sample of gold is cut and then tested. The sample is rubbed on TouchStone and a drop of acid is put on it and the goldsmith observes the residue using a magnifying lens. Based on experience, gold smith can determine purity of sample.\n"}
{"id": "20587985", "url": "https://en.wikipedia.org/wiki?curid=20587985", "title": "List of Korean inventions and discoveries", "text": "List of Korean inventions and discoveries\n\nThis is a list of Korean inventions and discoveries. The Koreans have made contributions across a number of scientific and technological domains. In particular, the country has played a role in the modern Digital Revolution through its large electronics industry with a number of modern revolutionary and widespread technologies in fields such as electronics and robotics introduced by Korean scientists, inventors, and entrepreneurs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "380375", "url": "https://en.wikipedia.org/wiki?curid=380375", "title": "Lyman Gilmore", "text": "Lyman Gilmore\n\nLyman Wiswell Gilmore, Jr. (June 11, 1874 – February 18, 1951) was an aviation pioneer. In Grass Valley, California, he built a steam-powered airplane and claimed that he flew it on May 15, 1902. Due to the requirement of a heavy boiler and the dependency on coal as a power source, the flights would have been unsustainable. Records and evidence relating to his claim were lost in a 1935 hangar fire.\n\nGilmore, in a 1936 interview, reported a successful tethered glider flight in 1893 and a free glider flight in 1894. Gilmore further added that (although he had not reported it until 1927) he made a controlled steam-powered flight on May 15, 1902; however, all records and papers related to his aircraft were destroyed in a fire.\n\nThere are photographs from 1898 showing Gilmore's machine, but none showing it in the air. The claims of the aircraft achieving flight are unconfirmed, and given the weight evident by the grounded aircraft photos, the possibility of flight is highly unlikely.\n\nLyman Gilmore was in contact with other flight pioneers like Samuel Langley and, eventually, the Wright brothers.\n\nIn 1902, Gilmore was granted two patents on steam engines. He invented in other areas too: for example, a rotary snowplow. On March 15, 1907, Gilmore opened the first commercial airfield, Gilmore Airfield, in Grass Valley. There is now a middle school named in his honor on the site of the airfield.\n\nIn 1935, Lyman's airplane hangar and the two aging monoplanes were destroyed by fire. The fire cancelled plans to exhibit the larger monoplane at the World Fair in Chicago. Gilmore began mining for gold and died a poor man in Nevada City, California. His grave can be found in Pine Grove Cemetery, about a half mile outside of town.\n\nThe Lyman Gilmore Elementary School in Grass Valley has the motto, \"Flying into the Future\" and photos of a mural depicting flight. School children made a YouTube presentation about Gilmore including old video footage of Gilmore and an interview with people who knew him.\n\n\n"}
{"id": "435799", "url": "https://en.wikipedia.org/wiki?curid=435799", "title": "Mechanically separated meat", "text": "Mechanically separated meat\n\nMechanically separated meat (MSM), mechanically recovered/reclaimed meat (MRM), or mechanically deboned meat (MDM) is a paste-like meat product produced by forcing pureed or ground beef, pork, mutton, turkey or chicken, under high pressure through a sieve or similar device to separate the bone from the edible meat tissue. It is sometimes called \"white slime\" as an analog to meat-additive pink slime and to meat extracted by advanced meat recovery systems, both of which are different processes. The process entails pureeing or grinding the carcass left after the manual removal of meat from the bones and then forcing the slurry through a sieve under pressure. This puree includes bone, bone marrow, skin, nerves, blood vessels, and the scraps of meat remaining on the bones. The resulting product is a blend primarily consisting of tissues not generally considered meat along with a much smaller amount of actual meat (muscle tissue). In some countries such as the United States, these non-meat materials are processed separately for human and non-human uses and consumption. The process is controversial; \"Forbes\", for example, called it a \"not-so-appetizing meat production process\".\n\nMechanically separated meat has been used in certain meat and meat products, such as hot dogs and bologna sausage, since the late 1960s. However, not all such meat products are manufactured using an MSM process.\n\nThe practice of mechanically harvesting leftover meat scraps dates to the 1950s, when mechanical hand tools were developed to help remove the remaining pieces of meat and connective tissue from animal carcasses to minimize waste, although said tissues were not automatically or necessarily \"wasted\" when used for other purposes. Primarily MSM was developed in and produced in countries with agriculture industries insufficiently productive to provide mass quantities of regularly processed meat for widespread and affordable consumption. By the 1960s, machines that did this more efficiently, and automatically, were developed. This allowed companies to use previously wasted materials and, in turn, sell the derived meat products to the public for a lower price. During the 1970s, these techniques became more common in other parts of the world, as well. In addition to poultry slaughterhouses, newcomers entered the market as they recognized the financial gains that mechanically separated meat processing allowed. Eastern European countries, especially, are known for their import of frozen chicken MSM.\n\nDuring the 1950s, mechanically separated meat was mostly used as a raw material for the production of hot dogs. Currently, luncheon meats, burgers, and mortadella are regularly made from MSM.\n\nQuestions arose in the 1980s as to the safety of mechanically separated meat. In 1982, a report published by U.S. Food Safety and Inspection Service (FSIS) on mechanically separated meat said it was safe and established a standard of identity for the food product. Some restrictions were made on how much can be used and the type of products in which it can be used. These restrictions were based on concerns for limiting intake of certain components in mechanically separated meat, such as calcium. Mechanically separated meat may not be described simply as \"meat\" on food labels, but must be labeled as \"mechanically separated\" pork, chicken, or turkey in the ingredients statement. Hot dogs can contain no more than 20% mechanically separated pork.\n\nConcerns were raised again when the bovine spongiform encephalopathy (BSE) epidemic, commonly known as \"mad cow disease\", occurred in the United Kingdom in 1986. Since bits of the spinal cord (the part most likely to be carrying the BSE prion) often got mixed in with the rest of the meat, products using mechanically separated meat taken from the carcasses of bovines were at higher risk for transmitting BSE to humans. As a result, in 1989, the United Kingdom tightened restrictions to help ensure pieces of the spinal cord would not be present in mechanically separated meat taken from bovines. In the mid-1990s, the UK government banned MRM from cattle backbone, in 1998 MRM from any ruminant backbone and in August 2001 from any ruminant bone. In 2001, the government prohibited the sale of MRM beef for human consumption to prevent the spread of bovine spongiform encephalopathy. \n\nSimilar United States Department of Agriculture (USDA) rules became effective November 4, 1996, and were later updated, stressing:\nIn 2018, a South African factory that produced polony, (an artificial pink-like bologna sausage imitation), and various other processed meats all from Mechanical Deboned Poultry, was associated with the deadliest listeriosis outbreak in history, that sickened 1000 people, and killed at least 180.\n\n\n"}
{"id": "3354769", "url": "https://en.wikipedia.org/wiki?curid=3354769", "title": "Motorola A780", "text": "Motorola A780\n\nThe Motorola A780 is a cellular PDA running the Linux operating system.\n\nIt was introduced in 2003 and sold in Europe and Asia. Some models include GPS and navigation software.\n\nThe Motorola A780 is a Linux-based smartphone. When the lid is closed, the phone appears like a traditional phone, with a keypad matrix and small display, actually a window to the larger display below the lid. When the lid is flipped open, a QVGA touch screen is revealed that can be used with fingers or a supplied stylus.\n\nThe phone is supplied with a number of applications including a POP and IMAP email client, Opera web browser, calendar and a viewer for PDF and Microsoft Office files. Calendar and address book can be synchronized with a Microsoft Exchange or SyncML server. The phone has a 1.3 megapixel camera recording still and video images. RealPlayer is included to play sound audio files and streamed audio and video. The phone has 48 megabytes of internal flash memory for storing user data and a slot for a microSD card. Both Bluetooth and USB are provided for communication with another computer. Character entry is via an on-screen QWERTY keyboard and hand writing recognition. Models including a GPS receiver are supplied with ALK Technologies' CoPilot Live navigation software with street level maps of Europe.\n\nThe phone has three processors: \n\nThe Linux operating system used, \"EZX Linux\", is a modified version of MontaVista Consumer Electronics Linux 3.0\n\nThis phone is popular with Linux enthusiasts. It is able to establish a TCP/IP connection between the phone and another computer over USB or Bluetooth. One can then telnet to the phone and be presented with a bash prompt. From the prompt one can, for example, mount an NFS drive(s) on the phone. The underlying operating system, Motorola EZX is Linux based, its kernel is open source. With the source code hosted on \"opensource.motorola.com\", it is possible to recompile and replace the kernel for this operating system. However Motorola did not publish a software development kit for native applications. Instead, they are expecting third party programs to be written in Java ME. The OpenEZX website is dedicated to providing free opensource software for this phone and others using the same OS.\n\n\n"}
{"id": "10789593", "url": "https://en.wikipedia.org/wiki?curid=10789593", "title": "Multifilament fishing line", "text": "Multifilament fishing line\n\nMultifilament line, also referred to as \"The Super Lines\", is a type of fishing line. It is a braided line which is made up of a type of polyethylene, an extremely thin line for its strength. By weight, polyethylene strands are five to ten times sturdier than steel. Multifilament line is similar to braided dacron in terms of sensitivity but a diameter about one-third that of monofilament.\n\nMultifilament works best on conventional and baitcasting reels. On spinning and spincasting reels, the line's limpness can make sure for awkward manipulation, as it doesn't \"spring\" off the reel like monofilament. Consequently, knot-tying is more difficult with multifilaments. Certain knots work better with superline, like the palomar knot. Applying a type of super glue will help to prevent other types of knots from slipping.\n\nThis type of fishing line is expensive, sometimes four times the cost of equivalent monofilament. This can become a considerable expense, especially considering that the line is so thin that you need more of it to fill a reel spool. Sometimes, a backing of monofilament or other line is used under the braided line on the spool.\n\n"}
{"id": "6346763", "url": "https://en.wikipedia.org/wiki?curid=6346763", "title": "Mylo (Sony)", "text": "Mylo (Sony)\n\nMy Life Online (mylo) was a device created and marketed by Sony for portable instant messaging and other Internet-based communications, browsing Internet web sites (using the Opera web browser) and playback and sharing of media files. The pocket-sized, tablet-shaped handheld device, which debuted in 2006, had a screen which slid up to reveal a QWERTY keyboard. The brand name 'mylo' stands for My Life Online. Using Wi-Fi instead of cellular networks, the mylo was targeted to the 18–24 age group.\n\nBy using WiFi networks for Internet connectivity, mylo provided users with the possibility of reducing connectivity costs by avoiding the necessity of using GSM, CDMA or 3G cellular networks which would usually be used for devices of this size and functionality.\n\nThe first version of the Sony mylo was launched on September 15, 2006 and includes 1 GB of onboard flash memory and a Memory Stick PRO Duo expansion slot.\n\nThe mylo is 23.9 mm (31/32 in) thick, 123 mm (4⅞ in) wide, 63 mm (2½ in) tall, and sports a 6.1 cm (2.4 in) QVGA (320 × 240) LCD screen. Its form factor is similar to the T-Mobile Sidekick in that it is held in landscape mode and has a slide out QWERTY keyboard. Its initial model colors were a glossy black and white.\n\nOn January 6, 2008, the second edition of the Sony mylo was announced at CES 2008 in Las Vegas. This version was based on PSP technology and included AIM, a touchscreen, and a camera. The Internet Browser was changed from Opera to NetFront. It supported Adobe Flash for viewing Flash content such as YouTube videos and for playing Flash-based games. The mylo COM–2 had 1 GB flash memory that could be used for music and video playback. Two colors were available: black and white. The mylo COM–2 featured wireless G connectivity. The keyboard was backlit as well. System software version 1.201 added support for video recording. The mylo COM–2 was released on SonyStyle.com on January 25, 2008 for $299. It was reduced to $199 before the holiday season and was quickly drawn from production due to its poor sales.\n\nThe mylo's menu of software applications is grouped by function.\nThe following applications in the main menu group use the Wi-Fi connection:\n\nCOM–1. The Opera web browser allows a number of configurations to help minimize the limitations of the small screen (320x240) including Zoom (%), Text Size (Small, Normal, Large) and View Mode (Normal or Fit to Screen).\n\nCOM–2. NetFront with support for Flash Lite is the web browser that is supplied with the mylo COM–2.\n\nCOM–1. The player supports the MP3, WMA (secured and unsecured) and ATRAC 3 audio formats. The image viewer supports JPG, PNG, and Bitmap formats. The video player supports the MPEG-4/AAC video format. The text application is a simple text editor.\n\nCOM–2. The player supports the MP3, WMA (secured and unsecured), AAC (no DRM) and ATRAC3 formats. The image viewers supports JPEG, PNG and BMP formats. The video player supports the MPEG-4/AAC Simple Profile and MPEG-4/AAC AVC H.264 baseline profile. It also supports WMV starting with firmware version 1.101, which was released in April 2008.\n\nOn both models DRM content in Windows Media Audio and Adaptive Transform Acoustic Coding 3 is limited to the internal 1GB memory. DRM playback is not possible via Memory Stick as Magic Gate is not available and Windows Media DRM is not currently possible on Memory Sticks.\n\n\nWith the USB mode configuration set to the default (MSC) the mylo acts as a USB mass-storage device. Music and other files can then be transferred from a PC with the included SonicStage software or by dragging and dropping using a file manager. With the USB mode set to MTP you can synchronize audio files via WMP 10. While connected via USB, the mylo screen displays the USB mode currently in use and suspends whatever program had been running.\n\nThe 802.11b Wi-Fi connection is started either with the Wireless LAN slider button or automatically when one of mylo's Internet applications (i.e., Skype, Yahoo! Messenger, Google Talk, Opera) attempt to access the network and an infrastructure mode Wi-Fi connection has not yet been established. The Wi-Fi mode can be switched to Ad Hoc mode to enable use of the Ad Hoc Application. Although the device is marketed for persons on the go, the Mylo does not have any commonly used Bluetooth capabilities, to provide easy connection with other wireless carrier enabled mobile devices.\n\nOwners of the COM-1 model receive access to any T-Mobile Hotspot for the first year. Owners of the COM–2 model receive access to over 10,000 Wayport Hotspot locations nationwide, through December 31, 2010. Over 9,000 of these locations are McDonald's restaurants.\n\nProcessor: Freescale i.MX21 (for the COM–1)\n\nAs a device, the mylo COM– 1 sports a 2.4-inch 320 by 240 LCD display, 400 MB for flash memory (upgradable to 4 GB), mini-USB connectivity, a Memory Stick Duo slot, integrated 802.11b wireless networking (supporting WEP and WPA-PSK security), and a lithium-ion battery offering up to 45 hours of music playback, 8 hours of video time, and up to 76 hours of VoIP talk time. Add to that a DC input for charging or running \"wired\" with the AC adapter, a 10-pin headphone/microphone interface (an adapter is included), a slide-out QWERTY keyboard for composing messages, and a total weight of about 157 grams (5.4 ounces), including the battery.\n\n\n"}
{"id": "30820393", "url": "https://en.wikipedia.org/wiki?curid=30820393", "title": "Oil mill", "text": "Oil mill\n\nAn oil mill is a grinding mill designed to crush or bruise oil-bearing seeds, such as linseed or peanuts, or other oil-rich vegetable material, such as olives or the fruit of the oil palm, which can then be pressed to extract vegetable oils, which may used as foods or for cooking, as oleochemical feedstocks, as lubricants, or as biofuels. The pomace or press cake - the remaining solid material from which the oil has been extracted - may also be used as a food or fertilizer.\n\nOil-rich vegetable materials have been processed mechanically to extract the valuable oils for thousands of years, typically using vertical millstones moving around a central post (edge runner stones or kollergangs in an edge mill) to crush or bruise the seeds or fruit which can then be stamped or pressed to extract the oil. A treadmill, windmill or watermill was later used to drive the milling and pressing machinery, replaced in modern times with steam and later other power sources. In some areas, watermills may be \"double\" water mills, with machinery for grinding wheat on one side of the watercourse and machinery for extracting oils on the other side.\n\nHistorical windmills could process between 100 and 200 tons of raw materials per year.\n\nModern mechanical oil mills can process up to 4,000 tons per day in hot pressing processes, and up to 25 tons per day cold pressed. Industrial oil pressing methods usually use a screw to crush the raw materials in a continuous process, before extraction of the oil from the press cake using a centrifuge or a solvent such as hexane.\n\nEdible oils may be extracted for culinary purposes. Non-edible oils can be used in the manufacture of soaps, paints and varnishes, or as fuel for oil lamps. Important feed stocks include soybeans, rapeseed (canola), sunflower seeds, cottonseed, and maize (corn), as well as peanuts, olives, various nuts, sesame seeds, safflower, grape seeds and flaxseed (linseed). Palm oil is extracted from the pulp of the oil palm fruit, palm kernel oil from the kernel of the oil palm, and coconut oil from the kernel of the coconut. \n\n\n"}
{"id": "46883456", "url": "https://en.wikipedia.org/wiki?curid=46883456", "title": "Oil–water separator", "text": "Oil–water separator\n\nAn oil water separator (OWS) is a piece of equipment used to separate oil and water mixtures into their separate components. There are many different types of oil-water separator. Each has different oil separation capability and are used in different industries. Oil water separators are designed and selected after consideration of oil separation performance parameters and life cycle cost considerations. \"Oil\" can be taken to mean mineral, vegetable and animal oils, and the many different hydrocarbons.\n\nOil water separators can be designed to treat a variety of contaminants in water including free floating oil, emulsified oil, dissolved oil and suspended solids. Not all oil separator types are capable of separating all contaminants. The most common performance parameters considered are:\nAn API oil–water separator is a device designed to separate gross amounts of oil and suspended solids from the wastewater effluents of oil refineries, petrochemical plants, chemical plants, natural gas processing plants and other industrial sources. The name is derived from the fact that such separators are designed according to API Publication 421, February 1990, published by the American Petroleum Institute. These separators can be used to separate large oil droplets, typically greater than 150 micron.\n\nThe purpose of shipboard oily water separator (OWS) is to separate oil and other contaminants that could be harmful for the oceans. They are most commonly found on board ships where they are used to separate oil from oily waste water such as bilge water before the waste water is discharged into the environment. These discharges of waste water must comply with the requirements laid out in Marpol 73/78.\n\nBilge water is a near-unavoidable product of shipboard operations. Oil leaks from running machinery, such as diesel generators, air compressors, and the main propulsion engine. Modern OWSs have alarms and automatic closure devices which are activated when the oil storage capacity of the oil water separator has been reached.\n\nA gravity plate separator contains a series of plates through which the contaminated water flows. The objective of the design is to allow oil droplets in the water to coalesce on the underside of the plate eventually forming larger oil droplets which floats off the plates and accumulates at the top of the chamber. The oil accumulating at the top is then transferred with some en-trained water to a waste oil tank. This type of oily water separator is very common for many industrial applications as well as in ships but it has some flaws that decrease efficiency. Oil particles that are sixty micrometers in size or smaller do not get separated. Also the presence of chemicals and surfactants in the water greatly reduce oil droplet coalescence, impeding the separation effect The variety of oily wastes in bilge water can limit removal efficiency especially when very dense and highly viscous oils such as bunker oil are present. Plates must be replaced when fouled, which increases the costs of operation.\n\n A centrifugal water–oil separator, \"centrifugal oil–water separator\" or \"centrifugal liquid–liquid separator\" is a device designed to separate oil and water by centrifugation. It generally contains a cylindrical container that rotates inside a larger stationary container. The denser liquid, usually water, accumulates at the periphery of the rotating container and is collected from the side of the device, whereas the less dense liquid, usually oil, accumulates at the rotation axis and is collected from the center. Centrifugal oil–water separators are used for waste water processing and for cleanup of oil spills on sea or on lake. Centrifugal oil–water separators are also used for filtering diesel and lubricating oils by removing the waste particles and impurity from them.\n\nAn oil water separation hydrocyclone is a device designed to separate oil from water by the use of a strong vortex. These separators are passive (no moving parts) and resemble long tapered pipes. They typically contain an inlet section, long tapered section and a long outlet section. In operation the strong vortex is created when the oily water is injected tangentially into the inlet end of the separator. This creates a centrifugal force, that accelerates as it moves down the tapered cone. The centripetal and centrifugal forces separate the heavier water component to the outside of the vortex while the lighter oil droplets are forced to the centre. The separated oils are removed through an orifice at the inlet end of the cone and treated water is discharged through the opposite end. The centrifugal forces generated inside the vortex of the better de-oiling hydrocyclone separators are of the order of 1,000 times the force of gravity. This is why smaller emulsified oil droplets as low as 15 microns can be removed.\n\nOil removal hydrocyclones, or de-oiling hydrocyclones, are very different in geometry, design and operation compared to the more common solid removal hydrocyclones. When correctly designed and operated oil removal hydrocyclones Hydrocyclones are very useful for removing both large oil droplets and smaller emulsified oil droplets in a broad range of applications across many industries. The technology has been successfully applied to treat oily water produced in the mining industry, meat processing, dairy manufacturing, petrochemical, oil refining, oil marketing and oil production operations.\n\nFlotation introduces gas bubbles to enhance oil removal. The gas bubbles attach to the oil droplets to increase the rise rate of the oil. Various flotation methods such as dissolved gas flotation (DGF), dissolved air flotation (DAF), and induced gas flotation (IGF) may be used. Typically this separation step is used following a primary oil–water separation step which is able to remove a large portion of the free oil.\n\nNut shell filtration uses nut shell media in a vessel to remove oil. Nut shell filters were designed to separate crude oil from oilfield produced water in the 1970s. Typically, nut shell filters are used as a polishing step to achieve low oil concentrations (<10 mg/L). Oil is collected in the interstitial spaces between the media and periodically removed during a backwash procedure.\n\nWastewater purification of oils and contaminates by electrochemical emulsification is actively in research and development. Electrochemical emulsification involves the generation of electrolytic bubbles that attract pollutants such as sludge and carry them to the top of the treatment chamber. Once at the top of the treatment chamber the oil and other pollutants are transferred to a waste oil tank.\n\nDownhole oil–water separation (DOWS) technology is an emerging technology that separates oil and gas from produced water at the bottom of the well, and re-injects most of the produced water into another formation which is usually deeper than the producing formation, while the oil and gas rich stream is pumped to the surface. DOWS effectively removes solids from the disposal fluid and thus avoids injectivity impairment caused by solids plugging. Simultaneous injection using DOWS minimizes the opportunity for the contamination of underground sources of drinking water (USDWs) through leaks in tubing and casing during the injection process.\n\nBioremediation is the use of microorganisms to treat contaminated water. A carefully managed environment is needed for the microorganisms which includes nutrients and hydrocarbons such as oil or other contaminates, and oxygen.\n\nIn pilot scale studies, bio-remediation was used as one stage in a multi-stage purification process involving a plate separator to remove the majority of the contaminants and was able to treat pollutants at very low concentrations including organic contaminates such as glycerol, solvents, jet fuel, detergents, and phosphates. After treatment of contaminated water, carbon dioxide, water and an organic sludge were the only residual products.\n\n"}
{"id": "43995424", "url": "https://en.wikipedia.org/wiki?curid=43995424", "title": "Panaya", "text": "Panaya\n\nPanaya is an Israeli software company. The company is a subsidiary of Infosys. Panaya's products include Change impact analysis, automated code remediation, collaborative test management and test-execution, and ALM acceleration. Panaya’s service runs on the Amazon Elastic Compute Cloud.\n\nPanaya was founded in 2006 as \"ChangeSoft Technologies\" by entrepreneur Yossi Cohen and is headquartered in Ra'anana, Israel.\n\nThe disruptive technology revolves around the impact analyses of code change, applied in code comparison of Enterprise resource planning (ERP) systems to help reduce the application life cycle cost. A first patent application was filed in 2008, followed by an extension in 2009, and a continuation in 2013.\n\nIn 2010, shares of Tamares Group, a private equity investment firm based in Vaduz, Liechtenstein, were acquired by Panaya as part of fundraising round.\n\nIn 2012, CGI Group, a Canadian multinational information technology (IT) consulting company, announced the launching of a new Panaya Practice.\n\nIn 2013 and 2014, Panaya laid off more than 25% of the company. In 2016, Panaya shut down their Israeli-based sales development and moved them to Boston and the United Kingdom and replaced their CEO.\n\nIn June 2014, Deloitte and Panaya announced an agreement to provide Oracle E-business Suite customers the ability to reduce the cost and risk of application change projects.\n\nIn February 2015, Infosys announced the acquisition of Panaya for a reported $200 million. Prior to the sale, Panaya was in crisis and was on the verge of shut down, after many waves of layoffs. In 2017, whistleblowers alleged that there were issues with the company’s acquisition by Infosys and that top executives of the company had personal interest in acquisition.\n\nGemini Partners and Benchmark Capital were the first investors to fund Panaya with $5 million in April 2006.\n\nIn August 2009, Panaya secured $5 million funding in a series B investment round led by Tamares Group with the participation of Benchmark Capital.\n\nIn June 2010, in a series C round, Battery Ventures, Benchmark and Tamares Group, invested a further $7 million. In September 2010, HPV, the venture capital fund of Hasso Plattner, co-founder of SAP AG, invested $6 million in exchange for company's stakes. Shares held by the Tamares Group, a private equity investment firm based in Vaduz, Liechtenstein, were acquired by HPV together with existing investors Benchmark Capital and Battery ventures.\n\nIn 2013, the existing investors, in a series D funding round led by Battery Ventures and joined by Benchmark Capital and HPV, injected an additional $16 million into Panaya. The company has raised $39 million to date.\n\nInfosys on 14 April, 2018 said it has started looking out for buyers of Skava and Panaya which were acquired during former CEO Vishal Sikka's tenure to build a so called software plus service model. The company has written off $90 Million in Panaya on a standalone basis, which is nearly half of the $200 million that it had paid to acquire the Israeli software firm in 2015.\n\n\n\nhttp://timesofindia.indiatimes.com/business/india-business/another-whistleblower-mail-alleges-issues-with-infosys-panaya-deal/articleshow/57244175.cms\n"}
{"id": "47451719", "url": "https://en.wikipedia.org/wiki?curid=47451719", "title": "Pancake machine", "text": "Pancake machine\n\nA pancake machine is an electrically-powered machine that automatically produces cooked pancakes. It is believed that the earliest known pancake machine was invented in the United States in 1928. Several types of pancake machines exist that perform in various manners, for both commercial and home use. Some are fully automatic in operation, while others are semi-automatic. Some companies mass-produce pancake machines, and some have been homemade. The Happy Egg Company constructed a novelty pancake machine in 2013 in commemoration of Pancake Day in the United Kingdom.\n\nIn 1928, a man in Portland, Oregon, invented an electric pancake machine that operated by the process of batter being dropped onto a revolving heated flattop grill from a storage cylinder atop the grill. The grill was heated using electricity. The amount of batter dropped was controlled by using controlled amounts of compressed air, which pushed batter out of the storage cylinder. As the batter revolved on the hot grill, the pancake was flipped halfway through the cooking process by a shelf atop the grill. After being flipped, the completed pancake was ejected from the machine upon contact with a gate.\n\nIn 1955 in the United States, an automatic pancake machine was developed by Vendo, which used a specially formulated pancake batter mix that was manufactured by the Quaker Oats Company's Aunt Jemima branch. The Vendo machine could produce pancakes \"in less than three minutes.\" It was a semi-automatic machine that performed all of the cooking functions except for the pouring of the pancake batter.\n\nIn 1956, four Racine, Wisconsin, engineers developed and fabricated two, 5' diameter gas burning pancake machines for the annual\nPancake Day sponsored by the Kiwanis Club there in Racine.\n\nVarious types of pancake machines exist, such as those that run pancake batter through a heated conveyor inside of a box unit, and those that automatically drop pancake batter onto a flattop grill. Some pancake machines, such as one developed by Crepe-Coer, cook both sides of a pancake simultaneously. Semi-automatic pancake machines also exist, which require some human interaction to function, such as the pouring of batter. Commercial pancake machines may be used in the foodservice industry, in cafeterias and by restaurants, and can serve to reduce the waste of stale pancake batter. Some hotels have pancake machines that guests are allowed to operate. They are also used in other environments in a self-service manner, such as in upscale airport lounges and hotels.\n\nHomemade versions of pancake machines have been constructed. An example of a homemade pancake machine is one constructed in 1977 by Ken Whitsett of the Ocala Kiwanis Club in Ocala, Florida, which was used for the organization's annual pancake day. The Kiwanis machine utilized a hopper filled with pancake batter that was manually dropped onto a revolving griddle. The pancakes were manually flipped and plated when cooking was completed. It required four people for its operation, and could produce between 750–1000 pancakes per hour.\n\nCommercial and home-consumer pancake machines are mass-produced by some companies in contemporary times.\n\nCommercial pancake machines are typically used in the commercial foodservice and hospitality industries.\n\nPopcake is a pancake machine brand that can produce 200 pancakes per hour. Individual Pancakes are produced in seconds by this machine. The machine was designed for use in commercial establishments such as cafeterias and convenience stores.\n\nPopcake is a U.S. company that produces Popcake-brand pancake machines. The Popcake machine was invented by Marek Szymanski, and as of July 2014 approximately 7,000 of them are used worldwide. This brand has features that allow users to adjust the size, quantity and doneness level of the pancakes produced. Plates of pancakes are produced in around two minutes time by the Popcake machine.\n\nConsumer versions of pancake machines for home use are simpler in operation compared to commercial machines, typically involving a basic griddle and a feature to adjust cooking temperature. Consumer machines are typically countertop-sized small appliances. Brands include the Severin Crepe Maker, the Cuisinart Griddle and Grill, Swan's Come Dine With Me Party Wok and Pancake Maker, the Roller Grill Single Plate Crepe Machine and the Andrew James Crepe Maker, among others. A review of various consumer machines published in \"The Daily Mail\" recommended various machines per situational uses. For example, the Roller Grill Single Plate Crepe Machine was recommended for use with large groups of people and the Severin Crepe Maker was recommended for \"technophobes\" who prefer a simple design The review recommended the Andrew James Crepe Maker as the overall best value.\n\nIn March 2015 in the U.S., the PancakeBot pancake machine received over $141,000 on Kickstarter. Its target donation request on the website was $50,000. PancakeBot can produce custom pancakes in various designs, which is performed by the use of pancake batter in a bottle that is moved by a programmable machine arm atop the griddle. The machine utilizes custom software to accomplish this.\n\nIn commemoration of Pancake Day in the United Kingdom, a novelty pancake machine was built by The Happy Egg Company in February 2013 that involved a complex series of steps to automatically produce pancakes from scratch. The machine involves the use of a freshly laid egg from a hen that rolls onto a turntable, which then moves the egg to an area where it is automatically cracked and mixed with other ingredients. After this point, the mixture is poured into a griddle, flipped to cook the other side, and then flipped onto a plate.\n\n"}
{"id": "2648839", "url": "https://en.wikipedia.org/wiki?curid=2648839", "title": "Quantum cascade laser", "text": "Quantum cascade laser\n\nQuantum cascade lasers (QCLs) are semiconductor lasers that emit in the mid- to far-infrared portion of the electromagnetic spectrum and were first demonstrated by Jerome Faist, Federico Capasso, Deborah Sivco, Carlo Sirtori, Albert Hutchinson, and Alfred Cho at Bell Laboratories in 1994.\n\nUnlike typical interband semiconductor lasers that emit electromagnetic radiation through the recombination of electron–hole pairs across the material band gap, QCLs are unipolar and laser emission is achieved through the use of intersubband transitions in a repeated stack of semiconductor multiple quantum well heterostructures, an idea first proposed in the paper \"Possibility of amplification of electromagnetic waves in a semiconductor with a superlattice\" by R.F. Kazarinov and R.A. Suris in 1971.\n\nWithin a bulk semiconductor crystal, electrons may occupy states in one of two continuous energy bands - the valence band, which is heavily populated with low energy electrons and the conduction band, which is sparsely populated with high energy electrons. The two energy bands are separated by an energy band gap in which there are no permitted states available for electrons to occupy. Conventional semiconductor laser diodes generate light by a single photon being emitted when a high energy electron in the conduction band recombines with a hole in the valence band. The energy of the photon and hence the emission wavelength of laser diodes is therefore determined by the band gap of the material system used.\n\nA QCL however does not use bulk semiconductor materials in its optically active region. Instead it consists of a periodic series of thin layers of varying material composition forming a superlattice. The superlattice introduces a varying electric potential across the length of the device, meaning that there is a varying probability of electrons occupying different positions over the length of the device. This is referred to as one-dimensional multiple quantum well confinement and leads to the splitting of the band of permitted energies into a number of discrete electronic subbands. By suitable design of the layer thicknesses it is possible to engineer a population inversion between two subbands in the system which is required in order to achieve laser emission. Because the position of the energy levels in the system is primarily determined by the layer thicknesses and not the material, it is possible to tune the emission wavelength of QCLs over a wide range in the same material system.\nAdditionally, in semiconductor laser diodes, electrons and holes are annihilated after recombining across the band gap and can play no further part in photon generation. However, in a unipolar QCL, once an electron has undergone an intersubband transition and emitted a photon in one period of the superlattice, it can tunnel into the next period of the structure where another photon can be emitted. This process of a single electron causing the emission of multiple photons as it traverses through the QCL structure gives rise to the name \"cascade\" and makes a quantum efficiency of greater than unity possible which leads to higher output powers than semiconductor laser diodes.\n\nQCLs are typically based upon a three-level system. Assuming the formation of the wavefunctions is a fast process compared to the scattering between states, the time independent solutions to the Schrödinger equation may be applied and the system can be modelled using rate equations. Each subband contains a number of electrons formula_1 (where formula_2 is the subband index) which scatter between levels with a lifetime formula_3 (reciprocal of the average intersubband scattering rate formula_4), where formula_2 and formula_6 are the initial and final subband indices. Assuming that no other subbands are populated, the rate equations for the three level lasers are given by:\n\nIn the steady state, the time derivatives are equal to zero and formula_10. The general rate equation for electrons in subband \"i\" of an \"N\" level system is therefore:\n\nUnder the assumption that absorption processes can be ignored, (i.e. formula_12, valid at low temperatures) the middle rate equation gives\n\nTherefore, if formula_14 (i.e. formula_15) then formula_16 and a population inversion will exist. The population ratio is defined as\n\nIf all \"N\" steady-state rate equations are summed, the right hand side becomes zero, meaning that the system is underdetermined, and it is possible only to find the relative population of each subband. If the total sheet density of carriers formula_18 in the system is also known, then the \"absolute\" population of carriers in each subband may be determined using:\n\nAs an approximation, it can be assumed that all the carriers in the system are supplied by doping. If the dopant species has a negligible ionisation energy then formula_18 is approximately equal to the doping density.\n\nThe scattering rates are tailored by suitable design of the layer thicknesses in the superlattice which determine the electron wave functions of the subbands. The scattering rate between two subbands is heavily dependent upon the overlap of the wave functions and energy spacing between the subbands. The figure shows the wave functions in a three quantum well (3QW) QCL active region and injector.\n\nIn order to decrease formula_21, the overlap of the upper and lower laser levels is reduced. This is often achieved through designing the layer thicknesses such that the upper laser level is mostly localised in the left-hand well of the 3QW active region, while the lower laser level wave function is made to mostly reside in the central and right-hand wells. This is known as a \"diagonal\" transition. A \"vertical\" transition is one in which the upper laser level is localised in mainly the central and right-hand wells. This increases the overlap and hence formula_21 which reduces the population inversion, but it increases the strength of the radiative transition and therefore the gain.\n\nIn order to increase formula_23, the lower laser level and the ground level wave functions are designed such that they have a good overlap and to increase formula_23 further, the energy spacing between the subbands is designed such that it is equal to the longitudinal optical (LO) phonon energy (~36 meV in GaAs) so that resonant LO phonon-electron scattering can quickly depopulate the lower laser level.\n\nThe first QCL was fabricated in the GaInAs/AlInAs material system lattice-matched to an InP substrate. This particular material system has a conduction band offset (quantum well depth) of 520 meV. These InP-based devices have reached very high levels of performance across the mid-infrared spectral range, achieving high power, above room-temperature, continuous wave emission.\n\nIn 1998 GaAs/AlGaAs QCLs were demonstrated by Sirtori \"et al.\" proving that the QC concept is not restricted to one material system. This material system has a varying quantum well depth depending on the aluminium fraction in the barriers. Although GaAs-based QCLs have not matched the performance levels of InP-based QCLs in the mid-infrared, they have proven to be very successful in the terahertz region of the spectrum.\n\nThe short wavelength limit of QCLs is determined by the depth of the quantum well and recently QCLs have been developed in material systems with very deep quantum wells in order to achieve short wavelength emission. The InGaAs/AlAsSb material system has quantum wells 1.6 eV deep and has been used to fabricate QCLs emitting at 3.05 μm. InAs/AlSb QCLs have quantum wells 2.1 eV deep and electroluminescence at wavelengths as short as 2.5 μm has been observed.\n\nQCLs may also allow laser operation in materials traditionally considered to have poor optical properties. Indirect bandgap materials such as silicon have minimum electron and hole energies at different momentum values. For interband optical transitions, carriers change momentum through a slow, intermediate scattering process, dramatically reducing the optical emission intensity. Intersubband optical transitions however, are independent of the relative momentum of conduction band and valence band minima and theoretical proposals for Si/SiGe quantum cascade emitters have been made.\n\nQCLs currently cover the wavelength range from 2.63 μm to 250 μm (and extends to 355 μm with the application of a magnetic field.)\n\nThe first step in processing quantum cascade gain material to make a useful light-emitting device is to confine the gain medium in an optical waveguide. This makes it possible to direct the emitted light into a collimated beam, and allows a laser resonator to be built such that light can be coupled back into the gain medium.\n\nTwo types of optical waveguides are in common use. A ridge waveguide is created by etching parallel trenches in the quantum cascade gain material to create an isolated stripe of QC material, typically ~10 um wide, and several mm long. A dielectric material is typically deposited in the trenches to guide injected current into the ridge, then the entire ridge is typically coated with gold to provide electrical contact and to help remove heat from the ridge when it is producing light. Light is emitted from the cleaved ends of the waveguide, with an active area that is typically only a few micrometers in dimension.\n\nThe second waveguide type is a buried heterostructure. Here, the QC material is also etched to produce an isolated ridge. Now, however, new semiconductor material is grown over the ridge. The change in index of refraction between the QC material and the overgrown material is sufficient to create a waveguide. Dielectric material is also deposited on the overgrown material around QC ridge to guide the injected current into the QC gain medium. Buried heterostructure waveguides are efficient at removing heat from the QC active area when light is being produced.\n\nAlthough the quantum cascade gain medium can be used to produce incoherent light in a superluminescent configuration, it is most commonly used in combination with an optical cavity to form a laser.\n\nThis is the simplest of the quantum cascade lasers. An optical waveguide is first fabricated out of the quantum cascade material to form the gain medium. The ends of the crystalline semiconductor device are then cleaved to form two parallel mirrors on either end of the waveguide, thus forming a Fabry–Pérot resonator. The residual reflectivity on the cleaved facets from the semiconductor-to-air interface is sufficient to create a resonator. Fabry–Pérot quantum cascade lasers are capable of producing high powers, but are typically multi-mode at higher operating currents. The wavelength can be changed chiefly by changing the temperature of the QC device.\n\nA distributed feedback (DFB) quantum cascade laser is similar to a Fabry–Pérot laser, except for a distributed Bragg reflector (DBR) built on top of the waveguide to prevent it from emitting at other than the desired wavelength. This forces single mode operation of the laser, even at higher operating currents. DFB lasers can be tuned chiefly by changing the temperature, although an interesting variant on tuning can be obtained by pulsing a DFB laser. In this mode, the wavelength of the laser is rapidly “chirped” during the course of the pulse, allowing rapid scanning of a spectral region.\n\nIn an external cavity (EC) quantum cascade laser, the quantum cascade device serves as the laser gain medium. One, or both, of the waveguide facets has an anti-reflection coating that defeats the optical cavity action of the cleaved facets. Mirrors are then arranged in a configuration external to the QC device to create the optical cavity.\n\nIf a frequency-selective element is included in the external cavity, it is possible to reduce the laser emission to a single wavelength, and even tune the radiation. For example, diffraction gratings have been used to create a tunable laser that can tune over 15% of its center wavelength.\n\nThere exists several methods to extend the tuning range of quantum cascade lasers using only monolithically integrated elements. Integrated heaters can extend the tuning range at fixed operation temperature to 0.7% of the central wavelength and superstructure gratings operating through the Vernier effect can extend it to 4% of the central wavelength, compared to <0.1% for a standard DFB device.\n\nThe alternating layers of the two different semiconductors which form the quantum heterostructure may be grown on to a substrate using a variety of methods such as molecular beam epitaxy (MBE) or metalorganic vapour phase epitaxy (MOVPE), also known as metalorganic chemical vapor deposition (MOCVD).\n\nFabry-Perot (FP) quantum cascade lasers were first commercialized in 1998, Distributed feedback (DFB) devices were first commercialized in 2004, and broadly-tunable external cavity quantum cascade lasers first commercialized in 2006. The high optical power output, tuning range and room temperature operation make QCLs useful for spectroscopic applications such as remote sensing of environmental gases and pollutants in the atmosphere and security. They may eventually be used for vehicular cruise control in conditions of poor visibility, collision avoidance radar, industrial process control, and medical diagnostics such as breath analyzers. QCLs are also used to study plasma chemistry.\n\nWhen used in multiple-laser systems, intrapulse QCL spectroscopy offers broadband spectral coverage that can potentially be used to identify and quantify complex heavy molecules such as those in toxic chemicals, explosives, and drugs.\n\nThe upcoming video game Star Citizen imagines external-cavity quantum cascade lasers as high-power weapons.\n\n"}
{"id": "3147940", "url": "https://en.wikipedia.org/wiki?curid=3147940", "title": "Quantum heterostructure", "text": "Quantum heterostructure\n\nQuantum heterostructure is a heterostructure in a substrate (usually a semiconductor material), where size restricts the movements of the charge carriers forcing them into a quantum confinement. This leads to the formation of a set of discrete energy levels at which the carriers can exist. Quantum heterostructures have sharper density of states than structures of more conventional sizes.\n\nQuantum heterostructures are important for fabrication of short-wavelength light-emitting diodes and diode lasers, and for other optoelectronic applications, e.g. high-efficiency photovoltaic cells.\n\nExamples of quantum heterostructures confining the carriers in quasi-two, -one and -zero dimensions are:\n\n"}
{"id": "32335474", "url": "https://en.wikipedia.org/wiki?curid=32335474", "title": "Real-time text", "text": "Real-time text\n\nReal-time text (RTT) is text transmitted instantly as it is typed or created. Recipients can immediately read the message while it is being written, without waiting.\n\nReal-time text is used for conversational text, in collaboration, and in live captioning. Technologies include TDD/TTY devices for the deaf, live captioning for TV, Text over IP (ToIP), some types of instant messaging, software that automatically captions conversations , captioning for telephony/video teleconferencing, telecommunications relay services including ip-relay, transcription services including Remote CART, TypeWell, collaborative text editing, streaming text applications, next-generation 9-1-1/1-1-2 emergency service. Obsolete TDD/TTY devices are being replaced by more modern real-time text technologies, including Text over IP, ip-relay, and instant messaging.\n\nDuring 2012, the Real-Time Text Taskforce (R3TF) designed a standard international symbol to represent real-time text, as well as the alternate name Fast Text to improve public education of the technology.\n\nWhile standard instant messaging is not real-time text (the message is only sent at the end of a thought, not while it is being composed), a real-time text option is found in some instant messaging software, including AOL Instant Messenger's \"Real-Time IM\" feature. Real-time text is also possible over any XMPP compatible chat networks, including those used by Apple iChat, Cisco WebEx, and Google Talk, by using appropriate software that has a real-time text feature. When present in IM programs, the real-time text feature can be turned on/off, just like other chat features such as audio. Real-time text programs date at least to the 1970s, with the talk program on the DEC PDP-11, which remains in use on Unix systems.\n\nCertain real-time text applications have a feature that allows the real-time text to be \"turned off\", for temporary purposes. This allows the sender to pre-compose the message as a standard IM or text message before transmitting.\n\nReal-time text is frequently used by the deaf, including IP-Relay services, TDD/TTY devices, and Text over IP. Real-time text allows the other person to read immediately, without waiting for the sender to finish composing his or her sentence/message. This allows conversational use of text, much like a hearing person can listen to someone speaking in real-time.\n\nCaptioned telephony is the streaming of real-time text captions in parallel with speech on a phone call. This is used by people who are hard of hearing to allow them to have the full benefit of listening as best they can, hearing all the intonation etc. in speech, yet have the captions for those words they cannot hear clearly enough. In the United States, captioned telephony is one of the free relay services that is available to anyone who is hard-of-hearing. Originally developed for use on the analog phone systems (where it requires a special phone) it is now available over IP using standard devices.\n\nCollaborative real-time editing is the utilization of real-time text for shared editing, rather than for conversation. Split screen chat, where conversational text appears continuously, is also considered real-time text. Some examples that provide this as a service are Apache Wave and its fork SwellRT, Etherpad, the editor Gobby, and most notably Google Docs.\n\nReal-time text is used in closed captioning and when captions are being streamed live continuously during live events. Transcription services including Communication Access Real-Time Translation and TypeWell frequently use real-time text, where text is streamed live to a remote display. This is used in court reporting, and is also used by deaf attendees at a conference. Also, real-time text provides an enhancement to text messaging on mobile phones, via real-time texting apps.\n\nReal-time text protocols include Text over IP (ToIP) designed around ITU-T T.140, IETF RFC4103, RFC5194, and XMPP Extension Protocol XEP-0301.\n\nAccording to ITU-T Multimedia Recommendation F.703, total conversation defines the simultaneous use of audio, video and real-time text. An instant messaging program that can enable all three features simultaneously, would be compliant. Real time text is an important part of it.\n\nReal-time text is also historically found in the old UNIX talk, BBS software such as Celerity BBS, and older versions of ICQ messaging software.\n\n"}
{"id": "18790525", "url": "https://en.wikipedia.org/wiki?curid=18790525", "title": "SCR-658 radar", "text": "SCR-658 radar\n\nThe SCR-658 radar was developed in conjunction with the SCR-268 radar. It was preceded by the SCR-258. Its primary purpose was to track weather balloons. Prior to this it was only possible to track weather balloons with a theodolite, causing difficulty with visual tracking in poor weather conditions. The set is small enough to be portable and carried in a Ben Hur trailer.\nThere is one known survivor at the Air Force museum in Dayton Ohio.\n\n\n\n"}
{"id": "40512668", "url": "https://en.wikipedia.org/wiki?curid=40512668", "title": "SITS:Vision", "text": "SITS:Vision\n\nSITS:Vision, also known just as SITS, is a database application used for course and student management in further and higher education institutions, developed and maintained by the Tribal Group. It is currently used by roughly 70% of the UK higher education sector as well as international institutions such as the University of Sydney and the University of Otago.\n\nSITS has been in existence since 1991 and was first developed by a former Registrar and IT Director of a UK university. The company, Strategic Information Technology Services, started in a residential property in Beverley, East Yorkshire before moving to commercial property in Middleton-on-the-Wolds. The business moved to Hessle, East Yorkshire, in 1997 and into the current premises in Hesslewood in 2000.\nSITS was acquired by Tribal Group PLC [TRB.L] in 2004.\n\nE:Vision (sometimes styled as eVision or e:Vision), is a web-based interface designed to interact with the SITS client.\n\nIt was created with the intention of allowing users to design web-based interfaces for student and/or staff interaction. This has the advantage of allowing the user to circumvent the SITS client which some customers find cumbersome.\n\nE:Vision interfaces are created using the traditional SITS client, but does not come as a standard component when purchasing SITS. The two basic type of interfaces that can be created are 'tasks' and 'vistas' - the former allowing workflows to be created wherein a user might complete a process or operation, the latter offers an area in which data can be viewed and edited.\n\n\n"}
{"id": "23979212", "url": "https://en.wikipedia.org/wiki?curid=23979212", "title": "SensorDynamics", "text": "SensorDynamics\n\nSensorDynamics was a European semiconductor and MEMS company specialized in developing and manufacturing high-volume micro- and wireless semiconductor sensor products for applications in automotive, industry and high-end consumer sectors. The company was acquired by Maxim Integrated in 2011 for $164 million. SensorDynamics developed and produced custom-made designs and standard components for use in vehicle stabilization, occupant protection, navigation systems, keyless go systems and autonomous energy generators for wireless and battery free controllers for industrial, automotive and high-end consumer application. With its headquarters in Graz, Austria, SensorDynamics had offices in Italy and Germany and a worldwide sales and distribution network. The company employed about 130 people in 2011.\n\nSensorDynamics was founded in 2003 by Herbert Gartner, Hubertus Christ, Jürgen Tittel and Volker Kempe. Financed by national and international Venture Capital investors the company successfully closed four equity rounds in 2004, 2007, 2009 and 2011. In 2005 SensorDynamics was ranked under the top 100 European high tech companies by Tornado Insider and awarded the Fast Forward Award 2005 in Austria. From the beginning the Company strongly cooperated with the Institute of Silicon Technology Fraunhofer Society in Itzehoe and expanded this cooperation with long term agreements in 2007. In October 2007 SensorDynamics started a deep cooperation with US consumer electronics supplier Kionix. In August 2008 EnOcean and SensorDynamics announced the launch world's first energy harvesting system on chip SOC product. In March 2009 SensorDynamics launched MEMS gyroscopes for industrial, medical and consumer applications. In November 2009 Chipworks selected SensorDynamics' MEMS product SD755 as product of the year 2009. In November 2010 SensorDynamics announced XY angular rate and an XYZ angular rate devices in 6x6x1.8 mm3 QFN40-packages. In December 2010 SensorDynamics announced worldwide's first fully characterized and specified 6 x 6 x 1.2 mm 6DoF IMU (six degrees of freedom inertial measurement unit) including evaluation boards. In July 2011 SensorDynamics was acquired by Maxim Integrated, a recognized leader in analog and mixed-signal semiconductor products. Maxim Integrated, headquartered in San Jose, California, is in Fortune 1000, and is included in the NASDAQ-100, the Russell 1000, and MSCI USA indices. Maxim was paying $164 million to acquire SensorDynamics.\n\nSensorDynamics focused on three product groups, for each of which the company had created a development platform to guarantee a maximum re-use of silicon proven analogue and digital IPs. This was the basis for both application-specific developments and adaptations of existing products.\n\nSensorDynamics had developed and produced world's first MEMS combo sensors in combining MEMS motion sensors (angular rate and/or acceleration) with sophisticated analogue-digital electronics (ASIC) in micro packages to form a large-scale of integrated micro sensor system.\nApplications included electronic stabilization systems, rollover detection and navigation sets for ’blind reckoning’ without GPS support.\n\nSensorDynamics had a wealth of expertise in the development and manufacturing of intelligent electronic signal conditioning circuitry of macro sensors such as core and planar coils, resistors, capacitive and magneto resistive sensors. Cooperation with the customer produces application-specific sensor systems that combined macro sensors and electronic signal conditioning at module level.\n\nExamples of wireless sensors were to be found in ‘Keyless go’ or tyre pressure systems. However, wireless and battery-less sensor systems are increasingly adopted in industrial applications; especially automation such as smart LF/RF applications.\nEnergy harvesting, in other words powering devices from the energy produced by movement, heat or light, was enabled by SensorDynamics' system on chip SOC products. Sensor systems that incorporated energy use optimization management – integrated processing of sensor signals through to the transmission of control signals at radio frequencies – was a special discipline of SensorDynamics.\n\n\nThe company had about 130 employees world-wide when it was acquired by Maxim Integrated in July 2011. Their key qualifications included: extensive experience in sensor system development, MEMS and semiconductor technology, testing and quality assurance as well as deep knowledge of the automotive electronics market and strong applications expertise. SensorDynamics as a company had been operational as such since 2003, although the nucleus of the team worked together in prior companies since the 1990s.\n\nAs SensorDynamics was also providing sensitive integrated products and systems to the automotive market, the quality and environmental management system was based on the ISO/TS16949 and ISO 14001 standard.\nSensorDynamics satisfied all automotive quality requirements (i.e. ISO/TS16949, ISO 14001, IEC61508, ISO 26262, AEC Q100).\n\nI4G Investment GmbH (2003 - 2011);\nSiemens Venture Capital (2004 - 2011);\nDEWB (2004 - 2011);\nSteirische Beteiligungsfinanzierungsgesellschaft mbH (2004 -2011);\nFIDURA Private Equity Fonds (2007 - 2011);\nPONTIS Capital (2007 -2011);\nGlobal Equity Partners (2004 - 2009);\nAustria Wirtschaftsservice GmbH (2003 - 2011);\nAustrian Research Promotion Agency (FFG) (2004 - today);\n\n"}
{"id": "265822", "url": "https://en.wikipedia.org/wiki?curid=265822", "title": "Thermostat", "text": "Thermostat\n\nA thermostat is a component which senses the temperature of a physical system and performs actions so that the system's temperature is maintained near a desired setpoint.\n\nThermostats are used in any device or system that heats or cools to a setpoint temperature, examples include building heating, central heating, air conditioners, HVAC systems, water heaters, as well as kitchen equipment including ovens and refrigerators and medical and scientific incubators. In scientific literature, these devices are often broadly classified as thermostatically controlled loads (TCLs). Thermostatically controlled loads comprise roughly 50% of the overall electricity demand in the United States. \n\nA thermostat operates as a \"closed loop\" control device, as it seeks to reduce the error between the desired and measured temperatures. Sometimes a thermostat combines both the sensing and control action elements of a controlled system, such as in an automotive thermostat.\n\nThe word thermostat is derived from the Greek words θεiμός \"thermos\", \"hot\" and στατός \"statos\", \"standing, stationary\".\n\nA thermostat exerts control by switching heating or cooling devices on or off, or by regulating the flow of a heat transfer fluid as needed, to maintain the correct temperature. A thermostat can often be the main control unit for a heating or cooling system, in applications ranging from ambient air control, to such as automotive coolant control. Thermostats are used in any device or system that heats or cools to a setpoint temperature, examples include building heating, central heating, air conditioners, as well as kitchen equipment including ovens and refrigerators and medical and scientific incubators.\n\nThermostats use different types of sensors to measure the temperature. In one form, the mechanical thermostat, a bimetallic strip in the form of a coil directly operates electrical contacts that control the heating or cooling source. Electronic thermostats, instead, use a thermistor or other semiconductor sensor that requires amplification and processing to control the heating or cooling equipment. A thermostat is an example of a \"bang-bang controller\" as the heating or cooling equipment output is not proportional to the difference between actual temperature and the temperature setpoint. Instead, the heating or cooling equipment runs at full capacity until the set temperature is reached, then shuts off. Increasing the difference between the thermostat setting and the desired temperature therefore does not change the time to achieve the desired temperature. The rate at which the target system temperature can change is determined both by the capacity of the heating or cooling equipment to respectively add or remove heat to or from a target system and the capacity of the target system to store heat.\n\nTo prevent excessively rapid cycling of the equipment when the temperature is near the setpoint, a thermostat can include some hysteresis. Instead of changing from \"on\" to \"off\" and vice versa instantly at the set temperature, a thermostat with hysteresis will not switch until the temperature has changed a little past the set temperature point. For example, a refrigerator set to 2°C might not start the cooling compressor until its food compartment's temperature reaches 3°C, and will keep it running until the temperature has been lowered to 1 °C. This reduces the risk of equipment wear from too frequent switching, although it introduces a target system temperature oscillation of a certain magnitude.\n\nTo improve the comfort of the occupants of heated or air-conditioned spaces, bimetal sensor thermostats can include an \"anticipator\" system to slightly warm the temperature sensor while the heating equipment is operating, or to slightly warm the sensor when the cooling system is not operating. When correctly adjusted this reduces any excessive hysteresis in the system and reduces the magnitude of temperature variations. Electronic thermostats have an electronic equivalent. \n\nEarly technologies included mercury thermometers with electrodes inserted directly through the glass, so that when a certain (fixed) temperature was reached the contacts would be closed by the mercury. These were accurate to within a degree of temperature.\n\nCommon sensor technologies in use today include:\n\nThese may then control the heating or cooling apparatus using:\n\nPossibly the earliest recorded examples of thermostat control were built by the Dutch innovator Cornelis Drebbel (1572–1633) around 1620 in England. He invented a mercury thermostat to regulate the temperature of a chicken incubator. This is one of the first recorded feedback-controlled devices.\n\nModern thermostat control was developed in the 1830s by Andrew Ure (1778–1857), a Scottish chemist, who invented the bi-metallic thermostat. The textile mills of the time needed a constant and steady temperature to operate optimally, so to achieve this Ure designed the bimetallic thermostat, which would bend as one of the metals expanded in response to the increased temperature and cut off the energy supply.\n\nWarren S. Johnson (1847–1911) of Wisconsin patented a bi-metal room thermostat in 1883, and two years later filed a patent for the first multi-zone thermostatic control system.\nAlbert Butz (1849–1905) invented the electric thermostat and patented it in 1886.\n\nOne of the first industrial uses of the thermostat was in the regulation of the temperature in poultry incubators. Charles Hearson, a British engineer, designed the first modern incubator for eggs that was taken up for use on poultry farms in 1879. The incubators incorporated an accurate thermostat to regulate the temperature so as to precisely simulate the experience of an egg being hatched naturally.\n\nThis covers only devices which both sense and control using purely mechanical means.\n\nDomestic water and steam based central heating systems have traditionally been controlled by bi-metallic strip thermostats, and this is dealt with later in this article. Purely mechanical control has been localised steam or hot-water radiator bi-metallic thermostats which regulated the individual flow. However, thermostatic radiator valves (TRV) are now being widely used.\n\nPurely mechanical thermostats are used to regulate dampers in some rooftop turbine vents, reducing building heat loss in cool or cold periods.\n\nSome automobile passenger heating systems have a thermostatically controlled valve to regulate the water flow and temperature to an adjustable level. In older vehicles the thermostat controls the application of engine vacuum to actuators that control water valves and flappers to direct the flow of air. In modern vehicles, the vacuum actuators may be operated by small solenoids under the control of a central computer.\n\nPerhaps the most common example of purely mechanical thermostat technology in use today is the internal combustion engine cooling system thermostat, used to maintain the engine near its optimum operating temperature by regulating the flow of coolant to an air-cooled radiator. This type of thermostat operates using a sealed chamber containing a wax pellet that melts and expands at a set temperature. The expansion of the chamber operates a rod which opens a valve when the operating temperature is exceeded. The operating temperature is determined by the composition of the wax. Once the operating temperature is reached, the thermostat progressively increases or decreases its opening in response to temperature changes, dynamically balancing the coolant recirculation flow and coolant flow to the radiator to maintain the engine temperature in the optimum range.\nOn many automobile engines, including all Chrysler Group and General Motors products, the thermostat does not restrict flow to the heater core. The passenger side tank of the radiator is used as a bypass to the thermostat, flowing through the heater core. This prevents formation of steam pockets before the thermostat opens, and allows the heater to function before the thermostat opens. Another benefit is that there is still some flow through the radiator if the thermostat fails.\n\nA thermostatic mixing valve uses a wax pellet to control the mixing of hot and cold water. A common application is to permit operation of an electric water heater at a temperature hot enough to kill \"Legionella\" bacteria (above ), while the output of the valve produces water that is cool enough to not immediately scald ().\n\nA wax pellet driven valve can be analyzed through graphing the wax pellet's hysteresis which consists of two thermal expansion curves; extension (motion) vs. temperature increase, and contraction (motion) vs. temperature decrease. The spread between the up and down curves visually illustrate the valve's hysteresis; there is always hysteresis within wax driven valves due to the phase change between solids and liquids. Hysteresis can be controlled with specialized blended mixes of hydrocarbons; tight hysteresis is what most desire, however some applications require broader ranges. Wax pellet driven valves are used in anti scald, freeze protection, over-temp purge, solar thermal, automotive, and aerospace applications among many others. \n\nThermostats are sometimes used to regulate gas ovens. It consists of a gas-filled bulb connected to the control unit by a slender copper tube. The bulb is normally located at the top of the oven. The tube ends in a chamber sealed by a diaphragm. As the thermostat heats up, the gas expands applying pressure to the diaphragm which reduces the flow of gas to the burner.\n\nA pneumatic thermostat is a thermostat that controls a heating or cooling system via a series of air-filled control tubes. This \"control air\" system responds to the pressure changes (due to temperature) in the control tube to activate heating or cooling when required. The control air typically is maintained on \"mains\" at 15-18 psi (although usually operable up to 20 psi). Pneumatic thermostats typically provide output/ branch/ post-restrictor (for single-pipe operation) pressures of 3-15 psi which is piped to the end device (valve/ damper actuator/ pneumatic-electric switch, etc.).\n\nThe pneumatic thermostat was invented by Warren Johnson in 1895 soon after he invented the electric thermostat. In 2009, Harry Sim was awarded a patent for a pneumatic-to-digital interface that allows pneumatically controlled buildings to be integrated with building automation systems to provide similar benefits as direct digital control (DDC).\n\nA wax pellet driven valve can be analyzed by graphing the wax pellet's hysteresis which consists of two thermal expansion curves; extension (motion) vs. temperature increase, and contraction (motion) vs. temperature decrease. The spread between the up and down curves visually illustrate the valve's hysteresis; there is always hysteresis within wax driven technology due to the phase change between solids and liquids. Hysteresis can be controlled with specialized blended mixes of hydrocarbons; tight hysteresis is what most desire, however specialized engineering applications require broader ranges. Wax pellet driven valves are used in anti scald, freeze protection, over-temp purge, solar thermal, automotive, and aerospace applications among many others.\n\nWater and steam based central heating systems have traditionally had overall control by wall-mounted bi-metallic strip thermostats. These sense the air temperature using the differential expansion of two metals to actuate an on/off switch. Typically the central system would be switched on when the temperature drops below the setpoint on the thermostat, and switched off when it rises above, with a few degrees of hysteresis to prevent excessive switching. Bi-metallic sensing is now being superseded by electronic sensors. A principal use of the bi-metallic thermostat today is in individual electric convection heaters, where control is on/off, based on the local air temperature and the setpoint desired by the user. These are also used on air-conditioners, where local control is required.\n\nThe illustration is the interior of a common two wire heat-only household thermostat, used to regulate a gas-fired heater via an electric gas valve. Similar mechanisms may also be used to control oil furnaces, boilers, boiler zone valves, electric attic fans, electric furnaces, electric baseboard heaters, and household appliances such as refrigerators, coffee pots and hair dryers. The power through the thermostat is provided by the heating device and may range from millivolts to 240 volts in common North American construction, and is used to control the heating system either directly (electric baseboard heaters and some electric furnaces) or indirectly (all gas, oil and forced hot water systems). \"Due to the variety of possible voltages and currents available at the thermostat, caution must be taken when selecting a replacement device.\"\n\n\nNot shown in the illustration is a separate bimetal thermometer on the outer case to show the actual temperature at the thermostat.\n\nAs illustrated in the use of the thermostat above, all of the power for the control system is provided by a thermopile which is a combination of many stacked thermocouples, heated by the pilot light. The thermopile produces sufficient electrical power to drive a low-power gas valve, which under control of one or more thermostat switches, in turn controls the input of fuel to the burner.\n\nThis type of device is generally considered obsolete as pilot lights can waste a surprising amount of gas (in the same way a dripping faucet can waste a large amount of water over an extended period), and are also no longer used on stoves, but are still to be found in many gas water heaters and gas fireplaces. Their poor efficiency is acceptable in water heaters, since most of the energy \"wasted\" on the pilot still represents a direct heat gain for the water tank. The Millivolt system also makes it unnecessary for a special electrical circuit to be run to the water heater or furnace; these systems are often completely self-sufficient and can run without any external electrical power supply. For tankless \"on demand\" water heaters, pilot ignition is preferable because it is faster than hot-surface ignition and more reliable than spark ignition.\n\nSome programmable thermostats - those that offer simple \"millivolt\" or \"two-wire\" modes - will control these systems.\n\nThe majority of modern heating/cooling/heat pump thermostats operate on low voltage (typically 24 volts AC) control circuits. The source of the 24 volt AC power is a control transformer installed as part of the heating/cooling equipment. The advantage of the low voltage control system is the ability to operate multiple electromechanical switching devices such as relays, contactors, and sequencers using inherently safe voltage and current levels. Built into the thermostat is a provision for enhanced temperature control using anticipation. A heat anticipator generates a small amount of additional heat to the sensing element while the heating appliance is operating. This opens the heating contacts slightly early to prevent the space temperature from greatly overshooting the thermostat setting. A mechanical heat anticipator is generally adjustable and should be set to the current flowing in the heating control circuit when the system is operating. A cooling anticipator generates a small amount of additional heat to the sensing element while the cooling appliance is not operating. This causes the contacts to energize the cooling equipment slightly early, preventing the space temperature from climbing excessively. Cooling anticipators are generally non-adjustable.\n\nElectromechanical thermostats use resistance elements as anticipators. Most electronic thermostats use either thermistor devices or integrated logic elements for the anticipation function. In some electronic thermostats, the thermistor anticipator may be located outdoors, providing a variable anticipation depending on the outdoor temperature. Thermostat enhancements include outdoor temperature display, programmability, and system fault indication. While such 24 volt thermostats are incapable of operating a furnace when the mains power fails, most such furnaces require mains power for heated air fans (and often also hot-surface or electronic spark ignition) rendering moot the functionality of the thermostat. In other circumstances such as piloted wall and \"gravity\" (fanless) floor and central heaters the low voltage system described previously may be capable of remaining functional when electrical power is unavailable.\n\nThere are no standards for wiring color codes, but convention has settled on the following terminal codes and colors.\nIn all cases, the manufacturer's instructions should be considered definitive.\nLine voltage thermostats are most commonly used for electric space heaters such as a baseboard heater or a direct-wired electric furnace. If a line voltage thermostat is used, system power (in the United States, 120 or 240 volts) is directly switched by the thermostat. With switching current often exceeding 40 amperes, using a low voltage thermostat on a line voltage circuit will result at least in the failure of the thermostat and possibly a fire. Line voltage thermostats are sometimes used in other applications, such as the control of fan-coil (fan powered from line voltage blowing through a coil of tubing which is either heated or cooled by a larger system) units in large systems using centralized boilers and chillers, or to control circulation pumps in hydronic heating applications.\n\nSome programmable thermostats are available to control line-voltage systems. Baseboard heaters will especially benefit from a programmable thermostat which is capable of continuous control (as are at least some Honeywell models), effectively controlling the heater like a lamp dimmer, and gradually increasing and decreasing heating to ensure an extremely constant room temperature (continuous control rather than relying on the averaging effects of hysteresis). Systems which include a fan (electric furnaces, wall heaters, etc.) must typically use simple on/off controls.\n\nNewer digital thermostats have no moving parts to measure temperature and instead rely on thermistors or other semiconductor devices such as a resistance thermometer (resistance temperature detector). Typically one or more regular batteries must be installed to operate it, although some so-called \"power stealing\" digital thermostats use the common 24 volt AC circuits as a power source, but will not operate on thermopile powered \"millivolt\" circuits used in some furnaces. Each has an LCD screen showing the current temperature, and the current setting. Most also have a clock, and time-of-day and even day-of-week settings for the temperature, used for comfort and energy conservation. Some advanced models have touch screens, or the ability to work with home automation or building automation systems.\n\nDigital thermostats use either a relay or a semiconductor device such as triac to act as a switch to control the HVAC unit. Units with relays will operate millivolt systems, but often make an audible \"click\" noise when switching on or off.\n\nHVAC systems with the ability to modulate their output can be combined with thermostats that have a built-in PID controller to achieve smoother operation.\nThere are also modern thermostats featuring adaptive algorithms to further improve the inertia prone system behaviour. For instance, setting those up so that the temperature in the morning at 7 a.m. should be 21 °C (69.8 °F), makes sure that at that time the temperature will be 21 °C (69.8 °F), where a conventional thermostat would just start working at that time. The algorithms decide at what time the system should be activated in order to reach the desired temperature at the desired time. Other thermostat used for process/industrial control where ON/OFF control is not suitable the PID control can also makes sure that the temperature is very stable (for instance, by reducing overshoots by fine tuning PID constants for set value(SV) or maintaining temperature in a band by deploying hysteresis control.)\n\nMost digital thermostats in common residential use in North America and Europe are programmable thermostats, which will typically provide a 30% energy savings if left with their default programs; adjustments to these defaults may increase or reduce energy savings. The programmable thermostat article provides basic information on the operation, selection and installation of such a thermostat.\n\n\nWith non-zoned (typical residential, one thermostat for the whole house) systems, when the thermostat's R (or Rh) and W terminals are connected, the furnace will go through its start-up procedure and produce heat.\n\nWith zoned systems (some residential, many commercial systems — several thermostats controlling different \"zones\" in the building), the thermostat will cause small electric motors to open valves or dampers and start the furnace or boiler if it's not already running.\n\nMost programmable thermostats will control these systems.\n\nDepending on what is being controlled, a forced-air air conditioning thermostat generally has an external switch for heat/off/cool, and another on/auto to turn the blower fan on constantly or only when heating and cooling are running. Four wires come to the centrally-located thermostat from the main heating/cooling unit (usually located in a closet, basement, or occasionally in the attic): One wire, usually red, supplies 24 volts AC power to the thermostat, while the other three supply control signals from the thermostat, usually white for heat, yellow for cooling, and green to turn on the blower fan. The power is supplied by a transformer, and when the thermostat makes contact between the 24 volt power and one or two of the other wires, a relay back at the heating/cooling unit activates the corresponding heat/fan/cool function of the unit(s).\n\nA thermostat, when set to \"cool\", will only turn on when the ambient temperature of the surrounding room is above the set temperature. Thus, if the controlled space has a temperature normally above the desired setting when the heating/cooling system is off, it would be wise to keep the thermostat set to \"cool\", despite what the temperature is outside. On the other hand, if the temperature of the controlled area falls below the desired degree, then it is advisable to turn the thermostat to \"heat\".\n\nThe heat pump is a refrigeration based appliance which reverses refrigerant flow between the indoor and outdoor coils. This is done by energizing a reversing valve (also known as a \"4-way\" or \"change-over\" valve). During cooling, the indoor coil is an evaporator removing heat from the indoor air and transferring it to the outdoor coil where it is rejected to the outdoor air. During heating, the outdoor coil becomes the evaporator and heat is removed from the outdoor air and transferred to the indoor air through the indoor coil. The reversing valve, controlled by the thermostat, causes the change-over from heat to cool. Residential heat pump thermostats generally have an \"O\" terminal to energize the reversing valve in cooling. Some residential and many commercial heat pump thermostats use a \"B\" terminal to energize the reversing valve in heating. The heating capacity of a heat pump decreases as outdoor temperatures fall. At some outdoor temperature (called the balance point) the ability of the refrigeration system to transfer heat into the building falls below the heating needs of the building. A typical heat pump is fitted with electric heating elements to supplement the refrigeration heat when the outdoor temperature is below this balance point. Operation of the supplemental heat is controlled by a second stage heating contact in the heat pump thermostat. During heating, the outdoor coil is operating at a temperature below the outdoor temperature and condensation on the coil may take place. This condensation may then freeze onto the coil, reducing its heat transfer capacity. Heat pumps therefore have a provision for occasional defrost of the outdoor coil. This is done by reversing the cycle to the cooling mode, shutting off the outdoor fan, and energizing the electric heating elements. The electric heat in defrost mode is needed to keep the system from blowing cold air inside the building. The elements are then used in the \"reheat\" function. Although the thermostat may indicate the system is in defrost and electric heat is activated, the defrost function is not controlled by the thermostat. Since the heat pump has electric heat elements for supplemental and reheats, the heat pump thermostat provides for use of the electric heat elements should the refrigeration system fail. This function is normally activated by an \"E\" terminal on the thermostat. When in emergency heat, the thermostat makes no attempt to operate the compressor or outdoor fan.\n\nThe thermostat should not be located on an outside wall or where it could be exposed to direct sunlight at any time during the day. It should be located away from the room's cooling or heating vents or device, yet exposed to general airflow from the room(s) to be regulated. An open hallway may be most appropriate for a single zone system, where living rooms and bedrooms are operated as a single zone. If the hallway may be closed by doors from the regulated spaces then these should be left open when the system is in use. If the thermostat is too close to the source controlled then the system will tend to \"short a cycle\", and numerous starts and stops can be annoying and in some cases shorten equipment life. A multiple zoned system can save considerable energy by regulating individual spaces, allowing unused rooms to vary in temperature by turning off the heating and cooling.\n\nIt has been reported that many thermostats in office buildings are non-functional dummy devices, installed to give tenants' employees an illusion of control. These dummy thermostats are in effect a type of placebo button. However, these thermostats are often used to detect the temperature in the zone, even though their controls are disabled. This function is often referred to as \"lockout\".\n\n\n"}
{"id": "984879", "url": "https://en.wikipedia.org/wiki?curid=984879", "title": "Thomas Dyer", "text": "Thomas Dyer\n\nThomas Dyer (January 13, 1805June 6, 1862; buried in Connecticut) served as mayor of Chicago, Illinois (1856–1857) for the Democratic Party. He also served as the founding president of the Chicago Board of Trade.\n\nHe was a former meat-packing partner of former mayor John Putnam Chapin, who was one of Chicago's first meat packers. Chapin built a slaughterhouse on the South Branch of the Chicago River in 1844.\n\n"}
{"id": "36538541", "url": "https://en.wikipedia.org/wiki?curid=36538541", "title": "ToggleKeys", "text": "ToggleKeys\n\nToggleKeys is a feature of Microsoft Windows. It is an accessibility function which is designed for people who have vision impairment or cognitive disabilities. When ToggleKeys is turned on, computer will provide sound cues when the locking keys (, , or ) are pressed. A high-pitched sound plays when the keys are switched on and a low-pitched sound plays when they are switched off.\n\nMicrosoft first introduced ToggleKeys with Windows 95. The feature is also used in later versions of Windows.\n\nPress the key for 5 seconds while holding. This feature can also be turned on and off via the Accessibility icon in the Windows Control Panel.\n\n"}
{"id": "2568547", "url": "https://en.wikipedia.org/wiki?curid=2568547", "title": "Video recorder scheduling code", "text": "Video recorder scheduling code\n\nVCR Plus+, G-Code, VideoPlus+ and ShowView are different names for the same scheduling system for programming video recorders. These names are all registered trademarks of Macrovision, whose corporate predecessor, Gemstar, developed these algorithms for use in integrated endecs.\n\nBefore the advent of on-screen displays, the only interface available for programming a home video recorder was a small VFD, LED or LCD panel and a small number of buttons. Correctly setting up a recording for a specific program was therefore a somewhat complex operation for many people. G-Code, VideoPlus+ and ShowView were introduced in the late 1980s to remove this difficulty.\n\nThe central concept of the system is a unique number, a PlusCode, assigned to each program, and published in television listings in newspapers and magazines (such as \"TV Guide\"). To record a program, the code number is taken from the newspaper and input into the video recorder, which would then record on the correct channel at the correct time. The number is generated by an algorithm from the date, time and channel of the program; as a result, it does not rely on an over-the-air channel to serve as a conduit to ensure the recording is properly timed. This means it will not compensate for a disrupted schedule due to live sporting events or bulletins for breaking news events, however many video recorders with these systems also incorporate Programme Delivery Control (PDC) and use that to alter times if possible.\n\nThe system has been licensed to television and VCR manufacturers in about 40 countries, but is branded under different names depending on the country. It is known as VCR Plus+ in the United States and Canada; G-Code in Japan, China, New Zealand and Australia; VideoPlus+ in Ireland and the United Kingdom; and ShowView in the rest of Europe as well as in South Africa. The system is branded as VideoPlus+/ShowView in Europe due to an existing trademark registration for \"VCR\" by Philips in that continent, and as G-Code (the \"G\" standing for the system's developer Gemstar) in Japan because VCR is not a common abbreviation there (\"VTR,\" for videotape recorder, is used instead). Because television programming schedules are different, the coding has to be adjusted in each of the regions and recording equipment is not interchangeable.\n\nThe actual algorithms used to encode and decode the television guide values from and to their time representations were published in 1992, but only for six-digit codes or less.\n\nSource code for seven and eight digit codes was written in C and Perl and posted anonymously in 2003. \n\n"}
{"id": "34685057", "url": "https://en.wikipedia.org/wiki?curid=34685057", "title": "Walter D'Arcy Ryan", "text": "Walter D'Arcy Ryan\n\nWalter D'Arcy Ryan (Kentville, Nova Scotia, Canada, April 17, 1870 – Schenectady, New York, USA, March 14, 1934) was an influential early lighting engineer who worked for General Electric as director of its Illuminating Engineering Laboratory. He pioneered skyscraper illumination, designed the Scintillator colored searchlights display, and was responsible for the lighting of the Panama-Pacific International Exposition in San Francisco and the Century of Progress Exposition in Chicago, in addition to the first complete illumination of Niagara Falls. He combined illumination into both an art and a science.\n\nRyan was born in Kentville, Nova Scotia, and educated in Canada for a military career, but instead emigrated to the United States around 1890. He worked for General Electric in Lynn, Massachusetts, and was rapidly promoted and put in charge of the Commercial Department, which developed into the Illuminating Engineering Laboratory, the world's first institution for research into lighting; this was formally established around 1908 in Schenectady, New York, with him at its head. He and his team developed and patented much of the technology for lighting applications, including the Ryan-Lite reflector-equipped headlamp.\n\nIn particular, under Ryan the Illuminating Engineering Laboratory developed the ornamental streetlighting scheme called the White Way after Broadway, and General Electric promoted it to towns and cities. The first installation was on Broadway Avenue in Los Angeles in 1905. Ryan described it as a way to provide \"cosmopolitan atmosphere and dignified aesthetic effects\".\n\nIn 1908, with Charles G. Armstrong, Ryan was responsible for the first illumination of an entire skyscraper: the new tower of the Singer Building on lower Broadway in New York City was lighted from the base to the 35th floor with arc searchlights, while the top of the tower was outlined with 1,600 incandescent bulbs, the technique that had been used to illuminate buildings up to that time. The illumination, which was planned when the building was designed, was bright enough for colors to be visible, but was patchy; Singer used retouched photographs for advertising. He later illuminated other skyscrapers, in particular in 1912 the General Electric Company Building in Buffalo, New York, which he lighted with arc searchlights and a revolving searchlight in changing colors on the top of the tower; this scheme prefigured the colored floodlighting of skyscrapers and Ryan's own work for the Panama-Pacific International Exposition three years later. Ryan was also responsible for lighting the interior of that building.\n\nRyan was in charge of lighting for the New York Hudson-Fulton Celebration of 1909, and for this lighted up all the major buildings, East River bridges, public places, and some stretches of the coastline of Manhattan. He installed colored searchlights on the Singer Building. The tower was reportedly visible from 40 miles away, and made a great impression:[W]hat every one of the visitors paused to gaze at was the Singer Building tower ... The main building was dark and gloomy, but from its center sprang a terra-cotta shaft set off with pale green pilasters rising to a golden cornice. The lights which illuminated it could not be seen, but it glowed against the sky.\n\nRyan also made use of searchlights for non-architectural display, in the Scintillator, sometimes called the Ryan Scintillator. This consisted of searchlights equipped with color filters and refracted through steam; the beams of light were also made to form different shapes, including a peacock's tail and a sunrise. It was on display at the Hudson-Fulton Celebration, with shows twice nightly at Riverside Drive and 155th Street.Forty huge searchlights of varying color shot enormous beams high in the air, now radiating in fan-like effect and changing from intensest white to the softer greens and yellows; now again shifting bodily from east to west and back again with frightful speed.The steam was supplied by a 200-horsepower boiler, and black powder and smoke bombs were also used.\n\nFor 30 nights in a row in 1907, he used 44 searchlights with colored filters, a form of the Scintillator, to illuminate the entirety of Niagara Falls for the first time. The \"New York Tribune\" reported: \"Presently the whole great stretch of the Falls was a mass of color; the whirling water beneath was like a pool of flame in the glow of the red searchlights.\"\n\nIn 1915, Ryan was the lighting designer for the San Francisco Panama-Pacific International Exposition. This marked the first widespread use of floodlighting at a fair, and also the first use of high-pressure gas mantle lamps and of high-wattage tungsten filament lamps. Previous expositions had used outline lighting with strings of incandescent bulbs and, more recently, arc lamps; Ryan restricted these to the \"Joy Zone\" (the midway) and also used screens, filters, and reflecting to manipulate the floodlights. The exhibition buildings were arranged around courtyards, as had become standard; a different color was used for each, down to guards' uniforms, trashcans and sand. Ryan floodlit the facades at night with increasing intensity higher up the towers, and used different colors in each courtyard. The centerpiece of the fair was the Tower of Jewels, 435 feet tall and covered with 102,000 suspended, mirror-backed Austrian cut-glass prisms, some colored and some clear, which refracted sunlight by day and reflected 54 searchlight beams by night. Two buildings were lit from within at night, one of them an \"Electric Kaleidoscope\" created by a circle of 12 moving floodlight beams aimed upwards at the glass dome of the Palace of Horticulture. Ryan carefully concealed both exterior floodlights and the red incandescent bulbs used to pick out architectural details, in order to avoid shadows and eye fatigue. He also phased in the night-time lighting at dusk rather than abruptly turning the lights on as had previously been done. The ambient lighting was intended to be beautiful and intimate, while three times a week, Scintillators created awe-inspiring overhead effects on themes such as \"Scotch Plaid, Ghost Dance, and Fighting Serpents\". In an article on the exposition for \"General Electric Review\", Ryan himself wrote of the atmosphere he intended to create:Soft radiant energy is everywhere; lights and shadows abound, fire spits from the mouths of serpents into the flaming gas cauldrons and sends its flickering rays over the composite Spanish-Gothic-Oriental grandeur. Mysterious vapors rise from steam-electric cauldrons and also from the beautiful central fountain group symbolizing the Earth in formation. The floodlighting was cheaper to install, maintain, and power than the festoons of electric bulbs, and allowed both artistic effects with shimmering light, accents, and contrasts with dark surroundings, and realism, including the highlighting of architectural details. This fair set the pattern for the lighting of future fairs.\n\nAlmost twenty years later, in 1933, Ryan was also the lighting designer for Chicago's Century of Progress Exposition, which again was notable for its lighting innovation. To match the \"art moderne\" architecture, Ryan used the new electric gaseous discharge lamps, especially neon (over 75,000 feet of neon tubing in addition to more than 15,000 incandescent lamps). The fair was conceived of in large part as a lighting display; building surfaces were brightly colored, smooth, and shiny with few openings, for optimum reflectivity, and the fair opened at night (with a fireworks display ignited by light from the star Arcturus that had originated at the time of the previous Chicago World's Fair, converted into an electrical pulse by a photo-electric cell). The exterior lighting required 3,000 kilowatts of power; the General Electric building had a simulated waterfall on its side that alone required 5,000 feet of blue tube lighting. But the use of new technology enabled Ryan to obtain the maximum amount of light for the fair's limited funds. Scintillators were used again, with smoke effects from the fireworks, but this time the modern lighting fixtures were placed in full view. The largest electric incandescent filament lamp in the world, 50 kilowatts, was on display. For its second year, in 1934, the illumination was increased by about half, and the Ford Building had a Pillar of Light created by 24 searchlights projecting a mile into the sky. The closing program for the exposition on October 31, 1934, was titled \"The Festival of Illumination: The Apotheosis of Man-Made Light.\" Again, the fair was influential: the expositions at Dallas and Cleveland the following year both emulated both the architecture and the lighting of the Chicago fair.\n\nIn June 1932, Ryan became a consulting engineer for General Electric; he died on March 14, 1934, after a heart attack. \n"}
{"id": "632850", "url": "https://en.wikipedia.org/wiki?curid=632850", "title": "Waste Isolation Pilot Plant", "text": "Waste Isolation Pilot Plant\n\nThe Waste Isolation Pilot Plant, or WIPP, is the world's third deep geological repository (after closure of Germany's Repository for radioactive waste Morsleben and the Schacht Asse II Salt Mine) licensed to permanently dispose of transuranic radioactive waste for 10,000 years that is left from the research and production of nuclear weapons. The plant is estimated to incur a total cost of $19B.\n\nIt is located approximately east of Carlsbad, New Mexico, in eastern Eddy County, in an area known as the southeastern New Mexico nuclear corridor which also includes the National Enrichment Facility near Eunice, New Mexico, the Waste Control Specialists low-level waste disposal facility just over the border near Andrews, Texas, and the International Isotopes, Inc. facility to be built near Eunice, New Mexico.\n\nIn 2010, the USDOE withdrew previous plans to develop Yucca Mountain nuclear waste repository in Nevada. WIPP was identified as a candidate for a facility to store waste for nuclear weapons defense related waste. Various mishaps at the plant in 2014 brought focus to the problem of what to do with this growing backlog of waste and whether or not WIPP would be a safe repository. The 2014 incidents involved a waste explosion and airborne release of radiological material that exposed 21 plant workers to internal doses of plutonium, which can lead to cancer of the lungs, liver, and bones.\n\nIn 1970 the United States Atomic Energy Commission (later merged into the Department of Energy) proposed a site in Lyons, Kansas for the isolation and storage of radioactive waste. Ultimately the Lyons site was deemed unusable due to local and regional opposition, and in particular the discovery of unmapped oil and gas wells located in the area. These wells were believed to potentially compromise the ability of the planned facility to contain nuclear waste. In 1973, as a result of these concerns, and because of positive interest from the southern New Mexico community, the DOE relocated the site of the proposed nuclear waste repository, now called the Waste Isolation Pilot Plant (WIPP), to the Delaware Basin salt beds located near Carlsbad, New Mexico.\n\nThe Delaware Basin is a sedimentary basin formed largely during the Permian Period approximately 250 million years ago. It is one of three sub-basins of the Permian Basin in West Texas and Southeastern New Mexico. It contains a 1,500-2,800 meter thick column of sedimentary rock that includes some of the most oil- and gas-rich rocks in the United States. An ancient shallow sea repeatedly filled the basin and evaporated while the basin slowly subsided, leaving behind a nearly impermeable 1,000 meters thick layer of evaporites, primarily salt, in the Salado and Castile Formations, geologically similar to other basins created by evaporitic inland seas. Over time the salt beds were covered by an additional 300 meters of soil and rock. As drilling in the Salado Formation salt beds began in 1975, scientists discovered that at the edge of the basin there had been geological disturbances that had moved interbed layers into a nearly vertical position. In response, the site was moved toward the more stable center of the basin where the Salado Formation salt beds are the thickest and perfectly horizontal.\n\nSome observers suggested, early in the investigations, that the geological complexity of the basin was problematic, causing the hollowed-out caverns to be unstable. However, what is considered by some to be instability is considered by others to be a positive aspect of salt as a host rock. As early as 1957 the National Academy of Sciences recommended salt for radioactive waste disposal because at depth it would plastically deform, a motion called \"salt creep\" in the salt-mining industry. This would gradually fill in and seal any openings created by the mining, and in and around the waste.\n\nExact placement of the construction site in the Delaware Basin changed multiple times due to safety concerns. Brine deposits located below the salt deposits in the Delaware Basin posed a potential safety problem. The brine was first discovered when a 1975 drilling released a pressurized deposit of the liquid from below the repository level. Constructing the plant near one of these deposits could, under specific circumstances, compromise the facility’s safety. The brine could leak into the repository and either dissolve radioactivity or entrain particulate matter with radioactive waste to the surface. The contaminated brine would then need to be cleaned and properly disposed of. There is no drinking water near the site, so possible water pollution is not a concern. After multiple deep drilling, a final site was selected. The site is located approximately 40 km east of Carlsbad.\n\nWaste is placed in rooms underground that have been excavated within a thick salt formation (Salado and Castile Formations) where salt tectonics have been stable for more than 250 million years. Because of plasticity effects, salt and water will flow to any cracks that develop, a major reason why the area was chosen as a host medium for the WIPP project.\n\nIn order to address growing public unrest concerning construction of the WIPP, the New Mexico Environmental Evaluation Group (EEG) was created in 1978. This group, charged with overseeing the WIPP, verified statements, facts, and studies conducted and released by the DOE regarding the facility. The stewardship this group provided effectively lowered public fear and let the facility progress with little public opposition in comparison to similar facilities around the nation such as Yucca Mountain in Nevada.\n\nThe EEG, in addition to acting as a check for the government agencies overseeing the project, acted as a valuable advisor. In a 1981 drilling, pressurized brine was again discovered. The site was set to be abandoned when the EEG stepped in and suggested a series of tests on the brine and the surrounding area. These tests were conducted and the results showed that the brine deposit was relatively small and was isolated from other deposits. Drilling in the area was deemed safe due to these results. This saved the project valuable money and time by preventing a drastic relocation.\n\nIn 1979 Congress authorized construction of the facility. In addition to formal authorization, Congress redefined the level of waste to be stored in the WIPP from high temperature to transuranic, or low level, waste. Transuranic waste often consists of materials which have come in contact with radioactive substances such as plutonium and uranium. This often includes gloves, tools, rags, and assorted machinery often used in the production of nuclear fuel and weapons. Although much less potent than nuclear reactor byproducts, this waste still remains radioactive for approximately 24,000 years. This change in classification led to a decrease in safety parameters for the proposed facility, allowing construction to continue at a faster pace.\n\nThe first extensive testing of the facility was due to begin in 1988. The proposed testing procedures involved interring samples of low level waste in the newly constructed caverns. Various structural and environmental tests would then be performed on the facility to verify its integrity and to prove its ability to safely contain nuclear waste. Opposition from various external organizations delayed actual testing into the early 1990s. Attempts at testing were resumed in October 1991 with US Secretary of Energy James Watkins announcing that he would begin transportation of waste to the WIPP.\n\nDespite apparent progress on the facility, construction still remained costly and complicated. Originally conceptualized in the 1970s as a warehouse for waste, the repository now had regulations similar to those of nuclear reactors. As of December 1991, the plant had been under construction for 20 years and was estimated to have cost over one billion dollars. At the time, WIPP officials reported over 28 different organizations claimed authority over operations of the facility.\n\nIn November 1991, a federal judge ruled that Congress must approve WIPP before any waste, even for testing purposes, was sent to the facility. This indefinitely delayed testing until Congress gave its approval. The 102nd United States Congress passed legislation allowing use of the WIPP. The House of Representatives approved the facility on October 6, 1992 and the Senate passed a bill allowing the opening of the facility on October 8 of the same year. The bill was met with much opposition in the Senate. Senator Richard H. Bryan fought the bill based on safety issues that concerned a similar facility located in Nevada, the state for which he was serving as senator. His efforts almost prevented the bill from passing. New Mexico senators Pete V. Domenici and Jeff Bingaman effectively reassured Senator Bryan that these issues would be addressed in the 103rd Congress. The final legislation provided safety standards requested by the House and an expedited timeline requested by the Senate.\n\nThe final legislation mandated that the Environmental Protection Agency (EPA) issue revised safety standards for the facility. It also required the EPA to approve testing plans for the facility within ten months. The legislation stated that the security standards mandated in the bill were only applicable to the WIPP in New Mexico and not to other facilities in the United States. This clause caused Senator Bryan to oppose the bill, as he wanted safety standards mandated by the bill to apply to the facility in Nevada as well.\n\nIn 1994, Congress ordered Sandia National Laboratories to begin an extensive evaluation of the facility against the standards set forth by the EPA. Evaluation of the facility continued for four years, resulting in a cumulative total of 25 years of evaluation. In May 1998, the EPA concluded that there was \"reasonable expectation\" that the facility would contain the vast majority of the waste interred there.\n\nThe first nuclear waste arrived to the plant on March 26, 1999. This waste shipment was from Los Alamos National Laboratory, a major nuclear weapons research and development facility located north of Albuquerque, New Mexico. Another shipment followed on April 6 of the same year. These shipments marked the beginning of plant operations. As of December 2010, the plant had received and stored 9,207 shipments () of waste. The majority of this waste was transported to the facility via railroad or truck. The final facility contains a total of 56 storage rooms located approximately 650 meters underground. Each room is 100 yards in length. The plant is estimated to continue accepting waste for 25 to 35 years and is estimated to cost a grand total of 19 billion dollars.\nOn February 5, 2014 at around 11 am, a salt haul truck caught fire, prompting an evacuation of the underground facility. Six workers were taken to a local hospital with smoke inhalation, and were released by the next day. Lab tests after the fire confirmed there was zero release of radiological material during, or as a result of, the fire. Underground air monitoring equipment was out of commission after the truck fire.\n\nOn February 15, 2014, authorities ordered workers to shelter in place at the facility after air monitors had detected unusually high radiation levels at 11:30pm the previous day. None of the facility's 139 workers were underground at the time of the incident. Later, trace amounts of airborne radiation consisting of americium and plutonium particles were discovered above ground, a half mile from the facility. In total, 21 workers were exposed, as reported by the \"Wall Street Journal\". The \"Carlsbad Current-Argus\" wrote \"the radiation leak occurred on the evening of February 14, according to new information made public at a news conference [on February 20]. Joe Franco, manager of the DOE Carlsbad Field Office, said an underground air monitor detected high levels of alpha and beta radiation activity consistent [\"sic\"] the waste buried at WIPP.\" Regarding the elevated levels of plutonium and americium detected outside the nuclear waste repository, Ryan Flynn, New Mexico Environment Secretary stated during a news conference, \"Events like this simply should never occur. From the state's perspective, one event is far too many.\"\n\nOn February 26, 2014, the DOE announced 13 WIPP above ground workers had tested positive for exposure to radioactive material. Other employees were in the process of being tested. On Thursday, February 27, DOE announced it sent out \"a letter to tell people in two counties what they do know so far. Officials said it is too early to know what that means for the workers' health.\" Additional testing would be done on employees who were working at the site the day after the leak. Above ground, 182 employees continued to work. A February 27 update included comments on plans to discover what occurred below ground first by using unmanned probes and then people.\n\nThe Southwest Research and Information Center released a report on April 15, 2014 that one or more of 258 contact handled radioactive waste containers located in Room 7, Panel 7 of the underground repository released radioactive and toxic chemicals. The location of the leak was estimated to be approximately from the air monitor that triggered the contaminants in the filtration system. The contaminants were spread through more than of underground tunnels, leading to the exhaust shaft into the surrounding above-ground environment. Air monitoring station #107, located away, detected the radiotoxins. The filter from Station #107 was analyzed by the Carlsbad Environmental Monitoring and Research Center (SMERC) and found to contain 0.64 becquerels (Bq) per cubic meter of air of americium-241 and 0.014 Bq of plutonium-239 and plutonium-240 per cubic meter of air (equivalent to 0.64 and 0.014 radioactive decay events per second per cubic meter of air). The DOE agreed that there was a release of radioactivity from the repository, and confirmed that \"The event took place starting at 14 February 2014 at 23:14 and continued to 15 February 2014 14:45. The DOE also confirmed that \"A large shift in wind direction can be seen to occur around 8:30 AM on 2/15/14.\" The EPA reported on the radiological release on their WIPP News page.\n\nAfter analysis by CMERC, the Station A filter was found on February 15, 2014 to be contaminated with 4,335.71 Bq of Am-241 per cubic meter, and 671.61 Bq of plutonium-239 and plutonium-240 per cubic meter. Bob Alvarez, former DOE official, stated that the long-term ramifications of the WIPP issue are grounded in the fact that the DOE has of transuranic waste that has not been disposed of due to the fact that there are no long-term disposition plans for transuranic waste, including 5 tons of plutonium that are in-situ at the Savannah River Site, as well as water from the Hanford Nuclear Reservation in Washington State. In an article in the Bulletin of the Atomic Scientists, Alvarez wrote that \"Wastes containing plutonium blew through the WIPP ventilation system, traveling 2,150 feet to the surface, contaminating at least 17 workers, and spreading small amounts of radioactive material into the environment.\" The URS Corporation, who oversees WIPP removed and demoted the contracted manager of the repository. Alvarez ponders the notion of \"contract handling\" of radioactive waste because it deploys conventional processing practices that do not take into consideration the tens of thousands of containers buried before 1970 at several Department of Energy sites. Alvarez states that the quantity of this pre-1970 plutonium waste is 1,300 times more than the amount permitted to \"leak\" into the environment at WIPP; however, much of this waste is simply buried a few feet underground at DOE sites.\n\nThe source of contamination was later found to be a barrel that exploded on February 14 because contractors at Los Alamos National Laboratory packed it with organic cat litter instead of clay cat litter. Other barrels with the same problem were then sealed in larger containers. Anthropologist Vincent Ialenti has examined the political, social, and financial triggers to this organic kitty litter error in detail, linking it to the accelerated pace of the Department of Energy's and State of New Mexico's 3706 nuclear waste cleanup campaign, which ran from 2011-2014. Ialenti's study was published in The Bulletin of the Atomic Scientists in July 2018.\n\nThe 2014 incidents raised the question of whether or not WIPP would be a safe replacement for the Yucca Mountain nuclear waste repository in Nevada, as a destination for all waste generated at U.S. commercial nuclear power plants. The cost of the 2014 accident was expected to exceed $2 billion and disrupted other programs in various nuclear industry sites. On January 9, 2017, the plant was formally reopened after three years of cleanup costing $500 million which is significantly less than forecasted. On April 10, the plant received its first shipment of waste since reopening.\n\nThe Waste Isolation Pilot Plant is where the highest temperature ever recorded in New Mexico at occurred during the summer of 1994.\n\nFollowing the interment of waste in the facility, estimated to be sometime between 2025 and 2035, the storage caverns will be collapsed and sealed with 13 layers of concrete and soil. Salt will then seep into and fill the various fissures and cracks surrounding the casks of waste. After approximately 75 years, the waste will be completely isolated from the environment.\n\nThe Yucca Mountain Nuclear Waste Repository is an unfinished, currently defunct deep geological repository in Nye County, Nevada. In 1987, Congress selected Yucca Mountain to be researched as the potential first permanent repository of nuclear waste, and directed the Department of Energy (DOE) to disregard other proposed sites and study the Yucca Mountain exclusively. Though under the Trump Administration, all long term storage researched has ceased, leading to the responsibility of nuclear waste management to the energy provider. These High-Level radioactive Waste (HLW) materials are now kept on-site within cemented dry casks, leaving the U.S. with no designated long-term storage site for HLW.\nWaste that is to be disposed of at WIPP must meet certain \"waste acceptance criteria\". It accepts transuranic waste generated from DOE activities. The waste must have radioactivity exceeding per gram from TRUs that produce alpha radiation with a half life greater than 20 years. This criterion includes plutonium, uranium, americium, and neptunium among others. Mixed waste contains both radioactive and hazardous constituents, and WIPP first received mixed waste on September 9, 2000. Mixed waste is joint-regulated by the EPA and the New Mexico Environment Department.\n\nThe containers may also contain a limited amount of liquids. The energy released from radioactive materials will dissociate water into hydrogen and oxygen (radiolysis). This could then create a potentially explosive environment inside the container. The containers must be vented, as well, to prevent this from happening.\n\nWaste is placed in rooms underground that have been excavated within a thick salt formation (Salado and Castile Formations) where salt tectonics have been stable for more than 250 million years. Because of plasticity effects, salt and water will flow to any cracks that develop, a major reason why the area was chosen as a host medium for the WIPP project. Because drilling or excavation in the area will be hazardous long after the area is actively used, there are plans to construct markers to deter inadvertent human intrusion for the next ten thousand years.\n\nThe Salado Formation is a massive bedded salt deposit (>99% NaCl) that has a simple hydrogeology. Because massive NaCl is somewhat plastic and holes close under pressure, the rock becomes non-porous by effectively closing pores and fractures. This has a significant effect on the overall hydraulic conductivities (water permeabilities) and molecular diffusion coefficients. These are on the order of ≤10 m/s and ≤10 m/s respectively.\n\nSince 1983, the DOE has been working with linguists, archaeologists, anthropologists, materials scientists, science fiction writers, and futurists to come up with a warning system. For the case of the WIPP, the markers, called \"passive institutional controls\", will include an outer perimeter of thirty-two -tall granite pillars built in a four-mile (6 km) square. These pillars will surround an earthen wall, tall and wide. Enclosed within this wall will be another 16 granite pillars. At the center, directly above the waste site, will sit a roofless, granite room providing more information. The team intends to etch warnings and informational messages into the granite slabs and pillars.\n\nThis information will be recorded in the six official languages of the United Nations (English, Spanish, Russian, French, Chinese, Arabic) as well as the Native American Navajo language native to the region, with additional space for translation into future languages. Pictograms are also being considered, such as stick figure images and the iconic \"The Scream\" from Edvard Munch's painting. Complete details about the plant will not be stored on site; instead, they would be distributed to archives and libraries around the world. The team plans to submit their final plan to the U.S. Government by around 2028.\n\nA portion of the site is used to house underground physics experiments which require shielding from cosmic rays. Although only moderately deep as such laboratories go (1585 meter water equivalent shielding), the site has several advantages. The salt is easy to excavate, dry (no water to pump out), and salt is much lower in naturally occurring radionuclides than rock.\n\nCurrent experiments housed at WIPP include the Enriched Xenon Observatory and Dark Matter Time Projection Chamber.\n\n\n\n"}
{"id": "588004", "url": "https://en.wikipedia.org/wiki?curid=588004", "title": "Wastewater treatment", "text": "Wastewater treatment\n\nwaste water treatment is a process used to convert wastewater into an effluent that can be returned to the water cycle with minimum impact on the environment, or directly reused. The latter is called water reclamation because treated wastewater can then be used for other purposes. The treatment process takes place in a wastewater treatment plant (WWTP), often referred to as a Water Resource Recovery Facility (WRRF) or a sewage treatment plant. Pollutants in municipal wastewater (households and small industries) are removed or broken down.\n\nThe treatment of wastewater is part of the overarching field of sanitation. Sanitation also includes the management of human waste and solid waste as well as stormwater (drainage) management. By-products from wastewater treatment plants, such as screenings, grit and sewage sludge may also be treated in a wastewater treatment plant.\n\nAlthough disposal or reuse occurs after treatment, it must be considered first. Since disposal or reuse are the objectives of wastewater treatment, disposal or reuse options are the basis for treatment decisions. Acceptable impurity concentrations may vary with the type of use or location of disposal. Transportation costs often make acceptable impurity concentrations dependent upon location of disposal, but expensive treatment requirements may encourage selection of a disposal location on the basis of impurity concentrations. Ocean disposal is subject to international treaty requirements. International treaties may also regulate disposal into rivers crossing international borders. Water bodies entirely within the jurisdiction of a single nation may be subject to regulations of multiple local governments. Acceptable impurity concentrations may vary widely among different jurisdictions for disposal of wastewater to evaporation ponds, infiltration basins, or injection wells.\n\nPhase separation transfers impurities into a non-aqueous phase. Phase separation may occur at intermediate points in a treatment sequence to remove solids generated during oxidation or polishing. Grease and oil may be recovered for fuel or saponification. Solids often require dewatering of sludge in a wastewater treatment plant. Disposal options for dried solids vary with the type and concentration of impurities removed from water.\n\nProduction of waste brine, however, may discourage wastewater treatment removing dissolved inorganic solids from water by methods like ion exchange, reverse osmosis, and distillation. \n\nSolids and non-polar liquids may be removed from wastewater by gravity when density differences are sufficient to overcome dispersion by turbulence. Gravity separation of solids is the primary treatment of sewage, where the unit process is called \"primary settling tanks\" or \"primary sedimentation tanks\". It is also widely used for the treatment of other wastewaters. Solids that are heavier than water will accumulate at the bottom of quiescent settling basins. More complex clarifiers also have skimmers to simultaneously remove floating grease like soap scum and solids like feathers or wood chips. Containers like the API oil-water separator are specifically designed to separate non-polar liquids.\n\nColloidal suspensions of fine solids may be removed by filtration through fine physical barriers distinguished from coarser screens or sieves by the ability to remove particles smaller than the openings through which the water passes. Other types of water filters remove impurities by chemical or biological processes described below.\n\nOxidation reduces the biochemical oxygen demand of wastewater, and may reduce the toxicity of some impurities. Secondary treatment converts organic compounds into carbon dioxide, water, and biosolids. Chemical oxidation is widely used for disinfection.\n\nSecondary treatment by biochemical oxidation of dissolved and colloidal organic compounds is widely used in sewage treatment and is applicable to some agricultural and industrial wastewaters. Biological oxidation will preferentially remove organic compounds useful as a food supply for the treatment ecosystem. Concentration of some less digestible compounds may be reduced by co-metabolism. Removal efficiency is limited by the minimum food concentration required to sustain the treatment ecosystem.\n\nChemical oxidation may remove some persistent organic pollutants and concentrations remaining after biochemical oxidation. Disinfection by chemical oxidation kills bacteria and microbial pathogens by adding ozone, chlorine or hypochlorite to wastewater.\n\nPolishing refers to treatments made following the above methods. These treatments may also be used independently for some industrial wastewater. Chemical reduction or pH adjustment minimizes chemical reactivity of wastewater following chemical oxidation. Carbon filtering removes remaining contaminants and impurities by chemical absorption onto activated carbon. Filtration through sand (calcium carbonate) or fabric filters is the most common method used in municipal wastewater treatment.\n\nWastewater treatment plants may be distinguished by the type of wastewater to be treated, i.e. whether it is sewage, industrial wastewater, agricultural wastewater or leachate.\n\nA typical municipal sewage treatment plant in an industrialized country may include primary treatment to remove solid material, secondary treatment to digest dissolved and suspended organic material as well as the nutrients nitrogen and phosphorus, and – sometimes but not always – disinfection to kill pathogenic bacteria. The sewage sludge that is produced in sewage treatment plants undergoes sludge treatment. Larger municipalities often include factories discharging industrial wastewater into the municipal sewer system. The term \"sewage treatment plant\" is now often replaced with the term \"wastewater treatment plant\". Sewage can also be treated by processes using \"Nature-based solutions\". \n\nTertiary treatment is a term applied to polishing methods used following a traditional sewage treatment sequence. Tertiary treatment is being increasingly applied in industrialized countries and most common technologies are micro filtration or synthetic membranes. After membrane filtration, the treated wastewater is nearly indistinguishable from waters of natural origin of drinking quality (without its minerals). Nitrates can be removed from wastewater by natural processes in wetlands but also via microbial denitrification. Ozone wastewater treatment is also growing in popularity, and requires the use of an ozone generator, which decontaminates the water as ozone bubbles percolate through the tank, but this treatment is energy intensive. The latest, and very promising treatment technology is the use aerobic granulation.\n\nDisposal of wastewaters from an industrial plant is a difficult and costly problem. Most petroleum refineries, chemical and petrochemical plants have onsite facilities to treat their wastewaters so that the pollutant concentrations in the treated wastewater comply with the local and/or national regulations regarding disposal of wastewaters into community treatment plants or into rivers, lakes or oceans. Constructed wetlands are being used in an increasing number of cases as they provided high quality and productive on-site treatment. Other industrial processes that produce a lot of waste-waters such as paper and pulp production has created environmental concern, leading to development of processes to recycle water use within plants before they have to be cleaned and disposed.\n\nIndustrial wastewater treatment plants are required where municipal sewage treatment plants are unavailable or cannot adequately treat specific industrial wastewaters. Industrial wastewater plants may reduce raw water costs by converting selected wastewaters to reclaimed water used for different purposes. Industrial wastewater treatment plants may reduce wastewater treatment charges collected by municipal sewage treatment plants by pre-treating wastewaters to reduce concentrations of pollutants measured to determine user fees.\n\nAlthough economies of scale may favor use of a large municipal sewage treatment plant for disposal of small volumes of industrial wastewater, industrial wastewater treatment and disposal may be less expensive than correctly apportioned costs for larger volumes of industrial wastewater not requiring the conventional sewage treatment sequence of a small municipal sewage treatment plant.\n\nAn industrial wastewater treatment plant may include one or more of the following rather than the conventional primary, secondary, and disinfection sequence of sewage treatment:\n\nAgricultural wastewater treatment for continuous confined animal operations like milk and egg production may be performed in plants using mechanized treatment units similar to those described under industrial wastewater; but where land is available for ponds, settling basins and facultative lagoons may have lower operational costs for seasonal use conditions from breeding or harvest cycles.\n\nLeachate treatment plants are used to treat leachate from landfills. Treatment options include: biological treatment, mechanical treatment by ultrafiltration, treatment with active carbon filters and reverse osmosis using disc tube module technology.\n\n"}
{"id": "39533958", "url": "https://en.wikipedia.org/wiki?curid=39533958", "title": "Wearable generator", "text": "Wearable generator\n\nA wearable generator is an article of clothing that contains some form of electrical generation system built in. The concept encompasses a variety of generation systems intended to supply small amounts of power to keep portable electronics in a good state of charge through natural motions of the body.\n\nThere are many great projects related to wearable technology or wearable power generation. One concept, for example, is an article of clothing that has the ability to convert the movements of the wearer into electricity using nano-ion pumps. It is based on nanotechnology and has the ability to generate electricity for the purposes of building muscle mass and improving coordination. Emergency workers like firemen and paramedics could use chest-implanted sensors to create a floor plan of unfamiliar buildings; making a rookie perform his job as efficiently as a veteran. With cameras becoming cheaper and smaller, wearable generators may also serve as a quick method to recharge the batteries on those devices. The environmental burden of disposing used batteries has contributed to e-waste; something that wearable generators may drastically reduce. Enough energy can theoretically be harnessed from a person's body heat to power a smartphone or tablet.\n"}
