{"id": "3935483", "url": "https://en.wikipedia.org/wiki?curid=3935483", "title": "AN/APG-77", "text": "AN/APG-77\n\nThe AN/APG-77 is a multifunction low probability of intercept radar installed on the F-22 Raptor fighter aircraft. The radar is built by Northrop Grumman.\n\nIt is a solid-state, active electronically scanned array (AESA) radar. Composed of 1956 transmit/receive modules, each about the size of a gum stick, it can perform a near-instantaneous beam steering (in the order of tens of nanoseconds).\n\nThe APG-77 provides 120° field of view in azimuth and elevation, which is the highest possible value for a flat phased array antenna. Unconfirmed sources suggest that APG-77 has an operating range of , against a target. A range of 400 km or more, against a target, with the APG-77v1 with newer GaAs modules, is believed to be possible while using more narrow beams.\n\nMore than 100 APG-77 AESA radars have been produced to date by Northrop Grumman, and much of the technology developed for the APG-77 is being used in the APG-81 radar for the F-35 Lightning II.\n\nThe APG-77v1 was installed on F-22 Raptors from Lot 5 and on. This provided full air-to-ground functionality (high-resolution synthetic aperture radar mapping, ground moving target indication and track (GMTI/GMTT), automatic cueing and recognition, combat identification, and many other advanced features).\n\n\n"}
{"id": "15684432", "url": "https://en.wikipedia.org/wiki?curid=15684432", "title": "AVADirect", "text": "AVADirect\n\nAVADirect Custom Computers is an American computer manufacturing company located in Twinsburg, Ohio. It specializes in the design, engineering, and production of customized computer systems. \n\nFreedom USA, Inc. d/b/a AVADirect Custom Computers was founded in 2000. The company was originally established on the business model of building and selling custom computers at trade shows in the Northeast quadrant of the United States during the millennial PC boom. As the company grew, the demand for an online marketplace increased, and company founders wanted to give customers the opportunity to customize computers with the maximum number of hardware options. In 2003 Freedom USA, Inc. changed its name and business model from on-site computer retail to a custom computer E-commerce business model and AVADirect Custom Computers was born.\n"}
{"id": "47801928", "url": "https://en.wikipedia.org/wiki?curid=47801928", "title": "Action based learning questions", "text": "Action based learning questions\n\nAction-based Learning Questions are questions that are based on the approach of Action learning where one solves real-life problems that involve taking action and reflecting upon the results. There are two types of questions - Closed Questions and Open Questions. Closed questions involve a technique which does not allow the respondents to develop their response, they can just say 'Yes' or 'No' whereas Open questions allow the respondents to expand or explore in their response. Action-based learning questions are beneficial in the education system. \n\nOne of the keys to effective Action learning is asking the 'right question'. When asked to the right people at the right time, these questions result in obtaining the necessary information. The Action Learning process, which primarily uses a questioning approach, can be more helpful than offering advice because it assumes that each person has the capacity to find their own answers.\n\nNormally, the purpose of asking a question is to obtain information. However, in Action Learning, the purpose is to help someone else to do one or more of the following:\n\nClosed questions involve a technique which does not allow the respondents to develop their response. It can do so by limiting respondents with a strict, limited list of answer choices. Answers are mostly monosyllabic words or short phrases. For example, some closed questions can only be answered by a “Yes” or “No”.\n\nClosed questions should not be interpreted as simple questions. They can be of varying levels of difficulty, and may make the respondent think before answering. Take this phrase for example: \"When two quantities are dependent on each other, does an increase in one always leads to an increase in the other?\".\n\n\"Usage Of Closed Questions:\"\n\n\nOpen questions allow the respondent to expand or explore in their response, and do not have a single correct response. This gives the respondent the freedom to discover new ideas, consider different possibilities, and decide on the course of action which is right for them.\n\nOpen-ended questions are not always long - they may be short as well as open-ended. Shorter questions often have equal or greater impact than longer ones. When asking shorter questions, it is easier to be perceived as abrupt or even rude. When questioning an Action Learning set, it is important to be aware of one's tone and language. The goal is usually to ask challenging questions, or to challenge the respondent's perspective.\n\n\"Usage Of Open Questions:\"\n\n\n\n"}
{"id": "40209131", "url": "https://en.wikipedia.org/wiki?curid=40209131", "title": "AdaCamp", "text": "AdaCamp\n\nAdaCamp was a series of unconferences organized by the Ada Initiative. AdaCamp was the only conference that focused on women's participation in open technology and culture, including the development of free and open source software and contributions to projects like Wikipedia. AdaCamps were among the projects and resources the Ada Initiative provided to make workplaces more friendly for women.\n\nAdaCamps were held in Melbourne (January 2012), Washington, D.C. (July 2012), San Francisco (June 2013), Portland (June 2014), Berlin (October 2014), Bangalore (November 2015), and Montreal (April 2015). One hundred women from 10 countries participated at the July 2012 event, and it was larger than the initial Melbourne AdaCamp.\n\nCo-founder Valerie Aurora said that the reasons for AdaCamp included \"to make progress quickly on difficult problems, to share knowledge, and to network with each other.\" As an unconference, attendees lead sessions on subjects that they chose. Along with women interested in open source software, attendees could include women interested in open access, open education, hackerspaces, digital liberties activism, wiki culture, and other topics.\n\nIn June 2015, Ada Initiative organizers announced the end of AdaCamp and an upcoming open source \"AdaCamp Toolkit\", a series of planning documents meant to outline how to run an event like AdaCamp.\n\n\n"}
{"id": "31768648", "url": "https://en.wikipedia.org/wiki?curid=31768648", "title": "Agway", "text": "Agway\n\nAgway of DeWitt, New York, is an American agricultural business that offers feed for livestock and poultry, as well as seed, fertilizers, and herbicides. \n\nAgway was formed on July 25, 1964, from a merger between the Grange League Federation and Eastern States Farmers' Exchange. In 1965 the Pennsylvania Farm Bureau Cooperative merged into Agway.\n\nIn 1980, Agway purchased dairy company HP Hood of Lynnfield, Massachusetts. It was sold to Catamount Dairy Holdings, LP of Boston in 1996 as part of downsizing due to overall financial losses since 1990. Agway also owned a significant share of Curtice-Burns Foods, Inc. of Rochester, New York from 1966 to 1994, part of holding company Pro-Fac Cooperative, Inc. from nearby Pittsford, New York which included the Birds Eye frozen foods brand.\n\nIn 1999, Agway sold or closed all its retail outlets and sold their warehouse system to Southern States Cooperative. On October 1, 2002 the company filed for Chapter 11 bankruptcy. The brand name is currently owned by Southern States Cooperative.\n"}
{"id": "7221088", "url": "https://en.wikipedia.org/wiki?curid=7221088", "title": "Air conditioning", "text": "Air conditioning\n\nAir conditioning (often referred to as AC, A/C, or air con) is the process of removing heat and moisture from the interior of an occupied space, to improve the comfort of occupants. Air conditioning can be used in both domestic and commercial environments. This process is most commonly used to achieve a more comfortable interior environment, typically for humans and animals; however, air conditioning is also used to cool/dehumidify rooms filled with heat-producing electronic devices, such as computer servers, power amplifiers, and even to display and store some delicate products, such as artwork.\n\nAir conditioners often use a fan to distribute the conditioned air to an occupied space such as a building or a car to improve thermal comfort and indoor air quality. Electric refrigerant-based AC units range from small units that can cool a small bedroom, which can be carried by a single adult, to massive units installed on the roof of office towers that can cool an entire building. The cooling is typically achieved through a refrigeration cycle, but sometimes evaporation or free cooling is used. Air conditioning systems can also be made based on desiccants (chemicals which remove moisture from the air) and subterraneous pipes that can distribute the heated refrigerant to the ground for cooling.\n\nIn the most general sense, air conditioning can refer to any form of technology that modifies the condition of air (heating, (de-)humidification, cooling, cleaning, ventilation, or air movement). In common usage, though, \"air conditioning\" refers to systems which cool air. In construction, a complete system of heating, ventilation, and air conditioning is referred to as HVAC.\n\nSince prehistoric times, snow and ice were used for cooling. The business of harvesting ice during winter and storing for use in summer became popular towards the late 17th century. This practice was replaced by mechanical ice-making machines.\n\nThe basic concept behind air conditioning is said to have been applied in ancient Egypt, where reeds were hung in windows and were moistened with trickling water. The evaporation of water cooled the air blowing through the window. This process also made the air more humid, which can be beneficial in a dry desert climate. In ancient Rome, water from aqueducts was circulated through the walls of certain houses to cool them. Other techniques in medieval Persia involved the use of cisterns and wind towers to cool buildings during the hot season.\n\nThe 2nd-century Chinese mechanical engineer and inventor Ding Huan of the Han Dynasty invented a rotary fan for air conditioning, with seven wheels in diameter and manually powered by prisoners of the time. In 747, Emperor Xuanzong (r. 712–762) of the Tang Dynasty (618–907) had the \"Cool Hall\" (\"Liang Dian\" ) built in the imperial palace, which the \"Tang Yulin\" describes as having water-powered fan wheels for air conditioning as well as rising jet streams of water from fountains. During the subsequent Song Dynasty (960–1279), written sources mentioned the air conditioning rotary fan as even more widely used.\n\nIn the 17th century, the Dutch inventor Cornelis Drebbel demonstrated \"Turning Summer into Winter\" as an early form of modern air conditioning for James I of England by adding salt to water.\n\nModern air conditioning emerged from advances in chemistry during the 19th century, and the first large-scale electrical air conditioning was invented and used in 1902 by US inventor Willis Carrier. The introduction of residential air conditioning in the 1920s helped enable the great migration to the Sun Belt in the United States.\n\nIn 1758, Benjamin Franklin and John Hadley, a chemistry professor at Cambridge University, conducted an experiment to explore the principle of evaporation as a means to rapidly cool an object. Franklin and Hadley confirmed that evaporation of highly volatile liquids (such as alcohol and ether) could be used to drive down the temperature of an object past the freezing point of water. They conducted their experiment with the bulb of a mercury thermometer as their object and with a bellows used to speed up the evaporation. They lowered the temperature of the thermometer bulb down to while the ambient temperature was . Franklin noted that, soon after they passed the freezing point of water , a thin film of ice formed on the surface of the thermometer's bulb and that the ice mass was about thick when they stopped the experiment upon reaching . Franklin concluded: \"From this experiment one may see the possibility of freezing a man to death on a warm summer's day.\"\n\nIn 1820, English scientist and inventor Michael Faraday discovered that compressing and liquefying ammonia could chill air when the liquefied ammonia was allowed to evaporate. In 1842, Florida physician John Gorrie used compressor technology to create ice, which he used to cool air for his patients in his hospital in Apalachicola, Florida. He hoped to eventually use his ice-making machine to regulate the temperature of buildings. He even envisioned centralized air conditioning that could cool entire cities. Though his prototype leaked and performed irregularly, Gorrie was granted a patent in 1851 for his ice-making machine. Though his process improved the artificial production of ice, his hopes for its success vanished soon afterwards when his chief financial backer died and Gorrie did not get the money he needed to develop the machine. According to his biographer, Vivian M. Sherlock, he blamed the \"Ice King\", Frederic Tudor, for his failure, suspecting that Tudor had launched a smear campaign against his invention. Dr. Gorrie died impoverished in 1855, and the dream of commonplace air conditioning went away for 50 years.\n\nJames Harrison's first mechanical ice-making machine began operation in 1851 on the banks of the Barwon River at Rocky Point in Geelong, Australia. His first commercial ice-making machine followed in 1853, and his patent for an ether vapor compression refrigeration system was granted in 1855. This novel system used a compressor to force the refrigeration gas to pass through a condenser, where it cooled down and liquefied. The liquefied gas then circulated through the refrigeration coils and vaporized again, cooling down the surrounding system. The machine produced of ice per day.\n\nThough Harrison had commercial success establishing a second ice company back in Sydney in 1860, he later entered the debate over how to compete against the American advantage of ice-refrigerated beef sales to the United Kingdom. He wrote: \"Fresh meat frozen and packed as if for a voyage, so that the refrigerating process may be continued for any required period\", and in 1873 prepared the sailing ship \"Norfolk\" for an experimental beef shipment to the United Kingdom. His choice of a cold room system instead of installing a refrigeration system upon the ship itself proved disastrous when the ice was consumed faster than expected.\n\nIn 1902, the first modern electrical air conditioning unit was invented by Willis Carrier in Buffalo, New York. After graduating from Cornell University, Carrier found a job at the Buffalo Forge Company. There, he began experimenting with air conditioning as a way to solve an application problem for the Sackett-Wilhelms Lithographing and Publishing Company in Brooklyn, New York. The first air conditioner, designed and built in Buffalo by Carrier, began working on 17 July 1902.\n\nDesigned to improve manufacturing process control in a printing plant, Carrier's invention controlled not only temperature but also humidity. Carrier used his knowledge of the heating of objects with steam and reversed the process. Instead of sending air through hot coils, he sent it through cold coils (filled with cold water). The air was cooled, and thereby the amount of moisture in the air could be controlled, which in turn made the humidity in the room controllable. The controlled temperature and humidity helped maintain consistent paper dimensions and ink alignment. Later, Carrier's technology was applied to increase productivity in the workplace, and The Carrier Air Conditioning Company of America was formed to meet rising demand. Over time, air conditioning came to be used to improve comfort in homes and automobiles as well. Residential sales expanded dramatically in the 1950s.\n\nIn 1906, Stuart W. Cramer of Charlotte was exploring ways to add moisture to the air in his textile mill. Cramer coined the term \"air conditioning\", using it in a patent claim he filed that year as an analogue to \"water conditioning\", then a well-known process for making textiles easier to process. He combined moisture with ventilation to \"condition\" and change the air in the factories, controlling the humidity so necessary in textile plants. Willis Carrier adopted the term and incorporated it into the name of his company.\n\nShortly thereafter, the first private home to have air conditioning was built in Minneapolis in 1914, owned by Charles Gates. Realizing that air conditioning would one day be a standard feature of private homes, particularly in regions with warmer climate, David St. Pierre DuBose (1898-1994) designed a network of ductwork and vents for his home \"Meadowmont\", all disguised behind intricate and attractive Georgian-style open moldings. This building is believed to be one of the first private homes in the United States equipped for central air conditioning.\n\nIn 1945, Robert Sherman of Lynn, Massachusetts invented a portable, in-window air conditioner that cooled, heated, humidified, dehumidified, and filtered the air.\n\nThe first air conditioners and refrigerators employed toxic or flammable gases, such as ammonia, methyl chloride, or propane, that could result in fatal accidents when they leaked. Thomas Midgley, Jr. created the first non-flammable, non-toxic chlorofluorocarbon gas, \"Freon\", in 1928. The name is a trademark name owned by DuPont for any chlorofluorocarbon (CFC), hydrochlorofluorocarbon (HCFC), or hydrofluorocarbon (HFC) refrigerant. The refrigerant names include a number indicating the molecular composition (e.g., R-11, R-12, R-22, R-134A). The blend most used in direct-expansion home and building comfort cooling is an HCFC known as chlorodifluoromethane (R-22).\n\nDichlorodifluoromethane (R-12) was the most common blend used in automobiles in the U.S. until 1994, when most designs changed to R-134A due to the ozone-depleting potential of R-12. R-11 and R-12 are no longer manufactured in the U.S. for this type of application, so the only source for air-conditioning repair purposes is the cleaned and purified gas recovered from other air conditioner systems. Several non-ozone-depleting refrigerants have been developed as alternatives, including R-410A. It was first commercially used by Carrier Corp. under the brand name \"Puron\".\n\nModern refrigerants have been developed to be more environmentally safe than many of the early chlorofluorocarbon-based refrigerants used in the early- and mid-twentieth century. These include HCFCs (R-22, as used in most U.S. homes before 2011) and HFCs (R-134a, used in most cars) have replaced most CFC use. HCFCs, in turn, are supposed to have been in the process of being phased out under the Montreal Protocol and replaced by HFCs such as R-410A, which lack chlorine. HFCs, however, contribute to climate change problems. Moreover, policy and political influence by corporate executives resisted change. Corporations insisted that no alternatives to HFCs existed. The environmental organization Greenpeace provided funding to a former East German refrigerator company to research an alternative ozone- and climate-safe refrigerant in 1992. The company developed a hydrocarbon mix of isopentane and isobutane, but as a condition of the contract with Greenpeace could not patent the technology, which led to its widespread adoption by other firms. Their activist marketing first in Germany led to companies like Whirlpool, Bosch, and later LG and others to incorporate the technology throughout Europe, then Asia, although the corporate executives resisted in Latin America, so that it arrived in Argentina produced by a domestic firm in 2003, and then finally with giant Bosch's production in Brazil by 2004.\n\nIn 1995, Germany made CFC refrigerators illegal. DuPont and other companies blocked the refrigerant in the U.S. with the U.S. EPA, disparaging the approach as \"that German technology\". Nevertheless, in 2004, Greenpeace worked with multinational corporations like Coca-Cola and Unilever, and later Pepsico and others, to create a corporate coalition called Refrigerants Naturally!. Then, four years later, Ben & Jerry's of Unilever and General Electric began to take steps to support production and use in the U.S. In 2011 the EPA decided in favor of the ozone- and climate-safe refrigerant for U.S. manufacture.\n\nIn the refrigeration cycle, heat is transported from a colder location to a hotter area. As heat would naturally flow in the opposite direction, work is required to achieve this. A refrigerator is an example of such a system, as it transports the heat out of the interior and into its environment. The refrigerant is used as the medium which absorbs and removes heat from the space to be cooled and subsequently ejects that heat elsewhere.\n\nCirculating refrigerant vapor enters the compressor, where its pressure and temperature are increased. The hot, compressed refrigerant vapor is now at a temperature and pressure at which it can be condensed and is routed through a condenser. Here it is cooled by air flowing across the condenser coils and condensed into a liquid. Thus, the circulating refrigerant removes heat from the system and the heat is carried away by the air. The removal of this heat can be greatly augmented by pouring water over the condenser coils, making it much cooler when it hits the expansion valve.\n\nThe condensed, pressurized, and still usually somewhat hot liquid refrigerant is next routed through an expansion valve (often nothing more than a pinhole in the system's copper tubing) where it undergoes an abrupt reduction in pressure. That pressure reduction results in flash evaporation of a part of the liquid refrigerant, greatly lowering its temperature. The cold refrigerant is then routed through the evaporator. A fan blows the interior warm air (which is to be cooled) across the evaporator, causing the liquid part of the cold refrigerant mixture to evaporate as well, further lowering the temperature. The warm air is therefore cooled and is pumped by an exhaust fan/ blower into the room. To complete the refrigeration cycle, the refrigerant vapor is routed back into the compressor. In order for the process to have any efficiency, the cooling/evaporative portion of the system must be separated by some kind of physical barrier from the heating/condensing portion, and each portion must have its own fan to circulate its own \"kind\" of air (either the hot air or the cool air).\n\nModern air conditioning systems are not designed to draw air into the room from the outside, they only recirculate the increasingly cool air on the inside. Because this inside air always has some amount of moisture suspended in it, the cooling portion of the process always causes ambient warm water vapor to condense on the cooling coils and to drip from them down onto a catch tray at the bottom of the unit from which it must then be routed outside, usually through a drain hole. As this moisture has no dissolved minerals in it, it will not cause mineral buildup on the coils. This will happen even if the ambient humidity level is low. If ice begins to form on the evaporative fins, it will reduce circulation efficiency and cause the development of more ice, etc. A clean and strong circulatory fan can help prevent this, as will raising the target cool temperature of the unit's thermostat to a point that the compressor is allowed to turn off occasionally. A failing thermistor may also cause this problem. Refrigerators without a defrost cycle may have this same issue. Dust can also cause the fins to begin blocking air flow with the same undesirable result: ice.\n\nBy running an air conditioner's compressor in the opposite direction, the overall effect can be completely reversed and the indoor area will become heated instead of cooled (see heat pump). The engineering of physical and thermodynamic properties of gas–vapor mixtures is called psychrometrics.\n\nA heat pump is an air conditioner in which the refrigeration cycle can be reversed, producing heating instead of cooling in the indoor environment. They are also commonly referred to as a \"reverse cycle air conditioner\". The heat pump is significantly more energy efficient than electric resistance heating. Some homeowners elect to have a heat pump system installed as a feature of a central air conditioner. When the heat pump is in heating mode, the indoor evaporator coil switches roles and becomes the condenser coil, producing heat. The outdoor condenser unit also switches roles to serve as the evaporator, and discharges cold air (colder than the ambient outdoor air).\n\nAir-source heat pumps are more popular in milder winter climates where the temperature is frequently in the range of 4–13 °C (40–55 °F), because heat pumps become inefficient in more extreme cold. This is because ice forms on the outdoor unit's heat exchanger coil, which blocks air flow over the coil. To compensate for this, the heat pump system must temporarily switch back into the regular air conditioning mode to switch the outdoor evaporator coil \"back\" to being the condenser coil, so that it can heat up and defrost. A heat pump system will therefore have a form of electric resistance heating in the indoor air path that is activated only in this mode in order to compensate for the temporary indoor air cooling, which would otherwise be uncomfortable in the winter.\n\nThe icing problem becomes much more severe with lower outdoor temperatures, so heat pumps are commonly installed in tandem with a more conventional form of heating, such as a natural gas or oil furnace, which is used instead of the heat pump during harsher winter temperatures. In this case, the heat pump is used efficiently during the milder temperatures, and the system is switched to the conventional heat source when the outdoor temperature is lower.\n\nAbsorption heat pumps are a kind of air-source heat pump, but they do not depend on electricity to power them. Instead, gas, solar power, or heated water is used as a main power source. An absorption pump dissolves ammonia gas in water, which gives off heat. Next, the water and ammonia mixture is depressurized to induce boiling, and the ammonia is boiled off, which absorbs heat from the outdoor air.\n\nSome more expensive fixed window air conditioning units have a true heat pump function. However, a window unit may only have an electric resistance heater.\n\nIn very dry climates, evaporative coolers, sometimes referred to as swamp coolers or desert coolers, are popular for improving coolness during hot weather. An evaporative cooler is a device that draws outside air through a wet pad, such as a large sponge soaked with water. The sensible heat of the incoming air, as measured by a dry bulb thermometer, is reduced. The temperature of the incoming air is reduced, but it is also more humid, so the total heat (sensible heat plus latent heat) is unchanged. Some of the sensible heat of the entering air is converted to latent heat by the evaporation of water in the wet cooler pads. If the entering air is dry enough, the results can be quite substantial.\n\nEvaporative coolers tend to feel as if they are not working during times of high humidity, when there is not much dry air with which the coolers can work to make the air as cool as possible for dwelling occupants. Unlike other types of air conditioners, evaporative coolers rely on the outside air to be channeled through cooler pads that cool the air before it reaches the inside of a house through its air duct system; this cooled outside air must be allowed to push the warmer air within the house out through an exhaust opening such as an open door or window. These coolers cost less and are mechanically simple to understand and maintain.\n\nAir conditioning can also be provided by a process called free cooling which uses pumps to circulate a coolant (typically water or a glycol mix) from a cold source, which in turn acts as a heat sink for the energy that is removed from the cooled space. Common storage media are deep aquifers or a natural underground rock mass accessed via a cluster of small-diameter boreholes, equipped with heat exchanger. Some systems with small storage capacity are hybrid systems, using free cooling early in the cooling season, and later employing a heat pump to chill the circulation coming from the storage. The heat pump is added because the temperature of the storage gradually increases during the cooling season, thereby declining its effectiveness.\n\nFree cooling systems can have very high efficiencies, and are sometimes combined with seasonal thermal energy storage (STES) so the cold of winter can be used for summer air conditioning. Free cooling and hybrid systems are mature technology.\n\nSince humans perspire to provide natural cooling by the evaporation of perspiration from the skin, drier air (up to a point) improves the comfort provided. The comfort air conditioner is designed to create a 50% to 60% relative humidity in the occupied space.\n\nRefrigeration air conditioning equipment usually reduces the absolute humidity of the air processed by the system. The relatively cold (below the dewpoint) evaporator coil condenses water vapor from the processed air, much like an ice-cold drink will condense water on the outside of a glass. Therefore, water vapor is removed from the cooled air and the relative humidity in the room is lowered. The water is usually sent to a drain or may simply drip onto the ground outdoors.\nThe heat is rejected by the condenser which is located outside of room to be cooled.\n\nMost modern air-conditioning systems feature a dehumidification cycle during which the compressor runs while the fan is slowed as much as possible to reduce the evaporator temperature and therefore condense more water. When the temperature falls below a threshold, both the fan and compressor are shut off to mitigate further temperature drops; this prevents moisture on the evaporator from being blown back into the room. When the temperature rises again, the compressor restarts and the fan returns to low speed.\n\nOccasionally, to thaw any ice produced, the fan runs with the compressor shut down; this function is less effective when ambient temperatures are low.\n\nInverter air conditioners use the inside coil temperature sensor to keep the evaporator as cold as possible. When the evaporator is too cold, the compressor is slowed or stopped with the indoor fan running.\n\nA specialized air conditioner that is used only for dehumidifying is called a dehumidifier. It also uses a refrigeration cycle, but differs from a standard air conditioner in that both the evaporator and the condenser are placed in the same air path. A standard air conditioner transfers heat energy out of the room because its condenser coil releases heat outside. However, since all components of the dehumidifier are in the \"same\" room, no heat energy is removed. Instead, the electric power consumed by the dehumidifier remains in the room as heat, so the room is actually \"heated\", just as by an electric heater that draws the same amount of power.\n\nIn addition, if water is condensed in the room, the amount of heat previously needed to evaporate that water also is re-released in the room (the latent heat of vaporization). The dehumidification process is the inverse of adding water to the room with an evaporative cooler, and instead releases heat. Therefore, an in-room dehumidifier always will warm the room and reduce the relative humidity indirectly, as well as reducing the humidity directly by condensing and removing water.\n\nInside the unit, the air passes over the evaporator coil first, and is cooled and dehumidified. The now dehumidified, cold air then passes over the condenser coil where it is warmed up again. Then the air is released back into the room. The unit produces warm, dehumidified air and can usually be placed freely in the environment (room) that is to be conditioned.\n\nDehumidifiers are commonly used in cold, damp climates to prevent mold growth indoors, especially in basements. They are also used to protect sensitive equipment from the adverse effects of excessive humidity in tropical countries.\n\nIn a thermodynamically closed system, any power dissipated into the system that is being maintained at a set temperature (which is a standard mode of operation for modern air conditioners) requires that the rate of energy removal by the air conditioner increase. This increase has the effect that, for each unit of energy input into the system (say to power a light bulb in the closed system), the air conditioner removes that energy. To do so, the air conditioner must increase its power consumption by the inverse of its \"efficiency\" (coefficient of performance) times the amount of power dissipated into the system. As an example, assume that inside the closed system a 100 W heating element is activated, and the air conditioner has a coefficient of performance of 200%. The air conditioner's power consumption will increase by 50 W to compensate for this, thus making the 100 W heating element cost a total of 150 W of power.\n\nIt is typical for air conditioners to operate at \"efficiencies\" of significantly greater than 100%. However, it may be noted that the input electrical energy is of higher thermodynamic quality (lower entropy) than the output thermal energy (heat energy).\n\nAir conditioner equipment power in the U.S. is often described in terms of \"tons of refrigeration\", with each approximately equal to the cooling power of one short ton (2000 pounds or 907 kilograms) of ice melting in a 24-hour period. The value is defined as 12,000 BTU per hour, or 3517 watts. Residential central air systems are usually from 1 to 5 tons (3.5 to 18 kW) in capacity.\n\nFor residential homes, some countries set minimum requirements for energy efficiency. In the United States, the efficiency of air conditioners is often (but not always) rated by the seasonal energy efficiency ratio (SEER). The higher the SEER rating, the more energy efficient is the air conditioner. The SEER rating is the BTU of cooling output during its normal annual usage divided by the total electric energy input in watt hours (W·h) during the same period.\n\nthis can also be rewritten as:\n\nFor example, a 5000 BTU/h air-conditioning unit, with a SEER of 10, would consume 5000/10 = 500 Watts of power on average.\n\nThe electrical energy consumed per year can be calculated as the average power multiplied by the annual operating time:\n\nAssuming 1000 hours of operation during a typical cooling season (i.e., 8 hours per day for 125 days per year).\n\nAnother method that yields the same result, is to calculate the total annual cooling output:\n\nThen, for a SEER of 10, the annual electrical energy usage would be:\n\nSEER is related to the coefficient of performance (COP) commonly used in thermodynamics and also to the Energy Efficiency Ratio (EER). The EER is the efficiency rating for the equipment at a particular pair of external and internal temperatures, while SEER is calculated over a whole range of external temperatures (i.e., the temperature distribution for the geographical location of the SEER test). SEER is unusual in that it is composed of an Imperial unit divided by an SI unit. The COP is a ratio with the same metric units of energy (joules) in both the numerator and denominator. They cancel out, leaving a dimensionless quantity. Formulas for the approximate conversion between SEER and EER or COP are available.\n\nFrom equation (2) above, a SEER of 13 is equivalent to a COP of 3.43, which means that 3.43 units of heat energy are pumped per unit of work energy.\n\nThe United States now requires that residential systems manufactured in 2006 have a minimum SEER rating of 13 (although window-box systems are exempt from this law, so their SEER is still around 10).\n\nWindow unit air conditioners are installed in an open window. The interior air is cooled as a fan blows it over the evaporator. On the exterior the heat drawn from the interior is dissipated into the environment as a second fan blows outside air over the condenser. A large house or building may have several such units, allowing each room to be cooled separately.\n\nIn 1971, General Electric introduced a popular portable in-window air conditioner designed for convenience and portability.\n\nPackaged terminal air conditioner (PTAC) systems are also known as wall-split air conditioning systems. They are ductless systems. PTACs, which are frequently used in hotels, have two separate units (terminal packages), the evaporative unit on the interior and the condensing unit on the exterior, with an opening passing through the wall and connecting them. This minimizes the interior system footprint and allows each room to be adjusted independently. PTAC systems may be adapted to provide heating in cold weather, either directly by using an electric strip, gas, or other heater, or by reversing the refrigerant flow to heat the interior and draw heat from the exterior air, converting the air conditioner into a heat pump. While room air conditioning provides maximum flexibility, when used to cool many rooms at a time it is generally more expensive than central air conditioning.\n\nThe first practical semi-portable air conditioning unit was invented by engineers at Chrysler Motors and offered for sale starting in 1935.\n\nSplit-system air conditioners come in two forms: mini-split and central systems. In both types, the inside-environment (evaporative) heat exchanger is separated by some distance from the outside-environment (condensing unit) heat exchanger.\n\nA mini-split system typically supplies air conditioned and heated air to a single or a few rooms of a building. Multi-zone systems are a common application of ductless systems and allow up to 8 rooms (zones) to be conditioned from a single outdoor unit. Multi-zone systems typically offer a variety of indoor unit styles including wall-mounted, ceiling-mounted, ceiling recessed, and horizontal ducted. Mini-split systems typically produce per hour of cooling. Multi-zone systems provide extended cooling and heating capacity up to 60,000 Btu's.\n\nAdvantages of the ductless system include smaller size and flexibility for zoning or heating and cooling individual rooms. The inside wall space required is significantly reduced. Also, the compressor and heat exchanger can be located farther away from the inside space, rather than merely on the other side of the same unit as in a PTAC or window air conditioner. Flexible exterior hoses lead from the outside unit to the interior one(s); these are often enclosed with metal to look like common drainpipes from the roof. In addition, ductless systems offer higher efficiency, reaching above 30 SEER.\n\nThe primary disadvantage of ductless air conditioners is their cost. Such systems cost about US$1,500 to US$2,000 per ton (12,000 BTU per hour) of cooling capacity. This is about 30% more than central systems (not including ductwork) and may cost more than twice as much as window units of similar capacity.\"\n\nAn additional possible disadvantage is that the cost of installing mini splits can be higher than some systems. However, lower operating costs and rebates or other financial incentives—offered in some areas—can help offset the initial expense.\n\nCentral (ducted) air conditioning offers whole-house or large-commercial-space cooling, and often offers moderate multi-zone temperature control capability by the addition of air-louver-control boxes.\n\nIn central air conditioning, the inside heat-exchanger is typically placed inside the central furnace/AC unit of the forced air heating system which is then used in the summer to distribute chilled air throughout a residence or commercial building.\n\nThe heat-exchanger cools the air that is being forced through it by the furnace blower. As the warm air comes in contact with this cool surface the water in the air condenses. By pulling the water molecules from the air. According to the psychometric chart as relative humidity decreases in order to feel cool you will have to lower the temperature even more. A common way to counteract this effect is by installing a whole-home humidifier. Similarly, installing a high efficient system this need to turn the temperature down wont have such and influence on your energy costs.\n\nA multi-split system is a conventional split system, which is divided into two parts (evaporator and condenser) and allows cooling or heating of several rooms with one external unit. In the outdoor unit of this air conditioner there is a more powerful compressor, ports for connecting several traces and automation with locking valves for regulating the volume of refrigerant supplied to the indoor units located in the room.\n\n\"Difference between split system and multi-split system\":\n\nOther common types of air conditioning system are multi-split systems, the difference between separate split system and multi-split system in several indoor units. All of them are connected to the main external unit, but the principle of their operation is similar to a simple split-system.\n\nIts unique feature is the presence of one main external unit that connected to several indoor units. Such systems might be the right solution for maintaining the microclimate in several offices, shops, large living spaces. Just few of outdoor units do not worsen the aesthetic appearance of the building.The main external unit can be connected to several different indoor types: floor, ceiling, cassette, etc.\n\nBefore selecting the installation location of air conditioner, several main factors need to be considered. First of all, the direction of air flow from the indoor units should not fall on the place of rest or work area. Secondly, there should not be any obstacles on the way of the airflow that might prevent it from covering the space of the premises as much as possible. The outdoor unit must also be located in an open space, otherwise the heat from the house will not be effectively discharged outside and the productivity of the entire system will drop sharply. It is highly advisable to install the air conditioner units in easily accessible places, for further maintenance during operation.\n\nThe main problem when installing a multi-split system is the laying of long refrigerant lines for connecting the external unit to the internal ones. While installing a separate split system, workers try to locate both units opposite to each other, where the length of the line is minimal. Installing a multi-split system creates more difficulties, since some of indoor units can be located far from the outside. The first models of multi-split systems had one common control system that did not allow you to set the air conditioning individually for each room. However, now the market has a wide selection of multi-split systems, in which the functional characteristics of indoor units operate separately from each other.\n\nThe selection of indoor units has one restriction: their total power should not exceed the capacity of the outdoor unit. In practice, however, it is very common to see a multi-split system with a total capacity of indoor units greater than the outdoor capacity by at least 20%. However, it is wrong to expect better performance when all indoor units are turned on at the same time, since the total capacity of the whole system is limited by the capacity of the outdoor unit. Simply put, the outdoor unit will distribute all its power to all operating indoor units in such a way that some of the rooms may not have a very comfortable temperature level. However, the calculation of the total power is not simple, since it takes into account not only the nominal power of the units, but also the cooling capacity, heating, dehumidification, humidification, venting, etc.\n\nA portable air conditioner can be easily transported inside a home or office. They are currently available with capacities of about and with or without electric-resistance heaters. Portable air conditioners are either evaporative or refrigerative.\n\nThe compressor-based refrigerant systems are air-cooled, meaning they use air to exchange heat, in the same way as a car radiator or typical household air conditioner does. Such a system dehumidifies the air as it cools it. It collects water condensed from the cooled air and produces hot air which must be vented outside the cooled area; doing so transfers heat from the air in the cooled area to the outside air.\n\nA portable system has an indoor unit on wheels connected to an outdoor unit via flexible pipes, similar to a permanently fixed installed unit.\n\nHose systems, which can be \"monoblock\" or \"air-to-air\", are vented to the outside via air ducts. The \"monoblock\" type collects the water in a bucket or tray and stops when full. The \"air-to-air\" type re-evaporates the water and discharges it through the ducted hose and can run continuously.\n\nA single-hose unit uses air from within the room to cool its condenser, and then vents it outside. This air is replaced by hot air from outside or other rooms (due to the negative pressure inside the room), thus reducing the unit's overall efficiency.\n\nModern units might have a coefficient of performance of approximately 3 (i.e., 1 kW of electricity will produce 3 kW of cooling). A dual-hose unit draws air to cool its condenser from outside instead of from inside the room, and thus is more effective than most single-hose units. These units create no negative pressure in the room.\n\nEvaporative coolers, sometimes called \"swamp coolers\", do not have a compressor or condenser. Liquid water is evaporated on the cooling fins, releasing the vapor into the cooled area. Evaporating water absorbs a significant amount of heat, the latent heat of vaporisation, cooling the air. Humans and animals use the same mechanism to cool themselves by sweating.\n\nEvaporative coolers have the advantage of needing no hoses to vent heat outside the cooled area, making them truly portable. They are also very cheap to install and use less energy than refrigerative air conditioners.\n\nAir-conditioning engineers broadly divide air conditioning applications into \"comfort\" and \"process\" applications.\n\nComfort applications aim to provide a building indoor environment that remains relatively constant despite changes in external weather conditions or in internal heat loads.\n\nAir conditioning makes deep plan buildings feasible, for otherwise they would have to be built narrower or with light wells so that inner spaces received sufficient outdoor air via natural ventilation. Air conditioning also allows buildings to be taller, since wind speed increases significantly with altitude making natural ventilation impractical for very tall buildings. Comfort applications are quite different for various building types and may be categorized as:\n\nWomen have, on average, a significantly lower resting metabolic rate than men. Using inaccurate metabolic rate guidelines for air conditioning sizing can result in oversized and less efficient equipment, and setting system operating setpoints too cold can result in reduced worker productivity.\n\nIn addition to buildings, air conditioning can be used for many types of transportation, including automobiles, buses and other land vehicles, trains, ships, aircraft, and spacecraft.\n\nAir conditioning is common in the US, with 88% of new single-family homes constructed in 2011 including air conditioning, ranging from 99% in the South to 62% in the West. In Canada, air conditioning use varies by province. In 2013, 55% of Canadian households reported having an air conditioner, with high use in Manitoba (80%), Ontario (78%), Saskatchewan (67%), and Quebec (54%) and lower use in Prince Edward Island (23%), British Columbia (21%), and Newfoundland and Labrador (9%). In Europe, home air conditioning is generally less common. Southern European countries such as Greece have seen a wide proliferation of home air-conditioning units in recent years. In another southern European country, Malta, it is estimated that around 55% of households have an air conditioner installed.\nIn India AC sales have dropped by 40% due to higher costs and stricter energy efficiency regulations.\n\nProcess applications aim to provide a suitable environment for a process being carried out, regardless of internal heat and humidity loads and external weather conditions. It is the needs of the process that determine conditions, not human preference. Process applications include these:\n\nIn both comfort and process applications, the objective may be to not only control temperature, but also humidity, air quality, and air movement from space to space.\n\nIn hot weather, air conditioning can prevent heat stroke, dehydration from excessive sweating and other problems related to hyperthermia. Heat waves are the most lethal type of weather phenomenon in developed countries. Air conditioning (including filtration, humidification, cooling and disinfection) can be used to provide a clean, safe, hypoallergenic atmosphere in hospital operating rooms and other environments where proper atmosphere is critical to patient safety and well-being. It is sometimes recommended for home use by people with allergies.\n\nPoorly maintained water cooling towers can promote the growth and spread of microorganisms, such as \"Legionella pneumophila\", the infectious agent responsible for Legionnaires' disease, or thermophilic actinomycetes. As long as the cooling tower is kept clean (usually by means of a chlorine treatment), these health hazards can be avoided or reduced. Excessive air conditioning can have a negative effect on skin, causing it to dry out, and can also cause dehydration.\n\nInnovation in air conditioning technologies continues, with much recent emphasis placed on energy efficiency. Production of the electricity used to operate air conditioners has an environmental impact, including the release of greenhouse gases.\n\nCylinder unloaders are a method of load control used mainly in commercial air conditioning systems. On a semi-hermetic (or open) compressor, the heads can be fitted with unloaders which remove a portion of the load from the compressor so that it can run better when full cooling is not needed. Unloaders can be electrical or mechanical.\n\nAccording to a 2015 government survey, 87% of the homes in the United States use air conditioning and 65% of those homes have central air conditioning. Most of the homes with central air conditioning have programmable thermostats, but approximately two-thirds of the homes with central air do not use this feature to make their homes more energy efficient.\n\nAlternatives to continual air conditioning can be used with less energy, lower cost, and with less environmental impact. These include:\n\nIn an automobile, the A/C system will use around 4 horsepower (3 kW) of the engine's power, thus increasing fuel consumption of the vehicle.\n\nThe selection of the working fluids (refrigerants) has a significant impact not only on the performance of the air conditioners but on the environment as well. Most refrigerants used for air conditioning contribute to global warming, and many also deplete the ozone layer. CFCs, HCFCs, and HFCs are potent greenhouse gases when leaked to the atmosphere.\n\nThe use of CFC as a refrigerant was once common, including the refrigerants R-11 and R-12 (sold under the brand name \"Freon-12\"). Freon refrigerants were commonly used during the 20th century in air conditioners due to their superior stability and safety properties. When they are released accidentally or deliberately, these chlorine-bearing refrigerants eventually reach the upper atmosphere. Once the refrigerant reaches the stratosphere, UV radiation from the Sun homolytically cleaves the chlorine-carbon bond, yielding a chlorine radical. These chlorine radicals catalyze the breakdown of ozone into diatomic oxygen, depleting the ozone layer that shields the Earth's surface from strong UV radiation. Each chlorine radical remains active as a catalyst until it binds with another radical, forming a stable molecule and quenching the chain reaction.\n\nPrior to 1994, most automotive air conditioning systems used R-12 as a refrigerant. It was replaced with R-134a refrigerant, which has no ozone depletion potential. Old R-12 systems can be retrofitted to R-134a by a complete flush and filter/dryer replacement to remove the mineral oil, which is not compatible with R-134a.\n\nR22 (also known as HCFC-22) has a global warming potential about 1,800 times higher than CO. It was phased out for use in new equipment by 2010, and is to be completely discontinued by 2020. Although these gasses can be recycled when air conditioning units are disposed of, uncontrolled dumping and leaking can release gas directly into the atmosphere.\n\nIn the UK, the Ozone Regulations came into force in 2000 and banned the use of ozone depleting HCFC refrigerants such as R22 in new systems. The Regulation banned the use of R22 as a \"top-up\" fluid for maintenance between 2010 (for virgin fluid) and 2015 (for recycled fluid). This means that equipment that uses R22 can still operate, as long as it does not leak. Although R22 is now banned, units that use the refrigerant can still be serviced and maintained.\n\nThe manufacture and use of CFCs has been banned or severely restricted due to concerns about ozone depletion (see also Montreal Protocol). In light of these environmental concerns, beginning on November 14, 1994, the U.S. Environmental Protection Agency has restricted the sale, possession and use of refrigerant to only licensed technicians, per rules under sections 608 and 609 of the Clean Air Act.\n\nAs an alternative to conventional refrigerants, other gases, such as CO (R-744), have been proposed. R-744 is being adopted as a refrigerant in Europe and Japan. It is an effective refrigerant with a global warming potential of 1, but it must use higher compression to produce an equivalent cooling effect.\n\nIn 1992, a non-governmental organization, Greenpeace, was spurred by corporate executive policies and requested that a European lab find substitute refrigerants. This led to two alternatives, one a blend of propane (R290) and isobutane (R600a), and one of pure isobutane. Industry resisted change in Europe until 1993, and in the U.S. until 2011, despite some supportive steps in 2004 and 2008 (see Refrigerant Development above).\n\n\n"}
{"id": "20760005", "url": "https://en.wikipedia.org/wiki?curid=20760005", "title": "Albert Charles Bartlett", "text": "Albert Charles Bartlett\n\nAlbert Charles Bartlett was an electrical engineer working for the General Electric Company in Wembley. He had some correspondence with Wilhelm Cauer on the subject of filter designs.\n\nHe published a treatment of geometrically symmetrical 2-port networks in 1927 and is responsible for Bartlett's bisection theorem which shows that any symmetrical network can be transformed into a symmetrical lattice network.\n\nHe also patented the idea of using the method of an active amplifier with \"negative resistance\" to cancel the inductance of a telephone line.\n\n\n\n"}
{"id": "50952034", "url": "https://en.wikipedia.org/wiki?curid=50952034", "title": "Balance of plant", "text": "Balance of plant\n\nBalance of plant (BOP) is a term generally used in the context of power engineering to refer to all the supporting components and auxiliary systems of a power plant needed to deliver the energy, other than the generating unit itself. These may include transformers, inverters, supporting structures etc., depending on the type of plant.\n\n"}
{"id": "12947840", "url": "https://en.wikipedia.org/wiki?curid=12947840", "title": "Baseefa", "text": "Baseefa\n\nBaseefa (\"British Approval Service for Electrical Equipment in Flammable Atmospheres\") is a British certification body for equipment intended for use in potentially explosive atmospheres. It is based in Buxton, Derbyshire, England. Baseefa is an ATEX Notified Body and an IECEx Certification Body and as such can provide ATEX certification, IECEx certification and supporting services. \n\nBaseefa Limited has operated as a private company since 2001. Prior to that, it was a part of the UK Health and Safety Executive (HSE), a government body. In 2011 it was acquired by SGS S.A. and renamed 'SGS Baseefa Ltd'. \n\n\n"}
{"id": "12452102", "url": "https://en.wikipedia.org/wiki?curid=12452102", "title": "Bernat Mill", "text": "Bernat Mill\n\nThe Bernat Mill, also known as Capron Mill, and later Bachman Uxbridge Worsted Company, was a yarn mill in Uxbridge, Massachusetts, USA, that was for the most part destroyed by fire on July 21, 2007.\n\nThis mill complex at Uxbridge had been a hub of manufacturing for Bernat, once based in Jamaica Plain, Massachusetts. The town of Uxbridge was the site of Bernat's main manufacturing unit in the later 20th century. This was the third largest yarn mill in the U.S. The Bernat mill and the town of Uxbridge have a role in U.S. history, and the history of the American Textile manufacturing.\n\n\"Bernat\" is a trademark of an existing company (Bernat.com) which manufactures yarn products.\n\nThe original mill, the \"Capron Mill\", was built in 1820, by John Capron, the father of Colonel John Capron and anti-slavery champion Effingham Capron on the Mumford River at Uxbridge Center. The first power looms for woolens were introduced at the Capron Mills. These were made in a machine shop at Cumberland, Rhode Island. These were reportedly the first power looms ever made for woolens in the USA. The first manufacture of \"satinet\" was at this mill. Uxbridge became famous for its Cashmere wool. The period of the Capron Mill, and later as the Bachman Uxbridge Worsted Company was famous for manufacture of clothing and the manufacture of military uniforms for the United States.\n\nThe mill was featured in the August 24, 1953 edition of \"Time Magazine,\" in an article entitled, \"The Pride of Uxbridge\" as the site of the \"Bachman Uxbridge Worsted Company\", which was then one of the most successful textile mills in New England. The \"Time Magazine\" article interviewed the CEO of Bachman Uxbridge Worsted Company, Harold Walter. This company had been started by Edward Bachman of New York City, and Harold Walters's father-in-law, Charles Arthur Root, of Uxbridge. This site was the hub of seven plants throughout the U.S., employing 6000 plus workers, and some of its wool synthetic blends dominated the women's fashion industry in the early 1950s.\n\nThe first woolen mill in the Blackstone Valley was built in Uxbridge in 1810 (3rd in US), and by 1953, the Bachman Uxbridge Worsted Company was on the verge of a merger with the debt laden American Woolen Company to become the largest US woolen manufacturer. The Town of Uxbridge had become synonymous with woolen manufacturing, \"blended fabrics\", complete vertical integration of textiles to clothing, and textile industry efficiency innovations, when production peaked in the early 1950s.\n\nResearch into textiles at Bachman Uxbridge Worsted Company produced a range of blended fabrics, including the \"wool-nylon serge\" used for army uniforms. The original U.S. Air Force Uniform produced at the factory was dubbed and patented \"Uxbridge Blue\" or \"Uxbridge 1683\", after blue dye color selected at \"Bachman Uxbridge\". This dye was used in the manufacture of uniforms from 1947.\n\n\"In January 1948 President Truman approved authorization of the proposed new blue Air Force uniform and a week after that Air Force Chief of Staff Hoyt Vandenberg officially circulated word that funding had been approved by the congressional appropriations committee. The new uniform, incorporating a shade of blue fabric (patented as 'Uxbridge Blue' and based on \"Uxbridge 1683 Blue, cable shade 84\", developed at the former Bachman-Uxbridge Worsted Company) would be available for distribution by September 1950\".\n“Shortly thereafter, on March 29, 1954, Time magazine reported; \"American Woolen Co. will ask its stockholders to approve a merger with Bachmann Uxbridge Worsted Corp. As a combined operation, troubled American Woolen (1953 sales, $73,494,160; net loss, $9,476,981) and Bachmann Uxbridge (1953 sales, $52,609,000; profit, $272,000) would be by far the biggest woolen manufacturer in the country. Textron, Inc., which wants American Woolen to merge with it, and claims to own almost 4% of American Woolen's stock, plans to fight the merger,\" see TIME CLOCK - TIME”\n\nThe merger of American Woolen and Bachman Uxbridge was however blocked by Textron, which emerged in the 1960s as a Providence, RI based conglomerate. This was Bachman Uxbridge’s last bid to be the largest woolen company in America. By 1964 the assets of Bachman Uxbridge were sold to Bernat Yarn of Jamaica Plain, MA\n\nAmerican Civil War uniforms, World War I Khaki overcoats, and World War II U.S. Army uniforms have all been manufactured in this mill. Latch hook yarn kits were developed by \"Bernat\", here circa 1968 and the name of the mill changes to the Bernat Mill, then the third largest U.S. yarn mill.\n\nEarly, on the Saturday morning of July 21, 2007, a fire erupted at the historic mill, devastating the complex on Mendon and Depot Street. 600 firefighters from 66 communities battled the blaze, but the complex was nearly totally destroyed. It took three days to extinguish the flames fully. At the time of the fire, the structure, had ceased operating as a mill and had been converted into space containing 65 small businesses. The business losses following the fire were estimated in the millions of dollars and between 300-500 people lost their jobs.\n\nThe fire marshall's report concluded that there was unpermitted welding occurring in a mill business, the sprinkler was not operable, and that both contributed to the fire. Mill owners planned to rebuild. \nThe two-state incident command, disaster response was viewed as a regional model. Senator John Kerry introduced loans from his committee in the U.S. Senate to support the business owners impacted by the fire. Governor Deval Patrick left the National Governors' Conference in Michigan to return to Uxbridge to be present for the immediate recovery. Governor Patrick invoked immediate state and federal aide to victims and businesses of the Uxbridge mill fire. Hurricane Katrina funds were applied to the relief efforts. As of 2009, plans to rebuild were on hold.\n\n\n"}
{"id": "8946964", "url": "https://en.wikipedia.org/wiki?curid=8946964", "title": "Betamovie", "text": "Betamovie\n\nBetamovie is the brand name for a range of consumer grade camcorders developed by Sony for the Betamax format. By \"camcorder\" is understood a single unit comprising a video camera and a video recorder.\n\nBetamovie records analog video on a standard Betamax cassette.\n\nA range of models was manufactured for the PAL and NTSC formats. The first model, BMC-100P (PAL) and BMC-110 (NTSC) was released in 1983 making it the first commercial consumer grade camcorder. While only standard beta units were available in PAL, several SuperBeta models were produced for the NTSC format.\n\nDue to constructional limitations, the Betamovie has no playback function. It is only capable of recording. This limitation in combination with the decline of the Betamax format in the late 1980s, caused Sony to abandon the Betamovie - line just a few years after its initial release in favor of its newly developed Video8 format.\n\nAs far back as the 1960s, cameras were available for the reel-to-reel portable VTRs of that time. These cameras were similar in size and weight to the cine-cameras of the day. They generally used a single video camera tube. However, these systems were not in common use by ordinary consumers.\n\nAfter the introduction of VHS and Betamax formats, in the mid 1970s, videocassette recorders (VCR) started gaining mass market traction. Thus, in 1982, 10% of UK households owned a VCR. The first two-piece camera/VCR systems emerged around 1980. These units included a portable VCR, which the user would carry by a shoulder strap, and a separate camera, which was connected to the VCR by a special cable. These systems were cumbersome and heavy by today's standards. For example, the portable VCR, Sony SL-3000, from 1980 weighed around 9 kg. without the battery. The accompanying camera (e.g. HVC-2000P) would weigh around 3 kg. Thus, the complete setup could easily weigh in excess of 13 kg.\n\nIn order to be more appealing to the typical consumer wanting a practical device for recording home movies, a more compact and preferably one-piece device was needed. The world's first such device, the Betamovie BMC-100/110 was released in 1983 by Sony. Although the term was not in common use at that time, such a device would later become known as a camcorder, a single unit comprising a video camera and a video recorder. BMC-100/110 weighed just 2,5 kg. and was a much less cumbersome solution than its predecessors. The whole device could be supported on a user's shoulder. In order to achieve such weight and size reductions, several key components had to be miniaturized. One major requirement for a one-piece camcorder was miniaturizing the recording head drum. Sony's solution to this involved recording a non-standard video signal which would become standard only when played back on full-sized VCRs. A side effect of this was that Betamovie camcorders were record-only. Keeping in mind that instant playback is one of the main advantages of video cameras over cine-cameras, lack of a playback function presented a considerable limitation.\n\nIn 1984, JVC presented it own version of a camcorder, the GR-C1, for the VHS format. Although it too had a miniature head drum, the JVC engineers developed a different solution to drum miniaturization which made it possible to record a standard video signal on the tape. Thereby, the user of a VHS camcorder could review footage on location and copy it to another VCR for editing. Sony was unable to duplicate the functionality of the VHS camcorders. This disadvantage was a primary reason for the early loss of market share by the Betamovie.\n\nDespite this development, Sony held on to the Betamovie for a couple of years more releasing some more advanced models, especially for the NTSC market. However, in 1987, Sony finally abandoned the Betamovie in favor of its newly developed Video8 format.\n\nBetamovie uses the standard-size Betamax cassette, but the recording method is non-standard. The tape is wrapped 300° around a head drum nearly 45 mm. in diameter with a single dual-azimuth head to write the video tracks. Compared to the normal Betamax head drum of 75 mm, the Betamovie head drum spins at 2500 rpm rather than 1500 rpm. The fields on the tape are written at 120% the normal speed. As the head does not pass the tape at normal speed, the signal has to be electronically \"time-compressed\", before it is written.\n\nWhen inserted into a standard Betamax VCR, the tape can be played back. However, the writing system employed in the Betamovie is not reversible. Therefore, playback within the camcorder is not feasible.\n\nThe early models have an optical viewfinder, which lets one see exactly what one is recording by looking directly through the lens, via a system of mirrors and prisms - similar to an SLR stills camera. Some of the later models have an electronic viewfinder. They are still record-only devices, though.\n\nAlso, the early models use a cathode ray tube as their image sensor and the BMC-100/110 has manual focus. Later models use CCD image sensors and feature autofocus.\n\nAll Betamovies for the PAL format record in standard Betamax video mode. Some of the models for the NTSC format can record in the enhanced SuperBeta mode or even the Super Hi-Band Beta mode.\n\nReleased in 1983. The first model and the world's first consumer grade camcorder. BMC-100 is the PAL model and BMC-110 is the NTSC model.\n\nThe camera uses a cathode ray tube as its image sensor. It features 6X power zoom, manual focus, and an optical viewfinder. It requires 35 Lux to operate. The BMC-110 only records in BII.\n\nThis camera features auto-focus. Apart from this, it is identical to the first model.\n\nReleased in 1985. A substantial redesign. This camera uses a CCD sensor and features time and date settings. BMC-500 is the last PAL Betamovie.\n\nReleased in 1985. NTSC model. This is an industrial/professional camcorder. It is similar to BMC-660. However, like BMC-1000, it features an electronic viewfinder.\n\nReleased in 1986. NTSC model. This camera records in SuperBeta mode, BII only.\n\nReleased in 1987. NTSC model. An upgraded model. Features an electronic viewfinder and records in SuperBeta BI and Super Hi-Band Beta BI. It is still a record-only unit.\n\nEDC-55 became the final and most advanced iteration of camcorders for the Betamax format.\n\nActually, it does not belong to the Betamovie - lineup and is only mentioned in this article for the purpose of distinguishing it from the true Betamovie camcorders.\n\nIt is a semiprofessional/prosumer device that only records in the ED-Beta format, the final high-definition variant of the Betamax format. This camera produces over 550 lines of resolution, records in Hi-Fi stereo and features insert audio and video editing. Unlike the Betamovie camcorders, it can play back its own recordings. It was only released for the NTSC format.\n\nThe AC-M100/110 is a combined AC Adapter and charger for a single NP-11 rechargeable battery. The output voltage to the camera is 9,6 V, 1 A. The battery is charged with 14 V, 1,2 A.\n\nThe principal difference between the M100 and M110 appears to be that the M100 can run on 110 - 240 V AC while the M110 can run on 100 - 240 V AC.\n\nThe BC-300 can charge three NP-11 batteries simultaneously.\n\nA Ni-Cad battery providing approximately one hour of continuous operation.\n\nA hard shell carrying case.\n\nLC-710 and LC-720 are designed for the BMC-100/110/200/220. The larger LC-710 can hold the camcorder, two NP-11 batteries and an AC-M100/110 adapter. The smaller LC-720 has no room for an AC-power adapter.\n\nLC-760 is for the BMC-500/550/660. It can hold the camcorder, two NP-11 batteries and a BC-300 adapter.\n\nLC-770 is for the GCS-1 and BMC-1000. It can hold the camcorder, two NP-11 batteries and a BC-300 adapter.\n\nLC-810 is a flexible jacket for protecting the body of a BMC-100/110/200/220.\n\nLC-850 is for BMC-500/550/660.\n\nAn externally mounted boom microphone.\n\nA wired remote control with a Record/Pause button.\n\nA car adapter with cable for connecting to the cigarette lighter socket in a car.\n\nVHS-C A competing camcorder tape format.\n\nVideo8 A successor format to Betamovie.\n"}
{"id": "29887866", "url": "https://en.wikipedia.org/wiki?curid=29887866", "title": "Bistable structure", "text": "Bistable structure\n\nIn mechanical engineering, a bistable structure is one that has two stable mechanical shapes, particularly where they are stabilized by different curvature axes.\n\nA common example of a bistable structure is a slap bracelet.\n\nBistable structures enable long tube-like structures to roll up into small cylinders.\n"}
{"id": "43217289", "url": "https://en.wikipedia.org/wiki?curid=43217289", "title": "Canadian Government Motion Picture Bureau", "text": "Canadian Government Motion Picture Bureau\n\nThe Canadian Government Motion Picture Bureau was created in September 1918 by an order in council as the Exhibits and Publicity Bureau and was renamed the Canadian Government Motion Picture Bureau on April 1, 1923 . The body was under the administration of the federal Department of Trade and Commerce.\n\nThe CGMPB was the first national film production unit in the world and was intended to promote trade and industry. Its first success was the bi-weekly series, \"Living Canada\", which began production in 1919 and was distributed throughout the British Empire and in numerous foreign countries including the United States.\n\nIts purpose, according to the Minister of Trade and Commerce, was \"advertising abroad Canada's scenic attractions, agricultural resources and industrial development\" and much of its production was devoted to producing travelogues and industrial films. It also produced early Canadian documentaries such as \"Lest We Forget\" (1935), a compilation film (using newsreel footage with staged sequences) recounting Canada's role in the First World War, written, directed and edited by Frank Badgley, the director of the Bureau from 1927 to 1941, and \"The Royal Visit\" (1939), co-written and edited by Badgley, which documented the 1939 royal tour of Canada by King George VI and his consort, Queen Elizabeth.\n\nThe Bureau's heyday was in the period from 1920 to 1931 when it had the largest and best equipped film studio in Canada and distributed its films throughout Canada and the British Empire as well as France, Belgium, the Netherlands, Argentina, Chile, Japan, China and the United States. At its peak in 1927, the Bureau had more than one thousand prints circulating in the United States alone. The Bureau continued to produce silent films until 1934. As a result, several government departments began producing their own promotional films rather than relying on the CGMPB.\n\nIn 1938, responding to a report by Canadian High Commissioner to the United Kingdom Vincent Massey recommending an in-depth study of the governments production of promotional films and concerns about American domination of screentime in Canadian theatres, Prime Minister William Lyon Mackenzie King commissioned British documentary film maker John Grierson to review the situation and make recommendations which became the basis of the \"National Film Act\" (1939) and the creation of the National Film Commission (later the National Film Board of Canada) which went on to absorb the CGMPB in 1941.\n\n\n"}
{"id": "38771512", "url": "https://en.wikipedia.org/wiki?curid=38771512", "title": "Cartosat", "text": "Cartosat\n\nThe Cartosat series of satellites are a type of earth observation satellites indigenously built by India. Up till now 8 Cartosat satellites have been launched by ISRO. The Cartosat series is a part of the Indian Remote Sensing Programme. They were specifically launched for Earth’s resource management and monitoring.\n\nThe first Cartosat satellite was Cartosat-1 which was launched by PSLV-C6, on May 5, 2005 from the then newly built Second Launch Pad at Sriharikota. Department of Space (DOS), Government of India had earlier launched a series of satellites for Earth’s resource management and monitoring. These satellites had been very successful in providing data in various scales ranging from 1:1 Million to 1:12,500 scale. Each of the Indian Remote Sensing satellite missions ensured data continuity while introducing improvements in the spatial, spectral and radiometric resolutions. Considering increase demand for large scale and topographic mapping data DOS launched the Cartosat-1 in 2005.\n\nCartosat-1 was launched by PSLV-C6 on 5 May 2005 from Satish Dhawan Space Centre's SLP at Sriharikota. Images from the satellite is available from GeoEye for worldwide distribution. The satellite covers the entire globe in 1867 orbits on a 126-day cycle. It carries two state-of-the-art panchromatic (PAN) cameras that take black and white stereoscopic pictures of the earth in the visible region of the electromagnetic spectrum. The two cameras with 2.5 m spatial resolution, acquire two images simultaneously, one forward looking (FORE)at +26 degrees and one aft of the satellite at -5 degrees for near instantaneous stereo data. The time difference between the acquisitions of the same scene by the two cameras is about 52 seconds.\n\nCartosat-2 was launched by PSLV-C7 on 10 January 2007 from Satish Dhawan Space Centre's FLP at Sriharikota. Cartosat-2 carries a state-of-the-art panchromatic (PAN) camera that take black and white pictures of the earth in the visible region of the electromagnetic spectrum. The swath covered by this high resolution PAN camera is 9.6 km and their spatial resolution is less than 1 metre. The satellite can be steered up to 45 degrees along as well as across the track. Cartosat-2 is an advanced remote sensing satellite capable of providing scene-specific spot imagery. The data from the satellite is used for detailed mapping and other cartographic applications at cad-astral level, urban and rural infrastructure development and management, as well as applications in Land Information System (LIS) and Geographical Information System (GIS).\n\nCartosat-2A was launched by PSLV-C9 on 28 April 2008 from Satish Dhawan Space Centre's SLP at Sriharikota along with 9 other satellites. It is a dedicated satellite for the Indian Armed Forces which is in the process of establishing an Aerospace Command. The satellite carries a panchromatic (PAN) camera capable of taking black-and-white pictures in the visible region of electromagnetic spectrum. The highly agile Cartosat-2A can be steered up to 45 degrees along as well as across the direction of its movement to facilitate imaging of any area more frequently.\n\nCartosat-2B was launched by PSLV-C15 on 12 July 2010 from Satish Dhawan Space Centre's FLP at Sriharikota. The satellite carries a panchromatic (PAN) camera capable of taking black-and-white pictures in the visible region of electromagnetic spectrum. The highly agile CARTOSAT-2B can be steered up to 26 degrees along as well as across the direction of its movement to facilitate imaging of any area more frequently.\n\nCartosat-2C is a much more capable satellite, having a resolution of 25 cm (10\"). It uses 1.2 m optics with 60% of weight removal compared to Cartosat-2. Other features include the use of adaptive optics, acousto optical devices, in-orbit focusing using MEMs and large area-light weight mirrors. The satellite was to be launched on board PSLV C-34 during 2014, but had been delayed and was finally launched on 22 June 2016. Potential uses include weather mapping, cartography, and strategic applications.\n\nCartosat-2D was launched by PSLV-C37 on 15 February 2017 from Satish Dhawan Space Centre's FLP at Sriharikota.\n\nCartosat-2E was launched by PSLV-C38 on 23 June 2017 from Satish Dhawan Space Centre's FLP, from first launchpad at Sriharikota.The PSLV-C38 rocket launched the 550 kg satellite, the fourth of the Cartosat-2 series,along with 30 other nano satellites.\n\nCartosat-2F was launched successfully by PSLV-C40 on 12 Jan 2018 from Satish Dhawan Space Centre's FLP, from first launchpad at Sriharikota. The PSLV-C40 rocket launched the 710 kg satellite, the seventh of the Cartosat-2 series, along with 30 other nano satellites belonging to India, Canada, Finland, France, Republic of Korea, UK and the USA.\n\n"}
{"id": "7063", "url": "https://en.wikipedia.org/wiki?curid=7063", "title": "Catapult", "text": "Catapult\n\nA catapult is a ballistic device used to launch a projectile a great distance without the aid of explosive devices—particularly various types of ancient and medieval siege engines. In use since ancient times, the catapult has proven to be one of the most effective mechanisms during warfare. In modern times the term can apply to devices ranging from a simple hand-held implement (also called a \"slingshot\") to a mechanism for launching aircraft from a ship.\n\nThe word 'catapult' comes from the Latin 'catapulta', which in turn comes from the Greek (\"katapeltēs\"), itself from κατά (\"kata\"), \"downwards\" and πάλλω (\"pallō\"), \"to toss, to hurl\". Catapults were invented by the ancient Greeks and in ancient India where they were used by the Magadhan king Ajatshatru around the early to mid 5th Century BC.\n\nThe catapult and crossbow in Greece are closely intertwined. Primitive catapults were essentially \"the product of relatively straightforward attempts to increase the range and penetrating power of missiles by strengthening the bow which propelled them\". The historian Diodorus Siculus (fl. 1st century BC), described the invention of a mechanical arrow-firing catapult (\"katapeltikon\") by a Greek task force in 399 BC. The weapon was soon after employed against Motya (397 BC), a key Carthaginian stronghold in Sicily. Diodorus is assumed to have drawn his description from the highly rated history of Philistus, a contemporary of the events then. The introduction of crossbows however, can be dated further back: according to the inventor Hero of Alexandria (fl. 1st century AD), who referred to the now lost works of the 3rd-century BC engineer Ctesibius, this weapon was inspired by an earlier foot-held crossbow, called the \"gastraphetes\", which could store more energy than the Greek bows. A detailed description of the \"gastraphetes\", or the \"belly-bow\", along with a watercolor drawing, is found in Heron's technical treatise \"Belopoeica\".\n\nA third Greek author, Biton (fl. 2nd century BC), whose reliability has been positively reevaluated by recent scholarship, described two advanced forms of the \"gastraphetes\", which he credits to Zopyros, an engineer from southern Italy. Zopyrus has been plausibly equated with a Pythagorean of that name who seems to have flourished in the late 5th century BC. He probably designed his bow-machines on the occasion of the sieges of Cumae and Milet between 421 BC and 401 BC. The bows of these machines already featured a winched pull back system and could apparently throw two missiles at once.\n\nPhilo of Byzantium provides probably the most detailed account on the establishment of a theory of belopoietics (\"belos\" = \"projectile\"; \"poietike\" = \"(art) of making\") circa 200 BC. The central principle to this theory was that \"all parts of a catapult, including the weight or length of the projectile, were proportional to the size of the torsion springs\". This kind of innovation is indicative of the increasing rate at which geometry and physics were being assimilated into military enterprises.\n\nFrom the mid-4th century BC onwards, evidence of the Greek use of arrow-shooting machines becomes more dense and varied: arrow firing machines (\"katapaltai\") are briefly mentioned by Aeneas Tacticus in his treatise on siegecraft written around 350 BC. An extant inscription from the Athenian arsenal, dated between 338 and 326 BC, lists a number of stored catapults with shooting bolts of varying size and springs of sinews. The later entry is particularly noteworthy as it constitutes the first clear evidence for the switch to torsion catapults which are more powerful than the flexible crossbows and came to dominate Greek and Roman artillery design thereafter. This move to torsion springs was likely spurred by the engineers of Philip II of Macedonia. Another Athenian inventory from 330 to 329 BC includes catapult bolts with heads and flights. As the use of catapults became more commonplace, so did the training required to operate them. Many Greek children were instructed in catapult usage, as evidenced by \"a 3rd Century B.C. inscription from the island of Ceos in the Cyclades [regulating] catapult shooting competitions for the young\". Arrow firing machines in action are reported from Philip II's siege of Perinth (Thrace) in 340 BC. At the same time, Greek fortifications began to feature high towers with shuttered windows in the top, which could have been used to house anti-personnel arrow shooters, as in Aigosthena. Projectiles included both arrows and (later) stones that were sometimes lit on fire. Onomarchus of Phocis first used catapults on the battlefield against Philip II of Macedon. Philip's son, Alexander the Great, was the next commander in recorded history to make such use of catapults on the battlefield as well as to use them during sieges.\n\nThe Romans started to use catapults as arms for their wars against Syracuse, Macedon, Sparta and Aetolia (3rd and 2nd centuries BC). The Roman machine known as an arcuballista was similar to a large crossbow. Later the Romans used ballista catapults on their warships.\n\nAjatshatru is recorded in Jaina texts as having used catapults in his campaign against the Licchavis.\n\nKing Uzziah, who reigned in Judah until 750 BC, is documented as having overseen the construction of machines to \"shoot great stones\" in .\n\nCastles and fortified walled cities were common during this period and catapults were used as siege weapons against them. As well as their use in attempts to breach walls, incendiary missiles, or diseased carcasses or garbage could be catapulted over the walls.\n\nDefensive techniques in the Middle Ages progressed to a point that rendered catapults largely ineffective. The Viking siege of Paris (885–6 A.D.) \"saw the employment by both sides of virtually every instrument of siege craft known to the classical world, including a variety of catapults\", to little effect, resulting in failure.\n\nThe most widely used catapults throughout the Middle Ages were as follows:\n\n\n\n\n\n\n\nThe last large scale military use of catapults was during the trench warfare of World War I. During the early stages of the war, catapults were used to throw hand grenades across no man's land into enemy trenches. They were eventually replaced by small mortars.\n\nIn the 1840s the invention of vulcanized rubber allowed the making of small hand-held catapults, either improvised from Y-shaped sticks or manufactured for sale; both were popular with children and teenagers. These devices were also known as slingshots in the USA.\n\nSpecial variants called aircraft catapults are used to launch planes from land bases and sea carriers when the takeoff runway is too short for a powered takeoff or simply impractical to extend. Ships also use them to launch torpedoes and deploy bombs against submarines. Small catapults, referred to as \"traps\", are still widely used to launch clay targets into the air in the sport of clay pigeon shooting.\n\nIn the 1990s and into the early 2000s, a powerful catapult, a trebuchet, was used by thrill-seekers first on private property and in 2001-2002 at Middlemoor Water Park, Somerset, England to experience being catapulted through the air for . The practice has been discontinued due a fatality at the Water Park. There had been an injury when the trebuchet was in use on private property. Injury and death occurred when those two participants failed to land onto the safety net. The operators of the trebuchet were tried, but found not guilty of manslaughter, though the jury noted that the fatality might have been avoided had the operators \"imposed stricter safety measures.\" Human cannonball circus acts use a catapult launch mechanism, rather than gunpowder, and are risky ventures for the human cannonballs.\n\nEarly launched roller coasters used a catapult system powered by a diesel engine or a dropped weight to acquire their momentum, such as Shuttle Loop installations between 1977-1978. The catapult system for roller coasters has been replaced by flywheels and later linear motors.\n\n\"Pumpkin chunking\" is another widely popularized use, in which people compete to see who can launch a pumpkin the farthest by mechanical means (although the world record is held by a pneumatic air cannon).\n\nIn January 2011, a homemade catapult was discovered that was used to smuggle cannabis into the United States from Mexico. The machine was found 20 feet from the border fence with bales of cannabis ready to launch.\n\n\n\n"}
{"id": "18615285", "url": "https://en.wikipedia.org/wiki?curid=18615285", "title": "Cement accelerator", "text": "Cement accelerator\n\nA cement accelerator is an admixture for the use in concrete, mortar, rendering or screeds. The addition of an accelerator speeds the setting time and thus cure time starts earlier. This allows concrete to be placed in winter with reduced risk of frost damage. Concrete is damaged if it does not reach a strength of before freezing.\n\nTypical chemicals used for acceleration today are calcium nitrate (Ca(NO)), calcium formate (Ca(HCOO)) and sodium nitrate (NaNO). Calcium chloride (CaCl) is the most efficient and least expensive accelerator and was formerly very popular but is corrosive to reinforcement bars so its use is not recommended and in many countries is actually prohibited. This \"de facto\" caution comes the fact that calcium chloride lowers the alkanity of cement-based concrete, which in turn increases the possibility of rebar corrosion. Thiocyanate compounds can also corrode reinforcing but are safe at recommended dosage levels. Sodium compounds might compromise the long term compressive strength if used with alkali-reactive aggregates.\n\nNovel alternatives include cement based upon calcium sulphoaluminate (CSA), which sets within 20 minutes, and develops sufficient rapid strength that an airport runway can be repaired in a six hour window, and be able to withstand aircraft use at the end of that time, as well as in tunnels and underground, where water and time limitations require extremely fast strength and setting.\n"}
{"id": "18700969", "url": "https://en.wikipedia.org/wiki?curid=18700969", "title": "Clayton Morris", "text": "Clayton Morris\n\nClayton Morris (born December 31, 1976) is an American real estate investor and host of the \"Investing in Real Estate\" podcast. He is the former co-host of \"Fox & Friends Weekend\" show on Fox News Channel and a former co-host of \"The Daily Buzz\" and \"Good Day Philadelphia\" on Fox's WTXF-TV before moving to Fox News Channel in 2009. He covered consumer technology for Fox and hosted weekly technology segments for Fox News Radio and Fox News. On September 4, 2017, he left Fox News.\n\nMorris was born in Philadelphia, Pennsylvania and attended Wilson High School in Spring Township, Berks County, Pennsylvania (today West Lawn, Pennsylvania). He graduated with a bachelor's degree from the University of Pittsburgh in 1999. He was a frequent guest-panelist on the Fox News late-night satire show, \"Red Eye w/ Greg Gutfeld\".\n\nHe and his wife, Natali, have three children and live in Maplewood, New Jersey.\n"}
{"id": "54469040", "url": "https://en.wikipedia.org/wiki?curid=54469040", "title": "Corega", "text": "Corega\n\nOriginally, in 1996, Corega Inc. was established by Allied Telesis K.K., headquartered in Yokohama, Kanagawa Japan, for offering networking hardware products for consumer market and small business. \nThe company (division) is basically fabless, designing the products, ordering them to the manufactures in Japan, Taiwan and China etc., as Allied Telesis does.\nThe company (division) offers networking hardware products, network router, network switch and wireless router.\nCorega products are sold and installed mostly in Japanese domestic market, but we can find several products at some online shopping, Amazon.com etc..\nThe business type and scope is same as Green House, Elecom, and Buffalo, these are also the companies mostly for consumer market and small business in Japan.\n\nIn 2009, Allied Telesis K.K. acquired Corega Inc, then it started as one brand (division) in Allied Telesis K.K., as Allied Telesis group restructure.\n\n\n"}
{"id": "39787431", "url": "https://en.wikipedia.org/wiki?curid=39787431", "title": "Corn sheller", "text": "Corn sheller\n\nA corn sheller is a hand-held device or a piece of machinery to shell corn kernels of the cob for feeding to livestock or for other uses.\n\nThe modern corn sheller is commonly attributed to Lester E. Denison from Middlesex County, Connecticut. Denison was issued a patent on August 12, 1839, for a freestanding, hand-operated machine that removed individual kernels of corn by pulling the cob through a series of metal-toothed cylinders which stripped the kernels off the cob. Soon after, other patents were granted for similar machines, sometimes having improvements over Denison's original design.\n\nThe operation of a corn sheller is similar to a threshing machine, but with some differences to deal with larger grain size and other differences of corn compared to wheat and other crops. Corn shellers can be powered by a hand crank, a tractor, a stationary engine, or by an electric motor. Whole corn cobs are fed in. They are pulled between two toothed wheels, usually made of metal. Each wheel spins the opposite direction of the other. The teeth pull the kernals off the cob until there are no kernels left. The kernels fall out through a screen into a container (such as a bucket) placed underneath the machine. The cob is then ejected out, since it cannot pass through the screen. Some models have a \"walker\", similar to a threshing machine or combine, to take the cobs out.\n\n"}
{"id": "3265848", "url": "https://en.wikipedia.org/wiki?curid=3265848", "title": "Department of Science and Technology (South Africa)", "text": "Department of Science and Technology (South Africa)\n\nThe Department of Science and Technology (DST) is the South African government department responsible for scientific research, including space programmes. The current Minister is Mmamoloko Kubayi-Ngubane.\n\nMuch of the department's work is ultimately carried out through various quasi-independent agencies (although still usually government bodies) including: \n\n"}
{"id": "26703961", "url": "https://en.wikipedia.org/wiki?curid=26703961", "title": "Design change", "text": "Design change\n\nA design change is the modification conducted to the product. It can happen at any stage in the product development process.\n\nThe design changes that happen early in the design process are less expensive when compared to those that take place after it is introduced into full-scale production. The cost of the change increases with its development time. Fundamentally, the design changes can be classified into pre production and post production design changes. The pre-production changes can happen in the conceptual design stage, prototype stage, detailing stage, testing stage. The post -production stage change will happen almost immediately the product is introduced into the production. This might be due to several reasons such as market response, design faults uncovering, design mistakes, not meeting customer requirements, so on and so forth. One of the tools to minimize this type of design change is House of Quality.\n\n"}
{"id": "17561319", "url": "https://en.wikipedia.org/wiki?curid=17561319", "title": "Electrochemical gas sensor", "text": "Electrochemical gas sensor\n\nElectrochemical gas sensors are gas detectors that measure the concentration of a target gas by oxidizing or reducing the target gas at an electrode and measuring the resulting current.\n\nBeginning his research in 1962, Mr. Naoyoshi Taguchi became the first person in the world to succeed in the development of a semiconductor device which could detect low concentrations of combustible and reducing gases when used with a simple electrical circuit. Devices based on this technology are often called \"TGS\" (Taguchi Gas Sensors).\n\nThe sensors contain two or three electrodes, occasionally four, in contact with an electrolyte. The electrodes are typically fabricated by fixing a high surface area precious metal on to the porous hydrophobic membrane. The working electrode contacts both the electrolyte and the ambient air to be monitored usually via a porous membrane. The electrolyte most commonly used is a mineral acid, but organic electrolytes are also used for some sensors. The electrodes and housing are usually in a plastic housing which contains a gas entry hole for the gas and electrical contacts.\n\nThe gas diffuses into the sensor, through the back of the porous membrane to the working electrode where it is oxidized or reduced. This electrochemical reaction results in an electric current that passes through the external circuit. In addition to measuring, amplifying and performing other signal processing functions, the external circuit maintains the voltage across the sensor between the working and counter electrodes for a two electrode sensor or between the working and reference electrodes for a three electrode cell. At the counter electrode an equal and opposite reaction occurs, such that if the working electrode is an oxidation, then the counter electrode is a reduction.\n\nThe magnitude of the current is controlled by how much of the target gas is oxidized at the working electrode. Sensors are usually designed so that the gas supply is limited by diffusion and thus the output from the sensor is linearly proportional to the gas concentration. This linear output is one of the advantages of electrochemical sensors over other sensor technologies, (e.g. infrared), whose output must be linearized before they can be used. A linear output allows for more precise measurement of low concentrations and much simpler calibration (only baseline and one point are needed).\n\nDiffusion control offers another advantage. Changing the diffusion barrier allows the sensor manufacturer to tailor the sensor to a particular target gas concentration range. In addition, since the diffusion barrier is primarily mechanical, the calibration of electrochemical sensors tends to be more stable over time and so electrochemical sensor based instruments require much less maintenance than some other detection technologies. In principle, the sensitivity can be calculated based on the diffusion properties of the gas path into the sensor, though experimental errors in the measurement of the diffusion properties make the calculation less accurate than calibrating with test gas.\n\nFor some gases such as ethylene oxide, cross sensitivity can be a problem because ethylene oxide requires a very active working electrode catalyst and high operating potential for its oxidation. Therefore gases which are more easily oxidized such as alcohols and carbon monoxide will also give a response. Cross sensitivity problems can be eliminated though through the use of a chemical filter, for example filters that allows the target gas to pass through unimpeded, but which reacts with and removes common interferences.\n\nWhile electrochemical sensors offer many advantages, they are not suitable for every gas. Since the detection mechanism involves the oxidation or reduction of the gas, electrochemical sensors are usually only suitable for gases which are electrochemically active, though it is possible to detect electrochemically inert gases indirectly if the gas interacts with another species in the sensor that then produces a response. Sensors for carbon dioxide are an example of this approach and they have been commercially available for several years.\n\n"}
{"id": "2933543", "url": "https://en.wikipedia.org/wiki?curid=2933543", "title": "Electronic funds transfer", "text": "Electronic funds transfer\n\nElectronic funds transfer (EFT) are electronic transfer of money from one bank account to another, either within a single financial institution or across multiple institutions, via computer-based systems, without the direct intervention of bank staff.\n\nAccording to the United States Electronic Fund Transfer Act of 1978 it is \"a funds transfer initiated through an electronic terminal, telephone, computer (including on-line banking) or magnetic tape for the purpose of ordering, instructing, or authorizing a financial institution to debit or credit a consumer’s account. \"\n\nEFT transactions are known by a number of names across countries and different payment systems. For example, in the United States, they may be referred to as \"electronic checks\" or \"e-checks\". In the United Kingdom, the term \"bank transfer\" and \"bank payment\" are used, while in several other European countries \"giro transfer\" is the common term.\n\nEFTs include, but are not limited to: \n\n\n"}
{"id": "1392242", "url": "https://en.wikipedia.org/wiki?curid=1392242", "title": "Energy tower (downdraft)", "text": "Energy tower (downdraft)\n\nThe energy tower is a device for producing electrical power. The brainchild of Dr. Phillip Carlson, expanded by Professor Dan Zaslavsky and Dr. Rami Guetta from the Technion. Energy towers spray water on hot air at the top of the tower, making the cooled air fall through the tower and drive a turbine at the tower's bottom.\n\nAn energy tower (also known as a downdraft energy tower, because the air flows down the tower) is a tall (1,000 meters) and wide (400 meters) hollow cylinder with a water spray system at the top. Pumps lift the water to the top of the tower and then spray the water inside the tower. Evaporation of water cools the hot, dry air hovering at the top. The cooled air, now denser than the outside warmer air, falls through the cylinder, spinning a turbine at the bottom. The turbine drives a generator which produces the electricity.\n\nThe greater the temperature difference between the air and water, the greater the energy efficiency. Therefore, downdraft energy towers should work best in a hot dry climate. Energy towers require large quantities of water. Salt water is acceptable, although care must be taken to prevent corrosion, so that desalination is an example to solve this problem.\n\nThe energy that is extracted from the air is ultimately derived from the sun, so this can be considered a form of solar power. Energy production continues at night, because air retains some of the day's heat after dark. However, power generation by the energy tower is affected by the weather: it slows down each time the ambient humidity increases (such as during a rainstorm), or the temperature falls.\n\nA related approach is the solar updraft tower, which heats air in glass enclosures at ground level and sends the heated air up a tower driving turbines at the base. Updraft towers do not pump water, which increases their efficiency, but do require large amounts of land for the collectors. Land acquisition and collector construction costs for updraft towers must be compared to pumping infrastructure costs for downdraft collectors. Operationally, maintaining the collector structures for updraft towers must be compared to pumping costs and pump infrastructure maintenance.\n\nZaslavsky and other authors estimate that depending on the site and financing costs, energy could be produced in the range of 1-4 cents per kWh, well below alternative energy sources other than hydro. Pumping the water requires about 50% of the turbine's output. Zaslavsky claims that the Energy Tower would achieve up to 70-80% of the Carnot limit. If the conversion efficiency turns out to be much lower, it is expected to have an adverse impact on projections made for cost of energy.\n\nProjections made by Altmann and by Czisch about conversion efficiency and about cost of energy (cents/kWh) are based only on model calculations, no data on a working pilot plant have ever been collected.\n\nActual measurements on the 50 kW Manzanares pilot solar updraft tower found a conversion efficiency of 0.53%, although SBP believe that this could be increased to 1.3% in a large and improved 100 MW unit. This amounts to about 10% of the theoretical limit for the Carnot cycle. It is important to note a significant difference between the up-draft and down-draft proposals. The usage of water as a working-medium dramatically increases the potential for thermal energy capture, and electrical generation, due to its specific heat capacity. While the design may have its problems (see next section) and the stated efficiency claims has yet to be demonstrated, it would be an error to extrapolate performance from one to the other simply because of similarities in the name.\n\n\nLarge industrial consumers often locate near cheap sources of electricity. However, many of these desert regions also lack necessary infrastructure, increasing capital requirements and overall risk.\n\nIn 2014 Maryland-based Solar Wind Energy, Inc. proposed to build a tower. Wind speeds were expected to reach . The company claims that a tower near San Luis, Arizona would produce up to 1,250 MW on sunny days and a yearly average of 435 MW. The company claims to have entitlements for a San Luis site and financing agreement (with JDF Capital Inc.) for up to US$1,585,000.\n\n\n\n"}
{"id": "2866722", "url": "https://en.wikipedia.org/wiki?curid=2866722", "title": "Fiberglass molding", "text": "Fiberglass molding\n\nFiberglass molding is a process in which fiberglass reinforced resin plastics are formed into useful shapes.\n\nThe process usually involves first making a mold and then using the mold to make the fiberglass component.\n\nThe fiberglass mold process begins with an object known as the plug or buck. This is an exact representation of the object to be made. The plug can be made from a variety of materials, usually certain types of foam. \n\nAfter the plug has been formed, it is sprayed with a mold release agent. The release agent will allow the mold to be separated from the plug once it is finished. The mold release agent is a special wax, and/or PVA (Polyvinyl alcohol). Polyvinyl alcohol, however, is said to have negative effects on the final mold's surface finish. \n\nOnce the plug has its release agent applied, gelcoat is applied with a roller, brush or specially-designed spray gun. The gelcoat is pigmented resin, and gives the mold surface a harder, more durable finish.\n\nOnce the release agent and gelcoat are applied, layers of fiberglass and resin are laid-up onto the surface. The fiberglass used will typically be identical to that which will be used in the final product. \n\nIn the laying-up process, a layer of fiberglass mat is applied, and resin is applied over it. A special roller is then used to remove air bubbles. Air bubbles, if left in the curing resin, would significantly reduce the strength of the finished mold. The fiberglass spray lay-up process is also used to produce molds, and can provide good filling of corners and cavities where a glass mat or weave may prove to be too stiff.\n\nOnce the final layers of fiberglass are applied to the mold, the resin is allowed to set up and cure. Wedges are then driven between the plug and the mold in order to separate the two. \n\nAdvanced techniques such as resin transfer molding are also used.\n\nThe component-making process involves building up a component on the fiberglass mold. The mold is a \"negative\" image of the component to be made, so the fiberglass will be applied inside the mold, rather than around it.\n\nAs in the mold-making process, release agent is first applied to the mold. Colored gelcoat is then applied. Layers of fiberglass are then applied, using the same procedure as before. Once completed and cured, the component is separated from the mold using wedges, compressed air or both.\n\n"}
{"id": "919057", "url": "https://en.wikipedia.org/wiki?curid=919057", "title": "Field-emission display", "text": "Field-emission display\n\nA field-emission display (FED) is a flat panel display technology that uses large-area field electron emission sources to provide electrons that strike colored phosphor to produce a color image. In a general sense, an FED consists of a matrix of cathode ray tubes, each tube producing a single sub-pixel, grouped in threes to form red-green-blue (RGB) pixels. FEDs combine the advantages of CRTs, namely their high contrast levels and very fast response times, with the packaging advantages of LCD and other flat-panel technologies. They also offer the possibility of requiring less power, about half that of an LCD system.\n\nSony was the major proponent of the FED design and put considerable research and development effort into the system during the 2000s. Sony's FED efforts started winding down in 2009, as LCD became the dominant flat-panel technology. In January 2010, AU Optronics announced that it acquired essential FED assets from Sony and intends to continue development of the technology. , no large-scale commercial FED production has been undertaken.\n\nFEDs are closely related to another developing display technology, the surface-conduction electron-emitter display (SED), differing primarily in details of the electron-emission system.\n\nFED display operates like a conventional cathode ray tube (CRT) with an electron gun that uses high voltage (10 kV) to accelerate electrons, which in turn excite the phosphors, but instead of a single electron gun, an FED display contains a grid of individual nanoscopic electron guns.\n\nAn FED screen is constructed by laying down a series of metal stripes onto a glass plate to form a series of cathode lines. Photolithography is used to lay down a series of rows of switching gates at right angles to the cathode lines, forming an addressable grid. At the intersection of each row and column a small patch of emitters is deposited, typically using methods developed from inkjet printers. The metal grid is laid on top of the switching gates to complete the gun structure.\n\nA high voltage-gradient field is created between the emitters and a metal mesh suspended above them, pulling electrons from the tips of the emitters. This is a highly non-linear process, and small changes in voltage will quickly cause the number of emitted electrons to saturate. The grid can be individually addressed, but only the emitters located at the crossing points of the powered cathode, gate lines will have enough power to produce a visible spot, and any power leaks to surrounding elements will not be visible. The non-linearity of the process allows avoidance of active matrix addressing schemes – once the pixel lights up, it will naturally glow. Non-linearity also means that the brightness of the sub-pixel is pulse-width modulated to control the number of electrons being produced, like in plasma displays.\n\nThe grid voltage sends the electrons flowing into the open area between the emitters at the back and the screen at the front of the display, where a second accelerating voltage additionally accelerates them towards the screen, giving them enough energy to light the phosphors. Since the electrons from any single emitter are fired toward a single sub-pixel, the scanning electromagnets are not needed.\n\nJust like any other displays with individually addressable sub-pixels, FED displays can potentially suffer from manufacturing problems that will result in dead pixels. However, the emitters are so small that many \"guns\" can power a sub-pixel, the screen can be examined for dead emitters and pixel brightness corrected by increasing the pulse width to make up for the loss through increased emissions from the other emitters feeding the same pixel.\n\nFEDs eliminate much of the electrical complexity of cathode ray tubes, including the heated filaments in the electron gun used to generate electrons and the electromagnets in the deflection yokes used to steer the beam, and are thus much more power efficient than a CRT of similar size. However, FED displays are technically worse than CRTs, as they are not capable of multiscanning.\n\nFlat-panel LCDs use a bright light source and filter out half of the light with a polarizer, and then filter most of the light to produce red, green and blue (RGB) sources for the sub-pixels. That means that, at best, only 1/6 (or less in practice) of the light being generated at the back of the panel reaches the screen. In most cases the LCD itself then filters out additional light in order to change the brightness of the sub-pixels and produce a color gamut. So in spite of using extremely efficient light sources like cold-cathode fluorescent lamps or high-power white LEDs, the overall efficiency of an LCD is not very high. Although the lighting process used in the FED is less efficient, only lit sub-pixels require power, which means that FEDs are more efficient than LCDs. Sony's 36\" FED prototypes have been shown drawing only 14 W when displaying brightly lit scenes, whereas a conventional LCD screen of similar size would normally draw well over 100 W.\n\nAvoiding the need for a backlighting system and thin-film transistor active matrix also greatly reduces the complexity of the set as a whole, while also reducing its front-to-back thickness. While an FED has two sheets of glass instead of the one in an LCD, the overall weight is likely to be less than of a similarly sized LCD. FEDs are also claimed to be cheaper to manufacture, as they have fewer total components and processes involved. However, they are not easy devices to manufacture as a reliable commercial device, and considerable production difficulties have been encountered. This led to a race with two other front-running technologies aiming to replace LCDs in television use, the active-matrix OLED and surface-conduction electron-emitter display (SED).\n\nOrganic light-emitting diodes (OLED) cells directly emit light. Therefore, OLEDs require no separate light source and are highly efficient in terms of light output. They offer the same high contrast levels and fast response times that FED offers. OLEDs are a serious competitor to FEDs, but suffer from the same sorts of problems bringing them to mass production.\n\nSEDs are very similar to FEDs, the primary difference between the two technologies is that SED uses a single emitter for each column instead of the individual spots of the FED. Whereas an FED uses electrons emitted directly toward the front of the screen, the SED uses electrons that are emitted from the vicinity of a small \"gap\" in a surface-conducting track laid down parallel to the plane of the panel, and extracted sideways to their original direction of motion. SED uses an emitter array based on palladium oxide laid down by an inkjet or silk-screen process. SED has been considered to be the variant of FED that is feasible to mass-produce, however, as of late 2009 no commercial SED display products have been made available by the industry.\n\nThe first concentrated effort to develop FED systems started in 1991 by Silicon Video Corporation, later Candescent Technologies. Their \"ThinCRT\" displays used metal emitters, originally built out of tiny molybdenum cones known as Spindt tips. They suffered from erosion due to the high accelerating voltages. Attempts to lower accelerating voltages and find suitable phosphors that would work at lower power levels, as well as address the erosion problem through better materials, were unsuccessful.\n\nCandescent pushed ahead with development in spite of problems, breaking ground on a new production facility in Silicon Valley in 1998, partnering with Sony. However the technology was not ready, and the company suspended equipment purchases in early 1999, citing \"contamination issues\". The plant was never completed, and after spending $600 million on development they filed for Chapter 11 protection in June 2004, and sold all of their assets to Canon that August.\n\nAnother attempt to address the erosion issues was made by Advance Nanotech, a subsidiary of SI Diamond Technology of Austin, Texas. Advance Nanotech developed a doped diamond dust, whose sharp corners appeared to be an ideal emitter. However the development never panned out and was abandoned in 2003. Advance Nanotech then applied their efforts to the similar SED display, licensing their technology to Canon. When Canon brought in Toshiba to help developing the display, Advance Nanotech sued, but ultimately lost in their efforts to re-negotiate the contracts based on their claim that Canon transferred the technology to Toshiba.\n\nRecent FED research focuses on carbon nanotubes (CNTs) as emitters. Nano-emissive display (NED) is Motorola's term for their carbon-nanotube-based FED technology. A prototype model was demonstrated in May 2005, but Motorola has now halted all FED-related development.\n\nFutaba Corporation has been running a Spindt-type development program since 1990. They have produced prototypes of smaller FED systems for a number of years and demonstrated them at various trade shows, but like the Candescent efforts no large-screen production has been forthcoming. Development continues on a nanotube based version.\n\nSony, having abandoned their efforts with Candescent, licensed CNT technology from Carbon Nanotechnologies Inc., of Houston, Texas, who were the public licensing agent for a number of technologies developed at Rice University's Carbon Nanotechnology Laboratory. In 2007 they demonstrated an FED display at a trade show in Japan and claimed they would be introducing production models in 2009. They later spun off their FED efforts to Field Emission Technologies Inc., which continued to aim for a 2009 release.\n\nTheir plans to start production at a former Pioneer factory in Kagoshima were delayed by financial issues in late 2008. On March 26, 2009 Field Emission Technologies Inc. (FET) announced that it was closing down due to the inability to raise capital.\n\nIn January 2010, Taiwanese AU Optronics Corporation (AUO) announced that it had acquired assets from Sony's FET and FET Japan, including \"patents, know-how, inventions, and relevant equipment related to FED technology and materials\". In November 2010, Nikkei reported that AUO plans to start mass production of FED panels in the fourth quarter of 2011, however AUO commented that the technology is still in the research stage and there are no plans to begin mass production at this moment.\n\n\n"}
{"id": "42182182", "url": "https://en.wikipedia.org/wiki?curid=42182182", "title": "G. van der Lee Rope Factory", "text": "G. van der Lee Rope Factory\n\nThe G. van der Lee Rope Factory (\"Touwfabriek G. van der Lee\") in Oudewater was the oldest family business in the Netherlands until it became part of the Hendrik Veder Group in 2013. Royal Tichelaar Makkum is now the country’s oldest family firm. Established around 1545 by Jan Pietersz. van der Lee, G. van der Lee is still one of the oldest companies in the Netherlands. De Koninklijke Tichelaar Makkum .\n\nSince its founding in 1545, it has always been located in Oudewater, and has always been owned and managed by direct descendants of rope maker Jan Pietersz. van der Lee. Jan Pietersz. van der Lee’s birth year, which is believed to be around 1545, serves as the factory’s founding year.\n\nJan Pietersz. van der Lee belonged to what you could call the upper class of Oudewater. He was the proprietor of most ropewalks of Oudewater as the Oudewater ledger of 1579 shows. Under the heading ‘Other income from ropewalk proprietors’ it reads: \n‘Received from Jan Pietersz., 15 five-cent pieces for ropewalks previously rented from the city’. \nThe son of Jan Pietersz. (1580–1625), a rope maker too, was the first one to inherit the property from his father. After Jan Pietersz’s death, the family business was handed down to Heijndrick (1620–1697) and Gijsbert Heijndricksz. van der Lee (1645–1694). According to records dating back to 1694, they owned a ‘coarse-thread ropery and ropery house with right of way and right to spin on Agterstraat in Oudewater’.\nGijsbert Heijndricksz’s three sons Adrianus, Cornelis and Jan took over the company after his death. \nJan Gijsbertsz’s oldest son was Gijsbert, but it was his brother Adrianus van der Lee (1721–1766) who continued the dynasty. Adrianus’ son Cornelis van der Lee (1753–1826) took charge of the business in 1787 with the purchase of ‘a ropewalk and farmland with shed and garden, located at the Bieze, with the garden wall leading to the Amsterdamse Veer in Oudewater’ from his uncle Gijsbert van der Lee.\n\nFrom the 16th to the 19th century, the craft of rope making remained pretty much unchanged. There were many steps involved in rope making, and each of these steps was done by hand with the aid of simple tools. In the first half of the 19th century the Dutch shipbuilding industry was booming. Many businesses, including the roperies, benefited from this favourable environment.\nIn the 1860s, a change occurred with the gradual arrival of the industrial revolution which also affected the Van der Lee family. Although Adrianus van der Lee (1787–1856) called himself a manufacturer, he was in fact a traditional rope maker. It was his son Gijsbert (1819–1903) who pulled the business into the modern world.\nIn 1880, Gijsbert van der Lee made the brave decision to introduce steam engines into the rope production. He bought an elongated piece of land in the Hekendorp polder just outside Oudewater where the new steam ropery was to be built. \nIn 1886, Gijsbert van der Lee, as stated in the permit issued by the Municipality of Hekendorp, commissioned the construction of a stone and iron building ‘with a kettle to melt the tar needed for tarring the ropes manufactured in the steam ropery’. This led to fierce protests from neighbours.\nAt the turn of the century, a mechanised spinning mill was built next to the steam ropery.\nOn 28 December 1903, the founder of the modern rope factory, Gijsbert van der Lee, died at the age of 84 at his villa ‘Klein Hekendorp’ opposite the factory on the IJsseldijk.\nGijsbert’s sons Adrianus and Cornelis Gijsbertus followed in their father’s footsteps and turned the ropery into a general partnership named G. van der Lee.\n\nOn 1 July 1919, Cornelis Gijsbertus left the company and G. van der Lee was passed on to his brother Adrianus (1854–1920) and his son Gijsbert (1883–1966).\nAfter Adrianus van der Lee’s death on 13 May 1920, a difficult time started for Gijsbert van der Lee. As part owner of the company, he had to share authority with his three brothers Klaas, Jan and Piet, his sister Marretje, his brother-in-law Arie Beijen and his mother Christina Vriesman.\nGijsbert van der Lee bought out his family in 1936, on the condition that Gijsbert’s brother Piet Van der Lee became the director of the Old Ropery in Vlaardingen. During this period, the way business was done started to change and the first lorry was bought to transport the ropes.\n\nIn 1960, Wim van der Lee and his manager made old and new rope-making technologies meet. They designed large braiding machines that could manufacture heavy hawsers for the shipping and off-shore industries. The ropes for these hawsers were twisted on the old ropewalk.\nThe industry was faced with a major challenge in 1965 when the first synthetic yarn hit the markets which had many times the resistance of manila, and, additionally, is impervious. With developments in the synthetic industry taking place at breakneck speed, the family decided to buy into synthetic fibre, which meant closing their own spinning mill.\nOn 1 July 1984, Wim van der Lee’s daughter and son, Saskia and Gijsbert van der Lee, and their cousin Willem Hendrik Schreijgrond, the oldest son of Christina van der Lee, who is Wim van der Lee’s sister, were appointed as supervisory directors alongside Johan Joseph Breen.\nBernhard de Wit was appointed as director. By importing raw materials the family could now specialise. This change in production led to a new factory being built in 1993. Van der Lee’s product range, which is made using modern scutchers and braiding machines, now largely varies to include all kinds of ropes, yet the old craft still lives on at the G. van der Lee Rope Factory; hawsers for fisheries, water sport and replicas of VOC ships and the ropes are still twisted on the 350-metre long ropewalk.\n\nThe G. Van der Lee Rope Factory became part of the Hendrik Veder Group in 2013.\n\n"}
{"id": "18207727", "url": "https://en.wikipedia.org/wiki?curid=18207727", "title": "Grindometer", "text": "Grindometer\n\nA grindometer is a device used to measure the particle size of suspensions, typically inks such as those used in printing, or paints. It consists of a steel block with a channel of varying depth machined into it, starting at a convenient depth for the type of suspension to be measured, and becoming shallower until it ends flush with the block's surface. The depth of the groove is marked off on a graduated scale next to it. The suspension to be tested is poured into the deep end of the groove, and scraped towards the shallow end with a flat metal scraper. At the point where the depth of the groove equals the largest particles in the suspension, irregularities (for example pinholes in an ink sample) will become visible.\n\nThe advantages of this method are that it uses a small sample and gives a very quick indication of the high end of the particle size distribution, allowing production processes to be followed in real time.\n\nThe following standards are relevant on conjunction with the use of grindometers: ASTM D 1210, ASTM D 1316, JIS K 5600-2-5, ISO 1524, EN ISO 1524, BS 3900-C6 \n"}
{"id": "25501419", "url": "https://en.wikipedia.org/wiki?curid=25501419", "title": "Indoor–outdoor thermometer", "text": "Indoor–outdoor thermometer\n\nAn indoor–outdoor thermometer is a thermometer that simultaneously provides a measurement of the indoor and outdoor temperatures. The outdoor part of the thermometer requires some kind of remote temperature sensing device. Conventionally, this was done by extending the bulb of the thermometer to the remote site. Modern instruments are more likely to use some form of electronic transducer.\n\nIn an indoor–outdoor thermometer based on a conventional liquid-in-glass thermometer, the stem of the outdoor thermometer is connected to the bulb by a long, flexible or semi-rigid capillary. The temperature scale is marked on the stem as usual. However, the temperature that is actually measured is the temperature at the bulb.\n\nAmbient corrections are difficult to achieve with this system and are not usually done. So it is not as accurate as a conventional precision thermometer. Rather, it is typically used for low-cost applications such as private houses. The main issue with accuracy is that if the bulb and the stem are at different levels, there is a change in reading due to the change in pressure head. A further problem is that changes in the ambient temperature of the indoor part of the device can cause a change in reading as well as the temperature of the outdoor part of the device. This effect can be minimised by making the bulb large and the capillary a small diameter. This ensures that changes in the outside temperature produce large changes in the column of liquid in the stem and will tend to swamp the smaller changes caused by the changes in the indoor temperature.\n\nCommon working liquids used are toluene and alcohol. Both of these have large temperature coefficients of expansion and do not freeze or boil in the temperature range of interest.\n\nThe sensors can be any of the types used in electronic thermometers. Thermistors are common and semiconductor junctions can also be used. Indoor-outdoor electronic thermometers are a frequent hobbyist project and are sometimes sold as kits. Many indoor-outdoor thermometers on sale are wireless devices requiring no physical connection to the sensor placed outside. In these cases the sensor needs to be battery-powered.\n\nThe primary purpose of the indoor-outdoor thermometer is to allow the outside temperature to be indicated inside a building, thus removing the need to go outside to take a temperature reading. They are also used in vehicles, and are particularly useful for municipal vehicles involved in snow and ice clearance. Building maintenance engineers can use an indoor-outdoor thermometer that has not been installed to get a quick reading of air temperature in a location inside a building. This is done by swinging the bulb of the outdoor sensor in the air while still attached to the instrument. This will get a faster reading because the bulb will come up to temperature much more quickly than the indoor sensor built into the instrument.\n\n"}
{"id": "536717", "url": "https://en.wikipedia.org/wiki?curid=536717", "title": "Information Technology Association of America", "text": "Information Technology Association of America\n\nThe Information Technology Association of America (ITAA), formerly the Association of Data Processing Service Organizations (ADAPSO), was a leading industry trade group for information technology companies. The Association's membership contained most of the world's major Information and communications technology (ICT) firms, accounting for over 90% of ICT goods and services sold in North America.\n\nOrganizational meetings of what was initially called the Data Actuating Technical Association (DATA) began in 1960. In 1961, the Association of Data Processing Service Organizations (ADAPSO) was founded, and formally incorporated in 1962. Its first president was Romuald Slimak, who had worked on the UNIVAC I. Initially headquartered in Pennsylvania, ADAPSO moved to the Washington, DC area in 1978 to be closer to government policymakers and advocates.\nADAPSO was renamed ITAA in 1991.\n\nIn March 2007 ITAA President Phil Bond expressed his desire in merging ITAA with another high tech trade association. On January 17, 2008, ITAA announced that it had agreed to so-called \"merger of equals\" with the Government Electronics and Information Technology Association (GEIA), and that the combined association would retain the ITAA name. Until earlier in the year GEIA had been an affiliate of EIA (a trade association formerly known as the Electronic Industries Alliance) EIA has been very financially successful, unlike ITAA. GEIA is slated to share in the distribution over $50 million in assets resulting in the breakup of EIA . [The fall of EIA: What happened? \nIn 2008 the ITAA merged with the CyberSecurity Industry Alliance and the Government Electronics Industry Association. \n\nIn 2009 the ITAA merged with the AeA (formerly the American Electronics Association) to form TechAmerica. Hank Steininger was the last ITAA board chair prior to the merger.\n\nITAA conducted surveys of CIOs.\n\nITAA actively lobbied on behalf of the funding for the Real ID.\n\nSome have asserted that Real ID will turn state driver’s licenses into a national identity card and impose numerous new burdens on taxpayers, citizens, immigrants, and state governments – while doing nothing to protect against terrorism. As a result, it is stirring intense opposition from many groups across the political spectrum. Critics have claimed that ITAA supports the national ID card because its member companies would benefit from financially from implementing the card.\n\nITAA published a series of newsletters, beginning with \"ADAPSO News\" in the early 1960s. Its last regular newsletter, the \"ITAA E-LETTER\", covered issues of the networked economy, including information and telecommunications public policy, and the businesses of electronic commerce, Internet service and enhanced telecommunications service providers. The ITAA E-LETTER was distributed free of charge by electronic mail.\n\nOther ADAPSO and ITAA publications included \"ADAPSO Agenda\" (\"later ITAA Agenda\"), \"Computer Services: Official Journal of the Association of Data Processing Service Organizations\", and \"Data\".\n\n"}
{"id": "17545102", "url": "https://en.wikipedia.org/wiki?curid=17545102", "title": "Instruments used in medical laboratories", "text": "Instruments used in medical laboratories\n\nThis is a list of instruments used in general in laboratories, including:\n"}
{"id": "49485312", "url": "https://en.wikipedia.org/wiki?curid=49485312", "title": "Kozma Soldatyonkov", "text": "Kozma Soldatyonkov\n\nKozma Terentyevich Soldatyonkov (; 22 October 1818 in Moscow, Russian Empire – 1 June 1901 in Kuntsevo, Moscow, Russian Empire) was a Russian industrialist, mecenate, philanthropist, art collector and a renowned publisher.\n\nIn 1865 the Soldatyonkov Publishing house was launched. Among its seminal publications were the Complete Works by Vissarion Belinsky and Konstantin Kavelin, \"Russian Fairytales\" by Alexander Afanasyev, the translations of \"Allgemeine Weltgeschichte\" by Georg Weber, \"The American Commonwealth\" by James Bryce, \"The History of the Decline and Fall of the Roman Empire\" by Edward Gibbon, \"The History of Scandinavian Literature\" by (translated by Konstantin Balmont), \"A Short History of the English People\" by John Richard Green and \"History of Rome\" by Theodor Mommsen, as well as 11 issues of the Economist's Library (edited by Mitrofan Shchepkin) featuring books by Adam Smith, David Ricardo, James Mill and others.\n\nA staunch Old Believer, he supported the Moscow Belokrinitskaya Hierarchy and financed its leader Bishop Pafnuty (Ovchinnikov)'s visit to London where he had talks with Alexander Hertsen, Nikolai Ogaryov and Vasily Kelsiyev.\n\nSoldatyonkov bequeathed his library (8 thousand books and 15 thousand journals) and art collection (258 paintings, including the best known works by Karl Bryullov, Alexander Ivanov, Vasily Perov and Pavel Fedotov, as well as 17 sculptures) to the Rumyantsev Museum. After 1924 they were shared between the Tretyakov Gallery and the Russian Museum. Part of Soldatyonkov's capital (2 million rubles) went, according to his will, to the construction of the hospital for the poor, later to be known as the Botkin Clinic.\n"}
{"id": "613351", "url": "https://en.wikipedia.org/wiki?curid=613351", "title": "List of computing people", "text": "List of computing people\n\nThis is a list of people who are important or notable in the field of computing, but who are not primarily computer scientists or programmers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "990315", "url": "https://en.wikipedia.org/wiki?curid=990315", "title": "List of former IA-32 compatible processor manufacturers", "text": "List of former IA-32 compatible processor manufacturers\n\nAs the 32-bit Intel Architecture became the dominant computing platform during the 1980s and 1990s, multiple companies have tried to build microprocessors that are compatible with that Intel instruction set architecture. Most of these companies were not successful in the mainstream computing market. So far, only AMD has had any market presence in the computing market for more than a couple of product generations. Cyrix was successful during the 386 and 486 generations of products, but did not do well after the Pentium was introduced.\n\nList of former IA-32 compatible microprocessor vendors:\n\n\n\n\n\n"}
{"id": "38931518", "url": "https://en.wikipedia.org/wiki?curid=38931518", "title": "Localized surface plasmon", "text": "Localized surface plasmon\n\nA localized surface plasmon (LSP) is the result of the confinement of a surface plasmon in a nanoparticle of size comparable to or smaller than the wavelength of light used to excite the plasmon. The LSP has two important effects: electric fields near the particle’s surface are greatly enhanced and the particle’s optical absorption has a maximum at the plasmon resonant frequency. The enhancement falls off quickly with distance from the surface and, for noble metal nanoparticles, the resonance occurs at visible wavelengths. For semiconductor nanoparticles, the maximum optical absorption is often in the near-infrared and mid-infrared region.\n\nThe plasmon resonant frequency is highly sensitive to the refractive index of the environment; a change in refractive index results in a shift in the resonant frequency. As the resonant frequency is easy to measure, this allows LSP nanoparticles to be used for nanoscale sensing applications. Nanostructures exhibiting LSP resonances are used to enhance signals in modern analytical techniques based on spectroscopy. Other applications that rely on efficient light to heat generation in the nanoscale are heat-assisted magnetic recording (HAMR) , photothermal cancer therapy, and thermophotovoltaics. So far, high efficiency applications using plasmonics have not been realized due to the high ohmic losses inside metals especially in the optical spectral range (visible and NIR). ,\n\n"}
{"id": "89970", "url": "https://en.wikipedia.org/wiki?curid=89970", "title": "Mars trilogy", "text": "Mars trilogy\n\nThe Mars trilogy is a series of award-winning science fiction novels by Kim Stanley Robinson that chronicles the settlement and terraforming of the planet Mars through the intensely personal and detailed viewpoints of a wide variety of characters spanning almost two centuries. Ultimately more utopian than dystopian, the story focuses on egalitarian, sociological, and scientific advances made on Mars, while Earth suffers from overpopulation and ecological disaster.\n\nThe three novels are \"Red Mars\" (1992), \"Green Mars\" (1993), and \"Blue Mars\" (1996). \"The Martians\" (1999) is a collection of short stories set in the same fictional universe. The main trilogy won a number of prestigious awards. \"Icehenge\" (1984), Robinson's first novel about Mars, is not set in this universe but deals with similar themes and plot elements. The trilogy shares some similarities with Robinson's more recent novel \"2312\" (2012), for instance, the terraforming of Mars and the extreme longevity of the characters in both novels.\n\n\"Red Mars\" starts in 2026 with the first colonial voyage to Mars aboard the \"Ares\", the largest interplanetary spacecraft ever built and home to a crew who are to be the first hundred Martian colonists. The ship was built from clustered space shuttle external fuel tanks which, instead of reentering Earth's atmosphere, had been boosted into orbit until enough had been amassed to build the ship. The mission is a joint American–Russian undertaking, and seventy of the First Hundred are drawn from these countries (except, for example, Michel Duval, a French psychologist assigned to observe their behavior). The book details the trip out, construction of the first settlement on Mars (eventually called Underhill) by Russian engineer Nadia Cherneshevsky, as well as establishing colonies on Mars' hollowed out asteroid-moon Phobos, the ever-changing relationships between the colonists, debates among the colonists regarding both the terraforming of the planet and its future relationship to Earth. The two extreme views on terraforming are personified by Saxifrage \"Sax\" Russell, who believes their very presence on the planet means some level of terraforming has already begun and that it is humanity's obligation to spread life as it is the most scarce thing in the known universe, and Ann Clayborne, who stakes out the position that humankind does not have the right to change entire planets at their will.\n\nRussell's view is initially purely scientific but in time comes to blend with the views of Hiroko Ai, the chief of the Agricultural Team who assembles a new belief system (the \"Areophany\") devoted to the appreciation and furthering of life (\"viriditas\"); these views are collectively known as the \"Green\" position, while Clayborne's naturalist stance comes to be known as \"Red.\" The actual decision is left to the United Nations Organization Mars Authority (UNOMA), which greenlights terraforming, and a series of actions get underway, including the drilling of \"moholes\" to release subsurface heat; thickening of the atmosphere according to a complicated bio-chemical formula that comes to be known as the \"Russell cocktail\" after Sax Russell; and the detonation of nuclear explosions deep in the sub-surface permafrost to release water. Additional steps are taken to connect Mars more closely with Earth, including the insertion of an Areosynchronous asteroid \"Clarke\" to which a space elevator cable is tethered.\n\nAgainst the backdrop of this development is another debate, one whose principal instigator is Arkady Bogdanov of the Russian contingent (possibly named in homage to the Russian polymath and science fiction writer Alexander Bogdanov - it is later revealed in Blue Mars that Alexander Bogdanov is an ancestor of Arkady's.). Bogdanov argues that Mars need not and should not be subject to Earth traditions, limitations, or authority. He is to some extent joined in this position by John Boone, famous as the \"First Man on Mars\" from a preceding expedition and rival to Frank Chalmers, the technical leader of the American contingent. Their rivalry is further exacerbated by competing romantic interest in Maya Katarina Toitovna, the leader of the Russian contingent. (In the opening of the book, Chalmers instigates a sequence of events that leads to Boone being assassinated; much of what follows is a retrospective examination of what led to that point.)\n\nEarth meanwhile increasingly falls under the control of transnational corporations (transnats) that come to dominate its governments, particularly smaller nations adopted as \"flags of convenience\" for extending their influence into Martian affairs. As UNOMA's power erodes, the Mars treaty is renegotiated in a move led by Frank Chalmers; the outcome is impressive but proves short-lived as the transnats find ways around it through loop-holes. Things get worse as the nations of Earth start to clash over limited resources, expanding debt, and population growth as well as restrictions on access to a new longevity treatment developed by Martian science—one that holds the promise of lifespans into the hundreds of years. In 2061, with Boone dead and exploding immigration threatening the fabric of Martian society, Bogdanov launches a revolution against what many now view as occupying transnat troops operating only loosely under an UNOMA rubber-stamp approval. Initially successful, the revolution proves infeasible on the basis of both a greater-than-expected willingness of the Earth troops to use violence and the extreme vulnerability of life on a planet without a habitable atmosphere. A series of exchanges sees the cutting of the space elevator, bombardment of several Martian cities (including the city where Bogdanov is himself organizing the rebellion; he is killed), the destruction of Phobos and its military complex, and the unleashing of a great flood of torrential groundwater freed by nuclear detonations.\n\nBy the end, most of the First Hundred are dead, and virtually all who remain have fled to a hidden refuge established years earlier by Ai and her followers. (One exception is Phyllis Boyle, who has allied herself with the transnats; she is on Clarke when the space elevator cable is cut and sent flying out of orbit to a fate unknown by the conclusion of the book.) The revolution dies and life on Mars returns to a sense of stability under heavy transnat control. The clash over resources on Earth breaks out into a full-blown world war leaving hundreds of millions dead, but cease-fire arrangements are reached when the transnats flee to the safety of the developed nations, which use their huge militaries to restore order, forming police-states. But a new generation of humans born on Mars holds the promise of change. In the meantime, the remaining First Hundred —including Russell, Clayborne, Toitovna, and Cherneshevsky— settle into life in Ai's refuge called Zygote, hidden under the Martian south pole.\n\n\"Green Mars\" takes its title from the stage of terraforming that has allowed plants to grow. It picks up the story 50 years after the events of \"Red Mars\" in the dawn of the 22nd century, following the lives of the remaining First Hundred and their children and grandchildren. Hiroko Ai's base under the south pole is attacked by UN Transitional Authority (UNTA) forces, and the survivors are forced to escape into a (less literal) underground organization known as the Demimonde. Among the expanded group are the First Hundred's children, the Nisei, a number of whom live in Hiroko's second secret base, Gamete.\n\nAs unrest in the multinational control over Mars's affairs grows, various groups start to form with different aims and methods. Watching these groups evolve from Earth, the CEO of the Praxis Corporation sends a representative, Arthur Randolph, to organize the resistance movements. This culminates into the Dorsa Brevia agreement, in which nearly all the underground factions take part. Preparations are made for a second revolution beginning in the 2120s, from converting moholes to missiles silos or hidden bases, sabotaging orbital mirrors, to propelling Deimos out of Mars' gravity well and out into deep space so it could never be used as a weapons platform as Phobos was.\n\nThe book follows the characters across the Martian landscape, which is explained in detail. Russell's character infiltrates the transnat terraforming project, with a carefully crafted fake identity as Stephen Lindholm. The newly evolving Martian biosphere is described at great length and with more profound changes mostly aimed at warming up the surface of Mars to the brink of making it habitable, from continent-sized orbital mirrors, another space elevator built (using another anchored asteroid that is dubbed \"New Clarke\"), to melting the northern polar ice cap, and digging moholes deep enough to form volcanoes. A mainstay of the novel is a detailed analysis of philosophical, political, personal, economic, and geological experiences of the characters. The story weaves back and forth from character to character, providing a picture of Mars as seen by them.\n\nStephen, alias Sax, eventually becomes romantically involved with Phyllis, who had survived the events of 2061 from the end of the first novel, but she discovers his true identity and has him arrested. Members of the underground launch a daring rescue from the prison facility where Sax suffers torture and interrogation that causes him to have a stroke; Maya kills Phyllis in the process of the rescue.\n\nThe book ends on a major event which is a sudden catastrophic rise in Earth's global sea levels not caused primarily by any greenhouse effect but by the eruption of a chain of volcanoes underneath the ice of West Antarctica, disintegrating the ice sheet and displacing the fragments into the ocean. The resultant flooding causes global chaos on Earth, creating the perfect moment for the Martian underground to seize control of Martian society from Earth. Following a series of largely bloodless coups, an extremist faction of Reds bombs a dam near Burroughs, the major city where the remaining United Nations forces have concentrated, in order to force the security forces to evacuate. The entire city is flooded and the population of the city has to walk a staggeringly long distance in the open Martian atmosphere (which just barely has the temperature, atmospheric pressure, and gas mixture to support human life) to Lybia Station, in order to resettle in other locations. With this, control of Mars is finally wrested away from Earth with minimal loss of life, leaving the weary survivors hopeful about the prospects of their newfound political autonomy.\n\n\"Blue Mars\" takes its title from the stage of terraforming that has allowed atmospheric pressure and temperature to increase so that liquid water can exist on the planet's surface, forming rivers and seas. It follows closely in time from the end of \"Green Mars\" and has a much wider scope than the previous two books, covering an entire century after the second revolution. As Earth is heavily flooded by the sudden melting of the Antarctic ice cap, the once mighty metanats are brought to their knees; as the Praxis Corporation paves a new way of \"democratic businesses\". Mars becomes the \"Head\" of the system, giving universal healthcare, free education, and an abundance of food. However, this sparks illegal immigration from Earth, so to ease the population strain on the Blue Planet, Martian scientists and engineers are soon put to the task of creating asteroid cities; where small planetoids of the Belt are hollowed out, given a spin to produce gravity, and a mini-sun is created to produce light and heat.\n\nWith a vast increase in sciences, technologies, and spacecraft manufacturing, this begins the \"Accelerando\"; where humankind spreads its civilization throughout the Solar System, and eventually beyond. As Venus, the Jovian moons, the Saturnian moons, and eventually Triton are colonized and terraformed in some way, Jackie Boone (the granddaughter of John Boone, the first man to walk on Mars from the first book) takes an interstellar vessel (made out of an asteroid) to another star system twenty light-years away, where they will start to terraform the planets and moons found there.\n\nThe remaining First Hundred are generally regarded as living legends. Reports of Hiroko's survival are numerous, and purported sightings occur all over the colonized solar system, but none are substantiated. Nadia and Art Randolph lead a constitutional congress in which a global system of government is established that leaves most cities and settlements generally autonomous, but subject to a central representative legislature and two systems of courts, one legal and the other environmental. The environmental court is packed with members of the Red faction as a concession (in exchange for their support in the congress, as much of their power was broken when they attempted and failed to violently expel remaining UN forces early on after the second revolution of Green Mars; yet they still retained enough power to stymie constitutional negotiations). Vlad, Marina, and Ursula, the original inventors of the longevity treatments, introduce a new economic system that is a hybrid of capitalism, socialism, and environmental conservationism. During a trip to Earth occurring alongside the congress, Nirgal (one of the original children to be born on Mars to the First Hundred, and something of a Mars-wide celebrity), Maya, and Sax negotiate an agreement that allows Earth to send a number of migrants equal to 10% of Mars' population to Mars every year. Following the adoption of the new constitution, Nadia is elected the first president of Mars and serves competently, although she does not enjoy politics. She and Art work together closely, and eventually fall in love and have a child.\n\nSax Russell devotes himself to various scientific projects, all the while continuing to recover from the effects of his stroke. Since the second revolution, he feels enormous guilt that his pro-terraforming position became the dominant one at the expense of the goals of Ann's anti-terraforming stance, as Sax and Ann have come to be regarded as the original champions of their respective positions. Sax becomes increasingly preoccupied with seeking forgiveness and approval from Ann, while Ann, depressed and bitter from her many political and personal losses, is suicidal and refuses to accept any more longevity treatments. However, when Sax witnesses Ann collapse into a coma during an attempt to demonstrate to her the beauty of the terraformed world, he arranges for her to be resuscitated and to be treated with the longevity treatment, both against her will.\n\nThe longevity treatments themselves begin to show weaknesses once those receiving them reach the two-century mark in age. The treatments reduce most aging processes to a negligible rate, but are much less effective when it comes to brain function, and in particular memory. Maya in particular suffers extreme lapses in memory, although she remains high functioning most of the time. Further, as people age, they begin to show susceptibility to strange, fatal conditions which have no apparent explanation and are resistant to any treatment. Most common is the event that comes to be known as the \"quick decline\", where a person of extremely advanced age and in apparently good health suffers a sudden fatal heart arrythmia and dies abruptly. The exact mechanism is never explained. Michel dies of the quick decline, while attending the wake of another First Hundred member. Russell speculates that Michel's quick decline was brought on by the shock of seeing Maya fail to remember Frank Chalmers (who was killed while escaping security forces in the first revolution) upon looking at a treasured photo of him on her refrigerator. As a result of this and Russell's own problems with memory, he organizes a team of scientists to develop medicine that will restore memory. The remaining members of the First Hundred, of which there are only 12, congregate in Underhill, and take the medicine. It works so well that Russell remembers his own birth. He and Ann Clayborne finally recall that they had been in love prior to leaving Earth the very first time, but both had been too socially inept and nervous about their chances for selection for the Mars voyage to reveal this to each other. Their famous argument over terraforming had been a mere continuation of a running conversation they had been having since they still lived on Earth. Through the memory treatment it is also revealed that Phyllis had been lobbying to free Sax from his torturers when she was murdered by Maya. Maya herself declines the treatment. Sax also distinctly recalls Hiroko assisting him in finding his rover in a storm before he nearly froze to death before disappearing once again and is convinced she remains alive, although the question of whether she is actually alive is never resolved.\n\nEventually, the anti-immigration factions of the Martian government provoke massive illegal immigration from Earth, risking another war; however, under the leadership of Ann and Sax, who have fallen in love again following their reconciliation, along with Maya, the Martian population unites to reconstitute the government to accept more immigration from Earth, diffusing the imminent conflict and ushering in a new golden age of harmony and security on Mars.\n\n\"The Martians\" is a collection of short stories that takes place over the timespan of the original trilogy of novels, as well as some stories that take place in an alternate version of the novels where the First Hundred's mission was one of exploration rather than colonization. Buried in the stories are several hints about the eventual fate of the Martian terraforming program.\n\n\nTrans-national Corporations, nicknamed \"transnats\", are extremely powerful multinational corporations that first emerge in the mid-21st century. Robinson tracks the evolution of the transnats into what he terms \"metanats\" (metanational). These multinational corporations have grown so large as a result of globalization that they have sufficient economic power to take over or strongly manipulate national governments, initially only relatively small third-world governments, but later, larger developed governments too, effectively running whole countries. In Robinson's future history, the metanational corporations become similar to nation-states in some respects, while continually attempting to take over competitors in order to become the sole controller of the interplanetary market. As the \"Mars\" trilogy draws to a close in the mid-23rd century, the metanational corporations are forced by a global catastrophe to concede more democratic powers to their workforces.\n\nAlthough there are many transnational and metanational corporations mentioned, two play an active role in the development of the plotline: Praxis, a largely benevolent and relatively democratic firm, and Subarashī, which plays a large role in the maltreatment of the citizens of Mars.\n\nGenetic engineering is first mentioned in \"Red Mars\"; it takes off when Sax creates an alga to withstand the harsh Martian temperature and convert its atmosphere into breathable air. Eventually this is done on a massive scale, with thousands of types of GE algae, lichen and bacteria being created to terraform the planet. In \"Green Mars\", GE animals began to be created to withstand the thin Martian atmosphere, and to produce a working planetary-biosphere. By \"Blue Mars\", GE is commonly being done on humans, willingly, to help them better adapt to the new worlds; to breathe thinner air (e.g. Russell), or to see better in the dimmer light of the outer planets.\n\nThe books also speculate on the colonization of other planets and moons in the Solar System, and include descriptions of settlements or terraforming efforts on Callisto, Mercury, Titania, Miranda and Venus. Toward the end of the last novel, humans are taking sub-light colony ships to other stars, taking advantage of the longevity treatments to survive the trip to their destinations.\n\nA great portion of \"Blue Mars\" is concerned with the effects of extreme longevity on its protagonists, most of whom have lived over two hundred years as a result of repeated longevity treatments. In particular, Robinson speculates on the psychological effects of ultra-longevity including memory loss, personality change, mental instability, and existential boredom.\n\nThe initial colonists from the \"Ares\" who established a permanent colony. Many of them later become leaders or exemplary figures in the transformation of Mars or its new society. The \"First Hundred\" actually consisted of 101, with Coyote being smuggled aboard the Ares by Hiroko.\n\nAn American astronaut, who was the first human to walk on Mars in the year 2020. He returns a public hero and uses his considerable influence to lobby for a second mission, this time one of colonization. Boone received a large amount of radiation on his first trip to Mars, more than the recommended dosage according to medical regulations. However, his celebrity status allows him to skirt this. On the second voyage, Boone is one of the \"First Hundred\" colonists sent to permanently colonize Mars. His accomplishments and natural charm yield him an informal leadership role. In the first chapter of \"Red Mars\", John Boone is assassinated in a plot instigated by Frank Chalmers. The narrative then steps back to the First Hundred's voyage to Mars aboard the spaceship \"Ares\". His ideas continue as a point of reference for the remainder of the trilogy. Boone's character portrayal is complex; in one light, Boone is a stereotypically simple, heroic figure, an everyman hero: his first words on his first trip to Mars are \"Well, here we are.\" He is almost uniformly cheerful and good-natured, and approaches everything he undertakes with hale bonhomie. But later in \"Red Mars\", Robinson switches to Boone's point of view, and it is in this section that it is revealed that late in life, Boone is addicted to omegendorph, a fictional drug that is based on endorphins in the human brain. In addition, it reveals that at least some of his seeming simplicity might simply be an act designed to further his political goals. Overall, Boone is presented as larger-than-life.\n\nHead of the American contingent, he is Machiavellian in his use of power. However, his cynicism is later shown to be a form of self-defense; Chalmers is at least partly driven by a hidden idealistic side. Early in the voyage to Mars, he becomes sexually involved with Maya Toitovna, the leader of the Russian contingent of the mission. During the second half of the voyage, Toitovna becomes involved with Boone. Already bitter that Boone became the first to walk on Mars instead of him as they were both candidates for the mission and that he was allowed to join the colonization trip despite his manipulations, Chalmers further despises Boone because of Toitovna's affection. His dislike culminates in his involvement in a plot to assassinate Boone, which ultimately succeeds and allows him to take over handling major affairs on Mars, which ultimately became his undoing as his ruthless governance and aggressive diplomatic work backfire on him during the revolution of 2061. In the final chapters of \"Red Mars\", Chalmers flees with Toitovna and other members of the First Hundred to join the hidden colonists at the polar ice cap but dies along the way when he is caught outside their vehicle during an aquifer flood in Valles Marineris.\n\nAn emotional woman who is at the center of a love triangle between Boone and Chalmers, she begins as head of the Russian contingent. The novels hint that she used both wit and seduction to rise through the ranks of the Russian space agency to become the leader of the first colonization mission. After the first revolution, she flees with other members of the First Hundred to the hidden colony in the pole. She becomes a school teacher of the children of the hidden colonists but later becomes a powerful political force. After the deaths of Chalmers and Boone, she falls in love with Michel Duval. She suffers heavily from bipolar disorder and from memory-related psychological disorders with growing age, which often lead her to isolate herself from others and sometimes turn violent. Throughout the novels, Maya takes an active political role, helping to keep the surviving First Hundred together during the failed revolution of 2061 and guiding the successful revolutions that occur decades later, despite her psychological problems.\n\nA Russian engineer who started out building nuclear reactors in Siberia, during the voyage and initial exploration of Mars, she does her best to avoid the squabbles of the other members of the First Hundred. Instead, she busies herself by building the first permanent habitation of Mars, Underhill, using programmed automated robots. She also helps to construct a new and larger habitat, and research facility in a nearby canyon. In the later books, she becomes a reluctant politician. Chernyshevski is in love with Bogdanov and is devastated when he is killed in an attack by anti-revolutionary forces associated with UNOMA, the transnationals and Phyllis Boyle during the first Martian revolution. In retaliation for Bogdanov's murder, she activates his hidden weapon system, built into Phobos, which causes the entire moon (a UNOMA/transnational military base) to decelerate in orbit and destructively aerobrake in Mars' atmosphere, utterly destroying it. In \"Blue Mars\", she falls in love with Art Randolph, with whom she eventually starts a family. After Martian independence, she grudgingly becomes the first president of Mars.\n\nA mechanical engineer with anarchist leanings, possibly based on the Russian Machist, Alexander Bogdanov (the character's ancestor) and Arkady Strugatsky, he is regarded by many other members of the First Hundred, particularly Boyle, as a troublemaker. He leads the team which establishes an outpost on the moon Phobos, and leads an uprising against the transnational corporation towards the end of first novel. Like Boone (with whom he was good friends), his political ideas (later known as Bogdanovism) weigh heavily on characters later in the series. In love with Nadia Chernyshevski, he is killed during the first Martian revolution in 2061.\n\nAn American physicist, he is a brilliant and creative scientist, and is greatly respected for his intellectual gifts. However, he is socially awkward and often finds it difficult to understand and relate to other people. Russell is a leader of the Green movement, the goal of which is to terraform Mars. During \"Green Mars\", Sax suffers a stroke while being tortured by government security forces and fellow member of the First Hundred, Phyllis Boyle (although according to later it is revealed she actually opposed Sax's torture). He subsequently suffers from Expressive aphasia and has to relearn how to speak and becomes less predictable in his actions. Originally apolitical, this event and a growing attachment to Mars itself leads Russell to become the physical architect of the second revolution. After memory issues become apparent in many of the remaining first hundred including Sax he begins work on an ambitious project to gather the remaining first hundred and have them try an experimental treatment he helped to develop. It is after this that Sax realizes his persistent attempts to please Ann are actually because he is also secretly in love with Ann Clayborne, who cannot stand him at first, but after decades on Mars, eventually reconciles. Saxifrage means \"stonebreaker\" and is the name for an Alpine plant that grows between stones.\n\nAn American geologist, Clayborne is one of the first areologists and maintains a stalwart desire to see Mars preserved in the state it holds when humans arrive. Clayborne early on debates Saxifrage Russell over the proper role of humanity on Mars and though initially apolitical, this stance marks her as the original \"Red,\" while Russell's hands-on terraforming reflects the antithesis of these views. Clayborne is shown to prefer solitude during much of the series, and even her relationship with fellow First Hundred settler Simon (with whom she has a child) is subject to introspective silence in most cases. Simon's death and the estrangement she finds from their son Peter when the latter emerges as a leading moderate \"green\" drive her to further isolation. Clayborne's relationship with Russell is shown to be complex, the two of them taking early opposite views but the situation slowly changing as Russell comes to appreciate what has been unleashed and what has indeed been lost as science gives way to commercial exploitation that he cannot control. During the events of \"Blue Mars\", Russell intervenes to save Clayborne's life; later, the two are revealed to have once shared an attraction that went astray because of a casual misinterpretation between them. Ann undergoes a drastic change towards Blue Mars due to the emergence of something inside of her that she describes as anti-Ann and something else that she can't quite describe.\n\nA Japanese expert on biology, agriculture, and ecological systems, it was Ai who smuggled Desmond \"Coyote\" Hawkins onto the \"Ares\" (the two were friends and lovers as students in London). She is the charismatic leader of the farm team, one of the important work groups and cliques among the First Hundred. She thus becomes the focus of many of the trilogy's central themes. Most importantly, she teaches the importance of maintaining a respectful relation with one's planet. On Mars, this is called the Areophany. In the secret colony Zygote, which Hiroko established, the first generation of children of the First Hundred, the ectogenes, are all the product of artificial insemination outside of any human body. Hiroko uses the ova of the female members of the First Hundred as the female genetic material and uses the sperm of the male members of the First Hundred to fertilize the ova. Although Hiroko is seldom at the center of the narrative, her influence is pervasive. She disappears for the final time in \"Green Mars\". Her ultimate fate is left unresolved. In Japanese, \"ai\" means love.\n\nA French psychologist pivotally involved in early psychological screening of First Hundred candidates in Antarctica which he describes as being a collection of double bind requirements. Duval is assigned to accompany the Mars mission and is treated as an observer rather than as a member of the team during the early events of \"Red Mars\". His aloof personality enforces this ostracism and also subverts his relationships with others, but in time it becomes clear that Duval is struggling with his own psychological issues perhaps more than anyone else from the expedition. During the first disappearance of the farm team, he is invited by Hiroko to flee with the farm team and establish Zygote, the first hidden colony. Duval desperately wants to return to Provence as he remembers it, and after visiting as a part of the Martian diplomatic mission to Earth, he becomes even more homesick. Duval falls in love with Maya Toitovna and guides her through particularly challenging psychological episodes throughout most of the series, dying late in \"Blue Mars\" of heart arrhythmia when Maya displays signs of very heavy temporary memory loss.\n\nNearly sixty when he arrives on Mars, a Russian biological scientist who is the oldest of the First Hundred. Taneev heads medical treatment and most research projects on Mars, becoming famous as the creator of the gerontological treatment used to regenerate human cellular systems and ushering in a new era of longevity. He lives in Acheron on the Great Escarpment in the north of Mars before fleeing to the hidden colony after the First Revolution but later returns to his research, falling victim to \"quick decline\" late in the events of \"Blue Mars\". For much of his Mars-centric life, Taneev lives in a ménage à trois with Ursula and Marina, the exact nature of which is never resolved.\nA Christian American biologist with a harsh personality that does not win her many friends among the First Hundred and gains particular enmity from Ann Clayborne. As the Mars situation develops, Boyle sides against most of the First Hundred in favor of the increasingly authoritarian United Nations Office of Mars Affairs (UNOMA) and its successor, the corporate/quasi-fascist United Nations Transitional Authority (UNTA). Her influence is strongest during the later events of \"Red Mars\", where by the 2061 revolution she has been placed in charge of the asteroid Clarke that serves as the counterweight of the First Space Elevator. The events of the revolution send Clarke (and Boyle) spinning off into the outer Solar System at the end of \"Red Mars\"; \"Green Mars\" finds her back in the equation, but her influence is greatly reduced against the backdrop of a much-expanded UNTA presence. Boyle engages in a brief sexual relationship with Saxifrage Russell (who despises her) while the latter is living under an assumed identity and is singularly capable of discerning who he really is, turning him over the UNTA. She is later present at a session in Kasei Vallis where Russell is being tortured, and is killed by Maya Toitovna. Later, as his memory recovers, Russell reveals that Boyle had been opposed to his torture and was demanding that he be released at the time that Maya's team freed him.\n\nA Trinidadian stowaway, he is a friend and supporter of Hiroko, and a fervent anarchist communist. Present in \"Red Mars\" only as a stowaway who eventually blends effortlessly into the Martian background, he is not even identified as anything more than Coyote until the beginning of \"Green Mars\". He becomes a leading figure in the underground and an unofficial coordinator of a developing gift economy.\n\nSince the trilogy covers over 200 years of human history, later immigrants and the children and grandchildren of the First Hundred eventually become important characters in their own right.\n\nThe Martians use the same terminology for different generations as Japanese Americans. People who immigrated from Earth are called issei, the first generation born on Mars are nisei, and the second-generation Martians are sansei. Third-generation Martians are called yonsai.\n\n\n\n\n\n\n\n\n\n\nIn an interview at UCSD, Robinson said that he was looking at a satellite photo of Mars and thought that would be a great place to go backpacking. He said the Mars trilogy grew out of that urge.\n\n\nThe Mars trilogy rights were at one point held by James Cameron, who planned a five-hour miniseries to be directed by Martha Coolidge, but he passed on the option. Later Gale Ann Hurd planned a similar mini-series for the Sci-Fi Channel, which also remained unproduced. Then, in October 2008, it was reported that AMC and Jonathan Hensleigh had teamed up and were planning to develop a television mini-series based on \"Red Mars\".\n\nOn September 24, 2014, SpikeTV announced it was working with producer Vince Gerardis to develop a TV series adaptation of \"Red Mars\". On December 8, 2015, SpikeTV formally greenlit a 10-episode first season of a TV series based on the novels, with J. Michael Straczynski serving as showrunner and writer. On March 25, 2016 \"Deadline\" reported that Straczynski had left his position as showrunner with Peter Noah replacing him but he too left due to creative differences with Spike. Spike has put the series on hold for further development.\n\nThe content of \"Green Mars\" and the cover artwork for \"Red Mars\" are included on the Phoenix DVD, carried onboard \"Phoenix\", a NASA lander that successfully touched down on Mars in May 2008. The First Interplanetary Library is intended to be a sort of time capsule for future Mars explorers and colonists.\n\nThe trilogy has been translated into Spanish, French, German, Russian, Chinese, Polish, Hebrew, Japanese, Italian, Romanian, Bulgarian and Serbian among others.\n\n\n"}
{"id": "21009", "url": "https://en.wikipedia.org/wiki?curid=21009", "title": "Marsh gas", "text": "Marsh gas\n\nMarsh gas, swamp gas and bog gas is a mixture of methane, hydrogen sulfide and carbon dioxide, produced naturally within some geographical marshes, swamps, and bogs. \n\nThe surface of marshes, swamps and bogs is initially porous vegetation that rots to form a crust that prevents oxygen from reaching the organic material trapped below. That is the condition that allows anaerobic digestion and fermentation of any plant or animal material which incidentally also produces methane.\n\nIn some cases there is sufficient heat, fuel and oxygen to allow spontaneous combustion and underground fires to smolder for some considerable time as occurred at a natural reserve in Spain. Such fires can cause surface subsidence presenting an unpredictable physical hazard and as well as environmental changes or damage to the local environment and the ecosystem it supports.\n\n"}
{"id": "49600241", "url": "https://en.wikipedia.org/wiki?curid=49600241", "title": "Micro Data Center", "text": "Micro Data Center\n\n]\n\nA micro data center (MDC) is a smaller or containerized (modular) data center architecture that is designed to solve different sets of problems that take different types of compute workload that does not require to traditional facilities. Whereas the size may vary from rack to container, a micro data center may include fewer than 4 servers in a single 19-inch rack. It may come with built-in security systems, cooling systems, and/or fire protection. Typically there are standalone rack-level systems containing all the components of a 'traditional' data center. including in rack cooling, power supply, power backup, security, fire and suppression. They could be rapidly deployed indoors or outdoors or also in rugged terrains.\n\nMid 2017, introduced by DOME, technology was demonstrated that packs 64 high-performance servers, storage, networking, power and cooling integrated in a 2U 19\" rack-unit. This packaging, sometimes called 'datacenter-in-a-box' allows deployments in spaces where traditional data centers do not fit, such as factory floors (IOT) and dense city centers especially for edge-computing and edge-analytics. \n\nTheir size, versatility and plug & play features make them ideal for use in remote locations, for a branch office, or even for use temporarily at locations that are in high risk zones.\n\nSee also Asperitas Microdatacenter and DOME MicroDataCenter\n\n\n11. BUANET Micro DC (https://voltampohm.wixsite.com/microdatacenter)\n"}
{"id": "11392946", "url": "https://en.wikipedia.org/wiki?curid=11392946", "title": "Minto wheel", "text": "Minto wheel\n\nThe Minto wheel is a heat engine named after Wally Minto. The engine consists of a set of sealed chambers arranged in a circle, with each chamber connected to the chamber opposite it. One chamber in each connected pair is filled with a liquid with a low boiling point (propane (\"T\" = −42 °C) and R-12 (\"T\" = −29.8 °C) are listed in the Mother Earth News articles). Ideally, the working fluid also has a high vapor pressure and density.\n\nAs the lower chamber in each pair is heated, the liquid begins to vaporize, forcing the remaining liquid to travel to the upper chamber. This fluid transfer causes a weight imbalance, which causes the wheel to rotate. Minto's pamphlet also suggests obtaining a pressure differential with a dissolved gas instead of a boiling gas. Soda water or propane dissolved in kerosene are suggested.\n\nThe Minto wheel operates on a small temperature gradient, and produces a large amount of torque, but at very low rotational speed. The speed of rotation is directly proportional to the surface area of the containers used, the volume, and the height of the wheel. The higher the ratio of surface area to volume, the greater the rate of revolution. \nIn 1881, the Iske brothers got a patent granted for a design similar to the Minto wheel.\nThe patent suggests lamps as heating sources.\n\nLater the same year, Israel L. Landis got a patent for a similar engine.\nDifferent to the Minto wheel and the Iske brothers' patent, the engine was oscillating, not revolving.\nIn the following years, the Iske brothers were granted various patents, including some relating to modification and/or improvements on engines similarly to the Minto Wheel and an oscillating engine similarly to Israel L. Landis design.\n\nThe oscillating types by the Iske Brothers and Landis are related to the drinking bird toy.\n\nThe drinking bird is dating back to 1910s~1930s.\nThe drinking bird was patented in the US in 1945 and 1946 by two different inventors.\nWally Minto experimented with different working fluids. With the working fluids he used, he got the required temperature difference down, enabling the engine - for example - to run on solar power.\nBased on the working fluid, his improved wheel is also known as \"Freon Power Wheel\". Popular Science reported about in its March 1976 issue.\n\nA working example of a Minto wheel was first published in a series of articles in The \"Mother Earth News\", Issues #38 March, #39 May and #40 July 1976. Test units constructed by \"Mother Earth News\" (Issue 40, July 1976) and the MythBusters (Episode 24, December 5, 2004 – \"Ming Dynasty Astronaut\") did work to convert temperature difference into torque; however not as well as overenthusiastic boosters claimed.\n\n\n"}
{"id": "18134915", "url": "https://en.wikipedia.org/wiki?curid=18134915", "title": "Parameter Value Language", "text": "Parameter Value Language\n\nIn computer programming, Parameter Value Language (PVL) is a markup language similar to XML. It is commonly employed for entries in the Planetary Database System used by NASA to store mission data, among other uses.\n\nThere are at least two \"dialects\" – \"USGS Isis Cube Label\" and \"NASA PDS 3 Label\". \n\n"}
{"id": "292280", "url": "https://en.wikipedia.org/wiki?curid=292280", "title": "Perforation", "text": "Perforation\n\nA perforation is a small hole in a thin material or web. There is usually more than one perforation in an organized fashion, where all of the holes are called a \"perforation\". The process of creating perforations is called perforating, which involves puncturing the workpiece with a tool.\n\nPerforations are usually used to allow easy separation of two sections of the material, such as allowing paper to be torn easily along the line. Packaging with perforations in paperboard or plastic film is easy for consumers to open. Other purposes include filtrating fluids, sound deadening, allowing light or fluids to pass through, and to create an aesthetic design.\n\nVarious applications include plastic films to allow the packages to breathe, medical films, micro perforated plate and sound and vapor barriers.\n\nRotary pinned perforation rollers are precision tools that can be used to perforate a wide variety of materials. The pins or needles can be used cold or heated. Cold perforation tools include needle punches.\n\nThere are a handful of manufacturers that specialize in hot and cold needle perforation tooling and equipment. In materials that have elasticity this can result in a \"volcano\" hole that is preferred in many applications.\n\nPinned rollers can be made from a variety of materials, including plastic, steel, and aluminum.\n\nIn more brittle films, cold perforation can cause slitting rather than creating a round hole, which can jeopardize the material's integrity under pressure. The solution to this is often heating the pin; i.e. hot pin perforation. Hot perforation melts a hole in the material, causing a reinforced ring around the hole. Hot needle perforation also assists when high density pin patterns are utilized, as the heat aids the perforation of the material.\n\nDie and punch sets can be used for thicker materials or materials that require large holes; this process is the most common for metalworking. The workpiece is sheared by pressing (either by machine or hand tool) the punch through the workpiece and into the die. The middle section of the workpiece is scrap; commonly known as the chad in paper and similar materials. The punch and die are shaped to produce the desired shaped hole. The clearance (the distance between the outside circumference of the punch and the inner circumference of the die) must be properly maintained to ensure a clean cut. Burrs are produced on the side of the workpiece that is against the die.\n\nCommon applications are fruit and vegetable bags, hole punching and ticket punching.\n\nLaser cutting can place many precise holes in a web. Laser perforations look similar in many respects to hot needle perforations. However, laser systems are expensive. The big advantage of laser perforation is the consistency of the hole size, compared to mechanical perforation. This is very important in modified atmosphere packaging for fresh produce. The laser perforation is often carried out on roll slitting machines (slitter rewinder) as the printed material is slit down to the finished roll size.\n\nPerforation frequently refers to the practice of creating a long series of holes or slits so that paper or plastics can be torn more easily along a given line: this is used in easy-open packaging. Since the creation of perforation devices in the 1840s and 1850s, it has seen use in several areas.\n\nPostage stamps are one common application of this, where small round holes are cut in lines to create individual pieces. Perforations on stamps are rather large, in the order of a millimeter, in comparison other perforated materials often have smaller holes.\n\nIt is common for cheque-books, notebooks and legal pads to have perforations making it easier to tear out individual pages or leafs. Perforation is used in ways to separate loose leaf (or even a form of graph paper from a ringed binder). A fine perforation next to the rings allows the page to be separated from the book with no confetti.\n\nScrewcaps on glass or plastic bottles are sealed with a ring at the bottom of the cap attached by perforation. Twisting the cap has the effect of rupturing the material between the perforations and indicating that the original seal has been broken.\nThe edges of film stock are perforated to allow it to be moved precise distances at a time continuously. Similarly, punched cards for use in looms and later in computers input and output devices in some cases were perforated to ensure correct positioning of the card in the device, and to encode information.\n\nPerforation of steel strips is used in the manufacture of some zesters and rasps.\n\nHistorically, perforation patterns other than linear were used to mark stamps. A series of patents had been issued in the late 19th Century for perforation machines to be used on rail lines for ticketing. Libraries and private collections used similar perforating stamps to mark ownership of books. End sheets, title pages, and image plates were punched with the namesake of the collection. Today, similarly elaborate perforation patterns continue to be used in orienteering.\n\nBread bags for some bread often have micro-perforations in the plastic, which is supposed to keep the bread fresh by releasing excess moisture. Similarly, bags of concrete use small perforations to allow air to escape while they are being filled.\n\n"}
{"id": "3473557", "url": "https://en.wikipedia.org/wiki?curid=3473557", "title": "Placement (electronic design automation)", "text": "Placement (electronic design automation)\n\nPlacement is an essential step in electronic design automation - the portion of the physical design flow that assigns exact locations for various circuit components within the chip's core area. An inferior placement assignment will not only affect the chip's performance but might also make it non-manufacturable by producing excessive wirelength, which is beyond available routing resources. Consequently, a placer must perform the assignment while optimizing a number of objectives to ensure that a circuit meets its performance demands. Together, the placement and routing steps of IC design are known as place and route.\n\nA placer takes a given synthesized circuit netlist together with a technology library and produces a valid placement layout. The layout is optimized according to the aforementioned objectives and ready for cell resizing and buffering — a step essential for timing and signal integrity satisfaction. Clock-tree synthesis and Routing follow, completing the physical design process. In many cases, parts of, or the entire, physical design flow are iterated a number of times until design closure is achieved.\n\nIn the case of application-specific integrated circuits, or ASICs, the chip's core layout area comprises a number of fixed height rows, with either some or no space between them. Each row consists of a number of sites which can be occupied by the circuit components. A free site is a site that is not occupied by any component. Circuit components are either standard cells, macro blocks, or I/O pads. Standard cells have a fixed height equal to a row's height, but have variable widths. The width of a cell is an integral number of sites. On the other hand, blocks are typically larger than cells and have variable heights that can stretch a multiple number of rows. Some blocks can have preassigned locations — say from a previous floorplanning process — which limit the placer's task to assigning locations for just the cells. In this case, the blocks are typically referred to by fixed blocks. Alternatively, some or all of the blocks may not have preassigned locations. In this case, they have to be placed with the cells in what is commonly referred to as mixed-mode placement.\n\nIn addition to ASICs, placement retains its prime importance in gate array structures such as field-programmable gate arrays (FPGAs). In FPGAs, placement maps the circuit's subcircuits into programmable FPGA logic blocks in a manner that guarantees the completion of the subsequent stage of routing.\n\nPlacement is usually formulated as a problem of constrained optimization. The constraint is to remove overlaps between all the instances in the netlist. The optimization objective can be of multiple, which typically include: \n\nPlacement is divided into global placement and detailed placement. Global placement introduces dramatic changes by distributing all the instances to appropriate locations in the global scale with minor overlaps allowed. Detailed placement shifts each instance to nearby legal location with very moderate layout change. Placement and overall design quality is most dependent on the global placement performance.\n\nAt early time, placement of integrated circuits is handled by combinatorial approaches. When IC design was of thousand-gate scale, simulated annealing methodologies such as TimberWolf exhibits the best performance. As IC design entered million-scale integration, placement was achieved by recursive hyper-graph partitioning like Capo.\n\nQuadratic placement later outperformed combinatorial solutions in both quality and stability. GORDIAN formulates the wirelength cost as a quadratic function while still spreads cells apart through recursive partitioning. The algorithm in first models placement density as a linear term into the quadratic cost function, and solves the placement problem by pure quadratic programming. Majority of the modern quadratic placers (KraftWerk, FastPlace, SimPL) are following this framework, each with different heuristics on how to determine the linear density force.\n\nNonlinear placement presents better performance over other categories of algorithms. The approach in first models wirelength by exponential (nonlinear) functions and density by local piece-wise quadratic functions, in order to achieve better accuracy thus quality improvement. Follow-up academic works mainly include APlace and NTUplace.\n\nePlace is the state of the art global placement algorithm. It spreads instances apart by simulating an electrostatic field, which introduces the minimum quality overhead thus achieves the best performance.\n\n\nThe following academic journals provide further information on EDA\n"}
{"id": "2518584", "url": "https://en.wikipedia.org/wiki?curid=2518584", "title": "Prescaler", "text": "Prescaler\n\nA prescaler is an electronic counting circuit used to reduce a high frequency electrical signal to a lower frequency by integer division. The prescaler takes the basic timer clock frequency (which may be the CPU clock frequency or may be some higher or lower frequency) and divides it by some value before feeding it to the timer, according to how the prescaler register(s) are configured. The prescaler values that may be configured might be limited to a few fixed values (powers of 2), or they may be any integer value from 1 to 2^P, where P is the number of prescaler bits.\n\nThe purpose of the prescaler is to allow the timer to be clocked at the rate a user desires. For shorter (8 and 16-bit) timers, there will often be a tradeoff between resolution (high resolution requires a high clock rate) and range (high clock rates cause the timer to overflow more quickly). For example, one cannot (without some tricks) achieve 1 µs resolution and a 1 sec maximum period using a 16-bit timer. In this example using 1 µs resolution would limit the period to about 65ms maximum. However the prescaler allows tweaking the ratio between resolution and maximum period to achieve a desired effect.\n\nPrescalers are typically used at very high frequency to extend the upper frequency range of frequency counters, phase locked loop (PLL) synthesizers, and other counting circuits. When used in conjunction with a PLL, a prescaler introduces a normally undesired change in the relationship between the frequency step size and phase detector comparison frequency. For this reason, it is common to either restrict the integer to a low value, or use a dual-modulus prescaler in this application. A dual-modulus prescaler is one that has the ability to selectively divide the input frequency by one of two (normally consecutive) integers, such as 32 and 33. Common fixed-integer microwave prescalers are available in modulus 2, 4, 8, 5 and 10, and can operate at frequencies in excess of 10 GHz.\n\nA prescaler is essentially a counter-divider, and thus the names may be used somewhat interchangeably.\n\n"}
{"id": "7706010", "url": "https://en.wikipedia.org/wiki?curid=7706010", "title": "Professional organizing", "text": "Professional organizing\n\nProfessional Organizing is an industry that developed in 1984 in Los Angeles, to help individuals and businesses design organizing systems and processes to improve quality of life, personal productivity, and greater efficiency.\n\nThere are approximately 3,500 Professional Organizers and Productivity Consultants who are members of the professional association, the National Association of Productivity and Organizing Professionals (NAPO).\n\nBeginning in 2002, a number of television programs aired on the subject: \"Life Laundry\" was followed by others such as, \"Clean Sweep\", \"Neat\", \"\", \"Hoarding: Buried Alive,\" \"Hoarders\", and , as well as magazines like \"Real Simple\".\n\nKaren Shortridge, an entrepreneur who, in the early 1980s, began organizing residential spaces for friends and family, was featured in a local Los Angeles newspaper in 1983. The story concluded with an announcement for a meeting for \"women who like to organize\", taking place later in the week.\n\nOstensibly, the purpose of a gathering of \"women who like to organize\" was social in nature; more like a \"kaffee klatsch\" to talk about children and grandchildren, recipes, husbands, and homemaking.\n\nBut two of the women in attendance: Maxine Ordesky and Stephanie Culp, had other ideas, having no interest in discussing wet diapers. Instead, they envisioned a group devoted to business building.\n\nAt the time, both women were self-employed entrepreneurs. Maxine began her business, \"The Creative Organizer\" in the early 1970s. Stephanie Culp began organizing prior to 1983. Her first company, \"The Grinning Idiot\", an all-purpose errand company [with the tag line, \"nothing is too moronic or mundane\"] brought her into contact with her first organizing client.\n\nAsking for a change to the meeting format – the room quickly divided. Those who wanted to keep it social, stayed behind in the boardroom at the bank. Maxine, Stephanie, and Ann Gambrell, a retired registered nurse, had been teaching classes on organizing homes and offices for an adult education program, and wrote an organizing column for the \"Daily Breeze\" newspaper in Torrance, California.\n\nA few short weeks later – Ann, Maxine, and Stephanie, were joined by Beverly Clower and Jeanne Schorr – for a meeting in Beverly's living room. As the group coalesced structurally and organizationally, and as the women began marketing their services for paid work, it became clear that \"to the outside world, organizers were glorified housekeepers or maids.\" Stephanie Culp made a compelling argument to add the word, \"Professional\" to the women's work as Organizers – \"a description that seemed outlandish at first, but which eventually overcame the ‘housekeeper’ perception and opened the starting gate to actually attracting clients.\"\n\nThe women agreed to the new terminology, and the \"Association of Professional Organizers\" [APO] was formed. APO filed for 501 (c) non-profit status with the intention of \"women helping women\". They established policies for chapters then personally managed the launch of the first four chapters – Los Angeles, San Diego, San Francisco, and New York City.\n\nWith the founding of the New York chapter in 1985, the five women created the National Association of Professional Organizers [NAPO]. That year, from August to September, the intrepid pioneers formalized the Association by electing officers, formulating dues, and holding elections. As meetings became monthly, the group expanded from living rooms to conference rooms, and a movement began.\n\nMaxine Ordesky, founding President, presided over APO, then NAPO, through the first five years. She recalls, \"We met in a small conference room located off a corridor in a local mall in West Los Angeles. At that first meeting, I stood up in front of 10 women, not realizing, in my wildest dreams, that the organizing profession would grow to thousands of members worldwide.\"\n\nThe first NAPO Conference was held in one of the break-out rooms in Los Angeles at the Universal Sheraton Hotel. \"A seemingly impossible idea\", as Stephanie Culp reminds us. Ann Gambrell recalled, \"We didn’t have a mailing list but we needed to fill seats. We didn’t want to be embarrassed with a low turnout so we invited our friends and husbands to help fill the room\".\n\nBeverly Clower described those early years as important to her own professional development, \"All these women had found their special niche, and we were all like sisters in this together. Women need to be heard and here we were: our clients and our colleagues were listening to us because we had something valuable to say\".\n\nGolden Circle, the first Code of Ethics, and 501(c)(3) status, just to name a few of the many ideas, programs, and policies that originated with the Founders.\n\nThe Founders [as Clower, Culp, Gambrell, Ordesky, and Schorr are fondly referred by NAPO members] started with a notion that women could be in business using skills that many of them already possessed. They then developed that vision by building on it as a team. In a time before computers, they did everything by hand. There was no funding available, so they used their own money.\n\nAll this happened because five women had the courage and determination to take an idea that they believed in totally; they became unstoppable.\n\nFounder Stephanie Culp is also the author of eight organizing books, producing relevant content at a time when only two other books were available: Stephanie Winston's \"Getting Organized\" and Don Aslett's \"Clutter's Last Stand\". Maxine Ordesky's \"The Complete Home Organizer\" was published in 1993.\n\nWatch the Founders' video, It started in LA...\n\nNAPO defines Professional Organizer and Productivity Consultant as follows:\n\nCertified Professional Organizers (CPOs) have proven industry proficiency by demonstrating they possess the body of knowledge and experience essential to professional organizing and productivity consulting. The CPO® credential identifies professional organizers who’ve documented a specific number of paid hours that include transferring organizing skills to the client, and passed the Board of Certification for Professional Organizers (BCPO®) examination. The credential provides the organizing and productivity industry a way to elevate its professional standards.\n\nThe National Association of Productivity and Organizing Professionals' education arm is known as NAPO University. Currently NAPO University offers two Specialist Certificates: Residential Organizing and Workplace Productivity. A third Specialist Certificate in Life Transitions will be launched in early 2018. NAPO University also offers a Business Resources Track to support Professional Organizers and Productivity Consultants in developing and growing their businesses.\n\nProfessional Organizers achieve the goal of creating and maintaining organizational systems by teaching others the basic principals of organization. Writer Julie Morgenstern suggests communicating these principals by using the acronym \"SPACE\", interpreted as: S=Sort, P=Purge, A=Assign a Home, C=Containerize and E=Equalize. The last step (\"E\") consists in monitoring how the new system that has been created is working, adjusting it if needed, and maintaining it. This principle is applicable to every type of organization.\n\nAs one of their main jobs, Professional Organizers help clients reduce excessive clutter (paper, books, clothing, shoes, office supplies, home decor items, etc.) in the home or in the office. Professional Organizers endeavor to help individuals and business owners take control of their surroundings, their time, their paper, and their systems for life. Professional Organizers help redirect paradigms into more useful cross-applications that ensure properly co-sustainable futures for their clients' spaces and processes.\n\nProfessional Organizers offer a wide variety of services, from designing a functional closet, to organizing a cross-country move. For homeowners, a Professional Organizer might plan and reorganize the space of a room, improve paper management, or coach in time-management, or goal-setting. In a business setting, Professional Organizers work closely with their clients to increase productivity by stream-lining paper-filing, electronic organization, and employee time-management.\n\n\nHome:\n"}
{"id": "34275855", "url": "https://en.wikipedia.org/wiki?curid=34275855", "title": "Roaring rails", "text": "Roaring rails\n\nRoaring rails is a phenomenon of short-wave corrugation observed in railroad tracks. Despite being a widely observed type of railroad track deformity, the reason it occurs was not known or researched until recently.\n\nRoaring rails are very short wave corrugations that range from 25 to 75 millimeters in length. It is commonly associated with high speed passenger, transit, and light axle load railroad operations.\n\nIt is generally accepted that a few distinct causes lie behind different wavelengths of railroad corrugation. One study indicates that the specific short-wave railroad deformity is mainly caused by pinned-pinned resonance, in which the rail vibrates as a fixed beam, as if pinned between periodically placed sleepers. The dynamic train-track interaction that causes fixed frequency vibrations at high speeds, commonly observed in light load metro operations, and the anti-resonance caused by the pinning of the rails on sleepers, causes deformation and the \"roaring\" corrugation of the rails.\n\n"}
{"id": "24352961", "url": "https://en.wikipedia.org/wiki?curid=24352961", "title": "SM4All", "text": "SM4All\n\nSM4All (Smart hoMes for All) is an international scientific research project funded by the European Community. It started on September 1, 2008 and will end on August 31, 2011. The SM4All project aims at studying and developing an innovative middleware platform for inter-working of smart embedded services in immersive and personcentric environments, through the use of composability and semantic techniques, in order to guarantee dynamicity, dependability and scalability, while preserving the privacy and security of the platform and its users. This is applied to the challenging scenario of private/home/building in presence of users with different abilities and needs (e.g., young able bodied aged and disabled).\n\nSAPIENZA Universita di Roma\n\nSwedish Defence Research Agency (FOI)\n\nElsag Datamat\n\nTechnische Universitaet Wien\n\nFondazione Santa Lucia\n\nGuger Technologies\n\nUniversity of Groningen\n\nThuiszorg Het Friese Land\n\nTelefonica Investigacion y Desarrollo\n\nKungliga Tekniska Hogskolan\n\nIn the design of the SM4All platform, there will be a specific focus on ontologies for describing service capabilities, to be used for obtaining the dynamic configuration and composition of the services, while preserving the privacy of the users. Within this project an innovative middleware platform for inter-working of smart embedded services by leveraging on peer-to-peer (P2P) technologies will be investigated. In particular, in the SM4All project, P2P, service orientation and context-awareness are merged in novel ways in order to define general reference architecture for embedded middleware targeted to immersive scenarios, among which the domotics and home-care have been selected as showcases.\n\nP2P systems (P2P) have become a popular technique to design large-scale distributed applications in unmanaged inter-domain settings, such as file sharing or chat systems, thanks to their capabilities to self-organize and evenly split the load among peers. The platform is inherently scalable and able to resist to devices’ churn and failures, while preserving the privacy of its human users as well as the security of the whole environment. The embedded systems are specialized computers used to control equipment such as the smart homes. To enable interoperation among heterogeneous devices and to provide a service-oriented basis, the project considers XML based protocols such as Web services and Universal Plug and Play (UPnP).\n\nFor example, a woman wants to take a bath. She enters this goal into the computer. Something happens then: The temperature in the bathroom will rise. The water runs in the bathtub with the preferred temperature. The cupboard opens to offer towels. If the woman is disabled, her nurse will be informed by the system.\n\nIn SM4All, the focus is on the process-oriented composition of stateful services. The idea is that a triggering condition in the home or a desire of the user can trigger the execution of a complex process. The process is defined in the moment that it needs to be executed. It automatically composes services available on home devices and appliances. The execution of the process thus depends on the context of the home, of its inhabitant and the available services. To achieve this it is necessary to identify the home context, to discover available devices and services and to compose them at execution time. In the SM4All project we consider Automated planning and scheduling approaches for the composition such as the \"Roman Model\" and the \"Barbarian\" constraint based approach.\n\nTwo techniques adopted in the SM4All project are\n\nBrain Computer Interface for Virtual-Reality Control\n\nAn electroencephalogram (EEG) based brain-computer interface (BCI) was connected with a Virtual Reality system in order to control a smart home application. Therefore, special control masks were developed which allowed using the P300 component of the EEG as input signal for the BCI system. Control commands for switching TV channels, for opening and closing doors and windows, for navigation and conversation were realized. Experiments with 12 subjects were made to investigate the speed and accuracy that can be achieved if several hundred of commands are used to control the smart home environment. \nThe study clearly shows that such a BCI system can be used for smart home control. The Virtual Reality approach is a very cost-effective way for testing the smart home environment together with the BCI system.\n\nService Composition\n\nSensors and devices collaborate one another as service providing and requesting nodes of a network. A service can be a purely information one (e.g., providing the temperature in a room) or a state changing one (closing all the lights in a room). Atomic services are then amenable to discovery and composition, enabling the home to perform complex tasks. The user can than express a goal, such as that of possibly according to some optimality measure. For instance the user may want to take a bath. The goal will be translated into a process invoking various services (turning on the heater in the bathroom, the lights, putting an alarm in the living room, checking that there is hot water available, etc.). Notice that the same goal can take different execution forms in different homes or in different conditions (e.g., the alarm in the living room is not available, but the goal of taking a bath is still satisfied).\n\n"}
{"id": "2113476", "url": "https://en.wikipedia.org/wiki?curid=2113476", "title": "Scale focus", "text": "Scale focus\n\nScale focus, or zone focus, is a type of focusing system used by many inexpensive cameras from the 1940s and 1950s. These cameras have an adjustable focus, but lack a focusing aid such as a rangefinder. It is necessary to determine the distance to the subject and set the focus using a scale printed on the lens. To get properly focused photographs with this type of camera requires accurate estimation of the distance to the subject, or measurement by other means, e.g. a tape measure.\n\nToday the term \"zone focus\" refers to the technique of setting a fixed focal distance (turning off autofocus), and often a fixed aperture, hence fixing the depth of field, and then taking photographs at that distance; the DOF can be read off of the DOF scale on the focus ring, hence the term \"zone\". This is frequently used in street photography to allow rapid and candid \"shooting from the hip\" (shooting without composing in the viewfinder) and avoiding autofocus lag. Zone focusing is also useful for pet action photography, where the time delay of focusing may be unworkable.\n\n"}
{"id": "27358495", "url": "https://en.wikipedia.org/wiki?curid=27358495", "title": "U.S. space exploration history on U.S. stamps", "text": "U.S. space exploration history on U.S. stamps\n\nWith the advent of unmanned and manned space flight a new era of American history had presented itself. Keeping with the tradition of honoring the country's history on U.S. postage stamps, the U.S. Post Office began honoring the various events with its commemorative postage stamp issues. The first U.S. Postage issue to depict a U.S. space vehicle was issued in 1948, the Fort Bliss issue. The first issue to commemorate a space project by name was the ECHO I communications satellite commemorative issue of 1960. Next was the Project Mercury issue of 1962. As U.S. space exploration progressed a variety of other commemorative issues followed, many of which bear accurate depictions of satellites, space capsules, lunar modules, space suits, and other items of interest.\n\nSpace exploration history is a popular topic, as record numbers of First-Day covers for postage stamps with space themes will attest. The Project Mercury issue of 1962 had more than three million 'First Day of Issue' cancellations, while the average number of First-Day cancels for other commemorative issues at that time was around half a million. In 1969, the Apollo VIII issue received 900,000 First-Day cancels while others received less than half this amount. As the advent of U.S. space exploration grew, so did the topic of Space Exploration on stamps.\n\nFort Bliss has a long and diverse history and functioned in many capacities over the years. By February 1946, over 100 Operation Paperclip scientists had arrived from Nazi Germany to develop rockets and were attached to the Office of the Chief of Ordnance Corps, Research and Development Service, Suboffice (Rocket), headed by Major James P. Hamill.\n\nThis stamp was issued on the 100th anniversary of Fort Bliss in El Paso, Texas, in its honor. Third Assistant Postmaster General Joseph L. Lawler dedicated the stamp in El Paso on November 5, 1948. The issue depicts what appears to be a rocket designed after the V-2 in the center, which technically makes it a \"space stamp\" in the Topographical world of philately.\n\nBased on findings made by Dr. Robert Goddard following World War I, the Germans hit a peak production of V-2's during 1944 and 1945 at Peenemunde. When they arrived in the United States, at Fort Bliss, they brought with them the knowledge of the V-2, and as such, the U.S. made arrangements with these scientists and employed their knowledge in developing rockets at Fort Bliss in Post War United States. It was this effort that led the way to the successful production of the great rockets that carried satellites and Astronauts into space.\n\nThe stamp's designer, Charles R. Chickering, intended the issue to salute the old as well as the new Fort Bliss and portray some of the highlights of the Fort's hundred-year history. Chickering, of the Bureau of Engraving and Printing, designed the stamp. C.A. Brooks engraved the vignette, and A.W. Christensen engraved the border, the lettering, and the numerals. The quantity issued was 64,561,000.\n\nFollowing the failure of the Delta rocket carrying Echo 1 on May 13, 1960, Echo 1A (commonly referred to as just Echo 1) was successfully put into a 944 to 1,048 mi orbit on August 12, 1960, by NASA. The 100 ft. diameter balloon was made of ultra thin (0.0050 in) metalized Mylar polyester film and was successfully used to reflect transcontinental and intercontinental telephone, radio, and television signals. The satellite also aided in the calculation of atmospheric density and solar pressure due to its large area-to-mass ratio.\nBecause its shiny surface the large balloon-like satellite was also reflective of visible light rays, Echo 1A was visible to the naked eye over most of the Earth. Echo 1 was a passive communications satellite: it functioned as a reflector, not a transmitter. After it was placed in a low Earth orbit (LEO), a signal would be transmitted from Earth to the Echo satellite and then reflected or bounced off its surface, and then returned to Earth. It was the pioneer of communications satellites. Because it was brighter than most stars, it was seen by more people than any other man-made object in space at that time. Upon reentry into Earth's atmosphere ECHO 1A burned up on May 24, 1968.\n\nThis was the first 'Space Stamp' with an actual subject of a real space vessel, unlike the generic or symbolic rocket depicted in the Fort Bliss issue of 1948. The U.S. Post Office issued this 4-cent Echo I \"Communications for Peace\" commemorative stamp through the Washington, D.C., post office on December 15, 1960. The stamp was produced and issued to honor the world's first communications satellite. Designed by Ervine Metzl, the stamp was printed by the rotary process, electric-eye perforated, and issued in panes of fifty stamps each. Quantities issued totaled more than 120 million.\n\nProject Mercury was the first human spaceflight program of the United States. It ran from 1959 through 1963 with the goal of putting a human in orbit around the Earth. The Mercury-Atlas 6 flight on February 20, 1962, was the first Mercury flight to achieve this goal.\n\nThe Post Office Department honored this first orbital flight of a United States astronaut on February 20, 1962, when it released the Project Mercury commemorative stamp, placed on sale throughout the country at the exact hour Colonel John Glenn's historic flight officially had returned to Earth safely.\n\nThe stamp features an image of the Mercury \"Friendship 7\" capsule circling the earth, against a field of stars. The spacecraft is now housed at the National Air and Space Museum, Washington, DC. Because the event was deemed so popular the number of quantities issued totaled more than 289 million, more than twice the average amount of quantities issued for commemorative postage issues of that time.\n\nThis issue has somewhat of an unusual history. It was one of the first issues printed on the new Giori Press (named after its inventor, Gualtiero Giori). It employed a series of specially cut rubber rollers that applied two or three different colored inks on the same printing plate. As the new press was being used to print the Project Mercury stamp before the mission took place and in case the mission failed or was canceled, the Bureau of Engraving and Printing kept word about the new press and the stamp issue's production a secret. To further assure that the project be kept secret the designer of this issue, Charles R. Chickering, worked from his home and simply claimed that he was away on vacation. The stamps, waiting at post offices around the U.S., were sealed and marked \"Top Secret\". Only after Glenn's trip were the postmasters allowed to open the package and see what was inside.\n\nRobert Hutchings Goddard (October 5, 1882 – August 10, 1945) is widely recognized as the \"father of rocketry,\" as he pioneered the modern propulsion rocket based on his knowledge of math, engineering and physics. His accomplishments included creating the first rocket propelled using liquid fuel and developing the first rocket to use internal vanes for guidance. He launched his first rocket in March 1926. Goddard continued to achieve many firsts in the field of rocketry with funding from institutions such as the Smithsonian. In 1919, the Smithsonian Institution published Robert Goddard's groundbreaking work, A Method of Reaching Extreme Altitudes. Other than from sources like the Smithsonian, Goddard received little public support for his research during his lifetime. He was the first to recognize the scientific potential of liquid fuel rockets in space travel and was instrumental in bringing about the design and construction of those rockets needed to implement those ideas.\n\nThough his work in the field was revolutionary, he was sometimes ridiculed by the public and in the press for his theories concerning spaceflight and therefore became protective of his privacy and his research work. Years after his death, as manned spaceflight finally became a reality, Goddard at long last came to be recognized as the man who pioneered modern rocketry and ultimately space exploration.\n\nOn October 5, 1964, the U.S. Post Office issued a postage stamp commemorating Robert Goddard. The stamp depicts an image of Goddard next to a rocket launching from the Kennedy Space Center. The Post Office released the stamp issues at a ceremony held in New Mexico. Goddard's wife, Esther Goddard, attended the ceremony. She was given the honor of pressing the button launching two rockets, one of which flew some mile and a half into the air. The two rockets each carried 1,000 first day covers, and after parachuting to the ground were recovered with the first day covers later sold to collectors. Designed by Robert J. Jones and printed on the Giori press, the quantities for this issue totaled more than 62 million.\n\nGemini IV was a June 1965 manned space flight in NASA's Gemini program. It was the second manned Gemini flight, the tenth manned American flight and the 18th spaceflight of all time (includes X-15 flights over ). It was crewed by James McDivitt and Ed White.\n\nThe highlight of the mission was the first space walk by an American, during which White remained tethered outside the spacecraft for 22 minutes. Tied to a tether, White fired his oxygen powered \"zip gun\" and floated out of the capsule. He traveled fifteen feet (five meters) out, and began to experiment with maneuvering. He found it easy, especially the pitch and yaw, although he thought the roll would use too much fuel.\nTwo 5-cent se-tenant stamps comprise one illustration of an astronaut during a space walk, honoring the space accomplishments of the United States. These issues were first placed on sale on September 29, 1967, at Kennedy Space Center in Florida.\n\nThe offset press and intaglio press were combined to produce this issue in sheets bearing one horizontal plate number. Offset printed the red stripes in the flags on the astronaut's spacesuit and capsule and light blue sky areas, as well as the inscription on the astronaut stamp. The Giori press printed dark blue sky areas, the aqua earth, and black tones on the capsule and astronaut. The inscription on the spaceship stamp was white.\n\nThis issue was designed by Paul Calle of Stamford, Connecticut, who based his design from photos taken on the Gemini IV mission. The issue was printed by the Bureau of Engraving and Printing. It was issued in panes of fifty, with an initial printing of 120 million.\n\nUp until the time of the Apollo VIII mission all manned ventures into space were confined to brief flights into space or to orbiting the Earth. Apollo VIII was the first human spaceflight mission to leave Earth orbit; the first to be captured by and escape from the gravitational field of another celestial body; and the first crew to voyage and then return to planet Earth from another celestial body – Earth's Moon. The three-man crew of mission Commander Frank Borman, Command Module Pilot James Lovell, and Lunar Module Pilot William Anders became the first humans to see the far side of the Moon with their own eyes, as well as the first humans to see planet Earth from beyond low Earth orbit.\n\nThe mission was accomplished with the first manned launch of a Saturn V rocket. Apollo VIII was the second manned mission of the Apollo Program.\nWhile orbiting the Moon each man on board read a section from the Biblical creation story (verses 1–10) from the Book of Genesis, and it is this unprecedented historical event that is theme of the Apollo VIII issue's design, the issue being inscribed with the words, \"In the beginning God...\" superimposed on the photograph \"Earthrise\", taken by Anders. Borman finished the broadcast by wishing a Merry Christmas to everyone on Earth.\n\nThis issue was first released in Houston Texas on May 5, 1969. Basing his design on the Anders' \"Earthrise\" photograph, and from the words they read from Genesis, Leonard E. Buckley designed the Apollo VIII commemorative issue of 1969. The issue was printed on the multi-color Giori Press. Quantities issued totaled more than 187 million.\nThe Apollo 11 mission landed the first humans on the Moon. Launched on July 16, 1969, the third lunar mission of NASA's Apollo Program was crewed by Commander Neil A. Armstrong, Command Module Pilot Michael Collins, and Lunar Module Pilot Edwin E. 'Buzz' Aldrin, Jr. On July 20, Armstrong and Aldrin became the first humans to walk on the Moon, while Collins orbited in the Command Module. The Apollo 11 mission reached President John F. Kennedy's goal of putting a man on the Moon's surface by the end of the 1960s.\n\nOn September 9, 1969, the U.S. Post office issued its first airmail stamp to depict a space exploration theme, the First Man on the Moon issue. The man depicted in the space suit is Neil Armstrong taking man's first step on the moon. This issue was designed by Paul Calle. The stamp's original master die was actually flown to the moon, and a letter with the stamp canceled on the way back. The quantities issued were more than 152 million making the issue quite common, and quite popular. Some issues are missing the red color resulting in the stripes of the flag emblem on Armstrong's arm being omitted making that issue quite scarce and expensive.\n\nThis 'Space Achievements' issue depicts the Earth, Sun, Lunar Module, the Lunar Rover and astronauts. Two 8-cent se-tenant stamps commemorating a decade of space achievements were placed on sale August 2, 1971, at Kennedy Space Center, Florida, and Houston, Texas. First day covers were postmarked at two different post offices (Houston, Texas and Huntsville, Alabama, location of the two tracking stations.) rather than the usual one because of extraordinary popularity of the space program at the time of issuance.\n\nThis issue was designed by Robert T. McCall of Paradise Valley, Arizona. Upon close examination of this issue one can see that it has an accurate depiction of the Lunar Rover, sitting on the Lunar surface. The Lunar landing module can also be seen in the background. This issue (pair) came in sheets of 50 (100 individual stamps), with an initial printing of 150 million.\n\nSkylab, a science and engineering laboratory, was the United States' first space station, and the second space station visited by a human crew. It was put into orbit by a Saturn V rocket on 14 May 1973. It was also the only space station NASA launched alone. The 100-ton space station was in Earth's orbit from 1973 to 1979 and it was visited by crews three times in 1973 and 1974.\n\nCircling 50 degrees north and south of the equator at an altitude of , Skylab had an orbital period of 93 minutes. There were a plethora of UV astronomy experiments done during the Skylab lifetime, as well as detailed X-ray studies of the Sun. The station was active until July 11, 1979, when it fell out of orbit.\n\nThe 10-cent Skylab commemorative stamp first day of release took place at Houston, Texas, on May 14, 1974. This issue commemorates the first anniversary of the launching of Skylab, and depicts the station as it was repaired, complete with \"umbrella\" and missing the lost solar panel. The stamp was designed by Robert T. McCall and was issued in sheets of fifty, with an initial printing of 140 million.\n\nIn March 1972, scientists at NASA launched \"Pioneer 10\" to gather scientific data about the solar system's largest planet, Jupiter, while the vessel was also receiving radio control and guidance signals and other information from Earth. The Pioneer space craft was expected to last for 21 months in the solar system and deliver accurate information over that period of time. The fastest man made object to enter space from Earth, the spacecraft was to begin collecting data at the Asteroid Belt and Jupiter and continue to relay information about other areas and phenomena of the solar system.\n\nAfter \"Pioneer 10\" passed through the asteroid belt, \"Pioneer 11\", was launched on a similar trajectory.\n\n\"Pioneer 10\" became the first spacecraft to get close enough to Jupiter to send back revelations about the properties and phenomena of the solar system's largest planet.\n\nThe 10-cent Pioneer commemorative stamp was issued on February 28, 1975, at Mountain View, California, and paid the domestic first-class rate for letters weighing less than one half ounce. Designed by Robert T. McCall and printed at the Bureau of Engraving and Printing on the Giori presses, the stamp shares numerous design elements with USA Scott 1557, the \"Mariner 10\" Issue of 1975.\n\nEngineered to explore the orbits of Venus and Mercury, \"Mariner 10\" launched in November 1973. The mission's two- year plan used the gravitational pull of Venus to reach Mercury. The probe also used solar winds to help with locomotion when fuel ran low. \"Mariner 10\" orbited the planets in the opposite direction of Earth's orbit.\n\nMariner's first photographed images, which revealed Venus's dense cloud cover, reached NASA scientists in February 1974. Data collected by \"Mariner 10\" helped determine Venus's rotation period and use of Earth's magnetic field. Photos of Mercury revealed its cratered surface and its large scarps and plains. Research also recorded its radical temperature variations between night and day.\n\nThe 10-cent \"Mariner 10\" commemorative stamp was issued on April 4, 1975, at Pasadena, California. It paid the domestic first-class rate for letters weighing less than a half ounce. The stamp shares numerous design elements with USA Scott 1556, and while they have different designers and dates of issue, the two are cataloged together under the heading \"U.S. Unmanned Accomplishments in Space.\"\n\nDesigned by Roy Gjertson, the \"Mariner 10\" commemorative postage issue was printed at the Bureau of Engraving and Printing in multi-color on the Giori presses.\nQuantities issued totaled more than 158 million.\n\nWith the Apollo–Soyuz mission, two nations collaborated on a space project for the first time. In July 1975, the United States launched the manned Apollo Command module to rendezvous with Russia's manned Soyuz module. A special docking station facilitated interaction among the astronauts. Television stations worldwide broadcast the historic docking and bi-national greeting made by the astronauts. The modules remained docked for two days, during which the teams performed numerous experiments.\n\nThe U.S. Postal Service issued this se-tenant pair of two 10-cent multicolored stamps on July 15, 1975, at Kennedy Space Center, Florida. Inexplicably, the circular program insignia on the left-hand stamp is rotated to the Soviet configuration, showing the red Soyuz section on the left. The Soviet Union also released stamps of similar design (Russia Scott 4339–4340) at the same time. This denomination paid the domestic first-class rate for letters weighing less than half ounce.\n\nRobert T. McCall designed the 'after link-up' image, and Anatoly Aksamit designed the 'before link-up' image. The Bureau of Engraving and Printing produced the stamps on the Andreotti press in sheets of ninety-six stamps, panes of twenty-four. Quantities issued for this pair of postage issues totaled more than 161 million.\n\nNASA's Viking program consisted of a pair of space probes sent to Mars, \"Viking I\" and \"Viking II\". Each vehicle was composed of two main parts, an orbiter designed to photograph the surface of Mars from orbit, and a lander designed to study the planet from the surface. The orbiters also served as communication relays for the landers once they touched down.\n\nUpon close examination one can discern the engraving of the landing component used to collect soil samples on Mars, the radio dish, landing gear and other equipment rendered by the engraver in this issue. The U.S. Postal Service released this issue at Hampton, Virginia, on July 20, 1978, the second anniversary of the Viking I lander's descent on to the Martian surface.\n\nThe 15-cent denomination paid the new domestic rate that had recently increased from 13-cents two months earlier for a first-class letter weighing less than half ounce. It was the second such stamp issued to fulfill that rate. Designed by Robert T. McCall, the issue was printed at the Bureau of Engraving and Printing on the Giori presses in sheets of 200 stamps which were then cut into panes of fifty stamps for sale at post offices.\n\nFirst day of issue ceremony took place July 20, 1978, at Hampton, Va. Quantities issued totaled more than 158 million.\n\nOn May 21, 1981, the Post Office released the Space Achievement commemorative issue, 18-cent stamps, in a block of eight format, one image being shared by four individual stamps, with four more aside them depicting various space exploration imagery. First day of issue occurred at the Kennedy Space Center, Florida. The four central stamps depict the Space Shuttle in its various modes of operation. The central theme of this issue involves the first actual 'in space' flights of the Space Shuttle and also pays tribute to twenty years of U.S. manned space exploration. The other stamps to the left and right sides are honoring the efforts of those who partook in the historical Moon walk, Skylab, and \"Pioneer 11\" missions. The images were modeled by Clarance Holbert. The stamp design by Robert T. McCall.\n\nThe U.S. Space Shuttle stamp of 1995 depicts the famous liftoff of the Space Shuttle \"Endeavour\", Mission STS-57, which took place on June 21, 1993, from Kennedy Space Center, Florida. This issue was designed by Phil Jordan of Falls Church, Virginia. As an extra security measure, elliptical perforations were used on sheets of stamps to identify genuine issues from those that might be counterfeited. The postage stamps were manufactured by Ashton-Potter (USA) Ltd. in the offset/intaglio process.\n\nOn June 22, 1995, in Anaheim, California, the USPS commemorated the Space Shuttle \"Challenger\", STS-7, with the issuance of a 3-dollar postage stamp. Again designed by Phil Jordan, the stamp features the \"Challenger\" Space Shuttle. The name of the Space Shuttle was inscribed in microtype and secretly blended into the design matrix to satisfy concern about matters of security. This was the first time that the Post Office used an actual photograph from a NASA mission, instead of an illustration based on a photograph. This issue was also printed by Ashton-Potter USA, Ltd., on the offset-intaglio printer.\n\nOn November 19, 1998, in New York City, the Post Office issued the Space Shuttle Piggyback $11.75 definitive Express Mail postage stamp. With all the fanfare surrounding the United States space program, this ceremony was in conjunction with the special First Day of Issue grand event that in New York that year. This issue was also designed by Phil Jordan and was printed by the Banknote Corporation of America.\n\n\n"}
{"id": "905653", "url": "https://en.wikipedia.org/wiki?curid=905653", "title": "Western saddle", "text": "Western saddle\n\nWestern saddles are used for western riding and are the saddles used on working horses on cattle ranches throughout the United States, particularly in the west. They are the \"cowboy\" saddles familiar to movie viewers, rodeo fans, and those who have gone on trail rides at guest ranches. This saddle was designed to provide security and comfort to the rider when spending long hours on a horse, traveling over rugged terrain.\n\nThe design of the Western saddle derives from the saddles of the Mexican \"vaqueros\" - the early horse trainers and cattle handlers of Mexico and the American Southwest. It was developed for the purpose of working cattle across vast areas, and came from a combination of the saddles used in the two main styles of horseback riding then practiced in Spain — \"la jineta\", the Moorish style which allowed great freedom of movement to the horse; and \"la estradiota\", later \"la brida\", the jousting style, which provided great security to the rider and strong control of the horse. A very functional item was also added: the saddle \"horn.\" This style of saddle allowed vaqueros to control cattle by use of a rope around the neck of the animal, tied or dallied (wrapped without a knot) around the horn.\n\nToday, although many Western riders have never roped a cow, the western saddle still features this historical element. (Some variations on the Western saddle design, such as those used in bronc riding, endurance riding and those made for the rapidly growing European market, do not have horns.) Another predecessor which may have contributed to the design of the Western saddle was the Spanish tree saddle, which was also influential in the design of the McClellan saddle of the American military, being used by all branches of the U.S. Army, but being particularly associated with the cavalry.\n\nThe Western saddle is designed to be comfortable when ridden in for many hours. Its history and purpose is to be a working tool for a cowboy who spends all day, every day, on horseback. For a beginning rider, the western saddle may give the impression of providing a more secure seat. However, this may be misleading; the horn is not meant to be a handle for the rider to hang onto, and the high cantle and heavy stirrups are not for forcing the rider into a rigid position. The development of an independent seat and hands is as critical for western riders as for English riders.\n\nThe modern western saddle begins with a \"tree\" that defines the shape of the bars, the seat, the swells, horn, and cantle. Traditional trees are made of wood covered with rawhide, coated with varnish or a similar modern synthetic coating. In some cases, the core of the horn may be of metal. Modern synthetic materials of various types have also been used instead of wood, but while lighter and less expensive, are generally considered weaker than traditional materials, some, such as fiberglass, dangerously so. A high-quality tree is at the heart of a good saddle, particularly those used for sports such as steer roping, where the equipment must withstand considerable force.\n\nThe tree is usually covered with leather on all visible parts of the saddle. The seat may have foam rubber or other materials added between the tree and the top layer of leather to provide additional comfort to the rider, and leather or foam padding may be used to slightly alter the contours of the seat. Sheepskin is placed on the underside of the saddle, covering both the tree and the underside of the skirts. The cinch rings, made of metal, are attached to the tree as described under \"Rigging,\" \"below.\" For decoration, metal conchos, lacing, and small plates, usually silver or a silver-like substitute, are added.\n\nThe leather parts of the saddle are often tooled into designs that range from simple to complex. The finest-quality saddles often have hand-carved tooling that itself is considered a work of art.\n\nThe Western saddle is different from an English saddle in that it has no padding between the tree and the external leather and fleece skirting. The weight bearing area of the saddle is large and usually covered with sheepskin, but it must be padded with a saddle blanket in order to provide a comfortable fit for the horse. Western saddles are extensively decorated—the carved leatherwork is often a true work of art—and intricately carved silver conchos and other additions are frequently added to the saddle for show purposes. More than any other style of saddle, the western saddle can be customized to be a true expression of the rider's taste and style. A fine quality western saddle, properly maintained, is intended to last for a person's lifetime, or even beyond.\n\nOther differences between the Western and English saddles include:\n\n\nWhile a western saddle is designed to be ridden for many hours at a stretch; for covering distance where time is a factor, such as with Endurance riding, the lighter English saddle dominates.\n\nThere are many types of Western saddle available. Some are general-purpose models while others emphasize either greater freedom for the horse or greater security for the rider, as may be necessary for specialized work in the various Western horse sports such as cutting, reining, barrel racing, team roping, equitation and western pleasure. Factors such as width of the swells, height of the cantle, depth of the seat, placement of the stirrups and type of rigging all influence the uses of a given design. For example, a saddle with wide swells, high cantle and deep seat is suitable for cutting, where a rider must remain in a secure, quiet seat on the horse. At the other end of the spectrum, a saddle with a \"slick fork\" - virtually no swells - and a low cantle is suited for calf roping, where a rider must dismount quickly, often while the horse is still in motion, and not be caught up on the saddle.\n\nThe most common variations include the following:\n\nThere are many variations of design and optional equipment elements that were influenced by geographic region, history, use and the body types of horses bred in a given area. Certain stylistic elements seen on some, but not all western saddles include:\n\nThere are several different sizes of trees commonly found in saddles. Trees differ in the width of gullet and bars of the saddle, pitch of the bars (steep to flat, usually between an angle of 86 to 94 degrees with 90 being common), and length of the bars. The tree also influences the shape of the pommel and cantle on the seat on the saddle, though the seat can be altered to fit a rider by adding padding and other materials to a far greater degree than the fit of the saddle tree's bars on a horse. A wider gullet sits lower on the horse, while a narrow gullet sits higher and is designed to fit horses with higher withers. The bars form the primary loading surface of the saddle as it site on the horse's back. A horse with a flat back and widely sprung ribs will require bars with a flatter pitch than a saddle made for a narrow horse, where a steeper pitch to the bars will keep the saddle placed properly. Most saddles are made with pre-manufactured trees which come in a limited range of sizes. Custom-made saddles may be able to have further alterations made to a standard tree.\n\n\nSaddle rigging refers to the arrangement of rings and plate hardware that connects the billets and girthing system that holds the saddle on the horse. Western saddle rigging can be either single or double. The front rigging consists of metal \"cinch rings\" on each side of the saddle to which a long, wide strap called a latigo is attached for holding the front cinch that goes around the heart girth of the horse, just behind the elbows. The back cinch is placed around the widest part of the horse's barrel, and is attached to the saddle either by reinforced slots in the leather skirting of the saddle, or, in particularly heavy-duty models, to a second set of rings.\n\nThe front cinch is secured to the saddle by means of a latigo on the left, and on the right, by either a latigo or a billet. Latigos are not removed until worn out or broken. They run through the ring or buckle of the cinch (also called a cinch ring), and back to the rigging, sometimes multiple times for extra security. Modern latigos have several holes at the end so that a cinch can be buckled at a set tension, though the cinch may also be secured by a knot called a \"latigo knot,\" which is a type of half-hitch. The off-side billet is a shorter, doubled piece of leather with holes along its length, somewhat heavier and less flexible than latigo leather. It runs through the rigging cinch ring and both ends buckle onto the cinch. Older saddles may use a latigo on the off side, but this is less common. Once adjusted to the horse, an off-side latigo or billet is seldom disconnected from the cinch, which remains attached to the saddle until it needs to be replaced, unlike the girth of an English saddle, which is to be removed on both sides when not in use. While leather is preferred for latigos, nylon web is sometimes used, particularly on cheaper saddles, though it is prone to slip when knotted and the holes may tear more easily.\n\nWhen used, a back cinch, made of several thicknesses of leather, is held on by a simple heavy leather billet on each side of the saddle that buckles just tight enough to touch the underside of the horse, but not tight enough to provoke discomfort or bucking. At the belly midline, the front and back cinches are joined by a light belly strap, called a cinch hobble, that prevents the back cinch from moving too far back.\n\nA saddle that has only a cinch in the front is \"single rigged\". A saddle that has both a front cinch and a back cinch (sometimes called a flank cinch, even though it should never go around the horse's flanks) is \"double rigged\". The rear rigging is meant to stabilize the saddle. The back cinch is always located just below the cantle and held in place with a cinch hobble to prevent it from slipping back; however, the position of the front rigging varies. The rig positions are named by how far they are from the cantle to the fork. Placement of the front rigging is a critical component of western saddle design. The closer the rigging is to the center of the saddle, the more the rider will be balanced over the horse's center of balance, allowing freer movement and agility of horse and rider. On the other hand, the more forward the rigging is set, especially when combined with a back cinch, the more the saddle will set down on the horse, placing the rider a bit behind the horse's center of balance, but creating greater security.\n\n\nCustom built saddles may be designed with any of the above rigging styles. Modern western saddles for riders who need speed and agility, such as barrel racing saddles, often have a 3/4 rigging, the closest placement to a center-fire rigging seen on modern saddles. The most popular modern rigging placement is the 7/8 rigging, which allows a rider to have a secure seat but more easily stay centered over a horse's center of balance and is often seen on saddles used for western equitation. A \"full double\" rigging is seen most often on saddles used for team roping, where the weight of the steer puts tremendous forward stress on the saddle, requiring rigging set well forward and both a front and back cinch to support the saddle. A few saddles are built with a three-way rigging plate that allows a saddle to be rigged in the full, 7/8 or 3/4 positions.\n\nThe front rigging is attached to the saddle in one of three ways: ring, flat plate or in-skirt. Ring rigging is made of rings on heavy leather straps attached directly to the saddle tree. This is the strongest attachment method, but a disadvantage is that it creates bulk under the legs and inhibits the free swing of the stirrups. The second style of attachment is the flat plate. This type has leather layers that are riveted around a metal plate and attached directly to the tree of the saddle. This is also a very strong type of ring attachment that reduces bulk under the leg and does not inhibit the swinging of the stirrups, though it is not as strong as ring rigging.\n\nThe third style is the in-skirt, where the rings or plates are attached directly to the saddle's skirt. The advantage of having an in-skirt rigging is that it provides the least amount of bulk under the leg compared to the other styles of attachment. Two variations exist, the built-in and the built-on. The \"built-in\" rigging design makes the attachment of the rings very strong by sandwiching the rings with layers of leather and then sewing and riveting them to the skirt. This design is strong enough for pleasure riding but not for roping. It has the least amount of bulk under the leg and is popular for show saddles. A \"built-on\" rigging attaches the plate to the surface of the skirt, a weak placement of low quality.\n\nHistoric saddles of the 19th century had rigging rings made of forged iron round stock, which would rust if it was exposed to the horse's sweat. The iron oxide would degrade and rot the leather that it came in contact with, which caused the rigging straps that were held by the rings to break. In order to correct this problem, saddle makers covered the metal rings in 4-5 ounces of medium thickness belly leather. This was a common remedy for the problem until approximately 1915, when brass rigging hardware became more common.\n\nThe Goodnight western sidesaddle that was developed in the 1870s by Charles Goodnight for his wife was a double rigged design. Goodnight developed this sidesaddle because there was a need to produce a woman's saddle for daily riding and work on the range. The saddle also had to fit a variety of horses on a day-to-day basis. it required two cinches. The cinches have a connecting strap, called a cinch hobble, to keep the rear girth from slipping back, which would cause the horse to buck. The girthing system still produced a shimmy in the rear, even though the rear cinch was brought up snugly against the horse. The double girthing system was not as secure as the balance strap seen on many modern sidesaddles.\n\n\n\n"}
